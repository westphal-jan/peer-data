{"id": "1611.05607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2016", "title": "Optical Flow Requires Multiple Strategies (but only one network)", "abstract": "We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.", "histories": [["v1", "Thu, 17 Nov 2016 08:31:56 GMT  (8096kb,D)", "http://arxiv.org/abs/1611.05607v1", null], ["v2", "Sun, 29 Jan 2017 22:37:31 GMT  (1640kb,D)", "http://arxiv.org/abs/1611.05607v2", null], ["v3", "Thu, 2 Feb 2017 10:52:03 GMT  (1640kb,D)", "http://arxiv.org/abs/1611.05607v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["tal schuster", "lior wolf", "david gadot"], "accepted": false, "id": "1611.05607"}, "pdf": {"name": "1611.05607.pdf", "metadata": {"source": "CRF", "title": "Optical Flow Requires Multiple Strategies (but only one network)", "authors": ["Tal Schuster", "Lior Wolf", "David Gadot"], "emails": ["talschuster@gmail.com,", "wolf@cs.tau.ac.il,", "dedigadot@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "In many AI challenges, including perception and planning, one specific problem requires multiple strategies. In the computer vision literature, this topic has gained little attention. Since a single model is typically trained, the conventional view is that of a unified, albeit complex, solution that captures all scenarios. Our work shows that careful consideration of the multifaceted nature of optical flow leads to a clear improvement in performing this task.\nIn optical flow, one can roughly separate between the small- and the large-displacement scenarios, and train model to apply different strategies to these different cases. The small displacement scenarios are characterized by relatively small appearance changes and require patch descriptors that can capture minute differences in appearance. The large displacement scenarios, on the other hand, require much more invariance in the matching process.\nState of the art methods in optical flow employ metric learning in order to learn the patch descriptors. We focus on the process of selecting negative samples during training and suggest two modifications. First, rather than selecting negative samples at random, we select negative samples that match the amount of displacement that the true match (the positive sample) undergoes, as is illustrated in Fig. 1. Second, we suggest gradually increasing the difficulty of the\nnegative samples during training. In the implementation of the second component, scheduling samples by difficulty, we combine two methods well known in the literature. The curriculum learning method [6] selects samples, stratified by difficulty, using a predefined order. The method of self-paced learning [24] identifies a set of easy samples by their loss, and learns using only those samples. The amount of samples defined as easy is increased over time. The Self-Paced-CurriculumInterleaving method we propose here combines in the selection process both the difficulty of a sample and its loss. However, in difference from the self-paced method, no samples are excluded during training. Instead, we control the level of the difficulty of instances used for training by selecting negative samples of appropriate distances.\nThe pipeline employed for computing optical flow is similar to the PatchBatch method [13]. We slightly mod-\nar X\niv :1\n61 1.\n05 60\n7v 1\n[ cs\n.C V\n] 1\n7 N\nov 2\n01 6\nify it by replacing the DrLIM loss with a Hinge loss. Our main contributions in this work are:\n\u2022 We analyze, for the first time, the need for multiple strategies in optical flow.\n\u2022 We propose a novel, psychologically inspired way to train a network to address multiple scenarios at once.\n\u2022 We show how, in optical flow, our proposed new scheme translates to a simple, unexpected, heuristic.\n\u2022 We improve the PatchBatch[13] pipeline itself. \u2022 State of the art results are demonstrated on the KITTI\n2012 and KITTI 2015 benchmarks."}, {"heading": "2. Related work", "text": "Many computer vision tasks require a pixel-wise image comparison (e.g. image retrieval, object recognition, multi-view reconstruction). To allow for the comparison to be invariant to scale, rotation, illumination, etc., image descriptors such as SIFT [28], SURF [5], HOG [10], and DAISY [35] have been used. Brox and Malik were the first to apply local descriptors to the problem of dense optical flow [7]. They found that the use of descriptors enables better performance for large displacement matching, but that the obtained solution has many outliers due to missing regularization constraints. In order to account for this, they used descriptors to build a sparse initial flow and interpolate it to a dense one using image smoothness assumptions. Following their success, many other models adopted the use of local descriptors [39, 30, 20, 34].\nWith the advent of deep learning methods, CNNs were shown to be extremely powerful in the related problem of stereo matching [33, 41]. For optical flow, a few CNN based models were proposed. In [37], a CNN is used to predict the flow from a single static image. FlowNet [11] is the first end-to-end CNN for optical flow and showed competitive results. In the PatchBatch [13] pipeline, a CNN was used for extracting patch descriptors that are then used for matching via the PatchMatch [4] Nearest Neighbor Field (NNF) algorithms. It achieved state of the art performance in the KITTI benchmarks [15, 29] as of last year.\nWhile the use of descriptors has greatly improved overall performance and accuracy, methods keep failing with large displacements, as we further discuss in Section 4. To solve this problem, extensive efforts have been devoted to methods for the integration of descriptors with local assumptions [7, 34, 30]. However, much less work was done in making the descriptors themselves more suitable for this scenario. A concurrent work [3], focused on decreasing the error for large displacements by down-sampling patches and adding a threshold to the loss function. However, this comes at the cost of reducing the accuracy obtained for small displacements.\nIn our work, we follow the PatchBatch pipeline and use a CNN to extract descriptors. We expand the work by analyzing different matching cases, specifically those of small and large displacements, and present a method for generating better matching descriptors for both cases."}, {"heading": "2.1. Learning for multiple strategies", "text": "The need for multiple strategies was found in several vision problems where the basic trained model could not optimize the solution for all sub-categories. An example is the work of Antipov et al. [1] for age estimation. Unsatisfied by the accuracy of the model for children of age 0-12, they train a sub-model only for those ages and employ it to samples that are classified as this category by another model that is run first.\nAnother common case is in fine-grained classification, e.g. determining the exact model of a car or a particular species of bird. The subtle differences between nearby species require, for example, to focus on specific body regions. However, different distinctions require different body parts and we can consider each body part as a separate decision strategy.\nIn order to achieve the required accuracy, some methods perform object segmentation [23] or part detection [22] to limit the search of each sub-class to the most relevant body parts. A different approach was shown in [14], where several models were trained on different samples to create per class expert models. At test time, the answer with the highest confidence is chosen. The latter approach achieved better results due to each model leveraging all of the input data, and learning individually the required features to gain expertise in its task."}, {"heading": "2.2. Learning for varied difficulty levels", "text": "Curriculum learning [6], inspired by the learning process of humans, was the first method to manipulate the order of samples shown to the model during training. Specifically, it is suggested to present the easy training samples first and the harder samples later, after performing stratification based on the difficulty level.\nIn self-paced learning [24], instead of using a predefined order, the difficulty of each sample is dynamically estimated during training by inspecting the associated loss. On each epoch, only the easier samples are being learned from and their amount is increased with time until the entire data is considered. In the work of [19], those two methods were combined to allow a prior knowledge of samples difficulty to be considered in the self-paced iterations.\nIt was recently proposed to eliminate from the training process samples that are either too easy or too hard [36]. For this purpose, specific percentiles on the loss were employed. Samples which did not meet the loss criteria were put aside for a predefined number of epochs.\nIn the problem of optical flow, large displacements are known to be more challenging. Moreover, as we show in Section 4, the descriptor extraction strategy should differ by displacement. Due to the correlation between the difficulty level and the required strategy, applying the existing gradual learning methods could result in acquiring specific strategies in different training stages with the possibility of unwanted carryover. In Section 5, we suggest novel learning techniques, which use all samples, support different strategies and apply an easy to hard order."}, {"heading": "3. The PatchBatch pipeline", "text": "The PatchBatch (PB) pipeline, as described in Fig. 2, consists of a CNN which generates per-pixel descriptors and an approximate nearest neighbor algorithm which is later used to compute the actual assignments. PatchBatch\u2019s ACCURATE network configuration generates descriptors with 512 float values. The assignment is computed by minimizing the L2 distance between descriptor vectors. To create each pixel\u2019s descriptor, the CNN uses a patch as an input. In most of the CNN configurations described in PatchBatch, the input is a 51 \u00d7 51 patch centered around the examined pixel. The CNN uses the grayscale data of the patch to extract a descriptor as similar as possible to the one extracted for the matching pixel on the second image.\nUsing the generated descriptors, PatchMatch [4] (PM) algorithm is used to compute initial flow assignments. PM is applied in both flow directions and is followed by a bidirectional consistency check that allows elimination of nonconsistent matches.\nIn the final step, the sparse-to-dense EpicFlow [32] (EF) algorithm creates the final estimation using the sparse flow and the original raw images. We refer the reader to the\nPatchBatch [13] paper and the published code1 for a more detailed description."}, {"heading": "3.1. Architecture improvements", "text": "In this paper, we improve the CNN that generates the descriptors. We achieve this by several means. First, we adopt the suggestion, that was partially tested in the original PB paper [13], to enlarge the patch size from 51\u00d751 to 71\u00d771 pixels. Second, to improve the training of the network we use two novelties: (1) We introduce a new learning method for multiple displacements detailed in Section 5. (2) We modify the loss function and use a new form of the Hinge loss. Third, we altered the initial random guess range of the PM algorithm on MPI-Sintel to be 100 instead of 10, to allow larger search distance and better utilization of our large displacements descriptors. For the KITTI benchmarks, this parameter remained unchanged (500)."}, {"heading": "3.2. Hinge loss with SD", "text": "Instead of the DrLIM [17] loss functions used in PatchBatch, we found the Hinge loss to achieve best results when integrated with our, further detailed, learning method. To allow the use of this loss, we construct the samples as triplets. For each patch, we collect a matching patch by the ground truth and a non-matching one. As a baseline, we use the same non-matching collecting method, which is a random patch up to 8 pixels from the matching one.\nWe define the loss function as:\nLH = 1\nn n\u2211 i=1 max(0,m+Di,match \u2212Di,non\u2212match)\n(1) where D is theL2 distance of the examined patch descriptor from the matching or non-matching one.\nIn the PatchBatch paper, an addition of a standard deviation parameter was found to produce better distinction between matching and non-matching samples. With that inspiration, we apply a similar addition to the Hinge loss:\nLH+SD = \u03bbLH + (1\u2212 \u03bb)(\u03c3Dmatch + \u03c3Dnon\u2212match) (2)\nWe used m = 100, \u03bb = 0.8 and a training set of n = 50k triplets for each epoch."}, {"heading": "4. Optical flow as a multifaceted problem", "text": "It is clear by examining the results of the common optical flow benchmarks that optical flow methods are challenged by large displacements. In the MPI-Sintel [8], where results are separated by the velocity of pixels, the current average end-point-error (EPE) of the top 10 ranked methods is 35.47\n1https://github.com/DediGadot/PatchBatch\nfor velocities higher than 40, while it is about 1.01 for velocities lower than 10. In KITTI2015 [29], there is no published estimation by velocity. However, there is separation of foreground vs. background regions. The current average outliers percentage for the top 10 methods is 26.43% for foreground versus 11.43% for background, which, assuming foreground objects typically move faster than background, supports the same observation. When evaluating the baseline PatchBatch model on a validating set, we notice an error (percent of pixels with euclidean error > 3) of 4.90% for displacements smaller than 10 and 42.15% for displacements larger than 40.\nThe challenge of matching at larger distances is exemplified in Fig. 3, which shows the L2 distance of the true match as a function of the ground truth displacement. Furthermore, as the distance increases, the average number of distractors in the second image, with higher similarity to patch in the first image than the true match, increases. This counting is performed in a radius of 25 pixels around the true match and is shown in Tab. 1 under the Baseline training set."}, {"heading": "4.1. Multiple strategies", "text": "When training the PatchBatch network only on displacements that are smaller than 30, we are able to improve most cases of small displacements, while, in most cases increasing the number of nearby distractors for large displacements. Conversely, training only on displacements larger than 30 pixels, achieved a lower amount of distractors for large displacements (Tab. 1). However, since there is no\nmechanism for selecting between the two networks, it is best to train one network that addresses both scenarios. Interestingly, when training just one network on all samples, the network seems to outperform the two specialized networks in the domain of very small displacements. This is probably a result of designing the PatchBatch method to excel in benchmarks that emphasize this category.\nLarge displacements are typically associated with larger differences in appearance, as demonstrated in Fig.4. Differences in the patch appearance for the small displacement case typically arise from objects moving within the patch faster than the middle pixel. In contrast, in large motions, we can expect much more pronounced changes in appearance due to the following: (1) As fast objects move, their background is more likely to change. (2) The view point changes more drastically, which leads to different object parts being occluded. (3) The distance and angle to light sources vary more quickly, leading to a change in illumination. (4) When a significant displacement occurs along the Z-axis of the camera, the object changes in both position and scale."}, {"heading": "5. Learning for multiple strategies and varying", "text": "difficulty\nAs baseline methods, we apply gradual learning methods from the literature. For applying curriculum learning [6], the samples need to be stratified by difficulty prior to training. Followed our previous findings, we define the difficulty level as the displacement value in the ground truth and\nincrease the maximum displacement of the sample pool in each epoch which we call curriculum by displacement.\nAnother curriculum implementation, which we call curriculum by distance, would be to use samples with all displacement values for each epoch, and to start the training using false samples that have a large euclidean distance in the image from the true matching. Decreasing that distance with training should provide harder false samples with time.\nWe also implement a self-paced model by learning only from the easy samples in each epoch. Easiness here is measured per sample by requiring a loss that is lower than a threshold. This threshold is increased as training progresses."}, {"heading": "5.1. Interleaving learning", "text": "Both the curriculum learning approach as well as the self-paced one utilize the difficulty diversification of the samples and suggest to learn from easy to hard. While this idea might seem appealing, and does work in many machine learning problems, it could cause the network to become overly adapted to different aspects of the problem at different training stages. In optical flow, models must excel in the low displacement task in order to be competitive. Therefore, the shift of attention to harder and harder tasks is potentially detrimental. In addition, if different strategies are required, the carryover from the easy task to the more challenging ones is not obvious.\nOur approach is motivated by psychological research. Kornell and Bjork, psychology researchers, found that for some cases, interleaving exemplars of different categories can enhance inductive learning [21]. Their tests showed that people learn better to distinguish classes, e.g. bird species, by learning in an interleaving sample order rather than blocks of the same class. Another example would be sports training, in which it is common to interleave simple basic exercises with more complex ones, incorporating at least part of the complex movements from very early, and going back to the basic movements even after these are mastered.\nInspired by their research, we present an interleaving learning method for machine learning. The idiomatic way of training ML models is to randomize the feeding order of the samples. When perceptual strategies and difficulty levels are unrelated, the random process might be sufficient. However, when the samples that require some strategy A are consistently harder than the ones required for strategy B, the frequent loss related to the samples associated with A would mean that the strategy B would be deprived of a training signal.\nTo preserve a random order of strategies, and, at the same time, facilitate the penalty of harder samples, we suggest that the learning process should consider the difficulty of each sample. This could be done by either taking the difficulty of the sample into account while computing the\npenalty or, when training by pairs or triplets of samples, by controlling the composition of these small reference groups."}, {"heading": "5.2. Interleaving learning for optical flow", "text": "The implementation of our model was done by using further patches as false samples for larger displacements. Thus, for the harder case of large displacements, we select false samples that should be easier to distinguish from the true ones and normalize the overall difficulty. From the strategy point of view, by presenting further away negatives for large displacements, the model learns to rely more on context and less on appearance changes for large displacements and conversely for small ones.\nThe chosen false sample distance is determined by:\nd = v(1\u2212X) X \u223c logN (\u00b5, \u03c3) (3)\nP (X = x) = 1\n\u03c3x \u221a 2\u03c0 e(\u2212\n(ln(x)\u2212\u00b5)2\n2\u03c32 ) (4)\nwhere v is the displacement of the matching pixels and X is sampled from a log-normal distribution [31].\nUsing a log-normal distribution, allows us to take samples mostly relative to the exemplar motion while also providing a small amount of harder samples. We used \u00b5 = 0 and \u03c3 = 1 as parameters and after sampling values for all of the batch samples, they were normalized to [0, 1].\nTo implement this method in our learning process, we collect the false sample along the line connecting the original and the destined coordinates of the patch. Specifically, we randomly select a sample from a radius of up to 8 pixels from the point with distance d from the true match on that line, in the direction of the position in the first image (see Fig. 5). Interestingly, for the purpose of creating dual strategy descriptors, it does not matter whether the samples are from along the motion line. However, in our experiments, it turned out that sampling this way slightly helps the subsequent PM step. This is probably because PM initially searches in a random distance from the original patch position. By taking a false match that is closer to the original location, we help eliminate those samples."}, {"heading": "5.3. Self-Paced Curriculum Interleaving learning", "text": "Given the interleaving learning method, which, unlike curriculum learning employs all samples at once, we can expand it by adding a dynamic control on the difficulty level. In order to maintain the category diversity, we simply modify the distance equation for epoch i to:\ndi = v(1\u2212X \u2212Ri) (5)\nwhere Ri is define as:\nRi = i\nm\ufe38\ufe37\ufe37\ufe38 curriculum\n\u00b7max(0, 1\u2212 li\u22121 linit\n)\ufe38 \ufe37\ufe37 \ufe38 self-paced\n(6)\nand m is the total epoch amount, li is the loss on epoch i and linit is some initial loss to compare. We took linit as the loss from epoch number 5. Until that epoch, self-pacing is not applied.\nThe curriculum addition enhances the global difficulty of false samples in each iteration by shorting the taken distance and, therefore, integrates an instructor-driven approach assuming the student will handle more difficult tasks with time. To add a student-driven portion, we use the self-paced component which allows a feedback from the model to influence the difficulty of the next iteration. Integrating all of this together, we get a learning method that learns all strategies simultaneously and in which the difficulty is increased over iterations and with a success feedback."}, {"heading": "6. Experiments", "text": "We perform two families of experiments. First, MNIST recognition experiments are presented as a testbed for the gradual learning schemes. Then, the main set of experiments is performed on the specific problem of optical flow."}, {"heading": "6.1. MNIST", "text": "In order to validate our learning methods on a task different from optical flow, we used the MNIST handwritten digit database [25]. This data set consists of images showing a digit from 0 to 9 with their true label. We divided the data into two different classes \u2013 class L contains digits 0..4 and class H contains 5..9 . To enable difficulty differentiation between samples, random noise was added to the top half of the images of H and to the bottom part of the L images. Furthermore, images from class H were rotated by a random angle of [0, 45] degrees with correlation to the noise amount, such that, samples that are more noisy are also rotated in larger angles.\nWhile referring noisier samples as harder, we trained a model using several methods. As curriculum learning, harder samples were added to the training pool in each epoch. In the self-paced model, the hardness of the samples to learn from was derived from the loss. Interleaving was implemented by using all of the noise range level in each epoch with a fewer noised samples for the harder H class against more for L class. An integration of interleaving with Curriculum and Self-Paced methods was also used by increasing the the amount of the noised H samples in each epoch. Finally, combining all three methods, as can be\nseen in Tab. 3, achieved the best results."}, {"heading": "6.2. Optical flow", "text": "To evaluate our work, we use the three most competitive optical flow benchmarks - KITTI2012 [15], KITTI2015 [29] and MPI-Sintel [8]. We use their data to conduct a series of experiments to measure the effect of each of our contributions and to submit our best results to compare with other methods.\nBy training the different models on a subset of 80% from the KITTI2012 dataset for 500 epochs and testing the results on the remaining 20% image pairs, we show a comparison of the models summarized in Tab. 2. Note that lower PatchMatch (PM) error is not always correlated with lower EpicFlow (EF) error because of the bidirectional consistency check that excludes some inconsistent results to generate a sparse flow as an input for EF.\nObserving Tab. 2, one can notice that the use of the Hinge loss instead of CENT [13], improved the PM results and has no such effect on the final EF output. However, combining with the batch standard deviation term (SD) and our interleaving learning (Inter) leads to an advantage of the Hinge loss. Our interleaving learning method outperforms both Curriculum learning and Self-Paced learning. The SPCI technique contributes an additional improvement. Integrating all of our architecture modifications with SPCI produces the lowest error percent on the validation set with a major improvement on the initial baseline. Moreover, the amount of nearby distractors with descriptors that are more similar to the original patch than the true match is reduced in more than a third from the baseline.\nAs a sanity-check experiment we evaluate an AntiInterleaving method. In this method, negative matches from different ranges were also used. However, the ratio was inverted \u2013 true matches of small displacements were matched with false samples with large distances and vice versa. The high error of this model, as can be seen in Tab. 2, implies that the use of different ranges for false matches was not the\nmain benefit of the interleaving method and it is the correlation with the displacement values that is the crucial factor."}, {"heading": "6.2.1 Sensitivity to appearance change", "text": "Part of what the networks learn is to behave differently to patches with different expected displacements. Those patches that are similar to patches that are associated with small displacements are treated differently than those which were associated, in the training set, with large displacements. To illustrate this, and compare the various learning methods, we explore the model behavior on nearby patches from the same image for varied displacement ranges. First, we measure the average distance d\u03040\u22125 of a patch descriptor from that of a patch that is 5 pixels away for pixels which undergo a displacement of up to 5 pixels. Note that for a 51 \u00d7 51 patch, only 18% of the pixels were completely replaced in such a small displacement. Then, we repeat this to patches from various displacement ranges, taking again the average distance from a patch of 5 pixels away. To normalize, we divide this average distance by the first average d\u0304L\u2212H d\u03040\u22125\n, for (L,H) \u2208 {(5, 10), (10, 40), (40, inf)}. The results in Tab. 4 show that while the PatchBatch original model reacts almost similarly for all displacement ranges, interleaving trained models have learned to be less sensitive to appearance changes for larger displacements. Moreover, using only gradual learning, leads to high sensitivity across all ranges. This can be the result of the carry-on from the early learning stages on small displacements where appearance sensitivity is more valuable."}, {"heading": "6.2.2 Benchmarks results", "text": "We train our model on three datasets and submit the results of each benchmark on the respectively trained model. Our results are directly comparable with the PatchBatch model, since we use the same procedure as theirs \u2013 Training the CNN for 4000 epochs on 80% of the training set and choosing the best configuration by selecting the one with the lowest validation error on samples from the remaining 20% of the data.\nThe results can be seen in Tab. 5, 6, 7. We succeed in improving results in all three benchmarks and achieve state of the art results for KITTI2012 [15] and KITTI2015 [29].\nWe evaluate our method only against methods not using additional information for the flow estimation, including those methods which used semantic segmentation.\nOn KITTI2015, as can be seen on Tab. 6, we reduced the error of both foreground and background areas, obtaining the lowest error for both cases. The increased accuracy for both regions is correlated with our previous experiments and corroborate our claim of extracting better descriptors for all scenarios.\nIn contrast to the error percent measurement of the KITTI benchmarks, MPI-Sintel uses an end-point-error (EPE) one. Compared to the original PatchBatch model, (Tab. 7) we succeed in preserving a low EPE for small displacements while dramatically reducing it for large ones. Our model does not achieve the best results when using the EPE measurement. However, when considering the percentage of large error displacements, as calculated from the error images, our SPCI model is second best and our interleaving model is third.\nOur trained models are available on the PatchBatch GitHub repository."}, {"heading": "7. Conclusions", "text": "Common sense dictates that most of the perceptual tasks are heterogeneous and require multiple strategies. The literature methods address training in accordance with the difficulty of specific samples. In our work, we show, for the first time, how to address both multiple sub-tasks and varying difficulty. The two are not independent \u2013 some sub-tasks are harder than others, and our SPCI method addresses this\nchallenge. Using the proposed method, we are able to improve a recently proposed optical flow method and obtain state of the art results on the two most competitive real-world benchmarks."}, {"heading": "Acknowledgments", "text": "This research is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)."}], "references": [{"title": "Apparent age estimation from face images combining general and children-specialized deep learning models", "author": ["G. Antipov", "M. Baccouche", "S.-A. Berrani", "J.-L. Dugelay"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation", "author": ["C. Bailer", "B. Taetz", "D. Stricker"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (CVPR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Cnn based patch matching for optical flow with thresholded hinge loss", "author": ["C. Bailer", "K. Varanasi", "D. Stricker"], "venue": "arXiv preprint arXiv:1607.08064,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The generalized patchmatch correspondence algorithm", "author": ["C. Barnes", "E. Shechtman", "D.B. Goldman", "A. Finkelstein"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Surf: Speeded up robust features", "author": ["H. Bay", "T. Tuytelaars", "L. Van Gool"], "venue": "In European conference on computer vision (ECCV),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Large displacement optical flow: descriptor matching in variational motion estimation", "author": ["T. Brox", "J. Malik"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "A naturalistic open source movie for optical flow evaluation", "author": ["D.J. Butler", "J. Wulff", "G.B. Stanley", "M.J. Black"], "venue": "European Conference on Computer Vision (ECCV), Part IV,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Full flow: Optical flow estimation by global optimization over regular grids", "author": ["Q. Chen", "V. Koltun"], "venue": "arXiv preprint arXiv:1604.03513,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["A. Dosovitskiy", "P. Fischery", "E. Ilg", "C. Hazirbas", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": "In IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Combinatorial regularization of descriptor matching for optical flow estimation", "author": ["B. Drayer", "T. Brox"], "venue": "In British Machine Vision Conference (BMVC),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Patchbatch: A batch augmented loss for optical flow", "author": ["D. Gadot", "L. Wolf"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Fine-grained classification via mixture of deep convolutional neural networks", "author": ["Z. Ge", "A. Bewley", "C. McCool", "P. Corke", "B. Upcroft", "C. Sanderson"], "venue": "In Winter Conference on Applications of Computer Vision (WACV),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "author": ["A. Geiger", "P. Lenz", "R. Urtasun"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Deep discrete flow", "author": ["F. G\u00fcney", "A. Geiger"], "venue": "In Asian Conference on Computer Vision (ACCV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Self-paced curriculum learning", "author": ["L. Jiang", "D. Meng", "Q. Zhao", "S. Shan", "A.G. Hauptmann"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Optical flow with geometric occlusion estimation and fusion of multiple frames", "author": ["R. Kennedy", "C.J. Taylor"], "venue": "In International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Learning concepts and categories is spacing the enemy of induction", "author": ["N. Kornell", "R.A. Bjork"], "venue": "Psychological science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Learning features and parts for fine-grained recognition", "author": ["J. Krause", "T. Gebru", "J. Deng", "L.-J. Li", "F.-F. Li"], "venue": "In Proceedings of the International Conference on Pattern Recognition (ICPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Fine-grained recognition without part annotations", "author": ["J. Krause", "H. Jin", "J. Yang", "L. Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "B. Packer", "D. Koller"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Spmbp: Sped-up patchmatch belief propagation for continuous mrfs", "author": ["Y. Li", "D. Min", "M.S. Brown", "M.N. Do", "J. Lu"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Fast guided global interpolation for depth and motion", "author": ["Y. Li", "D. Min", "M.N. Do", "J. Lu"], "venue": "In European Conference on Computer Vision (ECCV),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Object recognition from local scale-invariant features", "author": ["D.G. Lowe"], "venue": "In Proceedings of the International Conference on Computer Vision (ICCV),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "Object scene flow for autonomous vehicles", "author": ["M. Menze", "A. Geiger"], "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Discrete optimization for optical flow", "author": ["M. Menze", "C. Heipke", "A. Geiger"], "venue": "In German Conference on Pattern Recognition,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Statistical Analysis of Extreme Values: with Applications to Insurance, Finance, Hydrology and Other Fields, pages 31\u201332", "author": ["R.-D. Reiss", "M. Thomas"], "venue": "Birkhuser Basel,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Epicflow: Edge-preserving interpolation of correspondences for optical flow", "author": ["J. Revaud", "P. Weinzaepfel", "Z. Harchaoui", "C. Schmid"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Discriminative learning of deep convolutional feature point descriptors", "author": ["E. Simo-Serra", "E. Trulls", "L. Ferraz", "I. Kokkinos", "P. Fua", "F. Moreno-Noguer"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Sparse flow: Sparse matching for small to large displacement optical flow", "author": ["R. Timofte", "L. Van Gool"], "venue": "In 2015 IEEE Winter Conference on Applications of Computer Vision,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Daisy: An efficient dense descriptor applied to wide-baseline stereo", "author": ["E. Tola", "V. Lepetit", "P. Fua"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Learning to Count with CNN Boosting, pages 660\u2013676", "author": ["E. Walach", "L. Wolf"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2016}, {"title": "Dense optical flow prediction from a static image", "author": ["J. Walker", "A. Gupta", "M. Hebert"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "The global patch collider", "author": ["S. Wang", "S. Ryan Fanello", "C. Rhemann", "S. Izadi", "P. Kohli"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Dense, accurate optical flow estimation with piecewise parametric model", "author": ["J. Yang", "H. Li"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "(b) In the baseline method [13], negative samples are sampled regardless of the properties of the true match.", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "The curriculum learning method [6] selects samples, stratified by difficulty, using a predefined order.", "startOffset": 31, "endOffset": 34}, {"referenceID": 22, "context": "The method of self-paced learning [24] identifies a set of easy samples by their loss, and learns using only those samples.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "The pipeline employed for computing optical flow is similar to the PatchBatch method [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 12, "context": "\u2022 We improve the PatchBatch[13] pipeline itself.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": ", image descriptors such as SIFT [28], SURF [5], HOG [10], and DAISY [35] have been used.", "startOffset": 33, "endOffset": 37}, {"referenceID": 4, "context": ", image descriptors such as SIFT [28], SURF [5], HOG [10], and DAISY [35] have been used.", "startOffset": 44, "endOffset": 47}, {"referenceID": 9, "context": ", image descriptors such as SIFT [28], SURF [5], HOG [10], and DAISY [35] have been used.", "startOffset": 53, "endOffset": 57}, {"referenceID": 32, "context": ", image descriptors such as SIFT [28], SURF [5], HOG [10], and DAISY [35] have been used.", "startOffset": 69, "endOffset": 73}, {"referenceID": 6, "context": "Brox and Malik were the first to apply local descriptors to the problem of dense optical flow [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 36, "context": "Following their success, many other models adopted the use of local descriptors [39, 30, 20, 34].", "startOffset": 80, "endOffset": 96}, {"referenceID": 27, "context": "Following their success, many other models adopted the use of local descriptors [39, 30, 20, 34].", "startOffset": 80, "endOffset": 96}, {"referenceID": 18, "context": "Following their success, many other models adopted the use of local descriptors [39, 30, 20, 34].", "startOffset": 80, "endOffset": 96}, {"referenceID": 31, "context": "Following their success, many other models adopted the use of local descriptors [39, 30, 20, 34].", "startOffset": 80, "endOffset": 96}, {"referenceID": 30, "context": "With the advent of deep learning methods, CNNs were shown to be extremely powerful in the related problem of stereo matching [33, 41].", "startOffset": 125, "endOffset": 133}, {"referenceID": 38, "context": "With the advent of deep learning methods, CNNs were shown to be extremely powerful in the related problem of stereo matching [33, 41].", "startOffset": 125, "endOffset": 133}, {"referenceID": 34, "context": "In [37], a CNN is used to predict the flow from a single static image.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "FlowNet [11] is the first end-to-end CNN for optical flow and showed competitive results.", "startOffset": 8, "endOffset": 12}, {"referenceID": 12, "context": "In the PatchBatch [13] pipeline, a CNN was used for extracting patch descriptors that are then used for matching via the PatchMatch [4] Nearest Neighbor Field (NNF) algorithms.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "In the PatchBatch [13] pipeline, a CNN was used for extracting patch descriptors that are then used for matching via the PatchMatch [4] Nearest Neighbor Field (NNF) algorithms.", "startOffset": 132, "endOffset": 135}, {"referenceID": 14, "context": "It achieved state of the art performance in the KITTI benchmarks [15, 29] as of last year.", "startOffset": 65, "endOffset": 73}, {"referenceID": 26, "context": "It achieved state of the art performance in the KITTI benchmarks [15, 29] as of last year.", "startOffset": 65, "endOffset": 73}, {"referenceID": 6, "context": "To solve this problem, extensive efforts have been devoted to methods for the integration of descriptors with local assumptions [7, 34, 30].", "startOffset": 128, "endOffset": 139}, {"referenceID": 31, "context": "To solve this problem, extensive efforts have been devoted to methods for the integration of descriptors with local assumptions [7, 34, 30].", "startOffset": 128, "endOffset": 139}, {"referenceID": 27, "context": "To solve this problem, extensive efforts have been devoted to methods for the integration of descriptors with local assumptions [7, 34, 30].", "startOffset": 128, "endOffset": 139}, {"referenceID": 2, "context": "A concurrent work [3], focused on decreasing the error for large displacements by down-sampling patches and adding a threshold to the loss function.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "[1] for age estimation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "In order to achieve the required accuracy, some methods perform object segmentation [23] or part detection [22] to limit the search of each sub-class to the most relevant body parts.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "In order to achieve the required accuracy, some methods perform object segmentation [23] or part detection [22] to limit the search of each sub-class to the most relevant body parts.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "A different approach was shown in [14], where several models were trained on different samples to create per class expert models.", "startOffset": 34, "endOffset": 38}, {"referenceID": 5, "context": "Curriculum learning [6], inspired by the learning process of humans, was the first method to manipulate the order of samples shown to the model during training.", "startOffset": 20, "endOffset": 23}, {"referenceID": 22, "context": "In self-paced learning [24], instead of using a predefined order, the difficulty of each sample is dynamically estimated during training by inspecting the associated loss.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "In the work of [19], those two methods were combined to allow a prior knowledge of samples difficulty to be considered in the self-paced iterations.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "It was recently proposed to eliminate from the training process samples that are either too easy or too hard [36].", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "PatchMatch [4] is applied twice in order to get both flow directions.", "startOffset": 11, "endOffset": 14}, {"referenceID": 3, "context": "Using the generated descriptors, PatchMatch [4] (PM) algorithm is used to compute initial flow assignments.", "startOffset": 44, "endOffset": 47}, {"referenceID": 29, "context": "In the final step, the sparse-to-dense EpicFlow [32] (EF) algorithm creates the final estimation using the sparse flow and the original raw images.", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "We refer the reader to the PatchBatch [13] paper and the published code1 for a more detailed description.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "First, we adopt the suggestion, that was partially tested in the original PB paper [13], to enlarge the patch size from 51\u00d751 to 71\u00d771 pixels.", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "Instead of the DrLIM [17] loss functions used in PatchBatch, we found the Hinge loss to achieve best results when integrated with our, further detailed, learning method.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "In the MPI-Sintel [8], where results are separated by the velocity of pixels, the current average end-point-error (EPE) of the top 10 ranked methods is 35.", "startOffset": 18, "endOffset": 21}, {"referenceID": 26, "context": "In KITTI2015 [29], there is no published estimation by velocity.", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "For applying curriculum learning [6], the samples need to be stratified by difficulty prior to training.", "startOffset": 33, "endOffset": 36}, {"referenceID": 19, "context": "Kornell and Bjork, psychology researchers, found that for some cases, interleaving exemplars of different categories can enhance inductive learning [21].", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "where v is the displacement of the matching pixels and X is sampled from a log-normal distribution [31].", "startOffset": 99, "endOffset": 103}, {"referenceID": 0, "context": "We used \u03bc = 0 and \u03c3 = 1 as parameters and after sampling values for all of the batch samples, they were normalized to [0, 1].", "startOffset": 118, "endOffset": 124}, {"referenceID": 12, "context": "CENT [13] 9.", "startOffset": 5, "endOffset": 9}, {"referenceID": 12, "context": "86 CENT+SD [13] 8.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "Column L shows the results on digits [0, 4] with random noise on the image bottom, and column H shows the results on digits [5, 9] rotated randomly by 0 to 45 degrees with random noise at the top of the image .", "startOffset": 37, "endOffset": 43}, {"referenceID": 4, "context": "Column L shows the results on digits [0, 4] with random noise on the image bottom, and column H shows the results on digits [5, 9] rotated randomly by 0 to 45 degrees with random noise at the top of the image .", "startOffset": 124, "endOffset": 130}, {"referenceID": 8, "context": "Column L shows the results on digits [0, 4] with random noise on the image bottom, and column H shows the results on digits [5, 9] rotated randomly by 0 to 45 degrees with random noise at the top of the image .", "startOffset": 124, "endOffset": 130}, {"referenceID": 14, "context": "To evaluate our work, we use the three most competitive optical flow benchmarks - KITTI2012 [15], KITTI2015 [29] and MPI-Sintel [8].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "To evaluate our work, we use the three most competitive optical flow benchmarks - KITTI2012 [15], KITTI2015 [29] and MPI-Sintel [8].", "startOffset": 108, "endOffset": 112}, {"referenceID": 7, "context": "To evaluate our work, we use the three most competitive optical flow benchmarks - KITTI2012 [15], KITTI2015 [29] and MPI-Sintel [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "2, one can notice that the use of the Hinge loss instead of CENT [13], improved the PM results and has no such effect on the final EF output.", "startOffset": 65, "endOffset": 69}, {"referenceID": 2, "context": "65% CNN-HPM [3] 4.", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "92% PatchBatch+PS71 [13] 5.", "startOffset": 20, "endOffset": 24}, {"referenceID": 12, "context": "29% PatchBatch [13] 5.", "startOffset": 15, "endOffset": 19}, {"referenceID": 37, "context": "44% PH-Flow [40] 5.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "76% FlowFields [2] 5.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "We succeed in improving results in all three benchmarks and achieve state of the art results for KITTI2012 [15] and KITTI2015 [29].", "startOffset": 107, "endOffset": 111}, {"referenceID": 26, "context": "We succeed in improving results in all three benchmarks and achieve state of the art results for KITTI2012 [15] and KITTI2015 [29].", "startOffset": 126, "endOffset": 130}, {"referenceID": 2, "context": "46% CNN-HPM [3] 18.", "startOffset": 12, "endOffset": 15}, {"referenceID": 12, "context": "44% PatchBatch [13] 19.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "69% DiscreteFlow [30] 21.", "startOffset": 17, "endOffset": 21}, {"referenceID": 8, "context": "23% FullFlow [9] 23.", "startOffset": 13, "endOffset": 16}, {"referenceID": 29, "context": "26% EpicFlow [32] 25.", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "10% DeepFlow [39] 27.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "FlowFields+ [2] 5.", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "17 DeepDiscreteFlow [16] 5.", "startOffset": 20, "endOffset": 24}, {"referenceID": 23, "context": "82 SPM-BPv2 [26] 5.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "12 FullFlow [9] 5.", "startOffset": 12, "endOffset": 15}, {"referenceID": 35, "context": "14 GlobalPatchCollider [38] 6.", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": "45 DiscreteFlow [30] 6.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "07 EpicFlow [32] 6.", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "02 FGI [27] 6.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "98 TF+OFM [20] 6.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "76 Deep+R [12] 6.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "69 PatchBatch [13] 6.", "startOffset": 14, "endOffset": 18}], "year": 2016, "abstractText": "We show that the matching problem that underlies optical flow requires multiple strategies, depending on the amount of image motion and other factors. We then study the implications of this observation on training a deep neural network for representing image patches in the context of descriptor based optical flow. We propose a metric learning method, which selects suitable negative samples based on the nature of the true match. This type of training produces a network that displays multiple strategies depending on the input and leads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow benchmarks.", "creator": "LaTeX with hyperref package"}}}