{"id": "1611.06204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Nov-2016", "title": "Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks", "abstract": "Curriculum Learning emphasizes the order of training instances in a computational learning setup. The core hypothesis is that simpler instances should be learned early as building blocks to learn more complex ones. Despite its usefulness, it is still unknown how exactly the internal representation of models are affected by curriculum learning. In this paper, we study the effect of curriculum learning on Long Short-Term Memory (LSTM) networks, which have shown strong competency in many Natural Language Processing (NLP) problems. Our experiments on sentiment analysis task and a synthetic task similar to sequence prediction tasks in NLP show that curriculum learning has a positive effect on the LSTM's internal states by biasing the model towards building constructive representations i.e. the internal representation at the previous timesteps are used as building blocks for the final prediction. We also find that smaller models significantly improves when they are trained with curriculum learning. Lastly, we show that curriculum learning helps more when the amount of training data is limited.", "histories": [["v1", "Fri, 18 Nov 2016 19:38:59 GMT  (66kb,D)", "http://arxiv.org/abs/1611.06204v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["volkan cirik", "eduard hovy", "louis-philippe morency"], "accepted": false, "id": "1611.06204"}, "pdf": {"name": "1611.06204.pdf", "metadata": {"source": "CRF", "title": "Visualizing and Understanding Curriculum Learning for Long Short-Term Memory Networks", "authors": ["Volkan Cirik", "Eduard Hovy", "Louis-Philippe Morency"], "emails": ["vcirik@cs.cmu.edu", "hovy@cs.cmu.edu", "morency@cs.cmu.edu"], "sections": [{"heading": "Introduction", "text": "Inspired by the human learning process, Curriculum Learning (Elman, 1993; Bengio et al., 2009) is an algorithm that emphasizes the order of training instances in a computational learning setup. The main idea is that learning easy instances first could be helpful for learning more complex ones later in the training. The first algorithm proposed by Bengio et al. (2009), which we refer as one-pass curriculum, creates disjoint sets of training examples ordered by the complexity and used separately during training. The second algorithm called baby step curriculum uses an incremental approach where groups of more complex examples are incrementally added to the training set (Spitkovsky, Alshawi, and Jurafsky, 2010). These curriculum learning regimens were shown to improve performance in some Natural Language Processing and Computer Vision tasks (Pentina, Sharmanska, and Lampert, 2015; Spitkovsky, Alshawi, and Jurafsky, 2010).\nDespite its usefulness, it is still unknown how exactly computational models are affected internally by curriculum learning. An example of computational model particu-\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nlarly relevant to Natural Language Processing is Long ShortTerm Memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTM networks have shown competitive performance in several domains such as handwriting recognition (Graves et al., 2009) and parsing (Vinyals et al., 2015). Surprisingly, curriculum learning has not been studied in the context of LSTM networks to our knowledge. Detailed visualizations and analyses of curriculum learning regimens with LSTM will allow us to better understand how models are affected and provides us insights when to use these regimens. Knowing how curriculum learning works, we can design new extensions and understand the nature of tasks most suited for these learning regimens.\nIn this paper, we study the effect of curriculum learning on LSTM networks. We created experiments to directly compare two curriculum learning regimens, one-pass and baby step, with two baseline approaches that include the conventional technique of randomly ordering the training samples. We use two benchmarks for our analyses. First, a synthetic task is designed which is similar to several Natural Language Processing tasks where a sequence of symbols are observed and a particular function (e.g. analogous to a linguistic or a semantic phenomenon) is aimed to be learned. Second, we use sentiment analysis where the polarity of subjective opinions is classified \u2013 a fundamental task in Natural Language Processing. As mentioned previously, this is the first work studying LSTM networks on sentiment analysis with curriculum learning to our knowledge.\nOur visualizations and analyses on these two sequence tasks are designed to study three main factors. First, we compare the four learning regimens on how the LSTM network\u2019s internal representations change as the final prediction is computed. To this end, we simply decode the representations at intermediate steps. This analysis helps us understand how a model handles the task with the help of curriculum learning. Second, we investigate how the performance of models with different complexities are affected by curriculum learning. Smaller yet accurate models are crucial in limited resource settings. Third, we study how the performance of curriculum learning changes in low-resource setups. This analysis provides us a valuable information considering lowresource scenarios are common in several data-driven learning domains such as Natural Language Processing.\nar X\niv :1\n61 1.\n06 20\n4v 1\n[ cs\n.C L\n] 1\n8 N\nov 2\n01 6"}, {"heading": "Related Work", "text": "We review a list of topics related to our work in the context of curriculum learning (CL), analysis of neural networks and sentiment analysis with neural networks.\nCurriculum Learning. Motivated by children\u2019s language learning, Elman (1993) studies the effect of learning regimen on a synthetic grammar task. He shows that a Recurrent Neural Network (RNN) is able to learn a grammar when training data is presented from simple to complex order and fails to do so when the order is random. Bengio et al. (2009) investigate CL from an optimization perspective. Their experiments on synthetic vision and word representation learning tasks show that CL results in better generalization and faster learning. Spitkovsky, Alshawi, and Jurafsky (2010) apply a CL strategy to learn an unsupervised parser for sentences of length k and initialize the next parser for sentences of length k + 1 with the previously learned one. They show that learning a hybrid model using the parsers learned for each sentence lengths achieves a significant improvement over a baseline model. Pentina, Sharmanska, and Lampert (2015) investigate the CL in a multi-task learning setup and propose a model to learn the order of multiple tasks. Their experiments on a set of vision tasks show that learning tasks sequentially is better than learning them jointly. Jiang et al. (2015) provide a general framework for CL and Self-Paced Learning where model picks which instances to train based on a simplicity metric. The proposed framework is able to combine prior knowledge of curriculum with Self-Paced Learning in the learning objective.\nLong Short-Term Memory Networks. LSTM networks are a variant of RNNs (Elman, 1990) capable of storing information and propagating loss over long distance. Using a gating mechanism by controlling the information flow into the internal representation, it is possible to avoid the problems of training RNNs (Bengio, Simard, and Frasconi, 1994; Pascanu, Mikolov, and Bengio, 2012). Several architectural variants have been proposed to improve the basic model (Cho et al., 2014; Chung et al., 2015; Yao et al., 2015; Kalchbrenner, Danihelka, and Graves, 2015; Dyer et al., 2015; Grefenstette et al., 2015).\nVisualization of Neural Networks. Although many of the neural network studies provide quantitative analysis, there are few qualitative analyses of neural networks. Zeiler and Fergus (2014) visualize the feature maps of a Convolutional Neural Network (CNN) (LeCun et al., 1998). They show that feature maps at different layers show sensitivity to different shapes, textures, and objects. Similarly, Karpathy, Johnson, and Li (2015) analyze LSTM on character level language modeling. Their analysis shows that deeper models with gating mechanisms achieve better results. They show that some cells in LSTM learn to detect patterns and how RNNs learn to generalize to longer sequences. More recently, Li et al. (2016) use visualization to show how neural network models handle several linguistics phenomena such as composi-\ntionality and negation using sentiment analysis and sequence auto-encoding.\nSynthetic Tasks. Since the early days of neural networks, synthetic tasks were used to test the capabilities of the models (Fahlman, 1991) and often serve as unit tests for machine learning models (Weston et al., 2015). Similar to the first work on LSTM (Hochreiter and Schmidhuber, 1997), many of the contemporary neural network models (Graves, Wayne, and Danihelka, 2014; Kurach, Andrychowicz, and Sutskever, 2015; Sukhbaatar et al., 2015; Vinyals, Fortunato, and Jaitly, 2015) use synthetic tasks to compare and contrast several architectures. Inspired by these studies, we also use a synthetic task as one of our tasks to understand the effect of CL on LSTMs.\nSentiment Analysis with Neural Networks. Several approaches have been proposed to solve sentiment analysis using neural networks. Socher et al. (2013) propose Recursive Neural Networks to exploit the syntactic structure of a sentence. A number of extensions of this model have been proposed in the context of sentiment analysis (Irsoy and Cardie, 2014; Tai, Socher, and Manning, 2015). Other proposed approaches use CNN (Kalchbrenner, Grefenstette, and Blunsom, 2014; Kim, 2014) and the averaging of word vector models (Iyyer et al., 2015; Le and Mikolov, 2014).\nTo our knowledge, this work is the first to study how the internal representation of LSTM change in a curriculum learning setup."}, {"heading": "Curriculum Learning Regimens", "text": "Curriculum learning emphasizes the order of training instances, prioritizing simpler instances before the more complex ones. In this section, we describe two curriculum learning regimens: one-pass curriculum originally proposed by Bengio et al. (2009) and baby step curriculum from Spitkovsky, Alshawi, and Jurafsky (2010). For both regimens, we develop the curriculum C using the same strategy proposed by Spitkovsky, Alshawi, and Jurafsky (2010) who assume that shorter sequences are easier to learn.\nThe following subsections are describing the two curriculum learning regimens as well as two baseline learning regimens."}, {"heading": "One-Pass Curriculum", "text": "Bengio et al. (2009) propose to use a dataset with simpler instances in the first phase of the training. After some number of iterations, they switch to harder target dataset. The intuition is that after some training on simpler data, the model M is ready to handle the harder target data. Here, we name this regimen One-Pass curriculum (see Algorithm 1). The training data D is sorted by a curriculum C and distributed into k number of buckets. The training starts with the easiest bucket. Unlike the previous work (Bengio et al., 2009), we use early stopping \u2013 training stops for the bucket when the loss or task\u2019s accuracy criteria on held-out set do not get any better for p number of epochs. Afterward, the next bucket is being used and trained in the same way. The whole training is stopped after all buckets are used. Note that the model\nuses each bucket only one time for the training, hence the name.\nAlgorithm 1 One-Pass Curriculum 1: procedure OP-CURRICULUM(M ,D, C) 2: D\u2032 = sort(D, C) 3: {D1,D2, ...,Dk} = D\u2032 where C(da) < C(db) da \u2208\nDi , db \u2208 Dj , \u2200i < j 4: for s = 1...k do 5: while not converged for p epochs do 6: train(M , Ds) 7: end while 8: end for 9: end procedure"}, {"heading": "Baby Steps Curriculum", "text": "The intuition behind Baby Steps curriculum (Bengio et al., 2009; Spitkovsky, Alshawi, and Jurafsky, 2010) is that simpler instances in the training data should not be discarded, instead, the complexity of the training data should be increased. After distributing data into buckets based on a curriculum, training starts with the easiest bucket. When the loss or task\u2019s accuracy criteria on a held-out set do not get any better for p number of epochs, the next bucket and the current data bucket are merged. The whole training is stopped after all buckets are used (see Algorithm 2).\nAlgorithm 2 Baby Steps Curriculum 1: procedure BS-CURRICULUM(M ,D, C) 2: D\u2032 = sort(D, C) 3: {D1,D2, ...,Dk} = D\u2032 where C(da) < C(db) da \u2208\nDi , db \u2208 Dj , \u2200i < j 4: Dtrain = \u00d8 5: for s = 1...k do 6: Dtrain = Dtrain \u222a Ds 7: while not converged for p epochs do 8: train(M , Dtrain) 9: end while\n10: end for 11: end procedure"}, {"heading": "Baseline Regimens", "text": "The first baseline, named No-CL, is the common practice of shuffling the training data. For a neural network like our LSTM models, this means that training is performed as usual where one epoch sees all the training set in random order. For all experiments described in the following section, models learned with the No-CL regiment are trained 10 times1, to get a proper average performance.\nThe second baseline, named Sorted, also sees all the data at each epoch but the ordering of the training instances is based on the curriculum C. This is a simplification of the two CL regimens presented in the previous subsections since\n1Note that this favors No-CL due to lower variance in results.\nwe are not partitioning the data based on its complexity (i.e., based on the curriculum). We are simply reordering the training set. A comparison between the No-CL and Sorted baselines will allow us to study the importance of training instance ordering."}, {"heading": "Experiments", "text": "The main goal of our experiments is to better understand how a computational model, specifically LSTM networks, are affected internally by CL. We aim to observe (1) the effect of CL on the internal model representations, (2) how the number of model parameters affect the performance of CL, and (3) how the amount of data size change the contribution of CL.\nThe following subsections present LSTM network and our experimental probing methodology to analyze the LSTMs internal representations at different stages in the sequence modeling process."}, {"heading": "LSTM", "text": "We now describe LSTM networks. Let x1, .., xT be a sequence of one-hot coded sequence of symbols of length T . At each time step the LSTM updates its cells as follows:\n ifo m  = sigmsigmsigm tanh W t(xtW eht\u22121 )\n(1)\nct = f ct\u22121 + i m (2) ht = o tanh(ct) (3)\nIn above equations, W t is a [4n\u00d7 2n] matrix to calculate gate weights and new memory information g. W e is an embedding matrix for symbols. At time t, the sigmoid (sigm) and tanh (tanh) non-linearities are applied element-wise to the embedding representation of input xtW e and the output of the network from the previous time step ht\u22121. Vectors i, f, o \u2208 Rn are binary gates controlling the input, forget and output gates respectively. The vector m \u2208 Rn additively modifies the cell ct.\nWe use the final hidden representation hT of the LSTM to prediction with a projection matrix W p. In the case of regression, we use relu(W phT ) where W p is [1 \u00d7 n] and relu is rectified non-linearity. For classification, we predict one of k class labels using softmax(relu(W phT ))) where W p is [k \u00d7 n] and softmax is the softmax function.\nProbing Internal States of LSTM We aim to observe how the use of the internal representation of LSTM at intermediate steps changes depending on the learning regimen.\nEach internal representation ht is probed using the relu(W pht) functions learned for regression or the softmax(relu(W pht)) function learned for classification. By moving these probes along the sequences, we can study the intermediate representation at each time t."}, {"heading": "Digit Sum", "text": "We aim to simulate a low-resource sequence regression problem considering many of the NLP tasks only have a few thousand annotated samples. To this end, the Digit Sum task is posed as follows. Given a sequence of symbols of digits, the model is expected to predict the sum of digits. For instance given a sequence \u201d5 0 2 4 6\u201d the expected output is 17.\nDigit Sum task has similarities with our second sequence task, sentiment analysis, where digits are analogous to the word tokens in the natural language text and the summation is analogous to the subjective position of a sequence on a topic. Our two evaluation tasks also have some interesting differences which allow evaluating a broader range of sequence learning tasks. In sentiment analysis, the order of words makes a difference whereas, in the Digit Sum, the order of digits does not change the expected answer. Secondly, the learning setup is a classification of polarity levels for sentiment analysis whereas it is a regression for the Digit Sum. Dataset Details. We define the evaluation task in the Digit Sum dataset as the summation of 20 digits, a typical length of sentences in natural language. Both the validation and testing sets contain 200 sequences of 20 digits randomly generated. The training set consist of 1000 sequences each from length 2 to 20, allowing to develop the curriculum automatically following Spitkovsky, Alshawi, and Jurafsky (2010) procedure. This results in a dataset of size 19K instances2. Experimental Details. We used LSTM with hidden units of 2, 4, 8, ..., 512 without peephole connections. For all configurations the size of digit embeddings and hidden units\n2We experimented with 10x smaller dataset size and observed very similar results. We do not report these due to limited space.\nare the same. We use RMSprop (Dauphin et al., 2015) with learning rate 0.001 and decay rate of 0.9 with minibatches of size 128. The patience parameter p for early stopping is 10. We use Dropout (Srivastava et al., 2014) of rate in range {0,0.25,0.5} as suggested for LSTMs by Gal (2015)."}, {"heading": "Results", "text": "Probing Internal Model Representations. We analyze the behavior of the model by using the intermediate representations during processing of a sequence. As we discussed previously, we feed the hidden representations of each digit to the regression node to predict at each timestep of the input. Table 1 shows the input sequence, ground truth, and predictions of the best models for each learning regimen based on validation loss. The prediction of the model trained with One-Pass curriculum and Sorted Baseline shows no correlation with the running sum of the digits. The Baby Step curriculum model is able to predict similar values to running sum.\nA sequence model can learn to solve this task in numerous ways such as memorizing sequences due to overfitting, using a count table of the digits, or doing a running sum at each time step. To analyze this, we report the average differences between successive predictions (we call it \u2206) and the last input digit(see Figure 1). At each timestep, the model trained with Baby Step curriculum updates the hidden representation such that it correlates with the sum of digits observed up to that point. It is also interesting to observe that Baby Step curriculum shows better variance than the average of 10 random starts (No-CL). We emphasize that models are provided with the same sequences for training, yet, the learning regiments results in different models. Effects on Models With Different Complexities. Our next experiment studies the effect of learning regimens on models with different complexities. Figure 3 shows the\nMean Squared Loss (MSE) results for the Digit Sum with LSTMs with varying hidden unit sizes. Baby Step curriculum achieves consistently better results even if the model has much fewer parameters. Other regimens require the model to have the right complexity. Efficient training of small models is particularly important if we do not have large annotated datasets to train big models. In addition, from practical perspective, to obtain smaller yet accurate models enables deploying fast and accurate models to a limited resource setting (Bucilu, Caruana, and Niculescu-Mizil, 2006; Hinton, Vinyals, and Dean, 2015)."}, {"heading": "Sentiment Analysis", "text": "Sentiment analysis is an application of NLP to identify the polarity of subjective information in given source (Pang and\nLee, 2008). We use the Stanford Sentiment Treebank (SST) (Socher et al., 2013) an extension of a dataset (Pang and Lee, 2005) which has labels for 215,154 phrases in the parse trees of 11,855 sentences sampled from movie reviews. Realvalued sentiment labels are converted to an integer ordinal label in 0,..,4 by simple thresholding for five classes: very negative, negative, neutral, positive, and very positive. Therefore the task is posed as a 5-class classification problem. Dataset Details. We use the standard train/dev/test splits of 8544/1101/2210 for the 5-class classification problem. We flatten the annotated tree structure into sequences of phrases to use finer grained annotations. We treat the words within the span of an inner phrase as a sequence and use the phrase\u2019s annotation as label. This results in a bigger training set of 155019 instances. Experimental Details. We follow the previous work (Tai, Socher, and Manning, 2015) for the empirical setup. We use a single layer LSTM with 168 units for the 5-class classification task. We initialized the word embeddings using 300-\ndimensional Glove vectors (Pennington, Socher, and Manning, 2014) and fine-tuned them during training. For optimization, we used RMSprop (Dauphin et al., 2015) with learning rate 0.001 and decay rate of 0.9 with mini-batches of size 128. The patience parameter p for early stopping is 10.\nResults As the first step to our more detailed analysis, Table 2 reports the overall performance of the four learning regimens and the original results stated by Tai, Socher, and Manning (2015). The advantage of CL is most prominent when predicting sentiment for sentences with conjunctions (last column in Table 2). For conjunctions where a span of text contradicts or supports overall sentiment polarity, Baby Step model achieves significantly better results than others. We take a closer look at the LSTM modeling process using a similar probing technique used for the Digit Sum dataset. Probing Intermediate Representations. In Figure 2, we qualitatively show how different models process a sentence with a contrastive conjunction originally demonstrated by\nSocher et al. (2013). For each model, we plot the sentiment polarity and the probability of prediction for that polarity after observing a word token. Unlike the others, Baby Step model changes the sentiment at the appropriate time; after observing \u201cspice\u201d which constructs a positive statement with the sub-phrase \u201cbut it has just enough spice\u201d. Handling contrastive conjunctions requires a model to merge two conflicting signals (i.e. positive and negative) coming from two directions (i.e. left phrase and right phrase) in an accurate way Socher et al. (2013). Considering LSTM\u2019s limited capacity due to using only signal coming from previous timesteps (i.e processing the sentence from left to right), this result is particularly interesting because Baby Step CL boosts LSTM\u2019s performance. Effect of Training Data Size. To investigate the role of the amount of training data, we use a varying fraction of training data with learning regimens. Figure 4 shows the results. CL regimens help when training data is limited. When the amount of training data increases, the difference between the regimens gets lower. This result suggests that in lowresource setups, like many of the NLP problems, CL could be useful to improve a model\u2019s performance."}, {"heading": "Conclusion", "text": "We examined curriculum learning on two sequence prediction tasks. Our analyses showed that curriculum learning regimens based on shorter-first approach, help LSTM construct a partial representation of the sequence in a more intuitive way. We demonstrated that curriculum learning helps smaller models improve performance, contributes more in a low resource setup. Using a quantitative and qualitative analysis on sentiment analysis, we showed that a model trained with Baby Step curriculum significantly improves for sentences with conjunctions suggesting that curriculum learning helps LSTM learn longer sequences and functional role of the conjunctions."}], "references": [{"title": "Transition-based dependency parsing with stack", "author": ["A. N"], "venue": null, "citeRegEx": "N.,? \\Q2015\\E", "shortCiteRegEx": "N.", "year": 2015}, {"title": "A theoretically grounded application of dropout", "author": ["Y. 190\u2013196. Gal"], "venue": null, "citeRegEx": "Gal,? \\Q2015\\E", "shortCiteRegEx": "Gal", "year": 2015}, {"title": "Distilling the knowl", "author": ["O. Vinyals", "J. Dean"], "venue": "Neural Information Processing Systems,", "citeRegEx": "G. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G. et al\\.", "year": 2015}, {"title": "Deep recursive neural networks", "author": ["O. Irsoy", "C. Cardie"], "venue": null, "citeRegEx": "Irsoy and Cardie,? \\Q2014\\E", "shortCiteRegEx": "Irsoy and Cardie", "year": 2014}, {"title": "Self-paced curriculum learning", "author": ["N. Kalchbrenner", "I. Danihelka", "A. Graves"], "venue": "In AAAI,", "citeRegEx": "2015", "shortCiteRegEx": "2015", "year": 2015}, {"title": "Opinion mining and sentiment", "author": ["B. Linguistics. Pang", "L. Lee"], "venue": null, "citeRegEx": "Pang and Lee,? \\Q2008\\E", "shortCiteRegEx": "Pang and Lee", "year": 2008}, {"title": "Recursive deep models for", "author": ["A.Y. Ng", "C. Potts"], "venue": null, "citeRegEx": "Ng and Potts,? \\Q2013\\E", "shortCiteRegEx": "Ng and Potts", "year": 2013}, {"title": "Dropout: A simple way to prevent", "author": ["R. Salakhutdinov"], "venue": null, "citeRegEx": "Salakhutdinov,? \\Q2014\\E", "shortCiteRegEx": "Salakhutdinov", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["G. ton"], "venue": null, "citeRegEx": "ton,? \\Q2015\\E", "shortCiteRegEx": "ton", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "He shows that a Recurrent Neural Network (RNN) is able to learn a grammar when training data is presented from simple to complex order and fails to do so when the order is random. Bengio et al. (2009) investigate CL from an optimization perspective.", "startOffset": 26, "endOffset": 201}, {"referenceID": 0, "context": "He shows that a Recurrent Neural Network (RNN) is able to learn a grammar when training data is presented from simple to complex order and fails to do so when the order is random. Bengio et al. (2009) investigate CL from an optimization perspective. Their experiments on synthetic vision and word representation learning tasks show that CL results in better generalization and faster learning. Spitkovsky, Alshawi, and Jurafsky (2010) apply a CL strategy to learn an unsupervised parser for sentences of length k and initialize the next parser for sentences of length k + 1 with the previously learned one.", "startOffset": 26, "endOffset": 435}, {"referenceID": 0, "context": "He shows that a Recurrent Neural Network (RNN) is able to learn a grammar when training data is presented from simple to complex order and fails to do so when the order is random. Bengio et al. (2009) investigate CL from an optimization perspective. Their experiments on synthetic vision and word representation learning tasks show that CL results in better generalization and faster learning. Spitkovsky, Alshawi, and Jurafsky (2010) apply a CL strategy to learn an unsupervised parser for sentences of length k and initialize the next parser for sentences of length k + 1 with the previously learned one. They show that learning a hybrid model using the parsers learned for each sentence lengths achieves a significant improvement over a baseline model. Pentina, Sharmanska, and Lampert (2015) investigate the CL in a multi-task learning setup and propose a model to learn the order of multiple tasks.", "startOffset": 26, "endOffset": 796}, {"referenceID": 0, "context": "He shows that a Recurrent Neural Network (RNN) is able to learn a grammar when training data is presented from simple to complex order and fails to do so when the order is random. Bengio et al. (2009) investigate CL from an optimization perspective. Their experiments on synthetic vision and word representation learning tasks show that CL results in better generalization and faster learning. Spitkovsky, Alshawi, and Jurafsky (2010) apply a CL strategy to learn an unsupervised parser for sentences of length k and initialize the next parser for sentences of length k + 1 with the previously learned one. They show that learning a hybrid model using the parsers learned for each sentence lengths achieves a significant improvement over a baseline model. Pentina, Sharmanska, and Lampert (2015) investigate the CL in a multi-task learning setup and propose a model to learn the order of multiple tasks. Their experiments on a set of vision tasks show that learning tasks sequentially is better than learning them jointly. Jiang et al. (2015) provide a general framework for CL and Self-Paced Learning where model picks which instances to train based on a simplicity metric.", "startOffset": 26, "endOffset": 1043}, {"referenceID": 0, "context": "Visualization of Neural Networks. Although many of the neural network studies provide quantitative analysis, there are few qualitative analyses of neural networks. Zeiler and Fergus (2014) visualize the feature maps of a Convolutional Neural Network (CNN) (LeCun et al.", "startOffset": 17, "endOffset": 189}, {"referenceID": 0, "context": "Visualization of Neural Networks. Although many of the neural network studies provide quantitative analysis, there are few qualitative analyses of neural networks. Zeiler and Fergus (2014) visualize the feature maps of a Convolutional Neural Network (CNN) (LeCun et al., 1998). They show that feature maps at different layers show sensitivity to different shapes, textures, and objects. Similarly, Karpathy, Johnson, and Li (2015) analyze LSTM on character level language modeling.", "startOffset": 17, "endOffset": 431}, {"referenceID": 0, "context": "Visualization of Neural Networks. Although many of the neural network studies provide quantitative analysis, there are few qualitative analyses of neural networks. Zeiler and Fergus (2014) visualize the feature maps of a Convolutional Neural Network (CNN) (LeCun et al., 1998). They show that feature maps at different layers show sensitivity to different shapes, textures, and objects. Similarly, Karpathy, Johnson, and Li (2015) analyze LSTM on character level language modeling. Their analysis shows that deeper models with gating mechanisms achieve better results. They show that some cells in LSTM learn to detect patterns and how RNNs learn to generalize to longer sequences. More recently, Li et al. (2016) use visualization to show how neural network models handle several linguistics phenomena such as compositionality and negation using sentiment analysis and sequence auto-encoding.", "startOffset": 17, "endOffset": 714}, {"referenceID": 3, "context": "A number of extensions of this model have been proposed in the context of sentiment analysis (Irsoy and Cardie, 2014; Tai, Socher, and Manning, 2015).", "startOffset": 93, "endOffset": 149}, {"referenceID": 0, "context": "Sentiment Analysis with Neural Networks. Several approaches have been proposed to solve sentiment analysis using neural networks. Socher et al. (2013) propose Recursive Neural Networks to exploit the syntactic structure of a sentence.", "startOffset": 24, "endOffset": 151}, {"referenceID": 1, "context": "5} as suggested for LSTMs by Gal (2015).", "startOffset": 29, "endOffset": 40}, {"referenceID": 3, "context": "Results As the first step to our more detailed analysis, Table 2 reports the overall performance of the four learning regimens and the original results stated by Tai, Socher, and Manning (2015). The advantage of CL is most prominent when predicting sentiment for sentences with conjunctions (last column in Table 2). For conjunctions where a span of text contradicts or supports overall sentiment polarity, Baby Step model achieves significantly better results than others. We take a closer look at the LSTM modeling process using a similar probing technique used for the Digit Sum dataset. Probing Intermediate Representations. In Figure 2, we qualitatively show how different models process a sentence with a contrastive conjunction originally demonstrated by Socher et al. (2013). For each model, we plot the sentiment polarity and the probability of prediction for that polarity after observing a word token.", "startOffset": 188, "endOffset": 783}, {"referenceID": 3, "context": "Results As the first step to our more detailed analysis, Table 2 reports the overall performance of the four learning regimens and the original results stated by Tai, Socher, and Manning (2015). The advantage of CL is most prominent when predicting sentiment for sentences with conjunctions (last column in Table 2). For conjunctions where a span of text contradicts or supports overall sentiment polarity, Baby Step model achieves significantly better results than others. We take a closer look at the LSTM modeling process using a similar probing technique used for the Digit Sum dataset. Probing Intermediate Representations. In Figure 2, we qualitatively show how different models process a sentence with a contrastive conjunction originally demonstrated by Socher et al. (2013). For each model, we plot the sentiment polarity and the probability of prediction for that polarity after observing a word token. Unlike the others, Baby Step model changes the sentiment at the appropriate time; after observing \u201cspice\u201d which constructs a positive statement with the sub-phrase \u201cbut it has just enough spice\u201d. Handling contrastive conjunctions requires a model to merge two conflicting signals (i.e. positive and negative) coming from two directions (i.e. left phrase and right phrase) in an accurate way Socher et al. (2013). Considering LSTM\u2019s limited capacity due to using only signal coming from previous timesteps (i.", "startOffset": 188, "endOffset": 1325}], "year": 2016, "abstractText": "Curriculum Learning emphasizes the order of training instances in a computational learning setup. The core hypothesis is that simpler instances should be learned early as building blocks to learn more complex ones. Despite its usefulness, it is still unknown how exactly the internal representation of models are affected by curriculum learning. In this paper, we study the effect of curriculum learning on Long Short-Term Memory (LSTM) networks, which have shown strong competency in many Natural Language Processing (NLP) prob-", "creator": "LaTeX with hyperref package"}}}