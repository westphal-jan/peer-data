{"id": "1611.06671", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Ontology Driven Disease Incidence Detection on Twitter", "abstract": "In this work we address the issue of generic automated disease incidence monitoring on twitter. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We achieve this by training learners on concepts rather than individual words. For training we use a dataset containing mentions of influenza and Listeria and use the learned models to classify datasets containing mentions of an arbitrary selection of other diseases. We show that our ontological approach achieves good performance on this task using a variety of Natural Language Processing Techniques. We also show that word vectors can be learned directly from our concepts to achieve even better results.", "histories": [["v1", "Mon, 21 Nov 2016 07:32:56 GMT  (1192kb)", "http://arxiv.org/abs/1611.06671v1", "19 pages, 7 figures, 1 table"]], "COMMENTS": "19 pages, 7 figures, 1 table", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["mark abraham magumba", "peter nabende"], "accepted": false, "id": "1611.06671"}, "pdf": {"name": "1611.06671.pdf", "metadata": {"source": "CRF", "title": "Ontology Driven Disease Incidence Detection on Twitter", "authors": ["Mark Abraham Magumba", "Peter Nabende"], "emails": ["magumbamark@hotmail.com", "peter.nabende@gmail.com"], "sections": [{"heading": null, "text": "employ an ontology of disease related concepts and use it to obtain a conceptual representation of\ntweets. Unlike previous key word based systems and topic modeling approaches, our ontological\napproach allows us to apply more stringent criteria for determining which messages are relevant such as\nspatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will\nperform well on new data that may be lexically divergent. We achieve this by training learners on\nconcepts rather than individual words. For training we use a dataset containing mentions of influenza\nand Listeria and use the learned models to classify datasets containing mentions of an arbitrary selection\nof other diseases. We show that our ontological approach achieves good performance on this task using\na variety of Natural Language Processing Techniques. We also show that word vectors can be learned\ndirectly from our concepts to achieve even better results."}, {"heading": "Key words:", "text": "Epidemiology, Twitter, Sentiment Analysis, Text classification, Concept Ontology, Data Modelling\n1.0. Introduction: Unstructured data sources including social networking platforms such as Twitter and Facebook are increasingly being employed as data sources for disease surveillance [Schmidt 2012]. User data such as messages and user searches from such platforms have been used for both singular disease surveillance for example Google Flu Trends and Google Dengue trends for influenza and Dengue Fever [Google.com 2016] respectively and multiple disease surveillance for example by HealthMap.org1. These systems generally rely on a set of disease-related key words that filter a stream of documents to retain only those that are relevant. In this context, documents take on the form of messages posted by users; for example, in Twitter, documents are \u201ctweets\u201d which are short messages (limited to 140 characters in length) whereas, in Facebook, documents may comprise \u201cwall posts\u201d, \u201ccomments\u201d, page feeds to mention but a few. Although huge amounts of textual data are generated by users of the Web-based social networking platforms, most of the textual data present some challenges for developing\n1 http://www.healthmap.org/en/\nclassification models for disease surveillance. For example, data sparseness is a major challenge when using Twitter data whose tweets have deliberate misspellings, made up words, non-standard grammar, and high lexical diversity [Jarvis 2013; Eisenstein 2013].\nIn this paper, we propose an ontological approach for deriving features for classification models that can generally detect disease incidence in tweets and possibly other unstructured data sources. We require these models because these internet based surveillance methods typically count the volume of messages about a given disease topic as an indicator of actual disease activity. Generally speaking some positive correlation is assumed between the volume of messages about a given disease and its activity at a given time. However, in many cases this assumption is too weak as the volume of disease related messages can be influenced by panic and other factors as noted by Lampos and Cristianini [2010]. Therefore it is important to incorporate the semantic orientation of tweets as in many cases many messages about a given disease may actually mention it in a non-incidence related context or one that is spatio-temporally irrelevant. For instance, \u201cI remember when the Challenger went down, I was home sick with the flu!\u201d is an actual reference to an incidence of the flu but the Challenger disaster occurred in 1986 therefore it would be incorrect to count this mention for an outbreak in 2016.\nIn our approach we employ an ontology of concepts to overcome the problems of Natural Language Processing on Twitter. Our approach similar in many ways to the one applied by Collier et al [2008] in their BioCaster2 system for detecting and tracking infectious disease outbreaks using RSS newsfeeds from ProMED-mail3 service which is a global reporting system for outbreaks of emerging diseases and toxins. We have three main objectives: Firstly to create a catalog of words used to communicate illness, secondly to organize this catalogue into an ontology of concepts where concepts represent words that are related in meaning and finally to build classification models using our concepts as features. The classification task is to distinguish between relevant and irrelevant messages given a data set of tweets about a particular ailment.\n2.0. Related Work: The use of online unstructured data sources for disease surveillance constitutes a very active area of research. The general approach involves the use of a list of keywords to filter a stream of documents. For some systems like BioCaster [Collier et al, 2008] these key terms have been arranged into ontologies. Ontologies are used for explicit knowledge representations for inference support. In the domain of medicine and epidemiology, several ontologies have been created for the purpose of creating standardized vocabularies such as International Health Terminology Standards Organization\u2019s SNOMEDCT (Systematic Nomenclature of Medicine \u2013 Clinical terms), the Syndromic Surveillance Ontology (SSO) [Okhmatovskaia et al 2009], the Dictionary of Epidemiology [Porta 2008], the BioCaster Ontology [Collier et al 2007], the OBO ontologies such as DOID ontology for diseases [Osborne et al], OBO TRANS for disease transmission and SYMP ontology for symptoms [Scriml et al 2010] and OBO VO for vaccines [Yang et al 2011], the Epidemiology Ontology (EPO) [Pesquita et al 2014]. These ontologies have been developed with different foci and audiences in mind. For the most part the goal is to arrive at a standardized domain terminology for instance for SNOWMED-CT which is an attempt to harmonize divergent medical terminology and the European Epidemiology Ontology which does the same for select diseases. For the purpose of online disease surveillance, the BioCaster ontology [Collier et al 2008] and BioStorm (Biological Spatio-Temporal Outbreak Reasoning Module) [Buckeridge et al 2008] are the primary examples.\n2 https://sites.google.com/site/nhcollier/projects/biocaster 3 http://www.promedmail.org/\nIn our case our focus is to organize the words that people use to communicate illness with an aim of facilitating training of general purpose classifiers for disease detection with low variance on divergent data sets. As Twitter users are generally less technical we give less focus to technical terminology such as names of specific diseases, pharmacological terminology or strains of disease pathogens. Regarding the use of Twitter for disease surveillance, there already exists an impressive body of work including several ongoing projects. In terms of text processing, these fall into two broad categories: that is, rulebased systems and machine learning systems. Rule-based systems use static approaches such as the inclusion of particular keywords such as the work by Lee et al [2013]. Machine Learning Systems come in two flavors: supervised systems which train a learner using a set of labeled examples such as the Crowdbreaks project4 and unsupervised approaches such as Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA) and the LDA based ATAM (Ailment Topic Aspect Model) [Paul and Dredze 2014] which employ some theory such as the Distributional Hypothesis [Harris 1954] to arrive at some latent features. In addition, there are projects that combine both methods such as HealthTweets.org [Dredze et al, 2014].\nWhere supervised methods are employed at some point, obtaining labeled datasets is a challenge and different strategies have been tried by different projects; for instance Paul and Dredze [2014] use an SVM (Support Vector Machine) classifier during corpus generation for their ATAM model and rely on Amazon.com\u2019s Mechanical Turk (Mturk) tool [Callison-Burch and Dredze 2010]. Crowdbreaks relies on crowd sourced annotation from site visitors. However, it is a huge undertaking to create a statistically representative amount of annotated data which naturally hinders the applicability of supervised models to a general purpose, online application.\n3.0. A Concept Ontology of Human Disease Language We start out with the observation that people tend to use the similar words when communicating illness, furthermore there are certain words that are specific to certain diseases or groups of illnesses. These lexical differences originate from differences in diseases such as the site of infection. For instance in a collection of tweets describing respiratory infections the words cough, sneeze and sniffle are likely appear frequently whereas in a collection of tweets describing dermatological ailments the words itch, scratch, burning and eczema may be more frequent. If a word level classifier were built using a dataset that has a high proportion of tweets about respiratory illnesses, it may not encounter any tweet containing the word eczema even though it might be a strong predictor for skin disease. However, it is easy to see that cough, sneeze, eczema are part of a larger group of concepts, that is, they are all diseases symptoms of diseases. We can even go further and recognize that these words are part of a larger concept which is morbidity or being sick and hence bundle all sickness related words into a single concept.\nFurthermore, a word level model may achieve high classification performance on a given data set but good performance on new data requires that the training data was representative. However, on a medium like twitter there is such a huge lexical diversity that many words will be encountered just a handful of times even in a large data set meaning it will never learn certain relationships. This also means that many of the relationships learned by a learner may be spurious for instance in our own experiments when using unigram bag of words on the flu dataset which contains tweets about flu and Listeria we find that the word game is a strong positive predictor whereas its plural games is a strong negative predictor. This is because of phrases like \u201cI am going to have to bring my flu game\u201d which refers to indulging in sports whilst sick with the flu in homage to a remarkable feat by the legendary Micheal Jordan when he led the Chicago Bulls to victory against Utah Jazz in the 1997 NBA final whilst stricken\n4 http://www.crowdbreaks.com/\nwith the flu. \u201cFlu games\u201d on the other hand, is a reference to a brand of sporting footwear in homage to the same game. However, in a dataset containing tweets about norovirus or even tweets about the flu but from a geographic location outside North America linguistic differences may make such an inference invalid.\nHowever, we observe that there are some higher concepts that regardless of lexicon that are used to communicate illnesses. As an example, as earlier observed, words that refer to sickness or morbidity such as symptoms. By identifying such concepts and representing messages in terms of these concepts we can train a learner to learn relationships between concepts as opposed to relationships between individual words. In theory such a learner should generalize better as we expect its performance on lexically divergent datasets to be less variant therefore making a generic classifier more feasible.\nWe conceptualize each message as a concatenation of references to some higher conceptual objects. For the purpose of communicating disease incidence we model an ontology representing a hierarchy of concepts that describe disease incidence. These concepts are of two broad categories, those that directly describe disease incidence such as references to disease causing organisms such as bacteria and general linguistic terminology like words that imply negation and references to temporality such as words like \u201cnow\u201d and \u201cthen\u201d and descriptions of space and time. As depicted in figure 1, At the top of the hierarchy everything is conceptualized as an object, there are two types of objects that is real objects and abstract objects. Real objects refer to tangible things and abstract objects refer to concepts like time, negation and events. Real objects are either Living or inanimate.\nLiving things comprise all the key biological actors such as hosts (the organism that suffers disease which in this case is a person), pathogens (disease causing organisms) and vectors (disease spreading organisms). Inanimate objects are of two major classes namely environments and substances.\nThere are three additional types of concepts namely relationships and properties and actions. Relationships describe interactions between concept classes in the object hierarchy and correspond to OWL (Web Ontology Language) object properties whereas properties describe object and relationship attributes and correspond to OWL data properties.\nThe concept hierarchy allows for additional semantics including sub classing and inheritance. For instance figure 2 depicts sub classes for the People object excluding anatomy and population sub classes along with relationships and classes. Inheritance means that all definitions made in the top level object are inherited by the child classes in this case care provider, patient and government. In addition there is support for some polymorphic behavior meaning that even though inheritance allows for one time definition shared behavior in the hierarchy it is not necessarily implemented uniformly. For instance all objects have a \u201cMagnitude\u201d property that allows for quantification but this is invoked differently depending on the object for instance it could be a discrete quantity or quantification by degree.\n3.1. Concept Dictionaries: Finally, for each conceptual object be it a relationship, object or action there is a corresponding concept dictionary. In figure 2 these are indicated by the curly brackets. We also use a UML (Unified Modeling language) class diagram instead of the more appropriate RDF schema diagram because it is more compact. However, the full ontology is publically available in OWL/XML format at our Github repository5. In this case object properties in which the domain and range are the same class have been represented as UML class methods. The concept dictionary is simply a list of words related to the concept. Roughly speaking the members of a given concept dictionary are related by three relationships namely synonymy, hyponymy/hypernymy and meronymy. Synonymy refers to semantically equivalent words such as \u201cskin\u201d and \u201cdermis\u201d, hypernymy is refers to increasing generalization for instance \u201clocation\u201d is a hypernym of \u201chouse\u201d, hyponymy is the inverse of hypernymy as in \u201chouse\u201d would be a hyponym of \u201clocation\u201d. Generally speaking immediate hyponyms and hypernyms are grouped together with concept synonyms. Meronyms are similarly grouped together, meronymy implies a part of relationship for instance \u201carm\u201d is a meronym of \u201cbody\u201d.\nHowever the principle relationship between members of a concept is thematic for instance the \u201ctreatment\u201d concept bundles together treatment nouns like \u201cstethoscope\u201d and treatment verbs like\n5 https://github.com/MarkMagumba/Twitter-Disease-incidence-Description-Language-Ontology\n\u201cdiagnosed\u201d. Also as an implication in some cases even antonyms are bundled together for instance the two word \u201clarge\u201d and \u201csmall\u201d are both bundled into the \u201cextent\u201d concept dictionary.\n3.2. Temporality: Temporality is important for determination of relevance for instance the message, \u201cI have the flu!\u201d is relevant whilst \u201cI have had the flu only once in my life\u201d though referring to an actual case of the flu is not relevant. Time is dealt with by the time object which is an abstract object. Time is separated into two periods namely the window and other temporal references. The window refers to references to disease events that fall approximately within the communicable period which is the time period within which a disease is likely to be spread from one host to another. In this time it can be said there is an active case of a disease or reference is made to recent case of a disease with a maximum span of a few weeks. The window includes words like \u201ctoday\u201d and \u201cyesterday\u201d whereas other temporal references include words like \u201cyear\u201d and \u201cdecades\u201d where it is extremely unlikely that reference is being made to an ongoing case of a disease. Furthermore we take into account different facets of event history in the form of aspectual predication similarly to TimeML [Sauri et al 2005] which defines five event states shown here with examples (keywords in caps)\n1. Initiation (the event is beginning e.g. \u201cFlu season has BEGUN\u201d), 2. Re-initiation (an event that stops then starts again e.g. \u201cI am falling sick AGAIN!\u201d), 3. Continuation (the event is continuing e.g. \u201cI am STILL down with the flu\u201d) 4. Termination (An event that would otherwise continue is stopped e.g. \u201cI stopped my pain\nmedication\u201d) 5. Culmination (A process coming to conclusion e.g. \u201cI completed my therapy!\u201d)\nIn our work we make no distinction between initiation and re-initiation and combine them into a single \u201cCommencement\u201d concept. In the case of general events we do not make a distinction between culmination and termination. However in the case of disease events, we take care of culmination via two specialized event types namely Mortality (A patient dying from a disease) and Recovery (A patient going from being sick to being healthy. In addition as seen in figure 3 we define certain trend events which describe the direction of change of state subject to some interpretation and these are of three general types namely increment (A net positive change) ,stasis (No net change) and reduction (A net negative change).\nEvents also have duration and periodicity. Durations can be expressed in time lengths (e.g. \u201cdays\u201d, \u201cweeks\u201d as in the window or in relative terms such as \u201cuntil\u201d, \u201csince\u201d), periodicity refers to frequency for instance words such as \u201cnever\u201d and \u201calways\u201d. Finally temporality is dealt with by tensing. Generally we take advantage of English structure by relying on agreement of tense. For instance we do not make a distinction between the two words \u201covercoming\u201d and \u201covercame\u201d as these are simply returned as references to recovery. This distinction is only made for a few common words such as \u201chave\u201d and the verb to be, where mortality occurs in the same sentence as \u201chave\u201d it can be concluded that mortality must exist in the past tense. Tensing is further strengthened by Part of Speech Enriched Padding discussed in section 4.2.\n3.3. Locality: As depicted in figure 3, events also have locality. Locality is indicated by words such as home, school, state and in other cases is easily inferred particularly when users speak in the first person as even without an explicit reference to locality it is easily deducible that the user\u2019s location is also the disease event location . It is also denoted by prepositions like near, behind and adjacent. However in many cases the location is indeterminable. For a tweet to be classified as relevant both time and locality must be determinable.\n3.4. Magnitude Two types of magnitudes are catered for that is discrete quantities and extents. Quantities refer to countable magnitudes like number of victims and inexact quantization for instance when quantities are conveyed vaguely via words like \u201cmany\u201d and \u201cseveral\u201d. We do not perform any quantification and only keep track of quantification words for two particular purposes firstly where quantities are used as indicators of persons for instance in the sentence \u201cHUNDREDS contract deadly norovirus\u201d and secondly\nwhere quantities are used for negation for instance \u201cNONE of my workmates has zika virus\u201d. As with temporality, where quantities are used to represent persons we divide them by scale into two groups. The two groups are smaller quantities up to thousands and quantities larger than thousands. This is from the observation that disease incidence is reported in small numbers of up to a few thousand. Statements that refer to disease incidence in larger quantities such as \u201cMillions die from Malaria in Africa every year\u201d and \u201cBillions lost due to norovirus annually\u201d are generally informational and serve no purpose in a disease incidence monitoring sense. Extents on the other hand refer to quantification by degree via words like large, mild, complete and partial.\n3.5. General Order All objects are also subject to order which can be spatial, temporal or otherwise and is conveyed by temporal connectives such as before, after, while and positional adjectives such as fast, last, second, next and preceding.\n3.6. Linguistic Elements Linguistic elements include additional concepts such as exclamation, expletives (e.g. \u201cbitch\u201d, \u201cDamn\u201d), prepositions which are further broken down for our purposes. For instance even though the two words \u201con\u201d and \u201coff\u201d are grammatically both prepositions they are semantically opposite, the same applies to conjunctives such as \u201cbut\u201d and \u201cand\u201d which have totally different implications, and determiners (this, that, those) which may also carry different contextual implications. Additional concepts include negation which includes words such as \u201cnot\u201d, \u201cneither\u201d and \u201cnone\u201d.\n3.7. Semantic Ambiguity In some cases there is semantic ambiguity for instance the word \u201cfall\u201d can imply a kind of motion as in \u201cI am falling down\u201d and also getting sick for instance \u201cI am falling sick\u201d. It would be inaccurate to bundle it with other motion verbs like \u201cjump\u201d as that would be to tell the learner that they always imply similar connotations. Particularly in the case of polysemy, words may present different meanings meaning they can legitimately belong to multiple concepts, in this situation the most semantically accurate approach is to create special single word concepts.\n4.0. Feature Generation: We transform each tweet into a vector of features as follows. Firstly we flatten out our ontology into a set of concepts. As stated in section 3.1 each concept is associated with a group of words or tokens referred to as the concept dictionary. Each concept is effectively a list of words and the full ontology is basically a list of lists. In this sense it is a heavily redacted English dictionary containing only words we consider to be of epidemiological relevance. We also avoid technical terminology like names of drugs and specific diseases and focus on words that would appear in neighboring contexts.\nTo obtain the feature vector we simply tokenize each tweet and for each token we do a dictionary look up in our flattened ontology. If the token exists in the ontology we simply replace it with the corresponding concept. Figure 4 below is a pseudo code summary of the approach. We refer to the resulting representation as the Concept Normal Form (CNF). The result is that we can represent any document using a fixed set of concepts. Effectively we have a fixed size lexicon regardless of the original contents of any document or tweet.\nAs an example the sentence \u201cI have never had the flu\u201d is transformed into \u201cSELF_REF HAVE FREQUENCY HAVE THE OOV\u201d. SELF_REF refers to \u201cSelf references\u201d which is the concept class for terms that persons use to refer to themselves such as \u201cI\u201d, \u201cWe\u201d and \u201cUs\u201d used as an indicators of speaking in the first person, \u201cHAVE\u201d is the concept class for \u201chave\u201d or \u201chad\u201d which is a special concept class since the verb \u201cto have\u201d is conceptually ambiguous as it can legitimately indicate two senses that is falling sick or possession. The \u201cFREQUENCY\u201d terms refers to a reference to frequency concept which denotes temporal periodicity.\nThe \u201cOOV\u201d term at the end of the CNF representation stands for \u201cOut of Vocabulary\u201d. To arrive at the concept representation we merely perform a simple list lookup; for each term we iterate through all the categories to see if it exists in any category if it does then that category label is returned, otherwise the \u201cOOV\u201d flag is returned. Using our object model we generate 1531 terms corresponding to 136 categories for our training data which has a vocabulary of about 59,000 words. Needless to say most words are out of vocabulary. Rather than completely ignore these we introduce a two-step categorization as described in section 4.1.\n4.1. Part of Speech Enriched Padding Instead of completely ignoring out of vocabulary tokens we replace them with their part of speech tag therefore our previous example, \u201cI have never had the flu\u201d becomes \u201cSELF_REF HAVE FREQUENCY HAVE THE NN\u201d. For Part of Speech tagging we employ the Penn Tree Bank Part of Speech tags [Taylor et al 2003] in addition to some special tags for twitter specific phenomena such as \u201cRT\u201d for re-tweet, USR for users denoted by tokens beginning with \u201c@\u201d, HT for \u201chashtag\u201d denoted by tokens beginning with \u201c#\u201d and \u201cURL\u201d for http universal resource locators denoted by tokens beginning with \u201chttp\u201d. Figure 5 below depicts the transformations for the message \u201cI have never had the flu!\u201d\nThe value of this step is that it introduces more robust tensing because the Penn tree Bank standard differentiates between tenses and as stated before we can take advantage of agreement of tense in English language. As a result of the huge lexical diversity of twitter in some cases it is more useful to cluster terms using their part of speech information. Furthermore, state of the art part of speech tagging has been shown to be at least 90% accurate. This means that even though many terms will be out of vocabulary we are still able to determine any word\u2019s grammatical category with a fair amount of certainty and the learning algorithm can potentially learn more for instance without this step, \u201cI have never had the flu\u201d and \u201cWe have often had dinner parties\u201d are indistinguishable as they would have equivalent CNF representations. For part of speech tagging we employ the GATE6 twitie tagger application [Cunningham et al 2002; Derczynski et al 2013] which is a variant of the log linear maximum entropy Stanford Part of Speech Tagger [Toutanova and Manning 2000; Toutanova et al 2003] trained on twitter data.\n6 General Architecture for Text Engineering\n5.0. Methodology: The following is a pipeline for generic disease incidence detection on Twitter using our ontological approach. Figure 6 below summarizes the steps\n5.1. Corpus Generation: The pipeline is depicted in figure 6. The first step is the creation of the corpus. We obtain tweets from a basic twitter account using some specific keywords via a python script through Twitter\u2019s Streaming API. The tweets we download are those that are marked as public, this is the default security level and they are only marked private if expressly indicated by users. Next we partition the tweets into training and testing data sets. We created corpora for four diseases namely influenza + common cold + Listeria, stomach flu (gastro enteritis), norovirus, conjunctivitis 1 and conjunctivitis 2. There are two conjunctivitis data sets because for some tweets it is referred to as \u201cpink eye\u201d (conjunctivitis 2) and we were interested what impact such small linguistic differences may have on our models.\nAt this phase we eliminated duplicates as much as possible by removing any tweets with the \u201cRT\u201d tag (retweets) and manually reading through the tweets as many tweets were practically the same even though they did not have this tag differing only in minor details like the urls they contained. At the annotation phase we simply label each tweet as either positive or negative, where a positive tweet is a reference to an actual case of disease in the required time window which is basically the present time or the previous past to a period of a few weeks.\nThe influenza + common cold + Listeria data set comprises 13004 tweets of which 57.8% are positive and 43.2% are negative, The gastro enteritis data set comprises 646 tweets of which 84% are positive and 16% are negative, the noro virus data set comprises 1288 tweets of which 51% are positive and 49% are negative, conjunctivitis 1 comprises 656 tweets of which 55% are positive and 45% are negative and conjunctivitis 2 comprises 721 tweets of which 62.5% are positive and 37.5% are negative. The influenza + common cold + Listeria data set is used as the training data set and the other data sets as the evaluation data sets.\n5.2. Preprocessing: This involved transforming all tweets to lower case, removal of punctuations and part of speech tagging. For normalization and part of speech tagging we used GATE framework\u2019s Twitie tagger application after which we performed feature extraction using the method described in section 4.\n5.3. Model Training: We employed the following methods, first we did a Bag of Words (BOW) model using our original, untransformed dataset with a Stochastic Gradient Descent (SGD) classifier to obtain a baseline, then we extracted features using the procedure in section 4 and created a unigram Bag of Words model, a uningram + bigram model and a Doc2Vec + Logistic Regression classifier. For the Bag of Words models we used the SGD classifier with the same parameter settings.\nUsing Distributed Word Embeddings for text classification is a two-step process, first word vectors must be learned for each word in the vocabulary which is an unsupervised step then these vectors can be fed into a supervised learning method. The Doc2Vec algorithm is an extension of word2Vec first described by Mikolov [2013]. The word2vec algorithm takes a collection of documents and returns a vector (word embedding) for each word. The similarity between two words can be calculated from the cosine distance between their word embeddings. Generally vectors of words that are close in meaning are also close in value. It has also been shown that word2vec is capable of preserving non trivial semantic relationships between words for instance the vector for \u201cBrother\u201d \u2013 \u201cMan\u201d + \u201cWoman\u201d produce a result that is close to \u201cSister\u201d (Wikipedia, 2016). Word2vec may employ one of two approaches, either skip gram or Continuous Bag of Words (CBOW). DocVec extends word2vec to entire documents. Roughly speaking, there are two ways in which this can be achieved. That is by computing the average of context word vectors or the concatenation of context vectors. In our case we use the former approach.\nThere are several parameters that need to be tuned to obtain good word vectors but some of the more important ones include the size, the window and whether or not to use negative sampling and the number of noise words to be drawn. The size refers to the dimensionality of the word vectors. Generally, the more dimensions the better the performance up to some point where additional dimensions do not significantly alter performance. The window specifies how many context words to use. For instance if the window size is set to \u03b2, then for the CBOW architecture if the current word is at index i, the task is to predict the most likely words at positions i- 1, i \u2013 2.\u2026.i \u2013 \u03b2 and i + 1, i + 2\u2026..i + \u03b2. For skip gram architecture the learning task is the inverse that is to predict the most likely ith word given the words at positions i- 1, i \u2013 2\u2026.i \u2013 \u03b2 and i + 1, i + 2\u2026..i + \u03b2. The corresponding architectures to CBOW and skip gram in DocVec are distributed memory and distributed bag of words. In addition we found that increasing the number of training epochs improved the results. We obtained our best results with 200 dimensions, with negative sampling with 8 noise words, a context window of 5, and with 20 epochs with distributed memory. For a deeper explanation of these parameters see (Rehurek, 2016).\nWhen dealing with individual words determining model quality is straight forward for instance you would expect the vectors for \u201chuman\u201d and \u201cperson\u201d to be close but when dealing with coarse grained concepts as with our ontology we can\u2019t apply the same reasoning directly. This is because the concepts actually abstract several words as with the example given in section 3.1 of the treatment concept which bundles all treatment related words including treatment nouns and treatment verbs in different tenses. It becomes very difficult to determine for instance whether this concept is semantically closer to the immunity concept which bundles all words related to immunity or the morbidity concept which bundles all words that relate to the state of being sick such as \u201cillness\u201d and all symptoms we are presently able to list.\nHowever since the final representation is a combination of concepts and Part of Speech tags for out of vocabulary words, we can instead use the Part of Speech tags where it is more straightforward to determine such similarity. For instance in a good model we would expect that the closest \u201cconcepts\u201d to the NN (Noun, Singular) tag are NNS (Noun Plural) and NNP (Proper Noun Singular) and the most similar to JJ (Adjective) would be JJR (Adjective, Comparative) and JJS(Adjective, Superlative). In our model we find the closest concepts to NN are indeed NNP and NNS but for JJ we find NN and NNP to be closer than JJR and JJS which do not feature among the ten most similar concepts to JJ. This is not linguistically optimal but understandable. The word2vec algorithm relies on the distributional properties of individual tokens and adjectives occur in the same contexts as nouns and therefore would appear close in meaning to nouns. In addition as per our training data there are very few occurrences of JJS and JJR in comparison to JJ and NN. Nonetheless as we show in section 5.4, the learned vectors dramatically improve the results.\nFor Doc2Vec we employ our full corpus with the labels removed for the unsupervised portion. For the supervised learning portion we employ a logistic regression classifier. All code is written in python and we use the Scikit-learn package for all machine learning tasks [Pedregosa et al 2011] except the Doc2Vec algorithm for which we use the gensim package [Rehurek and Sojka 2010].\n5.4. Model Evaluation: The purpose of our ontology is to arrive at some symbolic representation of any document for the purpose of disease incidence monitoring. The resulting notation referred to here as the Concept Normal Form effectively compresses the vocabulary into a fixed length lexicon corresponding to a fixed set of conceptual objects presented in the ontology. To prove the quality and completeness of our representation we must show that there is no significant loss of information as a result of this transformation. We consider the task of differentiating tweets that contain a relevant reference from those that do not from a corpus of tweets containing references to diseases.\nFor a tweet to be relevant it must contain a reference to an ongoing or recent (not exceeding a maximum time span of a few weeks) disease event and its location must be determinable and not aggregated beyond a users\u2019 geographical country. All other references are deemed irrelevant. To test our model we compare results obtained on the baseline against those obtained with our ontology features. In addition we investigate the impact of using N-grams and word vectors in concert with our Concept Normal Form representation. Since the numbers of positive and negative examples in these data sets are imbalanced, we opt for precision, recall and f-measure as the performance metrics. For results on the training data set we used 10 fold cross validation. We are interested in the overall performance as well as the variance in performance across datasets as we have developed our ontology so as to minimize the effect of lexical differences between documents. For each method we use a single model trained on the flu + Listeria data set to classify all datasets and take a simple mathematical, unweighted average as the overall performance across all datasets. We also employ the performance variance as a measure of the impact of lexical differences between the data sets. A small variance means the classifier\u2019s performance on different data sets is similar and that our concept model is sufficiently general.\n6.0. Results The results are summarized in the table below:\nThe results overall indicate that better generalization is achieved with our method as the results variance is overall smaller when the CNF notation is used. For instance there is a nearly 700% reduction in model performance variance across data sets in comparison to the baseline just by transforming the data sets into the CNF representation without changing any other model parameter using unigram bag of words. The implication is that using the CNF, it is possible to create good generic classifiers for disease detection even on noisy data with high lexical diversity such as tweets.\nWe find there is little benefit in applying N-grams in terms of performance improvement across datasets by F1-measure. The only marked improvement occurs on the training data and in general anything beyond bigrams does not return any noticeable gains. In comparison to the unigram model there appears to be an increase in precision combined with a reduction in recall. However, the most remarkable results by far are produced when we use Doc2Vec in combination with the CNF representation. As shown in Table 1, it is the strongest approach both in terms of maximizing overall classification performance and minimizing performance variance across data sets. However, as expounded in section 5.3 it requires giving some thought to what a good word vector model would mean in terms of a conceptual representation. As a matter of fact some of our initial models returned extremely poor results although generally speaking it is our observation that word vectors improve the performance. We should also note that we have performed very little tuning beyond tuning the CNF\nword vectors. This was because from the onset we aimed to produce a general classifier but none the less we find the results to be remarkably good especially on the testing data sets.\nFigure 7, shows the precision recall behavior for the model on the different data sets for the best performing Doc2Vec model at different probability thresholds. The results in table 1 are for the default threshold of 0.5. The Logistical Regression classifier actually outputs a probability. For a given threshold, any results with a higher probability of being relevant than the threshold are classified positive. The larger the area under the curve (AUC) on the Precision Recall Curve the better the classifier in terms of Precision and Recall. Our results therefore, as indicated in the summary table in the plot, are testament to the strength of the Doc2Vec approach.\n7.0. Discussion: In information retrieval experiments care is usually taken to ensure that the vocabulary of the training\nand testing data are similar, in our case we have deliberately complicated the task by using a different\nsets of diseases to train and test. This was to simulate what would happen if our model encountered\ndata with a divergent lexicon as would be the case if it were deployed into an online system. The results\nshow that our ontology driven approach is more robust than word level lookup methods like unigram\nbag of words.\nFurthermore, the discovery that word2vec/Doc2vec vectors learned from our ontology concepts give\ngood classification performance is of great significance. This is because although word2vec/Doc2vec\nembeddings have been shown to dramatically improve the performance of text classification tasks, they\nrely on word level look up and there is no way to compute vectors for words that do not appear in the\ntraining corpus. However, the CNF is capable of representing any document using a fixed length\nvocabulary that is a concatenation of our ontology concepts and part of speech information. The fixed\nlength vocabulary implies that there can be no out of vocabulary tokens. Therefore, provided good\nvectors can be obtained for each concept very robust models can be obtained.\n8.0. Conclusion: We have described an ontology of concepts used to communicate disease incidence. We employed the ontology generate features for a classification model to partition a corpus of tweets containing mentions of diseases into relevant and irrelevant tweets. We show that our ontology based method not only allows for deeper semantic criteria to be employed. It also not only achieves high performance but results in more robust models whose performance is more stable on lexically divergent data sets.\n9.0. Acknowledgements: In addition I would like to thank the NORHED (Norwegian Program for Capacity building in Higher Education and Research development) for funding this research through its HI-TRAIN (Health Informatics Training and Research in East Africa for Improved Health Care) project.\n10.0. References: BRIGHTPLANET.COM, 2013. Twitter Firehose Vs Twitter API: What\u2019s the Difference and Why You Should Care?, retrieved July 17th 2016 from https://brightplanet.com/2013/06/twitter-firehose-vs-twitter-apiwhats-the-difference-and-why-should-you-care/\nBUCKERIDGE, D. L., OKHMATOVSKAIA, A., TU, S., O\u2019CONNOR, M., NYULAS, C., and MUSEN, M. A., 2008. Understanding Detection Performance In Public Health Surveillance: Modelling Aberrancy-Detection Algorithms, Journal of the American Medical Informatics Association, 15(6), pp: 760\u2013769, DOI: 10.1197/jamia.M2799, retrieved July 17th 2016 from http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2585528/\nCALLISON-BURCH, C. and DREDZE, M., 2010. Creating speech and language data with amazon\u2019s mechanical turk, In Workshop on Creating Speech and Language Data With Mechanical Turk at NAACLHLT, June 06-06, 20101, pp: 1-12,\nCOLLIER, N., DOAN, S., KAWAZOE, A., GOODWIN, R.M., CONWAY, M., TATENO, Y., et al., 2008. Biocaster: Detecting Public Health Rumors With A Web-Based Text Mining System. Bioinformatics, 24(24), pp: 2940-2941. DOI:10.1093/bioinformatics/btn534.\nCOLLIER, N., KAWAZOE, A., JIN, L., SHIGEMATSU, M., DIEN, D., BARRERO, A.R. et al., 2007. A Multilingual Ontology For Infectious Disease Outbreak Surveillance: Rationale, Design And Challenges, Journal of Language Resources and Evaluation, 40(405), DOI: 10.1007/s10579-007-9019-7 retrieved July 17th 2016 from http://link.springer.com/article/10.1007/s10579-007-9019-7\nCONWAY M., DOAN, S., KAWAZOE, A., and COLLIER, N., 2009. Classifying Disease Outbreak Reports using N-grams and Semantic features, International Journal of Medical Informatics. 78(12), DOI: 10.1016/j.ijmedinf.2009.03.010 retrieved July 17th 2016 from http://www.sciencedirect.com/science/article/pii/S1386505609000537\nCUNNINGHAM, H., MAYNARD, D., BONTCHEVA, K., and TABLAN, V., 2002. \u2018Gate: An Architecture For Development Of Robust HLT Applications\u2019 In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, Philadelphia, USA, July, 2002, pp: 168\u2013175.\nDERCZYNSKI, L., RITTER, A., CLARK, S. and BONTCHEVA, K., 2013. \u2018Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data\u2019, in Proceedings of the International Conference on Recent Advances in Natural Language Processing, September, 2013, Hisar, Bulgaria, pp: 7-13\nDREDZE, M., CHENG, R., PAUL. M.J. and BRONIATOWSKI, D., 2014., \u2018HealthTweets.org: A Platform for Public Health surveillance Using Twitter\u2019, in Proceedings Of The Twenty-Eighth AAAI Conference On\nArtificial Intelligence And The Twenty-Sixth Innovative Applications Of Artificial Intelligence Conference, July 27-31, Quebec, Canada\nEISENSTEIN, J., 2013. \u2018What To Do About Bad Language on the Internet\u2019, In Proceedings of NAACL-HLT, June 09-14, Atlanta, Georgia, pp: 359\u2013369\nGOOGLE.COM, 2016. retrieved July 17th 2016 from https://www.google.org/flutrends/about/\nHARRIS, Z., 1954. Distributional structure, Word, 10(23), pp: 146\u2013162\nJARVIS, B., 2013. Twitter Becomes a Tool for Tracking Flu Epidemics and Other Public Health Issues, The Washington Post , retrieved July 17th 2016 from https://www.washingtonpost.com/national/healthscience/twitter-becomes-a-tool-for-tracking-flu-epidemics-and-other-public-healthissues/2013/03/04/9d4315c2-6eef-11e2-aa58-243de81040ba_story.html\nKHOURY M.J. and DORMAN, J.S., 1998. The Human Genome Epidemiology Network. American Journal Of Epidemiology, 148(1), pp: 1-3\nLAMPOS, V. and CRISTIANINI, N., 2010. \u2018Tracking The Flu Pandemic By Monitoring The Social Web\u2019. In proceedings of 2nd Workshop on Cognitive Information Processing (CIP 2010), June 14-16, Naregno Elba island, Italy, pp: 411-416, DOI 10.1109/CIP.2010.5604088\nLEE, K., AGRAWAL, A. and CHOUDARY, A., 2013. \u2018Real Time Disease Surveillance Using Twitter Data: Case Study Flu and Cancer\u2019, in proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Chicago, Ilinois, USA, August 11-14, pp: 1474 \u2013 1477\nMIKOLOV, T., CHEN, K., CORRADO, G. and DEAN, J., 2013. \u2018Efficient Estimation Of Word Representations In Vector Space\u2019, In Workshop Proceedings of the International Conference on Learning Representations, Scottsdale, Arizona, USA, May 02-04\nOKHMATOVSKAIA, A., CHAPMAN, W., COLLIER, N., ESPINO, J. and BUCKERIDGE, D. L., 2009. SSO: The Syndromic Surveillance Ontologym retrieved July 17th 2016 from http://bioontology.stanford.edu/sites/default/files/SSO.pdf\nOSBORNE, J.D., FLATOW, J., HOLKO, M., LIN, S.M., KIBBE, W.A., ZHUE, L. et al., 2009. Annotating The Human Genome With Disease Ontology, BMC genomics, 10(1) retrieved July 17th 2016 from http://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-10-S1-S6\nPAUL, M.J., DREDZE, M., 2014. Discovering Health Topics in Social Media Using Topic Models. PLoS ONE 9(8), retrieved July 17th 2016 from http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0103408\nPEDREGROSA, F., VAROQUAUX, G., GRAMFORT, A., MICHEL, V., THIRION, B., GRISEL, O., et al, 2011. Scikit-learn: Machine Learning in Python, Journal of Machine Learning. vol(12), pp: 2825-2830\nPESQUIRA, C., FERREIRA, J.D, COUTO, M.F. and SILVA, M.J., 2014. The Epidemiology Ontology: An Ontology for Semantic Annotation of Epidemiological Resources, Journal of BioMedical Semantics, 5(4), retrieved July 17th 2016 from https://jbiomedsem.biomedcentral.com/articles/10.1186/2041-1480-5-4, DOI: 10.1186/2041-1480-5-4\nPORTA, M. (ed.), 2008. A Dictionary of Epidemiology, Oxford University Press, New York\nREHUREK, R., and SOJKA, P., 2010. \u2018Software Framework for Topic Modeling with Large Corpora\u2019, In proceedings of the LREC Workshop on New Challenges for NLP Frameworks, Valetta, Malta, May 17-23, pp: 46- 50\nREHUREK, R., 2016. Models.Word2vec \u2013 Deep Learning With Word2vec, Available at https://radimrehurek.com/gensim/models/word2vec.html , [Accessed on 7/17/16]\nSAUR\u00cd R., LITTMAN, J., KNIPPEN, B., GAIZAUSKAS, R., SETZER, A., and PUSTEJOVSKY J., 2005. TimeML Annotation Guidelines, Available at http://www.timeml.org , [Accessed on 7/17/16]\nSCHMIDT C.W., 2012. Trending Now: Using Social Media to Predict and Track Disease Outbreaks, Environ Health Perspective, 120(1), Available at http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3261963/, [Accessed on 7/17/16] DOI: 10.1289/ehp. 120-a30\nSCHRIML, L.M., ARZE, C., NADENDLA, S., GANAPATHY, A., FELIX, V., MAHURKA, A. et al., 2010. GeMInA, Genomic Metadata for Infectious Agents, a geospatial surveillance pathogen database, Nucleic Acids Research, 38(1)\nTAYLOR, A., MARCUS, M. and SANTORINI, B., 2003. \u2018The Penn Treebank: An Overview\u2019, In Treebanks: Building and Using Parsed Corpora, pp: 5-22, Springer, Netherlands, Dordrecht\nTOUTANOVA, K. and MANNING, C.D., 2000. \u2018Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger\u2019 in Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000), Hong Kong\nTOUTANOVA, K., KLEIN, D., MANNING, C.D. and SINGER, Y., 2003. \u2018Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network\u2019 in Proceedings of HLT-NAACL 2003, pp: 252-259.\nWIKIPEDIA.ORG, Word2Vec, retrieved July 17th from https://en.wikipedia.org/wiki/Word2vec\nYANG, B., SAYERS, S., XIANG, Z., and HE, Y., 2011. Protegen: A Web-Based Protective Antigen Database And Analysis System, Nucleic Acid Research, 39(Database issue), Available at https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3013795/, [Accessed on 23/9/16], DOI:10.1093/nar/gkg944"}], "references": [{"title": "Creating speech and language data with amazon\u2019s", "author": ["C. CALLISON-BURCH", "M. DREDZE"], "venue": null, "citeRegEx": "CALLISON.BURCH and DREDZE,? \\Q2010\\E", "shortCiteRegEx": "CALLISON.BURCH and DREDZE", "year": 2010}, {"title": "Classifying Disease Outbreak Reports", "author": ["CONWAY M", "S. DOAN", "A. KAWAZOE", "N. COLLIER"], "venue": null, "citeRegEx": "M. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "M. et al\\.", "year": 2009}, {"title": "Gate: An Architecture", "author": ["H. CUNNINGHAM", "D. MAYNARD", "K. BONTCHEVA", "V. TABLAN"], "venue": null, "citeRegEx": "CUNNINGHAM et al\\.,? \\Q2002\\E", "shortCiteRegEx": "CUNNINGHAM et al\\.", "year": 2002}, {"title": "Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data", "author": ["L. DERCZYNSKI", "A. RITTER", "S. CLARK", "K. BONTCHEVA"], "venue": "Proceedings of the International Conference on Recent Advances in Natural Language Processing, September,", "citeRegEx": "DERCZYNSKI et al\\.,? \\Q2013\\E", "shortCiteRegEx": "DERCZYNSKI et al\\.", "year": 2013}, {"title": "Twitter Becomes a Tool for Tracking Flu Epidemics and Other Public Health Issues, The Washington Post", "author": ["Z. HARRIS"], "venue": "Distributional structure,", "citeRegEx": "HARRIS,? \\Q1954\\E", "shortCiteRegEx": "HARRIS", "year": 1954}, {"title": "The Human Genome Epidemiology Network", "author": ["KHOURY M.J", "J.S. DORMAN"], "venue": "American Journal Of Epidemiology,", "citeRegEx": "M.J. and DORMAN,? \\Q1998\\E", "shortCiteRegEx": "M.J. and DORMAN", "year": 1998}, {"title": "Real Time Disease Surveillance Using Twitter Data: Case Study Flu and Cancer\u2019, in proceedings of the 19 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Chicago, Ilinois", "author": ["K. LEE", "A. AGRAWAL", "A. CHOUDARY"], "venue": "USA, August 11-14,", "citeRegEx": "LEE et al\\.,? \\Q2013\\E", "shortCiteRegEx": "LEE et al\\.", "year": 2013}, {"title": "SSO: The Syndromic Surveillance", "author": ["A. OKHMATOVSKAIA", "W. CHAPMAN", "N. COLLIER", "J. ESPINO", "D.L. BUCKERIDGE"], "venue": "Scottsdale, Arizona,", "citeRegEx": "OKHMATOVSKAIA et al\\.,? \\Q2009\\E", "shortCiteRegEx": "OKHMATOVSKAIA et al\\.", "year": 2009}, {"title": "Human Genome With Disease Ontology, BMC genomics", "author": ["M.J. PAUL", "M. DREDZE"], "venue": null, "citeRegEx": "PAUL and DREDZE,? \\Q2016\\E", "shortCiteRegEx": "PAUL and DREDZE", "year": 2016}, {"title": "The Epidemiology Ontology: An Ontology for Semantic Annotation of Epidemiological Resources", "author": ["C. PESQUIRA", "FERREIRA", "J.D", "M.F. COUTO", "M.J. SILVA"], "venue": "Journal of BioMedical Semantics,", "citeRegEx": "PESQUIRA et al\\.,? \\Q2014\\E", "shortCiteRegEx": "PESQUIRA et al\\.", "year": 2014}, {"title": "Software Framework for Topic Modeling with Large Corpora\u2019, In proceedings of the LREC Workshop on New Challenges for NLP Frameworks, Valetta, Malta, May 17-23", "author": ["R. REHUREK", "P. SOJKA"], "venue": "REHUREK,", "citeRegEx": "REHUREK and SOJKA,? \\Q2010\\E", "shortCiteRegEx": "REHUREK and SOJKA", "year": 2010}, {"title": "TimeML Annotation Guidelines, Available at http://www.timeml.org", "author": ["SAUR\u00cd R", "J. LITTMAN", "B. KNIPPEN", "R. GAIZAUSKAS", "A. SETZER", "PUSTEJOVSKY J"], "venue": "SCHMIDT C.W.,", "citeRegEx": "R. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "R. et al\\.", "year": 2005}, {"title": "GeMInA, Genomic Metadata for Infectious Agents, a geospatial surveillance pathogen database", "author": ["L.M. SCHRIML", "C. ARZE", "S. NADENDLA", "A. GANAPATHY", "V. FELIX", "A MAHURKA"], "venue": "Nucleic Acids Research,", "citeRegEx": "SCHRIML et al\\.,? \\Q2010\\E", "shortCiteRegEx": "SCHRIML et al\\.", "year": 2010}, {"title": "Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger", "author": ["Springer", "Netherlands", "K. Dordrecht TOUTANOVA", "C.D. MANNING"], "venue": "Building and Using Parsed Corpora,", "citeRegEx": "Springer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Springer et al\\.", "year": 2000}, {"title": "Cyclic Dependency Network", "author": ["B. YANG", "S. SAYERS", "Z. XIANG", "HE"], "venue": "[Accessed on 23/9/16],", "citeRegEx": "YANG et al\\.,? \\Q2003\\E", "shortCiteRegEx": "YANG et al\\.", "year": 2003}], "referenceMentions": [], "year": 2016, "abstractText": "In this work we address the issue of generic automated disease incidence monitoring on twitter. We employ an ontology of disease related concepts and use it to obtain a conceptual representation of tweets. Unlike previous key word based systems and topic modeling approaches, our ontological approach allows us to apply more stringent criteria for determining which messages are relevant such as spatial and temporal characteristics whilst giving a stronger guarantee that the resulting models will perform well on new data that may be lexically divergent. We achieve this by training learners on concepts rather than individual words. For training we use a dataset containing mentions of influenza and Listeria and use the learned models to classify datasets containing mentions of an arbitrary selection of other diseases. We show that our ontological approach achieves good performance on this task using a variety of Natural Language Processing Techniques. We also show that word vectors can be learned directly from our concepts to achieve even better results.", "creator": "http://www.convertapi.com                 "}}}