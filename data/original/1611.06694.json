{"id": "1611.06694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2016", "title": "Training Sparse Neural Networks", "abstract": "Deep neural networks with lots of parameters are typically used for large-scale computer vision tasks such as image classification. This is a result of using dense matrix multiplications and convolutions. However, sparse computations are known to be much more efficient. In this work, we train and build neural networks which implicitly use sparse computations. We introduce additional gate variables to perform parameter selection and show that this is equivalent to using a spike-and-slab prior. We experimentally validate our method on both small and large networks and achieve state-of-the-art compression results for sparse neural network models.", "histories": [["v1", "Mon, 21 Nov 2016 09:24:24 GMT  (76kb,D)", "http://arxiv.org/abs/1611.06694v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["suraj srinivas", "akshayvarun subramanya", "r venkatesh babu"], "accepted": false, "id": "1611.06694"}, "pdf": {"name": "1611.06694.pdf", "metadata": {"source": "CRF", "title": "Training Sparse Neural Networks", "authors": ["Suraj Srinivas", "Akshayvarun Subramanya", "R. Venkatesh Babu"], "emails": ["surajsrinivas@grads.cds.iisc.ac.in", "akshayvarun07@gmail.com", "venky@cds.iisc.ac.in"], "sections": [{"heading": "Introduction", "text": "For large-scale tasks such as image classification, large networks with many millions of parameters are often used (Krizhevsky, Sutskever, and Hinton 2012), (Simonyan and Zisserman 2015), (Szegedy et al. 2015). However, these networks typically use dense computations. Would it be advantageous to use sparse computations instead? Apart from having fewer number of parameters to store (O(mn) toO(k))1, sparse computations also decrease feedforward evaluation time (O(mnp) to O(kp))2. Further, having a lower parameter count may help in avoiding overfitting.\nRegularizers are often used to discourage overfitting. These usually restrict the magnitude (`2/`1) of weights. However, to restrict the computational complexity of neural networks, we need a regularizer which restricts the total number of parameters of a network. A common strategy to obtain sparse parameters is to apply sparsity-inducing regularizers such as the `1 penalty on the parameter vector. However, this is often insufficient in inducing sparsity in case of large non-convex problems like deep neural network training as shown in (Collins and Kohli 2014). The contribution of this paper is to be able to induce sparsity in a tractable way for such models.\nThe overall contributions of the paper are as follows.\n\u2022 We propose a novel regularizer that restricts the total number of parameters in the network. (Section 2)\n1For a matrix of size m\u00d7 n with k non-zero elements 2For matrix-vector multiplies with a dense vector of size p\n\u2022 We perform experimental analysis to understand the behaviour of our method. (Section 4)\n\u2022 We apply our method on LeNet-5, AlexNet and VGG-16 network architectures to achieve state-of-the-art results on network compression. (Section 4)"}, {"heading": "Problem Formulation", "text": "To understand the motivation behind our method, let us first define our notion of computational complexity of a neural network.\nLet \u03a6 = {gs1, gs2, ..., gsm} be a set of m vectors. This represents an m-layer dense neural network architecture where gsi is a vector of parameter indices for the i\nth layer, i.e; gsi = {0, 1}ni . Here, each layer gsi contains ni elements. Zero indicates absence of a parameter and one indicates presence. Thus, for a dense neural network, gi is a vector of all ones, i.e.; gsi = {1}ni . For a sparse parameter vector, gsi would consist of mostly zeros. Let us call \u03a6 as the index set of a neural network.\nFor these vectors, our notion of complexity is simply the total number of parameters in the network.\nDefinition 1. The complexity of a m-layer neural network with index set \u03a6 is given by \u2016\u03a6\u2016 = m\u2211 i=1 ni.\nWe now aim to solve the following optimization problem.\n\u03b8\u0302, \u03a6\u0302 = arg min \u03b8,\u03a6\n`(y\u0302(\u03b8,\u03a6), y) + \u03bb\u2016\u03a6\u2016 (1)\nwhere \u03b8 denotes the weights of the neural network, and \u03a6 the index set. `(y\u0302(\u03b8,\u03a6), y) denotes the loss function, which depends on the underlying task to be solved. Here, we learn both the weights as well as the index set of the neural network. Using the formalism of the index set, we are able to penalize the total number of network parameters. While easy to state, we note that this problem is difficult to solve, primarily because \u03a6 contains elements \u2208 {0, 1}.\nGate Variables How do we incorporate the index set formalism in neural networks? Assume that the index set (Gs in Fig. 1) is multiplied pointwise with the weight matrix. This results in a weight matrix that is effectively sparse, if the index set has\nar X\niv :1\n61 1.\n06 69\n4v 1\n[ cs\n.C V\n] 2\n1 N\nov 2\n01 6\nlots of zeros rather than ones. In other words, we end up learning two sets of variables to ensure that one of them - weights - becomes sparse. How do we learn such binary parameters in the first place ?\nTo facilitate this, we interpret index set variables (Gs) as draws from a bernoulli random variable. As a result, we end up learning the real-valued bernoulli parameters (G in Fig. 1), or gate variables rather than index set variables themselves. Here the sampled binary gate matrix Gs corresponds exactly to the index set, or the \u03a6 matrix described above. To clarify our notation, G and g stand for the real-valued gate variables, while the superscript (.)s indicates binary sampled variables.\nWhen we draw from a bernoulli distribution, we have two choices - we can either perform a unbiased draw (the usual sampling process), or we can perform a so-called maximumlikelihood (ML) draw. The ML draw involves simply thresholding the values ofG at 0.5. To ensure determinism, we use the ML draw or thresholding in this work."}, {"heading": "Promoting Sparsity", "text": "Given our formalism of gate variables, how do we ensure that the learnt bernoulli parameters are low - or in our case - mostly less than 0.5 ? One plausible option is to use the `2 or the `1 regularizer on the gate variables. However, this does not ensure that there will exist values greater than 0.5. To accommodate this, we require a bi-modal regularizer, i.e; a regularizer which ensures that some values are large, but most values are small.\nTo this end, we use a regularizer given by w \u00d7 (1 \u2212 w). This was introduced by (Murray and Ng 2010) to learn binary values for parameters. However, what is important for us is that this regularizer has the bi-modal property mentioned earlier, as shown in Fig. 2a\nOur overall regularizer is simply a combination of this bimodal regularizer as well the traditional `2 or `1 regularizer for the individual gate variables. Our objective function is now stated as follows.\n\u03b8\u0302, \u03a6\u0302 = arg min \u03b8,\u03a6 `(y\u0302(\u03b8,\u03a6), y) + \u03bb1 m\u2211 i=1 ni\u2211 j=1 gi,j(1\u2212 gi,j)\n+\u03bb2 m\u2211 i=1 ni\u2211 j=1 gi,j\n(2)\nwhere gi,j denotes the jth gate parameter in the ith layer. Note that for gi,j \u2208 {0, 1}, the second term in Eqn. 2 vanishes and the third term becomes \u03bb\u2016\u03a6\u2016, thus reducing to Eqn.1."}, {"heading": "An Alternate Interpretation", "text": "Now that we have arrived at the objective function in Eqn.2, it is natural to ask the question - how do we know that it solves the original objective in Eqn.1 ? We shall now derive Eqn.2 from this perspective.\nAssuming the formulation of gate variables, we can rewrite the objective in Eqn.1 as follows.\n\u03b8\u0302, \u03a6\u0302 = arg min \u03b8,G\n`(y\u0302(\u03b8,Gs), y) + \u03bb m\u2211 i=1 ni\u2211 j=1 gsi,j (3)\ngsi,j \u223c bernoulli(gi,j), \u2200 i, j\nwhere gs is the sampled version of gate variables g. Note that Eqn.3 is a stochastic objective function, arising from the fact that gs is a random variable. We can convert this to a real-valued objective by taking expectations. Note that expectation of the loss function is difficult to compute. As a result, we approximate it with a Monte-Carlo average.\n\u03b8\u0302, \u03a6\u0302 = arg min \u03b8,G\n1\nt \u2211 t (`(y\u0302(\u03b8,Gs), y)) + \u03bb m\u2211 i=1 ni\u2211 j=1 gi,j\ngsi,j \u223c bernoulli(gi,j), \u2200 i, j\nwhere E(gsi,j) = gi,j . While this formulation is sufficient to solve the original problem, we impose another condition on this objective. We would like to minimize the number of Monte-Carlo evaluations in the loss term. This amounts to reducing [ 1t \u2211 t(`(y\u0302(\u03b8,G\ns), y)) \u2212 E(`(y\u0302(\u03b8,Gs), y))]2 for a fixed t, or reducing the variance of the loss term. This is done by reducing the variance of gs, the only random variable in the equation. To account for this, we add another penalty term corresponding to Var(gs) = g \u00d7 (1 \u2212 g). Imposing this additional penalty and then using t = 1 gives us back Eqn.2."}, {"heading": "Relation to Spike-and-Slab priors", "text": "We observe that our problem formulation closely resembles spike-and-slab type priors used in Bayesian statistics for variable selection (Mitchell and Beauchamp 1988). Broadly\nspeaking, these priors are mixtures of two distributions - one with very low variance (spike), and another with comparatively large variance (slab). By placing a large mass on the spike, we can expect to obtain parameter vectors with large sparsity.\nLet us consider for a moment using the following prior for weight matrices of neural networks.\nP (W ) = 1\nZ \u220f i exp(\u2212 (1\u2212 \u03b4(wi)))\u03b1 N (wi|0, \u03c32)1\u2212\u03b1\n(4) Here, \u03b4(\u00b7) denotes the dirac delta distribution, and Z denotes the normalizing constant, and \u03b1 is the mixture coefficient. Also note that like (Mitchell and Beauchamp 1988), we assume that wi \u2208 [\u2212k, k] for some k > 0. This is visualized in Fig. 2b. Note that this is a multiplicative mixture of distributions, rather than additive. By taking negative logarithm of this term and ignoring constant terms, we obtain\n\u2212 logP (W ) = \u2212\u03b1 \u2211 i (1\u2212 \u03b4(wi)) + 1\u2212 \u03b1 2\u03c32 \u2211 i w2i (5)\nNote that the first term in this expression corresponds exactly to the number of non-zero parameters, i.e; the \u03bb \u2016\u03a6\u2016 term of Eqn. 1. The second term corresponds to the usual `2 regularizer on the weights of the network (rather than gates). As a result, we conclude that Eqn. 4 is a spike-and-slab prior which we implicitly end up using in this method.\nEstimating gradients for gate variables How do we estimate gradients for gate variables, given that they are binary stochastic variables, rather than real-valued and smooth? In other words, how do we backpropagate through the bernoulli sampling step? Bengio et al. (2013) investigated this problem and empirically verified the efficacy of different possible solutions. They conclude that the simplest way of computing gradients - the straight-through estimator works best overall. Our experiments also agree with this observation.\nThe straight-through estimator simply involves backpropagating through a stochastic neuron as if it were an identity function. If the sampling step is given by gs \u223c bernoulli(g), then the gradient dg s\ndg = 1 is used. Another issue of consideration is that of ensuring that g always lies in [0, 1] so that it is a valid bernoulli parameter. Bengio et al. (2013) use a sigmoid activation function to achieve this. Our experiments showed that clipping functions worked better. This can be thought of as a \u2018linearized\u2019 sigmoid. The clipping function is given by the following expression.\nclip(x) =  1, x \u2265 1 0, x \u2264 0 x, otherwise\nThe overall sampling function is hence given by gs \u223c bernoulli(clip(g)), and the straight-through estimator is used to estimate gradients overall."}, {"heading": "Comparison with LASSO", "text": "LASSO is commonly used method to attain sparsity and perform variable selection. The main difference between the above method and LASSO is that LASSO is primarily a shrinkage operator, i.e.; it shrinks all parameters until lots of them are close to zero. This is not true for the case of spike-and-slab priors, which can have high sparsity and encourage large values at the same time. This is due to the richer parameterization of these priors."}, {"heading": "Practical issues", "text": "In this section we shall discuss some practical issues pertaining to our method. Our method ironically uses twice the number of parameters as a typical neural network, as we have two sets of variables - weights and gates. As a result, model size doubles while training. However, we multiply the two to result in sparse matrices which considerably reduces model size at test time. Essentially we do not have to store both sets of parameters while testing,only a element-wise product of the two is required. Even though the model size\ndoubles at train time, we note that speed of training / feedforward evaluation is not affected due to the fact that only element-wise operations are used.\nOur method can be applied to both convolutional tensors as well as fully connected matrices. However while performing compression, we note that convolutional layers are less susceptible to compression that fully connected layers due to the small number of parameters they possess."}, {"heading": "Related Work", "text": "There have been many recent works which perform compression of neural networks. Weight-pruning techniques were popularized by LeCun et al.(1989) and Hassibi et al.(1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively. Recently, Srinivas and Babu (2015a) proposed a neuron pruning technique, which relied on neuronal similarity. In contrast, we perform weight pruning based on learning, rather than hand-crafted rules.\nPrevious attempts have also been made to sparsify neural networks. Han et al.(2015) create sparse networks by alternating between weight pruning and network training. A similar strategy is followed by Collins and Kohli (2014). On the other hand, our method performs both weight pruning and network training simultaneously. Further, our method has considerably less number of hyper-parameters to determine (\u03bb1, \u03bb2) compared to the other methods, which have n thresholds to be set for each of the n layers in a neural network.\nMany methods have been proposed to train models that are deep, yet have a lower parameterisation than conventional networks. Denil et al.(2013) demonstrated that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al.(2014) propose an Adaptive Fastfood transform, which is an efficient reparametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Novikov et al.(2015) use tensor decompositions to obtain a factorization of tensors with small number of parameters. Cheng et al.(2015) make use of circulant ma-\ntrices to re-paramaterize fully connected layers. Some recent works have also focussed on using approximations of weight matrices to perform compression. Gong et al.(2014) use a clustering-based product quantization approach to build an indexing scheme that reduces the space occupied by the matrix on disk. Note that to take full advantage of these methods, one needs to have fast implementations of the specific parameterization used. One the other hand, we use a sparse parameterization, fast implementations of which are available on almost every platform.\nOur work is very similar to that of Architecture Learning (Srinivas and Babu 2015b), which uses a similar framework to minimize the total number of neurons in a neural network. On the other hand, we minimize the total number of weights."}, {"heading": "Experiments", "text": "In this section we perform experiments to evaluate the effectiveness of our method. First, we perform some experiments designed to understand typical behaviour of the method. These experiments are done primarily on LeNet-5 (LeCun et al. 1998). Second, we use our method to perform network compression on two networks - LeNet-5 and AlexNet. These networks are trained on MNIST and ILSVRC-2012 dataset respectively. Our implementation is based on Lasagne, a Theano-based library."}, {"heading": "Analysis of Proposed method", "text": "We shall now describe experiments to analyze the behaviour of our method. First, we shall analyze the effect of hyperparameters. Second, we study the effect of varying model sizes on the resulting sparsity.\nFor all analysis experiments, we consider the LeNet-5 network. LeNet-5 consists of two 5 \u00d7 5 convolutional layers with 20 and 50 filters, and two fully connected layers with 500 and 10 (output layer) neurons. For analysis, we only study the effects sparsifying the third fully connected layer.\nEffect of hyper-parameters In Section 2.1 we described that we used maximum likelihood sampling (i.e.; thresholding) instead of unbiased sampling from a bernoulli. In these experiments, we shall study the relative effects of hyperparameters on both methods. In the sampling case, sparsity is difficult to measure as different samples may lead to slightly different sparsities. As a result, we measure expected sparsity as well the it\u2019s variance.\nOur methods primarily have the following hyperparameters: \u03bb1, \u03bb2 and the initialization for each gate value. As a result, if we have a network with n layers, we have n+2 hyper-parameters to determine.\nFirst, we analyze the effects of \u03bb1 and \u03bb2. We use different combinations of initializations for both and look at it\u2019s effects on accuracy and sparsity. As shown in Table 5, both the thresholding as well as the sparsity-based methods are similarly sensitive to the regularization constants.\nIn Section 2.3, we saw that \u03bb1 roughly controls the variance of the bernoulli variables while \u03bb2 penalizes the mean. In Table 5, we see that the mean sparsity for the pair (\u03bb1, \u03bb2) = (0, 1) is high, while that for (1, 0) is considerably lower. Also, we note that the variance of (1, 1) is smaller than that of (0, 1), confirming our hypothesis that \u03bb1 controls variance.\nOverall, we find that both networks are almost equally sparse, and that they yield very similar accuracies. However, the thresholding-based method is deterministic, which is why we primarily use this method.\nTo further analyze effects of \u03bb1 and \u03bb2, we plot sparsity values attained by our method by fixing one parameter and varying another. In Figure 3b we see that \u03bb1, or the variancecontrolling hyper-parameter, mainly stabilizes the training by reducing the sparsity levels. In Figure 3c we see that increasing \u03bb2 increases the sparsity level as expected.\nWe now study the effects of using different initializations for the gate parameters. We initialize all gate parameters of a layer with the same constant value. We also tried stochastic initialization for these gate parameters (Eg. from a Gaussian distribution), but we found no particular advantage in doing so. As shown in Figure 3a, both methods seem robust to varying initializations, with the thresholding method consis-\ntently giving higher sparsities. This robustness to initialization is advantageous to our method, as we no longer need to worry about finding good initial values for them."}, {"heading": "Compression Performance", "text": "We test compression performance on three different network architectures - LeNet-5, AlexNet (Krizhevsky, Sutskever, and Hinton 2012) and VGG-16.\nFor LeNet-5, we simply sparsify each layer. As shown in Table 1, we are able to remove about 96% of LeNet\u2019s parameters and only suffer a negligible loss in accuracy. Table 2 shows that we obtain state-of-the-art results on LeNet-5 compression. For Proposed Method - 1, we used (\u03bb1, \u03bb2) = (0.001, 0.05), while for Proposed Method-2, we used (\u03bb1, \u03bb2) = (0.01, 0.1). These choices were made using a validation set.\nNote that our method converts a dense matrix to a sparse matrix, so the total number of parameters that need to be stored on disk includes the indices of the parameters. As a result, we report the parameter count along with indices. This is similar to what has been done in (Collins and Kohli 2014). However, for ASIC implementations, one need not store indices as they can be built into the circuit structure.\nFor AlexNet and VGG-16, instead of training from scratch, we fine-tune the network from pre-trained weights. For such pre-trained weights, we found it be useful to preinitialize the gate variables so that we do not lose accuracy while starting to fine-tune. Specifically, we ensure that the gate variables corresponding to the top-k% weights in the W matrix are one, while the rest are zeros. We use this preinitialization instead of the constant initialization described previously.\nTo help pruning performance, we pre-initialize fully connected gates with very large sparsity (95%) and convolutional layers with very little sparsity. This means that 95% of gs parameters are zero, and rest are one. For gs = 1, the underlying gate values were g = 1 and for gs = 0, we used\ng = 0.49. This is to ensure good accuracy by preserving important weights while having large sparsity ratios. The resulting network ended up with a negligible amount of sparsity for convolutional layers and high sparsity for fully connected layers. For VGG-16, we pre-initialize the final two convolutional layers as well with 88% sparsity.\nWe run fine-tuning on AlexNet for 30k iterations (\u223c 18 hours), and VGG for 40k (\u223c 24 hours) iterations before stopping training based on the combination of compression ratio and validation accuracy. This is in contrast with (Han et al. 2015), who take about 173 hours to fine-tune AlexNet. The original AlexNet took 75 hours to train. All wall clock numbers are reported by training on a NVIDIA Titan X GPU. As shown in Table 6 and Table 7, we obtain favourable results when compared to the other network compression / sparsification methods."}, {"heading": "Conclusion", "text": "We have introduced a novel method to learn neural networks with sparse connections. This can be interpreted as learning weights and performing pruning simultaneously. By introducing a learning-based approach to pruning weights, we are able to obtain the optimal level of sparsity. This enables us to achieve state-of-the-art results on compression of deep neural networks."}], "references": [{"title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "author": ["L\u00e9onard Bengio", "Y. Courville 2013] Bengio", "N. L\u00e9onard", "A. Courville"], "venue": "arXiv preprint arXiv:1308.3432", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "An exploration of parameter redundancy in deep networks with circulant projections", "author": ["Cheng"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Cheng,? \\Q2015\\E", "shortCiteRegEx": "Cheng", "year": 2015}, {"title": "Memory bounded deep convolutional networks. CoRR abs/1412.1442", "author": ["Collins", "M.D. Kohli 2014] Collins", "P. Kohli"], "venue": null, "citeRegEx": "Collins et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2014}, {"title": "Predicting parameters in deep learning", "author": ["Denil"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denil,? \\Q2013\\E", "shortCiteRegEx": "Denil", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Denton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Denton,? \\Q2014\\E", "shortCiteRegEx": "Denton", "year": 2014}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Gong"], "venue": "arXiv preprint arXiv:1412.6115", "citeRegEx": "Gong,? \\Q2014\\E", "shortCiteRegEx": "Gong", "year": 2014}, {"title": "Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626", "author": ["Han"], "venue": null, "citeRegEx": "Han,? \\Q2015\\E", "shortCiteRegEx": "Han", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Stork Hassibi", "B. others 1993] Hassibi", "D. G Stork"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hassibi et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hassibi et al\\.", "year": 1993}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Sutskever Krizhevsky", "A. Hinton 2012] Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "LeCun,? \\Q1989\\E", "shortCiteRegEx": "LeCun", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["LeCun"], "venue": "Proceedings of the IEEE", "citeRegEx": "LeCun,? \\Q1998\\E", "shortCiteRegEx": "LeCun", "year": 1998}, {"title": "Bayesian variable selection in linear regression", "author": ["Mitchell", "T.J. Beauchamp 1988] Mitchell", "J.J. Beauchamp"], "venue": "Journal of the American Statistical Association", "citeRegEx": "Mitchell et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 1988}, {"title": "Acdc: A structured efficient linear layer. arXiv preprint arXiv:1511.05946", "author": ["Moczulski"], "venue": null, "citeRegEx": "Moczulski,? \\Q2015\\E", "shortCiteRegEx": "Moczulski", "year": 2015}, {"title": "An algorithm for nonlinear optimization problems with binary variables. Computational Optimization and Applications 47(2):257\u2013288", "author": ["Murray", "W. Ng 2010] Murray", "Ng", "K.-M"], "venue": null, "citeRegEx": "Murray et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2010}, {"title": "Tensorizing neural networks", "author": ["Novikov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Novikov,? \\Q2015\\E", "shortCiteRegEx": "Novikov", "year": 2015}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["Simonyan", "K. Zisserman 2015] Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "2015a. Data-free parameter pruning for deep neural networks", "author": ["Srinivas", "S. Babu 2015a] Srinivas", "R.V. Babu"], "venue": null, "citeRegEx": "Srinivas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2015}, {"title": "Learning the architecture of deep neural networks. arXiv preprint arXiv:1511.05497", "author": ["Srinivas", "S. Babu 2015b] Srinivas", "R.V. Babu"], "venue": null, "citeRegEx": "Srinivas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Szegedy"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Szegedy", "year": 2015}, {"title": "Deep fried convnets", "author": ["Yang"], "venue": "arXiv preprint arXiv:1412.7149", "citeRegEx": "Yang,? \\Q2014\\E", "shortCiteRegEx": "Yang", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "How do we estimate gradients for gate variables, given that they are binary stochastic variables, rather than real-valued and smooth? In other words, how do we backpropagate through the bernoulli sampling step? Bengio et al. (2013) investigated this problem and empirically verified the efficacy of different possible solutions.", "startOffset": 211, "endOffset": 232}, {"referenceID": 0, "context": "How do we estimate gradients for gate variables, given that they are binary stochastic variables, rather than real-valued and smooth? In other words, how do we backpropagate through the bernoulli sampling step? Bengio et al. (2013) investigated this problem and empirically verified the efficacy of different possible solutions. They conclude that the simplest way of computing gradients - the straight-through estimator works best overall. Our experiments also agree with this observation. The straight-through estimator simply involves backpropagating through a stochastic neuron as if it were an identity function. If the sampling step is given by g \u223c bernoulli(g), then the gradient dg s dg = 1 is used. Another issue of consideration is that of ensuring that g always lies in [0, 1] so that it is a valid bernoulli parameter. Bengio et al. (2013) use a sigmoid activation function to achieve this.", "startOffset": 211, "endOffset": 852}, {"referenceID": 4, "context": "Weight-pruning techniques were popularized by LeCun et al.(1989) and Hassibi et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 3, "context": "(1989) and Hassibi et al.(1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": "(1989) and Hassibi et al.(1993), who introduced Optimal Brain Damage and Optimal Brain Surgery respectively. Recently, Srinivas and Babu (2015a) proposed a neuron pruning technique, which relied on neuronal similarity.", "startOffset": 11, "endOffset": 145}, {"referenceID": 3, "context": "Han et al.(2015) create sparse networks by alternating between weight pruning and network training.", "startOffset": 0, "endOffset": 17}, {"referenceID": 3, "context": "Han et al.(2015) create sparse networks by alternating between weight pruning and network training. A similar strategy is followed by Collins and Kohli (2014). On the other hand, our method performs both weight pruning and network training simultaneously.", "startOffset": 0, "endOffset": 159}, {"referenceID": 2, "context": "Denil et al.(2013) demonstrated that most of the parameters of a model can be predicted given only a few parameters.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Denil et al.(2013) demonstrated that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al.(2014) propose an Adaptive Fastfood transform, which is an efficient reparametrization of fully-connected layer weights.", "startOffset": 0, "endOffset": 208}, {"referenceID": 2, "context": "Denil et al.(2013) demonstrated that most of the parameters of a model can be predicted given only a few parameters. At training time, they learn only a few parameters and predict the rest. Yang et al.(2014) propose an Adaptive Fastfood transform, which is an efficient reparametrization of fully-connected layer weights. This results in a reduction of complexity for weight storage and computation. Novikov et al.(2015) use tensor decompositions to obtain a factorization of tensors with small number of parameters.", "startOffset": 0, "endOffset": 421}, {"referenceID": 1, "context": "Cheng et al.(2015) make use of circulant matrices to re-paramaterize fully connected layers.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Cheng et al.(2015) make use of circulant matrices to re-paramaterize fully connected layers. Some recent works have also focussed on using approximations of weight matrices to perform compression. Gong et al.(2014) use a clustering-based product quantization approach to build an indexing scheme that reduces the space occupied by the matrix on disk.", "startOffset": 0, "endOffset": 215}], "year": 2016, "abstractText": "Deep neural networks with lots of parameters are typically used for large scale computer vision tasks such as image classification. This is a result of using dense matrix multiplications and convolutions. However, sparse computations are known to be much more efficient. In this work, we train and build neural networks which implicitly use sparse computations. We introduce additional gate variables to perform parameter selection and show that this is equivalent to using a spike-andslab prior. We experimentally validate our method on both small and large networks and achieve state-of-theart compression results for sparse neural network models.", "creator": "LaTeX with hyperref package"}}}