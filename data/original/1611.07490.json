{"id": "1611.07490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Can Co-robots Learn to Teach?", "abstract": "We explore beyond existing work on learning from demonstration by asking the question: Can robots learn to teach?, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct or collaborate with humans in executing complex tasks in uncertain environments? In this paper we pursue a solution to this problem by leveraging the idea that humans often implicitly decompose a higher level task into several subgoals whose execution brings the task closer to completion. We propose Dirichlet process based non-parametric Inverse Reinforcement Learning (DPMIRL) approach for reward based unsupervised clustering of task space into subgoals. This approach is shown to capture the latent subgoals that a human teacher would have utilized to train a novice. The notion of action primitive is introduced as the means to communicate instruction policy to humans in the least complicated manner, and as a computationally efficient tool to segment demonstration data. We evaluate our approach through experiments on hydraulic actuated scaled model of an excavator and evaluate and compare different teaching strategies utilized by the robot.", "histories": [["v1", "Tue, 22 Nov 2016 19:56:27 GMT  (1038kb,D)", "http://arxiv.org/abs/1611.07490v1", "9 pages, conference"]], "COMMENTS": "9 pages, conference", "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["harshal maske", "emily kieson", "girish chowdhary", "charles abramson"], "accepted": false, "id": "1611.07490"}, "pdf": {"name": "1611.07490.pdf", "metadata": {"source": "CRF", "title": "Can Co-robots Learn to Teach?", "authors": ["Harshal Maske", "Emily Kieson", "Girish Chowdhary", "Charles Abramson"], "emails": ["Champaign@illinois.edu,", "hmaske2@illinois.edu,", "girishc@illinois.edu}.", "{kieson@okstate.edu,", "charles.abramson@okstate.edu}."], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIn many real world robotic applications, human operators play a critical role in ensuring the safety and efficiency of the task. Some examples include heavy construction and agricultural robotics where human operators of co-robots such as excavators, tractors, and backhoes must make safetycritical decisions in real-time under uncertain and dynamically changing environments. The skill-gap between expert and novice operators of these robots is a significant limiting factor in ensuring safety, efficiency, and quality at work-sites. If a co-robot was able to learn from experts and utilize that knowledge to assist or teach novice operators, significant performance gains could be achieved. In this paper, we study the crucial problem of directly learning instruction policies for novice operators from demonstrations provided by skilled operators. Learning from Demonstration has been widely studied in the context of robots learning to do a task from teacher demonstrations [2]. However, when a robot needs to teach a human operator, the robot needs to do much more than just learning to imitate the demonstrated task. Rather, it has to simplify and decompose the tasks into human understandable task-primitives, and communicate back the essential sequence of actions to guide the human learning from the robot. This brings us to the very crucial question: How can Robots learn to Teach? We argue that there are\nHarshal Maske, and Asst. Prof. Girish Chowdhary\u2217 are with the Distributed Autonomous Systems laboratory (DASLAB), University of Illinois at Urbana Champaign,{hmaske2@illinois.edu, girishc@illinois.edu}. Emily Kieson, and Prof. Charles Abramson are with the Comparative Psychology Laboratory, Oklahoma State University, {kieson@okstate.edu, charles.abramson@okstate.edu}.\ntwo important aspects to answering this question: First is the development of practical algorithms that would allow a co-robot to extract latent subgoals or skills that a human teacher would have utilized to instruct other humans. Second, is the development of feedback strategies for providing the appropriate task specific guidance to the human based on the current task state. The approach formulated in this paper is designed to address both these aspects and is shown through extensive experimentations to enable robots to teach complex tasks to human operators (see figure 1).\nThe main contribution of this paper is a method to directly learn an instructional policy from human demonstrations. We define an instructional policy as a feedback policy that utilizes the robots current and past state to suggest the best set of future actions in order to proceed with a given task. This should be contrasted with existing LfD work that has focused primarily on the robot learning a policy for executing the task by itself. Yet, our approach is highly scalable and generalizable, and has been demonstrated to work on a realistic LfD problem with multiple degrees of freedom and uncertain operating conditions. Hence, it can also be used in a pure LfD form for complex real-world robotic tasks, such as those often encountered in construction. To ensure scalability and generalizability as well as to simplify\nar X\niv :1\n61 1.\n07 49\n0v 1\n[ cs\n.R O\n] 2\n2 N\nov 2\n01 6\ncommunication with the human learner, we introduce the notion of action primitives which are used in unsupervised segmentation of demonstration trajectories. Unlike the motion primitives, action primitives are defined in the joint space of a robot, and are hence universally generalizable to any articulated robot, allowing explicit segmentation of demonstration data without the need for computationally expensive and time consuming MCMC sampling that has been utilized by existing LfD approaches.\nThe developed approach is demonstrated in a rigorously designed large human-robot experiment with 113 human participants learning to perform the truck loading task with a scaled excavator in a different configuration (each time). Demonstrations from experts were collected and utilized by our algorithms to learn the instructional policy. Two different interfaces of communicating the correct action primitives to the human operators are investigated, and shown to lead to statistically significant improvement in the human learner\u2019s task completion time, task safety, and retention. Although all of the experiments are done with a scaled excavator, the results should translate well to working with real excavators, and have immediate significance for teleoperated robotics. In fact, we note that working with a scaled excavator can be more cognitively demanding than working from within a real excavator because the operator has to transpose themselves to the correct reference frame as the turret rotates."}, {"heading": "II. BACKGROUND", "text": ""}, {"heading": "A. Learning from Demonstration", "text": "Much of the existing work in LfD has focused on the problem of enabling the robot to learn a closed loop policy to perform a set of actions on its own. LfD has been successful in teaching robots tennis swings [20], walking gaits [12], and complex helicopter maneuvers [1]. A review of various LfD techniques is available in [2]. In a subset of methods called Inverse Reinforcement Learning (IRL), the reward function is learned from a set of expert demonstrations [1]. Learning reward is argued by [11] to be equivalent to high-level description of the task, that explains the expert\u2019s behavior in a richer sense than the policy alone.\nSome recent work in LfD has focused on performing automatic segmentation of demonstrations into simpler reusable primitives [8], [5], [3], [15]. The work by [14] uses the Beta Process Autoregressive Hidden Markov Model (BPAR-HMM) developed by [4] to perform auto-segmentation of time series data available from multiple demonstrations. For sequencing [3], [5] developed novel non-parametric probabilistic models in contrast [14] developed a finite state automaton that utilizes pose information of task objects and segment lengths for sequencing. In these methods, the segments or task primitives are loosely defined, with no bounds on the number of possible segments, thus limiting their re-usability. In this paper, we first propose a definition for segments, that is then utilized to perform segmentation of an unstructured demonstration. Moreover, this procedure does not rely on computationally inefficient Gibbs sampling for learning model parameters. Aforementioned algorithms\nattempt to directly learn the policy; next we discuss reward based IRL techniques for LfD. Our attempt is to leverage key concepts from these two school of thoughts to develop algorithm for inverse LfD to enable robots to assist or instruct humans."}, {"heading": "B. Bayesian Nonparametric Inverse Reinforcement Learning", "text": "Inverse Reinforcement Learning (IRL) is an LfD technique concerned with finding hidden reward function of an expert human demonstrator from the demonstrated state and action samples [22], [1]. Recently, an approach to solve IRL by automatically decomposing the reward function into a series of sugoals, which was viewed as local reward functions, was proposed [11]. We utilize the notion of executing subgoals in a particular sequence to perform a task as a key component of instruction policy model. This is based on research that deals with human expertise in complex environment [16], [6]. According to these findings, humans often form implicit decompositions of higher level tasks into several subgoals, so that the execution of each subgoal brings the task closer to completion.\nIn IRL [1] a Markov Decision Process (MDP) without the reward function R(s) i.e. MDP\\R is given. A demonstration set O consists of state action pairs, O = {(s1, a1), . . . , (sN , aN )}, where each pair Oi = (si, ai) indicates that the action ai was performed from the state si. Multiple set of demonstrations are used as an input to the IRL algorithm to obtain an estimate of reward function R\u0302(s), such that the corresponding optimal policy \u03c0\u2217 matches the observations. The IRL problem is ill-posed, since defining reward as, R\u0302(s) = c \u2200s \u2208 S, would make any set of state-action pairs trivially optimal. Moreover, it is possible to encounter dissimilar actions from a particular state si. This ambiguity was resolved by restricting the reward function to be of certain form [19], [21], [13]. Later [18] developed a standard Bayesian inference procedure to learn reward function.\nThe IRL methods cited above attempt to explain the entire observations set O with a single, complex reward function resulting in a large computational burden when the space of candidate reward functions is large. To overcome this limitation, Michini et. al. [11] developed Bayesian nonparametric IRL (BNIRL) that partitions the demonstration set O and explains each partition with a simple reward function or a \u201csubgoal\u201d Rg(s), which consists of a positive reward at a single coordinate g in the state (or feature) space (zero elsewhere). A Dirichlet process prior (Chinese Restaurant process (CRP) construction) is assumed over the unknown number of partitions that consists of observed state-action pairs Oi \u2261 (si, ai). Partition assignment for each observation Oi is denoted by zi. Posterior over the partition assignment is given by\nP (zi|z\u2212i,O) \u221d P (zi|z\u2212i)\ufe38 \ufe37\ufe37 \ufe38 CRP P (Oi|Rzi)\ufe38 \ufe37\ufe37 \ufe38 likelihood\n(1)\nwhere the first term denotes standard CRP prior and the second term evaluates the likelihood of the action ai given the\nsubgoal reward function Rzi corresponding to partition (or subgoal) identified by zi. This likelihood term is evaluated using exponential rationality model (similar to that in [18]):\nP (Oi|Rzi) = P (ai|si, zi) \u221d e\u03b1Q \u2217(si,ai,Rzi ) (2)\nwhere the parameter \u03b1 represents the degree of confidence in the demonstrator\u2019s ability to maximize reward. The evaluation of optimal action value function Q\u2217 requires substantial computation and becomes infeasible for large state spaces. Hence, the author developed an approximation based on action comparison for the action likelihood (2), as follows\nP (Oi|Rzi) = P (ai|si, zi) \u221d e\u03b1||ai\u2212aCL||2 (3)\nwhere aCL is the action given by some closed-loop controller attempting to go from state si to subgoal gzi . Note that zi is a partition assignment variable for state si, if zi = k, then gk is the coordinate of kth subgoal. This approximation to BNIRL [10], enables successful application of IRL to real-world learning scenario characterized by large state space such as quad-rotor domain. However, their approach was tested for a very basic maneuver of traversing four way-points in space. We utilize the structure of BNIRL, particularly the partitioning of high dimensional state space into finite subgoals, and propose a simplification that extends its application to more complex tasks in real world settings."}, {"heading": "C. Dirichlet Process Means for Gaussian mixture model", "text": "Dirichlet process (DP) is a well known prior for the parameters of a Gaussian mixture model when the number of mixture components are not known a-priori. Recently Kulis and Jordan [9] have shown that the Gibbs sampling algorithm for the Dirichlet process mixture approaches a hard clustering algorithm when the covariances of the Gaussian variables tend to zero. This results in a k-means-like clustering objective given by\nmin \u2211k c=1 \u2211 x\u2208lc ||x\u2212 \u00b5c|| 2 + \u03bbk\nwhere \u00b5c = 1|lc| \u2211 x\u2208lc x\nwhere k is the number of clusters, \u00b5c is the mean for cluster c, and lc is the set of data points x that belongs to the cluster c. This objective function is similar to that of k-means but includes a penalty \u03bb for the number of clusters. DPmeans algorithm behaves similarly to that of k-means with the exception that a new cluster is formed whenever a point is farther than \u03bb away from every existing cluster centroid. The algorithm is initialized with a single cluster whose mean is simply the global centroid. We use this approach to cluster the states by assuming Gaussian distribution over their euclidean norm."}, {"heading": "III. METHODOLOGY", "text": "In LfD literature dynamic movement primitive (DMP) framework [7] is a prevalent approach to imitate demonstrations. Given the start and goal position in task space, a robot uses the DMP to generate trajectory (or set of states s) for end-effector position and its mechanism, which is\nthen mapped to appropriate joint angle (or set of actions a) using inverse kinematics [17] and thus the entire stateaction policy map. We are interested to generate instruction policy for operators who perform the task, for which DMPs, which are encoded in terms of the trajectory, may not be the best solution. To track a given trajectory (i.e. the set of states s) in 3-D space is a complex task for an operator and therefore can present difficulties in teaching. On the other hand, instructions in terms of joystick movements are easily understandable by the operator and are arguably the simplest means of communicating desired actions to operators. So why can\u2019t we use the joint angles i.e. the set of actions a from DMP? We could, since the joint angles can be mapped to joystick movement, however in order to do so, we need to perform two complex transformations (s to a and a to joystick actuations). Furthermore, it is not desirable to continuously instruct joystick movement to reach the goal from a start position. To tackle this issue, we introduce the notion of action primitives designed to simplify robot to human teaching tasks."}, {"heading": "A. Action Primitives", "text": "The definition of action primitives is developed for articulated robots, however, it should be generalizable to other types of robots. For an articulated robot, a revolute joint provides single-axis rotation function, hence a joint j can take three broadly defined possible states: i) counterclockwise rotation, ii) stationary or non-zero noisy perturbation, or iii) clockwise rotation. Note that it is possible to add further resolution to this decomposition. Let rj be the variable that takes on values from the set {1, 2, 3} corresponding to these states respectively. Thus rj = 1 implies that the joint j is rotating counterclockwise and so on. The assignment of values {1, 2, 3} is arbitrary. Mathematically, an action primitive defines the action of a robot in terms of variable rj for each joint j. Action primitives are defined in the joint space of a robot and hence map directly to joystick movements. Secondly, in this formulation, an action primitive changes only when the direction of rotation, i.e. the variable rj changes for a joint j. This has two significant consequences: First an action primitive can be used to decompose a demonstration consisting of continuous state action spaces into finitely many discrete state-action pairs, this is the basis for segmentation approach discussed in next section. Second, known sequence of these finite number of action primitives can be used to generate step-wise instruction policy model to guide humans (discussed further in section III-D). An example of action primitive a\u03c4 at a time \u03c4 for a robot with n = 3 revolute joints is a\u03c4 = [r1 = 1; r2 = 2; r3 = 2] or just a\u03c4 = [1, 2, 2]T indicative of counterclockwise rotation for the first revolute joint. Clearly for a robot with n joints, there would exist finitely many action primitives, as opposed to infinitely many joint velocities obtained from DMPs."}, {"heading": "B. Action Primitive based Segmentation of Task demos", "text": "Action primitives are closely related to the joint space of a robot and is a least complicated mechanism to commu-\nnicate instructions to humans when compared to dynamic motion primitives or any other existing methods because they relate directly to joystick motion. We now describe how action primitives can be used for segmenting demonstrations. Demonstration data in the form of joint positions x\u03c4 , and joint velocities v\u03c4 sampled from a continuous demonstration of a task at time instants \u03c4 = 1, 2, . . . , T , is used as an input to the segmentation algorithm. Classification of sampled velocities into action primitives generates the required segmentation. Each segment will be an action primitive that spans over finite instants of time. But first we need to define distribution for each action primitive class. Recall that an action primitive is defined by the value of its variable rj for each joint j. As noted earlier each rj \u2208 {1, 2, 3}, hence we need three distributions for each joint j of the robot. To accomplish this, we cluster sampled velocities assuming Gaussian distribution for each cluster.\nLet v\u03c4 \u2208 Rn\u00d71 denote the sampled velocity at time \u03c4 , v\u03c4j be the velocity of joint j, and v\u00b7j be the set of all sampled velocities {v1j , v2j , . . . , vTj} of the joint j. We cluster velocities in the set v\u00b7j using k-means algorithm by setting k = 3 to obtain three clusters corresponding to rj \u2208 {1, 2, 3}. Our results show that applying k-means consistently produces clusters such as one shown in figure 2, which clearly demarcates the intended three different states for rj . From cluster members we calculate the mean \u00b5ji and variance \u03c32ji for each cluster i \u2208 {1, 2, 3}. Based on these cluster parameters we define the probability\np(rj = i|v\u03c4j) = N (v\u03c4j |\u00b5ji, \u03c32ji) (4)\nand assign rj as follows\nrj =  2 p(rj = 2) > \u03b7 1 p(rj = 1) > p(rj = 3)\n3 p(rj = 3) < p(rj = 1)\n(5)\nwhere \u03b7 is a threshold which can be set using a labeled trajectory data. This procedure is repeated for each joint j to obtain the action primitive a\u03c4 at each time instant \u03c4 . This process generates action primitive segments Ai comprised of similar action primitive a\u03c4 = ai observed continuously over time instants \u03c4 = {\u03c4i, \u03c4i+1, . . . , \u03c4j}. An example segmentation (discussed later in details) is shown in figure 6, where each colored segment is an action primitive segment (Ai) that has a particular action primitive (ai). Total of 14 unique action primitives (or action primitive segments) were discovered in a truck loading task performed using a model excavator having four revolute joints. Repetition of these unique action primitive segments generates the entire task. We associate each action primitive segment Ai with the corresponding end-effector pose si at the beginning of an action primitive. Each action primitive segment can then be represented by the pair (si, ai) where ai is the associated action primitive. Thus using segmentation procedure we obtain the set stateaction pairs O = {(s1, a1), (s2, a2), . . . , (sN , aN )}, where N is the total number of action primitive segments observed in a demonstration. The set O is used as an input to the\ninverse reinforcement learning described in the next section.\nWe had noted earlier that humans are good at decomposing a given ill-defined task into a series of actionable subgoals. Such decomposition is implicit within the human mind, and humans are not always able to clearly explain how they arrived at the decomposition [16], [6]. However, the key idea that is leveraged is to decompose complex task space into subgoals. This is achieved through DPMIRL formulation: a reward based partitioning of task space. Additionally action primitive segments are associated with the inferred subgoals, such that their proper sequencing generates instructions to translate from one subgoal to another.\nWe first propose a simplification of action likelihood approximation used in the BNIRL approach (discussed in section II-B). This simplification allows partitioning of stateaction pairs based on euclidean distance metric using Dirichlet process means [9]. Typically in an IRL problem entire state space is modeled as an MDP, whereas we define an MDP\\R as a tuple \u3008S,A, T, \u03b3,D\u3009 where the sets S and A consists of the state action pairs in the set O which is obtained from action primitive based segmentation. This step significantly reduces the computational burden, and potentially renders the analysis of any problem in high dimensional continuous state action space feasible. We simplify the likelihood term in equation (3), where the likelihood of an action ai w.r.t to a subgoal gzi is computed as P (ai|si, zi) \u221d e\u03b1||ai\u2212aCL||2 . As proposed in [10], aCL is the action of a closed-loop controller that attempts to go from si to subgoal gzi . A simple controller that would generate an action aCL to reduce the pose error between the subgoal gzi and the present state si is aCL \u221d (gzi \u2212 si). Since a demonstrator is invariably reducing the pose error between his/her state si and the subgoals, we argue that the action ai in the demonstration is also proportional to the pose error. Hence given a particular subgoal gzi , even ai \u221d (gzi \u2212 si). Substituting in equation (3) we obtain\nP (ai|si, zi) \u221d e\u03b1||\u03ba(gzi\u2212si)\u2212\u03bb(gzi\u2212si)||2 (6) \u221d e\u03b1 \u2032||(gzi\u2212si)||2 (7)\nwhere we let \u03ba and \u03bb to be scalar proportionality constants, and thus the original action comparison reduces to pose error\ncomparison between the current state si and the subgoals. This result is very intuitive in the sense that any state-action pair (si, ai) will be partitioned or assigned to the closest subgoal. Thus any action ai from a state si, is likely to either attain the assigned subgoal or recede away towards the next. This notion is utilized to partition the state action pairs in the set O, using euclidean distance metric on state locations i.e. ||si||2 for a state si. This is performed using DP-means algorithm discussed in section II-C. Thus we obtain clusters {c1, . . . , ck} where the number of clusters k is not known a-priori, and each cluster cj consists of member states {si \u2208 cj : zi = j}. Since each state si \u2208 cj is associated with an action primitive ai, we define a set Sj w.r.t the cluster cj as {(si, ai) \u2208 Sj : si \u2208 cj}. We define the subgoal as a multivariate Gaussian Xj \u223c N (\u00b5j ,\u03a3j) in n-dimensional space (formed by end-effector position and mechanism) whose parameters are obtained from the member states of cluster cj .\nTo ensure that the instruction policy for a given task, is generalizable to a novel task configuration it is important to define appropriate reference coordinate frames to compute the euclidean norm. Given a task configuration we define a coordinate frame centered on each known object. Let M be the number of objects and mj be the center of the jth object\u2019s coordinate frame w.r.t the base frame of the robot. We divide the states in the set S into M disjoint sets {S1, . . . , SM}, where each si is assigned to Sj such that arg minj ||si\u2212mj ||2. We run DP-means separately for each set Sj , to obtain subgoals as described in algorithm 1. Thus we have decomposed the task space into finite subgoals, most importantly defining subgoals as a multivariate Gaussian allows us to evaluate the likelihood of any state s in space w.r.t to each subgoal, to determine the most likely subgoal. This assignment allow us to select sequence of action primitives as instructions to reach next subgoal, this is discussed next.\nAlgorithm 1 DPMIRL Input: Action primitive segments (S,A) \u2208 O, M objects of interest centered at mj . - Assign state si to Sj s.t. arg minj ||si \u2212mj ||2 - Run DP-means algorithm for states in each Sj , to obtain set of clusters Cj = {cj1, . . . , cjk} - Set of subgoals X = \u2205, p = |X| = 0 for each set of clusters Cj do\nfor each cluster k in Cj do - Compute mean \u00b5kj and variance \u03a3kj for the member states s \u2208 cjk - Add Xp \u223c N (\u00b5kj ,\u03a3kj) to set X , increment p,\nend for end for Output: Set of subgoals X ."}, {"heading": "D. Task-come-Instruction Model", "text": "DPMIRL partitions the task space into subgoals this is analogous to expert human operators who also decompose\na given loosely defined task (such as leveling a construction site or loading a truck) into a series of actionable subgoals. This analogy is used as a building block for the instruction model that can be used by a robot to instruct or guide human operator. Action primitive based segmentation plays an important role in such a model, as the transition between these action primitives results in a transfer from one subgoal to another. Further an action primitives can be easily communicated to the human operator as they represent activation of single or multiple actuators which are in turn operated by specific joysticks. An example of action primitive and its corresponding communication in terms of joystick movements via a simple graphical interface is depicted in figure 4a.\nWe now elaborate the model construction which is generated autonomously based on the subgoals obtained using DPMIRL and the action primitive based segments. Construction of this model has to be such that the task is elaborated in the form of transition among the subgoals. One such construction using hierarchy of Markov chains also known as a Dynamical Bayesian Network is shown in figure 3, where the transition between subgoals is modeled by the topmost Markov chain. In this construction, we utilize a fact that each subgoal (Xi) generated using DPMIRL has an associated set Si of state-action primitive pairs, and there exists a Markov chain of variables {Zi1, . . . , ZiTi}, where each variable is an action primitive at time instants \u03c4 = 1, . . . , Ti, that results in a translation to the next subgoal Xj . Hence the Markov chain under each subgoal Xi models the transition among the action primitives associated with that subgoal. These transition models for action primitives are obtained from the segmentation of demonstration data and counting the transitions between the action primitive segments. The final layer of the model is actuator velocity variable yi\u03c4 that is conditional on the action primitive Zi\u03c4 , and is modeled as a Gaussian distribution over the sampled actuator velocities contained in the action primitive segment. Given the most likely subgoal (Xi) for the current state s and the previous action primitive Zi\u03c4\u22121, the generative model for getting\ninstruction in the form of next action primitive Zi\u03c4 , using the model in figure 3, is as follows:\nP (Xi+1|Xi) \u223c \u03a0 (8) P (Zi\u03c4 |Zi\u03c4\u22121, Xi) \u223c \u03c0(Xi) (9)\nP (yi\u03c4 |Zi\u03c4 ) \u223c F (\u03b8Zi\u03c4 ) (10)\nand the parameters for this model are the transition distribution \u03a0 for subgoals, the subgoal specific transition model \u03c0(Xi) for the associated action primitives, and the parameter vector \u03b8Zi\u03c4 that models conditional distribution of actuator velocities given the action primitive.\nE. Instruction Interface\nHuman robot interaction (HRI), an entire field of research is devoted to investigate most effective human-robot interfaces. A survey of the techniques typically utilized there is beyond the scope of this work. Rather, in this work we resort to exploratory but exhaustive evaluations using two fundamentally different styles of visual interfaces to communicate the instructional policy learned using methods developed in the previous sections. Our goal is to compare generic reward based interfaces against specific instructional interfaces. Given the nature of uncertainty and human presence around construction equipments, safety and situational awareness of the operator becomes another key aspect for the interface design. It is desirable therefore to have simple visual interface that reinforce desired skills while demanding minimal operator attention. On the other hand, the task is complex, so common wisdom is to err towards providing specific instructions on which actions to take. Accordingly, the first interface indicates the extent of desired actuation, current joystick position along with positive reinforcement by turning red colored bar to green, when an operator takes a desired action. This interface trades off operator\u2019s attention to communicate more information. On the other hand, the second interface provides positive reinforcement for desired actuation by changing the color of a circle representing each joystick. It does not provide any other information in terms of the direction on magnitude of joystick motion."}, {"heading": "IV. EXPERIMENTS", "text": ""}, {"heading": "A. Test Platform", "text": "To test the developed approach exhaustive experiments were performed on a 1/14th scaled 345D Wedico exca-\nvator model a 4 d.o.f robotic arm manipulator, controlled by a radio transmitter as seen in Figure 5, this model is constructed by Wedico, Germany (http://www.wedico.de/), which specializes in to-scale accurate and fully functional hydraulic construction equipments. Ideally the operator and the display panel shown in the figure 5 would be set up inside the excavator but this has been left for future work. However, the results reported here should generalize, furthermore, they are directly applicable to teleoperated robotics. The Wedico excavator lacked joint-angle encoders and internal proprioception, hence all the experiments were performed inside a motion capture facility to ensure real-time data input to the algorithm. In our experiments we record positions and velocities of the four actuators turret, boom, arm and bucket and denote them by vectors x and v respectively. We also record end-effector position w.r.t the base frame and bucket angle as four dimensional vector se."}, {"heading": "B. Learning Instruction Policy", "text": "To demonstrate our approach we selected truck loading task which is a standard task performed using an excavator, and also happens to be a benchmark operation for fuel consumption analysis (ISO11152) for the excavator family of equipments. Our goal is to learn instruction policy model from demonstrations and evaluate the effectiveness of the teaching interface. We obtained six set of demonstrations for\nthe truck loading task from a human expert. Each demonstration involved filling up of the truck with sand, for which joint positions x, joint velocities v and end-effector position se, sampled at 25Hz were recorded. These demonstration trajectories from the excavator model is used to learn the instruction policy in three steps.\nIn the first step we perform action primitive based segmentation of demonstration trajectories, according to the process described in section III-B. Segmentation of three demonstration set is shown in figure 6. Each of the figure depict two truck loading cycles for clarity although each demonstration consisted of 5 \u2212 6 cycles required to fill the truck to its capacity. We compared our segmentation approach with that of Beta process auto-regressive HMM of [14] in terms of computation time (table I) required to segment on a i7-6700K CPU @4GHz and 24 GB RAM machine. Computation time required by BP-AR-HMM scaled geometrically with data size, in comparison action primitive based segmentation scales almost linearly. Being computationally efficient action primitives can be used to analyze and infer tons of trajectory data from demonstrations. Each action primitive segment Ai is a state-action pair (si, ai) where si is the end-effector pose se from which the action primitive segment began, and ai is the associated action primitive. Thus using segmentation procedure we obtain a set of stateaction pairs Oj = {(s1, a1), (s2, a2), . . . , (sNj , aNj )}, where Nj is the total number of action primitive segments observed in jth demonstration. Each set Oj is used as an input to the DPMIRL algorithm, this is the second step which decomposes the task space into finite subgoals. This decomposition is crucial for instruction policy model which consists of action primitive sequence that guides the human operator from one subgoal to another. One such decomposition that results into six subgoals, three each w.r.t the two task objects (pile and truck) is shown in figure 7a. Note that clustering is based on euclidean distance metric defined on a four dimensional vector ((x, y, z) coordinate w.r.t the task object and the bucket angle), which explains the co-location of clusters in the figure, which are actually far apart in the fourth dimension of bucket angle. We followed similar procedure using BNIRL approach [11] for comparison, and the result is shown in figure 7b. Since the state sei of action primitive segments are not co-located with the subgoal location, decomposition obtained using the latter method is not suited for the instruction policy model. Hence as a third step we utilized the decomposition generated by DPMIRL to learn the parameters of instruction policy model (section III-D) which is then used to guide human operators in performing the truck loading task."}, {"heading": "C. Testing Instruction Policy Model", "text": "We tested the efficacy of the proposed instruction policy model by guiding novice operators in the performance of truck loading task in novel configurations. The process of instructing a human operator while performing a task is depicted (in green) in figure 1. At any given time instant \u03c4 , task space configuration comprising of end-effector position,\ndirt pile and truck position, joint positions and base frame position is available to the co-robot. Co-robot then generates the action primitive Zi\u03c4 using the instructional policy model, based on the previous action primitive Zi\u03c4\u22121 (calculated from joint position history) and the most likely current subgoal Xi (subgoal closest to the current end-effector position), using equations (8)-(10). Figure 4 shows the two different instruction strategies that are used by the robot to communicate the action primitive to the human operator, who then controls the actuator movement through joystick input.\nA total of 113 participants volunteered under IRB guidelines for the experiments and were split into three groups in order to test the hypothesized visual interfaces. Participants were randomly sorted into groups: Group 1 using the GUI Circles (4b), Group 2 using the GUI with Speed Bars (figure 4a), and Group 3 with no GUI (control). Each participant was instructed briefly on how the controllers work then given a minimum of three trials to attempt to scoop sand from the tub and deposit into the truck using the controls and the assistance of the selected training GUI. Participants were timed and videoed and their performance was evaluated in terms of cycle time (completion time for each cycle), number of action primitives executed per cycle, number of erroneous action primitives executed per cycle and the dump height. Dump height is the height above the truck at which the sand is dumped, and is a measure of preciseness. Since each participant performed more than three cycles of truck loading task, an average of these quantities is presented in figure 8. These results indicate that the instruction policy statistically significantly improves the performance of novice operators across all the parameters. Using instructions from visual interfaces also helped operators in maintaining a lower dump height which is essential in reducing spillage. Interestingly, these results show no significant difference between the two GUI types utilized, a substantial inference that heavily supports design of simple GUIs that focus on generic rewards rather than complex GUIs that focus on specific instructions."}, {"heading": "V. CONCLUSION", "text": "This paper presented a new method for tackling a class of inverse learning problems in co-robotics where the robot must learn from expert demonstration for teaching nonexpert humans. The main contribution of the work presented is a generalizable and scalable technique, that enables the robot to directly learn a instructional policy from expert demonstrations. The paper introduced the notion of action primitives for decomposition and representation of multiinput-multi-output trajectories of complex multi degree of freedom robots. The main advantage of using action primitives, instead of motion primitives which are typically parameterized in the trajectory space, is that action primitives are simpler for the robot to explain to the humans. Furthermore, action primitives can be used as building blocks for a variety of similar tasks utilizing the same base actions and can be generalized to robots with different scales but same joint space, such as other excavators with different boom, bucket, and arm lengths. Finally, action primitives\nlead to efficient unsupervised clustering of possible robot actions. We demonstrated that utilizing action primitives in a nonparametric unsupervised clustering framework leads to an instructional policy that can utilize current robot pose and subgoal as a feedback signal to provide the correct instruction. Two different interfaces for providing instructional feedback to the human learner were validated in a meticulously constructed large human-robot experiment with 113 human participants. Our results clearly show that there is statistically significant difference in learning rate, task performance times, and retention between guided and unguided operators. Interestingly, our results also show that with the feedback based instructional policy in place, even simpler operator instruction interfaces perform as well as complex interfaces."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Pieter Abbeel", "Andrew Y Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["Brenna D Argall", "Sonia Chernova", "Manuela Veloso", "Brett Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Learning from demonstration using a multi-valued function regressor for time-series data", "author": ["Jesse Butterfield", "Sarah Osentoski", "Graylin Jay", "Odest Chadwicke Jenkins"], "venue": "In 2010 10th IEEE-RAS International Conference on Humanoid Robots,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Joint modeling of multiple related time series via the beta process", "author": ["Emily B Fox", "Erik B Sudderth", "Michael I Jordan", "Alan S Willsky"], "venue": "arXiv preprint arXiv:1111.4226,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Incremental learning of subtasks from unsegmented demonstration", "author": ["Daniel H Grollman", "Odest Chadwicke Jenkins"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "The development of human expertise in a complex environment", "author": ["Michael Harr\u00e9", "Terry Bossomaier", "Allan Snyder"], "venue": "Minds and Machines,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Movement imitation with nonlinear dynamical systems in humanoid robots", "author": ["Auke Jan Ijspeert", "Jun Nakanishi", "Stefan Schaal"], "venue": "In Robotics and Automation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Robot learning from demonstration by constructing skill trees", "author": ["George Konidaris", "Scott Kuindersma", "Roderic Grupen", "Andrew Barto"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Revisiting k-means: New algorithms via bayesian nonparametrics", "author": ["Brian Kulis", "Michael I Jordan"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Scalable reward learning from demonstration", "author": ["Bernard Michini", "Mark Cutler", "Jonathan P How"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Bayesian nonparametric reward learning from demonstration", "author": ["Bernard Michini", "Thomas J Walsh", "Ali-Akbar Agha-Mohammadi", "Jonathan P How"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Learning from demonstration and adaptation of biped locomotion", "author": ["Jun Nakanishi", "Jun Morimoto", "Gen Endo", "Gordon Cheng", "Stefan Schaal", "Mitsuo Kawato"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Apprenticeship learning using inverse reinforcement learning and gradient methods", "author": ["Gergely Neu", "Csaba Szepesv\u00e1ri"], "venue": "arXiv preprint arXiv:1206.5264,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Incremental semantically grounded learning from demonstration", "author": ["Scott Niekum", "Sachin Chitta", "Andrew G Barto", "Bhaskara Marthi", "Sarah Osentoski"], "venue": "In Robotics: Science and Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Learning and generalization of complex tasks from unstructured demonstrations", "author": ["Scott Niekum", "Sarah Osentoski", "George Konidaris", "Andrew G Barto"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Problem solving and human expertise", "author": ["T.J. Nokes", "C.D. Schunn", "Michelene T.H. Chi"], "venue": "International Encyclopedia of Education,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Learning and generalization of motor skills by learning from demonstration", "author": ["Peter Pastor", "Heiko Hoffmann", "Tamim Asfour", "Stefan Schaal"], "venue": "In Robotics and Automation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Bayesian inverse reinforcement learning", "author": ["Deepak Ramachandran", "Eyal Amir"], "venue": "Urbana, 51(61801):1\u20134,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Maximum margin planning", "author": ["Nathan D Ratliff", "J Andrew Bagnell", "Martin A Zinkevich"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Dynamic movement primitives-a framework for motor control in humans and humanoid robotics", "author": ["Stefan Schaal"], "venue": "In Adaptive Motion of Animals and Machines,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["Umar Syed", "Robert E Schapire"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Brian D Ziebart", "Andrew L Maas", "J Andrew Bagnell", "Anind K Dey"], "venue": "In AAAI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Learning from Demonstration has been widely studied in the context of robots learning to do a task from teacher demonstrations [2].", "startOffset": 127, "endOffset": 130}, {"referenceID": 19, "context": "LfD has been successful in teaching robots tennis swings [20], walking gaits [12], and complex helicopter maneuvers [1].", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "LfD has been successful in teaching robots tennis swings [20], walking gaits [12], and complex helicopter maneuvers [1].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "LfD has been successful in teaching robots tennis swings [20], walking gaits [12], and complex helicopter maneuvers [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "A review of various LfD techniques is available in [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "In a subset of methods called Inverse Reinforcement Learning (IRL), the reward function is learned from a set of expert demonstrations [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": "Learning reward is argued by [11] to be equivalent to high-level description of the task, that explains the expert\u2019s behavior in a richer sense than the policy alone.", "startOffset": 29, "endOffset": 33}, {"referenceID": 7, "context": "primitives [8], [5], [3], [15].", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "primitives [8], [5], [3], [15].", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "primitives [8], [5], [3], [15].", "startOffset": 21, "endOffset": 24}, {"referenceID": 14, "context": "primitives [8], [5], [3], [15].", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "The work by [14] uses the Beta Process Autoregressive Hidden Markov Model (BPAR-HMM) developed by [4] to perform auto-segmentation of time series data available from multiple demonstrations.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "The work by [14] uses the Beta Process Autoregressive Hidden Markov Model (BPAR-HMM) developed by [4] to perform auto-segmentation of time series data available from multiple demonstrations.", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "For sequencing [3], [5] developed novel non-parametric probabilistic models in contrast [14] developed a finite state", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "For sequencing [3], [5] developed novel non-parametric probabilistic models in contrast [14] developed a finite state", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "For sequencing [3], [5] developed novel non-parametric probabilistic models in contrast [14] developed a finite state", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "Inverse Reinforcement Learning (IRL) is an LfD technique concerned with finding hidden reward function of an expert human demonstrator from the demonstrated state and action samples [22], [1].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "Inverse Reinforcement Learning (IRL) is an LfD technique concerned with finding hidden reward function of an expert human demonstrator from the demonstrated state and action samples [22], [1].", "startOffset": 188, "endOffset": 191}, {"referenceID": 10, "context": "Recently, an approach to solve IRL by automatically decomposing the reward function into a series of sugoals, which was viewed as local reward functions, was proposed [11].", "startOffset": 167, "endOffset": 171}, {"referenceID": 15, "context": "This is based on research that deals with human expertise in complex environment [16], [6].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "This is based on research that deals with human expertise in complex environment [16], [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "In IRL [1] a Markov Decision Process (MDP) without the reward function R(s) i.", "startOffset": 7, "endOffset": 10}, {"referenceID": 18, "context": "This ambiguity was resolved by restricting the reward function to be of certain form [19], [21], [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 20, "context": "This ambiguity was resolved by restricting the reward function to be of certain form [19], [21], [13].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "This ambiguity was resolved by restricting the reward function to be of certain form [19], [21], [13].", "startOffset": 97, "endOffset": 101}, {"referenceID": 17, "context": "Later [18] developed a standard Bayesian inference procedure to learn reward function.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "[11] developed Bayesian nonparametric IRL (BNIRL) that partitions the demonstration set", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "This likelihood term is evaluated using exponential rationality model (similar to that in [18]):", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "This approximation to BNIRL [10], enables successful application of IRL to real-world learning scenario characterized by large state space such as quad-rotor domain.", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "Recently Kulis and Jordan [9] have shown that the Gibbs sampling algorithm for the Dirichlet process mixture approaches a hard clustering algorithm when the covariances of the Gaussian variables tend to zero.", "startOffset": 26, "endOffset": 29}, {"referenceID": 6, "context": "In LfD literature dynamic movement primitive (DMP) framework [7] is a prevalent approach to imitate demon-", "startOffset": 61, "endOffset": 64}, {"referenceID": 16, "context": "a) using inverse kinematics [17] and thus the entire stateaction policy map.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "An example of action primitive a\u03c4 at a time \u03c4 for a robot with n = 3 revolute joints is a\u03c4 = [r1 = 1; r2 = 2; r3 = 2] or just a\u03c4 = [1, 2, 2] indicative of counterclockwise rotation for the first revolute joint.", "startOffset": 131, "endOffset": 140}, {"referenceID": 1, "context": "An example of action primitive a\u03c4 at a time \u03c4 for a robot with n = 3 revolute joints is a\u03c4 = [r1 = 1; r2 = 2; r3 = 2] or just a\u03c4 = [1, 2, 2] indicative of counterclockwise rotation for the first revolute joint.", "startOffset": 131, "endOffset": 140}, {"referenceID": 1, "context": "An example of action primitive a\u03c4 at a time \u03c4 for a robot with n = 3 revolute joints is a\u03c4 = [r1 = 1; r2 = 2; r3 = 2] or just a\u03c4 = [1, 2, 2] indicative of counterclockwise rotation for the first revolute joint.", "startOffset": 131, "endOffset": 140}, {"referenceID": 15, "context": "Such decomposition is implicit within the human mind, and humans are not always able to clearly explain how they arrived at the decomposition [16], [6].", "startOffset": 142, "endOffset": 146}, {"referenceID": 5, "context": "Such decomposition is implicit within the human mind, and humans are not always able to clearly explain how they arrived at the decomposition [16], [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 8, "context": "This simplification allows partitioning of stateaction pairs based on euclidean distance metric using Dirichlet process means [9].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "As proposed in [10], aCL is the action of a closed-loop controller that attempts to go from si to subgoal gzi .", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "To test the developed approach exhaustive experiments were performed on a 1/14 scaled 345D Wedico excaTABLE I: Comparison of Action primitive based segmentation with BP-AR-HMM [14] in terms of computation time", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "We compared our segmentation approach with that of Beta process auto-regressive HMM of [14] in terms of computation time (table I) required to segment on a i7-6700K CPU @4GHz and 24 GB RAM machine.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "We followed similar procedure using BNIRL approach [11] for comparison, and the result is shown in figure 7b.", "startOffset": 51, "endOffset": 55}], "year": 2016, "abstractText": "We explore beyond existing work on learning from demonstration by asking the question: \u201cCan robots learn to teach?\u201d, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct or collaborate with humans in executing complex tasks in uncertain environments? In this paper we pursue a solution to this problem by leveraging the idea that humans often implicitly decompose a higher level task into several subgoals whose execution brings the task closer to completion. We propose Dirichlet process based non-parametric Inverse Reinforcement Learning (DPMIRL) approach for reward based unsupervised clustering of task space into subgoals. This approach is shown to capture the latent subgoals that a human teacher would have utilized to train a novice. The notion of \u201caction primitive\u201d is introduced as the means to communicate instruction policy to humans in the least complicated manner, and as a computationally efficient tool to segment demonstration data. We evaluate our approach through experiments on hydraulic actuated scaled model of an excavator and evaluate and compare different teaching strategies utilized by the robot.", "creator": "LaTeX with hyperref package"}}}