{"id": "1611.07507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Nov-2016", "title": "Variational Intrinsic Control", "abstract": "In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.", "histories": [["v1", "Tue, 22 Nov 2016 20:44:39 GMT  (1051kb,D)", "http://arxiv.org/abs/1611.07507v1", "15 pages, 6 figures"]], "COMMENTS": "15 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["karol gregor", "danilo jimenez rezende", "daan wierstra"], "accepted": false, "id": "1611.07507"}, "pdf": {"name": "1611.07507.pdf", "metadata": {"source": "CRF", "title": "VARIATIONAL INTRINSIC CONTROL", "authors": ["Karol Gregor", "Danilo Rezende"], "emails": ["karolg@google.com", "danilor@google.com", "wierstra@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "In this paper we aim to provide an answer to the question what intrinsic options are available to an agent in a given state \u2013 that is, options that meaningfully affect the world. We define options as policies with a termination condition, and we are primarily concerned with their consequences \u2013 what states in the environment they reach upon termination. The set of all options available to an agent is independent of an agent\u2019s intentions \u2013 it is the set of all things that are possible for an agent to achieve. The purpose of this work is to provide an algorithm that aims to discover as many intrinsic options as it can, using an information theoretic learning criterion and training procedure.\nThis differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015). Limiting oneself to working with relatively small option spaces makes both credit assignment and planning over long time intervals easier. However, we argue that operating on the larger space of intrinsic options, as alluded to above, is in fact useful even though the space is vastly larger. First, the number of options is still much smaller than the number of all action sequences, since options are distinguished in terms of their final states, and many action sequences can reach the same state. Second, we aim to learn good representational embeddings of these options, where similar options are close in representational space and where we can rely on the power of generalization. In such embedded spaces a planner needs only choose a neighborhood of this space containing options that have sufficiently similar consequences.\nThe idea of goal and state embeddings, along with a universal value function for reaching these goals, was introduced in Schaul et al. (2015). This work allowed an agent to efficiently represent control over many goals and to generalize to new goals. However, the goals were assumed to be given. This paper extends that work and provides a mechanism for learning goals (options) while preserving their embedded nature.\nThere are at least two scenarios where our algorithm can be useful. One is the classical reinforcement learning case that aims to maximize an externally provided reward, as we explained above. In this case, rather than learning options to uniformly represent control, the agent can combine extrinsic reward with an intrinsic control maximization objective, biasing learning towards high reward options.\nThe second scenario is that in which the long-term goal of the agent is to get to a state with a maximal set of available intrinsic options \u2013 the objective of empowerment (Salge et al., 2014). This set of options consists of those that the agent knows how to use. Note that this is not the theoretical set of\nar X\niv :1\n61 1.\n07 50\n7v 1\n[ cs\n.L G\n] 2\n2 N\nov 2\n01 6\nall options: it is of no use to the agent that it is possible to do something if it is unable to learn how to do it. Thus, to maximize empowerment, the agent needs to simultaneously learn how to control the environment as well \u2013 it needs to discover the options available to it. The agent should in fact not aim for states where it has the most control according to its current abilities, but for states where it expects it will achieve the most control after learning. Being able to learn available options is thus fundamental to becoming empowered.\nLet us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016). The empowerment objective differs from this in a fundamental manner: the primary goal is not to understand or predict the observations but to control the environment. This is an important point \u2013 agents can often control an environment perfectly well without much understanding, as exemplified by canonical model-free reinforcement learning algorithms (Sutton & Barto, 1998), where agents only model action-conditioned expected returns. Focusing on such understanding might significantly distract and impair the agent, as such reducing the control it achieves.\nOur algorithm can be viewed as learning to represent the intrinsic control space of an agent. Developing this space should be seen as acquiring universal knowledge useful for accomplishing a multitude of different tasks, such as maximizing extrinsic or intrinsic reward (see Oudeyer et al. (2008) for an overview and useful references). This is analogous to unsupervised learning in data processing, where the goal is to find representations of data that are useful for other tasks. The crucial difference here, however, is that rather than simply finding representations, we learn explicit policies that an agent can choose to follow. Additionally, the algorithm explicitly estimates the amount of control it has in different states \u2013 intuitively, the total number of reliably reachable states \u2013 and can as such be used for an empowerment maximizing agent.\nA most common criterion for unsupervised learning is data likelihood. For a given data set, various algorithms can be compared based on this measure. No such commonly established measure exists for the comparison of unsupervised learning performance in agents. One of the primary difficulties is that in unsupervised learning the data is known, but in control, an agent exists in an environment and needs to act in it in order to discover what states and dynamics it contains. Nevertheless, we should be able to compare agents in terms of the amount of intrinsic control and empowerment they achieve in different states. Just like there are multiple methods and objectives for unsupervised learning (Goodfellow et al., 2016), we can devise multiple methods and objectives for unsupervised control. Data likelihood and empowerment are both information measures: likelihood measures the amount of information needed to describe data and empowerment measures the mutual information between action choices and final states. Therefore we suggest that what maximum likelihood is to unsupervised learning, mutual information between options and final states is to unsupervised control.\nThis information measure has been introduced in the empowerment literature before (Salge et al., 2014; Klyubin et al., 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)). Recently, Mohamed & Rezende (2015) proposed an algorithm that can utilize function approximation and deep learning techniques to operate in high-dimensional environments. However, this algorithm considers the mutual information between sequences of actions and final states. This corresponds to maximizing the empowerment over open loop options, where the agent a priori decides on a sequence of actions in advance, and then follows these regardless of (potentially stochastic) environment dynamics. Obviously this often limits performance severely as the agent cannot properly react to the environment, and it tends to lead to a significant underestimation of empowerment. In this paper we provide a new perspective on this measure, and instantiate two novel algorithms that use closed loop options where actions are conditioned on state. We show, on a number of tasks, that we can use these to both significantly increase intrinsic control and improve the estimation of empowerment.\nThe paper is structured as follows. First we formally introduce the notion of intrinsic control and its derivation from the mutual information principle. After that, we describe our algorithm for intrinsic control with explicit options and argue for its viability in the experimental section. Last, we touch on intrinsic control with implicit options, and demonstrate that it scales up even better. We conclude with a short discussion on the merits of the approach and possible extensions."}, {"heading": "2 INTRINSIC CONTROL AND THE MUTUAL INFORMATION PRINCIPLE", "text": "In this section we explain how we represent intrinsic options and the corresponding objective we optimize.\nWe define an option as an element \u2126 of a space and an associated policy \u03c0(a|s,\u2126) that chooses an action a in a state s when following \u2126. The policy \u03c0 has a special termination action that terminates the option and yields a final state sf . Now let us consider the following example spaces for \u2126. 1) \u2126 takes a finite number of values \u2126 \u2208 {1, . . . , n}. This is the simplest case in which for each i a separate policy \u03c0i is followed. 2) \u2126 is a binary vector of length n. This captures a combinatorial number 2n of possibilities. 3) \u2126 \u2208 Rd is a d-dimensional real vector. Here the space of options is infinite. It is expected that policies for nearby \u2126s will be similar in practice.\nWe need to express the knowledge about which regions of option space to consider. Imagine we start in a state s0 and follow an option \u2126. As environments and policies are typically stochastic, we might terminate at different final states at different times. The policy thus defines a probability distribution pJ(sf |s0,\u2126). Now consider two different options. If they lead to very similar states, they should inherently, intrinsically, not be seen as different from one another. So how do we express our knowledge regarding the effective intrinsic option set in a given state?\nTo help answer this question, consider an example of a discrete option case with three options \u21261,\u21262,\u21263. Assume that \u21261 always leads to a state s1 while both \u21262 and \u21263 always lead to a state s2. Then we would like to say that we really have two intrinsic options: \u21261 and (\u21262,\u21263). If we were to sample these options in order to maximize behavior diversity we would half of the time choose \u21261 and half of the time any one of \u21262,\u21263. The relative choice frequencies of \u21262 and \u21263 do not matter in this example. We express these choices by a probability distribution pC(\u2126|s0) which we call the controllability distribution.\nIntuitively, to maximize intrinsic control we should choose \u2126s that maximize the diversity of final states while, for given \u2126, controlling as precisely as possible what the ensuing final states are. The former can be expressed mathematically as entropy H(sf ) = \u2212 \u2211 sf p(sf |s0) log p(sf |s0) where\np(sf |s0) = \u2211 \u2126 p J(sf |s0,\u2126)pC(\u2126|s0). The latter, for a given \u2126, can be expressed as the negative log probability \u2212 log pJ(sf |s0,\u2126) (the number of bits needed to specify the final state given \u2126) which then needs to be averaged over \u2126 and sf . Subtracting these two quantities yields the objective we wish to optimize \u2013 the mutual information I(\u2126, sf |s0) between options and final states under probability distribution p(\u2126, sf |s0) = pJ(sf |s0,\u2126)pC(\u2126|s0):\nI(\u2126, sf |s0) =\u2212 \u2211 sf p(sf |s0) log p(sf |s0) + \u2211 \u2126,sf pJ(sf |s0,\u2126)pC(\u2126|s0) log pJ(sf |s0,\u2126), (1)\n= \u2212 \u2211 \u2126 pC(\u2126|s0) log pC(\u2126|s0) + \u2211 \u2126,sf pJ(sf |s0,\u2126)pC(\u2126|s0) log p(\u2126|s0, sf ). (2)\nThe mutual information is symmetric and the second line contains its reverse expression. This expression has a very intuitive interpretation associated with it: we should be able to tell options apart if we can infer them from final states. That is, if for two options \u21261 and \u21262, upon reaching state sf1, we can infer it was option \u21261 that was executed rather than \u21262, and when reaching a state sf2 we can infer it was option \u21262 rather than \u21261, then \u21261 and \u21262 can be said to be intrinsically different options. We would like to maximize the set of options \u2013 achieve a large entropy of p(\u2126|s0) (the first term of (2)). At the same time we wish to make sure these options achieve intrinsically different goals \u2013 that is, that they can be inferred from their final states. This entails maximizing log p(\u2126|s0, sf ), the average of which is the second term of (2). The advantage of this formulation is the absence of the term p(sf |s0) in the formulation, which is difficult to obtain as we would have to integrate over \u2126. In rewriting the derivation, however, the term p(\u2126|s0, sf ) was introduced, which we arrived at from pJ(sf |s0,\u2126)pC(\u2126|s0) using Bayes\u2019 rule. The quantity pJ(sf |s0,\u2126) is inherent to the environment, but obtaining Bayes\u2019 reverse p(\u2126|s0, sf ) is difficult. However, it has an interpretation as a prediction of \u2126 from final state sf . It would be fortuitous if we could train a separate function approximator to infer this quantity. Fortunately this is exactly what the variational bound (Mohamed & Rezende, 2015) provides (see Appendix 1 for derivation):\nIV B(\u2126, sf |s0) = \u2212 \u2211 \u2126 pC(\u2126|s0) log pC(\u2126|s0) + \u2211 \u2126,sf pJ(sf |s0,\u2126)pC(\u2126|s0) log q(\u2126|s0, sf ) (3)\nwhere q is an option inference function which can be an arbitrary distribution, and we have I \u2265 IV B . In this paper we train both the parameters of pC(\u2126|s0), q(\u2126|s0, sf ) and the parameters of policy \u03c0(a|s,\u2126) (which determines pJ(sf |s0,\u2126)) to maximize IV B ."}, {"heading": "3 INTRINSIC CONTROL WITH EXPLICIT OPTIONS", "text": "In this section we provide a simple algorithm to maximize the variational bound introduced above. Throughout we assume we have distributions, policies, and other possible functions parameterized using recent function approximation techniques such as neural networks, and state representations are formed from observations using recurrent neural networks. However, we only calculate the mutual information between options and final observations instead of final states, and leave the latter for future work. Algorithm 1 provides an outline of the basic training loop.\nAlgorithm 1 Intrinsic Control with Explicit Options Assume an agent in a state s0 for episode = 1,M do\nSample \u2126 \u223c pC(\u2126|s0) Follow policy \u03c0(a|\u2126, s) till termination state sf Regress q(\u2126|s0, sf ) towards \u2126 Calculate intrinsic reward rI = log q(\u2126|s0, sf )\u2212 log pC(\u2126|s0) Use a reinforcement learning algorithm update for \u03c0(a|\u2126, s) to maximize rI . Reinforce option prior pC(\u2126|s0) based on rI . Set s0 = sf\nend for Note: Empowerment at s is estimated by the reinforce baseline of pC , which tracks rI .\nThis algorithm is derived from (3) in Appendix 2. Note again that \u03c0 appears in (3) by determining the distribution of terminal states pJ(sf |s0,\u2126). Here we give an intuitive explanation of the algorithm. In a state s0 an agent tries an option \u2126 from its available options pC(\u2126|s0). Its goal is to choose actions that lead to a state sf from which this \u2126 can be inferred as well as possible using option inference function q(\u2126|s0, sf ). If it can infer this option well, then it means that other options don\u2019t lead to this state very often, and therefore this option is intrinsically different from others. This goal is expressed as the intrinsic reward rI (discussed in the next paragraph). The agent can use any reinforcement learning algorithm (Sutton & Barto, 1998), such as policy gradients (Williams, 1992) or Q-learning (Watkins, 1989; Werbos, 1977), to train a policy to maximize this reward. In the final state, it updates its option inference function q towards the actual \u2126 chosen (by taking the gradient of log q(\u2126|s0, sf )). It also reinforces the prior pC based on this reward \u2013 if the reward were high, it should choose this option more often. Note that we can also keep prior pC fixed, for example to the uniform Gaussian distribution. Then, different values of \u2126 will result in different behavior through learning.\nThe intrinsic reward rI equals, on average, the logarithm of the number of different options an agent has in a given state \u2013 that is, the empowerment in that state. This follows from the definition of mutual information (3) \u2013 it is the expression we get when we take a sample of \u2126 and sf . However, we also provide an intuitive explanation. The log pC(\u2126|s0) is essentially the negative logarithm of the number of different \u2126s we can choose (for the continuous case, imagine finely discretizing). However, not all \u2126s do different things. The region where q(\u2126|s0, sf ) is large defines a region of similar options. The empowerment essentially equals the number of such regions in the total region given by pC . Taking the logarithm of the ratio of the total number of options \u223c 1/pC to the number of options within a region \u223c 1/q gives us log q/pC = log q \u2212 log pC = rI . We train pC using policy gradients (Williams, 1992). During training we estimate a baseline to lower the variance of weight updates (see Appendix 2). This baseline tracks the expected return in\na given state \u2013 intrinsic reward in this case, which equals the empowerment. As such, the algorithm actually yields an explicit empowerment estimate."}, {"heading": "3.1 EXPERIMENTS", "text": ""}, {"heading": "3.1.1 GRID WORLD", "text": "We demonstrate the behavior of the algorithm on a simple example of a two-dimensional grid world. The agent lives on a grid and has five actions \u2013 it can move up, down, right, left and stay put. The environment is noisy in the following manner: after an agent takes a step, with probability 0.2 the agent is pushed in a random direction. We follow Algorithm 1. We choose \u2126 to be a discrete space of N = 30 options. We fix the option prior pC to be uniform (over 30 values). The goal is therefore to learn a policy \u03c0(a|s,\u2126) that would make the 30 options end at as different states as possible. This is measured by the function q(\u2126|sf ) which, from the state sf reached, tries to infer which option \u2126 was followed. At the end of an episode we get an intrinsic reward rI = \u2212 log p+ log q = logN + log q (logN because pC = 1/N is fixed). If a particular option is inferred correctly and with confidence, then log q will be close to zero and negative, and the reward will be large (\u2248 logN ). If it is wrong, however, then log q will be very negative and the reward small. As we are choosing options at random (from the uniform pC), in order to get a large reward on average, different options need to reach substantially different states in order for the q to be able to infer the chosen option. In a grid world we can plot at which locations a given option is inferred by q, which are the locations to which the option navigates. This is shown in the Figure 1 top, with each rectangle corresponding to a different option, and the intensity denoting the probability of predicting a given option. Thus, we see that indeed, different options learn to navigate to different, localized places in the environment.\nIn this example, we use Q-learning to learn the policy. In general we can express the Q function for a set of N different options by running the corresponding input states through a neural network and outputting N \u00d7 nactions values, one for each option and action. This way, we can update Q of all the options at the same time efficiently, on a triplet of experience st, at, st+1. In this experiment we use a linear function approximator, and terminate options with fixed probability 1 \u2212 \u03b3 = 0.05. We could also use a universal value function approximation (Schaul et al., 2015). For continuous option spaces we can still calculate the Q by passing an input through a neural network, but then update the result on several, randomly sampled options \u2126 at the same time. Such an option space is then an option embedding in itself."}, {"heading": "3.1.2 \u2019DANGEROUS\u2019 GRID WORLD", "text": "The second environment is also a grid world, but with special properties. It consists of two parts: a narrow corridor connected to an open square (see figure 2, top-left), blue denoting the walls. However the square is not just a simple grid world, but is somewhat dangerous. There are two types of cells arranged as a checkerboard lattice (as on a chess board). On one sub-lattice, only the left and right actions actually move the agent to the adjacent states and on the other sub-lattice only the up and down actions do. If the agent picks an action that is not one of these, it falls into a state\nwhere it is stuck for a long time. Furthermore, the move to an adjacent state only happens with some probability. Because of this, if the agent doesn\u2019t observe the environment, it quickly loses the information about which sub lattice it is on, and thus inevitably falls to the low empowerment state. To show this we computed the exact empowerment values at each location for an open-loop policy using the Blahut-Arimoto algorithm for horizons 1 to 6, the result is shown in figure 2(bottom). On the other hand, if the agent observes the environment, it knows which sub-lattice it is on and can always choose an action that doesn\u2019t let it fall. Thus it can safely navigate the square. In our experiments, the agent indeed accomplishes this, and it learns options inside the square (see figure 2, top-right)."}, {"heading": "3.2 THE IMPORTANCE OF CLOSED LOOP POLICIES", "text": "Classical empowerment (Salge et al., 2014; Mohamed & Rezende, 2015) maximizes mutual information between sequences of actions A = a1, . . . , aT and final states. That is, it maximizes the same objective function (3), but where \u2126 = A. This corresponds to maximizing empowerment over the space of open loop options. That is, options where an agent first commits to a sequence of actions and then blindly follows this sequence regardless of what the environment does. In contrast, in a closed-loop option every action is conditioned on the current state. We show that using open-loop options can lead to severe underestimation of empowerment in stochastic environments, resulting in agents that aim to reach low-empowerment states.\nWe demonstrate this effect in the \u2019dangerous grid world\u2019 environment, section 3.1.2. When using open loop options of length T , an agent at the center of the environment would have exponentially growing probability of being reset as a function of the option length T , resulting in an estimation of empowerment that quickly decreases with the option length, having its highest value inside the corridor at the top-left corner as shown in Figure 1 (bottom). A consequence of this is that such an agent would prefer being inside the corridor at the top-left corner, away from the center of the grid world.\nIn great contrast to the open loop case, when using closed loop options the empowerment will grow quadratically with the option length, resulting in agents that prefer staying at the center of the grid world.\nWhile this example might seem contrived, it is actually quite ubiquitous in the real world. For example, we can navigate around a city, whether walking or driving, quite safely. If we instead committed to a sequence of actions ahead, we would almost certainly be run over by a car, or if driving, crash into another car. The importance of the closed loop nature of policies is indeed well understood. What we have demonstrated here is that one should not use open loop policies even to measure empowerment."}, {"heading": "3.3 ADVANTAGES AND DISADVANTAGES", "text": "The advantages of Algorithm 1 are: 1) It is relatively simple, 2) it uses closed loop policies, 3) it can use general function approximation, 4) it is naturally formulated with combinatorial options spaces, both discrete and continuous and 5) it is model-free.\nThe primary problem we found with this algorithm is that it is difficult to make it work in practice with function approximation. We suggest there might be two reasons for this. First, the intrinsic reward is noisy and changing as the agent learns. This makes it difficult for the policy to learn. The algorithm worked as specified in those simple environments above when we used linear function approximation and a small, finite number of options. However, it failed when neural networks were substituted. We still succeeded by fixing the intrinsic reward for a period of time while learning the policy and vice versa. However, replacing the small option space by a continuous one made training even more difficult and only some runs succeeded. These problems are related to those in deep reinforcement learning (Mnih et al., 2015), where in order to make Q learning work well with function approximation, one needs to store a large number of experiences in memory and replay them. It is possible that more work in this direction would find good practices for training this algorithm with general function and distribution approximations.\nThe second problem is exploration. If the agent encounters a new state, it should like to go there, because it might correspond to some new option it hasn\u2019t considered before and therefore increase its control. However, when it gets there, the option inference function q has not learned it yet. It is likely inferring the incorrect option, therefore giving a low reward and therefore discouraging the agent from going there. While the overall objective is maximized when the agent has the most control, the algorithm has difficulty maximizing this objective because two functions \u2013 the intrinsic reward and the policy \u2013 have to match up. It does a good job of expressing what the options are in a region it is familiar with, but it seems to fail to push into new state regions. Hence, we introduce a new algorithm formulation in section 4 to address these issues."}, {"heading": "4 INTRINSIC CONTROL WITH IMPLICIT OPTIONS", "text": "To address the learning difficulties of Algorithm 1 we use the action space itself as the option space. This gives the inference function q grounded targets which makes it easier to train. Having a sensible q makes the policy easier to train. The elements of the algorithm are as follows. The controllability prior pC and policy \u03c0 in Algorithm 1 simply become a policy, which we denote by \u03c0p(at|spt ). The spt is an internal state that is calculated from (s p t\u22121, xt, at\u22121). In our implementation it is the state of a recurrent network. The q function in Algorithm 1 should infer the action choices made by \u03c0p knowing the final observation xf and thus becomes q = \u03c0q(at|sqt ) where sq is its internal state calculated from (sqt\u22121, xt, at\u22121, xf ). The logarithm of the number of action choices at t that are effectively different from each other \u2013 that can be distinguished based on the observation of the final state xf \u2013 is given by rI,t = log \u03c0q(at|sqt ) \u2212 log \u03c0p(at|s p t ). We now introduce an algorithm that will maximize the expected cumulative number of distinct actions by maximizing the intrinsic return RI = \u2211 t rI,t in algorithm 2.\nIn this setting, maximization of control is substantially simplified. Consider an experience x0, a0, . . . , xf generated by some policy. The learning of \u03c0q is a supervised learning problem of inferring the action choices that led to xf . Even the random action policy terminates at different states, and thus \u03c0q is able to train on such experiences, mimicking decisions that happen to lead to xf . The \u03c0p can be thought of as choosing among those \u03c0q that lead to diverse states, which in\nturn makes \u03c0q learn from experiences generated by those policies. The ability of \u03c0q to train on any experience motivated us to add an exploratory update in Algorithm 2.\nAlgorithm 2 Intrinsic Control with Implicit Options Full update Follow policy \u03c0p(at|spt ), s p t = f\np(spt\u22121, xt, at\u22121) resulting in experience x0, a0, . . . , xf . For each t, regress policy \u03c0q(at|sqt ), s q t = f\nq(sqt\u22121, xt, at\u22121, xf ) towards action at Calculate intrinsic reward rI = \u2211 t log \u03c0 q(at|sqt )\u2212 log \u03c0p(at|s p t ) Reinforce the policy \u03c0p with rI . Exploratory update Follow policy \u03c0p(at|spt ), s p t = f\np(spt\u22121, xt, at\u22121) with exploration ( or other) resulting in experience x\u20320, a \u2032 0, . . . , x \u2032 f . For each t, regress policy \u03c0q(a\u2032t|s q t ), s q t = f q(sqt\u22121, x \u2032 t, a \u2032 t\u22121, x \u2032 f ) towards action a \u2032 t Note: Empowerment is estimated by the reinforce baseline of \u03c0p.\nIn the experiments that follow, we use the following functions for policies (see Appendix 3 for the equations). Every input xt is passed through a fully connected, one layer neural network with a standard rectifier non-linearity, resulting in an embedding u(xt). This is passed to an LSTM recurrent net (Hochreiter & Schmidhuber, 1997) which outputs the policy probabilities \u03c0p over actions. For \u03c0q , we concatenate the embedding u(xt) and u(xf ) and pass through another one layer neural net to obtain a state v(xt, xf ). This, together with hidden state of the LSTM net, is passed to another LSTM network, which outputs the probabilities \u03c0q over actions.\nTwo facts are worth highlighting here. First, Algorithm 2 applies to general, partially observable environments since the policies \u03c0p and \u03c0q build their own internal states. However, more generally, we should use final states sf of the environment instead of observations xf as the set of states an agent can reach. We leave this aspect to future work. Second, the policy \u03c0p can be thought of as an implicit option. However, the embedding u(xf ) of the final observation (or state more generally) can be thought of as an explicit option and the policy \u03c0q as the policy implementing this option."}, {"heading": "4.1 EXPERIMENTS", "text": "We test this algorithm on several environments. The first one is a grid world of size 25 \u00d7 25 with four rooms (see Figure 3 left (no action noise)). A random action policy of length T leads to final states whose distance from the initial state is distributed approximately according to a Gaussian distribution of width \u223c \u221a T within a room. For T on the order of the environment size, such an agent rarely crosses to a different room, because of the narrow doors between the rooms. Figure 3 shows trajectories learned by Algorithm 2. We see that indeed they are extended, spanning large parts of the environment. Furthermore many trajectories cross to different rooms passing straight through the narrow doors without difficulty. This is interesting, because while the policy \u03c0q is conditioned on the final state, the policy \u03c0p that is actually followed was not given any notion of the final state explicitly. It implicitly learns to navigate through the doors to different parts of the environment.\nTo maximize intrinsic control, the distribution of final points that \u03c0p reaches should be uniform among the points that are reachable from a given state. This is because we can tell every point from any other point equally well. Figure 3 (center) shows the distribution of the end points reached by the algorithm for trajectories of length 25, starting at different points in the environment. For example, the top left square shows the end point distribution for starting at the top left corner. We see that the distribution is indeed roughly uniform among the reachable points. The average empowerment after learning reaches 6.0 nats which corresponds to exp(6.0) \u2248 403 different reachable states. In the second experiment we use a three dimensional simulated environment (Figure 3, right). At a given time the agent sees a particular view of its environment which is a 40 \u00d7 40 color image. The figure shows example trajectories that the agent follows (moving downwards in the figure) using policy \u03c0p. We see that the trajectories seem intuitively much more \u2018consistent\u2019 than those a random action policy would produce. The average empowerment achieved for trajectories of length 12 was 5.4 nats which corresponds to reaching exp(5.4) = 221 different states.\nThe third environment is again a grid world, but it contains blocks that the agent can push. The blocks cannot pass through each other or through the boundary, and the agent cannot push two blocks at the same time. In this case, the visual space is small, but there are combinatorially many possibilities. Figure 4 shows a typical trajectory. We see that the agent pushes the first block down, then it goes around the second block and pushes it up, then goes to the third one and pushes it down, and then arrives at its final position. The average empowerment is 7.1 nats which corresponds to being able to reach exp(7.1) \u2248 1200 different states."}, {"heading": "4.2 ELEMENTS BEYOND AN AGENT\u2019S CONTROL", "text": "One prevalent feature of the real world is that there are many elements beyond our control such as falling leaves or traffic. One of the important features of these algorithms is that intrinsic options represent things that an agent can actually control and as such does not have to model all the complexities of the real world \u2013 these algorithms are model-free. To demonstrate this property we introduce environments with elements beyond an agent\u2019s control.\nThe first environment is the same four room grid world used above, but with two distractors moving around at random, that live on different input feature planes. These distractors do not affect the agent, but the agent does observe them. The agent needs to learn to ignore them. We find that the agent reaches the same amount of empowerment with and without the distractors (see Figure 6 in Appendix 4).\nThe second environment consists of pairs of MNIST digits forming a 28\u00d7 (2\u221728) image. There are five actions that affect classes of the digits. The first action doesn\u2019t change the classes, the next two increase/decrease the class of the first digit and the next two increase/decrease the class of the second digit (wrapping around). When a class is chosen, a random digit from that class is selected. Thus the environment is visually complex, but has a small control space. Example trajectories followed by the agent are shown in Figure 5, left. The empowerment obtained by the agent with policies of length 10 actions is 4.6, which corresponds to exp(4.6) = 99.5 states. As there are 100 controllable states in the environment (but 600002 total states), we see that agent achieves nearly maximum possible intrinsic control."}, {"heading": "4.3 OPEN VS CLOSED LOOP OPTIONS", "text": "Next we compare agents utilizing open and closed loop options. We use the grid world environment but add noise as follows. After an agent takes a step, the environment pushes the agent in a random direction. While the closed loop policy can correct for the environment noise, for example by following a strategy of always going towards the goal, an open loop policy agent has less and less information on where it is in the environment as time goes by and it cannot navigate reliably towards the actual target location. We implement the open loop agent using the Algorithm 2 but we only feed the starting and the end states to the recurrent network. Table 1 shows the comparison. We see that the closed loop agent indeed performs much better."}, {"heading": "4.4 MAXIMIZING EXTRINSIC REWARD", "text": "Finally, while the primary focus of the paper is unsupervised control, we provide a proof-of-principle experiment that shows that learned policies can help in learning extrinsic reward. We consider the following situation. An agent is placed in an environment without being told what the objective is. The agent has an opportunity to explore and learn how to control the environment. After some amount of time, the agent is told the objective as an extrinsic reward rE , and has a limited amount of time to collect as much reward as possible. There could be a number of ways to use the learned policies \u03c0p and \u03c0q to maximize the extrinsic reward. In our case we simply combine the intrinsic and extrinsic rewards, and reinforce the policy \u03c0p in Algorithm 2 with r = rI + \u03b1rE where \u03b1 is a large constant (30 in our experiment). We use the 15\u00d715 four room grid world and, after different periods of time, we place a reward at location 3\u00d7 3. Figure 5 (right) shows the reward collected per episode after different amounts of unsupervised pre-training or using pure reinforce without the maximum control objective (red curve). We see that, indeed, the agent learns to collect reward significantly faster after having an opportunity to interact with the environment."}, {"heading": "5 CONCLUSION", "text": "In this paper we introduce a formalism of intrinsic control maximization for unsupervised option learning. We presented two algorithms in this framework and analyzed them in a diverse range of experiments. We demonstrated the importance of closed loop policies for estimating empowerment. Additionally, we also demonstrated the usefulness of unsupervised learning and intrinsic control for extrinsic reward maximization."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Shakir Mohamed, Frederic Besse, David Siver, Ivo Danihelka, Remi Munos, Ali Eslami, Tom Schaul, Nicolas Heess and Daniel Polani for useful discussions and comments."}], "references": [{"title": "An algorithm for computing the capacity of arbitrary discrete memoryless channels", "author": ["Suguru Arimoto"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Arimoto.,? \\Q1972\\E", "shortCiteRegEx": "Arimoto.", "year": 1972}, {"title": "The option-critic architecture", "author": ["Pierre-Luc Bacon", "Doina Precup"], "venue": "In NIPS Deep Reinforcement Learning Workshop,", "citeRegEx": "Bacon and Precup.,? \\Q2015\\E", "shortCiteRegEx": "Bacon and Precup.", "year": 2015}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos"], "venue": "arXiv preprint arXiv:1606.01868,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Computation of channel capacity and rate-distortion functions", "author": ["Richard Blahut"], "venue": "IEEE transactions on Information Theory,", "citeRegEx": "Blahut.,? \\Q1972\\E", "shortCiteRegEx": "Blahut.", "year": 1972}, {"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "URL http://www.deeplearningbook.org", "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Variational information maximizing exploration", "author": ["Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1605.09674,", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Empowerment: A universal agentcentric measure of control", "author": ["Alexander S Klyubin", "Daniel Polani", "Chrystopher L Nehaniv"], "venue": "IEEE Congress on Evolutionary Computation,", "citeRegEx": "Klyubin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Klyubin et al\\.", "year": 2005}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["Tejas D Kulkarni", "Karthik R Narasimhan", "Ardavan Saeedi", "Joshua B Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057,", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Time-regularized interrupting options", "author": ["Daniel J Mankowitz", "Timothy A Mann", "Shie Mannor"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "Mankowitz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mankowitz et al\\.", "year": 2014}, {"title": "Automatic discovery of subgoals in reinforcement learning using diverse density", "author": ["Amy McGovern", "Andrew G Barto"], "venue": null, "citeRegEx": "McGovern and Barto.,? \\Q2001\\E", "shortCiteRegEx": "McGovern and Barto.", "year": 2001}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Variational information maximisation for intrinsically motivated reinforcement learning", "author": ["Shakir Mohamed", "Danilo Jimenez Rezende"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mohamed and Rezende.,? \\Q2015\\E", "shortCiteRegEx": "Mohamed and Rezende.", "year": 2015}, {"title": "How can we define intrinsic motivation", "author": ["Pierre-Yves Oudeyer", "Frederic Kaplan"], "venue": "In Proc. 8th Int. Conf. Epigenetic Robot.: Modeling Cogn. Develop. Robot. Syst,", "citeRegEx": "Oudeyer and Kaplan,? \\Q2008\\E", "shortCiteRegEx": "Oudeyer and Kaplan", "year": 2008}, {"title": "Empowerment\u2013an introduction", "author": ["Christoph Salge", "Cornelius Glackin", "Daniel Polani"], "venue": "In Guided Self-Organization: Inception,", "citeRegEx": "Salge et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Salge et al\\.", "year": 2014}, {"title": "Universal value function approximators", "author": ["Tom Schaul", "Daniel Horgan", "Karol Gregor", "David Silver"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Curious model-building control systems", "author": ["J\u00fcrgen Schmidhuber"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1991}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "Compositional planning using optimal option models", "author": ["David Silver", "Kamil Ciosek"], "venue": "arXiv preprint arXiv:1206.6473,", "citeRegEx": "Silver and Ciosek.,? \\Q2012\\E", "shortCiteRegEx": "Silver and Ciosek.", "year": 2012}, {"title": "Learning options in reinforcement learning", "author": ["Martin Stolle", "Doina Precup"], "venue": "In International Symposium on Abstraction, Reformulation, and Approximation,", "citeRegEx": "Stolle and Precup.,? \\Q2002\\E", "shortCiteRegEx": "Stolle and Precup.", "year": 2002}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["Richard S Sutton", "Doina Precup", "Satinder Singh"], "venue": "Artificial intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1606.04695,", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis,", "citeRegEx": "Watkins.,? \\Q1989\\E", "shortCiteRegEx": "Watkins.", "year": 1989}], "referenceMentions": [{"referenceID": 21, "context": "This differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015).", "startOffset": 152, "endOffset": 337}, {"referenceID": 8, "context": "This differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015).", "startOffset": 152, "endOffset": 337}, {"referenceID": 9, "context": "This differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015).", "startOffset": 152, "endOffset": 337}, {"referenceID": 22, "context": "This differs from the traditional approach to option learning where the goal is to find a small number of options that are useful for a particular task (Sutton et al., 1999; McGovern & Barto, 2001; Stolle & Precup, 2002; Silver & Ciosek, 2012; Kulkarni et al., 2016; Mankowitz et al., 2014; Vezhnevets et al., 2016; Bacon & Precup, 2015).", "startOffset": 152, "endOffset": 337}, {"referenceID": 15, "context": "The idea of goal and state embeddings, along with a universal value function for reaching these goals, was introduced in Schaul et al. (2015). This work allowed an agent to efficiently represent control over many goals and to generalize to new goals.", "startOffset": 121, "endOffset": 142}, {"referenceID": 14, "context": "The second scenario is that in which the long-term goal of the agent is to get to a state with a maximal set of available intrinsic options \u2013 the objective of empowerment (Salge et al., 2014).", "startOffset": 171, "endOffset": 191}, {"referenceID": 16, "context": "Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016).", "startOffset": 211, "endOffset": 284}, {"referenceID": 2, "context": "Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016).", "startOffset": 211, "endOffset": 284}, {"referenceID": 6, "context": "Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016).", "startOffset": 211, "endOffset": 284}, {"referenceID": 2, "context": "Let us compare this to the commonly used intrinsic motivation objective of maximizing the amount of model-learning progress, measured as the difference in compression of its experience before and after learning (Schmidhuber, 1991; 2010; Bellemare et al., 2016; Houthooft et al., 2016). The empowerment objective differs from this in a fundamental manner: the primary goal is not to understand or predict the observations but to control the environment. This is an important point \u2013 agents can often control an environment perfectly well without much understanding, as exemplified by canonical model-free reinforcement learning algorithms (Sutton & Barto, 1998), where agents only model action-conditioned expected returns. Focusing on such understanding might significantly distract and impair the agent, as such reducing the control it achieves. Our algorithm can be viewed as learning to represent the intrinsic control space of an agent. Developing this space should be seen as acquiring universal knowledge useful for accomplishing a multitude of different tasks, such as maximizing extrinsic or intrinsic reward (see Oudeyer et al. (2008) for an overview and useful references).", "startOffset": 237, "endOffset": 1144}, {"referenceID": 4, "context": "Just like there are multiple methods and objectives for unsupervised learning (Goodfellow et al., 2016), we can devise multiple methods and objectives for unsupervised control.", "startOffset": 78, "endOffset": 103}, {"referenceID": 14, "context": "This information measure has been introduced in the empowerment literature before (Salge et al., 2014; Klyubin et al., 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)).", "startOffset": 82, "endOffset": 124}, {"referenceID": 7, "context": "This information measure has been introduced in the empowerment literature before (Salge et al., 2014; Klyubin et al., 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)).", "startOffset": 82, "endOffset": 124}, {"referenceID": 2, "context": ", 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)).", "startOffset": 53, "endOffset": 67}, {"referenceID": 0, "context": ", 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)).", "startOffset": 68, "endOffset": 83}, {"referenceID": 0, "context": ", 2005) along with methods for measuring it (such as Blahut (1972); Arimoto (1972)). Recently, Mohamed & Rezende (2015) proposed an algorithm that can utilize function approximation and deep learning techniques to operate in high-dimensional environments.", "startOffset": 68, "endOffset": 120}, {"referenceID": 23, "context": "The agent can use any reinforcement learning algorithm (Sutton & Barto, 1998), such as policy gradients (Williams, 1992) or Q-learning (Watkins, 1989; Werbos, 1977), to train a policy to maximize this reward.", "startOffset": 135, "endOffset": 164}, {"referenceID": 15, "context": "We could also use a universal value function approximation (Schaul et al., 2015).", "startOffset": 59, "endOffset": 80}, {"referenceID": 14, "context": "Classical empowerment (Salge et al., 2014; Mohamed & Rezende, 2015) maximizes mutual information between sequences of actions A = a1, .", "startOffset": 22, "endOffset": 67}, {"referenceID": 11, "context": "These problems are related to those in deep reinforcement learning (Mnih et al., 2015), where in order to make Q learning work well with function approximation, one needs to store a large number of experiences in memory and replay them.", "startOffset": 67, "endOffset": 86}], "year": 2016, "abstractText": "In this paper we introduce a new unsupervised reinforcement learning method for discovering the set of intrinsic options available to an agent. This set is learned by maximizing the number of different states an agent can reliably reach, as measured by the mutual information between the set of options and option termination states. To this end, we instantiate two policy gradient based algorithms, one that creates an explicit embedding space of options and one that represents options implicitly. The algorithms also provide an explicit measure of empowerment in a given state that can be used by an empowerment maximizing agent. The algorithm scales well with function approximation and we demonstrate the applicability of the algorithm on a range of tasks.", "creator": "LaTeX with hyperref package"}}}