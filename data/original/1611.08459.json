{"id": "1611.08459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2016", "title": "Neural Machine Translation with Latent Semantic of Image and Text", "abstract": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained end-to-end, requires image information only when training. Experiments conducted with an English--German translation task show that our model outperforms over the baseline.", "histories": [["v1", "Fri, 25 Nov 2016 14:10:39 GMT  (4451kb,D)", "http://arxiv.org/abs/1611.08459v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["joji toyama", "masanori misono", "masahiro suzuki", "kotaro nakayama", "yutaka matsuo"], "accepted": false, "id": "1611.08459"}, "pdf": {"name": "1611.08459.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Joji Toyama", "Masanori Misono", "Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "emails": ["toyama@weblab.t.u-tokyo.ac.jp", "misono@weblab.t.u-tokyo.ac.jp", "masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "sections": [{"heading": "1 INTRODUCTION", "text": "Neural machine translation (NMT) has achieved great success in recent years (Sutskever et al., 2014; Bahdanau et al., 2015). In contrast to statistical machine translation, which requires huge phrase and rule tables, NMT requires much less memory. However, the most standard model, NMT with attention (Bahdanau et al., 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al., 2016). To overcome this problem, Variational Neural Machine Translation (VNMT), which outperforms NMT with attention introduces a latent variable to capture the underlying semantic from source and target (Zhang et al., 2016). We follow the motivation of VNMT, which is to capture underlying semantic of a source.\nImage information is related to language. For example, we human beings understand the meaning of language by linking perceptual information given by the surrounding environment and language (Barsalou, 1999). Although it is natural and easy for humans, it is difficult for computers to understand different domain\u2019s information integrally. Solving this difficult task might, however, bring great improvements in natural language processing. Several researchers have attempted to link language and images such as image captioning by Xu et al. (2015) or image generation from sentences by Reed et al. (2016). They described the possibility of integral understanding of images and text. In machine translation, we can expect an improvement using not only text information but also image information because image information can bridge two languages.\nAs described herein, we propose the neural machine translation model which introduces a latent variable containing an underlying semantic extracted from texts and images. Our model includes an explicit latent variable z, which has underlying semantics extracted from text and images by introducing a Variational Autoencoder (VAE) (Kingma et al., 2014; Rezende et al., 2014). Our model,\n\u2217First two authors contributed equally.\nar X\niv :1\n61 1.\n08 45\n9v 1\n[ cs\n.C L\n] 2\n5 N\nov 2\n01 6\nGreen dotted lines denote that \u03c0 and encoded y are used only when training.\nwhich can be trained end-to-end, requires image information only when training. As described herein, we tackle the task with which one uses a parallel corpus and images in training, while using a source corpus in translating. It is important to define the task in this manner because we rarely have a corresponding image when we want to translate a sentence. During translation, our model generates a semantic variable z from a source, integrates variable z into a decoder of neural machine translation system, and then finally generates the translation. The difference between our model and VNMT is that we use image information in addition to text information.\nFor experiments, we used Multi30k (Elliott et al., 2016), which includes images and the corresponding parallel corpora of English and German. Our model outperforms the baseline with two evaluation metrics: METEOR (Denkowski & Lavie, 2014) and BLEU (Papineni et al., 2002). Moreover, we obtain some knowledge related to our model and Multi30k. Finally, we present some examples in which our model either improved, or worsened, the result.\nOur paper contributes to the neural machine translation research community in three ways.\n\u2022 We present the first neural machine translation model to introduce a latent variable inferred from image and text information. We also present the first translation task with which one uses a parallel corpus and images in training, while using a source corpus in translating.\n\u2022 Our translation model can generate more accurate translation by training with images, especially for short sentences.\n\u2022 We present how the translation of source is changed by adding image information compared to VNMT which does not use image information."}, {"heading": "2 BACKGROUND", "text": "Our model is the extension of Variational Neural Machine Translation (VNMT) (Zhang et al., 2016). Our model is also viewed as one of the multimodal translation models. In our model, VAE is used to introduce a latent variable. We describe the background of our model in this section."}, {"heading": "2.1 VARIATIONAL NEURAL MACHINE TRANSLATION", "text": "The VNMT translation model introduces a latent variable. This model\u2019s architecture shown in Figure 1 excludes the arrow from \u03c0. This model involves three parts: encoder, inferrer, and decoder. In the encoder, both the source and target are encoded by bidirectional-Recurrent Neural Networks (bidirectional-RNN) and a semantic representation is generated. In the inferrer, a latent variable z is\nmodeled from a semantic representation by introducing VAE. In the decoder, a latent variable z is integrated in the Gated Recurrent Unit (GRU) decoder; also, a translation is generated.\nOur model is followed by architecture, except that the image is also encoded to obtain a latent variable z."}, {"heading": "2.2 MULTIMODAL TRANSLATION", "text": "Multimodal Translation is the task with which one might one can use a parallel corpus and images. The first papers to study multimodal translation are Elliott et al. (2015) and Hitschler & Riezler (2016). It was selected as a shared task in Workshop of Machine Translation 2016 (WMT161). Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovicky\u0301 et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-jussa\u0300, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016). Here, we introduce end-to-end neural network translation models like our model.\nCaglayan et al. (2016) integrate an image into an NMT decoder. They simply put source context vectors and image feature vectors extracted from ResNet-50\u2019s \u2018res4f relu\u2019 layer (He et al., 2016) into the decoder called multimodal conditional GRU. They demonstrate that their method does not surpass the text-only baseline: NMT with attention.\nHuang et al. (2016) integrate an image into a head of source words sequence. They extract prominent objects from the image by Region-based Convolutional Neural Networks (R-CNN) (Girshick, 2015). Objects are then converted to feature vectors by VGG-19 (Simonyan & Zisserman, 2014) and are put into a head of source words sequence. They demonstrate that object extraction by R-CNN contributes greatly to the improvement. This model achieved the highest METEOR score in NMTbased models in WMT16, which we compare to our model in the experiment. We designate this model as CMU.\nCaglayan et al. (2016) argue that their proposed model did not achieve improvement because they failed to benefit from both text and images. We assume that they failed to integrate text and images because they simply put images and text into neural machine translation despite huge gap exists between image information and text information. Our model, however, presents the possibility of benefitting from images and text because text and images are projected to their common semantic space so that the gap of images and text would be filled."}, {"heading": "2.3 VARIATIONAL AUTO ENCODER", "text": "VAE was proposed in an earlier report of the literature Kingma et al. (2014); Rezende et al. (2014). Given an observed variable x, VAE introduces a continuous latent variable z, with the assumption that x is generated from z. VAE incorporates p\u03b8(x|z) and q\u03c6(z|x) into an end-to-end neural network. The lower bound is shown below.\nLVAE = \u2212DKL [q\u03c6(z|x)||p\u03b8(z)] + Eq\u03c6(z|x) [log p\u03b8(x|z)] \u2264 log p\u03b8(x) (1)"}, {"heading": "3 NEURAL MACHINE TRANSLATION WITH LATENT SEMANTIC OF IMAGE AND TEXT", "text": "We propose a neural machine translation model which explicitly has a latent variable containing an underlying semantic extracted from both text and image. This model can be seen as an extension of VNMT by adding image information.\nOur model can be drawn as a graphical model in Figure 3. Its lower bound is\nL = \u2212DKL [q\u03c6(z|x,y,\u03c0)||p\u03b8(z|x)] + Eq\u03c6(z|x,y,\u03c0) [log p\u03b8(y|z,x)] , (2)\nwhere x,y,\u03c0, z respectively denote the source, target, image and latent variable, and p\u03b8 and q\u03c6 respectively denote the prior distribution and the approximate posterior distribution. It is noteworthy in Eq. (2) that we want to model p(z|x,y,\u03c0), which is intractable. Therefore we model q\u03c6(z|x,y,\u03c0)\n1http://www.statmt.org/wmt16/\nz\nx y\nz\nx y\nFigure 2: VNMT\nz\nx y\nz\nx y\n\u03c0\nFigure 3: Our model\ninstead, and also model prior p\u03b8(z|x) so that we can generate a translation from the source in testing. Derivation of the formula is presented in the appendix.\nWe model all distributions in Eq. (2) by neural networks. Our model architecture is divisible into three parts: 1) encoder, 2) inferrer, and 3) decoder."}, {"heading": "3.1 ENCODER", "text": "In the encoder, the semantic representation he is obtained from the image, source, and target. We propose several methods to encode an image. We show how these methods affect the translation result in the Experiment section. This representation is used in the inferrer. This section links to the green part of Figure 1."}, {"heading": "3.1.1 TEXT ENCODING", "text": "The source and target are encoded in the same way as Bahdanau et al. (2015). The source is converted to a sequence of 1-of-k vector and is embedded to demb dimensions. We designate it as the source sequence. Then, a source sequence is put into bidirectional RNN. Representation hi is obtained by concatenating ~hi and ~hi : ~hi = RNN(~hi\u22121, Ewi), ~hi = RNN( ~hi+1, Ewi),hi = [~hi; ~hi], where Ewi is the embedded word in a source sentence, hi \u2208 Rdh , and ~hi, ~hi \u2208 R dh 2 . It is conducted through i = 0 to i = Tf , where Tf is the sequence length. GRU is implemented in bidirectional RNN so that it can attain long-term dependence. Finally, we conduct mean-pooling to hi and obtain the source representation vector as hf = 1Tf \u2211Tf i hi. The exact same process is applied to target to obtain target representation hg ."}, {"heading": "3.1.2 IMAGE ENCODING AND SEMANTIC REPRESENTATION", "text": "We use Convolutional Neural Networks (CNN) to extract feature vectors from images. We propose several ways of extracting image features.\nGlobal (G) The image feature vector is extracted from the image using a CNN. With this method, we use a feature vector in the certain layer as \u03c0. Then \u03c0 is encoded to the image representation vector h\u03c0 simply by affine transformation as\nh\u03c0 =W\u03c0\u03c0 + b\u03c0 where W\u03c0 \u2208 Rd\u03c0\u00d7dfc7 , b\u03c0 \u2208 Rd\u03c0 . (3)\nGlobal and Objects (G+O) First we extract some prominent objects from images in some way. Then, we obtain fc7 image feature vectors \u03c0 from the original image and extracted objects using a CNN. Therefore \u03c0 takes a variable length. We handle \u03c0 in two ways: average and RNN encoder. In average (G+O-AVG), we first obtain intermediate image representation vector h\u2032\u03c0 by affine transformation in Eq. (3). Then, the average of h\u2032\u03c0 becomes the image representation\nvector: h\u03c0 = \u2211l i h \u2032 \u03c0i l , where l is the length of h \u2032 \u03c0 .\nIn RNN encoder (G+O-RNN), we first obtain h\u2032\u03c0 by affine transformation in Eq. (3). Then, we encode h\u2032\u03c0 in the same way as we encode text in Section 3.1.1 to obtain h\u03c0 .\nGlobal and Objects into source and target (G+O-TXT) Thereby, we first obtain h\u2032\u03c0 by affine transformation in Eq. (3). Then, we put sequential vector h\u2032\u03c0 into the head of the source sequence and target sequence. In this case, we set d\u03c0 to be the same dimension as demb. In fact, the source sequence including h\u2032\u03c0 is only used to model q\u03c6(z|x,y,\u03c0). Context vector c (Eq. (15)) and p\u03b8(z|x) are computed by a source sequence that does not include h\u2032\u03c0 . We encode the source sequence including h\u2032\u03c0 as Section 3.1.1 to obtain hf and hg . In this case, h\u03c0 is not obtained. Image information is contained in hf and hg .\nAll representation vectors hf , hg and h\u03c0 are concatenated to obtain a semantic representation vector as he = [hf ;hg;h\u03c0], where he \u2208 Rde=2\u00d7dh+d\u03c0 (in G+O-TXT: he = [hf ;hg], where he \u2208 Rde=2\u00d7dh ). It is an input of the multimodal variational neural inferrer."}, {"heading": "3.2 INFERRER", "text": "We model the posterior q\u03c6(z|x,y,\u03c0) using a neural network and also the prior p\u03b8(z|x) by neural network. This section links to the black and grey part of Figure 1."}, {"heading": "3.2.1 NEURAL POSTERIOR APPROXIMATOR", "text": "Modeling the true posterior p\u03b8(z|x,y,\u03c0) is usually intractable. Therefore, we consider modeling of an approximate posterior q\u03c6(z|x,y,\u03c0) by introducing VAE. We assume that the posterior q\u03c6(z|x,y,\u03c0) has the following form:\nq\u03c6(z|x,y,\u03c0) = N (z;\u00b5(x,y,\u03c0),\u03c3(x,y,\u03c0)2I). (4)\nThe mean \u00b5 and standard deviation \u03c3 of the approximate posterior are the outputs of neural networks.\nStarting from the variational neural encoder, a semantic representation vector he is projected to latent semantic space as\nhz = g(W (1) z he + b (1) z ), (5)\nwhere W (1)z \u2208 Rdz\u00d7(de) b(1)z \u2208 Rdz . g(\u00b7) is an element-wise activation function, which we set as tanh(\u00b7). Gaussian parameters of Eq. (4) are obtained through linear regression as\n\u00b5 =W\u00b5hz + b\u00b5, log\u03c3 2 =W\u03c3hz + b\u03c3, (6)\nwhere \u00b5, log\u03c32 \u2208 Rdz ."}, {"heading": "3.2.2 NEURAL PRIOR MODEL", "text": "We model the prior distribution p\u03b8(z|x) as follows:\np\u03b8(z|x) = N (z;\u00b5\u2032(x),\u03c3\u2032(x) 2 I).\n(7)\n\u00b5\u2032 and \u03c3\u2032 are generated in the same way as that presented in Section 3.2.1, except for the absence of y and \u03c0 as inputs. Because of the absence of representation vectors, the dimensions of weight in equation (5) for prior model are W \u2032(1) z \u2208 Rdz\u00d7dh , b \u2032(1) z \u2208 Rdz . We use a reparameterization trick to obtain a representation of latent variable z: h\u2032z = \u00b5+\u03c3 , \u223c N (0, I). During translation, h\u2032z is set as the mean of p\u03b8(z|x). Then, h\u2032z is projected onto the target space as\nh\u2032e = g(W (2) z h \u2032 z + b (2) z ) where h \u2032 e \u2208 Rde . (8)\nh\u2032e is then integrated into the neural machine translation\u2019s decoder."}, {"heading": "3.3 DECODER", "text": "This section links to the orange part of Figure 1. Given the source sentence x and the latent variable z, decoder defines the probability over translation y as\np(y|z,x) = T\u220f j=1 p(yj |y<j , z,x). (9)\nHow we define the probability over translation y is fundamentally the same as VNMT, except for using conditional GRU instead of GRU. Conditional GRU involves two GRUs and an attention mechanism. We integrate a latent variable z into the second GRU. We describe it in the appendix."}, {"heading": "3.4 MODEL TRAINING", "text": "Monte Carlo sampling method is used to approximate the expectation over the posterior Eq. (2), Eq\u03c6(z|x,y,\u03c0) \u2248 1L \u2211L l=1 log p\u03b8(y|x,h (l) z ), where L is the number of samplings. The training objective is defined as\nL(\u03b8, \u03c6) = \u2212DKL [q\u03c6(z|x,y,\u03c0)||p\u03b8(z|x)] + 1\nL L\u2211 l=1 T\u2211 j=1 log p\u03b8(yj |y<j ,x,h(l)z ), (10)\nwhere hz = \u00b5+ \u03c3 \u00b7 , \u223c N (0, I). The first term, KL divergence, can be computed analytically and is differentiable because both distributions are Gaussian. The second term is also differentiable. We set L as 1. Overall, the objective L is differentiable. Therefore, we can optimize the parameter \u03b8 and variational parameter \u03c6 using gradient ascent techniques."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 EXPERIMENTAL SETUP", "text": "We used Multi30k (Elliott et al., 2016) as the dataset. Multi30k have an English description and a German description for each corresponding image. We handle 29,000 pairs as training data, 1,014 pairs as validation data, and 1,000 pairs as test data.\nBefore training, punctuation normalization and lowercase are applied to both English and German sentences by Moses (Koehn et al., 2007) scripts2. Compound-word splitting is conducted only to German sentences using Sennrich et al. (2016)3. Then we tokenize sentences2 and use them as training data. We produce vocabulary dictionaries from training data. The vocabulary becomes 10,211 words for English and 13,180 words for German after compound-word splitting.\nImage features are extracted using VGG-19 CNN (Simonyan & Zisserman, 2014). We use 4096- dimensional fc7 features. To extract the object\u2019s region, we use Fast R-CNN (Girshick, 2015). Fast R-CNN is trained on ImageNet and MSCOCO dataset 4.\nAll weights are initialized byN (0, 0.01I). We use the adadelta algorithm as an optimization method. The hyperparameters used in the experiment are presented in the Appendix. All models are trained with early stopping. When training, VNMT is fine-tuned by NMT model and our models are finetuned using VNMT. When translating, we use beam-search. The beam-size is set as 12. Before evaluation, we restore split words to the original state and de-tokenize2 generated sentences.\nWe implemented proposed models based on dl4mt5. Actually, dl4mt is fundamentally the same model as Bahdanau et al. (2015), except that its decoder employs conditional GRU6. We implemented VNMT also with conditional GRU so small difference exists between our implementation\n2https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/{normalize-punctuation, lowercase, tokenizer, detokenizer}.perl\n3https://github.com/rsennrich/subword-nmt 4https://github.com/rbgirshick/fast-rcnn/tree/coco 5https://github.com/nyu-dl/dl4mt-tutorial 6The architecture is described at https://github.com/nyu-dl/dl4mt-tutorial/blob/master/docs/cgru.pdf\nand originally proposed VNMT which employs normal GRU as a decoder. We evaluated results based on METEOR and BLUE using MultEval 7."}, {"heading": "4.2 RESULT", "text": "Table 1 presents experiment results. It shows that our models outperforms the baseline in both METEOR and BLEU. Figure 4 shows the plot of METEOR score of baselines and our models models in validation. Figure 5 shows the plot of METEOR score and the source sentence length."}, {"heading": "4.3 QUANTITATIVE ANALYSIS", "text": "Table 1 shows that G scores the best in proposed models. In G, we simply put the feature of the original image. Actually, proposed model does not benefit from R-CNN, presumably because we can not handle sequences of image features very well. For example, G+O-AVG uses the average of multiple image features, but it only makes the original image information unnecessarily confusing.\nFigure 4 shows that G and G+O-AVG outperforms VNMT almost every time, but all model scores increase suddenly in the 17,000 iteration validation. We have no explanation for this behavior. Figure 4 also shows that G and G+O-AVG scores fluctuate more moderately than others. We state that G and G+O-AVG gain stability by adding image information. When one observes the difference between the test score and the validation score for each model, baseline scores decrease more than proposed model scores. Especially, the G score increases in the test, simply because proposed models produce a better METEOR score on average, as shown in Figure 4.\nFigure 5 shows that G and G+O-AVG make more improvements on baselines in short sentences than in long sentences, presumably because q\u03c6(z|x,y,\u03c0) can model z well when a sentence is short. Image features always have the same dimension, but underlying semantics of the image and text differ. We infer that when the sentence is short, image feature representation can afford to approximate the underlying semantic, but when a sentence is long, image feature representation can not approximate the underlying semantic.\nMulti30k easily becomes overfitted, as shown in Figure 8 and 9 in the appendix. This is presumably because 1) Multi30k is the descriptions of image, making the sentences short and simple, and 2) Multi30k has 29,000 sentences, which could be insufficient. In the appendix, we show how the parameter setting affects the score. One can see that decay-c has a strong effect. Huang et al. (2016) states that their proposed model outperforms the baseline (NMT), but we do not have that observation. It can be assumed that their baseline parameters are not well tuned."}, {"heading": "4.4 QUALITATIVE ANALYSIS", "text": "We presented the top 30 sentences, which make the largest METEOR score difference between G and VNMT, to native speakers of German and get the overall comments. They were not informed of\n7https://github.com/jhclark/multeval, we use meteor1.5 instead of meteor1.4, which is the default of MultEval.\nour model training with image in addition to text. These comments are summarized into two general remarks. One is that G translates the meaning of the source material more accurately than VNMT. The other is that our model has more grammatical errors as prepositions\u2019 mistakes or missing verbs compared to VNMT. We assume these two remarks are reasonable because G is trained with images which mainly have a representation of noun rather than verb, therefore can capture the meaning of materials in sentence.\nFigure 6 presents the translation results and the corresponding image which G translates more accurately than VNMT in METEOR. Figure 7 presents the translation results and the corresponding image which G translates less accurately than VNMT in METEOR. Again, we note that our model does not use image during translating. In Figure 6, G translates \u201da white and black dog\u201d correctly while VNMT translates it incorrectly implying \u201da white dog and a black dog\u201d. We assume that G correctly translates the source because G captures the meaning of material in the source. In Figure 7, G incorrectly translates the source. Its translation result is missing the preposition meaning \u201dat\u201d, which is hardly represented in image.We present more translation examples in appendix."}, {"heading": "5 CONCLUSION", "text": "As described herein, we proposed the neural machine translation model that explicitly has a latent variable that includes underlying semantics extracted from both text and images. Our model outperforms the baseline in both METEOR and BLEU scores. Experiments and analysis present that our model can generate more accurate translation for short sentences. In qualitative analysis, we present that our model can translate nouns accurately while our model make grammatical errors."}, {"heading": "A DERIVATION OF LOWER BOUNDS", "text": "The lower bound of our model can be derived as follows: p(y|x) = \u222b p(y, z|x)dz\n= \u222b p(z|x)p(y|z,x)dz\nlog p(y|x) = log \u222b q(z|x,y,\u03c0)p(z|x)p(y|z,x)\nq(z|x,y,\u03c0) dz \u2265 \u222b q(z|x,y,\u03c0) log p(z|x)p(y|z,x)\nq(z|x,y,\u03c0) dz\n= \u222b q(z|x,y,\u03c0) ( log\np(z|x) q(z|x,y)\n+ log p(y|z,x) ) dz\n= \u2212DKL [q(z|x,y,\u03c0)||p(z|x)] + Eq(z|x,y,\u03c0) [log p(y|z,x)] = L"}, {"heading": "B CONDITIONAL GRU", "text": "Conditional GRU is implemented in dl4mt. Caglayan et al. (2016) extends Conditional GRU to make it capable of receiving image information as input. The first GRU computes intermediate representation s\u2032j as\ns\u2032j = (1\u2212 o\u2032j) s\u2032j + o\u2032j sj\u22121 (11) s\u2032j = tanh(W \u2032E [yj\u22121] + r \u2032 j (U \u2032sj\u22121)) (12)\nr\u2032j = \u03c3(W \u2032 rE [yj\u22121] + U \u2032 rsj\u22121) (13) o\u2032j = \u03c3(W \u2032 oE [yj\u22121] + U \u2032 osj\u22121) (14)\nwhere E \u2208 Rdemb\u00d7dt signifies the target word embedding, s\u2032j \u2208 Rdh denotes the hidden state, r\u2032j \u2208 Rdh and o\u2032j \u2208 Rdh respectively represent the reset and update gate activations. dt stands for the dimension of target; the unique number of target words. [W \u2032,W \u2032r,W \u2032 o] \u2208 Rdh\u00d7demb , [U \u2032, U \u2032r, U \u2032o] \u2208 Rdh\u00d7dh are the parameters to be learned.\nContext vector cj is obtained as\ncj = tanh  Tf\u2211 i=1 \u03b1ijhi  (15) \u03b1ij =\nexp(eij)\u2211Tf k=1 exp(ekj)\n(16)\neij = Uatttanh(Wcatthi +Watts \u2032 j) (17)\nwhere [Uatt,Wcatt,Watt] \u2208 Rdh\u00d7dh are the parameters to be learned. The second GRU computes sj from s\u2032j , cj and h \u2032 e as\nsj = (1\u2212 o\u2032j) sj + oj s\u2032j (18) sj = tanh(Wcj + rj (Us\u2032j) + V h\u2032e) (19)\nrj = \u03c3(Wrcj + Urs \u2032 j + Vrh \u2032 e) (20) oj = \u03c3(Wocj + Uos \u2032 j + Voh \u2032 e) (21)\nwhere sj \u2208 Rdh stands for the hidden state, rj \u2208 Rdh and oj \u2208 Rdh are the reset and update gate activations. [W,Wr,Wo] \u2208 Rdh\u00d7dh , [U,Ur, Uo] \u2208 Rdh\u00d7dh , [V, Vr, Vo] \u2208 Rdh\u00d7dz are the\nparameters to be learned. We introduce h\u2032e obtained from a latent variable here so that a latent variable can affect the representation sj through GRU units.\nFinally, the probability of y is computed as\nuj = Lutanh(E [yj\u22121] + Lssj + Lxcj) (22) P (yj |yj\u22121, sj , cj) = Softmax(uj) (23)\nwhere Lu \u2208 Rdt\u00d7demb , Ls \u2208 Rdemb\u00d7dh and Lc \u2208 Rdemb\u00d7dh are the parameters to be learned."}, {"heading": "C TRAINING DETAIL", "text": "C.1 HYPERPARAMETERS\nTable 2 presents parameters that we use in the experiments.\nWe found that Multi30k dataset is easy to overfit. Figure 8 and Figure 9 present training cost and validation METEOR score graph of the two experimental settings of the NMT model. Table 3 presents the hyperparameters which were used in the experiments. Large decay-c ans small batchsize give the better METEOR scores in the end. Training is stopped if there is no validation cost improvements over the last 10 validations.\n0 10000 20000 30000 40000 50000 60000 70000 iteration\n0\n20\n40\n60\n80\n100\nco st\nTraining cost\n1 2\nFigure 8: NMT Training Cost 0 10 20 30 40 50 60 70 iteration (x 1000)\n0\n10\n20\n30\n40\n50\n60\nM E T E O R\nValidation METEOR\n1 2\nFigure 9: NMT Validation METEOR score\nFigure 10 presents the English word length histogram of the Multi30k test dataset. Most sentences in the Multi30k are less than 20 words. We assume that this is one of the reasons why Multi30k is easy to overfit.\nC.2 COST GRAPH\nFigure 11 and 12 present the training cost and validation cost graph of each models. Please note that VNMT fine-tuned NMT, and other models fine-tuned VNMT.\nC.3 TRANSLATION EXAMPLES\nWe present some selected translations from VNMT and our proposed model (G). As of translation 3 to 5 our model give the better METEOR scores than VNMT and as of translation 6 to 8 VNMT give the better METEOR scores than our models."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Perceptual symbol Systems", "author": ["Lawrence W. Barsalou"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "Barsalou.,? \\Q1999\\E", "shortCiteRegEx": "Barsalou.", "year": 1999}, {"title": "Does Multimodality Help Human and Machine for Translation and Image Captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares", "Lo\u0131\u0308c Barrault", "Joost van de Weijer"], "venue": "WMT,", "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "DCU-UvA Multimodal MT System Report", "author": ["Iacer Calixto", "Desmond Elliott", "Stella Frank"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Calixto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Calixto et al\\.", "year": 2016}, {"title": "Meteor Universal: Language Specific Translation Evaluation for Any Target Language", "author": ["Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation,", "citeRegEx": "Denkowski and Lavie.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2014}, {"title": "Multilingual Image Description with Neural Sequence Models", "author": ["D. Elliott", "S. Frank", "E. Hasler"], "venue": "ArXiv e-prints,", "citeRegEx": "Elliott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Fast R-CNN", "author": ["Ross Girshick"], "venue": "In ICCV,", "citeRegEx": "Girshick.,? \\Q2015\\E", "shortCiteRegEx": "Girshick.", "year": 2015}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Multimodal Pivots for Image Caption Translation", "author": ["Julian Hitschler", "Stefan Riezler"], "venue": "arXiv preprint arXiv:1601.03916,", "citeRegEx": "Hitschler and Riezler.,? \\Q2016\\E", "shortCiteRegEx": "Hitschler and Riezler.", "year": 2016}, {"title": "Attention-based Multimodal Neural Machine Translation", "author": ["Po-Yao Huang", "Frederick Liu", "Sz-Rung Shiang", "Jean Oh", "Chris Dyer"], "venue": "In WMT,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Semi-supervised Learning with Deep Generative Models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Open Source Toolkit for Statistical Machine Translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": "In ACL,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "CUNI System for WMT16 Automatic Post-Editing and Multimodal Translation Tasks", "author": ["Jind\u0159ich Libovick\u00fd", "Jind\u0159ich Helcl", "Marek Tlust\u00fd", "Ond\u0159ej Bojar", "Pavel Pecina"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Libovick\u00fd et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Libovick\u00fd et al\\.", "year": 2016}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Generative Adversarial Text to Image Synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee"], "venue": null, "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models", "author": ["Danilo J. Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Costa-juss\u00e0. WMT 2016 Multimodal Translation System Description based on Bidirectional Recurrent Neural Networks with Double-Embeddings", "author": ["Sergio Rodr\u0131\u0301guez Guasch", "Marta R"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Guasch and R.,? \\Q2016\\E", "shortCiteRegEx": "Guasch and R.", "year": 2016}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "SHEF-Multimodal: Grounding Machine Translation on Images", "author": ["Kashif Shah", "Josiah Wang", "Lucia Specia"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Shah et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A shared Task on Multimodal Machine Translation and Crosslingual Image Description", "author": ["Lucia Specia", "Stella Frank", "Khalil Simaan", "Desmond Elliott"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "Specia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2016}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling Coverage for Neural Machine Translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "In ACL,", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In CVPR,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Variational Neural Machine Translation", "author": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "venue": "In EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "CONDITIONAL GRU Conditional GRU is implemented in dl4mt. Caglayan et al. (2016) extends Conditional GRU to make it capable of receiving image information", "author": ["B L"], "venue": null, "citeRegEx": "L,? \\Q2016\\E", "shortCiteRegEx": "L", "year": 2016}], "referenceMentions": [{"referenceID": 21, "context": "Neural machine translation (NMT) has achieved great success in recent years (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 76, "endOffset": 123}, {"referenceID": 0, "context": "Neural machine translation (NMT) has achieved great success in recent years (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 76, "endOffset": 123}, {"referenceID": 0, "context": "However, the most standard model, NMT with attention (Bahdanau et al., 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al.", "startOffset": 53, "endOffset": 76}, {"referenceID": 22, "context": ", 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al., 2016).", "startOffset": 213, "endOffset": 230}, {"referenceID": 24, "context": "To overcome this problem, Variational Neural Machine Translation (VNMT), which outperforms NMT with attention introduces a latent variable to capture the underlying semantic from source and target (Zhang et al., 2016).", "startOffset": 197, "endOffset": 217}, {"referenceID": 1, "context": "For example, we human beings understand the meaning of language by linking perceptual information given by the surrounding environment and language (Barsalou, 1999).", "startOffset": 148, "endOffset": 164}, {"referenceID": 10, "context": "Our model includes an explicit latent variable z, which has underlying semantics extracted from text and images by introducing a Variational Autoencoder (VAE) (Kingma et al., 2014; Rezende et al., 2014).", "startOffset": 159, "endOffset": 202}, {"referenceID": 15, "context": "Our model includes an explicit latent variable z, which has underlying semantics extracted from text and images by introducing a Variational Autoencoder (VAE) (Kingma et al., 2014; Rezende et al., 2014).", "startOffset": 159, "endOffset": 202}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015). In contrast to statistical machine translation, which requires huge phrase and rule tables, NMT requires much less memory. However, the most standard model, NMT with attention (Bahdanau et al., 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al., 2016). To overcome this problem, Variational Neural Machine Translation (VNMT), which outperforms NMT with attention introduces a latent variable to capture the underlying semantic from source and target (Zhang et al., 2016). We follow the motivation of VNMT, which is to capture underlying semantic of a source. Image information is related to language. For example, we human beings understand the meaning of language by linking perceptual information given by the surrounding environment and language (Barsalou, 1999). Although it is natural and easy for humans, it is difficult for computers to understand different domain\u2019s information integrally. Solving this difficult task might, however, bring great improvements in natural language processing. Several researchers have attempted to link language and images such as image captioning by Xu et al. (2015) or image generation from sentences by Reed et al.", "startOffset": 8, "endOffset": 1310}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015). In contrast to statistical machine translation, which requires huge phrase and rule tables, NMT requires much less memory. However, the most standard model, NMT with attention (Bahdanau et al., 2015) entails the shortcoming that the attention mechanism cannot capture the entire meaning of a sentence because it generates a target word while depending heavily on the relevant parts of the source sentence (Tu et al., 2016). To overcome this problem, Variational Neural Machine Translation (VNMT), which outperforms NMT with attention introduces a latent variable to capture the underlying semantic from source and target (Zhang et al., 2016). We follow the motivation of VNMT, which is to capture underlying semantic of a source. Image information is related to language. For example, we human beings understand the meaning of language by linking perceptual information given by the surrounding environment and language (Barsalou, 1999). Although it is natural and easy for humans, it is difficult for computers to understand different domain\u2019s information integrally. Solving this difficult task might, however, bring great improvements in natural language processing. Several researchers have attempted to link language and images such as image captioning by Xu et al. (2015) or image generation from sentences by Reed et al. (2016). They described the possibility of integral understanding of images and text.", "startOffset": 8, "endOffset": 1367}, {"referenceID": 13, "context": "Our model outperforms the baseline with two evaluation metrics: METEOR (Denkowski & Lavie, 2014) and BLEU (Papineni et al., 2002).", "startOffset": 106, "endOffset": 129}, {"referenceID": 24, "context": "Our model is the extension of Variational Neural Machine Translation (VNMT) (Zhang et al., 2016).", "startOffset": 76, "endOffset": 96}, {"referenceID": 2, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 9, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 3, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 12, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 18, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al.", "startOffset": 45, "endOffset": 192}, {"referenceID": 20, "context": ", 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016).", "startOffset": 86, "endOffset": 107}, {"referenceID": 7, "context": "They simply put source context vectors and image feature vectors extracted from ResNet-50\u2019s \u2018res4f relu\u2019 layer (He et al., 2016) into the decoder called multimodal conditional GRU.", "startOffset": 111, "endOffset": 128}, {"referenceID": 6, "context": "They extract prominent objects from the image by Region-based Convolutional Neural Networks (R-CNN) (Girshick, 2015).", "startOffset": 100, "endOffset": 116}, {"referenceID": 3, "context": "The first papers to study multimodal translation are Elliott et al. (2015) and Hitschler & Riezler (2016).", "startOffset": 53, "endOffset": 75}, {"referenceID": 3, "context": "The first papers to study multimodal translation are Elliott et al. (2015) and Hitschler & Riezler (2016). It was selected as a shared task in Workshop of Machine Translation 2016 (WMT161).", "startOffset": 53, "endOffset": 106}, {"referenceID": 2, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016). Here, we introduce end-to-end neural network translation models like our model. Caglayan et al. (2016) integrate an image into an NMT decoder.", "startOffset": 46, "endOffset": 397}, {"referenceID": 2, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016). Here, we introduce end-to-end neural network translation models like our model. Caglayan et al. (2016) integrate an image into an NMT decoder. They simply put source context vectors and image feature vectors extracted from ResNet-50\u2019s \u2018res4f relu\u2019 layer (He et al., 2016) into the decoder called multimodal conditional GRU. They demonstrate that their method does not surpass the text-only baseline: NMT with attention. Huang et al. (2016) integrate an image into a head of source words sequence.", "startOffset": 46, "endOffset": 734}, {"referenceID": 2, "context": "Although several studies have been conducted (Caglayan et al., 2016; Huang et al., 2016; Calixto et al., 2016; Libovick\u00fd et al., 2016; Rodr\u0131\u0301guez Guasch & Costa-juss\u00e0, 2016; Shah et al., 2016), they do not show great improvement, especially in neural machine translation (Specia et al., 2016). Here, we introduce end-to-end neural network translation models like our model. Caglayan et al. (2016) integrate an image into an NMT decoder. They simply put source context vectors and image feature vectors extracted from ResNet-50\u2019s \u2018res4f relu\u2019 layer (He et al., 2016) into the decoder called multimodal conditional GRU. They demonstrate that their method does not surpass the text-only baseline: NMT with attention. Huang et al. (2016) integrate an image into a head of source words sequence. They extract prominent objects from the image by Region-based Convolutional Neural Networks (R-CNN) (Girshick, 2015). Objects are then converted to feature vectors by VGG-19 (Simonyan & Zisserman, 2014) and are put into a head of source words sequence. They demonstrate that object extraction by R-CNN contributes greatly to the improvement. This model achieved the highest METEOR score in NMTbased models in WMT16, which we compare to our model in the experiment. We designate this model as CMU. Caglayan et al. (2016) argue that their proposed model did not achieve improvement because they failed to benefit from both text and images.", "startOffset": 46, "endOffset": 1311}, {"referenceID": 10, "context": "3 VARIATIONAL AUTO ENCODER VAE was proposed in an earlier report of the literature Kingma et al. (2014); Rezende et al.", "startOffset": 83, "endOffset": 104}, {"referenceID": 10, "context": "3 VARIATIONAL AUTO ENCODER VAE was proposed in an earlier report of the literature Kingma et al. (2014); Rezende et al. (2014). Given an observed variable x, VAE introduces a continuous latent variable z, with the assumption that x is generated from z.", "startOffset": 83, "endOffset": 127}, {"referenceID": 0, "context": "1 TEXT ENCODING The source and target are encoded in the same way as Bahdanau et al. (2015). The source is converted to a sequence of 1-of-k vector and is embedded to demb dimensions.", "startOffset": 69, "endOffset": 92}, {"referenceID": 11, "context": "Before training, punctuation normalization and lowercase are applied to both English and German sentences by Moses (Koehn et al., 2007) scripts2.", "startOffset": 115, "endOffset": 135}, {"referenceID": 6, "context": "To extract the object\u2019s region, we use Fast R-CNN (Girshick, 2015).", "startOffset": 50, "endOffset": 66}, {"referenceID": 4, "context": "1 EXPERIMENTAL SETUP We used Multi30k (Elliott et al., 2016) as the dataset. Multi30k have an English description and a German description for each corresponding image. We handle 29,000 pairs as training data, 1,014 pairs as validation data, and 1,000 pairs as test data. Before training, punctuation normalization and lowercase are applied to both English and German sentences by Moses (Koehn et al., 2007) scripts2. Compound-word splitting is conducted only to German sentences using Sennrich et al. (2016)3.", "startOffset": 39, "endOffset": 509}, {"referenceID": 0, "context": "Actually, dl4mt is fundamentally the same model as Bahdanau et al. (2015), except that its decoder employs conditional GRU6.", "startOffset": 51, "endOffset": 74}, {"referenceID": 9, "context": "The score of the CMU is from (Huang et al., 2016).", "startOffset": 29, "endOffset": 49}, {"referenceID": 9, "context": "Huang et al. (2016) states that their proposed model outperforms the baseline (NMT), but we do not have that observation.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "Although attention-based Neural Machine Translation have achieved great success, attention-mechanism cannot capture the entire meaning of the source sentence because the attention mechanism generates a target word depending heavily on the relevant parts of the source sentence. The report of earlier studies has introduced a latent variable to capture the entire meaning of sentence and achieved improvement on attention-based Neural Machine Translation. We follow this approach and we believe that the capturing meaning of sentence benefits from image information because human beings understand the meaning of language not only from textual information but also from perceptual information such as that gained from vision. As described herein, we propose a neural machine translation model that introduces a continuous latent variable containing an underlying semantic extracted from texts and images. Our model, which can be trained endto-end, requires image information only when training. Experiments conducted with an English\u2013German translation task show that our model outperforms over the baseline.", "creator": "LaTeX with hyperref package"}}}