{"id": "1611.08481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2016", "title": "GuessWhat?! Visual object discovery through multi-modal dialogue", "abstract": "We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.", "histories": [["v1", "Wed, 23 Nov 2016 20:56:13 GMT  (32203kb,D)", "http://arxiv.org/abs/1611.08481v1", "23 pages"], ["v2", "Mon, 6 Feb 2017 12:52:53 GMT  (32203kb,D)", "http://arxiv.org/abs/1611.08481v2", "23 pages; CVPR 2017 submission; seethis https URL"]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["harm de vries", "florian strub", "sarath chandar", "olivier pietquin", "hugo larochelle", "aaron courville"], "accepted": false, "id": "1611.08481"}, "pdf": {"name": "1611.08481.pdf", "metadata": {"source": "CRF", "title": "GuessWhat?! Visual object discovery through multi-modal dialogue", "authors": ["Harm de Vries", "Florian Strub", "Sarath Chandar", "Aaron Courville"], "emails": ["mail@harmdevries.com", "florian.strub@inria.fr", "sarathcse2008@gmail.com", "pietquin@google.com", "hlarochelle@twitter.com", "aaron.courville@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "People use natural language as the most effective way to communicate, including when it comes to describe the visual world around. They often need only a few words to refer to a specific object in a rich scene. Whenever such expressions unambiguously point to one object, we speak of a referring expression [23]. However, uniquely identifying the referred object is not always possible, as it depends on the listener\u2019s state of mind and the context of the scene. Many real life situations, therefore, require multiple exchanges before it is clear what object is referred to:\n- Did you see that dog? * You mean the one in the corner? - No, the one that\u2019s running. * Yes, what\u2019s up with that?\nA computer vision system able to hold conversations about what it sees would be an important step towards in-\ntelligent scene understanding. Such systems would be more transparent and interpretable because humans may naturally interact with them, for example by asking clarifying questions about what it perceives. Still, a fundamental challenge remains: how to create models that understand natural language descriptions and ground them in the visual world.\nThe last few years has seen an increasing interest from the computer vision community in tasks towards this goal. Thanks to advances in training deep neural networks [16] and the availability of large-scale classification datasets [26, 35, 49], automatic object recognition has now reached human-level performance [24]. As a result, attention has been shifted toward tasks involving higher-level image understanding. One prominent example is image captioning [26], the task of automatically producing natural lan-\n1\nar X\niv :1\n61 1.\n08 48\n1v 1\n[ cs\n.A I]\n2 3\nN ov\nIs it a person?\nIs it a snowboard? NoIs it the red one? Yes\nIs it a cow? Yes NoIs the cow on the left? No On the right ? Yes Is it an item being worn or held? Is it the one being held by the person in blue? Yes First cow near us? Is it the big cow in the middle? Yes Yes No\n#203974 #168019\nFigure 2: Two example games in the dataset. After a sequence of five questions we are able to locate the object (highlighted by a green mask).\nguage descriptions of an image. Visual Question Answering (VQA) [6] is another popular task that involves answering single open-ended questions concerning an image. Closer to our work, the ReferIt game [21] aims to generate a single expression that refers to one object in the image.\nOn the other hand, there has been a renewed interest in dialogue systems [31, 37], inspired by the success of datadriven approaches in other areas of natural language processing [11]. Traditionally, dialogue systems have been built through heavy engineering and hand-crafted expert knowledge, despite machine learning attempts for almost two decades [25, 40]. One of the difficulties comes from the lack of automatic evaluation as \u2013 contrary to machine translation \u2013 there is no evaluation metric that correlates well with human evaluation [27]. A promising alternative is goal-directed dialogue tasks [31, 40, 44, 43] where agents converse to pursue a goal rather than casually chit-chat. The agent\u2019s success rate in completing the task can then be used as an automatic evaluation metric. Many tasks have recently been introduced, including the bAbI tasks [44] for testing an agent\u2019s ability to answer questions about a short story, the movie dialog dataset [12] to assess an agent\u2019s capabilities regarding personal movie recommendation and a Wizardof-Oz framework [43] to evaluate an agent\u2019s performance for assisting users in finding restaurants.\nIn this paper, we bring these two fields together and propose a novel goal-directed task for multi-modal dialogue. The two-player game, called GuessWhat?!, extends the ReferIt game [21] to a dialogue setting. To succeed, both players must understand the relations between objects and how they are expressed in natural language. From a machine learning point of view, the GuessWhat?! challenge is the following: learn to acquire natural language by interaction on a visual task. Previous attempts in that direction [2, 43] do not ground natural language to their immediate environment; instead they rely on an external database through which a conversational agent searches.\nThe key contribution of this paper is the introduction of the GuessWhat?! dataset that contains 155,280 dialogues composed of 831,889 question/answer pairs on 66,537 images extracted from the MS COCO dataset [26]. We define three tasks that are based on the GuessWhat?! dataset and prototype deep learning baselines to establish their difficulty. The paper is organized as follows. First, we explain the rules of the GuessWhat?! game in Sec. 2. Then, Sec. 3 describes how GuessWhat?! relates to previous work. In Sec. 4.1 we highlight our design decisions in collecting the dataset, while Sec. 4.2 analyses many aspects of the dataset. Sec. 5 introduces the questioner and oracle tasks and their baseline models. Finally, Sec. 6 provides a final discussion of the GuessWhat?! game."}, {"heading": "2. GuessWhat?! game", "text": "GuessWhat?! is a cooperative two-player game in which both players see the picture of a rich visual scene with several objects. One player \u2013 the oracle \u2013 is randomly assigned an object (which could be a person) in the scene. This object is not known by the other player \u2013 the questioner \u2013 whose goal it is to locate the hidden object. To do so, the questioner can ask a series of yes-no questions which are answered by the oracle as shown in Fig 1 and 2. Note that the questioner is not aware of the list of objects, they can only see the whole picture. Once the questioner has gathered enough evidence to locate the object, they notify the oracle that they are ready to guess the object. We then reveal the list of objects, and if the questioner picks the right object, we consider the game successful. Otherwise, the game ends unsuccessfully. We also include a small penalty for every question to encourage the questioner to ask informative questions. Fig 8 and 9 in Appendix A display a full game from the perspective of the oracle and questioner, respectively.\nThe oracle role is a form of visual question answering where the answers are limited to Yes, No and N/A (not applicable). The N/A option is included to respond even when the question being asked is ambiguous or an answer simply cannot be determined. For instance, one cannot answer the question \u201dIs he wearing glasses?\u201d if the face of the selected person is not visible. The role of the questioner is much harder. They need to generate questions that progressively narrow down the list of possible objects. Ideally, they would like to minimize the number of questions necessary to locate the object. The optimal policy for doing so involves a binary search: eliminate half of the remaining objects with each question. Natural language is often very effective at grouping objects in an image scene. Such strategies depend on the picture, but we distinguish the following types:\nSpatial reasoning We group objects spatially within the image scene. One may use absolute spatial informa-\ntion \u2013 Is it on the bottom left of the picture? \u2013 or relative spatial location \u2013 Is it to the left of the blue car?\nVisual properties We group objects by their size \u2013 Is it big?, shape \u2013 Is it square? \u2013 or color \u2013 Is it blue?.\nObject taxonomy We can use the hierarchical structure of object categories, i.e. taxonomy, to group objects e.g. Is it a vehicle? to refer to both cars and trucks.\nInteraction We group objects by how we interact with them \u2013 Can you drive it?.\nThe goal of the GuessWhat?! task is to enable machines to understand natural descriptions and ground them into the visual world. Note that such higher-level reasoning only occurs when the scene is rich enough i.e. when there are enough objects in the scene. People otherwise tend to fall back to a linear search strategy by simply enumerating objects (often by their category names)."}, {"heading": "3. Related work", "text": "The GuessWhat?! game and the data collected from it present opportunities for the extension of current research on image captioning, visual question answering and dialogue systems. In the following, we describe previous work in these areas and relate them to the open challenges offered by GuessWhat?!. We also mention other relevant work on dataset collection. Image captioning Our work builds on top of the MS COCO dataset [26] which consists of 120k images with more than 800k object segmentations. In addition, the dataset provides 5 captions per image which initiated an explosion of interest from the research community into generating natural language descriptions of images. Several methods have been proposed [20, 42, 45], all inspired by the encoder-decoder approach [11, 41] that has proven successful for machine translation. Image captioning research uncovered successful approaches to automatically generate coherent, factual statements about images. Modeling the interactions in GuessWhat?! requires instead to model the process of asking useful questions about images. VQA datasets Visual Question Answering (VQA) tasks form another well known extension of the captioning task. They instead require answering a question given a picture (e.g. \u201dHow many zebras are there in the picture?\u201d, \u201dIs it raining outside?\u201d ). Recently, the VQA challenge [6] has provided a new dataset far bigger than previous attempts [15, 29] where, much like in GuessWhat?!, questions are free-form. An extensive body of work has followed from this publication, largely building on the image captioning literature [3, 28, 39, 46]. Unfortunately, many of these advanced methods were shown to marginally improve on simple baselines [19]. Recent work [3] also reports that trained\nmodels often report the same answer to a question irrespective of the image, suggesting that they largely exploit predictive correlations between questions and answers present in the dataset. The GuessWhat?! game and dataset attempt to circumvent these issues. Because of the questioner\u2019s aim to locate the hidden object, the generated questions are different in nature: they naturally favour spatial understanding of the scene and the attributes of the objects within it, making it more valuable to consult the image. Besides, it only contains binary questions, whose answers we find to be balanced and has twice more questions on average per picture. Goal-directed dialogue GuessWhat?! is also relevant to the goal-directed dialogue research community. Such systems are aimed at collaboratively achieving a goal with a user, such as retrieving information or solving a problem. Although goal-directed dialogue systems are appealing, they remain hard to design. Thus, they are usually restricted to specific domains such as train ticket sales, tourist information or call routing [32, 40, 47]. Besides, existing dialogue datasets are either limited to fewer than 100k example dialogues [12], unless they are generated with template formats [12, 43, 44] or simulation [33, 36] in which case they don\u2019t reflect the free-form of natural conversations. Finally, recent work on end-to-end dialogue systems fail to handle dynamical contexts. For instance, [43] intersects a dialogue with an external database to recommend restaurants. Well-known game-based dialogue systems [1, 2] also rely on static databases. In contrast, GuessWhat?! dialogues are heavily grounded by the images. The resulting dialogue is highly contextual and must be based on the content of the current picture rather than an external database. Thus, to the best of our knowledge, the GuessWhat?! dataset marks an important step for dialogue research, as it is the first large scale dataset that is both goaloriented and multi-modal. Human computation games GuessWhat?! is in line with Von Ahn\u2019s seminal work on human computation games [4, 5] who showed that games are an effective way to gather labeled data. The first ESP game [4] was developed to collect image tags, and was later extended to Peekaboom [5] to gather object segmentations. These games were developed more than a decade ago, when object recognition was in its infancy and served a different purpose than GuessWhat?!. ReferIt Probably closest to our work is the ReferIt game [21, 30, 48]. In this game, one player observes an annotated object in a scene, for which they need to generate an expression that refers to it (e.g. t\u0308he man wearing the white t-shirt\u00a8). The other player then receives this expression and subsequently clicks on the location of the object within the image. The original dataset [21] uses the IMAGEClef dataset [13], while three recent extensions [30, 48] were built on top of MS COCO. All three databases select images with only 2 \u2212 4 objects of the same category. In contrast,\nGuessWhat?! picks images with 3\u221220 objects without further restrictions on the object class, and thus contains three times more images than the ReferIt datasets. To further investigate the difference between ReferIt and GuessWhat?!, we compare three samples for the same selected object in Fig 14 in Appendix B. While ReferIt directly locates the object with a single expression, GuessWhat?! iteratively narrows down the object by means of positive and negative feedback on questions. We also observe that GuessWhat?! dialogues favor more abstract concepts, such as \u201dIs it edible?\u201d or \u201dIs it on oval plate?\u201d than ReferIt."}, {"heading": "4. GuessWhat?! Dataset", "text": ""}, {"heading": "4.1. Data collection", "text": "Images We use a subset of the training and validation images and objects of the MS COCO dataset [26]. We first discard objects that are too small (area < 500px2) to be decently located by a human observer. Then, we only keep images containing three to twenty objects, to avoid trivial or overly complicated images. In total, we keep 77,973 images with 609,543 objects. We verified that this selection does not significantly alter the original dataset distribution.\nAmazon Mechanical Turk The data collection was crowd-sourced on Amazon Mechanical Turk (AMT) [9]. We created two separate tasks \u2013 known as HITs on AMT \u2013 for the questioner and oracle roles, and rewarded the questioner slightly more than the oracle. We ensured the quality of the data collection by several means. First, the workers had to go through a qualification round which consisted of successfully completing 10 games while producing fewer than 4 mistakes or disconnects. After qualification, HITs continue to consist of a batch of 10 successful games. We incentivize the worker to produce as many successful dialogues in a row by providing bonuses for making fewer mistakes. Secondly, players could report on each other and players were banned after a certain number of reports. Thus, players were incentivized to cooperate. In the end, we only kept dialogues from qualified people and successful dialogues from the qualification round. In contrast to traditional dataset collection, our game requires an interactive session between two players. Fortunately, we found that the GuessWhat?! game was highly engaging. A total of more than 10K people participated in our HITs, and our top ten participants played over 2, 000 games each. Since questions were manually typed, they could contain spelling mistakes. Thus, we retrieved all questions containing words that do not occur in an English dictionary and manually corrected the 1000 most common words. For the remaining 30k questions, we created two HITs that to correct the spelling mistakes. See Figure 10 in Appendix A for further details."}, {"heading": "4.2. Data analysis", "text": "In the following, we explore properties of the data we collected using the GuessWhat?! game. We provide global statistics, examine the vocabulary used by the questioners and highlight the relationship between properties of objects to guess and the odds of having a successful dialogue.\nDataset statistics The raw GuessWhat?! dataset is composed of 155,281 dialogues containing 821,955 question/answer pairs on 66,537 unique images and 134,074 unique objects. The answers are respectively 52.2% no, 45.6% yes and 2.2% N/A. On average, there are 5.2 questions per dialogue and 2.3 dialogues per image. The dialogues contain 3,986,192 word tokens in total, making up 11,465 different words with at least one occurrence and 5,444 words with at least 3 occurrences. Moreover, 84.6% of the dialogues are successful, 8.4% are unsuccessful and 7.0% are not completed (disconnection, timeout etc.). Thus, different subsets co-exist in the GuessWhat?! dataset, we will refer to the dataset as full, finished and successful when we include all the dialogues, all finished dialogues (successful and unsuccessful) or only successful dialogues, respectively. For more details, the previous statistics are broken down into dataset types in Tab 1.\nQuestion distributions To get a better understanding of the GuessWhat?! games, we show the number of questions within a dialogue and the average number of questions given the number of objects within a image in Fig 3. First, the number of questions within a dialogue decreases exponentially, as players tend to shorten their dialogues to speed up the game (and therefore maximize their gains). More interestingly, we observe that the average number of questions given the number of objects within a image appears to follow a function that grows at a rate between logarithmically and linearly. A questioning strategy of simply listing objects (e.g. \u201dis it the chair\u201d, etc.) would imply linear growth in the number of questions, while the optimal binary search strategy would imply logarithmic growth. Thus the human questioners seem to imply a strategy that is somewhere in between. We conjecture three reasons why humans do not achieve the optimal search strategy. First, the questioner does not have access to the ground truth list of objects in the picture, and might, therefore, overestimate the number of objects. Second, some humans tend to favor a linear search\nstrategy. Finally, the questioner may ask additional questions to confirm that he has located the right object. This can be important in the presence of possible oracles errors.\nVocabulary To gain insight into the vocabulary used by the questioner, we compute the frequency of words in the GuessWhat?! corpus and display the most frequent words as a word cloud in Fig 3c. Several key words clearly stand out. As explained in Sec. 2, some of those key words refer to abstract object properties such as person or object, spatial locations such as right/left or side and visual features such as red/black/white. Furthermore, prepositions are also heavily used to express relationships between objects. To better understand the sequential aspect of the questions, we study the evolution of the vocabulary at each question round. We observe that questioners use abstract object properties such as human/object/furniture only at the beginning of the dialogues, and quickly switch to either spatial or visual terms such as left/right, white/red or table,chair. This can be highlighted by applying a Dynamic Topic Model [8] to study the evolution of topics over the course of the dialogue as shown in Fig 19 in Appendix C.\nElements of success To investigate whether certain object properties favour success, we compute the success ratio of dialogues relative to: the size of the unknown objects in Fig 4a, the number of objects within the images in Fig 4b, the object category, the location of objects within the images and the size of the dialogues in Fig 20, Fig 21a and Fig 21b in Appendix C, respectively. As one may expect, the more complex the scene is, the lower the success rate is. When there are only 3 objects, the questioner has 95% success rate, while this ratio drops to around 70% with 20 objects. Similarly, big objects are almost always found while the smallest one are only found 60% of the time. Questioners easily find objects in the middle of the picture but have more difficulties to find them on the border. Finally, objects from categories that are often grouped together, e.g. bananas or books, have a lower success rates.\nMiscellaneous In Fig 4c we break down the ratio of yesno answers within the dialogues. While the first yes-no answers are balanced for small dialogues, they often terminate with a final yes. In contrast, long dialogues often start with a higher proportion of negative answers which slowly decrease during the exchange."}, {"heading": "4.3. Dataset release", "text": "We split the GuessWhat?! dataset by randomly assigning 70%, 15% and 15% of the images and its corresponding dialogues to the training, validation and test set. This way of dividing the data ensures that we evaluate performance on images not seen during training. The GuessWhat?! dataset will be released on January 28, 20171."}, {"heading": "5. Baselines", "text": "We now empirically investigate the difficulty of the oracle and questioner tasks. To do so, we trained reasonable baselines for each task and measured their performance.\nFormally, a GuessWhat?! game revolves around an image I \u2208 RM\u00d7N containing a set of K segmented objects {O1, . . . , OK}. Each object Ok is assigned an object category ck \u2208 {1, . . . , C} and has a pixel-wise segmentation mask Sk \u2208 {0, 1}M\u00d7N to specify its location and size. The game further consists of a sequence of questions and answers D = {q1, a1, . . . , qJ , aJ}, produced by the questioner and oracle. We will use q<j and a<j to refer to the first j \u2212 1 questions and answers, respectively. Each question qj contains a sequence of Nj tokens, i.e. qj = {wj1, . . . , wjNj}, where wji is taken from a vocabulary V and represents the token at position i in question j. Each answer is either Yes, No or N/A, i.e. aj \u2208 {Yes, No, N/A}. Finally, the oracle has access to the identity of the correct object Ocorrect, and the prediction of the questioner will be denoted as Opredict."}, {"heading": "5.1. Oracle baselines", "text": "The oracle task requires to produce a yes-no answer for any object within a picture given a natural language question. We first introduce our model and then outline its results to get a better understanding of the GuessWhat?! dataset. Model We propose a simple neural network based approach to this model, illustrated in Fig 5. Specifically, we use an appropriate neural network architecture to embed each of the following information: the image I , the cropped object from S, its spatial information, its category c and the current question q. These embeddings are then concatenated as a single vector and fed as input to a single hidden layer MLP that outputs the final answer distribution using a softmax layer. Finally, we minimize the cross-entropy error during the training and report the classification error at evaluation time.\n1We will update the paper to specify the URL.\nThe details on how we compute the embeddings are as follows. To embed the full image, it is rescaled to a 224 by 224 image and is passed through a pre-trained VGG network to obtain its FC8 features. As for the selected object, it is first cropped by finding the smallest rectangle that encapsulates it, based on its segmentation mask. We then rescale the crop to a 224 by 224 square, before obtaining its FC8 features from the pre-trained VGG network. Although we could use the mask to drop out pixels around the selected object, we keep the crop as is since pre-trained VGG networks are exposed to such background noise during their training. We also embed the spatial information of the crop, to help locate the cropped object within the whole image. To do so, we follow the approach of [18, 48] and extract an 8-dimensional vector of the location of the bounding box:\nxspatial = [xmin, ymin, xmax, ymax,\nxcenter, ycenter, wbox, hbox] (1)\nwhere wbox and hbox denote the width and height of the bounding box, respectively. We normalize the image height and width such that coordinates range from \u22121 to 1, and place the origin at the center of the image. As for the object category, we convert its one-hot class vector into a dense category embedding using a learned look-up table. Finally, the embedding of the current natural language question q is computed using an Long Short-Term Memory (LSTM) network [17] where questions are first tokenized by using the word punct tokenizer from the python nltk toolkit [7]. For simplicity, we decided to ignore the question-answer pairs history q<t in our oracle baseline. Training setting We train all oracle models on the full dataset. During training, we keep the parameters of the VGG network fixed, and optimize the LSTM, object category/word look-up tables and MLP parameters by minimizing the negative log-likelihood of the correct answer. We use ADAM [22] for optimization and train for at most 15 epochs. We use early stopping on the validation set, and report the train, valid and test error. Results We report results for several oracle models using a different set of inputs in Table 2. We name the\nmodel after the input we feed to it. For instance, (Question+Category+Spatial+Image) refers to the network fed with the question q, the object category c, the spatial features xspatial and the full image I . The results of all subsets are reported in Table 6 in Appendix C.\nBecause the GuessWhat?! dataset is fairly balanced, simply outputting the most common answer in the training set \u2013 No \u2013 results in a high 50.8% error rate. Solely providing the image or crop features barely improves upon this result. Only using the question slightly improves the error rate to 41.2%. We speculate that this small bias comes from questioners that refer to objects that are never segmented or overrepresented categories. As hoped, we observe that the error rate significantly drops (< 31%) when we finally feed information on the object to guess (crop, spatial or category) to the model. We find that crop and category information are redundant: the (Question+Category) and (Question+Crop) model achieve respectively 29.2% and 25.7% error, while the combined model (Question+Category+Crop) achieves 24.7%. In general, we expect the object crop to contain additional information, such as color information, beside the object class. However, we find that the object category outperforms the object crop embedding. This might be partly due to the imperfect feature extraction from the crops. Finally, our best performing model combines object category and its spatial features along with the question."}, {"heading": "5.2. Questioner baselines", "text": "Given an image, the questioner must ask a series of questions and guess the correct object. We separate the questioner task into two different sub-tasks that are trained independently:\nGuesser Given an image I and a sequence of questions and answers DJ , predict the correct object Ocorrect from the\nset of all objects O.\nQuestion Generator Given an image I and a sequence of T questions and answers D\u2264T , produce a new question qT+1.\nIn general, one also needs a module to determine when to start guessing the object (and stop asking questions). In our baseline, we bypass this issue by fixing the number of questions to 5 for the question generator model.\nGuesser The role of the guesser model is to predict the correct object. To do so, the guesser has access to the image, the dialogue and the list of objects in the image. We encode the image by extracting its FC8 features from VGG16 network. A dialogue of a GuessWhat?! game is a sequence on two different levels: there is a variable number of question-answer pairs where each question in turn consists of a variable-length sequence of tokens. This can be encoded into a fixed size vector by using either an LSTM encoder [17] or an HRED encoder [38]. While the LSTM encoder considers the dialogue as one flat sequence, HRED explicitly models the hierarchy by two different Recurrent Neural Networks (RNN). First, an encoder RNN creates a fixed-size representation of a question or answer by reading in its tokens and taking the last hidden state of the RNN. This representation is then processed by the context RNN to obtain a representation of the current dialogue state. For both models, we concatenate the image and dialogue features and do a dot-product with the embedding for all the objects in the image, followed by a softmax to obtain a prediction distribution over the objects. Given the best performance of the \u201dQuestion+Category+Spat\u201d oracle model, we\nrepresent objects by their category and their spatial features. More precisely, we concatenate the 8-dimensional spatial representation (see Eq. 1) and the object category look-up and pass it through an MLP layer to get an embedding for the object. Note that the MLP parameters are shared to handle the variable number of objects in the image. See Fig 6 for an overview of the guesser with HRED and LSTM.\nTable 3 reports the results for the guesser baselines using human-generated dialogues. As a first baseline, we report the performance of a random guesser which does not use the dialogue information. We split the guesser results based on whether they use the VGG features or not. In general, we find that including VGG features does not improve the performance of the LSTM and HRED models. We hypothesize that the VGG features are a too coarse representation of the image scene, and that most of the visual information is already encoded in the question and the object features. Surprisingly, we find LSTMs to perform slightly better than the sophisticated HRED models.\nQuestion Generator The question generation task is hard for several reasons. First, it requires high-level visual understanding to ask meaningful questions. Second, the generator should be able to handle long-term context to ask a sequence of relevant questions, which is one of the most challenging problems in dialogue systems. Additionally, we evaluate the question generator using the imperfect oracle and imperfect guesser, which introduces compounding errors.\nHierarchical recurrent encoder decoder (HRED) [38] is the current state of the art method for natural language generation tasks. We extend this model by conditioning on the\nVGG features of the image as illustrated in Fig 7. Finally, we train our proposed model by maximizing the conditional log-likelihood:\nlogP (Q|A, I) = log J\u220f\nj=1\nP (qj |q<j , a<j , I) (2)\n= log J\u220f j=1 Nj\u220f i=1 P (wji|wj<i, a\u2264j , I) (3)\nwith respect to the described parameters. At test time, we use a beam-search to approximately find the most probable question qj . Evaluating the questioner model requires a pretrained oracle and a pre-trained guesser model. We use our questioner model to first generate a question which is then answered by the oracle model. We repeat this procedure 5 times to obtain a dialogue. We then use the best performing guesser model to predict the object and report its error as the metric for the QGEN model. Since we use ground truth answers during the QGEN training while we use oracle answers at test time, there is a mismatch between the training and testing procedure. This can be avoided by using the oracle answers also during training time. We call these models QGEN+GT and QGEN+ORACLE respectively.\nTable 4 shows the results. A guesser based on human generated dialogues achieves 38.7% error. The Question Generator models achieve reasonable performance which lies in between the random performance and the performance of the guesser on human dialogues. We observe that using the Oracle\u2019s answers while training the Question Generator introduces additional errors which significantly deteriorates performance. Some example dialogues generated by the QGen+GT model are shown in Fig. 22 and 23."}, {"heading": "6. Discussion", "text": "We introduced the GuessWhat?! game, a novel framework for multi-modal dialogue. To the best of our knowledge, we present the first large-scale dataset involving images and dialogue. A wide range of challenges may arise from this union as they rely on different fields of machine learning such as natural language understanding, generative models or computer vision. GuessWhat?! turns out to be an engaging game that greatly decreases the cost for collec-\ntion of a big dataset required for modern algorithms. As a second contribution, we introduced three tasks based on the questioner and oracle role. In each case, we prototyped a neural architecture as a first baseline. We analyzed these results and presented a quantitative description of the GuessWhat?! dataset.\nWe believe GuessWhat?! could allow for a myriad of other applications that may either be based on the game itself or extending the database to other tasks. For instance, it can be interesting to compute a confidence interval before proceeding to the final guess. Differently, GuessWhat?! could be a test bed for one-shot learning [14] of guessing new object categories, transfer learning on line-drawing images [10] or using questions from another language. Thus, GuessWhat?! dataset offers a opportunity to develop original machine learning tasks upon it. Acknowledgement The authors would like to acknowledge the stimulating environment provided by the MILA and SequeL labs. We thank all members of the MILA lab who participated in a trial run of the data collection, and all workers of AMT who participated in our HITs. We thank Jake Snell, Mengye Ren, Laurent Dinh, Jeremie Mary and Bilal Piot for helpful discussions. We acknowledge the following agencies for research funding and computing support: NSERC, Calcul Que\u0301bec, Compute Canada, the Canada Research Chairs and CIFAR, CHISTERA IGLU and CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020. SC is supported by a FQRNT-PBEEE scholarship."}, {"heading": "A. User interface", "text": "Figure 8, 9 presents the instructions for the oracle and questioner before they started their first game.\nB. GuessWhat?! samples\nIs it human?\nIs it leftmost? Is it in the middle?\nIs it an ear? Is it a cow? No\nIs it the one at the back ? Yes\nAre they five humans visible?\nIs it the third one from the left? Yes Does it have a horn?\nIs it a horn?\nYes\nYes Yes No\nYes\n#153521\nNo\nNo\nIs it a person? Is it in the hands of these girls? Is it a cat?\nYes\nYes No\nThe cat in the right side of the image?\nNo\n#23580 #16867"}, {"heading": "C. Additional database statistics", "text": "Figure 15 presents a word co-occurrence matrix of the GuessWhat?! dataset. Figure 16 and Figure 17a compares the object size and category distribution of GuessWhat?! with MS Coco.\npe rs\non 14\n.4 8\npe rs\non 3.\n20 =\nle ft\n2. 95 \u2197 fo od 1. 29 le ft 1. 69 ri gh t 2. 32 \u2197 an im al 1. 16 w ea ri ng 1. 20 ne w pe rs on 2. 28 \u2198 hu m an 1. 03 ri gh t 1. 02 ne w w ea ri ng 1. 66 \u2198 ob je ct 0. 77 fr on t 0. 97 ne w w ho le 1. 58\nne w\nca r\n0. 60\nw hi\nte 0.\n91 ne\nw w\nhi te\n1. 56 = ve hi cl e 0. 57 re d 0. 77 ne w re d 1. 26 = ca t 0. 41 ca r 0. 64 \u2198 bl ac k 1. 19 \u2197 al iv e 0. 37 bl ac k 0. 60 ne w fr on t 1. 14 \u2198 do g 0. 35 bl ue 0. 59 ne w bl ue 1. 10 =\n(a )D\nia lo\ngu es\nha vi\nng 3\nqu es\ntio ns\npe rs\non 8.\n20 pe\nrs on\n1. 98\n= le\nft 1.\n69 \u2197\nle ft\n1. 92\n= le\nft 2.\n13 =\nfo od\n1. 03\nle ft\n1. 03\n\u2197 pe\nrs on\n1. 41\n\u2198 ri\ngh t\n1. 77\n\u2197 ri\ngh t\n2. 04 = hu m an 0. 56 ri gh t 0. 66 ne w ri gh t 1. 26 = pe rs on 1. 20 \u2198 w hi te 1. 28 \u2197 an im al 0. 46 fr on t 0. 59 ne w w hi te 0. 84 \u2197 w hi te 1. 12 = pe rs on 1. 26 \u2198 ve hi cl e 0. 45 ca r 0. 51 \u2197 w ea ri ng 0. 82 \u2197 w ea ri ng 0. 93 = bl ac k 0. 90 \u2197 ob je ct 0. 42 w hi te 0. 48 ne w si de 0. 67 \u2197 bl ac k 0. 79 \u2197 w ea ri ng 0. 85 \u2198 ca r 0. 36 w ea ri ng 0. 48 ne w re d 0. 62 \u2197 re d 0. 72 = re d 0. 80 = fu rn itu re 0. 24 si de 0. 43 ne w fr on t 0. 58 \u2198 si de 0. 69 \u2198 w ho le 0. 80 ne w le ft 0. 24 re d 0. 39 ne w bl ac k 0. 55 ne w bl ue 0. 65 \u2197 bl ue 0. 75 = ed ib le 0. 20 ve hi cl e 0. 39 \u2198 bl ue 0. 54 ne w fr on t 0. 58 \u2198 fr on t 0. 73 =\n(b )D\nia lo\ngu es\nha vi\nng 5\nqu es\ntio ns\npe rs\non 5.\n89 pe\nrs on\n1. 44\n= le\nft 1.\n08 \u2197\nle ft\n1. 26\n= le\nft 1.\n33 =\nle ft\n1. 42\n= le\nft 1.\n65 =\nfo od\n0. 74\nle ft\n0. 73\n\u2197 pe\nrs on\n0. 96\n\u2198 ri\ngh t\n1. 08\n\u2197 ri\ngh t\n1. 22\n= ri\ngh t\n1. 39\n= ri\ngh t\n1. 54 = hu m an 0. 38 ri gh t 0. 42 ne w ri gh t 0. 89 = pe rs on 0. 82 \u2198 w hi te 0. 81 \u2197 w hi te 0. 88 = w hi te 0. 96 = ve hi cl e 0. 30 ta bl e 0. 37 \u2197 si de 0. 57 \u2197 w hi te 0. 67 \u2197 pe rs on 0. 80 \u2198 pe rs on 0. 84 = pe rs on 0. 90 = ob je ct 0. 28 fr on t 0. 36 ne w w hi te 0. 50 \u2197 si de 0. 60 \u2198 w ea ri ng 0. 59 \u2197 bl ac k 0. 63 \u2197 re d 0. 65 \u2197 ca r 0. 26 fo od 0. 35 \u2198 w ea ri ng 0. 48 \u2197 w ea ri ng 0. 54 = si de 0. 57 \u2198 re d 0. 57 \u2197 bl ac k 0. 63 \u2198 an im al 0. 26 si de 0. 35 ne w re d 0. 41 ne w re d 0. 49 = re d 0. 54 = w ea ri ng 0. 56 \u2198 bl ue 0. 57 \u2197 fu rn itu re 0. 20 ca r 0. 31 \u2198 ta bl e 0. 39 \u2198 ta bl e 0. 41 = bl ac k 0. 51 \u2197 bl ue 0. 54 \u2197 w ea ri ng 0. 52 \u2198 le ft 0. 14 w ea ri ng 0. 28 ne w fr on t 0. 38 \u2198 bl ac k 0. 41 \u2197 bl ue 0. 49 \u2197 si de 0. 53 \u2198 ne xt 0. 51\nne w\nbo at\n0. 14\nso m\net hi\nng 0.\n28 ne\nw ca\nr 0.\n37 \u2198\nbl ue\n0. 37\nne w\nfr on\nt 0.\n42 \u2197\nfr on\nt 0.\n45 =\nsi de\n0. 51\n\u2198\n(c )D\nia lo\ngu es\nha vi\nng 7\nqu es\ntio ns\nTa bl\ne 5:\nPr op\nor tio\nns of\nth e\nte n\nm os\ntc om\nm on\nw or\nds fo\nre ac\nh de\npt h\nof qu\nes tio\nns fo\nrs or\nte d\nby th\ne si\nze of\nth e\ndi al\nog ue\ns"}, {"heading": "D. All oracle baselines", "text": "E. Samples QGen+GT"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We introduce GuessWhat?!, a two-player guessing game<lb>as a testbed for research on the interplay of computer vision<lb>and dialogue systems. The goal of the game is to locate an<lb>unknown object in a rich image scene by asking a sequence<lb>of questions. Higher-level image understanding, like spa-<lb>tial reasoning and language grounding, is required to solve<lb>the proposed task. Our key contribution is the collection<lb>of a large-scale dataset consisting of 150K human-played<lb>games with a total of 800K visual question-answer pairs on<lb>66K images. We explain our design decisions in collecting<lb>the dataset and introduce the oracle and questioner tasks<lb>that are associated with the two players of the game. We<lb>prototyped deep learning models to establish initial base-<lb>lines of the introduced tasks.", "creator": "LaTeX with hyperref package"}}}