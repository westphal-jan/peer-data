{"id": "1611.08562", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Nov-2016", "title": "A Simple, Fast Diverse Decoding Algorithm for Neural Generation", "abstract": "In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed.", "histories": [["v1", "Fri, 25 Nov 2016 19:18:27 GMT  (268kb,D)", "http://arxiv.org/abs/1611.08562v1", null], ["v2", "Thu, 22 Dec 2016 16:31:52 GMT  (264kb,D)", "http://arxiv.org/abs/1611.08562v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "will monroe", "dan jurafsky"], "accepted": false, "id": "1611.08562"}, "pdf": {"name": "1611.08562.pdf", "metadata": {"source": "CRF", "title": "A Simple, Fast Diverse Decoding Algorithm for Neural Generation", "authors": ["Jiwei Li", "Will Monroe", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "wmonroe4@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "We further propose a variation that is capable of automatically adjusting its diversity decoding rates for different inputs using reinforcement learning (RL). We observe a further performance boost from this RL technique.1"}, {"heading": "1 Introduction", "text": "Neural generation models (Sutskever et al., 2014; 0; Cho et al., 2014; Kalchbrenner and Blunsom, 2013) are of growing interest for various applications such as machine translation (Sennrich et al., 2015b; Gulcehre et al., 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015; Luan et al., 2016), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015; Chopra et al., 2016), and image caption generation (Chen et al., 2015). Such models are trained by predicting an output sequence using the maximum-likelihood estimation (MLE) objective function at training time. At test time, the mostly likely sequence given the input assigned by the trained model is selected, usually using beam search.\n1This paper includes material from the unpublished script \u201cMutual Information and Diverse Decoding Improve Neural Machine Translation\u201d (Li and Jurafsky, 2016).\nOne serious issue with the beam search algorithm which has long been recognized (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004) is lack of diversity: candidates often differ only by punctuation or minor morphological variations, with most of the words overlapping. Lack of diversity can significantly hinder the sequence generation quality in a number of ways. First, for tasks like conversational response generation or image caption generation, there is no one truth. The decoding model needs to explore different paths to various sequences to avoid being trapped in a local minimum. Failing to do so will adversely affect the final output.2 Second, in many sequence generation tasks, the generated sequences need to be post-processed or re-ranked. This can be because of inherent flaws in the neural generation model,3 or because of the fact that many important human-developed features to date4 cannot be factorized into beam decoding. A widely-employed method is to generate a large N-best list first using beam search and then rerank the candidates based on a task-specific criterion. Lack of diversity in the N-best list significantly decreases the impact of the reranking process.5\nIn this paper, we propose a simple, fast, diversityfostering beam search model for neural decoding (the model can be obtained by changing just one\n2Vijayakumar et al. (2016) find this for image captioning. 3For instance, in neural response generation, an additional reranker is crucial for avoiding dull and generic responses (Li et al., 2015a; Sordoni et al., 2015; Shao et al., 2016).\n4e.g., position bias, Markov condition or bilingual attention symmetry features in machine translation (Cohn et al., 2016), or global discourse features in summarization.\n5For example, Shao et al. (2016) find that the re-ranking heuristics in conversational response generation works for shorter responses but hardly works at all for long responses, since the hypotheses from the beam search for long responses are mostly identical even when the beam size is large.\nar X\niv :1\n61 1.\n08 56\n2v 1\n[ cs\n.C L\n] 2\n5 N\nov 2\n01 6\nline of beam search code in MATLAB). The proposed algorithm uses standard beam search as its backbone but adds an additional intra-sibling ranking term, favoring choosing hypotheses from diverse parents (as demonstrated in Figure 1). The proposed model supports batched decoding using GPU, significantly speeding up the decoding process compared to other diversity fostering models for phrasebased MT systems (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004; Devlin and Matsoukas, 2012). The model is general, and can be easily adapted to all neural generation tasks. We evaluate the proposed algorithm in neural generation tasks such as conversational response generation, abstractive summarization and machine translation, and we demonstrate that the proposed algorithm yields more diverse generated sequences, leading to better final outputs.\nOn top of this algorithm, we further explore when diversity is needed and when it is not, and when it might even be detrimental. We then propose a model based on reinforcement learning (RL) that automatically adjusts the diversity rate for different inputs, yielding an additional performance boost.\nWe also discuss what properties of tasks make the diverse decoding strategy help more. We find that diverse decoding helps more on tasks with a more diverse space of ground truth outputs (e.g., response generation) than tasks in which the conditional entropy of the target distribution is already low enough (e.g., machine translation). The diverse decoding\nstrategy also helps more in tasks for which reranking is needed to incorporate features not considered in standard encoder-decoder models, such as documentlevel abstractive summarization."}, {"heading": "2 Related Work", "text": "Diverse decoding has been sufficiently explored in phrase-based MT, where the output space is approximated by the N-best list (Huang, 2008; Finkel et al., 2006). Attempts include compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al., 2013), blending multiple systems (Cer et al., 2013), and sampling translations proportional to their probability (Chatterjee and Cancedda, 2010). The most relevant is work from Gimpel et al. (2013) and Batra et al. (2012) that produces diverse N-best lists by adding a dissimilarity function based on N-gram overlaps, distancing the current translation from already-generated ones by choosing translations that have higher scores but are distinct from previous ones. While we draw on these intuitions, these existing diversity-promoting algorithms are tailored to phrase-based translation frameworks and not easily transplanted to neural MT decoding, which requires batched computation.\nSome recent work has started looking at decoding algorithms for neural generation tasks. Cho (2016) proposed a meta-algorithm that runs in parallel many\nchains of the noisy version of an inner decoding algorithm. Vijayakumar et al. (2016) proposed a diversityaugmented objective for image caption generation akin to Gimpel et al. (2013), but in the neural context. Shao et al. (2016) studied the diverse conversational response generation problem by using a stochastic search algorithm that reranks the hypothesis segment by segment, which injects diversity earlier in the decoding process.\nThe proposed RL based algorithm is inspired by a variety of recent reinforcement learning approaches in NLP for tasks such as dialogue (Dhingra et al., 2016), word compositions (Yogatama et al., 2016), machine translation (Ranzato et al., 2015), neural model visualization (Lei et al., 2016), and coreference (Clark and Manning, 2016)."}, {"heading": "3 Diverse Beam Decoding", "text": "In this section, we introduce the proposed algorithm. We first go over the vanilla beam search method and then detail the proposed algorithm which fosters diversity during decoding."}, {"heading": "3.1 Basics", "text": "Let X denote the source input, which is the input dialogue history for conversational response generation or a source sentence for machine translation. The input X is mapped to a vector representation, which is used as the initial input to the decoder. Each X is paired with a target sequence Y , which corresponds to a dialogue utterance in response generation or a target sentence in machine translation. Y consists a sequence of words, where Y = {y1, y2, ..., yny} and ny denotes the length of Y . A neural generation model defines a distribution over outputs and sequentially predicts tokens using a softmax function:\np(Y |X) = ny\u220f t=1 p(yt|X, y1, y2, ..., yt\u22121)\nAt test time, the goal is to find the sequence Y \u2217 that maximizes the probability given input X:\nY \u2217 = arg max X p(Y \u2217|X) (1)"}, {"heading": "3.2 Standard Beam Search for N-best lists", "text": "N-best lists are standardly generated from a model of p(Y |X) using a beam search decoder. As illustrated in Figure 1, at time step t\u2212 1 in decoding, the\ndecoder keeps track of K hypotheses, where K denotes the beam size, and their scores S(Yt\u22121|X) = log p(y1, y2, ..., yt\u22121|X). As it moves on to time step t, it expands each of the K hypotheses (denoted as Y kt\u22121 = {yk1 , yk2 , ..., ykt\u22121}, k \u2208 [1,K]) by selecting top K translations, denoted as yk,k \u2032\nt , k \u2032 \u2208 [1,K],\nleading to the construction of K \u00d7K new hypotheses:\n[Y kt\u22121, y k,k\u2032 t ], k \u2208 [1,K], k\u2032 \u2208 [1,K]\nThe score for each of the K \u00d7K hypotheses is computed as follows:\nS(Y kt\u22121, y k,k\u2032 t |x) = S(Y kt\u22121|x)+log p(y k,k\u2032\nt |x, Y kt\u22121) (2)\nIn a standard beam search model, the top K hypotheses are selected (from the K \u00d7 K hypotheses computed in the last step) based on the score S(Y kt\u22121, y k,k\u2032\nt |x). The remaining hypotheses are ignored when the algorithm proceeds to the next time step."}, {"heading": "3.3 Generating a Diverse N-best List", "text": "Unfortunately, the N-best lists outputted from standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006; Huang, 2008). The beam search algorithm can only keep a small proportion of candidates in the search space, and most of the generated translations in N-best list are similar, differing only by punctuation or minor morphological variations, with most of the words overlapping. Because this lack of diversity in the N-best list will significantly decrease the impact of the reranking process, it is important to find a way to generate a more diverse N-best list.\nWe propose changing the way S(Y kt\u22121, y k,k\u2032\nt |x) is computed in an attempt to promote diversity, as shown in Figure 1. For each of the hypotheses Y kt\u22121 (he and it), we generate the top K translations yk,k \u2032\nt , k \u2032 \u2208 [1,K] as in the standard beam search model. Next, we rank the K translated tokens generated from the same parental hypothesis based on p(yk,k \u2032\nt |x, Y kt\u22121) in descending order: he is ranks first among he is and he has, and he has ranks second; similarly for it is and it has.\nWe then rewrite the score for [Y kt\u22121, y k,k\u2032\nt ] by adding an additional term \u03b3k\u2032, where k\u2032 denotes the\nranking of the current hypothesis among its siblings (1 for he is and it is, 2 for he has and it has).\nS\u0302(Y kt\u22121, y k,k\u2032 t |x) = S(Y kt\u22121, y k,k\u2032 t |x)\u2212 \u03b3k\u2032 (3)\nWe call \u03b3 the diversity rate; it indicates the degree of diversity one wants to integrate into the beam search model.\nThe top K hypotheses are selected based on S\u0302(Y kt\u22121, y k,k\u2032\nt |x) as we move on to the next time step. By adding the additional term \u03b3k\u2032, the model punishes lower-ranked hypotheses among siblings (hypotheses descended from the same parent). When we compare newly generated hypotheses descended from different ancestors, the model gives more credit to top hypotheses from each of the different ancestors. For instance, even though the original score for it is is lower than he has, the model favors the former as the latter is more severely punished by the intra-sibling ranking part \u03b3k\u2032. The model thus generally favors choosing hypotheses from diverse parents, leading to a more diverse N-best list. The proposed model is straightforwardly implemented with a minor adjustment to the standard beam search."}, {"heading": "4 Automatically Learning Diversity Rate", "text": "One disadvantage of the algorithm described above is that a fixed value of diversity rate \u03b3 is applied to all examples. This could be problematic since the optimal diversity rate should theoretically vary from instance to instance. High diversity rate can even be detrimental if it pushes the decoding model away from the beam search algorithm too much. For example, Shao et al. (2016) discover that in response generation, standard beam search is already good enough for short responses but deteriorates as the sequence gets longer. As another example, Vijayakumar et al. (2016) argue that diverse decoding is beneficial for images with many objects in image caption generation, but this is not true when there are few objects in the image.\nA good diverse decoding algorithm should have the ability to automatically adjust its diversity rates for different inputs\u2014for example, using small diversity rates for images with fewer objects but larger rates for those with more objects. We thus propose an algorithm using reinforcement learning (denoted diverseRL) that is capable of learning different \u03b3 values for different inputs."}, {"heading": "4.1 Model", "text": "We first define a list \u0393 that contains the values that \u03b3 can take. For example, \u0393 might consist of the 21 values in the range [0,1] at regularly spaced intervals 0.05 apart.6 Our main idea is to use reinforcement learning (policy gradient methods) to discover the best diversity rate \u03b3(X) for a given input X with respect to the final evaluation metric. For each input X , we parameterize the action of choosing an associated diversity rate \u03b3(X) by a policy network \u03c0(\u03b3(X) = \u03b3\u2032|X) which is a distribution over the |\u0393| classes.7 We first map the input X to a vector representation hX using a recurrent net8, and then map hX to a policy distribution over different values of \u03b3 using a softmax function:\n\u03c0(\u03b3(X) = \u03b3\u2032|X) = exp(hTX \u00b7 h\u03b3\u2032)\u2211j=|\u0393|\nj=1 exp(h T X \u00b7 h\u0393j )\n(4)\nGiven an action, namely a choice of \u03b3\u2032 for \u03b3(X), we start decoding using the proposed diverse decoding algorithm and obtain an N-best list. Then we pick the best output\u2014the output with the largest reranking score, or the output with the largest probability if no reranking is needed. Using the selected output, we compute the evaluation score (e.g., BLEU) denoted R(\u03b3(X) = \u03b3\u2032), and this score is used as the reward9 for the action of choosing diversity rate \u03b3(X) = \u03b3\u2032. We use the REINFORCE algorithm (Williams, 1992), a kind of policy gradient method, to find the optimal diversity rate policy by maximizing the expectation of the final reward, denoted as follows:\nE\u03c0(\u03b3(X)=\u03b3\u2032|X)[R(\u03b3(X) = \u03b3 \u2032))] (5)\nThe expectation is approximated by sampling from \u03c0 and the gradient is computed based on the likelihood\n6This is just for illustration purpose. One can define any set of diversity rate values.\n7This means the probability of \u03b3(X) taking on two similar values (e.g., 0.05 and 0.1) are independent. An alternative is to make \u03b3 continuous. However, we find that using discrete values is good enough because of the large amount of training data, and discrete values are easier to implement.\n8This recurrent net shares parameters with the standard generation model.\n9This idea is inspired by recent work (Ranzato et al., 2015) that uses BLEU score as reward in reinforcement learning for machine translation. Our focus is different since we are only interested in learning the policy to obtain diversity rates \u03b3(X).\nratio (Glynn, 1987; VM et al., 1968):\n\u2207E(\u03b8) = [R(\u03b3(X)\u2212 b]\u2207 log \u03c0(\u03b3(X) = \u03b3\u2032|X) (6) where b denotes the baseline value.10 The model is trained to take actions (choosing a diversity rate \u03b3) that will lead to the highest value of final rewards."}, {"heading": "4.2 Training and Testing", "text": "To learn the policy \u03c0(\u03b3(X) = \u03b3\u2032)), we take a pretrained encoder-decoder model and run additional epochs over the training set in which we keep the encoder-decoder parameters fixed and do the diverse beam search using \u03b3(X) sampled from the policy distribution. Because decoding is needed for every training sample, training is extremely time-consuming. Luckily, since the sentence composition model involved in \u03c0(\u03b3(X) = \u03b3|X) shares parameters with the pre-trained encoder-decoder model, the only parameters needed to be learned are only those in the softmax function in Eq. 4, the number of which is relatively small. We therefore only take a small fraction of training examples (around 100,000 instances).\nSpecial attention is needed for tasks in which feature-based reranking is used for picking the final output: feature weights will change as \u03b3 changes because those weights are tuned based on the decoded N-best list for dev set, usually using MERT (Och, 2003). Different \u03b3 will lead to different dev set N-best lists and consequently different feature weights. We thus adjust feature weights using the dev set after every 10,000 instances. Training takes roughly 1 day."}, {"heading": "5 Experiments", "text": "We conduct experiments on three different sequence generation tasks: conversational response generation, abstractive summarization and machine translation, the details of which are described below.\n10Baseline value is estimated using another neural model that takes as input X and outputs a scalar b denoting the estimation of the reward. The baseline model is trained by minimizing the mean squared loss between the estimated reward b and actual cumulative reward r, ||r\u2212 b||2. We refer the readers to (Ranzato et al., 2015; Zaremba and Sutskever, 2015) for more details. The baseline estimator model is independent from the policy models and the error is not backpropagated back to them."}, {"heading": "5.1 Conversational Response Generation", "text": ""}, {"heading": "5.1.1 Dataset and Training", "text": "We used the OpenSubtitles (OSDb) dataset (Tiedemann, 2009), an open-domain movie script dataset containing roughly 60M-70M scripted lines spoken by movie characters. Our models are trained to predict the current turn given the preceding ones. We trained a two-layer encoder-decoder model with attention (0; Luong et al., 2015), with 512 units in each layer. We treat the two preceding dialogue utterances as dialogue history, simply concatenating them to form the source input.\nTo better illustrate in which scenarios the proposed algorithm offers the most help, we construct three different dev-test splits based on reference length, specifically: \u2022 Natural: Instances randomly sampled from the\ndataset. \u2022 Short: Instances with target reference length no\ngreater than 6. \u2022 Long: Instances with target reference length\ngreater than 16. Each set contains roughly 2,000 instances."}, {"heading": "5.1.2 Decoding and Reranking", "text": "We consider the following two settings:\nNon-Reranking No reranking is needed. We simply pick the output with highest probability using standard and diverse beam search.\nReranking Following Li et al. (2015a), we first generate an N-best list using vanilla or diverse beam search and rerank the generated responses by combining likelihood log p(Y |X), backward likelihood log p(X|Y ),11 sequence length L(Y ), and language model likelihood p(Y ). The linear combination of log(Y |X) and log p(X|Y ) is a generalization of the mutual information between the source and the target, which dramatically decreases the rate of dull and generic responses. Feature weights are optimized using MERT (Och, 2003) on N-best lists of response candidates.12\n11log p(Y |X) is trained in a similar way as standard SEQ2SEQ models with only sources and targets being swapped.\n12We set the minimum length and maximum length of a decoded target to 0.75 and 1.5 times the length of sources. Beam size K is set to 10. We then rerank the N-best list and pick the output target with largest final ranking score.\nNatural Short Long Without-Reranking\nvanilla 1.30 1.59 0.89 diverse 1.42 1.63 1.06\n(+9.1%) (+2.5%) (+19%) diverseRL 1.49 1.67 1.03\n(+15%) (+5.0%) (+16%) With-Reranking\nvanilla 1.88 2.52 1.17 diverse 2.21 2.75 1.68\n(+17%) (+9.2%) (+44%) diverseRL 2.32 2.79 1.70\n(+23%) (+11%) (+47%)"}, {"heading": "5.1.3 Evaluation", "text": "For automatic evaluations, we report (1) BLEU (Papineni et al., 2002), which has widely been used in response generation evaluation; and (2) diversity, which is the number of distinct unigrams and bigrams in generated responses scaled by the total number of generated tokens. This scaling is to avoid favoring long sentences, as described in Li et al. (2016a).\nWe also do human evaluation, as suggested by Liu et al. (2016). We employ crowdsourced judges to provide evaluations for a random sample of 200 items. Each output pair was ranked by 3 judges, who were asked to decide which of the two outputs was better. They were instructed to prefer outputs that were more specific (relevant) to the context. Ties\nwere permitted. Identical strings were automatically assigned the same score."}, {"heading": "5.1.4 Results", "text": "Results for BLEU scores and diversity scores are presented in Tables 1 and 2. We observe performance boosts introduced by the diverse decoding algorithm across all settings, but they are more significant in the reranking settings than the non-reranking ones. Within the reranking settings, the improvement is much more significant for longer responses than shorter ones. The explanation is as follows: due to the smaller search space for short responses, vanilla beam search could already be diverse and thus good enough. However, when it comes to longer responses, the neural generation could be trapped in a local decoding minimum, sometimes generating incoherent or even contradictory responses like \u201cI like fish but I don\u2019t like fish\u201d, which has also been noticed by Shao et al. (2016). For longer responses, hypotheses generated by standard beam search are nearly duplicates,\nas shown in Table 4. This dramatically decreases the impact of the reranking process.13\nOn the Natural set, the distribution of which is the same as the real-world set, we see a performance boost from the diverseRL model, which is able to automatically adjust diversity rate, over standard diverse decoding. For the short and long sets, the improvement from the diverseRL model is less significant, which is reasonable because the dataset has been pre-processed in such a way that examples are more similar to each other, leading the optimal diversity rate to be approximately the same.\nIn terms of human evaluation, we find that the diverseRL model produces responses with better general quality, winning 62 percent of the time when compared to standard beam search."}, {"heading": "5.2 Abstractive Summarization", "text": ""}, {"heading": "5.2.1 Training and Dataset", "text": "We consider two settings for abstractive summarization. For the first setting (denoted single), we follow the protocols described in Rush et al. (2015), in which the source input is the first sentence of the document to summarize. We train a word-level attention model for this setting. Our training dataset consists of 800K pairs.\nA good summarization system should have the ability to summarize a large chunk of text, separating wheat from chaff. We thus consider another setting in which each input consists of multiple sentences (denoted multi).14 We train a hierarchical model with attention at sentence level (Li et al., 2015b) for generation, which has been shown to yield better results than word-level encoder-decoder models for multisentence summarization (Nallapati et al., 2016).\nFor reranking, we employ various global features taken from or inspired by pre-neural work in summarization (Daume\u0301 III and Marcu, 2006; Nenkova and Vanderwende, 2005; McKeown et al., 1999). Features we consider include (1) average tf-idf score of constituent words in the generated output; (2) KLSum\n13Sampling could be another way of generating diverse responses. However, responses from sampling are usually incoherent, especially for long sequences. This is because the error accumulates as decoding goes on. A similar phenomenon has also been observed by Shao et al. (2016).\n14We consider 10 sentences at most for each input. Sentence position treated as a word feature which is associated with an embedding to be learned as suggested in Nallapati et al. (2016)\n(Haghighi and Vanderwende, 2009), which reflects the topic distribution overlap between the entire input document and the generated output;15 and (3) backward probability p(X|Y ), i.e., the probability of generating the entire document given the summary. For evaluation, we report ROUGE-2 scores (Lin, 2004) in Table 5.\nThe proposed diverse reranking algorithm helps in both cases, but more significantly in the reranking setting than the non-reranking one. This is because the mechanisms by which diverse decoding helps in the two settings are different: for the non-reranking setting, if the conditional entropy in the target distribution is low enough, the decoded string from standard BS will be close to the global optimum, and thus there won\u2019t be much space for improvement. For the reranking setting, however, the story is different: it is not just about finding the global optimal output based on the target distribution, but also about incorporating different criteria to make up for facets that are missed by the encoder-decoder model. This requires the N-best list to be diverse for the reranking strategy to make a significant difference.\nWe also find an improvement from the reranking model using document-level features over the nonreranking model. We did not observe a big performance boost from the diverseRL model over standard diverse model (around 0.3 ROUGE score boost) on this task. This is probably because the diverseRL model helps most when inputs are different and thus need different diverse decoding rates. In this task, input documents are all news articles and share similar properties, so a unified diversity rate tuned on the dev set might already be good enough. Results for\n15namely, the KL divergence between the topic distributions assigned by a variant of the LDA model (Blei et al., 2003) that identifies general, document-specific and topic-specific word clusters.\ndiverseRL are thus omitted for brevity. Interestingly, we find that the result for the multi setting is significantly worse than the single setting, which is also observed by Nallapati et al. (2016): adding more sentences leads to worse results. This illustrates the incapability of neural generation models to date to summarize long documents. The proposed diverse decoding model produces a huge performance boost in the multi setting."}, {"heading": "5.3 Machine Translation", "text": ""}, {"heading": "5.3.1 Dataset and Training", "text": "The models are trained on the WMT\u201914 training dataset containing 4.5 million pairs for EnglishGerman. We limit our vocabularies to the top 50K most frequent words for both languages. Words that are not in the vocabulary list are replaced by am unknown token. We use newstest2013 (3000 sentence pairs) as the development set and report translation performances in BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences). We trained neural SEQ2SEQ models (Sutskever et al., 2014) with attention (Luong et al., 2015; Cho et al., 2014). Unknown words are replaced using methods similar to those of Luong et al. (2015)."}, {"heading": "5.3.2 Decoding and Reranking", "text": "Again we consider both reranking settings and non-reranking settings, where for non-reranking we select the best output using standard beam search. For reranking, we first generate a large N-best list using the beam search algorithm.16 We then rerank the hypotheses using features that have been shown to be useful in neural machine translation (Sennrich et al., 2015a; Gulcehre et al., 2015; Cohn et al., 2016; Cheng et al., 2015), including (1) backward probability p(X|Y ); (2) language model probability p(Y ) trained from monolingual data (Gulcehre et al., 2015; Gulcehre et al., 2015);17 (3) bilingual symmetry: the\n16At each time step of decoding, we are presented withK\u00d7K word candidates whereK denotes the beam size. We first add all hypotheses with an EOS token generated at the current time step to the N-best list. Next, we preserve the top K unfinished hypotheses and move to the next time step. We therefore maintain constant batch size as hypotheses are completed and removed, by adding in more unfinished hypotheses. This allows the size of final N-best list for each input to be much larger than the beam size.\n17p(t) is trained using a single-layer LSTM recurrent models using monolingual data. We use News Crawl cor-\nagreement between attentions in German-English and English-German; and (4) target length. Again feature weights are optimized using MERT (Och, 2003) on N-best lists of response candidates in the dev set."}, {"heading": "5.3.3 Results", "text": "Experimental results are shown in Table 6. First, no significant improvement is observed for the proposed diverse BS model in the non-reranking setting. The explanation is similar to the abstractive summarization task: the vanilla BS algorithm with large beam size is already good enough at finding the global optimum, and the small benefit from diverse decoding for some examples might even be canceled out by others where diversity rate value is too large. The diverseRL model addresses the second issue, leading to a performance boost of +0.25. For the reranking setting, the performance boost is more significant, +0.6 for diverse BS and +0.9 for DiverseRL. Generally, we find the performance boost introduced by diverse decoding in machine translation is not as big as in summarization and response generation. We think this is the due to fact that the entropy for target distribution is already very low (perplexity less than 6), whereas standard BS is already fairly strong.18\npora from WMT13 (http://www.statmt.org/wmt13/ translation-task.html) as additional training data to train monolingual language models. We used a subset of the original dataset which roughly contains 60 millions sentences.\n18The difference between beam size 2 and beam size 12 in decoding for French-English translation is only around 0.3 BLEU score (Sutskever et al., 2014), which also confirms that the standard decoding algorithm is already strong on machine translation tasks."}, {"heading": "6 Discussion", "text": "In this paper, we introduce a general diversitypromoting decoding algorithm for neural generation. The model adds an intra-sibling ranking term to the standard beam search algorithm, favoring choosing hypotheses from diverse parents. The proposed model is a general, simple and fast algorithm that will bring a performance boost to all neural generation tasks for which a diverse N-best list is needed.\nOn top of this approach, we build a more sophisticated algorithm that is capable of automatically adjusting diversity rates for different inputs using reinforcement learning. We find that, at the expense of model complexity and training time (compared with the basic RL-free diverse decoding algorithm), the model is able to adjust its diversity rate to better values, yielding generation quality better than either standard beam search or the basic diverse decoding approach.\nWe also discuss for which tasks the diverse decoding strategy helps more. We find that diverse decoding helps more for tasks with high entropy in the target distribution and settings in which further reranking is employed.\nAcknowledgements We would especially like to thank Thang Luong Minh for insightful discussions and releasing relevant code, as well as Sida Wang, Ziang Xie, Chris Manning and other members from Stanford NLP group for helpful comments and suggestions. The authors also want to thank Michel Galley, Bill Dolan, Chris Brockett, Jianfeng Gao and other members of the NLP group at Microsoft Research for helpful discussions. Jiwei Li is supported by a Facebook Fellowship, which we gratefully acknowledge. This work is partially supported by the DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF- 15- 1-0462. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or Facebook."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Diverse m-best solutions in Markov random fields", "author": ["Dhruv Batra", "Payman Yadollahpour", "Abner GuzmanRivera", "Gregory Shakhnarovich."], "venue": "Computer Vision\u2013ECCV 2012, pages 1\u201316. Springer.", "citeRegEx": "Batra et al\\.,? 2012", "shortCiteRegEx": "Batra et al\\.", "year": 2012}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "Journal of machine Learning research, 3(Jan):993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Positive diversity tuning for machine translation system combination", "author": ["Daniel Cer", "Christopher D Manning", "Daniel Jurafsky."], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 320\u2013328.", "citeRegEx": "Cer et al\\.,? 2013", "shortCiteRegEx": "Cer et al\\.", "year": 2013}, {"title": "Minimum error rate training by sampling the translation lattice", "author": ["Samidh Chatterjee", "Nicola Cancedda."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 606\u2013615. Association for Computational Linguistics.", "citeRegEx": "Chatterjee and Cancedda.,? 2010", "shortCiteRegEx": "Chatterjee and Cancedda.", "year": 2010}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1504.00325.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "arXiv preprint arXiv:1512.04650.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Noisy parallel approximate decoding for conditional recurrent language model", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1605.03835.", "citeRegEx": "Cho.,? 2016", "shortCiteRegEx": "Cho.", "year": 2016}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush"], "venue": null, "citeRegEx": "Chopra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1609.08667.", "citeRegEx": "Clark and Manning.,? 2016", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."], "venue": "arXiv preprint arXiv:1601.01085.", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Bayesian queryfocused summarization", "author": ["Hal Daum\u00e9 III", "Daniel Marcu."], "venue": "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 305\u2013312. Associa-", "citeRegEx": "III and Marcu.,? 2006", "shortCiteRegEx": "III and Marcu.", "year": 2006}, {"title": "Trait-based hypothesis selection for machine translation", "author": ["Jacob Devlin", "Spyros Matsoukas."], "venue": "Proceedings of the 2012 Conference of the North American", "citeRegEx": "Devlin and Matsoukas.,? 2012", "shortCiteRegEx": "Devlin and Matsoukas.", "year": 2012}, {"title": "End-to-end reinforcement learning of dialogue agents for information access", "author": ["Bhuwan Dhingra", "Lihong Li", "Xiujun Li", "Jianfeng Gao", "Yun-Nung Chen", "Faisal Ahmed", "Li Deng."], "venue": "arXiv preprint arXiv:1609.00777.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines", "author": ["Jenny Rose Finkel", "Christopher D Manning", "Andrew Y Ng."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language", "citeRegEx": "Finkel et al\\.,? 2006", "shortCiteRegEx": "Finkel et al\\.", "year": 2006}, {"title": "A systematic exploration of diversity in machine translation", "author": ["Kevin Gimpel", "Dhruv Batra", "Chris Dyer", "Gregory Shakhnarovich", "Virginia Tech."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, October.", "citeRegEx": "Gimpel et al\\.,? 2013", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "Likelilood ratio gradient estimation: an overview", "author": ["Peter W Glynn."], "venue": "Proceedings of the 19th conference on Winter simulation, pages 366\u2013375. ACM.", "citeRegEx": "Glynn.,? 1987", "shortCiteRegEx": "Glynn.", "year": 1987}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1503.03535.", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Haghighi and Vanderwende.,? 2009", "shortCiteRegEx": "Haghighi and Vanderwende.", "year": 2009}, {"title": "Forest reranking: Discriminative parsing with non-local features", "author": ["Liang Huang."], "venue": "ACL, pages 586\u2013 594.", "citeRegEx": "Huang.,? 2008", "shortCiteRegEx": "Huang.", "year": 2008}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP, pages 1700\u2013 1709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Minimum Bayes-risk decoding for statistical machine translation", "author": ["Shankar Kumar", "William Byrne."], "venue": "Technical report, DTIC Document.", "citeRegEx": "Kumar and Byrne.,? 2004", "shortCiteRegEx": "Kumar and Byrne.", "year": 2004}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "arXiv preprint arXiv:1606.04155.", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1510.03055.", "citeRegEx": "Li et al\\.,? 2015a", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1506.01057.", "citeRegEx": "Li et al\\.,? 2015b", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chin-Yew Lin."], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, volume 8. Barcelona, Spain.", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1603.08023.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Lstm based conversation models", "author": ["Yi Luan", "Yangfeng Ji", "Mari Ostendorf."], "venue": "arXiv preprint arXiv:1603.09457.", "citeRegEx": "Luan et al\\.,? 2016", "shortCiteRegEx": "Luan et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Lattice-based minimum error rate training for statistical machine translation", "author": ["Wolfgang Macherey", "Franz Josef Och", "Ignacio Thayer", "Jakob Uszkoreit."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 725\u2013734.", "citeRegEx": "Macherey et al\\.,? 2008", "shortCiteRegEx": "Macherey et al\\.", "year": 2008}, {"title": "Towards multidocument summarization by reformulation: Progress and prospects", "author": ["Kathleen McKeown", "Judith Klavans", "Vasileios Hatzivassiloglou", "Regina Barzilay", "Eleazar Eskin."], "venue": "AAAI/IAAI, pages 453\u2013460.", "citeRegEx": "McKeown et al\\.,? 1999", "shortCiteRegEx": "McKeown et al\\.", "year": 1999}, {"title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "author": ["Ramesh Nallapati", "Bowen Zhou", "\u00c7a glar Gul\u00e7ehre", "Bing Xiang"], "venue": null, "citeRegEx": "Nallapati et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nallapati et al\\.", "year": 2016}, {"title": "The impact of frequency on summarization", "author": ["Ani Nenkova", "Lucy Vanderwende."], "venue": "Microsoft Research, Redmond, Washington, Tech. Rep. MSR-TR-2005-101.", "citeRegEx": "Nenkova and Vanderwende.,? 2005", "shortCiteRegEx": "Nenkova and Vanderwende.", "year": 2005}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160\u2013167. Association for Computational Linguistics.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for Computa-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["MarcAurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba."], "venue": "arXiv preprint arXiv:1511.06732.", "citeRegEx": "Ranzato et al\\.,? 2015", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1509.00685.", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Improving neural machine translation", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015b", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Generating long and diverse responses with neural conversational models", "author": ["Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil"], "venue": null, "citeRegEx": "Shao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shao et al\\.", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint arXiv:1506.06714.", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "News from opus-a collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent advances in natural language processing, volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Lattice minimum bayes-risk decoding for statistical machine translation", "author": ["Roy W Tromble", "Shankar Kumar", "Franz Och", "Wolfgang Macherey."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 620\u2013629. Associ-", "citeRegEx": "Tromble et al\\.,? 2008", "shortCiteRegEx": "Tromble et al\\.", "year": 2008}, {"title": "Diverse beam search: Decoding diverse solutions from neural sequence models", "author": ["Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra."], "venue": "arXiv preprint arXiv:1610.02424.", "citeRegEx": "Vijayakumar et al\\.,? 2016", "shortCiteRegEx": "Vijayakumar et al\\.", "year": 2016}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Stochastic optimization", "author": ["ALEKSAND. VM", "VI Sysoyev", "SHEMENEV. VV."], "venue": "Engineering Cybernetics, (5):11.", "citeRegEx": "VM et al\\.,? 1968", "shortCiteRegEx": "VM et al\\.", "year": 1968}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning, 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Bagging and boosting statistical machine translation systems", "author": ["Tong Xiao", "Jingbo Zhu", "Tongran Liu."], "venue": "Artificial Intelligence, 195:496\u2013527.", "citeRegEx": "Xiao et al\\.,? 2013", "shortCiteRegEx": "Xiao et al\\.", "year": 2013}, {"title": "Learning to compose words into sentences with reinforcement learning", "author": ["Dani Yogatama", "Phil Blunsom", "Chris Dyer", "Edward Grefenstette", "Wang Ling"], "venue": null, "citeRegEx": "Yogatama et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2016}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1505.00521, 362.", "citeRegEx": "Zaremba and Sutskever.,? 2015", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}], "referenceMentions": [{"referenceID": 42, "context": "Neural generation models (Sutskever et al., 2014; 0; Cho et al., 2014; Kalchbrenner and Blunsom, 2013)", "startOffset": 25, "endOffset": 102}, {"referenceID": 7, "context": "Neural generation models (Sutskever et al., 2014; 0; Cho et al., 2014; Kalchbrenner and Blunsom, 2013)", "startOffset": 25, "endOffset": 102}, {"referenceID": 21, "context": "Neural generation models (Sutskever et al., 2014; 0; Cho et al., 2014; Kalchbrenner and Blunsom, 2013)", "startOffset": 25, "endOffset": 102}, {"referenceID": 39, "context": "are of growing interest for various applications such as machine translation (Sennrich et al., 2015b; Gulcehre et al., 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 18, "context": "are of growing interest for various applications such as machine translation (Sennrich et al., 2015b; Gulcehre et al., 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 77, "endOffset": 124}, {"referenceID": 46, "context": ", 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015; Luan et al., 2016), abstractive summarization (Nallapati et al.", "startOffset": 44, "endOffset": 107}, {"referenceID": 41, "context": ", 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015; Luan et al., 2016), abstractive summarization (Nallapati et al.", "startOffset": 44, "endOffset": 107}, {"referenceID": 28, "context": ", 2015), conversational response generation (Vinyals and Le, 2015; Sordoni et al., 2015; Luan et al., 2016), abstractive summarization (Nallapati et al.", "startOffset": 44, "endOffset": 107}, {"referenceID": 32, "context": ", 2016), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015; Chopra et al., 2016), and image caption generation (Chen et al.", "startOffset": 35, "endOffset": 99}, {"referenceID": 37, "context": ", 2016), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015; Chopra et al., 2016), and image caption generation (Chen et al.", "startOffset": 35, "endOffset": 99}, {"referenceID": 9, "context": ", 2016), abstractive summarization (Nallapati et al., 2016; Rush et al., 2015; Chopra et al., 2016), and image caption generation (Chen et al.", "startOffset": 35, "endOffset": 99}, {"referenceID": 5, "context": ", 2016), and image caption generation (Chen et al., 2015).", "startOffset": 38, "endOffset": 57}, {"referenceID": 30, "context": "One serious issue with the beam search algorithm which has long been recognized (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004) is lack of diversity: candidates often differ only by punctuation or minor morphological variations, with most of the words overlapping.", "startOffset": 80, "endOffset": 148}, {"referenceID": 44, "context": "One serious issue with the beam search algorithm which has long been recognized (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004) is lack of diversity: candidates often differ only by punctuation or minor morphological variations, with most of the words overlapping.", "startOffset": 80, "endOffset": 148}, {"referenceID": 22, "context": "One serious issue with the beam search algorithm which has long been recognized (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004) is lack of diversity: candidates often differ only by punctuation or minor morphological variations, with most of the words overlapping.", "startOffset": 80, "endOffset": 148}, {"referenceID": 24, "context": "For instance, in neural response generation, an additional reranker is crucial for avoiding dull and generic responses (Li et al., 2015a; Sordoni et al., 2015; Shao et al., 2016).", "startOffset": 119, "endOffset": 178}, {"referenceID": 41, "context": "For instance, in neural response generation, an additional reranker is crucial for avoiding dull and generic responses (Li et al., 2015a; Sordoni et al., 2015; Shao et al., 2016).", "startOffset": 119, "endOffset": 178}, {"referenceID": 40, "context": "For instance, in neural response generation, an additional reranker is crucial for avoiding dull and generic responses (Li et al., 2015a; Sordoni et al., 2015; Shao et al., 2016).", "startOffset": 119, "endOffset": 178}, {"referenceID": 11, "context": ", position bias, Markov condition or bilingual attention symmetry features in machine translation (Cohn et al., 2016), or global discourse features in summarization.", "startOffset": 98, "endOffset": 117}, {"referenceID": 11, "context": ", position bias, Markov condition or bilingual attention symmetry features in machine translation (Cohn et al., 2016), or global discourse features in summarization. For example, Shao et al. (2016) find that the re-ranking heuristics in conversational response generation works for shorter responses but hardly works at all for long responses, since the hypotheses from the beam search for long responses are mostly identical even when the beam size is large.", "startOffset": 99, "endOffset": 198}, {"referenceID": 30, "context": "The proposed model supports batched decoding using GPU, significantly speeding up the decoding process compared to other diversity fostering models for phrasebased MT systems (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004; Devlin and Matsoukas, 2012).", "startOffset": 175, "endOffset": 271}, {"referenceID": 44, "context": "The proposed model supports batched decoding using GPU, significantly speeding up the decoding process compared to other diversity fostering models for phrasebased MT systems (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004; Devlin and Matsoukas, 2012).", "startOffset": 175, "endOffset": 271}, {"referenceID": 22, "context": "The proposed model supports batched decoding using GPU, significantly speeding up the decoding process compared to other diversity fostering models for phrasebased MT systems (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004; Devlin and Matsoukas, 2012).", "startOffset": 175, "endOffset": 271}, {"referenceID": 13, "context": "The proposed model supports batched decoding using GPU, significantly speeding up the decoding process compared to other diversity fostering models for phrasebased MT systems (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004; Devlin and Matsoukas, 2012).", "startOffset": 175, "endOffset": 271}, {"referenceID": 20, "context": "Diverse decoding has been sufficiently explored in phrase-based MT, where the output space is approximated by the N-best list (Huang, 2008; Finkel et al., 2006).", "startOffset": 126, "endOffset": 160}, {"referenceID": 15, "context": "Diverse decoding has been sufficiently explored in phrase-based MT, where the output space is approximated by the N-best list (Huang, 2008; Finkel et al., 2006).", "startOffset": 126, "endOffset": 160}, {"referenceID": 30, "context": "Attempts include compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 71, "endOffset": 139}, {"referenceID": 44, "context": "Attempts include compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 71, "endOffset": 139}, {"referenceID": 22, "context": "Attempts include compact representations like lattices and hypergraphs (Macherey et al., 2008; Tromble et al., 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 71, "endOffset": 139}, {"referenceID": 13, "context": ", 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al.", "startOffset": 65, "endOffset": 93}, {"referenceID": 49, "context": ", 2008; Kumar and Byrne, 2004), \u201ctraits\u201d like translation length (Devlin and Matsoukas, 2012), bagging/boosting (Xiao et al., 2013), blending multiple systems (Cer et al.", "startOffset": 112, "endOffset": 131}, {"referenceID": 3, "context": ", 2013), blending multiple systems (Cer et al., 2013), and sampling", "startOffset": 35, "endOffset": 53}, {"referenceID": 4, "context": "translations proportional to their probability (Chatterjee and Cancedda, 2010).", "startOffset": 47, "endOffset": 78}, {"referenceID": 3, "context": "translations proportional to their probability (Chatterjee and Cancedda, 2010). The most relevant is work from Gimpel et al. (2013) and Batra et al.", "startOffset": 48, "endOffset": 132}, {"referenceID": 1, "context": "(2013) and Batra et al. (2012) that produces diverse N-best lists by adding a dissimilarity function based on N-gram overlaps, distancing the current translation from already-generated ones by choosing translations that have higher scores but are distinct from previous ones.", "startOffset": 11, "endOffset": 31}, {"referenceID": 8, "context": "Cho (2016) proposed a meta-algorithm that runs in parallel many", "startOffset": 0, "endOffset": 11}, {"referenceID": 14, "context": "The proposed RL based algorithm is inspired by a variety of recent reinforcement learning approaches in NLP for tasks such as dialogue (Dhingra et al., 2016), word compositions (Yogatama et al.", "startOffset": 135, "endOffset": 157}, {"referenceID": 50, "context": ", 2016), word compositions (Yogatama et al., 2016), machine translation (Ranzato et al.", "startOffset": 27, "endOffset": 50}, {"referenceID": 36, "context": ", 2016), machine translation (Ranzato et al., 2015), neural model visualization (Lei et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 23, "context": ", 2015), neural model visualization (Lei et al., 2016), and coreference (Clark and Manning, 2016).", "startOffset": 36, "endOffset": 54}, {"referenceID": 10, "context": ", 2016), and coreference (Clark and Manning, 2016).", "startOffset": 25, "endOffset": 50}, {"referenceID": 38, "context": "Vijayakumar et al. (2016) proposed a diversityaugmented objective for image caption generation akin to Gimpel et al.", "startOffset": 0, "endOffset": 26}, {"referenceID": 14, "context": "(2016) proposed a diversityaugmented objective for image caption generation akin to Gimpel et al. (2013), but in the neural context.", "startOffset": 84, "endOffset": 105}, {"referenceID": 14, "context": "(2016) proposed a diversityaugmented objective for image caption generation akin to Gimpel et al. (2013), but in the neural context. Shao et al. (2016) studied the diverse conversational response generation problem by using a stochastic search algorithm that reranks the hypothesis segment by segment, which injects diversity earlier in the decoding process.", "startOffset": 84, "endOffset": 152}, {"referenceID": 15, "context": "Unfortunately, the N-best lists outputted from standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006; Huang, 2008).", "startOffset": 117, "endOffset": 151}, {"referenceID": 20, "context": "Unfortunately, the N-best lists outputted from standard beam search are a poor surrogate for the entire search space (Finkel et al., 2006; Huang, 2008).", "startOffset": 117, "endOffset": 151}, {"referenceID": 40, "context": "For example, Shao et al. (2016) discover that in response generation, standard beam search is already good enough for short responses but deteriorates as the sequence gets longer.", "startOffset": 13, "endOffset": 32}, {"referenceID": 40, "context": "For example, Shao et al. (2016) discover that in response generation, standard beam search is already good enough for short responses but deteriorates as the sequence gets longer. As another example, Vijayakumar et al. (2016) argue that diverse decoding is beneficial for images with many objects in image caption generation, but this is not true when there are few objects in the image.", "startOffset": 13, "endOffset": 226}, {"referenceID": 48, "context": "We use the REINFORCE algorithm (Williams, 1992), a kind of policy gradient method, to find the optimal diversity rate policy by maximizing the expectation of the final reward, denoted as follows:", "startOffset": 31, "endOffset": 47}, {"referenceID": 36, "context": "This idea is inspired by recent work (Ranzato et al., 2015) that uses BLEU score as reward in reinforcement learning for machine translation.", "startOffset": 37, "endOffset": 59}, {"referenceID": 17, "context": "ratio (Glynn, 1987; VM et al., 1968):", "startOffset": 6, "endOffset": 36}, {"referenceID": 47, "context": "ratio (Glynn, 1987; VM et al., 1968):", "startOffset": 6, "endOffset": 36}, {"referenceID": 34, "context": "Special attention is needed for tasks in which feature-based reranking is used for picking the final output: feature weights will change as \u03b3 changes because those weights are tuned based on the decoded N-best list for dev set, usually using MERT (Och, 2003).", "startOffset": 247, "endOffset": 258}, {"referenceID": 36, "context": "We refer the readers to (Ranzato et al., 2015; Zaremba and Sutskever, 2015) for more details.", "startOffset": 24, "endOffset": 75}, {"referenceID": 51, "context": "We refer the readers to (Ranzato et al., 2015; Zaremba and Sutskever, 2015) for more details.", "startOffset": 24, "endOffset": 75}, {"referenceID": 43, "context": "We used the OpenSubtitles (OSDb) dataset (Tiedemann, 2009), an open-domain movie script dataset containing roughly 60M-70M scripted lines spoken by movie characters.", "startOffset": 41, "endOffset": 58}, {"referenceID": 29, "context": "We trained a two-layer encoder-decoder model with attention (0; Luong et al., 2015), with 512 units in each layer.", "startOffset": 60, "endOffset": 83}, {"referenceID": 34, "context": "Feature weights are optimized using MERT (Och, 2003) on N-best lists of response candidates.", "startOffset": 41, "endOffset": 52}, {"referenceID": 24, "context": "Reranking Following Li et al. (2015a), we first generate an N-best list using vanilla or diverse beam search and rerank the generated responses by combining likelihood log p(Y |X), backward likelihood log p(X|Y ),11 sequence length L(Y ), and language model likelihood p(Y ).", "startOffset": 20, "endOffset": 38}, {"referenceID": 35, "context": "For automatic evaluations, we report (1) BLEU (Papineni et al., 2002), which has widely been used in response generation evaluation; and (2) diversity, which is the number of distinct unigrams and bigrams in generated responses scaled by the total number of generated tokens.", "startOffset": 46, "endOffset": 69}, {"referenceID": 24, "context": "This scaling is to avoid favoring long sentences, as described in Li et al. (2016a).", "startOffset": 66, "endOffset": 84}, {"referenceID": 27, "context": "We also do human evaluation, as suggested by Liu et al. (2016). We employ crowdsourced judges to provide evaluations for a random sample of 200 items.", "startOffset": 45, "endOffset": 63}, {"referenceID": 40, "context": "However, when it comes to longer responses, the neural generation could be trapped in a local decoding minimum, sometimes generating incoherent or even contradictory responses like \u201cI like fish but I don\u2019t like fish\u201d, which has also been noticed by Shao et al. (2016). For longer responses, hypotheses generated by standard beam search are nearly duplicates,", "startOffset": 249, "endOffset": 268}, {"referenceID": 37, "context": "For the first setting (denoted single), we follow the protocols described in Rush et al. (2015), in which the source input is the first sentence of the document to summarize.", "startOffset": 77, "endOffset": 96}, {"referenceID": 25, "context": "14 We train a hierarchical model with attention at sentence level (Li et al., 2015b) for gen-", "startOffset": 66, "endOffset": 84}, {"referenceID": 32, "context": "eration, which has been shown to yield better results than word-level encoder-decoder models for multisentence summarization (Nallapati et al., 2016).", "startOffset": 125, "endOffset": 149}, {"referenceID": 33, "context": "For reranking, we employ various global features taken from or inspired by pre-neural work in summarization (Daum\u00e9 III and Marcu, 2006; Nenkova and Vanderwende, 2005; McKeown et al., 1999).", "startOffset": 108, "endOffset": 188}, {"referenceID": 31, "context": "For reranking, we employ various global features taken from or inspired by pre-neural work in summarization (Daum\u00e9 III and Marcu, 2006; Nenkova and Vanderwende, 2005; McKeown et al., 1999).", "startOffset": 108, "endOffset": 188}, {"referenceID": 26, "context": "Sampling could be another way of generating diverse responses. However, responses from sampling are usually incoherent, especially for long sequences. This is because the error accumulates as decoding goes on. A similar phenomenon has also been observed by Shao et al. (2016). We consider 10 sentences at most for each input.", "startOffset": 4, "endOffset": 276}, {"referenceID": 26, "context": "Sampling could be another way of generating diverse responses. However, responses from sampling are usually incoherent, especially for long sequences. This is because the error accumulates as decoding goes on. A similar phenomenon has also been observed by Shao et al. (2016). We consider 10 sentences at most for each input. Sentence position treated as a word feature which is associated with an embedding to be learned as suggested in Nallapati et al. (2016) Single Multi Without-Reranking vanilla 11.", "startOffset": 4, "endOffset": 462}, {"referenceID": 19, "context": "(Haghighi and Vanderwende, 2009), which reflects the topic distribution overlap between the entire input document and the generated output;15 and (3) backward probability p(X|Y ), i.", "startOffset": 0, "endOffset": 32}, {"referenceID": 26, "context": "For evaluation, we report ROUGE-2 scores (Lin, 2004) in Table 5.", "startOffset": 41, "endOffset": 52}, {"referenceID": 2, "context": "namely, the KL divergence between the topic distributions assigned by a variant of the LDA model (Blei et al., 2003) that identifies general, document-specific and topic-specific word clusters.", "startOffset": 97, "endOffset": 116}, {"referenceID": 32, "context": "Interestingly, we find that the result for the multi setting is significantly worse than the single setting, which is also observed by Nallapati et al. (2016): adding more sentences leads to worse results.", "startOffset": 135, "endOffset": 159}, {"referenceID": 35, "context": "We use newstest2013 (3000 sentence pairs) as the development set and report translation performances in BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences).", "startOffset": 109, "endOffset": 132}, {"referenceID": 42, "context": "We trained neural SEQ2SEQ models (Sutskever et al., 2014) with attention (Luong et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 29, "context": ", 2014) with attention (Luong et al., 2015; Cho et al., 2014).", "startOffset": 23, "endOffset": 61}, {"referenceID": 7, "context": ", 2014) with attention (Luong et al., 2015; Cho et al., 2014).", "startOffset": 23, "endOffset": 61}, {"referenceID": 7, "context": ", 2015; Cho et al., 2014). Unknown words are replaced using methods similar to those of Luong et al. (2015).", "startOffset": 8, "endOffset": 108}, {"referenceID": 18, "context": "16 We then rerank the hypotheses using features that have been shown to be useful in neural machine translation (Sennrich et al., 2015a; Gulcehre et al., 2015; Cohn et al., 2016; Cheng et al., 2015), including (1) backward probability p(X|Y ); (2) language model probability p(Y ) trained from monolingual data (Gulcehre et al.", "startOffset": 112, "endOffset": 198}, {"referenceID": 11, "context": "16 We then rerank the hypotheses using features that have been shown to be useful in neural machine translation (Sennrich et al., 2015a; Gulcehre et al., 2015; Cohn et al., 2016; Cheng et al., 2015), including (1) backward probability p(X|Y ); (2) language model probability p(Y ) trained from monolingual data (Gulcehre et al.", "startOffset": 112, "endOffset": 198}, {"referenceID": 6, "context": "16 We then rerank the hypotheses using features that have been shown to be useful in neural machine translation (Sennrich et al., 2015a; Gulcehre et al., 2015; Cohn et al., 2016; Cheng et al., 2015), including (1) backward probability p(X|Y ); (2) language model probability p(Y ) trained from monolingual data (Gulcehre et al.", "startOffset": 112, "endOffset": 198}, {"referenceID": 18, "context": ", 2015), including (1) backward probability p(X|Y ); (2) language model probability p(Y ) trained from monolingual data (Gulcehre et al., 2015; Gulcehre et al., 2015);17 (3) bilingual symmetry: the", "startOffset": 120, "endOffset": 166}, {"referenceID": 18, "context": ", 2015), including (1) backward probability p(X|Y ); (2) language model probability p(Y ) trained from monolingual data (Gulcehre et al., 2015; Gulcehre et al., 2015);17 (3) bilingual symmetry: the", "startOffset": 120, "endOffset": 166}, {"referenceID": 34, "context": "Again feature weights are optimized using MERT (Och, 2003) on N-best lists of response candidates in the dev set.", "startOffset": 47, "endOffset": 58}, {"referenceID": 42, "context": "3 BLEU score (Sutskever et al., 2014), which also confirms that the standard decoding algorithm is already strong on machine translation tasks.", "startOffset": 13, "endOffset": 37}], "year": 2016, "abstractText": "In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an intersibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed. We further propose a variation that is capable of automatically adjusting its diversity decoding rates for different inputs using reinforcement learning (RL). We observe a further performance boost from this RL technique.1", "creator": "TeX"}}}