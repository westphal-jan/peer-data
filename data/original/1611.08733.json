{"id": "1611.08733", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Nov-2016", "title": "BliStrTune: Hierarchical Invention of Theorem Proving Strategies", "abstract": "Inventing targeted proof search strategies for specific problem sets is a difficult task. State-of-the-art automated theorem provers (ATPs) such as E allow a large number of user-specified proof search strategies described in a rich domain specific language. Several machine learning methods that invent strategies automatically for ATPs were proposed previously. One of them is the Blind Strategymaker (BliStr), a system for automated invention of ATP strategies.", "histories": [["v1", "Sat, 26 Nov 2016 18:48:43 GMT  (68kb,D)", "http://arxiv.org/abs/1611.08733v1", "Submitted to Certified Programs and Proofs (CPP 2017)"]], "COMMENTS": "Submitted to Certified Programs and Proofs (CPP 2017)", "reviews": [], "SUBJECTS": "cs.LO cs.AI cs.LG", "authors": ["jan jakubuv", "josef urban"], "accepted": false, "id": "1611.08733"}, "pdf": {"name": "1611.08733.pdf", "metadata": {"source": "CRF", "title": "BliStrTune: Hierarchical Invention of Theorem Proving Strategies", "authors": ["Jan Jakub\u016fv", "Josef Urban"], "emails": ["jakubuv@gmail.com", "josef.urban@gmail.com", "Mizar@Turing"], "sections": [{"heading": null, "text": "In this paper we introduce BliStrTune \u2013 a hierarchical extension of BliStr. BliStrTune allows exploring much larger space of E strategies by interleaving search for high-level parameters with their fine-tuning. We use BliStrTune to invent new strategies based also on new clause weight functions targeted at problems from large ITP libraries. We show that the new strategies significantly improve E\u2019s performance in solving problems from the Mizar Mathematical Library.\nKeywords Automated Theorem Proving, Machine Learning, Proof Search Heuristics, Clause Weight Functions"}, {"heading": "1. Introduction: ATP Strategy Invention", "text": "State-of-the-art automated theorem provers (ATPs) such as E (Schulz 2002, 2013) and Vampire (Kova\u0301cs and Voronkov 2013) achieve their performance by using sophisticated proof search strategies and their combinations. Constructing good ATP search strategies is a hard task that is potentially very rewarding. Until recently, there has been, however, little research in this direction in the ATP community.\nWith the arrival of large ATP problem sets and benchmarks extracted from the libraries of today\u2019s interactive theorem prover (ITP) systems (Blanchette et al. 2016a,b; Gauthier and Kaliszyk 2015; Kaliszyk and Urban 2014, 2015), automated generation of targeted ATP strategies became an attractive topic. It seems unlikely that manual (\u201ctheorydriven\u201d) construction of targeted strategies can scale to large numbers of ATP problems spanning many different areas of mathematics and computer science. Starting with Blind Strategymaker (BliStr) (Urban 2015) that was used to invent E\u2019s strategies for MaLARea (Urban et al. 2008; Kaliszyk et al. 2015b) on the 2012 Mizar@Turing competition prob-\nlems (Sutcliffe 2013), several systems have been recently developed to invent targeted ATP strategies (Scha\u0308fer and Schulz 2015; Ku\u0308hlwein and Urban 2015). The underlying methods used so far include genetic algorithms and iterated local search, as popularized by the ParamILS (Hutter et al. 2009) system.\nA particular problem of the methods based on iterated local search is that their performance degrades as the number of possible strategy parameters gets high. This is the case for E, where a domain specific language allows construction of astronomic numbers of strategies. This gets worse as more and more sophisticated templates for strategies are added to E, such as our recent family of conjecture-oriented weight functions implementing various notions of term-based similarity (Jakubu\u030av and Urban 2016). The pragmatic solution used in the original BliStr consisted of re-using manually pre-designed high-level strategy components, rather than allowing the system to explore the space of all possible strategies. This is obviously unsatisfactory.\nIn this work we introduce BliStrTune \u2013 a hierarchical extension of BliStr. BliStrTune allows exploring much larger space of E strategies by factoring the search into invention of good high-level strategy components and their low-level fine-tuning. The high-level and low-level inventions communicate to each other their best solutions, iteratively improving all parts of the strategy space. Together with our new conjecture-oriented weight functions, the hierarchical invention produces so far the strongest schedule of strategies on the small (bushy) versions of the Mizar@Turing problems. The improvement over Vampire 4.0 on the training set is nearly 10%, while the improvement on the testing (competition) set is over 5%.\nThe rest of the paper is organized as follows. Section 2 introduces the notion of proof search strategies, focusing on resolution/superposition ATPs and E prover. We also summarize our recent conjecture-oriented strategies that motivated the work on BliStrTune. Section 3 describes the ideas behind the original Blind Strategymaker based on the ParamILS system (see Section 3.1 for more details on ParamILS). Section 4 introduces the hierarchical invention algorithm and its implementation. The system is evaluated\nar X\niv :1\n61 1.\n08 73\n3v 1\n[ cs\n.L O\n] 2\n6 N\nov 2\n01 6\nin several ways in Section 5, showing significant improvements over the original BliStr and producing significantly improved ATP strategies."}, {"heading": "2. Proof Search Strategies", "text": "In this section we briefly describe the proof search of saturation-based automated theorem provers (ATPs). Section 2.1 describes the proof search control possibilities of E prover (Schulz 2002, 2013). Section 2.2 describes our previous development of similarity based clause selection strategies (Jakubu\u030av and Urban 2016) which we make use of and evaluate here.\nMany state-of-the-art ATPs are based on the given clause algorithm introduced by Otter (McCune 1989, 1990, 1994). The input problem T \u222a {\u00acC} is translated into a refutationally equivalent set of clauses. Then the search for a contradiction, represented by the empty clause, is performed maintaining two sets: the set P of processed clauses and the set U of unprocessed clauses. Initially, all the input clauses are unprocessed. The algorithm repeatedly selects a given clause g from U and generates all possible inferences using g and the processed clauses from P . Then, g is moved to P , and U is extended with the newly produced clauses. This process continues until a resource limit is reached, or the empty clause is inferred, or P becomes saturated, that is, nothing new can be inferred."}, {"heading": "2.1 Proof Search Strategies in E Prover", "text": "E (Schulz 2002, 2013) is a state-of-the-art theorem prover which we use as a basis for implementation. The selection of a given clause in E is implemented by a combination of priority and weight functions. A priority function assigns an integer to a clause and is used to pre-order clauses for weight evaluation. A weight function takes additional specific arguments and assigns to each clause a real number called weight. A clause evaluation function (CEF) is specified by a priority function, weight function, and its arguments. Each CEF selects the clause with the smallest pair (priority,weight) for inferences. Each CEF is specified using the syntax\nWeightFunction(PriorityFunction,...)\nwith a variable number of comma separated arguments of the weight function. E allows a user to select an expert heuristic on a command line in the format\n(n1*CEF1,...,nk*CEFk)\nwhere integer ni indicates how often the corresponding CEFi should be used to select the given clause. E additionally supports an auto-schedule mode where several expert heuristics are tried, each for a selected time period. The heuristics and time periods are automatically chosen based on input problem properties.\nOne of the well-performing weight functions in E, which we also use as a reference for evaluation of our weight functions, is the conjecture symbol weight. This weight function counts symbol occurrences with different weights based on their appearance in the conjecture as follows. Different weights \u03b4f, \u03b4c, \u03b4p, and \u03b4v are assigned to function, constant, and predicate symbols, and to variables. The weight of a symbol which appears in the conjecture is multiplied by \u03b3conj, typically \u03b3conj < 1 to prefer clauses with conjecture symbols. To compute a term weight, the given symbol weights are summed for all symbol occurrences. This evaluation is extended to equations and to clauses.\nApart from clause selection, E prover introduces other parameters which influence the choice of the inference rules, term orderings, literal selection, etc. The selected values of the parameters which control the proof search are called a protocol. Because protocol is a crucial notion in this paper, we provide a simple example for reader\u2019s convenience.\nEXAMPLE 1. Let us consider the following simplified E protocol written in E prover command line syntax as follows.\n-tKBO6 -WSelectComplexG\n-H\u2019(13*Refinedweight(PreferGoals,1,2,2,3,2),\n2*Clauseweight(ByCreationDate,-2,-1,0.5))\u2019\nThis protocol selects term ordering KBO6, literal selection function SelectComplexG, and two CEFs. The first CEF has frequency 13, weight function Refinedweight, priority function PreferGoals, and weight function arguments \u201c1,2,2,3,2\u201d. An exact meaning of specific protocol parameters can be found in E manual (Schulz 2013)."}, {"heading": "2.2 Similarity Based Clause Selection Strategies", "text": "Many of the best-performing weight functions in E are based on a similarity of a clause with the conjecture, for example, the conjecture symbol weight from the previous section. A natural question arises whether or not it makes sense to extend the symbol-based similarity to more complex termbased similarities. Previously we proposed (Jakubu\u030av and Urban 2016), implemented, and evaluated several weight functions which utilize conjecture similarity in different ways. Typically they extend the symbol-based similarity by similarity on terms. Using finer formula features improves the high-level premise selection task (Kaliszyk et al. 2015a), which motivated us on steering also the internal selection in E. The following sections summarizes the new weight functions which we further evaluate later in Section 5.1 and Section 5.3.\n2.2.1 Conjecture Subterm Weight (Term) The first of our weight functions is similar to the standard conjecture symbol weight, counting instead of symbols the number of subterms a term shares with the conjecture. The clause weight function Term takes five specific arguments \u03b3conj, \u03b4f, \u03b4c, \u03b4p and \u03b4v. The weight of a term equals weight \u03b4f for functional terms, \u03b4c for constants, \u03b4p for predicates, and\n\u03b4v for variables, possibly multiplied by \u03b3conj when t appears in the conjecture. To compute a clause weight, terms weights are summed for all subterms from a clause.\n2.2.2 Conjecture Frequency Weight (TfIdf) Term frequency \u2013 inverse document frequency, is a numerical statistic intended to reflect how important a word is to a document in a corpus (Leskovec et al. 2014). A term frequency is the number of occurrences of the term in a given document. A document frequency is the number of documents in a corpus which contain the term. The term frequency is typically multiplied by the logarithm of the inverse of document frequency to reduce frequency of terms which appear often. We define tf(t) as the number of occurrences of t in a conjecture. We consider a fixed set of clauses denoted Docs. We define df(t) as the count of clauses from Docs which contain t. Out weight function TfIdf takes one specific argument \u03b4doc to select documents, either (1) ax for the axioms (including the conjecture) or (2) pro for all the processed clauses. First we define the value tfidf(t) of term t as follows.\ntfidf(t) = tf(t) \u2217 log 1 + |Docs| 1 + df(t)\nThe weight of term t is computed as 11+tfidf(t) and extended to clauses.\n2.2.3 Conjecture Term Prefix Weight (Pref) The previous weight functions rely on an exact match of a term with a conjecture related term. The following weight function loosen this restriction and consider also partial matches. We consider terms as symbol sequences. Let max-pref(t) be the longest prefix t shares with a conjecture term. A term prefix weight (Pref) counts the length of max-pref(t) using weight arguments \u03b4match and \u03b4miss. These are used to define the weight of term t as follows.\n\u03b4match \u2217 |max-pref(t)|+ \u03b4miss \u2217 (|t| \u2212 |max-pref(t)|)\n2.2.4 Conjecture Levenshtein Distance Weight (Lev) A straightforward extension of Pref is to employ the Levenshtein distance (Levenshtein 1966) which measures a distance of two strings as the minimum number of edit operations (character insertion, deletion, or change) required to change one word into the other. Our weight function Lev defines the weight of term t as the minimal Levenshtein distance from t to some conjecture term. It takes additional arguments \u03b4ins, \u03b4del, \u03b4ch to assign different costs for edit operations.\n2.2.5 Conjecture Tree Distance Weight (Ted) The Levenshtein distance does not respect a tree structure of terms. To achieve that, we implement the Tree edit distance (Zhang and Shasha 1989) which is similar to Levenshtein but uses tree editing operations (inserting a node into a tree, deleting a node while reconnecting its child nodes to the deleted position, and renaming a node label). Our weight\nfunction Ted takes the same arguments as Lev above and term weight is defined similarly.\n2.2.6 Conjecture Structural Distance Weight (Struc) With Ted, a tree produced by the edit operations does not need to represent a valid term as the operations can change number of child nodes. To avoid this we define a simple structural distance which measures a distance of two terms by a number of generalization and instantiation operations. Generalization transforms an arbitrary term to a variable while instantiation does the reverse. Our weight function Struc takes additional arguments \u03b4miss, \u03b3inst, and \u03b3gen as penalties for variable mismatch and operation costs. The distance of a variable x to a term t is the cost of instantiating x by t, computed as \u2206Struc(x, t) = \u03b3inst \u2217 |t|. The distance of t to x is defined similarly but with \u03b3gen. A distance of non-variable terms t and s which share the top-level symbol is the sum of distances of the corresponding arguments. Otherwise, a generic formula \u2206Struc(t, x0) + \u2206Struc(x0, s) is used. The term weight is as for Lev but using \u2206Struc."}, {"heading": "3. Blind Strategymaker (BliStr)", "text": "In this section we describe Blind Strategymaker (BliStr) (Urban 2015) which we further extend in the following section. BliStr is a system that develops E prover protocols targeted for a given large set of problems. The main idea is to interleave (i) iterated low-timelimit local search for new protocols on small sets of similar easy problems with (ii) highertimelimit evaluation of the new protocols on all problems. The accumulated results of the global higher-timelimit runs are used to define and evolve the notion of similar easy problems, and to control the selection of the next protocol to be improved.\nThe main criterion for BliStr is as follows.\nCRITERION 1 (Max). Invent a set of E protocols that together solve as many of the given benchmark problems.\nTo ensure that the invented protocols perform well also on unknown but related problems a second criterion is considered.\nCRITERION 2 (Gen). The protocols should be reasonably general.\nTo simplify employment of the invented protocols, BliStr tries to achieve also the third criterion.\nCRITERION 3 (Size). The set of such protocols should not be too large.\nAs defined earlier, E protocols consist of many parameters and their values which influence the proof search. A huge number of weight function arguments within clause evaluation functions (CEFs, see Section 2.1) makes the set of meaningful protocol parameters very large for a straightforward use of iterative local search as done by the ParamILS (Hutter et al. 2009) system. Since ParamILS oth-\nerwise looks like the right tool for the task, a data-driven (\u201cblind\u201d) approach was applied in the original BliStr to get a smaller set of meaningful CEFs: the existing E protocols that were most useful on benchmarks of interest were used to extract a smaller set (a dozen) of CEFs. Making this CEFs choice more \u201cblind\u201d is the main contribution of this work and it is discussed in details in Section 4.\nEven after such reduction, the space of the protocol parameter-value combinations is so large that a random exploration seems unlikely to find good new protocols. The guiding idea in BliStr is to use again a data-driven approach. Problems in a given mathematical field often share a lot of structure and solution methods. Mathematicians become better and better by solving the problems, they become capable of doing larger and larger steps with confidence, and as a result they can gradually attack problems that were previously too hard for them. By this analogy, it is plausible to think that if the solvable problems become much easier for an ATP system, the system will be able to solve some more (harder, but related) problems. For this to work, a method that can improve an ATP on a set of solvable problems is needed. As already mentioned, the established ParamILS system can be used for this."}, {"heading": "3.1 ParamILS and Its Use in the BliStr Loop", "text": "Let A be an algorithm whose parameters come from a configuration space (product of possible values) \u0398. A parameter configuration is an element \u03b8 \u2208 \u0398, and A(\u03b8) denotes the algorithm A with the parameter configuration \u03b8. Given a distribution (set) of problem instances D, the algorithm configuration problem is to find the parameter configuration \u03b8 \u2208 \u0398 resulting in the best performance of A(\u03b8) on the distribution D. ParamILS is an a implementation of an iterated local search (ILS) algorithm for the algorithm configuration problem. In short, starting with an initial configuration \u03b80, ParamILS loops between two steps: (i) perturbing the configuration to escape from a local optimum, and (ii) iterative improvement of the perturbed configuration. The result of step (ii) is accepted if it improves the previous best configuration.\nTo fully determine how to use ParamILS in a particular case, A, \u0398, \u03b80, D, and a performance metric need to be instantiated. In our case, A is E run with a low timelimit tcutoff, \u0398 is the set of expressible E protocols, and as a performance metric we use the number of given-clause loops done by E during solving the problem. If E cannot solve a problem within the low timelimit, a sufficiently high value (106) is used. Since it is unlikely that there is one best E protocol for all of the given benchmark problems, it would be counterproductive to use all problems as the set D for ParamILS runs. Instead, BliStr partitions the set of all solvable problems into subsets on which the particular protocols perform best. See (Urban 2015) for the technical details of the BliStr heuristic for choosing the successive \u03b80 and D. The complete BliStr loop then iteratively co-evolves\nthe set of protocols, the set of solved problems, the matrix of the best results, and the set of the protocols eligible for the ParamILS improvement together with their problem sets."}, {"heading": "4. BliStrTune: Hierarchical Invention", "text": "BliStr uses a fixed set of CEFs for inventing new protocols. The arguments of these fixed CEFs (the priority function, weight function arguments) cannot be modified during the iterative protocol improvement done by ParamILS. A straightforward way to achieve invention (fine-tuning) of CEF arguments would be to extend the ParamILS configuration space \u0398. This, however, makes the configuration space grow from ca. 107 to 10120 of possible combinations. Preliminary experiments revealed that with a configuration space of this size ParamILS does not produce satisfactory results in a reasonable time.\nIn this section we describe our new extension of BliStr \u2013 BliStrTune \u2013 where the invention of good high-level protocol parameters (Section 4.1) is interleaved with the invention of good CEF arguments (Section 4.2). The basic idea behind BliStrTune is iterated hierarchical invention: The large space of the optimized parameters is naturally factored into two (in general several) layers, and at any time only one layer is subjected to invention, while the other layer(s) remain fixed. The results then propagate between the layers, and the layer-tuning and propagation are iterated. BliStrTune is experimentally evaluated in Section 5."}, {"heading": "4.1 Global Parameter Invention", "text": "The ParamILS runs used in the BliStrTune\u2019s global-tuning phase are essentially the same as in the case of BliStr, with the following minor exceptions. BliStr uses a fixed configuration space \u0398 for all ParamILS runs. This is possible because a small set (currently 12) of CEFs is hard coded in Blistr\u2019s \u0398. BliStrTune uses in the global-tuning phase a parametrized configuration space \u0398C where C is a collection of CEFs that can be different for each ParamILS run. This collection can be arbitrary but we use only the 50 best performing CEFs in order to limit the configuration space size for the global-tuning phase. The notion of \u201cbest performing CEFs\u201d develops in time and it is discussed in details in Section 4.3. Furthermore, BliStrTune introduces additional argument ccef to limit the maximum number of CEFs which can occur in a single protocol (ccef = 12 for the case of BliStr).\nBliStrTune\u2019s global-tuning usage of ParamILS is otherwise the same as in BliStr, that is, given \u0398C , the initial configuration \u03b80 \u2208 \u0398C , and problemsD, the result of the global tuning is a configuration \u03b81 \u2208 \u0398C which has the best found performance on D. This configuration \u03b81 then serves as an input for the next fine-tuning phase.\nEXAMPLE 2. Let us consider the E protocol from Example 1. In the global-tuning phase we instruct ParamILS to modify top level arguments, that is, term ordering (\u201c-t\u201d), literal selection (\u201c-W\u201d), CEF frequencies (\u201c13*\u201d and \u201c2*\u201d), and also the whole CEF blocks and their count. We do not, however, allow ParamILS to change CEF arguments (priority functions and weight function arguments). The whole CEF must be changed to another CEF from collection C."}, {"heading": "4.2 Invention of the CEF Arguments", "text": "Given the result of the global-tuning phase \u03b81 \u2208 \u0398C a new configuration space for the fine-tuning phase \u0398\u03b81 is constructed by (1) fixing the parameter values from \u03b81 and by (2) an introduction of new parameters that allow to change the values of the arguments of the CEFs used in \u03b81. In order to do that, we need to describe the space of the possible values of the CEF arguments.\nThe CEF arguments (see Section 2.1) consist of the priority function and the weight function specific arguments. Because of the different number and semantics of the weight function arguments, we do not allow to change the CEF\u2019s weight functions during the fine-tuning. They are fixed to the values provided in \u03b81. For each weight function argument, we know its type (such as the symbol weight, operation cost, weight multiplier, etc.). For each type we have pre-designed the set of reasonable values. For the original E weight functions, we extract the reasonable values from the auto-schedule mode of E. For our new weight functions, we use our preliminary experiments (Jakubu\u030av and Urban 2016) enhanced with our intuition.\nGiven the configuration space \u0398\u03b81 , a configuration \u03b81 \u2208 \u0398C can be easily converted to an equivalent configuration \u03b8\u20321 \u2208 \u0398\u03b81 by setting the parameter values to those CEFs arguments that were previously fixed in \u03b81 and C. Then we can run ParamILS with the configuration space \u0398\u03b81 , the initial configuration \u03b8\u20321, and with the same problem set D as in the global-tuning phase. The result is a configuration \u03b8\u20322 \u2208 \u0398\u03b81 providing the best found performance on D.\nThe global invention (global tuning) and the local invention (fine-tuning) phases can be iterated. To do that, we need to transform the result of the fine-tuning \u03b8\u20322 \u2208 \u0398\u03b81 to an equivalent initial configuration \u03b82 \u2208 \u0398C for the next globaltuning phase. In order to do that, the CEFs invented by \u03b8\u20322 must be present in the CEFs collection C. If this is not the case, we simply extendC with the new CEFs. In practice, we now use two iterations of this process (that is, two phases of global-tuning and two phases of fine-tuning) which was experimentally evaluated to provide good results.\nEXAMPLE 3. Recall the protocol from Example 1 and Example 2. In the fine-tuning phase we would fix all the top level arguments modified in global-tuning phase (\u201c-t\u201d, and so on, as described in Example 2) and we would instruct ParamILS to change individual CEF arguments. That is, the values\nPreferGoals,1,2,2,3,2\nByCreationDate,-2,-1,0.5\nmight be changed to different values while the rest of the protocol stays untouched."}, {"heading": "4.3 Maintaining Collections of CEFs", "text": "The global-tuning phase of BliStrTune requires the collection C of CEFs as an input. It is desirable that this collection C is limited in size (currently we use max. 50 CEFs) and that it contains the best performing CEFs.\nInitially, for each weight functionw defined in E, we have extracted the CEF most often used in the E auto-schedule mode. We have added a CEF for each of our new weight functions. This gave us the initial collection of 21 CEFs. Then we use a global database (shared by different BliStrTune runs) in which we store all CEFs together with the usage counter which states how often each CEF was used in a protocol invented by BliStrTune. Recall that in one BliStrTune iteration, ParamILS is ran four times (two phases of global-tuning and two phases of fine-tuning). Whenever a CEF is contained in a protocol invented by any BliStrTune iteration (after the four ParamILS runs), we increase the CEF usage counter, perhaps adding a new CEF to the database when used for the first time.\nTo select the 50 best performing CEFs we start with C = \u2205. We extract all the weight functions W used in the global CEF database. This set W stays constant because the database already contains all possible weight functions from the very beginning. For each w \u2208 W , we compute the list Cw of all CEFs from the database which use w and sort it by the usage counter. Then we iterate over W and for each w we move the most often used CEF from Cw to C. We repeat this until C has the desirable size (or we are out CEFs). This ensures that C contains at least one CEF for each weight function."}, {"heading": "5. Experimental Evaluation", "text": "This section provides an experimental evaluation1of BliStrTune system. In Section 5.1 we compare our improved BliStrTune with the original BliStr, and we use BliStrTune to evaluate the value added by the new weight functions. In Section 5.2 we evaluate the BliStrTune runs with different parameters. In Section 5.3 we discuss and compare several methods to construct a protocol scheduler that tries several protocols to solve a problem. Section 5.4 then compares the best protocol scheduler with state-of-the-art ATPs, namely, with E 1.9 using its auto-schedule mode and with Vampire 4.0.\nFor the evaluation we use problems from the Mizar@Turing division of the CASC 2012 (Turing100) competition mentioned in Section 1. These problems come from the MPTP\n1 All the experiments were run on 2x16 cores Intel(R) Xeon(R) CPU E52698 v3 @ 2.30GHz with 128 GB memory. One prover run was however limited to 1 GB memory limit.\ntranslation (Urban 2004, 2006; Alama et al. 2014) of the Mizar Mathematical Library (Grabowski et al. 2010). The problems are divided into 1000 training and 400 testing problems. The training problems were published before the competition, while the testing problems were used in the competition. This fits our evaluation setting: we can use BliStrTune to invent targeted protocols for the training problems and then evaluate them on the testing problems."}, {"heading": "5.1 Hierarchical Invention and Weight Functions", "text": "To evaluate the hierarchical invention we ran BliStr and BliStrTune with equivalent arguments. Furthermore, we ran two instances of BliStrTune to evaluate the performance added by the new weight functions from Section 2.2. The first instance was allowed to use only the original E 1.9 weight functions, while the second additionally used our new weight functions.\nBliStr and BliStrTune used the same input arguments. The first argument is the set of the training problems. We use the 1000 training problems from the Mizar@Turing competition in all experiments. Other arguments are:\nTimprove the time limit (seconds) for one ParamILS run,\ntcutoff the time limit for E prover runs within ParamILS,\nteval the time limit for the protocol evaluation in BliStr/Tune.\nIn BliStrTune, ParamILS is run four times in each iteration, hence we set Timprove = 100 in BliStrTune and Timprove = 400 in BliStr. 2 We set tcutoff = 1 and teval = 5 and additionally, in the case of BliStrTune, ccef = 6.\nThe results are shown in Figure 1. In each iteration (xaxis, logarithmic scale) we count the total number of the training problems solved (y-axis) by all the protocols invented so far, provided each protocol is given the time\n2 So that the times used to improve a protocol are equal.\nlimit teval. This metric gives us relatively good idea of the BliStr/Tune progress.\nThe original BliStr solved 673 problems, BliStrTune without the new weights solved 702 problems, while BliStrTune with the new weights solved 711 problems. From this and from the figure we can see that the greatest improvement is thanks to the hierarchical parameter invention. However, the new weight functions still provide 9 more solved problems which is a useful additional improvement."}, {"heading": "5.2 Influence of the BliStrTune Input Arguments", "text": "In this section we evaluate several BliStrTune runs with different input arguments. We run all the combinations of Timprove \u2208 {100, 300} and ccef \u2208 {6, 10} and tcutoff \u2208 {1, 2}. This gives us 6 different BliStrTune runs. We always set teval = 5 \u00b7 tcutoff.\nThe results are summarized in Table 1. Column iters contains the number of iterations executed by the appropriate BliStrTune run, proto is the total number of protocols generated, run time is the total run time of the given BliStrTune run, best proto is the number of training problems solved by the best protocol within teval time limit, and solved is the total number of the training problems solved by all the generated protocols, provided each protocol is given time limit teval. We can see that a huge amount protocols were generated. Only few of them were used for the final evaluation as described in Section 5.3. Those used for the final evaluation are considered \u201cuseful\u201d and the column useful states how many percent of the useful protocols come from the appropriate BliStrTune run.\nWe can see that the most useful runs are the basic runs with smaller Timprove which also have lower run times. Higher Timprove leads to higher run times but it produces better protocols in the sense that a smaller number of protocols can solve equal number of problems. From the table we can see that when tcutoff and ccef are increased, Timprove should be increased as well to provide ParamILS enough time for protocol improvement."}, {"heading": "5.3 Selecting Best Protocol Scheduler", "text": "The 6 runs of BliStrTune described above in Section 5.2 generated more than 900 different protocols. In this section we try to select the best subset of protocols and construct a protocol scheduler which sequentially tries several protocols to solve a problem. We only experiment with the simplest schedulers where the time limit for solving a problem is equally distributed among all the protocols within a scheduler. Hence the problem of scheduler construction is reduced to the selection of the right protocols.\nWe use three different ways to select scheduler protocols. Firstly we use a greedy approach as follows. We evaluate all the protocols on all the training problems with a fixed time limit t. Then we construct a greedy covering sequence which starts with the best protocol, and each next protocol in the sequence is the protocol that adds most solutions to\nthe union of problems solved by all previous protocols in the sequence. The resulting scheduler is denoted greedyt.\nSecond way to construct a scheduler is using state-of-theart contribution (SOTAC) used by CASC. A SOTAC for the problem is the inverse of the number of protocols that solved the problem. A protocol SOTAC is the average SOTAC over the problems it solves. We can sort the protocols by SOTAC and select first n protocols from this sequence. The resulting scheduler is denoted SOTACn.\nSOTAC of a protocol will be high even if the protocol solves only one problem which no other protocol can solve. That is why also the \u03a3-SOTAC value (Kaliszyk and Urban 2014) is introduced: the sum of problem SOTAC over all the problems. This gives us schedulers denoted \u03a3-SOTACn.\nThe evaluation of 12 different schedulers with 60 seconds time limit on the training problems is provided in Table 2.\nColumn protos specifies the count of protocols within the scheduler. We shall use this evaluation to select the best scheduler, hence the results on the 400 testing problems are provided for reference only. Column solved is the number of problems solved in 60s. Column V+ is a percentage gain/lost on a state-of-the-art prover Vampire 4.0 which solves 667 of the 1000 training problems and 266 of the 400 testing problems.\nWe can see that the best results are achieved by scheduler greedy1, which also gives the best results on the testing problems. Generally, it is better to run a bigger number of protocols with lower individual time limit.\nFurthermore, we can use the constructed schedulers to evaluate the contribution of our new weight functions by analyzing weight functions used in the schedulers. Table 4 summarizes the usage of different weight functions in the final schedulers. Our weight functions are referred to by their names from Section 2.2 while the original weights are called by their E prover names. Column count states how many times the corresponding weight function was used in some scheduler protocol, while column freq sums the frequencies of occurrences of CEFs which use the given weight function. We can see that our new weight function Term was the most often used weight function. Four of our weight functions were, however, not used very often which we attribute to their higher time complexity."}, {"heading": "5.4 Best Protocol Scheduler Evaluation", "text": "In this section we evaluate the best protocol scheduler greedy1 selected in previous Section 5.3 on the testing problems with 60 seconds time limit. We compare greedy1 with two state-of-the-art ATPs: (1) with E prover 1.9 in autoschedule mode and (2) with Vampire 4.0 in CASC mode.\nThe results are summarized in Table 3. We can see that E with scheduler greedy1 invented by BliStrTune outperforms Vampire by 5.2% and the improvement from E in auto-schedule mode is even more significant. Figure 2 provides a graphical representation of ATP\u2019s progress. For each second (x-axis, logarithmic scale) we count the number of problems solved so far (y-axis). We can see that greedy1 was outperforming Vampire during the whole evaluation."}, {"heading": "6. Conclusions and Future Work", "text": "In this paper we have described BliStrTune, an extension of a previously published system BliStr, which can be used for hierarchical invention of protocols targeted for a given benchmark problems. The main contribution of BliStrTune is that it considers a much bigger space of protocols by interleaving the global-tuning phase with argument fine-tuning. We have evaluated the original BliStr and our BliStrTune on the same input data and experimentally proved that BliStrTune outperforms BliStr. We have evaluated several ways of creating protocol schedulers and showed that E 1.9 with the best protocol scheduler constructed from BliStrTune protocols targeted for training problems outperforms state-ofthe-art ATP Vampire 4.0 on independent testing problems by more than 5%.\nFurthermore, we have used BliStrTune to evaluate a contribution of our previously designed weight functions in E prover. We have shown that the new weight functions allow us to solve more problems and that (at least two of them) were often used in the best scheduler protocols. Interest-\ningly, more complex structural weights (like Lev, Ted) were not used very often in the schedulers even though our previous experiments suggested they might be very useful. We attribute this to their higher time complexity and we would like to investigate this in our future research.\nSeveral topics are suggested for future work. We have shown that new weight functions can enhance E prover performance, hence more weight functions which consider term structure could be implemented. It seems that it will be better to design weight functions with lower time complexity, perhaps even providing approximate results (for example, some approximation of the Levenshtein distance which could be computed faster).\nAnother direction of our future research is to design more complex protocol schedulers. We have achieved good results with the simplest protocol schedulers where each protocol is given an equal amount of time when solving a problem. It would be interesting to design \u201csmarter\u201d schedulers and to see how many more problems can be solved.\nFurther direction of our future research are enhancements of the BliStr/Tune main loop. We could experiment with settings of various parameters, or with selection of training problems, or we could use parameter improvement methods other than ParamILS (Wang et al. 2016). Finally, we would like to make our implementation easier to use and to distribute it as a solid software package."}, {"heading": "Acknowledgments", "text": "Supported by the ERC Consolidator grant number 649043 AI4REASON."}], "references": [{"title": "Premise selection for mathematics by corpus analysis and kernel methods", "author": ["J. Alama", "T. Heskes", "D. K\u00fchlwein", "E. Tsivtsivadze", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Alama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Alama et al\\.", "year": 2014}, {"title": "Hammering towards QED", "author": ["J. Blanchette", "C. Kaliszyk", "L. Paulson", "J. Urban"], "venue": "Journal of Formalized Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "A learning-based fact selector for Isabelle/HOL", "author": ["J.C. Blanchette", "D. Greenaway", "C. Kaliszyk", "D. K\u00fchlwein", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Blanchette et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "Premise selection and external provers for HOL4", "author": ["T. Gauthier", "C. Kaliszyk"], "venue": "LNCS. Springer,", "citeRegEx": "Gauthier and Kaliszyk.,? \\Q2015\\E", "shortCiteRegEx": "Gauthier and Kaliszyk.", "year": 2015}, {"title": "Mizar in a nutshell", "author": ["A. Grabowski", "A. Korni\u0142owicz", "A. Naumowicz"], "venue": "J. Formalized Reasoning,", "citeRegEx": "Grabowski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grabowski et al\\.", "year": 2010}, {"title": "ParamILS: an automatic algorithm configuration framework", "author": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown", "T. St\u00fctzle"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "Hutter et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2009}, {"title": "Extending E prover with similarity based clause selection strategies", "author": ["J. Jakub\u016fv", "J. Urban"], "venue": "In Intelligent Computer Mathematics - 9th International Conference,", "citeRegEx": "Jakub\u016fv and Urban.,? \\Q2016\\E", "shortCiteRegEx": "Jakub\u016fv and Urban.", "year": 2016}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2014\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2014}, {"title": "MizAR 40 for Mizar 40", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Kaliszyk and Urban.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk and Urban.", "year": 2015}, {"title": "Efficient semantic features for automated reasoning over large theories", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": null, "citeRegEx": "Kaliszyk et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk et al\\.", "year": 2015}, {"title": "Machine learner for automated reasoning", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": "PAAR-2014. 4th Workshop on Practical Aspects of Automated Reasoning,", "citeRegEx": "Kaliszyk et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kaliszyk et al\\.", "year": 2015}, {"title": "First-order theorem proving and Vampire", "author": ["L. Kov\u00e1cs", "A. Voronkov"], "venue": "CAV, volume 8044 of LNCS,", "citeRegEx": "Kov\u00e1cs and Voronkov.,? \\Q2013\\E", "shortCiteRegEx": "Kov\u00e1cs and Voronkov.", "year": 2013}, {"title": "MaLeS: A framework for automatic tuning of automated theorem provers", "author": ["D. K\u00fchlwein", "J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "K\u00fchlwein and Urban.,? \\Q2015\\E", "shortCiteRegEx": "K\u00fchlwein and Urban.", "year": 2015}, {"title": "Mining of Massive Datasets, 2nd Ed", "author": ["J. Leskovec", "A. Rajaraman", "J.D. Ullman"], "venue": null, "citeRegEx": "Leskovec et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Leskovec et al\\.", "year": 2014}, {"title": "Binary Codes Capable of Correcting Deletions, Insertions and Reversals", "author": ["V. Levenshtein"], "venue": "Soviet Physics Doklady,", "citeRegEx": "Levenshtein.,? \\Q1966\\E", "shortCiteRegEx": "Levenshtein.", "year": 1966}, {"title": "Otter 3.0 reference manual and guide, volume 9700", "author": ["W.W. McCune"], "venue": "Argonne National Laboratory Argonne,", "citeRegEx": "McCune.,? \\Q1994\\E", "shortCiteRegEx": "McCune.", "year": 1994}, {"title": "Breeding theorem proving heuristics with genetic algorithms", "author": ["S. Sch\u00e4fer", "S. Schulz"], "venue": "Global Conference on Artificial Intelligence,", "citeRegEx": "Sch\u00e4fer and Schulz.,? \\Q2015\\E", "shortCiteRegEx": "Sch\u00e4fer and Schulz.", "year": 2015}, {"title": "E \u2013 a brainiac theorem prover", "author": ["S. Schulz"], "venue": "AI Communications,", "citeRegEx": "Schulz.,? \\Q2002\\E", "shortCiteRegEx": "Schulz.", "year": 2002}, {"title": "The 6th IJCAR automated theorem proving system competition - CASC-J6", "author": ["G. Sutcliffe"], "venue": "AI Commun.,", "citeRegEx": "Sutcliffe.,? \\Q2013\\E", "shortCiteRegEx": "Sutcliffe.", "year": 2013}, {"title": "MPTP - Motivation, Implementation, First Experiments", "author": ["J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Urban.,? \\Q2004\\E", "shortCiteRegEx": "Urban.", "year": 2004}, {"title": "MPTP 0.2: Design, implementation, and initial experiments", "author": ["J. Urban"], "venue": "J. Autom. Reasoning,", "citeRegEx": "Urban.,? \\Q2006\\E", "shortCiteRegEx": "Urban.", "year": 2006}, {"title": "BliStr: The Blind Strategymaker", "author": ["J. Urban"], "venue": "GCAI", "citeRegEx": "Urban.,? \\Q2015\\E", "shortCiteRegEx": "Urban.", "year": 2015}, {"title": "MaLARea SG1 - Machine Learner for Automated Reasoning with Semantic Guidance", "author": ["J. Urban", "G. Sutcliffe", "P. Pudl\u00e1k", "J. Vysko\u010dil"], "venue": "IJCAR, volume 5195 of LNCS,", "citeRegEx": "Urban et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Urban et al\\.", "year": 2008}, {"title": "Simple fast algorithms for the editing distance between trees and related problems", "author": ["K. Zhang", "D. Shasha"], "venue": "SIAM J. Comput.,", "citeRegEx": "Zhang and Shasha.,? \\Q1989\\E", "shortCiteRegEx": "Zhang and Shasha.", "year": 1989}], "referenceMentions": [{"referenceID": 22, "context": "Starting with Blind Strategymaker (BliStr) (Urban 2015) that was used to invent E\u2019s strategies for MaLARea (Urban et al. 2008; Kaliszyk et al. 2015b) on the 2012 Mizar@Turing competition problems (Sutcliffe 2013), several systems have been recently developed to invent targeted ATP strategies (Sch\u00e4fer and Schulz 2015; K\u00fchlwein and Urban 2015).", "startOffset": 107, "endOffset": 149}, {"referenceID": 5, "context": "The underlying methods used so far include genetic algorithms and iterated local search, as popularized by the ParamILS (Hutter et al. 2009) system.", "startOffset": 120, "endOffset": 140}, {"referenceID": 13, "context": "Term frequency \u2013 inverse document frequency, is a numerical statistic intended to reflect how important a word is to a document in a corpus (Leskovec et al. 2014).", "startOffset": 140, "endOffset": 162}, {"referenceID": 5, "context": "1) makes the set of meaningful protocol parameters very large for a straightforward use of iterative local search as done by the ParamILS (Hutter et al. 2009) system.", "startOffset": 138, "endOffset": 158}, {"referenceID": 0, "context": "translation (Urban 2004, 2006; Alama et al. 2014) of the Mizar Mathematical Library (Grabowski et al.", "startOffset": 12, "endOffset": 49}, {"referenceID": 4, "context": "2014) of the Mizar Mathematical Library (Grabowski et al. 2010).", "startOffset": 40, "endOffset": 63}], "year": 2016, "abstractText": "Inventing targeted proof search strategies for specific problem sets is a difficult task. State-of-the-art automated theorem provers (ATPs) such as E allow a large number of userspecified proof search strategies described in a rich domain specific language. Several machine learning methods that invent strategies automatically for ATPs were proposed previously. One of them is the Blind Strategymaker (BliStr), a system for automated invention of ATP strategies. In this paper we introduce BliStrTune \u2013 a hierarchical extension of BliStr. BliStrTune allows exploring much larger space of E strategies by interleaving search for high-level parameters with their fine-tuning. We use BliStrTune to invent new strategies based also on new clause weight functions targeted at problems from large ITP libraries. We show that the new strategies significantly improve E\u2019s performance in solving problems from the Mizar Mathematical Library.", "creator": "LaTeX with hyperref package"}}}