{"id": "1611.09534", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy % over both networks on a real-world large-scale product classification dataset that we collected fromWalmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "histories": [["v1", "Tue, 29 Nov 2016 09:05:11 GMT  (1490kb,D)", "http://arxiv.org/abs/1611.09534v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["tom zahavy", "alessandro magnani", "abhinandan krishnan", "shie mannor"], "accepted": false, "id": "1611.09534"}, "pdf": {"name": "1611.09534.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["IN E-COMMERCE", "Tom Zahavy", "Shie Mannor", "Abhinandan Krishnan"], "emails": ["AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Product classification is a key issue in e-commerce domains. A product is typically represented by metadata such as its title, image, color, weight and so on, and most of them are assigned manually by the seller. Once a product is uploaded to an e-commerce website, it is typically placed in multiple categories. Categorizing products helps e-commerce websites to provide costumers a better shopping experience, for example by efficiently searching the products catalog or by developing recommendation systems. A few examples of categories are internal taxonomies (for business needs), public taxonomies (such as groceries and office equipment) and the product\u2019s shelf (a group of products that are presented together on an e-commerce web page). These categories vary with time in order to optimize search efficiency and to account for special events such as holidays and sports events. In order to address these needs, e-commerce websites typically hire editors and use crowdsourcing platforms to classify products. However, due to the high amount of new products uploaded daily and the dynamic nature of the categories, machine learning solutions for product classification are very appealing as means to reduce the time and economic costs. Thus, precisely categorizing items emerges as a significant issue in e-commerce domains.\nA shelf is a group of products presented together on an e-commerce website page, and usually contain products with a given theme/category (e.g., Women boots, folding tables). Product to shelf classification is a challenging problem due to data size, category skewness, and noisy metadata and labels. In particular, it presents three fundamental challenges for machine learning algorithms. First, it is typically a multi-class problem with thousands of classes. Second, a product may belong to multiple shelves making it a multi-label problem. And last, a product has both an image and a text input making it a multi-modal problem.\nProducts classification is typically addressed as a text classification problem because most metadata of items are represented as textual features (Pyo et al., 2010). Text classification is a classic topic for natural language processing, in which one needs to assign predefined categories to text inputs.\nar X\niv :1\n61 1.\n09 53\n4v 1\n[ cs\n.C V\n] 2\n9 N\nov 2\nStandard methods follow a classical two-stage scheme of extraction of (handcrafted) features, followed by a classification stage. Typical features include bag-of-words or n-grams, and their TF-IDF. On the other hand, Deep Neural Networks use generic priors instead of specific domain knowledge (Bengio et al., 2013) and have been shown to give competitive results on text classification tasks (Zhang et al., 2015). In particular, Convolutional neural networks (CNNs) (Kim, 2014; Zhang et al., 2015; Conneau et al., 2016) and Recurrent NNs (Lai et al., 2015; Pyo et al., 2010; Xiao & Cho, 2016) can efficiently capture the sequentiality of the text. These methods are typically applied directly to distributed embedding of words (Kim, 2014; Lai et al., 2015; Pyo et al., 2010) or characters (Zhang et al., 2015; Conneau et al., 2016; Xiao & Cho, 2016), without any knowledge on the syntactic or semantic structures of a language. However, all of these architectures were only applied on problems with a small amount of labels (\u223c 20) while e-commerce shelf classification problems typically have thousands of labels with multiple labels per product.\nIn Image classification, CNNs are widely considered the best models, and achieve state-of-theart results on the ImageNet Large-Scale Visual Recognition Challenge (Russakovsky et al., 2015; Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015). However, as good as they are, the classification accuracy of machine learning systems is often limited in problems with many classes of object categories. One remedy is to leverage data from other sources, such as text data. However, the studies on multi-modal deep learning for large-scale item categorization are still rare to the best of our belief. In particular in a setting where there is a significant difference in discriminative power between the two types of signals.\nIn this work, we propose a multi-modal deep neural network model for product classification. Our design principle is to leverage the specific prior for each data type by using the current state-of-\nthe-art classifiers from the image and text domains. The final architecture has 3 main components (Figure 2, Right): a text CNN (Kim, 2014), an image CNN (Simonyan & Zisserman, 2014) and a policy network that learns to choose between them. We collected a large-scale data set of 1.2 million products from the Walmart.com website. Each product has a title and an image and needs to be classified to a shelf (label) with 2890 possible shelves. Examples from this dataset can be seen in Figure 1 and are also available on-line at the Walmart.com website. For most of the products, both the image and the title of each product contain relevant information for customers. However, it is interesting to observe that for some of the products, both input types may not be informative for shelf prediction (Figure 1). This observation motivates our work and raises interesting questions: which input type is more useful for product classification? is it possible to forge the inputs into a better architecture?\nIn our experiments, we show that the text CNN outperforms the image one. However, for a relatively large number of products (\u223c 8%), the image CNN is correct while the text CNN is wrong, indicating a potential gain from using a multi-modal architecture. We also show that the policy is able to choose between the two models and give a performance improvement over both state-of-the-art networks.\nTo the best of our knowledge, this is the first work that demonstrates a performance improvement on top-1 classification accuracy by using images and text on a large-scale classification problem. In particular, our main contributions are:\n\u2022 We demonstrate that the text classification CNN (Kim, 2014) outperforms the VGG network (Simonyan & Zisserman, 2014) on a real-world large-scale product to shelf classification problem.\n\u2022 We analyze the errors made by the different networks and show the potential gain of multimodality.\n\u2022 We propose a novel decision-level fusion policy that learns to choose between the text and image networks and improve over both."}, {"heading": "2 MULTI-MODALITY", "text": "Over the years, a large body of research has been devoted to improving classification using ensembles of classifiers (Kittler et al., 1998; Hansen & Salamon, 1990). Inspired by their success, these methods have also been used in multi-modal settings (e.g.,Guillaumin et al. (2010); Poria et al. (2016)), where the source of the signals, or alternatively their modalities, are different. Some examples include audio-visual speech classification (Ngiam et al., 2011), image and text retrieval (Kiros et al.), sentiment analysis and semi-supervised learning (Guillaumin et al., 2010).\nCombining classifiers from different input sources presents multiple challenges. First, classifiers vary in their discriminative power, thus, an optimal unification method should be able to adapt itself for specific combinations of classifiers. Second, different data sources have different state-ofthe-art architectures, typically deep neural networks, which vary in depth, width, and optimization algorithm; making it non-trivial to merge them. Moreover, a multi-modal architecture potentially has more local minima that may give unsatisfying results. Finally, most of the publicly available real-world big data classification datasets, an essential building block of deep learning systems, typically contain only one data type.\nNevertheless, the potential performance boost of multi-modal architectures has motivated researchers over the years. Frome et al. (2013) combined an image network (Krizhevsky et al., 2012) with a Skip-gram Language Model in order to improve classification results on ImageNet. However, they were not able to improve the top-1 accuracy prediction, possibly because the text input they used (image labels) didn\u2019t contain a lot of information. Other works, used multi-modality to learn good embedding but did not present results on classification benchmarks (Lynch et al., 2015; Kiros et al.; Gong et al., 2014). Kannan et al. (2011) suggested to improve text-based product classification by adding an image signal, training an image classifier and learning a decision rule between the two. However, they only experimented with a small dataset and a low number of labels, and it is not clear how to scale their method for extreme multi-class multi-label applications that characterize real-world problems in e-commerce.\nAdding modalities can improve the classification of products that have a non-informative input source (e.g., image or text). In e-commerce, for example, classifiers that rely exclusively on text suffer from short and non-informative titles, differences in style between vendors and overlapping text across categories (i.e., a word that helps to classify a certain class may appear in other classes). Figure 1 presents a few examples of products that have only one informative input type. These examples suggest that a multi-modal architecture can potentially outperform a classifier with a single input type.\nMost unification techniques for multi-modal learning are partitioned between feature-level fusion techniques and decision-level fusion techniques (Figure 2, Left)."}, {"heading": "2.1 FEATURE LEVEL FUSION", "text": "Feature-level fusion is characterized by three phases: (a) learning a representation, (b) supervised training, and (c) testing. The different unification techniques are distinguished by the availability of the data in each phase (Guillaumin et al., 2010). For example, in cross-modality training, the representation is learned from all the modalities, but only one modality is available for supervised training and testing. In other cases, all of the modalities are available at all stages but we may want (or not) to limit their usage given a certain budget. Another source for the distinction is the order in which phases (a) and (b) are made. For example, one may first learn the representation and then learn a classifier from it, or learn both the representation and the classifier in parallel. In the deep learning context, there are two common approaches. In the first approach, we learn an end-to-end deep NN; the NN has multiple input-specific pipes that include a data source followed by input specific layers. After a certain depth, the pipes are concatenated followed by additional layers such that the NN is trained end-to-end. In the second approach, input specific deep NNs are learned first, and a multi-modal representation vector is created by concatenating the input specific feature vectors (e.g., the neural network\u2019s last hidden layer). Then, an additional classifier learns to classify from the multi-modal representation vector. While multi-modal methods have shown potential to boost performance on small datasets (Poria et al., 2016), or on top-k accuracy measures (Frome et al., 2013), we are not familiar with works that succeeded with applying it on a large-scale classification problem and received performance improvement in top-1 accuracy."}, {"heading": "2.2 DECISION-LEVEL FUSION", "text": "In this approach, an input specific classifier is learned for each modality, and the goal is to find a decision rule between them. The decision rule is typically a pre-defined rule (Guillaumin et al., 2010) and is not learned from the data. For example, Poria et al. (2016) chose the classifier with the maximal confidence, while Krizhevsky et al. (2012) average classifier predictions. However, in this work we show that learning the decision rule yields significantly better results on our data."}, {"heading": "3 METHODS AND ARCHITECTURES", "text": "In this section, we give the details of our multi-modal product classification architecture. The architecture is composed of a text CNN and an image CNN which are forged together by a policy network, as can be seen in Figure 2, Right."}, {"heading": "3.1 MULTI LABEL COST FUNCTION", "text": "Our cost function is the weighted sigmoid cross entropy with logits, a common cost function for multi-label problems. Let x be the logits, z be the targets, q be a positive weight coefficient, used as a multiplier for the positive targets, and \u03c3(x) = 11+exp(\u2212x) . The loss is given by:\nCost(x,z;q) = \u2212 qz \u00b7 log(\u03c3(x))\u2212 (1\u2212 z) \u00b7 log(1\u2212 \u03c3(x)) = (1\u2212 z) \u00b7 x+ (1 + (q \u2212 1) \u00b7 z) \u00b7 log(1 + exp(\u2212x)).\nThe positive coefficient q, allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error. We found it to have a significant effect in practice."}, {"heading": "3.2 TEXT CLASSIFICATION", "text": "For the text signal, we use the text CNN architecture of Kim (2014). The first layer embeds words into low-dimensional vectors using random embedding (different than the original paper). The next layer performs convolutions over time on the embedded word vectors using multiple filter sizes (3, 4 and 5), where we use 128 filters from each size. Next, we max-pool-over-time the result of each convolution filter and concatenated all the results together. We add a dropout regularization layer (0.5 dropping rate), followed by a fully connected layer, and classify the result using a softmax layer. An illustration of the Text CNN can be seen in Figure 2."}, {"heading": "3.3 IMAGE CLASSIFICATION", "text": "For the image signal, we use the VGG Network (Simonyan & Zisserman, 2014). The input to the network is a fixed-size 224 x 224 RGB image. The image is passed through a stack of convolutional layers with a very small receptive field: 3 x 3. The convolution stride is fixed to 1 pixel; the spatial padding of the convolutional layer is 1 pixel. Spatial pooling is carried out by five max-pooling layers, which follow some of the convolutional layers. Max-pooling is performed over a 2 x 2 pixel window, with stride 2. A stack of convolutional layers is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 2890-way product classification and thus contains 2890 channels (one for each class). All hidden layers are followed by a ReLu non-linearity. The exact details can be seen in Figure 2."}, {"heading": "3.4 MULTI-MODAL ARCHITECTURE", "text": "We experimented with four types of multi-modal architectures. (1) Learning decision-level fusion policies from different inputs. (1a) Policies that use the text and image CNNs class probabilities as input (Figure 2). We experimented with architectures that have one or two fully connected layers (the two-layered policy is using 10 hidden units and a ReLu non-linearity between them). (1b) Policies that use the text and/or image as input. For these policies, the architecture of policy network was either the text CNN or the VGG network. In order to train policies, labels are collected from the image and text networks predictions, i.e., the label is 1 if the image network made a correct prediction while the text network made a mistake, and 0 otherwise. On evaluation, we use the policy predictions to select between the models, i.e., if the policy prediction is 1 we use the image network, and use the text network otherwise. (2) Pre-defined policies that average the predictions of the different CNNs or choose the CNN with the highest confidence. (3) End-to-end feature-level fusion, each input type is processed by its specific CNN. We concatenate the last hidden layers of the CNNs and add one or two fully connected layers. All the layers are trained together end-to-end (we also tried to initialize the input specific weights from pre-trained single-modal networks). (4) Multistep feature-level fusion. As in (3), we create shared representation vector by concatenating the last hidden layers. However, we now keep the shared representation fixed and learn a new classifier from it."}, {"heading": "4 EXPERIMENTS", "text": ""}, {"heading": "4.1 SETUP", "text": "Our dataset contains 1.2 million products (title image and shelf) that we collected from Walmart.com (offered online and can be viewed at the website) and were deemed the hardest to classify by the current production system. We divide the data into training (1.1 million) validation (50k) and test (50k). We train both the image network and the text network on the training dataset and evaluate them on the test dataset. The policy is trained on the validation dataset and is also evaluated on the test dataset. The objective is to classify the product\u2019s shelf, from 2890 possible choices. Each product is typically assigned to more than one shelf (3 on average), and the network is considered accurate if its most probable shelf is one of them."}, {"heading": "4.2 TRAINING THE TEXT ARCHITECTURE", "text": "Preprocess: we build a dictionary of all the words in the training data and embed each word using a random embedding into a one hundred dimensional vector. We trim titles with more than 40 words and pad shorter titles with nulls.\nWe experimented with different batch sizes, dropout rates, and filters stride, but found that the vanilla architecture (Kim, 2014) works well on our data. This is consistent with Zhang & Wallace (2015), who showed that text CNNs are not very sensitive to hyperparameters. We tuned the cost function positive coefficient parameter q, and found out that the value 30 performed best in practice (we will also use this value for the image network). The best CNN that we trained classified 70.1% of the products from the test set correctly (Table 1)."}, {"heading": "4.3 TRAINING THE IMAGE ARCHITECTURE", "text": "Preprocess: we re-size all the images into 224 x 224 pixels and reduce the image mean.\nThe VGG network that we trained classified 57% of the products from the test set correctly. This is a bit disappointing if we compare it to the performance of the VGG network on ImageNet (\u223c 75%). There are a few differences between these two datasets that may explain this gap. First, our data has 3 times more classes and contains multiple labels per image making the classification harder, and second, Figure 1 implies that some of our images are not informative for shelf classification. Some works claim that the features learned by VGG on ImageNet are global feature extractors (Lynch et al., 2015). We therefore decided to use the weights learned by VGG on ImageNet and learn only the last layer. This configuration yielded only 36.7% accuracy. We believe that the reason is that some of the ImageNet classes are irrelevant for e-commerce (e.g., vehicles and animals) while some relevant categories are misrepresented (e.g., electronics and office equipment). It could also be that our images follow some specific pattern of white background, well-lit studio etc., that characterizes e-commerce."}, {"heading": "4.4 ERROR ANALYSIS", "text": "Is a picture worth a thousand words? Inspecting Figure 3, we can see that the text network outperformed the image network on this dataset, classifying more products correctly. Similar results were reported before (Pyo et al., 2010; Kannan et al., 2011) but to the best of our knowledge, this is the first work that compares state-of-the-art text and image CNNs on a real-world large-scale ecommerce dataset. What is the potential of multi-modality? We identified that for 7.8% of the products the image network made a correct prediction while the text network was wrong. This observation is encouraging since it implies that there is a relative big potential to harness via multi-modality. We find this large gap surprising since different neural networks applied to the same problem tend to make the same mistakes (Szegedy et al., 2013). Unification techniques for multi-modal problems typically use the last hidden layer of each network as features (Frome et al., 2013; Lynch et al., 2015; Pyo et al., 2010). We therefore decided to visualize the activations of this layer using a tSNE map (Maaten & Hinton, 2008). Figure 3, depicts such a map for the activations of the text model (the image model yielded similar results). In particular,\nwe were looking for regions in the tSNE map where the image predictions are correct and the text is wrong (Figure 3, green). Finding such a region will imply that a policy network can learn good decision boundaries. However, we can see that there are no well-defined regions in the tSNE maps where the image network is correct and the title is wrong (green), thus implying that it might be hard to identify these products using the activations of the last layers."}, {"heading": "4.5 MULTI-MODAL UNIFICATION TECHNIQUES", "text": "Our error analysis experiment highlights the potential of merging image and text. Still, we found it hard to achieve the upper bound provided by the error analysis in practice. We now describe the policies that managed to achieve performance boost in top-1 accuracy % over the text and image networks, and then provide discussion on other approaches that we tried but didn\u2019t work.\nDecision-level fusion: We trained policies from different data sources (e.g., title, image, and each CNN class probabilities), using different architectures and different hyperparameters. Looking at Table 1, we can see that the best policies were trained using the class probabilities (the softmax probabilities) of the image and text CNNs as inputs. The amount of class probabilities that were used (top-1, top-3 or all) did not have a significant effect on the results, indicating that the top-1 probability contains enough information to learn good policies. This result makes sense since the top-1 probability measures the confidence of the network in making a prediction. Still, the top-3 probabilities performed slightly better, indicating that the difference between the top probabilities may also matter. We can also see that the 2-layer architecture outperformed the 1-layer, indicating that a linear policy is too simple, and deeper models can yield better results. Last, the cost function positive coefficient q had a big impact on the results. We can see that for q = 1, the policy network is more accurate in its prediction however it achieves worse results on shelf classification. For q = 5 we get the best results, while higher values of q (e.g., 7 or 10) resulted in inaccurate policies that did not perform well in practice.\nWhile it may not seem surprising that combining text and image will improve accuracy, in practice we found it extremely hard to leverage this potential. To the best of our knowledge, this is the first work that demonstrates a direct performance improvement on top-1 classification accuracy from using images and text on a large-scale classification problem. We experimented with pre-defined policies that do not learn from the data. Specifically, we tried to average the logits, following (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014), and to choose the network with the maximal confidence following (Poria et al., 2016). Both of these experiments yielded significantly worse results, probably, since the text network is much more accurate than the image one (Table 1). We also tried to learn policies from the text and/or the image input, using a policy network which is either a text CNN, a VGG network or a combination. However, all of these experiments resulted in policies that overfit the data and performed worse than the title model on the test data (Table 1). We also experimented with early stopping criteria, various regularization methods (dropout, l1, l2) and reduced model size but none could make the policy network generalize.\nFeature-level fusion: Training a CNN end-to-end can be tricky. For example, each input source has its own specific architecture, with specific learning rate and optimization algorithm. We experimented with training the network end-to-end, but also with first training each part separately and then learning the concatenated parts. We tried different unification approaches such as gating functions (Srivastava et al., 2015), cross products and a different number of fully connected layers after the concatenation. These experiments resulted in models that were inferior to the text model. While this may seem surprising, the only successful feature level fusion that we are aware of (Frome et al., 2013), was not able to gain accuracy improvement on top-1 accuracy."}, {"heading": "5 CONCLUSIONS", "text": "In this work, we investigated a multi-modal multi-class multi-label product classification problem and presented results on a challenging real-world dataset that we collected from Walmart.com. We discovered that the text network outperforms the image network on our dataset, and observed a big potential of fusing text and image inputs. Finally, we suggested a multi-modal decision-level fusion approach that leverages state-of-the-art results from image and text classification and forges them into a multi-modal architecture that outperforms both.\nState-of-the-art image CNNs are much larger than text CNNs, and take more time to train and to run. Thus, extracting image features during run time, or getting the image network predictions may be prohibitively expensive. In this context, an interesting observation is that feature level fusion methods require using the image signal for each product, while decision level fusion methods require using the image network selectively making them more appealing. Moreover, our experiments suggest that decision-level fusion performs better than feature-level fusion in practice.\nFinally, we were only able to realize a fraction of the potential of multi-modality. In the future, we plan to investigate deeper policy networks and more sophisticated measures of confidence. We also plan to investigate ensembles of image networks (Krizhevsky et al., 2012) and text networks (Pyo et al., 2010). We believe that the insights from training policy networks will eventually lead us to train end to end differential multi-modal networks."}], "references": [{"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Lecun. Very deep convolutional networks for natural language processing", "author": ["Alexis Conneau", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Yann"], "venue": "arXiv preprint arXiv:1606.01781,", "citeRegEx": "Conneau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Conneau et al\\.", "year": 2016}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Multimodal semi-supervised learning for image classification", "author": ["Matthieu Guillaumin", "Jakob Verbeek", "Cordelia Schmid"], "venue": "In CVPR 2010-23rd IEEE Conference on Computer Vision & Pattern Recognition,", "citeRegEx": "Guillaumin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Guillaumin et al\\.", "year": 2010}, {"title": "Neural network ensembles", "author": ["Lars Kai Hansen", "Peter Salamon"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Hansen and Salamon.,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon.", "year": 1990}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving product classification using images", "author": ["Anitha Kannan", "Partha Pratim Talukdar", "Nikhil Rasiwasia", "Qifa Ke"], "venue": "IEEE 11th International Conference on Data Mining. IEEE,", "citeRegEx": "Kannan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2011}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "On combining classifiers", "author": ["Josef Kittler", "Mohamad Hatef", "Robert PW Duin", "Jiri Matas"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Kittler et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kittler et al\\.", "year": 1998}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Recurrent convolutional neural networks for text classification", "author": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": null, "citeRegEx": "Lai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2015}, {"title": "Images don\u2019t lie: Transferring deep visual semantic features to large-scale multimodal learning to rank", "author": ["Corey Lynch", "Kamelia Aryafar", "Josh Attenberg"], "venue": "arXiv preprint arXiv:1511.06746,", "citeRegEx": "Lynch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lynch et al\\.", "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Multimodal deep learning", "author": ["Jiquan Ngiam", "Aditya Khosla", "Mingyu Kim", "Juhan Nam", "Honglak Lee", "Andrew Y Ng"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "Ngiam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ngiam et al\\.", "year": 2011}, {"title": "Fusing audio, visual and textual clues for sentiment analysis from multimodal content", "author": ["Soujanya Poria", "Erik Cambria", "Newton Howard", "Guang-Bin Huang", "Amir Hussain"], "venue": null, "citeRegEx": "Poria et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Poria et al\\.", "year": 2016}, {"title": "Large-scale item categorization in e-commerce using multiple recurrent neural networks", "author": ["Hyuna Pyo", "Jung-Woo Ha", "Jeonghee Kim"], "venue": null, "citeRegEx": "Pyo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pyo et al\\.", "year": 2010}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "arXiv preprint arXiv:1312.6199,", "citeRegEx": "Szegedy et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "Efficient character-level document classification by combining convolution and recurrent layers", "author": ["Yijun Xiao", "Kyunghyun Cho"], "venue": "arXiv preprint arXiv:1602.00367,", "citeRegEx": "Xiao and Cho.,? \\Q2016\\E", "shortCiteRegEx": "Xiao and Cho.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Ye Zhang", "Byron Wallace"], "venue": "arXiv preprint arXiv:1510.03820,", "citeRegEx": "Zhang and Wallace.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Products classification is typically addressed as a text classification problem because most metadata of items are represented as textual features (Pyo et al., 2010).", "startOffset": 147, "endOffset": 165}, {"referenceID": 0, "context": "On the other hand, Deep Neural Networks use generic priors instead of specific domain knowledge (Bengio et al., 2013) and have been shown to give competitive results on text classification tasks (Zhang et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 20, "context": ", 2013) and have been shown to give competitive results on text classification tasks (Zhang et al., 2015).", "startOffset": 85, "endOffset": 105}, {"referenceID": 8, "context": "In particular, Convolutional neural networks (CNNs) (Kim, 2014; Zhang et al., 2015; Conneau et al., 2016) and Recurrent NNs (Lai et al.", "startOffset": 52, "endOffset": 105}, {"referenceID": 20, "context": "In particular, Convolutional neural networks (CNNs) (Kim, 2014; Zhang et al., 2015; Conneau et al., 2016) and Recurrent NNs (Lai et al.", "startOffset": 52, "endOffset": 105}, {"referenceID": 1, "context": "In particular, Convolutional neural networks (CNNs) (Kim, 2014; Zhang et al., 2015; Conneau et al., 2016) and Recurrent NNs (Lai et al.", "startOffset": 52, "endOffset": 105}, {"referenceID": 11, "context": ", 2016) and Recurrent NNs (Lai et al., 2015; Pyo et al., 2010; Xiao & Cho, 2016) can efficiently capture the sequentiality of the text.", "startOffset": 26, "endOffset": 80}, {"referenceID": 16, "context": ", 2016) and Recurrent NNs (Lai et al., 2015; Pyo et al., 2010; Xiao & Cho, 2016) can efficiently capture the sequentiality of the text.", "startOffset": 26, "endOffset": 80}, {"referenceID": 8, "context": "These methods are typically applied directly to distributed embedding of words (Kim, 2014; Lai et al., 2015; Pyo et al., 2010) or characters (Zhang et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 11, "context": "These methods are typically applied directly to distributed embedding of words (Kim, 2014; Lai et al., 2015; Pyo et al., 2010) or characters (Zhang et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 16, "context": "These methods are typically applied directly to distributed embedding of words (Kim, 2014; Lai et al., 2015; Pyo et al., 2010) or characters (Zhang et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 20, "context": ", 2010) or characters (Zhang et al., 2015; Conneau et al., 2016; Xiao & Cho, 2016), without any knowledge on the syntactic or semantic structures of a language.", "startOffset": 22, "endOffset": 82}, {"referenceID": 1, "context": ", 2010) or characters (Zhang et al., 2015; Conneau et al., 2016; Xiao & Cho, 2016), without any knowledge on the syntactic or semantic structures of a language.", "startOffset": 22, "endOffset": 82}, {"referenceID": 10, "context": "In Image classification, CNNs are widely considered the best models, and achieve state-of-theart results on the ImageNet Large-Scale Visual Recognition Challenge (Russakovsky et al., 2015; Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015).", "startOffset": 162, "endOffset": 258}, {"referenceID": 6, "context": "In Image classification, CNNs are widely considered the best models, and achieve state-of-theart results on the ImageNet Large-Scale Visual Recognition Challenge (Russakovsky et al., 2015; Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2015).", "startOffset": 162, "endOffset": 258}, {"referenceID": 8, "context": "The final architecture has 3 main components (Figure 2, Right): a text CNN (Kim, 2014), an image CNN (Simonyan & Zisserman, 2014) and a policy network that learns to choose between them.", "startOffset": 75, "endOffset": 86}, {"referenceID": 8, "context": "\u2022 We demonstrate that the text classification CNN (Kim, 2014) outperforms the VGG network (Simonyan & Zisserman, 2014) on a real-world large-scale product to shelf classification problem.", "startOffset": 50, "endOffset": 61}, {"referenceID": 9, "context": "Over the years, a large body of research has been devoted to improving classification using ensembles of classifiers (Kittler et al., 1998; Hansen & Salamon, 1990).", "startOffset": 117, "endOffset": 163}, {"referenceID": 14, "context": "Some examples include audio-visual speech classification (Ngiam et al., 2011), image and text retrieval (Kiros et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 4, "context": "), sentiment analysis and semi-supervised learning (Guillaumin et al., 2010).", "startOffset": 51, "endOffset": 76}, {"referenceID": 4, "context": ",Guillaumin et al. (2010); Poria et al.", "startOffset": 1, "endOffset": 26}, {"referenceID": 4, "context": ",Guillaumin et al. (2010); Poria et al. (2016)), where the source of the signals, or alternatively their modalities, are different.", "startOffset": 1, "endOffset": 47}, {"referenceID": 10, "context": "(2013) combined an image network (Krizhevsky et al., 2012) with a Skip-gram Language Model in order to improve classification results on ImageNet.", "startOffset": 33, "endOffset": 58}, {"referenceID": 12, "context": "Other works, used multi-modality to learn good embedding but did not present results on classification benchmarks (Lynch et al., 2015; Kiros et al.; Gong et al., 2014).", "startOffset": 114, "endOffset": 167}, {"referenceID": 3, "context": "Other works, used multi-modality to learn good embedding but did not present results on classification benchmarks (Lynch et al., 2015; Kiros et al.; Gong et al., 2014).", "startOffset": 114, "endOffset": 167}, {"referenceID": 2, "context": "Frome et al. (2013) combined an image network (Krizhevsky et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Frome et al. (2013) combined an image network (Krizhevsky et al., 2012) with a Skip-gram Language Model in order to improve classification results on ImageNet. However, they were not able to improve the top-1 accuracy prediction, possibly because the text input they used (image labels) didn\u2019t contain a lot of information. Other works, used multi-modality to learn good embedding but did not present results on classification benchmarks (Lynch et al., 2015; Kiros et al.; Gong et al., 2014). Kannan et al. (2011) suggested to improve text-based product classification by adding an image signal, training an image classifier and learning a decision rule between the two.", "startOffset": 0, "endOffset": 514}, {"referenceID": 4, "context": "The different unification techniques are distinguished by the availability of the data in each phase (Guillaumin et al., 2010).", "startOffset": 101, "endOffset": 126}, {"referenceID": 15, "context": "While multi-modal methods have shown potential to boost performance on small datasets (Poria et al., 2016), or on top-k accuracy measures (Frome et al.", "startOffset": 86, "endOffset": 106}, {"referenceID": 2, "context": ", 2016), or on top-k accuracy measures (Frome et al., 2013), we are not familiar with works that succeeded with applying it on a large-scale classification problem and received performance improvement in top-1 accuracy.", "startOffset": 39, "endOffset": 59}, {"referenceID": 4, "context": "The decision rule is typically a pre-defined rule (Guillaumin et al., 2010) and is not learned from the data.", "startOffset": 50, "endOffset": 75}, {"referenceID": 4, "context": "The decision rule is typically a pre-defined rule (Guillaumin et al., 2010) and is not learned from the data. For example, Poria et al. (2016) chose the classifier with the maximal confidence, while Krizhevsky et al.", "startOffset": 51, "endOffset": 143}, {"referenceID": 4, "context": "The decision rule is typically a pre-defined rule (Guillaumin et al., 2010) and is not learned from the data. For example, Poria et al. (2016) chose the classifier with the maximal confidence, while Krizhevsky et al. (2012) average classifier predictions.", "startOffset": 51, "endOffset": 224}, {"referenceID": 8, "context": "For the text signal, we use the text CNN architecture of Kim (2014). The first layer embeds words into low-dimensional vectors using random embedding (different than the original paper).", "startOffset": 57, "endOffset": 68}, {"referenceID": 8, "context": "We experimented with different batch sizes, dropout rates, and filters stride, but found that the vanilla architecture (Kim, 2014) works well on our data.", "startOffset": 119, "endOffset": 130}, {"referenceID": 8, "context": "We experimented with different batch sizes, dropout rates, and filters stride, but found that the vanilla architecture (Kim, 2014) works well on our data. This is consistent with Zhang & Wallace (2015), who showed that text CNNs are not very sensitive to hyperparameters.", "startOffset": 120, "endOffset": 202}, {"referenceID": 12, "context": "Some works claim that the features learned by VGG on ImageNet are global feature extractors (Lynch et al., 2015).", "startOffset": 92, "endOffset": 112}, {"referenceID": 16, "context": "Similar results were reported before (Pyo et al., 2010; Kannan et al., 2011) but to the best of our knowledge, this is the first work that compares state-of-the-art text and image CNNs on a real-world large-scale ecommerce dataset.", "startOffset": 37, "endOffset": 76}, {"referenceID": 7, "context": "Similar results were reported before (Pyo et al., 2010; Kannan et al., 2011) but to the best of our knowledge, this is the first work that compares state-of-the-art text and image CNNs on a real-world large-scale ecommerce dataset.", "startOffset": 37, "endOffset": 76}, {"referenceID": 18, "context": "We find this large gap surprising since different neural networks applied to the same problem tend to make the same mistakes (Szegedy et al., 2013).", "startOffset": 125, "endOffset": 147}, {"referenceID": 2, "context": "Unification techniques for multi-modal problems typically use the last hidden layer of each network as features (Frome et al., 2013; Lynch et al., 2015; Pyo et al., 2010).", "startOffset": 112, "endOffset": 170}, {"referenceID": 12, "context": "Unification techniques for multi-modal problems typically use the last hidden layer of each network as features (Frome et al., 2013; Lynch et al., 2015; Pyo et al., 2010).", "startOffset": 112, "endOffset": 170}, {"referenceID": 16, "context": "Unification techniques for multi-modal problems typically use the last hidden layer of each network as features (Frome et al., 2013; Lynch et al., 2015; Pyo et al., 2010).", "startOffset": 112, "endOffset": 170}, {"referenceID": 10, "context": "Specifically, we tried to average the logits, following (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014), and to choose the network with the maximal confidence following (Poria et al.", "startOffset": 56, "endOffset": 109}, {"referenceID": 15, "context": ", 2012; Simonyan & Zisserman, 2014), and to choose the network with the maximal confidence following (Poria et al., 2016).", "startOffset": 101, "endOffset": 121}, {"referenceID": 2, "context": "While this may seem surprising, the only successful feature level fusion that we are aware of (Frome et al., 2013), was not able to gain accuracy improvement on top-1 accuracy.", "startOffset": 94, "endOffset": 114}, {"referenceID": 10, "context": "We also plan to investigate ensembles of image networks (Krizhevsky et al., 2012) and text networks (Pyo et al.", "startOffset": 56, "endOffset": 81}, {"referenceID": 16, "context": ", 2012) and text networks (Pyo et al., 2010).", "startOffset": 26, "endOffset": 44}], "year": 2016, "abstractText": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy % over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "creator": "LaTeX with hyperref package"}}}