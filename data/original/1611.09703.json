{"id": "1611.09703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2016", "title": "Semantic Parsing of Mathematics by Context-based Learning from Aligned Corpora and Theorem Proving", "abstract": "We study methods for automated parsing of informal mathematical expressions into formal ones, a main prerequisite for deep computer understanding of informal mathematical texts. We propose a context-based parsing approach that combines efficient statistical learning of deep parse trees with their semantic pruning by type checking and large-theory automated theorem proving. We show that the methods very significantly improve on previous results in parsing theorems from the Flyspeck corpus.", "histories": [["v1", "Tue, 29 Nov 2016 16:20:24 GMT  (97kb,D)", "http://arxiv.org/abs/1611.09703v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["cezary kaliszyk", "josef urban", "ji\\v{r}\\'i vysko\\v{c}il"], "accepted": false, "id": "1611.09703"}, "pdf": {"name": "1611.09703.pdf", "metadata": {"source": "META", "title": "Semantic Parsing of Mathematics by Context-based Learning from Aligned Corpora and Theorem Proving", "authors": ["Cezary Kaliszyk", "Josef Urban", "Ji\u0159\u0131\u0301 Vysko\u010dil"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Computer-understandable (formal) mathematics (Harrison, Urban, and Wiedijk 2014) is still far from taking over the mathematical mainstream. Despite recent impressive formalizations such as the Formal Proof of the Kepler conjecture (Flyspeck) (Hales et al. 2015), FeitThompson (Gonthier et al. 2013), seL4 (Klein et al. 2010), CompCert (Leroy 2009), and CCL (Bancerek and Rudnicki 2002), formalizing proofs is still largely unappealing to mathematicians. While research on AI and strong automation over large theories has taken off in the last decade (Blanchette et al. 2016), there has been so far little progress in automating the understanding of informal LATEX-written and ambiguous mathematical writings.\nAutomatic parsing of informal mathematical texts into formal ones has been for long time considered a hard or impossible task. Among the state-of-the-art Interactive Theorem Proving (ITP) systems such as HOL (Light) (Harrison 1996), Isabelle (Wenzel, Paulson, and Nipkow 2008), Mizar (Grabowski, Korni\u0142owicz, and Naumowicz 2010) and Coq (coq ), none includes automated parsing, instead relying on sophisticated formal languages and mechanisms (Garillot et al. 2009; Gonthier and Tassi 2012; Haftmann and Wenzel 2006; Rudnicki, Schwarzweller, and Trybulec 2001). The past work in this direction \u2013 most notably by Zinn (Zinn 2004) \u2013 has often been cited as discouraging from such efforts.\nRecently (Kaliszyk, Urban, and Vyskocil 2015b) proposed to automatically learn formal understanding of infor\u2217Supported by the Austrian Science Fund (FWF): P26201. \u2020Supported by the ERC Consolidator grant no. 649043 AI4REASON.\nmal mathematics from large aligned informal/formal corpora. Such learning can be additionally combined with strong semantic filtering methods such as typechecking and large-theory Automated Theorem Proving (ATP). Suitable aligned corpora are starting to appear today, the major example being the Flyspeck project and in particular its alignment (by Hales) with the detailed informal Blueprint for Formal Proofs (Hales 2012)."}, {"heading": "Contributions", "text": "In this paper, we first introduce the informal-to-formal setting (Sec. 2), summarize the probabilistic context-free grammar (PCFG) approach of (Kaliszyk, Urban, and Vyskocil 2015b) (Sec. 3), and extend this approach by fast contextaware parsing mechanisms. \u2022 Limits of the context-free approach. We demonstrate\non a minimal example, that the context-free setting is not strong enough to eventually learn correct parsing (Sec. 4) of relatively simple informal mathematical formulas.\n\u2022 Efficient context inclusion via discrimination trees. We propose and efficiently implement modifications of the CYK algorithm that take into account larger parsing subtrees (context) and their probabilities (Sec. 5). This modification is motivated by an analogy with large-theory reasoning systems and its efficient implementation is based on a novel use of fast theorem-proving data structures that extend the probabilistic parser.\n\u2022 Significant improvement of the informal-to-formal translation performance. The methods are evaluated, both by standard (non-semantic) machine-learning crossvalidation, and by strong semantic methods available in formal mathematics such as typechecking and largetheory automated reasoning (Sec. 6)."}, {"heading": "2 Informalized Flyspeck and PCFG", "text": "The ultimate goal of the informal-to-formal traslation is to automatically learn parsing on informal LATEX formulas that have been aligned with their formal counterparts, as for example done by Hales for his informal and formal Flyspeck texts (Hales 2012; Tankink et al. 2013). Instead of starting with LATEX where only hundreds of aligned examples are so far available for Flyspeck, we re-use the first large informal/formal corpus introduced in (Kaliszyk, Urban, and\nar X\niv :1\n61 1.\n09 70\n3v 1\n[ cs\n.C L\n] 2\n9 N\nov 2\n01 6\nVyskocil 2015b), based on informalized (or ambiguated) formal statements created from the HOL Light theorems in Flyspeck. This provides about 22000 informal/formal pairs of Flyspeck theorems."}, {"heading": "Informalized Flyspeck", "text": "The following transformations are applied in (Kaliszyk, Urban, and Vyskocil 2015b) to the HOL parse trees to obtain the aligned corpus:\n\u2022 Using the 72 overloaded instances defined in HOL Light/Flyspeck, such as (\"+\", \"vector_add\"). The constant vector_add would be replaced by + in the resulting sentence.\n\u2022 Getting the infix operators from HOL Light, and printing them as infix in the informalized sentences. Since + is declared as infix, vector_add u v, would thus result in u + v.\n\u2022 Getting all \u201cprefixed\u201d symbols from the list of 1000 most frequent symbols by searching for: real_, int_, vector_, nadd_, treal_, hreal_, matrix_, complex_ and making them ambiguous by forgetting the prefix.\n\u2022 Similar overloading of various other symbols that disambiguate overloading, for example the \u201cc\u201d-versions of functions such as ccos cexp clog csin, similarly for vsum, rpow, nsum, list_sum, etc.\n\u2022 Deleting brackets, type annotations, and the 10 most frequent casting functors such as Cx and real_of_num.\nThe Informal-To-Formal Translation Task The informal-to-formal translation task is to construct an AI system that will automatically produce the most probable formal (in this case HOL) parse trees for previously unseen informal sentences. For example, the informalized statement of the HOL theorem REAL_NEGNEG:\n! A0 -- -- A0 = A0\nhas the formal HOL Light representation shown in Fig. 1 (as a text) and in Fig. 2 (as a tree). Note that all overloaded symbols are disambiguated there, they are applied with the correct arity, and all terms are decorated with their result types. To solve the task, we allow (and assume) training on a sufficiently large corpus of such informal/formal pairs."}, {"heading": "Probabilistic Context Free Grammars", "text": "Given a large corpus of corresponding informal/formal formulas, how can we train an AI system for parsing the next informal formula into a formal one? The informal-to-formal domain differs from natural-language domains, where millions of examples of paired (e.g., English/German) sentences are available for training machine translation. The natural languages also have many more words (concepts) than in mathematics, and the sentences to a large extent also lack the recursive structure that is frequently encountered in mathematics. Given that there are currently only thousands of informal/formal examples, purely statistical alignment methods based on n-grams seem inadequate. Instead, the methods have to learn how to compose larger parse trees from smaller ones based on those encountered in the limited number of examples.\nA well-known approach ensuring such compositionality is the use of CFG (Context Free Grammar) parsers. This approach has been widely used, e.g., in wordsense disambiguation. A frequently used CFG algorithm is the CYK (Cocke\u2013Younger\u2013Kasami) chart-parser (Younger 1967), based on bottom-up parsing. By default CYK requires the CFG to be in the Chomsky Normal Form (CNF). The transformation to CNF can cause an exponential blowup of the grammar, however, an improved version of CYK gets around this issue (Lange and Lei\u00df 2009).\nIn linguistic applications the input grammar for the CFGbased parsers is typically extracted from the grammar trees which correspond to the correct parses of natural-language sentences. Large annonated treebanks of such correct parses exist for natural languages. The grammar rules extracted from the treebanks are typically ambiguous: there are multiple possible parse trees for a particular sentence. This is why CFG is extended by adding a probability to each grammar rule, resulting in Probabilistic CFG (PCFG)."}, {"heading": "3 PCFG for the Informal-To-Formal Task", "text": "The most straightforward PCFG-based approach would be to directly use the native HOL Light parse trees (Fig. 2) for extracting the PCFG. However, terms and types are there annotated with only a few nonterminals such as: Comb (application), Abs (abstraction), Const (higher-order constant), Var (variable), Tyapp (type application), and Tyvar (type variable). This would lead to many possible parses in the context-free setting, because the learned rules are very universal, e.g:"}, {"heading": "Comb -> Const Var. Comb -> Const Const. Comb -> Comb Comb.", "text": "The type information does not help to constrain the applications, and the last rule allows a series of several constants to be given arbitrary application order, leading to uncontrolled explosion."}, {"heading": "HOL Types as Nonterminals", "text": "The approach taken in (Kaliszyk, Urban, and Vyskocil 2015b) is to first re-order and simplify the HOL Light parse trees to propagate the type information at appropriate\nplaces. This gives the context-free rules a chance of providing meaningful pruning information. For example, consider again the raw HOL Light parse tree for REAL_NEGNEG (Fig. 1,2).\nInstead of directly extracting very general rules such as Comb -> Const Abs, each type is first compressed into an opaque nonterminal. This turns the parse tree of REAL_NEGNEG into (see also Fig. 3):\n(\"(Type bool)\" ! (\"(Type (fun real bool))\" (Abs\n(\"(Type real)\" (Var A0)) (\"(Type bool)\" (\"(Type\nreal)\" real_neg (\"(Type real)\" real_neg (\"(Type\nreal)\" (Var A0)))) = (\"(Type real)\" (Var A0))))))\nThe CFG rules extracted from this transformed tree thus become more targeted. For example, the two rules:\n\"(Type bool)\" -> \"(Type real)\" = \"(Type real)\".\n\"(Type real)\" -> real_neg \"(Type real)\".\nsay that equality of two reals has type bool, and negation applied to reals yields reals. Such learned probabilistic typing rules restrict the number of possible parses much more\nthan the general \u201capplication\u201d rules extracted from the original HOL Light tree. The rules still have a non-trivial generalization (learning) effect that is needed for the compositional behavior of the information extracted from the trees. For example, once we learn from the training data that the variable \u2018\u2018u\u2019\u2019 is mostly parsed as a real number, i.e.:\n\"(Type real)\" -> Var u.\nwe will be able to apply real_neg to \u2018\u2018u\u2019\u2019 even if the particular subterm \u2018\u2018real_neg u\u2019\u2019 has never yet been seen in the training examples, and the probability of this parse will be relatively high.\nIn other words, having the HOL types as semantic categories (corresponding e.g. to word senses when using PCFG for word-sense disambiguation) is a reasonable choice for the first experiments. It is however likely that even better semantic categories can be developed, based on more involved statistical and semantic analysis of the data such as latent semantics (Deerwester et al. 1990)."}, {"heading": "Semantic Concepts as Nonterminals", "text": "The last part of the original setting wraps ambiguous symbols, such as \u2018\u2018--\u2019\u2019, in their disambiguated semantic/formal concept nonterminals. In this case $#real_neg would be wrapped around \u2018\u2018--\u2019\u2019 in the training tree when \u2018\u2018--\u2019\u2019 is used as subtraction on reals. While the type annotation is often sufficient for disambiguation, such explicit disambiguation nonterminal is more precise and allows easier extraction of the HOL semantics from the constructed parse trees. The actual tree of REAL_NEGNEG used for training the grammar is thus as follows (see also Fig. 4):\n(\"(Type bool)\" ! (\"(Type (fun real bool))\" (Abs\n(\"(Type real)\" (Var A0)) (\"(Type bool)\" (\"(Type\nreal)\" ($#real_neg --) (\"(Type real)\" ($#real_-\nneg --) (\"(Type real)\" (Var A0)))) ($#= =) (\"(Type\nreal)\" (Var A0))))))"}, {"heading": "Modified CYK Parsing and Its Initial Performance", "text": "Once the PCFG is learned from such data, the CYK algorithm augmented with fast internal semantic checks is\nused to parse the informal sentences. The semantic checks are performed to require compatibility of the types of free variables in parsed subtrees. The most probable parse trees are then typechecked by HOL Light. This is followed by proof and disproof attempts by the HOL(y)Hammer system (Kaliszyk and Urban 2014), using all the semantic knowledge available in the Flyspeck library (about 22000 theorems). The first large-scale disambiguation experiment conducted over \u201cambiguated\u201d Flyspeck in (Kaliszyk, Urban, and Vyskocil 2015b) showed that about 40% of the ambiguous sentences have their correct parses among the best 20 parse trees produced by the trained parser. This is encouraging, but certainly invites further research in improving the statistical/semantic parsing methods."}, {"heading": "4 Limits of the Context-Free Grammars", "text": "A major limiting issue when using PCFG-based parsing algorithms is the context-freeness of the grammar. This is most obvious when using just the low-level term constructors as nonterminals, however it shows often also in the more advanced setting described above. In some cases, no matter how good are the training data, there is no way how to set up the probabilities of the parsing rules so that the required parse tree will have the highest probability. We show this on the following simple example.\nExample: Consider the following term t: 1 * x + 2 * x.\nwith the following simplified parse tree T0(t) (see also Fig. 5).\n(S (Num (Num (Num 1) * (Num x)) + (Num (Num 2) * (Num x))) .)\nWhen used as the training data (treebank), the grammar tree T0(t) results in the following set of CFG rules G(T0(t)):\nS -> Num . Num -> 1\nNum -> Num + Num Num -> 2\nNum -> Num * Num Num -> x\nThis grammar allows exactly the following five parse trees T4(t), ..., T0(t) when used on the original (non-bracketed) term t:\n(S (Num (Num 1) * (Num (Num (Num x) + (Num 2)) * (Num x))) .) (S (Num (Num 1) * (Num (Num x) + (Num (Num 2) * (Num x)))) .) (S (Num (Num (Num 1) * (Num (Num x) + (Num 2))) * (Num x)) .) (S (Num (Num (Num (Num 1) * (Num x)) + (Num 2)) * (Num x)) .) (S (Num (Num (Num 1) * (Num x)) + (Num (Num 2) * (Num x))) .)\nHere only the last tree corresponds to the original training tree T0(t). No matter what probabilities p(Rulei) are assigned to the grammar rules G(T0(t)), it is not possible to make the priority of + smaller than the priority of *. A context-free grammar forgets the context and cannot remember and apply complex mechanisms such as priorities. The probability of all parse trees is thus in this case always the same, and equal to:\np(T4(t)) = ... = p(T0(t)) = p(S -> Num .) \u00d7 p(Num -> Num + Num)\n\u00d7p(Num -> Num * Num) \u00d7 p(Num -> Num * Num)\n\u00d7p(Num -> 1) \u00d7 p(Num -> 2) \u00d7 p(Num -> x) \u00d7 p(Num -> x)\nWhile the example\u2019s correct parse does not strictly imply the priorities of + and * as we know them, it is clear that we would like the grammar to prefer parse trees that are in some sense more similar to the training data. One method that is frequently used for dealing with similar problems in the NLP domain is grammar lexicalization (Collins 1997). There an additional terminal can be appended to nonterminals and propagated from the subtrees, thus creating many more possible (more precise) nonterminals. This approach however does not solve the particular problem with operator priorities. We also believe that considering probabilities of larger subtrees in the data as we propose below is conceptually cleaner than lexicalization."}, {"heading": "5 Using Probabilities of Deeper Subtrees", "text": "Our solution is motivated by an analogy with the n-gram statistical machine-translation models, and also with the largetheory premise selection systems. In such systems, characterizing formulas by all deeper subterms and subformulas is feasible and typically considerably improves the performance of the algorithms (Kaliszyk, Urban, and Vyskocil 2015a). Considering subtrees of greater depth for updating the parsing probabilities may initially seem computationally involved. Below we however show that by using efficient ATP-style indexing datastructures such as discrimination trees, this approach becomes feasible, solving in a reasonably clean way some of the inherent problems of the context-free grammars mentioned above.\nIn more detail, our approach is as follows. We extract not just subtrees of depth 2 from the treebank (as is done by the standard PCFG), but all subtrees up to a certain depth. Other approaches \u2013 such as frequency-based rather than depthbased \u2013 are possible. During the (modified) CYK chart parsing, the probabilities of the parsed subtrees are adjusted by taking into account the statistics of such deeper subtrees extracted from the treebank. The extracted subtrees are technically treated as new \u201cgrammar rules\u201d of the form:\nroot of the subtree -> list of the children of the subtree\nFormally, for a treebank (set of trees) T, we thus define Gn(T) to be the grammar rules of depth n extracted from T. The standard context-free grammar G(T) then becomes G2(T), and we denote by Gn,m(T) where n \u2264 m the union1\n1In general, a grammar could pick only some subtree depths instead of their contiguous intervals, but we do not use such grammars now.\nGn(T) \u222a ... \u222a Gm(T) . The probabilities of these deeper grammar rules are again learned from the treebank. Our current solution treats the nonterminals on the left-hand sides as disjoint from the old (standard CFG) nonterminals when counting the probabilities (this can be made more complicated in the future). The right-hand sides of such new grammar rules thus contain larger subtrees, allowing to compute the parsing probabilities using more context/structural information than in the standard context-free case.\nFor the example term t from Section 4 this works as follows. After the extraction of all subtrees of depth 2 and 3 and the appropriate adjustment of their probabilities, we get a new extended set of probabilistic grammar rules G2,3(T0(t)) \u2283 G(T0(t)). This grammar could again parse all the five different parse trees T4(t), ..., T0(t) as in Section 4, but now the probabilities p(T4(t)), ..., p(T0(t)) would in general differ, and an implementation would be able to choose the training tree T0(t) as the most probable one. In the particular implementation that we use (see Section 5) its probability is:\np(T0(t)) = p(Num -> (Num 1)) \u00d7 p(Num -> (Num x))\u00d7\np(Num -> (Num 2)) \u00d7 p(Num -> (Num x))\u00d7\np(Num -> (Num Num * Num) + (Num Num * Num))\u00d7\np(S -> Num .)\nHere the second line from the bottom stands for the probability of a subtree of depth 3. For the case of the one-element treebank T0(t), p(T0(t)) would indeed be the highest probability. On the other hand, the probability of some of the other parses (e.g., T4(t) and T3(t) above) would remain unmodified, because in such parses there are no subtrees of depth 3 from the training tree T0(t).\nEfficient Implementation of Deeper Subtrees Discrimination trees (Robinson and Voronkov 2001), as first implemented by Greenbaum (Greenbaum 1986), index terms in a trie, which keeps single path-strings at each of the indexed terms. A discrimination tree can be constructed efficiently, by inserting terms in the traversal preorder. Since discrimination trees are based on path indexing, retrieval of matching subtrees during the parsing is straightforward.\nWe use a discrimination tree D to store all the subtrees Gn,m(T) from the treebank T and to efficiently retrieve them together with their probabilities during the chart parsing. The efficiency of the implementation is important, as we need to index about half a million subtrees in D for the experiments over Flyspeck. On the other hand, such numbers have become quite common in large-theory reasoning recently and do not pose a significant problem. For memory efficiency we use OCaml maps (implemented as AVL trees) in the internal nodes of D. The lookup time thus grows logarithmically with the number of trees in D, which is the main reason why we so far only consider trees of depth 3.\nWhen a particular cell in the CYK parsing chart is finished (i.e., all its possible parses are known), the subtree-based probability update is initiated. The algorithm thus consists of two phases: (i) the standard collecting of all possible parses of a particular cell, using the context-free rules G2(T) only, and (ii) the computation of probabilities, which involves also the deeper (contextual) subtrees G3,m(T).\nIn the second phase, every parse P of the particular cell is inspected, trying to find its top-level subtrees of depths 3, ...,m in the discrimination tree D. If a matching tree T is found in D, the probability of P is recomputed, using the probability of T . There are various ways how to combine the old context-free and the new contextual probabilities. The current method we use is to take the maximum of the probabilities, keeping them as competing methods. As mentioned above, the nonterminals in the new subtree-based rules are kept disjoint from the old context-free rules when computing the grammar rule probabilities. The usual effect is that a frequent deeper subtree that matches the parse P gives it more probability, because such a \u201cdeeper context parse\u201d replaces the corresponding two shallow (old context-free) rules, whose probabilities would have to be multiplied.\nOur speed measurement with depth 3 has shown that the new implementation is (surprisingly) faster. In particular, when training on all 21695 Flypeck trees and testing on 11911 of them with the limit of 10 best parses, the new version is 23% faster than the old one (10342.75 s vs. 13406.97 s total time). In this measurement the new version also failed to produce at least a single parse less often than the old version (631 vs 818). This likely means that the deeper subtrees help to promote the correct parse, which in the context-free version is considered at some point too improbable to make it into the top 10 parses and consequently discarded."}, {"heading": "6 Experimental Evaluation", "text": ""}, {"heading": "Machine Learning Evaluation", "text": "The main evaluation is done in the same cross-validation scenario as in (Kaliszyk, Urban, and Vyskocil 2015b). We create the ambiguous sentences (Sec. 2) and the disambiguated grammar trees from all 21695 Flyspeck theorems,2 permute them randomly and split into 100 equally sized chunks of about 217 trees and their corresponding sentences. The grammar trees serve for training and the ambiguous sentences for evaluation. For each testing chunk Ci (i \u2208 1..100) of 217 sentences we train the probabilistic grammar Pi on the union of the remaining 99 chunks of grammar trees (altogether about 21478 trees). Then we try to get the best 20 parse trees for all the 217 sentences in Ci using the grammar Pi. This is done for the simple context-free version (depth 2) of the algorithm (Section 3), as well as for the versions using deeper subtrees (Section 5). The numbers of correctly parsed formulas and their average ranks across the several 100-fold cross-validations are shown in Table 1.\nIt is clear that the introduction of deeper subtrees into the CYK algorithm has produced a significant improvement of the parsing precision. The number of correctly parsed formulas appearing among the top 20 parses has increased by 22% between the context-free (depth 2) version and the subtree-based version when using subtrees of depth 3, and it grows by 64% when using subtrees of depth 6.\nThe comparison of the average ranks is in general only a heuristic indicator, because the number of correct parses\n2About 1% of the longest Flyspeck formulas were removed from the evaluation to keep the parsing times manageable.\nfound differ so significantly between the methods.3 However, since the number of parses is higher in the betterranking methods, this improvement is also relevant. The average rank of the best subtree-based method (depth 6) is only about 56% of the context-free method. The results of the best method say that for 68% of the theorems the correct parse of an ambiguous statement is among the best 20 parses, and its average rank among them is 2.13."}, {"heading": "ATP Evaluation", "text": "In the ATP evaluation we measure how many of the correctly parsed formulas the HOL(y)Hammer system can prove, and thus help to confirm their validity. While the machinelearning evaluation is for simplicity done by randomization, regardless of the chronological order of the Flyspeck theorems, in the ATP evaluation we only allow facts that were already proved in Flyspeck before the currently parsed formula. Otherwise the theorem-proving task becomes too easy, because the premise-selection algorithm will likely select the theorem itself as the most relevant premise. Since this involves large amount of computation, we only compare the best new subtree-based method (depth 6) from Table 1 (subtree-6) with the old context-free method (subtree-2).\nIn the ATP evaluation, the number of the Flyspeck theorems is reduced from 21695 to 17018. This is due to omitting definitions and duplicities during the chronological processing and ATP problem generation. For actual theorem proving, we only use a single (strongest) HOL(y)Hammer method: the distance-weighted k-nearest neighbor (k-NN) (Dudani 1976) using the strongest combination of features (Kaliszyk, Urban, and Vyskocil 2015a), with IDF-based feature weighting (Kaliszyk and Urban 2013) and 128 premises, and running Vampire 4.0 (Kova\u0301cs and Voronkov 2013). Running the full portfolio of 14 AI/ATP HOL(y)Hammer strategies for hundreds of thousands problems would be too computationally expensive.\nTable 2 shows the results. In this evaluation we also detect situations when an ambiguated Flyspeck theorem T1 is parsed as a different known Flyspeck theorem T2. We call the latter situation other library theorem (OLT). The removal of definitions and duplicitites made the difference in the top20 correctly parsed sentences even higher, going from 33.8%\n3If the context-free version parsed only a few terms, but with the best rank, its average rank would be 1, but the method would still be much worse in terms of the overall number of correctly parsed terms.\nfor subtree-2 to 63.1% in subtree-6. This is an 81% improvement. A correspondingly high increase between subtree2 and subtree-6 is also in the number of situations when the first parse is correct (or OLT) and HOL(y)Hammer can prove it using the previous Flyspeck facts. The much greater easiness of proving an existing library theorem than proving a new theorem explains the relatively high number of provable OLTs when compared to their total number of occurences. Such OLT proofs are however very easy to filter out when using HOL(y)Hammer as a semantic filter for the informal-to-formal translation."}, {"heading": "7 Conclusion and Future Work", "text": "In comparison to the first results of (Kaliszyk, Urban, and Vyskocil 2015b), we have very significantly increased the success rate of the informal-to-formal translation task on the Flyspeck corpus. The overall improvement in the number of correct parses among the top 20 is 64% and even higher when omitting duplicities and definitions (81%). The average rank of the correct parse has decreased by 44%. We believe that the contextual approach to enhancing CYK we took is rather natural (in particular more natural than lexicalization), the discrimination tree indexing scales to this task, and the performance increase is very impressive.\nFuture work includes adding further semantic checks and better probabilistic ranking subroutines directly into the parsing process. The chart-parsing algorithm is easy to extend with such checks and subroutines, and already the current semantic pruning of parse trees that have incompatible variable types is extremely important. While some semantic relations might eventually be learnable by methods such as recurrent neural networks (RNNs), we believe that the current approach allows more flexible experimenting and nontrivial integration and feedback loops between advanced deductive and learning components. A possible use of RNNs in such a setup is for better ranking of subtrees and for global focusing of the parsing process.\nAn example of a more sophisticated deductive algorithm that should be easy to integrate is congruence closure over provably equal (or equivalent) parsing subtrees. For example, \u2018\u2018a * b * c\u2019\u2019 can be understood with different bracketing, different types of the variables and different interpretations of *. However, * is almost always associative across all types and interpretations. Human readers\nknow this, and rather than considering differently bracketed parses, they focus on the real problem, i.e., which types to assign to the variables and how to interpret the operator in the current context. To be able to emulate this ability, we would cache directly in the chart parsing algorithm the results of large-theory ATP runs on many previously encountered equalities, and use them for fast congruence closure over the subtrees."}], "references": [{"title": "A Compendium of Continuous Lattices in MIZAR", "author": ["G. Bancerek", "P. Rudnicki"], "venue": "J. Autom. Reasoning 29(3-4):189\u2013224.", "citeRegEx": "Bancerek and Rudnicki,? 2002", "shortCiteRegEx": "Bancerek and Rudnicki", "year": 2002}, {"title": "Hammering towards QED", "author": ["J.C. Blanchette", "C. Kaliszyk", "L.C. Paulson", "J. Urban"], "venue": "J. Formalized Reasoning 9(1):101\u2013148.", "citeRegEx": "Blanchette et al\\.,? 2016", "shortCiteRegEx": "Blanchette et al\\.", "year": 2016}, {"title": "Three generative, lexicalised models for statistical parsing", "author": ["M. Collins"], "venue": "Cohen, P. R., and Wahlster, W., eds., 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, 7-12 July", "citeRegEx": "Collins,? 1997", "shortCiteRegEx": "Collins", "year": 1997}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "JASIS 41(6):391\u2013407.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "The distance-weighted k-nearest-neighbor rule", "author": ["S.A. Dudani"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on SMC6(4):325\u2013327.", "citeRegEx": "Dudani,? 1976", "shortCiteRegEx": "Dudani", "year": 1976}, {"title": "Packaging mathematical structures", "author": ["F. Garillot", "G. Gonthier", "A. Mahboubi", "L. Rideau"], "venue": "Berghofer, S.; Nipkow, T.; Urban, C.; and Wenzel, M., eds., Theorem Proving in Higher Order Logics, 22nd International Conference, TPHOLs 2009, Munich, Germany, August 17-20, 2009. Proceedings, volume 5674 of", "citeRegEx": "Garillot et al\\.,? 2009", "shortCiteRegEx": "Garillot et al\\.", "year": 2009}, {"title": "A language of patterns for subterm selection", "author": ["G. Gonthier", "E. Tassi"], "venue": "Beringer, L., and Felty, A. P., eds., Interactive Theorem Proving - Third International Conference, ITP 2012, Princeton, NJ, USA, August 13-15, 2012. Proceedings, volume 7406 of Lecture Notes in Computer Science, 361\u2013376. Springer.", "citeRegEx": "Gonthier and Tassi,? 2012", "shortCiteRegEx": "Gonthier and Tassi", "year": 2012}, {"title": "A machine-checked proof of the Odd Order Theorem", "author": ["G. Gonthier", "A. Asperti", "J. Avigad", "Y. Bertot", "C. Cohen", "F. Garillot", "S.L. Roux", "A. Mahboubi", "R. O\u2019Connor", "S.O. Biha", "I. Pasca", "L. Rideau", "A. Solovyev", "E. Tassi", "L. Th\u00e9ry"], "venue": null, "citeRegEx": "Gonthier et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gonthier et al\\.", "year": 2013}, {"title": "Mizar in a nutshell", "author": ["A. Grabowski", "A. Korni\u0142owicz", "A. Naumowicz"], "venue": "J. Formalized Reasoning 3(2):153\u2013245.", "citeRegEx": "Grabowski et al\\.,? 2010", "shortCiteRegEx": "Grabowski et al\\.", "year": 2010}, {"title": "Input transformations and resolution implementation techniques for theorem-proving in first-order logic", "author": ["S. Greenbaum"], "venue": "Ph.D. Dissertation, University of Illinois at Urbana-Champaign.", "citeRegEx": "Greenbaum,? 1986", "shortCiteRegEx": "Greenbaum", "year": 1986}, {"title": "Constructive type classes in isabelle", "author": ["F. Haftmann", "M. Wenzel"], "venue": "Altenkirch, T., and McBride, C., eds., Types for Proofs and Programs, International Workshop, TYPES 2006, Nottingham, UK, April 18-21, 2006, Revised Selected Papers, volume 4502 of Lecture Notes in Computer Science, 160\u2013174. Springer.", "citeRegEx": "Haftmann and Wenzel,? 2006", "shortCiteRegEx": "Haftmann and Wenzel", "year": 2006}, {"title": "A formal proof of the Kepler", "author": ["T.C. Hales", "M. Adams", "G. Bauer", "D.T. Dang", "J. Harrison", "T.L. Hoang", "C. Kaliszyk", "V. Magron", "S. McLaughlin", "T.T. Nguyen", "T.Q. Nguyen", "T. Nipkow", "S. Obua", "J. Pleso", "J. Rute", "A. Solovyev", "A.H.T. Ta", "T.N. Tran", "D.T. Trieu", "J. Urban", "K.K. Vu", "R. Zumkeller"], "venue": null, "citeRegEx": "Hales et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hales et al\\.", "year": 2015}, {"title": "Dense Sphere Packings: A Blueprint for Formal Proofs, volume 400 of London Mathematical Society Lecture Note Series", "author": ["T. Hales"], "venue": "Cambridge University Press.", "citeRegEx": "Hales,? 2012", "shortCiteRegEx": "Hales", "year": 2012}, {"title": "History of interactive theorem proving", "author": ["J. Harrison", "J. Urban", "F. Wiedijk"], "venue": "Siekmann, J. H., ed., Computational Logic, volume 9 of Handbook of the History of Logic. Elsevier. 135\u2013214.", "citeRegEx": "Harrison et al\\.,? 2014", "shortCiteRegEx": "Harrison et al\\.", "year": 2014}, {"title": "HOL Light: A tutorial introduction", "author": ["J. Harrison"], "venue": "Srivas, M. K., and Camilleri, A. J., eds., FMCAD, volume 1166 of LNCS, 265\u2013269. Springer.", "citeRegEx": "Harrison,? 1996", "shortCiteRegEx": "Harrison", "year": 1996}, {"title": "Stronger automation for Flyspeck by feature weighting and strategy evolution", "author": ["C. Kaliszyk", "J. Urban"], "venue": "Blanchette, J. C., and Urban, J., eds., PxTP 2013, volume 14 of EPiC Series, 87\u201395. EasyChair.", "citeRegEx": "Kaliszyk and Urban,? 2013", "shortCiteRegEx": "Kaliszyk and Urban", "year": 2013}, {"title": "Learning-assisted automated reasoning with Flyspeck", "author": ["C. Kaliszyk", "J. Urban"], "venue": "J. Autom. Reasoning 53(2):173\u2013213.", "citeRegEx": "Kaliszyk and Urban,? 2014", "shortCiteRegEx": "Kaliszyk and Urban", "year": 2014}, {"title": "Efficient semantic features for automated reasoning over large theories", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": "Yang, Q., and Wooldridge, M., eds., IJCAI\u201915, 3084\u20133090. AAAI Press.", "citeRegEx": "Kaliszyk et al\\.,? 2015a", "shortCiteRegEx": "Kaliszyk et al\\.", "year": 2015}, {"title": "Learning to parse on aligned corpora (rough diamond)", "author": ["C. Kaliszyk", "J. Urban", "J. Vyskocil"], "venue": "Urban, C., and Zhang, X., eds., Interactive Theorem Proving - 6th International Conference, ITP 2015, Nanjing, China, August 24-27, 2015, Proceedings, volume 9236 of Lecture Notes in Computer Science, 227\u2013233.", "citeRegEx": "Kaliszyk et al\\.,? 2015b", "shortCiteRegEx": "Kaliszyk et al\\.", "year": 2015}, {"title": "seL4: formal verification of an operating-system kernel", "author": ["G. Klein", "J. Andronick", "K. Elphinstone", "G. Heiser", "D. Cock", "P. Derrin", "D. Elkaduwe", "K. Engelhardt", "R. Kolanski", "M. Norrish", "T. Sewell", "H. Tuch", "S. Winwood"], "venue": "Commun. ACM 53(6):107\u2013 115.", "citeRegEx": "Klein et al\\.,? 2010", "shortCiteRegEx": "Klein et al\\.", "year": 2010}, {"title": "First-order theorem proving and Vampire", "author": ["L. Kov\u00e1cs", "A. Voronkov"], "venue": "Sharygina, N., and Veith, H., eds., CAV, volume 8044 of LNCS, 1\u201335. Springer.", "citeRegEx": "Kov\u00e1cs and Voronkov,? 2013", "shortCiteRegEx": "Kov\u00e1cs and Voronkov", "year": 2013}, {"title": "To CNF or not to CNF? an efficient yet presentable version of the CYK algorithm", "author": ["M. Lange", "H. Lei\u00df"], "venue": "Informatica Didactica 8.", "citeRegEx": "Lange and Lei\u00df,? 2009", "shortCiteRegEx": "Lange and Lei\u00df", "year": 2009}, {"title": "Formal verification of a realistic compiler", "author": ["X. Leroy"], "venue": "Commun. ACM 52(7):107\u2013115.", "citeRegEx": "Leroy,? 2009", "shortCiteRegEx": "Leroy", "year": 2009}, {"title": "Handbook of Automated Reasoning (in 2 volumes)", "author": ["J.A. Robinson", "A. Voronkov", "eds"], "venue": null, "citeRegEx": "Robinson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Robinson et al\\.", "year": 2001}, {"title": "Commutative algebra in the Mizar system", "author": ["P. Rudnicki", "C. Schwarzweller", "A. Trybulec"], "venue": "J. Symb. Comput. 32(1/2):143\u2013 169.", "citeRegEx": "Rudnicki et al\\.,? 2001", "shortCiteRegEx": "Rudnicki et al\\.", "year": 2001}, {"title": "Formal mathematics on display: A wiki for Flyspeck", "author": ["C. Tankink", "C. Kaliszyk", "J. Urban", "H. Geuvers"], "venue": "Carette, J.; Aspinall, D.; Lange, C.; Sojka, P.; and Windsteiger, W., eds., MKM/Calculemus/DML, volume 7961 of LNCS, 152\u2013167. Springer.", "citeRegEx": "Tankink et al\\.,? 2013", "shortCiteRegEx": "Tankink et al\\.", "year": 2013}, {"title": "The Isabelle framework", "author": ["M. Wenzel", "L.C. Paulson", "T. Nipkow"], "venue": "Mohamed, O. A.; Mu\u00f1oz, C. A.; and Tahar, S., eds., TPHOLs, volume 5170 of LNCS, 33\u201338. Springer.", "citeRegEx": "Wenzel et al\\.,? 2008", "shortCiteRegEx": "Wenzel et al\\.", "year": 2008}, {"title": "Recognition and parsing of context-free languages in time n\u02c63", "author": ["D.H. Younger"], "venue": "Information and Control 10(2):189\u2013208.", "citeRegEx": "Younger,? 1967", "shortCiteRegEx": "Younger", "year": 1967}, {"title": "Understanding informal mathematical discourse", "author": ["C. Zinn"], "venue": "Ph.D. Dissertation, University of Erlangen-Nuremberg.", "citeRegEx": "Zinn,? 2004", "shortCiteRegEx": "Zinn", "year": 2004}], "referenceMentions": [{"referenceID": 11, "context": "Despite recent impressive formalizations such as the Formal Proof of the Kepler conjecture (Flyspeck) (Hales et al. 2015), FeitThompson (Gonthier et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 7, "context": "2015), FeitThompson (Gonthier et al. 2013), seL4 (Klein et al.", "startOffset": 20, "endOffset": 42}, {"referenceID": 19, "context": "2013), seL4 (Klein et al. 2010), CompCert (Leroy 2009), and CCL (Bancerek and Rudnicki 2002), formalizing proofs is still largely unappealing to mathematicians.", "startOffset": 12, "endOffset": 31}, {"referenceID": 22, "context": "2010), CompCert (Leroy 2009), and CCL (Bancerek and Rudnicki 2002), formalizing proofs is still largely unappealing to mathematicians.", "startOffset": 16, "endOffset": 28}, {"referenceID": 0, "context": "2010), CompCert (Leroy 2009), and CCL (Bancerek and Rudnicki 2002), formalizing proofs is still largely unappealing to mathematicians.", "startOffset": 38, "endOffset": 66}, {"referenceID": 1, "context": "While research on AI and strong automation over large theories has taken off in the last decade (Blanchette et al. 2016), there has been so far little progress in automating the understanding of informal LTEX-written and ambiguous mathematical writings.", "startOffset": 96, "endOffset": 120}, {"referenceID": 14, "context": "Among the state-of-the-art Interactive Theorem Proving (ITP) systems such as HOL (Light) (Harrison 1996), Isabelle (Wenzel, Paulson, and Nipkow 2008), Mizar (Grabowski, Korni\u0142owicz, and Naumowicz 2010) and Coq (coq ), none includes automated parsing, instead relying on sophisticated formal languages and mechanisms (Garillot et al.", "startOffset": 89, "endOffset": 104}, {"referenceID": 5, "context": "Among the state-of-the-art Interactive Theorem Proving (ITP) systems such as HOL (Light) (Harrison 1996), Isabelle (Wenzel, Paulson, and Nipkow 2008), Mizar (Grabowski, Korni\u0142owicz, and Naumowicz 2010) and Coq (coq ), none includes automated parsing, instead relying on sophisticated formal languages and mechanisms (Garillot et al. 2009; Gonthier and Tassi 2012; Haftmann and Wenzel 2006; Rudnicki, Schwarzweller, and Trybulec 2001).", "startOffset": 316, "endOffset": 433}, {"referenceID": 6, "context": "Among the state-of-the-art Interactive Theorem Proving (ITP) systems such as HOL (Light) (Harrison 1996), Isabelle (Wenzel, Paulson, and Nipkow 2008), Mizar (Grabowski, Korni\u0142owicz, and Naumowicz 2010) and Coq (coq ), none includes automated parsing, instead relying on sophisticated formal languages and mechanisms (Garillot et al. 2009; Gonthier and Tassi 2012; Haftmann and Wenzel 2006; Rudnicki, Schwarzweller, and Trybulec 2001).", "startOffset": 316, "endOffset": 433}, {"referenceID": 10, "context": "Among the state-of-the-art Interactive Theorem Proving (ITP) systems such as HOL (Light) (Harrison 1996), Isabelle (Wenzel, Paulson, and Nipkow 2008), Mizar (Grabowski, Korni\u0142owicz, and Naumowicz 2010) and Coq (coq ), none includes automated parsing, instead relying on sophisticated formal languages and mechanisms (Garillot et al. 2009; Gonthier and Tassi 2012; Haftmann and Wenzel 2006; Rudnicki, Schwarzweller, and Trybulec 2001).", "startOffset": 316, "endOffset": 433}, {"referenceID": 28, "context": "The past work in this direction \u2013 most notably by Zinn (Zinn 2004) \u2013 has often been cited as discouraging from such efforts.", "startOffset": 55, "endOffset": 66}, {"referenceID": 12, "context": "Suitable aligned corpora are starting to appear today, the major example being the Flyspeck project and in particular its alignment (by Hales) with the detailed informal Blueprint for Formal Proofs (Hales 2012).", "startOffset": 198, "endOffset": 210}, {"referenceID": 12, "context": "2 Informalized Flyspeck and PCFG The ultimate goal of the informal-to-formal traslation is to automatically learn parsing on informal LTEX formulas that have been aligned with their formal counterparts, as for example done by Hales for his informal and formal Flyspeck texts (Hales 2012; Tankink et al. 2013).", "startOffset": 275, "endOffset": 308}, {"referenceID": 25, "context": "2 Informalized Flyspeck and PCFG The ultimate goal of the informal-to-formal traslation is to automatically learn parsing on informal LTEX formulas that have been aligned with their formal counterparts, as for example done by Hales for his informal and formal Flyspeck texts (Hales 2012; Tankink et al. 2013).", "startOffset": 275, "endOffset": 308}, {"referenceID": 27, "context": "A frequently used CFG algorithm is the CYK (Cocke\u2013Younger\u2013Kasami) chart-parser (Younger 1967), based on bottom-up parsing.", "startOffset": 79, "endOffset": 93}, {"referenceID": 21, "context": "The transformation to CNF can cause an exponential blowup of the grammar, however, an improved version of CYK gets around this issue (Lange and Lei\u00df 2009).", "startOffset": 133, "endOffset": 154}, {"referenceID": 3, "context": "It is however likely that even better semantic categories can be developed, based on more involved statistical and semantic analysis of the data such as latent semantics (Deerwester et al. 1990).", "startOffset": 170, "endOffset": 194}, {"referenceID": 16, "context": "This is followed by proof and disproof attempts by the HOL(y)Hammer system (Kaliszyk and Urban 2014), using all the semantic knowledge available in the Flyspeck library (about 22000 theorems).", "startOffset": 75, "endOffset": 100}, {"referenceID": 2, "context": "One method that is frequently used for dealing with similar problems in the NLP domain is grammar lexicalization (Collins 1997).", "startOffset": 113, "endOffset": 127}, {"referenceID": 9, "context": "Efficient Implementation of Deeper Subtrees Discrimination trees (Robinson and Voronkov 2001), as first implemented by Greenbaum (Greenbaum 1986), index terms in a trie, which keeps single path-strings at each of the indexed terms.", "startOffset": 129, "endOffset": 145}, {"referenceID": 4, "context": "For actual theorem proving, we only use a single (strongest) HOL(y)Hammer method: the distance-weighted k-nearest neighbor (k-NN) (Dudani 1976) using the strongest combination of features (Kaliszyk, Urban, and Vyskocil 2015a), with IDF-based feature weighting (Kaliszyk and Urban 2013) and 128 premises, and running Vampire 4.", "startOffset": 130, "endOffset": 143}, {"referenceID": 15, "context": "For actual theorem proving, we only use a single (strongest) HOL(y)Hammer method: the distance-weighted k-nearest neighbor (k-NN) (Dudani 1976) using the strongest combination of features (Kaliszyk, Urban, and Vyskocil 2015a), with IDF-based feature weighting (Kaliszyk and Urban 2013) and 128 premises, and running Vampire 4.", "startOffset": 260, "endOffset": 285}, {"referenceID": 20, "context": "0 (Kov\u00e1cs and Voronkov 2013).", "startOffset": 2, "endOffset": 28}], "year": 2016, "abstractText": "We study methods for automated parsing of informal mathematical expressions into formal ones, a main prerequisite for deep computer understanding of informal mathematical texts. We propose a context-based parsing approach that combines efficient statistical learning of deep parse trees with their semantic pruning by type checking and large-theory automated theorem proving. We show that the methods very significantly improve on previous results in parsing theorems from the Flyspeck corpus.", "creator": "TeX"}}}