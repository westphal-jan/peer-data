{"id": "1612.00901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2016", "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition", "abstract": "Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most object-role combinations are rare, and current state-of-the-art models significantly underperform in this sparse data regime. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across role-noun combinations and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.", "histories": [["v1", "Sat, 3 Dec 2016 00:31:52 GMT  (3527kb,D)", "http://arxiv.org/abs/1612.00901v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["mark yatskar", "vicente ordonez", "luke zettlemoyer", "ali farhadi"], "accepted": false, "id": "1612.00901"}, "pdf": {"name": "1612.00901.pdf", "metadata": {"source": "CRF", "title": "Commonly Uncommon: Semantic Sparsity in Situation Recognition", "authors": ["Mark Yatskar", "Vicente Ordonez", "Luke Zettlemoyer", "Ali Farhadi"], "emails": ["ali]@cs.washington.edu,", "vicente@cs.virginia.edu"], "sections": [{"heading": "1. Introduction", "text": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces. In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44]. In this paper, we consider situation\nrecognition, a prototypical structured classification problem with significant semantic sparsity, and develop new models and semantic data augmentation techniques that significantly improve performance by better modeling the underlying semantic structure of the task.\nSituation recognition [44] is the task of producing structured summaries of what is happening in images, including activities, objects and the roles those objects play within the activity. This problem can be challenging because many activities, such as carrying, have very open ended semantic roles, such as item, the thing being carried (see Figure 1); nearly any object can be carried and the training data will never contain all possibilities. This is a prototypical instance of semantic sparsity: rare outputs constitute a large portion of required predictions (35% in the imSitu dataset [44], see Figure 2), and current state-of-the-art per-\n1\nar X\niv :1\n61 2.\n00 90\n1v 1\n[ cs\n.C V\n] 3\nD ec\nformance for situation recognition drops significantly when even one participating object has few samples for it\u2019s role (see Figure 3). We propose to address this challenge in two ways by (1) building models that more effectively share examples of objects between different roles and (2) semantically augmenting our training set to fill in rarely represented noun-role combinations.\nWe introduce a new compositional Conditional Random Field formulation (CRF) to reduce the effects of semantic sparsity by encouraging sharing between nouns in different roles. Like previous work [44], we use a deep neural network to directly predict factors in the CRF. In such models, required factors for the CRF are predicted using a global image representation through a linear regression unique to each factor. In contrast, we propose a novel tensor composition function that uses low dimensional representations of nouns and roles, and shares weights across all roles and nouns to score combinations. Our model is compositional, independent representations of nouns and roles are combined to predict factors, and allows for a globally shared representation of nouns across the entire CRF.\nThis model is trained with a new form of semantic data augmentation, to provide extra training samples for rarely observed noun-role combinations. We show that it is possible to generate short search queries that correspond to partial situations (i.e. \u201cman carrying baby\u201d or \u201ccarrying on back\u201d for the situations in Figure 1) which can be used for web image retrieval. Such noisy data can then be incorporated in pre-training by optimizing marginal likelihood, effectively performing a soft clustering of values for unlabeled aspects of situations. This data also supports, as we will show, self training where model predictions are used to prune the set of images before training the final predictor.\nExperiments on the imSitu dataset [44] demonstrate that our new compositional CRF and semantic augmentation techniques reduce the effects of semantic sparsity, with strong gains for relatively rare configurations. We show that each contribution helps significantly, and that the combined approach improves performance relative to a strong CRF baseline by 6.23% and 9.57% on top-5 verb and nounrole accuracy, respectively. On uncommon predictions, our methods provide a relative improvement of 8.76% on average across all measures. Together, these experiments demonstrate the benefits of effectively targeting semantic sparsity in structured classification tasks."}, {"heading": "2. Background", "text": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d. In general, formulations build on semantic role labelling [17], a problem in natural language processing where verbs are automatically paired with their arguments in a sentence (for example, see [8]). Each semantic role corresponds to a question about an event, (for example, in the first image of Figure 1, the semantic role agent corresponds to \u201cwho is doing the carrying?\u201d and agentpart corresponds to \u201chow is the item being carried?\u201d).\nWe study situation recognition in imSitu [44], a largescale dataset of human annotated situations containing over 500 activities, 1,700 roles, 11,000 nouns, 125,000 images. imSitu images are collected to cover a diverse set of situations. For example, as seen in Figure 2, 35% of situations annotated in the imSitu development set contain at\nleast one rare role-noun pair. Situation recognition in imSitu is a strong test bed for evaluating methods addressing semantic sparsity: it is large scale, structured, easy to evaluate, and has a clearly measurable range of semantic sparsity across different verbs and roles. Furthermore, as seen in Figure 3, semantic sparsity is a significant challenge for current situation recognition models.\nFormal Definition In situation recognition, we assume a discrete sets of verbs V , nouns N , and frames F . Each frame f \u2208 F is paired with a set of semantic roles Ef . Every element in V is mapped to exactly one f . The verb set V and frame set F are derived from FrameNet [13], a lexicon for semantic role labeling, while the noun set N is drawn from WordNet [34]. Each semantic role e \u2208 Ef is paired with a noun value ne \u2208 N \u222a{\u2205}, where \u2205 indicates the value is either not known or does not apply. The set of pairs of semantic roles and their values is called a realized frame, Rf = {(e, ne) : e \u2208 Ef}. Realized frames are valid only if each e \u2208 Ef is assigned exactly one noun ne.\nGiven an image, the task is to predict a situation, S = (v,Rf ), specified by a verb v \u2208 V and a valid realized frame Rf , where f refers to a frame mapped by v . For example, in the first image of Figure 1, the predicted situations is S = (carrying, {(agent,man), (item,baby), (agentpart,chest), (place,outside)})."}, {"heading": "3. Methods", "text": "This section presents our compositional CRFs and semantic data augmentation techniques."}, {"heading": "3.1. Compositional Conditional Random Field", "text": "Figure 4 shows an overview of our compositional conditional random field model, which is described below.\nConditional Random Field Our CRF for predicting a situation, S = (v,Rf ), given an image i, decomposes over the verb v and semantic role-value pairs (e, ne) in the realized frame Rf = {(e, ne) : e \u2208 Ef}, similarly to previous work [44]. The full distribution, with potentials for verbs \u03c8v and semantic roles \u03c8e takes the form:\np(S|i; \u03b8) \u221d \u03c8v(v, i; \u03b8) \u220f\n(e,ne)\u2208Rf\n\u03c8e(v, e, ne, i; \u03b8) (1)\nThe CRF admits efficient inference: we can enumerate all verb-semantic roles that occur and then sum all possible semantic role values that occurred in a dataset.\nEach potential in the CRF is log linear:\n\u03c8v(v, i; \u03b8) = e \u03c6v(v,i,\u03b8) (2)\n\u03c8e(v, e, ne, i; \u03b8) = e \u03c6e(v,e,ne,i,\u03b8) (3)\nwhere \u03c6e and \u03c6v encode scores computed by a neural network. To learn this model, we assume that for an image i in dataset Q there can, in general, be a set Ai of possible ground truth situations 1. We optimize the log-likelihood of observing at least one situation S \u2208 Ai:\n\u2211 i\u2208Q log ( 1\u2212 \u220f S\u2208Ai (1\u2212 p(S|i; \u03b8)) )\n(4)\nCompositional Tensor Potential In previous work, the CRF potentials (Equation 2 and 3 ) are computed using a global image representation, a p-dimensional image vector gi \u2208 Rp, derived by the VGG convolutional neural network [40]. Each potential value is computed by a linear regression with parameters, \u03b8, unique for each possible decision of verb and verb-role-noun (we refer to this as image regression in Figure 4), for example for the verb-role-noun potential in Equation 3:\n\u03c6e(v, e, ne, i, \u03b8) = g T i \u03b8v,e,ne (5)\nSuch a model does not directly represent the fact that nouns are reused between different roles, although the underlying neural network could hypothetically learn to encode such reuse during fine tuning. Instead, we introduce compositional potentials that make such reuse explicit.\nTo formulate our compositional potential, we introduce a set of m-dimensional vectors D = {dn \u2208 Rm|n \u2208 N}, one vector for each noun in N , the set of nouns. We create a set matrices T = {H(v,e) \u2208 Rp\u00d7o|(v, e) \u2208 Ef}, one matrix for each verb, semantic role pair occurring in all frames Ef , that map image representations to o-dimensional verbrole representations. Finally, we introduce a tensor of global composition weights, C \u2208 Rm\u00d7o\u00d7p. We define a tensor weighting function, T , which takes as input a verb, v, semantic role, e, noun, n, and image representation, gi as:\nT (v, e, n, gi) = C (dn \u2297 gTi H(v,e) \u2297 gi) (6)\nThe tensor weighting function constructs an image specific verb-role representation by multiplying the global image vector and the verb-role matrix gTi H(v,e). Then, it combines a global noun representation, the image specific role representation, and the global image representation with outer products. Finally, it weights each dimension of the outer product with a weight from C. The weights in C indicate which features of the 3-way outer product are important. The final potential is produced by summing up all of the elements of the tensor produced by T :\n\u03c6e(v, e, ne, i) =\nM\u2211\nx=0\nO\u2211\ny=0\nP\u2211\nz=0\nT (v, e, ne, gi)[x, y, z] (7)\n1imSitu provides three realized frames per example image.\nThe tensor produced by T in general will be high dimensional and very expressive. This allows use of small dimensionality representations, making the function more robust to small numbers of samples for each noun.\nThe potential defined in Equation 7 can be equivalently formulated as :\n\u03c6e(v, e, ne, i) = g T i A(dne \u2297 gTi H(v,e)) (8)\nWhere A is a matrix with the same parameters as C but flattened to layout the noun and role dimensions together. By aligning terms with Equation 5, one can see that tensor potential offers an alternative parametrized to the linear regression that uses many more general purpose parameters, those of C. Furthermore, it eliminates any one parameter from ever being uniquely associated with one regression, instead compositionally using noun and verb-role representations to build up the parameters of the regression."}, {"heading": "3.2. Semantic Data Augmentation", "text": "Situation recognition is strongly connected to language. Each situation can be thought of as simple declarative sentence about an activity happening in an image. For example, the first situation in Figure 1 could be expressed as \u201cman carrying baby on chest outside\u201d by knowing the prototypical ordering of semantic roles around verbs and inserting prepositions. This relationship can be used to reduce se-\nmantic sparsity by using image search to find images that could contain the elements of a situations.\nWe convert annotated situations to phrases for semantic augmentation by exhaustively enumerating all possible sub-pieces of realized situations that occur in the imSitu training set (see Section 4 for implementation details). For example, in first situation of Figure 1, we get the pieces: (carrying, {(agent,man)}), (carrying, {(agent,man), (item,baby)}), ect. Each of these substructures is converted deterministically to a phrase using a template specific for every verb. For example, the template for carrying is \u201c{agent} carrying {item} {with agentpart} {in place}.\u201d Partial situations are realized into phrases by taking the first gloss in Wordnet of the synset associated with every noun in the substructure, inserting them into the corresponding slots of the template, and discarding unused slots. For example, the phrases for the sub-pieces above are realized as \u201cman carrying\u201d and \u201cman carrying baby.\u201d These phrases are used to retrieve images from Google image search and construct a set, W = {(i, v, Rf )}, of images annotated with a verb and partially complete realized frames, by assigning retrieved images to the sub-piece that generated the retrieval query.2\n2While these templates do not generate completely fluent phrases, preliminary experiments found them sufficiently accurate for image search because often no phrase could retrieve correct images. Longer phrases tended to have much lower precision.\nPre-training Images retrieved from the web can be incorporated in a pre-training phase. The images retrieved only have partially specified realized situations as labels. To account for this, we instead compute the marginal likelihood, p\u0302, of the partially observed situations in W :\np\u0302(S|i; \u03b8) \u221d \u03c8v(v, i; \u03b8) \u220f\n(e,ne)\u2208Rf\n\u03c8e(v, e, ne, i; \u03b8)\n\u00d7 \u220f\ne/\u2208Rf\u2227e\u2208Ef\n\u2211\nn\n\u03c8e(v, e, n, i; \u03b8) (9)\nDuring pretraining, we optimize the marginal log-likelihood of W . This objective provides a partial clustering over the unobserved roles left unlabeled during the retrieval process.\nSelf Training Images retrieved from the web contain significant noise. This is especially true for role-noun combinations that occur infrequently, limiting their utility for pretraining. Therefore, we also consider filtering images in W after a model has already been trained on fully supervised data from imSitu. We rank images in W according to p\u0302 as computed by the trained model and filter all those not in the top-k for every unique Rf in W . We then pretrain on this subset of W , train again on imSitu, and then increase k. We repeat this process until the model no longer improves."}, {"heading": "4. Experimental Setup", "text": "Models All models were implemented in Caffe [21] and use a pretrained VGG network [40] for the base image representation with the final two fully connected layers replaced with two fully connected layers of dimensionality 1024. We finetune all layers of VGG for all models. For our tensor potential we use noun embedding size, m = 32, and role embedding size o = 32, and the final layer of our VGG network as the global image representation where p = 1024. Larger values of m and o did seem to improve results but were too slow to pretrain so we omit them. In experiments where we use the image regression in conjunction with a compositional potential, we remove regression parameters associated with combinations seen fewer than 10 times on the imSitu training set to reduce overfitting.\nBaseline We compare our models to two alternative methods for introducing effective sharing between nouns. The first baseline (Noun potential in Table 1 and 2) adds a potential into the baseline CRF for nouns independent of roles. We modify the probability, from Equation 9 of a situation, S, given an image i, to not only decompose by pairs of roles, e and nouns ne in a realized frame Rf , but also nouns ne: p(S|i; \u03b8) \u221d \u03c8v(v, i; \u03b8) \u220f\n(e,ne)\u2208Rf\n\u03c8e(v, e, ne, i; \u03b8)\u03c8ne(ne, i)\n(10)\nThe added potential, \u03c8ne , is computed using a regression from a global image representation for each unique ne.\nThe second baseline we consider is compositional but does not use a tensor based composition method. The model instead constructs many verb-role representations and combines them with noun representations using inner-products (Inner product composition in Table 1 and 2). In this model, as in the tensor model in Section 3, we use a global image representation gi \u2208 Rp and a set noun vectors, dn \u2208 Rm for every noun n. We also assume t verb-role matrices Ht,v,e \u2208 Ro\u00d7p for every verb-role in Ef . We compute the corresponding potential as in Equation 11:\n\u03c6e(v, e, ne, i) = \u2211\nk\ndTneH(k,v,e)qi (11)\nThe model is motivated by compositional models used for semantic role labeling [14] and allows us to trade-off the need to reduce parameters associated with nouns and expressivity. We grid search values of t such that t \u00b7 o was at most 256, the largest size network we could afford to run and o = m, a requirement on the inner product. We found the best setting at t = 16, o = m = 16.\nDecoding We experimented with two decoding methods for finding the best scoring situation under the CRF models. Systems which used the compositional potentials performed better when first predicting a verb vm using the max-marginal over semantic roles: vm = argmaxv \u2211 (e,ne)\np(v,Rf |i) and then predict a realized frame, Rmf , with max score for v\nm: Rmf = argmaxRf p(v\nm, Rf |i). All other systems performed better maximizing jointly for both verb and realized frame.\nOptimization All models were trained with stochastic gradient descent with momentum 0.9 and weight decay 5e4. Pretraining in semantic augmentation was conducted with initial learning rate of 1e-3, gradient clipping at 100, and batch size 360. When training on imSitu data, we use an initial learning rate of 1e-5. For all models, the learning rate was reduced by a factor of 10 when the model did not improve on the imSitu dev set.\nSemantic Augmentation In experiments with semantic augmentation, images were retrieved using Google image search. We retrieved 200 medium sized, full-color, safe search filtered images per query phrase. We produced over 1.5 million possible query phrases from the imSitu training set, the majority extremely rare. We limited the phrases to any that occur between 10 and 100 times in imSitu and for phrases that occur between 3 and 10 times we accepted only those containing at most one noun. Roughly 40k phrases\nwere used to retrieve 5 million images from the web. All duplicate images occurring in imSitu were removed. For pretraining, we ran all experiments up to 50k updates (roughly 4 epochs). For self training, we only self train on rare realized frames (those 10 or fewer times in imSitu train set). Self training yielded diminishing gains after two iterations and we ran the first iteration at k=10 and the second at k=20.\nEvaluation We use the standard data split for imSitu[44] with 75k train, 25k development, and 25k test images. We follow the evaluation setup defined for imSitu, evaluating verb predictions (verb) and semantic role-value pair predictions (value) and full structure correctness (value-all). We report accuracy at top-1, top-5 and given the ground truth verb and the average across all measures (mean). We also report performance for examples requiring rare (10 or fewer examples in the imSitu training set) predictions."}, {"heading": "5. Results", "text": "Compositional Tensor Potential Our results on the full imSitu dev set are presented in Table 1 in rows 1-5. Overall results demonstrate that adding a noun potential (row 2) and our baseline composition model (row 3) are ineffective and perform worse than the baseline CRF (row 1). We hypothesize that systematic variation in object appearance\nbetween roles is challenging for these models. Our tensor composition model (row 4) is able to better capture such variation and effectively share information among nouns, reflected by improvements in value and value-all accuracy given ground truth verbs while maintaining high top-1 and top-5 verb accuracy. However, as expected, many situations cannot be predicted only compositionally based on nouns (consider that a horse sleeping looks very different than a horse swimming and nothing like a person sleeping). Combination of the image regression potential and our tensor composition potential (row 5) yields the best performance, indicating they are modeling complementary aspects of the problem. Our final model (row 5) only trained on imSitu data outperforms the baseline on every measure, improving over 1.70 points overall.\nResults on the rare portion of the imSitu dataset are presented in Table 2 in rows 1-5. Our final model (row 5) provides the best overall performance (mean column) on rare cases among models trained only on imSitu data, improving by 0.64 points on average. All models struggle to get correctly entire structures (value-all columns), indicating rare predictions are extremely hard to get completely correct while the baseline model which only uses image regression potentials performs the best. We hypothesize that image regression potentials may allow the model to more easily coordinate predictions across roles simultaneously because\nrole-noun combinations that always co-occur will always have the same set of regression weights.\nSemantic Data Augmentation Our results on the full imSitu development set are presented in Table 1 in rows 6- 8. Overall results indicate that semantic data augmentation helps all models, while our tensor model (row 7) benefits more than the baseline (row 6). Self training improves the tensor model slightly (row 8), making it perform better on top-1 and top-5 predictions but hurting performance given gold verbs. On average, our final model outperforms the baseline CRF trained on identical data by 2.04 points.\nResults on the rare portion of the imSitu dataset are presented in Table 2 in rows 6-8. Surprisingly, on rare cases semantic augmentation hurts the baseline CRF (line 6). Rare instance image search results are extremely noisy. On close inspection, many of the returned results do not contain the target activity at all but instead contain target nouns. We hypothesize that without an effective global noun representation, the baseline CRF cannot extract meaningful information from such extra data. On the other hand, our tensor model (line 7) improves on these rare cases overall and with self training improves further (line 8).\nOverall Results Experiments show that (a) our tensor model is able perform better in comparable data settings, (b) our semantic augmentation techniques largely benefit all models, and (c) our tensor model benefits more from semantic augmentation. We also present our full performance on top-5 verb across all numbers of samples in Figure 5. While our compositional CRF with semantic augmentation outperforms the baseline CRF, both models continue to struggle on uncommon cases. Our techniques seem to give most benefit for examples requiring predictions of structures seen between 5 and 35 times, while providing somewhat less benefit to even rarer ones. It is challenging future work to make\nfurther improvements for extremely rare outputs. We also evaluated our models on the imSitu test set exactly once. The results are summarized in Table 3 for the full imSitu test set and in Table 4 for the rare portion. General trends established on the imSitu dev set are supported. We provide examples in Figure 6 of predictions our final system made on rare examples from the development set."}, {"heading": "6. Related Work", "text": "Learning to cope with semantic sparsity is closely related to zero-shot or k-shot learning. Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings. For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and\ngeneralization [42]. Recent efforts to gain insight on such issues extract subject-verb-object (SVO) triplets from captions and count prediction failures on rare tuples [3]. Our use of imSitu to study semantic sparsity circumvents the need for intermediate processing of captions and generalizes to verbs with more than two arguments.\nCompositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction. Recently, bilinear pooling [30] and compact bilinear pooling [16] have been proposed as second-order feature representations for tasks such as fine grained recognition and visual question answer. We build on such methods, using low dimensional embeddings of semantic units and expressive outer product computations.\nUsing the web as a resource for image understanding has been studied through NEIL [6], a system which continuously queries for concepts discovered in text, and Levan [10], which can create detectors from user speci-\nfied queries. Web supervision has also been explored for pretraining convolutional neural networks [5] or for finegrained bird classification [5] and common sense reasoning [38]. Yet we are the first to explore the connection between semantic sparsity and language for automatically generating queries for semantic web augmentation and we are able to show improvement on a large scale, fully supervised structured prediction task."}, {"heading": "7. Conclusion", "text": "We studied situation recognition, a prototypical instance of a structured classification problem with significant semantic sparsity. Despite the fact that the vast majority of the possible output configurations are rarely observed in the training data, we showed it was possible in introduce new compositional models that effectively share examples among required outputs and semantic data augmentation techniques that significantly improved performance. In the future, it will be important to introduce similar techniques for related problems with semantic sparsity and generalize these ideas to the zero-shot learning."}], "references": [{"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Learning to generalize to new compositions in image understanding", "author": ["Y. Atzmon", "J. Berant", "V. Kezami", "A. Globerson", "G. Chechik"], "venue": "arXiv preprint arXiv:1608.07639,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["M. Baroni", "A. Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Webly supervised learning of convolutional networks", "author": ["X. Chen", "A. Gupta"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Extracting visual knowledge from web data", "author": ["X. Chen", "A. Shrivastava", "A. Gupta. Neil"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Semi-Supervised and Latent-Variable Models of Natural Language Semantics", "author": ["D. Das"], "venue": "PhD thesis,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learning everything about anything: Webly-supervised visual concept learning", "author": ["S. Divvala", "A. Farhadi", "C. Guestrin"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["H. Fang"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Describing objects by their attributes", "author": ["A. Farhadi"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Background to framenet", "author": ["C.J. Fillmore"], "venue": "International Journal of lexicography,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Semantic role labelling with neural network factors", "author": ["N. FitzGerald"], "venue": "In EMNLP,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Compact bilinear pooling", "author": ["Y. Gao", "O. Beijbom", "N. Zhang", "T. Darrell"], "venue": "arXiv preprint arXiv:1511.06062,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Automatic labeling of semantic roles", "author": ["D. Gildea", "D. Jurafsky"], "venue": "Computational linguistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition", "author": ["S. Guadarrama"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Visual semantic role labeling", "author": ["S. Gupta", "J. Malik"], "venue": "arXiv preprint arXiv:1505.04474,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Framing image description as a ranking task: Data, models and evaluation", "author": ["M. Hodosh"], "venue": "metrics. JAIR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Referitgame: Referring to objects in photographs of natural scenes", "author": ["S. Kazemzadeh", "V. Ordonez", "M. Matten", "T.L. Berg"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Attributebased classification for zero-shot visual object categorization", "author": ["C.H. Lampert", "H. Nickisch", "S. Harmeling"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["C.H. Lampert"], "venue": "In CVPR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Is this a wampimuk", "author": ["A. Lazaridou"], "venue": "In ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Low-rank tensors for scoring dependency structures", "author": ["T. Lei", "Y. Zhang", "R. Barzilay", "T. Jaakkola"], "venue": "Association for Computational Linguistics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Predicting deep zeroshot convolutional neural networks using textual descriptions", "author": ["J. Lei Ba", "K. Swersky", "S. Fidler"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In European Conference on Computer Vision", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Bilinear cnn models for fine-grained visual recognition", "author": ["T.-Y. Lin", "A. RoyChowdhury", "S. Maji"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin"], "venue": "In ECCV", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Visual relationship detection with language priors", "author": ["C. Lu", "R. Krishna", "M. Bernstein", "L. Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Explain images with multimodal recurrent neural networks", "author": ["J. Mao"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1995}, {"title": "Im2text: Describing images using 1 million captioned photographs", "author": ["V. Ordonez"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}, {"title": "Describing common human visual actions in images", "author": ["M. Ronchi", "P. Perona"], "venue": "In British Machine Vision Conference (BMVC),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases", "author": ["F. Sadeghi", "S.K. Divvala", "A. Farhadi"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Grounded models of semantic representation", "author": ["C. Silberer"], "venue": "In EMNLP,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2013}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Grounded semantic role labeling", "author": ["S. Yang", "Q. Gao", "C. Liu", "C. Xiong", "S.-C. Zhu", "Y.J. Chai"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}, {"title": "Situation recognition: Visual semantic role labeling for image understanding", "author": ["M. Yatskar", "L. Zettlemoyer", "A. Farhadi"], "venue": "In Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "See no evil, say no evil: Description generation from densely labeled images", "author": ["M. Yatskar"], "venue": "*SEM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces.", "startOffset": 62, "endOffset": 66}, {"referenceID": 1, "context": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces.", "startOffset": 94, "endOffset": 97}, {"referenceID": 22, "context": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces.", "startOffset": 121, "endOffset": 125}, {"referenceID": 43, "context": "Many visual classification problems, such as image captioning [29], visual question answering [2], referring expressions [23], and situation recognition [44] have structured, semantically interpretable output spaces.", "startOffset": 153, "endOffset": 157}, {"referenceID": 36, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 297, "endOffset": 311}, {"referenceID": 45, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 297, "endOffset": 311}, {"referenceID": 8, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 297, "endOffset": 311}, {"referenceID": 43, "context": "In contrast to classification tasks such as ImageNet [37], these problems typically suffer from semantic sparsity; there is a combinatorial number of possible outputs, no dataset can cover them all, and performance of existing models degrades significantly when evaluated on rare or unseen inputs [3, 46, 9, 44].", "startOffset": 297, "endOffset": 311}, {"referenceID": 43, "context": "Situation recognition [44] is the task of producing structured summaries of what is happening in images, including activities, objects and the roles those objects play within the activity.", "startOffset": 22, "endOffset": 26}, {"referenceID": 43, "context": "This is a prototypical instance of semantic sparsity: rare outputs constitute a large portion of required predictions (35% in the imSitu dataset [44], see Figure 2), and current state-of-the-art per-", "startOffset": 145, "endOffset": 149}, {"referenceID": 43, "context": "Like previous work [44], we use a deep neural network to directly predict factors in the CRF.", "startOffset": 19, "endOffset": 23}, {"referenceID": 43, "context": "Figure 3: Verb and role-noun prediction accuracy of a baseline CRF [44] on the imSitu dev set as a function of the frequency of the least observed role-noun pair in the training set.", "startOffset": 67, "endOffset": 71}, {"referenceID": 43, "context": "Experiments on the imSitu dataset [44] demonstrate that our new compositional CRF and semantic augmentation techniques reduce the effects of semantic sparsity, with strong gains for relatively rare configurations.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d.", "startOffset": 101, "endOffset": 117}, {"referenceID": 35, "context": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d.", "startOffset": 101, "endOffset": 117}, {"referenceID": 42, "context": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d.", "startOffset": 101, "endOffset": 117}, {"referenceID": 43, "context": "Situation Recognition Situation recognition has been recently proposed to model events within images [19, 36, 43, 44], in order to answer questions beyond just \u201cWhat activity is happening?\u201d such as \u201cWho is doing it?\u201d, \u201cWhat are they doing it to?\u201d, \u201cWhat are they doing it with?\u201d.", "startOffset": 101, "endOffset": 117}, {"referenceID": 16, "context": "In general, formulations build on semantic role labelling [17], a problem in natural language processing where verbs are automatically paired with their arguments in a sentence (for example, see [8]).", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "In general, formulations build on semantic role labelling [17], a problem in natural language processing where verbs are automatically paired with their arguments in a sentence (for example, see [8]).", "startOffset": 195, "endOffset": 198}, {"referenceID": 43, "context": "We study situation recognition in imSitu [44], a largescale dataset of human annotated situations containing over 500 activities, 1,700 roles, 11,000 nouns, 125,000 images.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "The verb set V and frame set F are derived from FrameNet [13], a lexicon for semantic role labeling, while the noun set N is drawn from WordNet [34].", "startOffset": 57, "endOffset": 61}, {"referenceID": 33, "context": "The verb set V and frame set F are derived from FrameNet [13], a lexicon for semantic role labeling, while the noun set N is drawn from WordNet [34].", "startOffset": 144, "endOffset": 148}, {"referenceID": 43, "context": "Conditional Random Field Our CRF for predicting a situation, S = (v,Rf ), given an image i, decomposes over the verb v and semantic role-value pairs (e, ne) in the realized frame Rf = {(e, ne) : e \u2208 Ef}, similarly to previous work [44].", "startOffset": 231, "endOffset": 235}, {"referenceID": 39, "context": "Compositional Tensor Potential In previous work, the CRF potentials (Equation 2 and 3 ) are computed using a global image representation, a p-dimensional image vector gi \u2208 R, derived by the VGG convolutional neural network [40].", "startOffset": 223, "endOffset": 227}, {"referenceID": 20, "context": "Models All models were implemented in Caffe [21] and use a pretrained VGG network [40] for the base image representation with the final two fully connected layers replaced with two fully connected layers of dimensionality 1024.", "startOffset": 44, "endOffset": 48}, {"referenceID": 39, "context": "Models All models were implemented in Caffe [21] and use a pretrained VGG network [40] for the base image representation with the final two fully connected layers replaced with two fully connected layers of dimensionality 1024.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "The model is motivated by compositional models used for semantic role labeling [14] and allows us to trade-off the need to reduce parameters associated with nouns and expressivity.", "startOffset": 79, "endOffset": 83}, {"referenceID": 43, "context": "im Si tu 1 Baseline: Image Regression [44] 32.", "startOffset": 38, "endOffset": 42}, {"referenceID": 43, "context": "im Si tu 1 Baseline: image regression [44] 19.", "startOffset": 38, "endOffset": 42}, {"referenceID": 43, "context": "Evaluation We use the standard data split for imSitu[44] with 75k train, 25k development, and 25k test images.", "startOffset": 52, "endOffset": 56}, {"referenceID": 43, "context": "top-1 predicted verb top-5 predicted verbs ground truth verbs verb value value-all verb value value-all value value-all mean imSitu Baseline: Image Regression [44] 32.", "startOffset": 159, "endOffset": 163}, {"referenceID": 43, "context": "top-1 predicted verb top-5 predicted verbs ground truth verbs verb value value-all verb value value-all value value-all mean imSitu Baseline: Image Regression [44] 20.", "startOffset": 159, "endOffset": 163}, {"referenceID": 23, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 25, "endOffset": 37}, {"referenceID": 24, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 25, "endOffset": 37}, {"referenceID": 11, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 25, "endOffset": 37}, {"referenceID": 38, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 60, "endOffset": 76}, {"referenceID": 27, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 60, "endOffset": 76}, {"referenceID": 14, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 60, "endOffset": 76}, {"referenceID": 25, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 60, "endOffset": 76}, {"referenceID": 31, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 99, "endOffset": 107}, {"referenceID": 17, "context": "Attribute-based learning [24, 25, 12], cross-modal transfer [39, 28, 15, 26] and using text priors [32, 18] have all been proposed but they study classification or other simplified settings.", "startOffset": 99, "endOffset": 107}, {"referenceID": 44, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 21, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 6, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 10, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 32, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 19, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 34, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 30, "context": "For the structured case, image captioning models [45, 22, 7, 11, 33, 20, 35, 31] have been observed to suffer from a lack of diversity and", "startOffset": 49, "endOffset": 80}, {"referenceID": 41, "context": "generalization [42].", "startOffset": 15, "endOffset": 19}, {"referenceID": 2, "context": "Recent efforts to gain insight on such issues extract subject-verb-object (SVO) triplets from captions and count prediction failures on rare tuples [3].", "startOffset": 148, "endOffset": 151}, {"referenceID": 40, "context": "Compositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction.", "startOffset": 127, "endOffset": 131}, {"referenceID": 26, "context": "Compositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction.", "startOffset": 152, "endOffset": 156}, {"referenceID": 3, "context": "Compositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction.", "startOffset": 174, "endOffset": 177}, {"referenceID": 0, "context": "Compositional models have been explored in a number of applications in natural language processing, such as sentiment analysis [41], dependency parsing [27], text similarity [4], and visual question answering [1] as effective tools for combining natural language elements for prediction.", "startOffset": 209, "endOffset": 212}, {"referenceID": 29, "context": "Recently, bilinear pooling [30] and compact bilinear pooling [16] have been proposed as second-order feature representations for tasks such as fine grained recognition and visual question answer.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Recently, bilinear pooling [30] and compact bilinear pooling [16] have been proposed as second-order feature representations for tasks such as fine grained recognition and visual question answer.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "Using the web as a resource for image understanding has been studied through NEIL [6], a system which continuously queries for concepts discovered in text, and Levan [10], which can create detectors from user specified queries.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "Using the web as a resource for image understanding has been studied through NEIL [6], a system which continuously queries for concepts discovered in text, and Levan [10], which can create detectors from user specified queries.", "startOffset": 166, "endOffset": 170}, {"referenceID": 4, "context": "Web supervision has also been explored for pretraining convolutional neural networks [5] or for finegrained bird classification [5] and common sense reasoning [38].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Web supervision has also been explored for pretraining convolutional neural networks [5] or for finegrained bird classification [5] and common sense reasoning [38].", "startOffset": 128, "endOffset": 131}, {"referenceID": 37, "context": "Web supervision has also been explored for pretraining convolutional neural networks [5] or for finegrained bird classification [5] and common sense reasoning [38].", "startOffset": 159, "endOffset": 163}], "year": 2016, "abstractText": "Semantic sparsity is a common challenge in structured visual classification problems; when the output space is complex, the vast majority of the possible predictions are rarely, if ever, seen in the training set. This paper studies semantic sparsity in situation recognition, the task of producing structured summaries of what is happening in images, including activities, objects and the roles objects play within the activity. For this problem, we find empirically that most object-role combinations are rare, and current state-of-the-art models significantly underperform in this sparse data regime. We avoid many such errors by (1) introducing a novel tensor composition function that learns to share examples across role-noun combinations and (2) semantically augmenting our training data with automatically gathered examples of rarely observed outputs using web data. When integrated within a complete CRF-based structured prediction model, the tensor-based approach outperforms existing state of the art by a relative improvement of 2.11% and 4.40% on top-5 verb and noun-role accuracy, respectively. Adding 5 million images with our semantic augmentation techniques gives further relative improvements of 6.23% and 9.57% on top-5 verb and noun-role accuracy.", "creator": "LaTeX with hyperref package"}}}