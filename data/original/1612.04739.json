{"id": "1612.04739", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Dec-2016", "title": "An Architecture for Deep, Hierarchical Generative Models", "abstract": "We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.", "histories": [["v1", "Thu, 8 Dec 2016 11:17:16 GMT  (5553kb,D)", "http://arxiv.org/abs/1612.04739v1", "Published in NIPS 2016"]], "COMMENTS": "Published in NIPS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["philip bachman"], "accepted": true, "id": "1612.04739"}, "pdf": {"name": "1612.04739.pdf", "metadata": {"source": "CRF", "title": "An Architecture for Deep, Hierarchical Generative Models", "authors": ["Philip Bachman"], "emails": ["phil.bachman@maluuba.com"], "sections": [{"heading": "1 Introduction", "text": "Training deep, directed generative models with many layers of latent variables poses a challenging problem. Each layer of latent variables introduces variance into gradient estimation which, given current training methods, tends to impede the flow of subtle information about sophisticated structure in the target distribution. Yet, for a generative model to learn effectively, this information needs to propagate from the terminal end of a stochastic computation graph, back to latent variables whose effect on the generated data may be obscured by many intervening sampling steps.\nOne approach to solving this problem is to use recurrent, sequential stochastic generative processes with strong interactions between their inference and generation mechanisms, as introduced in the DRAW model of Gregor et al. [5] and explored further in [1, 19, 22]. Another effective technique is to use lateral connections for merging bottom-up and top-down information in encoder/decoder type models. This approach is exemplified by the Ladder Network of Rasmus et al. [17], and has been developed further for, e.g. generative modelling and image processing in [8, 23].\nModels like DRAW owe much of their success to two key properties: they decompose the process of generating data into many small steps of iterative refinement, and their structure includes direct deterministic paths between all latent variables and the final output. In parallel, models with lateral connections permit different components of a model to operate at well-separated levels of abstraction, thus generating a hierarchy of representations. This property is not explicitly shared by DRAW-like models, which typically reuse the same set of latent variables throughout the generative process. This makes it difficult for any of the latent variables, or steps in the generative process, to individually capture abstract properties of the data. We distinguish between the depth used by DRAW and the depth made possible by lateral connections by describing them respectively as sequential depth and hierarchical depth. These two types of depth are complimentary, rather than competing.\nOur contributions focus on increasing hierarchical depth without forfeiting trainability. We combine the benefits of DRAW-like models and Ladder Networks by developing a class of models which we\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n61 2.\n04 73\n9v 1\n[ cs\n.L G\n] 8\nD ec\n2 01\ncall Matryoshka Networks (abbr. MatNets), due to their deeply nested structure. In Section 2, we present the general architecture of a MatNet. In the MatNet architecture we:\n\u2022 Combine the ability of, e.g. LapGANs [3] and Diffusion Nets [21] to learn hierarchicallydeep generative models with the power of jointly-trained inference/generation1.\n\u2022 Use lateral connections, shortcut connections, and residual connections [7] to provide direct paths through the inference network to the latent variables, and from the latent variables to the generated output \u2014 this makes hierarchically-deep models easily trainable in practice.\nSection 2 also presents several extensions to the core architecture including: mixture-based prior distributions, a method for regularizing inference to prevent overfitting in practical settings, and a method for modelling the reconstruction distribution p(x|z) with a lightweight, local autoregressive model. In Section 3, we present experiments showing that MatNets offer state-of-the-art performance on standard benchmarks for modelling simple images and compelling qualitative performance on challenging imputation problems for natural images. Finally, in Section 4 we provide further discussion of related work and promising directions for future work."}, {"heading": "2 The Matryoshka Network Architecture", "text": "Matryoshka Networks combine three components: a top-down network (abbr. TD network), a bottomup network (abbr. BU network), and a set of merge modules which merge information from the BU and TD networks. In the context of stochastic variational inference [10], all three components contribute to the approximate posterior distributions used during inference/training, but only the TD network participates in generation. We first describe the MatNet model formally, and then provide a procedural description of its three components. The full architecture is summarized in Fig. 1.\n1A significant downside of LapGANs and Diffusion Nets is that they define their inference mechanisms a priori. This is computationally convenient, but prevents the model from learning abstract representations."}, {"heading": "2.1 Formal Description", "text": "The distribution p(x) generated by a MatNet is encoded in its top-down network. To model p(x), the TD network decomposes the joint distribution p(x, z) over an observation x and a sequence of latent variables z \u2261 {z0, ..., zd} into a sequence of simpler conditional distributions:\np(x) = \u2211\n(zd,...,z0)\np(x|zd, ..., z0)p(zd|zd\u22121, ..., z0)...p(zi|zi\u22121, ..., z0)...p(z0), (1)\nwhich we marginalize with respect to the latent variables to get p(x). The TD network is designed so that each conditional p(zi|zi\u22121, ..., z0) can be truncated to p(x|hti) using an internal TD state hti. See Eqns. 7/8 in Sec. 2.2 for procedural details.\nThe distribution q(z|x) used for inference in an unconditional MatNet involves the BU network, TD network, and merge modules. This distribution can be written:\nq(zd, ..., z0|x) = q(z0|x)q(z1|z0, x)...q(zi|zi\u22121, ..., z0, x)...q(zd|zd\u22121, ..., z0, x), (2)\nwhere each conditional q(zi|zi\u22121, ..., z0, x) can be truncated to q(zi|hmi+1) using an internal merge state hmi+1 produced by the ith merge module. See Eqns. 10/11 in Sec. 2.2 for procedural details.\nMatNets can also be applied to conditional generation problems like inpainting or pixel-wise segmentation. For, e.g. inpainting with known pixels xk and missing pixels xu, the predictive distribution of a conditional MatNet is given by:\np(xu|xk) = \u2211\n(zd,...,z0)\np(xu|zd, ..., z0, xk)p(zd|zd\u22121, ..., z0, xk)...p(z1|z0, xk)p(z0|xk). (3)\nEach conditional p(zi|zi\u22121, ..., z0, xk) can be truncated to p(zi|hm:gi+1), where h m:g i+1 indicates state in a merge module belonging to the generator network. Crucially, conditional MatNets include BU networks and merge modules that participate in generation, in addition to the BU networks and merge modules used by both conditional and unconditional MatNets during inference/training.\nThe distribution used for inference in a conditional MatNet is given by:\nq(zd, ..., z0|xk, xu) = q(zd|zd\u22121, ..., z0, xk, xu)...q(z1|z0, xk, xu)q(z0|xk, xu), (4)\nwhere each conditional q(zi|zi\u22121, ..., z0, xk, xu) can be truncated to q(zi|hm:ii+1), where hm:ii+1 indicates state in a merge module belonging to the inference network. Note that, in a conditional MatNet the distributions p(\u00b7|\u00b7) are not allowed to condition on xu, while the distributions q(\u00b7|\u00b7) can. MatNets are well-suited to training with Stochastic Gradient Variational Bayes [10]. In SGVB, one maximizes a lower-bound on the data log-likelihood based on the variational free-energy:\nlog p(x) \u2265 E z\u223cq(z|x) [log p(x|z)]\u2212KL(q(z|x) || p(z)), (5)\nfor which p and q must satisfy a few simple assumptions and KL(q(z|x) || p(z)) indicates the KL divergence between the inference distribution q(z|x) and the model prior p(z). This bound is tight when the inference distribution matches the true posterior p(z|x) in the model joint distribution p(x, z) = p(x|z)p(z) \u2014 in our case given by Eqns. 1/3. For brevity, we only explicitly write the free-energy bound for a conditional MatNet, which is:\nlog p(xu|xk) \u2265 E q(zd,...,z0|xk,xu)\n[ log p(xu|zd, ..., z0, xk) ] \u2212 (6)\nKL(q(zd, ..., z0|xk, xu)||p(zd, ..., z0|xk)).\nWith SGVB we can optimize the bound in Eqn. 6 using the \u201creparametrization trick\u201d to allow easy backpropagation through the expectation over z \u223c q(z|xk, xu). See [10, 18] for more details about this technique. The bound for unconditional MatNets is nearly identical \u2014 it just removes xk."}, {"heading": "2.2 Procedural Description", "text": "Structurally, top-down networks in MatNets comprise sequences of modules in which each module f ti receives two inputs: a deterministic top-down state h t i from the preceding module f t i\u22121, and some\nlatent variables zi. Module f ti produces an updated state h t i+1 = f t i (h t i, zi; \u03b8 t), where \u03b8t indicates the TD network\u2019s parameters. By defining the TD modules appropriately, we can reproduce the architectures for LapGANs, Diffusion Nets, and Probabilistic Ladder Networks [23]. Motivated by the success of LapGANs and ResNets [7], we use TD modules in which the latent variables are concatenated with the top-down state, then transformed, after which the transformed values are added back to the top-down state prior to further processing. If the adding occurs immediately before, e.g. a ReLU, then the latent variables can effectively gate the top-down state by knocking particular elements below zero. This allows each stochastic module in the top-down network to apply small refinements to the output of preceding modules. MatNets thus perform iterative stochastic refinement through hierarchical depth, rather than through sequential depth as in DRAW2.\nMore precisely, the top-down modules in our convolutional MatNets compute:\nhti+1 = lrelu(h t i + conv(lrelu(conv([h t i; zi], v t i)), w t i)), (7)\nwhere [x;x\u2032] indicates tensor concatenation along the \u201cfeature\u201d axis, lrelu(\u00b7) indicates the leaky ReLU function, conv(h,w) indicates shape-preserving convolution of the input h with the kernel w, and wti/v t i indicate the trainable parameters for module i in the TD network. We elide bias terms for brevity. When working with fully-connected models we use stochastic GRU-style state updates rather than the stochastic residual updates in Eq. 7. Exhaustive descriptions of the modules can be found in our code at: https://github.com/Philip-Bachman/MatNets-NIPS.\nThese TD modules represent each conditional p(zi|zi\u22121, ..., z0) in Eq. 1 using p(zi|hti). TD module f ti places a distribution over zi using parameters [\u00b5\u0304i; log \u03c3\u0304 2 i ] computed as follows:\n[\u00b5\u0304i; log \u03c3\u0304 2 i ] = conv(lrelu(conv(h t i, v t i)), w t i), (8)\nwhere we use \u00b7\u0304 to distinguish between Gaussian parameters from the generator network and those from the inference network (see Eqn. 11). The distributions p(\u00b7) all depend on the parameters \u03b8t. Bottom-up networks in MatNets comprise sequences of modules in which each module receives input only from the preceding BU module. Our BU networks are all deterministic and feedforward, but sensibly augmenting them with auxiliary latent variables [16, 15] and/or recurrence is a promising topic for future work. Each non-terminal module f bi in the BU network computes an updated state: hbi = f b i (h b i+1; \u03b8\nb). The final module, f b0 , provides means and log-variances for sampling z0 via reparametrization [10]. To align BU modules with their counterparts in the TD network, we number them in reverse order of evaluation. We structured the modules in our BU networks to take advantage of residual connections. Specifically, each BU module f bi computes:\nhbi = lrelu(h b i+1 + conv(lrelu(conv(h b i+1, v b i )), w b i )), (9)\nwith operations defined as for Eq. 7. These updates can be replaced by GRUs, LSTMs, etc.\nThe updates described in Eqns. 7 and 9 both assume that module inputs and outputs are the same shape. We thus construct MatNets using groups of \u201cmeta modules\u201d, within which module input/output shapes are constant. To keep our network design (relatively) simple, we use one meta module for each spatial scale in our networks (e.g. scales of 14x14, 7x7, and fully-connected for MNIST). We connect meta modules using layers which may upsample, downsample, and change feature dimension via strided convolution. We use standard convolution layers, possibly with up or downsampling, to feed data into and out of the bottom-up and top-down networks.\nDuring inference, merge modules compare the current top-down state with the state of the corresponding bottom-up module, conditioned on the current merge state, and choose a perturbation of the top-down information to push it towards recovering the bottom-up network\u2019s input (i.e. minimize reconstruction error). The ith merge module outputs [\u00b5i; log \u03c32i ;h m i+1] = f m i (h b i , h t i, h m i ; \u03b8\nm), where \u00b5i and log \u03c32i are the mean and log-variance for sampling zi via reparametrization, and h m i+1 gives the updated merge state. As in the TD and BU networks, we use a residual update:\nhmi+1 = lrelu(h m i + conv(lrelu(conv([h m i ;h b i ;h t i], u i i)), v i i)) (10)\n[\u00b5i; log \u03c3 2 i ] = conv(h m i+1, w i i), (11)\n2Current DRAW-like models can be extended to incorporate hierarchical depth, and our models can be extended to incorporate sequential depth.\nin which the convolution kernels uii, v i i , and w i i constitute the trainable parameters of this module. Each merge module thus computes an updated merge state and then reparametrizes a diagonal Gaussian using a linear function of the updated merge state.\nIn our experiments all modules in all networks had their own trainable parameters. We experimented with parameter sharing and GRU-style state in our convolutional models. The stochastic convolutional GRU is particularly interesting when applied depth-wise (rather than time-wise as in [19]), as it implements a stochastic Neural GPU [9] trainable by variational inference and capable of multi-modal dynamics. We saw no performance gains with these changes, but they merit further investigation.\nIn unconditional MatNets, the top-most latent variables z0 follow a zero-mean, unit-variance Gaussian prior, except in our experiments with mixture-based priors. In conditional MatNets, z0 follows a distribution conditioned on the known values xk. Conditional MatNets use parallel sets of BU and merge modules for the conditional generator and the inference network. BU modules in the conditional generator observe a partial input xk, while BU modules in the inference network observe both xk and the unknown values xu (which the model is trained to predict). The generative BU and merge modules in a conditional MatNet interact with the TD modules analogously to the BU and merge modules used for inference. Our models used independent Bernoullis, diagonal Gaussians, or \u201cintegrated\u201d Logistics (see [11]) for the final output distribution p(x|zd, ..., z0)/p(xu|zd, ..., z0, xk)."}, {"heading": "2.3 Model Extensions", "text": "We also develop several extensions for the MatNet architecture. The first is to replace the zero-mean, unit-variance Gaussian prior over z0 with a Gaussian Mixture Model, which we train simultaneously with the rest of the model. When using a mixture prior, we use an analytical approximation to the required KL divergence. For Gaussian distribution q, and Gaussian mixture p with components {p1, ..., pk} with uniform mixture weights, we use the KL approximation:\nKL(q || p) \u2248 log 1\u2211k i=1 e \u2212KL(q || pi) . (12)\nOur tests with mixture-based priors are only concerned with qualitative behaviour, so we do not worry about the approximation error in Eqn. 12.\nThe second extension is a technique for regularizing the inference model to prevent overfitting beyond that which is present in the generator. This regularization is applied by optimizing:\nmaximize q E x\u223cp(x)\n[ E\nz\u223cq(z|x) [log p(x|z)]\u2212KL(q(z|x) || p(z))\n] . (13)\nThis maximizes the free-energy bound for samples drawn from our model, but without changing their true log-likelihood. By maximizing Eqn. 13, we implicitly reduce KL(q(z|x) || p(z|x)), which is the gap between the free-energy bound and the true log-likelihood. A similar regularizer can be constructed for minimizing KL(p(z|x) || q(z|x)). We use (13) to reduce overfitting, and slightly boost test performance, in our experiments with MNIST and Omniglot.\nThe third extension off-loads responsibility for modelling sharp local dynamics in images, e.g. precise edge placements and small variations in textures, from the latent variables onto a local, deterministic autoregressive model. We use a simplified version of the masked convolutions in the PixelCNN of [25], modified to condition on the output of the final TD module in a MatNet. This modification is easy \u2014 we just concatenate the final TD module\u2019s output and the true image, and feed this into a PixelCNN with, e.g. five layers. A trick we use to improve gradient flow back to the MatNet is to feed the MatNet\u2019s output directly into each internal layer of the PixelCNN. In the masked convolution layers, connections to the MatNet output are unrestricted, since they are already separated from the ground truth by an appropriately-monitored noisy channel. Larger, more powerful mechanisms for combining local autoregressions and conditioning information are explored in [26]."}, {"heading": "3 Experiments", "text": "We measured quantitative performance of MatNets on three datasets: MNIST, Omniglot [13], and CIFAR 10 [12]. We used the 28x28 version of Omniglot described in [2], which can be found at: https://github.com/yburda/iwae. All quantitative experiments measured performance in\nterms of negative log-likelihood, with the CIFAR 10 scores rescaled to bits-per-pixel and corrected for discrete/continuous observations as described in [24]. We used the IWAE bound from [2] to evaluate our models, with 2500 samples in the bound. We performed additional experiments measuring the qualitative performance of MatNets using Omniglot, CelebA faces [14], LSUN 2015 towers, and LSUN 2015 churches. The latter three datasets are 64x64 color images with significant detail and non-trivial structure. Complete hyperparameters for model architecture and optimization can be found in the code at https://github.com/Philip-Bachman/MatNets-NIPS.\nWe performed three quantitative tests using MNIST. The first tests measured generative performance on dynamically-binarized images using a fully-connected model (for comparison with [2, 23]) and on the fixed binarization from [20] using a convolutional model (for comparison with [25, 19]). MatNets improved on existing results in both settings. See the tables in Fig. 2. Our third tests with MNIST measured performance of conditional MatNets for structured prediction. For this, we recreated the tests described in [22]. MatNet performance on these tests was also strong, though the prior results were from a fully-connected model, which skews the comparison.\nWe also measured quantitative performance using the 32x32 color images of CIFAR 10. We trained two models on this data \u2014 one with a Gaussian reconstruction distribution and dequantization as described in [24], and the other which added a local autoregression and used the \u201cintegrated Logistic\u201d likelihood described in [11]. The Gaussian model fell just short of the best previously reported result for a variational method (from [6]), and well short of the Pixel RNN presented in [25]. Performance on this task seems very dependent on a model\u2019s ability to predict pixel intensities precisely along edges. The ability to efficiently capture global structure has a relatively weak benefit. Mistaking a cat for a dog costs little when amortized over thousands of pixels, while misplacing a single edge can spike the reconstruction cost dramatically. We demonstrate the strength of this effect in Fig. 4, where we plot how the bits paid to encode observations are distributed among the modules in the network over the course of training for MNIST, Omniglot, and CIFAR 10. The plots show a stark difference between these distributions when modelling simple line drawings vs. when modelling more natural\nimages. For CIFAR 10, almost all of the encoding cost was spent in the 32x32 layers of the network closest to the generated output. This was our motivation for adding a lightweight autoregression to p(x|z), which significantly reduced the gap between our model and the PixelRNN. Fig. 5 shows some samples from our model, which exhibit occasional glimpses of global and local structure.\nOur final quantitative test used the Omniglot handwritten character dataset, rescaled to 28x28 as in [2]. These tests used the same convolutional architecture as on MNIST. Our model outperformed previous results, as shown in Fig. 2. Using Omniglot we also experimented with placing a mixture-based prior distribution over the top-most latent variables z0. The purpose of these tests was to determine whether the model could uncover latent class structure in the data without seeing any label information. We visualize results of these tests in Fig. 3. Additional description is provided in the figure caption. We placed a slight penalty on the entropy of the posterior distributions for each input to the model, to encourage a stronger separation of the mixture components. The inputs assigned to each mixture component (based on their posteriors) exhibit clear stylistic coherence.\nIn addition to qualitative tests exploring our model\u2019s ability to uncover latent factors of variation in Omniglot data, we tested the performance of our models at imputing missing regions of higher resolution images. These tests used images of celebrity faces, churches, and towers. These images include far more detail and variation than those in MNIST/Omniglot/CIFAR 10. We used two-stage models for these tests, in which each stage was a conditional MatNet. The first stage formed an initial guess for the missing image content, and the second stage then refined that guess. Both stages used the same architectures for their inference and generator networks. We sampled imputation problems by placing three 20x20 occluders uniformly at random in the image. Each stage had single TD modules at scales 32x32, 16x16, 8x8, and fully-connected. We trained models for roughly 200k updates, and show imputation performance on images from a test set that was held out during training. Results are shown in Fig. 5."}, {"heading": "4 Related Work and Discussion", "text": "Previous successful attempts to train hierarchically-deep models largely fall into a class of methods based on deconstructing, and then reconstructing data. Such approaches are akin to solving mazes by starting at the end and working backwards, or to learning how an object works by repeatedly disassembling and reassembling it. Examples include LapGANs [3], which deconstruct an image by repeatedly downsampling it, and Diffusion Nets [21], which deconstruct arbitrary data by subjecting it to a long sequence of small random perturbations. The power of these approaches stems from the way in which gradually deconstructing the data leaves behind a trail of crumbs which can be followed back to a well-formed observation. In the generative models of [3, 21], the deconstruction processes were defined a priori, which avoided the need for trained inference. This makes training significantly\neasier, but subverts one of the main motivations for working with latent variables and sample-based approximate inference, i.e. the ability to capture salient factors of variation in the inferred relations between latent variables and observed data. This deficiency is beginning to be addressed by, e.g. the Probabilistic Ladder Networks of [23], which are a special case of our architecture in which the deterministic paths from latent variables to observations are removed and the conditioning mechanism in inference is more restricted.\nReasoning about data through the posteriors induced by an appropriate generative model motivates some intriguing work at the intersection of machine learning and cognitive science. This work shows that, in the context of an appropriate generative model, powerful inference mechanisms are capable of exposing the underlying factors of variation in fairly sophisticated data. See, e.g. Lake et al. [13]. Techniques for training coupled generation and inference have now reached a level that makes it possible to investigate these ideas while learning models end-to-end [4].\nIn future work we plan to apply our models to more \u201cinteresting\u201d generative modelling problems, including more challenging image data and problems in language/sequence modelling. The strong performance of our models on benchmark problems suggests their potential for solving difficult structured prediction problems. Combining the hierarchical depth of MatNets with the sequential depth of DRAW is also worthwhile."}], "references": [{"title": "Data generation as sequential decision making", "author": ["P. Bachman", "D. Precup"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Importance weighted auto-encoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "[cs.LG],", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Deep generative models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "A. Szlam", "R. Fergus"], "venue": "[cs.CV],", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Attend, infer, repeat: Fast scene understanding with generative models", "author": ["S.M.A. Eslami", "N. Heess", "T. Weber", "Y. Tassa", "K. Kavucuoglu", "G.E. Hinton"], "venue": "[cs.CV],", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Towards conceptual compression", "author": ["K. Gregor", "F. Besse", "D.J. Rezende", "I. Danihelka", "D. Wierstra"], "venue": "In arXiv:1604.08772v1 [stat.ML],", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "[cs.CV],", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Recombinator networks: Learning coarse-to-fine feature aggregation", "author": ["S. Honari", "J. Yosinski", "P. Vincent", "C. Pal"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Neural gpus learn algorithms", "author": ["L. Kaiser", "I. Sutskever"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["D.P. Kingma", "T. Salimans", "M. Welling"], "venue": "[cs.LG],", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G.E. Hinton"], "venue": "Master\u2019s thesis, University of Toronto,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenebaum"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Auxiliary deep generative models", "author": ["L. Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Hierarchical variational models", "author": ["R. Ranganath", "D. Tran", "D.M. Blei"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "H. Valpola", "M. Honkala", "M. Berglund", "T. Raiko"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "One-shot generalization in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "I. Danihelka", "K. Gregor", "D. Wierstra"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Deep unsupervised learning using nonequilibrium thermodynamics", "author": ["J. Sohl-Dickstein", "E.A. Weiss", "N. Maheswaranathan", "S. Ganguli"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "H. Lee", "X. Yan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "How to train deep variational autoencoders and probabilistic ladder networks", "author": ["C.K. S\u00f8nderby", "T. Raiko", "L. Maal\u00f8e", "S.K. S\u00f8nderby", "O. Winther"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Generative image modeling using spatial lstms", "author": ["L. Theis", "M. Bethge"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Kavucuoglu. Pixel recurrent neural networks", "author": ["A. van den Oord", "N. Kalchbrenner"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["A. van den Oord", "N. Kalchbrenner", "O. Vinyals", "L. Espeholt", "A. Graves", "K. Kavucuoglu"], "venue": "[cs.CV],", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "[5] and explored further in [1, 19, 22].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[5] and explored further in [1, 19, 22].", "startOffset": 28, "endOffset": 39}, {"referenceID": 18, "context": "[5] and explored further in [1, 19, 22].", "startOffset": 28, "endOffset": 39}, {"referenceID": 21, "context": "[5] and explored further in [1, 19, 22].", "startOffset": 28, "endOffset": 39}, {"referenceID": 16, "context": "[17], and has been developed further for, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "generative modelling and image processing in [8, 23].", "startOffset": 45, "endOffset": 52}, {"referenceID": 22, "context": "generative modelling and image processing in [8, 23].", "startOffset": 45, "endOffset": 52}, {"referenceID": 2, "context": "LapGANs [3] and Diffusion Nets [21] to learn hierarchicallydeep generative models with the power of jointly-trained inference/generation1.", "startOffset": 8, "endOffset": 11}, {"referenceID": 20, "context": "LapGANs [3] and Diffusion Nets [21] to learn hierarchicallydeep generative models with the power of jointly-trained inference/generation1.", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "\u2022 Use lateral connections, shortcut connections, and residual connections [7] to provide direct paths through the inference network to the latent variables, and from the latent variables to the generated output \u2014 this makes hierarchically-deep models easily trainable in practice.", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "In the context of stochastic variational inference [10], all three components contribute to the approximate posterior distributions used during inference/training, but only the TD network participates in generation.", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "This module draws conditional samples of the latent variables via reparametrization [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 9, "context": "MatNets are well-suited to training with Stochastic Gradient Variational Bayes [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "See [10, 18] for more details about this technique.", "startOffset": 4, "endOffset": 12}, {"referenceID": 17, "context": "See [10, 18] for more details about this technique.", "startOffset": 4, "endOffset": 12}, {"referenceID": 22, "context": "By defining the TD modules appropriately, we can reproduce the architectures for LapGANs, Diffusion Nets, and Probabilistic Ladder Networks [23].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "Motivated by the success of LapGANs and ResNets [7], we use TD modules in which the latent variables are concatenated with the top-down state, then transformed, after which the transformed values are added back to the top-down state prior to further processing.", "startOffset": 48, "endOffset": 51}, {"referenceID": 15, "context": "Our BU networks are all deterministic and feedforward, but sensibly augmenting them with auxiliary latent variables [16, 15] and/or recurrence is a promising topic for future work.", "startOffset": 116, "endOffset": 124}, {"referenceID": 14, "context": "Our BU networks are all deterministic and feedforward, but sensibly augmenting them with auxiliary latent variables [16, 15] and/or recurrence is a promising topic for future work.", "startOffset": 116, "endOffset": 124}, {"referenceID": 9, "context": "The final module, f b 0 , provides means and log-variances for sampling z0 via reparametrization [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "The stochastic convolutional GRU is particularly interesting when applied depth-wise (rather than time-wise as in [19]), as it implements a stochastic Neural GPU [9] trainable by variational inference and capable of multi-modal dynamics.", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "The stochastic convolutional GRU is particularly interesting when applied depth-wise (rather than time-wise as in [19]), as it implements a stochastic Neural GPU [9] trainable by variational inference and capable of multi-modal dynamics.", "startOffset": 162, "endOffset": 165}, {"referenceID": 10, "context": "Our models used independent Bernoullis, diagonal Gaussians, or \u201cintegrated\u201d Logistics (see [11]) for the final output distribution p(x|zd, .", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "We use a simplified version of the masked convolutions in the PixelCNN of [25], modified to condition on the output of the final TD module in a MatNet.", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "Larger, more powerful mechanisms for combining local autoregressions and conditioning information are explored in [26].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "We measured quantitative performance of MatNets on three datasets: MNIST, Omniglot [13], and CIFAR 10 [12].", "startOffset": 83, "endOffset": 87}, {"referenceID": 11, "context": "We measured quantitative performance of MatNets on three datasets: MNIST, Omniglot [13], and CIFAR 10 [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 1, "context": "We used the 28x28 version of Omniglot described in [2], which can be found at: https://github.", "startOffset": 51, "endOffset": 54}, {"referenceID": 21, "context": "The lower-right table presents results from the structured prediction task in [22], in which 1-3 quadrants of an MNIST digit are visible, and NLL is measured on predictions for the unobserved quadrants.", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "terms of negative log-likelihood, with the CIFAR 10 scores rescaled to bits-per-pixel and corrected for discrete/continuous observations as described in [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 1, "context": "We used the IWAE bound from [2] to evaluate our models, with 2500 samples in the bound.", "startOffset": 28, "endOffset": 31}, {"referenceID": 13, "context": "We performed additional experiments measuring the qualitative performance of MatNets using Omniglot, CelebA faces [14], LSUN 2015 towers, and LSUN 2015 churches.", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "The first tests measured generative performance on dynamically-binarized images using a fully-connected model (for comparison with [2, 23]) and on the fixed binarization from [20] using a convolutional model (for comparison with [25, 19]).", "startOffset": 131, "endOffset": 138}, {"referenceID": 22, "context": "The first tests measured generative performance on dynamically-binarized images using a fully-connected model (for comparison with [2, 23]) and on the fixed binarization from [20] using a convolutional model (for comparison with [25, 19]).", "startOffset": 131, "endOffset": 138}, {"referenceID": 19, "context": "The first tests measured generative performance on dynamically-binarized images using a fully-connected model (for comparison with [2, 23]) and on the fixed binarization from [20] using a convolutional model (for comparison with [25, 19]).", "startOffset": 175, "endOffset": 179}, {"referenceID": 24, "context": "The first tests measured generative performance on dynamically-binarized images using a fully-connected model (for comparison with [2, 23]) and on the fixed binarization from [20] using a convolutional model (for comparison with [25, 19]).", "startOffset": 229, "endOffset": 237}, {"referenceID": 18, "context": "The first tests measured generative performance on dynamically-binarized images using a fully-connected model (for comparison with [2, 23]) and on the fixed binarization from [20] using a convolutional model (for comparison with [25, 19]).", "startOffset": 229, "endOffset": 237}, {"referenceID": 21, "context": "For this, we recreated the tests described in [22].", "startOffset": 46, "endOffset": 50}, {"referenceID": 23, "context": "We trained two models on this data \u2014 one with a Gaussian reconstruction distribution and dequantization as described in [24], and the other which added a local autoregression and used the \u201cintegrated Logistic\u201d likelihood described in [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "We trained two models on this data \u2014 one with a Gaussian reconstruction distribution and dequantization as described in [24], and the other which added a local autoregression and used the \u201cintegrated Logistic\u201d likelihood described in [11].", "startOffset": 234, "endOffset": 238}, {"referenceID": 5, "context": "The Gaussian model fell just short of the best previously reported result for a variational method (from [6]), and well short of the Pixel RNN presented in [25].", "startOffset": 105, "endOffset": 108}, {"referenceID": 24, "context": "The Gaussian model fell just short of the best previously reported result for a variational method (from [6]), and well short of the Pixel RNN presented in [25].", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": "Our final quantitative test used the Omniglot handwritten character dataset, rescaled to 28x28 as in [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "Examples include LapGANs [3], which deconstruct an image by repeatedly downsampling it, and Diffusion Nets [21], which deconstruct arbitrary data by subjecting it to a long sequence of small random perturbations.", "startOffset": 25, "endOffset": 28}, {"referenceID": 20, "context": "Examples include LapGANs [3], which deconstruct an image by repeatedly downsampling it, and Diffusion Nets [21], which deconstruct arbitrary data by subjecting it to a long sequence of small random perturbations.", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "In the generative models of [3, 21], the deconstruction processes were defined a priori, which avoided the need for trained inference.", "startOffset": 28, "endOffset": 35}, {"referenceID": 20, "context": "In the generative models of [3, 21], the deconstruction processes were defined a priori, which avoided the need for trained inference.", "startOffset": 28, "endOffset": 35}, {"referenceID": 22, "context": "the Probabilistic Ladder Networks of [23], which are a special case of our architecture in which the deterministic paths from latent variables to observations are removed and the conditioning mechanism in inference is more restricted.", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "Techniques for training coupled generation and inference have now reached a level that makes it possible to investigate these ideas while learning models end-to-end [4].", "startOffset": 165, "endOffset": 168}], "year": 2016, "abstractText": "We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.", "creator": "LaTeX with hyperref package"}}}