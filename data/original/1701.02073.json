{"id": "1701.02073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jan-2017", "title": "Neural Personalized Response Generation as Domain Adaptation", "abstract": "In this paper, we focus on the personalized response generation for conversational systems. Based on the sequence to sequence learning, especially the encoder-decoder framework, we propose a two-phase approach, namely initialization then adaptation, to model the responding style of human and then generate personalized responses. For evaluation, we propose a novel human aided method to evaluate the performance of the personalized response generation models by online real-time conversation and offline human judgement. Moreover, the lexical divergence of the responses generated by the 5 personalized models indicates that the proposed two-phase approach achieves good results on modeling the responding style of human and generating personalized responses for the conversational systems.", "histories": [["v1", "Mon, 9 Jan 2017 06:42:57 GMT  (367kb,D)", "http://arxiv.org/abs/1701.02073v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["weinan zhang", "ting liu", "yifa wang", "qingfu zhu"], "accepted": false, "id": "1701.02073"}, "pdf": {"name": "1701.02073.pdf", "metadata": {"source": "CRF", "title": "Neural Personalized Response Generation as Domain Adaptation", "authors": ["Wei-Nan Zhang", "Ting Liu", "Yifa Wang", "Qingfu Zhu"], "emails": ["wnzhang@ir.hit.edu.cn"], "sections": [{"heading": null, "text": "In this paper, we focus on the personalized response generation for conversational systems. Based on the sequence to sequence learning, especially the encoder-decoder framework, we propose a two-phase approach, namely initialization then adaptation, to model the responding style of human and then generate personalized responses. For evaluation, we propose a novel human aided method to evaluate the performance of the personalized response generation models by online real-time conversation and offline human judgement. Moreover, the lexical divergence of the responses generated by the 5 personalized models indicates that the proposed two-phase approach achieves good results on modeling the responding style of human and generating personalized responses for the conversational systems.\nKeywords: Personalized Response Generation, Conversational\nSystems, Sequence to Sequence Learning, Domain Adaptation"}, {"heading": "1. Introduction", "text": "Conversational system, which is also called conversational robot, virtual agent or chatbot, etc, is an interesting and challenging research of artificial intelligence. It can be applied to a large number of scenarios of human-computer interaction, such as question answering [1], negotiation [2, 3], e-commence [4], tutoring [5], etc. Recently, conversational system usually plays the role of virtual companion or assistant of hu-\nEmail address: wnzhang@ir.hit.edu.cn (Wei-Nan Zhang)\nPreprint submitted to arXiv January 10, 2017\nar X\niv :1\n70 1.\n02 07\n3v 1\n[ cs\n.C L\n] 9\nJ an\nman. For example, the virtual assistant on mobile phone is one of the most popular application of d system, such as, Siri, Cortana, Facebook M, Viv, etc.\nDespite the functional success of the existing conversational systems, literature that tries to model the personalized responding of a conversational system is still sparse. Table 1 shows an example of the responses of different personality to a given post.\nFrom Table 1, we can see that the Response #1 is a briefly definite response to the post. The Response #2 is an emotional response. And the Response #3 gives another suggestion on dressing. It is obvious that although the 3 responses are relevant to post, they represent the different personalities and language styles respectively that may further impact the chatting process and the user experience.\nTo address the problem of generating personalized response for conversational systems, in this paper, we proposed an optimized sequence to sequence (seq2seq) learning framework, which is based on the recurrent neural network, to modeling the input post and generating the personalized response. Unlike the previous seq2seq approaches for response generation that learn to encode and decode in the same way, the proposed approach fully adopts the advantages of the seq2seq approaches and then learns to adapt to generate personalized response.\nThe contributions of this paper are two-fold:\n\u2022 We proposed a two-phase approach, namely initialization then adaptation, to\nlearn to generate the personalized response for conversational systems.\n\u2022 We proposed a novel human aided method to evaluate the personality of the\ngenerated response.\nThe rest of the paper is organized as follows: We will detail the proposed two-phase\napproach to generating personalized response in the next section. In the following section, the experimental results and analysis will be presented. The last two sections are the related work and conclusion respectively."}, {"heading": "2. Our Approach", "text": "Recently, a number of research works have been proposed to generate sentence or response based on the recurrent neural network (RNN) [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]. The target of these work is that given an input sentence x, the RNN based model tries to generate an output sentence y that can maximize the conditional probability of p(y|x). For the response generation of a conversational system, this type of approach usually includes two parts, namely the encoder and decoder. The encoder is to convert the input sentence into a vector which represents the complete semantics of the input sentence. The decoder then generates the output sentence character by character (or word by word) according to the information on the encoding phase."}, {"heading": "2.1. Personalized Response Generation", "text": "Inspired by the RNN encoder-decoder framework, which is proposed by [17] and [6],\nin this paper, we proposed a personalized response generation approach for conversational systems.\nFigure 1 shows the framework of the proposed approach.\nFrom Figure 1, we can see that the proposed approach consists of two components, namely initialization then adaptation, the first of which pre-trains the responding model of the conversational system using the large scale general training data and the second step fine-tunes the model on the small size of personalized training data.\nNext, we will detail the proposed personalized response generation model. Typically, the encoder and decoder are implemented by the GRU [22, 23] or LSTM [24, 25, 26] based RNN. The encoder reads the input sentence word by word and outputs the hidden state of each word. These states are denoted as H which is also called annotations. Here, hi represents the hidden state at time i and it is computed by its last hidden state hi\u22121 and the input word at time i, Xi. Therefore, the hidden state at time t can be denoted as:\nht = f(ht\u22121, Xt); H = {h1, h2, ..., hT } (1)\nHere, T equals to the length (the number of words) of the input sentence and f is a non-linear function which can be implemented as LSTM [6] or GRU [23].\nThe encoder then converts these hidden states to a context vector c as a summary\nof the semantics of the input sentence.\nc = q({h1, h2, ..., hT }) (2)\nWhere, c can be implemented in many ways, such as [6] set c = hT .\nFor the decoding process, si denotes the hidden state at time i. It is also computed by a non-linear function f , of which the variables are the output yi\u22121 and the hidden state si\u22121 at last time. The hidden state of the decoder at time t can be computed as:\nst = f(st\u22121, yt\u22121) (3)\nNote that the context vector c, which is generated from the encoder, is also used to initialize the first hidden state [6] or all of the hidden states [17] of the decoder to make sure that the decoder can be conditioned by the encoder. Therefore, the hidden state of the decoder at time t is updated as:\nst = f(st\u22121, yt\u22121, c) (4)\nThe output of the decoder at the state st is to map to a distribution over the vocabulary by using the maxout activation function [27]\nIn the proposed approach, the response generation model is implemented by an RNN encoder-decoder framework, which is inspired by [18]. For the personalized response generation, we proposed a two-phase training approach to first initialize the model using the large scale general training data and then use the personalized training data to fine-tune the model to generate personalized response. All the parameters of the responding model are shared between the initialization and adaptation process.\nTo balance the generation performance and training time of the model, in this paper,\nwe choose the GRU as the non-linear function f for both encoder and decoder.\nIn this paper, we utilize a weighted sum scheme [18] to dynamically compute the\nci for each state in the encoding process as:\nci = T\u2211 j=1 \u03b1ijhj (5)\nThe weight \u03b1ij of each hidden state hj is computed as:\n\u03b1ij = exp(eij)\u2211T k=1 exp(eik)\n(6)\nWhere, eij = a(si\u22121, hi) is a feedforward neural network, which can be called as the alignment model or attention model [18]."}, {"heading": "2.2. Generation Quality Optimization", "text": "Compared with the response that is generated by the general RNN encoder-decoder model, the two-phase training approach could generate distinct and personalized response. However, we found another problem that when the first word is decoded to a high frequency word in the vocabulary, such as \u201cWe\u201d, \u201cI\u201d, \u201cYes\u201d, etc, the personalized response model is tend to generate a general response.\nTo address the above problem, we utilized a learning scheme to generate the first word in decoding process, namely Learning to Start (LTS) model [28]. Unlike the classic RNN encoder-decoder framework that uses a special character \u201c</s>\u201d to generate the first word in decoding process, the LTS model independently learns to predict the\nfirst word using the context vector, which is generated from the encoding process. The LTS model can be formalized as:\ny0 = \u03c3((\u03c3(Wic) + bi)E + be) (7)\nHere, c is the context vector which is computed by Equation (5). E represents the word embedding matrix of the decoder, bi and be are bias items. Wi is a learnable matrix that is trained to model the conditional dependence of the context vector c and the first word in decoding process.\nBy ignoring the bias items, the Equation (7) can be transformed as follows:\ny0 = g(c, E) (8)\nWe thus found that the LTS is to model the relation between the context vector that is generated from the encoder and the embedding matrix of the decoder. According to the distribution of the generation probability over the decoding vocabulary, LTS predicts the first word for the decoder and then the decoding process goes on until the response is generated completely. In this paper, we use different vocabularies for encoding and decoding to model the personality of the speaker and responder respectively."}, {"heading": "3. Experiments and Analysis", "text": ""}, {"heading": "3.1. Experiment Data and Settings", "text": "As the proposed personalized response generation approach includes two phases, there are two separate training data sets, namely general training data and personalized training data (See Figure 1). The data for general training is collected from several Chinese online forums. It contains 1 million one-to-one post and response pairs and the vocabulary contains 35 thousands words. Here, one-to-one means one post is only corresponded to one response. In this paper, LTP (http://www.ltp-cloud.com/) toolkit is used for Chinese word segmentation for all the data.\nFor the personalized training, we invited 5 volunteers. Each of them shared 2,000 messages of their chatting history from the use of instant messaging service without any privacy information. The general messages, such as \u201c\u4f60\u597d(Hello)\u201d, \u201c\u65e9\u4e0a\u597d(Good Morning)\u201d, \u201c\u518d\u89c1(See you)\u201d, etc, are removed from the personalized training data.\nNote that, to train the personalized responding model, these messages from the chatting history can only be used as responses. Therefore, we need to collect a unique post for each of the messages. We built a search engine, which is based on the Lucene1 toolkit, to retrieve similar responses from the general training data for the messages. For each message, the post of the most similar response to the message was chosen as the post of the message. Therefore, for each volunteer, we thus obtained 2,000 post and message pairs for personalized training. After training, we have 5 personalized responding models that correspond to the 5 volunteers for the test.\nThe parameter settings in the response generation model are as follow: The dimension of the hidden layer of the encoder and decoder is 1,024. The dimension of the word embedding is 500 which is obtained by using the word2vec toolkit [29]. The word2vec is trained with the SogouCS&CA corpus (2008 version)2, which is widely used for Chinese text analysis [30, 31]. The details of the training corpus for training word embedding are shown in Table 2.\nThe encoder-decoder framework is implemented by using Theano toolkit [32]. The batch size is set to 128. The iteration times are set to 10 and 8 for the general training and personalized training respectively."}, {"heading": "3.2. Test and Evaluation", "text": "It is a non-trivial task to evaluate the performance of a responding model automatically. As [7] said: \u201cAutomatic evaluation of response generation is still an open problem.\u201d. The BLEU socre [33], which is widely used in machine translation, is not a suitable evaluation metric for response generation. As the responses to the same post\n1http://lucene.apache.org/ 2http://www.sogou.com/labs/dl/cs.html\nmay share less common words, it is impossible to construct a reference set with adequate coverage. Meanwhile, the Perplexity, which is an evaluation metric for language modeling, is also not reasonable for evaluate the relevance between post and response.\nTo address the above issues, we design a novel human aided evaluation method based on the online real-time chatting and offline human judgment. The diagram of the evaluation method is shown in Figure 2.\nThe evaluation method includes a volunteer, a tester and a chatbot. The volunteer and the tester are communicating through an instant messaging service. Here, the tester is told to talk to a volunteer through the instant messaging service without any constraints and preconditions as well as the tester do not know the existence of the chatbot. During the chatting, each of the tester\u2019s messages is sent to the volunteer and the chatbot simultaneously. The Shelter in Figure 2 represents that the volunteer could not see the response that is generated by the chatbot before it is sent to the tester. The question mark \u201c?\u201d denotes that the volunteer needs to randomly decide whether to respond by himself/herself or let the chatbot sends its response. The reason is to reduce the preference of the volunteer to the response of the chatbot.\nWhen the chatting is finished, the tester is asked to judge whether each of the responses is from the volunteer or someone else. We defined the imitation rate, rimi to evaluate the personality of the proposed dialogue generation approach. Here, we\nuse nimi to denote the number of responses that are generated by the chatbot, but are judged as the responses of the volunteer. ngr is the total number of responses that are generated by the chatbot. The imitation rate is thus represented as:\nrimi = nimi ngr\n(9)\nWe can obviously see from Equation (9) that the imitation rate can reflect the ability of the chatbot on imitating the personalized responding style of the volunteers. The larger the imitation rate is, the better the chatbot imitates the volunteers.\nNote that we totally recruit 5 volunteers and 1 tester. All of them are not participating in the training data collecting and coding. They also have no experience on developing the relevant techniques. The tester and the volunteers have known each other very well before the evaluation as well."}, {"heading": "3.3. Experimental Results", "text": "As we described on the above section, we use the imitation rate to evaluate the personality of the responses which are generated by the chatbot. Table 3 shows the experimental results of the proposed approach to generating personalized responses.\nHere, the tester sends 50 posts to each volunteer and the 5 test sets shared a small amount of common messages. Meanwhile, the messages sent to the tester are hybrid\nresponses. That is to say, in a conversation, the responses to the tester are given by the chatbot and a volunteer in a random inserting way. Therefore, based on the above conditions, the judgement of the tester on the hybrid responses can be seen as a \u201crelative\u201d evaluation on the ability of the chatbot for imitating the personalized responding style of the volunteer.\nMoreover, we plan to evaluate the \u201cabsolute\u201d ability of the chatbot on imitating the personalized responding style of the volunteer. We first randomly sample 50 posts that are sent to the volunteers by the tester in previous evaluation. We then use the 5 trained models to generate personalized responses of the 50 posts respectively. Finally, we again ask the tester to judge whether each of the responses is from the volunteer or someone else. Table 4 shows the results of the human judgement on the 5 personalized response generation models.\nFrom Table 4, we can see that the rimi scores of the 5 personalized responding models (PRM #1 - #5) are much lower than those of the corresponding volunteers in Table 3. There are two reasons. First, on the online real-time chatting, the responses generated by chatbot are randomly inserted to the conversation with the volunteers\u2019 responses. It may increase the confusion of the tester on the judgement. Second, the online real-time chatting process is context-aware. Due to the \u201ccoherent model\u201d in mind, the volunteers may tend to extend the conversation topic. It may make the tester believe that the previous responses are also typed by the volunteer rather than generated by a chatbot.\nW or\nd Fr\neq ue\nnc y\nVocabulary\nPM #1 PM #2 PM #3 PM #4 PM #5\nFigure 3: The distributions of the responding words on the vocabulary.\nTo further verify the ability of the PRM #1 - #5 on modeling the personalized responding style, we calculate the lexical distributions of the responses generated by the PRM #1 - #5 respectively. Figure 3 shows the distributions of the responding words on the vocabulary.\nHere, to calculate the distribution, we combine all the 250 posts of the tester on chatting with the 5 volunteers. After removing the duplicate posts, we totally obtain 230 posts. We then utilize the trained models of personalized response generation, PRM #1 - #5, to generate the responses of the 230 posts. The vocabulary size in Figure 3 is 111 after removing the stop words. From Figure 3, we can see that the lexical distributions of the responses generated by the 5 personalized models are quite different. It indicates that through the personalized training, the PRM #1 - #5 models can generate the responses with different responding styles, which is embodied in the different lexical distributions. For an intuitive understanding, Table 5 shows the real cases of the same post with different responses generated by PRM #1 - #5 models.\nNext, for each of the personalized responding models, it is also necessary to validate the personality of the generated response towards the corresponding volunteer\u2019s. Therefore, we calculate the percentage of the overlapping words between the volunteer\u2019s responses and the chatbot generated responses for a same post set. We totally obtain 35 common posts from the 5 volunteers. The percentages of the overlapping words (stop words are removed) are shown in Table 6.\nIn Table 6, the ridge of the percentages along the diagonal illustrates the ability\nof the proposed personalized responding models, PRM #1 - #5, on capturing the the personalized responding style of the 5 volunteers."}, {"heading": "4. Related Work", "text": "In this section, we will present the related work of the response generation in two\naspects.\n\u2022 Task-oriented Dialogue Generation\nThe most successful research on the task-oriented dialogue system is mainly based on the partially observed Markov decision process (POMDP) [34]. The task oriented dialogue system mainly focuses on the dialogue state tracking, action classification, policy and reward learning, etc. Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42]. These approaches have the limitation on the scalability to new domains. [43] proposed a statistical language generator which used a dynamic Bayesian networks to generate dialogue response. [44] learned to generate paraphrases in dialogue through a factored language model that was training from the data collected by crowdsourcing. Both of them completely learn from the data and thus has no limitation on domain transfer. Recently, as the powerful of deep neural network on learning from large-scale data,\n[16] proposed a statistical dialogue generator based on a joint recurrent and convolutional neural network, which can directly learn from the data without any semantic alignment or handcrafted rules. Further, [15] and [14] proposed a semantically conditioned LSTM to generate dialogue response and then compared it with an RNN encoder-decoder generator on multi-domain data to verify the ability of domain adaptation of the two generators.\n\u2022 Non-task-oriented Dialogue Generation\nA non-task-oriented dialogue system is sometimes called an open-domain dialogue system. [45] proposed an unsupervised approach to modeling dialogue response by clustering the raw utterances. They then presented an end-to-end dialogue response generator by using a phrase-based statistical machine translation model [46]. [47] introduced a search-based system, namely IRIS, to generate dialogues using vector space model and then released the experimental corpus for research and development [48]. [49] introduced the knowledge bases that obtained from the Web to deal with the out-of-domain request. Recently, benefit from the advantages of the sequence to sequence learning framework with neural networks [6], [7] and [19] had drawn inspiration from the neural machine translation [17, 18] and proposed an RNN encoder-decoder based approach to generate dialogue by considering the last one sentence and a larger range of context respectively. [8] presented a hierarchical neural network, which is inspired by [50], to build an end-to-end dialogue system. [10] and [28] focused on resolving the generating of safe, commonplace, high frequency responses on the sequence to sequence neural network. Most recently, [12] captured the advantages of the RNN encoder-decoder on response generation and the deep reinforcement learning on dialogue rewarding to generate context-aware dialogue responses.\nThe most similar work to this paper is [21]. They devote to handling the information consistency of a speaker by integrating the speaker embedding and word embedding into a sequence to sequence learning framework. In this paper, we focus on modeling the personalized responding style of human by presenting a domain adaptation approach, which is also based on the sequence to sequence learning framework.\nWe believe that the information consistency and responding style are two aspects of the human personality. We leave the work of jointly exploring the consistency and responding style for personalized dialogue generation in future."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a neural response generation approach, which is based on the encoder-decoder framework, to learn the responding style of human (the volunteers) and then generate personalized response for conversational systems. The proposed approach extend the traditional encoder-decoder approach to response generation by introducing a domain adaptation scheme, namely initialization-then-adaptation. We also proposed a novel human aided method to evaluate the ability of the chatbot on imitating the responding styles of the volunteers. Experimental results on validating the lexical distribution and the word overlapping indicate that the 5 responding models can capture the personalized responding styles of the 5 volunteers. In future, we will further improve the performance of personalized responding models by introducing the contextual information."}], "references": [{"title": "An artificially intelligent chat agent that answers adolescents\u2019 questions related to sex", "author": ["R. Crutzen", "G.Y. Peters", "S.D. Portugal", "E.M. Fisser", "J.J.J. Grolleman"], "venue": "drugs, and alcohol: An exploratory study, Journal of Adolescent Health 48 (5) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Negochat-a: a chat-based negotiation agent with bounded rationality", "author": ["A. Rosenfeld", "I. Zuckerman", "E. Segalhalevi", "O. Drein", "S. Kraus"], "venue": "Autonomous Agents and Multi-Agent Systems 30 (1) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Live-chat agent assignments to heterogeneous e-customers under imperfect classification", "author": ["P. Goes", "N. Ilk", "W.T. Yue", "J.L. Zhao"], "venue": "ACM TMIS 2 (4) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Lsa for intuitive chat agents tutoring system", "author": ["G. Pilato", "G. Vassallo", "M. Gentile", "A. Augello", "S. Gaglio"], "venue": "", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le", "I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS 4 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A persona-based neural conversation model", "author": ["J. Li", "M. Galley", "C. Brockett", "G. Spithourakis", "J. Gao", "B. Dolan"], "venue": "in: ACL", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["A. Graves"], "venue": "Neural Computation 9 (8) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with lstm, Neural Computation", "author": ["A. Gers F", "J. Schmidhuber", "F. Cummins"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS 26 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Automatic online news issue construction in web environment", "author": ["C. Wang", "M. Zhang", "S. Ma", "L. Ru"], "venue": "in: WWW", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Theano: Deep learning on gpus with python", "author": ["J. Bergstra", "F. Bastien", "O. Breuleux", "P. Lamblin", "R. Pascanu", "O. Delalleau", "G. Desjardins", "D. Warde-Farley", "I.J. Goodfellow", "A. Bergeron", "Y. Bengio"], "venue": "in: NIPS", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gasic", "B. Thomson", "J.D. Williams"], "venue": "Proceedings of the IEEE 101 (5) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Exploiting a probabilistic hierarchical model for generation", "author": ["S. Bangalore", "O. Rambow"], "venue": "in: COLING", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models", "author": ["A. Belz"], "venue": "Natural Language Engineering 14 (4) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "Individuality and alignment in generated dialogues", "author": ["A. Isard", "C. Brockmann", "J. Oberlander"], "venue": "in: INLG", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Natural language generation as planning under uncertainty for spoken dialogue systems", "author": ["V. Rieser", "O. Lemon"], "venue": "in: EACL", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "M", "author": ["F. Mairesse"], "venue": "A. Walker, Trainable generation of big-five personality styles through data-driven parameter estimation., in: ACL", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Training a sentence planner for spoken dialogue using boosting", "author": ["M.A. Walker", "O.C. Rambow", "M. Rogati"], "venue": "Computer Speech & Language 16 (3C4) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2002}, {"title": "R", "author": ["D.S. Paiva"], "venue": "Evans, Empirically-based control of natural language generation., in: ACL", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "M", "author": ["F. Mairesse"], "venue": ", Jurcicek, F. Ek, S. Keizer, B. Thomson, K. Yu, S. Young, Phrasebased statistical language generation using graphical models and active learning, in: ACL", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic language generation in dialogue using factored language models", "author": ["F. Mairesse", "S. Young"], "venue": "Computational Linguistics 40 (4) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Data-driven response generation in social media", "author": ["A. Ritter", "C. Cherry", "W.B. Dolan"], "venue": "in: EMNLP", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Iris: a chat-oriented dialogue system based on the vector space model", "author": ["R.E. Banchs", "H. Li"], "venue": "in: ACL", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Movie-dic: a movie dialogue corpus for research and development", "author": ["R.E. Banchs"], "venue": "in: ACL", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2012}, {"title": "Luke", "author": ["D. Ameixa", "L. Coheur", "P. Fialho", "P. Quaresma"], "venue": "I am Your Father: Dealing with Out-of-Domain Requests by Using Movies Subtitles", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion", "author": ["A. Sordoni", "Y. Bengio", "H. Vahabi", "C. Lioma", "J. Grue Simonsen", "J.Y. Nie"], "venue": "in: CIKM", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "It can be applied to a large number of scenarios of human-computer interaction, such as question answering [1], negotiation [2, 3], e-commence [4], tutoring [5], etc.", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "It can be applied to a large number of scenarios of human-computer interaction, such as question answering [1], negotiation [2, 3], e-commence [4], tutoring [5], etc.", "startOffset": 124, "endOffset": 130}, {"referenceID": 2, "context": "It can be applied to a large number of scenarios of human-computer interaction, such as question answering [1], negotiation [2, 3], e-commence [4], tutoring [5], etc.", "startOffset": 143, "endOffset": 146}, {"referenceID": 3, "context": "It can be applied to a large number of scenarios of human-computer interaction, such as question answering [1], negotiation [2, 3], e-commence [4], tutoring [5], etc.", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Recently, a number of research works have been proposed to generate sentence or response based on the recurrent neural network (RNN) [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21].", "startOffset": 133, "endOffset": 193}, {"referenceID": 5, "context": "Recently, a number of research works have been proposed to generate sentence or response based on the recurrent neural network (RNN) [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21].", "startOffset": 133, "endOffset": 193}, {"referenceID": 4, "context": "Personalized Response Generation Inspired by the RNN encoder-decoder framework, which is proposed by [17] and [6], in this paper, we proposed a personalized response generation approach for conversational systems.", "startOffset": 110, "endOffset": 113}, {"referenceID": 6, "context": "Typically, the encoder and decoder are implemented by the GRU [22, 23] or LSTM [24, 25, 26] based RNN.", "startOffset": 79, "endOffset": 91}, {"referenceID": 7, "context": "Typically, the encoder and decoder are implemented by the GRU [22, 23] or LSTM [24, 25, 26] based RNN.", "startOffset": 79, "endOffset": 91}, {"referenceID": 4, "context": ", hT } (1) Here, T equals to the length (the number of words) of the input sentence and f is a non-linear function which can be implemented as LSTM [6] or GRU [23].", "startOffset": 148, "endOffset": 151}, {"referenceID": 4, "context": "Where, c can be implemented in many ways, such as [6] set c = hT .", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "Note that the context vector c, which is generated from the encoder, is also used to initialize the first hidden state [6] or all of the hidden states [17] of the decoder to make sure that the decoder can be conditioned by the encoder.", "startOffset": 119, "endOffset": 122}, {"referenceID": 8, "context": "The output of the decoder at the state st is to map to a distribution over the vocabulary by using the maxout activation function [27] In the proposed approach, the response generation model is implemented by an RNN encoder-decoder framework, which is inspired by [18].", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "The dimension of the word embedding is 500 which is obtained by using the word2vec toolkit [29].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "The word2vec is trained with the SogouCS&CA corpus (2008 version)2, which is widely used for Chinese text analysis [30, 31].", "startOffset": 115, "endOffset": 123}, {"referenceID": 11, "context": "The encoder-decoder framework is implemented by using Theano toolkit [32].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "\u2022 Task-oriented Dialogue Generation The most successful research on the task-oriented dialogue system is mainly based on the partially observed Markov decision process (POMDP) [34].", "startOffset": 176, "endOffset": 180}, {"referenceID": 13, "context": "Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 185, "endOffset": 217}, {"referenceID": 14, "context": "Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 185, "endOffset": 217}, {"referenceID": 15, "context": "Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 185, "endOffset": 217}, {"referenceID": 16, "context": "Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 185, "endOffset": 217}, {"referenceID": 17, "context": "Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 185, "endOffset": 217}, {"referenceID": 18, "context": "Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 185, "endOffset": 217}, {"referenceID": 19, "context": "Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42].", "startOffset": 185, "endOffset": 217}, {"referenceID": 20, "context": "[43] proposed a statistical language generator which used a dynamic Bayesian networks to generate dialogue response.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[44] learned to generate paraphrases in dialogue through a factored language model that was training from the data collected by crowdsourcing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "They then presented an end-to-end dialogue response generator by using a phrase-based statistical machine translation model [46].", "startOffset": 124, "endOffset": 128}, {"referenceID": 23, "context": "[47] introduced a search-based system, namely IRIS, to generate dialogues using vector space model and then released the experimental corpus for research and development [48].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[47] introduced a search-based system, namely IRIS, to generate dialogues using vector space model and then released the experimental corpus for research and development [48].", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "[49] introduced the knowledge bases that obtained from the Web to deal with the out-of-domain request.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Recently, benefit from the advantages of the sequence to sequence learning framework with neural networks [6], [7] and [19] had drawn inspiration from the neural machine translation [17, 18] and proposed an RNN encoder-decoder based approach to generate dialogue by considering the last one sentence and a larger range of context respectively.", "startOffset": 106, "endOffset": 109}, {"referenceID": 26, "context": "[8] presented a hierarchical neural network, which is inspired by [50], to build an end-to-end dialogue system.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "The most similar work to this paper is [21].", "startOffset": 39, "endOffset": 43}], "year": 2017, "abstractText": "In this paper, we focus on the personalized response generation for conversational systems. Based on the sequence to sequence learning, especially the encoder-decoder framework, we propose a two-phase approach, namely initialization then adaptation, to model the responding style of human and then generate personalized responses. For evaluation, we propose a novel human aided method to evaluate the performance of the personalized response generation models by online real-time conversation and offline human judgement. Moreover, the lexical divergence of the responses generated by the 5 personalized models indicates that the proposed two-phase approach achieves good results on modeling the responding style of human and generating personalized responses for the conversational systems.", "creator": "LaTeX with hyperref package"}}}