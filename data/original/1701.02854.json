{"id": "1701.02854", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "Towards Decoding as Continuous Optimization in Neural Machine Translation", "abstract": "In this work, we propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. The resulting optimisation problem can then be tackled using a whole range of continuous optimisation algorithms which have been developed and used in the literature mainly for training. Our approach is general and can be applied to other sequence-to-sequence neural models as well. We make use of this powerful decoding approach to intersect an underlying NMT with a language model, to intersect left-to-right and right-to-left NMT models, and to decode with soft constraints involving coverage and fertility of the source sentence words. The experimental results show the promise of the proposed framework.", "histories": [["v1", "Wed, 11 Jan 2017 06:02:44 GMT  (30kb)", "https://arxiv.org/abs/1701.02854v1", "v1 with preliminary results"], ["v2", "Wed, 8 Feb 2017 00:15:08 GMT  (70kb,D)", "http://arxiv.org/abs/1701.02854v2", "v2 with more results"], ["v3", "Thu, 9 Feb 2017 09:26:30 GMT  (172kb,D)", "http://arxiv.org/abs/1701.02854v3", "v2a with supplementary material"], ["v4", "Sat, 22 Jul 2017 16:35:43 GMT  (98kb,D)", "http://arxiv.org/abs/1701.02854v4", "EMNLP 2017 Camera Ready Paper"]], "COMMENTS": "v1 with preliminary results", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["cong duy vu hoang", "gholamreza haffari", "trevor cohn"], "accepted": true, "id": "1701.02854"}, "pdf": {"name": "1701.02854.pdf", "metadata": {"source": "CRF", "title": "Towards Decoding as Continuous Optimisation in Neural Machine Translation", "authors": ["Cong Duy Vu Hoang", "Gholamreza Haffari"], "emails": ["vhoang2@student.unimelb.edu.au", "gholamreza.haffari@monash.edu", "t.cohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "Sequence to sequence learning with neural networks (Graves, 2013; Sutskever et al., 2014; Lipton et al., 2015) is typically associated with two phases: training and decoding (a.k.a. inference). Model parameters are learned by optimising the training objective, so that the model generalises well when the unknown test data is decoded. The majority of literature have been focusing on developing better training paradigms or network architectures, but the decoding problem is arguably under-investigated. Conventional heuristic-based approaches for approximate infer-\nence include greedy, beam, and stochastic search. Greedy and beam search have been empirically proved to be adequate for many sequence to sequence tasks, and are the standard methods for decoding in NMT.\nHowever, these approximate inference approaches have several drawbacks. Firstly, due to sequential decoding of symbols of the target sequence, the inter-dependencies among the target symbols are not fully exploited. For example, when decoding the words of the target sentence in a left-to-right manner, the right context is not exploited leading potentially to inferior performance (see Watanabe and Sumita (2002a) who apply this idea in traditional statistical MT). Secondly, it is not trivial to apply greedy or beam search to decode in NMT models involving global features or constraints, e.g., intersecting left-to-right and right-to-left models which do not follow the same generation order. These global constraints capture different aspects and can be highly useful in producing better and more diverse translations.\nWe introduce a novel decoding framework (\u00a7 3) that effectively relaxes this discrete optimisation problem into a continuous optimisation problem. This is akin to linear programming relaxation approach for approximate inference in graphical models with discrete random variables where the exact inference is NP-hard (Sontag, 2010; Belanger and McCallum, 2016). Our continuous optimisation problems are challenging due to the non-linearity and non-convexity of the relaxed decoding objective. We make use of stochastic gradient descent (SGD) and exponentiated gradient (EG) algorithms, which are mainly used for training in the literature, for decoding based on our relaxation approach. Our decoding framework is ar X\niv :1\n70 1.\n02 85\n4v 4\n[ cs\n.C L\n] 2\n2 Ju\nl 2 01\n7\npowerful and flexible, as it enables us to decode with global constraints involving intersection of multiple NMT models (\u00a74). We present experimental results on Chinese-English and GermanEnglish translation tasks, confirming the effectiveness of our relaxed optimisation method for decoding (\u00a75)."}, {"heading": "2 Neural Machine Translation", "text": "We briefly review the attentional neural translation model proposed by Bahdanau et al. (2015) as a sequence-to-sequence neural model onto which we will apply our decoding framework.\nIn neural machine translation (NMT), the probability of the target sentence y given a source sentence x is written as:\nP\u0398 (y|x) = |y|\u2211 i=1 log P\u0398 (yi|y<i,x) (1)\nyi|y<i,x \u223c softmax (f (\u0398,y<i,x))\nwhere f is a non-linear function of the previously generated sequence of words y<i, the source sentence x, and the model parameters \u0398. In this paper, we realise f as follows:\nf (\u0398,y<i,x) = Wo \u00b7MLP ( ci,E yi\u22121 T , gi ) + bo\ngi = RNN \u03c6 dec ( ci,E yi\u22121 T , gi\u22121 ) where MLP is a single hidden layer neural network with tanh activation function, and Eyi\u22121T is the embedding of the target word yi\u22121 in the embedding matrix ET \u2208 Rne\u00d7|VT | of the target language vocabulary VT and ne is the embedding dimension. The state gi of the decoder RNN is a function of yi\u22121, its previous state gi\u22121, and the context ci = \u2211|x| j=1 \u03b1ijhj summarises parts of the source sentence which are attended to, where\n\u03b1i = softmax(ei) ; eij = MLP (gi\u22121,hj)\nhj = biRNN \u03b8 enc ( E xj S , \u2212\u2192 h j\u22121, \u2190\u2212 h j+1 ) In above, \u2212\u2192 h i and \u2190\u2212 h i are the states of the left-toright and right-to-left RNNs encoding the source sentence, and ExjS is the embedding of the source word xj in the embedding matrix ES \u2208 Rn \u2032 e\u00d7|VS | of the source language vocabulary VS and n\u2032e is the embedding dimension.\nGiven a bilingual corpus D, the model parameters are learned by maximizing the (regularised)\nconditional log-likelihood:\n\u0398\u2217 := argmax\u0398 \u2211\n(x,y)\u2208D\nlog P\u0398 (y | x) . (2)\nThe model parameters \u0398 include the weight matrix Wo \u2208 R|VT |\u00d7nh and the bias bo \u2208 R|VT | \u2013 with nH denoting the hidden dimension size \u2013 as well as the RNN encoder biRNN\u03b8enc / decoder RNN\u03c6dec parameters, word embedding matrices, and those of the attention mechanism. The model is trained end-to-end by optimising the training objective using stochastic gradient descent (SGD) or its variants. In this paper, we are interested in the decoding problem though which is outlined in the next section."}, {"heading": "3 Decoding as Continuous Optimisation", "text": "In decoding, we are interested in finding the highest probability translation for a given source sentence:\nminimisey \u2212 P\u0398 (y | x) s.t. y \u2208 Yx (3)\nwhere Yx is the space of possible translations for the source sentence x. In general, searching Yx to find the highest probability translation is intractable due to long-range dependency terms in eqn (1) which prevents dynamic programming for efficient search algorithms in this exponentiallylarge space of possible translations with respect to the input length |x|.\nWe now formulate this discrete optimisation problem as a continuous one, and then use standard algorithms for continuous optimisation for decoding. Let us assume that the maximum length of a possible translation for a source sentence is known and denote it by `. The best translation for a given source sentence solves the following optimisation problem:\ny\u2217 = arg min y1,...,y` \u2211\u0300 i=1 \u2212 log P\u0398 (yi | y<i,x) (4)\ns.t. \u2200i \u2208 {1 . . . `} : yi \u2208 VT .\nEquivalently, we can re-write the above discrete optimisation problem as follows:\narg min y\u03031,...,y\u0303` \u2212 \u2211\u0300 i=1 y\u0303i \u00b7 log softmax (f (\u0398, y\u0303<i,x)) s.t. \u2200i \u2208 {1 . . . `} : y\u0303i \u2208 I|VT | (5)\nAlgorithm 1 The EG Algorithm for Decoding by Optimisation 1: For all i initialise y\u03020i \u2208 \u2206|VT | 2: for t = 1, . . . ,MaxIter do . Q(.) is defined as eqn (6) 3: For all i, w : calculate\u2207t\u22121i,w = \u2202Q(y\u0302t\u221211 ,...,y\u0302 t\u22121 ` ) \u2202y\u0302i(w) . using backpropagation\n4: For all i, w : update y\u0302ti(w) \u221d y\u0302t\u22121i (w) \u00b7 exp ( \u2212\u03b7\u2207t\u22121i,w ) . \u03b7 is the step size\n5: return arg mintQ(y\u0302t1, . . . , y\u0302t`)\nwhere y\u0303i are vectors using the one-hot representation of the target words I|VT |.\nWe now convert the optimisation problem (5) to a continuous one by dropping the integrality constraints y\u0303i \u2208 I|V | and require the variables to take values from the probability simplex:\narg min y\u03021,...,y\u0302` \u2212 \u2211\u0300 i=1 y\u0302i \u00b7 log softmax (f (\u0398, y\u0302<i,x))\ns.t. \u2200i \u2208 {1 . . . `} : y\u0302i \u2208 \u2206|VT |\nwhere \u2206|VT | is the |VT |-dimensional probability simplex, i.e., {y\u0302i \u2208 [0, 1]|VT | : \u2016y\u0302i\u20161 = 1}. Intuitively, this amounts to replacing EyiT with the expected embedding of target language words Ey\u0302i(w)[E w T ] under the distribution y\u0302i in the NMT model. After solving the above constrained continuous optimisation problem, there is no guarantee that the resulting solution {y\u0302\u2217i }`i=1 to include one-hot vectors corresponding to target language words. It instead will have distributions over target language vocabulary for each random variable of interest in prediction, so we need a technique to round up this fractional solution. Our method is to put all of the probability mass on the word with the highest probability1 for each y\u0302\u2217i . We leave exploration of more elaborate projection techniques to the future work.\nIn the context of graphical models, the above relaxation technique gives rise to linear programming for approximate inference (Sontag, 2010; Belanger and McCallum, 2016). However, our decoding problem is much harder due to the nonlinearity and non-convexity of the objective function operating on high dimensional space for deep models. We now turn our attention to optimisation algorithms to effectively solve the decoding optimisation problem.\n1If there are multiple words with the same highest probability mass, we choose one of them arbitrarily."}, {"heading": "3.1 Exponentiated Gradient (EG)", "text": "Exponentiated gradient (Kivinen and Warmuth, 1997) is an elegant algorithm for solving optimisation problems involving simplex constraints. Recall our constrained optimisation problem:\narg min y\u03021,...,y\u0302` Q(y\u03021, . . . , y\u0302`) s.t. \u2200i \u2208 {1 . . . `} : y\u0302i \u2208 \u2206|VT |\nwhere Q(y\u03021, . . . , y\u0302`) is defined as\n\u2212 \u2211\u0300 i=1 y\u0302i \u00b7 log softmax (f (\u0398, y\u0302<i,x)) . (6)\nEG is an iterative algorithm, which updates each distribution y\u0302ti in the current time-step t based on the distributions of the previous time-step as follows:\n\u2200w \u2208 VT : y\u0302ti(w) = 1 Zti y\u0302t\u22121i (w) exp\n( \u2212\u03b7\u2207t\u22121i,w ) where \u03b7 is the step size, \u2207t\u22121i,w = \u2202Q(y\u0302t\u221211 ,...,y\u0302 t\u22121 ` )\n\u2202y\u0302i(w)\nand Zti is the normalisation constant\nZti = \u2211 w\u2208VT y\u0302t\u22121i (w) exp ( \u2212\u03b7\u2207t\u22121i,w ) .\nThe partial derivatives \u2207i,w are calculated using the back propagation algorithm treating y\u0302i\u2019s as parameters and the original parameters of the model \u0398 as constants. Adapting EG to our decoding problem leads to Algorithm 1. It can be shown that the EG algorithm is a gradient descent algorithm for minimising the following objective function subject to the simplex constraints:\nQ(y\u03021, . . . , y\u0302`)\u2212 \u03b3 \u2211\u0300 i=1 \u2211 w\u2208VT y\u0302i(w) log 1 y\u0302i(w)\n= Q(y\u03021, . . . , y\u0302`)\u2212 \u03b3 \u2211\u0300 i=1 Entropy(y\u0302i) (7)\nIn other words, the algorithm looks for the maximum entropy solution which also maximizes the log likelihood under the model. There are intriguing parallels with the maximum entropy formulation of log-linear models (Berger et al., 1996). In our setting, the entropy term acts as a prior which discourages overly-confident estimates without sufficient evidence."}, {"heading": "3.2 Stochastic Gradient Descent (SGD)", "text": "To be able to apply SGD to our optimisation problem, we need to make sure that the simplex constraints are kept intact. One way to achieve this is by changing the optimisation variables from y\u0302i to r\u0302i through the softmax transformation, i.e. y\u0302i = softmax (r\u0302i). The resulting unconstrained optimisation problem then becomes\nargmin r\u03021,...,r\u0302` \u2212 \u2211\u0300 i=1 softmax (r\u0302i) \u00b7 log softmax (f (\u0398, y\u0302<i,x))\nwhere EyiT is replaced with the expected embedding of the target words under the distribution resulted from the Esoftmax(r\u0302i) [E w T ] in the model.\nTo apply SGD updates, we need the gradient of the objective function with respect to the new variables r\u0302i which can be derived with the backpropagation algorithm based on the chain rule:\n\u2202Q \u2202r\u0302i(w) = \u2211 w\u2032\u2208VT \u2202Q(.) \u2202y\u0302i(w\u2032) \u2202y\u0302i(w \u2032) \u2202r\u0302i(w)\nThe resulting SGD algorithm is summarized in Algorithm 2."}, {"heading": "4 Decoding in Extended NMT", "text": "Our decoding framework allows us to effectively and flexibly add additional global factors over the output symbols during inference. This in enabling by allowing decoding for richer global models, for which there is no effective means of greedy decoding or beam search. We outline several such models, and their corresponding relaxed objective functions for optimisation-based decoding.\nBidirectional Ensemble. Standard NMT generates the translation in a left-to-right manner, conditioning each target word on its left context. However, the joint probability of the translation can be decomposed in a myriad of different orders; one compelling alternative would be to condition each target word on its right context, i.e., generating the target sentence from right-to-left. We would\nnot expect a right-to-left model to outperform a left-to-right, however, as the left-to-right ordering reflects the natural temporal order of spoken language. However, the right-to-left model is likely to provide a complementary signal in translation as it will be bringing different biases and making largely independent prediction errors to those of the left-to-right model. For this reason, we propose to use both models, and seek to find translations that have high probability according both models (this mirrors work on bidirectional decoding in classical statistical machine translation by Watanabe and Sumita (2002b).) Decoding under the ensemble of these models leads to an intractable search problem, not well suited to traditional greedy or beam search algorithms, which require a fixed generation order of the target words. This ensemble decoding problem can be formulated simply in our linear relaxation approach, using the following objective function:\nC+bidir :=\u2212 \u03b1 log P\u0398\u2190 (y | x) \u2212 (1\u2212 \u03b1) log P\u0398\u2192 (y | x) ; (8)\nwhere \u03b1 is an interpolation hyper-parameter, which we set to 0.5; \u0398\u2192 and \u0398\u2190 are the pretrained left-to-right and right-to-left models, respectively. This bidirectional agreement may also lead to improvement in translation diversity, as shown in (Li and Jurafsky, 2016) in a re-ranking evaluation.\nBilingual Ensemble. Another source of complementary information is in terms of the translation direction, that is forward translation from the source to the target language, and reverse translation in the target to source direction. The desire now is to find a translation which is good under both the forward and reverse translation models. This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see (Lopez and Resnik, 2006)). More specifically, we decode for the best translation in the intersection of the source-to-target and targetto-source models by minimizing the following objective function:\nC+biling :=\u2212 \u03b1 log P\u0398s\u2192t (y | x) \u2212 (1\u2212 \u03b1) log P\u0398s\u2190t (x | y) ; (9)\nwhere \u03b1 is an interpolation hyper-parameter to be fine-tuned; and \u0398s\u2192t and \u0398s\u2190t are the pre-trained\nAlgorithm 2 The SGD Algorithm for Decoding by Optimisation 1: For all i initialise r\u03020i 2: for t = 1, . . . ,MaxIter do . Q(.) is defined in eqn (6) and y\u0302i = softmax(r\u0302i) 3: For all i, w : calculate\u2207t\u22121i,w = \u2211 w\u2032\u2208VT \u2202Q(y\u0302t\u221211 ,...,y\u0302 t\u22121 ` ) \u2202y\u0302i(w\u2032) \u2202y\u0302i(w \u2032) \u2202r\u0302i(w) . using backpropagation\n4: For all i, w : update r\u0302ti(w) = r\u0302 t\u22121 i (w)\u2212 \u03b7\u2207t\u22121i,w . \u03b7 is the step size 5: return arg mintQ(softmax(r\u0302t1), . . . , softmax(r\u0302t`))\nsource-to-target and target-to-source models, respectively. Decoding for the best translation under the above objective function leads to an intractable search problem, as the reverse model is global over the target language, meaning there is no obvious means of search with greedy algorithm or alike.\nDiscussion. There are two important considerations on how best to initialise the relaxed optimisation in the above settings, and how best to choose the step size. As the relaxed optimisation problem is, in general, non-convex, finding a plausible initialisation is likely to be important for avoiding local optima. Furthermore, a proper step size is a key in the success of the EG-based and SGD-based optimisation algorithms, and there is no obvious method how to best choose its value. We may also adaptively change the step size using (scheduled) annealing or via the line search. We return to this considerations in the experimental evaluation."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Setup", "text": "Datasets. We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks corpus (Cettolo et al., 2014) and WMT\n20162 corpus. The statistics of the datasets can be found in Table 1.\nNMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit3 (Cohn et al., 2016), and using the dynet deep learning library4 (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For vocabulary sizes, we have chosen the word frequency cut-off 5 for creating the vocabularies for all datasets. For large-scale dataset with WMT, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) so that the neural MT system can tackle the unknown word problem (Luong et al., 2015).5 For training our neural models, the best perplexity scores on the development set is used for early stopping, which usually occurs after 5-8 epochs.\nEvaluation Metrics. We evaluated in terms of search error, measured using the model score of the inferred solution (either continuous or discrete), as well as measuring the end translation quality with case-insensitive BLEU (Papineni et al., 2002). The continuous cost measures \u2212 1|y\u0302| logP\u0398 (y\u0302 | x) under the model \u0398; the discrete model score has the same formulation, albeit using the discrete rounded solution y (see \u00a73). Note the cost can be used as a tool for selecting the best inference solution, as well as assessing convergence, as we illustrate below.\n2http://www.statmt.org/wmt16/ translation-task.html\n3https://github.com/duyvuleo/Mantidae 4https://github.com/clab/dynet 5With this BPE method, the OOV rates of tune and test\nsets are lower than 1%."}, {"heading": "5.2 Results and Analysis", "text": "Initialisation and Step Size. As our relaxed optimisation problems are not convex, local optima are likely to be a problem. We test this empirically, focusing on the effect that initialisation and step size, \u03b7, have on the inference quality.\nFor plausible initialisation states, we evaluate different strategies: uniform in which the relaxed variables y\u0302 are initialised to 1|VT | ; and greedy or beam whereby y\u0302 are initialised based on an already good solution produced by a baseline decoder with greedy (gdec) or beam (bdec). Instead of using the Viterbi outputs as a one-hot representation, we initialise to the probability prediction6 vectors, which serves to limit attraction of the initialisation condition, which is likely to be a local (but not global) optima.\nFigure 1 illustrates the effect of initialisation on the EG algorithm, in terms of search error (left and middle) and translation quality (right), as we vary the number of iterations of inference. There is clear evidence of non-convexity: all initialisation methods can be seen to converge using all three measures, however they arrive at highly different solutions. Uniform initialisation is clearly not a viable approach, while greedy and beam initialisation both yield much better results. The best initialisation, beam, outperforms both greedy and beam decoding in terms of BLEU.\nNote that the EG algorithm has fairly slow convergence, requiring at least 100 iterations, irrespective of the initialisation. To overcome this,\n6Here, the EG algorithm uses softmax normalization whereas the SGD algorithm uses pre-softmax one.\nwe use momentum (Qian, 1999) to accelerate the convergence by modifying the term \u2207ti,w in Algorithm 1 with a weighted moving average of past gradients:\n\u2207t\u22121i,w = \u03b3\u2207t\u22122i,w + \u03b7 \u2202Q(y\u0302t\u221211 , . . . , y\u0302 t\u22121 ` )\n\u2202y\u0302i(w)\nwhere we set the momentum term \u03b3 = 0.9. The EG with momentum (EG-MOM) converges after fewer iterations (about 35), and results in marginally better BLEU scores. The momentum technique is usually used for SGD involving additive updates; it is interesting to see it also works in EG with multiplicative updates.\nThe step size, \u03b7, is another important hyperparameter for gradient based search. We tune the step size using line search over [10, 400] over the development set. Figure 1 illustrates the effect of changing step size from 50 to 400 (compare EG and EG-400 with uniform), which results in a marked difference of about 10 BLEU points, underlining the importance of tuning this value. We found that EG with momentum had less of a reliance on step size, with optimal values in [10, 50]; we use this setting hereafter.\nContinuous vs Discrete Costs. Another important question is whether the assumption behind continuous relaxation is valid, i.e., if we optimise a continuous cost to solve a discrete problem, do we improve the discrete output? Although the continuous cost diminishes with inference iterations (Figure 1 centre), and appears to converge to an optima, it is not clear whether this corresponds to a better discrete output (note that the BLEU scores\ndo show improvements Figure 1.) Figure 2 illustrates the relation between the two cost measures, showing that in almost all cases the discrete and continuous costs are identical. Linear relaxation effectively fails only for a handful of cases, where the nearest discrete solution is significantly worse than it would appear using the continuous cost.\nEG vs SGD. Both the EG and SGD algorithms are iterative methods for solving the relaxed optimisation problem with simplex constraints. We measure empirically their difference in terms of quality of inference and speed of convergence, as illustrated in Figure 3. Observe that SGD requires 150 iterations for convergence, whereas EG requires many fewer (50). This concurs with previous work on learning structured prediction models with EG (Globerson et al., 2007). Further, the EG algorithm consistently produces better results in terms of both model cost and BLEU.\nEG vs Reranking. Reranking is an alternative method for integrating global factors into the existing NMT systems. We compare our EG decoding algorithm against the reranking approach with bidirectional factor where the N-best outputs of a left-to-right decoder is re-scored with the forced decoder operateing in a right-to-left fashion. The\nresults are shown in Table 2. Our EG algorithm initialised with the reranked output achieves the best BLEU score. We also compare reranking with EG algorithm initialised with the beam decoder, where for direct comparison we filter out sentences with length greater than that of the beam output in the k-best lists. These results show that the EG algorithm is capable of effectively exploiting the search space.\nAs opposed to re-ranking, our approach does not need a pipeline, e.g. to produce and score nbest lists, to tune the weights etc. The run time complexity of our approach is comparable with that of reranking: our model needs repeated application of the NMT global factors to navigate through the search space, whereas re-ranking needs to use the underlying NMT model to generate the the nbest list and generate their global NMT scores. Note that our method swaps some sparse 1-hot vector operations for dense and requires a back-propagation pass, where both operations are relatively cheap on GPUs. Overall our expectation is that for sufficiently large k to get improvements in BLEU, our relaxed decoding is potentially faster.\nComputational Efficiency. We also quantify the computational efficiency of the proposed decoding approach. Benchmarking on a GPU Titan X for decoding 506 sentences of BTEC zh\u2192en, it takes 0.02s/sentence for greedy, 0.07s/sentence\nfor beam 5, 0.11s/sentence for beam 10, and 3.1s/sentence for our relaxed EG decoding (with an average of 35 EG iterations). More concretely, our relaxed EG decoding includes: 0.94s/sentence for the forward step, 2.09s/sentence for the backward step, and<0.01s/sentence for the update and additional steps. It turns out that the backward step is the most computationally expensive step, limiting the practical applicability of the proposed decoding approach. Addressing this important issue is left for our future research.\nMain Results. Table 3 shows our experimental results across all datasets, evaluating the EG algorithm and its variants.7 For the EG algorithm with greedy initialisation (top), we see small but consistent improvements in terms of BLEU. Beam initialisation led to overall higher BLEU scores, and again demonstrating a similar pattern of improvements, albeit of a lower magnitude, over the initialisation values.\nNext we evaluate the capability of our inference method with extended NMT models, where approximate algorithms such as greedy or beam search are infeasible. With the bidirectional ensemble, we obtained the statistically significant BLEU score improvements compared to the unidirectional models, for either greedy or beam initialisation. This is interesting in the sense that\n7Given the aforementioned analysis and space constraints, here we reported the results for the EG algorithm only.\nthe unidirectional right-to-left model always performs worse than the left-to-right model. However, our method with bidirectional ensemble is capable of combining their strengths in a unified setting. For the bilingual ensemble, we see similar effects, with better BLEU score improvements in most cases, albeit of a lower magnitude, over the bidirectional one. This is likely to be due to a disparity with the training condition for the models, which were learned independently of one another.\nOverall, decoding in extended NMT models leads to performance improvements compared to the baselines. This is one of the main findings in this work, and augurs well for its extension to other global model variants."}, {"heading": "6 Related Work", "text": "Decoding (inference) for neural models is an important task; however, there is limited research in this space perhaps due to the challenging nature of this task, with only a few works exploring some extensions to improve upon them. The most widely-used inference methods include sampling (Cho, 2016), greedy and beam search (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia), and reranking (Birch, 2016; Li and Jurafsky, 2016).\nCho (2016) proposed to perturb the neural model by injecting noise(s) in the hidden transition function of the conditional recurrent neural language model during greedy or beam search, and execute multiple parallel decoding runs. This strategy can improves over greedy and beam search; however, it is not clear how, when and where noise should be injected to be beneficial. Recently, Wiseman and Rush (2016) proposed beam search optimisation while training neural models, where the model parameters are updated in case the gold standard falls outside of the beam. This exposes the model to its past incorrect predicted labels, hence making the training more robust. This is orthogonal to our approach where we focus on the decoding problem with a pre-trained model.\nReranking has also been proposed as a means of global model combination: Birch (2016) and Li and Jurafsky (2016) re-rank the left-to-right decoded translations based on the scores of a rightto-left model, learning to more diverse translations. Related, Li et al. (2016) learn to adjust the beam diversity with reinforcement learning.\nPerhaps most relevant is Snelleman (2016), per-\nformed concurrently to this work, who also proposed an inference method for NMT using linear relaxation. Snelleman\u2019s method was similar to our SGD approach, however he did not manage to outperform beam search baselines with an encoderdecoder. In contrast we go much further, proposing the EG algorithm, which we show works much more effectively than SGD, and demonstrate how this can be applied to inference in an attentional encoder-decoder. Moreover, we demonstrate the utility of related optimisation for inference over global ensembles of models, resulting in consistent improvements in search error and end translation quality.\nRecently, relaxation techniques have been applied to deep models for training and inference in text classification (Belanger and McCallum, 2016; Belanger et al., 2017), and fully differentiable training of sequence-to-sequence models with scheduled-sampling (Goyal et al., 2017). Our work has applied the relaxation technique specifically for decoding in NMT models."}, {"heading": "7 Conclusions", "text": "This work presents the first attempt in formulating decoding in NMT as a continuous optimisation problem. The core idea is to drop the integrality (i.e. one-hot vector) constraint from the prediction variables and allow them to have soft assignments within the probability simplex while minimising the loss function produced by the neural model. We have provided two optimisation algorithms \u2013 exponentiated gradient (EG) and stochas-\ntic gradient descent (SGD) \u2013 for optimising the resulting contained optimisation problem, where our findings show the effectiveness of EG compared to SGD. Thanks to our framework, we have been able to decode when intersecting left-to-right and right-to-left as well as source-to-target and targetto-source NMT models. Our results show that our decoding framework is effective and lead to substantial improvements in translations8 generated from the intersected models, where the typical greedy or beam search algorithms are not applicable.\nThis work raises several compelling possibilities which we intend to address in future work, including improving the decoding speed, integrating additional constraints such as word coverage and fertility into decoding,9 and applying our method to other intractable structured prediction such as parsing."}, {"heading": "Acknowledgments", "text": "We thank the reviewers for valuable feedbacks and discussions. Cong Duy Vu Hoang is supported by Australian Government Research Training Program Scholarships at the University of Melbourne, Australia. Trevor Cohn is supported by the ARC Future Fellowship. This work is partly supported by an ARC DP grant to Trevor Cohn and Gholamreza Haffari.\n8Some comparative translation examples are included in Figure 4.\n9These constraints have only been used for training in the previous works (Cohn et al., 2016; Mi et al., 2016)."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["References Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proc. of 3rd International Conference on Learning Representa-", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Endto-End Learning for Structured Prediction Energy Networks", "author": ["D. Belanger", "B. Yang", "A. McCallum."], "venue": "ArXiv e-prints.", "citeRegEx": "Belanger et al\\.,? 2017", "shortCiteRegEx": "Belanger et al\\.", "year": 2017}, {"title": "Structured prediction energy networks", "author": ["David Belanger", "Andrew McCallum."], "venue": "Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML\u201916, pages 983\u2013992. JMLR.org.", "citeRegEx": "Belanger and McCallum.,? 2016", "shortCiteRegEx": "Belanger and McCallum.", "year": 2016}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L. Berger", "Vincent J. Della Pietra", "Stephen A. Della Pietra."], "venue": "Comput. Linguist., 22(1):39\u201371.", "citeRegEx": "Berger et al\\.,? 1996", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Edinburgh Neural Machine Translation Systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers. Berlin, Germany.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Report on the 11th IWSLT Evaluation Campaign", "author": ["M. Cettolo", "J. Niehues", "S. Stuker", "L. Bentivogli", "M. Federico."], "venue": "Proc. of The International Workshop on Spoken Language Translation (IWSLT).", "citeRegEx": "Cettolo et al\\.,? 2014", "shortCiteRegEx": "Cettolo et al\\.", "year": 2014}, {"title": "Noisy Parallel Approximate Decoding for Conditional Recurrent Language Model", "author": ["K. Cho."], "venue": "ArXiv e-prints.", "citeRegEx": "Cho.,? 2016", "shortCiteRegEx": "Cho.", "year": 2016}, {"title": "Incorporating Structural Alignment Biases into an Attentional Neural Translation Model", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari."], "venue": "Proceedings of the 2016 Conference of", "citeRegEx": "Cohn et al\\.,? 2016", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Exponentiated Gradient Algorithms for Log-linear Structured Prediction", "author": ["Amir Globerson", "Terry Y. Koo", "Xavier Carreras", "Michael Collins."], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 305\u2013312, New", "citeRegEx": "Globerson et al\\.,? 2007", "shortCiteRegEx": "Globerson et al\\.", "year": 2007}, {"title": "Differentiable scheduled sampling for credit assignment", "author": ["Kartik Goyal", "Chris Dyer", "Taylor BergKirkpatrick."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short", "citeRegEx": "Goyal et al\\.,? 2017", "shortCiteRegEx": "Goyal et al\\.", "year": 2017}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["A. Graves."], "venue": "ArXiv e-prints.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber."], "venue": "Neural Comput., 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors", "author": ["Jyrki Kivinen", "Manfred K. Warmuth."], "venue": "Inf. Comput., 132(1):1\u201363.", "citeRegEx": "Kivinen and Warmuth.,? 1997", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Mutual Information and Diverse Decoding Improve Neural Machine Translation", "author": ["J. Li", "D. Jurafsky."], "venue": "ArXiv e-prints.", "citeRegEx": "Li and Jurafsky.,? 2016", "shortCiteRegEx": "Li and Jurafsky.", "year": 2016}, {"title": "A Simple, Fast Diverse Decoding Algorithm for Neural Generation", "author": ["J. Li", "W. Monroe", "D. Jurafsky."], "venue": "ArXiv e-prints.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A Critical Review of Recurrent Neural Networks for Sequence Learning", "author": ["Z.C. Lipton", "J. Berkowitz", "C. Elkan."], "venue": "ArXiv e-prints.", "citeRegEx": "Lipton et al\\.,? 2015", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Word-based alignment, phrase-based translation: Whats the link", "author": ["Adam Lopez", "Philip Resnik."], "venue": "Proceedings of 7th Biennial Conference of the Association for Machine Translation in the Americas (AMTA).", "citeRegEx": "Lopez and Resnik.,? 2006", "shortCiteRegEx": "Lopez and Resnik.", "year": 2006}, {"title": "Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Coverage Embedding Models for Neural Machine Translation", "author": ["Haitao Mi", "Baskaran Sankaran", "Zhiguo Wang", "Abe Ittycheriah."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 955\u2013960, Austin,", "citeRegEx": "Mi et al\\.,? 2016", "shortCiteRegEx": "Mi et al\\.", "year": 2016}, {"title": "DyNet: The Dynamic Neural Network Toolkit", "author": ["Y. Oda", "M. Richardson", "N. Saphra", "S. Swayamdipta", "P. Yin."], "venue": "ArXiv e-prints.", "citeRegEx": "Oda et al\\.,? 2017", "shortCiteRegEx": "Oda et al\\.", "year": 2017}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 295\u2013302.", "citeRegEx": "Och and Ney.,? 2002", "shortCiteRegEx": "Och and Ney.", "year": 2002}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "On the momentum term in gradient descent learning algorithms", "author": ["Ning Qian."], "venue": "Neural Networks, 12(1):145 \u2013 151.", "citeRegEx": "Qian.,? 1999", "shortCiteRegEx": "Qian.", "year": 1999}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Decoding neural machine translation using gradient descent", "author": ["Emanuel Snelleman."], "venue": "Master\u2019s thesis, Chalmers University of Technology, Gothenburg, Sweden.", "citeRegEx": "Snelleman.,? 2016", "shortCiteRegEx": "Snelleman.", "year": 2016}, {"title": "Approximate Inference in Graphical Models using LP Relaxations", "author": ["David Sontag."], "venue": "Ph.D. thesis, Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science.", "citeRegEx": "Sontag.,? 2010", "shortCiteRegEx": "Sontag.", "year": 2010}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS\u201914, pages 3104\u20133112, Cambridge, MA,", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Bidirectional Decoding for Statistical Machine Translation", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, COLING \u201902, pages 1\u20137, Stroudsburg, PA, USA. Association", "citeRegEx": "Watanabe and Sumita.,? 2002a", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2002}, {"title": "Bidirectional decoding for statistical machine translation", "author": ["Taro Watanabe", "Eiichiro Sumita."], "venue": "Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1\u20137.", "citeRegEx": "Watanabe and Sumita.,? 2002b", "shortCiteRegEx": "Watanabe and Sumita.", "year": 2002}, {"title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "author": ["Sam Wiseman", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296\u20131306, Austin, Texas. Asso-", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}], "referenceMentions": [{"referenceID": 10, "context": "Sequence to sequence learning with neural networks (Graves, 2013; Sutskever et al., 2014; Lipton et al., 2015) is typically associated with two phases: training and decoding (a.", "startOffset": 51, "endOffset": 110}, {"referenceID": 26, "context": "Sequence to sequence learning with neural networks (Graves, 2013; Sutskever et al., 2014; Lipton et al., 2015) is typically associated with two phases: training and decoding (a.", "startOffset": 51, "endOffset": 110}, {"referenceID": 15, "context": "Sequence to sequence learning with neural networks (Graves, 2013; Sutskever et al., 2014; Lipton et al., 2015) is typically associated with two phases: training and decoding (a.", "startOffset": 51, "endOffset": 110}, {"referenceID": 27, "context": "For example, when decoding the words of the target sentence in a left-to-right manner, the right context is not exploited leading potentially to inferior performance (see Watanabe and Sumita (2002a) who apply this idea in traditional statistical MT).", "startOffset": 171, "endOffset": 199}, {"referenceID": 25, "context": "This is akin to linear programming relaxation approach for approximate inference in graphical models with discrete random variables where the exact inference is NP-hard (Sontag, 2010; Belanger and McCallum, 2016).", "startOffset": 169, "endOffset": 212}, {"referenceID": 2, "context": "This is akin to linear programming relaxation approach for approximate inference in graphical models with discrete random variables where the exact inference is NP-hard (Sontag, 2010; Belanger and McCallum, 2016).", "startOffset": 169, "endOffset": 212}, {"referenceID": 0, "context": "We briefly review the attentional neural translation model proposed by Bahdanau et al. (2015) as a sequence-to-sequence neural model onto which we will apply our decoding framework.", "startOffset": 71, "endOffset": 94}, {"referenceID": 25, "context": "In the context of graphical models, the above relaxation technique gives rise to linear programming for approximate inference (Sontag, 2010; Belanger and McCallum, 2016).", "startOffset": 126, "endOffset": 169}, {"referenceID": 2, "context": "In the context of graphical models, the above relaxation technique gives rise to linear programming for approximate inference (Sontag, 2010; Belanger and McCallum, 2016).", "startOffset": 126, "endOffset": 169}, {"referenceID": 12, "context": "Exponentiated gradient (Kivinen and Warmuth, 1997) is an elegant algorithm for solving optimisation problems involving simplex constraints.", "startOffset": 23, "endOffset": 50}, {"referenceID": 3, "context": "There are intriguing parallels with the maximum entropy formulation of log-linear models (Berger et al., 1996).", "startOffset": 89, "endOffset": 110}, {"referenceID": 27, "context": "For this reason, we propose to use both models, and seek to find translations that have high probability according both models (this mirrors work on bidirectional decoding in classical statistical machine translation by Watanabe and Sumita (2002b).) Decoding under the ensemble of these models leads to an intractable search problem, not well suited to traditional greedy or beam search algorithms, which require a fixed generation order of the target words.", "startOffset": 220, "endOffset": 248}, {"referenceID": 13, "context": "This bidirectional agreement may also lead to improvement in translation diversity, as shown in (Li and Jurafsky, 2016) in a re-ranking evaluation.", "startOffset": 96, "endOffset": 119}, {"referenceID": 20, "context": "This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see (Lopez and Resnik, 2006)).", "startOffset": 107, "endOffset": 126}, {"referenceID": 16, "context": "This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see (Lopez and Resnik, 2006)).", "startOffset": 200, "endOffset": 224}, {"referenceID": 5, "context": "We conducted our experiments on datasets with different scales, translating between Chinese\u2192English using the BTEC corpus, and German\u2192English using the IWSLT 2015 TED Talks corpus (Cettolo et al., 2014) and WMT 20162 corpus.", "startOffset": 180, "endOffset": 202}, {"referenceID": 7, "context": "We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit3 (Cohn et al., 2016), and using the dynet deep learning library4 (Neubig et al.", "startOffset": 97, "endOffset": 116}, {"referenceID": 11, "context": "We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences.", "startOffset": 35, "endOffset": 69}, {"referenceID": 4, "context": "For large-scale dataset with WMT, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) so that the neural MT system can tackle the unknown word problem (Luong et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 17, "context": ", 2016) so that the neural MT system can tackle the unknown word problem (Luong et al., 2015).", "startOffset": 73, "endOffset": 93}, {"referenceID": 21, "context": "We evaluated in terms of search error, measured using the model score of the inferred solution (either continuous or discrete), as well as measuring the end translation quality with case-insensitive BLEU (Papineni et al., 2002).", "startOffset": 204, "endOffset": 227}, {"referenceID": 22, "context": "we use momentum (Qian, 1999) to accelerate the convergence by modifying the term \u2207i,w in Algorithm 1 with a weighted moving average of past gradients:", "startOffset": 16, "endOffset": 28}, {"referenceID": 8, "context": "This concurs with previous work on learning structured prediction models with EG (Globerson et al., 2007).", "startOffset": 81, "endOffset": 105}, {"referenceID": 6, "context": "The most widely-used inference methods include sampling (Cho, 2016), greedy and beam search (Sutskever et al.", "startOffset": 56, "endOffset": 67}, {"referenceID": 13, "context": ", 2015, inter alia), and reranking (Birch, 2016; Li and Jurafsky, 2016).", "startOffset": 35, "endOffset": 71}, {"referenceID": 2, "context": "Recently, relaxation techniques have been applied to deep models for training and inference in text classification (Belanger and McCallum, 2016; Belanger et al., 2017), and fully differentiable training of sequence-to-sequence models with scheduled-sampling (Goyal et al.", "startOffset": 115, "endOffset": 167}, {"referenceID": 1, "context": "Recently, relaxation techniques have been applied to deep models for training and inference in text classification (Belanger and McCallum, 2016; Belanger et al., 2017), and fully differentiable training of sequence-to-sequence models with scheduled-sampling (Goyal et al.", "startOffset": 115, "endOffset": 167}, {"referenceID": 9, "context": ", 2017), and fully differentiable training of sequence-to-sequence models with scheduled-sampling (Goyal et al., 2017).", "startOffset": 98, "endOffset": 118}, {"referenceID": 7, "context": "These constraints have only been used for training in the previous works (Cohn et al., 2016; Mi et al., 2016).", "startOffset": 73, "endOffset": 109}, {"referenceID": 18, "context": "These constraints have only been used for training in the previous works (Cohn et al., 2016; Mi et al., 2016).", "startOffset": 73, "endOffset": 109}], "year": 2017, "abstractText": "We propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation. We convert decoding \u2013 basically a discrete optimization problem \u2013 into a continuous optimization problem. The resulting constrained continuous optimisation problem is then tackled using gradient-based methods. Our powerful decoding framework enables decoding intractable models such as the intersection of left-to-right and right-to-left (bidirectional) as well as source-to-target and target-to-source (bilingual) NMT models. Our empirical results show that our decoding framework is effective, and leads to substantial improvements in translations generated from the intersected models where the typical greedy or beam search is not feasible. We also compare our framework against reranking, and analyse its advantages and disadvantages.", "creator": "TeX"}}}