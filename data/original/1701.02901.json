{"id": "1701.02901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2017", "title": "A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions", "abstract": "We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.", "histories": [["v1", "Wed, 11 Jan 2017 09:32:47 GMT  (55kb,D)", "http://arxiv.org/abs/1701.02901v1", "Conference of the European Chapter of the Association for Computational Linguistics (EACL). April 2017, Val\\`encia, Spain"]], "COMMENTS": "Conference of the European Chapter of the Association for Computational Linguistics (EACL). April 2017, Val\\`encia, Spain", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["antonio toral", "v\\'ictor m s\\'anchez-cartagena"], "accepted": false, "id": "1701.02901"}, "pdf": {"name": "1701.02901.pdf", "metadata": {"source": "CRF", "title": "A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions", "authors": ["Antonio Toral", "V\u0131\u0301ctor M. S\u00e1nchez-Cartagena"], "emails": ["a.toral.ruiz@rug.nl", "vmsanchez@prompsit.com"], "sections": [{"heading": "1 Introduction", "text": "A new paradigm to statistical machine translation, neural MT (NMT), has emerged very recently and has already surpassed the performance of the mainstream approach in the field, phrasebased MT (PBMT), for a number of language pairs, e.g. (Sennrich et al., 2015; Luong et al., 2015; Costa-Jussa\u0300 and Fonollosa, 2016; Chung et al., 2016).\nIn PBMT (Koehn, 2010) different models (translation, reordering, target language, etc.) are trained independently and combined in a loglinear scheme in which each model is assigned a\n\u2217Work partly done at his previous position in Dublin City University, Ireland.\ndifferent weight by a tuning algorithm. On the contrary, in NMT all the components are jointly trained to maximise translation quality. NMT systems have a strong generalisation power because they encode translation units as numeric vectors that represent concepts, whereas in PBMT translation units are encoded as strings. Moreover, NMT systems are able to model long-distance phenomena thanks to the use of recurrent neural networks, e.g. long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (Chung et al., 2014).\nThe translations produced by NMT systems have been evaluated thus far mostly in terms of overall performance scores, be it by means of automatic or human evaluations. This has been the case of last year\u2019s news translation shared task at the First Conference on Machine Translation (WMT16).1 In this translation task, outputs produced by participant MT systems, the vast majority of which fall under either the phrase-based or neural approaches, were evaluated (i) automatically with the BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics, and (ii) manually by means of ranking translations (Federmann, 2012) and monolingual semantic similarity (Graham et al., 2016). In all these evaluations, the performance of each system is measured by means of an overall score, which, while giving an indication of the general performance of a given system, does not provide any additional information.\nIn order to understand better the new NMT paradigm and in what respects it provides better (or worse) translation quality than state-of-theart PBMT, Bentivogli et al. (2016) conducted a detailed analysis for the English-to-German language direction. In a nutshell, they found out that NMT (i) decreases post-editing effort, (ii) de-\n1http://www.statmt.org/wmt16/ translation-task.html\nar X\niv :1\n70 1.\n02 90\n1v 1\n[ cs\n.C L\n] 1\n1 Ja\nn 20\n17\ngrades faster than PBMT with sentence length and (iii) results in a notable improvement regarding reordering.\nIn this paper we delve further in this direction by conducting a multilingual and multifaceted evaluation in order to find answers to the following research questions. Whether, in comparison to PBMT, NMT systems result in:\n\u2022 considerably different output and higher degree of variability;\n\u2022 more or less fluent output;\n\u2022 more or less monotone translations;\n\u2022 translations with better or worse word order;\n\u2022 better or worse translations depending on sentence length;\n\u2022 less or more errors for different error categories: inflectional, reordering and lexical;\nHereunder we specify the main differences and similarities between this work and that of Bentivogli et al. (2016):\n\u2022 Language directions. They considered 1 while our study comprises 9.\n\u2022 Content. They dealt with transcribed speeches while we work with news stories. Previous research has shown that these two types of content pose different challenges for MT (Ruiz and Federico, 2014).\n\u2022 Size of evaluation data. Their test set had 600 sentences while our test sets span from 1 999 to 3 000 depending on the language direction.\n\u2022 Reference type. Their references were both independent from the MT output and also post-edited, while we have access only to single independent references.\n\u2022 Analyses. While some analyses overlap, some are novel in our experiments. Namely, output similarity, fluency and degree of reordering performed.\nOur analyses are conducted on the best PBMT and NMT systems submitted to the WMT16 translation task for each language direction. This (i) guarantees the reproducibility of our results as all the MT outputs are publicly available, (ii) ensures\nthat the systems evaluated are state-of-the-art, as they are the result of the latest developments at top MT research groups worldwide, and (iii) allows the conclusions that will be drawn to be rather general, as 6 languages from 4 different families (Germanic, Slavic, Romance and Finno-Ugric) are covered in the experiments.\nThe rest of the paper is organised as follows. Section 2 describes the experimental setup. Subsequent sections cover the experiments carried out in which we measured different aspects of NMT, namely: output similarity (Section 3), fluency (Section 4), degree of reordering and quality of word order (Section 5), sentence length (Section 6), and amount of errors for different error categories (Section 7). Finally, Section 8 holds the conclusions and proposals for future work."}, {"heading": "2 Experimental Setup", "text": "The experiments are run on the best2 PBMT3 and NMT constrained systems submitted to the news translation task of WMT16. Out of the 12 language directions at the translation task, we conduct experiments on 9.4 These are the language pairs between English (EN) and Czech (CS), German (DE), Finnish (FI), Romanian (RO) and Russian (RU) in both directions (except for Finnish, where only the EN\u2192FI direction is covered as no NMT system was submitted for the opposite direction, FI\u2192EN). Finally, there was an additional language at the shared task, Turkish, that is not considered here, as either none of the systems submitted was neural (Turkish\u2192EN), or there was one such system but its performance was extremely low (EN\u2192Turkish) and hence most probably not representative of the state-of-the-art in NMT.\nTable 1 shows the main characteristics of the 2According to the human evaluation (Bojar et al., 2016, Sec. 3.4). When there are not statistically significant differences between two or more NMT or PBMT systems (i.e. they belong to the same equivalence class), we pick the one with the highest BLEU score. If two NMT or PBMT systems were the best according to BLEU (draw), we pick the one with the best TER score.\n3Many of the PBMT systems contain neural features, mainly in the form of language models. If the best PBMT submission contains any neural features we use this as the PBMT system in our analyses as long as none of these features is a fully-fledged NMT system. This was the case of the best submission in terms of BLEU for RU\u2192EN (JunczysDowmunt et al., 2016)\n4Some experiments are run on a subset of these languages due to the lack of required tools for some of the languages involved.\nbest PBMT and NMT systems submitted to the WMT16 news translation task. It should be noted that all the NMT systems listed in the table fall under the encoder-decoder architecture with attention (Bahdanau et al., 2015) and operate on subword units. Word segmentation is carried out with the help of a lexicon in the EN\u2192FI direction (Sa\u0301nchez-Cartagena and Toral, 2016) and in an unsupervised way in the remaining directions (Sennrich et al., 2016)."}, {"heading": "2.1 Overall Evaluation", "text": "First, and in order to contextualise our analyses below, we report the BLEU scores achieved by the best NMT and PBMT systems for each language direction at WMT16\u2019s news translation task in Table 2.5 The best NMT system clearly outperforms the best PBMT system for all language directions out of English (relative improvements range from 5.5% for EN\u2192RO to 17.6% for EN\u2192FI) and the human evaluation (Bojar et al., 2016, Sec. 3.4) confirms these results. In the opposite direction, the human evaluation shows that the best NMT system outperforms the best PBMT system for all language directions except when the source language is Russian. This slightly differs from the automatic evaluation, according to which NMT outperforms PBMT for translations from Czech (3.3% relative improvement) and German (9.9%) but underperforms PBMT for translations from Romanian (-3.7%) and Russian (-3.8%).\n5We report the official results from http://matrix. statmt.org/matrix for the test set newstest2016 using normalised BLEU (column z BLEU-cased-norm)."}, {"heading": "3 Output Similarity", "text": "The aim of this analysis is to assess to which extent translations produced by NMT systems are different from those produced by PBMT systems. We measure this by taking the outputs of the top n6 NMT and PBMT systems submitted to each language direction7 and checking their pairwise overlap in terms of the chrF1 (Popovic\u0301, 2015) automatic evaluation metric.8\nWe would consider NMT outputs considerably different (with respect to PBMT) if they resemble each other (i.e. high pairwise overlap between NMT outputs) more than they do to PBMT systems (i.e. low overlap between an output by NMT and another by PBMT). This analysis is carried out only for language directions out of English, as for all the language directions into English there was, at most, 1 NMT submission.\nTable 3 shows the results. We can observe the same trends for all the language directions, namely: (i) the highest overlaps are between pairs of PBMT systems; (ii) next, we have overlaps between NMT systems; (iii) finally, overlaps between PBMT and NMT are the lowest.\nWe can conclude then that NMT systems lead\n6The number of systems considered is different for each language direction as it depends on the number of systems submitted. Namely, we have considered 2 NMT and 2 PBMT into Czech, 3 NMT and 5 PBMT into German, 2 NMT and 4 PBMT into Finnish, 2 NMT and 4 PBMT into Romanian and 2 NMT and 3 PBMT into Russian.\n7In order to make sure that all systems considered are truly different (rather than different runs of the same system) we consider only 1 system per paradigm (NMT and PBMT) submitted by each team for each language direction.\n8Throughout our analyses we use this metric as it has been shown to correlate better with human judgements than the de facto standard automatic metric, BLEU, when the target language is a morphologically rich language such as Finnish, while its correlation is on par with BLEU for languages with simpler morphology such as English (Popovic\u0301, 2015).\nto considerably different outputs compared to PBMT. The fact that there is higher inter-system variability in NMT than in PBMT (i.e. overlaps between pairs of NMT systems are lower than between pairs of PBMT systems) may surprise the reader, considering the fact that all NMT systems belong to the same paradigm (encoderdecoder with attention) while for some language directions (EN\u2192DE, EN\u2192FI and EN\u2192RO) there are PBMT systems belonging to two different paradigms (pure phrase-based and hierarchical). However, the higher variability among NMT translations can be attributed, we believe, to the fact that NMT systems use numeric vectors that represent concepts instead of strings as translation units."}, {"heading": "4 Fluency", "text": "In this experiment we aim to find out whether the outputs produced by NMT systems are more or less fluent than those produced by PBMT systems. To that end, we take perplexity of the MT outputs on neural language models (LMs) as a proxy for fluency. The LMs are built using TheanoLM (Enarvi and Kurimo, 2016). They contain 100 units in the projection layer, 300 units in the LSTM layer, and 300 units in the tanh layer, following the setup described by Enarvi and Kurimo (2016, Sec. 3.2). The training algorithm is Adagrad (Duchi et al., 2011) and we used 1 000 word classes obtained with mkcls from the training corpus. Vocabulary is limited to the most frequent 50 000 tokens.\nLMs are trained on a random sample of 4 million sentences selected from the News Crawl 2015 monolingual corpora, available for all the languages considered.9\nTable 4 shows the results. For all the language directions considered but one, perplexity is higher on the PBMT output compared to the NMT output. The only exception is translation into Finnish, in which perplexity on the PBMT output is slightly lower, probably because its fluency was improved by reranking it with a neural LM similar to the one we use in this experiment (Sa\u0301nchez-Cartagena and Toral, 2016). The average relative difference, i.e. considering all language directions, is notable at \u221210.45%. Thus, our experiment shows that the\n9http://data.statmt.org/ wmt16/translation-task/ training-monolingual-news-crawl.tgz\noutputs produced by NMT systems are, in general, more fluent than those produced by PBMT systems.\nOne may argue that the perplexity obtained for NMT outputs is lower than that for PBMT outputs because the LMs we used to measure perplexity follow the same model as the decoder of the NMT architecture (Bahdanau et al., 2015) and hence perplexity on a neural LM is not a valid proxy for fluency. However, the following facts support our strategy:\n\u2022 The manual evaluation of fluency carried out at the WMT16 shared translation task (Bojar et al., 2016, Sec. 3.5) already confirmed that NMT systems consistently produce more fluent translations than PBMT systems. That manual evaluation only covered language directions into English. In this experiment, we extend that conclusion to language directions out of English.\n\u2022 Neural LMs consistently outperform n-gram based LMs when assessing the fluency of real text (Kim et al., 2016; Enarvi and Kurimo, 2016). Thus, we have used the most accurate automatic tool available to measure fluency."}, {"heading": "5 Reordering", "text": "In this section we measure the amount of reordering performed by PBMT and NMT systems. Our objective is to empirically determine whether: (i) the recurrent neural networks in NMT systems produce more changes in the word order of a sen-\ntence than an PBMT decoder; and whether (ii) these neural networks make the word order of the translations closer to that of the reference.\nIn order to measure the amount of reordering, we used the Kendall\u2019s tau distance between word alignments obtained from pairs of sentences (Birch, 2011, Sec. 5.3.2). As the distance needs to be computed from permutations,10 we turned word aligments into permutations by means of the algorithm defined by Birch (2011, Sec. 5.2).\nFor each language direction, we computed word alignments between the source-language side of the test set and the target-language reference, the PBMT output and the NMT output by means of MGIZA++ (Gao and Vogel, 2008). As the test sets are rather small for word alignment (1 999 to 3 000 sentence pairs depending on the language pair), we append bigger parallel corpora to help ensure accurate word alignments and avoid data sparseness. For languages for which in-domain (news) parallel training data is available (German and Russian), we append that dataset (News Commentary). For the remaining languages (Finnish and Romanian) we use the whole Europarl corpus.\nThe amount of reordering performed by each system can be estimated as the distance between the word alignments produced by that system and\n10A permutation between a source-language sentence and a target-language sentence is defined as the set of operations that need to be carried out over the words in the sourcelanguage sentence to reflect the order of the words in the target-language sentence (Birch, 2011, Sec. 5.2).\na monotone word alignment. The similarity between the reorderings produced by each MT system and the reorderings in the reference translation can also be estimated as the distance between the corresponding word alignments. Table 5 shows the value of these distances for the language pairs included in our evaluation. The average over all the sentences in the test set of the distance proposed by Birch (2011) is depicted.\nIt can be observed that the amount of reordering introduced by both types of MT systems is lower than the quantity of reordering in the reference translation. NMT generally produces more changes in the structure of the sentence than PBMT. This is the case for all language pairs but two (EN\u2192DE and EN\u2192FI). A possible explanation for these two exceptions is the following: in the former language pair, the PBMT system is hierarchical (Williams et al., 2016) while in the latter, the output was reranked with neural LMs.\nConcerning the similarity between the reorderings produced by both MT systems and those in the reference translation, out of 9 directions, in 5 directions the NMT system performs a reordering closer to the reference, in 1 direction the PBMT system performs a reordering closer to the reference and in the remaining 3 directions the differences are not statistically significant. That is, NMT generally produces reorderings which are closer to the reference translation. The exceptions to this trend, however, do not exactly correspond\nto the language pairs for which NMT underperformed PBMT.\nIn summary, NMT systems achieve, in general, a higher degree of reordering than pure, phrasebased PBMT systems, and, overall, this reordering results in translations whose word order is closer to that of the reference translation."}, {"heading": "6 Sentence Length", "text": "In this experiment we aim to find out whether the performances of NMT and PBMT are somehow sensitive to sentence length. In this regard, Bentivogli et al. (2016) found that, for transcribed speeches, NMT outperformed PBMT regardless of sentence length while also noted that NMT\u2019s performance degraded faster than PBMT\u2019s as sentence length increases. It should be noted, however, that sentences in our content type, news, are considerably longer than sentences in transcribed speeches.11 Hence, the current experiment will determine to what extent the findings on transcribed speeches stand also for texts made of longer sentences.\nWe split the source side of the test set in subsets of different lengths: 1 to 5 words (1-5), 6 to 10 and so forth up to 46 to 50 and finally longer than 50 words (> 50). We then evaluate the outputs of the top PBMT and NMT submissions for those subsets with the chrF1 evaluation metric. Figure 1 presents the results for the language direction EN\u2192FI. We can observe that NMT outperforms PBMT up to sentences of length 36- 40, while for longer sentences PBMT outperforms NMT, with PBMT\u2019s performance remaining fairly\n11According to Ruiz et al. (2014), sentences of transcribed speeches in English average to 19 words while sentences in news average to 24 words.\nstable while NMT\u2019s clearly decreases with sentence length. The results for the other language directions exhibit similar trends.\nFigure 2 shows the relative improvements of NMT over PBMT for each sentence length subset, averaged over all the 9 language directions considered. We observe a clear trend of this value decreasing with sentence length and in fact we found a strong negative Pearson correlation (-0.79) between sentence length and the relative improvement (chrF1) of the best NMT over the best PBMT system.\nThe correlations for each language direction are shown in Table 6. We observe negative correlations for all the language directions except for DE\u2192EN."}, {"heading": "7 Error Categories", "text": "In this experiment we assess the performance of NMT versus PBMT systems on a set of error categories that correspond to five word-level error classes: inflection errors, reordering errors, missing words, extra words and incorrect lexical choices. These errors are detected automatically using the edit distance, word error rate (WER), precision-based and recall-based position-\nindependent error rates (hPER and rPER, respectively) as implemented in Hjerson (Popovic\u0301, 2011). These error classes are then defined as follows:\n\u2022 Inflection error (hINFer). A word for which its full form is marked as a hPER error while its base form matches the base form in the reference.\n\u2022 Reordering error (hRer). A word that matches the reference but is marked as a WER error.\n\u2022 Missing word (MISer). A word that occurs as deletion error in WER, is also a rPER error and does not share the base form with any hypothesis error.\n\u2022 Extra word (EXTer). A word that occurs as insertion error in WER, is also a hPER error and does not share the base form with any reference error.\n\u2022 Lexical choice error (hLEXer). A word that belongs neither to inflectional errors nor to missing or extra words.\nDue to the fact that it is difficult to disambiguate between three of these categories, namely missing words, extra words and lexical choice errors (Popovic\u0301 and Ney, 2011), we group them in a unique category, which we refer to as lexical errors.\nAs input, the tool requires the full forms and base forms of the reference translations and MT outputs. For base forms, we use stems for practical reasons. These are produced with the Snow-\nball stemmer from NLTK12 for all languages except for Czech, which is not supported. For this language we used the aggresive variant in czech stemmer.13\nTables 7 and 8 show the results for language directions out of English and into English, respectively. For all language directions, we observe that NMT results in a notable decrease of both inflection (\u221214.6% on average for language directions out of EN and \u22127.91% for language directions into EN) and reordering (\u221212.82% from EN and \u221211.94 into EN) errors. The reduction of reordering errors is compatible with the results of the experiment presented in Section 5.14\nDifferences in performance for the remaining error category, lexical errors, are much smaller. In addition, the results for that category show a mixed picture in terms of which paradigm is better, which makes it difficult to derive conclusions that apply regardless of the language pair. Out of English, NMT results in slightly less errors (0.59% decrease on average) for all target languages except for RO (2.17% increase). Similarly, in the opposite language direction, NMT also results in slightly better performance overall (1.35% error reduction on average), and looking at individual\n12http://www.nltk.org 13http://research.variancia.com/czech_\nstemmer/ 14Although the results depicted both in this section and in Section 5 show that NMT performs better reordering in general, results for particular language pairs are not exactly the same in both sections. This is due to the fact that the quality of the reordering is computed in different ways. In this section, only those words that match the reference are considered when identifying reordering errors, while in Section 5 all the words in the sentence are taken into account. That said, in Section 5 the precision of the results depends on the quality of word alignment.\nlanguage directions NMT outperforms PBMT for all of them except RU\u2192EN."}, {"heading": "8 Conclusions", "text": "We have conducted a multifaceted evaluation to compare NMT versus PBMT outputs across a number of dimensions for 9 language directions. Our aim has been to shed more light on the strengths and weaknesses of the newly introduced NMT paradigm, and to check whether, and to what extent, these generalise to different families of source and target languages. Hereunder we summarise our findings:\n\u2022 The outputs of NMT systems are considerably different compared to those of PBMT systems. In addition, there is higher intersystem variability in NMT, i.e. outputs by pairs of NMT systems are more different between them than outputs by pairs of PBMT systems.\n\u2022 NMT outputs are more fluent. We have corroborated the results of the manual evaluation of fluency at WMT16, which was conducted only for language directions into English, and we have shown evidence that this finding is true also for directions out of English.\n\u2022 NMT systems introduce more changes in word order than pure PBMT systems, but less than hierarchical PBMT systems.15 Nevertheless, for most language pairs, including those for which the best PBMT system is hierarchical, NMT\u2019s reorderings are closer to the reorderings in the reference than those of PBMT. This corroborates the findings on reordering by Bentivogli et al. (2016).\n\u2022 We have found negative correlations between sentence length and the improvement brought by NMT over PBMT for the majority of the languages examined. While for most sentence lengths NMT outperforms PBMT, for very long sentences PBMT outperforms NMT. The latter was not the case in the work by Bentivogli et al. (2016). We believe the reason behind this different finding is twofold. Firstly, the average sentence\n15The latter finding applies only to one language direction as only for that one the best PBMT system is hierarchical.\nlength in their evaluation dataset was considerably shorter; and secondly, the NMT systems included in our evaluation operate on subword units, which increases the effective sentence length they have to deal with.\n\u2022 NMT performs better in terms of inflection and reordering consistently across all language directions. We thus confirm that the findings of Bentivogli et al. (2016) regarding these two error types apply to a wide range of language directions. Differences regarding lexical errors are much smaller and inconsistent across language directions; for 7 of them NMT outperforms PBMT while for the remaining 2 the opposite is true.\nThe results for some of the evaluations, especially error categories (Section 7) have been analysed only superficially, looking at what conclusions can be derived that apply regardless of language direction. Nevertheless, all our data is publicly released,16 so we encourage interested parties to use this resource to conduct deeper languagespecific studies."}, {"heading": "Acknowledgments", "text": "The research leading to these results is supported by the European Union Seventh Framework Programme FP7/2007-2013 under grant agreement PIAP-GA-2012-324414 (Abu-MaTran) and by Science Foundation Ireland through the CNGL Programme (Grant 12/CE/I2267) in the ADAPT Centre (www.adaptcentre.ie) at Dublin City University."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "arXiv preprint arXiv:1608.04631.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Reordering metrics for statistical machine translation", "author": ["Alexandra Birch."], "venue": "Ph.D. thesis, The University of Edinburgh.", "citeRegEx": "Birch.,? 2011", "shortCiteRegEx": "Birch.", "year": 2011}, {"title": "Findings of the 2016 conference on machine translation", "author": ["Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Conference on Machine Translation, pages 131\u2013", "citeRegEx": "Post et al\\.,? 2016", "shortCiteRegEx": "Post et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "arXiv preprint arXiv:1603.00810.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Compu-", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "The JHU Machine Translation Systems for WMT 2016", "author": ["Shuoyang Ding", "Kevin Duh", "Huda Khayrallah", "Philipp Koehn", "Matt Post."], "venue": "Proceedings of the First Conference on Machine Translation, pages 272\u2013280, Berlin, Germany, August.", "citeRegEx": "Ding et al\\.,? 2016", "shortCiteRegEx": "Ding et al\\.", "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A Joint Sequence Translation Model with Integrated Reordering", "author": ["Nadir Durrani", "Helmut Schmid", "Alexander Fraser."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages", "citeRegEx": "Durrani et al\\.,? 2011", "shortCiteRegEx": "Durrani et al\\.", "year": 2011}, {"title": "TheanoLM \u2013 An Extensible Toolkit for Neural Network Language Modeling", "author": ["Seppo Enarvi", "Mikko Kurimo."], "venue": "Proceedings of the 17th Annual Conference of the International Speech Communication Association.", "citeRegEx": "Enarvi and Kurimo.,? 2016", "shortCiteRegEx": "Enarvi and Kurimo.", "year": 2016}, {"title": "Appraise: An opensource toolkit for manual evaluation of machine translation output", "author": ["Christian Federmann."], "venue": "The Prague Bulletin of Mathematical Linguistics, 98:25\u201335, September.", "citeRegEx": "Federmann.,? 2012", "shortCiteRegEx": "Federmann.", "year": 2012}, {"title": "Parallel implementations of word alignment tool", "author": ["Qin Gao", "Stephan Vogel."], "venue": "Software Engineering, Testing, and Quality Assurance for Natural", "citeRegEx": "Gao and Vogel.,? 2008", "shortCiteRegEx": "Gao and Vogel.", "year": 2008}, {"title": "Can machine translation systems be evaluated by the crowd alone", "author": ["Yvette Graham", "Timothy Baldwin", "Alistair Moffat", "Justin Zobel."], "venue": "Natural Language Engineering, FirstView:1\u201328, 1.", "citeRegEx": "Graham et al\\.,? 2016", "shortCiteRegEx": "Graham et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "The AMU-UEDIN Submission to the WMT16 News Translation Task: Attentionbased NMT Models as Feature Functions in Phrasebased SMT", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich."], "venue": "Proceedings of the First Conference", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander Rush."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pages 2741\u20132749, Phoenix, Arizona, USA, February.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, volume 4, pages 388\u2013395, Barcelona, Spain.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "NRC Russian-English Machine Translation System for WMT 2016", "author": ["Chi-kiu Lo", "Colin Cherry", "George Foster", "Darlene Stewart", "Rabib Islam", "Anna Kazantseva", "Roland Kuhn."], "venue": "Proceedings of the First Conference on Machine Translation, pages", "citeRegEx": "Lo et al\\.,? 2016", "shortCiteRegEx": "Lo et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412\u20131421, Lisbon,", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Towards automatic error analysis of machine translation output", "author": ["Maja Popovi\u0107", "Hermann Ney."], "venue": "Comput. Linguist., 37(4):657\u2013688, December.", "citeRegEx": "Popovi\u0107 and Ney.,? 2011", "shortCiteRegEx": "Popovi\u0107 and Ney.", "year": 2011}, {"title": "Hjerson: An open source tool for automatic error classification of machine translation output", "author": ["Maja Popovi\u0107."], "venue": "The Prague Bulletin of Mathematical Linguistics, 96:59\u201367.", "citeRegEx": "Popovi\u0107.,? 2011", "shortCiteRegEx": "Popovi\u0107.", "year": 2011}, {"title": "chrF: character n-gram F-score for automatic MT evaluation", "author": ["Maja Popovi\u0107."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392\u2013395, Lisbon, Portugal, September.", "citeRegEx": "Popovi\u0107.,? 2015", "shortCiteRegEx": "Popovi\u0107.", "year": 2015}, {"title": "Complexity of spoken versus written language for machine translation", "author": ["Nicholas Ruiz", "Marcello Federico."], "venue": "17th Annual Conference of the European Association for Machine Translation, EAMT, pages 173\u2013180, Dubrovnik, Croatia, June.", "citeRegEx": "Ruiz and Federico.,? 2014", "shortCiteRegEx": "Ruiz and Federico.", "year": 2014}, {"title": "Abu-matran at wmt 2016 translation task: Deep learning, morphological segmentation and tuning on character sequences", "author": ["V\u0131\u0301ctor M. S\u00e1nchez-Cartagena", "Antonio Toral"], "venue": "In Proceedings of the First Conference on Machine Translation,", "citeRegEx": "S\u00e1nchez.Cartagena and Toral.,? \\Q2016\\E", "shortCiteRegEx": "S\u00e1nchez.Cartagena and Toral.", "year": 2016}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1511.06709.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Edinburgh Neural Machine Translation Systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation, pages 371\u2013376, Berlin, Germany, August.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "Proceedings of AMTA, pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Edinburgh\u2019s statistical machine translation systems for wmt16", "author": ["Philip Williams", "Rico Sennrich", "Maria Nadejde", "Matthias Huck", "Barry Haddow", "Ond\u0159ej Bojar."], "venue": "Proceedings of the First Conference on Machine Translation, pages 399\u2013410,", "citeRegEx": "Williams et al\\.,? 2016", "shortCiteRegEx": "Williams et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 28, "context": "(Sennrich et al., 2015; Luong et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 0, "endOffset": 96}, {"referenceID": 21, "context": "(Sennrich et al., 2015; Luong et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 0, "endOffset": 96}, {"referenceID": 6, "context": "(Sennrich et al., 2015; Luong et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 0, "endOffset": 96}, {"referenceID": 5, "context": "(Sennrich et al., 2015; Luong et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Chung et al., 2016).", "startOffset": 0, "endOffset": 96}, {"referenceID": 19, "context": "In PBMT (Koehn, 2010) different models (translation, reordering, target language, etc.", "startOffset": 8, "endOffset": 21}, {"referenceID": 15, "context": "long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (Chung et al.", "startOffset": 30, "endOffset": 64}, {"referenceID": 4, "context": "long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (Chung et al., 2014).", "startOffset": 90, "endOffset": 110}, {"referenceID": 22, "context": "1 In this translation task, outputs produced by participant MT systems, the vast majority of which fall under either the phrase-based or neural approaches, were evaluated (i) automatically with the BLEU (Papineni et al., 2002) and TER (Snover et al.", "startOffset": 203, "endOffset": 226}, {"referenceID": 30, "context": ", 2002) and TER (Snover et al., 2006) metrics, and (ii) manually by means of ranking translations (Federmann, 2012) and monolingual semantic similarity (Graham et al.", "startOffset": 16, "endOffset": 37}, {"referenceID": 12, "context": ", 2006) metrics, and (ii) manually by means of ranking translations (Federmann, 2012) and monolingual semantic similarity (Graham et al.", "startOffset": 68, "endOffset": 85}, {"referenceID": 14, "context": ", 2006) metrics, and (ii) manually by means of ranking translations (Federmann, 2012) and monolingual semantic similarity (Graham et al., 2016).", "startOffset": 122, "endOffset": 143}, {"referenceID": 1, "context": "In order to understand better the new NMT paradigm and in what respects it provides better (or worse) translation quality than state-of-theart PBMT, Bentivogli et al. (2016) conducted a detailed analysis for the English-to-German language direction.", "startOffset": 149, "endOffset": 174}, {"referenceID": 1, "context": "Hereunder we specify the main differences and similarities between this work and that of Bentivogli et al. (2016):", "startOffset": 89, "endOffset": 114}, {"referenceID": 26, "context": "Previous research has shown that these two types of content pose different challenges for MT (Ruiz and Federico, 2014).", "startOffset": 93, "endOffset": 118}, {"referenceID": 8, "context": "EN\u2192CS PBMT Phrase-based, word clusters (Ding et al., 2016) NMT Unsupervised word segmentation and backtranslated monolingual corpora (Sennrich et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 29, "context": ", 2016) NMT Unsupervised word segmentation and backtranslated monolingual corpora (Sennrich et al., 2016)", "startOffset": 82, "endOffset": 105}, {"referenceID": 31, "context": "EN\u2192DE hierarchical PBMT String-to-tree, neural and dependency language models (Williams et al., 2016) NMT Same as for EN\u2192CS EN\u2192FI PBMT Phrase-based, rule-based and unsupervised word segmentation, operation sequence model (Durrani et al.", "startOffset": 78, "endOffset": 101}, {"referenceID": 10, "context": ", 2016) NMT Same as for EN\u2192CS EN\u2192FI PBMT Phrase-based, rule-based and unsupervised word segmentation, operation sequence model (Durrani et al., 2011), bilingual neural language model (Devlin et al.", "startOffset": 127, "endOffset": 149}, {"referenceID": 7, "context": ", 2011), bilingual neural language model (Devlin et al., 2014), re-ranked with a recurrent neural language model (S\u00e1nchez-Cartagena and Toral, 2016) NMT Rule-based word segmentation, backtranslated monolingual corpora (S\u00e1nchez-Cartagena and Toral, 2016)", "startOffset": 41, "endOffset": 62}, {"referenceID": 27, "context": ", 2014), re-ranked with a recurrent neural language model (S\u00e1nchez-Cartagena and Toral, 2016) NMT Rule-based word segmentation, backtranslated monolingual corpora (S\u00e1nchez-Cartagena and Toral, 2016)", "startOffset": 58, "endOffset": 93}, {"referenceID": 27, "context": ", 2014), re-ranked with a recurrent neural language model (S\u00e1nchez-Cartagena and Toral, 2016) NMT Rule-based word segmentation, backtranslated monolingual corpora (S\u00e1nchez-Cartagena and Toral, 2016)", "startOffset": 163, "endOffset": 198}, {"referenceID": 31, "context": "EN\u2192RO PBMT Phrased-based, operation sequence model, monolingual and bilingual neural language models (Williams et al., 2016) NMT Same as for EN\u2192CS EN\u2192RU PBMT Phrase-based, word clusters, bilingual neural language model (Ding et al.", "startOffset": 101, "endOffset": 124}, {"referenceID": 8, "context": ", 2016) NMT Same as for EN\u2192CS EN\u2192RU PBMT Phrase-based, word clusters, bilingual neural language model (Ding et al., 2016) NMT Same as for EN\u2192CS CS\u2192EN PBMT Same as for EN\u2192CS NMT Same as for EN\u2192CS DE\u2192EN PBMT Phrase-based, pre-reordering, compound splitting (Williams et al.", "startOffset": 102, "endOffset": 121}, {"referenceID": 31, "context": ", 2016) NMT Same as for EN\u2192CS CS\u2192EN PBMT Same as for EN\u2192CS NMT Same as for EN\u2192CS DE\u2192EN PBMT Phrase-based, pre-reordering, compound splitting (Williams et al., 2016) NMT Same as for EN\u2192CS plus reranked with a right-to-left model RO\u2192EN PBMT Phrase-based, operation sequence model, monolingual neural language model (Williams et al.", "startOffset": 141, "endOffset": 164}, {"referenceID": 31, "context": ", 2016) NMT Same as for EN\u2192CS plus reranked with a right-to-left model RO\u2192EN PBMT Phrase-based, operation sequence model, monolingual neural language model (Williams et al., 2016) NMT Same as for EN\u2192CS RU\u2192EN PBMT Phrase-based, lemmas in word alignment, sparse features, bilingual neural language model and transliteration (Lo et al.", "startOffset": 156, "endOffset": 179}, {"referenceID": 20, "context": ", 2016) NMT Same as for EN\u2192CS RU\u2192EN PBMT Phrase-based, lemmas in word alignment, sparse features, bilingual neural language model and transliteration (Lo et al., 2016) NMT Same as for EN\u2192CS", "startOffset": 150, "endOffset": 167}, {"referenceID": 0, "context": "It should be noted that all the NMT systems listed in the table fall under the encoder-decoder architecture with attention (Bahdanau et al., 2015) and operate on subword units.", "startOffset": 123, "endOffset": 146}, {"referenceID": 27, "context": "Word segmentation is carried out with the help of a lexicon in the EN\u2192FI direction (S\u00e1nchez-Cartagena and Toral, 2016) and in an unsupervised way in the remaining directions (Sennrich et al.", "startOffset": 83, "endOffset": 118}, {"referenceID": 29, "context": "Word segmentation is carried out with the help of a lexicon in the EN\u2192FI direction (S\u00e1nchez-Cartagena and Toral, 2016) and in an unsupervised way in the remaining directions (Sennrich et al., 2016).", "startOffset": 174, "endOffset": 197}, {"referenceID": 18, "context": "If the difference between them is statistically significant according to paired bootstrap resampling (Koehn, 2004) with p = 0.", "startOffset": 101, "endOffset": 114}, {"referenceID": 25, "context": "We measure this by taking the outputs of the top n6 NMT and PBMT systems submitted to each language direction7 and checking their pairwise overlap in terms of the chrF1 (Popovi\u0107, 2015) automatic evaluation metric.", "startOffset": 169, "endOffset": 184}, {"referenceID": 25, "context": "Throughout our analyses we use this metric as it has been shown to correlate better with human judgements than the de facto standard automatic metric, BLEU, when the target language is a morphologically rich language such as Finnish, while its correlation is on par with BLEU for languages with simpler morphology such as English (Popovi\u0107, 2015).", "startOffset": 330, "endOffset": 345}, {"referenceID": 11, "context": "The LMs are built using TheanoLM (Enarvi and Kurimo, 2016).", "startOffset": 33, "endOffset": 58}, {"referenceID": 9, "context": "The training algorithm is Adagrad (Duchi et al., 2011) and we used 1 000 word classes obtained with mkcls from the training corpus.", "startOffset": 34, "endOffset": 54}, {"referenceID": 27, "context": "The only exception is translation into Finnish, in which perplexity on the PBMT output is slightly lower, probably because its fluency was improved by reranking it with a neural LM similar to the one we use in this experiment (S\u00e1nchez-Cartagena and Toral, 2016).", "startOffset": 226, "endOffset": 261}, {"referenceID": 0, "context": "One may argue that the perplexity obtained for NMT outputs is lower than that for PBMT outputs because the LMs we used to measure perplexity follow the same model as the decoder of the NMT architecture (Bahdanau et al., 2015) and hence perplexity on a neural LM is not a valid proxy for fluency.", "startOffset": 202, "endOffset": 225}, {"referenceID": 17, "context": "\u2022 Neural LMs consistently outperform n-gram based LMs when assessing the fluency of real text (Kim et al., 2016; Enarvi and Kurimo, 2016).", "startOffset": 94, "endOffset": 137}, {"referenceID": 11, "context": "\u2022 Neural LMs consistently outperform n-gram based LMs when assessing the fluency of real text (Kim et al., 2016; Enarvi and Kurimo, 2016).", "startOffset": 94, "endOffset": 137}, {"referenceID": 18, "context": "If the difference between the distances depicted in the two last columns is statistically significant according to paired bootstrap resampling (Koehn, 2004) with p = 0.", "startOffset": 143, "endOffset": 156}, {"referenceID": 13, "context": "For each language direction, we computed word alignments between the source-language side of the test set and the target-language reference, the PBMT output and the NMT output by means of MGIZA++ (Gao and Vogel, 2008).", "startOffset": 196, "endOffset": 217}, {"referenceID": 2, "context": "A permutation between a source-language sentence and a target-language sentence is defined as the set of operations that need to be carried out over the words in the sourcelanguage sentence to reflect the order of the words in the target-language sentence (Birch, 2011, Sec. 5.2). a monotone word alignment. The similarity between the reorderings produced by each MT system and the reorderings in the reference translation can also be estimated as the distance between the corresponding word alignments. Table 5 shows the value of these distances for the language pairs included in our evaluation. The average over all the sentences in the test set of the distance proposed by Birch (2011) is depicted.", "startOffset": 257, "endOffset": 690}, {"referenceID": 31, "context": "A possible explanation for these two exceptions is the following: in the former language pair, the PBMT system is hierarchical (Williams et al., 2016) while in the latter, the output was reranked with neural LMs.", "startOffset": 127, "endOffset": 150}, {"referenceID": 1, "context": "In this regard, Bentivogli et al. (2016) found that, for transcribed speeches, NMT outperformed PBMT regardless of sentence length while also noted that NMT\u2019s performance degraded faster than PBMT\u2019s as sentence length increases.", "startOffset": 16, "endOffset": 41}, {"referenceID": 24, "context": "independent error rates (hPER and rPER, respectively) as implemented in Hjerson (Popovi\u0107, 2011).", "startOffset": 80, "endOffset": 95}, {"referenceID": 23, "context": "Due to the fact that it is difficult to disambiguate between three of these categories, namely missing words, extra words and lexical choice errors (Popovi\u0107 and Ney, 2011), we group them in a unique category, which we refer to as lexical errors.", "startOffset": 148, "endOffset": 171}, {"referenceID": 1, "context": "This corroborates the findings on reordering by Bentivogli et al. (2016).", "startOffset": 48, "endOffset": 73}, {"referenceID": 1, "context": "The latter was not the case in the work by Bentivogli et al. (2016). We believe the reason behind this different finding is twofold.", "startOffset": 43, "endOffset": 68}, {"referenceID": 1, "context": "We thus confirm that the findings of Bentivogli et al. (2016) regarding these two error types apply to a wide range of language directions.", "startOffset": 37, "endOffset": 62}], "year": 2017, "abstractText": "We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.", "creator": "TeX"}}}