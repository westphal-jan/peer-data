{"id": "1701.06225", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jan-2017", "title": "Predicting Demographics of High-Resolution Geographies with Geotagged Tweets", "abstract": "In this paper, we consider the problem of predicting demographics of geographic units given geotagged Tweets that are composed within these units. Traditional survey methods that offer demographics estimates are usually limited in terms of geographic resolution, geographic boundaries, and time intervals. Thus, it would be highly useful to develop computational methods that can complement traditional survey methods by offering demographics estimates at finer geographic resolutions, with flexible geographic boundaries (i.e. not confined to administrative boundaries), and at different time intervals. While prior work has focused on predicting demographics and health statistics at relatively coarse geographic resolutions such as the county-level or state-level, we introduce an approach to predict demographics at finer geographic resolutions such as the blockgroup-level. For the task of predicting gender and race/ethnicity counts at the blockgroup-level, an approach adapted from prior work to our problem achieves an average correlation of 0.389 (gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms this prior approach with an average correlation of 0.671 (gender) and 0.692 (race).", "histories": [["v1", "Sun, 22 Jan 2017 22:16:46 GMT  (165kb,D)", "http://arxiv.org/abs/1701.06225v1", "6 pages, AAAI-17 preprint"]], "COMMENTS": "6 pages, AAAI-17 preprint", "reviews": [], "SUBJECTS": "cs.LG cs.SI stat.ML", "authors": ["omar montasser", "daniel kifer"], "accepted": true, "id": "1701.06225"}, "pdf": {"name": "1701.06225.pdf", "metadata": {"source": "CRF", "title": "Predicting Demographics of High-Resolution Geographies with Geotagged Tweets", "authors": ["Omar Montasser", "Daniel Kifer"], "emails": ["ovm5033@psu.edu,", "dkifer@cse.psu.edu"], "sections": [{"heading": "1 Introduction", "text": "Social media data has become increasingly important with applications to many fields such as health (Culotta 2014a) and sociolinguistics (Eisenstein et al. 2014). Furthermore, social media has been used to complement traditional survey methods as a faster and cheaper approach to collect information and make predictions (Benton et al. 2016).\nCollecting demographics data is usually a long and costly process which limits the rate and resolution at which this collection may be performed. The U.S. Census Bureau releases a population census every 10 years with demographics data on multiple geographic resolutions (U.S. Census Bureau 2010). These geographic resolutions follow a hierarchy: each state is divided into counties, then each county is divided into tracts, then each tract is divided into block groups, and each block group is divided into blocks. The census contains demographics data at the block-level and moving up to the state-level. Also, The American Community Survey (ACS), which is a statistical survey of a sam-\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nple of people prepared by the U.S. Census Bureau, contains 1-year, 3-year, and 5-year moving average estimates of demographics but with lower resolutions (county-level and blockgroup-level) and higher error. Another important issue is that the administrative boundaries change every census, for example, the 2000 block-level boundaries are different from the 2010 block-level boundaries. This raises challenges for researchers who study populations over decades because they need to crosswalk from one census to the next. So, to compare 2000 demographics with 2010 demographics, they need to figure out how would the 2000 demographics look like using boundary definitions from 2010.\nUsing geotagged Tweets to predict demographics of geographic units can be a complementary method to survey collection methods as long as their bias can be corrected. This would be a cheaper and faster approach to estimate demographics data at resolutions finer than what traditional sources offer. In addition, this approach is not confined to administrative geographic boundaries but can be adapted to custom geographic boundaries. Also, it is more flexible by permitting collecting demographics data at different time intervals (e.g. 6 months).\nWe present a method to predict demographics of highresolution geographic units using geotagged Tweets. The main idea is to learn to predict demographics of a region based on characteristics of Tweets in that region. An important aspect of our approach is that we do not require labeling of individual Tweets. To evaluate our method, we train models to predict gender and race/ethnicity demographics of Census predefined geographies at different resolutions (block, blockgroup, tract, and county) using 2010 Census demographics data as ground truth. At the block-level, we achieve an average correlation of 0.585 (gender) and 0.487 (race) on a held-out test dataset. We find that our approach significantly improves upon the results of a competing approach adapted from prior work. We also find that for 95% of blocks, blockgroups, tracts, and counties with at least 100 Twitter users, the relative prediction error is at most 1.98, 1.15, 0.90, and 0.78 respectively.\nWe discuss related work in Section 2, we introduce necessary definitions, notations, and formally define the problem in Section 3, we describe our approach in Section 4, and we present experiments in Section 5.\nar X\niv :1\n70 1.\n06 22\n5v 1\n[ cs\n.L G\n] 2\n2 Ja\nn 20\n17"}, {"heading": "2 Related Work", "text": "The availability of large-scale geotagged Twitter data has spurred a lot of interest in predicting demographics of geographic units. It is viewed as a cheaper and faster method to draw inferences that can complement traditional survey methods. There are two strategies in general. First is predicting demographics of geographic units directly. Second is predicting demographics of Twitter users in those geographic units individually and then performing some form of aggregation.\nPrior work that employs the first strategy spans several applications. Eisenstein, Smith, and Xing (2011) predicted demographics of Zip Code Tabulation Areas (ZCTA) using geotagged Tweets and Census data. Similarly, Mohammady and Culotta (2014) predicted race composition of the 100 most populous counties from geotagged Tweets and then used that for individual-level labeling of Twitter users. There is also prior work that focused on health statistics. Schwartz et al. (2013b) predicted life-satisfaction of counties using county demographics and Tweets. Culotta (2014a) predicted several health statistics (such as obesity rates) of the 100 most populous counties. Eichstaedt et al. (2015) predicted atherosclerotic heart disease mortality rates of counties from Tweets. Ireland et al. (2015) predicted HIV rates of counties from Tweets. Loff, Reis, and Martins (2015) predicted well-being of states by estimating the Gallup-Healthways index. Benton et al. (2016) used supervised topic models to predict responses to miscellaneous survey questions such as percentage of smokers at the statelevel.\nAs described, these prior approaches made predictions at relatively coarse geographic resolutions such as the countylevel which may be due to limitations in ground truth data availability at finer resolutions (e.g. health statistics). Our approach predicts demographics at finer geographic resolutions such as the block-level. Furthermore, we explore how large a population needs to be (i.e. resolution) in order to get accurate predictions.\nRelated to the second strategy, there is a rich body of work on individual-level prediction tasks focusing on attributes of social media users. These attributes include: gender (Burger et al. 2011; Bergsma et al. 2013; Bamman, Eisenstein, and Schnoebelen 2014; Vicente, Batista, and Carvalho 2015), age (Schwartz et al. 2013a; Moseley, Alm, and Rege 2014), ethnicity (Chang et al. 2010; Chen et al. 2015), income and socio-economic status (Preot\u0327iucPietro et al. 2015; Lampos et al. 2016). In addition, several works predicted a wide-range of attributes including demographics and emotions (Culotta, Kumar, and Cutler 2015; Volkova et al. 2015). Adapting these methods to solve our problem poses several challenges. For example, it requires supervised training and labeled demographics for large numbers of Twitter users. These labels have to be collected yearly to account for concept drift associated with changing generations. Also, once Twitter users are labeled, it is not immediately obvious how to combine these observations to arrive at the demographics of geographic units since Twitter users with geotagged tweets are a highly biased sample (Malik et al. 2015). To reduce the sampling bias, Culotta\n(2014b) weighted the contributions of Twitter users based on their demographics. Also, Almeida and Pappa (2015) sampled a subset of Twitter users in an area according to the distribution of the real population in that area. Both approaches require individual-level labelling of Twitter users. Our method however directly trains to fix the bias. For example, we know the characteristics of Twitter users in an area (biased) and we know the population characteristics of an area, and we learn a function (model) that maps one into the other."}, {"heading": "3 Preliminaries", "text": "In this section, we setup some necessary definitions, notations, and formally define the problem. Our dataset is a set of Tweets {t1, t2, ..., tm}. Each Tweet ti is a tuple of the form (loci, uidi, < wi1, w i 2, w i 3, ... >) where loc is the GPS location, uid is the user id, and < w1, w2, w3, ... > is the sequence of tokens in the Tweet. We have a set of geographic units {g1, g2, ..., gn}. Each gi is a tuple of the form (shapei,yi) where shape is the boundary definition and y is ground truth count data of a demographic variable (e.g. gender) of the geographic unit. If we have a demographic variable with k mutually exclusive categories, then y is a k dimensional vector where yj is the count of category j, j \u2208 [1, k] (e.g. for gender, y1 is the count of males and y2 is the count of females). For each Tweet ti, we map it to a geographic unit gj if shapej contains loci. Thus, we group the Tweets into disjoint bags that correspond to the geographic units:\n({tj1} N1 j=1,y1), ({t j 2} N2 j=1,y2), ..., ({t j n} Nn j=1,yn)\nwhere geographic unit i has a set of observed Tweets {tji} Ni j=1 with a total of Ni Tweets and a demographic variable vector yi. Using such a dataset, our goal is to learn a model f such that for an unseen geographic unit g with a set of Tweets {tjg} Ng j=1, f({tjg} Ng j=1) = y\u0302g, where y\u0302g is an estimate of the true unknown demographics count yg. Mathematical notation is defined in table 1."}, {"heading": "4 Method", "text": "In this section, we describe our approach to learn the model f . Our approach relies on computing a feature vector xi \u2208 RD given {tji} Ni j=1, for each geographic unit i \u2208 [1, n]. Then, we fit a model to predict y\u0302i given xi. Below, we discuss possible feature engineering and modeling choices."}, {"heading": "4.1 Feature Engineering", "text": "In accordance with prior work, we focus on features that are based on lexical content. This is motivated by exploring the most predictive linguistic patterns of demographics. First we discuss possible lexical features, then we discuss possible normalization and transformation schemes that can be applied to these features.\nFeatures There are several possible lexical features that can be used to represent geographic units. These include but are not limited to: lexicons, latent topics, words and phrases (bag-of-words), and embeddings. These features are not mutually exclusive and can be combined together.\nLexicons are predefined word-to-category mappings that can be used to represent each geographic unit by the frequency of each category (Schwartz et al. 2013b; Culotta 2014a). Lexicons usually have stronger domain assumptions (compared to bag-of-words) (OConnor, Bamman, and Smith 2011) and are limited to specific applications such as health and personality (Culotta 2014a). So, we do not explore using lexicons as features.\nEach geographic unit can also be represented by its distribution over a set of latent topics, most commonly learned using latent Dirichlet allocation (Blei, Ng, and Jordan 2003). Schwartz et al. (2013b) report that this is better than using lexicons for their task of predicting well-being. Benton et al. (2016) explored variants of topic models that are guided by supervision to generate feature representations. Interestingly, they found that the bag-of-words representation is competitive with the best supervised topic models.\nWe explored using embeddings as features by learning representations of geographic units using Paragraph Vector (Le and Mikolov 2014). This is a similar technique to Word2Vec (Mikolov and Dean 2013), but rather than learning representations of words, it learns representations of paragraphs or documents. We model each gi as a document consisting of sentences which are the Tweets {tji} Ni j=1. We found that bag-of-words is competitive with this approach. Bag-of-words uses words and/or phrases as features instead of using categories or topics. We use this representation because it is simpler and has weaker domain assumptions (OConnor, Bamman, and Smith 2011).\nNormalizations We discuss different ways of counting and normalizing occurrences of words. Our discussion is based on using a bag-of-words representation but these techniques can also be applied to lexicons. We start with computing raw counts of tweeted words, cw,i and uw,i, for each word w and region gi where i \u2208 [1, n]. cw,i is the number of times a word w is tweeted in region gi. It is oblivious to the number of Twitter users that usedw. So, it is possible for the feature vector to be skewed by Twitter users that use a word w many times, either in one or many Tweets. To account for that, uw,i counts the number of distinct Twitter users that used w in gi. Since the distribution of geotagged Tweets and geotag Twitter users is not uniform across regions (Malik et al. 2015), using raw counts such as cw,i and uw,i as feature values will result in highly imbalanced feature vectors for geographic units. To balance these differences between geo-\ngraphic units, we normalize cw,i by dividing by the number of total tweeted words in region gi, Ci; and uw,i by dividing by the number of Twitter users that tweeted at least once in region gi, Ui. Normalizations help differentiate between geographic units as they take into account the size of total observations.\nUsing these mutually exclusive schemes, we compute feature values vi,w for each geographic unit gi and each wordw in the set of tweeted words in gi,Wi (forw /\u2208Wi, vi,w = 0): \u2022 Raw Word: vi,w = cw,i. \u2022 Normalized Word: vi,w = cw,iCi . \u2022 Raw User: vi,w = uw,i. \u2022 Normalized User: vi,w = uw,iUi\nIn our experiments our feature set includes only words that appear in the training split. This is to ensure that we account for the effect of out-of-vocabulary words.\nTransformations After computing a bag-of-words representation using Raw Word, Raw User, Normalized Word, or Normalized User, we perform feature transformations on the representation. We explore different transformations: Term Frequency-Inverse Document Frequency (TFIDF), Anscombe, Logistic, and Gaussian. Not every transformation is applied to every representation, for example, TFIDF is applied to Raw Word and Raw User, and the rest are applied to Normalized Word and Normalized User.\nIf we have a feature vector vi for a geographic unit gi (computed using Raw Word, Raw User, Normalized Word, or Normalized User), we transform it to xi by applying an element-wise transformation on each vi,w. We apply the transformation only on vi,w 6= 0 to preserve the sparsity of our feature vectors.\nTFIDF: The word distribution across all geographic units has a long-tail shape, with few words appearing in all geographic units and less-frequent words appearing in few geographic units. The motivation behind using TFIDF is to help the model take that into account, and re-weight the word counts inversely. Each geographic unit represents a document, we learn the inverse document frequency in the following manner:\nidf(w) = log n\n1 + |{i \u2208 [1, n] : w \u2208Wi}| We use this transformation with Raw Word or Raw User, so vi,w = cw,i or vi,w = uw,i. Then, xi,w = vi,w(idf(w) + 1). Note that we add a 1 to idf(w) because we do not want to completely ignore words that appear in all geographic units. In our experiments, we learn idf(\u00b7) based only on the training split.\nAnscombe: We applied this transformation to stabilize the variance of word frequencies. The distribution of a word may be right-skewed (i.e. appears a lot in a few geographic units and appears little elsewhere), the Anscombe transform helps adjust this skewness and make the distribution roughly symmetric. It helps turn a random variable distribution to be more Gaussian (Anscombe 1948). Schwartz et al. (2013a) applied this transformation in predicting select demographics of Facebook users. We use this transformation with\nNormalized Word or Normalized User, so vi,w = cw,i Ci or vi,w = uw,i Ui . Then, xi,w = 2 \u221a vi,w + 3 8 .\nLogistic and Gaussian: Inspired by the use of activation functions in neural networks, we explored applying a non-linear activation function \u03c6(\u00b7) on our word frequencies (computed with Normalized Word or Normalized User). Our intuition is that the non-linearity of \u03c6(\u00b7) would help increase the capacity of the model. We separately used \u03c6(x) = 11+e\u2212x (Logistic) and \u03c6(x) = e\n\u2212x2 (Gaussian). We set xi,w = \u03c6(vi,w), where vi,w =\ncw,i Ci or vi,w = uw,i Ui . We also explored other activation functions such as TanH, ArcTan, and Softsign but they did not show promising results."}, {"heading": "4.2 Modeling", "text": "We explore two variants of the problem: predicting demographics of geographic units when population size is unknown and when population size is known.\nPopulation Size is Unknown In this setting we would like to predict demographic counts (e.g. gender) yi,j of a region gi for each category (e.g. male and female) j \u2208 [1, k] without access to the population size of gi. We choose a linear regression model for scalability. For each category j \u2208 [1, k], we optimize the following objective function:\nwj = argmin wj\n1\n2n n\u2211 i=1 (wj \u00b7 xi \u2212 yi,j)2 + \u03bb||wj ||22\nwhere wj is the weight vector learned for category j, and \u03bb is an l2 regularization parameter to prevent overfitting. We also explored l1 and ElasticNet regularizations, but they yielded similar results.\nFor a region gu with an unknown demographic category count yu,j , we map the bag of Tweets {tju} Nu j=1 in region gu to a transformed feature vector xu (we use the same configuration that is used in optimizing the objective function, e.g. Raw Word with TFIDF) and then estimate yu,j , where \u02c6yu,j = wj \u00b7 xu.\nPopulation Size is Known In this setting we assume that we have access to the true population count pi in region gi. We fit a linear regression model to predict log yi,jyi,q which is log of the ratio of a demographic category count yi,j to another demographic category count yi,q . We choose one demographic category as the denominator (e.g. q = 1) and then learn wj for j \u2208 [2, k] by optimizing the following objective function:\nwj = argmin wj\n1\n2n n\u2211 i=1 (wj \u00b7 xi \u2212 log yi,j yi,q )2 + \u03bb||wj ||22\nTo estimate a demographic category count yi,j for region gi using pi, we compute:\n\u02c6yi,j =  1 1+ \u2211k m=2 e wm\u00b7xi pi j = q ewj \u00b7xi\n1+ \u2211k\nm=2 e wm\u00b7xi\npi j 6= q ,"}, {"heading": "5 Experiments", "text": "We evaluate our approach and competing approaches (baselines) on both variants of the problem using Census predefined geographies at different resolutions: block, block group, tract, and county. Among the four, block-level is the highest resolution, and county-level is the lowest resolution. In the following subsections we provide details about our experiments: baselines, data, preprocessing, training, and results."}, {"heading": "5.1 Baselines", "text": "We compare our approach with an approach adapted from (Mohammady and Culotta 2014), where they used Tweets to predict race/ethnicity composition of counties. In their approach, they used a bag-of-words representation normalized by Twitter users with tweeted words and words from description fields of Twitter users as features. We adapt their approach by using the same types of features. We compute a bag-of-words representation using User Normalization, and then we train a model with this representation and evaluate it on both variants of the problem. Note that in our competing configurations we do not use features from the description field of Twitter users.\nIn the setting where population size is known, we also compare our models with a baseline that always uses gender and race/ethnicity proportions at the national level to predict category counts of blocks, blockgroups, tracts, and counties. The 2015 national level estimates of proportions are: Male (49.2%), Female (50.8%), White (61.6%), Black or African American (12.4%), Asian (5.4%), Hispanic or Latino (17.6%), Other (3%) (U.S. Census Bureau 2016)."}, {"heading": "5.2 Data", "text": "We collected a large dataset of geotagged Tweets using Twitter\u2019s Streaming API from June 12, 2013 to January 31, 2014. We only included Tweets composed in the contiguous U.S. which consists of the 48 adjoining states and Washington D.C. and does not include Alaska and Hawaii for example. We used a bounding box of [125.0011, 66.9326]W \u00d7 [24.9493, 49.5904]N .\nBased on a Tweet\u2019s GPS coordinates, we annotate it with the geographic identifier (GEOID) of the block that it appeared in. The U.S. Census Bureau provides geographic boundary files (shapefiles) for each state, where each shapefile contains the boundary definitions for all the blocks in that state. This enables us to map each Tweet to its respective block. We used 2010 shapefile definitions to match with 2010 Census demographics data. Overall, we had 565,350,007 Tweets annotated with block-level GEOIDs.\nDemographics Data We used data from the 2010 Census. The U.S. Census Bureau provides aggregate count data on different demographics such as gender, age, and race at multiple geographic levels (including block-level to county-level). We specifically used data from the Summary File 1 tables P12 and P5, for gender and race/ethnicity, respectively. We used the Data Finder tool provided by the National Historical Geographic Information System (Minnesota Population Center 2011) to collect this data. We\ncollected gender and race/ethnicity data at the block-level up to the county-level. For gender we used two categories: Male and Female. For race/ethnicity we used five categories: White, African American or Black, Asian, Hispanic, and Other. In each case, the categories are mutually exclusive. Note that there is a time difference of two and a half years between the demographics data and Twitter data. This can bias results and is a direction for future work."}, {"heading": "5.3 Preprocessing", "text": "Twitter is filled with spam and organizational accounts that post content we deem irrelevant to our application, as we are interested in content produced by personal accounts. To reduce the likelihood of including content from organizational/spam accounts, we removed Tweets from accounts with more than 1000 followers or 1000 friends (Lee, Eoff, and Caverlee 2011; McCorriston, Jurgens, and Ruths 2015) and Tweets containing URLs (Guo and Chen 2014). We also removed Retweets by checking for the existence of retweeted status field or the RT token in Tweet text itself. Consequently, our dataset got narrowed down to 423,622,202 Tweets with 4,027,594 unique Twitter users.\nTo build a bag-of-words representation, we split the text of the Tweets into unigram tokens. There are several things to consider when tokenizing Tweets such as: hashtags, username mentions, emails, html entities, emoticons, etc. For this task, we used Twokenize (Ott 2013), a tokenizer designed for Tweets which treats hashtags, emoticons, blocks of punctuation marks, and other symbols as tokens. After tokenizing, we removed username mentions, emails, single punctuation marks, and English stopwords. We converted all tokens to lower case. We also chose to keep emoticons as OConnor et al. (2010) showed in their analysis that groups with certain demographics (high percentage of Hispanics) use emoticons a lot."}, {"heading": "5.4 Training", "text": "For a given geographic resolution, we randomly split the geographic units that have Tweets into 90% training and 10% testing (e.g. we train on 90% of blocks, and predict demographics of the remaining 10%). 10% of the geographic units in the training split were chosen randomly as a validation set. Note that this splitting is done separately for each geographic resolution. We have ntrain - ntest examples: 5,188,608 - 576,513 (block); 194,610 - 21,624 (blockgroup); 65,239 - 7,249 (tract); 2,798 - 311 (county). Then we compute a configuration (e.g. Raw User with TFIDF) and use all the words that appear in the training split as features (more than 22 million features). Since this is a large-scale learning problem we utilize stochastic gradient descent (SGD) to fit our models. To use SGD, we have to choose a learning rate update policy. We used inverse scaling:\n\u03b7\u03c4 = \u03b70 \u03c4\u03c1\nWhere \u03b70 is the initial learning rate, \u03c4 is the time step (indexed by epoch and training example), and \u03c1 is a hyperparameter that affects the decrease rate of the learning rate. There are several hyper-parameters that need to be selected\nbefore training. We performed a grid search on the following hyper-parameters: \u2022 \u03bb \u2208 {10\u22126, 10\u22125, 10\u22124, 0.001, 0.01, 0.1} \u2022 \u03b70 \u2208 {10\u22126, 10\u22125, 10\u22124, 0.001, 0.01, 0.1, 1.0, 10.0}\nThe hyper-parameter combination that scored the highest R2 (coefficient of determination) score on the validation set was used for training on the entire training split (training and validation). We shuffled the training dataset after each epoch of SGD training. We fixed the number of epochs to 10 and \u03c1 = 0.25. Early experiments showed that training for more epochs (e.g. 100) does not improve the performance significantly, and likewise changing the value of \u03c1."}, {"heading": "5.5 Results", "text": "We evaluate the baselines and our models (with different feature engineering configurations) on both variants of the problem (population count is unknown vs. known). In both variants we predict demographic category counts (yj for j \u2208 [1, k]) and evaluate the performance on the test split using Pearson correlation r and the coefficient of determination R2. Note that R2 compares the performance of a model relative to the baseline of always predicting the average value of the test set. Table 2 summarizes the results of our experiments (due to limited space we include only results of our best configurations). For Pearson correlations, all of them are statistically significant using a two-tailed test with p-value < 10\u22124.\nPopulation Size is Unknown In this setting, we find that our models outperform the baseline adapted from (Mohammady and Culotta 2014), where correlation r (averaged across gender and race) improves by a factor of 2.84 (block), 1.41 (blockgroup), 1.31 (tract), and 1.21 (county) using our User Normalization with Gaussian configuration.\nWe find that our feature transformations (e.g. Anscombe) improve upon the results of plain bag-of-words representations significantly. Compared with plain User Normalization (which is better than plain Word Normalization), correlation r improves by a factor of 1.88 averaged across gender and race and then across resolutions using User Normalization with Gaussian. This tells us that such transformations help predict demographics better.\nPopulation Size is Known In this setting, we find that predictions improve overall and are better than predictions in the other variant (this is expected because we know total population size). For the task of predicting gender counts (in this case q corresponds to female), the baselines and our models perform comparably to each other. This is the case because there is little variation in gender proportions across geographies, so if total population is known, even a baseline that predicts half of that for both categories will do well.\nFor the task of predicting race counts (in this case q corresponds to white), we find that our models outperform the baseline of predicting counts based on national-level proportions. Correlation r improves by a factor of 1.15 (block), 2.05 (blockgroup), 2.42 (tract), and 1.12 (county) using our Raw User with TFIDF configuration. Compared to the baseline adapted from (Mohammady and Culotta 2014), corre-\nlation r improves by a factor of 1.01 (block), 1.07 (block group), 1.11 (tract), and 1.05 (county).\nWe have shown that when we are interested in learning proportions of demographics, our approach outperforms existing baselines. Interestingly, we find that our best configuration is Raw User with TFIDF (which is not the case with the other variant). This may in part be due to the fact that User Normalization reduces the skewness of feature vectors and the fact that the TFIDF transformation increases the importance of less-frequent words and dampens the importance of more frequent words.\nPrediction Accuracy vs. Number of Twitter Users We explore the finest geographic resolution that we can predict\ndemographics at, with reasonable accuracy. We plot the average relative error across gender and race versus number of Twitter users (those with geotagged Tweets) in Figure 1. We find that 95% of geographic regions with at least 100 Twitter users, have low relative errors. In these regions, the relative error is at most 1.98 (block), 1.15 (blockgroup), 0.90 (tract), and 0.78 (county)."}, {"heading": "6 Conclusion", "text": "In this paper, we have shown that geotagged Tweets can be used to estimate demographics of high-resolution geographies. Our method can be used as an alternative or a complement to survey methods. We have shown that certain feature transformations such as Anscombe, TFIDF, Logistic, and Gaussian significantly improve prediction performance relative to competing baselines. We have also shown that the our method is able to learn proportions of demographic categories and can provide accurate predictions at regions with at least 100 Twitter users.\nFor future work, it is worth bringing attention to the effect of data sampling rate on prediction. According to Eisenstein et al. (2014), word frequencies normalized by users (Normalized User) are not invariant to the sampling rate of the data. If we remove half the tweets, then these frequencies will decrease because the number of users will decrease more slowly than raw word counts. So, it would be interesting to investigate methods that can minimize the variance of such normalizations to the sampling rate."}, {"heading": "7 Acknowledgments", "text": "This work was supported by NSF award #1054389. We also thank Guangqing Chi for providing the Twitter data."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "In this paper, we consider the problem of predicting demographics of geographic units given geotagged Tweets that are composed within these units. Traditional survey methods that offer demographics estimates are usually limited in terms of geographic resolution, geographic boundaries, and time intervals. Thus, it would be highly useful to develop computational methods that can complement traditional survey methods by offering demographics estimates at finer geographic resolutions, with flexible geographic boundaries (i.e. not confined to administrative boundaries), and at different time intervals. While prior work has focused on predicting demographics and health statistics at relatively coarse geographic resolutions such as the county-level or state-level, we introduce an approach to predict demographics at finer geographic resolutions such as the blockgroup-level. For the task of predicting gender and race/ethnicity counts at the blockgrouplevel, an approach adapted from prior work to our problem achieves an average correlation of 0.389 (gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms this prior approach with an average correlation of 0.671 (gender) and 0.692 (race).", "creator": "LaTeX with hyperref package"}}}