{"id": "1701.08796", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2017", "title": "Learning from various labeling strategies for suicide-related messages on social media: An experimental study", "abstract": "Suicide is an important but often misunderstood problem, one that researchers are now seeking to better understand through social media. Due in large part to the fuzzy nature of what constitutes suicidal risks, most supervised approaches for learning to automatically detect suicide-related activity in social media require a great deal of human labor to train. However, humans themselves have diverse or conflicting views on what constitutes suicidal thoughts. So how to obtain reliable gold standard labels is fundamentally challenging and, we hypothesize, depends largely on what is asked of the annotators and what slice of the data they label. We conducted multiple rounds of data labeling and collected annotations from crowdsourcing workers and domain experts. We aggregated the resulting labels in various ways to train a series of supervised models. Our preliminary evaluations show that using unanimously agreed labels from multiple annotators is helpful to achieve robust machine models.", "histories": [["v1", "Mon, 30 Jan 2017 19:41:04 GMT  (1192kb,D)", "http://arxiv.org/abs/1701.08796v1", "8 pages, 4 figures, 7 tables"]], "COMMENTS": "8 pages, 4 figures, 7 tables", "reviews": [], "SUBJECTS": "cs.LG cs.CY cs.SI", "authors": ["tong liu", "qijin cheng", "christopher m homan", "vincent m b silenzio"], "accepted": false, "id": "1701.08796"}, "pdf": {"name": "1701.08796.pdf", "metadata": {"source": "CRF", "title": "Learning from various labeling strategies for suicide-related messages on social media: An experimental study", "authors": ["Tong Liu", "Qijin Cheng", "Christopher M. Homan", "Vincent M.B. Silenzio"], "emails": ["tl8313@rit.edu", "chengqj@connect.hku.hk", "cmh@cs.rit.edu", "silenzio@urmc."], "sections": [{"heading": null, "text": "CCS Concepts \u2022Human-centered computing \u2192 Collaborative and social computing; \u2022Computing methodologies\u2192Machine learning;\nACM ISBN X-XXXXX-XX-X/XX/XX.\nDOI: xx.xxxx/xxxx\nKeywords Social media; suicide prevention; humans-in-the-loop; crowdsourcing"}, {"heading": "1. INTRODUCTION", "text": "Social media provides a public lens into the daily lives and personal emotions of its users. Since people sometimes post about suicidal or self-harm-related thoughts, social media such as Facebook have established suicide reporting and prevention mechanisms via specific links and buttons to let users report when they or their friends encounter direct life threats. Though this is a big step in the right direction, it alone does little to advance our scientific understanding of suicide, or how to predict its likelihood.\nResearchers are increasingly using machine learning techniques to observe and study a wide range of mental health problems from social media activities. Most existing approaches rely heavily on data that are increasingly annotated by crowdsourced workers. Obtaining labels for issues this way for topics as subjective as mental health is challenging, as both crowdsourcing workers and domain experts often have different interpretations of the texts they read and label. Thus having multiple annotators look at and annotation each data item is absolutely.\nWhen aggregating these multiple annotations into intelligent models for prevention purposes, how the training corpora is constructed\u2014more specifically, what final ground truth labels researchers determine to apply into the supervised algorithms\u2014will make significant differences to the final performance of output models. But to our best knowledge, there are few studies that have investigated how this kind of social media data were annotated and used differently in supervised learning settings.\nIn this paper, we study approaches for annotation and la-\nar X\niv :1\n70 1.\n08 79\n6v 1\n[ cs\n.L G\nbel aggregation to train a classifier to detect suicide-related tweets gathered from Twitter using keyword- and locationbased queries. Crowdsourced workers perform the first round of annotations, and then experts (the authors) annotated those tweets on which the crowdsourced workers disagreed. Our underlying assumption is that crowdsourced annotations with high inter-annotator agreement can provide us reliable usable corpora in supervised modeling of automatic classifiers. We experimented with different ways for aggregating multiple annotations into gold-standard labels and compared the effects of each. Our main contributions are:\n\u2022 We perform multiple rounds of annotations, involving both crowdsourcing workers and experts, to provide a diverse set of judgements about suicide-related discourse on social media data.\n\u2022 We propose a variety of strategies for selecting the data to annotate and for determining ground truth labels in the succeeding training phase.\n\u2022 We conduct experiments to compare\u2014in terms of how each method affects machine learning performance\u2014 sets of training data labeled by different method.\n\u2022 We demonstrate that a labeling method based only on unanimously labeled data from both crowdsourced workers and experts is the best way to train supervised models, especially when the problem domain is subjective."}, {"heading": "2. RELATED WORK", "text": "Kumar et al. [4] presented their research on the prevalence of Werther effect (copycat suicides following a celebrity\u2019s suicide) in social media. They examined posting activities and contents after public figures\u2019 suicide, and observed a virtual analog of sorts of the Werther effect: that people post more frequently with expressions of suicidal tendencies, and the linguistic measures change towards negative and biased direction.\n\u201cThe Durkheim Project\u201d [8] further analyzed an opt-in database of veterans\u2018 social media and mobile phone data to seek real-time assessments and predictive analytics for psychological suicide risk factors. They developed text-based prediction models from single keywords and multi-word phrases, achieved about 65% accuracy in identifying statistically significant signals of suicidality, and suggested the usefulness of computerized textual analytics of social media data to estimate the risk of suicide. But the group did not give details about the data \u2013 how they determined the labels for training data to build the system.\nIn [2], Burnap et al. focused on building a multi-class machine classifier with competitive accuracy when assigning tweets to particular class of suicidal communications and provided more details about their experiments. They requested at least four annotations per tweet from a crowdsourcing platform in order to limit the amount of subjectivity in the process of labeling suicidal tweets and kept tweets with high agreement (> 75% \u2013 at least three out of four annotators agreed on the dominant class of each tweet) to train classification models. Their empirical findings suggest that it is feasible for crowdsouring workers who are unknown to each other and without being influenced by each other\u2018s judgement to reach agreement on the disclosure of\nsuicidal ideation. Also based on the experiments and conclusions from [10], non-expert contributors could produce comparable quality of annotations when evaluating against those gold standard annotations from experts. And it is similarly effective to use the labeled tweets with high interannotator agreement among multiple non-expert annotators from crowdsourcing platforms to build robust models as doing so on expert-labeled data."}, {"heading": "3. DATA", "text": ""}, {"heading": "3.1 Twitter Sampled Data \u2013 Source 1", "text": "Inspired by [4], we searched for historical Twitter posts worldwide that were related to Robin Williams\u2019s suicide case and relevant information about suicide preventions using seven keywords and phrases suggested by suicide prevention experts and social workers, which are\u201cRobin Williams\u201d, \u201csuicide\u201d, \u201cdepression\u201d, \u201cParkinson\u2019s disease\u201d, \u201cseek help\u201d, \u201csuicide lifeline\u201d, and \u201ccrisis hotline\u201d. We downloaded ten percent of Twitter messages that covered the scope of six months before and after Robin Williams\u2019 death (August 11th, 2014) and contained at least one of the above terms via DataSift API1. This random sampling yielded approximately 1.7 million unique tweets in English from public accounts all over the world."}, {"heading": "3.2 Twitter Regional Data \u2013 Source 2", "text": "We took, as a representative sample of typical Twitter use, historical Twitter data from three metropolitan centers in the United States that cover a range of population densities. Most of the tweets in this set are in English."}, {"heading": "4. ANNOTATIONS", "text": ""}, {"heading": "4.1 Annotation Task Design", "text": "We examined and borrowed a series of pattern matching rules from [2] to generate many Twitter posts which are possibly related to suicide ideation or suicidal thoughts. This initial rule-based filter acts as our first classification model (C0) that extracts suicide-related posts. C0 searches for a wide range of expressions which include: suicidal / depression / cutting / bad / sad / these ... thoughts / feelings, want / wanted / wanting to die, end / ending it all, end my life, can\u2019t take (it) anymore, can\u2019t / don\u2019t want to live any more, don\u2019t want to be alive, can\u2019t go on, call / ask for help, offer of help, stop bullying, kill / killing / hate myself, fuck / fucking, boyfriend / girlfriend, just ... like, talk / speak to someone / somebody, web / blog / health / advice, miss / missing you / her / him, took / taken (my / your / his / her) own life, hanged / hanging / overdose, etc.\nWe ran C0 on our pooled dataset and randomly selected 2,000 matched tweets (1,200 tweets from source 1 and 800 from source 2 ) for manual annotations and validations. In particular, we anonymized the data to minimize the disclosure of personal information (@names) or URLs that may reveal cues about users\u2019 online identities."}, {"heading": "4.2 Round1: Crowdsourced Annotations", "text": "We first published this combination of 2,000 tweets on\n1http://datasift.com/\nCrowdFlower2, five tweets per page, to invite workers to finish the labeling tasks as instructed. For each tweet, five annotators were paid fairly to choose only one label to best describe the category from four given choices (with one sentence in following parentheses to provide more descriptions):\n\u2022 A. Suicidal thoughts (The author or the author\u2019s friend is at risk of suicide/distress.)\n\u2022 B. Supportive messages or helpful information (The author is providing supportive messages/helpful information related to suicide/distress.)\n\u2022 C. Reaction to suicide news/movie/music (The author is spreading/reacting/commenting to suicide news/ movie/music.)\n\u2022 D. Other (The author is using suicide/distress words to describe something else.)\nThe rationale behind the design of multiple choice questions is: our data collection method (source 1 especially) inevitably introduced tweets covering topics such as category B or C among four choices, which are not necessarily our focus on the personal suicidal disclosures detection in this case study. At the same time, this setting is useful to manually reduce the complexities of automatic classification: Annotators intuitively differentiate the contents so that form some explicit boundaries between the target class (suicidal) and noises before passing data into the supervised learning algorithms in the following classifier modeling phase.\n4.2.1 System Aggregated Labels (R1S) CrowdFlower by default automatically aggregated five re-\nsponses into a summarized result for each tweet based on the majority vote of the trusted workers.\n4.2.2 Unanimous Voted Labels (R1U) There are 415 tweets labeled with unanimous agreement\namong five workers, i.e. five workers gave the same label to one tweet. The remaining 1,585 tweets were not labeled unanimously which have lower inter-annotator agreement.\n4.2.3 Observation The percentage of tweets with unanimous labels in this an-\nnotation round (20.75%) is much smaller than that of some published experiments using the similar annotation strategy but for different social issues: Liu et al. asked crowdsourcing workers to differentiate job-themed tweets from the rest topics in [5], and harvested more than half of their published tweets with full agreement among five annotators per tweet (64.85%). This observation and comparison intuitively suggests that non-expert crowdsourcing workers working on suicide-related annotation tasks have diverse types of understanding about this topic because of its sensitivity and ambiguity. We examined this part of data which were interpreted differently among five annotators per message in the subsequent section.\n2https://www.crowdflower.com/: This is an Amazon Mechanical Turk type crowdsourcing platform. Its software as a service platform allows requesters to access online workforce to clean, label and enrich data."}, {"heading": "4.3 Round2: Expert Annotations", "text": "Experts were introduced into this phase to actively inspect what kinds of tweets that cause the divergent opinions from crowdsourcing workers. The 1,585 tweets with diversified labels from Round1 were then published internally to two experts to have them labeled twice. The tweets with unanimous labels from crowdsourcing workers were not reannotated by experts because unanimous votes are hypothesized to be reliable as expert\u2019 labels.\n4.3.1 Determining the Labels We assigned the identical labels from the two experts to\nthe gold standard label for each tweet (R2U). We compromised for those tweets annotated differently by two experts by adopting the system aggregated labels from Round1 crowdsourced annotations (R2S)."}, {"heading": "4.4 Annotations Summary", "text": "Table 1 records the percentages of tweets in four categories (A, B, C, and D) collected in different sources of annotations, with five distinct strategies to determine the final ground truth label for each tweet.\nFor the tweets with gold standard unanimous labels from experts (R2U in Table 1), we compared them to the labels aggregated from the crowdsourced annotations (R1S) and found a total of 871 common tweets having the same annotation results, which is approximately 83.59% of the number of tweets with unanimous agreement between two experts. Among them, 47 tweets belong to\u201cA. suicidal thoughts\u201d, 130 \u201cB. supportive messages or helpful information\u201d, 270 \u201cC. reaction to suicide news/movie/music\u201dand 424\u201cD. other\u201d (see the comparisons between R1S and R2U in Figure 1).\nIn Figure 2 we further compared the annotations between two experts in Round2. We assessed their inter-annotator agreement on multiple choice labeling tasks using Cohen\u2019s kappa [3] as \u03ba = 0.523.\nIn Table 2, we presented several samples from two rounds of annotations with their labels contributed by crowdsourcing workers (R1) and experts (R2).\nThis table exemplifies the complexity of labeling suiciderelated tweets among multiple annotators. For example, tweets like \u201cAccording to a British law passed in 1845, attempting to commit suicide was a capital offence. Offenders could be hanged for trying.\u201d declares that suicide was criminalized in the past. It might reinforce the stigma of suicide and make people unwilling to seek help timely, or\nA B C D R1S\nA\nB\nC\nD\nR 2 U\n47 2 1 6\n6 130 30 37\n9 10 270 37\n10 6 17 424 50\n100\n150\n200\n250\n300\n350\n400\nFigure 1: Comparisons between R1S and R2U .\ncan be understood as some way to raise the public awareness of suicide. Giving another example, some tweets are talking about what depression feels like without any explicit phrases about depression or suicide prevention. Some annotators classified such messages as \u201cSupportive messages or helpful information\u201d while they may actually worsen another person\u2019s existing negative feelings. Tweets mentioning suicide bombing/attack is an additional category about which annotator have diverse understandings. Such ambiguities make annotators, even experts, label such kind of tweets differently."}, {"heading": "5. MODELING EXPERIMENTS", "text": "To simplify the modeling process to identify tweets which express personal suicide ideation and suicidal thoughts and differentiate between this and other types of suicide-related messages, we grouped tweets with labels in category B, C and D into one class and formed the data points into binary categories: suicidal (positive) vs. others (negative).\nA B C D Expert1\nA\nB\nC\nD\nE x p e rt\n2\n56 33 10 4\n14 203 21 0\n4 67 326 11\n148 138 93 457\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\nFigure 2: Comparisons between two expert annotators in Round2.\nFive combinations of data in Table 1 were entered in our feature extraction and modeling pipeline to study the influence of different labeling strategies to classification modeling performances."}, {"heading": "5.1 Model", "text": "To control the environment variables of this experimental study, we selected support vector machines (SVMs) as our supervised learning methods to build a series of classification models. An SVM model takes in a set of training data, each labeled as belonging to one specific category, forms an optimal separating hyperplane to maximize the margin of input training data that are represented as data points in feature space. This algorithm outputs a discriminative classifier which is able to categorize new examples (provide a predicted class label) after mapping them into the same feature space. We used the scikit-learn implementation [7] of SVMs in the experiments."}, {"heading": "5.2 Feature Preparation", "text": "We relied on the textual representations (N-grams) to train and evaluate a series of SVM classifiers. Due to the noisy nature of Twitter, where people frequently write short, informal spellings and grammars, we pre-processed tweets as the following steps: (1) replaced personal information (@names) with @SOMEONE, and recognizable URLs with HTTP : //LINK, (2) utilized a revised Twokenizer system which was specially trained on Twitter texts [6] to tokenize raw messages, and (3) completed stemming and lemmatization using WordNet Lemmatizer [1].\nThe statistics of N-grams (unigrams, bigrams and trigrams) extracted from different sets of training data with mixed labeling strategies are summarized in Table 3. We used the top 10,000 unique N-grams as features in the modeling process of C1 to C5."}, {"heading": "5.3 Parameter Selection", "text": "Considering the class imbalance situations in each training dataset, we determined the optimal learning parameters by grid-searching on a range of class weights for the positive and negative classes, and then chose the set which optimized the area under the receiver operating characteristic curve (AUC)3."}, {"heading": "5.4 Performance Evaluation", "text": "5.4.1 K-fold Cross-Validation K-fold cross-validation relieves our straits that we do not\nhave too many labeled data in total: if further partitioning the available data into three sets\u2014training set, validation set and test set\u2014would drastically reduce the number of data used for learning purpose, and that the testing results could possibly depend on a particular random split for the pair of training-validation sets due to the potential over-fitting risks: the parameters can be tweaked until the estimator achieves the optimal performance on the validation set which ultimately causes that our final evaluation results could no longer reflect the model\u2019s generalizability on the unseen data. Though this evaluation approach is potentially computationally expensive, it does not waste too much data which is a major advantage in solving problem where the number of labeled samples is very small like our case. In our experiments, K is set to 10.\n5.4.2 Learning Curve A learning curve illustration shows the training scores and\ncross-validation scores of an estimator for varying numbers of training samples which helps us understand how much the benefits we could get by adding more training data. It is also a tool to understand whether the estimator suffers more from a bias error or a variance error during the modeling process4."}, {"heading": "6. RESULTS AND DISCUSSIONS", "text": "Having the core supervised learning algorithm and dimensions of features as constant variables, we conducted our experiments by training five SVM classifiers (C1 to C5) using 3We tested a variety of objective tuning functions during the grid-search process and concluded that AUC could achieve the best precision, recall and F1-score on the positive class. 4For an estimator, the bias error is its average error for different training sets. The variance reflects its sensitivity to varying numbers of training data.\nfive sets of training data described in Table 3. We analyzed their similarities and differences in the following subsections:"}, {"heading": "6.1 Learning Curve", "text": "Figure 3 shows the learning curves for models C1 to C5 during the training process with training data gradually added.\nWe observed several similarities from Figure 3: (1) For each model, the trending line of training scores (dashed lines with circles) and that of the cross-validation scores (solid lines with squares) did not converge to a value that is too low with increasing size of the training set; and (2) The differences between training scores and cross-validation scores for each model are continuously great with more training samples added gradually as the X axis marked. Even at the point of the maximum number of training samples used, the training score is greater than the cross-validation score. These observations suggest that we will benefit from adding more training samples to increase its generalization performance as well as reduce its bias error for each model.\nWe also noticed in Figure 3 that the cross-validation scores for five models reached different points on the Y axis when the maximum number of training data used, representing that they achieved different performances in the end. Among them, C4 reaches the highest cross-validation score. C4 also has the least variance errors according to our experiments, suggesting it has good stability over others. The cross-validation scores in C2 increase and converge to the training score very quickly using the least small number of training samples among five models, but the fluctuations of cross-validation scores during the C2 training process are significant, showing that its performance is not comparably stable."}, {"heading": "6.2 Outstanding Features", "text": "In Table 4 \u2013 8, we present the top 20 features for both positive and negative classes of C1 to C5 respectively according to their weights in creating the separating hyperplane to break the suicidal tweets away from the opposite class based on the labels from different determination strategies.\nComparing these features from C1 to C5, we observed that suicide, depression and feel appear in the suicidal class of each model, suggesting that suicidal ideation is closely related to these signal words. Myself, me, kill myself, my, im, my depression show that suspected suicidal users focus more on themselves with frequent use of first person singular pronouns. This series of self-oriented linguistic characters match the definition of suicide that it is an act of intentionally taking one\u2019s own life and cause one\u2019s own death, and are consistent with previous research findings [4, 9]. The primary root-word commit shows up in both classes of each model, but in distinct verb tenses: people in suicidal class write committing suicide, committing frequently which are in present tense and describing their ongoing actions, while commits, committed, commits suicide are used as common vocabularies (simple present tense) or past suicide tragedies (past tense). Thought and tried (to) are another set of commonly used words for users in suicidal class which implies that people expressed their thoughts or even attempts to end their own lives. Bomber, suicide bomber, robin williams, and williams appear as top features for non-suicidal tweets across a few datasets due to the fact that many tweets containing those phrases are reposting or commenting to suicide\nTable 3: Statistics of features from different sources of annotations, to train models C1 to C5.\nInput Data Output Model Unigrams (%) Bigrams (%) Trigrams (%) Total N-grams Count\nR1S C1 10.48 39.55 49.98 45,582 R1U C2 15.89 40.75 43.36 10,493 R2U C3 12.37 40.14 47.49 25,620 R1U + R2U C4 11.55 40.01 48.44 33,678 R1U + R2U + R2S C5 10.48 39.55 49.98 45,582\n0 200 400 600 800 1000 1200 1400 1600 1800 # of training examples\n0.6\n0.7\n0.8\n0.9\n1.0\nsc o re\nC1 T C1 CV\nC2 T C2 CV\nC3 T C3 CV\nC4 T C4 CV\nC5 T C5 CV\nFigure 3: Learning curves for models C1 to C5 during the training process. Dashed lines with circle markers represent training scores for each model, abbreviated as T in legend box. Solid lines with square markers represent cross-validation scores, noted as CV. C1: blue; C2: green; C3: cyan; C4: red; and C5: yellow.\nnews."}, {"heading": "6.3 Performance Evaluations", "text": "Figure 4 compares the five models according to seven performance metrics. We observed that C4 stands out in average precision5, precision, F1 score and ROC AUC comparisons. The performances of C3 are slightly lower than those of C4 in most measures. C2 has score 0 in precision, recall and F1 score due to its bad performance which the number of correctly classified positives is 0 \u2013 This result from C2 is trained with the least size of training data, with only 12 positive samples. C2 still has comparable performances in some measures and even surpasses scores of C4 in accuracy and F1 weighted score6. This results from the greater dispar-\n5This score corresponds to the area under the precisionrecall curve. 6This measure accounts for class imbalance issue. It calculates metrics for each class and finds their average, weighted by the number of true instances for each class.\nity between positive and negative class in R1U than that in other training data. C1 and C5 generally achieved lower performance scores than other three models which were trained using only the unanimously labeled results from annotators (crowdsourcing workers, experts or their combination), suggesting some anti-correlations between lower inter-annotator agreement labels (R1S and R2S) and the robustness of output models."}, {"heading": "7. CONCLUSIONS", "text": "We raised a very foundational research question about determining the ground truth label for social media data before proceeding to building supervised classifiers, especially when the topic is sensitive, subjective and ambiguous. We controlled the settings of experiments (supervised learning algorithm and dimensions of features), and altered the input training data obtained from different labeling sources (crowdsourcing workers and experts) and strategies (majority votes and unanimous votes). We presented preliminary\nfindings on the outcome of learning from multiple annotators on suicide-related annotation tasks. Our results show that it is helpful to use unanimous labels from crowdsourcing workers and experts as training data to build models. Though domain knowledge and experience are necessary in labeling the suicide-related data, our results provide some evidence that multiple crowdsourcing workers, when they reach high inter-annotator agreement, can provide reliable quality of annotations.\nThere are several interesting directions to future work. We did not investigate that how often the multiple workers unanimously agreed on the wrong outcome though it would be very uncommon. Examination of the annotated data without unanimous labels among multiple crowdsourcing workers will help understand where the disagreement is\nTable 6: Top 20 features for both classes of C3.\nSuicidal weights Others weights\nmyself 0.422 people -0.119 suicide 0.385 by -0.113\nme 0.342 with -0.103 kill 0.341 be -0.100 kill myself 0.292 one -0.085 my 0.230 after -0.083 day 0.228 please -0.078 depression 0.200 you -0.076 feel 0.168 we -0.074 so 0.156 their -0.067 my depression 0.153 of -0.067 alive 0.152 year -0.066\nrather 0.150 girl -0.065 im 0.148 need -0.061\ncommit 0.148 williams -0.061 tried to 0.146 committed -0.060 almost 0.136 today -0.058 tried 0.130 always -0.058 suicide is 0.129 he -0.058 cutting 0.128 get -0.057\nTable 7: Top 20 features for both classes of C4.\nSuicidal weights Others weights\nmyself 0.440 with -0.149 suicide 0.436 people -0.139\nme 0.339 by -0.111 kill 0.314 out -0.104 kill myself 0.300 after -0.098 depression 0.251 you -0.083\nday 0.233 get -0.077 tried 0.221 we -0.073 im 0.212 need -0.072\nalive 0.194 your -0.071 tried to 0.191 he -0.070\nfeel 0.187 be -0.068 my 0.185 girl -0.068 commit 0.185 boyfriend -0.068 last 0.168 bitch -0.066 so 0.167 always -0.065\nrather 0.165 soldier -0.065 my depression 0.161 are -0.061\nagain 0.156 need to -0.061 everything 0.148 williams -0.060\nand propose solutions to improve public awareness and understanding of suicide so that we could rely more on crowdsourcing platforms to reduce the overall annotation costs. We also plan to investigate to what extent we could reduce the reliability loss of the disagreement among multiple annotators on the final output supervised models. Finally, given our observation of common heavily-weighted features expressed in suicidal posts, we might develop more complicated language models to automatically detect suicide ideation which could be helpful to provide decision making support to psychologists and psychiatrists, and ultimately care and support those vulnerable communities.\n8. ACKNOWLEDGMENTS\nWe thank reviewers for their comments and suggestions. We are grateful to Cecilia Ovesdotter Alm, Rui Li, Ray Ptucha, Emily Prud\u2019hommeaux and Xuan Guo for helpful conversations. This work was supported by US National Institutes of Health grant R25TW010012-01 and Hong Kong General Research Fund GRF17628916."}, {"heading": "9. REFERENCES", "text": "[1] S. Bird, E. Klein, and E. Loper. Natural language\nprocessing with Python. O\u2019Reilly Media, Inc., 2009.\n[2] P. Burnap, W. Colombo, and J. Scourfield. Machine classification and analysis of suicide-related communication on twitter. In Proceedings of the 26th ACM Conference on Hypertext & Social Media, pages 75\u201384. ACM, 2015.\n[3] J. Cohen. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37\u201346, 1960.\n[4] M. Kumar, M. Dredze, G. Coppersmith, and M. De Choudhury. Detecting changes in suicide content manifested in social media following celebrity suicides. In Proceedings of the 26th ACM Conference on Hypertext & Social Media, pages 85\u201394. ACM, 2015.\n[5] T. Liu, C. M. Homan, C. O. Alm, A. M. White, M. C. Lytle, and H. A. Kautz. Understanding discourse on work and job-related well-being in public social media. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1044\u20131053, Berlin, Germany, August 2016. Association for Computational Linguistics.\n[6] O. Owoputi, B. O\u2019Connor, C. Dyer, K. Gimpel, N. Schneider, and N. A. Smith. Improved part-of-speech tagging for online conversational text with word clusters. Association for Computational Linguistics, 2013.\n[7] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. Journal of Machine Learning Research, 12(Oct):2825\u20132830, 2011.\n[8] C. Poulin, B. Shiner, P. Thompson, L. Vepstas, Y. Young-Xu, B. Goertzel, B. Watts, L. Flashman, and T. McAllister. Predicting the risk of suicide by analyzing the text of clinical notes. PLoS ONE, 9(1):1\u20137, 01 2014.\n[9] S. Rude, E.-M. Gortner, and J. Pennebaker. Language use of depressed and depression-vulnerable college students. Cognition & Emotion, 18(8):1121\u20131133, 2004.\n[10] R. Snow, B. O\u2019Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 254\u2013263. Association for Computational Linguistics, 2008."}], "references": [{"title": "Natural language processing with Python", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Machine classification and analysis of suicide-related communication on twitter", "author": ["P. Burnap", "W. Colombo", "J. Scourfield"], "venue": "In Proceedings of the 26th ACM Conference on Hypertext & Social Media,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A coefficient of agreement for nominal scales", "author": ["J. Cohen"], "venue": "Educational and psychological measurement,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1960}, {"title": "Detecting changes in suicide content manifested in social media following celebrity suicides", "author": ["M. Kumar", "M. Dredze", "G. Coppersmith", "M. De Choudhury"], "venue": "In Proceedings of the 26th ACM Conference on Hypertext & Social Media,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Understanding discourse on work and job-related well-being in public social media", "author": ["T. Liu", "C.M. Homan", "C.O. Alm", "A.M. White", "M.C. Lytle", "H.A. Kautz"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["O. Owoputi", "B. O\u2019Connor", "C. Dyer", "K. Gimpel", "N. Schneider", "N.A. Smith"], "venue": "Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Predicting the risk of suicide by analyzing the text of clinical notes", "author": ["C. Poulin", "B. Shiner", "P. Thompson", "L. Vepstas", "Y. Young-Xu", "B. Goertzel", "B. Watts", "L. Flashman", "T. McAllister"], "venue": "PLoS ONE, 9(1):1\u20137,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Language use of depressed and depression-vulnerable college students", "author": ["S. Rude", "E.-M. Gortner", "J. Pennebaker"], "venue": "Cognition & Emotion,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural language processing, pages 254\u2013263", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "[4] presented their research on the prevalence of Werther effect (copycat suicides following a celebrity\u2019s suicide) in social media.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "\u201cThe Durkheim Project\u201d [8] further analyzed an opt-in database of veterans\u2018 social media and mobile phone data to seek real-time assessments and predictive analytics for psychological suicide risk factors.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "In [2], Burnap et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 9, "context": "Also based on the experiments and conclusions from [10], non-expert contributors could produce comparable quality of annotations when evaluating against those gold standard annotations from experts.", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "Inspired by [4], we searched for historical Twitter posts worldwide that were related to Robin Williams\u2019s suicide case and relevant information about suicide preventions using seven keywords and phrases suggested by suicide prevention experts and social workers, which are\u201cRobin Williams\u201d, \u201csuicide\u201d, \u201cdepression\u201d, \u201cParkinson\u2019s disease\u201d, \u201cseek help\u201d, \u201csuicide lifeline\u201d, and \u201ccrisis hotline\u201d.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "We examined and borrowed a series of pattern matching rules from [2] to generate many Twitter posts which are possibly related to suicide ideation or suicidal thoughts.", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "asked crowdsourcing workers to differentiate job-themed tweets from the rest topics in [5], and harvested more than half of their published tweets with full agreement among five annotators per tweet (64.", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "We assessed their inter-annotator agreement on multiple choice labeling tasks using Cohen\u2019s kappa [3] as \u03ba = 0.", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "We used the scikit-learn implementation [7] of SVMs in the experiments.", "startOffset": 40, "endOffset": 43}, {"referenceID": 5, "context": "Due to the noisy nature of Twitter, where people frequently write short, informal spellings and grammars, we pre-processed tweets as the following steps: (1) replaced personal information (@names) with @SOMEONE, and recognizable URLs with HTTP : //LINK, (2) utilized a revised Twokenizer system which was specially trained on Twitter texts [6] to tokenize raw messages, and (3) completed stemming and lemmatization using WordNet Lemmatizer [1].", "startOffset": 340, "endOffset": 343}, {"referenceID": 0, "context": "Due to the noisy nature of Twitter, where people frequently write short, informal spellings and grammars, we pre-processed tweets as the following steps: (1) replaced personal information (@names) with @SOMEONE, and recognizable URLs with HTTP : //LINK, (2) utilized a revised Twokenizer system which was specially trained on Twitter texts [6] to tokenize raw messages, and (3) completed stemming and lemmatization using WordNet Lemmatizer [1].", "startOffset": 440, "endOffset": 443}, {"referenceID": 3, "context": "This series of self-oriented linguistic characters match the definition of suicide that it is an act of intentionally taking one\u2019s own life and cause one\u2019s own death, and are consistent with previous research findings [4, 9].", "startOffset": 218, "endOffset": 224}, {"referenceID": 8, "context": "This series of self-oriented linguistic characters match the definition of suicide that it is an act of intentionally taking one\u2019s own life and cause one\u2019s own death, and are consistent with previous research findings [4, 9].", "startOffset": 218, "endOffset": 224}], "year": 2017, "abstractText": "Suicide is an important but often misunderstood problem, one that researchers are now seeking to better understand through social media. Due in large part to the fuzzy nature of what constitutes suicidal risks, most supervised approaches for learning to automatically detect suicide-related activity in social media require a great deal of human labor to train. However, humans themselves have diverse or conflicting views on what constitutes suicidal thoughts. So how to obtain reliable gold standard labels is fundamentally challenging and, we hypothesize, depends largely on what is asked of the annotators and what slice of the data they label. We conducted multiple rounds of data labeling and collected annotations from crowdsourcing workers and domain experts. We aggregated the resulting labels in various ways to train a series of supervised models. Our preliminary evaluations show that using unanimously agreed labels from multiple annotators is helpful to achieve robust machine models.", "creator": "LaTeX with hyperref package"}}}