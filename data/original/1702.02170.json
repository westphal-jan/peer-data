{"id": "1702.02170", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Feb-2017", "title": "How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks", "abstract": "Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning).", "histories": [["v1", "Tue, 7 Feb 2017 19:21:50 GMT  (2049kb,D)", "http://arxiv.org/abs/1702.02170v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stanis{\\l}aw jastrzebski", "damian le\\'sniak", "wojciech marian czarnecki"], "accepted": false, "id": "1702.02170"}, "pdf": {"name": "1702.02170.pdf", "metadata": {"source": "CRF", "title": "How to evaluate word embeddings? On importance of data efficiency and simple supervised tasks", "authors": ["Stanis\u0142aw Jastrzebski", "Damian Le\u015bniak", "Wojciech Marian Czarnecki"], "emails": ["stanislaw.jastrzebski@uj.edu.pl"], "sections": [{"heading": null, "text": "Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning).\nIn order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non\u2013linearly encoded in the embedding space, which questions the cosine\u2013based, unsupervised, evaluation methods. All results and analysis scripts are available online."}, {"heading": "1 Introduction", "text": "Using word embeddings remains a standard practice in modern NLP systems, both in shallow and deep architectures [Goldberg, 2015]. By encoding information about words in a relatively simple algebraic structure [Arora et al., 2016] they enable\nfast transfer to the task of interest1. The importance of word representation learning has lead to developing multiple algorithms, but lack of principled evaluation hinders moving the field forward, which motivates developing more principled ways of evaluating word representations. Word embeddings are not only hard to evaluate, but also challenging to train. Recent practice shows that one often needs to tune algorithm, corpus and hyperparameters towards the target task [Lai et al., 2016, Sharp et al., 2016b], which challenges the promise of broad applicability of unsupervised pretraining.\nEvaluation methods of word embeddings can be roughly divided into two groups: extrinsic and intrinsic [Schnabel and Labutov, 2015]. In the former approach embeddings are used in a downstream task (eg. POS tagging), while in the latter embeddings are tested directly for preserving syntactic of semantic relations. The most popular intrinsic task is Word Similarity (WS) which evaluates how well dot product between two vectors reproduce score assigned by human annotators. Intrinsic evaluations always assume a very specific model for recovering given property.\nDespite popularity of word embeddings, there is no clear consensus what evaluation methods should be used, and both intrinsic and downstream evaluations are criticized [Tsvetkov et al., 2015a, Faruqui et al., 2016]. On top of that, different evaluation schemes usually lead to different rankings of embeddings [Schnabel and Labutov, 2015]. For instance, it has been shown [Baroni and Dinu, 2014] that neural-based word embeddings perform consistently better then count-based models and later, using WS and WA tasks, it was argued otherwise [Levy et al., 2015]. Recent research in evaluation methods focuses on representative or inter-\n1Algebraic structure refers to the fact that words can be decomposed into a overcomplete basis, such that each word can be expressed as a sparse sum of base vectors\nar X\niv :1\n70 2.\n02 17\n0v 1\n[ cs\n.C L\n] 7\nF eb\n2 01\n7\npretable set of tasks [Nayak et al., 2016, K\u00f6hn, 2015a], analysing intrinsic evaluation [Chiu et al., 2016, Faruqui et al., 2016], as well as proposing improvements to intrinsic evaluation [Avraham and Goldberg, 2016, Tsvetkov et al., 2015b].\nIn this paper we employ a transfer learning view, in which the main goal of representation learning is to make subsequent learning fast, i.e. use resulting word embeddings to maximize performance at the lowest sample complexity possible [Bengio et al., 2013, Glorot et al., 2011]2. Surprisingly, researchers rarely report model (using given word representation) performance under varying (benchmark) dataset sizes and model classes3, which is crucial for correct evaluation of transfer, especially given increasing importance of small data regime applications. Motivated by this, we propose an evaluation focused on data efficiency. To quantify precisely accessible information, we additionally propose focusing only on simple (supervised) tasks, as complex downstream tasks are challenging to interpret. In addition, we propose principled improvements to WS and WA tasks, which try to address some of the critiques both benchmarks have received in the literature [Faruqui et al., 2016], in authors\u2019 opinion mostly due to their purely unsupervised nature."}, {"heading": "2 Proposal", "text": "Our main goal is to better align evaluation of word embeddings with their transfer application. Future performance is correlated with the amount of easily accessible and useful information. By easily accessible information, we mean information that model can quickly learn to use. Useful information is defined as one that correlates well with the final task.\nFirst argument for data efficiency focused evaluation is the growing evidence that pretrained word embeddings provide little benefit under various settings, especially deep learning models [Zhang et al., 2015, Zhang and Wallace, 2015, Andreas and Klein]. We hypothesize that most of the improvements (in downstream tasks) reported in literature are caused by small size of the supervised dataset, which is reasonable from the trans-\n2Alternative goals might include maximizing interpretability, or analysing unsupervised corpora.\n3What is claimed here is that vast majority of papers doesn\u2019t take into consideration those factors. Nevertheless, there are notable exceptions [Andreas and Klein, Qu et al., 2015, Amir et al., 2017].\nfer learning point of view. Therefore, measuring performance after seeing just a subset of the supervised dataset is crucial for comparing word embeddings. Another argument is the empiricial difference between how easily accessible is the information in various embeddings. As our experiment show, commonly used dense representations achieve different learning speeds. This effect should be even stronger for sparse representations, for which feature dimensions can have very strict semantic meaning [Faruqui and Dyer, 2015a]. An argument can be also made from theoretical point of view; it is easy to show that any injective (and thus not losing information) embedding preserves all information about corpora (see Appendix for details), i.e. having enough training data makes embeddings dispensable.\nSecond part of the proposal is to focus on simple supervised tasks to directly evaluate useful information content. In certain applications, like tagging, choosing the right, specialized, word embeddings is crucial for obtaining state of the art results [Sharp et al., 2016a, Lample et al., 2016]. We also confirm empirically that word embeddings trade off capacity between different information. In this work we pose hypothesis, that specialization of word embeddings can be best evaluated by checking what simple information is most easily recoverable. While word level classification problems (like noun classification) were proposed previously [K\u00f6hn, 2015b], here we also suggest including tests for recovery of relations between words (exemplified in experiments by Similarity and Analogy tasks) .\nImportance of simple supervised tasks can be also seen in the light of algebraic structure that is encoded in word representation space. It has been observed in practice that word embeddings have useful information only in a small subspace [Sattigeri and Thiagarajan, 2016, Rothe and Sch\u00fctze, 2016, Astudillo et al., 2015]. Thus, simple supervised tasks are closely aligned with the actual use of word embeddings and allow to quantify how quickly model can extract the most salient subspace (which leads to faster learning in general).\nOur final remark is about diversity of models. Commonly used WS and WA datasets are solved by a constant models, i.e. model which does not learn from the data. We argue that such evaluation is not generally informative. If we are interested in how well our vector space helps solving a given\nproblem, we should in theory fit all possible models and pick the one that has the best generalization capabilities. While this is impractical, it illustrates that fixing one specific model gives answer to a different question, thus drawing general conclusions from it can be highly biased. A good rule of thumb might be to include representatives of typical model classes, or at least match the model with class of models we are interested in (which rarely will be constant), which concludes our guidelines for a correct evaluation.\nWe leave out details from the proposal how to order embeddings, as this is determined by the specific research question given evaluation should answer. A sensible default is to report AUC of learning curve for each task, and pick set of tasks that are most interesting to the researcher.\nTo summarize:\n\u2022 Evaluation should focus on data efficiency (if transfer is the main goal of representation learning).\n\u2022 Tasks should be supervised and simple.\n\u2022 Unless focus is on specific application, evaluation should focus on a diverse set of models (including nonlinear and linear ones) and datasets (testing for various information content).\nIf we follow those guidelines, we truly approximate (for a given trained embedding) generalization error under distribution of tasks, dataset size and classifiers,\nE [ Lt(cl(VU(Xtm),Y t m)(VU(x),y) ] , (1)\nVU denotes the embeddings trainedw on U, Lt denotes task t loss and expectation is taken with respect to:\n\u2022 p(cl) \u2013 distribution of classifiers,\n\u2022 p(Xtm,Ytm) \u2013 distribution of training datasets, where we first sample task t and then uniformly sample dataset Xtm of size m.\n\u2022 x,y \u2013 i.i.d. examples following training data distribution.\nDistribution over classifiers and tasks should be carefully tuned to researcher\u2019s needs, as we will argue soon. Further theoretical analysis is included in Appendix, and in the rest of the paper we present practical arguments for the proposed evaluation scheme."}, {"heading": "3 Experiments", "text": "In this section we define specific metrics and tasks and perform exemplary evaluation of several pretrained embeddings in the advocated setting. Specifically, we try to empirically answer several questions, all geared towards providing experimental validation for the three main points of proposal:\n\u2022 Do supervised versions of WA and WS benchmarks provide additional insights?\n\u2022 How stable is the ranking of embeddings under changing dataset size?\n\u2022 Are there embeddings that benefit from nonlinear models?\nThe first question will aid understanding how useful are simple supervised tasks coupled with data efficiency. Second question shows that ranking of embeddings do change under transfer learning evaluation. Last question explores if any embeddings encode information in a \u201cnon-linear\u201d fashion; while one of the main goals of representation learning is disentangling factors of variation, usually learned representations are entangled and dense, which poses interesting question how hard it is to extract useful patterns from them. In this paper we report only a subset of results with the most interesting conclusions, all results (along with code) are also made available online for further analysis."}, {"heading": "3.1 Datasets and models", "text": "Datasets are divided into 4 categories: Similarity, Analogy, Sentence and Single word. Analogy datasets are composed of quadruples (two pairs of words in a specific relation, for instance (king, queen, man, woman)). Similarity datasets are composed of pairs of words and assigned mean rank by human annotators. Sentence and Single word datasets have binary targets. In total our experimentation include 15 datasets:\n\u2022 Similarity: SimLex999 [Hill et al., 2015], MEN [Bruni et al., 2014], WordSimilarity353 [Finkelstein et al., 2001] and Rare Words [Luong et al., 2013].\n\u2022 Analogy: 4 categories from WordRep [Gao et al., 2014]4.\n\u2022 Sentence: Stanford Sentiment Treebank [Socher et al., 2013] and News20 (3 binary datasets) [Tsvetkov et al., 2015a].\n\u2022 Single word: Datasets constructed from lexicons collected in [Faruqui and Dyer, 2015b]: POS tagging (3 datasets for verb, noun and adjective), word sentiment (1 dataset), word color association (1 dataset) and WordNet synset membership (2 datasets).\nModels for each datasets include both nonlinear and linear variants. When model is non-\n4We experimented with MSR and Google datasets and observed that models easily overfit if the train and test sets share the same words (not 3-tuples). WordRep dataset is a set of pairs which we split into disjoint sets.\nlinear, for robustness we include in search a fallback to a simpler linear or constant model. Additionally, in the case of Similarity and Analogy we include commonly used constant models. Similarity between 2 vectors is approximated by their cosine similarity (cos(~v1, ~v2)). In the case of Analogy tasks embedding is evaluated for its ability to infer 4th word out from the first three and we use the following well-known constant models: 3COSADD (argmax~v\u2208V cos(~v, ~v2\u2212 ~v1+ ~v3)) and 3COSMUL (argmax~v\u2208V\nccos(~v, ~v3)ccos(~v, ~v2) ccos(~v, ~v1)+ )5. For each task class we evaluate a different set of classifiers:\n\u2022 Similarity: cosine similarity, Random Forest (RF), Support Vector Regression (SVR) with RBF kernel6.\n\u2022 Analogy: 3COSADD, 3COSMUL [Levy et al., 2015] and regression neural network (performing regression on the 4th word given the rest of the quadruple, see Appendix for further information).\n\u2022 Sentence: Logistic Regression, Support Vector Machine (SVM) with RBF kernel taking as input averaged embedding vector and Convolutional Neural Network (CNN) [Kim, 2014] taking as input concatenation of embedding vectors.\n\u2022 Single word: RF, SVM (with RBF kernel), Naive Bayes, k-Nearest Neighbor Classifier and Logistic Regression."}, {"heading": "3.2 Embeddings", "text": "Our objective was to cover representatives of embeddings emerging from both shallow and deeper architectures. Deep embeddings are harder to train, so for the scope of this paper we decided to include pretrained and publicly available vectors7. Setup includes following \u201cshallow\u201d pretrained embeddings: GloVe (100 and 300 dimensions) [Pennington et al., 2014], Hellinger PCA (HPCA) [Lebret and Collobert, 2014], PDC (100 and 300 dimensions) and HDC (300 dimensions) [Sun et al., 2015], Additionally following \u201cdeep\u201d embeddings are evaluated: Neural Translation Machine (NMT,\n5ccos(~v1, ~v2) = 1+cos( ~v1, ~v2) 2 6We also tried RankSVM [Lee and Lin, 2014], but it did not perform better than other models, while being very computationally intensive.\n7Vocabularies were lowercased and intersected before perfoming experiments. Vectors were normalized to a unit length.\nactivations of the deep model are extracted as word embeddings) [Hill et al., 2014], morphological embeddings (morph, which can learn morphological differences between words directly) [Luong et al., 2013] and HPCA variant trained using autoencoder architecture [Lebret and Collobert, 2015]. In some experiments we additionally include publicly available pretrained skip-gram embeddings on Google News corpora and skipn-gram embeddings trained on Wikipedia corpora [Ling et al., 2015] (used commonly in syntax demanding tasks, like tagging)."}, {"heading": "3.3 Results", "text": "For each dataset we first randomly select test set and run evaluation for increasing sizes of training dataset, thus scores approximate generalization error after seeing increasing amounts of data. Splits are repeated 6 times in total to reduce noise. Thus, for each task results are 6 learning curves, with a score for each subset of data (see Fig. 2).\nRanks of embeddings at each point are calculated using a greedy sequential procedure, where we assign embeddings the same rank if their scores (each point on the curve is represented by 6 scores) are not significantly different, as tested using pairwise ANOVA test. All results are available online8.\n3.4 Learnable Similarity and Analogy tasks\nOur first question was validating that adding learnable Similarity and Analogy tasks introduce any\n8Results will be posted online upon publication.\nadditional insights. Positive answer to this question motivates introduction of simple tasks with varying dataset size, ideally defined on single or pair of words.\nFor solving Analogy we implemented a shallow neural network. Interestingly, WordRep authors [Gao et al., 2014] reported low accuracies (often even below 5%) on most analogy questions and we were able to improve absolute score upon the tested subset on average by absolute 11% (see Tab. 2). Having learnable models for Similarity and Analogy datasets enables reusing many publicly available datasets in the new context. Also, we can robustly evaluate if given information about relation between two words is present in the embedding based on analogy questions. In the case of HASCONTEXT and INSTANCEOF datasets no embeddings can recover analogy answers using static models (achieved accuracy is below 3%), but actually some embeddings do have information about the relations. In both cases HDC consistently outperforms other embeddings reaching around 25% accuracy, see Fig. 4 and Tab. 2.\nIn the case of Similarity dataset the best performing model was Support Vector Regression, similarly as in Analogy datasets we also improve over the constant models. What is more, we can draw novel conclusions. Interesting example is NMT performance on SimLex. It was claimed in [Hill et al., 2014] that NMT embeddings are better at encoding true similarity between words, but SVR on Glove embeddings performs better after training on the whole dataset (i.e. at the end of learning curve), see Fig.2."}, {"heading": "3.5 Rank stability under changing dataset size", "text": "Second question was how stable are the orderings under growing dataset. To this end we have measured rank at the beginning (30% of data) and end of training. Mean absolute value of change of ranking is approximately (averaged over all categories) 0.6 with standard deviation of 0.2. This means that usually an embeddings has a changed rank after training, which establishes usefulness of measuring data efficiency for the tested embeddings.\nInterestingly, when averaged over many experiments, final ordering of embeddings tends to be similar, see Tab. 1 and Fig. 3. This is mainly because (tested) embeddings have different data efficiency properties for different tasks, i.e. none of embeddings is consistently more data efficient than others. On top of that standard deviation of both rank at the end and beginning is around 2.5, which further reinforces findings from [Schnabel and Labutov, 2015] that embeddings orderings are very task dependent.\nMeasuring data efficiency is crucial for a realistic (i.e. as close to application as possible) evaluation of representations. Besides a more accurate and practical ordering of embeddings, it also allows one to draw new conclusions, which is exemplified by differences between GloVe100 and GloVe300 (elaborated on in the next section). Another interesting point is that rank change after training on full dataset is relatively low for Single word datasets, which suggests that simple information about words like noun or verb is always quickly accessible to models, but more complicated information like relationships or similarities\nbetween pair of words are not, see Tab. 3."}, {"heading": "3.6 Linear vs non\u2013linear models", "text": "Our last question was how stable is the ordering under changing model type. More specifically, are there embeddings especially fitted for use with linear models? Clearly some embeddings in fact are, see Fig. 1. This is an important empirical fact for practictioners, which motivates including such evaluation in experiments. In particular, it clearly shows that typically used evaluation does not answer the question \u201cis there information about task X in the embedding Y\u201d but only \u201cis information about task X stored in embedding Y easily separable by a static (or linear) classifier\u201d.\nAn illustrative example is the difference in performance between two pretrained GloVe embeddings of different dimensionality (100 and 300). It has been shown previously that lower dimensional GloVe embeddings are better at syntactic tasks [Lai et al.], but our evaluation reveals more complicated picture, that GloVe objective might encourage some sort of nonlinear encoding. We can see that by significantly better rank of GloVe100 at the beginning of learning of Single word datasets (mean rank 1.8), but lower at the end (mean rank 2.3), see Tab. 3. This is also visible when averaged over all categories, see Tab. 1."}, {"heading": "3.7 Discussion", "text": "Performed eperiments show usefulness of the additional and more granular level of analysis enabled. Researcher can ask more precise questions, like \u201cis it worth fitting syntax specific embeddings even when supervised dataset size is large?\u201d (to which answer is positive based on our experiments) or \u201cis HASINSTANCE relation encoded in the space?\u201d (to which answer is also positive for some embeddings). Unfortunately, there is already a large volatility of final embeddings ordering when using standard evaluation, and our proposed scheme at times makes it even more challenging to decide which embeddings are optimal. This hints, that purely unsupervised large scale pretraining might not be suitable for NLP applications. Most importantly, evaluation should be more targeted, either at some specific application area, or at specific properties of representation.\nOne of the presented arguments for including supervised models for testing information content is algebraic interpretation of word embeddings [Arora et al., 2016]. The algebraic structure\npresent in the representation space enables one to decompose word embedding space into a set of concepts (so each word vector can be well approximated by a sum of few concepts)9. Theoretically, tasks defined on single words should test for existence of such concepts, but in our case including (supervised) Analogy tasks was very useful, as those tasks are still very challenging for current embeddings. For Analogy tasks (see Fig. 4) achieved accuracy scores are below 25%, whereas in the case of Single word average accuracy is around 80% (and fitting classifier adds on average only 2%). These higher order (or subspace) focused tasks are also well aligned with the application of word embeddings, because empirically models tend to focus on a small subspace in the vector space.\n9This decomposition can be obtained using standard methods like k-SVD."}, {"heading": "4 Conclusions", "text": "As exemplified by experiments, proposed evaluation reveals differences between embeddings along usually overlooked dimensions: data efficiency, non-linearity of downstream model and simple supervised tasks (including recovery of higher order relations between words). Interesting new conclusions can be reached, including differences between different size GloVe embeddings or performance of non-linear models on similarity benchmarks.\nAdditionally, obtained results reinforce conclusions from other published studies that there are no universally good embeddings and finding such might not be achievable, or a well posed problem. One should take great care when designing evaluation and specify what is the main focus. For instance, if the main goal of the word embeddings is to be useful in transfer, one should include advocated data efficiency metrics. New word embedding algorithms are moving away from typical pretraining scheme, with increasing focus on specialized word embeddings and applications under very limited dataset size, where fast learning is crucial. We hope that proposed evaluation methodology will help advance research in these scenarios."}, {"heading": "A Theoretical analysis", "text": "Let us first try to answer the question what is the information about the task and how can it be measured given some data representation. For simplicity let us assume that task is a binary classification, but the same reasoning applies to multiclass, multilabel, regressions etc. A quite natural, machine learning perspective is to define information stored as a Bayes risk of optimal model trained to perform this task. Obviously, raw representation already has some non-negative Bayes risk, which cannot be reduced during any embedding. Actually, it is quite easy to show, that nearly every embedding preserves all the information contained in the source representation.\nObservation 1. Every injective embedding preserves all the information about the task.\nProof. Let us assume that Bayesian optimal classifier (the one obtaining the Bayes risk RX ) on the input space X be called o. Furthermore, let our embedding (learned in arbitrary manner, supervised or not) be a function E : X \u2192 X \u2032. According to assumptions, E is injective, thus for every x \u2208 X there exists unique x\u2032 \u2208 X \u2032 such that E(x) = x\u2032. Let us call the corresponding inverse assignment E\u22121 (defined only on the image of E). Consequently we can define classifier on E(X) \u2282 X \u2032 through o\u2032(x\u2032) = o(E\u22121(x\u2032)). It is now easy to show, that Bayes risks of these two models are exactly the same\nRX = \u222b \u2211\ny\n`(o(x), y)p(y|x)p(x)dx\n= \u222b \u2211 y `(o(E\u22121(E(x))), y)p(y|x)p(x)dx\n= \u222b \u2211 y `(o\u2032(E(x), y)p(y|x)p(x)dx\n= RX \u2032 .\nThis remains an open question how frequent in general are injective embeddings. If one considers continuous spaces as X then this is extremely small class of functions (especially if dim(X \u2032) \u2264 dim(X )). However in case of natural language processing (and many other fields), the input space is actually discrete or even finite. In such case, non-injective embeddings are rare phenomenon. In particular, for any finite set, probability of selecting at random linear projection which gives non-injective embedding is zero.\nThe above reasoning is in some sense trivial, yet still worth underlying, as it gives an important notion of what should be measured when evaluating embeddings. Even though Bayes risk is the same for both spaces, the complexity of inferring o\u2032 can be completely different from complexity of inferring o. We argue, that this is a crucial element - to measure how hard is to learn o\u2032 (or any reasonable approximation). There are two basic dimensions of such analysis:\n\u2022 check how complex the set of hypotheses H 3 o\u2032 needs to be in order to be able to find it using given data,\n\u2022 verify how well one can approximate o\u2032 as a function of growing training size. In other words - how fast an estimator of o\u2032 converges.\nThus, to really distinguish various embeddings, we should rather ask what is the best achievable performance under limited amount of data or under constrained class of models, which is theoretical argument for data efficiency oriented evaluation."}, {"heading": "B Regression neural network for word analogy task", "text": "Let D be the dimension of a given word embedding. We assume that all embedded words have euclidean norm equal to one \u2013 this guarantees that the scalar product of two embedded words is also their cosine similarity. The word analogy task is defined in the following way: given (embedded) words v1, v2 and v3, predict word v4 that satisfies analogy \u201cv1 is related to v2 as v3 is related to v4\u201d. Our estimator of v4 is defined as:\nv\u03024 = \u2212W1v1 +W2v2 +W3v3 + b \u2016\u2212W1v1 +W2v2 +W3v3 + b\u20162\nwhere the model trainable parameters are:\n\u2022 W1, W2 and W3 \u2013 D\u00d7D matrices initialized with identities,\n\u2022 b \u2013 D-dimensional vector initialized with zeros,\nand the cost is defined as: \u2212 \u2211 j \u3008vj4, v\u0302 j 4\u3009.\nThe model was trained with gradient descent optimization on minibatches. Hyperparameters: learning rate, number of epochs, optimizer, batch size and (boolean) fallback to constant model were chosen using cross-validation. The actual prediction has two steps:\n\u2022 calculate v\u03024,\n\u2022 choose (embedded) word v that minimizes \u3008v, v\u03024\u3009.\nObserve that this model is initialized in such a way, that it is equivalent to 3COSADD \u2013 the idea is to check, if applying trainable affine transformations to input vectors would boost 3COSADD performance. It should also be noted that this approach turned out to be very computationally intensive."}], "references": [{"title": "Expanding subjective lexicons for social media mining with embedding subspaces", "author": ["Silvio Amir", "Ram\u00f3n Fern\u00e1ndez Astudillo", "Wang Ling", "Paula C. Carvalho", "M\u00e1rio J. Silva"], "venue": "CoRR, abs/1701.00145,", "citeRegEx": "Amir et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Amir et al\\.", "year": 2017}, {"title": "Linear algebraic structure of word senses, with applications to polysemy", "author": ["Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski"], "venue": "arXiv preprint arXiv:1601.03764,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "Improving reliability of word similarity evaluation by redesigning annotation task and performance", "author": ["Oded Avraham", "Yoav Goldberg"], "venue": "measure. CoRR,", "citeRegEx": "Avraham and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Avraham and Goldberg.", "year": 2016}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Intrinsic evaluation of word vectors fails to predict extrinsic performance", "author": ["Billy Chiu", "Anna Korhonen", "Sampo Pyysalo"], "venue": "ACL 2016,", "citeRegEx": "Chiu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2016}, {"title": "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks", "author": ["M. Faruqui", "Y. Tsvetkov", "P. Rastogi", "C. Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Nondistributional word vector representations", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "CoRR, abs/1506.05230,", "citeRegEx": "Faruqui and Dyer.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2015}, {"title": "Non-distributional word vector representations", "author": ["Manaal Faruqui", "Chris Dyer"], "venue": "In Proceedings of ACL,", "citeRegEx": "Faruqui and Dyer.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2015}, {"title": "Wordrep: A benchmark for research on learning word representations", "author": ["Bin Gao", "Jiang Bian", "Tie-Yan Liu"], "venue": "CoRR, abs/1407.1640,", "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "Proceedings of the Twenty-eight International Conference on Machine Learning,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A Primer on Neural Network Models for Natural Language", "author": ["Yoav Goldberg"], "venue": "URL http://arxiv.org/abs/1510", "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Embedding word similarity with neural machine", "author": ["Felix Hill", "Kyunghyun Cho", "S\u00e9bastien Jean", "Coline Devin", "Yoshua Bengio"], "venue": "translation. CoRR,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Simlex999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": "Computational Linguistics,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "What\u2019s in an embedding? analyzing word embeddings through multilingual evaluation", "author": ["Arne K\u00f6hn"], "venue": null, "citeRegEx": "K\u00f6hn.,? \\Q2015\\E", "shortCiteRegEx": "K\u00f6hn.", "year": 2015}, {"title": "What\u2019s in an embedding? analyzing word embeddings through multilingual evaluation", "author": ["Arne K\u00f6hn"], "venue": null, "citeRegEx": "K\u00f6hn.,? \\Q2015\\E", "shortCiteRegEx": "K\u00f6hn.", "year": 2015}, {"title": "How to generate a good word embedding", "author": ["Siwei Lai", "Kang Liu", "Shizhu He", "Jun Zhao"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Lai et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lai et al\\.", "year": 2016}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Kazuya Kawakami", "Sandeep Subramanian", "Chris Dyer"], "venue": "In In proceedings of NAACL-HLT (NAACL 2016).,", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Word embeddings through hellinger pca", "author": ["R\u00e9mi Lebret", "Ronan Collobert"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Lebret and Collobert.,? \\Q2014\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2014}, {"title": "the sum of its parts\": Joint learning of word and phrase representations with autoencoders", "author": ["R\u00e9mi Lebret", "Ronan Collobert"], "venue": "CoRR, abs/1506.05703,", "citeRegEx": "Lebret and Collobert.,? \\Q2015\\E", "shortCiteRegEx": "Lebret and Collobert.", "year": 2015}, {"title": "Large-scale linear ranksvm", "author": ["Ching-Pei Lee", "Chih-Jen Lin"], "venue": "Neural computation,", "citeRegEx": "Lee and Lin.,? \\Q2014\\E", "shortCiteRegEx": "Lee and Lin.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Better word representations with recursive neural networks for morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Evaluating word embeddings using a representative suite of practical tasks", "author": ["Neha Nayak", "Gabor Angeli", "Christopher D. Manning"], "venue": "In RepEval Workshop,", "citeRegEx": "Nayak et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nayak et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Association for Computational Linguistics", "author": ["October"], "venue": "URL http://www.aclweb.org/ anthology/D14-1162.", "citeRegEx": "October,? 2014", "shortCiteRegEx": "October", "year": 2014}, {"title": "Word embedding calculus in meaningful ultradense subspaces. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Rothe and Sch\u00fctze.,? \\Q2016\\E", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2016}, {"title": "Sparsifying word representations for deep unordered sentence modeling", "author": ["Prasanna Sattigeri", "Jayaraman J Thiagarajan"], "venue": "ACL 2016,", "citeRegEx": "Sattigeri and Thiagarajan.,? \\Q2016\\E", "shortCiteRegEx": "Sattigeri and Thiagarajan.", "year": 2016}, {"title": "Labutov. Evaluation methods for unsupervised word embeddings. In EMNLP, pages 298\u2013307", "author": ["Tobias Schnabel", "Igor"], "venue": "The Association for Computational Linguistics,", "citeRegEx": "Schnabel and Igor,? \\Q2015\\E", "shortCiteRegEx": "Schnabel and Igor", "year": 2015}, {"title": "Creating causal embeddings for question answering with minimal supervision", "author": ["Rebecca Sharp", "Mihai Surdeanu", "Peter Jansen", "Peter Clark", "Michael Hammond"], "venue": "CoRR, abs/1609.08097,", "citeRegEx": "Sharp et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharp et al\\.", "year": 2016}, {"title": "Creating causal embeddings for question answering with minimal supervision", "author": ["Rebecca Sharp", "Mihai Surdeanu", "Peter Jansen", "Peter Clark", "Michael Hammond"], "venue": "CoRR, abs/1609.08097,", "citeRegEx": "Sharp et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharp et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "A sensitivity analysis of (and practitioners\u2019 guide to) convolutional neural networks for sentence classification", "author": ["Ye Zhang", "Byron Wallace"], "venue": "arXiv preprint arXiv:1510.03820,", "citeRegEx": "Zhang and Wallace.,? \\Q2015\\E", "shortCiteRegEx": "Zhang and Wallace.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Using word embeddings remains a standard practice in modern NLP systems, both in shallow and deep architectures [Goldberg, 2015].", "startOffset": 112, "endOffset": 128}, {"referenceID": 1, "context": "By encoding information about words in a relatively simple algebraic structure [Arora et al., 2016] they enable fast transfer to the task of interest1.", "startOffset": 79, "endOffset": 99}, {"referenceID": 22, "context": "For instance, it has been shown [Baroni and Dinu, 2014] that neural-based word embeddings perform consistently better then count-based models and later, using WS and WA tasks, it was argued otherwise [Levy et al., 2015].", "startOffset": 200, "endOffset": 219}, {"referenceID": 6, "context": "critiques both benchmarks have received in the literature [Faruqui et al., 2016], in authors\u2019 opinion mostly due to their purely unsupervised nature.", "startOffset": 58, "endOffset": 80}, {"referenceID": 13, "context": "\u2022 Similarity: SimLex999 [Hill et al., 2015],", "startOffset": 24, "endOffset": 43}, {"referenceID": 4, "context": "MEN [Bruni et al., 2014], WordSimilarity353 [Finkelstein et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 24, "context": ", 2001] and Rare Words [Luong et al., 2013].", "startOffset": 23, "endOffset": 43}, {"referenceID": 9, "context": "\u2022 Analogy: 4 categories from WordRep [Gao et al., 2014]4.", "startOffset": 37, "endOffset": 55}, {"referenceID": 33, "context": "\u2022 Sentence: Stanford Sentiment Treebank [Socher et al., 2013] and News20 (3 binary datasets) [Tsvetkov et al.", "startOffset": 40, "endOffset": 61}, {"referenceID": 22, "context": "\u2022 Analogy: 3COSADD, 3COSMUL [Levy et al., 2015] and regression neural network (performing regression on the 4th word given", "startOffset": 28, "endOffset": 47}, {"referenceID": 26, "context": "Setup includes following \u201cshallow\u201d pretrained embeddings: GloVe (100 and 300 dimensions) [Pennington et al., 2014], Hellinger PCA (HPCA) [Lebret and Collobert, 2014], PDC (100 and 300 dimensions) and HDC (300 dimensions) [Sun et al.", "startOffset": 89, "endOffset": 114}, {"referenceID": 19, "context": ", 2014], Hellinger PCA (HPCA) [Lebret and Collobert, 2014], PDC (100 and 300 dimensions) and HDC (300 dimensions) [Sun et al.", "startOffset": 30, "endOffset": 58}, {"referenceID": 21, "context": "2 We also tried RankSVM [Lee and Lin, 2014], but it did not perform better than other models, while being very computationally intensive.", "startOffset": 24, "endOffset": 43}, {"referenceID": 12, "context": "activations of the deep model are extracted as word embeddings) [Hill et al., 2014], morphological embeddings (morph, which can learn morphological differences between words directly) [Luong et al.", "startOffset": 64, "endOffset": 83}, {"referenceID": 24, "context": ", 2014], morphological embeddings (morph, which can learn morphological differences between words directly) [Luong et al., 2013] and HPCA variant trained using", "startOffset": 108, "endOffset": 128}, {"referenceID": 20, "context": "autoencoder architecture [Lebret and Collobert, 2015].", "startOffset": 25, "endOffset": 53}, {"referenceID": 23, "context": "pora [Ling et al., 2015] (used commonly in syntax demanding tasks, like tagging).", "startOffset": 5, "endOffset": 24}, {"referenceID": 9, "context": "Low constant model scores are similar to numbers reported in [Gao et al., 2014].", "startOffset": 61, "endOffset": 79}, {"referenceID": 9, "context": "thors [Gao et al., 2014] reported low accuracies (often even below 5%) on most analogy questions and we were able to improve absolute score upon the tested subset on average by absolute 11% (see Tab.", "startOffset": 6, "endOffset": 24}, {"referenceID": 12, "context": "It was claimed in [Hill et al., 2014] that NMT embeddings are better at encoding true similarity between words, but SVR on Glove embeddings performs better after training on the whole dataset (i.", "startOffset": 18, "endOffset": 37}, {"referenceID": 1, "context": "One of the presented arguments for including supervised models for testing information content is algebraic interpretation of word embeddings [Arora et al., 2016].", "startOffset": 142, "endOffset": 162}, {"referenceID": 9, "context": "from WordRep benchmark [Gao et al., 2014]).", "startOffset": 23, "endOffset": 41}], "year": 2017, "abstractText": "Maybe the single most important goal of representation learning is making subsequent learning faster. Surprisingly, this fact is not well reflected in the way embeddings are evaluated. In addition, recent practice in word embeddings points towards importance of learning specialized representations. We argue that focus of word representation evaluation should reflect those trends and shift towards evaluating what useful information is easily accessible. Specifically, we propose that evaluation should focus on data efficiency and simple supervised tasks, where the amount of available data is varied and scores of a supervised model are reported for each subset (as commonly done in transfer learning). In order to illustrate significance of such analysis, a comprehensive evaluation of selected word embeddings is presented. Proposed approach yields a more complete picture and brings new insight into performance characteristics, for instance information about word similarity or analogy tends to be non\u2013linearly encoded in the embedding space, which questions the cosine\u2013based, unsupervised, evaluation methods. All results and analysis scripts are available online.", "creator": "LaTeX with hyperref package"}}}