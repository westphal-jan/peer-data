{"id": "1702.02265", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2017", "title": "Neural Machine Translation with Source-Side Latent Graph Parsing", "abstract": "This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, so the parser is optimized according to the translation objective. Experimental results show that our model significantly outperforms the previous best results on the standard English-to-Japanese translation dataset.", "histories": [["v1", "Wed, 8 Feb 2017 03:32:23 GMT  (131kb,D)", "http://arxiv.org/abs/1702.02265v1", null], ["v2", "Mon, 20 Feb 2017 01:47:16 GMT  (279kb,D)", "http://arxiv.org/abs/1702.02265v2", "Add results of using \"Pre-training\" in Table 2 and 3, with analysis on the learned latent graphs and translations"], ["v3", "Sun, 16 Apr 2017 22:46:08 GMT  (377kb,D)", "http://arxiv.org/abs/1702.02265v3", "Reported additional results"], ["v4", "Mon, 24 Jul 2017 14:52:06 GMT  (383kb,D)", "http://arxiv.org/abs/1702.02265v4", "Accepted as a full paper at the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017)"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kazuma hashimoto", "yoshimasa tsuruoka"], "accepted": true, "id": "1702.02265"}, "pdf": {"name": "1702.02265.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation with Source-Side Latent Graph Parsing", "authors": ["Kazuma Hashimoto", "Yoshimasa Tsuruoka"], "emails": ["hassy@logos.t.u-tokyo.ac.jp", "tsuruoka@logos.t.u-tokyo.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014). Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b; Sennrich and Haddow, 2016; Stahlberg et al., 2016). The existing syntax-based NMT models employ a syntactic parser trained by supervised learning and hence rely on human-annotated treebanks. An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences along with the target task (Socher et al., 2011; Yogatama et al., 2016).\nWe present a novel NMT model that can learn a task-specific latent graph structure for each source-side sentence. The graph structure is similar to the dependency structure of the sentence, but it can have cycles and is learned specifically for the translation task. Unlike the aforementioned\nAll the calculated electronic band structures are metallic .\n0.42 0.34 0.20\n0.44\napproach of learning single syntactic trees, our latent graphs are composed of \u201csoft\u201d connections, i.e., the edges have real-valued weights (Figure 1). Our model consists of two parts: one is a taskindependent parsing component, which we call a latent graph parser, and the other is an attentionbased NMT model. Our experiments show that our model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset."}, {"heading": "2 Latent Graph Parser", "text": "We model the latent graph parser based on dependency parsing. In dependency parsing, a sentence is represented as a tree structure where each node corresponds to a word in the sentence and a unique root node (ROOT) is added. Assuming a sentence of length N , the parent node Hwi \u2208 {w1, . . . , wN ,ROOT} (Hwi 6= wi) of the word wi (1 \u2264 i \u2264 N) is called the head. Then, the sentence is represented as a set of tuples (wi, Hwi , `wi), where `wi is a dependency label.\nIn this paper, we remove the constraint of using the tree structures and represent the sentence as a set of tuples (wi, p(Hwi |wi), p(`wi |wi)), where p(Hwi |wi) is the probability distribution of wi\u2019s parent nodes, and p(`wi |wi) is the probability distribution of the dependency labels. For example, p(Hwi = wj |wi) is the probability that wj is the parent node of wi. Here, we assume that a special token \u3008EOS\u3009 is appended to the end of the sentence, and we treat the \u3008EOS\u3009 token as ROOT.\nar X\niv :1\n70 2.\n02 26\n5v 1\n[ cs\n.C L\n] 8\nF eb\n2 01\n7\nThis approach is similar to that of graph-based dependency parsing in that a sentence is represented with a set of weighted arcs between the words. To obtain the latent graph representation of the sentence, we use a dependency parsing model based on multi-task learning (Hashimoto et al., 2016b)."}, {"heading": "2.1 Word Representations", "text": "The i-th input word wi is represented with the concatenation of its word embedding vdp(wi) \u2208 Rd1 and its character n-gram embedding c(wi) \u2208 Rd1 : x(wi) = [vdp(wi); c(wi)], where d1 is the dimensionality of the embeddings. c(wi) is computed as the average of the embeddings of the character n-grams in wi."}, {"heading": "2.2 POS Tagging Layer", "text": "The latent graph parser builds upon multilayer bi-directional Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units (Graves and Schmidhuber, 2005). In the first layer, POS tagging is handled by computing a hidden state h(1)i = [ \u2212\u2192 h (1) i ; \u2190\u2212 h (1) i ] \u2208 R2d1 for wi, where \u2212\u2192 h\n(1) i = LSTM(\n\u2212\u2192 h\n(1) i\u22121, x(wi)) \u2208 Rd1 and\u2190\u2212\nh (1) i = LSTM(\n\u2190\u2212 h\n(1) i+1, x(wi)) \u2208 Rd1 are hidden\nstates of the forward and backward LSTMs, respectively. h(1)i is then fed into a softmax classifier to predict a probability distribution p(1)i \u2208 RC (1) for word-level tags, where C(1) is the number of classes. The model parameters of this layer can be learned not only by human-annotated data, but also by backpropagation from higher layers, which are described in the next section."}, {"heading": "2.3 Dependency Parsing Layer", "text": "In the second layer, dependency parsing is handled. A hidden state h(2)i \u2208 R2d1 is computed by \u2212\u2192 h\n(2) i = LSTM(\n\u2212\u2192 h\n(2) i\u22121, [x(wi); y(wi);\n\u2212\u2192 h\n(1) i ])\nand \u2190\u2212 h\n(2) i = LSTM(\n\u2190\u2212 h\n(2) i+1, [x(wi); y(wi);\n\u2190\u2212 h\n(1) i ]),\nwhere y(wi) = W (1) ` p (1) i \u2208 Rd2 is the output information from the first layer, and W (1)` \u2208 Rd2\u00d7C(1) is a weight matrix.\nThen, the latent graph representation is obtained by computing the probability\np(Hwi = wj |wi) = exp (m(i, j))\u2211 k 6=i exp (m(i, k)) , (1)\nwhere m(i, k) = h(2)Tk Wdph (2) i (1 \u2264 k \u2264 N + 1, k 6= i) is a scoring function with a weight matrix Wdp \u2208 R2d1\u00d72d1 . In the model of Hashimoto\net al. (2016b) and another related model (Zhang et al., 2017), the parameters of the parsing model were learned only by human-annotated data, but we allow the model parameters to be learned by the translation task.\nNext, [h(2)i ; z(Hwi)] is fed into a softmax classifier to predict the probability distribution p(`wi |wi), where z(Hwi) \u2208 R2d1 is the weighted average of the hidden states of the parent nodes: \u2211 j 6=i p(Hwi = wj |wi)h (2) j . Now we have the latent graph representation (wi, p(Hwi |wi), p(`wi |wi)) of the input sentence."}, {"heading": "3 NMT with Latent Graph Parser", "text": "The latent graph representation described in Section 2 can be ideally used for any sentence-level tasks, and here we apply it to an Attention-based NMT (ANMT) model (Luong et al., 2015). We modify the encoder and the decoder in the ANMT model to learn the latent graph representation."}, {"heading": "3.1 Encoder with Dependency Composition", "text": "The ANMT model first encodes the information about the input sentence and then generates a sentence in another language. The encoder represents the word wi with a word embedding venc(wi) \u2208 Rd3 . It should be noted that venc(wi) is different from vdp(wi) because each component is separately modeled. The encoder then takes the word embedding venc(wi) and the hidden state h (2) i as the input to a uni-directional LSMT: h(enc)i = LSTM(h\n(enc) i\u22121 , [venc(wi);h (2) i ]) \u2208 Rd3 . That is,\nthe encoder of our model is a three-layer LSTM, where the first two layers are bi-directional.\nIn the sequential LSTMs, relationships between words in distant positions are not explicitly considered. In our model, we explicitly incorporate such relationships into the encoder by defining a dependency composition function:\ndep(wi) = tanh(Wdep[h enc i ;h(Hwi); p(`wi |wi)]), (2) where h(Hwi) = \u2211 j 6=i p(Hwi = wj |wi)h (enc) j is the weighted average of the hidden states of the parent nodes."}, {"heading": "3.2 Decoder with Attention Mechanism", "text": "The decoder of our model is a single-layer LSTM, and the initial state is initialized with h(enc)N+1 and its corresponding memory cell. Given the t-th hidden state h(dec)t \u2208 Rd3 , the decoder predicts the\nt-th word in the target language using an attention mechanism. The attention mechanism in Luong et al. (2015) computes the weighted average of the hidden states h(enc)i of the encoder:\ns(i, t) = exp (h\n(dec) t \u00b7h (enc) i )\u2211N+1\nj=1 exp (h (dec) t \u00b7h (enc) j )\n, (3)\nat = \u2211N+1 i=1 s(i, t)h (enc) i , (4)\nwhere s(i, t) is a scoring function which specifies how much each source-side hidden state contributes to the word prediction.\nMoreover, like the attention mechanism over constituency tree nodes (Eriguchi et al., 2016b), our model pays attention to the dependency composition vectors:\na\u2032t = N\u2211 i=1 s\u2032(i, t)dep(wi), (5)\nwhere s\u2032(i, t) is computed in the same way as Equation (3). To predict the target word, a hidden state h\u0303(dec)t \u2208 Rd3 is then computed as follows:\nh\u0303 (dec) t = tanh(W\u0303 [h (dec) t ; at; a \u2032 t]), (6)\nwhere W\u0303 \u2208 Rd3\u00d73d3 is a weight matrix. h\u0303(dec)t is fed into a softmax classifier to predict a target word distribution. h\u0303(dec)t is also used in the transition of the decoder LSTM along with a word embedding vdec(wt) \u2208 Rd3 of the target word wt: h (dec) t+1 = LSTM(h (dec) t , [vdec(wt); h\u0303 (dec) t ]).\nThe overall model parameters, including those of the latent graph parser, are jointly learned by minimizing the negative log-likelihood of the prediction probabilities of the target words in training data. To speed up the training, we use BlackOut sampling (Ji et al., 2016). By this joint learning using Equation (2) and (5), the latent graph representations are automatically learned according to the target task.\nImplementation Tips Inspired by Zoph et al. (2016), we further speed up BlackOut sampling by sharing noise samples across words in the same sentences. This technique has proven to be effective in RNN language modeling, and we have found that it is also effective in the NMT model. Next, we have found it effective to share the model parameters of the target word embeddings and the softmax weight matrix for word prediction (Inan et al., 2016). Finally, we have found that a parameter averaging technique (Hashimoto et al., 2013) is helpful in improving translation accuracy."}, {"heading": "4 Experimental Settings", "text": "We briefly describe our experimental settings, and more details are available in the appendix."}, {"heading": "4.1 Data", "text": "We used an English-to-Japanese translation task of the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016b), since it has been shown that syntactic information is useful in English-to-Japanese translation (Eriguchi et al., 2016b; Neubig et al., 2015). We constructed a small training dataset including 100,000 translation pairs, and a large training dataset including 1,346,946 pairs. The development data includes 1,790 pairs, and the test data 1,812 pairs."}, {"heading": "4.2 Parameter Optimization and Translation", "text": "We set (d1, d2) = (100, 50) for the latent graph parser, d3 = 256 for the small training dataset, and d3 = 512 for the large training dataset. The training was performed by mini-batch stochastic gradient descent with momentum. At test time, we used a beam search algorithm with statistics of sentence lengths (Eriguchi et al., 2016b) and length normalization (Cho et al., 2014)."}, {"heading": "5 Results and Discussion", "text": ""}, {"heading": "5.1 Results on Small Training Data", "text": "Table 1 shows the effects of learning the latent graph parser. Averaged scores with standard deviations across five different runs of the model training are shown. Evaluation metrics are BLEU, RIBES (Isozaki et al., 2010), and perplexity scores. Note that lower perplexity scores indicate better accuracy.\nThe result of \u201cw/ pre-training\u201d is obtained by pre-training the latent parser using the widelyused Wall Street Journal (WSJ) training data. We follow Chen and Manning (2014) to generate the training data for dependency parsing. The parser including the POS tagger is first trained for 10 epochs in advance according to the multi-task learning procedure of Hashimoto et al. (2016b),\nand then the overall NMT model is trained. The result suggests that the pre-training can improve translation accuracy.\nBaseline A is constructed by removing the dependency composition in Equation (2), forming a sequential NMT model with the multi-layer encoder. Our model is slightly better than Baseline A, but there is no significant difference. Baseline B is constructed by fixing p(Hwi = wj |wi) to 1N for all the words in the same sentence. Our model significantly outperforms Baseline B, which shows that our adaptive learning is more effective than using the fixed graph weights."}, {"heading": "5.2 Results on Large Training Data", "text": "Table 2 shows the BLEU and RIBES scores on the development data achieved with the large training dataset. The latent parser was not pre-trained in this experiment. The averaging technique and attention-based unknown word replacement (Jean et al., 2015; Hashimoto et al., 2016a) improve the scores. Note that unlike widely-used ensemble techniques, the averaging technique does not increase computational cost. In terms of the comparison between our proposed model and Baseline A, we did not observe significant difference.\nTable 3 shows our results on the test data, and the previous best results summarized in Nakazawa et al. (2016a) and the WAT website1 are also shown. Confidence intervals (p \u2264 0.05) of the BLEU scores by bootstrap resampling (Noreen, 1989) are (38.23, 39.98) and (37.95, 39.78) for our model and Baseline A, respectively. Those of the RIBES scores are (82.07, 83.18) and (81.57, 82.72). Therefore, the BLEU score of our model (39.19) is significantly better than all\n1http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation/list.php?t=1&o=1.\nof the previous results, but not that of Baseline A. The NMT models in Cromieres et al. (2016) and Eriguchi et al. (2016b) rely on ensemble techniques while our results are obtained using single models. Moreover, our model is more compact2 than the NMT model in Cromieres et al. (2016). By applying the ensemble technique to our model and Baseline A, the BLEU and RIBES scores are further improved, and both of the scores are significantly better than the previous best scores.\nLearned Latent Graphs Figure 1 shows an example of the learned latent graph obtained for a sentence taken from the development data of the translation task. It has long-range dependencies and cycles as well as ordinary left-to-right dependencies. We have observed that the punctuation mark \u201c.\u201d is often pointed to by other words with large weights. This is primarily because the hidden state corresponding to the mark in each sentence has rich information about the sentence.\nTo measure the correlation between the latent graphs and human-defined dependencies, we parsed the sentences on the development data of the WSJ corpus and converted the graphs into dependency trees by Eisner\u2019s algorithm (Eisner, 1996). For evaluation, we followed Chen and Manning (2014) and measured Unlabeled Attachment Score (UAS). The UAS is 24.52%, which shows that the implicitly-learned latent graphs are partially consistent with the human-defined syntactic structures. We checked the most dominant gold dependency labels which were assigned for the dependencies detected by our model. The labels whose ratio is more than 3% are nn, amod, prep, pobj, dobj, nsubj, num, det, advmod, and poss. We see that dependencies between words in distant positions, such as subject-verb-object relations, can be captured."}, {"heading": "6 Conclusion and Future Work", "text": "We have presented an end-to-end NMT model by jointly learning translation and source-side latent graph representations. On English-to-Japanese translation, our NMT model outperforms the previous best models by a large margin. In future work, we investigate the effectiveness of our approach in different types of target tasks.\n2Our training time is within five days on a c4.8xlarge machine of Amazon Web Service by our CPU-based C++ code, while it is reported that the training time is more than two weeks in Cromieres et al. (2016) by their GPU code."}, {"heading": "Acknowledgments", "text": "We thank Yuchen Qiao and Kenjiro Taura for their help in speeding up our training code. We also thank Akiko Eriguchi for providing the preprocessed data and helpful comments. This work was supported by CREST, JST."}, {"heading": "A Supplemental Material", "text": "A.1 Data Preprocessing\nWe followed the data preprocessing instruction for the English-to-Japanese task in Eriguchi et al.\n(2016b). The English sentences were tokenized by the tokenizer in the Enju parser (Miyao and Tsujii, 2008), and the Japanese sentences were segmented by the KyTea tool3. Among the first 1,500,000 translation pairs in the training data, we selected 1,346,946 pairs where the maximum sentence length is 50. This is the large training dataset, and we further sampled 100,000 pairs to construct the small training dataset. For the small dataset, we built the vocabulary with words whose minimum frequency is two, and for the large dataset, we used words whose minimum frequency is three for English and five for Japanese. As a result, the vocabulary of the target language was 23,532 for the small dataset, and 65,680 for the large dataset. A special token \u3008UNK\u3009was used to replace words which were not included in the vocabularies. The character n-grams (n = 2, 3, 4) were also constructed from each training dataset with the same frequency settings.\nA.2 Parameter Optimization\nThe word and character n-gram embeddings of the latent graph parser were initialized with the pretrained embeddings in Hashimoto et al. (2016b). The weight matrices in the latent graph parser were initialized with uniform random values in [\u2212 \u221a 6\u221a\nrow+col ,+\n\u221a 6\u221a\nrow+col ], where row and col are\nthe number of rows and columns of the matrices, respectively. All the bias vectors and the weight matrices in the softmax layers were initialized with zeros, and the bias vectors of the forget gates in the LSTMs were initialized by ones (Jozefowicz et al., 2015).\nThe word embeddings and the weight matrices of the NMT model were initialized with uniform random values in [\u22120.1,+0.1]. For training the model, the mini-batch size was set to 128, and the momentum rate was set to 0.75 for the small training dataset and 0.70 for the large training dataset. A gradient clipping technique was used with a clipping value of 1.0. The initial learning rate was set to 1.0, and the learning rate was halved when translation accuracy decreased. We used the BLEU scores obtained by greedy translation as the translation accuracy and checked it at every half epoch of the model training. We saved the model parameters at every half epoch and used the saved model parameters for the parameter averaging technique. For regularization,\n3http://www.phontron.com/kytea/.\nwe used L2-norm regularization with a coefficient of 10\u22126 and applied dropout (Hinton et al., 2012) to Equation (6) with a dropout rate of 0.8. The beam size was 12 for the small training data, and 50 for the large training data."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences. Unlike existing pipelined approaches using syntactic parsers, our end-to-end model learns a latent graph parser as part of the encoder of an attention-based neural machine translation model, so the parser is optimized according to the translation objective. Experimental results show that our model significantly outperforms the previous best results on the standard Englishto-Japanese translation dataset.", "creator": "LaTeX with hyperref package"}}}