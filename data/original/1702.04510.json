{"id": "1702.04510", "review": {"conference": "AAAI", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "A Dependency-Based Neural Reordering Model for Statistical Machine Translation", "abstract": "In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-the-art statistical MT system that uses sparse dependency-based reordering features.", "histories": [["v1", "Wed, 15 Feb 2017 09:08:21 GMT  (2774kb,D)", "http://arxiv.org/abs/1702.04510v1", "7 pages, 3 figures, Proceedings of AAAI-17"]], "COMMENTS": "7 pages, 3 figures, Proceedings of AAAI-17", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["christian hadiwinoto", "hwee tou ng"], "accepted": true, "id": "1702.04510"}, "pdf": {"name": "1702.04510.pdf", "metadata": {"source": "META", "title": "A Dependency-Based Neural Reordering Model for Statistical Machine Translation", "authors": ["Christian Hadiwinoto", "Hwee Tou Ng"], "emails": ["nght}@comp.nus.edu.sg"], "sections": [{"heading": "Introduction", "text": "In a machine translation (MT) system, determining the correct word order of translated words is crucial as word order reflects meaning. As different languages have different ordering of words, reordering of words is required to produce the correct translation output. Reordering in MT remains a major challenge for language pairs with a significant word order difference. Phrase-based MT systems (Koehn, Och, and Marcu 2003), which achieve state-of-the-art performance, generally adopt a reordering model based on the span of a phrase and the span of its adjacent phrase (Tillmann 2004; Koehn et al. 2005; Galley and Manning 2008).\nIncorporating the dependency parse tree of an input (source) sentence is beneficial for reordering, as the dependency tree captures the relationships between words in a sentence, through the dependency relation label between two words. The dependency parse tree of a source sentence can be utilized in reordering integrated within phrasebased statistical MT (SMT), by defining dependency-based features in the SMT log-linear model (Chang et al. 2009; Hadiwinoto, Liu, and Ng 2016).\nRecently, neural networks have been applied to natural language processing (NLP) to minimize feature engineering and to utilize continuous word representation. It\nCopyright c\u00a9 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nhas found application in MT reordering, applied in the reranking of translation output candidates (Li et al. 2014; Cui, Wang, and Li 2016) and in the pre-ordering approach (reordering the source sentence before translation) (de Gispert, Iglesias, and Byrne 2015; Miceli-Barone and Attardi 2015). Nevertheless, reordering integrated with translation has not benefited from the neural approach.\nIn this paper, we propose applying a neural network (NN) model in reordering integrated within translation. We apply our neural classifier on two words linked by a dependency relation link, either a head-child or sibling link, in order to predict if two words need to be swapped in the translation. The prediction is used to guide the decoding process of stateof-the-art phrase-based SMT."}, {"heading": "A Neural Classifier for Dependency-Based Reordering", "text": "We propose two neural classifiers, one to predict the correct order of the translated target words of two source words with a head-child relation, and the other for two source words with a sibling relation. Each binary classifier takes a set of features related to the two source words as its input and predicts if the translated words should be swapped (positive) or remain in order (negative)."}, {"heading": "Input Representation", "text": "The head-child classifier predicts the order of the translated words of a source word xc and its head word xh (where xg is the head word of xh) using the following input features: \u2022 The head word xh, its part-of-speech (POS) tag T (xh),\nand the dependency label L(xh) linking xh to xg \u2022 The child word xc, its POS tag T (xc), and the dependency\nlabel L(xc) linking xc to xh \u2022 The signed distance d(xh, xc) between the head and the\nchild in the original source sentence, with the following possible values: \u2013 \u22122 if xc is on the left of xh and there is at least one\nother child between them \u2013 \u22121 if xc is on the left of xh and there is no other child\nbetween them \u2013 +1 if xc is on the right of xh and there is no other child\nbetween them\nar X\niv :1\n70 2.\n04 51\n0v 1\n[ cs\n.C L\n] 1\n5 Fe\nb 20\n17\n\u2013 +2 if xc is on the right of xh and there is at least one other child between them\n\u2022 A Boolean \u03c9(xh, xc) to indicate if any punctuation symbol, which is also the child of xh, exists between xh and xc\nThe sibling classifier predicts the order of the translated words of two source words xl and xr, where xl is to the left of xr and both have the common head word xh, using the following features: \u2022 The left child word xl, its POS tag T (xl), the dependency\nlabel L(xl) linking xl to xh, and the signed distance to its head d(xh, xl)\n\u2022 The right child word xr, its POS tag T (xr), the dependency label L(xr) linking xr to xh, and the signed distance to its head d(xh, xr) \u2022 The head word xh and its POS tag T (xh) \u2022 A Boolean \u03c9(xl, xr) to indicate if any punctuation sym-\nbol, which is also the child of xh, exists between xl and xr"}, {"heading": "Feed-Forward Layers", "text": "As shown in Figure 1a, the classifier is a feed-forward neural network whose input layer contains the features. Each feature is mapped by a lookup table to a continuous vector representation, and the resulting vectors are concatenated and fed into (multiplied by) a series of hidden lay-\ners (weight matrices) using the rectified linear activation function, relu(x) = max(0, x). Given the hidden-layertransformed embedding vector x, a weight vector W, and a bias value b, the prediction output \u03c3 is defined as:\nz = W \u00b7 x+ b (1)\n\u03c3(z) = 1\n1 + e\u2212z (2)\nWe initialize the hidden layers and the embedding layer for non-word features (POS tags, dependency labels, and Boolean indicators) by a random uniform distribution. For word features xh, xc, xl, and xr, we initialize their embeddings by the dependency-driven embedding scheme of (Bansal, Gimpel, and Livescu 2014). This scheme is a modified skip-gram model, which given an input word, predicts its context (surrounding words), resulting in a mapping such that words with similar surrounding words have similar continuous vector representations (Mikolov et al. 2013). Similarly, defining the dependency information (i.e., label, head word, and child word) as context produces a mapping such that words with similar head and child words have similar continuous vector representations.\nDependency-driven embedding can be obtained from a dependency-parsed corpus, where each training instance is formulated as (note: xg is the head of xh):\nL(xh)<GL> xg<G> xh xc L(xc)<L> The skip-gram model is trained with a window size of 1 (denoting one context item on the left and one on the right).\nFollowing (Bansal, Gimpel, and Livescu 2014), the items marked by <> subscripts serve as the context, have different continuous vector representations from the words (xh and xc), and are filtered out from the embedding vocabulary after training."}, {"heading": "Neural Network Training", "text": "The training instances for the neural classifiers are obtained from a word-aligned parallel corpus. Two source-side words with head-child or sibling relation are extracted with their corresponding order label, swapped or in order, depending on the positions of their aligned target-side words. Figure 1 shows the training instances extracted with their corresponding features. For the head-child classifier, the features containing the child information are distinguished based on whether the child is on the left or right of the head.\nThe NN classifiers are trained using back-propagation to minimize the cross-entropy objective function:\nL = \u2212 1 T T\u2211 i=1 yi log y\u0302i + (1\u2212 yi) log(1\u2212 y\u0302i) (3)\nwhere xi is the i-th training instance, yi is its corresponding label (1 for swapped and 0 for in order), and y\u0302i is the classifier prediction probability for swapped. To prevent model overfitting, we used the dropout strategy (Srivastava et al. 2014) on the input embedding layer."}, {"heading": "Reordering in Phrase-Based SMT", "text": "We adopt the phrase-based SMT approach, using a beam search decoding algorithm (Koehn 2004a). Each source phrase and one of its possible translations represent an alternative in the search for the translated sentence. While the search produces translation from left to right in the translation output order, it picks the source phrases in any order to enable reordering for a language pair with different word order. The translation output is picked based on a score computed by a log-linear model, comprising the weighted sum of feature function values (Och and Ney 2002).\nA phrase-based SMT system typically includes a distance-based reordering penalty (DBR) (Koehn, Och, and Marcu 2003), to discourage long-distance reordering, and phrase-based reordering models (PBRM). The latter comprises the phrase-based lexicalized reordering (PBLR) model (Tillmann 2004; Koehn et al. 2005) and the hierarchical reordering (HR) model (Galley and Manning 2008). These models are the conventional reordering models widely used in phrase-based SMT."}, {"heading": "Dependency-Based Decoding Features", "text": "Phrase-based decoding can take into account the source dependency parse tree to guide its search. To encourage structural cohesion during translation, we add a dependency distortion penalty (DDP) feature (Cherry 2008) to discourage translation output in which the translated words of a phrase in a source dependency parse subtree are split.\nWe also incorporate the sparse dependency swap (DS) features of our prior work (Hadiwinoto, Liu, and Ng 2016).\nThe features involve considering a source word x being translated during beam search and each of the as yet untranslated source words x\u2032, where x\u2032 is the head, the child, or the sibling of x in the source dependency parse tree. x\u2032 can be on the left of x in the source sentence, resulting in x and x\u2032 being swapped in the translation output; or x\u2032 can be on the right of x, resulting in x and x\u2032 following the same order (in order). This principle is used to guide word pair translation ordering through sparse feature templates for head-child word pair and sibling word pair, in which each word x in a word pair is represented by its dependency label (L(x)), POS tag (T (x)), and their combination.\nSpecifically, the sparse dependency swap (DS) features of (Hadiwinoto, Liu, and Ng 2016) are based on a feature template for a head word xh and its child word xc, their dependency labels and POS tags, whether xh is on the p \u2208 {left, right} of xc in the source sentence, and the ordering of the pair in the translation output o \u2208 {in order, swapped}:\nHhc(xh, xc, p, o) =  hhc(L(xh), L(xc), p, o)hhc(T (xh), T (xc), p, o)hhc(L(xh), T (xc), p, o) hhc(T (xh), L(xc), p, o)  (4) Similarly, there is another feature template for two sibling words, xl on the left of xr sharing a common head word, their dependency labels and POS tags, and the ordering of the pair in the translation output o \u2208 {in order, swapped}:\nHsib(xl, xr, o) =  hsib(L(xl), L(xr), o)hsib(T (xl), T (xr), o)hsib(L(xl), T (xr), o) hsib(T (xl), L(xr), o)  (5)"}, {"heading": "Incorporating Neural Classifier", "text": "We incorporate the neural classifier by defining one decoding feature function for the head-child classifier, and another decoding feature function for the sibling classifier. We also employ model ensemble by training multiple head-child and sibling classifiers, each with a different random seed for hidden layer initialization. Within the log-linear model, the value of each neural classifier feature function is its prediction log-probability. Each feature function is assigned a different weight obtained from tuning on development data."}, {"heading": "Experimental Setup", "text": ""}, {"heading": "Data Set and Toolkits", "text": "We conducted experiments on a phrase-based Chinese-toEnglish SMT system built using Moses (Koehn et al. 2007). Our parallel training corpora are from LDC, which we divide into older corpora1 and newer corpora2. Due to the dominant\n1LDC2002E18, LDC2003E14, LDC2004E12, LDC2004T08, LDC2005T06, and LDC2005T10.\n2LDC2007T23, LDC2008T06, LDC2008T08, LDC2008T18, LDC2009T02, LDC2009T06, LDC2009T15, LDC2010T03, LDC2013T11, LDC2013T16, LDC2014T04, LDC2014T11, LDC2014T15, LDC2014T20, and LDC2014T26.\nolder corpora, we duplicate the newer corpora of various domains ten times to achieve better domain balance. To reduce the possibility of alignment errors, parallel sentences in the corpora that are longer than 85 words in either Chinese (after word segmentation) or English are discarded. In the end, the final parallel texts consist of about 8.8M sentence pairs, 228M Chinese tokens, and 254M English tokens (a token can be a word or punctuation symbol). We also added two dictionaries3, having 1.81M Chinese tokens and 2.03M English tokens in total, by concatenating them to our training parallel texts. To train the Chinese word embeddings as described above, we concatenate the Chinese side of our parallel texts with Chinese Gigaword version 5 (LDC2011T13), resulting in 2.08B words in total.\nAll Chinese sentences in our experiment are first wordsegmented using a maximum entropy-based Chinese word segmenter (Low, Ng, and Guo 2005) trained on the Chinese Treebank (CTB) segmentation standard. Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al. 1993)4. For building the phrase table, which follows word alignment, the maximum length of a phrase is set to 7 words for both the source and target sides.\nThe language model (LM) is a 5-gram model trained on the English side of the FBIS parallel corpus (LDC2003E14) and the monolingual corpus English Gigaword version 4 (LDC2009T13), consisting of 107M sentences and 3.8B tokens altogether. Each individual Gigaword sub-corpus5 is used to train a separate language model and so is the English side of FBIS. These individual language models are then interpolated to build one single large LM, via perplexity tuning on the development set.\nTraining the neural reordering classifier involves LDC manually-aligned corpora, from which we extracted 572K head-child pairs and 1M sibling pairs as training instances6, while retaining 90,233 head-child pairs and 146,112 sibling pairs as held-out tuning instances7. The latter is used to pick the best neural network parameters.\nOur translation development set is MTC corpus version 1 (LDC2002T01) and version 3 (LDC2004T07). This development set has 1,928 sentence pairs in total, 49K Chinese tokens and 58K English tokens on average across the four reference translations. Weight tuning is done by using the pairwise ranked optimization (PRO) algorithm (Hopkins and May 2011).\nWe parse the Chinese sentences by the Mate parser, which jointly performs POS tagging and dependency parsing (Bohnet and Nivre 2012), trained on Chinese Treebank (CTB) version 8.0 (LDC2013T21).\nOur translation test set consists of the NIST MT evaluation sets from 2002 to 2006, and 20088.\n3LDC2002L27 and LDC2005T34. 4The default when running GIZA++ with Moses. 5AFP, APW, CNA, LTW, NYT, and Xinhua 6LDC2012T20, LDC2012T24, LDC2013T05, LDC2013T23, LDC2014T25, LDC2015T04, and LDC2015T18. 7LDC2012T16. 8LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14,"}, {"heading": "Baseline System", "text": "Our phrase-based baseline SMT system includes the conventional reordering models, i.e., distance-based reordering penalty (DBR) and phrase-based reordering model (PBRM), both phrase-based lexicalized reordering (PBLR) and hierarchical reordering (HR). We also use the dependency-based reordering features, including the distortion penalty (DDP) feature and the sparse dependency swap (DS) features.\nTo constrain the decoding process, we set punctuation symbols as reordering constraint across which phrases cannot be reordered, as they form the natural boundaries between different clauses. In addition, a distortion limit is set such that reordering cannot be longer than a certain distance. To pick the translation output, we also use n-best minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) instead of the default maximum a-posteriori (MAP) decoding."}, {"heading": "Neural Reordering", "text": "We replaced DS features by our dependency-based neural reordering classifier, in which we set the word vocabulary to the 100,000 most frequent words in our parallel training corpora, replacing other words with a special UNK token, in addition to all POS tags, dependency labels, and Boolean features. We set the embedding dimension size to 100, the lower hidden layer dimension size to 200, and the upper hidden layer dimension size to 100. We trained for 100 epochs, with 128 mini-batches per epoch, and used a dropout rate of 0.5. For model ensemble, we trained 10 classifiers for headchild reordering and 10 for sibling reordering, each of which forming one feature function."}, {"heading": "Experimental Results", "text": "The translation quality of the system output is measured by case-insensitive BLEU (Papineni et al. 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)9. Statistical significance testing between systems is conducted by bootstrap resampling (Koehn 2004b).\nTable 1 shows the experimental results. The distortion limit of all the systems is set to 14. As shown in the table, when the word embedding features are initialized using dependency context (Bansal, Gimpel, and Livescu 2014), which is our default scheme, our translation system with single neural classifier is able to improve over our strong baseline system (DBR+PBRM+DDP+DS) by +0.32 BLEU point, while an ensemble model of 10 neural classifiers improves over our baseline system by +0.57 BLEU point. The results show that the neural reordering classifier is able to replace the sparse dependency swap features and achieves better performance.\nIn addition to the dependency-driven embedding initialization scheme of (Bansal, Gimpel, and Livescu 2014), we are also interested in testing other word embedding schemes. Additional experiments use two other initialization schemes: (1) random initialization and (2) the original skip-gram model of (Mikolov et al. 2013) with a window size of 5.\nLDC2010T17, and LDC2010T21. 9ftp://jaguar.ncsl.nist.gov/mt/resources/ mteval-v11b.pl\nAs shown in Table 1, using dependency-driven embedding initialization scheme yields the best improvement over our baseline. On the other hand, random initialization of word embedding yields worse results compared to our baseline, showing a significant drop. Using the skip-gram word embedding model yields average results comparable to the baseline.\nWe are also interested in testing the performance of the dependency-based reordering features in the absence of the conventional phrase-based reordering models. Table 1 shows that the system with dependency distortion penalty (DDP) and sparse dependency swap (DS) features is unable to outperform the system with only conventional phrasebased reordering models (DBR+PBRM). However, our neural classifier approach, without the conventional reordering models, significantly outperforms the conventional reordering models by +1.05 BLEU point."}, {"heading": "Discussion", "text": "Dependency swap features capture the dependency label and POS tag of the two words to be reordered, but not the actual words themselves. While using words as sparse features may result in too many parameters, the continuous word representation in our neural approach alleviates this problem. In addition, the neural network model also learns useful combinations of individual features. While dependency swap features (Hadiwinoto, Liu, and Ng 2016) define features as pairs of dependency label and POS tag, the hidden layer of a NN can dynamically choose the information to take into account for the reordering decision.\nUsing neural classifiers with dependency-based word embedding initialization yields significant improvement, whereas random initialization and skip-gram initialization of word embeddings yield no improvement. This shows the importance of capturing dependency information in the word\nembeddings for reordering. Figure 2 shows the baseline phrase-based SMT system with conventional phrase-based reordering models (DBR+PBRM) and sparse dependency swap features produces an incorrect translation output. The sparse dependency swap features prefer the Chinese words \u201czhiyi (one of)\u201d and \u201czuzhi (organization)\u201d, where \u201czhiyi\u201d is the head word of \u201czuzhi\u201d, to remain in order after translation, based on their dependency labels and POS tags. However, the Chinese expression \u201cNOUN zhiyi\u201d should be swapped in the English translation, resulting in \u201cone of NOUN\u201d10.\nOur experimental results also show that without conventional phrase-based reordering models, the sparse dependency-based features are unable to outperform the conventional reordering models, whereas the neural dependency-based reordering model outperforms the conventional reordering models. This further demonstrates the strength of our dependency-based neural reordering approach.\nOur approach applies syntax to SMT with beam search decoding. This is different from prior approaches requiring chart parsing decoding such as the hierarchical phrasebased (Chiang 2007), tree-to-string (Liu, Liu, and Lin 2006), string-to-tree (Marcu et al. 2006), and tree-to-tree (Zhai et al. 2011) SMT approaches.\nThe end-to-end neural MT (NMT) approach has recently been proposed for MT. However, the most recent NMT papers tested on the same NIST Chinese-to-English test sets (Wang et al. 2016; Zhang et al. 2016) show lower absolute BLEU scores (by 2 to 7 points) compared to our scores. Following the approach of (Junczys-Dowmunt, Dwojak, and\n10The ordering is further aggravated by wrongly swapping \u201cISO\u201d and \u201czuzhi (organization)\u201d, due to the translation output score being the weighted sum of features including LM, which prefers such a translation.\nHoang 2016), our own implemented NMT system (single system without ensemble), when trained on the same corpora and tested on the same NIST test sets in this paper, achieves an average BLEU score of 38.97, lower by 0.58 point compared to our best SMT system (p < 0.01). This shows that our neural dependency-based reordering model outperforms the NMT approach. NMT also requires longer time to train (18 days) compared to our best SMT system (3 days)."}, {"heading": "Related Work", "text": "Phrase-based SMT reordering can utilize the dependency parse of the input sentence. Chang et al. (2009) utilized the traversed paths of dependency labels to guide phrase reordering. Hadiwinoto, Liu, and Ng (2016) introduced a technique to determine the order of two translated words with corresponding source words that are related through the dependency parse during beam search. They defined sparse decoding features to encourage or penalize the reordering of two words, based on the POS tag and dependency relation label of each word, but not the words themselves.\nNeural reordering models have been applied to re-rank translation candidates generated by the translation decoder. Li et al. (2014) introduced a recursive auto-encoder model to represent phrases and determine the phrase orientation probability. Cui, Wang, and Li (2016) introduced long short-term memory (LSTM) recurrent neural networks to predict the translation word orientation probability. These approaches did not use dependency parse and they were not applied directly during decoding.\nSource dependency parse is also used in the pre-ordering approach, which pre-orders words in a source sentence into target word order and then translates the target-ordered source sentence into the target language. While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination. De Gispert, Iglesias, and Byrne (2015) introduced a feed-forward neural network to pre-order the de-\npendency parse tree nodes (words). However, they did not explore dependency-driven embeddings and model ensemble. Miceli-Barone and Attardi (2015) treat pre-ordering as a traversal on the dependency parse tree, guided by a recurrent neural network. In these approaches, the translation possibility is limited to one target ordering. In contrast, applying a neural reordering model jointly with beam search allows for multiple ordering alternatives and interaction with other models, such as the phrase-based reordering models. We can even build multiple neural models (ensemble) and assign a different weight to each of them to optimize translation quality.\nOur neural reordering classifier serves as a decoding feature function in SMT, leveraging the decoding. This is similar to prior work on neural decoding features, i.e., neural language model (Vaswani et al. 2013) and neural joint model (Devlin et al. 2014), a source-augmented language model. However, these features are not about word reordering.\nWhile continuous representation of words is originally defined for words (Mikolov et al. 2013), we also define continuous representation for POS tags, dependency labels, and indicator features. Extending continuous representation to non-word features is also done in neural dependency parsing (Chen and Manning 2014; Andor et al. 2016), which shows better performance by using continuous feature representation over the traditional discrete representation."}, {"heading": "Conclusion", "text": "We have presented a dependency-based reordering approach for phrase-based SMT, guided by neural classifier predictions. It shows that MT can be improved by a neural network approach by not requiring explicit feature combination and by using dependency-driven continuous word representation. Our experiments also show that our neural reordering approach outperforms our prior reordering approach employing sparse dependency-based features."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["D. Andor", "C. Alberti", "D. Weiss", "A. Severyn", "A. Presta", "K. Ganchev", "S. Petrov", "M. Collins"], "venue": "ACL 2016.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Tailoring continuous word representation for dependency parsing", "author": ["M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "ACL 2014 Short Papers.", "citeRegEx": "Bansal et al\\.,? 2014", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "A transition-based system for joint part-of-speech tagging and labeled non-projective dependency parsing", "author": ["B. Bohnet", "J. Nivre"], "venue": "EMNLP-CoNLL 2012.", "citeRegEx": "Bohnet and Nivre,? 2012", "shortCiteRegEx": "Bohnet and Nivre", "year": 2012}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["P.F. Brown", "V.J. Della Pietra", "S.A. Della Pietra", "R.L. Mercer"], "venue": "Computational Linguistics 19(2).", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Discriminative reordering with Chinese grammatical relations features", "author": ["P.-C. Chang", "H. Tseng", "D. Jurafsky", "C.D. Manning"], "venue": "SSST-3.", "citeRegEx": "Chang et al\\.,? 2009", "shortCiteRegEx": "Chang et al\\.", "year": 2009}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["D. Chen", "C.D. Manning"], "venue": "EMNLP 2014.", "citeRegEx": "Chen and Manning,? 2014", "shortCiteRegEx": "Chen and Manning", "year": 2014}, {"title": "Cohesive phrase-based decoding for statistical machine translation", "author": ["C. Cherry"], "venue": "ACL-08: HLT.", "citeRegEx": "Cherry,? 2008", "shortCiteRegEx": "Cherry", "year": 2008}, {"title": "Hierarchical phrase-based translation", "author": ["D. Chiang"], "venue": "Computational Linguistics 33(2).", "citeRegEx": "Chiang,? 2007", "shortCiteRegEx": "Chiang", "year": 2007}, {"title": "LSTM neural reordering feature for statistical machine translation", "author": ["Y. Cui", "S. Wang", "J. Li"], "venue": "NAACL HLT 2016.", "citeRegEx": "Cui et al\\.,? 2016", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Fast and accurate preordering for SMT using neural networks", "author": ["A. de Gispert", "G. Iglesias", "B. Byrne"], "venue": "In NAACL HLT", "citeRegEx": "Gispert et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gispert et al\\.", "year": 2015}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R. Schwartz", "J. Makhoul"], "venue": "ACL 2014.", "citeRegEx": "Devlin et al\\.,? 2014", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A simple and effective hierarchical phrase reordering model", "author": ["M. Galley", "C.D. Manning"], "venue": "EMNLP 2008.", "citeRegEx": "Galley and Manning,? 2008", "shortCiteRegEx": "Galley and Manning", "year": 2008}, {"title": "To swap or not to swap? Exploiting dependency word pairs for reordering in statistical machine translation", "author": ["C. Hadiwinoto", "Y. Liu", "H.T. Ng"], "venue": "AAAI-16.", "citeRegEx": "Hadiwinoto et al\\.,? 2016", "shortCiteRegEx": "Hadiwinoto et al\\.", "year": 2016}, {"title": "Tuning as ranking", "author": ["M. Hopkins", "J. May"], "venue": "EMNLP 2011.", "citeRegEx": "Hopkins and May,? 2011", "shortCiteRegEx": "Hopkins and May", "year": 2011}, {"title": "Source-side preordering for translation using logistic regression and depth-first branch-and-bound search", "author": ["L. Jehl", "A. de Gispert", "M. Hopkins", "W. Byrne"], "venue": null, "citeRegEx": "Jehl et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jehl et al\\.", "year": 2014}, {"title": "Is neural machine translation ready for deployment? A case study on 30 translation directions", "author": ["M. Junczys-Dowmunt", "T. Dwojak", "H. Hoang"], "venue": "CoRR abs/1610.01108.", "citeRegEx": "Junczys.Dowmunt et al\\.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation", "author": ["P. Koehn", "A. Axelrod", "A.B. Mayne", "C. Callison-Burch", "M. Osborne", "D. Talbot"], "venue": "IWSLT 2005.", "citeRegEx": "Koehn et al\\.,? 2005", "shortCiteRegEx": "Koehn et al\\.", "year": 2005}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "the ACL 2007 Demo and Poster Sessions.", "citeRegEx": "Koehn et al\\.,? 2007", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Statistical phrasebased translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "HLT-NAACL 2003.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models", "author": ["P. Koehn"], "venue": "AMTA 2004.", "citeRegEx": "Koehn,? 2004a", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn"], "venue": "EMNLP 2004.", "citeRegEx": "Koehn,? 2004b", "shortCiteRegEx": "Koehn", "year": 2004}, {"title": "Minimum Bayes-risk decoding for statistical machine translation", "author": ["S. Kumar", "W. Byrne"], "venue": "HLT-NAACL 2004.", "citeRegEx": "Kumar and Byrne,? 2004", "shortCiteRegEx": "Kumar and Byrne", "year": 2004}, {"title": "Source-side classifier preordering for machine translation", "author": ["U. Lerner", "S. Petrov"], "venue": "EMNLP 2013.", "citeRegEx": "Lerner and Petrov,? 2013", "shortCiteRegEx": "Lerner and Petrov", "year": 2013}, {"title": "A neural reordering model for phrase-based translation", "author": ["P. Li", "Y. Liu", "M. Sun", "T. Izuha", "D. Zhang"], "venue": "COLING 2014.", "citeRegEx": "Li et al\\.,? 2014", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Tree-to-string alignment template for statistical machine translation", "author": ["Y. Liu", "Q. Liu", "S. Lin"], "venue": "ACLCOLING 2006.", "citeRegEx": "Liu et al\\.,? 2006", "shortCiteRegEx": "Liu et al\\.", "year": 2006}, {"title": "A maximum entropy approach to Chinese word segmentation", "author": ["J.K. Low", "H.T. Ng", "W. Guo"], "venue": "SIGHAN4.", "citeRegEx": "Low et al\\.,? 2005", "shortCiteRegEx": "Low et al\\.", "year": 2005}, {"title": "SPMT: statistical machine translation with syntactified target language phrases", "author": ["D. Marcu", "W. Wang", "A. Echihabi", "K. Knight"], "venue": "EMNLP 2006.", "citeRegEx": "Marcu et al\\.,? 2006", "shortCiteRegEx": "Marcu et al\\.", "year": 2006}, {"title": "Non-projective dependency-based pre-reordering with recurrent neural network for machine translation", "author": ["A.V. Miceli-Barone", "G. Attardi"], "venue": "ACL 2015.", "citeRegEx": "Miceli.Barone and Attardi,? 2015", "shortCiteRegEx": "Miceli.Barone and Attardi", "year": 2015}, {"title": "Efficient estimation of word representation in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "ICLR 2013 Workshop.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["F.J. Och", "H. Ney"], "venue": "ACL 2002.", "citeRegEx": "Och and Ney,? 2002", "shortCiteRegEx": "Och and Ney", "year": 2002}, {"title": "A systematic comparison of various statistical alignment models", "author": ["F.J. Och", "H. Ney"], "venue": "Computational Linguistics 29(1).", "citeRegEx": "Och and Ney,? 2003", "shortCiteRegEx": "Och and Ney", "year": 2003}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "ACL 2002.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(Jun).", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "A unigram orientation model for statistical machine translation", "author": ["C. Tillmann"], "venue": "HLT-NAACL 2004: Short Papers.", "citeRegEx": "Tillmann,? 2004", "shortCiteRegEx": "Tillmann", "year": 2004}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "EMNLP 2013.", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Memoryenhanced decoder for neural machine translation", "author": ["M. Wang", "Z. Lu", "H. Li", "L. Qun"], "venue": "EMNLP 2016.", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple but effective approaches to improving tree-to-tree model", "author": ["F. Zhai", "J. Zhang", "Y. Zhou", "C. Zong"], "venue": "MT Summit XIII.", "citeRegEx": "Zhai et al\\.,? 2011", "shortCiteRegEx": "Zhai et al\\.", "year": 2011}, {"title": "Variational neural machine translation", "author": ["B. Zhang", "D. Xiong", "J. Su", "H. Duan", "M. Zhang"], "venue": "EMNLP 2016.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003), which achieve state-of-the-art performance, generally adopt a reordering model based on the span of a phrase and the span of its adjacent phrase (Tillmann 2004; Koehn et al. 2005; Galley and Manning 2008).", "startOffset": 199, "endOffset": 258}, {"referenceID": 16, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003), which achieve state-of-the-art performance, generally adopt a reordering model based on the span of a phrase and the span of its adjacent phrase (Tillmann 2004; Koehn et al. 2005; Galley and Manning 2008).", "startOffset": 199, "endOffset": 258}, {"referenceID": 11, "context": "Phrase-based MT systems (Koehn, Och, and Marcu 2003), which achieve state-of-the-art performance, generally adopt a reordering model based on the span of a phrase and the span of its adjacent phrase (Tillmann 2004; Koehn et al. 2005; Galley and Manning 2008).", "startOffset": 199, "endOffset": 258}, {"referenceID": 4, "context": "The dependency parse tree of a source sentence can be utilized in reordering integrated within phrasebased statistical MT (SMT), by defining dependency-based features in the SMT log-linear model (Chang et al. 2009; Hadiwinoto, Liu, and Ng 2016).", "startOffset": 195, "endOffset": 244}, {"referenceID": 23, "context": "has found application in MT reordering, applied in the reranking of translation output candidates (Li et al. 2014; Cui, Wang, and Li 2016) and in the pre-ordering approach (reordering the source sentence before translation) (de Gispert, Iglesias, and Byrne 2015; Miceli-Barone and Attardi 2015).", "startOffset": 98, "endOffset": 138}, {"referenceID": 27, "context": "2014; Cui, Wang, and Li 2016) and in the pre-ordering approach (reordering the source sentence before translation) (de Gispert, Iglesias, and Byrne 2015; Miceli-Barone and Attardi 2015).", "startOffset": 115, "endOffset": 185}, {"referenceID": 28, "context": "This scheme is a modified skip-gram model, which given an input word, predicts its context (surrounding words), resulting in a mapping such that words with similar surrounding words have similar continuous vector representations (Mikolov et al. 2013).", "startOffset": 229, "endOffset": 250}, {"referenceID": 32, "context": "To prevent model overfitting, we used the dropout strategy (Srivastava et al. 2014) on the input embedding layer.", "startOffset": 59, "endOffset": 83}, {"referenceID": 19, "context": "We adopt the phrase-based SMT approach, using a beam search decoding algorithm (Koehn 2004a).", "startOffset": 79, "endOffset": 92}, {"referenceID": 29, "context": "The translation output is picked based on a score computed by a log-linear model, comprising the weighted sum of feature function values (Och and Ney 2002).", "startOffset": 137, "endOffset": 155}, {"referenceID": 33, "context": "The latter comprises the phrase-based lexicalized reordering (PBLR) model (Tillmann 2004; Koehn et al. 2005) and the hierarchical reordering (HR) model (Galley and Manning 2008).", "startOffset": 74, "endOffset": 108}, {"referenceID": 16, "context": "The latter comprises the phrase-based lexicalized reordering (PBLR) model (Tillmann 2004; Koehn et al. 2005) and the hierarchical reordering (HR) model (Galley and Manning 2008).", "startOffset": 74, "endOffset": 108}, {"referenceID": 11, "context": "2005) and the hierarchical reordering (HR) model (Galley and Manning 2008).", "startOffset": 49, "endOffset": 74}, {"referenceID": 6, "context": "To encourage structural cohesion during translation, we add a dependency distortion penalty (DDP) feature (Cherry 2008) to discourage translation output in which the translated words of a phrase in a source dependency parse subtree are split.", "startOffset": 106, "endOffset": 119}, {"referenceID": 17, "context": "We conducted experiments on a phrase-based Chinese-toEnglish SMT system built using Moses (Koehn et al. 2007).", "startOffset": 90, "endOffset": 109}, {"referenceID": 30, "context": "Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al.", "startOffset": 51, "endOffset": 69}, {"referenceID": 3, "context": "Then the parallel corpus is word-aligned by GIZA++ (Och and Ney 2003) using IBM Models 1, 3, and 4 (Brown et al. 1993)4.", "startOffset": 99, "endOffset": 118}, {"referenceID": 13, "context": "Weight tuning is done by using the pairwise ranked optimization (PRO) algorithm (Hopkins and May 2011).", "startOffset": 80, "endOffset": 102}, {"referenceID": 2, "context": "We parse the Chinese sentences by the Mate parser, which jointly performs POS tagging and dependency parsing (Bohnet and Nivre 2012), trained on Chinese Treebank (CTB) version 8.", "startOffset": 109, "endOffset": 132}, {"referenceID": 21, "context": "To pick the translation output, we also use n-best minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) instead of the default maximum a-posteriori (MAP) decoding.", "startOffset": 85, "endOffset": 107}, {"referenceID": 31, "context": "The translation quality of the system output is measured by case-insensitive BLEU (Papineni et al. 2002), for which the brevity penalty is computed based on the shortest reference (NIST-BLEU)9.", "startOffset": 82, "endOffset": 104}, {"referenceID": 20, "context": "Statistical significance testing between systems is conducted by bootstrap resampling (Koehn 2004b).", "startOffset": 86, "endOffset": 99}, {"referenceID": 28, "context": "Additional experiments use two other initialization schemes: (1) random initialization and (2) the original skip-gram model of (Mikolov et al. 2013) with a window size of 5.", "startOffset": 127, "endOffset": 148}, {"referenceID": 33, "context": "We used the following prior reordering features as baseline: (1) distance-based reordering (DBR) (Koehn, Och, and Marcu 2003); (2) phrase-based reordering models (PBRM), comprising phrase-based lexicalized reordering (Tillmann 2004; Koehn et al. 2005) and hierarchical reordering (Galley and Manning 2008); (3) dependency distortion penalty (DDP) (Cherry 2008); and (4) sparse dependency swap features (DS) (Hadiwinoto, Liu, and Ng 2016).", "startOffset": 217, "endOffset": 251}, {"referenceID": 16, "context": "We used the following prior reordering features as baseline: (1) distance-based reordering (DBR) (Koehn, Och, and Marcu 2003); (2) phrase-based reordering models (PBRM), comprising phrase-based lexicalized reordering (Tillmann 2004; Koehn et al. 2005) and hierarchical reordering (Galley and Manning 2008); (3) dependency distortion penalty (DDP) (Cherry 2008); and (4) sparse dependency swap features (DS) (Hadiwinoto, Liu, and Ng 2016).", "startOffset": 217, "endOffset": 251}, {"referenceID": 11, "context": "2005) and hierarchical reordering (Galley and Manning 2008); (3) dependency distortion penalty (DDP) (Cherry 2008); and (4) sparse dependency swap features (DS) (Hadiwinoto, Liu, and Ng 2016).", "startOffset": 34, "endOffset": 59}, {"referenceID": 6, "context": "2005) and hierarchical reordering (Galley and Manning 2008); (3) dependency distortion penalty (DDP) (Cherry 2008); and (4) sparse dependency swap features (DS) (Hadiwinoto, Liu, and Ng 2016).", "startOffset": 101, "endOffset": 114}, {"referenceID": 7, "context": "This is different from prior approaches requiring chart parsing decoding such as the hierarchical phrasebased (Chiang 2007), tree-to-string (Liu, Liu, and Lin 2006), string-to-tree (Marcu et al.", "startOffset": 110, "endOffset": 123}, {"referenceID": 26, "context": "This is different from prior approaches requiring chart parsing decoding such as the hierarchical phrasebased (Chiang 2007), tree-to-string (Liu, Liu, and Lin 2006), string-to-tree (Marcu et al. 2006), and tree-to-tree (Zhai et al.", "startOffset": 181, "endOffset": 200}, {"referenceID": 36, "context": "2006), and tree-to-tree (Zhai et al. 2011) SMT approaches.", "startOffset": 24, "endOffset": 42}, {"referenceID": 35, "context": "However, the most recent NMT papers tested on the same NIST Chinese-to-English test sets (Wang et al. 2016; Zhang et al. 2016) show lower absolute BLEU scores (by 2 to 7 points) compared to our scores.", "startOffset": 89, "endOffset": 126}, {"referenceID": 37, "context": "However, the most recent NMT papers tested on the same NIST Chinese-to-English test sets (Wang et al. 2016; Zhang et al. 2016) show lower absolute BLEU scores (by 2 to 7 points) compared to our scores.", "startOffset": 89, "endOffset": 126}, {"referenceID": 4, "context": "Chang et al. (2009) utilized the traversed paths of dependency labels to guide phrase reordering.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Chang et al. (2009) utilized the traversed paths of dependency labels to guide phrase reordering. Hadiwinoto, Liu, and Ng (2016) introduced a technique to determine the order of two translated words with corresponding source words that are related through the dependency parse during beam search.", "startOffset": 0, "endOffset": 129}, {"referenceID": 23, "context": "Li et al. (2014) introduced a recursive auto-encoder model to represent phrases and determine the phrase orientation probability.", "startOffset": 0, "endOffset": 17}, {"referenceID": 23, "context": "Li et al. (2014) introduced a recursive auto-encoder model to represent phrases and determine the phrase orientation probability. Cui, Wang, and Li (2016) introduced long short-term memory (LSTM) recurrent neural networks to predict the translation word orientation probability.", "startOffset": 0, "endOffset": 155}, {"referenceID": 22, "context": "While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination.", "startOffset": 85, "endOffset": 127}, {"referenceID": 14, "context": "While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination.", "startOffset": 85, "endOffset": 127}, {"referenceID": 14, "context": "While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination. De Gispert, Iglesias, and Byrne (2015) introduced a feed-forward neural network to pre-order the dependency parse tree nodes (words).", "startOffset": 110, "endOffset": 242}, {"referenceID": 14, "context": "While the preordering step typically utilizes a classifier with feature combinations (Lerner and Petrov 2013; Jehl et al. 2014), a neural network can replace the classifier to avoid feature combination. De Gispert, Iglesias, and Byrne (2015) introduced a feed-forward neural network to pre-order the dependency parse tree nodes (words). However, they did not explore dependency-driven embeddings and model ensemble. Miceli-Barone and Attardi (2015) treat pre-ordering as a traversal on the dependency parse tree, guided by a recurrent neural network.", "startOffset": 110, "endOffset": 449}, {"referenceID": 34, "context": ", neural language model (Vaswani et al. 2013) and neural joint model (Devlin et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 10, "context": "2013) and neural joint model (Devlin et al. 2014), a source-augmented language model.", "startOffset": 29, "endOffset": 49}, {"referenceID": 28, "context": "While continuous representation of words is originally defined for words (Mikolov et al. 2013), we also define continuous representation for POS tags, dependency labels, and indicator features.", "startOffset": 73, "endOffset": 94}, {"referenceID": 5, "context": "Extending continuous representation to non-word features is also done in neural dependency parsing (Chen and Manning 2014; Andor et al. 2016), which shows better performance by using continuous feature representation over the traditional discrete representation.", "startOffset": 99, "endOffset": 141}, {"referenceID": 0, "context": "Extending continuous representation to non-word features is also done in neural dependency parsing (Chen and Manning 2014; Andor et al. 2016), which shows better performance by using continuous feature representation over the traditional discrete representation.", "startOffset": 99, "endOffset": 141}], "year": 2017, "abstractText": "In machine translation (MT) that involves translating between two languages with significant differences in word order, determining the correct word order of translated words is a major challenge. The dependency parse tree of a source sentence can help to determine the correct word order of the translated words. In this paper, we present a novel reordering approach utilizing a neural network and dependency-based embeddings to predict whether the translations of two source words linked by a dependency relation should remain in the same order or should be swapped in the translated sentence. Experiments on Chinese-to-English translation show that our approach yields a statistically significant improvement of 0.57 BLEU point on benchmark NIST test sets, compared to our prior state-of-theart statistical MT system that uses sparse dependency-based reordering features.", "creator": "TeX"}}}