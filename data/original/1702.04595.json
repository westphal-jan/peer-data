{"id": "1702.04595", "review": {"conference": "iclr", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2017", "title": "Visualizing Deep Neural Network Decisions: Prediction Difference Analysis", "abstract": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "histories": [["v1", "Wed, 15 Feb 2017 13:25:26 GMT  (7723kb,D)", "http://arxiv.org/abs/1702.04595v1", "ICLR2017"]], "COMMENTS": "ICLR2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["luisa m zintgraf", "taco s cohen", "tameem adel", "max welling"], "accepted": true, "id": "1702.04595"}, "pdf": {"name": "1702.04595.pdf", "metadata": {"source": "CRF", "title": "VISUALIZING DEEP NEURAL NETWORK DECISIONS: PREDICTION DIFFERENCE ANALYSIS", "authors": ["Luisa M Zintgraf", "Taco S Cohen", "Tameem Adel", "Max Welling"], "emails": ["lmzintgraf@gmail.com,", "tameem.hesham@gmail.com,", "m.welling}@uva.nl"], "sections": [{"heading": "1 INTRODUCTION", "text": "Over the last few years, deep neural networks (DNNs) have emerged as the method of choice for perceptual tasks such as speech recognition and image classification. In essence, a DNN is a highly complex non-linear function, which makes it hard to understand how a particular classification comes about. This lack of transparency is a significant impediment to the adoption of deep learning in areas of industry, government and healthcare where the cost of errors is high.\nIn order to realize the societal promise of deep learning - e.g., through self-driving cars or personalized medicine - it is imperative that classifiers learn to explain their decisions, whether it is in the lab, the clinic, or the courtroom. In scientific applications, a better understanding of the complex dependencies learned by deep networks could lead to new insights and theories in poorly understood domains.\nIn this paper, we present a new, probabilistically sound methodology for explaining classification decisions made by deep neural networks. The method can be used to produce a saliency map for each (instance, node) pair that highlights the parts (features) of the input that constitute most evidence for or against the activation of the given (internal or output) node. See figure 1 for an example.\nIn the following two sections, we review related work and then present our approach. In section 4 we provide several demonstrations of our technique for deep convolutional neural networks (DCNNs) trained on ImageNet data, and further how the method can be applied when classifying MRI brain scans of HIV patients with neurodegenerative disease.\nar X\niv :1\n70 2.\n04 59\n5v 1\n[ cs\n.C V\n] 1\n5 Fe\nb 20\n17"}, {"heading": "2 RELATED WORK", "text": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper.\nOne such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation. They also show that there is a close connection to using deconvolutional networks for visualization, proposed by Zeiler & Fergus (2014). Other methods include Shrikumar et al. (2016), who compare the activation of a unit when a specific input is fed forward through the net to a reference activation for that unit. Zhou et al. (2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above. The idea of our method is similar to another analysis Zeiler & Fergus (2014) make: they estimate the importance of input pixels by visualizing the probability of the (correct) class as a function of a gray patch occluding parts of the image. In this paper, we take a more rigorous approach at both removing information from the image and evaluating the effect of this.\nIn the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007). These are independent of the input image, and, as argued by Gaonkar & Davatzikos (2013) and Haufe et al. (2014), interpreting these weights can be misleading in general.\nThe work presented in this paper is based on an instance-specific method by Robnik-\u0160ikonja & Kononenko (2008), the prediction difference analysis, which is reviewed in the next section. Our main contributions are three substantial improvements of this method: conditional sampling (section 3.1), multivariate analysis (section 3.2), and deep visualization (section 3.3)."}, {"heading": "3 APPROACH", "text": "Our method is based on the technique presented by Robnik-\u0160ikonja & Kononenko (2008), which we will now review. For a given prediction, the method assigns a relevance value to each input feature with respect to a class c. The basic idea is that the relevance of a feature xi can be estimated by measuring how the prediction changes if the feature is unknown, i.e., the difference between p(c|x) and p(c|x\\i), where x\\i denotes the set of all input features except xi.\nTo find p(c|x\\i), i.e., evaluate the prediction when a feature is unknown, the authors propose three strategies. The first is to label the feature as unknown (which only few classifiers allow). The second is to re-train the classifier with the feature left out (which is clearly infeasible for DNNs and high-dimensional data like images). The third approach is to simulate the absence of a feature by marginalizing the feature:\np(c|x\\i) = \u2211 xi p(xi|x\\i)p(c|x\\i, xi) (1)\n(with the sum running over all possible values for xi). However, modeling p(xi|x\\i) can easily become infeasible with a large number of features. Therefore, the authors approximate equation (1) by assuming that feature xi is independent of the other features, x\\i:\np(c|x\\i) \u2248 \u2211 xi p(xi)p(c|x\\i, xi) . (2)\nThe prior probability p(xi) is usually approximated by the empirical distribution for that feature.\nOnce the class probability p(c|x\\i) is estimated, it can be compared to p(c|x). We stick to an evaluation proposed by the authors referred to as weight of evidence, given by\nWEi(c|x) = log2 (odds(c|x))\u2212 log2 ( odds(c|x\\i) ) , (3)\nAlgorithm 1 Evaluating the prediction difference using conditional and multivariate sampling Input: classifier with outputs p(c|x), input image x of size n\u00d7 n, inner patch size k, outer patch size l > k, class of interest c, probabilistic model over patches of size l \u00d7 l, number of samples S Initialization: WE = zeros(n*n), counts = zeros(n*n) for every patch xw of size k \u00d7 k in x do\nx\u2032 = copy(x) sumw = 0 define patch x\u0302w of size l \u00d7 l that contains xw for s = 1 to S do\nx\u2032w \u2190 xw sampled from p(xw|x\u0302w\\xw) sumw += p(c|x\u2032) . evaluate classifier\nend for p(c|x\\xw) := sumw/S WE[coordinates of xw] += log2(odds(c|x))\u2212 log2(odds(c|x\\xw)) counts[coordinates of xw] += 1\nend for Output: WE / counts . point-wise division\nwhere odds(c|x) = p(c|x)/(1 \u2212 p(c|x)). To avoid problems with zero probabilities, Laplace correction p\u2190 (pN + 1)/(N +K) is used, where N is the number of training instances and K the number of classes.\nThe method produces a relevance vector (WEi)i=1...m (m being the number of features) of the same size as the input, which reflects the relative importance of all features. A large prediction difference means that the feature contributed substantially to the classification, whereas a small difference indicates that the feature was not important for the decision. A positive value WEi means that the feature has contributed evidence for the class of interest: removing it would decrease the confidence of the classifier in the given class. A negative value on the other hand indicates that the feature displays evidence against the class: removing it also removes potentially conflicting or irritating information and the classifier becomes more certain in the investigated class."}, {"heading": "3.1 CONDITIONAL SAMPLING", "text": "In equation (3), the conditional probability p(xi|x\\i) of a feature xi is approximated using the marginal distribution p(xi). This is a very crude approximation. In images for example, a pixel\u2019s value is highly dependent on other pixels. We propose a much more accurate approximation, based on the following two observations: a pixel depends most strongly on a small neighborhood around it, and the conditional of a pixel given its neighborhood does not depend on the position of the pixel in the image. For a pixel xi, we can therefore find a patch x\u0302i of size l\u00d7 l that contains xi, and condition on the remaining pixels in that patch:\np(xi|x\\i) \u2248 p(xi|x\u0302\\i) . (4)\nThis greatly improves the approximation while remaining completely tractable.\nFor a feature to become relevant when using conditional sampling, it now has to satisfy two conditions: being relevant to predict the class of interest, and be hard to predict from the neighboring pixels. Relative to the marginal method, we therefore downweight the pixels that can easily be predicted and are thus redundant in this sense."}, {"heading": "3.2 MULTIVARIATE ANALYSIS", "text": "Robnik-\u0160ikonja & Kononenko (2008) take a univariate approach: only one feature at a time is removed. However, we would expect that a neural network is relatively robust to just one feature of a high-dimensional input being unknown, like a pixel in an image. Therefore, we will remove several features at once by again making use of our knowledge about images by strategically choosing these feature sets: patches of connected pixels. Instead of going through all individual pixels, we go through all patches of size k \u00d7 k in the image (k \u00d7 k \u00d7 3 for RGB images and k \u00d7 k \u00d7 k for 3D images like MRI scans), implemented in a sliding window fashion. The patches are overlapping, so that ultimately an individual pixel\u2019s relevance is obtained by taking the average relevance obtained from the different patches it was in.\nAlgorithm 1 and figure 2 illustrate how the method can be implemented, incorporating the proposed improvements."}, {"heading": "3.3 DEEP VISUALIZATION OF HIDDEN LAYERS", "text": "When trying to understand neural networks and how they make decisions, it is not only interesting to analyze the input-output relation of the classifier, but also to look at what is going on inside the hidden layers of the network. We can adapt the method to see how the units of any layer of the network influence a node from a deeper layer. Mathematically, we can formulate this as follows. Let h be the vector representation of the values in a layer H in the network (after forward-propagating the input up to this layer). Further, let z = z(h) be the value of a node that depends on h, i.e., a node in a subsequent layer. Then the analog of equation (2) is given by the expectation:\ng(z|h\\i) \u2261 Ep(hi|h\\i) [z(h)] = \u2211 hi p(hi|h\\i)z(h\\i, hi) , (5)\nwhich expresses the distribution of z when unit hi in layer H is unobserved. The equation now works for arbitrary layer/unit combinations, and evaluates to the same as equation (1) when the input-output relation is analyzed. To evaluate the difference between g(z|h) and g(z|h\\i), we will in general use the activation difference, ADi(z|h) = g(z|h)\u2212 g(z|h\\i) , for the case when we are not dealing with probabilities (and equation (3) is not applicable)."}, {"heading": "4 EXPERIMENTS", "text": "In this section, we illustrate how the proposed visualization method can be applied, on the ImageNet dataset of natural images when using DCNNs (section 4.1), and on a medical imaging dataset of MRI scans when using a logistic regression classifier (section 4.2). For marginal sampling we always use the empirical distribution, i.e., we replace a feature (patch) with samples taken directly from other images, at the same location. For conditional sampling we use a multivariate normal distribution. For both sampling methods we use 10 samples to estimate p(c|x\\i) (since no significant difference was observed with more samples). Note that all images are best viewed digital and in color.\nOur implementation is available at github.com/lmzintgraf/DeepVis-PredDiff."}, {"heading": "4.1 IMAGENET: UNDERSTANDING HOW A DCNN MAKES DECISIONS", "text": "We use images from the ILSVRC challenge (Russakovsky et al., 2015) (a large dataset of natural images from 1000 categories) and three DCNNs: the AlexNet (Krizhevsky et al., 2012), the GoogLeNet (Szegedy et al., 2015) and the (16-layer) VGG network (Simonyan & Zisserman, 2014). We used the publicly available pre-trained models that were implemented using the deep learning framework caffe (Jia et al., 2014). Analyzing one image took us on average 20, 30 and 70 minutes for the respective classifiers AlexNet, GoogLeNet and VGG (using the GPU implementation of caffe and mini-batches with the standard settings of 10 samples and a window size of k = 10).\nThe results shown here are chosen from among a small set of images in order to show a range of behavior of the algorithm. The shown images are quite representative of the performance of the method in general. Examples on randomly selected images, including a comparison to the sensitivity analysis of Simonyan et al. (2013), can be seen in appendix A.\nWe start this section by demonstrating our proposed improvements (sections 3.1 - 3.3).\nMarginal vs Conditional Sampling\nFigure 3 shows visualizations of the spatial support for the highest scoring class, using marginal and conditional sampling (with k = 10 and l = 14). We can see that conditional sampling leads to results that are more refined in the sense that they concentrate more around the object. We can also see that marginal sampling leads to pixels being declared as important that are very easily predictable conditioned on their neighboring pixels (like in the saxophone example). Throughout our experiments, we have found that conditional sampling tends to give more specific and fine-grained results than marginal sampling. For the rest of our experiments, we therefore show results using conditional sampling only.\nMultivariate Analysis\nFor ImageNet data, we have observed that setting k = 10 gives a good trade-off between sharp results and a smooth appearance. Figure 4 shows how different window sizes influence the resolution of the visualization. Surprisingly, removing only one pixel does have a measurable effect on the prediction, and the largest effect comes from sensitive pixels. We expected that removing only one pixel does not have any effect on the classification outcome, but apparently the classifier is sensitive even to these small changes. However when using such a small window size, it is difficult to make sense of the sign information in the visualization. If we want to get a good impression of which parts in the image are evidence for/against a class, it is therefore better to use larger windows. If k is chosen too large however, the results tend to get blurry. Note that these results are not just simple averages of one another, but a multivariate approach is indeed necessary to observe the presented results.\nDeep Visualization of Hidden Network Layers\nOur third main contribution is the extension of the method to neural networks; to understand the role of hidden layers in a DNN. Figure 5 shows how different feature maps in three different layers of the GoogLeNet react to the input of a tabby cat (see figure 6, middle image). For each feature map in a convolutional layer, we first compute the relevance of the input image for each hidden unit in that map. To estimate what the feature map as a whole is doing, we show the average of the relevance vectors over all units in that feature map. The first convolutional layer works with different types of simple image filters (e.g., edge detectors), and what we see is which parts of the input image respond\npositively or negatively to these filters. The layer we picked from somewhere in the middle of the network is specialized to higher level features (like facial features of the cat). The activations of the last convolutional layer are very sparse across feature channels, indicating that these units are highly specialized.\nTo get a sense of what single feature maps in convolutional layers are doing, we can look at their visualization for different input images and look for patterns in their behavior. Figure 6 shows this for four different feature maps from a layer from the middle of the GoogLeNet network. We can directly see which kind of features the model has learned at this stage in the network. For example, one feature map is mostly activated by the eyes of animals (third row), and another is looking mostly at the background (last row).\nPenultimate vs Output Layer\nIf we visualize the influence of the input features on the penultimate (pre-softmax) layer, we show only the evidence for/against this particular class, without taking other classes into consideration. After the softmax operation however, the values of the nodes are all interdependent: a drop in the probability for one class could be due to less evidence for it, or because a different class becomes more likely. Figure 7 compares visualizations for the last two layers. By looking at the top three scoring classes, we can see that the visualizations in the penultimate layer look very similar if the classes are similar (like different dog breeds). When looking at the output layer however, they look rather different. Consider the case of the elephants: the top three classes are different elephant subspecies, and the visualizations of the penultimate layer look similar since every subspecies can be identified by similar characteristics. But in the output layer, we can see how the classifier decides for one of the three types of elephants and against the others: the ears in this case are the crucial difference.\nNetwork Comparison\nWhen analyzing how neural networks make decisions, we can also compare how different network architectures influence the visualization. Here, we tested our method on the AlexNet, the GoogLeNet and the VGG network. Figure 8 shows the results for the three different networks, on two input images. The AlexNet seems to more on contextual information (the sky in the balloon image), which could be attributed to it having the least complex architecture compared to the other two networks. It is also interesting to see that the VGG network deems the basket of the balloon as very important compared to all other pixels. The second highest scoring class in this case was a parachute - presumably, the network learned to not confuse a balloon with a parachute by detecting a square basket (and not a human)."}, {"heading": "4.2 MRI DATA: EXPLAINING CLASSIFIER DECISIONS IN MEDICAL IMAGING", "text": "To illustrate how our visualization method can also be useful in a medical domain, we show some experimental results on an MRI dataset of HIV and healthy patients. In such settings, it is crucial that the practitioner has some insight into the algorithm\u2019s decision when classifying a patient, to weigh this information and incorporate it in the overall diagnosis process.\nThe dataset used here is referred to as the COBRA dataset. It contains 3D MRIs from 100 HIV patients and 70 healthy individuals, included in the Academic Medical Center (AMC) in Amsterdam, The Netherlands. Of these subjects, diffusion weighted MRI data were acquired. Preprocessing of the data was performed with software developed in-house, using the HPCN-UvA Neuroscience Gateway and using resources of the Dutch e-Science Grid Shahand et al. (2015). As a result, Fractional Anisotropy (FA) maps were computed. FA is sensitive to microstructural damage and therefore expected to be, on average, decreased in patients. Subjects were scanned on two 3.0 Tesla scanner systems, 121 subjects on a Philips Intera system and 39 on a Philips Ingenia system. Patients and controls were evenly distributed. FA images were spatially normalized to standard space Andersson et al. (2007), resulting in volumes with 91\u00d7 109\u00d7 91 = 902, 629 voxels.\nWe trained an L2-regularized Logistic Regression classifier on a subset of the MRI slices (slices 29-40 along the first axis) and on a balanced version of the dataset (by taking the first 70 samples of the HIV class) to achieve an accuracy of 69.3% in a 10-fold cross-validation test. Analyzing one image took around half an hour (on a CPU, with k = 3 and l = 7, see algorithm 1). For conditional sampling, we also tried adding location information in equation (2), i.e., we split up the 3D image into a 20 \u00d7 20 \u00d7 20 grid and also condition on the index in that grid. We found that this slightly improved the interpretability of the results, since the pixel values in the special case of MRI scans does depend on spacial location as well.\nFigure 9 (first row) shows one way via which the prediction difference results could be presented to a physician, for an HIV sample. By overlapping the prediction difference and the MRI image, the exact regions can be pointed out that are evidence for (red parts) or against (blue parts) the classifier\u2019s decision. The second row shows the results using the weights of the logistic regression classifier, which is a commonly used method in neuroscientific literature. We can see that they are considerably noisier (in the sense that, compared to our method, the voxels relevant for the classification decisions are more scattered), and also, they are not specific to the given image. Figure 10 shows the visualization results for four healthy, and four HIV samples. We can clearly see that the patterns for the two classes are distinct, and there is some pattern to the decision of the classifier, but which is still specific to the input image. Figure 11 shows the same (HIV) sample as in figure 9 along different axes, and figure 12 shows how the visualization changes with different patch sizes. We believe that both varying the slice and patch size can give different insights to a clinician, and in clinical practice, a 3D animation where these parameters can be adjusted would be very useful for analyzing the visualization result.\nIn general we can assume that the better the classifier, the closer the explanations for its decisions are to the true class difference. For clinical practice it is therefore crucial to have very good classifiers. This will increase computation time, but in many medical settings, longer waiting times for test results are common and worth the wait if the patient is not in an acute life threatening condition (e.g., when predicting HIV or Alzheimer from MRI scans, or the field of cancer diagnosis and detection). The presented results here are for demonstration purposes of the visualization method, and we claim no medical validity. A thorough qualitative analysis incorporating expert knowledge was outside the scope of this paper."}, {"heading": "5 FUTURE WORK", "text": "In our experiments, we used a simple multivariate normal distribution for conditional sampling. We can imagine that using more sophisticated generative models will lead to better results: pixels that are easily predictable by their surrounding are downweighted even more. However this will also significantly increase the computational resources needed to produce the explanations. Similarly, we could try to modify equation (4) to get an even better approximation by using a conditional distribution that takes more information about the whole image into account (like adding spatial information for the MRI scans).\nTo make the method applicable for clinical analysis and practice, a better classification algorithm is required. Also, software that visualizes the results as an interactive 3D model will improve the usability of the system."}, {"heading": "6 CONCLUSION", "text": "We presented a new method for visualizing deep neural networks that improves on previous methods by using a more powerful conditional, multivariate model. The visualization method shows which pixels of a specific input image are evidence for or against a node in the network. The signed information offers new insights - for research on the networks, as well as the acceptance and usability in domains like healthcare. While our method requires significant computational resources, real-time 3D visualization is possible when visualizations are pre-computed. With further optimization and powerful GPUs, pre-computation time can be reduced a lot further. In our experiments, we have presented several ways in which the visualization method can be put into use for analyzing how DCNNs make decisions."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by AWS in Education Grant award. We thank Facebook and Google for financial support, and our reviewers for their time and valuable, constructive feedback.\nThis work was also in part supported by: Innoviris, the Brussels Institute for Research and Innovation, Brussels, Belgium; the Nuts-OHRA Foundation (grant no. 1003-026), Amsterdam, The Netherlands; The Netherlands Organization for Health Research and Development (ZonMW) together with AIDS Fonds (grant no 300020007 and 2009063). Additional unrestricted scientific grants were received from Gilead Sciences, ViiV Healthcare, Janssen Pharmaceutica N.V., Bristol-Myers Squibb, Boehringer Ingelheim, and Merck&Co.\nWe thank Barbara Elsenga, Jane Berkel, Sandra Moll, Maja Tott\u00e9, and Marjolein Martens for running the AGEhIV study program and capturing our data with such care and passion. We thank Yolanda Ruijs-Tiggelman, Lia Veenenberg-Benschop, Sima Zaheri, and Mariska Hillebregt at the HIV Monitoring Foundation for their contributions to data management. We thank Aafien Henderiks and Hans-Erik Nobel for their advice on logistics and organization at the Academic Medical Center. We thank all HIV-physicians and HIV-nurses at the Academic Medical Center for their efforts to include the HIV-infected participants into the AGEhIV Cohort Study, and the Municipal Health Service Amsterdam personnel for their efforts to include the HIV-uninfected participants into the AGEhIV Cohort Study. We thank all study participants without whom this research would not be possible.\nAGEhIV Cohort Study Group. Scientific oversight and coordination: P. Reiss (principal investigator), F.W.N.M. Wit, M. van der Valk, J. Schouten, K.W. Kooij, R.A. van Zoest, E. Verheij, B.C. Elsenga (Academic Medical Center (AMC), Department of Global Health and Amsterdam Institute for Global Health and Development (AIGHD)). M. Prins (co-principal investigator), M.F. Schim van der Loeff, M. Martens, S. Moll, J. Berkel, M. Tott\u00e9, G.R. Visser, L. May, S. Kovalev, A. Newsum, M. Dijkstra (Public Health Service of Amsterdam, Department of Infectious Diseases). Datamanagement: S. Zaheri, M.M.J. Hillebregt, Y.M.C. Ruijs, D.P. Benschop, A. el Berkaoui (HIV Monitoring Foundation). Central laboratory support: N.A. Kootstra, A.M. Harskamp-Holwerda, I. Maurer, T. Booiman, M.M. Mangas Ruiz, A.F. Girigorie, B. Boeser-Nunnink (AMC, Laboratory for Viral Immune Pathogenesis and Department of Experimental Immunology). Project management and administrative support: W. Zikkenheiner, F.R. Janssen (AIGHD). Participating HIV physicians and nurses: S.E. Geerlings, M.H. Godfried, A. Goorhuis, J.W.R. Hovius, J.T.M. van der Meer, F.J.B. Nellen, T. van der Poll, J.M. Prins, P. Reiss, M. van der Valk, W.J. Wiersinga, M. van Vugt, G. de Bree, F.W.N.M. Wit; J. van Eden, A.M.H. van Hes, M. Mutschelknauss , H.E. Nobel, F.J.J. Pijnappel, M. Bijsterveld, A. Weijsenfeld, S. Smalhout (AMC, Division of Infectious Diseases). Other collaborators: J. de Jong, P.G. Postema (AMC, Department of Cardiology); P.H.L.T. Bisschop, M.J.M. Serlie (AMC, Division of Endocrinology and Metabolism); P. Lips (Free University Medical Center Amsterdam); E. Dekker (AMC, Department of Gastroenterology); N. van der Velde (AMC, Division of Geriatric Medicine); J.M.R. Willemsen, L. Vogt (AMC, Division of Nephrology); J. Schouten, P. Portegies, B.A. Schmand, G.J. Geurtsen (AMC, Department of Neurology); F.D. Verbraak, N. Demirkaya (AMC, Department of Ophthalmology); I. Visser (AMC, Department of Psychiatry); A. Schad\u00e9 (Free University Medical Center Amsterdam, Department of Psychiatry); P.T. Nieuwkerk, N. Langebeek (AMC, Department of Medical Psychology); R.P. van Steenwijk, E. Dijkers (AMC, Department of Pulmonary medicine); C.B.L.M. Majoie, M.W.A. Caan, T. Su (AMC, Department of Radiology); H.W. van Lunsen, M.A.F. Nievaard (AMC, Department of Gynaecology); B.J.H. van den Born, E.S.G. Stroes, (AMC, Division of Vascular Medicine); W.M.C. Mulder (HIV Vereniging Nederland)."}, {"heading": "A RANDOM RESULTS", "text": ""}], "references": [{"title": "Non-linear optimisation. fmrib technical report tr07ja1", "author": ["Jesper LR Andersson", "Mark Jenkinson", "Stephen Smith"], "venue": "University of Oxford FMRIB Centre: Oxford,", "citeRegEx": "Andersson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Andersson et al\\.", "year": 2007}, {"title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "author": ["Sebastian Bach", "Alexander Binder", "Gr\u00e9goire Montavon", "Frederick Klauschen", "Klaus-Robert M\u00fcller", "Wojciech Samek"], "venue": "PloS one,", "citeRegEx": "Bach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bach et al\\.", "year": 2015}, {"title": "Describing the brain in autism in five dimensions\u2014magnetic resonance imaging-assisted diagnosis of autism spectrum disorder using a multiparameter classification approach", "author": ["Christine Ecker", "Andre Marquand", "Janaina Mour\u00e3o-Miranda", "Patrick Johnston", "Eileen M Daly", "Michael J Brammer", "Stefanos Maltezos", "Clodagh M Murphy", "Dene Robertson", "Steven C Williams"], "venue": "The Journal of Neuroscience,", "citeRegEx": "Ecker et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ecker et al\\.", "year": 2010}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "Dept. IRO, Universite\u0301 de Montre\u0301al, Tech. Rep,", "citeRegEx": "Erhan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2009}, {"title": "Analytic estimation of statistical significance maps for support vector machine based multi-variate image analysis and classification", "author": ["Bilwaj Gaonkar", "Christos Davatzikos"], "venue": null, "citeRegEx": "Gaonkar and Davatzikos.,? \\Q2013\\E", "shortCiteRegEx": "Gaonkar and Davatzikos.", "year": 2013}, {"title": "On the interpretation of weight vectors of linear models in multivariate neuroimaging", "author": ["Stefan Haufe", "Frank Meinecke", "Kai G\u00f6rgen", "Sven D\u00e4hne", "John-Dylan Haynes", "Benjamin Blankertz", "Felix Bie\u00dfmann"], "venue": null, "citeRegEx": "Haufe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Haufe et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Automatic classification of mr scans in alzheimer\u2019s disease", "author": ["Stefan Kl\u00f6ppel", "Cynthia M Stonnington", "Carlton Chu", "Bogdan Draganski", "Rachael I Scahill", "Jonathan D Rohrer", "Nick C Fox", "Clifford R Jack", "John Ashburner", "Richard SJ Frackowiak"], "venue": null, "citeRegEx": "Kl\u00f6ppel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kl\u00f6ppel et al\\.", "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Classifying brain states and determining the discriminating activation patterns: Support vector machine on functional mri data", "author": ["Janaina Mourao-Miranda", "Arun LW Bokde", "Christine Born", "Harald Hampel", "Martin Stetter"], "venue": null, "citeRegEx": "Mourao.Miranda et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Mourao.Miranda et al\\.", "year": 2005}, {"title": "Explaining classifications for individual instances", "author": ["Marko Robnik-\u0160ikonja", "Igor Kononenko"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "Robnik.\u0160ikonja and Kononenko.,? \\Q2008\\E", "shortCiteRegEx": "Robnik.\u0160ikonja and Kononenko.", "year": 2008}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "A data-centric neuroscience gateway: design, implementation, and experiences", "author": ["Shayan Shahand", "Ammar Benabdelkader", "Mohammad Mahdi Jaghoori", "Mostapha al Mourabit", "Jordi Huguet", "Matthan WA Caan", "Antoine HC Kampen", "S\u00edlvia D Olabarriaga"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "Shahand et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shahand et al\\.", "year": 2015}, {"title": "Not just a black box: Learning important features through propagating activation differences", "author": ["Avanti Shrikumar", "Peyton Greenside", "Anna Shcherbina", "Anshul Kundaje"], "venue": "arXiv preprint arXiv:1605.01713,", "citeRegEx": "Shrikumar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shrikumar et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034,", "citeRegEx": "Simonyan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Support vector machine learning-based fmri data group analysis", "author": ["Ze Wang", "Anna R Childress", "Jiongjiong Wang", "John A Detre"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Understanding neural networks through deep visualization", "author": ["Jason Yosinski", "Jeff Clune", "Anh Nguyen", "Thomas Fuchs", "Hod Lipson"], "venue": "arXiv preprint arXiv:1506.06579,", "citeRegEx": "Yosinski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yosinski et al\\.", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "vision\u2013ECCV", "citeRegEx": "Zeiler and Fergus.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler and Fergus.", "year": 2014}, {"title": "Learning deep features for discriminative localization", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network.", "startOffset": 239, "endOffset": 305}, {"referenceID": 15, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network.", "startOffset": 239, "endOffset": 305}, {"referenceID": 18, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network.", "startOffset": 239, "endOffset": 305}, {"referenceID": 7, "context": "In the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al.", "startOffset": 169, "endOffset": 211}, {"referenceID": 2, "context": "In the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al.", "startOffset": 169, "endOffset": 211}, {"referenceID": 9, "context": ", 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007).", "startOffset": 78, "endOffset": 126}, {"referenceID": 17, "context": ", 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007).", "startOffset": 78, "endOffset": 126}, {"referenceID": 1, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper. One such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation.", "startOffset": 240, "endOffset": 587}, {"referenceID": 1, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper. One such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation. They also show that there is a close connection to using deconvolutional networks for visualization, proposed by Zeiler & Fergus (2014). Other methods include Shrikumar et al.", "startOffset": 240, "endOffset": 933}, {"referenceID": 1, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper. One such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation. They also show that there is a close connection to using deconvolutional networks for visualization, proposed by Zeiler & Fergus (2014). Other methods include Shrikumar et al. (2016), who compare the activation of a unit when a specific input is fed forward through the net to a reference activation for that unit.", "startOffset": 240, "endOffset": 980}, {"referenceID": 1, "context": "Broadly speaking, there are two approaches for understanding DCNNs through visualization investigated in the literature: find an input image that maximally activates a given unit or class score to visualize what the network is looking for (Erhan et al., 2009; Simonyan et al., 2013; Yosinski et al., 2015), or visualize how the network responds to a specific input image in order to explain a particular classification made by the network. The latter will be the subject of this paper. One such instance-specific method is class saliency visualization proposed by Simonyan et al. (2013) who measure how sensitive the classification score is to small changes in pixel values, by computing the partial derivative of the class score with respect to the input features using standard backpropagation. They also show that there is a close connection to using deconvolutional networks for visualization, proposed by Zeiler & Fergus (2014). Other methods include Shrikumar et al. (2016), who compare the activation of a unit when a specific input is fed forward through the net to a reference activation for that unit. Zhou et al. (2016) and Bach et al.", "startOffset": 240, "endOffset": 1131}, {"referenceID": 1, "context": "(2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above.", "startOffset": 11, "endOffset": 30}, {"referenceID": 1, "context": "(2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above. The idea of our method is similar to another analysis Zeiler & Fergus (2014) make: they estimate the importance of input pixels by visualizing the probability of the (correct) class as a function of a gray patch occluding parts of the image.", "startOffset": 11, "endOffset": 263}, {"referenceID": 1, "context": "(2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above. The idea of our method is similar to another analysis Zeiler & Fergus (2014) make: they estimate the importance of input pixels by visualizing the probability of the (correct) class as a function of a gray patch occluding parts of the image. In this paper, we take a more rigorous approach at both removing information from the image and evaluating the effect of this. In the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007). These are independent of the input image, and, as argued by Gaonkar & Davatzikos (2013) and Haufe et al.", "startOffset": 11, "endOffset": 975}, {"referenceID": 1, "context": "(2016) and Bach et al. (2015) also generate interesting visualization results for individual inputs, but are both not as closely related to our method as the two papers mentioned above. The idea of our method is similar to another analysis Zeiler & Fergus (2014) make: they estimate the importance of input pixels by visualizing the probability of the (correct) class as a function of a gray patch occluding parts of the image. In this paper, we take a more rigorous approach at both removing information from the image and evaluating the effect of this. In the field of medical image classification specifically, a widely used method for visualizing feature importances is to simply plot the weights of a linear classifier (Kl\u00f6ppel et al., 2008; Ecker et al., 2010), or the p-values of these weights (determined by permutation testing) (Mourao-Miranda et al., 2005; Wang et al., 2007). These are independent of the input image, and, as argued by Gaonkar & Davatzikos (2013) and Haufe et al. (2014), interpreting these weights can be misleading in general.", "startOffset": 11, "endOffset": 999}, {"referenceID": 11, "context": "We use images from the ILSVRC challenge (Russakovsky et al., 2015) (a large dataset of natural images from 1000 categories) and three DCNNs: the AlexNet (Krizhevsky et al.", "startOffset": 40, "endOffset": 66}, {"referenceID": 8, "context": ", 2015) (a large dataset of natural images from 1000 categories) and three DCNNs: the AlexNet (Krizhevsky et al., 2012), the GoogLeNet (Szegedy et al.", "startOffset": 94, "endOffset": 119}, {"referenceID": 16, "context": ", 2012), the GoogLeNet (Szegedy et al., 2015) and the (16-layer) VGG network (Simonyan & Zisserman, 2014).", "startOffset": 23, "endOffset": 45}, {"referenceID": 6, "context": "We used the publicly available pre-trained models that were implemented using the deep learning framework caffe (Jia et al., 2014).", "startOffset": 112, "endOffset": 130}, {"referenceID": 6, "context": "We used the publicly available pre-trained models that were implemented using the deep learning framework caffe (Jia et al., 2014). Analyzing one image took us on average 20, 30 and 70 minutes for the respective classifiers AlexNet, GoogLeNet and VGG (using the GPU implementation of caffe and mini-batches with the standard settings of 10 samples and a window size of k = 10). The results shown here are chosen from among a small set of images in order to show a range of behavior of the algorithm. The shown images are quite representative of the performance of the method in general. Examples on randomly selected images, including a comparison to the sensitivity analysis of Simonyan et al. (2013), can be seen in appendix A.", "startOffset": 113, "endOffset": 702}, {"referenceID": 11, "context": "Preprocessing of the data was performed with software developed in-house, using the HPCN-UvA Neuroscience Gateway and using resources of the Dutch e-Science Grid Shahand et al. (2015). As a result, Fractional Anisotropy (FA) maps were computed.", "startOffset": 162, "endOffset": 184}, {"referenceID": 0, "context": "FA images were spatially normalized to standard space Andersson et al. (2007), resulting in volumes with 91\u00d7 109\u00d7 91 = 902, 629 voxels.", "startOffset": 54, "endOffset": 78}], "year": 2017, "abstractText": "This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).", "creator": "LaTeX with hyperref package"}}}