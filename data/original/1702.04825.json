{"id": "1702.04825", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2017", "title": "Learning to Use Learners' Advice", "abstract": "In this paper, we study a variant of the framework of online learning using expert advice with limited/bandit feedback---we consider each expert a learning entity and thereby capture scenarios that are more realistic and practical for real-world applications. In our setting, the feedback at any time $t$ is limited in a sense that it is only available to the expert $i^t$ that has been selected by the central algorithm (forecaster), i.e., only the expert $i^t$ receives feedback from the environment and gets to learn at time $t$. We consider a generic black-box approach whereby the forecaster doesn't control or know the learning dynamics of the experts apart from knowing the following no-regret learning property: the average regret of any expert $j$ vanishes at a rate of at least $O(t_j^{\\beta-1})$ with $t_j$ learning steps where $\\beta \\in [0, 1]$ is a parameter. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees in the worst-case. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to \"guide\" the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., not allowing the selected expert $i^t$ to learn at time $t$ for some time steps. Then, we design a novel no-regret learning algorithm \\algo for this problem setting by carefully guiding the feedbacks observed by experts. We prove that \\algo achieves the worst-case expected cumulative regret of $O(T^\\frac{1}{2 - \\beta})$ after $T$ time steps and matches the regret bound of $\\Theta(T^\\frac{1}{2})$ for the special case of multi-armed bandits.", "histories": [["v1", "Thu, 16 Feb 2017 00:22:16 GMT  (258kb,D)", "https://arxiv.org/abs/1702.04825v1", null], ["v2", "Fri, 17 Feb 2017 21:39:29 GMT  (360kb,D)", "http://arxiv.org/abs/1702.04825v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["adish singla", "hamed hassani", "reas krause"], "accepted": false, "id": "1702.04825"}, "pdf": {"name": "1702.04825.pdf", "metadata": {"source": "CRF", "title": "Learning to Use Learners\u2019 Advice", "authors": ["Adish Singla", "Hamed Hassani", "Andreas Krause"], "emails": ["ADISH.SINGLA@INF.ETHZ.CH", "HAMED@INF.ETHZ.CH", "KRAUSEA@ETHZ.CH"], "sections": [{"heading": null, "text": "In the spirit of competing against the best action in hindsight in multi-armed bandits problem, our goal here is to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to \u201cguide\u201d the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., not allowing the selected expert it to learn at time t for some time steps. Then, we design a novel no-regret learning algorithm LEARNEXP for this problem setting by carefully guiding the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret ofO(T 1 2\u2212\u03b2 ) after T time steps and matches the regret bound of \u0398(T 1 2 ) for the special case of multi-armed bandits."}, {"heading": "1. Introduction", "text": "Many real-world applications involve repeatedly making decisions under uncertainty\u2014for instance, choosing one of the several items to recommend to the user, dynamically allocating resources among available stock options in a financial market, or sequentially deciding the next medical test in healthcare. Furthermore, the feedback is often limited in these settings in a sense that only the loss/reward associated with the action taken by the system is observed, referred to as the bandit feedback setting. Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses). In this paper, we investigate this framework with an important practical consideration:\nHow do we use the advice of experts when they themselves are learning entities?\n\u00a9 A. Singla, H. Hassani & A. Krause.\nar X\niv :1\n70 2.\n04 82\n5v 2\n[ cs\n.L G\n] 1\n7 Fe\nb 20"}, {"heading": "1.1. Motivating Applications", "text": "Modeling experts as learning entities realistically captures many practical scenarios of how one would define/encounter these experts in real-world applications, such as seeking advice from fellow players or friends, aggregating prediction recommendations from trading agents or different marketplaces, product testing with human participants who might adapt over time, information acquisition from crowdsourcing participants who might learn over time, the problem of meta-learning and hyperparameter tuning whereby different learning algorithms are treated as experts (cf. Baram et al. (2004); Hsu and Lin (2015)), and many more.\nAs a concrete running example, we consider the problem of learning to offer personalized deals / discount coupons to users enabling new businesses to incentivize and attract more customers (Edelman et al., 2011; Singla et al., 2016). An emerging trend is deal-aggregator sites like Yipit1 providing personalized coupon recommendation services to their users by aggregating and selecting coupons from daily-deal marketplaces like Groupon and LivingSocial1. One of the primary goals of these recommendation systems like Yipit (corresponding to the central algorithm / forecaster in our setting) is to design better selection strategies for choosing coupons from different marketplaces (corresponding to the experts in our setting). However, these marketplaces (experts) themselves would be learning to optimize the coupons to offer, for instance, the discount price or the type of the coupon based on historic interactions with users (Edelman et al., 2011)."}, {"heading": "1.2. Experts as Learning Entities: Challenges and Our Results", "text": "We now provide an overview of our approach, the main challenges in designing a forecaster with no-regret guarantees, and our results.\nThe interaction model. We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action atit recommended by the expert i\nt, and incurs a loss lt(atit) set by the adversary.\nThe notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the \u201cbest\u201d expert. Similar to the notion of competing against the best action in hindsight in multi-armed bandits problem, we want to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert (cf. Section 2.3 for a formal definition).\nExperts as no-regret learners and blackbox approach. In our setting, the experts themselves are learning entities. Formally, we assume that the experts are no-regret learners, i.e., the average regret of any expert j vanishes at a rate of at least O(t\u03b2\u22121j ) with tj learning steps where \u03b2 \u2208 [0, 1] is a parameter known to the forecaster. We consider the following natural notion of bandit/limited feedback: only the selected expert it receives feedback and gets to learn at time t; all other experts that have not been selected at time t experience no change in their learning state at this time. We\n1. http://yipit.com/; http://www.groupon.com/; https://livingsocial.com/\nconsider a generic black-box approach in which the forecaster does not know and cannot control the internal learning dynamics of the experts.\nChallenges and hardness result. It turns out that modeling these experts as learning entities leads to a challenging twist in this well-studied and foundational online learning framework. In this paper, we prove the following hardness result for our problem setting: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving noregret guarantees in the worst-case. Somewhat surprisingly, this hardness result holds when playing against an oblivious (non-adaptive) adversary and even if restricting the experts to be implementing some well-studied online learning algorithms, for instance, the HEDGE algorithm (Freund and Schapire, 1995). The fundamental challenge leading to this hardness result arises from the fact that the forecaster\u2019s selection strategy affects the feedback sequences observed by the experts which in turn alters their learning process.\n\u201cGuided\u201d feedbacks and achieving no-regret guarantees. In order to circumvent this hardness result, we consider the following practical assumption: we allow the forecaster to \u201cguide\u201d the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., the selected expert it would not learn at time t for some time steps. For instance, in the motivating application of offering personalized deals to users, the deal-aggregator site (forecaster) often primarily interacts with users on behalf of the individual daily-deal marketplaces (experts) and hence can control the flow of feedback to these marketplaces. Alternatively, we note that this process of guiding and restricting the feedback can be achieved via coordination between the forecaster and the selected expert it with a 1-bit of communication at time t. Given this additional control, we design a novel algorithm LEARNEXP for the forecaster which carefully guides the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret of O(T 1 2\u2212\u03b2 ) after T time steps against an oblivious adversary for a rich family of no-regret learning algorithms that experts may be implementing. For the special case of multiarmed bandits, algorithm LEARNEXP is equivalent to that of the well-studied EXP3 algorithm and hence matches the optimal regret bound of \u0398(T 1 2 ).\nConnections to the existing results. Maillard and Munos (2011) studied the problem of competing against an adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models. For this problem, the authors introduced the EXP4/EXP3 algorithm, i.e., EXP4 meta-algorithm with experts executing EXP3 algorithms proving a regret of O(T 2 3 ) (cf. Bubeck and Cesa-Bianchi (2012) for a variant of the algorithm). This EXP4/EXP3 algorithm is perhaps closest to ours, as it involves a forecaster where the experts are the learning entities. However, we note that our hardness result does not contradict their regret bounds\u2014the key difference in their setting is that the forecaster has the power to modify the losses as seen by experts, and it provides an unbiased estimate of the losses to these experts. Moreover, their analysis is specific to the experts implementing the EXP3 or bandit algorithms, whereas the focus of this paper is to present a more generic learning framework in which experts as learning entities may implement a broad class of learning algorithms. Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours. Note that, in applications where the experts directly receive feedback from the environment, implementing the strategies of Maillard and Munos (2011); Agarwal et al. (2016) would require the forecaster to communicate the probability p with which the expert it was selected at time t. However, our proposed idea of guiding the feedback\nProtocol 1: The interaction between adversary ADV, algorithm ALGO, and experts foreach t = 1, 2, . . . , T do\n/* Adversary generates the following */ 1 a private loss vector lt, i.e., lt(a) \u2200 a \u2208 A 2 a private feedback vector f t, i.e., f t(a) \u2200 a \u2208 A 3 a public context xt \u2208 X\n/* Selecting an expert and performing an action */ 4 ALGO selects an expert it \u2208 [N ] denoted as EXPit 5 ALGO performs the action atit recommended by EXPit /* Feedback and updates */ 6 ALGO incurs (and observes) loss lt(atit) and updates its selection strategy 7 \u2200j \u2208 [N ] : j 6= it, EXPj does not observe any feedback and makes no update 8 EXPit observes feedback f t(atit) from the environment and updates its learning state\nend\ncan be achieved via coordination between the forecaster and the selected expert it with a 1-bit of communication at time t."}, {"heading": "2. The Model", "text": "We have the following entities in our problem setting: (i) an algorithm ALGO as the forecaster; (ii) the adversary ADV acting on behalf of the environment; and (iii) N experts EXPj \u2200j \u2208 {1, . . . N} (henceforth denoted as [N ])."}, {"heading": "2.1. Specification of the Interaction", "text": "Protocol 1 provides a high-level specification of the interaction between the N + 2 entities. The sequential decision making process proceeds in rounds t = 1, 2, . . . , T (henceforth denoted as [T ]); for simplicity we assume that T is known in advance to the algorithm and the results in this paper can be extended to an unknown horizon via the usual doubling trick (Cesa-Bianchi and Lugosi, 2006). Each expert EXPj where j \u2208 N is associated with a set of actions Aj and the action set of the algorithm ALGO is given by A = \u222aj\u2208[N ]Aj . For the clarify of presentation in defining the loss and feedback vectors, we will consider that the action sets of experts are disjoint.2\nAt any time t, the adversary ADV generates a private loss vector lt (i.e., lt(a) \u2200 a \u2208 A) and a private feedback vector f t (i.e., f t(a) \u2200 a \u2208 A). Additionally, the adversary ADV generates a context xt \u2208 X that is accessible to all the experts while recommending their actions at time t\u2014this context essentially encodes all the side information from the environment accessible to the experts at time t (e.g., this context could represent preferences of a user arriving at time t in an online recommendation system). Simultaneously, the algorithm ALGO (possibly with some randomization) selects expert EXPit to seek advice. The selected expert EXPit recommends an action ait \u2208 Ait \u2286 A (possibly with its internal randomization) which is then performed by the algorithm. As feedback, the algorithm ALGO observes the loss lt(atit) and updates its strategy on how to select experts in the future. All the experts apart from the one selected (i.e., EXPj \u2200 j 6= it) observe no feedback and\n2. Note that assuming the disjoint action sets across experts is w.l.o.g., as we can still simulate the shared actions by enforcing a constraint that the losses generated by the adversary are same for the shared actions at any given time.\nmake no update at this time. The selected expert EXPit observes a feedback from the environment denoted as f t(atit) and updates its learning state. At the end of time t, the algorithm ALGO incurs a loss of lt(atit).\nSo far, we have considered a generic notion of the feedback received by the selected expert\u2014 this feedback essentially depends on the application setting and is supposed to be \u201ccompatible\u201d with the learning algorithm used by an expert. As a concrete example, consider an expert EXPj implementing the EXP3 algorithm and taking action atj at time t, then the feedback f\nt(atj) received by this expert (if selected at time t) is the loss lt(atj); for the case of expert EXPj implementing the HEDGE algorithm, the feedback f t(atj) received by this expert (if selected at time t) is the set of losses lt(a) \u2200 a \u2208 Aj . The feedback could be more general, for instance, receiving a binary signal of rejection or acceptance of the offered deal when an expert is implementing a dynamic pricing based algorithm via the partial monitoring framework (Cesa-Bianchi and Lugosi, 2006; Barto\u0301k et al., 2014). Also, we note that the special case of standard multi-armed bandits is captured by the setting in which Aj is a singleton for every expert j \u2208 [N ].\nWe assume that the losses are bounded in the range [0, lmax] for some known lmax \u2208 R+; w.l.o.g. we will use lmax = 1 (Auer et al., 2002). We consider an oblivious (non-adaptive adversary) as is usual in the literature (Freund and Schapire, 1995; Auer et al., 2002), i.e., the loss vector lt, the feedback vector f t, and the context xt at any time t do not depend on the actions taken by ALGO, and hence can be considered to be fixed in advance. Apart from that, no other restrictions are put on the adversary, and it has complete knowledge about the algorithm ALGO and the learning dynamics of the experts."}, {"heading": "2.2. Specification of the Experts", "text": "We consider a generic black-box approach in which ALGO does not know and cannot control the internal dynamics of the experts. In order to formally state the objective and guarantees we seek, we now provide a generic specification of the experts. At time t, let us denote an instance of feedback received by EXPit by a tuple h = (atit , x\nt, f t(atit)). For any expert EXPj where j \u2208 [N ], let Htj = (h1, h2, . . .) denote the feedback history for EXPj , i.e., an ordered sequence of feedback instances observed by EXPj up to time t. The length |Htj | denotes the number of learning steps for EXPj up to time t. At time t, the action atj recommended by EXPj to the algorithm, if this expert is selected, is given by atj = \u03c0j(x\nt,Htj) where \u03c0j is a (possibly randomized) function of EXPj , taking as input a context and a history of feedback sequence, and outputs an action a \u2208 Aj . Importantly, this history Htj is dependent on the execution of the algorithm ALGO\u2014 for clarify of presentation, we denote it asHtj,ALGO.\nNo-regret learning dynamics. To be able to say anything meaningful in this setting, we introduce the constraint of no-regret learning dynamics on the experts.3 Let us consider any sequence of loss vector l, feedback vector f , and context x given by S = ( (l\u03c4 , f \u03c4 , x\u03c4 ) ) \u03c4={1,2,...} generated arbitrarily by ADV and let |S| denotes its length. Consider a setting in which an expert EXPj for any j \u2208 [N ] is selected at every time step. At every time step \u03c4 \u2208 [|S|], EXPj recommends an action a\u03c4j , accumulates the loss l(a \u03c4 j ), and observes the feedback f(a \u03c4 j ). In this setting, EXPj observes feedback at every time step and we denote this \u201ccomplete\u201d history of feedback sequence at any time \u03c4 \u2208 |S| as Htj,1 whereby 1 denotes the fact that this expert is selected and receives feedback with\n3. In order to prove the no-regret guarantees for our algorithm LEARNEXP in Section 4, this constraint is required to hold only for the best expert against which we want to be competitive, a less stringent requirement.\nprobability 1 at every time step. Then, the no-regret learning dynamics of EXPj parameterized by \u03b2j \u2208 [0, 1] guarantees that the expected average regret vanishes as follows4:\nE [ 1\n|S| |S|\u2211 \u03c4=1 l\u03c4 ( \u03c0j(x \u03c4 ,H\u03c4j,1) )] \u2212 E [ 1 |S| |S|\u2211 \u03c4=1 l\u03c4 ( \u03c0j(x \u03c4 ,H|S|j,1) )] \u2264 O(|S|\u03b2j\u22121) (1)\nwhere the expectation is w.r.t. the randomization of function \u03c0j . We assume that parameter \u03b2 \u2208 [0, 1] upper bounds the regret rate parameters of individual experts and is a parameter known to the forecaster.5"}, {"heading": "2.3. Our Objective: No-Regret Guarantees", "text": "Intuitively, we want to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always using the advice of one single expert\u2014such a policy ensures that the single expert gets more feedback to improve its learning state and hence incur less cumulative loss. This is a challenging problem when the experts are learning entities. For instance, what may go wrong is that the best expert could have a slow rate of learning/convergence thus incurring high losses in the beginning, misleading the algorithm to essentially \u201cdownweigh\u201d this expert. This is turn further exacerbates the problem for the best expert in the bandit feedback setting as this expert will be selected less and will have fewer learning steps to improve its state. This adds new challenges to the classic trade-off between exploration and exploitation, suggesting the need to explore at higher rate to tackle this problem.\nLet us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012). Given that the experts are learning entities, naturally the losses incurred at any time step are dependent on the history of the forecaster\u2019s actions as that history defines the current learning state of the individual experts. Given this subtle issue of history dependent losses, the usual notion of external regret does not provide any meaningful guarantees in terms of competing against the \u201cbest expert in hindsight\u201d (see below for a formal definition); the bounds given by the external regret are only w.r.t. the post hoc sequence of actions performed and losses observed during the execution of the algorithm (cf. Maillard and Munos (2011); Arora et al. (2012); McMahan and Streeter (2009) for more discussion on this).\nWe consider the following natural notion of regret in this paper: our goal is to be competitive w.r.t. the best expert in hindsight, that is, competitive w.r.t. the cumulative loss that any expert could have received with the optimal actions it could have taken in hindsight. Formally, the expected cumulative regret of ALGO against the best expert in hindsight is given by:\nREG(T,ALGO) := T\u2211 t=1 E [ lt ( \u03c0it(x t,Htit,ALGO) )] \u2212 min j\u2208[N ] E [ T\u2211 t=1 lt ( \u03c0j(x t,HTj,1) )]\n(2)\nwhere the expectation is w.r.t. the randomization of the algorithm as well as any internal randomization of the experts. Our goal is to design an algorithm ALGO for the forecaster so that the regret REG(T,ALGO) grows sublinearly in time T .\n4. Note that this is a weaker notion of regret\u2014any deterministic policy \u03c0j that always outputs a constant action has \u03b2j = 0. However, \u03b2j = 0 would be the right way to characterize the learning dynamics of this expert for our setting. 5. Again for our algorithm LEARNEXP in Section 4, \u03b2 only needs to upper bound the regret rate for the best expert against which we want to be competitive."}, {"heading": "3. Hardness Result", "text": "We show in this section that, in the absence of any coordination between the forecaster and experts, it is impossible to design a forecaster that achieves no-regret guarantees in the worst-case. Somewhat surprisingly, we prove this hardness result when playing against an oblivious (non-adaptive) adversary and when restricting the experts to be implementing the well-studied HEDGE algorithm (Freund and Schapire, 1995). We formally state this hardness result in the Theorem 1 below.\nTheorem 1 There is a setting in which each of the experts has no-regret learning dynamics with parameter \u03b2 = 12 ; however, any algorithm ALGO (forecaster) will suffer a positive average regret, i.e., REG(T,ALGO) = \u2126(T ).\nThe proof is given in the Appendix, we briefly outline the main ideas below. Our setting for proving this theorem consists of two experts EXP1 and EXP2. The first expert EXP1 has two actions given by A1 = {a1, a2}, and the second expert EXP1 has only one action given by A2 = {b}. The action set of the algorithm ALGO is given by A = {a1, a2, b}. The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.e., the regret rate parameter is \u03b21 = 0.5; the expert EXP2 has only one action to play as in the standard multi-armed bandit with \u03b22 = 0.6 Figures 1(a), 1(b), and 1(c) show the cumulative loss sequences L1, L2, and L3 for three different scenarios\u2014the adversary at t = 0 uniformly at random picks one of these scenarios and uses that loss sequence.\nThe main idea of the proof uses the following arguments. We consider the case where the forecaster is facing the sequence L1 (chosen by the adversary with probability 13 at t = 0). We then divide the time horizon T into different slots and discuss the execution behavior of the forecaster and experts over these time slots. Specifically, our claim is that in the time slot t \u2208 (T4 , T 2 ], the expert\n6. In fact, this hardness result holds even when considering a powerful forecaster which knows exactly the learning algorithms used by the experts, and is able to see the losses {lt(a1), lt(a2), lt(a3)} at every time t \u2208 [T ].\nEXP1 would not be selected for T12 \u2212 o(T )) time steps. As a result, in the time slot t \u2208 ( 11T 12 , T ], the expert EXP1 would select action a2 almost surely, and a1 would only be selected o(T ) number of times, leading to a positive average regret for the forecaster. Informally speaking, our negative example shows that the forecaster\u2019s selection strategy could add \u201cblind spots\u201d in the feedback history seen by the experts and that they might not be able to \u201crecover\u201d from this. The key fundamental challenge leading to this hardness result is that the forecaster\u2019s selection strategy affects the feedback sequences observed by the experts, which in turn alters the experts\u2019 learning process."}, {"heading": "4. Our Algorithm LEARNEXP", "text": "In this section, we introduce a practical assumption that allows the forecaster to \u201cguide\u201d the learning process of experts, and then we design our main algorithm LEARNEXP with provable no-regret guarantees."}, {"heading": "4.1. Guided Feedbacks", "text": "In order to circumvent the hardness result proved in Section 3, we now consider a practical assumption motivated by the application setting of deal-aggregator sites, as discussed in Section 1. Usually, a deal-aggregator site interacts with users on behalf of the individual daily-deal marketplaces (experts) and hence could control the flow of feedback to these marketplaces. Hence, we allow the forecaster to \u201cguide\u201d the learning process of the experts by filtering/blocking some of the feedback the experts receive from the environment. Recall that at time t, as per the interaction model presented in Section 2, the selected expert EXPit observes feedback f t(atit) from the environment. We now consider the setting with the following additional power in the hands of the forecaster: In order to guide the learning process of the experts, the forecaster at time t could block the feedback, i.e., the expert EXPit would not observe feedback at time t and hence would not learn at this time (just like any other experts who were not selected at time t). Alternatively, we note that this process of guiding the feedback could be achieved via coordination between the forecaster and the selected expert EXPit with a 1-bit communication at time t."}, {"heading": "4.2. Algorithm LEARNEXP", "text": "With this additional power of the forecaster to guide feedback, we develop our main algorithm LEARNEXP, presented in Algorithm 2. The selection strategy of the algorithm LEARNEXP is similar to the EXP family of algorithms, and in particular is equivalent to the EXP3 algorithm by Auer et al. (2002). The core idea of guiding the feedbacks observed by experts is presented in Lines 10,11, and 12.\nBy default, as per the Protocol 1, the selected expert EXPit always observes feedback at time t\u2014for this protocol, the hardness result of Theorem 1 applies. Our algorithm LEARNEXP instead decides whether the expert EXPit should observe/use the feedback based on the outcome \u03bet of a coin flip with probability \u03b7\nN \u00b7pt it\n. By choosing this particular probability, the algorithm LEARNEXP\nensures that the probability that any expert EXPj observes feedback at time t is constant over time and is given by \u03b7N . The key parameter of the algorithm \u03b7 would be fixed in Theorem 3 based on the regret rate \u03b2 to achieve the desired guarantees on the regret.\nThe guarantees in Theorem 3 mean that by adding this additional control/coordination in our model, we are able to circumvent the hardness result of Theorem 1. Interestingly, if we consider any\nAlgorithm 2: LEARNEXP 1 Parameters: \u03b7 \u2208 (0, 1] 2 Initialize: time t = 1, weights wtj = 1 \u2200j \u2208 [N ]\nforeach t = 1, 2, . . . , T do /* Selecting an expert and performing an action */\n3 \u2200j \u2208 [N ], define probability ptj = (1\u2212 \u03b7) \u00b7 wtj(\u2211\nk\u2208[N ]w t k\n) + \u03b7 N\n4 Draw it from the multinomial distribution (ptj)j\u2208[N ] 5 Perform action atit recommended by the expert EXPit /* Observing the loss and making updates */ 6 Observe loss lt(atit) 7 \u2200j \u2208 [N ], do the following:\n8 Set l\u0303tj as follows: l\u0303 t j =\nlt(atit)\nptit for j = it, else l\u0303tj = 0\n9 Update wt+1j \u2190 wtj \u00b7 exp(\u2212 \u03b7 \u00b7 l\u0303tj N )\n/* Guiding the feedback */\n10 \u03bet \u223c Bernoulli( \u03b7 N \u00b7 ptit ) 11 if (\u03bet = 1) then 12 EXPit observes feedback f t(atit) from the environment and updates its learning state\nend end\nexpert EXPj for j \u2208 [N ], the historyHtj at any time t under this guided feedback setting would only contain a subset of the feedback instances that it would have received without guiding (i.e., where \u03bet = 1 \u2200t \u2208 [T ]). By carefully allowing the expert to observe a strictly smaller set of feedback instances allows us to ensure that the expert EXPj achieves low regret. Considering the example we use in the proof of Theorem 1 to show the hardness results, this means that by carefully guiding the feedback received by experts, our algorithm LEARNEXP ensures that there are no \u201cblind spots\u201d in the feedback history of any expert. However, in order to achieve this, the algorithm is required to explore at a higher rate, as is evident by the value of \u03b7 in Theorem 3."}, {"heading": "4.3. Theoretical Guarantees", "text": "Next, we analyze the theoretical guarantees of our algorithm LEARNEXP. One approach to doing this is to consider a particular class of no-regret learning algorithms that experts implement and prove guarantees for that class. Instead, we introduce a novel, generic notion of \u201csmooth\u201d no-regret learning\u2014our theoretical guarantees are then proven for the experts that have no-regret and smooth learning dynamics. Next, we introduce this notion and then discuss (see Proposition 2) the class of no-regret learning algorithms that also satisfy the constraint of smooth learning dynamics."}, {"heading": "4.3.1. SMOOTH LEARNING DYNAMICS", "text": "In our bandit feedback setting, not all the experts can observe feedback at a given time step, and hence the history of feedback instances received by any particular expert is naturally \u201csparse\u201d. To formally state the behavior of the learning algorithm under this sparse feedback, we now introduce a new notion, termed smooth learning dynamics, to complement the no-regret learning dynamics defined in (1). Consider the same fixed sequence S as used in defining (1) and an expert EXPj . However, instead of observing feedback at every time step, let\u2019s say that the expert EXPj only gets to observe the feedback sporadically at a rate of \u03b1 \u2208 (0, 1]\u2014we call this an \u03b1-sparse history, denoted asHlj,\u03b1. Then, the constraint of smooth learning dynamics ensures that the expected regret of the expert EXPj when receiving the above-mentioned sparse feedback vanishes (smoothly w.r.t. rate \u03b1) as follows:\nE [ 1\n|S| |S|\u2211 \u03c4=1 l\u03c4 ( \u03c0j(x \u03c4 ,H\u03c4j,\u03b1) )] \u2212 E [ 1 |S| |S|\u2211 \u03c4=1 l\u03c4 ( \u03c0j(x \u03c4 ,H|S|j,1) )] \u2264 O((\u03b1 \u00b7 |S|)\u03b2j\u22121) (3)\nwhere the expectation is w.r.t. the randomization of function \u03c0j as well as w.r.t. the randomization in generating this sparse history. The following proposition states that a rich class of online learning algorithms indeed have smooth learning dynamics that can be used by the experts, cf. Appendix for the proof.\nProposition 2 A rich class of no-regret online learning algorithms based on gradient-descent style updates have smooth learning dynamics including the Online Mirror Descent family of algorithms with exact or estimated gradients (Shalev-Shwartz, 2011) and Online Convex Programming via greedy projections (Zinkevich, 2003)."}, {"heading": "4.3.2. NO-REGRET GUARANTEES OF LEARNEXP", "text": "Next, we prove the no-regret guarantees of our algorithm LEARNEXP, formally stated in Theorem 3. The following theorem (stating only the leading terms w.r.t. the T and dropping any other constants like N ) provides the no-regret guarantees of LEARNEXP against the best expert in hindsight as per (2). The proof is given in the Appendix.\nTheorem 3 Let T be the fixed time horizon. Consider that the best expert j\u2217 \u2208 [N ] has no-regret smooth learning dynamics parameterized by \u03b2j\u2217 \u2208 [0, 1] and LEARNEXP is invoked with input \u03b2 \u2208 [0, 1] such that \u03b2 \u2265 \u03b2j\u2217 . Set parameters \u03b7 = \u0398 ( T \u2212 1\u2212\u03b2 2\u2212\u03b2 \u00b7 N 1\u2212\u03b2 2\u2212\u03b2 \u00b7 (logN)( 1 2 \u00b71{\u03b2=0}) ) . Then, for sufficiently large T , the worst-case expected cumulative regret of LEARNEXP against the best expert in hindsight is:\nREG(T, LEARNEXP) \u2264 O ( T 1 2\u2212\u03b2 \u00b7N 1 2\u2212\u03b2 \u00b7 (logN)( 1 2 \u00b71{\u03b2=0}) ) For the special case of multi-armed bandits (where \u03b2 = 0), this regret bound matches the bound of \u0398(T 1 2 )\u2014in fact, for this special case, our algorithm LEARNEXP is exactly equivalent to EXP3. For an important case when experts are implementing algorithms like HEDGE or EXP3 (where \u03b2 = 12 ), our algorithm LEARNEXP achieves the bound of O(T 2 3 )."}, {"heading": "5. Background and Related Work", "text": "In this section, we provide an overview of the relevant literature."}, {"heading": "5.1. Background", "text": "We begin with a background on the framework of learning using expert advice with bandit feedback. Using expert advice. The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study of using expert advice for prediction problems, and Freund and Schapire (1995) introduced the algorithm HEDGE for the general problem of dynamically allocating resources among a set of options using expert advice.\nUsing expert advice with bandit feedback. However, the feedback is often limited in these settings in a sense that only the loss/reward associated with the action taken by the system is observed, referred to as the bandit feedback setting. To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006).\nFurthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015)."}, {"heading": "5.2. Related Work", "text": "Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP. Restless bandits are notoriously difficult to tackle (cf. (Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state. Besbes et al. (2014) considered another type of restless bandits with stochastic reward functions, however these distributions change adversarially with a budget on the allowed variation. Our approach is similar in spirit to the rested bandits; however, none of the frameworks above would model the learning dynamics of the experts in the adversarial setting we consider.\nNon-oblivious/adaptive adversary. As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models. However, none of the frameworks of non-oblivious/adaptive adversary listed above model learning dynamics in our setting: It would require an adversary with unbounded memory to apply the results of Arora et al. (2012), and an adversary with unbounded number of models to apply the techniques of Maillard and Munos (2011).\nContextual bandits. Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014). We refer the reader to the paper by McMahan and Streeter (2009) for more discussion on the connection between the framework of contextual bandits and learning using expert advice with bandit feedback.\nLearning in games. An orthogonal line of research studies the interaction of agents in multiplayer games where each agent uses a no-regret learning algorithm (Blum and Monsour, 2007; Syrgkanis et al., 2015). The questions tackled in this line of research are very different as it focuses on the interactions of the agents, their individual as well as social utilities, and the convergence of the game to equilibrium. This orthogonal line of research reassures that the no-regret learning dynamics that we consider in this paper are indeed important and natural dynamics that are also prevalent in other application domains."}, {"heading": "6. Conclusions", "text": "In this paper, we investigated the online learning framework using expert advice with bandit feedback with an important practical consideration: how do we use the advice of the experts when they themselves are learning entities? As our first contribution, we proved the hardness result stating that it is impossible to achieve no-regret guarantees when the experts receive feedback directly from the environment and there is no further coordination between forecaster/experts. Our hardness result sheds light on the complexity of the problem when applying this online learning framework to real-world applications whereby it is natural for experts to exhibit learning dynamics.\nThen, we considered a practical assumption of \u201cguided\u201d feedbacks whereby the forecaster can block/filter the feedback received by the selected expect from the environment. Under this setting, we proposed a novel algorithm LEARNEXP\u2014we proved that LEARNEXP achieves the worst-case expected cumulative regret of O(T 1 2\u2212\u03b2 ) after T time steps where \u03b2 is a parameter characterizing the individual no-regret learning dynamics of the best expert. This regret bound matches the bound of \u0398(T 1 2 ) for the special case of multi-armed bandits.\nThere are a number of research directions for future work. An interesting question to tackle is whether it is possible to design a forecaster in our setting with a worst-case cumulative regret of \u0398(T\n1 2 ) when the individual experts have no-regret learning dynamics with \u03b2 = 12 . In this paper, in\norder to circumvent the hardness result, we considered the power of blocking/filtering the feedbacks, which can equivalently be achieved with a 1-bit of communication at every time step. An interesting direction would be to consider other practical ways of coordination and to understand the minimal coordination required to achieve no-regret guarantees."}, {"heading": "Appendix A. Proof of Theorem 1", "text": "In this section, we give a proof of the hardness result by discussing a generic and simple setting in which any forecaster suffers a positive average regret.\nThe setting. Our setting consists of two experts EXP1 and EXP2. The first expert EXP1 has two actions given by A1 = {a1, a2}, and the second expert EXP1 has only one action given by A2 = {b}. The action set of the forecaster, or algorithm ALGO, is given by A = {a1, a2, b}. The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.e., the regret rate parameter is \u03b21 = 0.5 (see (1)); the expert EXP2 has only one action to play as in the standard multi-armed bandit with \u03b22 = 0 (see (1)). The forecaster knows parameter \u03b2 = 0.5 which upper bounds the regret rate of the individual experts.7\nLoss sequences. Figures 1(a), 1(b), and 1(c) shows the cumulative loss sequences L1, L2, and L3 for three different scenarios\u2014the adversary at t = 0 uniformly at random picks one of scenarios and uses that loss sequence. These plots show the cumulative losses of the three actions A = {a1, a2, b} for three different sequences. For the first scenario with cumulative loss sequences L1 shown in Figure 1(a), we show in Figure 2 the instantaneous losses of the different actions. Figures 2(a) and 2(b) show the losses of actions for EXP1; 2(c) shows the losses of action for EXP2.\nModel specification. To fully specify the model and Protocol 1, we specify now the feedback vector, and the context over time. The context xt is constant over time and plays no role in our setting. The experts receive the following feedback when selected: the expert EXP1 would observe the losses {lt(a1), lt(a2)} when it = 1; and the expert EXP2 would observe the loss {lt(b)} when it = 2.\nExecution behavior. We now divide the time horizon T into different slots and discuss the execution behavior of the forecaster and experts over these time slots. Specifically, let us consider the case where the forecaster is facing the sequence L1 (chosen by adversary with probability 13 at t = 0) with cumulative losses shown in Figure 1(a) and instantaneous losses of the actions shown in Figure 2. For a clarity of presentation, we shall use \u2206 = 0.01 as a constant in rest of the proof below.\n\u2022 t \u2208 [0, T4 ]: In this time slot, the expert EXP1 would be selected almost surely by the forecaster and the number of times EXP2 would be selected is o(T ). If this is not the case, then this forecaster would suffer positive average regret on the loss sequence L3 for third scenario in Figure 1(c). 8 The loss incurred by the forecaster at any time t in this time slot is at least 0.5.\n\u2022 t \u2208 (T4 , T 2 ]: This is the first crucial time slot whereby forecaster\u2019s selection strategy would\nadd \u201cblind spots\u201d to the feedback received by EXP1. The key argument is that in this time slot, the forecaster cannot select expert EXP1 for more than T6 + o(T ) timesteps\u2014if this happens, than this forecaster would have a positive average regret on the loss sequence L2 for second scenario in Figure 1(b). In other words, in this period, the expert EXP1 has missed seeing the feedback for T12 \u2212 o(T )) timesteps. Clearly, the loss incurred by the forecaster at any time t in this time slot is at least 0.5\u2212 3\u22062 .\n7. In fact, this hardness result holds even when considering a powerful forecaster which knows exactly the learning algorithms used by the experts, and is able to see the losses {lt(a1), lt(a2), lt(b)} at every time t \u2208 [T ]. 8. Note here that the loss sequence L3 is exactly equal to L1 up to time T/4. Furthermore, although we have assumed that the setting chosen by the adversary is L1, we should bear in mind that the forecaster (who can not distinguish between the losses at least up to time T/4) should play in a way that it does not suffer positive average regret for L3.\n\u2022 t \u2208 (T2 , 11T 12 ]: In this time slot, the expert EXP1 would be selected almost surely and the\nnumber of times EXP2 would be selected is o(T ). The loss incurred by the forecaster at any time t in this time slot is at least 0.5.\n\u2022 t \u2208 (11T12 , T ]: This is the second crucial time slot which would lead to the positive average regret for the forecaster. We note that the forecaster still does the \u201cright\u201d thing in this time slot, i.e., the expert EXP1 would be selected almost surely and the number of times EXP2 would be selected is o(T ). However, the expert EXP1 has missed observing feedback for ( T12 \u2212o(T )) time steps in the slot ( T 4 , T 2 ]. Note also that no coordination is permitted between\nthe forecaster and the experts, and hence, EXP1 is not aware of the time steps that it misses the feedback. As a result, at the start of this time slot, the cumulative loss of action a1 (as perceived by EXP1 based on observed history) is at least 0.5 \u00b7 T12 more than the cumulative loss of action a2 (as perceived by EXP1 based on observed history). The expert EXP1 who is playing HEDGE algorithm in our setting would select action a2 almost surely and a1 would be selected o(T ) number of times.\nPositive average regret. Let us now compute the regret of the forecaster when experiencing loss sequence L1 as discussed above. The cumulative loss of the \u201cbest expert in hindsight\u201d is given by that of EXP1 always playing action a1. Based on Figure 2(a), this is given by:\u2211\nt\u2208[T ]\nlt(a1) = 1 \u00b7 T\n4 + 0.5 \u00b7 (11T 12 \u2212 T 2 ) = T \u00b7 (1 2 \u2212 1 24 ) The cumulative loss of the forecaster as per the execution behavior discussed above can be lower\nbounded as follows:\u2211 t\u2208[T ] lt(atit) \u2265 0.5 \u00b7 T 4 + ( 0.5\u2212 3\u2206 2 ) \u00b7 T 4 + 0.5 \u00b7 (11T 12 \u2212 T 2 ) + 0.5 \u00b7 ( T 12 \u2212 o(T ) ) = T \u00b7\n(1 2 \u2212 3\u2206 8 \u2212 o(T ) 2 \u00b7 T )\nHence, the total regret of the forecaster is lower bounded by:\nREG(ALGO, T ) \u2265 T \u00b7 (1\n2 \u2212 3\u2206 8 \u2212 o(T ) 2 \u00b7 T ) \u2212 T \u00b7 (1 2 \u2212 1 24 ) = T \u00b7 ( 1 24 \u2212 3\u2206 8 \u2212 o(T ) 2 \u00b7 T )\nRecall that the constant \u2206 = 0.01, hence the average regret of the forecaster is lower bounded by limT 7\u2192\u221e REG(ALGO,T ) T \u2265 91 2400 .\nAs this sequence L1 is selected by the adversary uniformly at random with probability 13 , this means that the forecaster would suffer a positive average regret. As we discussed above, any forecaster which doesn\u2019t have the above-mentioned execution behavior in the timeslot t \u2208 [0, T4 ] or t \u2208 (T4 , T 2 ] would suffer a positive average regret for L3 and L2 loss sequences."}, {"heading": "Appendix B. Proof of Proposition 2", "text": "OCP algorithms. Assume that and expert EXPj is performing Online Convex Programming (OCP) via greedy projections. We will show that such an algorithm has smooth learning dynamics. Note that OCP has regret of size O( \u221a T ) (i.e. \u03b2j = 1/2). Consider the \u03b1-OCP algorithm that proceeds according to Algorithm 3. Proving smooth learning dynamics for OCP is equivalent to showing that \u03b1-OCP suffers a regret of size O( \u221a T/\u03b1). More precisely, we have the following Lemma.\nAlgorithm 3: \u03b1-OCP 1 Problem setting: Convex set S; sequence of convex loss functions f t : S \u2192 R+ 2 Parameters: Learning rates \u03b7t for t \u2208 [T ] 3 Initialize: w0 \u2208 S arbitrarily\nforeach t = 1, 2, . . . , T do 4 wt+1/2 = wt \u2212 \u03b7tBtzt where: (i) zt \u2208 \u2202f t(wt), and (ii) random variables Bt are independent\nBernoulli with parameter \u03b1 (i.e. Pr(Bt = 1) = 1\u2212 Pr(Bt = 0) = \u03b1), and (iii) \u03b7t = 1/ \u221a 1 + \u2211t \u03c4=1B \u03c4\n5 wt+1 = ProjS(w t+1/2)\nend\nLemma 4 Let \u2016S\u2016 denote the diameter of the convex set S and L denotes an upper bound on the magnitude of the gradient at any time t \u2208 T . Then, the expected regret of the \u03b1-OCP algorithm is given by\nE[ T\u2211 t=1 (f t(wt)\u2212 f t(u))] \u2264 \u2016S\u2016 2 2 \u00b7 \u221a T \u03b1 + L2 \u00b7 \u221a T \u03b1 (4)\nwhere the expectation is w.r.t. the sequence of Bernoulli random variables Bt for t \u2208 [T ].\nProof We can equivalently write the updates in the \u03b1-OCP procedure as follows:\nwt+1/2 = wt \u2212 \u03b7tBtzt\n= wt \u2212 (\u03b7t \u00b7 \u03b1) \u00b7 (B t\n\u03b1 )zt\n= wt \u2212 \u03b7\u0303t \u00b7 z\u0303t\nwhere \u03b7\u0303t = (\u03b7t \u00b7 \u03b1) and z\u0303t = (Bt\u03b1 )z t. Note that E[z\u0303t|w1:t] = E[zt] \u2208 \u2202f t(wt). We have:\nE [ T\u2211 t=1 (f t(wt)\u2212 f t(u)) ]\n(5)\n= E [ T\u2211 t=1 E [ (f t(wt)\u2212 f t(u))|w1:t ]] (6)\n\u2264 E [ T\u2211 t=1 E [ < \u2202f t(wt), wt \u2212 u > |w1:t ]] (7)\n= E [ T\u2211 t=1 E [ < z\u0303t, wt \u2212 u > |w1:t ]] (8)\n= E [ T\u2211 t=1 1 2\u03b1\u03b7t E [ \u2225\u2225wt \u2212 w\u2225\u22252 \u2212 \u2225\u2225\u2225wt+1/2 \u2212 w\u2225\u2225\u22252 + \u03b72t\u03b12 \u2225\u2225\u2225z\u0303t\u2225\u2225\u22252 |w1:t]] (9)\n\u2264 E [ T\u2211 t=1 1 2\u03b1\u03b7t E [ \u2225\u2225wt \u2212 w\u2225\u22252 \u2212 \u2225\u2225wt+1 \u2212 w\u2225\u22252 + \u03b72t\u03b12 \u2225\u2225\u2225z\u0303t\u2225\u2225\u22252 |w1:t]] (10)\n= E [ T\u2211 t=1 \u2225\u2225wt \u2212 w\u2225\u22252 2\u03b1\u03b7t \u2212 \u2225\u2225wt+1 \u2212 w\u2225\u22252 2\u03b1\u03b7t ] + \u03b1 2 E [ T\u2211 t=1 \u03b7t||\u03b8t||2 ]\n(11)\n\u2264 E [\u2225\u2225w1 \u2212 w\u2225\u22252\n2\u03b1\u03b71 \u2212 ||w T+1 \u2212 w||2 2\u03b1\u03b7T + 1 2 T\u2211 t=2 \u2225\u2225wt \u2212 w\u2225\u22252 ( 1 \u03b1\u03b7t \u2212 1 \u03b1\u03b7t\u22121 ) ] + \u03b1 2 E [ T\u2211 t=1 \u03b7t||z\u0303t||2 ]\n(12) \u2264 ||S||2E [ 1\n2\u03b1\u03b7T\n] + \u03b1 2 E [ T\u2211 t=1 \u03b7t||z\u0303t||2 ]\n(13)\nNow note that E[\u03b7t||z\u0303t||2] = E [ E[\u03b7t||z\u0303t||2|w1:t] ] = E [ \u03b7t||zt||2/\u03b1] \u2264 L2E[\u03b7t]/\u03b1. We thus obtain\nE [ T\u2211 t=1 (f t(wt)\u2212 f t(u)) ] \u2264 ||S||2E [ 1 2\u03b1\u03b7T ] + L2 2 E [ T\u2211 t=1 \u03b7t ] . (14)\nWe next recall that \u03b7t = 1\u221a 1+ \u2211t \u03c4=1B \u03c4 . By using the multiplicative Chernoff bound (as B\u03c4 \u2019s are Bernoulli random variables) we obtain\nPr(\u03b7t \u2265 \u221a 2/(t\u03b1)) = Pr( t\u2211\n\u03c4=1\nB\u03c4 \u2264 t\u03b1/2) \u2264 exp(\u2212 t\u03b1 12 ).\nHence, we obtain E[\u03b7t] \u2264 \u221a\n2/(\u03b1t)+exp(\u2212t\u03b1/12). Also, due to concavity of the function h(x) =\u221a x, we have that E[1/\u03b7T ] \u2264 \u221a T\u03b1. We finally obtain\nE [ T\u2211 t=1 (f t(wt)\u2212 f t(u)) ] \u2264 ||S||2 \u221a T/\u03b1+ L2 \u221a T/\u03b1+ T\u2211 t=1 exp (\u2212t\u03b1/12)\n\u2264 ||S||2 \u221a T/\u03b1+ L2 \u221a T/\u03b1+ 1/(1\u2212 exp (\u2212\u03b1/12))\n\u2264 ||S||2 \u221a T/\u03b1+ L2 \u221a T/\u03b1+ 24/\u03b1,\nwhere the last line is because 1/(1\u2212 exp(\u2212\u03b1/12)) \u2264 24/\u03b1 for \u03b1 \u2264 1.\nOMD Algorithms. We now consider the case that expert EXPj is performing an algorithm inside the Online Mirror Descent (OMD) family of algorithms. We assume that the algorithm has a regret of order O( \u221a T ) for any time horizon T (i.e. \u03b2j = 1/2). We also assume that the algorithm uses the doubling trick. Consider the standard online learning scenario where at any time t \u2208 [T ] a convex function f t : S \u2192 R is assigned (S is assumed to be a convex region). The proofs proceeds in 3 steps.\nStep 1. \u03b1-OMD with a fixed time horizon We first analyze the algorithm \u03b1-OMD given in 4 which is run for a fixed (deterministic) number of steps.\nAlgorithm 4: \u03b1-OMD 1 Problem setting: Convex set S; sequence of convex loss functions f t : S \u2192 R+ 2 Parameters: a link function g : Rd \u2192 S; time horizon T 3 Initialize: time \u03c4 = 1, auxiliary variable \u03b8\u03c4 = 0 \u2208 Rd\nforeach \u03c4 = 1, 2, . . . , T do 4 Predict vector w\u03c4 = g(\u03b8\u03c4 ) 5 Update \u03b8\u03c4+1 = \u03b8\u03c4 \u2212B\u03c4z\u03c4 where: (a) z\u03c4 \u2208 \u2202f \u03c4 (w\u03c4 ), (b) B\u03c4 is an independent Bernoulli\nrandom variable with parameter \u03b1 (i.e. Pr(B\u03c4 = 1) = 1\u2212 Pr(B\u03c4 = 0) = \u03b1). end\nLemma 5 Let R be a 1/\u03b7- strongly convex function over S with respect to a norm || \u00b7 ||. Assume that \u03b1-OMD is run on the sequence with a link function\ng(\u03b8) = arg max w\u2208S\n(\u3008w, \u03b8\u3009 \u2212R(w))\nFurthermore, assume that f t is L-Lipshitz with respect to norm || \u00b7 ||. Then\nE[ T\u2211 t=1 (f t(wt)\u2212 f t(u))] \u2264 R(u)/\u03b1+ \u03b7TL2. (15)\nProof For the sake of analysis, we introduce the following slightly modified procedure:\n1. Initialize \u03b8\u03031 = \u03b81/\u03b1.\n2. At time \u03c4 = 1, 2, \u00b7 \u00b7 \u00b7 , T , let w\u0303\u03c4 = g\u0303(\u03b8\u0303\u03c4 ), and \u03b8\u0303\u03c4+1 = \u03b8\u0303\u03c4 \u2212 z\u0303\u03c4 . Here, we have z\u0303\u03c4 = B\u03c4\u03b1 z \u03c4 ,\nand the function g\u0303 is defined as g\u0303(\u03b8) = arg maxw\u2208S(\u3008w, \u03b8\u3009 \u2212R(w)/\u03b1).\nIt is straight forward to justify for any \u03c4 \u2208 [T ] that \u03b8\u0303\u03c4 = \u03b8\u03c4/\u03b1 and w\u0303\u03c4 = w\u03c4 . Also note that E[z\u0303\u03c4 |z\u03031:\u03c4\u22121] = z\u03c4 \u2208 \u2202f \u03c4 (w\u03c4 ). Hence, the modified procedure (\u03b8\u0303\u03c4 , w\u0303\u03c4 ) is precisely a stochastic\nOMD procedure with with link function g\u0303. By using Theorem 4.1 in Shalev-Shwartz (2011), w\u0303\u03c4 = w\u03c4 , and the fact that R(\u00b7)/\u03b1 is a 1/(\u03b7\u03b1)-strongly convex function, we obtain\nE[ T\u2211 t=1 (f t(wt)\u2212 f t(u))] \u2264 sup u\u2208S R(u)/\u03b1+ \u03b7\u03b1 T\u2211 \u03c4=1 E[||z\u0303\u03c4 ||2].\nWe finally note that E[||z\u0303\u03c4 ||2] = E [ E[||z\u0303\u03c4 ||2 | z\u03031:\u03c4\u22121] ] \u2264 L2/\u03b1.\nThe result of the Lemma is now immediate.\nStep 2. \u03b1-OMD with a random time horizon From Lemma 5, for \u03b7 = O(1/ \u221a T ), the algorithm \u03b1-OMD suffers a O( \u221a T/\u03b1) regret after any fixed time T . Recall now that at any time the algorithm is only given feedback with independent probability \u03b1. We are assuming that the algorithm used by the expert EXPj is performing the doubling trick, i.e., it runs in blocks whose size get doubled consecutively and within each block the learning rate is fixed. As a result, after the algorithm receives sufficient feedback to finish a block, it restarts OMD and changes the learning rate for the next block (which has twice the size). In order to analayze the regret suffered in each block, we need to consider a slightly different version of \u03b1-OMD which stops after a randomly chosen time.\nLemma 6 (\u03b1-OMD with a random time horizon) Assume that we run the \u03b1-OMD procedure until the time, call it Tstop, such that following stopping criterion has been fulfilled:\nTstop\u2211 \u03c4=1 B\u03c4 = M. (16)\nWe the have\nE[ Tstop\u2211 t=1 (f t(wt)\u2212 f t(u))] \u2264 R(u)/\u03b1+ \u03b7ML2/\u03b1+ 14L||S|| \u221a M/\u03b12, (17)\nwhere \u2016S\u2016 denote the diameter of the convex set S and L denotes an upper bound on the Lipshitz parameter of all the functions ft.\nProof We can write\nE[ Tstop\u2211 t=1 (f t(wt)\u2212 f t(u))]\n= E [M/\u03b1\u2211 t=1 (f t(wt)\u2212 f t(u)) ] \u2212 ( E[ M/\u03b1\u2211 t=1 (f t(wt)\u2212 f t(u))]\u2212 E[ Tstop\u2211 t=1 (f t(wt)\u2212 f t(u))] ) \u2264 E[ M/\u03b1\u2211 t=1 (f t(wt)\u2212 f t(u))] + L||S|| \u00d7 E[|Tstop \u2212M/\u03b1|],\nwhere the last step follows from the fact that for any two u, v \u2208 S we have |f(u) \u2212 f(v)| \u2264 L||S||. The first term above can be bounded using Lemma 5. We thus need to upper-bound the expected value of ||Tstop \u2212 M/\u03b1||. As B\u03c4 \u2019s are Bernoulli(\u03b1) random variables, we expect that Tstop concentrates around M/\u03b1. By using the multiplicative Chernoff bound we have\nPr(Tstop \u2265M/\u03b1+ \u03b2) = Pr( M/\u03b1+\u03b2\u2211 \u03c4=1 B\u03c4 \u2264M)\n\u2264 Pr( M/\u03b1+\u03b2\u2211 \u03c4=1 B\u03c4 \u2264 (M + \u03b1\u03b2)(1\u2212 \u03b1\u03b2 M + \u03b1\u03b2 )) \u2264 exp(\u2212 (\u03b1\u03b2) 2\n3(M + \u03b1\u03b2) ).\nSimilarly, we can show that\nPr(Tstop \u2264M/\u03b1\u2212 \u03b2) \u2264 exp(\u2212 (\u03b1\u03b2)2\n3(M \u2212 \u03b1\u03b2) ).\nWe thus obtain,\nE[|Tstop \u2212M/\u03b1|] \u2264 M/\u03b1\u2211 j=0 Pr(Tstop \u2264M/\u03b1\u2212 j) + \u221e\u2211 j=0 Pr(Tstop \u2265M/\u03b1+ j)\n\u2264 M/\u03b1\u2211 j=0 exp(\u2212 (\u03b1j) 2 3(M \u2212 \u03b1j) ) + \u221e\u2211 j=0 exp(\u2212 (\u03b1j) 2 3(M + \u03b1j) )\n\u2264 2 \u221e\u2211 j=0 exp(\u2212 (\u03b1j) 2 3(M + \u03b1j) )\n\u2264 2 \u221e\u2211 k=0 \u221a M/\u03b12 exp(\u2212 Mk 2 3(M + k \u221a M) )\n\u2264 2 \u221a M/\u03b12 \u221e\u2211 k=0 exp(\u2212k 6 )\n\u2264 14 \u221a M/\u03b12.\nStep 3. Putting things together When the algorithm run by EXPj is using the doubling trick, for each round (with a block of size M ), the algorithm needs to be given M feedbacks (from the forecaster) until it switches to the next round (i.e. it doubles the block-length and restarts the algorithm). Therefore, due to the fact that feedback from the forecaster is sent with independent probability \u03b1, the total time needed for the algorithm to switch to the next round is as the one given in Lemma 6. As a result, the regret suffered in the current round is upper-bounded O( \u221a M/\u03b12) (from Lemma 6). Note here that the time spent\nin each round to give M feedbacks to the algorithm (i.e. Tstop in Lemma 6) is roughly M/\u03b1. Now, assume that the total time taken by the algorithm is T . The algorithm (which is given feedback with probability \u03b1 and plays according to the doubling trick) will be given feedback in T\u03b1 time units (on average). As a result, it is not hard to see that total regret (after summing up over all the rounds played by the algorithm and using Jensen) becomes \u221a T/\u03b1. Hence, the proposition is proved also for the OMD algorithms with regret O( \u221a T )."}, {"heading": "Appendix C. Proof of Theorem 3", "text": "In this section, we provide the proof of Theorem 3 for the no-regret guarantees of our algorithm LEARNEXP. We follow a step by step approach, beginning with the bounds on external regret of LEARNEXP.\nStep 1. Bounds on external regret of LEARNEXP By directly using the bounds of EXP3 algorithm, cf. Theorem 3.1 from Auer et al. (2002), we can state the following bounds on the external regret of our algorithm LEARNEXP against any expert EXPk where k \u2208 [N ]. Note that these bounds given by the external regret are only w.r.t. to the post hoc sequence of actions performed and losses observed during the execution of the algorithm\nT\u2211 t=1 E [ lt ( \u03c0it(x t,Htit,LEARNEXP) )] \u2212 E [ T\u2211 t=1 lt ( \u03c0k(x t,Htk,LEARNEXP) )] \u2264 c \u00b7 \u03b7 \u00b7 T + (logN) \u00b7N \u03b7\n(18)\nwhere c is a constant given by c = e\u2212 1.\nStep2. No-regret and smooth learning dynamics of the experts We note that during the execution of LEARNEXP in Algorithm 2, we have sparse feedbacks whereby the experts receive feedback instances sporadically at rate defined by \u03b1 = \u03b7N , cf. Section 4. Hence, by definition, we have Htj,LEARNEXP \u2261 Htj,\u03b1 \u2200j \u2208 [N ] where \u03b1 = \u03b7 N , cf. Section 4. By definition, the no-regret smooth learning dynamics of the expert k guarantees:\nE [ T\u2211 t=1 lt ( \u03c0k(x t,Htk,LEARNEXP) )] \u2212 E [ T\u2211 t=1 lt ( \u03c0k(x t,HTk,1) )] \u2264 O ( T \u00b7 ( \u03b1 \u00b7 T )\u03b2k\u22121) = O (T \u03b2k \u00b7N1\u2212\u03b2k \u03b71\u2212\u03b2k ) , (19)\nwhere \u03b2k is the parameter defining the rate of growth of regret, cf. Section 4.\nStep3. Putting it together Let us rewrite the regret of the algorithm, copying from Equation 2:\nREG(T, LEARNEXP) := T\u2211 t=1 E [ lt ( \u03c0it(x t,Htit,LEARNEXP) )] \u2212 min j\u2208[N ] E [ T\u2211 t=1 lt ( \u03c0j(x t,HTj,1) )]\n(20)\nCombining Eq.18 and Eq.19 from above, and using the definition of REG from Equation 20 above, we get:\nREG(T, LEARNEXP) \u2264 O ( \u03b7 \u00b7 T + (logN) \u00b7N\n\u03b7 + T \u03b2k \u00b7N1\u2212\u03b2k \u03b71\u2212\u03b2k\n) (21)\nStep4. Optimizing \u03b7 Next, we will optimize the value of \u03b7 in terms of T and N . Note that EXPk above corresponds to any expert. Hence, let us set k = j\u2217 where j\u2217 corresponds to the best expert EXPj\u2217 that we want to compete against. As per assumptions of the theorem, the best expert indeed has no-regret smooth learning dynamics with \u03b2j\u2217 \u2208 [0, 1]. Stating this in terms k = j\u2217, we can write down the regret as follows:\nREG(T, LEARNEXP) \u2264 O ( \u03b7 \u00b7 T + (logN) \u00b7N\n\u03b7 + T \u03b2j\u2217 \u00b7N1\u2212\u03b2j\u2217 \u03b71\u2212\u03b2j\u2217\n) (22)\nHowever, note that algorithm doesn\u2019t know \u03b2j\u2217 and hence cannot directly optimize the value of \u03b7. As per the theorem statement, the LEARNEXP is invoked with input \u03b2 \u2208 [0, 1] such that \u03b2 \u2265 \u03b2j\u2217 .\nStep4.1 Optimizing \u03b7 for known \u03b2j\u2217 , i.e., \u03b2 = \u03b2j\u2217 To begin with, let us first optimize \u03b7 for case when \u03b2 = \u03b2j\u2217 . In order to find the optimal dependency of \u03b7 on T , we set \u03b7 \u223c T\u2212z , and the value z will be found to minimize the external regret. By this choice of \u03b7, the following terms stated as the powers of T appear in (22):\n{T 1\u2212z, T z, T z+\u03b2j\u2217 \u00b7(1\u2212z)} (23)\nSolving for optimal value of z to minimize the power of T in the leading term, we get z = 1\u2212\u03b2j\u22172\u2212\u03b2j\u2217 . Next, we find the optimal dependency of \u03b7 on N . Note that, when \u03b2 = 0, we have optimal dependency of \u03b7 on N as (N \u00b7 log(N)) 1 2 . In general, the optimal dependency of \u03b7 to N can be found by setting \u03b7 \u223c N z , which gives us from (22) the following terms stated as the powers of N , where only the leading terms w.r.t. T are kept:\n{N z, N (1\u2212\u03b2j\u2217 )\u00b7(1\u2212z)} (24)\nSolving for optimal value of z to minimize the power of T in the leading term, we get z = 1\u2212\u03b2j\u22172\u2212\u03b2j\u2217 . For any \u03b2j\u2217 \u2208 [0, 1], we can thus write the optimal value of \u03b7 as:\n\u03b7 = T \u2212 1\u2212\u03b2j\u2217 2\u2212\u03b2j\u2217 \u00b7N 1\u2212\u03b2j\u2217 2\u2212\u03b2j\u2217 \u00b7 (logN)( 1 2 \u00b71{\u03b2j\u2217=0}) (25)\nBy keeping only the leading term of T , we can write this as follows:\nREG(T, LEARNEXP) \u2264 O ( T 1 2\u2212\u03b2j\u2217 \u00b7N 1 2\u2212\u03b2j\u2217 \u00b7 (logN)( 1 2 \u00b71{\u03b2j\u2217=0}) ) (26)\nStep4.2 Optimizing \u03b7 for unknown \u03b2j\u2217 , i.e., \u03b2 \u2265 \u03b2j\u2217 When \u03b2j\u2217 is not known exactly, and \u03b2 only upper bounds \u03b2j\u2217 , we can still optimize \u03b7 w.r.t. \u03b2 to get the same \u03b7 as stated above, replacing \u03b2j\u2217 by \u03b2j (note that 1/(2\u2212 \u03b2) is increasing in \u03b2). By keeping only the leading term of T , we can write the regret as follows:\nREG(T, LEARNEXP) \u2264 O ( T 1 2\u2212\u03b2 \u00b7N 1 2\u2212\u03b2 \u00b7 (logN)( 1 2 \u00b71{\u03b2=0}) ) (27)\nThis gives us the desired bound stated in Theorem 3."}], "references": [{"title": "Taming the monster: A fast and simple algorithm for contextual bandits", "author": ["Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert Schapire"], "venue": "In ICML,", "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2014}, {"title": "Corralling a band of bandit algorithms", "author": ["Alekh Agarwal", "Haipeng Luo", "Behnam Neyshabur", "Robert E. Schapire"], "venue": null, "citeRegEx": "Agarwal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2016}, {"title": "Online bandit learning against an adaptive adversary: from regret to policy regret", "author": ["Raman Arora", "Ofer Dekel", "Ambuj Tewari"], "venue": "In ICML,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches", "author": ["Baruch Awerbuch", "Robert D Kleinberg"], "venue": "In STOC,", "citeRegEx": "Awerbuch and Kleinberg.,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch and Kleinberg.", "year": 2004}, {"title": "Online choice of active learning algorithms", "author": ["Yoram Baram", "Ran El-Yaniv", "Kobi Luz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Baram et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Baram et al\\.", "year": 2004}, {"title": "Partial monitoring \u2013 Classification, regret bounds, and algorithms", "author": ["G\u00e1bor Bart\u00f3k", "Dean P Foster", "D\u00e1vid P\u00e1l", "Alexander Rakhlin", "Csaba Szepesv\u00e1ri"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2014}, {"title": "Optimal exploration-exploitation in a multi-armedbandit problem with non-stationary rewards", "author": ["Omar Besbes", "Yonatan Gur", "Assaf J. Zeevi"], "venue": "In NIPS,", "citeRegEx": "Besbes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Besbes et al\\.", "year": 2014}, {"title": "Contextual bandit algorithms with supervised learning guarantees", "author": ["Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E Schapire"], "venue": "In AISTATS,", "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Beygelzimer et al\\.", "year": 2011}, {"title": "Learning, regret minimization, and equilibria", "author": ["Avrim Blum", "Yishay Monsour"], "venue": null, "citeRegEx": "Blum and Monsour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Monsour.", "year": 2007}, {"title": "Regret analysis of stochastic and nonstochastic multiarmed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicolo Cesa-Bianchi"], "venue": "Machine Learning,", "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck and Cesa.Bianchi.", "year": 2012}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D.P. Helmbold", "D. Haussler", "R. Schapire", "M. Warmuth"], "venue": "Journal of the ACM,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "To groupon or not to groupon: The profitability of deep discounts", "author": ["Benjamin Edelman", "Sonia Jaffe", "Scott Duke Kominers"], "venue": "Marketing Letters,", "citeRegEx": "Edelman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Edelman et al\\.", "year": 2011}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In COLT,", "citeRegEx": "Freund and Schapire.,? \\Q1995\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1995}, {"title": "Bandit processes and dynamic allocation indices", "author": ["John C Gittins"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "Active learning by learning", "author": ["Wei-Ning Hsu", "Hsuan-Tien Lin"], "venue": "In AAAI,", "citeRegEx": "Hsu and Lin.,? \\Q2015\\E", "shortCiteRegEx": "Hsu and Lin.", "year": 2015}, {"title": "Multiarmed bandits with limited expert advice", "author": ["Satyen Kale"], "venue": "In COLT, pages 107\u2013122,", "citeRegEx": "Kale.,? \\Q2014\\E", "shortCiteRegEx": "Kale.", "year": 2014}, {"title": "The epoch-greedy algorithm for contextual multi-armed bandits", "author": ["J. Langford", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Langford and Zhang.,? \\Q2007\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2007}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire"], "venue": "In WWW,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "The weighted majority algorithm", "author": ["Nick Littlestone", "Manfred K. Warmuth"], "venue": "Info and Computation,", "citeRegEx": "Littlestone and Warmuth.,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth.", "year": 1994}, {"title": "Adaptive bandits: Towards the best history-dependent strategy", "author": ["Odalric-Ambrym Maillard", "R\u00e9mi Munos"], "venue": "In AISTATS,", "citeRegEx": "Maillard and Munos.,? \\Q2011\\E", "shortCiteRegEx": "Maillard and Munos.", "year": 2011}, {"title": "Tighter bounds for multi-armed bandits with expert advice", "author": ["H.B. McMahan", "M.J. Streeter"], "venue": "In COLT,", "citeRegEx": "McMahan and Streeter.,? \\Q2009\\E", "shortCiteRegEx": "McMahan and Streeter.", "year": 2009}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2011}, {"title": "Actively learning hemimetrics with applications to eliciting user preferences", "author": ["Adish Singla", "Sebastian Tschiatschek", "Andreas Krause"], "venue": "In ICML,", "citeRegEx": "Singla et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Singla et al\\.", "year": 2016}, {"title": "Adapting to a changing environment: the brownian restless bandits", "author": ["Aleksandrs Slivkins", "Eli Upfal"], "venue": "In COLT, pages 343\u2013354,", "citeRegEx": "Slivkins and Upfal.,? \\Q2008\\E", "shortCiteRegEx": "Slivkins and Upfal.", "year": 2008}, {"title": "Fast convergence of regularized learning in games", "author": ["Vasilis Syrgkanis", "Alekh Agarwal", "Haipeng Luo", "Robert E Schapire"], "venue": "In NIPS,", "citeRegEx": "Syrgkanis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Syrgkanis et al\\.", "year": 2015}, {"title": "Restless bandits: Activity allocation in a changing world", "author": ["P. Whittle"], "venue": "Journal of applied probability,", "citeRegEx": "Whittle.,? \\Q1988\\E", "shortCiteRegEx": "Whittle.", "year": 1988}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 14, "context": "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).", "startOffset": 143, "endOffset": 251}, {"referenceID": 3, "context": "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).", "startOffset": 143, "endOffset": 251}, {"referenceID": 11, "context": "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).", "startOffset": 143, "endOffset": 251}, {"referenceID": 10, "context": "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).", "startOffset": 143, "endOffset": 251}, {"referenceID": 13, "context": "As a concrete running example, we consider the problem of learning to offer personalized deals / discount coupons to users enabling new businesses to incentivize and attract more customers (Edelman et al., 2011; Singla et al., 2016).", "startOffset": 189, "endOffset": 232}, {"referenceID": 24, "context": "As a concrete running example, we consider the problem of learning to offer personalized deals / discount coupons to users enabling new businesses to incentivize and attract more customers (Edelman et al., 2011; Singla et al., 2016).", "startOffset": 189, "endOffset": 232}, {"referenceID": 13, "context": "However, these marketplaces (experts) themselves would be learning to optimize the coupons to offer, for instance, the discount price or the type of the coupon based on historic interactions with users (Edelman et al., 2011).", "startOffset": 202, "endOffset": 224}, {"referenceID": 5, "context": "Baram et al. (2004); Hsu and Lin (2015)), and many more.", "startOffset": 0, "endOffset": 20}, {"referenceID": 5, "context": "Baram et al. (2004); Hsu and Lin (2015)), and many more.", "startOffset": 0, "endOffset": 40}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002).", "startOffset": 120, "endOffset": 139}, {"referenceID": 3, "context": ", when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds.", "startOffset": 65, "endOffset": 84}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect).", "startOffset": 121, "endOffset": 381}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the \u201cbest\u201d expert.", "startOffset": 121, "endOffset": 929}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the \u201cbest\u201d expert.", "startOffset": 121, "endOffset": 961}, {"referenceID": 3, "context": "We consider an online setting similar to that of adversarial online learning using experts\u2019 advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the \u201cbest\u201d expert.", "startOffset": 121, "endOffset": 993}, {"referenceID": 14, "context": "Somewhat surprisingly, this hardness result holds when playing against an oblivious (non-adaptive) adversary and even if restricting the experts to be implementing some well-studied online learning algorithms, for instance, the HEDGE algorithm (Freund and Schapire, 1995).", "startOffset": 244, "endOffset": 271}, {"referenceID": 11, "context": "Somewhat surprisingly, this hardness result holds when playing against an oblivious (non-adaptive) adversary and even if restricting the experts to be implementing some well-studied online learning algorithms, for instance, the HEDGE algorithm (Freund and Schapire, 1995). The fundamental challenge leading to this hardness result arises from the fact that the forecaster\u2019s selection strategy affects the feedback sequences observed by the experts which in turn alters their learning process. \u201cGuided\u201d feedbacks and achieving no-regret guarantees. In order to circumvent this hardness result, we consider the following practical assumption: we allow the forecaster to \u201cguide\u201d the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., the selected expert it would not learn at time t for some time steps. For instance, in the motivating application of offering personalized deals to users, the deal-aggregator site (forecaster) often primarily interacts with users on behalf of the individual daily-deal marketplaces (experts) and hence can control the flow of feedback to these marketplaces. Alternatively, we note that this process of guiding and restricting the feedback can be achieved via coordination between the forecaster and the selected expert it with a 1-bit of communication at time t. Given this additional control, we design a novel algorithm LEARNEXP for the forecaster which carefully guides the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret of O(T 1 2\u2212\u03b2 ) after T time steps against an oblivious adversary for a rich family of no-regret learning algorithms that experts may be implementing. For the special case of multiarmed bandits, algorithm LEARNEXP is equivalent to that of the well-studied EXP3 algorithm and hence matches the optimal regret bound of \u0398(T 1 2 ). Connections to the existing results. Maillard and Munos (2011) studied the problem of competing against an adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models.", "startOffset": 245, "endOffset": 1973}, {"referenceID": 8, "context": "Bubeck and Cesa-Bianchi (2012) for a variant of the algorithm).", "startOffset": 0, "endOffset": 31}, {"referenceID": 0, "context": "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours.", "startOffset": 49, "endOffset": 71}, {"referenceID": 0, "context": "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours.", "startOffset": 49, "endOffset": 144}, {"referenceID": 0, "context": "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours. Note that, in applications where the experts directly receive feedback from the environment, implementing the strategies of Maillard and Munos (2011); Agarwal et al.", "startOffset": 49, "endOffset": 352}, {"referenceID": 0, "context": "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours. Note that, in applications where the experts directly receive feedback from the environment, implementing the strategies of Maillard and Munos (2011); Agarwal et al. (2016) would require the forecaster to communicate the probability p with which the expert it was selected at time t.", "startOffset": 49, "endOffset": 375}, {"referenceID": 11, "context": ", T (henceforth denoted as [T ]); for simplicity we assume that T is known in advance to the algorithm and the results in this paper can be extended to an unknown horizon via the usual doubling trick (Cesa-Bianchi and Lugosi, 2006).", "startOffset": 200, "endOffset": 231}, {"referenceID": 11, "context": "The feedback could be more general, for instance, receiving a binary signal of rejection or acceptance of the offered deal when an expert is implementing a dynamic pricing based algorithm via the partial monitoring framework (Cesa-Bianchi and Lugosi, 2006; Bart\u00f3k et al., 2014).", "startOffset": 225, "endOffset": 277}, {"referenceID": 6, "context": "The feedback could be more general, for instance, receiving a binary signal of rejection or acceptance of the offered deal when an expert is implementing a dynamic pricing based algorithm via the partial monitoring framework (Cesa-Bianchi and Lugosi, 2006; Bart\u00f3k et al., 2014).", "startOffset": 225, "endOffset": 277}, {"referenceID": 3, "context": "we will use lmax = 1 (Auer et al., 2002).", "startOffset": 21, "endOffset": 40}, {"referenceID": 14, "context": "We consider an oblivious (non-adaptive adversary) as is usual in the literature (Freund and Schapire, 1995; Auer et al., 2002), i.", "startOffset": 80, "endOffset": 126}, {"referenceID": 3, "context": "We consider an oblivious (non-adaptive adversary) as is usual in the literature (Freund and Schapire, 1995; Auer et al., 2002), i.", "startOffset": 80, "endOffset": 126}, {"referenceID": 3, "context": "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).", "startOffset": 88, "endOffset": 169}, {"referenceID": 11, "context": "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).", "startOffset": 88, "endOffset": 169}, {"referenceID": 10, "context": "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).", "startOffset": 88, "endOffset": 169}, {"referenceID": 2, "context": "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012). Given that the experts are learning entities, naturally the losses incurred at any time step are dependent on the history of the forecaster\u2019s actions as that history defines the current learning state of the individual experts. Given this subtle issue of history dependent losses, the usual notion of external regret does not provide any meaningful guarantees in terms of competing against the \u201cbest expert in hindsight\u201d (see below for a formal definition); the bounds given by the external regret are only w.r.t. the post hoc sequence of actions performed and losses observed during the execution of the algorithm (cf. Maillard and Munos (2011); Arora et al.", "startOffset": 89, "endOffset": 817}, {"referenceID": 2, "context": "Maillard and Munos (2011); Arora et al. (2012); McMahan and Streeter (2009) for more discussion on this).", "startOffset": 27, "endOffset": 47}, {"referenceID": 2, "context": "Maillard and Munos (2011); Arora et al. (2012); McMahan and Streeter (2009) for more discussion on this).", "startOffset": 27, "endOffset": 76}, {"referenceID": 14, "context": "Somewhat surprisingly, we prove this hardness result when playing against an oblivious (non-adaptive) adversary and when restricting the experts to be implementing the well-studied HEDGE algorithm (Freund and Schapire, 1995).", "startOffset": 197, "endOffset": 224}, {"referenceID": 14, "context": "The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.", "startOffset": 42, "endOffset": 69}, {"referenceID": 3, "context": "The selection strategy of the algorithm LEARNEXP is similar to the EXP family of algorithms, and in particular is equivalent to the EXP3 algorithm by Auer et al. (2002). The core idea of guiding the feedbacks observed by experts is presented in Lines 10,11, and 12.", "startOffset": 150, "endOffset": 169}, {"referenceID": 23, "context": "Proposition 2 A rich class of no-regret online learning algorithms based on gradient-descent style updates have smooth learning dynamics including the Online Mirror Descent family of algorithms with exact or estimated gradients (Shalev-Shwartz, 2011) and Online Convex Programming via greedy projections (Zinkevich, 2003).", "startOffset": 228, "endOffset": 250}, {"referenceID": 28, "context": "Proposition 2 A rich class of no-regret online learning algorithms based on gradient-descent style updates have smooth learning dynamics including the Online Mirror Descent family of algorithms with exact or estimated gradients (Shalev-Shwartz, 2011) and Online Convex Programming via greedy projections (Zinkevich, 2003).", "startOffset": 304, "endOffset": 321}, {"referenceID": 3, "context": "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.", "startOffset": 46, "endOffset": 119}, {"referenceID": 22, "context": "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.", "startOffset": 46, "endOffset": 119}, {"referenceID": 8, "context": "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.", "startOffset": 46, "endOffset": 119}, {"referenceID": 22, "context": "Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al.", "startOffset": 117, "endOffset": 145}, {"referenceID": 8, "context": "Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al.", "startOffset": 188, "endOffset": 214}, {"referenceID": 4, "context": ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al.", "startOffset": 37, "endOffset": 67}, {"referenceID": 5, "context": ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015).", "startOffset": 137, "endOffset": 176}, {"referenceID": 16, "context": ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015).", "startOffset": 137, "endOffset": 176}, {"referenceID": 25, "context": "(Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state.", "startOffset": 0, "endOffset": 26}, {"referenceID": 9, "context": "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al.", "startOffset": 20, "endOffset": 51}, {"referenceID": 5, "context": "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study of using expert advice for prediction problems, and Freund and Schapire (1995) introduced the algorithm HEDGE for the general problem of dynamically allocating resources among a set of options using expert advice.", "startOffset": 52, "endOffset": 79}, {"referenceID": 5, "context": "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study of using expert advice for prediction problems, and Freund and Schapire (1995) introduced the algorithm HEDGE for the general problem of dynamically allocating resources among a set of options using expert advice.", "startOffset": 52, "endOffset": 178}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback.", "startOffset": 16, "endOffset": 35}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006).", "startOffset": 16, "endOffset": 596}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios.", "startOffset": 16, "endOffset": 678}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy.", "startOffset": 16, "endOffset": 1759}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP.", "startOffset": 16, "endOffset": 2035}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP. Restless bandits are notoriously difficult to tackle (cf. (Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state.", "startOffset": 16, "endOffset": 2322}, {"referenceID": 3, "context": "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees\u2014we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP. Restless bandits are notoriously difficult to tackle (cf. (Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state. Besbes et al. (2014) considered another type of restless bandits with stochastic reward functions, however these distributions change adversarially with a budget on the allowed variation.", "startOffset": 16, "endOffset": 2495}, {"referenceID": 21, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012).", "startOffset": 129, "endOffset": 175}, {"referenceID": 2, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012).", "startOffset": 129, "endOffset": 175}, {"referenceID": 19, "context": "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).", "startOffset": 111, "endOffset": 176}, {"referenceID": 18, "context": "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).", "startOffset": 111, "endOffset": 176}, {"referenceID": 0, "context": "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).", "startOffset": 111, "endOffset": 176}, {"referenceID": 9, "context": "An orthogonal line of research studies the interaction of agents in multiplayer games where each agent uses a no-regret learning algorithm (Blum and Monsour, 2007; Syrgkanis et al., 2015).", "startOffset": 139, "endOffset": 187}, {"referenceID": 26, "context": "An orthogonal line of research studies the interaction of agents in multiplayer games where each agent uses a no-regret learning algorithm (Blum and Monsour, 2007; Syrgkanis et al., 2015).", "startOffset": 139, "endOffset": 187}, {"referenceID": 0, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret.", "startOffset": 156, "endOffset": 197}, {"referenceID": 0, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models.", "startOffset": 156, "endOffset": 412}, {"referenceID": 0, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models. However, none of the frameworks of non-oblivious/adaptive adversary listed above model learning dynamics in our setting: It would require an adversary with unbounded memory to apply the results of Arora et al. (2012), and an adversary with unbounded number of models to apply the techniques of Maillard and Munos (2011).", "startOffset": 156, "endOffset": 774}, {"referenceID": 0, "context": "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary\u2019s reward generation policy is restricted to a pre-specified set of known models. However, none of the frameworks of non-oblivious/adaptive adversary listed above model learning dynamics in our setting: It would require an adversary with unbounded memory to apply the results of Arora et al. (2012), and an adversary with unbounded number of models to apply the techniques of Maillard and Munos (2011). Contextual bandits.", "startOffset": 156, "endOffset": 877}, {"referenceID": 0, "context": ", 2010; Langford and Zhang, 2007; Agarwal et al., 2014). We refer the reader to the paper by McMahan and Streeter (2009) for more discussion on the connection between the framework of contextual bandits and learning using expert advice with bandit feedback.", "startOffset": 34, "endOffset": 121}, {"referenceID": 14, "context": "The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.", "startOffset": 42, "endOffset": 69}, {"referenceID": 23, "context": "1 in Shalev-Shwartz (2011), w\u0303\u03c4 = w\u03c4 , and the fact that R(\u00b7)/\u03b1 is a 1/(\u03b7\u03b1)-strongly convex function, we obtain", "startOffset": 5, "endOffset": 27}, {"referenceID": 3, "context": "1 from Auer et al. (2002), we can state the following bounds on the external regret of our algorithm LEARNEXP against any expert EXPk where k \u2208 [N ].", "startOffset": 7, "endOffset": 26}], "year": 2017, "abstractText": "In this paper, we study a variant of the framework of online learning using expert advice with limited/bandit feedback. We consider each expert as a learning entity, seeking to more accurately reflecting certain real-world applications. In our setting, the feedback at any time t is limited in a sense that it is only available to the expert i that has been selected by the central algorithm (forecaster), i.e., only the expert i receives feedback from the environment and gets to learn at time t. We consider a generic black-box approach whereby the forecaster does not control or know the learning dynamics of the experts apart from knowing the following no-regret learning property: the average regret of any expert j vanishes at a rate of at leastO(t j ) with tj learning steps where \u03b2 \u2208 [0, 1] is a parameter. In the spirit of competing against the best action in hindsight in multi-armed bandits problem, our goal here is to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to \u201cguide\u201d the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., not allowing the selected expert i to learn at time t for some time steps. Then, we design a novel no-regret learning algorithm LEARNEXP for this problem setting by carefully guiding the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret ofO(T 1 2\u2212\u03b2 ) after T time steps and matches the regret bound of \u0398(T 1 2 ) for the special case of multi-armed bandits.", "creator": "LaTeX with hyperref package"}}}