{"id": "1702.05659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2017", "title": "On Loss Functions for Deep Neural Networks in Classification", "abstract": "Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.", "histories": [["v1", "Sat, 18 Feb 2017 21:39:36 GMT  (1236kb,D)", "http://arxiv.org/abs/1702.05659v1", "Presented at Theoretical Foundations of Machine Learning 2017 (TFML 2017)"]], "COMMENTS": "Presented at Theoretical Foundations of Machine Learning 2017 (TFML 2017)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["katarzyna janocha", "wojciech marian czarnecki"], "accepted": false, "id": "1702.05659"}, "pdf": {"name": "1702.05659.pdf", "metadata": {"source": "CRF", "title": "On Loss Functions for Deep Neural Networks in Classification", "authors": ["Katarzyna Janocha", "Wojciech Marian Czarnecki"], "emails": ["kasiajanocha@gmail.com,", "lejlot@google.com"], "sections": [{"heading": "1 Introduction", "text": "For the last few years the Deep Learning (DL) research has been rapidly developing. It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11]. One of the biggest advantages of DL is enormous flexibility in designing each part of the architecture, resulting in numerous ways of putting priors over data inside the model itself [6], finding the most efficient activation functions [2] or learning algorithms [4]. However, to authors\u2019 best knowledge, most of the community still keeps one element nearly completely fixed \u2013 when it comes to classification, we use log loss (applied to softmax activation of the output of the network). In this paper we try to address this issue by performing both theoretical and empirical analysis of effects various loss functions have on the training of deep nets.\n1\nar X\niv :1\n70 2.\n05 65\n9v 1\n[ cs\n.L G\n] 1\n8 Fe\nb 20\n2 It is worth noting that Tang et al. [13] showed that well fitted hinge loss can outperform log loss based networks in typical classification tasks. Lee et al. [8] used squared hinge loss for classification tasks, achieving very good results. From slightly more theoretical perspective Choromanska et al. [1] also considered L1 loss as a deep net objective. However, these works seem to be exceptions, appear in complete separation from one another, and usually do not focus on any effect of the loss function but the final performance. Our goal is to show these losses in a wider context, comparing one another under various criteria and provide insights into when \u2013 and why \u2013 one should use them.\nTable 1: List of losses analysed in this paper. y is true label as one-hot encoding, y\u0302 is true label as +1/-1 encoding, o is the output of the last layer of the network, \u00b7(j) denotes jth dimension of a given vector, and \u03c3(\u00b7) denotes probability estimate.\nsymbol name equation"}, {"heading": "L1 L1 loss \u2016y \u2212 o\u20161", "text": ""}, {"heading": "L2 L2 loss \u2016y \u2212 o\u201622", "text": "L1 \u25e6 \u03c3 expectation loss \u2016y \u2212 \u03c3(o)\u20161 L2 \u25e6 \u03c3 regularised expectation loss1 \u2016y \u2212 \u03c3(o)\u201622 L\u221e \u25e6 \u03c3 Chebyshev loss maxj |\u03c3(o)(j) \u2212 y(j)| hinge hinge [13] (margin) loss \u2211 j max(0, 1 2 \u2212 y\u0302 (j)o(j))\nhinge2 squared hinge (margin) loss \u2211 j max(0, 1 2 \u2212 y\u0302 (j)o(j))2\nhinge3 cubed hinge (margin) loss \u2211 j max(0, 1 2 \u2212 y\u0302 (j)o(j))3\nlog log (cross entropy) loss \u2212 \u2211 j y (j) log \u03c3(o)(j)\nlog2 squared log loss \u2212 \u2211 j [y (j) log \u03c3(o)(j)]2 tan Tanimoto loss \u2212 \u2211 j \u03c3(o) (j)y(j)\n\u2016\u03c3(o)\u201622+\u2016y\u201622\u2212 \u2211 j \u03c3(o) (j)y(j)\nDCS Cauchy-Schwarz Divergence [3] \u2212 log \u2211 j \u03c3(o) (j)y(j)\n\u2016\u03c3(o)\u20162\u2016y\u20162\nThis work focuses on 12 loss functions, described in Table 1. Most of them appear in deep learning (or more generally \u2013 machine learning) literature, however some in slightly different context than a classification loss. In the following section we present new insights into theoretical properties of a couple of these losses and then provide experimental evaluation of resulting models\u2019 properties, including the effect on speed of learning, final performance, input data and label noise robustness as well as convergence for simple dataset under limited resources regime."}, {"heading": "2 Theory", "text": "Let us begin with showing interesting properties of Lp functions, typically considered as purely regressive losses, which should not be used in classification. L1 is often used as an auxiliary loss in deep nets to ensure sparseness of representations. Similarly, L2 is sometimes (however nowadays quite rarely) applied to weights in order to prevent them from growing to infinity. In this section we show that \u2013 despite their regression\n1See Proposition 1\n3 roots \u2013 they still have reasonable probabilistic interpretation for classification and can be used as a main classification objective.\nWe use the following notation: {(xi,yi)}Ni=1 \u2282 Rd\u00d7{0, 1}K is a training set, an iid sample from unknown P (x,y) and \u03c3 denotes a function producing probability estimates (usually sigmoid or softmax).\nProposition 1. L1 loss applied to the probability estimates p\u0302(y|x) leads to minimisation of expected misclassification probability (as opposed to maximisation of fully correct labelling given by the log loss). Similarly L2 minimises the same factor, but regularised with a half of expected squared L2 norm of the predictions probability estimates.\nProof. InK-class classification dependent variables are vectors yi \u2208 {0, 1}K with L1(yi) = 1, thus using notation pi = p\u0302(y|xi)\nL1 = 1N \u2211\ni \u2211 j |p(j)i \u2212 y (j) i | = 1N \u2211 i [\u2211 j y (j) i (1\u2212 p (j) i ) + (1\u2212 y (j) i )p (j) i ] = 1\nN \u2211 i [\u2211 j y (j) i \u2212 2 \u2211 j y (j) i p (j) i + \u2211 j p (j) i ] = 2\u2212 2 1 N \u2211 i [\u2211 j y (j) i p (j) i ] .\nConsequently if we sample label according to pi then probability that it actually matches one hot encoded label in yi equals P (l\u0302 = l|l\u0302 \u223c pi, l \u223c yi) = \u2211 j y (j) i p (j) i , and consequently\nL1 = 2\u2212 2 1N \u2211\ni [\u2211 j y (j) i p (j) i ] \u2248 \u22122EP (x,y) [ P (l\u0302 = l|l\u0302 \u223c pi, l \u223c yi) ] + const.\nAnalogously for L2,\nL2 = \u22122 1N \u2211\ni [\u2211 j y (j) i p (j) i ] + 1 N \u2211 i L2(yi) 2 + 1 N \u2211 i L2(pi) 2\n\u2248 \u22122EP (x,y) [ P (l\u0302 = l|l\u0302 \u223c pi, l \u223c yi) ] + EP (x,y)[L2(pi)2] + const.\nFor this reason we refer to these losses as expectation loss and regularised expectation loss respectively. One could expect that this should lead to higher robustness to the outliers/noise, as we try to maximise the expected probability of good classification as opposed to the probability of completely correct labelling (which log loss does). Indeed, as we show in the experimental section \u2013 this property is true for all losses sharing connection with expectation losses.\nSo why is using these two loss functions unpopular? Is there anything fundamentally wrong with this formulation from the mathematical perspective? While the following observation is not definitive, it shows an insight into what might be the issue causing slow convergence of such methods.\nProposition 2. L1, L2 losses applied to probabilities estimates coming from sigmoid (or softmax) have non-monotonic partial derivatives wrt. to the output of the final layer (and the loss is not convex nor concave wrt. to last layer weights). Furthermore, they vanish in both infinities, which slows down learning of heavily misclassified examples.\nProof. Let us denote sigmoid activation as \u03c3(x) = (1 + e\u2212x)\u22121 and, without loss of generality, compute partial derivative of L1 when network is\n4 presented with xp with positive label. Let op denote the output activation for this sample.\n\u2202(L1 \u25e6 \u03c3) \u2202o (op) = \u2202 \u2202o\n( |1\u2212 (1 + e\u2212o)\u22121| ) (op) = \u2212 e\u2212op\n(e\u2212op + 1)2\nlim o\u2192\u2212\u221e\n\u2212 e \u2212o\n(e\u2212o + 1)2 = 0 = lim o\u2192\u221e \u2212 e\n\u2212o\n(e\u2212o + 1)2 ,\nwhile at the same time \u2212 e 0\n(e0+1)2 = \u2212 1 4 < 0, completing the proof of both\nnon-monotonicity as well as the fact it vanishes when point is heavily misclassified. Lack of convexity comes from the same argument since second derivative wrt. to any weight in the final layer of the model changes sign (as it is equivalent to first derivative being non-monotonic). This comes directly from the above computations and the fact that op = \u3008w,hp\u3009+ b for some internal activation hp, layer weights w and layer bias b. In a natural way this is true even if we do not have any hidden layers (model is linear). Proofs for L2 and softmax are completely analogous.\nGiven this negative result, it seems natural to ask whether a similar property can be proven to show which loss functions should lead to fast convergence. It seems like the answer is again positive, however based on the well known deep learning hypothesis that deep models learn well when dealing with piece-wise linear functions. An interesting phenomenon in classification based on neural networks is that even in a deep linear model or rectifier network the top layer is often non-linear, as it uses softmax or sigmoid activation to produce probability estimates. Once this is introduced, also the partial derivatives stop being piece-wise linear. We believe that one can achieve faster, better convergence when we ensure that architecture together with loss function, produces a piecewise linear partial derivatives (but not constant) wrt. to final layer activations, especially while using first order optimisation methods. This property is true only for L2 loss and squared hinge loss (see Figure 1) among all considered ones in this paper.\nFinally we show relation between Cauchy-Schwarz Divergence loss and the log loss, justifying its introduction as an objective for neural nets.\nProposition 3. Cauchy-Schwarz Divergence loss is equivalent to cross entropy loss regularised with half of expected Renyi\u2019s quadratic entropy of the predictions.\n5 Proof. Using the fact that \u2200i\u2203!j : y(j)i = 1 we get that log \u2211 j p (j) i y\n(j) i =\u2211\nj y (j) i logp (j) i as well as \u2016yi\u20162 = 1, consequently\nDCS = \u2212 1N \u2211\ni log\n\u2211 j p (j) i y (j) i \u2016pi\u20162\u2016yi\u20162 = \u2212 1 N \u2211 i log \u2211 j p (j) i y (j) i + 1 N \u2211 i log \u2016pi\u20162\u2016yi\u20162\n= \u2212 1 N \u2211 i \u2211 j y (j) i logp (j) i + 1 2N \u2211 i log \u2016pi\u201622 \u2248 Llog + 12EP (x,y)[H2(pi)]"}, {"heading": "3 Experiments", "text": "We begin the experimental section with two simple 2D toy datasets. The first one is checkerboard \u2013 4 class classification problem where [-1,1] square is divided into 64 small squares with cyclic class assignment. The second one, spiral, is a 4 class generalisation of the well known 2 spirals dataset. Both datasets have 800 training and 800 testing samples. We train rectifier neural network having from 0 to 5 hidden layers with 200 units in each of them. Training is performed using Adam [4] with learning rate of 0.00003 for 60,000 iterations with batch size of 50 samples. In these simple\nproblems one can distinguish (Figure 2) two groups of losses \u2013 one able to fit to our very dense, low-dimensional data and one struggling to reduce error to 0. The second group consists of L1, Chebyshev, Tanimoto and expectation loss. This division becomes clear once we build a relatively deep model (5 hidden layers), while for shallow ones this distinction is not very clear (3 hidden layers) or is even completely lost (1 hidden layer or linear model). To further confirm the lack of ability to easily overfit we also ran an experiment in which we tried to fit 800 samples from uniform distribution over [\u22121, 1] with randomly assigned 4 labels and achieved analogous partitioning.\n6 During following, real data-based experiments, we focus on further investigation of loss functions properties emerging after application to deep models, as well as characteristics of the created models. In particular, we show that lack of ability to reduce training error to 0 is often correlated with robustness to various types of noise (despite not underfitting the data).\nLet us now proceed with one of the most common datasets used in deep learning community \u2013 MNIST [7]. We train network consisting from 0 to 5 hidden layers, each followed by ReLU activation function and dropout [12] with 50% probability. Each hidden layer consists of 512 neurons, and whole model is trained using Adam [4] with learning rate of 0.00003 for 100,000 iterations using batch size of 100 samples. There are\nfew interesting findings, visible on Figure 3. First, results obtained for a linear model (lack of hidden layers) are qualitatively different from all the remaining ones. For example, using regularised expectation loss leads to the strongest model in terms of both training accuracy and generalisation capabilities, while the same loss function is far from being the best one once we introduce non-linearities. This shows two important things: first \u2013 observations and conclusions drawn from linear models do not seem to transfer to deep nets, and second \u2013 there seems to be an interesting codependence between learning dynamics coming from training rectifier nets and loss functions used. As a side note, 93% testing accuracy, obtained by L2 \u25e6 \u03c3 and DCS, is a very strong result on MNIST using linear model without any data augmentation or model regularisation.\nSecond interesting observation regards the speed of learning. It appears that (apart from linear models) hinge2 and hinge3 losses are consistently the fastest in training, and once we have enough hidden layers (basically more than 1) also L2. This matches our theoretical analysis of\n7 these losses in the previous section. At the same time both expectation losses are much slower to train, which we believe to be a result of their vanishing partial derivatives in heavily misclassified points (Proposition 2). It is important to notice that while higher order hinge losses (especially 2nd) actually help in terms of both speed and final performance, the same property does not hold for higher order log losses. One possible explanation is that taking a square of log loss only reduces model\u2019s certainty in classification (since any number between 0 and 1 taken to 2nd power decreases), while for hinge losses the metric used for penalising marginoutliers is changed, and both L1 metric (leading to hinge) as well as any other Lp norm (leading to hinge\np) make perfect sense. Third remark is that pure L1 does not learn at all (ending up with 20% accuracy) due to causing serious \u201cjumps\u201d in the model because of its partial derivatives wrt. to net output always being either -1 or 1. Consequently, even after classifying a point correctly, we are still heavily penalised for it, while with losses like L2 the closer we are to the correct classification - the smaller the penalty is.\nFinally, in terms of generalisation capabilities margin-based losses seem to outperform the remaining families. One could argue that this is just a result of lack of regularisation in the rest of the losses, however we underline that all the analysed networks use strong dropout to counter the overfitting problem, and that typical L1 or L2 regularisation penalties do not work well in deep networks.\nFor CIFAR10 dataset we used a simple convnet, consisting of 3 layers of convolutions, each of size 5x5 and 64 filters, with ReLU activation functions, batch-normalisation and pooling operations in between them (max pooling after first layer and then two average poolings, all 3x3 with stride 2), followed by a single fully connected hidden layer with 128 ReLU neurons, and final softmax layer with 10 neurons. As one can see in Figure 3, despite completely different architecture than before, we obtain very similar results \u2013 higher order margin losses lead to faster training and significantly stronger models. Quite surprisingly \u2013 L2 loss also exhibits similar property. Expectation losses again learn much slower (with the regularised one \u2013 training at the level of log loss and unregularised \u2013 significantly worse). We would like to underline that this is a very simple architecture, far from the state-of-the art models for CIFAR10, however we wish to avoid using architectures which are heavily overfitted to the log loss. Furthermore, the aim of this paper is not to provide any stateof-the-art models, but rather to characterise effects of loss functions on deep networks.\nAs the final interesting result in these experiments, we notice that Cauchy-Schwarz Divergence as the optimisation criterion seems to be a consistently better choice than log loss. It performs equally well or better on both MNIST and CIFAR10 in terms of both learning speed and the final performance. At the same time this information theoretic measure is very rarely used in DL community, and rather exploited in shallow learning (for both classification [3] and clustering [10]).\nNow we focus on the impact these losses have on noise robustness of the deep nets. We start by performing the following experiment on previously trained MNIST classifiers: we add noise sampled from N (0, I) to each xi and observe how quickly (in terms of growing ) network\u2019s training accuracy drops (Figure 4). The first crucial observation is that both expectation losses perform very well in terms of input noise robustness. We believe that this is a consequence of what Proposition 1 showed about\n8\ntheir probabilistic interpretation \u2013 that they lead to minimisation of the expected misclassification, which is less biased towards outliers than log loss (or other losses that focus on maximisation of probability of correct labelling of all samples at the same time). For log loss a single heavily misclassified point has an enormous impact on the overall error surface, while for these two losses \u2013 it is minor. Secondly, margin based losses also perform well on this test, usually slightly worse than the expectation losses, but still better than log loss. This shows that despite no longer maximising the misclassification margin while being used in deep nets \u2013 they still share some characteristics with their linear origins (SVM). In another, similar experiment, we focus on the generalisation capabilities of the networks trained with increasing amount of label noise in the training set (Figure 4) and obtain analogous results, showing that robustness to the noise of expectation and margin losses is high for both input and label noise for deep nets, while again \u2013 slightly different results are obtained for linear models, where log loss is more robust than the margin-based ones. What is even more interesting, a completely non-standard loss function \u2013 Tanimoto loss \u2013 performs extremely well on this task. We believe that its exact analysis is one of the important future research directions."}, {"heading": "4 Conclusions", "text": "This paper provides basic analysis of effects the choice of the classification loss function has on deep neural networks training as well as their final characteristics. We believe the obtained results will lead to a wider\n9 adoption of various losses in DL work \u2013 where up till now log loss is unquestionable favourite.\nIn the theoretical section we show that, surprisingly, losses which are believed to be applicable mostly to regression, have a valid probabilistic interpretation when applied to deep network-based classifiers. We also provide theoretical arguments explaining why using them might lead to slower training, which might be one of the reasons DL practitioners have not yet exploited this path. Our experiments lead to two crucial conclusions. First, that intuitions drawn from linear models rarely transfer to highly-nonlinear deep networks. Second, that depending on the application of the deep model \u2013 losses other than log loss are preferable. In particular, for purely accuracy focused research, squared hinge loss seems to be a better choice at it converges faster as well as provides better performance. It is also more robust to noise in the training set labelling and slightly more robust to noise in the input space. However, if one works with highly noised dataset (both input and output spaces) \u2013 the expectation losses described in detail in this paper \u2013 seem to be the best choice, both from theoretical and empirical perspective.\nAt the same time this topic is far from being exhausted, with a large amount of possible paths to follow and questions to be answered. In particular, non-classical loss functions such as Tanimoto loss and CauchySchwarz Divergence are worth further investigation."}], "references": [{"title": "The loss surfaces of multilayer networks", "author": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Maximum entropy linear manifold for learning discriminative lowdimensional representation", "author": ["Wojciech Marian Czarnecki", "Rafal Jozefowicz", "Jacek Tabor"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Exploring strategies for training deep neural networks", "author": ["Hugo Larochelle", "Yoshua Bengio", "J\u00e9r\u00f4me Louradour", "Pascal Lamblin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Wavenet: A generative model for raw audio", "author": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "10 Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Information theoretic learning", "author": ["Jose C Principe", "Dongxin Xu", "John Fisher"], "venue": "Unsupervised adaptive filtering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1929}, {"title": "Deep learning using linear support vector machines", "author": ["Yichuan Tang"], "venue": "arXiv preprint arXiv:1306.0239,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11].", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11].", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11].", "startOffset": 221, "endOffset": 224}, {"referenceID": 9, "context": "It evolved from tricky pretraining routines [6] to a highly modular, customisable framework for building machine learning systems for various problems, spanning from image recognition [5], voice recognition and synthesis [9] to complex AI systems [11].", "startOffset": 247, "endOffset": 251}, {"referenceID": 5, "context": "One of the biggest advantages of DL is enormous flexibility in designing each part of the architecture, resulting in numerous ways of putting priors over data inside the model itself [6], finding the most efficient activation functions [2] or learning algorithms [4].", "startOffset": 183, "endOffset": 186}, {"referenceID": 1, "context": "One of the biggest advantages of DL is enormous flexibility in designing each part of the architecture, resulting in numerous ways of putting priors over data inside the model itself [6], finding the most efficient activation functions [2] or learning algorithms [4].", "startOffset": 236, "endOffset": 239}, {"referenceID": 3, "context": "One of the biggest advantages of DL is enormous flexibility in designing each part of the architecture, resulting in numerous ways of putting priors over data inside the model itself [6], finding the most efficient activation functions [2] or learning algorithms [4].", "startOffset": 263, "endOffset": 266}, {"referenceID": 11, "context": "[13] showed that well fitted hinge loss can outperform log loss based networks in typical classification tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] also considered L1 loss as a deep net objective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "L1 L1 loss \u2016y \u2212 o\u20161 L2 L2 loss \u2016y \u2212 o\u20162 L1 \u25e6 \u03c3 expectation loss \u2016y \u2212 \u03c3(o)\u20161 L2 \u25e6 \u03c3 regularised expectation loss \u2016y \u2212 \u03c3(o)\u20162 L\u221e \u25e6 \u03c3 Chebyshev loss maxj |\u03c3(o) \u2212 y| hinge hinge [13] (margin) loss \u2211 j max(0, 1 2 \u2212 \u0177 o) hinge squared hinge (margin) loss \u2211 j max(0, 1 2 \u2212 \u0177 o) hinge cubed hinge (margin) loss \u2211 j max(0, 1 2 \u2212 \u0177 o) log log (cross entropy) loss \u2212 \u2211 j y (j) log \u03c3(o) log squared log loss \u2212 \u2211 j [y (j) log \u03c3(o)]", "startOffset": 174, "endOffset": 178}, {"referenceID": 2, "context": "tan Tanimoto loss \u2212 \u2211 j \u03c3(o) y \u2016\u03c3(o)\u20162+\u2016y\u20162\u2212 \u2211 j \u03c3(o) (j)y(j) DCS Cauchy-Schwarz Divergence [3] \u2212 log \u2211 j \u03c3(o) y \u2016\u03c3(o)\u20162\u2016y\u20162", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "The first one is checkerboard \u2013 4 class classification problem where [-1,1] square is divided into 64 small squares with cyclic class assignment.", "startOffset": 69, "endOffset": 75}, {"referenceID": 3, "context": "Training is performed using Adam [4] with learning rate of 0.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "Let us now proceed with one of the most common datasets used in deep learning community \u2013 MNIST [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 10, "context": "We train network consisting from 0 to 5 hidden layers, each followed by ReLU activation function and dropout [12] with 50% probability.", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "Each hidden layer consists of 512 neurons, and whole model is trained using Adam [4] with learning rate of 0.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "At the same time this information theoretic measure is very rarely used in DL community, and rather exploited in shallow learning (for both classification [3] and clustering [10]).", "startOffset": 155, "endOffset": 158}, {"referenceID": 8, "context": "At the same time this information theoretic measure is very rarely used in DL community, and rather exploited in shallow learning (for both classification [3] and clustering [10]).", "startOffset": 174, "endOffset": 178}], "year": 2017, "abstractText": "Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design \u2013 one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors\u2019 opinion, underrepresented \u2013 while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.", "creator": "LaTeX with hyperref package"}}}