{"id": "1702.05865", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Hemingway: Modeling Distributed Optimization Algorithms", "abstract": "Distributed optimization algorithms are widely used in many industrial machine learning applications. However choosing the appropriate algorithm and cluster size is often difficult for users as the performance and convergence rate of optimization algorithms vary with the size of the cluster. In this paper we make the case for an ML-optimizer that can select the appropriate algorithm and cluster size to use for a given problem. To do this we propose building two models: one that captures the system level characteristics of how computation, communication change as we increase cluster sizes and another that captures how convergence rates change with cluster sizes. We present preliminary results from our prototype implementation called Hemingway and discuss some of the challenges involved in developing such a system.", "histories": [["v1", "Mon, 20 Feb 2017 05:51:18 GMT  (1623kb,D)", "http://arxiv.org/abs/1702.05865v1", "Presented at ML Systems Workshop at NIPS, Dec 2016"]], "COMMENTS": "Presented at ML Systems Workshop at NIPS, Dec 2016", "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.LG", "authors": ["xinghao pan", "shivaram venkataraman", "zizheng tai", "joseph gonzalez"], "accepted": false, "id": "1702.05865"}, "pdf": {"name": "1702.05865.pdf", "metadata": {"source": "CRF", "title": "Hemingway: Modeling Distributed Optimization Algorithms", "authors": ["Xinghao Pan", "Shivaram Venkataraman", "Zizheng Tai", "Joseph Gonzalez"], "emails": ["jegonzal}@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "With growing data sizes and the advent of cloud computing infrastructure [31], distributed machine learning is used in a number of applications like machine translation, computer vision, speech recognition etc. As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.\nThe performance (or time to converge to error) of distributed optimization algorithms depends on the cluster setup used for training. For example, assuming a data-parallel setup, the time taken for one iteration of full gradient descent depends on the time taken to compute the gradient in parallel and the time taken to communicate the gradient values. As the cluster size increases the computation time decreases but the communication time increases and thus choosing an optimal cluster size is important for optimal performance [31].\nHowever, in addition to performance, the convergence rates of algorithms also change based on the size of the cluster used. For example, in CoCoA [14], a communication efficient dual coordinate ascent algorithm, each machine executes a local learning procedure and the resulting dual vectors are then averaged across machines at the end of each iteration. With a fixed data size, using a larger number of machines will lower the time spent in local learning but lead to a worse convergence rate. Thus, as we increase the cluster size, the time per-iteration decreases but the number of iterations required to reach the desired error increases.\nFurther, as the computation vs. communication balance and convergence rates differ across algorithms (e.g., first-order methods [5] vs. second-order methods [26]), it is often hard to predict which algorithm will be the most appropriate for a given cluster setup. Finally, the convergence rates also depend on data properties. Thus while theoretical analysis can provide upper bounds on the number of iterations\n\u2217Joint first authors\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n70 2.\n05 86\n5v 1\n[ cs\n.D C\n] 2\n0 Fe\nb 20\nrequired, it is often difficult to translate this to how the algorithm will work in practice for a given dataset.\nThe combination of the above factors in conjunction with the dependence on the data, complicates the choice of optimization algorithm and cluster configuration. Choosing the best algorithm, cluster setup thus becomes a trial-and-error process and users have few tools that can guide them in this process.\nIn this paper, we propose addressing this problem by building a system that can model the convergence rate of various algorithms and thereby help users select the best one for their use case. To do this we propose modeling the convergence rate of algorithms as we scale the cluster size and we split our modeling into two parts: based on Ernest [31], we first build a computational model that helps us understand how the time taken per-iteration varies as we increase the scale of computation; we then build a separate model for the convergence rate as we scale the computation and we show how combining these two models can help us determine the optimal configuration.\nWe propose Hemingway, a prototype implementation and present initial evaluation results from running our system with CoCoA+ [19]. We also outline some of the remaining challenges in making such a system practical. These include designing data acquisition methods that can minimize the amount of data required to build the above mentioned models and also extending our work to non-convex domains like deep-learning."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Distributed computing", "text": "The widespread adoption of cloud computing platforms like Amazon EC2, Microsoft Azure, Google Compute Engine etc., means that users can now choose their computing substrate in terms of the number of cores, memory per machine and also in terms of the number of machines to use. However having additional choice comes with its own challenges; the performance of machine learning jobs can vary significantly based on the resources chosen and thus a number of recent efforts [13, 31] have focused on recommending the best cluster configuration for a given workload.\nOne of the important decisions users have to make is choosing the cluster size or the degree of parallelism to use. Having a higher degree of parallelism usually lowers the computation time but could increase the amount of time spent in communication. Specifically in the context of iterative optimization algorithms, varying the degree of parallelism changes the time taken per-iteration and we study its impact in Section 2.3."}, {"heading": "2.2 Distributed optimization algorithms", "text": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23]. These algorithms are typically iterative and each iteration can be expressed using a bulksynchronous step in a distributed framework like Hadoop MapReduce or Spark [33].\nOne of the main differences among the various algorithms is how their convergence rates change as the degree of parallelism increases. For, methods like full-gradient descent (GD) where the gradient is evaluated on all the data points at each iteration, the convergence rate remains the same irrespective of the parallelism. However this is not the case for stochastic methods like mini-batch SGD. For mini-batch SGD with batch size b, the optimization error after running for T iterations is O(1/ \u221a bT + 1/T ) [18, 8]. Thus although b times more examples are processed in an iteration, the mini-batch method only achieves a O( \u221a b) improvement in convergence when compared to serial SGD where one example is processed at a time. Thus as the mini-batch size is increased in large clusters, the convergence rate, in terms of number of examples examined, typically degrades.\nSimilar effects can be seen in other algorithms. Recent work in CoCoA [14], CoCoA+ [19] perform local updates using coordinate descent [28] and obtain convergence rates that only degrade with the number of machines rather than the mini-batch size. This is advantageous as the number of machines is usually smaller than the mini-batch size which is order of data size. Similar rates have also been shown for re-weighted stochastic gradient methods [35]. In summary we see that for stochastic methods while increasing the degree of parallelism can improve performance, the convergence rates degrade and thus users need to make a careful trade-off in choosing the appropriate configuration.\nFinally, the rates discussed in the previous paragraph are upper bounds and are usually applicable for the worst-case inputs. However in practice, input data can behave much better and it is difficult for users to accurately predict how each algorithm will perform for a new dataset."}, {"heading": "2.3 Case Study", "text": "To highlight the convergence and performance variations, we perform a simple binary classification experiment to predict a single digit (5) using the MNIST dataset. We ran CoCoA [14] on Apache Spark [33] with linear SVM as the loss function. We measure performance in terms of time per outer iteration of CoCoA and also compute the primal sub-optimality at the end of every iteration. The experiments were run on a eight-node YARN cluster where each machine has 48 cores, 256 GB RAM and 480 GB SSD storage. The machines are further partitioned into Spark executors that each have 4 cores and 20GB of RAM each. We vary the degree of parallelism by changing the number of executors used. We run the algorithm until the primal sub-optimality reaches 1e\u2212 4 or 500 iterations are completed and results from varying the degree of parallelism are shown in Figure 1.\nFrom Figure 1(a), we can see that while the time taken per iteration goes down as we scale from 1 to 32, but that performance degrades as we use more than 32 cores. Further, even in the regime where performance improves, we see that improvements are not linear; i.e. doubling the number of cores does not result in halving the time per iteration. Both of these effects are due to the computation vs. communication balance in the system. For MNIST, a small dataset with just 60000 rows, we see that communication costs start to dominate with higher number of cores and this leads to the poor scaling behavior. Similar effects have been observed in prior work for larger datasets as well [31].\nIn Figure 1(b) we see that how the primal sub-optimality across iterations changes as we vary the degree of parallelism. In this case we see that using 1 core means that the algorithm converges in around 10 iterations while using 16 cores takes around 50 iterations to converge to 1E \u2212 4 suboptimality. This shows how the convergence rate can degrade as we increase the degree of parallelism and why it is important for users to tune this appropriately.\nFinally, as known in theory, the observed convergence rate in practice also varies depending on the specific algorithm used. Figure 1(c) shows the convergence for CoCoA, CoCoA+, Splash [35] and parallel SGD with local updates while using 16 cores. From the figure we see that both CoCoA and CoCoA+ perform much better than SGD-based methods. We also see that while CoCoA+ converges faster in the first 50 iterations, CoCoA performs slightly better beyond 50 iterations. Thus based on the desired level of convergence and data properties the appropriate algorithm to use can differ."}, {"heading": "3 Modeling Optimization Algorithms", "text": "In the previous section we saw how the changing degree of parallelism affects performance and convergence. We next describe our approach to addressing this problem in Hemingway. We begin with high level goals for our system and then discuss how we can break down the problem into two parts: modeling the system and modeling the algorithm."}, {"heading": "3.1 Goals and assumptions", "text": "The goal of our system, Hemingway, is to develop an interface where users can specify an end-toend requirements and the system will automatically select the appropriate algorithm and degree of parallelism to use. Such a system can be used along with existing libraries of machine learning algorithms like MLlib [20], Vowpal Wabbit [16], SystemML [11] etc. Specifically we would like to support use cases of the form: given a relative error goal of , choose the fastest algorithm and configuration; or given a target latency of t seconds choose an algorithm that will achieve the minimum training loss. An idealized example of our system is shown in Figure 2.\nOur assumption is that the dataset used by the distributed optimization algorithm is static and that the algorithm is iterative with each iteration expressed as a bulk synchronous paralllel (BSP) [30] job2. We also assume homogeneous machines and network3. Further our current study is restricted to convex loss functions. We discuss extensions to these assumptions in Section 6."}, {"heading": "3.2 Overall model", "text": "We denote the objective value attained by an algorithm after running for time t when running on m machines as h(t,m). Our goal is to build a model that can predict the value of h so that we can compare different configurations and optimization algorithms. Our key insight in this work is that we can split this task into a decomposition of two models: a system-level model f(m) that returns the time taken per iteration given m machines and a model for convergence g(i,m) that predicts the objective value after i iterations given m machines. Thus the overall model can be obtained by combining our above two models: h(t,m) = g(t/f(m),m).\nThe main benefit of this approach is that we can train the two models independently and reuse them based on changes. For example if there are new machine types or networking hardware changes in a datacenter we can retrain just the system model and reuse the convergence model. Similarly if there are changes to the algorithm in terms of parameter tuning, we only retrain the convergence model."}, {"heading": "3.2.1 Modeling the system", "text": "To build the system level model, we propose re-using the approach in Ernest [31], a performance prediction framework that minimizes the time and resources spent in building a performance model. We summarize the main ideas in Ernest below and specifically on how it applies to ML algorithms.\nThe response variable in our model is the running time f(m) of each BSP iteration as a function of number of machines m. The functional form of f is guided by different components and how they\n2 In a BSP job, machines iteratively perform local computations prior to communicating information (typically an updated model or gradients) with other machines. Crucially, a synchronization barrier ensures all machines complete their computations and communications for the iteration before the next iteration begins\n3In practice, we can approximate this well using the same AWS instance type.\ninfluence the time taken for an iteration. The components include the time spent in computation, which scales inversely to the number of machines and time taken by common communication patterns like broadcast, tree-reduction and shuffles, which increase as we increase the number of machines. Putting these terms together the model for an algorithm like mini-batch SGD or CoCoA looks like\nf(m) = \u03b80 + \u03b81 \u00d7 (size/m) + \u03b82 \u00d7 log(m) + \u03b83 \u00d7m where size represents the amount of input data and m represents the number of machines used. Ernest fits the above model by measuring performance on small samples of data and using the model we can then extrapolate the time taken at larger scales. Results in [31] show that the prediction error for the time taken per-iteration of mini-batch SGD is within 12%, while using samples smaller than 10% of input data.\nFor certain ML algorithms, we may need to modify the above model to include terms that reflect other computation or communication patterns. For example while the computation costs in first order methods typically scale linearly with number of examples, using second order methods like SDNA [26] could incur super-linear computation or communication costs."}, {"heading": "3.2.2 Modeling algorithm convergence", "text": "Most popular optimization algorithms today take an iterative approach towards minimizing an objective value. We capture this behavior with a bivariate function g(i,m) that returns the objective value4 after the algorithm is executed for i iterations on m machines. Optimization algorithms are typically accompanied by analyses of upper bound convergence rates; these rates can help guide us in putting together a functional form for g(i,m). For example, CoCoA has a upper bound convergence rate of g(i,m) \u2264 ( 1\u2212 c0m )i c1, where c0 and c1 are data-dependent constants. We point out, however, that the actual observed convergence rates can differ from the theoretical upper bounds, so it is important not to overly constrain g\u2019s functional form. In this paper, we have assumed linear forms of g for ease of fitting with ordinary least squares or Lasso. For ease of fitting with ordinary least squares or Lasso, we assume in this paper a linear form for g = \u2211k j=1 \u03bbj\u03c6j(i,m), where \u03bbj\u2019s are parameters to be estimated, and \u03c6j(i,m)\u2019s are possibly non-linear features. Non-linear functional forms of g can, in general, be fitted by minimizing the squared error between the model and the data."}, {"heading": "4 Preliminary Experiments With CoCoA+", "text": "We demonstrate the ability of Hemingway to model algorithm convergence by fitting a linear model to an example run of CoCoA+. We used CoCoA+ to solve a binary classification problem on MNIST, and varied the degree of parallelism m from 1 to 128 in powers of 2. The algorithm was terminated when the primal sub-optimality reached 1e\u2212 4, or after 500 iterations. We then fit a linear model to log(P (i,m) \u2212 P \u2217) using using LassoCV from scikit-learn [24], where P (i,m) is the primal objective value at iteration i with m parallelism. A range of fractional, polynomial, and logarithmic terms were used as the features of our model.\nWe also collected data for estimating the Ernest system model using Amazon EC2 R3.xlarge instances with 30.5GB RAM and 4 cores each.\nFigure5 3(a) shows the fit of our learned Hemingway model, and illustrates that we are able to capture the convergence trends exhibited by CoCoA+. We were also able to combine the Ernest and Hemingway models together to capture the convergence trends as a function of time, as seen in Figure 3(b).\nFor the Hemingway model to be practically useful, we need to be able to use it for predicting convergence at unobserved settings of m and i. We consider two such scenarios below."}, {"heading": "4.1 Predicting for unobserved degree of parallelism", "text": "In the first scenario, we have collected convergence results for some values of m, and would like to predict the convergence trend for a yet-unobserved degree of parallelism. This can be simulated by a\n4 We are usually interested in the objective value, but the Hemingway approach is data-driven and thus sufficiently flexible to handle other metrics such as test classification accuracy, precision, recall, etc.\n5Plots showing the first 100 iterations are provided in Appendix A.\nleave-one-m-out cross validation with our data. For example, we predict the convergence g(i, 128) for m = 128 parallelism by using data collected from m = 1, 2, 4, 8, 16, 32, 64. Figure5 4(a) shows the resultant cross validated models are good fits for the true convergence. Hence, we are able to estimate the trend of convergence for unobserved values of m."}, {"heading": "4.2 Forward prediction", "text": "Secondly, we consider prediction of convergence at future iterations. Figures5 5(a) and 5(b) respectively show the fit of models for predicting 1 and 10 iterations ahead, given a window of 50 iterations in the past. In both cases, predictions become more accurate when i is sufficiently large to provide enough information for modeling.\nWe also applied the Ernest and Hemingway models in combination to predict convergence in future time. Figures5 6(a) and 6(b) show that our models can capture the convergence trends at 1s and 5s in the future."}, {"heading": "5 Related Work", "text": "Similar decoupling of systems and algorithmic efficiencies had been proposed in DimmWitted [34] for studying running times of optimization algorithms in NUMA machines. Tradeoffs of different access methods, and data and model replication techniques were examined in terms of their statistical and hardware efficiencies. In Hemingway, we similarly propose decoupling the statistical and hardware execution models, but additionally we also propose an automatic method to choose the best configuration given a new algorithm.\nA number of previous studies have looked at optimizing the performance of large scale machine learning algorithms. On the systems side, physical execution engines that can choose between row or column storage layouts [34], and choose between algorithms given a fixed iteration budget [29] have been proposed. Further systems like STRADS [15] also study the trade-offs between using model parallelism and data parallelism with parameter servers. In Hemingway we focus on black-box modeling convergence rates of distributed optimization algorithms as opposed to physical execution and our formulation can reuse the execution models developed by existing systems.\nRecently, distributed deep learning frameworks like Tensorflow [1] and MXNET [6] have implemented various execution strategies [21]. As a consequence, systems like [25] have been developed to model the cost of each operator and to choose the optimal number of CPUs, GPUs [12] given asynchronous execution. These systems are typically \u201cwhite-box,\u201d i.e. they exploit the characteristics of specific algorithms like asynchronous SGD and specific operators in a convolutional neural network. In Hemingway we proposed using a black-box model that can be applied across a number of distributed optimization algorithms.\nThere has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7]. In particular, [7] attempts to learn an optimizer for black-box functions. Meta-learning approaches typically require much more data and training than Hemingway, since the meta-learner models are usually deep neural networks with large numbers of parameters. Furthermore, the objective of these meta-learners is to minimize the underlying objective function, and usually do not model or account for the system efficiency."}, {"heading": "6 Challenges", "text": "While our initial experiments show promise in terms of the utility and insights that can be gained from modeling convergence rates, there are number of challenges we are addressing to make this system practical.\nTraining time. One of the important aspects of any modeling based approach is the amount of time it takes to train the model before it can be make useful suggestions to the user. While our initial experiments have shown that convergence rates can be extrapolated across iterations and partition sizes, we plan to study if techniques to minimize data acquisition like experiment design can be used to minimize the time spent in data collection.\nTraining resources. Closely related to the time it takes to train a model, is the amount of resources used to train a model. For cloud computing settings, this is especially important as launching a large number of machines to collect training data could be expensive. While prior work in Ernest [31] discussed how the system-level model can be trained using a small number of machines and data samples, we plan to investigate if similar approximations can be made for the convergence model. This would similar to bootstrap where we would try to extrapolate the convergence model on the entire dataset based on the rates observed on a random subset of the data.\nAdaptive algorithms. We believe that using our modeling approach we can also study the development on new algorithms that adapt based on the requirements. For example while using a large number of cores might be appropriate at the beginning while the function value is far from optimal, we can then adaptively change the degree of parallelism as we get closer to convergence. Decisions on whether it is worthwhile to make such a change and when such changes should be made can be taken using the models we build.\nAsynchronous algorithms. While BSP algorithms have clear synchronization barriers between iterations, the same is not true of asynchronous algorithms such as Hogwild! [27]. Nevertheless, many of these algorithms have a natural notion of an epoch, typically comprised of a single pass over the entire dataset. By using an epoch as a unit of work done, we believe it is still possible to model both the algorithm convergence and system performance. Further investigation is required to see if linear models will suffice, or if more complex modeling, e.g. using queueing theory, is required.\nNon-Convex loss functions. Finally while our current efforts are focused on convex loss functions like logistic or hinge, we also plan to study if similar ideas can be used to model optimization of non-convex functions used in settings like deep learning."}, {"heading": "7 Conclusion", "text": "In this paper we studied how distributed optimization algorithms scale in terms of performance and convergence rate and showed how choosing the optimal configuration could significantly impact training time. To address this, we propose Hemingway, a system that models the convergence rate for distributed optimization algorithms and we present some of the challenges involved in building such a system."}, {"heading": "A Additional Plots For CoCoA+ Experiments", "text": ""}], "references": [{"title": "et al", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo"], "venue": "TensorFlow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1601.01705", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "and N", "author": ["M. Andrychowicz", "M. Denil", "S. Gomez", "M.W. Hoffman", "D. Pfau", "T. Schaul"], "venue": "de Freitas. Learning to learn by gradient descent by gradient descent. In Advances in Neural Information Processing Systems, pages 3981\u20133989", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Designing neural network architectures using reinforcement learning", "author": ["B. Baker", "O. Gupta", "N. Naik", "R. Raskar"], "venue": "arXiv preprint arXiv:1611.02167", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "COMPSTAT", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "C", "author": ["T. Chen", "M. Li", "Y. Li", "M. Lin", "N. Wang", "M. Wang", "T. Xiao", "B. Xu"], "venue": "Zhang, , and Z. Zhang. MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. In Workshop on Machine Learning Systems (NIPS)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "and N", "author": ["Y. Chen", "M.W. Hoffman", "S.G. Colmenarejo", "M. Denil", "T.P. Lillicrap"], "venue": "de Freitas. Learning to learn for global optimization of black box functions. arXiv preprint arXiv:1611.03824", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "Journal of Machine Learning Research, 13(Jan):165\u2013202", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "RL\u0302 2: Fast reinforcement learning via slow reinforcement learning", "author": ["Y. Duan", "J. Schulman", "X. Chen", "P.L. Bartlett", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1611.02779", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, 12:2121\u20132159", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Systemml: Declarative machine learning on mapreduce", "author": ["A. Ghoting", "R. Krishnamurthy", "E. Pednault", "B. Reinwald", "V. Sindhwani", "S. Tatikonda", "Y. Tian", "S. Vaithyanathan"], "venue": "ICDE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Omnivore: An optimizer for multi-device deep learning on cpus and gpus", "author": ["S. Hadjis", "C. Zhang", "I. Mitliagkas", "D. Iter", "C. Re"], "venue": "arXiv preprint arXiv:1606.04487", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "No one (cluster) size fits all: automatic cluster sizing for data-intensive analytics", "author": ["H. Herodotou", "F. Dong", "S. Babu"], "venue": "SOCC", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["M. Jaggi", "V. Smith"], "venue": "Communication-efficient distributed dual coordinate ascent. In NIPS", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "STRADS: A Distributed Framework for Scheduled Model Parallel Machine Learning", "author": ["J.K. Kim", "Q. Ho", "S. Lee", "X. Zheng", "W. Dai", "G.A. Gibson", "E.P. Xing"], "venue": "Eurosys, pages 5:1\u20135:16", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "and A", "author": ["J. Langford", "L. Li"], "venue": "Strehl. Vowpal wabbit online learning project", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "et al", "author": ["M. Li", "D.G. Andersen"], "venue": "Communication efficient distributed machine learning with the parameter server. In NIPS", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient mini-batch training for stochastic optimization", "author": ["M. Li", "T. Zhang", "Y. Chen", "A.J. Smola"], "venue": "KDD, pages 661\u2013670", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["C. Ma", "V. Smith"], "venue": "Adding vs. averaging in distributed primal-dual optimization. In ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["X. Meng", "J. Bradley", "B. Yuvaz", "E. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D. Tsai", "M. Amde", "S. Owen"], "venue": "Mllib: Machine learning in apache spark. Journal of Machine Learning Research, 17(34)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Asynchrony begets momentum", "author": ["I. Mitliagkas", "C. Zhang", "S. Hadjis", "C. Re"], "venue": "with an application to deep learning. arXiv preprint arXiv:1605.09774", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Res: Regularized stochastic bfgs algorithm", "author": ["A. Mokhtari", "A. Ribeiro"], "venue": "IEEE Transactions on Signal Processing, 62(23)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["P. Moritz", "R. Nishihara", "M.I. Jordan"], "venue": "AISTATS", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u20132830", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Sdna: stochastic dual newton ascent for empirical risk minimization", "author": ["Z. Qu", "P. Richt\u00e1rik", "M. Tak\u00e1\u010d", "O. Fercoq"], "venue": "arXiv preprint arXiv:1502.02268", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "Advances in Neural Information Processing Systems, pages 693\u2013701", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss minimization", "author": ["S. Shalev-Shwartz", "T. Zhang"], "venue": "Journal of Machine Learning Research", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Keystoneml: Optimizing pipelines for large-scale advanced analytics", "author": ["E.R. Sparks", "S. Venkataraman", "T. Kaftan", "M.J. Franklin", "B. Recht"], "venue": "arXiv preprint arXiv:1610.09451", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "A bridging model for parallel computation", "author": ["L.G. Valiant"], "venue": "Communications of the ACM, 33(8):103\u2013111", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1990}, {"title": "Ernest: Efficient Performance Prediction for Large-Scale Advanced Analytics", "author": ["S. Venkataraman", "Z. Yang", "M. Franklin", "B. Recht", "I. Stoica"], "venue": "NSDI", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["T. Yang"], "venue": "NIPS", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "NSDI", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Dimmwitted: A study of main-memory statistical analytics", "author": ["C. Zhang", "C. R\u00e9"], "venue": "PVLDB, 7(12):1283\u20131294", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2014}, {"title": "Splash: User-friendly Programming Interface for Parallelizing Stochastic Algorithms", "author": ["Y. Zhang", "M.I. Jordan"], "venue": "CoRR, abs/1506.07552", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["M.A. Zinkevich", "M. Weimer"], "venue": "Parallelized stochastic gradient descent. In NIPS", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 29, "context": "With growing data sizes and the advent of cloud computing infrastructure [31], distributed machine learning is used in a number of applications like machine translation, computer vision, speech recognition etc.", "startOffset": 73, "endOffset": 77}, {"referenceID": 33, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 90, "endOffset": 102}, {"referenceID": 13, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 90, "endOffset": 102}, {"referenceID": 9, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 90, "endOffset": 102}, {"referenceID": 4, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 136, "endOffset": 139}, {"referenceID": 30, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 167, "endOffset": 179}, {"referenceID": 16, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 167, "endOffset": 179}, {"referenceID": 13, "context": "As a result, recent research has proposed a number of distributed optimization algorithms [35, 14, 10] that handle large input datasets [5] and minimize communication [32, 17, 14] to scale across large clusters.", "startOffset": 167, "endOffset": 179}, {"referenceID": 29, "context": "As the cluster size increases the computation time decreases but the communication time increases and thus choosing an optimal cluster size is important for optimal performance [31].", "startOffset": 177, "endOffset": 181}, {"referenceID": 13, "context": "For example, in CoCoA [14], a communication efficient dual coordinate ascent algorithm, each machine executes a local learning procedure and the resulting dual vectors are then averaged across machines at the end of each iteration.", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": ", first-order methods [5] vs.", "startOffset": 22, "endOffset": 25}, {"referenceID": 24, "context": "second-order methods [26]), it is often hard to predict which algorithm will be the most appropriate for a given cluster setup.", "startOffset": 21, "endOffset": 25}, {"referenceID": 29, "context": "To do this we propose modeling the convergence rate of algorithms as we scale the cluster size and we split our modeling into two parts: based on Ernest [31], we first build a computational model that helps us understand how the time taken per-iteration varies as we increase the scale of computation; we then build a separate model for the convergence rate as we scale the computation and we show how combining these two models can help us determine the optimal configuration.", "startOffset": 153, "endOffset": 157}, {"referenceID": 18, "context": "We propose Hemingway, a prototype implementation and present initial evaluation results from running our system with CoCoA+ [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "However having additional choice comes with its own challenges; the performance of machine learning jobs can vary significantly based on the resources chosen and thus a number of recent efforts [13, 31] have focused on recommending the best cluster configuration for a given workload.", "startOffset": 194, "endOffset": 202}, {"referenceID": 29, "context": "However having additional choice comes with its own challenges; the performance of machine learning jobs can vary significantly based on the resources chosen and thus a number of recent efforts [13, 31] have focused on recommending the best cluster configuration for a given workload.", "startOffset": 194, "endOffset": 202}, {"referenceID": 33, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 103, "endOffset": 114}, {"referenceID": 4, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 103, "endOffset": 114}, {"referenceID": 34, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 103, "endOffset": 114}, {"referenceID": 13, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 143, "endOffset": 155}, {"referenceID": 18, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 143, "endOffset": 155}, {"referenceID": 30, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 143, "endOffset": 155}, {"referenceID": 21, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 193, "endOffset": 201}, {"referenceID": 22, "context": "Large scale optimization algorithms used in practice include first-order methods based on parallel SGD [35, 5, 36], coordinate descent methods [14, 19, 32] and quasi-newton methods like L-BFGS [22, 23].", "startOffset": 193, "endOffset": 201}, {"referenceID": 31, "context": "These algorithms are typically iterative and each iteration can be expressed using a bulksynchronous step in a distributed framework like Hadoop MapReduce or Spark [33].", "startOffset": 164, "endOffset": 168}, {"referenceID": 17, "context": "For mini-batch SGD with batch size b, the optimization error after running for T iterations is O(1/ \u221a bT + 1/T ) [18, 8].", "startOffset": 113, "endOffset": 120}, {"referenceID": 7, "context": "For mini-batch SGD with batch size b, the optimization error after running for T iterations is O(1/ \u221a bT + 1/T ) [18, 8].", "startOffset": 113, "endOffset": 120}, {"referenceID": 13, "context": "Recent work in CoCoA [14], CoCoA+ [19] perform local updates using coordinate descent [28] and obtain convergence rates that only degrade with the number of machines rather than the mini-batch size.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "Recent work in CoCoA [14], CoCoA+ [19] perform local updates using coordinate descent [28] and obtain convergence rates that only degrade with the number of machines rather than the mini-batch size.", "startOffset": 34, "endOffset": 38}, {"referenceID": 26, "context": "Recent work in CoCoA [14], CoCoA+ [19] perform local updates using coordinate descent [28] and obtain convergence rates that only degrade with the number of machines rather than the mini-batch size.", "startOffset": 86, "endOffset": 90}, {"referenceID": 33, "context": "Similar rates have also been shown for re-weighted stochastic gradient methods [35].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "We ran CoCoA [14] on Apache Spark [33] with linear SVM as the loss function.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "We ran CoCoA [14] on Apache Spark [33] with linear SVM as the loss function.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "Similar effects have been observed in prior work for larger datasets as well [31].", "startOffset": 77, "endOffset": 81}, {"referenceID": 33, "context": "Figure 1(c) shows the convergence for CoCoA, CoCoA+, Splash [35] and parallel SGD with local updates while using 16 cores.", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Such a system can be used along with existing libraries of machine learning algorithms like MLlib [20], Vowpal Wabbit [16], SystemML [11] etc.", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "Such a system can be used along with existing libraries of machine learning algorithms like MLlib [20], Vowpal Wabbit [16], SystemML [11] etc.", "startOffset": 118, "endOffset": 122}, {"referenceID": 10, "context": "Such a system can be used along with existing libraries of machine learning algorithms like MLlib [20], Vowpal Wabbit [16], SystemML [11] etc.", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "Our assumption is that the dataset used by the distributed optimization algorithm is static and that the algorithm is iterative with each iteration expressed as a bulk synchronous paralllel (BSP) [30] job2.", "startOffset": 196, "endOffset": 200}, {"referenceID": 29, "context": "To build the system level model, we propose re-using the approach in Ernest [31], a performance prediction framework that minimizes the time and resources spent in building a performance model.", "startOffset": 76, "endOffset": 80}, {"referenceID": 29, "context": "Results in [31] show that the prediction error for the time taken per-iteration of mini-batch SGD is within 12%, while using samples smaller than 10% of input data.", "startOffset": 11, "endOffset": 15}, {"referenceID": 24, "context": "For example while the computation costs in first order methods typically scale linearly with number of examples, using second order methods like SDNA [26] could incur super-linear computation or communication costs.", "startOffset": 150, "endOffset": 154}, {"referenceID": 23, "context": "We then fit a linear model to log(P (i,m) \u2212 P \u2217) using using LassoCV from scikit-learn [24], where P (i,m) is the primal objective value at iteration i with m parallelism.", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "Similar decoupling of systems and algorithmic efficiencies had been proposed in DimmWitted [34] for studying running times of optimization algorithms in NUMA machines.", "startOffset": 91, "endOffset": 95}, {"referenceID": 32, "context": "On the systems side, physical execution engines that can choose between row or column storage layouts [34], and choose between algorithms given a fixed iteration budget [29] have been proposed.", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "On the systems side, physical execution engines that can choose between row or column storage layouts [34], and choose between algorithms given a fixed iteration budget [29] have been proposed.", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "Further systems like STRADS [15] also study the trade-offs between using model parallelism and data parallelism with parameter servers.", "startOffset": 28, "endOffset": 32}, {"referenceID": 0, "context": "Recently, distributed deep learning frameworks like Tensorflow [1] and MXNET [6] have implemented various execution strategies [21].", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "Recently, distributed deep learning frameworks like Tensorflow [1] and MXNET [6] have implemented various execution strategies [21].", "startOffset": 77, "endOffset": 80}, {"referenceID": 20, "context": "Recently, distributed deep learning frameworks like Tensorflow [1] and MXNET [6] have implemented various execution strategies [21].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "As a consequence, systems like [25] have been developed to model the cost of each operator and to choose the optimal number of CPUs, GPUs [12] given asynchronous execution.", "startOffset": 138, "endOffset": 142}, {"referenceID": 1, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 167, "endOffset": 173}, {"referenceID": 3, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 167, "endOffset": 173}, {"referenceID": 2, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 201, "endOffset": 210}, {"referenceID": 8, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 201, "endOffset": 210}, {"referenceID": 6, "context": "There has been renewed interest in the machine learning community in \u201cmeta-learning,\u201d where machine learning techniques are used to automatically learn optimal models [2, 4] or optimization algorithms [3, 9, 7].", "startOffset": 201, "endOffset": 210}, {"referenceID": 6, "context": "In particular, [7] attempts to learn an optimizer for black-box functions.", "startOffset": 15, "endOffset": 18}, {"referenceID": 29, "context": "While prior work in Ernest [31] discussed how the system-level model can be trained using a small number of machines and data samples, we plan to investigate if similar approximations can be made for the convergence model.", "startOffset": 27, "endOffset": 31}, {"referenceID": 25, "context": "While BSP algorithms have clear synchronization barriers between iterations, the same is not true of asynchronous algorithms such as Hogwild! [27].", "startOffset": 142, "endOffset": 146}], "year": 2017, "abstractText": "Distributed optimization algorithms are widely used in many industrial machine learning applications. However choosing the appropriate algorithm and cluster size is often difficult for users as the performance and convergence rate of optimization algorithms vary with the size of the cluster. In this paper we make the case for an ML-optimizer that can select the appropriate algorithm and cluster size to use for a given problem. To do this we propose building two models: one that captures the system level characteristics of how computation, communication change as we increase cluster sizes and another that captures how convergence rates change with cluster sizes. We present preliminary results from our prototype implementation called Hemingway and discuss some of the challenges involved in developing such a system.", "creator": "LaTeX with hyperref package"}}}