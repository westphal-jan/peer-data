{"id": "1702.06166", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2017", "title": "Bayesian Boolean Matrix Factorisation", "abstract": "Boolean matrix factorisation (BooMF) infers interpretable decompositions of a binary data matrix into a pair of low-rank, binary matrices: One containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for BooMF and derive a Metropolised Gibbs sampler that facilitates very efficient parallel posterior inference. Our method outperforms all currently existing approaches for Boolean Matrix factorization and completion, as we show on simulated and real world data. This is the first method to provide full posterior inference for BooMF which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering, and crucially it improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11,000 genes on commodity hardware.", "histories": [["v1", "Mon, 20 Feb 2017 20:31:39 GMT  (355kb,D)", "http://arxiv.org/abs/1702.06166v1", null], ["v2", "Sat, 25 Feb 2017 14:17:44 GMT  (414kb,D)", "http://arxiv.org/abs/1702.06166v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG cs.NA q-bio.GN q-bio.QM stat.ME", "authors": ["tammo rukat", "christopher c holmes", "michalis k titsias", "christopher yau"], "accepted": true, "id": "1702.06166"}, "pdf": {"name": "1702.06166.pdf", "metadata": {"source": "CRF", "title": "Bayesian Boolean Matrix Factorisation", "authors": ["Tammo Rukat", "Chris C. Holmes", "Michalis K. Titsias", "Christopher Yau"], "emails": ["tammo.rukat@stats.ox.ac.uk", "cholmes@stats.ox.ac.uk", "mtitsias@aueb.gr", "c.yau@bham.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Boolean Matrix Factorization (BooMF) aims to decompose a binary data matrixX\u2208{0, 1}N\u00d7D into an approximate Boolean product of two low rank matrices, Z\u2208{0, 1}N\u00d7L and U\u2208{0, 1}D\u00d7L. The Boolean product is a special case of a matrix product between binary matrices where all values larger than zero are set to one:\nxnd = L\u2228 l=1 znl \u2227 uld . (1)\n\u2217tammo.rukat@stats.ox.ac.uk \u2020cholmes@stats.ox.ac.uk \u2021mtitsias@aueb.gr \u00a7c.yau@bham.ac.uk\nar X\niv :1\n70 2.\n06 16\n6v 1\n[ st\nat .M\nL ]\n2 0\nBooMF provides a framework for learning from binary data where the inferred codes U provide a basis and the indicator variables Z encode the presence or absence of these codes. This representation is illustrated in the calculator digits example in Fig. 1. We can think of BooMF as clustering with joint assignments, where each observation is assigned to a subset of L cluster centroids or codes. The L-dimensional indicators provide a compact representation of which codes were allocated to each observation. As stated in eq. (1), a feature xnd takes a value of one if it equals one in any of the assigned codes.\nBooMF can have many real-world applications ranging from topic modelling (Blei, 2012) to collaborating filtering (Su & Khoshgoftaar, 2009) and computer vision (La\u0301zaro-Gredilla et al., 2016). In this paper we will consider a challenging application in the analysis of high-throughput single cell Genomics data where BooMF is used to identify latent gene signatures (codes) that correspond to key cellular pathways or biological processes from large gene expression datasets consisting of 1.3 million cells across 11,000 genes. Genes are expressed if one or more relevant biological processes are active, which is naturally modelled by the Boolean OR operation. We introduce the OrMachine, a Bayesian approach to BooMF, and fit the model using a fast and scalable Metropolised Gibbs sampling algorithm. We show that this formulation significantly outperforms the previous state-of-the-art message passing approaches for learning BooMF models (Ravanbakhsh et al., 2016). Secondly, we introduce multi-layered extensions of the Bayesian BooMF that can capture hierarchical dependencies in the latent representations."}, {"heading": "2 Related Work", "text": "We give a brief overview of closely related methods. The Discrete Basis Problem (Miettinen et al., 2006) provides a greedy heuristic algorithm to solve BooMF without recourse to an underlying probabilistic model. It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014). In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process. Point estimates are inferred by deterministic annealing. Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes. In contrast to the Boolean OR, the likelihood of an observation increases with the number of active hidden codes. The Indian Buffet process serves as non-parametric prior over the latent space and a Gibbs sampler infers the distribution over the unbounded number of hidden causes.\nA similar approach to ours is the work by Ravanbakhsh et al. (2016). The authors tackle BooMF using a probabilistic graphical model and derive a message passing algorithm to perform MAP inference. Their method is shown to have state-of-the-art performance for BooMF and completion. It therefore serves us as baseline benchmark in these task. The message passing approach has recently been employed by La\u0301zaro-Gredilla et al. (2016) in a hierarchical network combined with pooling\nlayers to infer the building blocks of binary images."}, {"heading": "3 The OrMachine", "text": ""}, {"heading": "3.1 Model Formulation", "text": "The OrMachine is a probabilistic generative model for Boolean matrix factorisation. A matrix of N binary observations xn \u2208 {0, 1}D is generated from a discrete mixture of L binary codes ul \u2208 {0, 1}D. Binary latent variables znl denote whether or not code l is used in generating a particular observation n. The probability for that observation to be one is greater than 1/2 if codes and latent variables in at least one latent dimension both equal one; conversely, if there exists no dimension where codes and latent variables both equal one, the probability for the observation to be one is less than 1/2. The exact magnitude of this probability is inferred from the data and, for later notational convenience, is parametrised as the logistic sigmoid of a global dispersion parameter \u03c3(\u03bb)=(1+e\u2212\u03bb)\u22121, with \u03bb \u2208 R+. Next we give a full description of the likelihood and prior distributions used in the OrMachine.\nThe likelihood function is factorised across the N observations and D features with each factor given by\np(xnd|u, z, \u03bb) = \u03c3(\u03bb); if x=min(1,uTd zn)1\u2212\u03c3(\u03bb); if x 6=min(1,uTd zn) (2) = \u03c3 [ \u03bbx\u0303nd ( 1\u2212 2\n\u220f l (1\u2212 znluld)\n)] , (3)\nwhere tilde denotes the {0, 1} \u2192 {\u22121, 1} mapping so that for any binary variable x \u2208 {0, 1}, x\u0303=2x\u22121. The expression inside the parentheses of eq. (3) encodes the OR operation and evaluates to 1 if znl=uld=1 for at least one l, and to \u22121 otherwise. The dispersion parameter controls the noise in the generative process, in the sense that as \u03bb \u2192 \u221e, all probabilities tend to 0 or 1 and the model describes a deterministic Boolean matrix product. Note that the likelihood can be computed efficiently from expression (3) as we describe in detail in the next section.\nWe further assume independent Bernoulli priors on all variables uld and znl. Such priors can allow to promote denseness or sparsity in codes and latent variables. Notice that the designation of U as codes and Z as latent variables is not necessary since inside the model these matrices appear in a completely symmetric manner. If we transpose the matrix of observations X , then codes and latent variables merely swap roles.\nFinally, we do not place a prior on the dispersion parameter \u03bb, but maximise it using an EM-type algorithm described below."}, {"heading": "3.2 Fast Posterior Inference", "text": "The full joint distribution of all data and random variables is given by\np(X,U ,Z, \u03bb) = p(X|U ,Z, \u03bb)p(U )p(Z) . (4)\nThe full conditional for znl (and analogous for uld) is:\np(znl|rest) = \u03c3 [ \u03bbz\u0303nl\n\u2211 d x\u0303nduld \u220f l\u2032 6=l (1\u2212znl\u2032ul\u2032d) + logit(p(znl))\n] . (5)\nNotice that the independent Bernoulli prior enters the expression as an additive term inside the sigmoid function. This term vanishes for the uninformative Bernoulli prior p(z) = 1/2.\nThe form of eq. (5) allows for computationally efficient evaluation of the conditionals. The underlying principle is that once certain conditions are met, the result of the full conditional is known without considering the remainder of a variable\u2019s Markov blanket. For instance, when computing updates for znl, terms in the sum over d necessarily evaluate to zero if one of the following conditions is met: (i) uld = 0 or (ii) znl\u2032ul\u2032d = 1 for some l\u2032 6= l. This leads to Algorithm 1 for fast evaluation of the conditionals.\nAlgorithm 1 Computation of the full conditional of znl accumulator = 0 for d in 1, . . . , D do\nif uld = 0 then continue (next iteration over d) end if for l\u2032 in 1, . . . , L do\nif l\u2032 6= l and znl\u2032 = 1 and ul\u2032d = 1 then continue (next iteration over d)\nend if end for accumulator = accumulator + x\u0303nd\nend for p(znl|rest) = \u03c3 (\u03bb \u00b7 z\u0303nl \u00b7 accumulator)\nTo infer the posterior distribution over all variables uld and znl we could iteratively sample from the above conditionals using standard Gibbs sampling. In practise we use a modification of this procedure which is referred to as Metropolised Gibbs sampler and was proposed by Liu (1996). We always propose to flip the current state resulting in a Hastings acceptance probability equal to the mutation probability of p(z|rest)/(1 \u2212 p(z|rest)). This expression is always larger than the full\nAlgorithm 2 Sampling from the OrMachine for i in 1, . . . ,max-iters do\nfor n in 1, . . . ,N (in parallel) do for l in 1, . . . ,L do\nCompute p(znl|rest) following Algorithm 1 Flip znl with probability [p(znl|rest)\u22121\u22121]\u22121\nend for end for for d in 1, . . . , d (in parallel) do\nfor l in 1, . . . ,L do Compute p(uld|rest) following Algorithm 1 Flip uld with probability [p(uld|rest)\u22121\u22121]\u22121\nend for end for Set \u03bb to its MLE according to eq. (7).\nend for\nconditional itself and is therefore, by Peskun\u2019s theorem (Peskun, 1973), guaranteed to yield Monte Carlo estimates with lower variance.\nAfter every sweep through all variables, the dispersion parameter \u03bb is updated to maximize the likelihood akin to the M-step of a Monte Carlo EM algorithm. Specifically, given the current values of the codes U and latent variables Z we can compute how many observations xnd are correctly predicted by the model,\nP = \u2211 n,d I\n[ xnd = 1\u2212\n\u220f l (1\u2212 znluld)\n] . (6)\nThis allows to rewrite the likelihood as \u03c3(\u03bb)P\u03c3(\u2212\u03bb)ND\u2212P which can be subsequently maximized with respect to \u03bb to yield the update\n\u03c3(\u03bb\u0302) = P\nND . (7)\nThe alternation between sampling (U ,Z) and updating the dispersion parameter is carried out up to convergence; see Algorithm 2 for all steps of the procedure."}, {"heading": "3.3 Dealing with Missing Data", "text": "We can handle unobserved data, by marginalising the likelihood over the missing observations. More precisely, if X = (Xobs,Xmis) is the decomposition of the full matrix into the observed part Xobs\nand the missing partXmis, after marginalisation the initial likelihood p(X|U ,Z, \u03bb) from eq. (4) simplifies to p(Xobs|U ,Z, \u03bb). Then, a standard implementation can be based on indexing the observed components inside matrix X and modifying the inference procedure so that the posterior conditionals of znl and uld involve only sums over observed elements in rows and columns of X . A simpler and completely equivalent implementation, which we follow in our experiments, is to represent the data as x\u0303nd \u2208 {\u22121, 0, 1} where missing observations are encoded as zeros, each contributing the constant factor \u03c3(0)=1/2 to the full likelihood, so that\np(X|U ,Z, \u03bb) = C p(Xobs|U ,Z, \u03bb),\nwhere C is a constant. Thus, the missing values have no contribution to the posterior over U and Z which is also clear from the form of the full conditionals in eq. (5) that depend on a sum weighted by xnds. Furthermore, for the update of the dispersion parameter in eq. (7), we need to subtract the number of all missing observations in the denominator. The dispersion now indicates the fraction of correctly predicted observations in the observed data.\nFollowing the inference procedure, we can impute missing data based on a Monte Carlo estimate of the predictive distribution of some unobserved xnd:\n1 S S\u2211 s=1 p(xnd|U (s),Z(s), \u03bb\u0302),\nwhere each (U (s),Z(s)) is a posterior sample. A much faster approximation of the predictive distribution can be obtained by p(xns|U\u0302 , Z\u0302, \u03bb\u0302) where we simply plug in the posterior mean estimates for (U ,Z) into the predictive distribution. For the simulated data in Section 4.2 we find both methods to perform equally well, and therefore follow the second, faster approach for all remaining experiments."}, {"heading": "3.4 Multi-Layer OrMachine", "text": "BooMF learns patterns of correlation in the data. In analogy to multi-layer neural networks, we can build a hierarchy of correlations by applying another layer of factorisation to the factor matrix Z. This is reminiscent of the idea of deep exponential families, as introduced by Ranganath et al. (2015). The ability to learn features at different levels of abstraction is commonly cited as an explanation for the success that deep neural networks have across many domains of application (Lin & Tegmark, 2016; Goodfellow et al., 2016). In the present setting, with stochasticity at every step of the generative process and posterior inference, we are able to infer truly meaningful and interpretable hierarchies of abstraction.\nTo give an example, we determine the optimal multi-layer architecture for representing the calculator digit toy dataset as introduced in Fig. 1. We observe 50 digits and consider 70% of the data points randomly as unobserved. We then train multi-layer OrMachines with various depths and layer\nwidths, drawing 100 samples following 200 iterations of burn-in. In order to enforce distributed representations, we choose independent Bernoulli sparsity priors for the codes: p(uld)=0.01, 0.05, 0.1 for each layer, respectively. Superior performance in reconstructing the unobserved data is achieved by a 3-hidden layer architecture with hidden layers of size L1=7, L2=4, L3=2. This 3-layer model reduces the reconstruction error from 1.4% to 0.4% compared to the single-layer model with width L=7. Maximum likelihood estimates of the dispersion for the three layers are \u03bb\u0302 = [1.0, 0.93, 0.8]. The first layers infers the seven bars that compose all digits. We plot the probabilities that each prototype induces in the observation layer, given by the one-hot activations of zl=1...L in Fig. 2. They are depicted alongside the average posterior mean of the representations for each digit in the training data, constructed from the prototypes. This example illustrates, that the multi-layer OrMachine infers interpretable higher-order correlations and exploits them to achieve significant improvements in missing data imputation."}, {"heading": "3.5 Practical Implementation and Speed", "text": "The algorithm is implemented in Python with the core sampling routines in compiled Cython. The binary data is represented as {\u22121, 1} with missing data encoded as 0. This economical representation of data and variables as integer types simplifies computations considerably. Algorithm 1 is implemented in parallel across the observations [n] = {1, . . . , N} and conversely updates for uld are implemented in parallel across all features [d] = {1, . . . , D}.\nThe computation time scales linearly in each dimension. A single sweep through high-resolution calculator digits toy dataset withND=1.7\u00d7106 data points and L=7 latent dimensions takes approximately 1 second on a desktop computer. A single sweep through the approximately 1.4\u00d71010 data points presented in the biological example in Section 5.2 with L=2 latent dimensions takes approximately 5 minutes executed on 24 computing cores. For all examples presented here 10\u201320 iterations suffice for the algorithm to converge to a (local) posterior mode."}, {"heading": "4 Experiments on Simulated Data", "text": "In this section we probe the performance of the OrMachine (OrM) at random matrix factorisation and completion tasks. Message passing (MP) has been shown to compare favourably with other state-of-the-art methods for BooMF (Ravanbakhsh et al., 2016) and is therefore the focus of our comparison. The following settings for MP and the OrM are used throughout our experiments: For MP, we use the Python implementation provided by the authors. We also proceed with their choice of hyper-parameters settings as experimentation with different learning rates and maximum number of iterations did not lead to any improvements. For the OrMachine, we initialise the parameters uniformly at random and run 100 iterations of the Metropolised Gibbs sampler as burn-in phase. We then draw 100 samples that our posterior mean and MAP estimates are based on. Note that around\n10 sampling steps are usually sufficient for convergence."}, {"heading": "4.1 Random Matrix Factorisation", "text": "We generate a quadratic matrix X\u2208{0, 1}N\u00d7N of rank L by taking the Boolean product of two random N\u00d7L factor matrices. The Boolean product X of two rank L binary matrices that are sampled iid from a Bernoulli distribution with parameter p has an expected value of E(X)=1\u2212(1\u2212p2)L. Since we generally prefer X to be neither sparse nor dense, we fix its expected density to d=0.5, unless stated otherwise. This ensures that a simple bias in either method is not met with reward. Bits in the data are flipped at random with probabilities ranging from 5% to 45%. Factor matrices of the correct underlying dimension are inferred and the data is reconstructed from the inferred factorisation. An example of this task is shown in Fig. 3. Results for the reconstruction error, defined as the fraction of correctly reconstructed data points, are depicted in Fig. 4. The OrM outperforms MP under all conditions. All experiments were repeated 10 times, with error bars denoting standard deviations. Fig. 4 (top) reproduces the experimental settings of Fig. 2 in Ravanbakhsh et al. (2016) and shows that the OrMachine consistently enables error-free reconstruction of a 1000\u00d71000 matrix of rank L=5 for up to 30% bit flip probability. Notably, MP performs worse for smaller noise levels. It was hypothesised by Ravanbakhsh et al. (2016) that symmetry breaking at higher noise levels helps message passage to converge to a better solution. This illustrates the general difficulty for belief propagation to converge to optimal solutions in densely connected graphs, which the Gibbs sampler seems to circumvent.\nFig. 4 (middle) demonstrates the consistently superior performance of the OrMachine for a more challenging example of 100\u00d7100 matrices of rank 7. The reconstruction performance of both methods is similar for lower noise levels, while the OrMachine consistently outperforms MP for larger noise levels. For biased data with E[xnd]=0.7 in Fig. 4 (bottom), we observe a similar pattern with a larger performance gap for higher noise levels. Even for a bit flip-probability of 50% the OrMachine retains a reconstruction error of 32%, which is achieved by levering the bias in the data. Such behaviour can be further encouraged by setting each variable\u2019s Bernoulli prior to its expected value based on the density of the data matrix in an Empirical-Bayes fashion."}, {"heading": "4.2 Random Matrix Completion", "text": "We further investigate the problem of matrix completion or collaborative filtering, where bits of the data matrix are unobserved and reconstructed from the inferred factor matrices. Following the procedure outlined in Section 4.1, we generate random matrices of rank 5 and size 250\u00d7250. We only observe a random subset of the data, ranging from 0.5% and 3.5%. The missing data is reconstructed from the inferred factor matrices. As shown in Fig. 5, the OrMachine outperforms message passing throughout. The plot indicates means and standard deviations from 10 repetitions of the whole\nexperiment. Notably, the OrMachine does not only provide a MAP estimate but also an estimate of the posterior probability for each xnd. Fig. 5 (bottom) shows an estimate of the density of the posterior means for the correctly and incorrectly completed matrix entries. The distribution of incorrect predictions peaks around a probability of 1/2, indicating that the OrMachine\u2019s uncertainty about its reconstruction provides further useful information about the missing data. For instance, this information can be used to control for false positives or false negatives, simply by setting a threshold for the posterior mean."}, {"heading": "5 Experiments on Real World Data", "text": ""}, {"heading": "5.1 MovieLens Matrix Completion", "text": "We investigate the OrMachine\u2019s performance for collaborative filtering on a real world dataset. The MovieLens-1M dataset1 contains 106 integer film ratings from 1 to 5 from 6000 users for 4000 films, i.e. 1/24 of the possible ratings are available. Similarly, the MovieLens 100k dataset contains 943 users and 1682 films. Following Ravanbakhsh et al. (2016), we binarise the data taking the global mean as threshold. We observe only a fraction of the available data, varying from 1% to 95%, and reconstruct the remaining available data following the procedure in Section 4.2 with L=2 latent dimensions. Reconstruction accuracies are given as fractions of correctly reconstructed unobserved ratings in Table 1.\nThe given values are means from 10 randomly initialised runs of each algorithm. The corresponding standard deviations are always smaller than 0.2%. The OrMachine is more accurate than message passing in all cases, except for the 1M dataset with 95% available ratings. The OrMachine\u2019s advantages is particularly big if only little data is observed. Increasing the latent dimension L to values of 3 or 4 yields no consistent improvement, while a further increase is met with diminishing returns.\nWe achieve the best performance for a two-layer OrMachine, with different architectures performing best for different amounts of observed data. An OrMachine with two hidden layers of sizes 4 and 2 respectively yields the best average performance. As indicated in Table 1, it provides better results throughout but exceeds the performance of the shallow OrMachine rarely by more than 1%. This indicates that there is not much higher order structure in the data and is unsurprising, given the sparsity of the observations and the low dimensionality of the first hidden layer.\nWe illustrate a further advantage of full posterior inference for collaborative filtering. We can choose a threshold for how likely we want a certain prediction to take a certain value and trade off false with true positives. A corresponding ROC curve for the MovieLens 100k dataset, where 10% of the available data was observed, is shown in Fig. 6.\n1The MovieLens dataset is available online: https://grouplens.org/datasets/movielens/."}, {"heading": "5.2 Explorative Analysis of Single Cell Gene Expression Profiles", "text": "Single-cell RNA expression analysis is a revolutionary experimental technique, that facilitates the measurement of gene expression on the level of a single cell (Blainey & Quake, 2014). In recent years this has led to the discovery of new cell types and to a better understanding of tissue heterogeneity (Trapnell, 2015). The latter is particularly relevant in cancer research where it helps to understand the cellular composition of a tumour and its relationship to disease progression and treatment (Patel et al., 2014). Here we apply the OrMachine to binarised gene expression profiles of about 1.3 million cells for about 28 thousand genes per cell. Cell specimens were obtained from cortex, hippocampus, and subventricular zone of E18 (embryonic day 18) mice, the data is publicly available2. Only 7% of the data points are non-zero. We set all non-zero expression levels to one, retaining the essential information of whether or not a particular gene is expressed. We remove genes that are expressed in fewer than 1% of cells with roughly 11 thousand genes remaining. This leaves us with approximately 1.4 \u00b7 1010 data points. We apply the OrMachine for latent dimensions L = 2, . . . , 10. The algorithm converges to a posterior mode after 10\u201320 iteration, taking roughly an hour on a 4-core desktop computer and 10\u201330 minutes on a cluster with 24 cores. We draw 125 samples and discard the first 25 as burn-in.\nFactorisations with different latent dimensionality form hierarchies of representations, where features that appear together in codes for lower dimensions are progressively split apart when moving to a higher dimensional latent space. We illustrate our approach to analysing the inferred factorisations on calculator digits in Fig. 7. Each row corresponds to an independently trained OrMachine with the L increasing from 3 to 7. We observe denser patterns dividing up consecutively until only the seven constituent bars remain. This is a form of hierarchical clustering without imposing any hierarchical structure on the data, in contrast to traditional hierarchical clustering methods. We perform the same analysis on the single cell gene expression data with the results for both, gene patterns and specimen pattern shown in Fig. 8. Furthermore, we run a gene set enrichment analysis for the genes that are unique to each inferred code, looking for associated biological states. This is done using the Enrichr analysis tool (Chen et al., 2013) and a mouse gene atlas (Su et al., 2004). Biological states are denoted together with the logarithm to base 10 of their adjusted p-value. Increasing the latent dimensionality leads to a more distributed representation with subtler, biologically plausible patterns. The columns in Fig. 8 are ordered to emphasise the hierarchical structure within the gene sets and their assignments. For example, in the first column for L=5 and second column for L=6, a gene set with significant overlap to two biological processes (olfactory bulb and hippocampus) splits into two gene sets each corresponding to one of the two processes. In the specimen assignments (8B) this is associated with an increase in posterior uncertainty as to which cell expresses this property. Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013). This examples illustrates\n2https://support.10xgenomics.com\nthe OrMachine\u2019s ability to scale posterior inference to massive datasets. It enables the discovery of readily interpretable patterns, representations and hierarchies, all of which are biologically plausible."}, {"heading": "6 Conclusion", "text": "We have developed the OrMachine, a probabilistic model for Boolean matrix factorisation. The extremely efficient Metropolised Gibbs sampler outperforms state of the art methods in matrix factorisation and completion. It is the first method that infers posterior distributions for Boolean matrix factorisation, which is highly relevant in practical applications. Despite full posterior inference, the proposed methods scales to very large datasets. We have shown that tenths of billions of data points can be handled on commodity hardware. The OrMachine can readily accommodate missing data and prior knowledge.\nLayers of OrMachines can be stacked, akin to deep belief networks, inferring representations at different levels of abstraction. This leads to improved reconstruction performance in simulated and real world data. Future work includes further experiments on the ability to learn deep probabilistic abstractions. We have inferred the optimal model architecture based on reconstruction performance for hold\u2013out data, leaving more principled methods for future work."}, {"heading": "7 Acknowledgements", "text": "T.R. is supported by a studentship from the UK Engineering and Physical Sciences Research Council and F. Hoffman La-Roche. C.Y. is supported by a UK Medical Research Council New Investigator Research Grant (Ref. No. MR/L001411/1), the Wellcome Trust Core Award Grant Number 090532/Z/09/Z, the John Fell Oxford University Press (OUP) Research Fund. We thank Dr. Satu Nahkuri for support and advice regarding the development of this work."}], "references": [{"title": "Fast algorithms for mining association rules", "author": ["Agrawal", "Rakesh", "Srikant", "Ramakrishnan"], "venue": "In Proc. 20th Int. Conf. Very Large Data Bases, VLDB,", "citeRegEx": "Agrawal et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 1994}, {"title": "Dissecting genomic diversity, one cell at a time", "author": ["Blainey", "Paul C", "Quake", "Stephen R"], "venue": "Nat. Methods,", "citeRegEx": "Blainey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Blainey et al\\.", "year": 2014}, {"title": "Probabilistic topic models", "author": ["Blei", "David M"], "venue": "Communications of the ACM,", "citeRegEx": "Blei and M.,? \\Q2012\\E", "shortCiteRegEx": "Blei and M.", "year": 2012}, {"title": "Enrichr: Interactive and collaborative html5 gene list enrichment analysis tool", "author": ["Chen", "Edward Y", "Tan", "Christopher M", "Kou", "Yan", "Duan", "Qiaonan", "Wang", "Zichen", "Meirelles", "Gabriela", "Clark", "Neil R", "Ma\u2019ayan", "Avi"], "venue": "BMC Bioinformatics,", "citeRegEx": "Chen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "L1 and chl1 cooperate in thalamocortical axon targeting", "author": ["G.P. Demyanenko", "P.F. Siesser", "A.G. Wright", "L.H. Brennaman", "U. Bartsch", "M. Schachner", "P.F. Maness"], "venue": "Cerebral Cortex,", "citeRegEx": "Demyanenko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Demyanenko et al\\.", "year": 2010}, {"title": "Deep learning. Book in preparation for", "author": ["Goodfellow", "Ian", "Bengio", "Yoshua", "Courville", "Aaron"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Hierarchical compositional feature learning", "author": ["L\u00e1zaro-Gredilla", "Miguel", "Liu", "Yi", "Phoenix", "D Scott", "George", "Dileep"], "venue": "arXiv preprint arXiv:1611.02252,", "citeRegEx": "L\u00e1zaro.Gredilla et al\\.,? \\Q2016\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla et al\\.", "year": 2016}, {"title": "Why does deep and cheap learning work so well? 2016", "author": ["Lin", "Henry W", "Tegmark", "Max"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "peskun\u2019s theorem and a modified discrete-state gibbs sampler", "author": ["Liu", "J. Miscellanea"], "venue": null, "citeRegEx": "Liu and Miscellanea.,? \\Q1996\\E", "shortCiteRegEx": "Liu and Miscellanea.", "year": 1996}, {"title": "Robo1 and robo2 cooperate to control the guidance of major axonal tracts in the mammalian forebrain", "author": ["G. Lopez-Bendito", "N. Flames", "L. Ma", "C. Fouquet", "Meglio", "T. Di", "A. Chedotal", "M. Tessier-Lavigne", "O. Marin"], "venue": "Journal of Neuroscience,", "citeRegEx": "Lopez.Bendito et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lopez.Bendito et al\\.", "year": 2007}, {"title": "Mdl4bmf: Minimum description length for boolean matrix factorization", "author": ["Miettinen", "Pauli", "Vreeken", "Jilles"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "Miettinen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Miettinen et al\\.", "year": 2014}, {"title": "The Discrete Basis Problem, pp. 335\u2013346", "author": ["Miettinen", "Pauli", "Mielik\u00e4inen", "Taneli", "Gionis", "Aristides", "Das", "Gautam", "Mannila", "Heikki"], "venue": null, "citeRegEx": "Miettinen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Miettinen et al\\.", "year": 2006}, {"title": "Single-cell rna-seq highlights intratumoral heterogeneity in primary glioblastoma", "author": ["A.P. Patel", "I. Tirosh", "J.J. Trombetta", "A.K. Shalek", "S.M. Gillespie", "H. Wakimoto", "D.P. Cahill", "B.V. Nahed", "W.T. Curry", "R.L. Martuza", "D.N. Louis", "O. Rozenblatt-Rosen", "M.L. Suva", "A. Regev", "B.E. Bernstein"], "venue": "Science, 344(6190):1396\u20131401,", "citeRegEx": "Patel et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Patel et al\\.", "year": 2014}, {"title": "Optimum monte-carlo sampling using markov chains", "author": ["Peskun", "Peter H"], "venue": "Biometrika, 60(3):607\u2013612,", "citeRegEx": "Peskun and H.,? \\Q1973\\E", "shortCiteRegEx": "Peskun and H.", "year": 1973}, {"title": "Selenoprotein w expression and regulation in mouse brain and neurons", "author": ["Raman", "Arjun V", "Pitts", "Matthew W", "Seyedali", "Ali", "Hashimoto", "Ann C", "Bellinger", "Frederick P", "Berry", "Marla J"], "venue": "Brain and Behavior,", "citeRegEx": "Raman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Raman et al\\.", "year": 2013}, {"title": "Deep exponential families", "author": ["Ranganath", "Rajesh", "Tang", "Linpeng", "Charlin", "Laurent", "Blei", "David M"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Boolean matrix factorization and noisy completion via message passing", "author": ["Ravanbakhsh", "Siamak", "P\u00f3czos", "Barnab\u00e1s", "Greiner", "Russell"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Ravanbakhsh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ravanbakhsh et al\\.", "year": 2016}, {"title": "Multi-assignment clustering for boolean data", "author": ["Streich", "Andreas P", "Frank", "Mario", "Basin", "David", "Buhmann", "Joachim M"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning - ICML \u201909,", "citeRegEx": "Streich et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Streich et al\\.", "year": 2009}, {"title": "A gene atlas of the mouse and human protein-encoding transcriptomes", "author": ["A.I. Su", "T. Wiltshire", "S. Batalov", "H. Lapp", "K.A. Ching", "D. Block", "J. Zhang", "R. Soden", "M. Hayakawa", "G. Kreiman", "M.P. Cooke", "J.R. Walker", "J.B. Hogenesch"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Su et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Su et al\\.", "year": 2004}, {"title": "A survey of collaborative filtering techniques", "author": ["Su", "Xiaoyuan", "Khoshgoftaar", "Taghi M"], "venue": "Adv. in Artif. Intell.,", "citeRegEx": "Su et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Su et al\\.", "year": 2009}, {"title": "Defining cell types and states with single-cell genomics", "author": ["Trapnell", "Cole"], "venue": "Genome Research,", "citeRegEx": "Trapnell and Cole.,? \\Q2015\\E", "shortCiteRegEx": "Trapnell and Cole.", "year": 2015}, {"title": "Expression profiling reveals differential gene induction underlying specific and nonspecific memory for pheromones in mice", "author": ["Upadhya", "Sudarshan C", "Smith", "Thuy K", "Brennan", "Peter A", "Mychaleckyj", "Josyf C", "Hegde", "Ashok N"], "venue": "Neurochemistry International,", "citeRegEx": "Upadhya et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Upadhya et al\\.", "year": 2011}, {"title": "A non-parametric bayesian method for inferring hidden causes", "author": ["Wood", "Frank", "Griffiths", "Thomas", "Ghahramani", "Zoubin"], "venue": null, "citeRegEx": "Wood et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2012}, {"title": "Gene expression patterns of hippocampus and cerebral cortex of senescence-accelerated mouse treated with huang-lian-jie-du decoction", "author": ["Zheng", "Yue", "Cheng", "Xiao-Rui", "Zhou", "Wen-Xia", "Zhang", "Yong-Xiang"], "venue": "Neuroscience Letters,", "citeRegEx": "Zheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2008}, {"title": "2016), who also provide comparison to other state-of the art", "author": ["Ravanbakhsh"], "venue": null, "citeRegEx": "Ravanbakhsh,? \\Q2016\\E", "shortCiteRegEx": "Ravanbakhsh", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "BooMF can have many real-world applications ranging from topic modelling (Blei, 2012) to collaborating filtering (Su & Khoshgoftaar, 2009) and computer vision (L\u00e1zaro-Gredilla et al., 2016).", "startOffset": 159, "endOffset": 189}, {"referenceID": 16, "context": "We show that this formulation significantly outperforms the previous state-of-the-art message passing approaches for learning BooMF models (Ravanbakhsh et al., 2016).", "startOffset": 139, "endOffset": 165}, {"referenceID": 11, "context": "The Discrete Basis Problem (Miettinen et al., 2006) provides a greedy heuristic algorithm to solve BooMF without recourse to an underlying probabilistic model.", "startOffset": 27, "endOffset": 51}, {"referenceID": 0, "context": "It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014).", "startOffset": 39, "endOffset": 61}, {"referenceID": 17, "context": "In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process.", "startOffset": 58, "endOffset": 80}, {"referenceID": 0, "context": "It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014). In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process. Point estimates are inferred by deterministic annealing. Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes.", "startOffset": 40, "endOffset": 526}, {"referenceID": 0, "context": "It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014). In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process. Point estimates are inferred by deterministic annealing. Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes. In contrast to the Boolean OR, the likelihood of an observation increases with the number of active hidden codes. The Indian Buffet process serves as non-parametric prior over the latent space and a Gibbs sampler infers the distribution over the unbounded number of hidden causes. A similar approach to ours is the work by Ravanbakhsh et al. (2016). The authors tackle BooMF using a probabilistic graphical model and derive a message passing algorithm to perform MAP inference.", "startOffset": 40, "endOffset": 929}, {"referenceID": 0, "context": "It is based on association rule mining (Agrawal et al., 1994) and has more recently been extended to automatically select the optimal dimensionality of the latent space based on the minimum description length principle (Miettinen & Vreeken, 2014). In contrast, multi assignment clustering for Boolean data (Streich et al., 2009) leverages on a probabilistic model for BooMF, adding a further global noise source to the generative process. Point estimates are inferred by deterministic annealing. Similarly, Wood et al. (2012) develop a probabilistic model to infer hidden causes. In contrast to the Boolean OR, the likelihood of an observation increases with the number of active hidden codes. The Indian Buffet process serves as non-parametric prior over the latent space and a Gibbs sampler infers the distribution over the unbounded number of hidden causes. A similar approach to ours is the work by Ravanbakhsh et al. (2016). The authors tackle BooMF using a probabilistic graphical model and derive a message passing algorithm to perform MAP inference. Their method is shown to have state-of-the-art performance for BooMF and completion. It therefore serves us as baseline benchmark in these task. The message passing approach has recently been employed by L\u00e1zaro-Gredilla et al. (2016) in a hierarchical network combined with pooling", "startOffset": 40, "endOffset": 1292}, {"referenceID": 5, "context": "The ability to learn features at different levels of abstraction is commonly cited as an explanation for the success that deep neural networks have across many domains of application (Lin & Tegmark, 2016; Goodfellow et al., 2016).", "startOffset": 183, "endOffset": 229}, {"referenceID": 14, "context": "This is reminiscent of the idea of deep exponential families, as introduced by Ranganath et al. (2015). The ability to learn features at different levels of abstraction is commonly cited as an explanation for the success that deep neural networks have across many domains of application (Lin & Tegmark, 2016; Goodfellow et al.", "startOffset": 79, "endOffset": 103}, {"referenceID": 16, "context": "Message passing (MP) has been shown to compare favourably with other state-of-the-art methods for BooMF (Ravanbakhsh et al., 2016) and is therefore the focus of our comparison.", "startOffset": 104, "endOffset": 130}, {"referenceID": 16, "context": "2 in Ravanbakhsh et al. (2016) and shows that the OrMachine consistently enables error-free reconstruction of a 1000\u00d71000 matrix of rank L=5 for up to 30% bit flip probability.", "startOffset": 5, "endOffset": 31}, {"referenceID": 16, "context": "2 in Ravanbakhsh et al. (2016) and shows that the OrMachine consistently enables error-free reconstruction of a 1000\u00d71000 matrix of rank L=5 for up to 30% bit flip probability. Notably, MP performs worse for smaller noise levels. It was hypothesised by Ravanbakhsh et al. (2016) that symmetry breaking at higher noise levels helps message passage to converge to a better solution.", "startOffset": 5, "endOffset": 279}, {"referenceID": 16, "context": "Following Ravanbakhsh et al. (2016), we binarise the data taking the global mean as threshold.", "startOffset": 10, "endOffset": 36}, {"referenceID": 12, "context": "The latter is particularly relevant in cancer research where it helps to understand the cellular composition of a tumour and its relationship to disease progression and treatment (Patel et al., 2014).", "startOffset": 179, "endOffset": 199}, {"referenceID": 3, "context": "This is done using the Enrichr analysis tool (Chen et al., 2013) and a mouse gene atlas (Su et al.", "startOffset": 45, "endOffset": 64}, {"referenceID": 18, "context": ", 2013) and a mouse gene atlas (Su et al., 2004).", "startOffset": 31, "endOffset": 48}, {"referenceID": 9, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}, {"referenceID": 23, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}, {"referenceID": 4, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}, {"referenceID": 21, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}, {"referenceID": 14, "context": "Typical genes for each of the biological states are annotated (Lopez-Bendito et al., 2007; Zheng et al., 2008; Demyanenko et al., 2010; Upadhya et al., 2011; Raman et al., 2013).", "startOffset": 62, "endOffset": 177}], "year": 2017, "abstractText": "Boolean matrix factorisation (BooMF) infers interpretable decompositions of a binary data matrix into a pair of low-rank, binary matrices: One containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for BooMF and derive a Metropolised Gibbs sampler that facilitates very efficient parallel posterior inference. Our method outperforms all currently existing approaches for Boolean Matrix factorization and completion, as we show on simulated and real world data. This is the first method to provide full posterior inference for BooMF which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering, and crucially it improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11,000 genes on commodity hardware.", "creator": "LaTeX with hyperref package"}}}