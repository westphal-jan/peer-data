{"id": "1702.06703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Feb-2017", "title": "Data Distillation for Controlling Specificity in Dialogue Generation", "abstract": "People speak at different levels of specificity in different situations. Depending on their knowledge, interlocutors, mood, etc.} A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network--based conversational agent this ability. Our approach involves alternating between \\emph{data distillation} and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity.", "histories": [["v1", "Wed, 22 Feb 2017 08:32:47 GMT  (156kb,D)", "http://arxiv.org/abs/1702.06703v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiwei li", "will monroe", "dan jurafsky"], "accepted": false, "id": "1702.06703"}, "pdf": {"name": "1702.06703.pdf", "metadata": {"source": "CRF", "title": "Data Distillation for Controlling Specificity in Dialogue Generation", "authors": ["Jiwei Li", "Will Monroe", "Dan Jurafsky"], "emails": ["jiweil@stanford.edu", "wmonroe4@stanford.edu", "jurafsky@stanford.edu"], "sections": [{"heading": null, "text": "People speak at different levels of specificity in different situations.1 A conversational agent should have this ability and know when to be specific and when to be general.\nWe propose an approach that gives a neural network\u2013based conversational agent this ability. Our approach involves alternating between data distillation and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity.\nWe then train a reinforcement learning system for selecting among this pool of generation models, to choose the best level of specificity for a given input. Compared to the original generative model trained without distillation, the proposed system is capable of generating more interesting and higher-quality responses, in addition to appropriately adjusting specificity depending on the context.\nOur research constitutes a specific case of a broader approach involving training multiple subsystems from a single dataset distinguished by differences in a specific property one wishes to model. We show that from such a set of subsystems, one can use reinforcement learning to build a system that tailors its output to different input contexts at test time.\n1Depending on their knowledge, interlocutors, mood, etc."}, {"heading": "1 Introduction", "text": "People use different levels of specificity in their language depending on many factors about the context of a conversation: one\u2019s interlocutor, one\u2019s mood, how familiar one is with the topic discussed, how well one understands the other\u2019s utterances, and so forth all influence the decision to respond with generics or specifics. A good dialogue agent should have a similar ability to vary the level of specificity of the responses it generates in an input-dependent way.\nWhen humans speak, we can imagine that each has a series of language models in his mind, each of which is able to generate a sensible response, but which differ in specificity. One picks the appropriate model according to the current situation (whether one understands the input utterance, whether one is interested in the topic, etc.) and generates a dialogue utterance using the selected model. Motivated by this line of thinking, we ask whether a conversational agent could consider a pool of dialogue models that vary in language specificity and pick the best one for producing a response to any given input.\nOne seemingly straightforward approach would be to split the training data by language specificity and train separate generation models on each split. However, this requires classifying data by text specificity, a problem which poses significant challenges. Language specificity has been historically studied for noun phrases, and a few specificityindicative features have been identified, such as singular terms, negations, or actual/non-actual moods (Enc\u0327, 1991; Lyons, 1995). However, there is no generally agreed criterion for defining the level of specificity of an arbitrary unit of natural language, let alone automatically generating sequences to have different levels of specificity.\nIn this paper, we propose an iterative data distillation approach for addressing this issue.2\n2The model is inspired by the concept of distillation in\nar X\niv :1\n70 2.\n06 70\n3v 1\n[ cs\n.C L\n] 2\n2 Fe\nb 20\n17\nThe proposed system operates as follows: a neural sequence-to-sequence generation (SEQ2SEQ) model is first trained and used to generate (decode) responses to inputs in a dataset. A list of the most common responses is constructed, and training examples with outputs that are semantically close to these common responses are removed (distilled). This process is then repeated by training another SEQ2SEQ model (from scratch) on the remaining data, decoding using the trained model, collecting generic responses and distilling more data. As the process iterates, responses that are generic are gradually distilled, and the trained models gradually increase in specificity.\nAt the end of the entire data distillation process, we are presented with a pool of generation models, all of which are able to produce sensible responses to input messages but differ in degree of specificity. This pool of models is analogous to specificity-varying models in a human\u2019s mind. When presented with an input dialogue message, the dialogue system needs to pick one model out of the pool, Which model to choose depends on how well the bot understands the input message, how knowledgeable it is regarding the topic discussed, etc.3 To imbue the agent with this ability, we use reinforcement learning to train a model to pick the an appropriate level of specificity by selecting one of pre-trained generative models from the pool.\nExperimental results show that models trained from different rounds of data distillation exhibit a clear spectrum of specificity. Models trained in early rounds of data distillation yield better responses. We also show that the reinforcement learning model is able to choose levels of specificity that are appropriate for a variety of inputs.\nOur research constitutes a specific case of a broader approach involving training multiple subsystems from a single dataset distinguished by differences in a specific property one wishes to model (here, specificity), especially when this property is hard to model in a supervised learning setting. We show that from such a set of subsystems, one can use reinforcement learning to build a system that tailors its output to different input contexts at test time.\nchemistry, which separates chemical mixtures by gradually increasing the temperature to a point at which one or more compounds in the mixture will vaporize.\n3We leave handling other factors that should influence specificity (such as the current mood of the bot and nonlinguistic characteristics of the interlocutor) for future work."}, {"heading": "2 Related Work", "text": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al., 2015; Mou et al., 2016). The goal of controlling output specificity is closely related to recent attempts to address this issue. Li et al. (2016a) propose using mutual information as an alternative training objective function in place of maximum likelihood, in which an N-best list generated by p(t|s) is reranked by the backward probability p(s|t).\nThe aim of this work is more general: instead of attempting to always avoid generic responses, our goal is to provide the system with the flexibility to generate responses at different levels of specificity. Blindly avoiding generating generic responses does not reflect how humans speak: we do say dull, generic things like I don\u2019t know what you are talking about, to communicate that we indeed do not understand part of the conversation, or to dismiss something as incorrect or nonsensical. A good dialogue system should have the ability to decide when to say generic things and when not to.\nData manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013). The proposed system differs from these subdata selection methods in both goals and implementation: we combine a series of models trained on different subsets of data, with the goal of increasing model performance rather than preserving the model\u2019s performance while reducing the size of the training data.\nThe system we propose is also related to data manipulation strategies such as boosting (Breiman, 1996b), a type of ensemble method (Dietterich, 2002; Zhou et al., 2002; Krogh et al., 1995) that uses subsets of the original data to produce a series of models and then \u201dboosts\u201d their performance by combining them together; and bagging (Breiman, 1996a), which generates additional data for training using the original dataset to produce multisets\nof the same size as the original data, decreasing training variance."}, {"heading": "3 Data Distillation", "text": "In the section, we describe the proposed data distillation model in detail. We use OpenSubtitles (Tiedemann, 2009) as our training dataset.4"}, {"heading": "3.1 Distilling common responses", "text": "We first use the following simple example to illustrate the core idea of our system: consider a model that predicts a multinomial distribution over an output variable (e.g., which fruit to choose). The probability of picking apple is 0.3, orange 0.25, blueberry 0.15, blackberry 0.15, and raspberry 0.15. Outputs that are generic are usually highly probable, since the high diversity of specific outputs results in each having smaller probability mass. We thus treat apple as the most generic fruit, and the various berries as more specific. Maximum likelihood estimation at test time will lead the model to always choose apple, since it has the largest probability. Observing that apple is the most common output, we will remove all apples from the training set and retrain the model, which will pick orange this time, since it has the greatest probability after apples are removed. We then remove oranges and repeat the process. With successive iterations of this distillation process, we will gradually obtain models that produce more specific outputs.\nIn the context of dialogue response generation, our approach works as follows: for each iteration, we first train a SEQ2SEQ model using attention (Bahdanau et al., 2014; Luong et al., 2015) on the original training set. Next, we use the trained model to decode responses to a number of input examples. We decode only a subset of the training set, 1 million responses in total. One could also use a held-out dataset for decoding, but the source of input messages is fairly unimportant in identifying the most frequent responses. We use greedy decoding (beam search with beam size 1). We then collect the most common responses in a list, denoted by L. A response is considered generic if its frequency of occurrence exceeds a threshold,\n4OpenSubtitles is a large, noisy, open-domain dataset of lines from movie scripts. The noise in the dataset is largely due to the lack of speaker labels for lines of the subtitles. Following Vinyals et al. (2015), we train our models to predict the current line given the preceding ones, assuming that each line constitutes a full speaker turn and that consecutive turns belong to the same conversation. Both assumptions are occasionally untrue but yield reasonable results.\nInput: training data D Output: sequence of trained models M\nM \u2190 \u2205 for i\u2190 1 to N = 8 do\ntrain a SEQ2SEQ model m on D until convergence M \u2190M +m decode subset of input messages in D using model m collect top frequent decoded responses L for all instances e \u2208 D do\ncompute relevance score R(e) using Eq. 1 end for D\u00ac \u2190 top examples by R(e) distill D\u00ac: D \u2190 D \u2212D\u00ac\nend for return M\nAlgorithm 1: A brief summary of the proposed data distillation algorithm.\nwhich is empirically set to 100 in this work. We then compare each response in the training data to each highly frequent response from the list L and assign a relevance score R(e) to each training example e based on the cosine similarity between e and the sequence most similar to it in L:\nR(e) = maxe\u2032\u2208L cos(e, e\u2032) (1)\nWe use the encoder part of the trained encoderdecoder model to map these sequences to vector representations, which are used to compute the cosine similarity. In this way, sentences that are semantically similar to frequent responses are assigned high relevance scores.5 We then remove (distill) examples from the training data with the highest relevance scores6 and retrain a new SEQ2SEQ model on the data that remains. An outline of the distillation algorithm is shown in Algorithm 1."}, {"heading": "3.2 Choosing a specificity model", "text": "The data distillation process produces a pool of SEQ2SEQ models, each trained on the dataset remaining after a different data distillation round. When presented with an input message at test time, the system has to decide which generation model from the pool to use to decode a response to the input. We repeat the data distillation process 8 times, which means we have 8 models in the pool to choose from.7 The system should have the ability to choose different models in response to properties\n5Other options include skip-thought vectors (Kiros et al., 2015) and bag-of-word representations. We find using the trained encoder works decently well.\n6The amount to remove is empirically set to 8\u201310%. 7It requires two Tesla K40 GPUs to fit the 8 models in\nmemory.\nof different inputs. For example, a good dialogue system should give concrete responses when asked things that it is sure about, but generic ones when the input message is difficult to understand.\nWe use reinforcement learning to train a model to make this choice. Given an input message X from a held-out dataset, we parameterize the action of choosing the generative model with index i from the pool G = {gi} of SEQ2SEQ models trained with data distillation as a policy network \u03c0(gi|X), which produces a distribution over |G| classes. To compute the distribution, we first map the input X to a vector representation hX using an LSTM and then map hX to a policy distribution over different gi \u2208 G using a softmax function:\n\u03c0(g = gi|X) = exp(hTX \u00b7 hgi)\u2211j=|G|\nj=1 exp(h T X \u00b7 hgj )\n(2)\nwhere hgi is an output vector for each model gi that is randomly initialized and then trained. Given an action, namely a choice of a generative model gi, we start decoding given the input message X using that model. Decoding generates an output response y, which yields a reward R(y) evaluating response quality according to some metric. The reward signal R(y) is used to train the policy network.\nWe use the REINFORCE algorithm (Williams, 1992), a kind of policy gradient method, to find the optimal policy by maximizing the expected reward E\u03c0(gi|X)[R(y)]. The expectation is approximated by sampling from \u03c0 and the gradient is computed using the likelihood ratio (Aleksandrov et al., 1968):\n\u2207E(\u03b8) \u2248 [R(y)\u2212 b]\u2207 log \u03c0(gi|X)) (3)\nwhere b denotes a baseline value. 8\nAdversarial evaluation for reward calculation One remaining question is how to assign a reward R to a generated response y given the input X , which boils down to the fundamental question of how to evaluate the general quality of a generated response. Dialogue quality is traditionally evaluated (Sordoni et al., 2015, e.g.) using word-overlap metrics such as BLEU and METEOR scores used for machine translation, which have recently been found to correlate poorly with human evaluations (Liu et al., 2016). Recent work has begun using\n8The baseline value is estimated using another neural model. We refer the readers to Ranzato et al. (2015) and Zaremba and Sutskever (2015) for more details.\nmore flexible and reliable evaluation metrics; automatic prediction of human ratings (Lowe et al., 2016) is one such metric, but this approach requires a large amount of human labeling effort to train a prediction model.\nWe employ adversarial evaluation (Li et al., 2016c; Anjuli and Oriol, 2016) for reward calculation. The idea of adversarial evaluation, first proposed by Bowman et al. (2015), is to train a discriminator (or evaluator) function to labels dialogues as machine-generated (negative) or human-generated (positive), a binary classification task. For our system, we use positive examples taken directly from training dialogues, while negative examples are decoded using generative models from different rounds of data distillation. To be specific, for each input message, we randomly sample a SEQ2SEQ model from the pool to decode a response to the input and use the response as a negative example. The evaluator is a hierarchical neural model (Serban et al., 2016b): dialogue utterances (i.e., source messages and responses) are first mapped to vector representations using an LSTM. Another LSTM is applied to the sequence of utterance representations to produce a dialogue representation, which is then fed to a binary classifier.\nGiven a pre-trained evaluator D, an input source X and a machine generated target y decoded by the chosen generative model, the reward R used to update the policy \u03c0 is the probability that the evaluator D assigns to labeling y as a human-generated response. The policy update influences the choice of generative model for decoding the current input X . We refer readers to (Li et al., 2016c) for more details about the adversarial evaluation."}, {"heading": "3.3 Stochastic Greedy Sampling", "text": "Language specificity also relates to language diversity. Utterances with lower levels of diversity are usually generic because generic responses are usually generic in the same way. Modeling diversity also provides an indirect way to handle the issue of specificity.\nMoreover, there is a degree of randomness in human language production: in the real world, if we ask a person the same question twice, even with the same environment and surroundings, it is unlikely that the person will give the same answer both times. Sampling from the distribution not only better mimic the way humans generate tokens, but also provides a way to handle the issue of language specificity .\nOne simple solution is to sample directly from the distribution p(y|x) in all cases. However, we observe that sampling leads to incoherent, ungrammatical, or even irrelevant responses. We expect there to be a sweet spot on the spectrum of randomness, between full sampling on one end and greedy or beam search on the other.9\nWe propose a straightforward algorithm called Stochastic Greedy Sampling, in which instead of sampling from the full distribution over all candidate tokens, the model only samples from the few (e.g., 5) words with the highest probability. The model provides with both the flexibility of incorporating randomness and the rigidity of adhering to a pre-trained generation model at the same time.\nAgain, we use Adversarial Evaluation for comparing purposes. We report AdverSuc and machinevs-random proposed by Anjuli and Oriol (2016). machine-vs-random denotes the the accuracy of distinguishing between machine-generated responses and randomly sampled responses using a machine evaluator, trained in a way similar to the evaluator in AdverSuc. Table 1 presents results for AdverSuc and machine-vs-random results for greedy decoding, pure sampling and the proposed stochastic greedy model. As can be seen, sampling all the time obtains the best score for AdverSuc, but also extremely low score for machine-vs-random accuracy, which indicates the inferiority of the always sampling strategy. The proposed stochastic greedy model perform better than always taking greedy actions as in greedy. This indicates that properly combining greedy search and sampling will potentially lead to better results."}, {"heading": "4 Experimental Results", "text": "In this section, we present the results of experiments.\n9Since greedy decoding has been shown to generate higher-quality responses than beam search in dialogue response generation (Li et al., 2016a), we focus on greedy decoding. However, all algorithms can be easily adapted to use beam-search decoding."}, {"heading": "4.1 Comparing generative models from different iterations", "text": "It is interesting to first compare the generative models and the remaining training data from each of the 8 rounds of data distillation. We use Iter+N to denote the generation model trained on the dataset after N repetitions of data distillation.\nPerplexity and diversity The size of the training dataset after each round of data distillation and the perplexity of the corresponding trained models on the full development set is shown in the first two columns of Table 2. Perplexity increases for models trained with more data distillation (as expected, since distillation removes opportunities for the model to learn to produce the most common outputs).\nHowever, we expect models trained with distillation to complement the model trained on the entire dataset by better modeling more specific outputs. To quantify the potential of the pool of generation models to complement each other when used in different contexts, we also report oracle perplexity (\u201coracle-ppl\u201d) as a function of the number of iterations K: for each example, we identify the generation model (out of Iter1 through IterK) that assigns the highest probability to the true output. Oracle perplexity is the perplexity computed using these maximal probabilities, instead of the probabilities assigned by any one model. This is equivalent to the perplexity of a model with an RL policy network that chooses perfectly every time. We expect to find that oracle perplexity on the development set decreases when adding the models trained in the first few rounds of data distillation, after which it levels off. This confirms that there are benefits to be had from choosing smartly among the different models.\nTable 2 also shows a measure of the diversity of generated responses, namely, the number of distinct unigrams (\u201cdiv-1\u201d) and bigrams (\u201cdiv-2\u201d) in generated responses as a fraction of the total generated tokens, as described in (Li et al., 2016a). As can be seen, as the data distillation process proceeds and more generic responses are distilled, the system generates increasingly diverse responses.\nDistilled responses The highest-frequency responses from different rounds of data distillation are shown in Table 3. Top responses are more generic for models trained in earlier iterations. In iteration 1, the top responses are broadly generic statements of uncertainty (\u201cI don\u2019t know\u201d, \u201cI am not sure\u201d) or agreement (\u201ci think you are right\u201d or \u201cthat\u2019s a good idea\u201d), but the meanings of frequent responses start diverging as the distillation algorithm proceeds. The number of the occurrences of the top frequent responses from different iterations also validates this point, with the number gradually decreasing.\nTable 4 presents sampled outputs from the generation models trained after different rounds of\nthe distillation. Responses from Iter1 are usually generic but safe, mostly i don\u2019t know what to do/what you are talking about and that\u2019s a good idea. As the amount of distilled data increases, the corresponding model generates increasingly concrete responses but has a greater risk of outputting confusing or irrelevant responses."}, {"heading": "4.2 Choosing the correct model for decoding", "text": "Next, we present results from the proposed reinforcement learning model and analyze how it decides which model to pick from the pool.\nThe distribution over different models used to decode input messages in the development set is shown in Figure 1. As can be seen, the RL model chooses to decode using the model trained on the entire dataset (i.e., Iter1) for 16 percent of all inputs. The models trained after 2, 3 and 4 rounds are responsible for decoding responses to approximately half of the inputs.\nHuman evaluation For human evaluation, we follow protocols defined in Li et al. (2016b), employing crowdsourced judges to evaluate a random\nsample of 200 items. We present labelers with an input message and the generated outputs from three models, Iter1, Iter2, and RL, and ask them to rank the three outputs by quality. Note that the outputs from the RL model can be the same as those from Iter1 or Iter2 if the RL model chooses that partic-\nular model (Iter1 or Iter2) for decoding. In these cases, a tie is automatically recorded. Figure 2 shows the proportions of the outputs ranked first by the human labelers. As can be seen, the reinforcement learning model performs best 60 percent of the time, followed by Iter2, which wins 37 percent\nof the time. Table 5 shows pairwise human judgements between the three models extracted from the three-instance ranking. It is interesting to see that Iter2 generally outperforms Iter1, winning on 62 percent of the examples. This is consistent with the fact that the RL model tends to prefer Iter2 more often.\nAdversarial evaluation Table 6 reports adversarial success and machine-vs-random accuracy described in Li et al. (2016c). Adversarial success (AdverSuc) refers to the percentage of machinegenerated responses that are able to fool an trained evaluator model into believe that they are generated by humans; machine-vs-random accuracy denotes the accuracy of a trained evaluator model (a different evaluator from the one used in adversarial success) at distinguishing between machine-generated responses and human utterances randomly sampled without regard for the input. Superior models should obtain higher values of both adversarial suc-\ncess and machine-vs-random accuracy. We refer readers to Li et al. (2016c) for more details. We observe that the RL model performs better than always using the model trained on the full dataset (Iter1) or choosing a distillation model at random (as one would expect, since the RL model is trained to optimize adversarial success).\nAnalyzing results Table 4 shows example choices made of the RL model in response to different inputs. When input messages are vague and hard to reply to, the RL model usually picks Iter1, which in turn outputs safe responses like \u201cthat \u2019s great\u201d or \u201ci don \u2019t know what you are talking about\u201d. The RL model has a tendency to pick models from the latter stages of distillation training if all of the generation models from the different iterations of distillation are able to output meaningful responses, since models from the later stages output produce more diverse and interesting outputs. We also observe a high correlation between the number of unknown words in the source sentence and the choice to use Iter1."}, {"heading": "5 Conclusion", "text": "In this paper, we investigate the language specificity issue in dialogue generation. We propose a data distillation method, which trains a series of generation models that exhibit different levels of specificity and uses a reinforcement learning model to choose the model best suited for decoding depending on the dialogue context.\nThe success of the proposed system confirms the importance of data processing in training a successful open-domain dialogue system. We anticipate that strategies resembling the one we propose can be used more generally for controlling properties of dialogue generation other than specificity, by training several models on different subsets of a single dataset that differ in the desired property, and choosing among these to produce outputs that tailor the quality of interest to the situation at hand."}], "references": [{"title": "Stochastic optimization", "author": ["V.M. Aleksandrov", "V.I. Sysoyev", "V.V. Shemeneva."], "venue": "Engineering Cybernetics 5:11\u201316.", "citeRegEx": "Aleksandrov et al\\.,? 1968", "shortCiteRegEx": "Aleksandrov et al\\.", "year": 1968}, {"title": "Adversarial evaluation of dialogue models", "author": ["Kannan Anjuli", "Vinyals Oriol"], "venue": null, "citeRegEx": "Anjuli and Oriol.,? \\Q2016\\E", "shortCiteRegEx": "Anjuli and Oriol.", "year": 2016}, {"title": "Online sequence-to-sequence reinforcement learning for open-domain conversational agents", "author": ["Nabiha Asghar", "Pasca Poupart", "Jiang Xin", "Hang Li."], "venue": "arXiv preprint arXiv:1612.03929 .", "citeRegEx": "Asghar et al\\.,? 2016", "shortCiteRegEx": "Asghar et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."], "venue": "arXiv preprint arXiv:1511.06349 .", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Bagging predictors", "author": ["Leo Breiman."], "venue": "Machine learning 24(2):123\u2013140.", "citeRegEx": "Breiman.,? 1996a", "shortCiteRegEx": "Breiman.", "year": 1996}, {"title": "Bias, variance, and arcing classifiers", "author": ["Leo Breiman"], "venue": null, "citeRegEx": "Breiman.,? \\Q1996\\E", "shortCiteRegEx": "Breiman.", "year": 1996}, {"title": "Ensemble learning", "author": ["Thomas G Dietterich."], "venue": "The handbook of brain theory and neural networks 2:110\u2013125.", "citeRegEx": "Dietterich.,? 2002", "shortCiteRegEx": "Dietterich.", "year": 2002}, {"title": "The semantics of specificity", "author": ["M\u00fcrvet En\u00e7."], "venue": "Linguistic inquiry pages 1\u201325.", "citeRegEx": "En\u00e7.,? 1991", "shortCiteRegEx": "En\u00e7.", "year": 1991}, {"title": "Scaling the indian buffet process via submodular maximization", "author": ["Zoubin Ghahramani."], "venue": "arXiv preprint arXiv:1304.3285 .", "citeRegEx": "Ghahramani.,? 2013", "shortCiteRegEx": "Ghahramani.", "year": 2013}, {"title": "Submodular optimization with submodular cover and submodular knapsack constraints", "author": ["Rishabh K Iyer", "Jeff A Bilmes."], "venue": "Advances in Neural Information Processing Systems. pages 2436\u20132444.", "citeRegEx": "Iyer and Bilmes.,? 2013", "shortCiteRegEx": "Iyer and Bilmes.", "year": 2013}, {"title": "Skip-thought vectors", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler."], "venue": "Advances in neural information processing systems. pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Neural network ensembles, cross validation, and active learning. Advances in neural information processing systems 7:231\u2013238", "author": ["Anders Krogh", "Jesper Vedelsby"], "venue": null, "citeRegEx": "Krogh and Vedelsby,? \\Q1995\\E", "shortCiteRegEx": "Krogh and Vedelsby", "year": 1995}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "Proc. of NAACL-HLT .", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky."], "venue": "arXiv preprint arXiv:1606.01541 .", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Adversarial reinforcement learning for neural dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Tianlin Shi", "Dan Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Towards an automatic turing test: Learning to evaluate dialogue responses", "author": ["Ryan Lowe", "Michael Noseworthy", "Iulian Serban", "Nicolas Angelard-Gontier", "Yoshua Bengio", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1605.05414 .", "citeRegEx": "Lowe et al\\.,? 2016", "shortCiteRegEx": "Lowe et al\\.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Linguistic semantics: An introduction", "author": ["John Lyons."], "venue": "Cambridge University Press.", "citeRegEx": "Lyons.,? 1995", "shortCiteRegEx": "Lyons.", "year": 1995}, {"title": "Coherent dialogue with attention-based language models", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R Walter."], "venue": "arXiv preprint arXiv:1611.06997 .", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin."], "venue": "arXiv preprint arXiv:1607.00970 .", "citeRegEx": "Mou et al\\.,? 2016", "shortCiteRegEx": "Mou et al\\.", "year": 2016}, {"title": "Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets", "author": ["Adarsh Prasad", "Stefanie Jegelka", "Dhruv Batra."], "venue": "Advances in Neural Information Processing Systems. pages 2645\u20132653.", "citeRegEx": "Prasad et al\\.,? 2014", "shortCiteRegEx": "Prasad et al\\.", "year": 2014}, {"title": "Sequence level training with recurrent neural networks", "author": ["Marc\u2019Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Data-driven response generation in social media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, pages 583\u2013593.", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Multimodal variational encoder-decoders", "author": ["Iulian V Serban", "II Ororbia", "G Alexander", "Joelle Pineau", "Aaron Courville."], "venue": "arXiv preprint arXiv:1612.00377 .", "citeRegEx": "Serban et al\\.,? 2016a", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of AAAI.", "citeRegEx": "Serban et al\\.,? 2016b", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Generative deep neural networks for dialogue: A short review", "author": ["Iulian Vlad Serban", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1605.06069 .", "citeRegEx": "Serban et al\\.,? 2016d", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Meg Mitchell", "JianYun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "Proceedings of", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "News from OPUS \u2013 A collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent Advances in Natural Language Processing. volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "Proceedings of ICML Deep Learning Workshop.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Submodularity in data subset selection and active learning", "author": ["Kai Wei", "Rishabh Iyer", "Jeff Bilmes."], "venue": "Proceedings of the 32nd International Conference on Machine Learning, Lille, Fran. pages 6\u201311.", "citeRegEx": "Wei et al\\.,? 2015", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Reinforcement learning neural Turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1505.00521 .", "citeRegEx": "Zaremba and Sutskever.,? 2015", "shortCiteRegEx": "Zaremba and Sutskever.", "year": 2015}, {"title": "Submodular attribute selection for action recognition in video", "author": ["Jingjing Zheng", "Zhuolin Jiang", "Rama Chellappa", "Jonathon P Phillips."], "venue": "Advances in Neural Information Processing Systems. pages 1341\u20131349.", "citeRegEx": "Zheng et al\\.,? 2014", "shortCiteRegEx": "Zheng et al\\.", "year": 2014}, {"title": "Ensembling neural networks: many could be better than all", "author": ["Zhi-Hua Zhou", "Jianxin Wu", "Wei Tang."], "venue": "Artificial intelligence 137(1):239\u2013263.", "citeRegEx": "Zhou et al\\.,? 2002", "shortCiteRegEx": "Zhou et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 8, "context": "Language specificity has been historically studied for noun phrases, and a few specificityindicative features have been identified, such as singular terms, negations, or actual/non-actual moods (En\u00e7, 1991; Lyons, 1995).", "startOffset": 194, "endOffset": 218}, {"referenceID": 19, "context": "Language specificity has been historically studied for noun phrases, and a few specificityindicative features have been identified, such as singular terms, negations, or actual/non-actual moods (En\u00e7, 1991; Lyons, 1995).", "startOffset": 194, "endOffset": 218}, {"referenceID": 24, "context": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al.", "startOffset": 70, "endOffset": 198}, {"referenceID": 31, "context": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al.", "startOffset": 70, "endOffset": 198}, {"referenceID": 2, "context": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al.", "startOffset": 70, "endOffset": 198}, {"referenceID": 20, "context": "Generic responses in open-domain dialogue End-to-end dialogue systems (Ritter et al., 2011; Serban et al., 2016c; Vinyals and Le, 2015; Serban et al., 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al.", "startOffset": 70, "endOffset": 198}, {"referenceID": 29, "context": ", 2016), tend to generate highly generic and commonplace responses (Sordoni et al., 2015; Mou et al., 2016).", "startOffset": 67, "endOffset": 107}, {"referenceID": 21, "context": ", 2016), tend to generate highly generic and commonplace responses (Sordoni et al., 2015; Mou et al., 2016).", "startOffset": 67, "endOffset": 107}, {"referenceID": 2, "context": ", 2016d,a; Asghar et al., 2016; Mei et al., 2016), tend to generate highly generic and commonplace responses (Sordoni et al., 2015; Mou et al., 2016). The goal of controlling output specificity is closely related to recent attempts to address this issue. Li et al. (2016a) propose using mutual information as an alternative training objective function in place of maximum likelihood, in which an N-best list generated by p(t|s) is reranked by the backward probability p(s|t).", "startOffset": 11, "endOffset": 273}, {"referenceID": 32, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 35, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 22, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 9, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 10, "context": "Data manipulation The idea of training with data distillation is inspired by a variety of work in the active learning and subdata selection literature, the key idea of which is to select a subset of a large dataset to train a classifier with minimal performance loss, for when the training dataset is extremely large or training is extremely timeintensive (Wei et al., 2015; Zheng et al., 2014; Prasad et al., 2014; Ghahramani, 2013; Iyer and Bilmes, 2013).", "startOffset": 356, "endOffset": 456}, {"referenceID": 7, "context": "The system we propose is also related to data manipulation strategies such as boosting (Breiman, 1996b), a type of ensemble method (Dietterich, 2002; Zhou et al., 2002; Krogh et al., 1995) that uses subsets of the original data to produce a series of models and then \u201dboosts\u201d their performance by combining them together; and bagging (Breiman, 1996a), which generates additional data for training using the original dataset to produce multisets", "startOffset": 131, "endOffset": 188}, {"referenceID": 36, "context": "The system we propose is also related to data manipulation strategies such as boosting (Breiman, 1996b), a type of ensemble method (Dietterich, 2002; Zhou et al., 2002; Krogh et al., 1995) that uses subsets of the original data to produce a series of models and then \u201dboosts\u201d their performance by combining them together; and bagging (Breiman, 1996a), which generates additional data for training using the original dataset to produce multisets", "startOffset": 131, "endOffset": 188}, {"referenceID": 5, "context": ", 1995) that uses subsets of the original data to produce a series of models and then \u201dboosts\u201d their performance by combining them together; and bagging (Breiman, 1996a), which generates additional data for training using the original dataset to produce multisets", "startOffset": 153, "endOffset": 169}, {"referenceID": 30, "context": "We use OpenSubtitles (Tiedemann, 2009) as our training dataset.", "startOffset": 21, "endOffset": 38}, {"referenceID": 3, "context": "In the context of dialogue response generation, our approach works as follows: for each iteration, we first train a SEQ2SEQ model using attention (Bahdanau et al., 2014; Luong et al., 2015) on the original training set.", "startOffset": 146, "endOffset": 189}, {"referenceID": 18, "context": "In the context of dialogue response generation, our approach works as follows: for each iteration, we first train a SEQ2SEQ model using attention (Bahdanau et al., 2014; Luong et al., 2015) on the original training set.", "startOffset": 146, "endOffset": 189}, {"referenceID": 11, "context": "Other options include skip-thought vectors (Kiros et al., 2015) and bag-of-word representations.", "startOffset": 43, "endOffset": 63}, {"referenceID": 33, "context": "We use the REINFORCE algorithm (Williams, 1992), a kind of policy gradient method, to find the optimal policy by maximizing the expected reward E\u03c0(gi|X)[R(y)].", "startOffset": 31, "endOffset": 47}, {"referenceID": 0, "context": "The expectation is approximated by sampling from \u03c0 and the gradient is computed using the likelihood ratio (Aleksandrov et al., 1968):", "startOffset": 107, "endOffset": 133}, {"referenceID": 16, "context": ") using word-overlap metrics such as BLEU and METEOR scores used for machine translation, which have recently been found to correlate poorly with human evaluations (Liu et al., 2016).", "startOffset": 164, "endOffset": 182}, {"referenceID": 17, "context": "more flexible and reliable evaluation metrics; automatic prediction of human ratings (Lowe et al., 2016) is one such metric, but this approach requires a large amount of human labeling effort to train a prediction model.", "startOffset": 85, "endOffset": 104}, {"referenceID": 22, "context": "We refer the readers to Ranzato et al. (2015) and Zaremba and Sutskever (2015) for more details.", "startOffset": 24, "endOffset": 46}, {"referenceID": 22, "context": "We refer the readers to Ranzato et al. (2015) and Zaremba and Sutskever (2015) for more details.", "startOffset": 24, "endOffset": 79}, {"referenceID": 1, "context": "We employ adversarial evaluation (Li et al., 2016c; Anjuli and Oriol, 2016) for reward calculation.", "startOffset": 33, "endOffset": 75}, {"referenceID": 26, "context": "The evaluator is a hierarchical neural model (Serban et al., 2016b): dialogue utterances (i.", "startOffset": 45, "endOffset": 67}, {"referenceID": 1, "context": ", 2016c; Anjuli and Oriol, 2016) for reward calculation. The idea of adversarial evaluation, first proposed by Bowman et al. (2015), is to train a discriminator (or evaluator) function to labels dialogues as machine-generated (negative) or human-generated (positive), a binary classification task.", "startOffset": 9, "endOffset": 132}, {"referenceID": 1, "context": "We report AdverSuc and machinevs-random proposed by Anjuli and Oriol (2016). machine-vs-random denotes the the accuracy of dis-", "startOffset": 52, "endOffset": 76}, {"referenceID": 13, "context": "Since greedy decoding has been shown to generate higher-quality responses than beam search in dialogue response generation (Li et al., 2016a), we focus on greedy decoding.", "startOffset": 123, "endOffset": 141}, {"referenceID": 13, "context": "Table 2 also shows a measure of the diversity of generated responses, namely, the number of distinct unigrams (\u201cdiv-1\u201d) and bigrams (\u201cdiv-2\u201d) in generated responses as a fraction of the total generated tokens, as described in (Li et al., 2016a).", "startOffset": 226, "endOffset": 244}, {"referenceID": 13, "context": "Human evaluation For human evaluation, we follow protocols defined in Li et al. (2016b), employing crowdsourced judges to evaluate a random", "startOffset": 70, "endOffset": 88}, {"referenceID": 13, "context": "Adversarial evaluation Table 6 reports adversarial success and machine-vs-random accuracy described in Li et al. (2016c). Adversarial success (AdverSuc) refers to the percentage of machinegenerated responses that are able to fool an trained evaluator model into believe that they are generated by humans; machine-vs-random accuracy denotes the accuracy of a trained evaluator model (a different evaluator from the one used in adversarial success) at distinguishing between machine-generated responses and human utterances randomly sampled without regard for the input.", "startOffset": 103, "endOffset": 121}, {"referenceID": 13, "context": "We refer readers to Li et al. (2016c) for more details.", "startOffset": 20, "endOffset": 38}], "year": 2017, "abstractText": "People speak at different levels of specificity in different situations.1 A conversational agent should have this ability and know when to be specific and when to be general. We propose an approach that gives a neural network\u2013based conversational agent this ability. Our approach involves alternating between data distillation and model training : removing training examples that are closest to the responses most commonly produced by the model trained from the last round and then retrain the model on the remaining dataset. Dialogue generation models trained with different degrees of data distillation manifest different levels of specificity. We then train a reinforcement learning system for selecting among this pool of generation models, to choose the best level of specificity for a given input. Compared to the original generative model trained without distillation, the proposed system is capable of generating more interesting and higher-quality responses, in addition to appropriately adjusting specificity depending on the context. Our research constitutes a specific case of a broader approach involving training multiple subsystems from a single dataset distinguished by differences in a specific property one wishes to model. We show that from such a set of subsystems, one can use reinforcement learning to build a system that tailors its output to different input contexts at test time. Depending on their knowledge, interlocutors, mood, etc.", "creator": "LaTeX with hyperref package"}}}