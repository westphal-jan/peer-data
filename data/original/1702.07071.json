{"id": "1702.07071", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Pronunciation recognition of English phonemes /\\textipa{@}/, /{\\ae}/, /\\textipa{A}:/ and /\\textipa{2}/ using Formants and Mel Frequency Cepstral Coefficients", "abstract": "The Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /\\textipa{@}/, /{\\ae}/, /\\textipa{A}:/ and /\\textipa{2}/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70\\% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 \\% was achieved and an accuracy of 71\\% when /\\textipa{@}/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does.", "histories": [["v1", "Thu, 23 Feb 2017 02:31:03 GMT  (824kb,D)", "http://arxiv.org/abs/1702.07071v1", "11 pages, pre-print version"]], "COMMENTS": "11 pages, pre-print version", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["keith y patarroyo", "vladimir vargas-calder\\'on"], "accepted": false, "id": "1702.07071"}, "pdf": {"name": "1702.07071.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Keith Y. Patarroyo", "Vladimir Vargas-Calder\u00f3n"], "emails": ["/@/,", "/@/,", "/@/", "vvargasc@unal.edu.co"], "sections": [{"heading": null, "text": "Pronunciation recognition of English phonemes /@/, /\u00e6/, /A:/ and /2/ using Formants and Mel Frequency Cepstral Coefficients\nKeith Y. Patarroyo and Vladimir Vargas-Caldero\u0301n\u2217\nThe Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /@/, /\u00e6/, /A:/ and /2/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 % was achieved and an accuracy of 71% when /@/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does.\nKeywords: sound processing, formants, Mel frequency cepstral coefficients, pronunciation recognition, machine learning."}, {"heading": "1 Introduction", "text": "Throughout computing history, scientists have developed a vast amount of theories and algorithms for speech recognition that are widely known (e.g. see the work by Lee (1988)). Many of them are motivated by using some of the principles of the human ear operation (Davis & Mermelstein, 1980). In recent years, the blooming of high-speed processing computers has allowed us to start using deep machine learning to improve the efficacy of our speech recognizers (Baker et al., 2009). The techniques that are currently used for speech recognizers yield high prediction rates (Hinton et al., 2012). The most common and successful technique is the implementation of multilayered neural networks (Zegers, 1998; Wellekens, 1998). The performance of such techniques seeded the question of implementing recognizers as objective evaluators of speech ability in humans.\nThis takes great relevance in the field of pronunciation teaching, not only because it provides the teachers (especially teachers who are non-native speakers of the taught language) a tool for assessing objectively the pronunciation of their students (Neri, Mich, Gerosa, & Giuliani, 2008), and of themselves, but also because if a student has access to such a tool, he or she could learn pronunciation autonomously (Hinks, 2003). Unquestionably, the main goal in pronunciation teaching is to improve the ability of a language learner to use their vocal tract to produce sounds that are recognized as native sounds by native speakers of that language. Although this remains a challenge,\n\u2217Physics Department. National University of Colombia. Corresponding author e-mail: vvargasc@unal.edu.co\nar X\niv :1\n70 2.\n07 07\n1v 1\n[ cs\n.C L\n] 2\n3 Fe\nb 20\n17\nparticularly in the early stages of learning a language (Mirzaei, Gowhary, Azizifar, & Esmaeili, 2015), it also provides a means by which students can improve the learning and retention of grammatical structures (Martin & Jackson, 2016). Also, since different languages have different sound systems, it is normal for language learners to struggle learning them.\nFor instance, the phonemes /@/, /\u00e6/, /A:/ and /2/ found in the English language are troublesome for many English learners because of their similarity. For example, Spanish and Italian speakers tend to mix these phonemes into a single one: /a/. There are cases of learners from other languages, such as Azerbaijani, in which this confusion has been studied (Ghaffarvand Mokari & Werner, 2016).\nThis article aims to study the ability of two of the most historically important speech recognition tools to differentiate between these phonemes, i.e. how good tools would they be in evaluating the correct pronunciation of these phonemes. These tools are the formants (computed with Linear Predictive Coding), attractive for its simplicity in vowel pronunciation recognition, and the Mel Frequency Cepstral Coefficients, attractive for its computational speed in continuous speech recognition. Both tools were tested with the Vocal Joystick Vowel Corpus, from the Washington University."}, {"heading": "2 Theoretical Framework", "text": ""}, {"heading": "2.1 Sound Formation", "text": "The frequency range of the pressure waves in the air that compose the audible sounds by humans goes from 20Hz to 20kHz (Rosen & Howell, 2011). The process by which sound is produced by humans is the following (for a complete understanding of sound production by humans see reference (O\u2019Grady & Dobrovolosky, 1997)).\nThe diaphragm and intercostal contractions make the lungs generate a flow of air from the chest to the mouth. This air first passes through the larynx, wherein the vocal cords are. These are muscles that can be geometrically distributed in several ways, each of which is excited by the passing air creating modes of vibration or glottal states that result in sound. The pharynx, the oral cavity and the nasal cavity are filters and resonators of the aforementioned sounds. Also, the tongue and lips allow us to rapidly articulate and change the shape of the vocal cavity in order to filter some frequencies.\nIn this study, we are interested in vowels, which are voiced glottal states produced with little obstruction of the vocal tract. This means having the lips open and also the tongue without contact with the palate."}, {"heading": "2.2 Human Hearing Detection Principles", "text": "It is also convenient to mention some of the principles that the human ear uses to detect sound signals. The human ear analyses pressure variations in the air into different frequencies. Roughly, the human ear does a Fourier transform of the pressure signal P (t), where t is the time, and transmits |P (\u03c9)|2, where \u03c9 is the frequency, to the brain (Sethna, 2006).\nHowever, this initial model falls short for many analyses. Some of the reasons are (Lyons, n.d.): the tonal information is changing as a word or tune progresses; the difference between two closely spaced frequencies is hard to recognize for humans, especially at high frequencies; and humans hear loudness on a non-linear scale. All of these factors propose a significant challenge to mimic the human hearing system.\nDavis and Mermelstein (1980) developed a method (Mel Frequency Cepstral Coefficients) to mimic this process, and has been the state of the art in the field of speech recognition since then."}, {"heading": "2.3 Formants and Linear Predictive Coding (LPC)", "text": "The vocal tract can be thought of as a vibrating cavity, or resonator, whose geometry (morphology) is constantly changing in continuous speech. This change in the geometry allows the superposition of different modes of vibration, and thereby different sounds.\nExperiments have shown that for English vowels there are some characteristic frequencies of vibration called formants (Hillenbrand, Getty, Wheeler, & Clark, 1994; Hunter & Kebede, 2012; Deterding, 2006), that correspond to maxima of vibration, i.e. where the acoustic energy is focused. The first formant is roughly located from 0 to 1kHz, while the second formant is located from 1kHz to 2kHz, and so on (these frequency boundaries are not rigid, because there might be some second formants below 1kHz, as seen in the references). In the studies mentioned only the two first formants are taken into account for characterising each vowel. However, there are studies like (Prica & Ilic\u0301, 2010) in which 3 formants are used to characterise the Serbian vowels in order to improve accuracy when performing classification of vowel sounds.\nFormants can be found by performing an acoustic power spectrum of a sound signal (or a spectrograph of the signal), and identifying the peaks in the spectrograph. This can be done by computing the envelope of the signal\u2019s frequency spectrum. Since the envelope contains information about the energy peaks, the formants can be determined. To predict the envelope of a signal s[n(\u2206t)] sampled in discrete time steps n = 0, 1, 2, . . ., the Linear Predictive Coding method was introduced (Deng & O\u2019Shaughnessy, 2003). The goal of LPC is to predict s[n] with a linear combination of s[n \u2212 1], s[n \u2212 2], . . . , s[n \u2212M ], where M is the integer that sets the number of predicting signal samplings."}, {"heading": "2.4 Mel Frequency Cepstral Coefficients (MFCC)", "text": "This method aims to mimic the procedure performed by the human hearing system to decode a sound signal. It can be summarised in the steps shown in table 1 (Lyons, n.d.)."}, {"heading": "2.5 Decision Tree Classifiers (DTC)", "text": "Classifiers are computer tools that learn patterns from labelled data, so that when a new sample is presented to the computer, it will classify the sample into one of the different labels. In other words, consider a data set D = {(di, li)}, where di is a vector of N components (features). The vector di is also called a sample. li is a label. The classifier is a function f(d) : F \u2192 L, where F is the vector space of the samples and L the space of the labels.\nTo see how DTCs work, consider the following example. Suppose that a data set consists of samples in one dimension: each sample is the salary of every single person in Colombia. The labels for this data set are the social stratum. We might have people that earn a lot of money, but they live in stratum 2, or vice versa. However, these cases are strange and in general there is a structure that allows predicting the label (social stratum) from the samples. The DTC will try to learn the salary ranges for each single social stratum, so that when a new sample is presented to the model, it will classify it in a specific label. The boundaries of the salary ranges are random at first, but as the learning process advances, the boundaries become more accurate and clear.\nTo test how well the DTC is able to predict labels, a data set D\u2032 called the testing data is normally built. D\u2032 consists of M samples with known labels. Only the samples, and not the labels, are presented to the DTC. The percentage of correctly predicted labels will tell the accuracy or effectiveness of the DTC. For a more thorough explanation, refer to (Tan, Steinbach & Kumar, 2005)."}, {"heading": "3 Methodology", "text": "The pipeline of this study (see Figure 1) was to extract and prepare audio files from the Vocal Joystick Vowel Corpus. Then, formants and cepstral coefficients were computed\nfor each file. The set of files with their corresponding formants and cepstral coefficients was split into training and testing data. A DTC was trained with the training data, and tested with the testing data, yielding an accuracy percentage. The characteristics of each step are explained in this section."}, {"heading": "3.1 Corpus and Audio Processing", "text": "In this study we used the Vocal Joystick Vowel Corpus (Bilmes, Wright, Xiao, Malkin Kilanski, 2006). This project selected a group of nine monophthongs and 12 vowelto-vowel transitions. Sounds with different duration, amplitude and intonation were recorded for each vowel:\n\u2022 Duration: short (1 second), long (2 seconds) and nudge (very short repetitions of the same vowel).\n\u2022 Amplitude: quiet, normal, loud, quiet to loud and loud to quiet.\n\u2022 Intonation: level, rising and falling.\nFrom this corpus, we considered all the sound files available in the corpus corresponding to the phonemes /@/, /\u00e6/, /A:/ and /2/ whose duration was either short or long, whose amplitude was quite, normal or loud, and whose intonation was in a single level. A total of 784 files satisfied these conditions."}, {"heading": "3.2 Data Preparation", "text": "An example of a raw data file from the corpus is shown in Figure 2a. It can be seen that there are silent parts and the amplitude is not normalised. Also, even though we selected a single level of intonation, it is clear that the amplitude of the sound decreases as time progresses. Therefore, for each file we deleted the silent part. Then, a section of duration 40ms was selected so that the signal does not change considerably and is approximately periodic (Figure 2b). Finally, a Hamming window was applied to the resulting sound signal with the purpose of smoothing the Fourier Transform used in the MFCC extraction (Figure 2c)."}, {"heading": "3.3 Evaluation of Formants and MFCC Performance with a DTC", "text": "Then, the LPC Python implementation by Danilo Bellini was used to compute the formants (Bellini, 2016). To do the calculation of the MFCC, the Python implementation by James Lyons (Lyons, 2016) was used. Finally, the set of computed formants and the set of computed cepstral coefficients were partitioned into two groups each. One of the groups contained two thirds of the data, and the other one contained a third. The former was the training data, and the later was the testing data. To exemplify this procedure, denote S = {(s, v)} as the set of the sound signal files, each of which is labelled by a vowel v. Let F (s) be a function that computes the formants F of a sound signal s. Let X = {(F (s), v)\u2200s \u2208 S} be the data set of formants, labelled by their respective vowel. Let Xtr \u2282 X be the set of training data and Xte \u2282 X be the set of testing data. The DTC learns the ranges corresponding to each vowel in the formants space by creating a function DTC(F (s)) that takes the values of the labels, i.e. the phonemes /@/, /\u00e6/, /A:/ and /2/. For instance, if after the learning process is finished, a sample F (s), labelled by the phoneme /@/, is presented to the DTC, and DTC(F (s)) = /\u00e6/, then the DTC failed to predict the correct phoneme. If instead DTC(F (s)) = /@/ then the DTC was successful in the prediction. The accuracy is thus defined as the percentage of correctly predicted phonemes from the testing data. We used the accuracy to test the quality of prediction by the DTC using formants and Mel coefficients separately. This gives a measure of how well could formants and cepstral coefficients help in the objective evaluation of pronunciation."}, {"heading": "4 Results, Analysis and Discussion", "text": ""}, {"heading": "4.1 Formants", "text": "From the 784 sound files, 142 were discarded for this analysis and for the one corresponding to the MFCC. The reason to discard these files was that some recorded sounds contained parts in which the signal was not periodic due to a trembling voice from the speaker. This could cause the LPC and MFCC algorithms to be affected. The criterion used to discard the data is now explained. Denote the recordings or sound signals that belong to vowel v as (s, v). Also, denote the mean value of the i-th formant (i = 1, 2) and the standard deviation of the i-th formant, for a vowel v, as (\u00b5i, v), (\u03c3i, v), respectively. The remaining 642 sound signals satisfied the condition\n|Fi(s)\u2212 \u00b5i| < 1.5\u03c3i, (1) for each vowel v, where Fi refers to the i-th formant. The computed formants for these sound signals are shown in figure 3.\nThe DTC trained with the formants yielded a 70% accuracy. Although this success rate of the predictive model is a very low rate, it can be considered as good taking into account that only two features were used to predict the data: the first two formants. This means that with little information about four very similar sounding phonemes, we were able to predict, with an accuracy of 70%, the vowel of a sound taken at random from the recordings obtained from the corpus.\nFurthermore, Figure 3 shows a non-Gaussian distribution in either formant for every phoneme. This is supported by observations in which it was seen that formants do not only depend on the geometry of the vocal tract of each person, but also are statistically dependent on the age, as shown in (Hawkings & Midgley, 2005); as well as on gender, as shown in (Hillenbrand, Getty, Clark, & Wheeler, 1995). The formants measured are shown in table 2 and compared with measurements from other studies.\nStudies shown in Table 2 show that formants vary a lot with age, gender and geography. Therefore, we see that identifying a phoneme by its formants is not trivial, but as we showed, can be done with an estimated accuracy of 70%. Despite the low accuracy, formants keep being used for vowel pronunciation research, as in (Rasilo & Ra\u0308sa\u0308nen, 2017), in which the process of infants learning their native language was simulated through a Leaning Virtual Infant that received sounds from its caregivers. At first the Learning Virtual Infant babbled randomly, but as the learning process advanced, it began to compute formants to identify similar sounding vowels and to improve its pronunciation."}, {"heading": "4.2 MFCC", "text": "13 MFCC were obtained for every sound signal. In order to visualise the MFCC of every sound signal, a dimension reduction from 13 to 2 (i.e. from the MFCC space to a plane) was performed with Principal Component Analysis (PCA). This method projects the data from the high-dimensional space onto a plane that preserves the maximum possible variance of the data. The distribution of the MFCC for the different phonemes on the plane computed with PCA is shown in Figure 4.\nAs it can be seen from Figure 4a, the MFCC of the phonemes seem mixed. In particular, note that the points corresponding to the phoneme /A:/ are mixed with the ones corresponding to the phoneme /2/. This can also be seen in the formants figure. Ignoring the /A:/ points produces Figure 4b, in which the clusters of data can be identified.\nPartitioning the data shown in Figure 4a in training and testing data, an accuracy of 56% was accomplished with the DTC. While if the data shown in Figure 4b was used, an accuracy of 71% was measured. These very low accuracies show that the MFCC method lacks sensibility to subtle differences between the studied sound signals.\nBut, why are the MFCC so good at speech recognition then? The first thing to mention is that the MFCC are normally used to train models that are more complex than a DTC, like neural networks. The goal in speech recognition is to understand words, instead of single sounds. To check the accuracy of the MFCC at recognising different sounding sounds, we proceeded to analyse the phonemes /2/, /e/, /u/ and /o/. The PCA results are shown in Figure 5.\nUsing the data shown in Figure 5a, the accuracy was measured in 73%. Clearly the MFCC corresponding to the phoneme /o/ were mixed with the ones corresponding to the phoneme /u/. To avoid this noise, if /o/ is removed, an accuracy of 88% was reached. This shows that MFCC are a reasonable method to recognise different phonemes."}, {"heading": "5 Conclusions and Perspectives", "text": "The Vocal Joystick Corpus was used to build a data set of sounds corresponding to the phonemes /@/, /\u00e6/, /A:/ and /2/. From each of the sound signals, both formants and MFCC were computed. These were used to train a DTC that acted as a classifier. Using formants, an accuracy of 70% was achieved. Using MFCC, an acuraccy of 52% was measured. MFCC\u2019s performance was much better when the DTC was trained with the coefficients computed from the phonemes /2/, /e/ and /u/, reaching 88%. However, these speech recognition tools fail to correctly predict the phonemes of similar sounding signals.\nAs a response to this deficiency, computer and linguistics scientists work to build computational tools that learn subtle differences between sounds in order to assess a person\u2019s pronunciation correctly. For instance, a research in the Chinese language showed results that improved significantly the previous recognition models in detecting mispronunciation of phonemes (Wei, Hu, Hu, & Wang, 2009). However, the advance of technology to support pronunciation learning must be accompanied by designed learning programs that help students to learn autonomously. More people need to be encouraged to work in this interdisciplinary field that is pioneering and improving language teaching."}], "references": [{"title": "Developments and directions in speech recognition and understanding, Part 1 [DSP Education", "author": ["J. Baker", "Li Deng", "J. Glass", "S. Khudanpur", "Chin-hui Lee", "N. Morgan", "D. O\u2019Shaughnessy"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Expressive Digital Signal Processing (DSP) package for Python, 2016", "author": ["Bellini", "D. (n.d"], "venue": "Retrieved November", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Transactions On Acoustics, Speech, And Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1980}, {"title": "Speech processing (1st ed., pp. 19-26)", "author": ["L. Deng", "D. O\u2019Shaughnessy"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "The North Wind versus a Wolf: short texts for the description and measurement of English pronunciation", "author": ["D. Deterding"], "venue": "Journal Of The International Phonetic Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Perceptual assimilation predicts acquisition of foreign language sounds: The case of Azerbaijani learners\u2019 production and perception of Standard Southern British English vowels", "author": ["P. Ghaffarvand Mokari", "S. Werner"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Formant frequencies of RP monophthongs in four age groups of speakers", "author": ["S. Hawkins", "J. Midgley"], "venue": "Journal Of The International Phonetic Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Acoustic characteristics of American English vowels", "author": ["J. Hillenbrand", "L. Getty", "M. Clark", "K. Wheeler"], "venue": "The Journal Of The Acoustical Society Of America,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Speech technologies for pronunciation feedback and evaluation", "author": ["R. Hincks"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition. Retrieved from http://static.googleusercontent.com/media/research.google.com/ en//pubs/archive/38131.pdf", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N Jailty"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Formant frequencies of British English vowels produced by native speakers of Farsi. Soci\u00e9t\u00e9 Fran\u00e7aise d\u2019Acoustique", "author": ["G. Hunter", "H. Kebede"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "On large-vocabulary speaker-independent continuous speech recognition", "author": ["K. Lee"], "venue": "Speech Communication,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "Mel Frequency Cepstral Coefficient (MFCC) tutorial, Practical Cryptography", "author": ["J. Lyons"], "venue": "Retrieved October", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Pronunciation Training Facilitates the Learning and Retention of L2 Grammatical Structures", "author": ["I. Martin", "C. Jackson"], "venue": "Foreign Language Annals,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Comparing the Phonological Performance of Kurdish and Persian EFL Learners in Pronunciation of English Vowels", "author": ["K. Mirzaei", "H. Gowhary", "A. Azizifar", "Z. Esmaeili"], "venue": "Procedia - Social And Behavioral Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "The effectiveness of computer assisted pronunciation training for foreign language learning by children", "author": ["A. Neri", "O. Mich", "M. Gerosa", "D. Giuliani"], "venue": "Computer Assisted Language Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Human Time-Frequency Acuity Beats the Fourier Uncertainty Principle", "author": ["J. Oppenheim", "M. Magnasco"], "venue": "Physical Review Letters,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Recognition of vowels in continuous speech by using formants. Facta Universitatis - Series: Electronics And Energetics", "author": ["B. Prica", "S. Ili\u0107"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "An online model for vowel imitation learning", "author": ["H. Rasilo", "O. R\u00e4s\u00e4nen"], "venue": "Speech Communication,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2017}, {"title": "Signals and systems for speech and hearing (2nd ed., p. 163)", "author": ["S. Rosen", "P. Howell"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Statistical mechanics: Entropy, Order Parameters and Complexity (1st ed., p. 299)", "author": ["J. Sethna"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Introduction to Data", "author": ["P. Tan", "M. Steinbach", "V. Kumar"], "venue": "Mining (1st ed.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}, {"title": "A new method for mispronunciation detection using Support Vector Machine based on Pronunciation Space Models", "author": ["S. Wei", "G. Hu", "Y. Hu", "R. Wang"], "venue": "Speech Communication,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Introduction to Speech Recognition Using Neural Networks. Bruges: ESANN. Retrieved from https://www.elen.ucl.ac.be/Proceedings/ esann/esannpdf/es1998-456.pdf", "author": ["C. Wellekens"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Speech Recognition Using Neural Networks (Master of Science with a major in Electrical Engineering)", "author": ["P. Zegers"], "venue": "University of Arizona", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1998}], "referenceMentions": [], "year": 2017, "abstractText": "The Vocal Joystick Vowel Corpus, by Washington University, was used to study monophthongs pronounced by native English speakers. The objective of this study was to quantitatively measure the extent at which speech recognition methods can distinguish between similar sounding vowels. In particular, the phonemes /@/, /\u00e6/, /A:/ and /2/ were analysed. 748 sound files from the corpus were used and subjected to Linear Predictive Coding (LPC) to compute their formants, and to Mel Frequency Cepstral Coefficients (MFCC) algorithm, to compute the cepstral coefficients. A Decision Tree Classifier was used to build a predictive model that learnt the patterns of the two first formants measured in the data set, as well as the patterns of the 13 cepstral coefficients. An accuracy of 70% was achieved using formants for the mentioned phonemes. For the MFCC analysis an accuracy of 52 % was achieved and an accuracy of 71% when /@/ was ignored. The results obtained show that the studied algorithms are far from mimicking the ability of distinguishing subtle differences in sounds like human hearing does.", "creator": "LaTeX with hyperref package"}}}