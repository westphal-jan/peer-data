{"id": "1702.07203", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2017", "title": "Utilizing Lexical Similarity for pivot translation involving resource-poor, related languages", "abstract": "We investigate the use of pivot languages for phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora. We show that subword-level pivot translation via a related pivot language is: (i) highly competitive with the best direct translation model and (ii) better than a pivot model which uses an unrelated pivot language, but has at its disposal large parallel corpora to build the source-pivot (S-P) and pivot-target (P-T) translation models. In contrast, pivot models trained at word and morpheme level are far inferior to their direct counterparts. We also show that using multiple related pivot languages can outperform a direct translation model. Thus, the use of subwords as translation units coupled with the use of multiple related pivot languages can compensate for the lack of a direct parallel corpus. Subword units make pivot models competitive by (i) utilizing lexical similarity to improve the underlying S-P and P-T translation models, and (ii) reducing loss of translation candidates during pivoting.", "histories": [["v1", "Thu, 23 Feb 2017 13:13:53 GMT  (28kb)", "http://arxiv.org/abs/1702.07203v1", "Submitted to ACL 2017, 10 pages, 9 tables"], ["v2", "Wed, 4 Oct 2017 20:55:03 GMT  (25kb)", "http://arxiv.org/abs/1702.07203v2", "Accepted at IJCNLP 2017, 7 pages, 7 tables"]], "COMMENTS": "Submitted to ACL 2017, 10 pages, 9 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anoop kunchukuttan", "maulik shah", "pradyot prakash", "pushpak bhattacharyya"], "accepted": false, "id": "1702.07203"}, "pdf": {"name": "1702.07203.pdf", "metadata": {"source": "CRF", "title": "Utilizing Lexical Similarity for pivot translation involving resource-poor, related languages", "authors": ["Anoop Kunchukuttan", "Maulik Shah", "Pradyot Prakash"], "emails": ["anoopk@cse.iitb.ac.in", "maulik.shah@cse.iitb.ac.in", "pradyot@cse.iitb.ac.in", "pb@cse.iitb.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n07 20\n3v 1\n[ cs\n.C L\n] 2\n3 Fe\nb 20\n17\nfor phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora. We show that subword-level pivot translation via a related pivot language is: (i) highly competitive with the best direct translation model and (ii) better than a pivot model which uses an unrelated pivot language, but has at its disposal large parallel corpora to build the source-pivot (S-P) and pivot-target (P-T) translation models. In contrast, pivot models trained at word and morpheme level are far inferior to their direct counterparts. We also show that using multiple related pivot languages can outperform a direct translation model. Thus, the use of subwords as translation units coupled with the use of multiple related pivot languages can compensate for the lack of a direct parallel corpus. Subword units make pivot models competitive by (i) utilizing lexical similarity to improve the underlying S-P and P-T translation models, and (ii) reducing loss of translation candidates during pivoting."}, {"heading": "1 Introduction", "text": "Related languages are those that exhibit lexical and structural similarities on account of sharing a common ancestry or being in contact for a long period of time (Bhattacharyya et al., 2016). Machine Translation between related languages is a major requirement since there is substantial government, commercial and cultural communication among people speaking related languages (Europe, India and South-East Asia being prominent examples and linguistic regions in Africa pos-\nsibly in the future). These translation requirements generally fall into the following scenarios: [A] between related languages, [B] to/from a lingua franca like English. However, most of these language pairs have few or no parallel corpora.\nWhen limited parallel corpus is available, translation using subword units like characters, orthographic syllables, etc. is useful in scenario [A] (Vilar et al., 2007; Kunchukuttan and Bhattacharyya, 2016c). This method utilizes lexical similarity between the related languages. Lexical similarity refers to sharing of many words with similar form (spelling/pronunciation) and meaning among related languages e.g. blindness is andhapana in Hindi, aandhaLepaNaa in Marathi. On the other hand, if no parallel corpus is available between two languages, pivot based SMT provides a systematic way of using a third language (called the pivot language) with which the source and target languages share parallel corpora. The general pivot based approach does not make any assumptions about relatedness between the source, pivot and target languages.\nIn this paper, we propose methods for translation involving related languages when no parallel corpus is available. For scenario [A] (translation between related languages), we utilize lexical similarity between the languages to improve pivot translation by: (i) using a pivot language that is related to source and target languages, (ii) using a subword translation unit viz. orthographic syllables (OS), (iii) identifying the pivot language based on its similarity with the source and/or target language. The contributions of our work are:\nWe show that using a pivot language related to the source and target languages along with subword level translation (with OS as translation unit) is very competitive with the best direct translation systems. Specifically, we show that our ap-\nproach:\n\u2022 significantly outperforms morpheme and word level pivot translations (about 15% and 60% increase in BLEU score respectively).\n\u2022 is competitive with a skyline OS level direct translation model (achieving about 90% of direct system\u2019s BLEU score) and better than the corresponding word-level direct translation model.\n\u2022 is better than a word level pivot translation model which uses an unrelated pivot language and is trained on a large parallel corpus.\n\u2022 is more robust to change of translation domain as compared to word and morpheme level pivot models.\nWe show that use of multiple pivot languages can compensate for the lack of direct parallel corpora. Specifically,\n\u2022 We show that combining pivot systems using different pivot languages can outperform the best bilingual direct translation system. Thus multilinguality acts as booster to deliver the best translation results between related languages.\n\u2022 Our investigations on the choice of pivot language suggests that a pivot language closer to the target language may be preferable for better translation performance.\nTo the best of our knowledge, ours is the first work that shows that a pivot system can be very competitive with (or even outperform) a direct system (in the restricted case of related languages).\nThe rest of the paper is organized as follows. Section 2 discusses related work. Since we utilize lexical similarity between languages we discuss this concept in Section 3. Section 4 explains our method for translation between related languages. Section 5 describes our experimental setup.\nThe next two sections analyse the results of our experiments on pivot translation between related languages: Section 6 discusses the effect of subword units, while Section 7 discusses the effect of multiple pivot languages and factors affecting choice of pivot languages. Section 8 analyzes the results of the our initial investigations for translation between unrelated languages via a pivot which is related to either source/target (but not both) (scenario B). Finally, we summarize our work and discuss future work in Section 9."}, {"heading": "2 Related Work", "text": "Our work straddles two strands of research in statistical machine translation: (i) subwords as basic units for translation between related languages, and (ii) pivot-based machine translation.\nModeling the lexical similarity among related languages is the key to building good-quality SMT systems with limited parallel corpora. This can be achieved by subword level transformations. Lexical similarity can be modelled in the standard SMT pipeline by transliteration of words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014). An alternative method to improve translation quality is to use subwords as basic translation units. Subword units like character (Vilar et al., 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees. We have used orthographic syllables as the basic translation unit in our experiments, since they have been shown to outperform other subword units.\nPivot translation provides a systematic way for translation between two languages through an intermediate language. Multiple approaches to pivoting have been proposed viz. (i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007). We use triangulation in our experiments as it has been shown to outperform other approaches.\nIn the context of translation involving related languages, there has been some work using subword level units for pivot-based SMT. Characterbased pivot translation has been explored for one leg of the pivot system (S-P or P-T) when the pivot is related to either the source or target (but not both) (Tiedemann, 2012; Tiedemann and Nakov, 2013). In our scenario, the source, target and pivot languages are all related. Hence subword level translation is possible for both legs (source-pivot and pivot-target). Morpheme level pivot translation has been shown to be better than word level pivot translation (More et al., 2015). More et al.\n(2015) and Dabre et al. (2015) have experimented with multiple pivot languages, but the translation accuracy is lower than the direct model."}, {"heading": "3 Lexical Similarity in related languages", "text": "Two words are said to be lexically similar if they have similar form (spelling/pronunciation) and meaning e.g. time is samay in Hindi, samayam in Malayalam. These words could be cognates, lateral borrowings or loan words from other languages. Two languages are said to be lexically similar if they share a lot of lexically similar words. Lexical similarity is a key characteristic of related languages.\nEthnologue1 computes the percentage of lexical similarity between two linguistic varieties by comparing a set of standardized wordlists and counting those forms that show similarity in both form and meaning (Rensch, 1992). However, this requires extensive fieldwork and linguistic analysis.\nFor our analysis, we use a simpler measure of lexical similarity which depends on the orthography, rather than phonology. For languages which use scripts with a high grapheme-to-phoneme correspondence, this is a reasonable approach to take. We use the Longest Common Subsequence Ratio (LCSR) as a measure of lexical similarity between two strings (Melamed, 1995):\nLCSR(s1, s2) = |LCS(s1, s2)|\nmax(|s1|, |s2|) (1)\nwhere, s1, s2 are two strings and LCS is the\nlongest common subsequence between them.\nLCSR can be a versatile tool to analyze related languages and can be applied to different linguistic entities to measure their lexical similarity. In addition to its utility for measuring lexical similarity between words (Melamed, 1999; Kondrak, 2005), it can also be used to compare sentences (assuming the two sentences to be a sequence of characters). The sentences could be from the same language (e.g. reference translation and output of MT system (Tiedemann, 2012)) or different languages (e.g. parallel translations (Kunchukuttan and Bhattacharyya, 2016c)). To compute LCSR across languages, the script should be either the same or a correspondence between characters should exist. The lexical similarity between two languages can also be computed by averaging over the lexical similarities of sentence\n1 www.ethnologue.com\npairs in a parallel corpus. Table 1 shows the lexical similarities between various Indian languages which we have used in our experiments. These were computed on the multilingually aligned ILCI parallel corpus (Jha, 2012). The pairwise rankings of languages as measured by LCSR agree with the general consensus about similarity between Indian languages."}, {"heading": "4 Pivot Translation for Related Languages", "text": "We first train phrase-based SMT models between S-P and P-T language pairs with subword units (orthographic syllables in our case). We create a pivot translation system by combining the S-P and P-T models using phrase table triangulation. If multiple pivot languages are available, linear interpolation is used to combine pivot translation models. In this section, we describe each component of our system and the design choices."}, {"heading": "4.1 Orthographic Syllable level translation", "text": "We use orthographic syllables (OS) (Kunchukuttan and Bhattacharyya, 2016c) as the basic units of translation. It is a linguisticallymotivated, variable-length unit which consists of a consonant core with zero or more vowels (a C+V combination) (e.g. spacious would be segmented as spa ciou s). Since the vocabulary is much smaller than the morpheme and word level models, data sparsity is not a problem. The variable length units provide appropriate context for translation between related languages.\nThis unit has outperformed character n-gram, word and morpheme level models for the task of translation between related languages. Or-\nthographic syllables outperform other units even when: (i) the languages are not very closely related (ii) the languages do not have a genetic relation, but only a contact relation.\nUsing OS level models for pivot translation ensures that the underlying S-P and P-T translation models are better than the corresponding models trained on other translation units."}, {"heading": "4.2 Using a related pivot language via Triangulation", "text": "We use phrase-table triangulation to fuse the srcpivot and pivot-tgt OS level models for generating the pivot model\u2019s phrase table. Triangulation joins the two tables on the common phrases in the pivot language and recomputes the probabilities in the phrase table (direct/inverse phrase and lexical translation probabilities) by assuming a generative process and making a few independence assumptions:\nP (s\u0304|t\u0304) = \u2211\np\u0304\nP (s\u0304|p\u0304, t\u0304)P (p\u0304|t\u0304) (2)\n\u2248 \u2211\np\u0304\nP (s\u0304|p\u0304)P (p\u0304|t\u0304) (3)\nwhere s\u0304, p\u0304 and t\u0304 are source, pivot and target lan-\nguage phrases respectively\nAs compared to word and morpheme level models, OS level pivot models are likely to find more common pivot language phrases because of the smaller vocabulary size. Hence, data sparsity, a problem recognized by Dabre et al. (2015) and More et al. (2015), will be a lesser impediment to pivoting for OS level models."}, {"heading": "4.3 Multiple Pivot Languages", "text": "When multiple pivot languages are available, we can either use all pivot languages or choose the best pivot language.\nThe choice of pivot language is affected by its relatedness to the source and target language (Paul et al., 2013). We studied how lexical similarity of the pivot language to the source and/or target language affects translation quality.\nWe also experimented with combining multiple pivot language translation models using linear interpolation (Bisazza et al., 2011). Linear interpolation assigns weights to each phrase table and the feature values for each phrase pair are interpolated\nusing these weights:\nP (s\u0304|t\u0304) = \u2211\ni\n\u03b1iPi(s\u0304|t\u0304) (4)\nsubject to: \u2211\ni \u03b1i = 1, \u03b1i \u2265 0 where, \u03b1i= interpolation weight for phrase table i.\nWe experimented with different strategies for determining the interpolation weights using linguistic similarity between the languages."}, {"heading": "5 Experimental Setup", "text": "This section describes languages and datasets used in our experiments and details of our system."}, {"heading": "5.1 Languages", "text": "We experimented with multiple languages from the two major language families of the Indian subcontinent (Indo-Aryan branch of Indo-European and Dravidian). The Indian subcontinent is considered a linguistic area (Emeneau, 1956) due to convergence of linguistic properties as a result of contact between languages over a long period of time. Specifically, there is substantial overlap between the vocabulary of these languages to varying degrees due to cognates, language contact and loan-words from Sanskrit and English. All these languages have a rich inflectional morphology with Dravidian languages (and Marathi to some degree), being agglutinative. Table 1 shows the languages involved and their lexical similarities."}, {"heading": "5.2 Dataset", "text": "We used the multilingual Indian Language Corpora Initiative (ILCI) corpus2 for our experiments (Jha, 2012), containing sentences from tourism and health domains. The corpus contains sentences aligned across 11 languages. This multilingual alignment enables a fair comparison of (i) direct and pivot translation systems, (ii) multiple pivot languages, and (iii) lexical similarity of languages.\nThe data split is as follows \u2013 training: 44,777, tuning 1K, test: 2K sentences. Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.8M (news websites), mal: 200K, ben: 400K, pan: 100K (Quasthoff et al., 2006) sentences]. We\n2available on request from TDIL (tdil-dc.in)\nused the target language side of the parallel corpora for morpheme and OS level LMs."}, {"heading": "5.3 System details", "text": "PBSMT systems were trained using the Moses system (Koehn et al., 2007), withmgiza3 for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning. Cube pruning with pop-limit=1000 was used for decoding (Kunchukuttan and Bhattacharyya, 2016a). We trained 5-gram LMs with Kneser-Ney smoothing for word and morpheme level models and 10-gram LMs for orthographic syllable level models.\nWe use unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morphemes. These segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006). We use the tmtriangulate for phrase-table triangulation 4. Prior to triangulation, we prune phrase pairs with direct phrase translation probability less than 0.01 to make the triangulation process more efficient. We use the combine-ptables (Bisazza et al., 2011), (part of the Moses distribution), for linear interpolation of phrase tables."}, {"heading": "6 Discussion: Subword units for pivot translation", "text": "In this section, we present results related to different subword units and analyse them."}, {"heading": "6.1 Comparison of different subword units", "text": "Table 2 compares pivot-based SMT systems built with different units. We experimented with language triples over various combinations of the language families (Indo-Aryan and Dravidian).\nWe observe that in each case the OS level pivot model significantly outperforms word and morph-level pivot models (an average improvement of about 61% and 14% improvement respectively). The OS level models show the greatest improvement over other units when the source and target languages belong to different families, showing that OS level models can utilize the lexical similarity between these languages. Translation between agglutinative Dravidian languages also shows a major improvement.\nOS level pivot models are better than other units for two reasons. One, the underlying S-P and PT translation models are better (average 16% and 3% improvement over word and morph-level models). However, this alone does not the explain the substantial improvement in OS level pivot translation. The triangulation process, which is a join on common keys, is faced with sparsity arising from the large word and morpheme phrase-table vocabulary. The word level triangulated table actually loses translation candidates because of sparsity. On the other hand, the OS phrase table vocabulary is smaller, so the impact of sparsity is limited and the triangulated phrase table size increases by a few multiples for OS level models."}, {"heading": "6.2 Comparison of pivot models with direct models", "text": "We also compared the OS-level pivot system with direct system trained on different translation units (See Table 3). It outperforms a word-level direct translation system between source and target by 5%, which is encouraging. Even more remarkable is that the OS level pivot model is competitive with the morph and OS level direct translation models (achieving about 95% and 91% of their respective BLEU scores). To put this fact in perspective, the BLEU scores of morph and word-level pivot systems are far below their corresponding direct systems (about 15% and 35% respectively).\nThese observations strongly suggests that pivoting at the OS level can reconstruct the direct translation system better compared to the word and morph level pivot systems, and the resultant OS level pivot systems are quite close to the best direct translation systems."}, {"heading": "6.3 Using an unrelated pivot language", "text": "In the experiments described so far, we have considered a related pivot language with which the source and target language share small parallel corpora. We compare this to a very likely pivot scenario - the pivot language is an unrelated language like English with which the source and target languages share a lot of parallel corpora.\nFor this experiment, we used Google Translate5 as a translation system using an unrelated pivot. It is known that Google Translate uses English as a pivot language for many translation pairs where a direct translation corpus is not available (TAUS, 2013). For the languages we experimented with, this can be attested by the fact that English words turn up in the translations provided by Google Translate. Google Translate is presumably trained on large corpora, certainly orders of\n3 github.com/moses-smt/mgiza 4github.com/tamhd/MultiMT 5 https://translate.google.com\nmagnitude larger than our translation systems, but at the word/morpheme level.\nWe compared our pivot translation models (using a related pivot language \u2013 Telugu \u2013, trained on tourism and health domains) with Google Translate (using an unrelated pivot \u2013 English \u2013 ) by testing on an agriculture domain test set of 1000 sentences from the ILCI corpus. This ensures a fair comparison between the two systems. Table 4 shows the Google Translate results alongside our pivot translation results. The word/morpheme level translation using an unrelated pivot and large corpora is inferior to an OS level system using a related pivot and trained on small parallel corpora.\nFor the language pairs involving mal as source language, we see that the Google Translate system produces a large number of OOVs. For the hin-mal language pair, the output of Google Translate was slightly shorter than reference translation, and the precision and recall were lower as compared to the OS-level model (as measured using METEOR)."}, {"heading": "6.4 Cross-Domain Translation", "text": "We also investigated if the OS level pivot models are robust to domain change by evaluating the translation models trained on tourism & health domains on an agriculture domain test set of 1000 sentences (from the ILCI corpus). Table 5 shows the results of these experiments. In this crossdomain translation scenario too, the OS level pivot models outperforms morph level pivot models, and is equivalent to a direct morph level model. The OS-level models systems experience much lesser drop in BLEU scores vis-a-vis direct models, in contrast to the morph level models. Since morph-level pivot models encounter unknown vocabulary in a new domain, they are less resistant to domain change than OS level models."}, {"heading": "7 Discussion: Multiple pivot languages", "text": "We discuss the results of experiments with multiple pivots and study the choice of pivot language. The experiments in this section refer to OS level pivot models unless otherwise specified."}, {"heading": "7.1 Combining Multiple Pivot Models", "text": "We investigated if multiple pivot translation systems can act as a substitute for a direct translation system. For this we combined multiple pivot translation systems using linear interpolation. We tried various weighting strategies: equal weighting as well as proportional to lexical similarity of the pivot to (i) source, (ii) target, (iii) average of (i) and (ii) (see Table 6 for results).\nFor each weighting strategy, the interpolated system outperformed not just the individual pivot systems, but also the direct translation system. We see more than 1.5 BLEU point improvement over the best single pivot model. Previous studies have shown that word and morph level multiple pivot systems were not able to outperform the direct system, possibly due to the effect of sparsity on triangulation (More et al., 2015; Dabre et al., 2015). Thus, it is remarkable that multi-linguality could help overcome the lack of a direct translation system between the two languages. Once the ill-effects of data sparsity are reduced due to the use of OS level pivot, multiple pivot languages can maximize translation performance because: (i) they bring in more translation options, and (ii) they improve the estimates of feature values with evidence from multiple languages.\nWe observe that equal weighting as well as assigning interpolation weights proportional to the average lexical similarity of the pivot to source and target are the best interpolation strategies."}, {"heading": "7.2 Augmenting direct translation with pivot translation", "text": "We augmented the direct translation system with each of the interpolated systems discussed in the previous section using equal-weighted interpolation. In addition, we also tried combining all the pivot systems and the direct system using equalweighted interpolation (all interpolate). The results are shown in Table 7. We observe that the augmented system further improved the translation accuracy by about 1 BLEU point over the direct system. Thus, the use of all related languages via pivoting and interpolation gave the best translation between a language pair."}, {"heading": "7.3 Choice of pivot language", "text": "Previous studies on the choice of pivot language have been impeded by the morphological diversity of the pivot languages and morphologically poorer pivots tend to perform better (More et al., 2015; Dabre et al., 2015; Paul et al., 2013). Thus the varied levels of sparsity induced by morphological properties dictated the choice of pivot, rather than the intrinsic properties of the pivot language. Sparsity is not a major concern with OS level models, hence we are able to study the effect of language properties on the choice of the pivot language.\nWe studied if the lexical similarity of the pivot language to the source and/or target language had\nan impact on the translation quality. We studiedmal-hin translation andmar-ben translation for multiple pivot languages. Table 8 shows the translation accuracy for different pivot languages along with lexical similarity of the pivot language to the (i) source (ii) target and (iii) average of (i) and (ii). The choice of pivot language makes a limited but observable difference (within 1 BLEU point).\nFor Marathi-Bengali translation, Hindi is the most beneficial pivot and it is most similar to the target as well as equidistant from both source and target. For Malayalam-Hindi translation, Marathi is the most useful pivot, with Gujarati close behind. Gujarati is most similar to the target as well as equidistant from both source and target. Gujarati is more similar to Marathi.\nThese observations suggest that a pivot language which is either closer to the target language or equidistant from both source and target is more useful than having a pivot which is closer to the source language. This provides further evidence to the observations by Paul et al. (2013) that target language features are more important for \u201ccoherent\u201d language pairs."}, {"heading": "8 Translation involving an unrelated language", "text": "So far, we have reported our investigations related to scenario [A]. But Scenario [B] (translation between unrelated languages via a pivot related to source or target) is also a common situation. For instance, no parallel corpus may exists for Marathi-English translation, but HindiEnglish parallel corpus may be available (Hindi and Marathi are related languages). In this case, we adopt the transfer approach to pivot translation, but the two legs of the pivot translation use different translation units. Marathi-Hindi trans-\nlation occurs at the subword level, while HindiEnglish translation occurs at the word level. In this section, we present the results of our investigations into scenario [B]. We experimented with Indian language to English and vice versa via Hindi as pivot. We trained our Hindi-English translation models on the ILCI corpus. For EnglishIndian language (IL) translation, rule-based source reordering (Ramanathan et al., 2008) was used to overcome the structural divergence between English and Indian languages (English is an SVO languages and Indian languages are SOV).\nTable 9 shows the results for the proposed approach for Indian Language-Hindi-English translation and vice versa . It also shows a comparison with word level direct translation results. For IL-hin-eng translation, the pivot system using OS model for IL-hin is better than the one using a morph model for IL-hin as well as a direct word level IL-eng translation model. However, the gains are minor compared to the gains we observed when all languages were related. For eng-hin-IL translation, using an OS level IL-hin pivot offers no advantage over a morph level ILhin pivot. Possibly, the OS model is sensitive to word order errors made by the eng-hin leg of the pivot translation.\nTo summarize, the use of OS level model as one leg of a transfer-based pivot translation has only a minor benefit for translation between unrelated languages through a pivot related to either source or pivot. Better solutions need to be investigated."}, {"heading": "9 Conclusion & Future Work", "text": "We investigated the use of pivot translation for translation involving related languages when direct parallel corpora are not available. We show that pivot translation between related languages can rival or outperform direct translation if subword level translation is done using multiple related pivot languages. Subword units make this possible by using lexical similarity and reducing losses in pivoting (owing to small vocabulary).\nOur observations also hold lessons for the design of multilingual translation systems. It is better to invest in building/mining a parallel corpus through a related pivot (preferably closer to the target) than an unrelated pivot. When building translation systems between a lingua franca and an unrelated language, building/mining parallel corpora between the lingua franca and the related lan-\nguages is not the best choice. Rather, it is better to consider one of the related languages as a pivot language. These design decisions can ensure better translation accuracy with lower investment in the development of parallel corpora.\nOur results show that better methods are needed for pivot translation involving unrelated languages. The major problem current methods face is data sparsity resulting from word/morpheme level translation. Subword level translation between arbitrary languages is viable in the neural MT framework - hence, NMT may be a good research direction to tackle this translation scenario."}], "references": [{"title": "Catalanenglish statistical machine translation without parallel corpus: bridging through spanish", "author": ["Jose B Marino Adri\u2018a De Gispert"], "venue": "In In Proc. of 5th International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "Gispert.,? \\Q2006\\E", "shortCiteRegEx": "Gispert.", "year": 2006}, {"title": "Statistical machine translation between related languages", "author": ["Pushpak Bhattacharyya", "Mitesh Khapra", "Anoop Kunchukuttan."], "venue": "www.cfilt.iitb.ac.in/publications/naacl-2016-tutorial.pdf. NAACL Tutorials.", "citeRegEx": "Bhattacharyya et al\\.,? 2016", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "Fill-up versus interpolation methods for phrase-based smt adaptation", "author": ["Arianna Bisazza", "Nick Ruiz", "Marcello Federico", "FBK-Fondazione Bruno Kessler."], "venue": "IWSLT. pages 136\u2013143.", "citeRegEx": "Bisazza et al\\.,? 2011", "shortCiteRegEx": "Bisazza et al\\.", "year": 2011}, {"title": "HindEnCorp \u2013 Hindi-English and Hindi-only Corpus for Machine Translation", "author": ["Ond\u0159ej Bojar", "Vojt\u011bch Diatka", "Pavel Rychl\u00fd", "Pavel Stra\u0148\u00e1k", "V\u0131\u0301t Suchomel", "Ale\u0161 Tamchyna", "Daniel Zeman"], "venue": "In Proceedings of the 9th International Conference", "citeRegEx": "Bojar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bojar et al\\.", "year": 2014}, {"title": "Batch tuning strategies for statistical machine translation", "author": ["Colin Cherry", "George Foster."], "venue": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.", "citeRegEx": "Cherry and Foster.,? 2012", "shortCiteRegEx": "Cherry and Foster.", "year": 2012}, {"title": "Machine translation by triangulation: Making effective use of multi-parallel corpora", "author": ["Trevor Cohn", "Mirella Lapata."], "venue": "ACL.", "citeRegEx": "Cohn and Lapata.,? 2007", "shortCiteRegEx": "Cohn and Lapata.", "year": 2007}, {"title": "Leveraging small multilingual corpora for smt using many pivot languages", "author": ["Raj Dabre", "Fabien Cromieres", "Sadao Kurohashi", "Pushpak Bhattacharyya."], "venue": "HLT-NAACL. pages 1192\u20131202.", "citeRegEx": "Dabre et al\\.,? 2015", "shortCiteRegEx": "Dabre et al\\.", "year": 2015}, {"title": "Hindi-to-Urdu machine translation through transliteration", "author": ["Nadir Durrani", "Hassan Sajjad", "Alexander Fraser", "Helmut Schmid."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Durrani et al\\.,? 2010", "shortCiteRegEx": "Durrani et al\\.", "year": 2010}, {"title": "India as a lingustic area", "author": ["Murray B Emeneau."], "venue": "Language .", "citeRegEx": "Emeneau.,? 1956", "shortCiteRegEx": "Emeneau.", "year": 1956}, {"title": "The TDIL program and the Indian Language Corpora Initiative", "author": ["Girish Nath Jha."], "venue": "Language Resources and Evaluation Conference.", "citeRegEx": "Jha.,? 2012", "shortCiteRegEx": "Jha.", "year": 2012}, {"title": "Moses: Open source toolkit for Statistical Machine Translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Cognates and word alignment in bitexts", "author": ["Grzegorz Kondrak."], "venue": "Proceedings of the tenth machine translation summit (mt summit x) pages 305\u2013312.", "citeRegEx": "Kondrak.,? 2005", "shortCiteRegEx": "Kondrak.", "year": 2005}, {"title": "Faster decoding for subword level phrasebased smt between related languages", "author": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya."], "venue": "Third Workshop on NLP for Similar Languages, Varieties and Dialects.", "citeRegEx": "Kunchukuttan and Bhattacharyya.,? 2016a", "shortCiteRegEx": "Kunchukuttan and Bhattacharyya.", "year": 2016}, {"title": "Learning variable length units for SMT between related languages via byte pair encoding", "author": ["Anoop Kunchukuttan", "Pushpak Bhattacharyya."], "venue": "ArXiv e-prints, arxiv:1610.06510 .", "citeRegEx": "Kunchukuttan and Bhattacharyya.,? 2016b", "shortCiteRegEx": "Kunchukuttan and Bhattacharyya.", "year": 2016}, {"title": "Orthographic syllable as basic unit for smt between related languages", "author": ["Anoop Kunchukutt n an Pushpak Bhattacharyya."], "venue": "Empirical Methods in Natural Language Processing.", "citeRegEx": "Bhattacharyya.,? 2016c", "shortCiteRegEx": "Bhattacharyya.", "year": 2016}, {"title": "The IIT Bombay SMT System for ICON 2014 Tools contest", "author": ["Anoop Kunchukuttan", "Ratish Pudupully", "Rajen Chatterjee", "Abhijit Mishra", "Pushpak Bhattacharyya."], "venue": "NLP Tools Contest at ICON 2014.", "citeRegEx": "Kunchukuttan et al\\.,? 2014", "shortCiteRegEx": "Kunchukuttan et al\\.", "year": 2014}, {"title": "Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons", "author": ["I Dan Melamed."], "venue": "Third Workshop on Very Large Corpora.", "citeRegEx": "Melamed.,? 1995", "shortCiteRegEx": "Melamed.", "year": 1995}, {"title": "Bitext maps and alignment via pattern recognition", "author": ["I Dan Melamed."], "venue": "Computational Linguistics 25(1):107\u2013130.", "citeRegEx": "Melamed.,? 1999", "shortCiteRegEx": "Melamed.", "year": 1999}, {"title": "Augmenting pivot based smt with word segmentation", "author": ["Rohit More", "Anoop Kunchukuttan", "Raj Dabre", "Pushpak Bhattacharyya."], "venue": "International Conference on Natural Language Processing.", "citeRegEx": "More et al\\.,? 2015", "shortCiteRegEx": "More et al\\.", "year": 2015}, {"title": "Combining word-level and character-level models for machine translation between closely-related languages", "author": ["Preslav Nakov", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short", "citeRegEx": "Nakov and Tiedemann.,? 2012", "shortCiteRegEx": "Nakov and Tiedemann.", "year": 2012}, {"title": "How to choose the best pivot language for automatic translation of low-resource languages", "author": ["Michael Paul", "Andrew Finch", "Eiichrio Sumita."], "venue": "ACM Transactions on Asian Language Information Processing (TALIP) 12(4):14.", "citeRegEx": "Paul et al\\.,? 2013", "shortCiteRegEx": "Paul et al\\.", "year": 2013}, {"title": "Corpus portal for search in monolingual corpora", "author": ["Uwe Quasthoff", "Matthias Richter", "Christian Biemann."], "venue": "Proceedings of the fifth international conference on language resources and evaluation.", "citeRegEx": "Quasthoff et al\\.,? 2006", "shortCiteRegEx": "Quasthoff et al\\.", "year": 2006}, {"title": "Simple syntactic and morphological processing can help english-hindi statistical machine translation", "author": ["Ananthakrishnan Ramanathan", "Jayprasad Hegde", "Ritesh M Shah", "Pushpak Bhattacharyya", "M Sasikumar."], "venue": "IJCNLP. pages 513\u2013520.", "citeRegEx": "Ramanathan et al\\.,? 2008", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2008}, {"title": "Morphological Processing for English-Tamil Statistical Machine Translation", "author": ["Loganathan Ramasamy", "Ond\u0159ej Bojar", "Zden\u011bk \u017dabokrtsk\u00fd."], "venue": "Proceedings of the Workshop on Machine Translation and Parsing in Indian Languages.", "citeRegEx": "Ramasamy et al\\.,? 2012", "shortCiteRegEx": "Ramasamy et al\\.", "year": 2012}, {"title": "Calculating lexical similarity", "author": ["Calvin R Rensch."], "venue": "Eugene H. Casad, editor, Windows on bilingualism, Summer Institute of Linguistics and the University of Texas at Arlington.", "citeRegEx": "Rensch.,? 1992", "shortCiteRegEx": "Rensch.", "year": 1992}, {"title": "Character-based PBSMT for closely related languages", "author": ["J\u00f6rg Tiedemann."], "venue": "Proceedings of the 13th Conference of the European Association for Machine Translation.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Character-based pivot translation for under-resourced languages and domains", "author": ["J\u00f6rg Tiedemann."], "venue": "EACL.", "citeRegEx": "Tiedemann.,? 2012", "shortCiteRegEx": "Tiedemann.", "year": 2012}, {"title": "Analyzing the use of character-level translation with sparse and noisy datasets", "author": ["J\u00f6rg Tiedemann", "Preslav Nakov."], "venue": "RANLP.", "citeRegEx": "Tiedemann and Nakov.,? 2013", "shortCiteRegEx": "Tiedemann and Nakov.", "year": 2013}, {"title": "A comparison of pivot methods for phrase-based statistical machine translation", "author": ["Masao Utiyama", "Hitoshi Isahara."], "venue": "HLT-NAACL. pages 484\u2013491.", "citeRegEx": "Utiyama and Isahara.,? 2007", "shortCiteRegEx": "Utiyama and Isahara.", "year": 2007}, {"title": "Can we translate letters", "author": ["David Vilar", "Jan-T Peter", "Hermann Ney"], "venue": "In Proceedings of the Second Workshop on Statistical Machine Translation", "citeRegEx": "Vilar et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vilar et al\\.", "year": 2007}, {"title": "Morfessor 2.0: Python implementation and extensions for morfessor baseline", "author": ["Sami Virpioja", "Peter Smit", "Stig-Arne Gr\u00f6nroos", "Mikko Kurimo"], "venue": null, "citeRegEx": "Virpioja et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Virpioja et al\\.", "year": 2013}, {"title": "Pivot language approach for phrase-based statistical machine translation", "author": ["Hua Wu", "Haifeng Wang."], "venue": "Machine Translation 21(3):165\u2013181.", "citeRegEx": "Wu and Wang.,? 2007", "shortCiteRegEx": "Wu and Wang.", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "long period of time (Bhattacharyya et al., 2016).", "startOffset": 20, "endOffset": 48}, {"referenceID": 29, "context": "is useful in scenario [A] (Vilar et al., 2007; Kunchukuttan and Bhattacharyya, 2016c).", "startOffset": 26, "endOffset": 85}, {"referenceID": 7, "context": "Lexical similarity can be modelled in the standard SMT pipeline by transliteration of words while decoding (Durrani et al., 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al.", "startOffset": 107, "endOffset": 129}, {"referenceID": 19, "context": ", 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014).", "startOffset": 27, "endOffset": 81}, {"referenceID": 15, "context": ", 2010) or post-processing (Nakov and Tiedemann, 2012; Kunchukuttan et al., 2014).", "startOffset": 27, "endOffset": 81}, {"referenceID": 29, "context": "Subword units like character (Vilar et al., 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 29, "endOffset": 66}, {"referenceID": 25, "context": "Subword units like character (Vilar et al., 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 29, "endOffset": 66}, {"referenceID": 27, "context": ", 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 43, "endOffset": 70}, {"referenceID": 13, "context": ", 2007; Tiedemann, 2009), character n-gram (Tiedemann and Nakov, 2013), orthographic syllables (Kunchukuttan and Bhattacharyya, 2016c) and Byte Pair Encoded units (Kunchukuttan and Bhattacharyya, 2016b) have been explored and have been shown to improve translation quality to varying degrees.", "startOffset": 163, "endOffset": 202}, {"referenceID": 28, "context": "(i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007).", "startOffset": 88, "endOffset": 115}, {"referenceID": 28, "context": "(i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007).", "startOffset": 149, "endOffset": 218}, {"referenceID": 31, "context": "(i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007).", "startOffset": 149, "endOffset": 218}, {"referenceID": 5, "context": "(i) synthetic corpus generation (Adri\u2018a De Gispert, 2006) (ii) transferbased/pipelining (Utiyama and Isahara, 2007) (iii) phrase-table triangulation (Utiyama and Isahara, 2007; Wu and Wang, 2007; Cohn and Lapata, 2007).", "startOffset": 149, "endOffset": 218}, {"referenceID": 26, "context": "Characterbased pivot translation has been explored for one leg of the pivot system (S-P or P-T) when the pivot is related to either the source or target (but not both) (Tiedemann, 2012; Tiedemann and Nakov, 2013).", "startOffset": 168, "endOffset": 212}, {"referenceID": 27, "context": "Characterbased pivot translation has been explored for one leg of the pivot system (S-P or P-T) when the pivot is related to either the source or target (but not both) (Tiedemann, 2012; Tiedemann and Nakov, 2013).", "startOffset": 168, "endOffset": 212}, {"referenceID": 18, "context": "Morpheme level pivot translation has been shown to be better than word level pivot translation (More et al., 2015).", "startOffset": 95, "endOffset": 114}, {"referenceID": 6, "context": "(2015) and Dabre et al. (2015) have experimented with multiple pivot languages, but the translation accuracy is lower than the direct model.", "startOffset": 11, "endOffset": 31}, {"referenceID": 24, "context": "Ethnologue computes the percentage of lexical similarity between two linguistic varieties by comparing a set of standardized wordlists and counting those forms that show similarity in both form and meaning (Rensch, 1992).", "startOffset": 206, "endOffset": 220}, {"referenceID": 16, "context": "We use the Longest Common Subsequence Ratio (LCSR) as a measure of lexical similarity between two strings (Melamed, 1995):", "startOffset": 106, "endOffset": 121}, {"referenceID": 17, "context": "In addition to its utility for measuring lexical similarity between words (Melamed, 1999; Kondrak, 2005), it can also be used to compare sentences (assuming the two sentences to be a sequence of characters).", "startOffset": 74, "endOffset": 104}, {"referenceID": 11, "context": "In addition to its utility for measuring lexical similarity between words (Melamed, 1999; Kondrak, 2005), it can also be used to compare sentences (assuming the two sentences to be a sequence of characters).", "startOffset": 74, "endOffset": 104}, {"referenceID": 26, "context": "reference translation and output of MT system (Tiedemann, 2012)) or different languages (e.", "startOffset": 46, "endOffset": 63}, {"referenceID": 9, "context": "These were computed on the multilingually aligned ILCI parallel corpus (Jha, 2012).", "startOffset": 71, "endOffset": 82}, {"referenceID": 6, "context": "Hence, data sparsity, a problem recognized by Dabre et al. (2015) and More et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 6, "context": "Hence, data sparsity, a problem recognized by Dabre et al. (2015) and More et al. (2015), will be a lesser impediment to pivoting for OS level models.", "startOffset": 46, "endOffset": 89}, {"referenceID": 20, "context": "The choice of pivot language is affected by its relatedness to the source and target language (Paul et al., 2013).", "startOffset": 94, "endOffset": 113}, {"referenceID": 2, "context": "We also experimented with combining multiple pivot language translation models using linear interpolation (Bisazza et al., 2011).", "startOffset": 106, "endOffset": 128}, {"referenceID": 8, "context": "The Indian subcontinent is considered a linguistic area (Emeneau, 1956) due to convergence of linguistic properties as a result of contact between languages over a long period of time.", "startOffset": 56, "endOffset": 71}, {"referenceID": 9, "context": "We used the multilingual Indian Language Corpora Initiative (ILCI) corpus for our experiments (Jha, 2012), containing sentences from tourism and health domains.", "startOffset": 94, "endOffset": 105}, {"referenceID": 3, "context": "Language models for word-level systems were trained on the target side of training corpora plus monolingual corpora from various sources [hin: 10M (Bojar et al., 2014), tam: 1M (Ramasamy et al.", "startOffset": 147, "endOffset": 167}, {"referenceID": 23, "context": ", 2014), tam: 1M (Ramasamy et al., 2012), mar: 1.", "startOffset": 17, "endOffset": 40}, {"referenceID": 21, "context": "8M (news websites), mal: 200K, ben: 400K, pan: 100K (Quasthoff et al., 2006) sentences].", "startOffset": 52, "endOffset": 76}, {"referenceID": 10, "context": "PBSMT systems were trained using the Moses system (Koehn et al., 2007), withmgiza for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning.", "startOffset": 50, "endOffset": 70}, {"referenceID": 4, "context": ", 2007), withmgiza for alignment, the grow-diag-final-and heuristic for symmetrization of word alignments, and Batch MIRA (Cherry and Foster, 2012) for tuning.", "startOffset": 122, "endOffset": 147}, {"referenceID": 12, "context": "Cube pruning with pop-limit=1000 was used for decoding (Kunchukuttan and Bhattacharyya, 2016a).", "startOffset": 55, "endOffset": 94}, {"referenceID": 30, "context": "We use unsupervised morphological segmenters trained with Morfessor (Virpioja et al., 2013) for obtaining morphemes.", "startOffset": 68, "endOffset": 91}, {"referenceID": 21, "context": "These segmenters were trained on the ILCI corpus and the Leipzig corpus (Quasthoff et al., 2006).", "startOffset": 72, "endOffset": 96}, {"referenceID": 2, "context": "We use the combine-ptables (Bisazza et al., 2011), (part of the Moses distribution), for linear interpolation of phrase tables.", "startOffset": 27, "endOffset": 49}, {"referenceID": 18, "context": "Previous studies have shown that word and morph level multiple pivot systems were not able to outperform the direct system, possibly due to the effect of sparsity on triangulation (More et al., 2015; Dabre et al., 2015).", "startOffset": 180, "endOffset": 219}, {"referenceID": 6, "context": "Previous studies have shown that word and morph level multiple pivot systems were not able to outperform the direct system, possibly due to the effect of sparsity on triangulation (More et al., 2015; Dabre et al., 2015).", "startOffset": 180, "endOffset": 219}, {"referenceID": 18, "context": "Previous studies on the choice of pivot language have been impeded by the morphological diversity of the pivot languages and morphologically poorer pivots tend to perform better (More et al., 2015; Dabre et al., 2015; Paul et al., 2013).", "startOffset": 178, "endOffset": 236}, {"referenceID": 6, "context": "Previous studies on the choice of pivot language have been impeded by the morphological diversity of the pivot languages and morphologically poorer pivots tend to perform better (More et al., 2015; Dabre et al., 2015; Paul et al., 2013).", "startOffset": 178, "endOffset": 236}, {"referenceID": 20, "context": "Previous studies on the choice of pivot language have been impeded by the morphological diversity of the pivot languages and morphologically poorer pivots tend to perform better (More et al., 2015; Dabre et al., 2015; Paul et al., 2013).", "startOffset": 178, "endOffset": 236}, {"referenceID": 20, "context": "This provides further evidence to the observations by Paul et al. (2013) that target language features are more important for \u201ccoherent\u201d language pairs.", "startOffset": 54, "endOffset": 73}, {"referenceID": 22, "context": "For EnglishIndian language (IL) translation, rule-based source reordering (Ramanathan et al., 2008) was used to overcome the structural divergence between English and Indian languages (English is an SVO languages and Indian languages are SOV).", "startOffset": 74, "endOffset": 99}], "year": 2017, "abstractText": "We investigate the use of pivot languages for phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora. We show that subword-level pivot translation via a related pivot language is: (i) highly competitive with the best direct translation model and (ii) better than a pivot model which uses an unrelated pivot language, but has at its disposal large parallel corpora to build the source-pivot (S-P) and pivot-target (P-T) translation models. In contrast, pivot models trained at word and morpheme level are far inferior to their direct counterparts. We also show that using multiple related pivot languages can outperform a direct translation model. Thus, the use of subwords as translation units coupled with the use of multiple related pivot languages can compensate for the lack of a direct parallel corpus. Subword units make pivot models competitive by (i) utilizing lexical similarity to improve the underlying S-P and P-T translation models, and (ii) reducing loss of translation candidates during pivoting.", "creator": "LaTeX with hyperref package"}}}