{"id": "1702.07444", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Bandits with Movement Costs and Adaptive Pricing", "abstract": "We extend the model of Multi-armed Bandit with unit switching cost to incorporate a metric between the actions. We consider the case where the metric over the actions can be modeled by a complete binary tree, and the distance between two leaves is the size of the subtree of their least common ancestor, which abstracts the case that the actions are points on the continuous interval $[0,1]$ and the switching cost is their distance. In this setting, we give a new algorithm that establishes a regret of $\\widetilde{O}(\\sqrt{kT} + T/k)$, where $k$ is the number of actions and $T$ is the time horizon. When the set of actions corresponds to whole $[0,1]$ interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of $\\widetilde{\\Theta}(T^{2/3})$, which is the same rate one obtains when there is no penalty for movements. As our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of $\\widetilde{O}(T^{2/3})$ compared to the best fixed price in hindsight, which outperform the previous regret bound of $\\widetilde{O}(T^{3/4})$ for the problem.", "histories": [["v1", "Fri, 24 Feb 2017 01:51:48 GMT  (32kb)", "http://arxiv.org/abs/1702.07444v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["tomer koren", "roi livni", "yishay mansour"], "accepted": false, "id": "1702.07444"}, "pdf": {"name": "1702.07444.pdf", "metadata": {"source": "CRF", "title": "Bandits with Movement Costs and Adaptive Pricing", "authors": ["Tomer Koren"], "emails": ["tkoren@google.com", "rlivni@cs.princeton.edu", "mansour@tau.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n07 44\n4v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n? kT ` T {kq, where k is the number of actions and T is the time\nhorizon. When the set of actions corresponds to whole r0, 1s interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of r\u0398pT 2{3q, which is the same rate one obtains when there is no penalty for movements.\nAs our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of rOpT 2{3q compared to the best fixed price in hindsight, which outperform the previous regret bound of rOpT 3{4q for the problem."}, {"heading": "1 Introduction", "text": "Multi-Armed Bandit (MAB) is a well studied model in computational learning theory and operations research. In MAB a learner repeatedly selects actions and observes their rewards. The goal of the learner is to minimize the regret, which is the difference between her loss and the loss of the best action in hindsight. This simple model already abstracts beautifully the exploration-exploitation tradeoff, and allows for a systematic study of this important issue in decision making. The basic results for MAB show that even when an adversary selects the sequence of losses, the learner can guarantee a regret of \u0398p ? kT q, where k is the number of actions and T is the number of time steps (5, 3; see also 13). The simplicity of the MAB comes at a price. Essentially, the system is stateless, and previous actions have no influence on the losses assigned to actions in the future. A more involved model of sequential decision making is Markov Decision Processes (MDPs) where the environment is modeled by a finite set of states, and actions are not only associated with losses but also with stochastic transitions between states. Unfortunately, for the adversarial setting there are mostly hardness results even in limited cases [1].\nIntroducing switching costs is a step of incorporating dependencies in the learner\u2019s action selection. The unit switching cost has a unit cost per each changing of actions. In such a setting a tight bound of r\u0398pk1{3T 2{3q is known [18]. Our main goal is to extend this basic model to the case of MAB with movement costs, where the cost associated with switching between arms is given by a metric that determines the distance between any pair of arms. Such a model already introduces a very interesting dependency in the action selection process for the learner. Specifically, we study a metric between actions which is modeled by a complete binary tree, where the distance between two actions is proportional to the number of nodes in the subtree of their least common ancestor. This abstracts the case where the arms are associated with k points on the real line and the switching cost between arms is the absolute difference between the corresponding points (actually, the tree metric only upper bounds distances on a line, but this upper bound is sufficient for our applications). Note that we do not assume that pairs of actions with low movement cost have similar losses: our model retains the full generality of the loss functions, and only imposes a metric structure on the cost of movement between arms.\nOur main result is an efficient MAB algorithm, called the Slowly Moving Bandit (SMB) algorithm, that guarantees expected regret of at most rOp ? kT `T {kq. As we elaborate later, this result implies that for k \u010f T 1{3 we can achieve an optimal regret r\u0398pT 2{3q, and for k \u011b T 1{3 we obtain an optimal regret rate of r\u0398p ? kT q. It is worth discussing the implication of our bound. The bound of r\u0398pT 2{3q for k \u010f T 1{3 is tight due to the lower bound of Dekel et al. [18], which applies already for k \u201c 2 actions. The bound of r\u0398p ? kT q for k \u011b T 1{3 is tight due to the classic lower bound for MAB even without movement costs [5]. Surprising, for a large action set (i.e., k \u011b T 1{3) we lose nothing in the regret by introducing movement costs to the problem! Another surprising consequence of our bound is that there is no loss in the regret by increasing the number of actions from k \u201c 2 to k \u201c \u0398pT 1{3q when movement costs are present.\nThe main application of our SMB algorithm is for adaptive pricing with patient buyers [20]. In this adaptive pricing problem, we have a seller which would like to maximize his revenue. He is faced with a stream of patient buyers. Each buyer has a private value and a window of time in which she would like to purchase the item. The buyer buys at the lowest price in its window, in case it is below its value. (The seller publishes sufficient prices into the future, such that the buyer can observe all the relevant prices.) The adaptive price setting is related to the MAB problem with movement costs in the following way. The prices are continuous (say, r0, 1s) and the reward is the revenue gain by the seller. The rewards are given by a one\u2013sided Lipschitz function (specifically, we receive\nthe reward whenever we post a price which is at most the private value, and zero otherwise). This allows us to apply our bandit algorithm via discretization of the continuous space. The challenge, though, remains to control the cost the seller pays which stems from the buyer\u2019s patience.\nThe seller benchmark is the best single price. Using a single price implies that the buyers either buy immediately, or never buy. The movement cost models the loss due to having the buyer patient, which can be thought as the difference between the price of the item when the buyer arrives and the price at which it buys. (Note that there might be a gain, since it might be that when the buyer arrives the price is too high, but later lower prices make him buy. We ignore this effect for now.) Our main result is that the seller can use our SMB algorithm and guarantee a regret of at most rOpT 2{3q, using T 1{3 equally-spaced prices. This is in contrast to a regret of rOpT 3{4q which is achieved by applying a standard switching cost technique together with a discretization argument [20].\nIt is interesting to observe qualitatively how our algorithm performs. It is much more likely to make small changes than large ones; roughly speaking, the probability of a change drops exponentially in the magnitude of the change. Conceptually, this is a highly desirable property of a pricing algorithm, and arguably, of any regret minimization algorithm: we would like to slightly perturb the prices over time without a sever impact on the buyers, and only rarely make very large changes in the pricing.\nFinally, another application of our algorithm is for the case that we have continuous actions on an interval, and the losses of the actions are Lipschitz. Our algorithm can handle movement cost which are also Lipschitz on the interval. (We stress that in our application the losses are deterministic and not stochastic.)"}, {"heading": "1.1 Related Work", "text": "With a uniform unit switching cost (i.e., when switching between any two actions has a unit cost), it is known that there is a tight r\u2126pk1{3T 2{3q lower bound for the MAB problem [18], which is in contrast to the Op ? kT q regret upper bound without switching costs.\nClassical MAB algorithms such as Exp3 [5] guarantee a regret of rOp ? kT q without movement costs. However, they are not guaranteed to move slowly between actions, and in fact, it is known that Exp3 might make r\u2126pT q switches between actions in the worst case (see 18), which makes it inappropriate to directly handle movement costs.\nOur adaptive pricing application follows the model of Feldman et al. [20]. There, for a finite set of k prices show a matching bound of r\u0398pT 2{3q on the regret. For continuous prices they remark that their upper bound can be used to derive an rOpT 3{4q regret bound. Our SMB algorithm improves this regret bound to rOpT 2{3q. There is a slight difference in the exact feedback model between [20] and here: in both models when a buyer arrives, the sell time is uniquely determined; however, in [20] the seller observes the purchase only at the actual time of the sell, whereas here we assume the seller observes the sell when the buyer arrives and decides when to purchase. We remark, though, that as discussed in [20] all lower bounds derived there apply to the current feedback model too.\nThere is a vast literature on online pricing (e.g., 6, 8, 7, 9, 11). The main difference of our adaptive pricing model is the patience of our buyers, which correlates between the prices at nearby time steps.\nFor the case of continuous prices and a single seller, when one consider impatient buyers, a simple discretization argument can be used to achieve a regret of rOpT 2{3q, and there exists a similar lower bound of \u2126pT 2{3q [25]. More generally, learning Lipschitz functions on a closed interval has been studied by Kleinberg [24], where an optimal r\u0398pT 2{3q regret bound is shown via discretization. Our results show that even if one adds a movement cost (which is the distance) to the problem, there\nis no change in the regret. There are many works on continuous action MAB [24, 16, 4, 12, 28]. Most of the works relate the change in the payoff to the change in the action in various ways. Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property. We differ from that line of work. Our assumption is about the switching cost (rather than the losses) being related to the distance between the actions.\nThe work of Guha and Munagala [21] discusses a stochastic MAB, in the spirit of the Gittins index, where there is both a switching cost and a play cost, and gives a constant approximation algorithm. We differ from that work both in the model, their model is stochastic and our is adversarial, and in the result, their is a multiplicative approximation and our is a regret.\nApproximating an arbitrary metric using randomized trees (i.e., k-HST) has a long history in the online algorithms literature, starting with the work of Bartal [10]. The main goal is to derive a simpler metric representation (using randomized trees) that will both upper and lower bound the given metric. In this work we need only an upper bound on the metric, and therefore we can use a deterministic complete binary tree."}, {"heading": "2 Setup and Formal Statement of Results", "text": ""}, {"heading": "2.1 Bandits with Movement Costs", "text": "In this section we consider the Multi-Armed Bandit (MAB) problem with movement costs. In this problem, that can be described as a game between an online learner and an adversary continuing for T rounds, where there is a set K \u201c t1, . . . , ku of k \u011b 2 arms (or actions) that the learner can choose from. The set of arms is equipped with a metric \u2206pi, jq P r0, 1s that determines the movement distance between any pair of arms i, j P K.\nFirst, before the game begins, the adversary fixes a sequence \u21131, . . . , \u2113T P r0, 1sk of loss vectors assigning loss values in r0, 1s to the arms.1 Then, on each round t \u201c 1, . . . , T , the learner picks an arm it P K, possibly at random, and suffer the associated loss \u2113tpitq. In addition to incurring this loss, the learner also pays a cost of \u2206pit, it\u00b41q that results from her movement from arm it\u00b41 to arm it. At the end of each round t, the learner receives bandit feedback : she gets to observe the single number \u2113tpitq, and this number only. (The movement cost is common knowledge.)\nThe goal of the learner, over the course of T rounds of the game, is to minimize her expected movement-regret, which is defined as the difference between her (expected) total costs\u2014including both the losses she has incurred as well as her movement costs\u2014and the total costs of the best fixed action in hindsight (that incur no movement costs, since it is the same action in all time steps); namely, the movement regret with respect to a sequence \u21131:T of loss vectors and the metric \u2206 equals\nRegretMCp\u21131:T ,\u2206q \u201c E \u00ab T\u00ff\nt\u201c1\n\u2113tpitq ` T\u00ff\nt\u201c2\n\u2206pit, it\u00b41q ff \u00b4 min\ni\u2039PK\nT\u00ff\nt\u201c1\n\u2113tpi\u2039q .\nHere, the expectation is taken with respect to the player\u2019s randomization in choosing the actions i1, . . . , iT .\nMAB with a tree metric. Our focus in this paper is on a metric induced over the actions by a complete binary tree T with k leaves. We consider the MAB setting where each action i is associated with a leaf of the tree T . (For simplicity, we assume that k is a power of two.)\n1Throughout, we assume that the adversary is oblivious, namely, that it cannot react to the learner\u2019s actions.\nWe number the levels of the tree T from the leaves to the root. Let levelpvq be the level of node v in T , where the level of the leaves is 0. Given two leaves i and j, let lcapi, jq be their least common ancestor in T . Then, given actions i and j let dT pi, jq be the level of their least common ancestor in T , i.e., dT pi, jq \u201c levelplcapi, jqq. The movement cost between i and j is then\n\u2206T pi, jq \u201c 1k2 dT pi,jq P r0, 1s . (1)\nOur first main result bounds that movement cost with respect to the given metric:\nTheorem 1. There exists an algorithm (see Algorithm 1 in Section 4) that for any sequence of loss functions \u21131, . . . , \u2113T guarantees that\nRegretMCp\u21131:T ,\u2206T q \u201c rO \u02c6?\nkT ` T k\n\u02d9 .\nFor k \u011b T 1{3 the theorem gives an optimal regret bound of rOp ? kT q. For k \u010f T 1{3, we can extend a binary tree with k leaves by turning each leaf into a node whose subtree is a balanced binary tree and we obtain a new tree with at most 2T 1{3 leaves. We then associate with each new leaf as its action the action induced by its parent at the level of original leaves. One can show that the movements between the level of the original actions is then controlled by OpT 2{3q and we can then exploit this construction to achieve a regret bound of rOpT 2{3q. In any movement cost problem with at least two arms of fixed constant distance, a lower bound regret of 2-arm switching cost applies, hence we observe that these rates are optimal for every k \u010f T [18].\nContinuum-armed bandit with movement cost. We can apply Algorithm 1 to the problem of learning Lipschitz functions over the real line with movement regret associated with standard metric over the interval. In this setting we assume an arbitrary sequence of functions f1, . . . , fT : r0, 1s \u00de\u00d1 r0, 1s where each function ft is L-Lipschitz. i.e.,\n|ftpxq \u00b4 ftpyq| \u010f L|x\u00b4 y| @ x, y P r0, 1s .\nLet xt be the action selected by the player at time t. The objective is then to minimize the movement regret, defined:\nRegretMCpf1:T , | \u00a8 |q \u201c E \u00ab T\u00ff\nt\u201c1\nftpxtq ` T\u00ff\nt\u201c1\n|xt \u00b4 xt`1| ff \u00b4 min\nxPr0,1s\nT\u00ff\nt\u201c1\nftpxq .\nOne application of our algorithm is a regret bound for Lipschitz functions:\nTheorem 2. There exists an algorithm (based on Algorithm 1) that for every sequence of LLipschitz loss functions f1, . . . , fT , with L \u011b 1, achieves:\nRegretMCpf1:T , | \u00a8 |q \u201c rO ` L1{3T 2{3 \u02d8 .\nWe emphasize that even without movement costs, there is an r\u2126pT 2{3q lower bound in this setting [24]; hence, the regret bound of Theorem 2 is essentially optimal.\nWe also note that the result, in fact, holds for any metric \u2206 that is L-Lipschitz (for exact statement see Theorem 12)."}, {"heading": "2.2 Adaptive Pricing", "text": "We consider the following model of online learning, with respect to a stream of patient buyers with patience at most \u03c4 . In our setting the seller posts at time t \u201c 1 prices \u03c11, . . . , \u03c1\u03c4`1 for the next \u03c4 days in advance. Then at each time step t the seller posts price for the t` \u03c4 day \u03c1t`\u03c4 and receives as feedback her revenue for day t. The revenue at time t depends on buyer bt and the sequence of prices \u03c1t, \u03c1t`1, . . . , \u03c1t`\u03c4 in the manner described below.\nEach buyer bt, in our setting, is a mapping from a sequence of prices to revenues, parameterized by her value vt and her patience \u03c4t. The buyer proceed by observing prices \u03c1t, . . . , \u03c1t`\u03c4t , and purchases the item at the lowest price among these prices, if it does not exceed her value. Thus the revenue from the buyer at time t is described as follows:\nbtp\u03c1t, . . . , \u03c1t`\u03c4 q \u201c # mint\u03c1t, . . . , \u03c1t`\u03c4tu if mint\u03c1t, . . . , \u03c1t`\u03c4tu \u010f vt, 0 otherwise.\nNote that at time t the buyer decides whether it will purchase and when. Here, we assume that the buyer also gets to order the good at day of arrival (at price and time decided by him according to his patience and private value), thus the seller observes the buyer\u2019s decision at time t, namely the feedback at time t is given by btp\u03c1t, . . . , \u03c1t`\u03c4 q. We note that this feedback model differs from Feldman et al. [20] where the buyer buy at day of purchase. However, we note that both lower and upper bounds derived by Feldman et al. apply to our feedback model as noted there in the discussion.\nOur objective is to construct an algorithm that minimizes the regret which is the difference between revenue obtained by the best fixed price in hindsight and the expected revenue obtained by the seller, given a sequence b1:T of buyers:\nRegretpb1:T q \u201c max \u03c1\u02daPP\nT\u00ff\nt\u201c1\nbtp\u03c1\u02da, . . . , \u03c1\u02daq \u00b4 E \u00ab T\u00ff\nt\u201c1\nbtp\u03c1t, . . . \u03c1t`\u03c4 q ff .\nOur main result with respect to adaptive pricing is as follows:\nTheorem 3. There exists an algorithm (see Algorithm 2 in Section 5) that for any sequence of buyers b1, . . . ,bT with maximum patience \u03c4 achieves the following regret bound:\nRegretpb1:T q \u201c rOp\u03c4 1{3T 2{3q . It is interesting to note that even though a lower bound of \u2126pT 2{3q stems from two different sources we can still achieve a regret rate of rOpT 2{3q. Indeed, Kleinberg and Leighton [25] showed that optimizing over the continuum r0, 1s leads to a lower bound of \u2126pT 2{3q, irrespective of the patience of the buyers. Second, Feldman et al. [20] showed that whenever the seller wishes to optimize between more than two prices, a lower bound of \u2126pT 2{3q holds for patient buyers.\nIn this work we deal with both obstacles together\u2014patient buyers and optimization over the r0, 1s interval\u2014yet the two obstacles can be dealt without leading to a regret bound that is necessarily worse then each obstacle alone.\nOur solution to the adaptive pricing problem is based on employing a MAB with movement costs algorithm that allows small change in the prices. The reason one needs to employ an algorithm with small movement cost stems from the memory of the buyers: roughly speaking, whenever the seller encounters a buyer with patience, the potential revenue of the seller will be the revenue at time t minus any discount price that buyer may encounter on future days. Indeed, for the case of two prices, Feldman et al. [20] constructed a sequence of buyers that reduces the problem to MAB with switching cost: a step in demonstrating a \u2126pT 2{3q regret bound: thus a fluctuation in prices is indeed a cause for a high regret."}, {"heading": "3 Overview of the approach and techniques", "text": "In this section we give an informal overview of the main ideas in the paper and describe the techniques used in our solution. We begin with the main ideas behind our main result: an optimal and efficient algorithm for MAB problems with movement costs. Later we continue with the adaptive pricing problem, and show how it is abstracted as an instance of the MAB problem with movement costs.\nFrom continuum-armed to multi-armed. In our main applications, we consider actions that are associated to points on the interval r0, 1s equipped with the natural metric \u2206px, yq \u201c |x \u00b4 y|. As a preliminary step, we use discretization in order to make the action space finite and capture the setting by the MAB framework. That is, we reduce the problem of minimizing regret over the entire r0, 1s interval to regret minimization over k actions associated with the equally-spaced points K \u201c t 1\nk , 2 k , . . . , 1u. Our challenge is to then to design a regret minimization algorithm over\nA whose cumulative movement cost with respect to the metric r\u2206pi, jq \u201c |i\u00b4 j|{k is bounded. Our approach builds upon the basic techniques underlying the Exp3 algorithm for the basic MAB problem, which we recall here. Exp3maintains over rounds a distribution pt over the k actions and chooses an action it \u201e pt; thereafter, it updates its sampling distribution multiplicatively via pt`1piq 9 ptpiq \u00a8 expp\u00b4\u03b7\u2113\u0304tpiqq, where \u2113\u0304t is an unbiased estimator of true loss vector \u2113t constructed using only the observed feedback \u2113tpitq. Specifically, the estimator used by Exp3 is\n\u2113\u0304tpiq \u201c 1tit \u201c iu ptpiq \u2113tpitq @ i P K .\nA simple computation shows that \u2113\u0304t is indeed an unbiased estimator of \u2113t, namely that Er\u2113\u0304ts \u201c \u2113t, and the crucial bound for Exp3 is then obtained by controlling a variance term of the form Erpt \u00a8 \u2113\u03042t s, and showing that it is of the order rOpkq at all rounds t. This in turn implies the rOp ? kT q bound of Exp3.\nControlling movements with a tree. As a first step in controlling the movement costs of our algorithm, one can think of an easier problem of controlling the number of times the algorithm switches between actions in the left part of the interval, namely in AL \u201c t 1k , . . . , 12u, and actions in the right part of the interval, AR \u201c t12 ` 1k , . . . , 1u. Indeed, since each such switch might incur a high movement cost (potentially close to 1), any algorithm for MAB with movement costs must avoid making such switches too often. In principle, a solution to this simpler problem can be then lifted to a solution to the actual movement costs problem by applying it recursively to each side of the interval.\nThe thought experiment above motivates our tree-based metric: this metric assigns a fixed cost of 1 to any movement between the left and right parts of the interval\u2014that correspond to the topmost left and right subtrees\u2014and recursively, a cost of 2d{k for any movement between subtrees in level d of the tree. The tree metric is always an upper bound on the natural metric on the interval, namely r\u2206pi, jq \u010f 1\nk 2dT pi,jq \u201c r\u2206T pi, jq, so that controlling movement costs with respect to\nr\u2206T suffices for controlling movement costs with respect to the natural distance on r0, 1s. While this upper bound might occasionally be very loose,2 the tree-metric effectively captures the difficulties of the original movement costs problem with the natural metric over r0, 1s.\nHence, we can henceforth focus on constructing an algorithm with low movement costs with respect to a tree-based metric over a full binary tree. To accomplish this, we will regulate the\n2For example, the distance between 1 2 \u00b4 1 k and 1 2 ` 1 k according to the metric \u2206T is 1.\nprobability of switching the ancestral node. Namely, if we denote by Adpiq the subtree at level d of the tree containing action i, our goal is to design an algorithm that switches between actions i and j such that Adpiq \u2030 Adpjq with probability at most 2\u00b4d. This would ensure that the expected contribution of level d in the tree to the movement cost of the algorithm is Op1{kq per round. Indeed, switching between subtrees at level d (while not making a switch at higher levels) results with a movement cost of roughly 2d{k. Overall, the contribution of all layers in the tree to the total movement cost would then be OppT {kq log kq, as required.\nLazy sampling. Our challenge now is to construct an algorithm that switches infrequently between subtrees at higher levels of the tree. However, recall that typical bandit algorithms choose their actions i1, . . . , iT at random from sampling distributions p1, . . . , pT maintained throughout the evolution of game. In order to guarantee that consecutive actions it and it\u00b41 will belong to the same subtree with high probability, the algorithm would have to sample ii in a way which is highly correlated with the preceding action it\u00b41.\nSuppose that the marginals of the subtrees at some level d does not change between the distributions pt\u00b41 and pt; namely, that the cumulative probability assigned to the leaves of each such subtree by both pt\u00b41 and pt is the same. In this case, we argue that we can sample our new action it at time t, based on the preceding action it\u00b41, from the conditional distribution ptp\u00a8 | Adpit\u00b41qq. In other words, if we think of sampling an action i from pt as sampling a path in the tree leading to the leaf associated with i, then for determining it on round t we copy the top d edges from the path at time t \u00b4 1, and only sample the remaining bottom edges (those contained in the subtree Adpit\u00b41q) according to the new distribution pt. Intuitively, this can be justified because the distribution of the top d edges in the path leading to it is the same as that of the top d edges in the path leading to it\u00b41, so we may as well keep the random bits associated with them and only resample bits associated with the remaining edges from fresh.\nThe lazy sampling scheme sketched above raises a major difficulty in the analysis: since it is sampled from a conditional of pt that might be very different from pt itself, it is no longer clear that it is distributed according to the \u201ccorrect\u201d distribution. In other words, conditioned on pt (which intuitively is a summary of the past), the random variable it is certainly not distributed according to pt. Nevertheless, our analysis demonstrates a crucial property of the distributions pt maintained the sampling scheme, which is sufficient for the regret analysis: we show that for all subtrees A at all levels of the tree, it holds that\nE \u201e 1ti P Au ptpAq  \u201c 1 .\nThat is, even though it is sampled indirectly from pt, it is still distributed according to pt in a certain sense.\nRebalancing the marginals. The lazy sampling we described above reduced the problem of controlling the frequency of movements in the actions i1, . . . , iT , to controlling the frequency in which the marginal distribution of p1, . . . , pT over subtrees is updated by our algorithm. Next, we describe how the latter is accomplished (where the frequency of update is exponentially-decreasing with the level of the subtree). To illustrate the technique, let us consider an easier problem: instead of demanding infrequent updates for subtrees in all levels, we shall only attempt to rebalance the marginals at the topmost level, with the goal of making them being updated with probability at most 2\u00b4D \u201c 1{k. We will demonstrate how the estimator r\u2113t can be modified in a way that induces such infrequent updates at the top level. Denote the left subtree at the top level by AL (containing actions 1\nk , . . . , 1 2 ) and the right topmost subtree by AR (containing actions 1 2 ` 1 k , . . . , 1). First, we\nchoose\n\u03c3t \u201c # 1\u00b4 1 \u03b4 with probability \u03b4;\n1 with probability 1\u00b4 \u03b4.\nThen, for A P tAL, ARu we set\nr\u2113tpiq \u201c \u2113\u0304tpiq \u00b4 \u03c3t\n\u03b7 log\n\u02dc \u00ff\njPA\nptpjq ptpAq e\u00b4\u03b7\u2113\u0304tpjq\n\u00b8 @ i P A .\nHere, \u2113\u0304t is the basic Exp3 estimator discussed earlier. In terms of estimation, r\u2113t is still an unbiased estimator of the true vector \u2113t: since Er\u03c3ts \u201c 0 it follows that Err\u2113ts \u201c \u2113t. However, the added term has a balancing effect at the top level of the tree: a simple computation reveals that if \u03c3t \u201c 1 (which occurs with high probability), the multiplicative update of the algorithm applied on the vector r\u2113t ensures that ptpALq \u201c pt`1pALq and ptpARq \u201c pt`1pARq. In other words, with probability 1\u00b4 \u03b4, the cumulative (i.e., marginal) probability of both subtrees at the top level is remained fixed between rounds t and t` 1.\nThe balancing effect we achieved comes at a price: for small values of \u03b4 the magnitude of r\u2113t becomes large, as it might be the case that \u03c3t \u00ab \u00b41{\u03b4. Nevertheless, it is not hard to show that the variance term Erpt \u00a8 r\u21132t s is bounded by Opk ` 1{\u03b4q. In particular, for \u03b4 \u201c 1{k we retain a variance bound of Opkq, while changing the marginals of the two top subtrees with probability no larger than 1{k. As a result, by sampling accordingly from the slowly-changing distributions pt we can ensure that the movements at the top level contribute at most OpT {kq to the total movement cost of the algorithm.\nEvidently, the estimator described above only remedies the problem at the top level, and the movement costs at lower levels of the tree might still be very large (effectively, within each subtree the algorithm does nothing but simulating Exp3 on the leaves). Still, using a similar yet more involved technique we can induce a balancing effect at all levels simultaneously and ensure that the marginal probabilities of the subtrees at level d are modified by the algorithm with probability at most 2\u00b4d. The construction adds a balancing term corresponding to each level of the tree in a recursive manner that takes into account the balancing terms at lower levels.\nFrom adaptive pricing to bandits. We now discuss how to reduce adaptive pricing with patient buyers to a MAB problem with movement costs. We employ a reduction similar to the one used by [25]; however, the patience of the buyers introduce some difficulties, as we discuss below. For now, we ignore the buyers\u2019 patience and give the idea of the reduction in the simplest case.\nIntuitively, in order to adaptively pick prices from the interval r0, 1s so as to minimize regret with respect to the best fixed price in hindsight, we could directly apply a standard MAB algorithm, e.g., Exp3, over a discretization A \u201c t 1\nk , 2 k , . . . , 1u of the interval, treating each of the k prices as an\narm that generates a reward whenever it is pulled. Furthermore, since the buyers\u2019 valuations are not disclosed after purchase, the feedback observed by the seller is very limited and nicely captured by the MAB abstraction. Since the buyers\u2019 valuations are one-sided Lipschitz, the best price in A will lose at most OpT {kq in total revenue as compared to the best fixed price in the entire r0, 1s interval. Thus, provided an algorithm that achieves rOp ? kT q expected regret with respect to the best price in A, we could pick k \u201c \u0398pT 1{3q and obtain the optimal rOpT 2{3q regret for the pricing problem.\nPatient buyers and movement costs. A main complication in the above MAB approach arises from the buyers\u2019 patience: the revenue extracted from a single buyer is determined not only by the price posted by the seller on the day of the buyer\u2019s arrival, but also by prices posted on the subsequent days subject to the buyer\u2019s patience. As a result, if the seller change prices abruptly on consecutive days, a strategic buyer\u2014that purchases in the minimal price, if at all\u2014could make use of this fact to gain the item at a lower price, which lowers the revenue of the seller. Roughly speaking, the latter additional cost to the seller is controlled by the absolute difference between the prices she posted at consecutive days. Thus, the pricing problem with patient buyers can be reduced to a MAB problem with movement costs, where the online player suffers an additional movement cost each time she changes actions, and the movement cost is determined by the metric (absolute value distance) between the respective actions.\nThe reduction sketched above is made precise in Section 5, where we also address an additional difficulty stemming from the adaptivity of the feedback signal observed by the seller: the latter is contaminated by the effect of prices posted at earlier rounds on the buyers, and has to be treated carefully."}, {"heading": "4 The Slowly Moving Bandit Algorithm", "text": "In this section we present the Slowly Moving Bandit (SMB) algorithm: our optimal algorithm for the Multi-armed bandit problem with movement costs.\nIn order to present the algorithm we require few additional notations. Recall that in our setting, we consider a complete binary tree of depth D \u201c log2 k whose leaves are identified with the actions 1, . . . , k (in this order). For any level 0 \u010f d \u010f D and arm i P K, let Adpiq be the set of leaves that share a common ancestor with i at level d (where level d \u201c 0 are the singletons). We denote by Ad the collection of all k{2d subsets of leaves:\nAd \u201c ! t1, . . . , 2du, t2d ` 1, . . . , 2 \u00a8 2du, . . . , tk \u00b4 2d ` 1, . . . , ku ) @ 0 \u010f d \u010f D .\nThe SMB algorithm is presented in Algorithm 1. The algorithm is based on the multiplicative update method, and in that sense is reminiscent of the Exp3 algorithm [5]. Similarly to Exp3, the algorithm computes at each round t an estimator r\u2113t to the true, unrevealed loss vector \u2113t using the single loss value \u2113tpitq observed on that round.\nAs discussed in Section 3, in addition to being an (almost) unbiased estimate for the true loss vector, the estimator r\u2113t used by SMB has the additional property of inducing slowly-changing sampling distributions pt, that allow for sampling the actions it in a way that the overall movement cost is controlled. This is achieved by choosing at random, at each round t, a level dt of the tree to be rebalanced by the algorithm using the balancing vectors \u2113\u0304t,d. For reasons that will become apparent later on, the level dt is determined by choosing a random sign \u03c3t,d for each level d in the tree and identifying the bottommost level with a negative sign. Then, as we show in the analysis, the terms \u2113\u0304t,d defined using the signs \u03c3t,d have a balancing effect at levels d \u011b dt.\nA major difficulty inherent to our approach, also common to many bandit optimization settings (e.g., 17, 2, 14), is the fact that the estimated losses r\u2113tpiq might receive negative values that are very high in absolute value. Indeed, the balancing term \u2113\u0304t,d corresponding to level d is roughly as large as 2d{ptpitq, and might appear in negative sign in r\u2113t. Algorithm 1 resolves this issue by zeroing-out the estimator r\u2113t whenever it chooses an action whose probability is too small, which ensures that the \u2113\u0304t,d terms never become too large. We remark that the standard approaches used to resolve such issues (the simplest of which is mixing the distribution pt with the uniform distribution over\nthe k actions) fail in our case, as they break the rebalancing effect which is tailored to the specific multiplicative update of the algorithm.\nInitialize p1 \u201c u, d0 \u201c D and i0 \u201e p1; for t \u201c 1, . . . , T : (1) Choose action it \u201e ptp \u00a8 | Adt\u00b41pit\u00b41qq, observe loss \u2113tpitq (2) Choose \u03c3t,0, . . . , \u03c3t,D\u00b41 P t\u00b41,`1u uniformly at random;\nlet dt \u201c mint0 \u010f d \u010f D : \u03c3t,d \u0103 0u where \u03c3t,D \u201c \u00b41 (3) Compute vectors \u2113\u0304t,0, . . . , \u2113\u0304t,D\u00b41 recursively via\n\u2113\u0304t,0piq \u201c 1tit \u201c iu ptpiq \u2113tpitq ,\nand for all d \u011b 1:\n\u2113\u0304t,dpiq \u201c \u00b4 1\n\u03b7 log\n\u00a8 \u02dd \u00ff\njPAdpiq\nptpjq ptpAdpiqq e\u00b4\u03b7p1`\u03c3t,d\u00b41q\u2113\u0304t,d\u00b41pjq\n\u02db \u201a\n(4) Define Bt \u201c tptpAdpitqq \u0103 2d\u03b7 for some 0 \u010f d \u0103 Du and set\nr\u2113t \u201c #\n0 if it P Bt; \u2113\u0304t,0 ` \u0159D\u00b41 d\u201c0 \u03c3t,d\u2113\u0304t,d otherwise\n(5) Update:\npt`1piq \u201c ptpiq e\u00b4\u03b7r\u2113tpiq\u0159k\nj\u201c1 ptpjq e\u00b4\u03b7 r\u2113tpjq\n@ i P K\nAlgorithm 1: The SMB algorithm.\nThe following theorem is the main result of this section. Theorem 1 is an immediate corollary.\nTheorem 4. For any sequence of loss functions \u21131, . . . , \u2113T , The SMB algorithm (Algorithm 1) guarantees that\nRegretp\u21131:tq \u201c O \u02c6 log k\n\u03b7 ` \u03b7Tk log k\n\u02d9 .\nIn particular, by setting \u03b7 \u201c 1{ ? kT the expected regret of the algorithm is bounded by Op ? Tk log kq. Furthermore, for the metric \u2206T (see Eq. (1)), the expected total movement cost of the algorithm is Er\u0159Tt\u201c2 \u2206T pit, it\u00b41qs \u201c OppT {kq log kq.\nThe rest of the section focuses on proving Theorem 4. We begin by stating a useful technical bound that we use throughout our analysis to control the magnitude of the balancing vectors \u2113\u0304t,d. For a proof of the lemma, see Section 4.5 below.\nLemma 5. For all t and 0 \u010f d \u0103 D the following holds almost surely:\n0 \u010f \u2113\u0304t,dpiq \u010f 1tit P Adpiqu ptpAdpiqq\nd\u00b41\u017a\nh\u201c0\np1` \u03c3t,hq @ i P K . (2)\nIn particular, if \u03c3t,h \u201c \u00b41 then \u2113\u0304t,d \u201c 0 for all d \u0105 h.\nOne useful implication of the lemma is that, since \u2113\u0304t,d \u201c 0 for all d \u0105 dt, we can express our estimator r\u2113t in the following equivalent form:\nr\u2113t \u201c \u2113\u0304t,0 \u00b4 \u2113\u0304t,dt ` dt\u00b41\u00ff\nh\u201c0\n\u2113\u0304t,h . (3)"}, {"heading": "4.1 Rebalancing the marginals", "text": "Our first step is to show that the marginals of the distributions pt over subtrees of actions are not modified by the algorithm with high probability, as a result of adding the balancing vectors \u2113\u0304t,d.\nLemma 6. For all d \u011b dt we have that pt`1pAq \u201c ptpAq for all A P Ad.\nFor the proof, we require the next technical result about the balancing vectors \u2113\u0304t,d computed by the algorithm.\nLemma 7. If \u03c3t,0 \u201c . . . \u201c \u03c3t,d\u00b41 \u201c 1 then: \u00ff\niPA\nptpiqe\u00b4\u03b7\u2113\u0304t,dpiq \u201c \u00ff\niPA\nptpiqe\u00b4\u03b7 r\u2113t,dpiq @ A P Ad ,\nwhere r\u2113t,d \u201c \u2113\u0304t,0 ` \u0159d\u00b41 h\u201c0 \u2113\u0304t,h.\nProof. The proof proceeds by induction on d. For the base case d \u201c 0, the claim follows trivially as \u2113\u0304t,0 \u201c r\u2113t,0. Next, we assume the claim is true for some value of d \u011b 0 and prove it for d ` 1. Pick any A P Ad`1 and write A \u201c A1 Y A2 where A1, A2 are disjoint sets from Ad. Notice that the vector \u2113\u0304t,d is uniform over A1 and A2, namely \u2113\u0304t,dpiq \u201c cA1 for all i P A1 for some cA1 \u011b 0, and similarly \u2113\u0304t,dpiq \u201c cA2 for all i P A2 for some cA2 \u011b 0. Hence, we have\n\u00ff\niPA\nptpiqe\u00b4\u03b7 r\u2113t,d`1piq \u201c\n\u00ff\niPA\nptpiqe\u00b4\u03b7 r\u2113t,dpiqe\u00b4\u03b7\u2113\u0304t,dpiq\n\u201c e\u00b4\u03b7cA1 \u00ff\niPA1\nptpiqe\u00b4\u03b7 r\u2113t,dpiq ` e\u00b4\u03b7cA2\n\u00ff\niPA2\nptpiqe\u00b4\u03b7 r\u2113t,dpiq\n\u201c e\u00b4\u03b7cA1 \u00ff\niPA1\nptpiqe\u00b4\u03b7\u2113\u0304t,dpiq ` e\u00b4\u03b7cA2 \u00ff\niPA2\nptpiqe\u00b4\u03b7\u2113\u0304t,dpiq\n\u201c \u00ff\niPA\nptpiqe\u00b4\u03b7\u2113\u0304t,dpiqe\u00b4\u03b7\u2113\u0304t,dpiq\n\u201c \u00ff\niPA\nptpiqe\u00b42\u03b7\u2113\u0304t,dpiq ,\nwhere the third equality uses the induction hypothesis. On the other hand, by the recursive definition of \u2113\u0304t,d`1 and the fact that \u2113\u0304t,d`1 is uniform over A, we have\n\u00ff\niPA\nptpiqe\u00b4\u03b7\u2113\u0304t,d`1piq \u201c ptpAq \u00ff\niPA\nptpiq ptpAq e\u00b4\u03b7p1`\u03c3t,dq\u2113\u0304t,dpiq \u201c \u00ff\niPA\nptpiqe\u00b42\u03b7\u2113\u0304t,dpiq .\nCombining both observations, we obtain \u00ff\niPA\nptpiqe\u00b4\u03b7\u2113\u0304t,d`1piq \u201c \u00ff\niPA\nptpiqe\u00b4\u03b7 r\u2113t,d`1piq\nwhich concludes the inductive argument.\nWe can now prove Lemma 6.\nProof of Lemma 6. It is enough to prove that pt`1pAq \u201c ptpAq for all A P Adt , as each set in Ad for d \u0105 dt is a disjoint union of sets from Adt .\nObserve that if it P Bt (see Algorithm 1 for the definition of Bt) then r\u2113t \u201c 0 and the claim is certainly true as pt`1 \u201c pt in this case. Thus, we henceforth assume that it R Bt, in which case r\u2113t \u201c r\u2113t,dd \u00b4 \u2113\u0304t,dd where r\u2113t,dt \u201c \u2113\u0304t,0 ` \u0159dt\u00b41 h\u201c0 \u2113\u0304t,h (recall Eq. (3)). Now, pick any A P Adt and j P A. Since \u2113\u0304t,dtpiq \u201c cA for all i P A for some cA \u011b 0, and using Lemma 7 we obtain\ne\u00b4\u03b7cA \u201c \u00ff\niPA\nptpiq ptpAq e\u00b4\u03b7\u2113\u0304t,dt piq \u201c \u00ff\niPA\nptpiq ptpAq e\u00b4\u03b7 r\u2113t,dtpiq . (4)\nOn the other hand, from r\u2113t \u201c r\u2113t,dt \u00b4 \u2113\u0304t,dt it follows that e\u00b4\u03b7 r\u2113tpiq \u201c e\u00b4\u03b7r\u2113t,dt piq{e\u00b4\u03b7cA for all i P A, and by Eq. (4) we have\n\u00ff\niPA\nptpiqe\u00b4\u03b7 r\u2113tpiq \u201c\n\u0159 iPA ptpiqe\u00b4\u03b7 r\u2113t,dtpiq\ne\u00b4\u03b7cA \u201c ptpAq .\nIn words, the multiplicative update does not change the probabilities of the sets in Adt , hence pt`1pAq \u201c ptpAq for all A P Adt as required."}, {"heading": "4.2 Lazy sampling", "text": "Our next step is to show that the sampling scheme employed by Algorithm 1 is valid and gives rise to low movement costs on expectation. Specifically, we would like to show that in a certain sense, the action it on round t is distributed in expectation according to the distribution pt, even though it is sampled from a conditional of pt in a way that is highly correlated with the preceding action it\u00b41. Furthermore, we will show that the correlations in the sampling scheme are designed in a way that the expected movement between consecutive actions is small. These properties are formalized in the following lemma.\nLemma 8. For all t and 0 \u010f d \u0103 D the following hold:\n\u2022 for all A P Ad we have\nE \u201e 1tit P Au ptpAq  \u201c 1 ; (5)\n\u2022 with probability at least 1\u00b4 2\u00b4pd`1q, we have that Adpitq \u201c Adpit\u00b41q.\nEq. (5) is central to our analysis below, and virtually all of our probabilistic arguments involving the random variables it and pt will be based on this property. We remark that if we were to sample it directly from the distribution specified by pt, then Eq. (5) would have been trivially true. However, the it are sampled from a conditional of pt that might be very different from pt itself; nevertheless, the lemma shows that Eq. (5) still continues to hold under the skewed sampling process.\nLemma 8 also implies the slow-movement property of the algorithm: at the high levels of the tree, where the subtrees are \u201cwide\u201d, the actions it and it\u00b41 are very likely to belong to the same subtree. The probability of switching subtrees increases exponentially with the level in the tree: at the lower levels, where the subtrees are \u201cnarrow\u201d, subtree switches may occur more often as the movement cost incurred by such switches is low.\nProof of Lemma 8. The second statement is true since we pick it`1 \u201e ptpi | Adtpitqq, so that Adpit`1q \u2030 Adpitq can occur only if d \u0103 dt. This happens with probability 2\u00b4pd`1q.\nNext, we show Eq. (5) by induction on t. For t \u201c 1 the statement is true since i1 \u201e p1. For the induction step, condition on dt and fix any d \u011b dt and A P Ad. By Lemma 6 we have that ptpAq \u201c pt`1pAq. Also it P A if and only if it`1 P A, since d \u011b dt implies that it P A if and only if Adtpitq \u010e A and Adtpit`1q \u201c Adtpitq. Hence, we have\nE \u201e 1tit`1 P Au pt`1pAq \u02c7\u030c \u02c7\u030c dt  \u201c E \u201e 1tit P Au ptpAq \u02c7\u030c \u02c7\u030c dt  \u201c E \u201e 1tit P Au ptpAq  \u201c 1 , (6)\nwhere the last equality holds true since dt depends solely on \u03c3t,0, . . . , \u03c3t,D\u00b41 which are independent of it and pt (note that this equality then holds for any set A, regardless of the fact that A P Ad).\nNext, we consider any d \u0103 dt and A P Ad. Let A1 P Adt be the subtree such that A \u010e A1, and recall that it`1 \u201e pt`1pi | Adtpitqq. Hence,\nE \u201e 1tit`1 P Au pt`1pAq \u02c7\u030c \u02c7\u030c it P A1, pt`1, dt  \u201c E \u201e 1tit`1 P Au pt`1pA | A1qpt`1pA1q \u02c7\u030c \u02c7\u030c it P A1, pt`1, dt  \u201c 1 pt`1pA1q . (7)\nSince it P A1 implies that it`1 P A1, we have\nE \u201e 1tit`1 P Au pt`1pAq \u02c7\u030c \u02c7\u030c dt, pt`1  \u201c E \u201e 1tit`1 P A1u \u00a8 E \u201e 1tit`1 P Au pt`1pAq \u02c7\u030c \u02c7\u030c it P A1, pt`1, dt  \u02c7\u030c \u02c7\u030c dt, pt`1  . (8)\nTaking Eqs. (7) and (8) together and taking the expectation over pt`1, we obtain that for every d \u0103 dt:\nE \u201e 1tit`1 P Au pt`1pAq \u02c7\u030c \u02c7\u030c dt  \u201c E \u201e 1tit`1 P A1u pt`1pA1q \u02c7\u030c \u02c7\u030c dt  \u201c 1 ,\nwhere last equality follows from Eq. (6) as A1 P Adt . To conclude, we showed that for all d we have:\nE \u201e 1tit`1 P Au pt`1pAq \u02c7\u030c \u02c7\u030c dt  \u201c 1 .\nTaking the expectation over dt, we obtain the desired result."}, {"heading": "4.3 Bounding the bias and variance", "text": "Next, we turn to bound the variance of the loss estimates r\u2113t and the bias of their expectations from the true loss vectors. These bounds would become useful for controlling the expected regret of the underlying multiplicative updates scheme.\nWe begin with analyzing the bias of our estimator. The following lemma shows that our estimates are \u201coptimistic\u201d, in the sense that they always bound the true losses from below, yet they do not overly underestimate the losses incurred by the algorithm. The proof is somewhat involved, as a result of the \u201cbad events\u201d Bt under which the estimated loss vectors r\u2113t are being zeroed-out, thereby introducing biases into the estimation.\nLemma 9. For all t, we have Err\u2113tpiqs \u010f \u2113tpiq and Er\u2113tpitqs \u010f Erpt \u00a8 r\u2113ts ` \u03b7k log2 k.\nProof. Observe that, by Eq. (5) of Lemma 8,\nEr\u2113\u0304t,0piqs \u201c \u2113tpiqE \u201e 1tit \u201c iu ptpiq  \u201c \u2113tpiq .\nWe now prove that Err\u2113t,0piqs \u010f Er\u2113\u0304t,0piqs for all i, which would imply the first claim. Denote Bt \u201c i | ptpAdpiqq \u0103 2d\u03b7 for some 0 \u010f d \u0103 D ( . Then, by construction we have Err\u2113tpiq | it P Bts \u201c 0 \u010f Er\u2113\u0304t,0piq | it P Bts. Also, since Er\u03c3t,ds \u201c 0 and \u03c3t,d is independent of it and \u2113\u0304t,d (the latter only depends on \u03c3t,0, . . . , \u03c3t,d\u00b41), we have\nErr\u2113t | it R Bts \u201c Er\u2113\u0304t,0 | it R Bts ` D\u00b41\u00ff\nd\u201c0\nEr\u03c3t,dsEr\u2113\u0304t,d | it R Bts \u201c Er\u2113\u0304t,0 | it R Bts . (9)\nTogether, we obtain Err\u2113t,0piqs \u010f Er\u2113\u0304t,0piqs as required. Next, to bound Er\u2113tpitqs observe that Erpt \u00a8 r\u2113t | it P Bts \u201c 0 and, similarly to Eq. (9),\nErpt \u00a8 r\u2113t | it R Bts \u201c Erpt \u00a8 \u2113\u0304t,0 | it R Bts \u201c Er\u2113tpitq | it R Bts .\nDenote \u03b2t \u201c P rit P Bts. Then\nEr\u2113tpitqs \u201c \u03b2tEr\u2113tpitq | it P Bts ` p1\u00b4 \u03b2tqEr\u2113tpitq | it R Bts \u010f \u03b2t ` p1\u00b4 \u03b2tqErpt \u00a8 r\u2113t | it R Bts \u201c \u03b2t ` Erpt \u00a8 r\u2113ts ,\nwhere for the inequality we used the fact that \u2113tpitq \u010f 1. To complete the proof, we have to show that \u03b2t \u010f \u03b7k log2 k. To this end, write\nPrit P Bts \u010f D\u00b41\u00ff\nd\u201c0\nPrptpAdpitqq \u0103 2d\u03b7s .\nUsing Eq. (5) to write\nE\n\u201e 1\nptpAdpitqq\n \u201c k\u00ff\ni\u201c1\n1\n|Adpiq| E \u201e 1tit P Adpiqu ptpAdpiqq  \u201c k\u00ff\ni\u201c1\n1\n|Adpiq| \u201c |Ad| \u201c\nk\n2d\ntogether with Markov\u2019s inequality, we obtain\nP \u201d ptpAdpitqq \u0103 2d\u03b7 \u0131 \u201c P\n\u201e 1\nptpAdpitqq \u0105 1 2d\u03b7\n \u010f k\n2d \u00a8 2d\u03b7 \u201c k\u03b7 .\nWe conclude that \u03b2t \u201c Prit P Bts \u010f \u03b7k log2 k, as required.\nOur next step is to bound the relevant variance term of the estimator r\u2113t.\nLemma 10. For all t, we have Erpt \u00a8 r\u21132t s \u010f 2k log2 k.\nProof. Observe that\nr\u21132t piq \u010f \u02dc \u2113\u0304t,0piq ` D\u00b41\u00ff\nd\u201c0\n\u03c3t,d\u2113\u0304t,dpiq \u00b82 .\nSince Er\u03c3t,ds \u201c 0 and Er\u03c3t,d\u03c3t,d1s \u201c 0 for all d \u2030 d1, we have for all i that\nErr\u21132t piqs \u201c Err\u21132t,0piqs ` D\u00b41\u00ff\nd\u201c0\nEr\u2113\u03042t,dpiqs \u010f 2 D\u00b41\u00ff\nd\u201c0\nEr\u2113\u03042t,dpiqs . (10)\nOn the other hand, for all d we have by Lemma 5 that\npt \u00a8 \u2113\u03042t,d \u010f \u0159k\ni\u201c1 ptpiq1tit P Adpiqu ptpAdpitqq2\nd\u00b41\u017a\nh\u201c0\np1` \u03c3t,hq2\n\u201c 1 ptpAdpitqq\nd\u00b41\u017a\nh\u201c0\np1` \u03c3t,hq2\n\u201c k\u00ff\ni\u201c1\n1 |Adpiq| 1tit P Adpiqu ptpAdpiqq\nd\u00b41\u017a\nh\u201c0\np1` \u03c3t,hq2 .\nSince it is independent of the \u03c3t,h, and recalling Eq. (5), we get\nEtrpt \u00a8 \u2113\u03042t,ds \u010f k\u00ff\ni\u201c1\n1\n|Adpiq| E \u201e 1tit P Adpiqu ptpAdpiqq  d\u00b41\u017a\nh\u201c0\nErp1` \u03c3t,hq2s \u201c k\u00ff\ni\u201c1\n2d\n|Adpiq| \u201c 2d|Ad| \u201c k .\nTogether with Eq. (10), this gives\nErpt \u00a8 r\u21132t s \u010f 2 D\u00b41\u00ff\nd\u201c0\nErpt \u00a8 \u2113\u03042t,ds \u010f 2k log2 k."}, {"heading": "4.4 Concluding the proof", "text": "To conclude the proof and obtain a regret bound, we will use the following well-known second-order regret bound for the multiplicative weights (MW) method, essentially due to [15] (see also [2] for the version given here). For completeness, we give a proof of this bound in Section 4.5 below.\nLemma 11 (Second-order regret bound for MW). Let \u03b7 \u0105 0 and let c1, . . . , cT P Rk be real vectors such that ctpiq \u011b \u00b41{\u03b7 for all t and i. Consider a sequence of probability vectors q1, . . . , qT P \u2206k defined by q1 \u201c p 1k , . . . , 1k q, and for all t \u0105 1:\nqt`1piq \u201c qtpiq e\u00b4\u03b7ctpiq\u0159k\nj\u201c1 qtpjq e\u00b4\u03b7ctpjq @ i P rks .\nThen, for all i\u02da P rks we have that T\u00ff\nt\u201c1\nqt \u00a8 ct \u00b4 T\u00ff\nt\u201c1\nctpi\u02daq \u010f log k\n\u03b7 ` \u03b7\nT\u00ff\nt\u201c1\nqt \u00a8 c2t .\nWe now have all we need in order to prove our main result.\nProof of Theorem 4. First, we bound the expected movement cost. Lemma 8 says that with probability at least 1\u00b4 2\u00b4pd`1q, the actions it and it\u00b41 belong to the same subtree on level d of the tree, which means that \u2206pit, it\u00b41q \u010f 2d{k with the same probability. Hence,\nEr\u2206pit, it\u00b41qs \u010f D\u00b41\u00ff\nd\u201c0\n2d\nk P\n\u201e \u2206pit, it\u00b41q \u0105 2d\nk\n \u010f D\u00b41\u00ff\nd\u201c0\n1 2k \u201c log2 k 2k ,\nand the cumulative movement cost is then OppT {kq log kq. We turn to analyze the cumulative loss of the algorithm. We begin by observing that r\u2113tpiq \u011b \u00b41{\u03b7 for all t and i. To see this, notice that r\u2113t \u201c 0 unless it R Bt, in which case we have, by Lemma 5 and the definition of Bt,\n0 \u010f \u2113\u0304t,dpiq \u010f 2d ptpAdpitqq \u010f 1 \u03b7 @ 0 \u010f d \u0103 D ,\nand since r\u2113t has the form r\u2113t \u201c \u2113\u0304t,0 ` \u0159dt\u00b41\nh\u201c0 \u2113\u0304t,h \u00b4 \u2113\u0304t,dt (recall Eq. (3)), we see that r\u2113tpiq \u011b \u00b41{\u03b7. Hence, we can use second-order bound of Lemma 11 on the vectors r\u2113t to obtain\nT\u00ff\nt\u201c1\npt \u00a8 r\u2113t \u00b4 T\u00ff\nt\u201c1\nr\u2113tpi\u02daq \u010f log k\n\u03b7 ` \u03b7\nT\u00ff\nt\u201c1\npt \u00a8 r\u21132t\nfor any fixed i\u02da P rks. Taking expectations and using Lemmas 9 and 10, we have\nE\n\u00ab T\u00ff\nt\u201c1\n\u2113tpitq ff \u00b4 T\u00ff\nt\u201c1\n\u2113tpi\u02daq \u010f log2 k\n\u03b7 ` 2\u03b7Tk log2 k .\nChoosing \u03b7 \u201c 1{ ? Tk, we get a regret bound of Op ? Tk log kq."}, {"heading": "4.5 Additional technical proofs", "text": "Here we give a proof of our technical lemma bounding the magnitude of the balancing terms \u2113\u0304t,d.\nProof of Lemma 5. We will prove the claim by induction on d. For the base case d \u201c 0, Eq. (2) follows directly from our definitions and the fact that 0 \u010f \u2113tpiq \u010f 1 for all i. Next, we prove that Eq. (2) holds for some d assuming it hold for all d1 \u0103 d. Since p1 ` \u03c3t,d\u00b41q\u2113\u0304t,d\u00b41piq \u011b 0 for all i by the induction hypothesis, the recursive definition of \u2113\u0304t,d implies that\n\u2113\u0304t,dpiq \u011b \u00b4 1\n\u03b7 log\n\u02dc \u00ff\njPAdpiq\nptpjq ptpAdpjqq\n\u00b8 \u201c 0 .\nFurthermore, the definition of \u2113\u0304t,d together with the convexity of \u00b4 log x and Jensen\u2019s inequality give\n\u2113\u0304t,dpiq \u010f p1` \u03c3d\u00b41q \u00ff\njPAdpiq\nptpjq ptpAdpjqq \u2113\u0304t,d\u00b41pjq\n\u010f 1tit P Adpiqu ptpAdpiqq\n\u00ff\njPAd\u00b41piq\nptpjq ptpAd\u00b41pjqq\nd\u00b41\u017a\nh\u201c0\np1` \u03c3t,hq\n\u201c 1tit P Adpiqu ptpAdpiqq\nd\u00b41\u017a\nh\u201c1\np1` \u03c3t,hq ,\nwhere in the second inequality we used the induction hypothesis. This concludes the inductive argument.\nFinally, for completeness, we give a proof of Lemma 11 being central to our regret analysis.\nProof of Lemma 11. The proof follows the standard analysis of exponential weighting schemes: let wtpiq \u201c exp ` \u00b4 \u03b7\u0159t\u00b41s\u201c1 cspiq \u02d8 and let Wt \u201c \u0159 iPV wtpiq. Then qtpiq \u201c wtpiq{Wt and we can write\nWt`1\nWt \u201c\nk\u00ff\ni\u201c1\nwt`1piq Wt\n\u201c k\u00ff\ni\u201c1\nwtpiq exp ` \u00b4\u03b7 ctpiq \u02d8\nWt\n\u201c k\u00ff\ni\u201c1\nqtpiq exp ` \u00b4\u03b7 ctpiq \u02d8\n\u010f k\u00ff\ni\u201c1\nqtpiq ` 1\u00b4 \u03b7ctpiq ` \u03b72ctpiq2 \u02d8\n\u201c 1\u00b4 \u03b7 k\u00ff\ni\u201c1\nqtpiqctpiq ` \u03b72 k\u00ff\ni\u201c1\nqtpiqctpiq2 ,\nwhere the inequality uses the inequality ex \u010f 1` x` x2 valid for x \u010f 1. Taking logarithms, using logp1\u00b4 xq \u010f \u00b4x for all x \u010f 1, and summing over t \u201c 1, . . . , T yields\nlog WT`1\nW1 \u010f\nT\u00ff\nt\u201c1\nk\u00ff\ni\u201c1\n` \u00b4\u03b7 qtpiqctpiq ` \u03b72 qtpiqctpiq2 \u02d8 .\nMoreover, for any fixed action i\u02da, we also have\nlog WT`1 W1 \u011b log wT`1pkq W1 \u201c \u00b4\u03b7\nT\u00ff\nt\u201c1\nctpi\u02daq \u00b4 log k .\nPutting together and rearranging gives the result."}, {"heading": "4.6 Learning Continuum\u2013Arm Bandit with Lipschitz Loss Functions", "text": "In this section we turn to show how to reduce the problem of learning Lipschitz functions to MAB with tree-metric movement costs. Specifically we aim at proving Theorem 2. Specifically we prove the following statement, Theorem 12. Set k \u201c L2{3T 1{3 and \u03b7 \u201c 1{ ? kT . Consider a procedure that receives actions from Algorithm 1 and returns as feedback ftp itk q then for every sequence of L-Lipschitz loss functions f1, . . . , fT and an L-Lipschitz metric \u2206, we have that:\nRegretMCpf1:T ,\u2206q \u201c rO ` L1{3T 2{3 \u02d8 .\nIn particular, the result holds for L \u011b 1 and \u2206pxt, xt`1q \u201c |xt \u00b4 xt`1|.\nProof. First note that for every x\u02da P r0, 1s we can find x \u201c t 1 k , 2 k , . . . , 1u such that ftpxq \u00b4 ftpx\u02daq \u010f L{k \u201c L1{3T\u00b41{3, hence\nT\u00ff\nt\u201c1\n` ftpxq \u00b4 ftpx\u02daq \u02d8 \u201c L1{3T 2{3.\nTherefore if we can show that the regret against every x\u02da P t 1 k , 2 k , . . . 1u is bounded by OpL1{3T 2{3q we obtain that the same regret bound is true for every x P r0, 1s. Next, we apply Algorithm 1 on the a fully balanced tree where we associate with the leaves t1, . . . , ku the actions t 1 k , 2 k . . . , 1u. One can then show that |i\u00b4j| k \u010f \u2206T pi, jq. We then obtain by Theorem 4 that for every x P t 1 k , 2 k . . . , 1u:\nE\n\u00ab T\u00ff\nt\u201c1\nftpxtq ff \u00b4min\nx\nT\u00ff\nt\u201c1\nftpxq \u201c Op\u03b7kT q \u201c rOpL1{3T 2{3q .\nAs to the second term in the regret we obtain that\nE\n\u00ab T\u00ff\nt\u201c1\n\u2206pxt, xt`1q ff \u010f L T\u00ff\nt\u201c1\n|xt \u00b4 xt`1| \u010f E \u00ab L T\u00ff\nt\u201c1\n\u2206T pit, it`1q ff \u201c rO \u02c6 L T\nk\n\u02d9 \u201c rOpL1{3T 2{3q .\nTaken together we obtain that\nE\n\u00ab T\u00ff\nt\u201c1\nftpxtq ` T\u00ff\nt\u201c1\n\u2206pxt, xt`1q ff \u00b4 min\nxPt 1 k ,...,1u\nT\u00ff\nt\u201c1\nftpxq \u201c rOpL1{3T 2{3q ."}, {"heading": "5 Online Pricing with Patient Buyers", "text": "In this section we present our reduction of adaptive pricing with patient buyers to a MAB with movement costs.\nThe reduction is presented in Algorithm 2 and uses our algorithm for MAB with movement costs (Algorithm 1) as a black-box. The algorithm divides the time interval T into \u03c4 blocks and the updates the price on T \u201c T {\u03c4 rounds. At each round t the algorithm publishes a fixed price for the whole block of \u03c4 consecutive days. Then, as feedback, the algorithm receives the mean revenue for those days, which we denote by\nr1t \u201c 1\n\u03c4\nt\u03c4\u00ff\nk\u201cpt\u00b41q\u03c4`1\nbkp\u03c1k, . . . , \u03c1k`\u03c4 q .\nThus, we can consider the algorithm as an online algorithm over T rounds: where at each round t the algorithm announces a fixed action \u03c11t`1 (the price for the next \u03c4 days) and receives at the end of the round as feedback r1t. Note that prices are always announced \u03c4 days in advance, as required.\nThe algorithm draws \u03b21, . . . , \u03b2T unbiased Bernoulli random variables, and this sequence determines the switches in prices and updates. The algorithm posts a new price only on rounds where \u03b2t \u201c 0 and \u03b2t`1 \u201c 1, and invoke the update rule of Algorithm 1 only on rounds where \u03b2t`1 \u201c 0 and \u03b2t`2 \u201c 1. Note that these two events never co-occur, and further the algorithm exploits the feedback only on days prior to a switch, thus guaranteeing that the feedback is always on days when prices are fixed throughout the present and future block.\nAs discussed briefly in Section 3, the main difficulty in reducing the adaptive pricing problem to MAB, which Algorithm 2 overcomes, is in that the feedback function is not only a function of the current posted price (which is in fact the price tomorrow) but also of past prices. For example, for \u03c4 \u201c 1 the revenue at time t is a function of \u03c1t and \u03c1t`1, where only \u03c1t`1 needs be posted at time t. Algorithm 2 overcomes this issue by employing techniques from [19] for handling adaptive feedback. The tools developed there allow regret minimization when feedback is taken only in time steps when the price is fixed for a period of time. Relying on these techniques, we construct an\nParameters: horizon T , and maximal patience \u03c4 Initialize, T \u201c T {p2\u03c4q, k \u201c T 1{3, \u03b7 \u201c 2{ ? Tk Initialize an instance B of SMBpk, \u03b7q Draw i.i.d. unbiased Bernoulli r.v. \u03b20, . . . , \u03b2T Sample i1 \u201e B, set \u03c111 \u201c i1{k Announce prices \u03c11 \u201c \u03c12 \u201c . . . , p\u03c4 \u201c \u03c111 For t \u201c 1, . . . , T (1) If \u03b2t \u201c 0 and \u03b2t`1 \u201c 1, sample it`1 \u201e B; otherwise set it`1 \u201c it (2) Set \u03c11\nT`1 \u201c it`1{k and announce prices: \u03c1t\u03c4`1 \u201c \u00a8 \u00a8 \u00a8 \u201c \u03c1pt`1q\u03c4 \u201c \u03c11t`1\n(3) Collect revenues rpt\u00b41q\u03c4`1, . . . , rt\u03c4 and set\nr1tp\u03c11tq \u201c 1\n\u03c4\nt\u03c4\u00ff\nk\u201cpt\u00b41q\u03c4`1\nrk\n(4) If \u03b2t`1 \u201c 0, \u03b2t`2 \u201c 1, update B with feedback ft \u201c 1\u00b4 r1tp\u03c11tq\nAlgorithm 2: Adaptive pricing with patient buyers.\nalgorithm that produces a sequence of prices with low regret if each buyer bt would observe price \u03c1t. However, in our setting, a buyer may buy at a consecutive time steps; the additional cost we suffer is bounded by the potential cost of switching to lower prices, namely, by the movement cost of the algorithm.\nThe main result of this section, stated earlier in Theorem 3, shows that Algorithm 2 attains a regret bound of Op\u03c41{3T 2{3q against any sequence of buyers with patience at most \u03c4 :\nThe remainder of the section is devoted to proving Theorem 3. We begin by establishing additional notation required for the proof. We will denote the expected revenue from the buyers at each block as follows:\nbtp\u03c11t, \u03c11t`1q \u201c 1\n\u03c4\npt`1q\u03c4\u00ff\nk\u201ct\u03c4`1\nbkp\u03c1k, . . . , \u03c1k`\u03c4tq .\nNote that since the blocks are of size \u03c4 , each buyer can see at most prices that are published on the next block, hence \u03c1k`\u03c4t either equals \u03c1 1 t or \u03c1 1 t`1. In turn, this means that the expected revenue is indeed a function of \u03c11t and \u03c1 1 t`1 alone.\nWe will further denote the expected revenue from buyers if they observe only the price at time of arrival as follows:\nbtp\u03c11tq \u201c 1\n\u03c4\npt`1q\u03c4\u00ff\nk\u201ct\u03c4`1\nbkp\u03c11t, . . . , \u03c11tq .\nFirst, we are estimating the performance on the subsequence of rounds where the algorithm exploits the received feedback.\nLemma 13. Let \u03b21, . . . , \u03b2T be a sequence of unbiased Bernoulli random variables, denote\nS \u201c tt P rT s : \u03b2t`1 \u201c 0, \u03b2t`2 \u201c 1u,\nand denote the elements of S in increasing order S \u201c tts1 \u010f ts2 , . . . ,\u010f ts|S|u. For any price\n\u03c1\u02da P t 1 k , 2 k , . . . , 1u, Algorithm 2 enjoys the following guarantee:\nE\n\u00ab \u00ff\ntPS\nbtp\u03c1\u02daq \u00b4 btp\u03c11tq ff \u201c rOpT 2{3q ,\nand\nE\n\u00bb \u2013 |S|\u00ff\ns\u201c1\n|\u03c11ts \u00b4 \u03c11ts`1 |\nfi fl \u201c rOpT 2{3q .\nProof. For each sequence of buyers b1, . . . ,bT , define a sequence of loss functions \u21131 . . . , \u2113T according to:\n\u2113tpiq \u201c 1\u00b4 bt \u02c6 i\nk\n\u02d9 .\nFirst note that for every t P S we have \u03c11t \u201c \u03c11t`1. The algorithm, in turn, announces the same price \u03c11t for all days: tpt \u00b4 1q\u03c4 ` 1, . . . , pt` 1q\u03c4u, hence the revenue obtained from buyer bk for every pt \u00b4 1q\u03c4 ` 1 \u010f k \u010f t\u03c4 is given by btp\u03c11t, \u03c11tq. Hence, the feedback used to update the algorithm B at round t is\nft \u201c 1\u00b4 r1t \u201c 1\u00b4 1\n\u03c4\nt\u03c4\u00ff\nk\u201cpt\u00b41q\u03c4`1\nbkp\u03c1k, . . . , \u03c1k`\u03c4 q \u201c 1\u00b4 t\u03c4\u00ff\nk\u201cpt\u00b41q\u03c4`1\n1 \u03c4 bkp\u03c11tq \u201c \u2113tpitq .\nIn words, we have shown that at every step t P S, Algorithm 2 receive action it and return to Algorithm 1 as feedback \u2113tpitq. Thus Algorithm 2 applies Algorithm 1 on the sequence of losses t\u2113tutPS . As a corollary we have that:\nE\n\u00ab \u00ff\ntPS\nbtp\u03c1\u02daq \u00b4 btp\u03c11tq \u02c7\u030c \u02c7\u030c \u02c7 S ff \u201c E \u00ab \u00ff\ntPS\n\u2113tpi\u02daq \u00b4 \u2113tpitq \u02c7\u030c \u02c7\u030c \u02c7 S ff \u201c Op\u03b7k|S|q .\nTaking expectation over S and noting Er|S|s \u201c 1 4 T we get that\nE\n\u00ab \u00ff\ntPS\nbtp\u03c1\u02daq \u00b4 btp\u03c11t, q ff \u201c OpT 2{3q .\nAs in Section 4.6, note that if we associate with the prices the corresponding actions on the tree we obtain that |\u03c11t\u00b4 \u03c11t`1| \u010f \u2206T pit, it`1q hence we obtain as a second guarantee that the movement cost of the algorithm is given by\nE\n\u00bb \u2013 |S|\u00ff\ns\u201c1\n|\u03c11ts \u00b4 \u03c11ts\u00b41 | \u02c7\u030c \u02c7\u030c \u02c7\u030c S fi fl \u201c E \u00bb \u2013 |S|\u00ff\ns\u201c1\n1 k |its \u00b4 its\u00b41 | \u02c7\u030c \u02c7\u030c \u02c7\u030c S fi fl \u010f E \u00bb \u2013 |S|\u00ff\ns\u201c1\n1 k \u2206pits , its\u00b41q \u02c7\u030c \u02c7\u030c \u02c7\u030c S fi fl \u201c O p 1 k |S|q .\nAgain taking expectation over S we get that\nE\n\u00bb \u2013 |S|\u00ff\ns\u201c1\n|\u03c11ts \u00b4 \u03c11ts\u00b41 |\nfi fl \u201c rO ` 1 k T \u02d8 .\nNext, we upper bound the regret over the expected regret over the blocks of buyers, b\u0304t:\nLemma 14. For every \u03c1\u02da P t 1 k , 2 k , . . . , 1u we have that\nE\n\u00bb \u2013 T\u00ff\nt\u201c1\nbtp\u03c1\u02daq \u00b4 btp\u03c11t, \u03c11t`1q\nfi fl \u010f 4E \u00ab \u00ff\ntPS\nbtp\u03c1\u02daq \u00b4 btp\u03c11tq ff ` E \u00bb \u2013 |S|\u00ff\ns\u201c1\n|\u03c11ts \u00b4 \u03c11ts\u00b41 |\nfi fl .\nProof. First note that for every \u03c1\u02da we have\nE\n\u00ab \u00ff\ntPS\nbtp\u03c1\u02daq ff \u201c E \u00ab T\u00ff\nt\u201c1\nbtp\u03c1\u02daq\u03b2t`2p1\u00b4 \u03b2t`1q ff .\nSince the Bernoulli random variables are independent of bt and \u03c1 \u02da we get that\nE\n\u00ab \u00ff\ntPS\nbtp\u03c1\u02daq ff \u201c E \u00ab T\u00ff\nt\u201c1\nbtp\u03c1\u02daq\u03b2t`2p1\u00b4 \u03b2t`1q ff \u201c 1\n4 E\n\u00ab T\u00ff\nt\u201c1\nbtp\u03c1\u02daq ff . (11)\nSimilarly we have that\nE\n\u00ab \u00ff\ntPS\nbtp\u03c11tq ff \u201c E \u00bb \u2013 T\u00ff\nt\u201c1\nbtp\u03c11tq\u03b2t`2p1\u00b4 \u03b2t`1q\nfi fl \u201c 1\n4 E\n\u00bb \u2013 T\u00ff\nt\u201c1\nbtp\u03c11tq\nfi fl ,\nwhere the equality holds since \u03c11t is independent of \u03b2t`1 and \u03b2t`2. We can bound btp\u03c11t, \u03c11t`1q \u011b btp\u03c11t, \u03c11tq \u00b4 |\u03c11t \u00b4 \u03c11t`1|. Hence btp\u03c11t, \u03c11t`1q \u011b bp\u03c11tq \u00b4 |\u03c11t \u00b4 \u03c11t`1|, and we obtain:\nE\n\u00bb \u2013 T\u00ff\nt\u201c1\nbtp\u03c11t, \u03c11t`1q\nfi fl \u011b E \u00bb \u2013 T\u00ff\nt\u201c1\nbtp\u03c11tq \u00b4 |\u03c11t \u00b4 \u03c11t`1|\nfi fl\n\u201c 4E \u00ab \u00ff\ntPS\nbtp\u03c11tq ff \u00b4 T\u00ff\nt\u201c1\nE \u201c |\u03c11t \u00b4 \u03c11t`1| \u2030\n\u201c 4E \u00ab \u00ff\ntPS\nbtp\u03c11tq ff \u00b4 E \u00bb \u2013 |S|\u00ff\nt\u201cs\n|\u03c11ts \u00b4 \u03c11ts\u00b41 |\nfi fl , (12)\nwhere last equality is true since, we have that \u03c11t \u201c \u03c11t`1 unless \u03c11t\u00b41 P S in which case we have that \u03c11t\u00b41 \u201c \u03c11t \u201c \u03c11ts for some s and \u03c11t`1 \u201c \u03c11ts`1 . Taken together with Eqs. (11) and (12) we obtain the desired result.\nWe are now ready to prove the main result of this section.\nProof of Theorem 3. First, for any \u03c1 P t 1 k , . . . , 1u, by employing Lemma 14 we have the following:\nE\n\u00ab T\u00ff\nt\u201c1\nbtp\u03c1, . . . , \u03c1q \u00b4 btp\u03c1t, . . . , \u03c1t`\u03c4 q ff \u201c T\u00ff\nt1\u201c1\nt1\u03c4\u00ff\nt\u201cpt1\u00b41q\u03c4`1\n` b1tp\u03c1, . . . , \u03c1q \u00b4 btp\u03c1t, . . . , \u03c1t`\u03c4 q \u02d8\n\u201c \u03c4E\n\u00bb \u2013 T\u00ff\nt\u201c1\nbtp\u03c1q \u00b4 btp\u03c11t, \u03c11t`1q\nfi fl\n\u010f \u03c4 4 E\n\u00ab \u00ff\ntPS\nbtp\u03c1q \u00b4 btp\u03c11tq ff ` \u03c4E \u00bb \u2013 |S|\u00ff\ns\u201c1\n|\u03c11ts \u00b4 \u03c11ts\u00b41 |\nfi fl .\nNext, for any \u03c1\u02da P r0, 1s there exist \u03c1 P t 1 k , . . . , 1u such that \u03c1\u02da \u0105 \u03c1 and btp\u03c1\u02da, . . . , \u03c1\u02daq \u0103 btp\u03c1, . . . , \u03c1q ` 1k . Hence, for every \u03c1\u02da P r0, 1s we obtain that\nT\u00ff\nt\u201c1\nbtp\u03c1\u02da, . . . , \u03c1\u02daq \u00b4 E \u00ab T\u00ff\nt\u201c1\nbtp\u03c1t, . . . \u03c1t`\u03c4 q ff\n\u010f \u03c4 4 E\n\u00ab \u00ff\ntPS\nbtp\u03c1q \u00b4 btp\u03c11tq ff ` \u03c4E \u00bb \u2013 |S|\u00ff\ns\u201c1\n|\u03c11ts \u00b4 \u03c11ts\u00b41 |\nfi fl`OpT\nk q .\nBy Lemma 13 we now obtain\nT\u00ff\nt\u201c1\nbtp\u03c1\u02da, . . . , \u03c1\u02daq \u00b4 E \u00ab T\u00ff\nt\u201c1\nbtp\u03c1t, . . . , \u03c1t`\u03c4 q ff \u201c O \u02c6a \u03c4kT ` \u03c4T\nk ` T k\n\u02d9 \u201c Op\u03c4 1{3T 2{3q ,\nand using our choice of k gives the result."}], "references": [{"title": "Online learning in markov decision processes with adversarially chosen transition probability distributions", "author": ["Yasin Abbasi", "Peter L Bartlett", "Varun Kanade", "Yevgeny Seldin", "Csaba Szepesvari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Online learning with feedback graphs: Beyond bandits", "author": ["Noga Alon", "Nicol\u00f2 Cesa-Bianchi", "Ofer Dekel", "Tomer Koren"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Jean-Yves Audibert", "S\u00e9bastien Bubeck"], "venue": "In COLT,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Improved rates for the stochastic continuum-armed bandit problem", "author": ["P. Auer", "R. Ortner", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 20th Annual Conference on Learning Theory, pages 454\u2013 468", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Approximation algorithms and online mechanisms for item pricing", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "In Proceedings of the 7th ACM Conference on Electronic Commerce,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Sequential item pricing for unlimited supply", "author": ["Maria-Florina Balcan", "Florin Constantin"], "venue": "In International Workshop on Internet and Network Economics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Item pricing for revenue maximization", "author": ["Maria-Florina Balcan", "Avrim Blum", "Yishay Mansour"], "venue": "In Proceedings of the 9th ACM conference on Electronic commerce,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Dynamic pricing for impatient bidders", "author": ["Nikhil Bansal", "Ning Chen", "Neva Cherniavsky", "Atri Rurda", "Baruch Schieber", "Maxim Sviridenko"], "venue": "ACM Transactions on Algorithms (TALG),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Probabilistic approximations of metric spaces and its algorithmic applications", "author": ["Yair Bartal"], "venue": "Annual Symposium on Foundations of Computer Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}, {"title": "Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms", "author": ["Omar Besbes", "Assaf Zeevi"], "venue": "Operations Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "X -armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "C. Szepesv\u00e1ri"], "venue": "Journal of Machine Learning Research, 12:1587\u20131627", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems", "author": ["S\u00e9bastien Bubeck", "Nicol\u00f2 Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Kernel-based methods for bandit convex optimization", "author": ["S\u00e9bastien Bubeck", "Ronen Eldan", "Yin Tat Lee"], "venue": "arXiv preprint arXiv:1607.03084,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Improved second-order bounds for prediction with expert advice", "author": ["Nicolo Cesa-Bianchi", "Yishay Mansour", "Gilles Stoltz"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Regret and convergence bounds for a class of continuum-armed bandit problems", "author": ["E.W. Cope"], "venue": "IEEE Transactions on Automatic Control, 54(6):1243\u20131253", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "The price of bandit information for online optimization", "author": ["Varsha Dani", "Sham M Kakade", "Thomas P Hayes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Bandits with switching costs: T2/3 regret", "author": ["Ofer Dekel", "Jian Ding", "Tomer Koren", "Yuval Peres"], "venue": "In Symposium on Theory of Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "The blinded bandit: Learning with adaptive feedback", "author": ["Ofer Dekel", "Elad Hazan", "Tomer Koren"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Online pricing with strategic and patient buyers", "author": ["Michal Feldman", "Tomer Koren", "Roi Livni", "Yishay Mansour", "Aviv Zohar"], "venue": "In Annual Conference on Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Multi-armed bandits with metric switching costs", "author": ["Sudipto Guha", "Kamesh Munagala"], "venue": "In International Colloquium on Automata, Languages, and Programming,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Sharp dichotomies for regret minimization in metric spaces. In Proceedings of the twenty-first annual ACM-SIAM symposium on Discrete Algorithms, pages 827\u2013846", "author": ["Robert Kleinberg", "Aleksandrs Slivkins"], "venue": "Society for Industrial and Applied Mathematics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Multi-armed bandits in metric spaces", "author": ["Robert Kleinberg", "Aleksandrs Slivkins", "Eli Upfal"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Nearly tight bounds for the continuum-armed bandit problem", "author": ["Robert D. Kleinberg"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert D. Kleinberg", "Frank Thomson Leighton"], "venue": "In 44th Symposium on Foundations of Computer Science FOCS,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Multi-armed bandits on implicit metric spaces", "author": ["Aleksandrs Slivkins"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Ranked bandits in metric spaces: learning diverse rankings over large document collections", "author": ["Aleksandrs Slivkins", "Filip Radlinski", "Sreenivas Gollapudi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Unimodal bandits", "author": ["J.Y. Yu", "S. Mannor"], "venue": "Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Unfortunately, for the adversarial setting there are mostly hardness results even in limited cases [1].", "startOffset": 99, "endOffset": 102}, {"referenceID": 17, "context": "In such a setting a tight bound of r \u0398pk1{3T 2{3q is known [18].", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "[18], which applies already for k \u201c 2 actions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The bound of r \u0398p ? kT q for k \u011b T 1{3 is tight due to the classic lower bound for MAB even without movement costs [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "The main application of our SMB algorithm is for adaptive pricing with patient buyers [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "This is in contrast to a regret of r OpT 3{4q which is achieved by applying a standard switching cost technique together with a discretization argument [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": ", when switching between any two actions has a unit cost), it is known that there is a tight r \u03a9pk1{3T 2{3q lower bound for the MAB problem [18], which is in contrast to the Op ? kT q regret upper bound without switching costs.", "startOffset": 140, "endOffset": 144}, {"referenceID": 4, "context": "Classical MAB algorithms such as Exp3 [5] guarantee a regret of r Op ? kT q without movement costs.", "startOffset": 38, "endOffset": 41}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "There is a slight difference in the exact feedback model between [20] and here: in both models when a buyer arrives, the sell time is uniquely determined; however, in [20] the seller observes the purchase only at the actual time of the sell, whereas here we assume the seller observes the sell when the buyer arrives and decides when to purchase.", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "There is a slight difference in the exact feedback model between [20] and here: in both models when a buyer arrives, the sell time is uniquely determined; however, in [20] the seller observes the purchase only at the actual time of the sell, whereas here we assume the seller observes the sell when the buyer arrives and decides when to purchase.", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "We remark, though, that as discussed in [20] all lower bounds derived there apply to the current feedback model too.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "For the case of continuous prices and a single seller, when one consider impatient buyers, a simple discretization argument can be used to achieve a regret of r OpT 2{3q, and there exists a similar lower bound of \u03a9pT 2{3q [25].", "startOffset": 222, "endOffset": 226}, {"referenceID": 23, "context": "More generally, learning Lipschitz functions on a closed interval has been studied by Kleinberg [24], where an optimal r \u0398pT 2{3q regret bound is shown via discretization.", "startOffset": 96, "endOffset": 100}, {"referenceID": 23, "context": "There are many works on continuous action MAB [24, 16, 4, 12, 28].", "startOffset": 46, "endOffset": 65}, {"referenceID": 15, "context": "There are many works on continuous action MAB [24, 16, 4, 12, 28].", "startOffset": 46, "endOffset": 65}, {"referenceID": 3, "context": "There are many works on continuous action MAB [24, 16, 4, 12, 28].", "startOffset": 46, "endOffset": 65}, {"referenceID": 11, "context": "There are many works on continuous action MAB [24, 16, 4, 12, 28].", "startOffset": 46, "endOffset": 65}, {"referenceID": 27, "context": "There are many works on continuous action MAB [24, 16, 4, 12, 28].", "startOffset": 46, "endOffset": 65}, {"referenceID": 22, "context": "Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property.", "startOffset": 105, "endOffset": 121}, {"referenceID": 25, "context": "Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property.", "startOffset": 105, "endOffset": 121}, {"referenceID": 26, "context": "Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property.", "startOffset": 105, "endOffset": 121}, {"referenceID": 21, "context": "Specifically, there is an extensive literature on the Lipschitz MAB problem and various variants thereof [23, 26, 27, 22], where the expectation of the reward of arms have a Lipschitz property.", "startOffset": 105, "endOffset": 121}, {"referenceID": 20, "context": "The work of Guha and Munagala [21] discusses a stochastic MAB, in the spirit of the Gittins index, where there is both a switching cost and a play cost, and gives a constant approximation algorithm.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": ", k-HST) has a long history in the online algorithms literature, starting with the work of Bartal [10].", "startOffset": 98, "endOffset": 102}, {"referenceID": 17, "context": "In any movement cost problem with at least two arms of fixed constant distance, a lower bound regret of 2-arm switching cost applies, hence we observe that these rates are optimal for every k \u010f T [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 23, "context": "We emphasize that even without movement costs, there is an r \u03a9pT 2{3q lower bound in this setting [24]; hence, the regret bound of Theorem 2 is essentially optimal.", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "[20] where the buyer buy at day of purchase.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Indeed, Kleinberg and Leighton [25] showed that optimizing over the continuum r0, 1s leads to a lower bound of \u03a9pT 2{3q, irrespective of the patience of the buyers.", "startOffset": 31, "endOffset": 35}, {"referenceID": 19, "context": "[20] showed that whenever the seller wishes to optimize between more than two prices, a lower bound of \u03a9pT 2{3q holds for patient buyers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] constructed a sequence of buyers that reduces the problem to MAB with switching cost: a step in demonstrating a \u03a9pT 2{3q regret bound: thus a fluctuation in prices is indeed a cause for a high regret.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "We employ a reduction similar to the one used by [25]; however, the patience of the buyers introduce some difficulties, as we discuss below.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "The algorithm is based on the multiplicative update method, and in that sense is reminiscent of the Exp3 algorithm [5].", "startOffset": 115, "endOffset": 118}, {"referenceID": 14, "context": "4 Concluding the proof To conclude the proof and obtain a regret bound, we will use the following well-known second-order regret bound for the multiplicative weights (MW) method, essentially due to [15] (see also [2] for the version given here).", "startOffset": 198, "endOffset": 202}, {"referenceID": 1, "context": "4 Concluding the proof To conclude the proof and obtain a regret bound, we will use the following well-known second-order regret bound for the multiplicative weights (MW) method, essentially due to [15] (see also [2] for the version given here).", "startOffset": 213, "endOffset": 216}, {"referenceID": 18, "context": "Algorithm 2 overcomes this issue by employing techniques from [19] for handling adaptive feedback.", "startOffset": 62, "endOffset": 66}], "year": 2017, "abstractText": "We extend the model of Multi-armed Bandit with unit switching cost to incorporate a metric between the actions. We consider the case where the metric over the actions can be modeled by a complete binary tree, and the distance between two leaves is the size of the subtree of their least common ancestor, which abstracts the case that the actions are points on the continuous interval r0, 1s and the switching cost is their distance. In this setting, we give a new algorithm that establishes a regret of r Op ? kT ` T {kq, where k is the number of actions and T is the time horizon. When the set of actions corresponds to whole r0, 1s interval we can exploit our method for the task of bandit learning with Lipschitz loss functions, where our algorithm achieves an optimal regret rate of r \u0398pT 2{3q, which is the same rate one obtains when there is no penalty for movements. As our main application, we use our new algorithm to solve an adaptive pricing problem. Specifically, we consider the case of a single seller faced with a stream of patient buyers. Each buyer has a private value and a window of time in which they are interested in buying, and they buy at the lowest price in the window, if it is below their value. We show that with an appropriate discretization of the prices, the seller can achieve a regret of r OpT 2{3q compared to the best fixed price in hindsight, which outperform the previous regret bound of r OpT 3{4q for the problem.", "creator": "LaTeX with hyperref package"}}}