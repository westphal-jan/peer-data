{"id": "1702.07507", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Feb-2017", "title": "Use Generalized Representations, But Do Not Forget Surface Features", "abstract": "Only a year ago, all state-of-the-art coreference resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using word embeddings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving coreference resolution.", "histories": [["v1", "Fri, 24 Feb 2017 09:26:10 GMT  (25kb)", "http://arxiv.org/abs/1702.07507v1", "CORBON workshop@EACL 2017"]], "COMMENTS": "CORBON workshop@EACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nafise sadat moosavi", "michael strube"], "accepted": false, "id": "1702.07507"}, "pdf": {"name": "1702.07507.pdf", "metadata": {"source": "CRF", "title": "Use Generalized Representations, But Do Not Forget Surface Features", "authors": ["Nafise Sadat Moosavi"], "emails": ["nafise.moosavi@h-its.org", "michael.strube@h-its.org"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 2.\n07 50\n7v 1\n[ cs\n.C L\n] 2\n4 Fe\nb 20\n17\nerence resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using word embeddings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving coreference resolution."}, {"heading": "1 Introduction", "text": "Coreference resolution is the task of finding different mentions that refer to the same entity in a given text. Anaphoricity detection is an important step for coreference resolution. An anaphoricity detection module discriminates mentions that are coreferent with one of the previous mentions. If a system recognizes mentionm as non-anaphoric, it does not need to make any coreferent links for the pairs in which m is the anaphor.\nThe current state-of-the-art coreference resolvers (Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), as well as their anaphoricity detection modules, use deep neural networks, word embeddings and a small set of features describing surface properties of mentions. While it is shown that this small set of features has significant impact on the overall performance (Clark and Manning, 2016a), their use is very limited in the state-of-the-art systems in comparison to the embedding features.\nIn this paper, we first introduce a new neural model for anaphoricity detection that considerably outperforms the anaphoricity detection of the state-of-the-art coreference resolver, i.e. deepcoref introduced by Clark and Manning (2016a). However, we show that a simple SVM model that is adapted from our coreferent mention detection approach (Moosavi and Strube, 2016), significantly outperforms the more complex neural models. We show that the SVM model also generalizes better than the neural model on a new domain other than the CoNLL dataset."}, {"heading": "2 Discriminating Mentions for Coreference Resolution", "text": "The recognition of various categories of mentions can be beneficial for coreference resolution. The detection of the following categories is most common in the literature: (1) non-referential, (2) discourse-old, and (3) coreferent mentions. One can also discriminate other categories of mentions like mentions that are unlikely to be antecedents or discourse-new mentions (Uryupina, 2009). However, they are not common in comparison to the above categories."}, {"heading": "2.1 Non-Referential Mentions", "text": "Non-referential mentions do not refer to an entity. These mentions only fill a syntactic position. For instance, \u201cit\u201d in \u201cit is raining\u201d is a non-referential mention. The approaches proposed by Evans (2001), Mu\u0308ller (2006), Bergsma et al. (2008), Bergsma and Yarowsky (2011) are examples of detecting non-referential cases of the pronoun it. Byron and Gegg-Harrison (2004) present a more general approach for detecting non-referential noun phrases."}, {"heading": "2.2 Discourse-Old Mentions", "text": "Each mention can be assessed from the point of view of the discourse model (Prince, 1992). According to the discourse model, a mention may be new, old or inferable. Mentions which introduce a new entity into the discourse are discourse-new mentions. A discourse-new mention may be a singleton or it may be the first mention of a coreference chain. For instance, The first \u201cPlato\u201d in Example 2.1 is a discourse-new mention.\nExample 2.1. Plato was a philosopher in Classical Greece. This philosopher is the founder of the Academy in Athens. Plato died at the age of 81.\nA discourse-old mention refers to an entity that is already evoked in the discourse. Except for first mentions of coreference chains, other coreferent mentions are discourse-old. For instance, \u201cthis philosopher\u201d and the second \u201cPlato\u201d in Example 2.1 are discourse-old mentions.\nA mention is inferable if the hearer can infer the identity of the mention from another entity that has already been evoked in the discourse. \u201cthe windows\u201d in Example 2.2 is an inferable mention.\nExample 2.2. I walked into the room. The windows were all open.\nThe detection of discourse-old mentions is commonly referred to as anaphoricity detection (e.g. Zhou and Kong (2009), Ng (2009), Wiseman et al. (2015), Lassalle and Denis (2015), inter alia) while the task of anaphoric mention detection, based on its original definition, is of no use for coreference resolution. Mentions whose interpretations do not depend on previous mentions are called non-anaphoric mentions (van Deemter and Kibble, 2000). For example, both \u201dPlato\u201ds in Example 2.1 are non-anaphoric.\nFor consistency with the coreference literature, we refer to the task of discourse-old mention detection as anaphoricity detection.\nCurrently, all the state-of-the-art coreference resolvers learn anaphoricity detection jointly with coreference resolution (Wiseman et al., 2015; Wiseman et al., 2016; Clark and Manning, 2016a). The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009), Uryupina (2009) are examples of independent anaphoricity detection approaches."}, {"heading": "2.3 Coreferent Mentions", "text": "Marneffe et al. (2015) discriminate mentions as coreferent vs. non-coreferent. Coreferent mentions are those mentions that appear in a coreference chain. A non-coreferent mention therefore can be a non-referential noun phrase or a referential noun phrase whose entity is only mentioned once (i.e. singleton). The proposed approaches of Recasens et al. (2013), Marneffe et al. (2015), and Moosavi and Strube (2016) discriminate mentions for coreference resolution this way."}, {"heading": "3 Anaphoricity Detection Models", "text": "Anaphoricity detection is the most common approach for discriminating mentions for a coreference resolver. All of the state-of-the-art coreference resolvers use anaphoricity detection. In this paper, we compare three different anaphoricity detection approaches: two approaches using neural networks and word embeddings, and one using an SVM model and surface features. Clark and Manning (2016a) introduce the first neural model. Since Clark and Manning (2016a) train their anaphoricity model jointly with the coreference model, we refer to this model as the joint model. We introduce a new anaphoricity detection model as the second neural model using a Long-Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997). The third approach is adapted from our state-of-the-art coreferent mention detection (Moosavi and Strube, 2016)."}, {"heading": "3.1 Joint Model", "text": "As one of the neural models for anaphoricity detection, we consider the anaphoricity module of deep-coref1 , the state-of-the-art coreference resolution system introduced by Clark and Manning (2016a). This model has three layers for encoding different types of information regarding a mention. The first layer encodes the word embeddings of the head, first, last, two previous/following words, and the syntactic parent of the mention. The second layer encodes the averaged word embeddings of the five previous/following words, all words of the mention, sentence words, and document words. The third layer encodes the following features of a mention: type, length, position and whether it is embedded in another mention. The outputs of these three\n1Available at https://github.com/clarkkev/deep-coref\nlayers are combined into one vector and then get passed through a network with two hidden layers. This anaphoricity model is trained jointly with the deep-coref coreference model."}, {"heading": "3.2 LSTMModel", "text": "In this section we propose a new neural model for anaphoricity detection. Apart from the properties of the mention itself, we consider a limited number of surrounding words. We first generalize the context of a mention by removing the mention from the context and replacing it with a special placeholder. In our experiments, we consider the 10 previous and following words of a mention. We concatenate the mention tokens and the head token to the generalized word sequence. We separate the head and mention tokens in the concatenated sequence using two different placeholders.\nThe word embeddings of the above sequence are encoded using a bidirectional LSTM. LSTMs show convincing results on generating meaningful representations for various NLP tasks (e.g. Sutskever et al. (2014) and Vinyals et al. (2014)).\nWe also incorporate a set of surface features that contains (1) mention type (proper, nominal (definite, indefinite), pronouns (he, I, it, she, they, we, you)), (2) string match in the text, (3) string match in the previous context, (4) head match in the text, (5) head match in the previous context, (6) contains tokens of another mention, (7) contains tokens of a previous mention, (8) contained in another mention, (9) contained in a previous mention, and (10) embedded in another mention. These features are concatenated with the output of the bidirectional LSTM and get passed through one more layer that generates the output.\nWe also experiment with a more complex model including two different LSTMs for encoding mentions and their surrounding words. We consider longer sequences of previous words and an attention mechanism for processing the long sequence. However, the performance did not improve upon the LSTM model while it considerably increased the training time."}, {"heading": "3.2.1 Implementation Details", "text": "Hyperparameters are tuned on the CoNLL 2012 development set. We minimize the cross entropy loss using gradient-based optimization and the Adam update rule (Kingma and Ba, 2014). We use minibatches of size 50. A dropout (Hinton et al., 2012) with a rate of 0.3 is applied\nto the output of LSTM. We initialize the embeddings with the 300-dimensional Glove embeddings (Pennington et al., 2014). The size of LSTM\u2019s hidden layer is set to 128. The model is trained in only one epoch."}, {"heading": "3.3 SVMModel", "text": "Our SVM model introduced in Moosavi and Strube (2016), achieves state-of-theart results for coreferent mention detection. This model uses the following set of features: lemmas and POS tags of all words of a mention, lemmas and POS tags of the two previous/following words, mention string, mention length, mention type (proper, nominal, pronoun, list), string match in the text, and head match in the text. We use a similar SVM model for anaphoricity detection. In addition to the features we used for coreferent mention detection, we also add the following features for anaphoricity detection: string match in the previous context, head match in the previous context, mention words are contained in another mention, mention words are contained in a previous mention, mention contains words of another mention, mention contains words of a previous mention. Similar to Moosavi and Strube (2016), we use an anchored SVM (Goldberg and Elhadad, 2007) with a polynomial kernel of degree two and remove feature-values that occur less than 10 times. The use of an anchored SVM with pruning helps the model to generalize better on new domains (Goldberg and Elhadad, 2009)."}, {"heading": "4 Performance Evaluation", "text": "We evaluate the anaphoricity models on the CoNLL 2012 dataset. It is worth noting that all of the examined anaphoricity detectors in this section use the same mention detection module and results are reported using system detected mentions. The performance of the mention detection module is of crucial importance for anaphoricity detection. Therefore, it is important that the compared anaphoricity detectors use the same mention detection.\nThe LSTM model that is described in Section 3.2 is denoted as LSTM in Table 1. In order to investigate the effect of the used surface features, we also report the results of the LSTM model without using these features (LSTM\u2217).\nThe following observations can be drawn from\nTable 1: Results on the CoNLL 2012 test set.\nthe results of Table 1: (1) our LSTMmodel outperforms the joint model while using less features and being trained independently, (2) the results of the LSTM\u2217 model is considerably lower than those of LSTM, especially for recognizing anaphoric mentions, and (3) the simple SVM model outperforms the neural models in detecting both anaphoric and non-anaphoric mentions."}, {"heading": "4.1 Generalization Evaluation", "text": "In order to investigate the generalization on new domains, we evaluate the LSTM and SVM models on the WikiCoref dataset (Ghaddar and Langlais, 2016). The WikiCoref dataset is annotated according to the same annotation guideline as that of CoNLL. Therefore, it is an appropriate dataset for performing out-of-domain evaluations when CoNLL is used for training. For the experiments of Table 2, all models are trained on the CoNLL 2012 training data and tested on the WikiCoref dataset.\nThe word dictionary that is used for the LSTM model is built based on the CoNLL 2012 training data. All words that are not included in this dictionary are treated as out of vocabulary words with randomly initialized word embeddings. We further improve the performance of LSTM on WikiCoref, by adding the words from the WikiCoref dataset into its dictionary. The LSTM model trained with this extended dictionary is denoted as LSTM\u2020 in Table 2. LSTM\u2020 results are still lower than those of the SVMmodel while SVM does not use any information from the test dataset. Pruning rare lexical features from the training data along the incorporation of part of speech tags, which are far more generalizable than lexical features, could explain the generalizability of the SVM model on the new domain."}, {"heading": "5 Analysis Based on Mention Types", "text": "We analyze the output of the LSTM and SVM models on the CoNLL 2012 test set to see how well they perform for different types of men-\ntions. As can be seen from Table 3, there is not much difference between the performance of LSTM and SVM for recognizing anaphoric pronouns. SVM detects anaphoric proper names better while LSTM is better at recognizing anaphoric common nouns.\nWe also analyze the output of LSTM\u2217. As can be seen, the incorporation of surface features does not affect the detection of anaphoric pronouns very much while it mainly affects the detection of anaphoric proper names by about 24 percent.\nIn order to see whether the same pattern holds for coreference resolution, we compare the recall and precision errors of the best coreference system that only uses surface features, i.e. cort (Martschat and Strube, 2015) with singleton features (Moosavi and Strube, 2016) 2, and the stateof-the-art deep coreference resolver, i.e. deepcoref (Clark and Manning, 2016a). The comparison of the errors for the CoNLL 2012 test set is shown in Table 4. We use the error analysis tool of cort introduced by Martschat and Strube (2014) for the results of Table 4. As can be seen from Table 4, while deep-coref is significantly better than cort for resolving common nouns and specially pronouns, its result does not go far beyond that of cort when it comes to resolving proper names.\n2Available at https://github.com/ns-moosavi/cort/tree/sin"}, {"heading": "6 Discussion", "text": "In this paper we analyze the effect of surface features for anaphoricity detection, which is a small but an important step for coreference resolution. Our analysis shows that surface features, as it was known, are important. Based on our results, the effects of incorporating surface properties and generalized representations are different for different types of mentions. These results suggest that apart from a unified model, we should consider different models or at least different features for processing different types of mentions and do not put all the burden on a single model to learn the differences. The works by Lassalle and Denis (2013) and Denis and Baldridge (2008) are examples of models in which distinct models have been used for various types of mentions. Besides, our analysis shows the importance of surface features for proper names. Word embeddings are very useful for capturing semantic relatedness. A coreference resolver that uses word embeddings has a great advantage in better resolution of common nouns and pronouns. However, the use of surface features in current state-of-the-art coreference resolvers is very limited. Before going towards using more sophisticated knowledge sources, there are still easy victories that can be achieved by incorporating more generalizable surface properties, especially for proper names."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Kevin Clark for his help with the deep-coref software and MarkChristoph Mu\u0308ller for his helpful comments. We would also like to thank the four anonymous reviewers for their detailed comments on an earlier draft of the paper. This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany. The first author has been supported by a Heidelberg Institute for Theoretical Studies PhD. scholarship."}], "references": [{"title": "NADA: A robust system for non-referential pronoun detection", "author": ["Bergsma", "Yarowsky2011] Shane Bergsma", "David Yarowsky"], "venue": "In Anaphora Processing and Applications. Proceedings of the 8th Discourse Anaphora and Anaphor Resolution", "citeRegEx": "Bergsma et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergsma et al\\.", "year": 2011}, {"title": "Distributional identification of non-referential pronouns", "author": ["Dekang Lin", "Randy Goebel"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Bergsma et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bergsma et al\\.", "year": 2008}, {"title": "Eliminating non-referring noun phrases from coreference resolution", "author": ["Byron", "Gegg-Harrison2004] Donna Byron", "Whitney Gegg-Harrison"], "venue": "In Proceedings the Discourse Anaphora and Reference Resolution Conference,", "citeRegEx": "Byron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Byron et al\\.", "year": 2004}, {"title": "Improving coreference resolution by learning entity-level distributed representations", "author": ["Clark", "Manning2016a] Kevin Clark", "Christopher D. Manning"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Clark et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Clark", "Manning2016b] Kevin Clark", "Christopher D. Manning"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Clark et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2016}, {"title": "Specialized models and ranking for coreference resolution", "author": ["Denis", "Baldridge2008] Pascal Denis", "Jason Baldridge"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Denis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Denis et al\\.", "year": 2008}, {"title": "Applying machine learning toward an automatic classification of it", "author": ["Richard Evans"], "venue": "Literary and Linguistic Computing,", "citeRegEx": "Evans.,? \\Q2001\\E", "shortCiteRegEx": "Evans.", "year": 2001}, {"title": "WikiCoref: An English coreference-annotated corpus of Wikipedia articles", "author": ["Ghaddar", "Langlais2016] Abbas Ghaddar", "Philippe Langlais"], "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation", "citeRegEx": "Ghaddar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ghaddar et al\\.", "year": 2016}, {"title": "SVM model tampering and anchored learning: a case study in Hebrew NP chunking", "author": ["Goldberg", "Elhadad2007] Yoav Goldberg", "Michael Elhadad"], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Goldberg et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2007}, {"title": "On the role of lexical features in sequence labeling", "author": ["Goldberg", "Elhadad2009] Yoav Goldberg", "Michael Elhadad"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore,", "citeRegEx": "Goldberg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2009}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580", "author": ["Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R. Salakhutdinov"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Improving pairwise coreference models through feature space hierarchy learning", "author": ["Lassalle", "Denis2013] Emmanuel Lassalle", "Pascal Denis"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Lassalle et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lassalle et al\\.", "year": 2013}, {"title": "Joint anaphoricity detection and coreference resolution with constrained latent structures", "author": ["Lassalle", "Denis2015] Emmanuel Lassalle", "Pascal Denis"], "venue": "In Proceedings of the 29th Conference on the Advancement of Artificial Intelligence,", "citeRegEx": "Lassalle et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lassalle et al\\.", "year": 2015}, {"title": "Modeling the lifespan of discourse entities with application to coreference resolution", "author": ["Marta Recasens", "Christopher Potts"], "venue": "Journal of Artificial Intelligent Research,", "citeRegEx": "Marneffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2015}, {"title": "Recall error analysis for coreference resolution", "author": ["Martschat", "Strube2014] Sebastian Martschat", "Michael Strube"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Martschat et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2014}, {"title": "Latent structures for coreference resolution. Transactions of the Association for Computational Linguistics, 3:405\u2013418", "author": ["Martschat", "Strube2015] Sebastian Martschat", "Michael Strube"], "venue": null, "citeRegEx": "Martschat et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Martschat et al\\.", "year": 2015}, {"title": "Search space pruning: A simple solution for better coreference resolvers", "author": ["Moosavi", "Strube2016] Nafise Sadat Moosavi", "Michael Strube"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Moosavi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Moosavi et al\\.", "year": 2016}, {"title": "Automatic detection of nonreferential it in spoken multi-party dialog", "author": ["Christoph M\u00fcller"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy,", "citeRegEx": "M\u00fcller.,? \\Q2006\\E", "shortCiteRegEx": "M\u00fcller.", "year": 2006}, {"title": "Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution", "author": ["Ng", "Cardie2002] Vincent Ng", "Claire Cardie"], "venue": "In Proceedings of the 19th International Conference on Computational Linguistics, Taipei, Taiwan,", "citeRegEx": "Ng et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2002}, {"title": "Learning noun phrase anaphoricity to improve coreference resolution", "author": ["Vincent Ng"], "venue": "In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Ng.,? \\Q2004\\E", "shortCiteRegEx": "Ng.", "year": 2004}, {"title": "Graph-cut-based anaphoricity determination for coreference resolution", "author": ["Vincent Ng"], "venue": "In Proceedings of Human Language Technologies 2009: The Conference of the North American Chapter of the Association", "citeRegEx": "Ng.,? \\Q2009\\E", "shortCiteRegEx": "Ng.", "year": 2009}, {"title": "GloVe: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "The ZPG letter: Subjects, definiteness, and information-status", "author": ["Ellen F. Prince"], "venue": "Discourse Description. Diverse Linguistic Analyses of a FundRaising Text,", "citeRegEx": "Prince.,? \\Q1992\\E", "shortCiteRegEx": "Prince.", "year": 1992}, {"title": "The life and death of discourse entities: Identifying singleton mentions", "author": ["MarieCatherine de Marneffe", "Christopher Potts"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter", "citeRegEx": "Recasens et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Recasens et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Detecting Anaphoricity and Antecedenthood for Coreference Resolution", "author": ["Olga Uryupina"], "venue": "Procesamiento del Lenguaje Natural,", "citeRegEx": "Uryupina.,? \\Q2009\\E", "shortCiteRegEx": "Uryupina.", "year": 2009}, {"title": "On coreferring: Coreference in MUC and related annotation schemes", "author": ["van Deemter", "Kibble2000] Kees van Deemter", "Rodger Kibble"], "venue": null, "citeRegEx": "Deemter et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Deemter et al\\.", "year": 2000}, {"title": "Learning anaphoricity and antecedent ranking features for coreference resolution", "author": ["Wiseman et al.2015] Sam Wiseman", "Alexander M. Rush", "Stuart Shieber", "Jason Weston"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Wiseman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2015}, {"title": "Learning global features for coreference resolution", "author": ["Wiseman et al.2016] Sam Wiseman", "Alexander M. Rush", "Stuart Shieber"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Wiseman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "Global learning of noun phrase anaphoricity in coreference resolution via label propagation", "author": ["Zhou", "Kong2009] Guodong Zhou", "Fang Kong"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Zhou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 30, "context": "The current state-of-the-art coreference resolvers (Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), as well as their", "startOffset": 51, "endOffset": 125}, {"referenceID": 27, "context": "discourse-new mentions (Uryupina, 2009).", "startOffset": 23, "endOffset": 39}, {"referenceID": 4, "context": "The approaches proposed by Evans (2001), M\u00fcller (2006), Bergsma et al.", "startOffset": 27, "endOffset": 40}, {"referenceID": 4, "context": "The approaches proposed by Evans (2001), M\u00fcller (2006), Bergsma et al.", "startOffset": 27, "endOffset": 55}, {"referenceID": 0, "context": "The approaches proposed by Evans (2001), M\u00fcller (2006), Bergsma et al. (2008), Bergsma and Yarowsky (2011) are examples of", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "The approaches proposed by Evans (2001), M\u00fcller (2006), Bergsma et al. (2008), Bergsma and Yarowsky (2011) are examples of", "startOffset": 56, "endOffset": 107}, {"referenceID": 24, "context": "Each mention can be assessed from the point of view of the discourse model (Prince, 1992).", "startOffset": 75, "endOffset": 89}, {"referenceID": 21, "context": "Zhou and Kong (2009), Ng (2009), Wiseman et al.", "startOffset": 22, "endOffset": 32}, {"referenceID": 21, "context": "Zhou and Kong (2009), Ng (2009), Wiseman et al. (2015), Lassalle and Denis (2015), inter alia) while the task of anaphoric mention detection, based on its original definition, is of no use", "startOffset": 22, "endOffset": 55}, {"referenceID": 21, "context": "Zhou and Kong (2009), Ng (2009), Wiseman et al. (2015), Lassalle and Denis (2015), inter alia) while the task of anaphoric mention detection, based on its original definition, is of no use", "startOffset": 22, "endOffset": 82}, {"referenceID": 21, "context": "The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009),", "startOffset": 27, "endOffset": 48}, {"referenceID": 21, "context": "The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009),", "startOffset": 27, "endOffset": 59}, {"referenceID": 21, "context": "The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009),", "startOffset": 27, "endOffset": 70}, {"referenceID": 21, "context": "The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009),", "startOffset": 27, "endOffset": 92}, {"referenceID": 26, "context": "Sutskever et al. (2014) and Vinyals et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 26, "context": "Sutskever et al. (2014) and Vinyals et al. (2014)).", "startOffset": 0, "endOffset": 50}, {"referenceID": 10, "context": "A dropout (Hinton et al., 2012) with a rate of 0.", "startOffset": 10, "endOffset": 31}, {"referenceID": 23, "context": "We initialize the embeddings with the 300-dimensional Glove embeddings (Pennington et al., 2014).", "startOffset": 71, "endOffset": 96}], "year": 2017, "abstractText": "Only a year ago, all state-of-the-art coreference resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using word embeddings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving coreference resolution.", "creator": "LaTeX with hyperref package"}}}