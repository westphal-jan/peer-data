{"id": "1702.07983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2017", "title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks", "abstract": "Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of back-propagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator's output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.", "histories": [["v1", "Sun, 26 Feb 2017 03:19:13 GMT  (284kb,D)", "http://arxiv.org/abs/1702.07983v1", "11 pages, 3 figures"]], "COMMENTS": "11 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["tong che", "yanran li", "ruixiang zhang", "r devon hjelm", "wenjie li", "yangqiu song", "yoshua bengio"], "accepted": false, "id": "1702.07983"}, "pdf": {"name": "1702.07983.pdf", "metadata": {"source": "META", "title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks", "authors": ["Tong Che", "Yanran Li", "Ruixiang Zhang", "R Devon Hjelm", "Wenjie Li", "Yangqiu Song", "Yoshua Bengio"], "emails": ["<tong.che@umontreal.ca>."], "sections": [{"heading": "1. Introduction", "text": "Generative models are appealing because they provide ways to obtain insights on the underlying data distribution and statistics. In particular, these models play a pivot role in many natural language processing tasks such as language modeling, machine translation, and dialogue generation. However, the generated sentences are often unsatisfactory (Sordoni et al., 2015; Bowman et al., 2015; Serban et al., 2017; Wiseman & Rush, 2016). For example, they often lack of consistency in long-term semantics and have less coherence in high-level topics and syntactics (Bowman et al., 2015; Zhang et al., 2016a).\nThis is largely attributed to the defect in the dominant training approach for existing discrete generative models. To\n*Equal contribution 1Montreal Institute for Learning Algorithms, Universite\u0301 de Montre\u0301al, Montre\u0301al, QC H3T 1J4, Canada 2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 3The Hong Kong University of Science and Technology 4IVADO. Correspondence to: Tong Che <tong.che@umontreal.ca>.\ngenerate discrete sequences, it is popular to adopt autoregressive models through teacher forcing (Williams & Zipser, 1989) which, nevertheless, causes the exposure bias problem (Ranzato et al., 2016). The existing approach trains auto-regressive models to maximize the conditional probabilities of next tokens based on the ground-truth histories. In other words, during training, auto-regressive generative models are only exposed to the ground truths from the data distribution rather than those from the model distribution, i.e., its own predictions. It prohibits the trained model to take advantage of learning in the the context of its previous generated words to make the next prediction, resulting in a bias and difficulty in approaching the true underlying distribution (Ranzato et al., 2016; Bengio et al., 2015). Another limitation of teacher forcing is that it is inapplicable to those auto-regressive models with latent random variables, which have performed better than autoregressive (deterministic state) recurrent neural networks (i.e. usual RNNs, LSTMs or GRUs) on multiple tasks (Serban et al., 2017; Miao et al., 2016; Zhang et al., 2016a).\nAn alternative and attractive solution to training autoregressive models is using generative adversarial networks (GAN) (Goodfellow et al., 2014). The above discussed problem can be prevented if the generative models were able to visit its own predictions during training and had an overall view on the generated sequences. We suggest to facilitate the training of autoregressive models with an additional discriminator under the GAN setting. With a discriminator trained to separate real versus generated sequences, the generative model is able to make use of the knowledge of the discriminator to improve itself. Since the discriminator is trained on the entire sequence, it can in principle provide the training signal to avoid the problem of exposure bias.\nHowever, it is nontrivial to apply GANs to discrete data as it is difficult to optimize of the generator using the signal provided by the discriminator. In fact, it is usually very hard to push the generated distribution to the real data distribution, if not impossible, by moving the generated sequence (e.g., a faulty sentence) towards a \u201ctrue\u201d one (e.g., a correct sentence) in a high dimensional discrete state space. As standard back-propagation fails in discrete settings, the generator can be optimized using the discriminator\u2019s output as a reward via reinforcement learning. Unfortunately,\nar X\niv :1\n70 2.\n07 98\n3v 1\n[ cs\n.A I]\n2 6\nFe b\n20 17\neven with careful pre-training, we found that the policy has difficulties to get positive and stable reward signals from the discriminator.\nTo tackle these limitations, we propose MaximumLikelihood Augmented Discrete Generative Adversarial Networks (MaliGAN). At the core of this model is the novel GAN training objective which sidesteps the stability issue happening when using the discriminator output as a direct reinforcement learning reward. Alternatively, we develop a normalized maximum likelihood optimization target inspired by (Norouzi et al., 2016b). We use importance sampling and several variance reduction techniques in order to successfully optimize this objective. The procedure was discovered independently from us by Hjelm et al. (2017) in the context of image generation.\nThe new target brings several attractive properties in the proposed MaliGAN. First, it is theoretically consistent and easier to optimize (Section 3.2). Second, it allows the model not only to maximize the likelihood of good behaviors, but also to minimize the likelihood of bad behaviors, with the help of a GAN discriminator. Equipped with these strengths, the model focuses more on improving itself by gaining beneficial knowledge that is not yet well acquired, and excluding the most probable and harmful behaviors. Combined with several proposed variance reduction techniques, the proposed MaliGAN successfully and stably models discrete data sequences (Section 4)."}, {"heading": "2. Preliminaries and Overview", "text": "The basic framework for discrete sequence generation is to fit a set of data {xi}Ni=1 coming from an underlying generating distribution pd by training a parameterized autoregressive probabilistic model p\u03b8.\nIn this work, we aim to generate discrete data, especially discrete sequential data, under the GAN setting (Goodfellow et al., 2014). GAN defines a framework for training generative models by posing it as a minimax game against a discriminative model. The goal of the generator G is to match its distribution pg to the real data distribution pd. To achieve this, the generator transforms noise z sampled from p(z) to a data sample G(z). Following this, the discriminator D is trained to distinguish between the samples coming from pd and pg , and can be used to provide a training signal to the generator.\nWhen applying the GAN framework to discrete data, the discontinuity prohibits the update of the generator parameters via standard back-propagation. To tackle this, one way is to employ a typical reinforcement learning (RL) strategy that directly uses the GAN discriminator\u2019s output, D or logD as a reward. In practice, the problem is usually solved by REINFORCE-like algorithms (Williams, 1992),\nperhaps with some variance reduction techniques.\nFormally, we train a generator G(x) together with a discriminator D(x). In its original form, the discriminator is trained to distinguish between the generating distribution p\u03b8 and the real data distribution pd. The generator is then trained to maximize Ex\u223cp\u03b8 [logD(x)]. Namely, the objective for the generator to optimize is as follows:\nLGAN (\u03b8) = \u2212Ex\u223cp\u03b8 [logD(x)]\n\u2248 \u2212 1 n n\u2211 i=1 logD(xi), xi \u223c p\u03b8.\nOur work is related to the viewpoint of casting the GAN training as a reinforcement learning problem with a moving reward signal monotone in D(x). Define the normalized probability distribution q\u2032(x) = 1Z(D)D(x)\n1/\u03c4 in some bounded region to guarantee integrability (note that D is an approximation to pdp+pd if D is well trained) and also put a maximum-entropy regularizer H(p\u03b8) to encourage diversity, yielding the regularized loss:\nLGAN (\u03b8) = \u2212Ex\u223cp\u03b8 [logD(x)]\u2212 \u03c4H(p\u03b8) = \u03c4KL(p\u03b8||q\u2032) + c(D)\n(1)\nwhere c(D) is a constant depending only on D. Hence, optimizing the traditional GAN is basically equivalent to optimizing the KL-divergence KL(p\u03b8||q\u2032). One major problem with this approach is that q\u2032 always moves with D, which is undesirable for both stability and convergence. When we have some samples xi \u223c p\u03b8, we want to change \u03b8 a bit in order to adjust the likelihood of samples xi to improve the quality of the generator. However, since initially p generates very bad sequences, it have little chance of generating good sequences in order to get positive rewards. Though the dedicated pre-training and variance reduction mechanisms help (Yu et al., 2017), the RL algorithm based on the moving reward signal still seems very unstable and does not work on large scale datasets.\nWe therefore propose to utilize the information of the discriminator as an additional source of training signals, on top of the maximum-likelihood objective. We employ importance sampling to make the objective trainable. The novel training objective has much less variance than that in vanilla reinforcement learning approaches that directly adopt D or logD as reward signals. The analysis and discussions will be presented in more detail in Section 3.2."}, {"heading": "3. Maximum-Likelihood Augmented Discrete Generative Adversarial Networks", "text": "In this section, we present the details of the proposed model. At the heart of this model is a novel training ob-\njective that significantly reduces the variance during training, including the theoretical and practical analysis on the objective\u2019s equivalence and attractive properties. We also show how this core algorithm can be combined with several variance reduction techniques to form the full MaliGAN algorithm for discrete sequence generation."}, {"heading": "3.1. Basic Model of MaliGAN", "text": "We propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks (MaliGAN) to generate the discrete data. With MaliGAN, we train a discriminator D(x) with the standard objective that GAN employs. What is different from GANs is a novel objective for the generator to optimize, using importance sampling, which makes the training procedure closer to maximum likelihood (MLE) training of auto-regressive models, and thus being more stable and with less variance in the gradients.\nTo do so, we keep a delayed copy p\u2032(x) of the generator whose parameters are updated less often in order to stabilize training. From the basic property of GANs, we know that an optimal D has the property D(x) = pdpd+p\u2032 . So in this case, we have pd = D1\u2212Dp\n\u2032. Therefore, we set the target distribution q for maximum likelihood training to be D\n1\u2212Dp \u2032. Let rD(x) = D(x) 1\u2212D(x) , we define the augmented target distribution as:\nq(x) = 1\nZ(\u03b8\u2032)\nD(x)\n1\u2212D(x) p\u2032(x) =\nrD(x) Z(\u03b8\u2032) p\u2032(x)\nRegarding q as a fixed probability distribution, then the target to optimize is:\nLG(\u03b8) = KL(q(x)||p\u03b8(x))\nThis objective has an attractive property that q is a \u201cfixed\u201d distribution during training, i.e., if D is sufficiently trained, then q is always approximately the data generating distribution pd. By defining the gradient as \u2207LG = Eq[\u2207\u03b8 log p\u03b8(x)], we have the following importance sampling formula:\n\u2207LG = Ep\u2032 [ q(x)\np\u2032(x) \u2207\u03b8 log p\u03b8(x)]\n= 1\nZ Ep\u03b8 [rD(x)\u2207\u03b8 log p\u03b8(x)]\nwhere we assume that p\u2032 = p\u03b8 and the delayed generator is only one step behind the current update in the experiments. This importance sampling procedure was discovered independently from us by (Hjelm et al., 2017). We propose to optimize the generator using the following novel gradient estimator:\n\u2207LG(\u03b8) \u2248 m\u2211 i=1 ( rD(xi)\u2211 i rD(xi) \u2212 b)\u2207 log p\u03b8(xi) = E({xi}m1 )\n(2)\nwhere b is a baseline from reinforcement learning in order to reduce variance. In practice, we let b increase very slowly from 0 to 1. Combined with the objective of the discriminator in an ordinary GAN, we get the proposed MaliGAN algorithm as shown in Algorithm 1.\nAlgorithm 1 MaliGAN Require: A generator p with parameters \u03b8.\nA discriminator D(x) with parameters \u03b8d. A baseline b.\n1: for number of training iterations do 2: for k steps do 3: Sample a minibatch of samples {xi}mi=1 from p\u03b8 . 4: Sample a minibatch of samples {yi}mi=1 from pd. 5: Update the parameter of discriminator by taking gradient\nascend of discriminator loss\u2211 i [\u2207\u03b8d logD(yi)] + \u2211 i [\u2207\u03b8d log(1\u2212D(xi))]\n6: end for 7: Sample a minibatch of samples {xi}mi=1 from p\u03b8 . 8: Update the generator by applying gradient update\nm\u2211 i=1 ( rD(xi)\u2211 i rD(xi) \u2212 b)\u2207 log p\u03b8(xi)\n9: end for"}, {"heading": "3.2. Analysis", "text": "The proposed objective in Eq. 2 is also theoretically guaranteed to be sound. In the following theorem, we show that our training objective approximately optimizes the KL divergence KL(q(x)||p\u03b8(x)) when D is close to optimal. What\u2019s more, the objective still makes sense when D is well trained but far from optimal.\nTheorem 3.1. We have the following two theoretical guarantees for our new training objective.\n(i) If discriminatorD(x) is optimal between delayed generator p\u2032 and real data distribution pd, we have the following equation.\nEpd [log p\u03b8(x)] = 1\nZ(\u03b8\u2032) Ep\u2032 [rD(x) log p\u03b8(x)]\nwhere Z(\u03b8\u2032) = Ep\u2032 [rD(x)] = 1.\n(ii) If D(x) is trained well but not sufficiently, namely, \u2200x, D(x) lies between 0.5 and pdpd+p\u2032 , we have the property that for m\u2192\u221e, almost surely\nE({xi}m1 ) \u00b7 \u2207\u03b8KL(pd||p\u03b8) > 0 (3) The above gives us a condition for our objective to still push the generator in a descent direction even when the discriminator is not trained to optimality.\nIn addition to its attractiveness in theory, we now demonstrate why the gradient estimator in Eq. 2 of\u2207LG(\u03b8) practically can produce better training signal for the generator than the original GAN objective. Similar discussions can\nbe found in (Bornschein & Bengio, 2015; Norouzi et al., 2016a).\nIn the original GAN setting from a reinforcement learning perspective, e.g. the inclusive KL in Eq. 1, the free running auto-regressive model can be viewed as an RL agent exploring the state space and getting a reward, D or logD, at the end of the exploration. The model then tries to adjust the probability of each of its exploration paths according to this reward. However, this gradient estimator would be drastically inefficient when almost all generated paths had a very small discriminator output. Unfortunately, this is very common in GAN training and cannot even be solved with a carefully selected baseline.\nIn the MaliGAN objective, however, the partition function Z is estimated using the samples from the minibatch, which helps dealing with the above dilemma. When we choose, for example, baseline b = 1, we can see that the sum of the weights on the generated paths are zero, and the probability of each path is adjusted not according to the absolute value of the discriminator output, but its relative quality in that minibatch. This ensures that the model can always learn something as long as there exist some generations better than others in that mini-batch. Furthermore, the previous theorem ensures the consistency of the mini-batch level normalization procedure.\nFrom a theoretical point of view, this normalization procedure also helps. Although at the first glance, when D is optimal, one can prove that Z = 1, so estimating Z seems to only introduce additional variance to the model. However, using this estimator in fact reduces the variance due to the following reason: rD(x) is actually a function with singularity when x is in a region \u2126 in the data space on which D(x) \u2248 1. Even with very careful pre-training, such a region \u2126 rD 0 and p\u2032(\u2126) \u2248 0, making the ratio blow up. In our target 1Z(\u03b8\u2032)Ep\u2032 [rD(x) log p\u03b8(x)], since it is almost impossible to get samples from \u2126 with p\u2032 in a reasonable size mini-batch, the actual distribution we are sampling from is a \u201cregularized\u201d distribution p\\\u2126 where p\\\u2126(\u2126) = 0 and p\\\u2126 \u2248 p\u2032. So when doing importance sampling to estimate our training objective \u2207LG = Epd [\u2207\u03b8 log p\u03b8(x)] with small mini-batches, we are actually doing normalizedweights importance sampling based on p\\\u2126: \u2207LG \u2248 Ep\\\u2126 [rD(x)\u2207\u03b8 log p\u03b8(x)]/Ep\\\u2126 [rD(x)]. Since the Monte Carlo estimator has much more variance to estimate Ep\u2032 [rD(x)\u2207\u03b8 log p\u03b8(x)] than Ep\\\u2126 [rD(x)\u2207\u03b8 log p\u03b8(x)], in practical mini-batch training settings, we can view that we are doing importance sampling with the distribution p\\\u2126, and this objective has much less variance compared to importance sampling with p\u2032 on rD which has an infinite singularity. This is why estimating Z = Ep\\\u2126 [rD(x)] is important in order to reduce the variance in the mini-batch training setting.\nWhen training auto-regressive models with teacher forcing, a serious problem is exposure bias (Ranzato et al., 2016; Norouzi et al., 2016b; Lamb et al., 2016). Namely, the model is only trained on demonstrated behaviors (real data samples), but we also want it to be trained on free-running behaviors. When we set a positive baseline b > 0, the model first generates m samples, and then tries to adjust the probabilities of each generated samples by trying to reinforce the best behaviors and exclude the worse behaviors relatively to those in the mini-batch."}, {"heading": "3.3. Variance Reduction in MaliGAN", "text": "The proposed renormalized objective in MaliGAN supports much more stable training behavior than the RL objective in a standard GAN. Nevertheless, when the long sequence generation procedure consists of multiple steps of random sampling, we find it is better to further integrate the following advanced variance reduction techniques."}, {"heading": "3.3.1. MONTE CARLO TREE SEARCH", "text": "Instead of using the same weight for all time steps in one sample, we use the following formula which is well known in the RL literature:\nEp\u03b8 [rD(x)\u2207p(x)] = Ep\u03b8 [ L\u2211 t=1 Q(at, st)\u2207p\u03b8(at|st)]\nwhere Q(a, s) stands for the \u201cexpected total reward\u201d given by rD = D1\u2212D of generating token a given previous generation s, which can be estimated with, e.g., Monte Carlo tree search (MCTS, Silver et al. (2016)).\nThus, following the gradient estimator presented in Theorem 3.1, we derive another gradient estimator:\n\u2207LG(\u03b8) \u2248 \u2211 i Li\nm \u2211 Q(ait, s i t) m,Li\u2211 i,t Q(ait, s i t)\u2207 log p\u03b8(ait|sit)\nwhere m is the size of the mini-batch. Using Monte Carlo tree search brings in several benefits. First, it allows different steps of the generated sample to be adjusted with different weights. Second, it gives us a more stable estimator of the partition function Z. Both of these two properties can dramatically reduce the variance of our proposed estimator."}, {"heading": "3.3.2. MIXED MLE-MALI TRAINING", "text": "When dealing with long sequences, the above model may result in accumulated variance. To alleviate the issue, we significantly reduce the variance by clamping the input using the training data for N time steps, and switch to free running mode for the remaining T \u2212 N time steps. Then during our training procedure, inspired from Ranzato et al. (2016), we slowly move N from T towards 0.\nThe training objective is equivalent to setting q in the last\nsection to:\nq(x0, x1, \u00b7 \u00b7 \u00b7xL) = pd(x0, \u00b7 \u00b7 \u00b7xN )q(xN+1, \u00b7 \u00b7 \u00b7xL|x0, \u00b7 \u00b7 \u00b7xN )\nWe also assume D is trained on the real samples and fake samples generated by\npf (x0, \u00b7 \u00b7 \u00b7xL) = pd(x0, \u00b7 \u00b7 \u00b7xN )p\u03b8(xN+1, \u00b7 \u00b7 \u00b7xL|x0, \u00b7 \u00b7 \u00b7xN )\nLet x\u2264N = (x0, x1, \u00b7 \u00b7 \u00b7xN ),x>N = (xN+1, \u00b7 \u00b7 \u00b7xL), we have:\n\u2207LG =Eq[\u2207 log p\u03b8(x)] =Epd [\u2207 log p\u03b8(x\u2264N )] + Eq[\u2207 log p\u03b8(x>N |x<N )] =Epd [\u2207 log p\u03b8(x0, x1, \u00b7 \u00b7 \u00b7xT )]\n+ 1\nZ Ep\u03b8 [ L\u2211 t=N+1 rD(x)\u2207 log p\u03b8(at|st)]\nFor each sample xi from the real data batch, if it has length larger than N , we fix the first N words of xi, and then sample n times from our model till the end of the sequence, and get n samples {xi,j}nj=1.\nWe then have the following series of mini-batch estimators for each 0 \u2264 N \u2264 T :\n\u2207LNG \u2248 m,n\u2211\ni=1,j=1\n( rD(xi,j)\u2211 j rD(xi,j) \u2212 b)\u2207 log p\u03b8(x>Ni,j |x \u2264N i )\n+ 1\nm m\u2211 i=1 N\u2211 t=0 p\u03b8(a i t|sit) = EN (xi,j)\n(4)\nOne difference is that in this model, we normalize the coefficients rD(xi,j) based only on samples generated from a single real data sample xi. The reason of using this trick will be explained in next sub-section.\nWe have the following theorem which guarantees the theoretical property of this estimator.\nTheorem 3.2. When D is correctly trained but not optimal in the sense of Theorem 3.1, when m \u2192 \u221e, we almost surely have \u22000 \u2264 N \u2264 T ,\nEN (xi,j) \u00b7 \u2207\u03b8KL(pd||p\u03b8) > 0 (5)"}, {"heading": "3.3.3. SINGLE REAL DATA BASED RENORMALIZATION", "text": "Many generative models have multiple layers of randomnesses. For example, in auto-regressive models, the samples are generated via multiple sampling steps. Other examples include hierarchical generative models like deep Boltzmann machines and deep belief networks (Salakhutdinov & Hinton, 2009; Hinton, 2009).\nIn these models, high-level random variables are usually responsible for modeling high-level decisions or \u201cmodes\u201d of the probability distribution. Changing them can result in\nmuch larger effects than that from changing low-level variables. Motivated by this observation, in each mini-batch we first draw a mini-batch of samples (e.g. 32) of highlevel latent variables, and then for each high level value we draw a number of low level data samples (e.g. 32). Then we re-estimate the partition function Z from the lowlevel samples that are generated by each high-level samples. Because lower-level sampling has a much smaller variance, the model can receive better gradient signals from the weights provided by the discriminator.\nThis sampling principle is corresponding to applying the mixed MLE-Mali training discussed above in the autoregressive settings. In this case we first sample a few data samples, then fix the firstN words and let the network generate a lot of samples after N as our next mini-batch. We refer this full algorithm to sequential MaliGAN with Mixed MLE Training, which is summarized in Algorithm 2.\nAlgorithm 2 Sequential MaliGAN with Mixed MLE Training Require: A generator p with parameters \u03b8.\nA discriminator D(x) with parameters \u03b8d. Maximum sequence length T , step size K. A baseline b, sampling multiplicity m.\n1: N = T 2: Optional: Pretrain model using pure MLE with some epochs. 3: for number of training iterations do 4: N = N - K 5: for k steps do 6: Sample a minibatch of sequences {yi}mi=1 from pd. 7: While keeping the first N steps the same as {yi}mi=1,\nsample a minibatch of sequences {xi}mi=1 from p\u03b8 from time step N .\n8: Update the discriminator by taking gradient ascend of discriminator loss.\u2211\ni [\u2207\u03b8d logD(yi)] + \u2211 i [\u2207\u03b8d log(1\u2212D(xi))]\n9: end for 10: Sample a minibatch of sequences {xi}mi=1 from pd. 11: For each sample xi with length larger than N in the mini-\nbatch, clamp the generator to the first N words of s, and freely run the model to generate m samples xi,j , j = 1, \u00b7 \u00b7 \u00b7m till the end of the sequence.\n12: Update the generator by applying the mixed MLE-Mali gradient update\n\u2207LNG \u2248 m,n\u2211\ni=1,j=1\n( rD(xi,j)\u2211 j rD(xi,j) \u2212 b)\u2207 log p\u03b8(x>Ni,j |x\u2264Ni )\n+ 1\nm m\u2211 i=1 N\u2211 t=0 p\u03b8(a i t|sit)\n13: end for\nThe reason why doing this single real sample based renormalization is beneficial can be summarized around two elements. First, consider S is a sample from the training set. The first N words S\u2264N should be completed by our model. The conditional distribution pd(S\u2032>N |S\u2264N ) should be much simpler than the full distribution pd. Namely,\npd(S \u2032 >N |S\u2264N ) consists of only one or a few \u201cmodes\u201d. So this renormalization technique can be viewed as trying to train the model on these simpler conditional distributions, which gives more stable gradients.\nSecond, this normalization scheme makes our model robust to mode missing, which is a common failure pattern when training GANs (Che et al., 2016). Single sample based renormalization ensures that for every real sample S, the model can receive a moderately strong training signal for how to perform better on generating S>N conditioned on S\u2264N . However, in batch-wise renormalization as in the basic MaliGAN, this is not possible because there might be some completions S\u2032 with rD(S\u2032) very large, so other training samples in that mini-batch receives very little gradient signals."}, {"heading": "4. Experiments", "text": "To examine the effectiveness of the proposed algorithms, we conduct experiments on three discrete sequence generation tasks. We achieve promising results on all three tasks, including a standard and challenging language modeling task. From the empirical results and the following analysis, we demonstrate the soundness of MaliGAN and show its robustness to overfitting."}, {"heading": "4.1. Discrete MNIST", "text": "We first evaluate MaliGAN on the binarized image generation task for the MNIST hand-written digits dataset, similar with Hjelm et al. (2017). The original datasets have 60,000 and 10,000 samples in the training and testing sets, respectively. We split the training set and randomly selected 10,000 samples for validation. We adopted as the generator a deep convolutional neural network based on the DCGAN architecture (Radford et al., 2015). To generate the discrete samples, we sample from the generator\u2019s output binomial distribution. We adopt Algorithm 1 of MaliGAN for training and use the single latent variable renormalization technique for variance reduction.\nTo compare our proposed MaliGAN with the models trained using the discriminator\u2019s output as a direct reward, we also train a generator with the same network architecture, but use the output of the discriminator as the weight of generated samples. We denote it as the REINFORCE-like model. The comparison results are shown in Figure 1 and Figure 2.\nThe two figures in the first line are training losses of the generator and discriminator from the proposed MaliGAN. We can see the training process of MaliGAN with variance reduction techniques is stable and the loss curve is meaningful. The bottom two figures in Figure 2 are samples generated by the REINFORCE-like model and by Mali-\nGAN. Clearly, the samples generated by MaliGAN have much better visual quality and resemble closely the training data."}, {"heading": "4.2. Poem Generation", "text": "We examine the effectiveness of our model on a Chinese poem generation task. Typically, there are two genres of Chinese poems. We refer with Poem-5 and Poem-7 to those consisting of 5 or 7 Chinese characters each in a short sentence, respectively. We use the dataset provided in (Zhang & Lapata, 2014), and split them in the standard way 1.\nThe generator is a one-layer LSTM (Hochreiter & Ju\u0308rgen Schmidhuber, 1997) with 32 hidden units for Poem-5 and 100 for Poem-7. Our discriminators are two-layer BiLSTMs with 32 hidden neurons. We denote our models trained with Algorithm 1 and Algorithm 2 as MaliGANbasic and MaliGAN-full. We choose two compared models, the auto-regressive model with same architecture but trained with maximum likelihood (MLE), and SeqGAN (Yu et al., 2017). Following Yu et al. (2017), we report the BLEU-2 scores in Table 4.2 (Papineni et al., 2002).\nMaliGAN-full obtained the best BLEU-2 scores on par on both tasks, and MaliGAN-basic was the next best. Clearly, MLE lagged far behind despite the same architecture, which should be attributed to the inherent defect in the MLE teacher-forcing training framework. As pointed by previous researchers Wiseman & Rush (2016), BLEU might not be a proper evaluation metric, we also calculate the Perplexity of these four models, obtaining qualitatively\n1http://homepages.inf.ed.ac.uk/mlap/Data/ EMNLP14/\nsimilar results. The best scores are reported in Table 4.2 and the Perplexity curves are illustrated in Figure 3.\nFrom the above figures, we can see how our models perform during the training procedure. Although with some oscillations, both MaliGAN-basic and MaliGAN-full achieved lower perplexity. Especially on Poem-7 from Figure 3, our proposed models both prevent overfitting when MLE ended up with that. A comparison between the training curve of MaliGAN-basic and that of MaliGAN-full, we can find that the latter has less variance. This demonstrates the effectiveness of the advanced variance reduction techniques in our full model. The peak in the MLE curve on Poem-5 in Figure 3 is, however, unlikely to be a result of overfitting because that MLE \u201crecovered\u201d from it fast and continued to convergence till the end. In fact, we find it harder to train a stable MLE model on Poem-5 than on Poem-7. We conjecture this resulted from the intricate mutual influence between the improper evaluation and the small training data size."}, {"heading": "4.3. Sentence-Level Language Modeling", "text": "We also examine the proposed algorithm on a more challenging task, sentence-level language modeling, which can be considered as a fundamental task with applications to various discrete sequence generation tasks. To explore the possibilities and limitations of our algorithm, we conduct extensive experiments on the standard Penn Treebank (PTB) dataset (Marcus et al., 1993) through parameter searching and model ablations. For evaluation we report sentence-level perplexity, which is the averaged perplexity on all sentences in the test set. For simplicity and efficiency, we adopt a 1-layer GRU (Cho et al., 2014) as our generator, and set the same setting for the baseline model trained with standard teacher forcing(Williams & Zipser, 1989). We use a Bi-directional GRU net-\nwork as our discriminator. To stabilize training and provide good initialization for the generator, we first pre-train our generator on the training set using teacher forcing, then we train two models, MaliGAN-basic and MaliGANfull. MaliGAN-basic is trained with Algorithm 1 without MCTS. MaliGAN-full is trained by Algorithm 2 with all the variance reduction techniques included.\nNote that the computational cost of MCTS is very large, so we remove all sentences longer than 35 words in the training set. We set N = 30 and K = 5 at the beginning of the training and pre-train our discriminator to make it reliable enough to provide informative and correct signals for the generator. The perplexity shown in Table 4.3 is achieved by our best performing model, which has 200 hidden neurons and 200 dimensions for word embeddings.\nFrom Table 4.3 we can see, the simplest model trained by MaliGAN reduced the perplexity of the baseline effectively. Both the basic and the full model, i.e., MaliGANbasic and MaliGAN-full obtained a notably lower perplexity compared with the MLE model. Although the PTB dataset is much more difficult, we obtain results consistent with Table 4.2. It is encouraging to see that our model is more robust to overfitting in consideration of the relative small size of the PTB data. These results strengthen our belief to realize our algorithm on even larger datasets, which we leave as a future work.\nThe positive result again demonstrates the effectiveness of MaliGAN, whose primary component is the novel optimization objective we propose in Eq. 2. Besides, we also gain insights from the model ablation tests about the advanced variance reduction techniques provided in Section 3.3. Combined with the Perplexity curve in Figure 3, we can see that with advanced techniques, MaliGANfull performed in a more stable way during training and can to some extent achieve lower perplexity scores than MaliGAN-basic. We believe these fruitful techniques will be beneficial in other similar problem settings."}, {"heading": "5. Related Work", "text": "To improve the performance of discrete auto-regressive models, some researchers aim to tackle the exposure bias problem, which is discussed detailed in (Ranzato et al., 2016; Serban et al., 2016; Wiseman & Rush, 2016). The problem occurs when the training algorithm prohibits models to be exposed to their own predictions during training.\nThe second issue is the discrepancy between the objective during training and the evaluation metric during testing, which is analyzed in Ranzato et al. (2016) and then summarized as Loss-Evaluation Mismatch by Wiseman & Rush (2016). Typically, the objectives in training auto-regressive models are to maximize the word-level probabilities, while in test-time, we often evaluate the models using sequencelevel metrics, such as BLEU (Papineni et al., 2002). To alleviate these two issues, the most straightforward way is to add the evaluation metrics into the objective in the training phase. Because these metrics are often discrete which cannot be utilized through standard back-propagation, researchers generally seek help from reinforcement learning. Ranzato et al. (2016) exploits REINFORCE algorithm (Williams, 1992) and proposes several model variants to well situate the algorithm in text generation applications. Liu et al. (2016) shares similar idea and directly optimizes image caption metrics through policy gradient methods (Igel, 2005). There exists a third issue, namely Label Bias, especially in sequence-to-sequence learning framework, which obstacles the MLE trained models to be optimized globally (Andor et al., 2016; Wiseman & Rush, 2016)\nTo addresses the abovementioned issues in training autoregressive models, we propose to formulate the problem under the setting of generative adversarial networks. Initially proposed by Goodfellow et al. (2014), generative adversarial network (GAN) has attracted a lot of attention because it provides a powerful framework to generate promising samples through a min-max game. Researchers have successfully applied GAN to generate promising images conditionally (Mirza & Osindero, 2014; Reed et al., 2016; Zhang et al., 2016b) and unconditionally (Radford et al., 2015; Nguyen et al., 2016), to realize image manipulation and super-resolution (Zhu et al., 2016; S\u00f8nderby et al., 2017; Ledig et al., 2016), and to produce video sequences (Mathieu et al., 2016; Zhou & Berg, 2016; Saito & Matsumoto, 2016). Despite these successes, the feasibility and advantage on applying GAN to text generation are restrictedly explored yet noteworthy.\nIt is appealing to generate discrete sequences using GAN as discussed above. The generative models are able to utilize the discriminator\u2019s output to make up the information of its own distribution, which is inaccessible if trained by teacher forcing (Williams & Zipser, 1989; Ranzato et al., 2016). However, it is nontrivial to train GAN on discrete data due to its discontinuity nature. The instability inherent in GAN training makes things even worse (Salimans et al., 2016; Che et al., 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017). Lamb et al. (2016) exploits adversarial domain adaption to regularize the training of recurrent neural networks. Yu et al. (2017) applies GAN to discrete sequence generation by directly optimizing the discrete dis-\ncriminator\u2019s rewards. They adopt Monte Carlo tree search technique (Silver et al., 2016). Similar technique has been employed in Li et al. (2017) which improves response generation by using adversarial learning.\nIn Bornschein & Bengio (2015), which inspired us, the authors propose a way of doing mini-batch reweighting when training latent variable models with discrete variables. However, they make use of inference network which are infeasible in the GAN setting.\nOur work is also closely related to Norouzi et al. (2016b). In Norouzi et al. (2016b), they propose to work with the objective KL(pd||p\u03b8) in a conditional generation setting. In this case, the situation is similar with ours because rewards such as BLEU scores are available. However, conditional generation metrics such as BLEU scores are decomposable to each time steps, so this property can make them able to directly sample from the augmented distributions, which is not possible for sequence-level GANs, e.g., language modeling. So we have to use importance sampling to train the model."}, {"heading": "6. Discussions and Future Work", "text": "In spite of their great popularity on continuous datasets such as images, GANs haven\u2019t yet achieved an equivalent success in discrete domains such as natural language processing. We observed that the main cause of this gap is that while the discriminator can almost perfectly discriminate the good samples from the bad ones, it is notoriously difficult to pass this information to the generator due to the difficulty of credit assignment through discrete computation and inherent instability of RL algorithms applied to dynamic environments with sparse reward.\nIn this work, we take a different approach. We start first from the maximum likelihood training objective KL(pd||p\u03b8), and then use importance sampling combined with the discriminator output to derive a novel training objective. We argue that although this objective looks similar to the objective used in reinforcement learning, the normalization in fact does reduce the variance of the estimator by ignoring the region \u2126 in the data space around the singularity of rD in which the generator p\u03b8 has almost zero probability to get samples from. Namely, by estimating the partition function Z using samples, we are approximately doing normalized importance sampling with another distribution p\\\u2126 which has much lower variance c.f. Section 3.2. Practically, this single real sample normalization process combined with mixed training (Ranzato et al., 2016) successfully avoided the missing mode problem by providing equivalent training signal for each mode.\nBesides successfully reducing the variances of normal reinforcement learning algorithms, our algorithm is surpris-\ningly robust to overfitting. Teacher forcing is prone to overfit, because by maximizing the likelihood of the training data, the model can easily fit not only the regularities but also the noise in the data. However in our model, if the generator tries to fit too much noise in the data, the generated sample will not look good and hopefully the discriminator will be able to capture the differences between the generated and the real samples very easily.\nAs for future work, we are going to train the model on large datasets such as Google\u2019s one billion words (Chelba et al., 2014) and on conditional generation cases such as dialogue generation."}], "references": [{"title": "Globally normalized transition-based neural networks", "author": ["Andor", "Daniel", "Alberti", "Chris", "Weiss", "David", "Severyn", "Aliaksei", "Presta", "Alessandro", "Ganchev", "Kuzman", "Petrov", "Slav", "Collins", "Michael"], "venue": null, "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Towards principled methods for training generative adversarial networks", "author": ["Arjovsky", "Mart\u0131\u0301n", "Bottou", "L\u00e9on"], "venue": null, "citeRegEx": "Arjovsky et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Reweighted wakesleep", "author": ["Bornschein", "J\u00f6rg", "Bengio", "Yoshua"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Bornschein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bornschein et al\\.", "year": 2015}, {"title": "Generating sentences from a continuous space", "author": ["Bowman", "Samuel R", "Vilnis", "Luke", "Vinyals", "Oriol", "Dai", "Andrew M", "Jozefowicz", "Rafal", "Bengio", "Samy"], "venue": "arXiv preprint arXiv:1511.06349,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Mode regularized generative adversarial networks", "author": ["Che", "Tong", "Li", "Yanran", "Jacob", "Athul Paul", "Bengio", "Yoshua", "Wenjie"], "venue": "arXiv preprint arXiv:1612.02136,", "citeRegEx": "Che et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Che et al\\.", "year": 2016}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Chelba", "Ciprian", "Mikolov", "Tomas", "Schuster", "Mike", "Ge", "Qi", "Brants", "Thorsten", "Koehn", "Phillipp"], "venue": "CoRR, abs/1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho", "Kyunghyun", "Van Merri\u00ebnboer", "Bart", "Bahdanau", "Dzmitry", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Boundary-seeking generative adversarial networks. 2017", "author": ["Hjelm", "R Devon", "Jacob", "Athul", "Che", "Tong", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Hjelm et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hjelm et al\\.", "year": 2017}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["C. Igel"], "venue": null, "citeRegEx": "Igel,? \\Q2005\\E", "shortCiteRegEx": "Igel", "year": 2005}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["Lamb", "Alex", "Goyal", "Anirudh", "Zhang", "Ying", "Saizheng", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Lamb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["Ledig", "Christian", "Theis", "Lucas", "Husz\u00e1r", "Ferenc", "Caballero", "Jose", "Aitken", "Andrew", "Tejani", "Alykhan", "Totz", "Johannes", "Wang", "Zehan", "Shi", "Wenzhe"], "venue": "arXiv preprint arXiv:1609.04802,", "citeRegEx": "Ledig et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ledig et al\\.", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Shi", "Tianlin", "Ritter", "Alan", "Jurafsky", "Dan"], "venue": "arXiv preprint arXiv:1701.06547,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Optimization of image description metrics using policy gradient methods", "author": ["Liu", "Siqi", "Zhu", "Zhenhai", "Ye", "Ning", "Guadarrama", "Sergio", "Murphy", "Kevin"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Mathieu", "Michael", "Couprie", "Camille", "LeCun", "Yann"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Mathieu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2016}, {"title": "Neural variational inference for text processing", "author": ["Miao", "Yishu", "Yu", "Lei", "Blunsom", "Phil"], "venue": null, "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["Mirza", "Mehdi", "Osindero", "Simon"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "author": ["Nguyen", "Anh", "Yosinski", "Jason", "Bengio", "Yoshua", "Dosovitskiy", "Alexey", "Clune", "Jeff"], "venue": "arXiv preprint arXiv:1612.00005,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Norouzi", "Mohammad", "Bengio", "Samy", "Chen", "Zhifeng", "Jaitly", "Navdeep", "Schuster", "Mike", "Wu", "Yonghui", "Schuurmans", "Dale"], "venue": "In NIPS,", "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["Norouzi", "Mohammad", "Bengio", "Samy", "Jaitly", "Navdeep", "Schuster", "Mike", "Wu", "Yonghui", "Schuurmans", "Dale"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Norouzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Radford", "Alec", "Metz", "Luke", "Chintala", "Soumith"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "Radford et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Sequence level training with recurrent neural networks", "author": ["Ranzato", "Marc\u2019Aurelio", "Chopra", "Sumit", "Auli", "Michael", "Zaremba", "Wojciech"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Ranzato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Generative adversarial text-to-image synthesis", "author": ["Reed", "Scott", "Akata", "Zeynep", "Yan", "Xinchen", "Logeswaran", "Lajanugen", "Schiele", "Bernt", "Lee", "Honglak"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Reed et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Temporal generative adversarial nets", "author": ["Saito", "Masaki", "Matsumoto", "Eiichi"], "venue": "arXiv preprint arXiv:1611.06624,", "citeRegEx": "Saito et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Saito et al\\.", "year": 2016}, {"title": "Deep boltzmann machines", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey E"], "venue": "In AISTATS,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Improved techniques for training", "author": ["Salimans", "Tim", "Goodfellow", "Ian J", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "gans. CoRR,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Building end-toend dialogue systems using generative hierarchical neural network models", "author": ["Serban", "Iulian V", "Sordoni", "Alessandro", "Bengio", "Yoshua", "Courville", "Aaron", "Pineau", "Joelle"], "venue": "In AAAI-16,", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Sutskever", "Ilya", "Lillicrap", "Timothy", "Leach", "Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis"], "venue": "search. Nature,", "citeRegEx": "Sutskever et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2016}, {"title": "Amortised map inference for image super-resolution", "author": ["S\u00f8nderby", "Casper Kaae", "Caballero", "Jose", "Theis", "Lucas", "Shi", "Wenzhe", "Husz\u00e1r", "Ferenc"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "S\u00f8nderby et al\\.,? \\Q2017\\E", "shortCiteRegEx": "S\u00f8nderby et al\\.", "year": 2017}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Sordoni", "Alessandro", "Galley", "Michel", "Auli", "Michael", "Brockett", "Chris", "Ji", "Yangfeng", "Mitchell", "Margaret", "Nie", "Jian-Yun", "Gao", "Jianfeng", "Dolan", "William B"], "venue": "In HLT-NAACL,", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["Williams", "Ronald J", "Zipser", "David"], "venue": "Neural computation,", "citeRegEx": "Williams et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1989}, {"title": "Sequence-tosequence learning as beam-search optimization", "author": ["Wiseman", "Sam", "Rush", "Alexander M"], "venue": "Proceeddings of Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Wiseman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wiseman et al\\.", "year": 2016}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["Yu", "Lantao", "Zhang", "Weinan", "Wang", "Jun", "Yong"], "venue": "In Thirty-First AAAI Conference on Artificial Intelligence", "citeRegEx": "Yu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2017}, {"title": "Variational neural machine translation", "author": ["Zhang", "Biao", "Xiong", "Deyi", "Su", "Jinsong", "Duan", "Hong", "Min"], "venue": "In EMNLP,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "author": ["Zhang", "Han", "Xu", "Tao", "Li", "Hongsheng", "Shaoting", "Huang", "Xiaolei", "Wang", "Xiaogang", "Metaxas", "Dimitris"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Chinese poetry generation with recurrent neural networks", "author": ["Zhang", "Xingxing", "Lapata", "Mirella"], "venue": "In EMNLP, pp", "citeRegEx": "Zhang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2014}, {"title": "Learning temporal transformations from time-lapse videos", "author": ["Zhou", "Yipin", "Berg", "Tamara L"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["Zhu", "Jun-Yan", "Kr\u00e4henb\u00fchl", "Philipp", "Shechtman", "Eli", "Efros", "Alexei A"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "Zhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 32, "context": "However, the generated sentences are often unsatisfactory (Sordoni et al., 2015; Bowman et al., 2015; Serban et al., 2017; Wiseman & Rush, 2016).", "startOffset": 58, "endOffset": 144}, {"referenceID": 4, "context": "However, the generated sentences are often unsatisfactory (Sordoni et al., 2015; Bowman et al., 2015; Serban et al., 2017; Wiseman & Rush, 2016).", "startOffset": 58, "endOffset": 144}, {"referenceID": 4, "context": "For example, they often lack of consistency in long-term semantics and have less coherence in high-level topics and syntactics (Bowman et al., 2015; Zhang et al., 2016a).", "startOffset": 127, "endOffset": 169}, {"referenceID": 24, "context": "generate discrete sequences, it is popular to adopt autoregressive models through teacher forcing (Williams & Zipser, 1989) which, nevertheless, causes the exposure bias problem (Ranzato et al., 2016).", "startOffset": 178, "endOffset": 200}, {"referenceID": 24, "context": "It prohibits the trained model to take advantage of learning in the the context of its previous generated words to make the next prediction, resulting in a bias and difficulty in approaching the true underlying distribution (Ranzato et al., 2016; Bengio et al., 2015).", "startOffset": 224, "endOffset": 267}, {"referenceID": 2, "context": "It prohibits the trained model to take advantage of learning in the the context of its previous generated words to make the next prediction, resulting in a bias and difficulty in approaching the true underlying distribution (Ranzato et al., 2016; Bengio et al., 2015).", "startOffset": 224, "endOffset": 267}, {"referenceID": 18, "context": "usual RNNs, LSTMs or GRUs) on multiple tasks (Serban et al., 2017; Miao et al., 2016; Zhang et al., 2016a).", "startOffset": 45, "endOffset": 106}, {"referenceID": 8, "context": "An alternative and attractive solution to training autoregressive models is using generative adversarial networks (GAN) (Goodfellow et al., 2014).", "startOffset": 120, "endOffset": 145}, {"referenceID": 9, "context": "The procedure was discovered independently from us by Hjelm et al. (2017) in the context of image generation.", "startOffset": 54, "endOffset": 74}, {"referenceID": 8, "context": "In this work, we aim to generate discrete data, especially discrete sequential data, under the GAN setting (Goodfellow et al., 2014).", "startOffset": 107, "endOffset": 132}, {"referenceID": 36, "context": "Though the dedicated pre-training and variance reduction mechanisms help (Yu et al., 2017), the RL algorithm based on the moving reward signal still seems very unstable and does not work on large scale datasets.", "startOffset": 73, "endOffset": 90}, {"referenceID": 9, "context": "This importance sampling procedure was discovered independently from us by (Hjelm et al., 2017).", "startOffset": 75, "endOffset": 95}, {"referenceID": 24, "context": "When training auto-regressive models with teacher forcing, a serious problem is exposure bias (Ranzato et al., 2016; Norouzi et al., 2016b; Lamb et al., 2016).", "startOffset": 94, "endOffset": 158}, {"referenceID": 12, "context": "When training auto-regressive models with teacher forcing, a serious problem is exposure bias (Ranzato et al., 2016; Norouzi et al., 2016b; Lamb et al., 2016).", "startOffset": 94, "endOffset": 158}, {"referenceID": 24, "context": "Then during our training procedure, inspired from Ranzato et al. (2016), we slowly move N from T towards 0.", "startOffset": 50, "endOffset": 72}, {"referenceID": 5, "context": "Second, this normalization scheme makes our model robust to mode missing, which is a common failure pattern when training GANs (Che et al., 2016).", "startOffset": 127, "endOffset": 145}, {"referenceID": 23, "context": "We adopted as the generator a deep convolutional neural network based on the DCGAN architecture (Radford et al., 2015).", "startOffset": 96, "endOffset": 118}, {"referenceID": 9, "context": "We first evaluate MaliGAN on the binarized image generation task for the MNIST hand-written digits dataset, similar with Hjelm et al. (2017). The original datasets have 60,000 and 10,000 samples in the training and testing sets, respectively.", "startOffset": 121, "endOffset": 141}, {"referenceID": 36, "context": "We choose two compared models, the auto-regressive model with same architecture but trained with maximum likelihood (MLE), and SeqGAN (Yu et al., 2017).", "startOffset": 134, "endOffset": 151}, {"referenceID": 36, "context": "We choose two compared models, the auto-regressive model with same architecture but trained with maximum likelihood (MLE), and SeqGAN (Yu et al., 2017). Following Yu et al. (2017), we report the BLEU-2 scores in Table 4.", "startOffset": 135, "endOffset": 180}, {"referenceID": 36, "context": "The result of SeqGAN is directly taken from (Yu et al., 2017).", "startOffset": 44, "endOffset": 61}, {"referenceID": 16, "context": "To explore the possibilities and limitations of our algorithm, we conduct extensive experiments on the standard Penn Treebank (PTB) dataset (Marcus et al., 1993) through parameter searching and model ablations.", "startOffset": 140, "endOffset": 161}, {"referenceID": 7, "context": "For simplicity and efficiency, we adopt a 1-layer GRU (Cho et al., 2014) as our generator, and set the same setting for the baseline model trained with standard teacher forcing(Williams & Zipser, 1989).", "startOffset": 54, "endOffset": 72}, {"referenceID": 24, "context": "To improve the performance of discrete auto-regressive models, some researchers aim to tackle the exposure bias problem, which is discussed detailed in (Ranzato et al., 2016; Serban et al., 2016; Wiseman & Rush, 2016).", "startOffset": 152, "endOffset": 217}, {"referenceID": 29, "context": "To improve the performance of discrete auto-regressive models, some researchers aim to tackle the exposure bias problem, which is discussed detailed in (Ranzato et al., 2016; Serban et al., 2016; Wiseman & Rush, 2016).", "startOffset": 152, "endOffset": 217}, {"referenceID": 11, "context": "(2016) shares similar idea and directly optimizes image caption metrics through policy gradient methods (Igel, 2005).", "startOffset": 104, "endOffset": 116}, {"referenceID": 0, "context": "There exists a third issue, namely Label Bias, especially in sequence-to-sequence learning framework, which obstacles the MLE trained models to be optimized globally (Andor et al., 2016; Wiseman & Rush, 2016)", "startOffset": 166, "endOffset": 208}, {"referenceID": 21, "context": "The second issue is the discrepancy between the objective during training and the evaluation metric during testing, which is analyzed in Ranzato et al. (2016) and then summarized as Loss-Evaluation Mismatch by Wiseman & Rush (2016).", "startOffset": 137, "endOffset": 159}, {"referenceID": 21, "context": "The second issue is the discrepancy between the objective during training and the evaluation metric during testing, which is analyzed in Ranzato et al. (2016) and then summarized as Loss-Evaluation Mismatch by Wiseman & Rush (2016). Typically, the objectives in training auto-regressive models are to maximize the word-level probabilities, while in test-time, we often evaluate the models using sequencelevel metrics, such as BLEU (Papineni et al.", "startOffset": 137, "endOffset": 232}, {"referenceID": 21, "context": "The second issue is the discrepancy between the objective during training and the evaluation metric during testing, which is analyzed in Ranzato et al. (2016) and then summarized as Loss-Evaluation Mismatch by Wiseman & Rush (2016). Typically, the objectives in training auto-regressive models are to maximize the word-level probabilities, while in test-time, we often evaluate the models using sequencelevel metrics, such as BLEU (Papineni et al., 2002). To alleviate these two issues, the most straightforward way is to add the evaluation metrics into the objective in the training phase. Because these metrics are often discrete which cannot be utilized through standard back-propagation, researchers generally seek help from reinforcement learning. Ranzato et al. (2016) exploits REINFORCE algorithm (Williams, 1992) and proposes several model variants to well situate the algorithm in text generation applications.", "startOffset": 137, "endOffset": 775}, {"referenceID": 13, "context": "Liu et al. (2016) shares similar idea and directly optimizes image caption metrics through policy gradient methods (Igel, 2005).", "startOffset": 0, "endOffset": 18}, {"referenceID": 25, "context": "Researchers have successfully applied GAN to generate promising images conditionally (Mirza & Osindero, 2014; Reed et al., 2016; Zhang et al., 2016b) and unconditionally (Radford et al.", "startOffset": 85, "endOffset": 149}, {"referenceID": 23, "context": ", 2016b) and unconditionally (Radford et al., 2015; Nguyen et al., 2016), to realize image manipulation and super-resolution (Zhu et al.", "startOffset": 29, "endOffset": 72}, {"referenceID": 20, "context": ", 2016b) and unconditionally (Radford et al., 2015; Nguyen et al., 2016), to realize image manipulation and super-resolution (Zhu et al.", "startOffset": 29, "endOffset": 72}, {"referenceID": 41, "context": ", 2016), to realize image manipulation and super-resolution (Zhu et al., 2016; S\u00f8nderby et al., 2017; Ledig et al., 2016), and to produce video sequences (Mathieu et al.", "startOffset": 60, "endOffset": 121}, {"referenceID": 31, "context": ", 2016), to realize image manipulation and super-resolution (Zhu et al., 2016; S\u00f8nderby et al., 2017; Ledig et al., 2016), and to produce video sequences (Mathieu et al.", "startOffset": 60, "endOffset": 121}, {"referenceID": 13, "context": ", 2016), to realize image manipulation and super-resolution (Zhu et al., 2016; S\u00f8nderby et al., 2017; Ledig et al., 2016), and to produce video sequences (Mathieu et al.", "startOffset": 60, "endOffset": 121}, {"referenceID": 17, "context": ", 2016), and to produce video sequences (Mathieu et al., 2016; Zhou & Berg, 2016; Saito & Matsumoto, 2016).", "startOffset": 40, "endOffset": 106}, {"referenceID": 8, "context": "Initially proposed by Goodfellow et al. (2014), generative adversarial network (GAN) has attracted a lot of attention because it provides a powerful framework to generate promising samples through a min-max game.", "startOffset": 22, "endOffset": 47}, {"referenceID": 24, "context": "The generative models are able to utilize the discriminator\u2019s output to make up the information of its own distribution, which is inaccessible if trained by teacher forcing (Williams & Zipser, 1989; Ranzato et al., 2016).", "startOffset": 173, "endOffset": 220}, {"referenceID": 28, "context": "The instability inherent in GAN training makes things even worse (Salimans et al., 2016; Che et al., 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017).", "startOffset": 65, "endOffset": 154}, {"referenceID": 5, "context": "The instability inherent in GAN training makes things even worse (Salimans et al., 2016; Che et al., 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017).", "startOffset": 65, "endOffset": 154}, {"referenceID": 1, "context": "The instability inherent in GAN training makes things even worse (Salimans et al., 2016; Che et al., 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017).", "startOffset": 65, "endOffset": 154}, {"referenceID": 1, "context": ", 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017). Lamb et al. (2016) exploits adversarial domain adaption to regularize the training of recurrent neural networks.", "startOffset": 33, "endOffset": 76}, {"referenceID": 1, "context": ", 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017). Lamb et al. (2016) exploits adversarial domain adaption to regularize the training of recurrent neural networks. Yu et al. (2017) applies GAN to discrete sequence generation by directly optimizing the discrete discriminator\u2019s rewards.", "startOffset": 33, "endOffset": 187}, {"referenceID": 1, "context": ", 2016; Arjovsky & Bottou, 2017; Arjovsky et al., 2017). Lamb et al. (2016) exploits adversarial domain adaption to regularize the training of recurrent neural networks. Yu et al. (2017) applies GAN to discrete sequence generation by directly optimizing the discrete discriminator\u2019s rewards. They adopt Monte Carlo tree search technique (Silver et al., 2016). Similar technique has been employed in Li et al. (2017) which improves response generation by using adversarial learning.", "startOffset": 33, "endOffset": 416}, {"referenceID": 21, "context": "Our work is also closely related to Norouzi et al. (2016b). In Norouzi et al.", "startOffset": 36, "endOffset": 59}, {"referenceID": 21, "context": "Our work is also closely related to Norouzi et al. (2016b). In Norouzi et al. (2016b), they propose to work with the objective KL(pd||p\u03b8) in a conditional generation setting.", "startOffset": 36, "endOffset": 86}, {"referenceID": 24, "context": "Practically, this single real sample normalization process combined with mixed training (Ranzato et al., 2016) successfully avoided the missing mode problem by providing equivalent training signal for each mode.", "startOffset": 88, "endOffset": 110}, {"referenceID": 6, "context": "As for future work, we are going to train the model on large datasets such as Google\u2019s one billion words (Chelba et al., 2014) and on conditional generation cases such as dialogue generation.", "startOffset": 105, "endOffset": 126}], "year": 2017, "abstractText": "Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of backpropagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator\u2019s output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.", "creator": "LaTeX with hyperref package"}}}