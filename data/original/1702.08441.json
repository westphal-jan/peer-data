{"id": "1702.08441", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2017", "title": "Monte Carlo Action Programming", "abstract": "This paper proposes Monte Carlo Action Programming, a programming language framework for autonomous systems that act in large probabilistic state spaces with high branching factors. It comprises formal syntax and semantics of a nondeterministic action programming language. The language is interpreted stochastically via Monte Carlo Tree Search. Effectiveness of the approach is shown empirically.", "histories": [["v1", "Sat, 25 Feb 2017 11:48:50 GMT  (691kb,D)", "http://arxiv.org/abs/1702.08441v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.PL", "authors": ["lenz belzner"], "accepted": false, "id": "1702.08441"}, "pdf": {"name": "1702.08441.pdf", "metadata": {"source": "CRF", "title": "Monte Carlo Action Programming", "authors": ["Lenz Belzner"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Online Planning, Action Programming, MCTS"}, {"heading": "1 Introduction", "text": "We consider the problem of sequential decision making in highly complex and changing domains. These domains are characterized by large probabilistic state spaces and high branching factors. Additional challenges for system design are occurrence of unexpected events and/or changing goals at runtime.\nA state of the art candidate for responding to this challenge is behavior synthesis with online planning [1,2,3]. Here, a planning agent evaluates possible behavioral choices w.r.t. current situation and background knowledge at runtime. At some point, it acts according to this evaluation and observes the actual outcome of the action. Planning continues, incorporating the observed result. Planning performance directly correlates with search space cardinality.\nThis paper introduces Monte Carlo Action Programming (MCAP) to reduce search space cardinality through specification of heuristic knowledge in the form of procedural nondeterministic programs. MCAP is based on stochastic interpretation of nondeterministic action programs by Monte Carlo Tree Search (MCTS) [1,4]. Combining search space constraints and stochastic interpretation enables program evaluation in large probabilistic domains with high branching factors. From the perspective of online planning, MCAP provides a formal nondeterministic action programming language that allows to specify plan sketches for autonomous systems. From the perspective of action programming, MCAP introduces stochastic interpretation with MCTS. This enables effective program interpretation in very large, complex domains.\nWe will discuss MCTS and action programming in Section 2. Section 3 introduces MCAP. In Section 4 we empirically compare MCTS and MCAP specifications for online planning. We conclude and sketch venues for further research in Section 5.\nar X\niv :1\n70 2.\n08 44\n1v 1\n[ cs\n.A I]\n2 5\nFe b\n20 17"}, {"heading": "2 Related Work", "text": "We briefly review Monte Carlo Tree Search in Section 2.1 and action programming in Section 2.2."}, {"heading": "2.1 Monte Carlo Tree Search", "text": "Monte Carlo Tree Search (MCTS) is a framework for statistical search in very large state spaces with high branching factors based on a generative model of the domain (i.e. a simulation). It yields good performance even without heuristic assessment of intermediate states in the search space. The MCTS framework originated from research in computer Go [4,5]. The game Go exposes the mentioned characteristics. Also, not many good heuristics are known for Go. Nevertheless, specialized Go programs based on the MCTS algorithm are able to play on the niveau of a human professional player [6]. MCTS is also commonly used in autonomous planning [2,3] and has been applied successfully to a huge number of other search tasks [1].\nMCTS adds nodes to the tree iteratively. Nodes represent states and store metadata about search paths that lead through them. Gathered metadata comprises mean reward (i.e. node value) and the number of searches that passed through the node. It enables assessment of exploration vs. exploitation: Should search be directed to already explored, promising parts of the search space? Or should it gather information about previously unexplored areas?\nFigure 1 shows the basic principle of MCTS. Based on node information, MCTS selects an action w.r.t. a given tree policy. The successor state is determined by simulating action execution. Selection is repeated as long as simulation leads to a state that is represented by a node in the tree. Otherwise, a new node\nrepresenting the simulated outcome state is added to the tree (expansion). Then, a default policy is executed (e.g. uniform random action execution). Gathered reward is stored in the new node (simulation or rollout). This gives a first estimation of the new node\u2019s value. Finally, the rollout\u2019s value is backpropagated through the tree and the corresponding node values are updated. MCTS repeats this procedure iteratively. Algorithm 1 shows the general MCTS approach in pseudocode. Here, v0 is the root node of the search tree. vl denotes the last node visited by the tree policy. \u2206 is the value of the rollout from vl according to the default policy.\nAlgorithm 1 General MCTS approach [1]\n1: procedure mcts(s0) 2: create root node v0 with state s0 3: while within computational budget do 4: vl \u2190 treepolicy(v0) 5: \u2206\u2190 defaultpolicy(vl) 6: backup(vl,\u2206)\n7: return a(bestchildv0())\nMCTS can be interrupted at any time and yields an estimation of quality for all actions in the root state. The best action (w.r.t. node information) is executed and its real outcome is observed. MCTS continues reusing the tree built so far. Eventually, nodes representing past states are pruned from the tree."}, {"heading": "2.2 Action Programming", "text": "Nondeterministic action programs define sketches for system behavior that are interpreted at runtime, leaving well-defined choices to be made by the system at runtime. Interpreting an action program typically provides a measure of quality for particular instantiations of theses sketches. Concrete traces are then executed w.r.t. to this quality metric.\nWell-established action programming languages are Golog [7,8] and Flux [9]. Each is interpreted w.r.t. a particular formal specification of domain dynamics: The situation calculus and the fluent calculus are concerned with specification of action effects and domain dynamics in first order logic [10,11]. For both Golog and Flux, Prolog interpreters have been implemented.\nThe MCAP framework differs from these formalisms and their respective languages: (a) MCAP does not provide nor require a specific formal representation of domain dynamics. Rather, any form of domain simulation suffices. (b) MCAP does not explore the search space exhaustively. Rather, programs are interpreted stochastically by MCTS. The search space is explored iteratively. Program interpretation is directed to promising areas of the search space based on previous interpretations. Search can be interrupted any time yielding an action recommen-\ndation accounting for current situation and a given program. Recommendation quality depends on the number of simulations used for search [1]."}, {"heading": "3 Monte Carlo Action Programming", "text": "This Section introduces Monte Carlo Action Programming (MCAP), a nondeterministic procedural programming framework for autonomous systems. The main idea of the MCAP framework is to allow to specify behavioral blueprints that leave choices to an agent. A MCAP is a nondeterministic program. MCAP programs are interpreted probabilistically by MCTS. MCAPs constrain the MCTS search space w.r.t. a procedural nondeterministic program."}, {"heading": "3.1 Framework Parameters", "text": "The MCAP framework requires the following specification.\n1. A generative domain model that captures the probability distribution of successor states w.r.t. current state and executed action (Equation 1). The model does not have to be explicit: The framework only requires a simulation that allows to query one particular successor state.\nsimulate : P (S | S \u00d7 A) (1) (2)\n2. A reward function R that encodes the quality of a state w.r.t. system goals (Equation 3).\nR : S \u2192 R (3)\n3. A discount factor \u03b3 \u2208 [0; 1] weights the impact of potential future decision on the current situation. A discount factor of zero means that only immediate consequences of action are considered. A discount factor of one means that all future consequences influence the current decision equally, regardless of their temporal distance. 4. A maximum search depth hmax \u2208 N."}, {"heading": "3.2 Syntax", "text": "Equation 4 defines syntax of the MCAP language. is the empty program, A denotes specified action space, ; is a sequential operator, + is nondeterministic choice, \u2016 denotes interleaving concurrency. Q denotes the query space for conditional evaluation (see Equation 23). ? denotes querying the current execution context. \u25e6 denotes a conditional loop.\nP := A P;P P+P P \u2016 P ?(Q){P} \u00ac?(Q){P} \u25e6 {Q}{P} (4)\nNormal Form We define a normal form Pnorm for MCAPs. Each program in normal form is a choice of programs with an action prefix and any tail program.\nPnorm := \u2211 (A;P) (5)\nEquations 6 to 13 define a term reduction system that ensures transformation of programs to their normal form.\n;p = p (6)\np+ p = p (7)\n(p1 + p2);p = (p1;p) + (p2;p) (8)\np;(p1 + p2) = (p;p1) + (p;p2) (9) p1 \u2016 (p2 + p3) = (p1 \u2016 p2) + (p1 \u2016 p3) (10) (a1;p1) \u2016 (a2;p2) = (a1;(p1 \u2016 (a2;p2)))\n+ (a2;((a1;p1) \u2016 p2)) (11) a1 \u2016 (a2;p) = (a1;a2;p) + (a2;(a1 \u2016 p)) (12)\na1 \u2016 a2 = (a1;a2) + (a2;a1) (13)"}, {"heading": "3.3 Semantics", "text": "This Section formalizes MCAP semantics in the context of MCTS interpretation. Search Tree We introduce a formal representation of the search tree. Its purpose is to accumulate information about computation traces w.r.t. simulation and system action choices. Tree nodes represent states \u2208 S and actions \u2208 A. State nodes VS and action nodes VA alternate (Equations 14 and 15). Nodes contain aggregation of metadata D that guides further search. Aggregated data are visitation count and node value (Equation 16).\nVS \u2286 S \u00d7D \u00d7 2VA (14) VA \u2286 A\u00d7D \u00d7 2VS \u00d7 P (15) D \u2286 N\u00d7 R (16)\nWhile it is possible to use a DAG instead of a tree [12], we will concentrate on the tree setting in this paper for the sake of simplicity.\nFramework Operations Equations 17 to 21 show the functional signatures of MCAP framework operations. We will define each one in the rest of this Section.\nselect : VS \u2192 VA (17) expand : S \u00d7 P \u2192 VS (18) rollout : S \u00d7 P \u00d7 N\u2192 R (19) update : VS \u2192 VS (20) update : VA \u2192 VA (21)\nSelection Equation 22 shows UCB1 action selection. It is a popular instantiation of the MCTS tree policy based on regret minimization [13,14]. q(va) denotes the current value aggregated in the metadata of action node va. #(vs) and #(va) denote the number of searches that visited the corresponding node stored in its metadata (see also Algorithm 2, lines 2 and 10). UCB1 favors actions that expose high value (first term of the sum), and adds a bias towards actions that have not been well explored (second term of the sum). The parameter c is a constant to control the tendency towards exploration.\nselect(vs) = argmaxva\u2208va(vs)\n( q(va) + c \u00b7 \u221a 2 ln #(vs)\n#(va)\n) (22)\nQueries Our framework requires specification of a query representation and a satisfaction function of queries and states to enable conditional computation. Queries Q are evaluated w.r.t. a given state \u2208 S and yield a set of substitutions for query variables (Equation 23). It returns the set of substitutions for variables in the query for which the query holds in the state. In case the query is ground and holds, the set containing the empty substitution {\u2205} is returned. If the query does not hold, it returns the empty set \u2205. We write ` in infix notation and s 6` q \u21d4 s ` q = \u2205.\n`: Q\u00d7 S \u2192 2\u0398 (23)\nInterpretation of MCAPs Expansion of the tree is constrained by a given MCAP through interpreting it w.r.t a given state. The potential program function constrains the search space w.r.t. given action program and current system state. It maps an MCAP and a given state to the set of normalized MCAPs that result from (a) nondeterministic choices and (b) interpretations of queries.\npot : S \u00d7 P \u2192 2Pnorm (24)\nEquations 25 to 31 define MCAP interpretation by the potential program function inductively on the structure of P.\npot (s, ) = \u2205 (25) pot (s, a) = {a; } (26)\npot (s, p;p\u2032) = \u22c3\np\u2032\u2032\u2208pot(s,p)\n(p\u2032\u2032;p\u2032) (27)\npot ( s, \u2211 i pi ) = \u22c3 i pot(s, pi) (28)\npot(s, ?{q}{p}) = \u22c3\n\u03b8\u2208s`q\npot (s, \u03b8(p)) (29)\npot(s,\u00ac?{q}{p}) = { pot(s, p) if s 6` q \u2205 otherwise\n(30)\npot(s, \u25e6{q}{p}) = pot(s, ?{q}{p}; \u25e6 {q}{p}) (31)\nExpansion Equation 32 shows the MCAP expansion mechanism. s \u2208 S denotes the state for which a new node is added. p is the MCAP to be executed in state s. Potential programs pot(s, p) in normal form define the set of action node children for actions a that contain the corresponding tail programs p\u2032. Thus, a MCAP effectively constrains the search space. d0 \u2208 D, d0 = (0, 0) defines initial node metadata.\nexpand(s, p) = (s, d0,va) where va = \u22c3\n(a,p\u2032)\u2208pot(s,p)\n(a, d0, \u2205, p\u2032) (32)\nRollout After expansion a rollout is performed. A number of simulation steps is performed (i.e. until maximum search depth hmax is reached) and the reward for resulting states is aggregated. An MCAP p defines the rollout\u2019s default policy. Actions and corresponding tail programs are selected uniformly random from the set of potential programs in each state s encountered in the rollout.\nrollout(s, p, h) ={ R(s) if h = hmax\nR(s) + \u03b3 \u00b7 rollout(s\u2032, p\u2032, h+ 1) otherwise\nwhere (a, p\u2032) \u223c pot(s, p) \u2227 s\u2032 \u223c simulate(s\u2032|s, a) (33)\nValue Update After a node is expanded its value is determined by a rollout. The newly created value is then incorporated to the search tree by value backpropagation along the search path. In general any kind of value update mechanism is feasible, e.g. a mean update as used by many MCTS variants. MCAP uses dynamic programming (i.e. a Bellman update) for updating node values [15]. An action\u2019s value is the weighted sum of its successor states\u2019 values (Equation 34). A state\u2019s value is the currently obtained reward and the value of the currently optimal action (Equation 35).\nupdate(va) = \u2211\nvs\u2208vs(va)\n#(vs) #(va) v(vs) (34)\nupdate(vs) = R(s(vs)) + max va\u2208va(vs) q(va) (35)\nAlgorithm 2 shows the interplay of selection, aggregation of metadata, simulation, expansion, rollout and value update for Monte Carlo Action Programming.\nAlgorithm 3 shows the integration of MCAP with online planning. While the system is running, a given MCAP is repeatedly evaluated and executed until termination (lines 2 \u2013 4). Evaluation is performed by MCTS until a certain budget is reached (lines 6 \u2013 8). The currently best action w.r.t. MCAP interpretation is determined (line 9). If there is no such action, the program terminated (line 10). Otherwise, the best action is executed and the outcome observed (lines 13\nAlgorithm 2 Monte Carlo Action Programming\nRequire: hmax, R, pot, simulate\n1: procedure mcap(vs, h) 2: #(vs)\u2190 #(vs) + 1 . increase state node count 3: if h = hmax then 4: return . reached maximum search depth 5: if va(vs) = \u2205 then 6: return . no action is available 7: va \u2190 select(vs) . select action node 8: #(va)\u2190 #(va) + 1 . increase action node count 9: s\u2032 \u223c simulate(va) . simulate action outcome 10: if \u2203vs\u2032 \u2208 vs(va) : s(vs\u2032) = s\u2032 then . successor exists 11: mcap(vs\u2032 , h+ 1) . recursive call through the tree 12: va \u2190 update(va) . update action quality 13: vs \u2190 update(vs) . update state value 14: else 15: vs\u2032 \u2190 expand(s\u2032, p(va)) . create successor node 16: r \u2190 rollout(s\u2032, p(va), h) . estimate node value 17: d(vs\u2032)\u2190 (0, r) . set state node metadata 18: vs(va)\u2190 vs(va) \u222a {vs\u2032} . add successor node\nand 14). In case the new state is already represented in the search tree, the corresponding state node is used as new root for further search (lines 15 and 16). Otherwise, a new root node is created (line 18)."}, {"heading": "4 Experimental Evaluation", "text": ""}, {"heading": "4.1 Example Domain", "text": "We introduce the rescue domain as illustrating example. Robots can move around a connected graph of positions and lift or drop victims. The number of victims a robot can carry is limited by its capacity. A position may be on fire, in which case a robot cannot move there. At every time step the fire attribute of a position may change depending on how many of the position\u2019s neighbors are on fire. A safe position never catches fire. The class diagram of the rescue domain is shown in Figure 2. A particular state of the domain is an instantiation of this class diagram.\nPossible system actions are:\n1. Move(R,P ): Robot R moves to target position P if it is connected to the robot\u2019s current position and is not on fire. 2. Extinguish(R,P ): Robot R extinguishes fire at a neighbor position P . 3. Lift(R, V ): Robot R lifts victim V (at same location) if it has capacity left. 4. Drop(R, V ): Robot R drops lifted victim V at the current location. 5. Noop: Does nothing.\nAlgorithm 3 Online MCAP\nRequire: initial state sinit, MCAP pinit, budget\n1: vinit \u2190 expand(sinit, pinit) . initial node 2: while running do 3: online-mcap(vinit)\n4: procedure online-mcap(vs) 5: while budget do 6: mcap(vs) . repeatedly update vs w.r.t. MCAP 7: vmaxa \u2190 argmaxva\u2208va(vs) q(va) . get optimal action 8: if vmaxa = null then 9: return . MCAP terminated 10: execute a(vmaxa ) . execute action 11: observe s\u2032 . observe the outcome 12: if vs\u2032 \u2208 vs(vmaxa ) then . previously considered 13: vs \u2190 vs\u2032 . reuse planning result 14: else . previously unconsidered 15: vs \u2190 expand(vs, p(vmaxa ))"}, {"heading": "4.2 Setup & Results", "text": "Effectiveness of the MCAP framework was evaluated empirically for the rescue domain. A simulation of the domain was used as generative model. Reward R(s) was defined as the number of victims located at safe positions in state s. Also, each victim not burning provided a reward of 0.1. Maximum search depth was set to hmax = 40 and the discount factor was set to \u03b3 = 0.9. Experiments were conducted with randomized initial states, each consisting of twenty positions with 30% connectivity. Three positions were safe, ten victims and ten fires were located randomly on unsafe positions. Robot capacity was set to two. This setup yields a state space containing more than 1019 possible states. Fires ignited or ceased probabilistically at unsafe positions. Actions succeeded or failed probabilistically (p = 0.05). This yields a branching factor of 2 \u00b7 217 for each action.\nIn the experiments using plain MCTS all actions \u2208 A were evaluated at each step. Algorithm 4 shows pseudocode for the program used to determine the action to evaluate in the experiments with MCAP. Both MCTS and MCAP used 1000 playouts at each step for action evaluation.\nAlgorithm 4 Pseudocode of the MCAP used in the experiments\n1: while true do 2: if self.position.safe \u2227 self.victims 6= \u2205 then 3: \u2211 v\u2208self.victims self.drop(v) 4: else if \u00ac(self.pos.safe) \u2227 self.position.victims 6= \u2205 then 5: \u2211 v\u2208self.position.victims self.lift(v) 6: else 7: \u2211 a\u2208A a\nSystem performance was measured with the statistical model checker Multivesta [16]. Two metrics of system behavior with and without MCAP search space constraints were assessed: Ratios of safe victims and burning victims.\nFigure 3 compares the average results for behavior synthesis with plain MCTS and with MCAP within a 0.1 confidence interval. The effect of MCAP search space reduction on system performance can clearly be seen. The configuration making use of online MCAP interpretation achieves larger ratios of safe victims and manages the reduction of burning victim ratios better than the configuration not making use of MCAP. With plain MCTS, search is distracted by low reward regions due to avoiding burning victims. MCAP search identifies high reward regions where victims are saved within the given budget.\nA similar experiment with unexpected events illustrates robustness of the approach. Here, every twenty steps all currently carried victims fell to the ground (i.e. were located at their carrier\u2019s position). Also, fires ignited such that overall at least ten fires were burning immediately after these events. Note that the simulation of the domain used for plain MCTS and MCAP did not simulate these events. The planning system managed to recover from the unexpected situations\nautonomously (Figure 5). As for the basic experiment, the configuration with MCAP performed significantly better that the configuration using plain MCTS.\nIn a third experiment the reward function was changed unexpectedly for the system. Before step 25, a reward is provided exclusively for avoiding burning victims. From step 25 on the reward function from the previous experiments was used, providing reward for safe victims. The planner did not simulate the change of reward when evaluating action traces. MCAP outperformed plain MCTS by reacting more effectively to the change of reward function. Figure 4 shows the results of this experiment."}, {"heading": "5 Conclusion", "text": "This paper proposed Monte Carlo Action Programming, a programming language framework for autonomous systems that act in large probabilistic state spaces. It comprises formal syntax and semantics of a nondeterministic action programming language. The language is interpreted stochastically via Monte Carlo Tree Search. The effectiveness of search space constraint specification in the MCAP framework was shown empirically. Online interpretation of MCAP provides system performance and robustness in the face of unexpected events.\nA possible venue for further research is the extension of MCAP to domains with continuous time and hybrid systems. Here, discrete programs are interpreted w.r.t. continuously evolving domain values [17]. It would also be interesting to evaluate to what extend manual specification techniques as MCAP could be combined with online representation learning (e.g. statistical relational learning [18] and deep learning [19]): How to constrain system behavior if perceptual abstraction is unknown at design time or changes at runtime?"}], "references": [{"title": "A survey of Monte Carlo tree search methods", "author": ["C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton"], "venue": "IEEE Transactions on Computational Intelligence and AI in Games 4(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Trial-based Heuristic Tree Search for Finite Horizon MDPs", "author": ["T. Keller", "M. Helmert"], "venue": "Proceedings of the 23rd International Conference on Automated Planning and Scheduling (ICAPS 2013), AAAI Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Reverse iterative deepening for finite-horizon mdps with large branching factors", "author": ["A. Kolobov", "P. Dai", "M. Mausam", "D.S. Weld"], "venue": "Proceedings of the 22nd International Conference on Automated Planning and Scheduling, ICAPS.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Monte-carlo tree search in production management problems", "author": ["G. Chaslot", "S. De Jong", "J.T. Saito", "J. Uiterwijk"], "venue": "Proceedings of the 18th BeNeLux Conference on Artificial Intelligence, Citeseer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Temporal-difference search in computer go", "author": ["D. Silver", "R.S. Sutton", "M. M\u00fcller"], "venue": "In Borrajo, D., Kambhampati, S., Oddi, A., Fratini, S., eds.: Proceedings of the Twenty-Third International Conference on Automated Planning and Scheduling, ICAPS 2013, Rome, Italy, June 10-14, 2013, AAAI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "The grand challenge of computer go: Monte carlo tree search and extensions", "author": ["S. Gelly", "L. Kocsis", "M. Schoenauer", "M. Sebag", "D. Silver", "C. Szepesv\u00e1ri", "O. Teytaud"], "venue": "Commun. ACM 55(3)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Congolog, a concurrent programming language based on the situation calculus", "author": ["G. De Giacomo", "Y. Lesp\u00e9rance", "H.J. Levesque"], "venue": "Artificial Intelligence 121(1)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Indigolog: A high-level programming language for embedded reasoning agents", "author": ["G. Giacomo", "Y. Lesp\u00e9rance", "H.J. Levesque", "S. Sardina"], "venue": "In El Fallah Seghrouchni, A., Dix, J., Dastani, M., Bordini, R.H., eds.: Multi-Agent Programming:. Springer US", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Flux: A logic programming method for reasoning agents", "author": ["M. Thielscher"], "venue": "Theory and Practice of Logic Programming (TPLP) 5(4-5)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Decision-theoretic, high-level agent programming in the situation calculus", "author": ["C. Boutilier", "R. Reiter", "M. Soutchanski", "S Thrun"], "venue": "AAAI/IAAI.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Introduction to the fluent calculus", "author": ["M. Thielscher"], "venue": "Electron. Trans. Artif. Intell. 2", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1998}, {"title": "Ucd: Upper confidence bound for rooted directed acyclic graphs", "author": ["A. Saffidine", "T. Cazenave", "J. M\u00e9hat"], "venue": "Knowledge-Based Systems 34", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Bandit based monte-carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "Machine Learning: ECML 2006. Springer", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning 47(2-3)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Princeton University Press, Princeton, NJ, USA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1957}, {"title": "Multivesta: Statistical model checking for discrete event simulators", "author": ["S. Sebastio", "A. Vandin"], "venue": "Proceedings of the 7th International Conference on Performance Evaluation Methodologies and Tools, ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "The algorithmic analysis of hybrid systems", "author": ["R. Alur", "C. Courcoubetis", "N. Halbwachs", "T.A. Henzinger", "P.H. Ho", "X. Nicollin", "A. Olivero", "J. Sifakis", "S. Yovine"], "venue": "Theoretical computer science 138(1)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Introduction to statistical relational learning", "author": ["L. Getoor"], "venue": "MIT press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation 18(7)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "A state of the art candidate for responding to this challenge is behavior synthesis with online planning [1,2,3].", "startOffset": 105, "endOffset": 112}, {"referenceID": 1, "context": "A state of the art candidate for responding to this challenge is behavior synthesis with online planning [1,2,3].", "startOffset": 105, "endOffset": 112}, {"referenceID": 2, "context": "A state of the art candidate for responding to this challenge is behavior synthesis with online planning [1,2,3].", "startOffset": 105, "endOffset": 112}, {"referenceID": 0, "context": "MCAP is based on stochastic interpretation of nondeterministic action programs by Monte Carlo Tree Search (MCTS) [1,4].", "startOffset": 113, "endOffset": 118}, {"referenceID": 3, "context": "MCAP is based on stochastic interpretation of nondeterministic action programs by Monte Carlo Tree Search (MCTS) [1,4].", "startOffset": 113, "endOffset": 118}, {"referenceID": 3, "context": "The MCTS framework originated from research in computer Go [4,5].", "startOffset": 59, "endOffset": 64}, {"referenceID": 4, "context": "The MCTS framework originated from research in computer Go [4,5].", "startOffset": 59, "endOffset": 64}, {"referenceID": 5, "context": "Nevertheless, specialized Go programs based on the MCTS algorithm are able to play on the niveau of a human professional player [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 1, "context": "MCTS is also commonly used in autonomous planning [2,3] and has been applied successfully to a huge number of other search tasks [1].", "startOffset": 50, "endOffset": 55}, {"referenceID": 2, "context": "MCTS is also commonly used in autonomous planning [2,3] and has been applied successfully to a huge number of other search tasks [1].", "startOffset": 50, "endOffset": 55}, {"referenceID": 0, "context": "MCTS is also commonly used in autonomous planning [2,3] and has been applied successfully to a huge number of other search tasks [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "1: Monte Carlo Tree Search [1].", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "Algorithm 1 General MCTS approach [1] 1: procedure mcts(s0) 2: create root node v0 with state s0 3: while within computational budget do 4: vl \u2190 treepolicy(v0) 5: \u2206\u2190 defaultpolicy(vl) 6: backup(vl,\u2206) 7: return a(bestchildv0())", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Well-established action programming languages are Golog [7,8] and Flux [9].", "startOffset": 56, "endOffset": 61}, {"referenceID": 7, "context": "Well-established action programming languages are Golog [7,8] and Flux [9].", "startOffset": 56, "endOffset": 61}, {"referenceID": 8, "context": "Well-established action programming languages are Golog [7,8] and Flux [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "a particular formal specification of domain dynamics: The situation calculus and the fluent calculus are concerned with specification of action effects and domain dynamics in first order logic [10,11].", "startOffset": 193, "endOffset": 200}, {"referenceID": 10, "context": "a particular formal specification of domain dynamics: The situation calculus and the fluent calculus are concerned with specification of action effects and domain dynamics in first order logic [10,11].", "startOffset": 193, "endOffset": 200}, {"referenceID": 0, "context": "Recommendation quality depends on the number of simulations used for search [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 11, "context": "While it is possible to use a DAG instead of a tree [12], we will concentrate on the tree setting in this paper for the sake of simplicity.", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "It is a popular instantiation of the MCTS tree policy based on regret minimization [13,14].", "startOffset": 83, "endOffset": 90}, {"referenceID": 13, "context": "It is a popular instantiation of the MCTS tree policy based on regret minimization [13,14].", "startOffset": 83, "endOffset": 90}, {"referenceID": 14, "context": "a Bellman update) for updating node values [15].", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "[*] victims [1] position", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "System performance was measured with the statistical model checker Multivesta [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "continuously evolving domain values [17].", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "statistical relational learning [18] and deep learning [19]): How to constrain system behavior if perceptual abstraction is unknown at design time or changes at runtime?", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "statistical relational learning [18] and deep learning [19]): How to constrain system behavior if perceptual abstraction is unknown at design time or changes at runtime?", "startOffset": 55, "endOffset": 59}], "year": 2017, "abstractText": "This paper proposes Monte Carlo Action Programming, a programming language framework for autonomous systems that act in large probabilistic state spaces with high branching factors. It comprises formal syntax and semantics of a nondeterministic action programming language. The language is interpreted stochastically via Monte Carlo Tree Search. Effectiveness of the approach is shown empirically.", "creator": "LaTeX with hyperref package"}}}