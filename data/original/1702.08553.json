{"id": "1702.08553", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2017", "title": "Diameter-Based Active Learning", "abstract": "To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.", "histories": [["v1", "Mon, 27 Feb 2017 21:59:24 GMT  (576kb,D)", "https://arxiv.org/abs/1702.08553v1", "18 pages, 2 figures"], ["v2", "Fri, 9 Jun 2017 00:39:57 GMT  (574kb,D)", "http://arxiv.org/abs/1702.08553v2", "16 pages, 2 figures"]], "COMMENTS": "18 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["christopher tosh", "sanjoy dasgupta"], "accepted": true, "id": "1702.08553"}, "pdf": {"name": "1702.08553.pdf", "metadata": {"source": "CRF", "title": "Diameter-Based Active Learning", "authors": ["Christopher Tosh", "Sanjoy Dasgupta"], "emails": ["ctosh@cs.ucsd.edu", "dasgupta@cs.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "In many situations where a classifier is to be learned, it is easy to collect unlabeled data but costly to obtain labels. This has motivated the pool-based active learning model, in which a learner has access to a collection of unlabeled data points and is allowed to ask for individual labels in an adaptive manner. The hope is that choosing these queries intelligently will rapidly yield a low-error classifier, much more quickly than with random querying. A central focus of active learning is developing efficient querying strategies and understanding their label complexity.\nOver the past decade or two, there has been substantial progress in developing such rigorously-justified active learning schemes for general concept classes. For the most part, these schemes can be described as mellow: rather than focusing upon maximally informative points, they query any point whose label cannot reasonably be inferred from the information received so far. It is of interest to develop more aggressive strategies with better label complexity.\nAn exception to this general trend is the aggressive strategy of [12], whose label complexity is known to be optimal in its dependence on a key parameter called the splitting index. However, this strategy has been primarily of theoretical interest because it is difficult to implement algorithmically. In this paper, we introduce a variant of the methodology that yields efficient algorithms. We show that it admits roughly the same label complexity bounds as well as having promising experimental performance.\nAs with the original splitting index result, we operate in the realizable setting, where data can be perfectly classified by some function h\u2217 in the hypothesis class H. At any given time during the active learning process, the remaining candidates\u2014that is, the elements of H consistent with the data so far\u2014are called the version space. The goal of aggressive active learners is typically to pick queries that are likely to shrink this version space rapidly. But what is the right notion of size? Dasgupta [12] pointed out that the diameter of the version space is what matters, where the distance between two classifiers is taken to be the fraction of points on which they make different predictions. Unfortunately, the diameter is a difficult measure to work with because it cannot, in general, be decreased at a steady rate. Thus the earlier work used a procedure that has quantifiable label complexity but is not conducive to implementation.\nWe take a fresh perspective on this earlier result. We start by suggesting an alternative, but closely related, notion of the size of a version space: the average pairwise distance between hypotheses in the version space, with respect to some underlying probability distribution \u03c0 on H. This distribution \u03c0 can be arbitrary\u2014that is, there is no requirement that the target h\u2217 is chosen from it\u2014but should be chosen so\nar X\niv :1\n70 2.\n08 55\n3v 2\n[ cs\n.L G\n] 9\nJ un\n2 01\nthat it is easy to sample from. When H consists of linear separators, for instance, a good choice would be a log-concave density, such as a Gaussian.\nAt any given time, the next query x is chosen roughly as follows:\n\u2022 Sample a collection of classifiers h1, h2, . . . , hm from \u03c0 restricted to the current version space V .\n\u2022 Compute the distances between them; this can be done using just the unlabeled points.\n\u2022 Any candidate query x partitions the classifiers {hi} into two groups: those that assign it a + label (call these V +x ) and those that assign it a \u2212 label (call these V \u2212x ). Estimate the average-diameter after labeling x by the sum of the distances between classifiers hi within V + x , or those within V \u2212 x , whichever\nis larger.\n\u2022 Out of the pool of unlabeled data, pick the x for which this diameter-estimate is smallest.\nThis is repeated until the version space has small enough average diameter that a random sample from it is very likely to have error less than a user-specified threshold . We show how all these steps can be achieved efficiently, as long as there is a sampler for \u03c0.\nDasgupta [12] pointed out that the label complexity of active learning depends on the underlying distribution, the amount of unlabeled data (since more data means greater potential for highly-informative points), and also the target classifier h\u2217. That paper identifies a parameter called the splitting index \u03c1 that captures the relevant geometry, and gives upper bounds on label complexity that are proportional to 1/\u03c1, as well as showing that this dependence is inevitable. For our modified notion of diameter, a different averaged splitting index is needed. However, we show that it can be bounded by the original splitting index, with an extra multiplicative factor of log(1/ ); thus all previously-obtained label complexity results translate immediately for our new algorithm."}, {"heading": "2 Related work", "text": "The theory of active learning has developed along several fronts. One of these is nonparametric active learning, where the learner starts with a pool of unlabeled points, adaptively queries a few of them, and then fills in the remaining labels. The goal is to do this with as few errors as possible. (In particular, the learner does not return a classifier from some predefined parametrized class.) One scheme begins by building a neighborhood graph on the unlabeled data, and propagating queried labels along the edges of this graph [24, 7, 10]. Another starts with a hierarchical clustering of the data and moves down the tree, sampling at random until it finds clusters that are relatively pure in their labels [13]. The label complexity of such methods have typically be given in terms of smoothness properties of the underlying data distribution [6, 22].\nAnother line of work has focused on active learning of linear separators, by querying points close to the current guess at the decision boundary [3, 14, 4]. Such algorithms are close in spirit to those used in practice, but their analysis to date has required fairly strong assumptions to the effect that the underlying distribution on the unlabeled points is logconcave. Interestingly, regret guarantees for online algorithms of this sort can be shown under far weaker conditions [8].\nThe third category of results, to which the present paper belongs, considers active learning strategies for general concept classes H. Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far. The label complexity of these methods can be bounded in terms of a quantity known as the disagreement coefficient [20]. In the realizable case, the canonical such algorithm is that of [9], henceforth referred to as CAL. Other methods use a prior distribution \u03c0 over the hypothesis class, sometimes assuming that the target classifier is a random draw from this prior. These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16]. Perhaps the most widely-used of these methods is the latter, query-by-committee, henceforth QBC. As mentioned earlier, shrinking \u03c0-mass is not an optimal strategy if low misclassification error is the ultimate goal. In particular, what matters is not\nthe prior mass of the remaining version space, but rather how different these candidate classifiers are from each other. This motivates using the diameter of the version space as a yardstick, which was first proposed in [12] and is taken up again here."}, {"heading": "3 Preliminaries", "text": "Consider a binary hypothesis class H, a data space X , and a distribution D over X . For mathematical convenience, we will restrict ourselves to finite hypothesis classes. (We can do this without loss of generality when H has finite VC dimension, since we only use the predictions of hypotheses on a pool of unlabeled points; however, we do not spell out the details of this reduction here.) The hypothesis distance induced by D over H is the pseudometric\nd(h, h\u2032) := Prx\u223cD(h(x) 6= h\u2032(x)).\nGiven a point x \u2208 X and a subset V \u2282 H, denote\nV +x = {h \u2208 V : h(x) = 1}\nand V \u2212x = V \\ V +x . Given a sequence of data points x1, . . . , xn and a target hypothesis h\u2217, the induced version space is the set of hypotheses that are consistent with the target hypotheses on the sequence, i.e.\n{h \u2208 H : h(xi) = h\u2217(xi) for all i = 1, . . . , n}."}, {"heading": "3.1 Diameter and the Splitting Index", "text": "The diameter of a set of hypotheses V \u2282 H is the maximal distance between any two hypotheses in V , i.e.\ndiam(V ) := max h,h\u2032\u2208V\nd(h, h\u2032).\nWithout any prior information, any hypothesis in the version space could be the target. Thus the worst case error of any hypothesis in the version space is the diameter of the version space. The splitting index roughly characterizes the number of queries required for an active learning algorithm to reduce the diameter of the version space below .\nWhile reducing the diameter of a version space V \u2282 H, we will sometimes identify pairs of hypotheses h, h\u2032 \u2208 V that are far apart and therefore need to be separated. We will refer to {h, h\u2032} as an edge. Given a set of edges E = {{h1, h\u20321}, . . . , {hn, h\u2032n}} \u2282 (H 2 ) , we say a data point x \u03c1-splits E if querying x separates at least a \u03c1 fraction of the pairs, that is, if\nmax {\u2223\u2223E+x |, |E\u2212x \u2223\u2223} \u2264 (1\u2212 \u03c1)|E|\nwhere E+x = E \u2229 (H+x\n2\n) and similarly for E\u2212x . When attempting to get accuracy > 0, we need to only\neliminate edge of length greater than . Define\nE = {{h, h\u2032} \u2208 E : d(h, h\u2032) > }.\nThe splitting index of a set V \u2282 H is a tuple (\u03c1, , \u03c4) such that for all finite edge-sets E \u2282 ( V 2 ) ,\nPrx\u223cD(x \u03c1-splits E ) \u2265 \u03c4.\nThe following theorem, due to Dasgupta [12], bounds the sample complexity of active learning in terms of the splitting index. The O\u0303 notation hides polylogarithmic factors in d, \u03c1, \u03c4 , log 1/ , and the failure probability \u03b4.\nTheorem 1 (Dasgupta 2005). Suppose H is a hypothesis class with splitting index (\u03c1, , \u03c4). Then to learn a hypothesis with error ,\n(a) any active learning algorithm with \u2264 1/\u03c4 unlabeled samples must request at least 1/\u03c1 labels, and\n(b) if H has VC-dimension d, there is an active learning algorithm that draws O\u0303(d/(\u03c1\u03c4) log2(1/ )) unlabeled data points and requests O\u0303((d/\u03c1) log2(1/ )) labels.\nUnfortunately, the only known algorithm satisfying (b) above is intractable for all but the simplest hypothesis classes: it constructs an -covering of the hypothesis space and queries points which whittle away at the diameter of this covering. To overcome this intractability, we consider a slightly more benign setting in which we have a samplable prior distribution \u03c0 over our hypothesis space H."}, {"heading": "3.2 An Average Notion of Diameter", "text": "With a prior distribution, it makes sense to shift away from the worst-case to the average-case. We define the average diameter of a subset V \u2282 H as the expected distance between two hypotheses in V randomly drawn from \u03c0, i.e.\n\u03a6(V ) := Eh,h\u2032\u223c\u03c0|V [d(h, h \u2032)]\nwhere \u03c0|V is the conditional distribution induced by restricting \u03c0 to V , that is, \u03c0|V (h) = \u03c0(h)/\u03c0(V ) for h \u2208 V .\nIntuitively, a version space with very small average diameter ought to put high weight on hypotheses that are close to the true hypothesis. Indeed, given a version space V with h\u2217 \u2208 V , the following lemma shows that if \u03a6(V ) is small enough, then a low error hypothesis can be found by two popular heuristics: random sampling and MAP estimation.\nLemma 2. Suppose V \u2282 H contains h\u2217. Pick > 0.\n(a) (Random sampling) If \u03a6(V ) \u2264 \u03c0|V (h\u2217) then Eh\u223c\u03c0|V [d(h\u2217, h)] \u2264 .\n(b) (MAP estimation) Write pmap = maxh\u2208V \u03c0|V (h). Pick 0 < \u03b1 < pmap. If\n\u03a6(V ) \u2264 2 (min{\u03c0|V (h\u2217), pmap \u2212 \u03b1})2 ,\nthen d(h\u2217, h) \u2264 for any h with \u03c0|V (h) \u2265 pmap \u2212 \u03b1.\nProof. Part (a) follows from\n\u03a6(V ) = Eh,h\u2032\u223c\u03c0|V [d(h, h \u2032)] \u2265 \u03c0|V (h\u2217)Eh\u223c\u03c0|V [d(h \u2217, h)].\nFor (b), take \u03b4 = min(\u03c0|V (h\u2217), pmap \u2212 \u03b1) and define V\u03c0,\u03b4 = {h \u2208 V : \u03c0|V (h) \u2265 \u03b4}. Note that V\u03c0,\u03b4 contains h\u2217 as well as any h \u2208 V with \u03c0|V (h) \u2265 pmap \u2212 \u03b1.\nWe claim diam(V\u03c0,\u03b4) is at most . Suppose not. Then there exist h1, h2 \u2208 V\u03c0,\u03b4 satisfying d(h1, h2) > , implying\n\u03a6(V ) = Eh,h\u2032\u223c\u03c0|V [d(h, h \u2032)]\n\u2265 2 \u00b7 \u03c0|V (h1) \u00b7 \u03c0|V (h2) \u00b7 d(h1, h2) > 2\u03b42 .\nBut this contradicts our assumption on \u03a6(V ). Since both h, h\u2217 \u2208 V\u03c0,\u03b4, we have (b)."}, {"heading": "3.3 An Average Notion of Splitting", "text": "We now turn to defining an average notion of splitting. A data point x \u03c1-average splits V if\nmax\n{ \u03c0(V +x ) 2\n\u03c0(V )2 \u03a6(V +x ),\n\u03c0(V \u2212x ) 2\n\u03c0(V )2 \u03a6(V \u2212x )\n} \u2264 (1\u2212 \u03c1)\u03a6(V ).\nAnd we say a set S \u2282 H has average splitting index (\u03c1, , \u03c4) if for any subset V \u2282 S such that \u03a6(V ) > ,\nPrx\u223cD (x \u03c1-average splits V ) \u2265 \u03c4.\nIntuitively, average splitting refers to the ability to significantly decrease the potential function\n\u03c0(V )2\u03a6(V ) = Eh,h\u2032\u223c\u03c0[1(h, h\u2032 \u2208 V ) d(h, h\u2032)]\nwith a single query. While this potential function may seem strange at first glance, it is closely related to the original splitting index. The following lemma, whose proof is deferred to Section 5, shows the splitting index bounds the average splitting index for any hypothesis class.\nLemma 3. Let \u03c0 be a probability measure over a hypothesis class H. If H has splitting index (\u03c1, , \u03c4), then it has average splitting index ( \u03c14dlog(1/ )e , 2 , \u03c4).\nDasgupta [12] derived the splitting indices for several hypothesis classes, including intervals and homogeneous linear separators. Lemma 3 implies average splitting indices within a log(1/ ) factor in these settings.\nMoreover, given access to samples from \u03c0|V , we can easily estimate the quantities appearing in the definition of average splitting. For an edge sequence E = ({h1, h\u20321}, . . . , {hn, h\u2032n}), define\n\u03c8(E) := n\u2211 i=1 d(hi, h \u2032 i).\nWhen hi, h \u2032 i are i.i.d. draws from \u03c0|V for all i = 1, . . . , n, which we denote E \u223c (\u03c0|V )2\u00d7n, the random variables \u03c8(E), \u03c8(E\u2212x ), and \u03c8(E + x ) are unbiased estimators of the quantities appearing in the definition of average splitting.\nLemma 4. Given E \u223c (\u03c0|V )2\u00d7n, we have \u2022 E [\n1 n\u03c8(E)\n] = \u03a6(V ) and\n\u2022 E [\n1 n\u03c8(E + x ) ] = \u03c0(V +x ) 2 \u03c0(V )2 \u03a6(V + x ) for any x \u2208 X . Similarly for E\u2212x and V \u2212x .\nProof. From definitions and linearity of expectations, it is easy to observe E[\u03c8(E)] = n\u03a6(V ). By the independence of hi, h \u2032 i, we additionally have\nE [ 1\nn \u03c8(E+x )\n] = 1\nn E  \u2211 {hi,h\u2032i}\u2208E + x d(hi, h \u2032 i)  = 1\nn E  \u2211 {hi,h\u2032i}\u2208E 1[hi \u2208 V +x ]1[h\u2032i \u2208 V +x ] d(hi, h\u2032i)  = 1\nn \u2211 {hi,h\u2032i}\u2208E ( \u03c0(V +x ) \u03c0(V ) )2 E [ d(hi, h \u2032 i) |hi, h\u2032i \u2208 V +x ] = ( \u03c0(V +x )\n\u03c0(V )\n)2 \u03a6(V +x ).\nRemark: It is tempting to define average splitting in terms of the average diameter as\nmax{\u03a6(V +x ),\u03a6(V \u2212x )} \u2264 (1\u2212 \u03c1)\u03a6(V ).\nHowever, this definition does not satisfy a nice relationship with the splitting index. Indeed, there exist hypothesis classes V for which there are many points which 1/4-split E for any E \u2282 ( V 2 ) but for which every x \u2208 X satisfies max{\u03a6(V +x ),\u03a6(V \u2212x )} \u2248 \u03a6(V ).\nThis observation is formally proven in the appendix."}, {"heading": "4 An Average Splitting Index Algorithm", "text": "Suppose we are given a version space V with average splitting index (\u03c1, , \u03c4). If we draw O\u0303(1/\u03c4) points from the data distribution then, with high probability, one of these will \u03c1-average split V . Querying that point will result in a version space V \u2032 with significantly smaller potential \u03c0(V \u2032)2\u03a6(V \u2032).\nIf we knew the value \u03c1 a priori, then Lemma 4 combined with standard concentration bounds [21, 1] would give us a relatively straightforward procedure to find a good query point:\n1. Draw E\u2032 \u223c (\u03c0|V )2\u00d7M and compute the empirical estimate \u03a6\u0302(V ) = 1M\u03c8(E \u2032).\n2. Draw E \u223c (\u03c0|V )2\u00d7N for N depending on \u03c1 and \u03a6\u0302.\n3. For suitable M and N , it will be the case that with high probability, for some x,\n1\nN max\n{ \u03c8(E+x ), \u03c8(E \u2212 x ) } \u2248 (1\u2212 \u03c1)\u03a6\u0302.\nQuerying that point will decrease the potential.\nHowever, we typically would not know the average splitting index ahead of time. Moreover, it is possible that the average splitting index may change from one version space to the next. In the next section, we describe a query selection procedure that adapts to the splittability of the current version space."}, {"heading": "4.1 Finding a Good Query Point", "text": "Algorithm 2, which we term select, is our query selection procedure. It takes as input a sequence of data points x1, . . . , xm, at least one of which \u03c1-average splits the current version space, and with high probability finds a data point that \u03c1/8-average splits the version space.\nselect proceeds by positing an optimistic estimate of \u03c1, which we denote \u03c1\u0302t, and successively halving it until we are confident that we have found a point that \u03c1\u0302t-average splits the version space. In order for this algorithm to succeed, we need to choose nt and mt such that with high probability (1) \u03a6\u0302t is an accurate estimate of \u03a6(V ) and (2) our halting condition will be true if \u03c1\u0302t is within a constant factor of \u03c1 and false otherwise. The following lemma, whose proof is in the appendix, provides such choices for nt and mt.\nLemma 5. Let \u03c1, , \u03b40 > 0 be given. Suppose that version space V satisfies \u03a6(V ) > . In select, fix a round t and data point x \u2208 X that exactly \u03c1-average splits V (that is, max{\u03c0|V (V +x )2\u03a6(V +x ), \u03c0|V (V \u2212x )2\u03a6(V \u2212x )} = (1\u2212 \u03c1)\u03a6(V )). If mt \u2265 48\u03c1\u03022t log 4 \u03b40 and nt \u2265 max { 32 \u03c1\u03022t \u03a6\u0302t , 40 \u03a6\u03022t } log 4\u03b40 then with probability 1\u2212 \u03b40,\n(a) \u03a6\u0302t \u2265 (1\u2212 \u03c1\u0302t/4)\u03a6(V );\n(b) if \u03c1 \u2264 \u03c1\u0302t/2, then 1nt max {\u03c8(E + x ), \u03c8(E \u2212 x )} > (1\u2212 \u03c1\u0302t)\u03a6\u0302t; and\n(c) if \u03c1 \u2265 2\u03c1\u0302t, then 1nt max {\u03c8(E + x ), \u03c8(E \u2212 x )} \u2264 (1\u2212 \u03c1\u0302t)\u03a6\u0302t.\nAlgorithm 1 dbal\nInput: Hypothesis class H, prior distribution \u03c0 Initialize V = H while 1n\u03c8(E) \u2265 3 4 for E \u223c (\u03c0|V )\n2\u00d7n do Draw m data points x = (x1, . . . , xm) Query point xi = select(V,x) and set V to be consistent with the result end while return Current version space V in the form of the queried points (x1, h \u2217(x1)), . . . , (xK , h \u2217(xK))\nAlgorithm 2 select\nInput: Version space V , prior \u03c0, data x = (x1, . . . , xm) Set \u03c1\u03021 = 1/2 for t = 1, 2, . . . do\nDraw E\u2032 \u223c (\u03c0|V )2\u00d7mt and compute \u03a6\u0302t = 1mt\u03c8(E \u2032) Draw E \u223c (\u03c0|V )2\u00d7nt If \u2203xi s.t. 1nt max { \u03c8(E+xi), \u03c8(E \u2212 xi) } \u2264 (1 \u2212 \u03c1\u0302t)\u03a6\u0302t, then halt and return xi Otherwise, let \u03c1\u0302t+1 = \u03c1\u0302t/2\nend for\nGiven the above lemma, we can establish a bound on the number of rounds and the total number of hypotheses select needs to find a data point that \u03c1/8-average splits the version space.\nTheorem 6. Suppose that select is called with a version space V with \u03a6(V ) \u2265 and a collection of points x1, . . . , xm such that at least one of xi \u03c1-average splits V . If \u03b40 \u2264 \u03b4/(2m(2+ log(1/\u03c1))), then with probability at least 1 \u2212 \u03b4, select returns a point xi that (\u03c1/8)-average splits V , finishing in less than dlog(1/\u03c1)e + 1 rounds and sampling O (( 1 \u03c12 + log(1/\u03c1) \u03a6(V )2 ) log 1\u03b40 ) hypotheses in total.\nRemark 1: It is possible to modify select to find a point xi that (c\u03c1)-average splits V for any constant c < 1 while only having to draw O(1) more hypotheses in total. First note that by halving \u03c1\u0302t at each step, we immediately give up a factor of two in our approximation. This can be made smaller by taking narrower steps. Additionally, with a constant factor increase in mt and nt, the approximation ratios in Lemma 5 can be set to any constant.\nRemark 2: At first glance, it appears that select requires us to know \u03c1 in order to calculate \u03b40. However, a crude lower bound on \u03c1 suffices. Such a bound can always be found in terms of . This is because any version space is ( /2, , /2)-splittable [12, Lemma 1]. By Lemma 3, so long as \u03c4 is less than /4, we can substitute 8dlog(2/ )e for \u03c1 in when we compute \u03b40.\nProof of Theorem 6. Let T := dlog(1/\u03c1)e+ 1. By Lemma 5, we know that for rounds t = 1, . . . , T , we don\u2019t return any point which does worse than \u03c1\u0302t/2-average splits V with probability 1 \u2212 \u03b4/2. Moreover, in the T -th round, it will be the case that \u03c1/4 \u2264 \u03c1\u0302T \u2264 \u03c1/2, and therefore, with probability 1\u2212 \u03b4/2, we will select a point which does no worse than \u03c1\u0302T /2-average split V , which in turn does no worse than \u03c1/8-average split V .\nNote that we draw mt+nt hypotheses at each round. By Lemma 5, for each round \u03a6\u0302t \u2265 3\u03a6(V )/4 \u2265 3 /4. Thus\n# of hypotheses drawn = T\u2211 t=1\n( 48\n\u03c1\u03022t +\n32\n\u03c1\u03022t \u03a6\u0302t +\n40\n\u03a6\u03022t\n) log 4\n\u03b40 \u2264 T\u2211 t=1 ( 96 \u03c1\u03022t + 72 \u03a6(V )2 ) log 4 \u03b40\nGiven \u03c1\u0302t = 1/2 t and T \u2264 2 + log 1/\u03c1, we have\nT\u2211 t=1 1 \u03c1\u03022t = T\u2211 t=1 22t \u2264 ( T\u2211 t=1 2t )2 \u2264 ( 22+log 1/\u03c1 )2 = 16 \u03c12 .\nPlugging in \u03b40 \u2264 \u03b42m(2+log(1/\u03c1)) , we recover the theorem statement."}, {"heading": "4.2 Active Learning Strategy", "text": "Using the select procedure as a subroutine, Algorithm 1, henceforth DBAL for Diameter-based Active Learning, is our active learning strategy. Given a hypothesis class with average splitting index (\u03c1, /2, \u03c4), DBAL queries data points provided by select until it is confident \u03a6(V ) < .\nDenote by Vt the version space in the t-th round of DBAL. The following lemma, which is proven in the appendix, demonstrates that the halting condition (that is, \u03c8(E) < 3 n/4, where E consists of n pairs sampled from (\u03c0|V )2) guarantees that with high probability DBAL stops when \u03a6(Vt) is small.\nLemma 7. The following holds for DBAL:\n(a) Suppose that for all t = 1, 2, . . . ,K that \u03a6(Vt) > . Then the probability that the termination condition is ever true for any of those rounds is bounded above by K exp ( \u2212 n32 ) .\n(b) Suppose that for some t = 1, 2, . . . ,K that \u03a6(Vt) \u2264 /2. Then the probability that the termination condition is not true in that round is bounded above by K exp ( \u2212 n48 ) .\nGiven the guarantees on the select procedure in Theorem 6 and on the termination condition provided by Lemma 7, we get the following theorem.\nTheorem 8. Suppose that H has average splitting index (\u03c1, /2, \u03c4). Then DBAL returns a version space V satisfying \u03a6(V ) \u2264 with probability at least 1\u2212 \u03b4 while using the following resources:\n(a) K \u2264 8\u03c1 ( log 2 + 2 log 1 \u03c0(h\u2217) ) rounds, with one label per round,\n(b) m \u2264 1\u03c4 log 2K \u03b4 unlabeled data points sampled per round, and (c) n \u2264 O ((\n1 \u03c12 + log(1/\u03c1) 2 ) ( log mK\u03b4 + log log 1 )) hypotheses sampled per round.\nProof. From definition of the average splitting index, if we draw m = 1\u03c4 log 2K \u03b4 unlabeled points per round, then with probability 1\u2212\u03b4/2, each of the first K rounds will have at least one data point that \u03c1-average splits the current version space. In each such round, if the version space has average diameter at least /2, then with probability 1 \u2212 \u03b4/4 select will return a data point that \u03c1/8-average splits the current version space while sampling no more than n = O (( 1 \u03c12 + 1 2 log 1 \u03c1 ) log mK log 1 \u03b4 ) hypotheses per round by Theorem 6.\nBy Lemma 7, if the termination check uses n\u2032 = O ( 1 log\n1 \u03b4\n) hypotheses per round, then with probability\n1 \u2212 \u03b4/4 in the first K rounds the termination condition will never be true when the current version space has average diameter greater than and will certainly be true if the current version space has diameter less than /2.\nThus it suffices to bound the number of rounds in which we can \u03c1/8-average split the version space before encountering a version space with /2.\nSince the version space is always consistent with the true hypothesis h\u2217, we will always have \u03c0(Vt) \u2265 \u03c0(h\u2217). After K = 8\u03c1 ( log 2 + 2 log 1 \u03c0(h\u2217) ) rounds of \u03c1/8-average splitting, we have\n\u03c0(h\u2217)2\u03a6(VK) \u2264 \u03c0(VK)2\u03a6(VK) \u2264 (\n1\u2212 \u03c1 8\n)K \u03c0(V0) 2\u03a6(V0) \u2264 \u03c0(h\u2217)2\n2\nWhere we have used the fact that \u03c0(V )2\u03a6(V ) \u2264 1 for any set V \u2282 H. Thus in the first K rounds, we must terminate with a version space with average diameter less than ."}, {"heading": "5 Proof of Lemma 3", "text": "In this section, we give the proof of the following relationship between the original splitting index and our average splitting index.\nLemma 3. Let \u03c0 be a probability measure over a hypothesis class H. If H has splitting index (\u03c1, , \u03c4), then it has average splitting index ( \u03c14dlog(1/ )e , 2 , \u03c4).\nThe first step in proving Lemma 3 is to relate the splitting index to our estimator \u03c8(\u00b7). Intuitively, splittability says that for any set of large edges there are many data points which remove a significant fraction of them. One may suspect this should imply that if a set of edges is large on average, then there should be many data points which remove a significant fraction of their weight. The following lemma confirms this suspicion.\nLemma 9. Suppose that V \u2282 H has splitting index (\u03c1, , \u03c4), and say E = ({h1, h\u20321}, . . . , {hn, h\u2032n}) is a sequence of hypothesis pairs from V satisfying 1n\u03c8(E) > 2 . Then if x \u223c D, we have with probability at least \u03c4 ,\nmax { \u03c8(E+x ), \u03c8(E \u2212 x ) } \u2264 (\n1\u2212 \u03c1 4dlog(1/ )e\n) \u03c8(E).\nProof. Consider partitioning E as\nE0 = {{h, h\u2032} \u2208 E : d(h, h\u2032) < } and Ek = {{h, h\u2032} \u2208 E : d(h, h\u2032) \u2208 [2k\u22121 , 2k )\nfor k = 1, . . . ,K with K = dlog 1 e. Then E0, . . . , EK are all disjoint and their union is E. Define E1:K = \u222aKk=1Ek.\nWe first claim that \u03c8(E1:K) > \u03c8(E0). This follows from the observation that because \u03c8(E) \u2265 2n and each edge in E0 has length less than , we must have\n\u03c8(E1:K) = \u03c8(E)\u2212 \u03c8(E0) > 2n \u2212 n > \u03c8(E0).\nNext, observe that because each edge {h, h\u2032} \u2208 Ek with k \u2265 1 satisfies d(h, h\u2032) \u2208 [2k\u22121 , 2k ), we have\n\u03c8(E1:K) = K\u2211 k=1 \u2211 {h,h\u2032}\u2208Ek d(h, h\u2032) \u2264 K\u2211 k=1 2k |Ek|.\nSince there are only K summands on the right, at least one of these must be larger than \u03c8(E1:K)/K. Let k denote that index and let x be a point which \u03c1-splits Ek. Then we have\n\u03c8((E1:K) + x ) \u2264 \u03c8(E1:K)\u2212 \u03c8(Ek \\ (Ek)+x ) \u2264 \u03c8(E1:K)\u2212 \u03c12k\u22121 |Ek|\n\u2264 (\n1\u2212 \u03c1 2K\n) \u03c8(E1:K).\nSince \u03c8(E1:K) \u2265 \u03c8(E0), we have\n\u03c8(E+x ) \u2264 \u03c8(E0) + (\n1\u2212 \u03c1 2K\n) \u03c8(E1:K) \u2264 ( 1\u2212 \u03c1\n4K\n) \u03c8(E).\nSymmetric arguments show the same holds for E\u2212x . Finally, by the definition of splitting, the probability of drawing a point x which \u03c1-splits Ek is at least \u03c4 , giving us the lemma.\nWith Lemma 9 in hand, we are now ready to prove Lemma 3.\nProof of Lemma 3. Let V \u2282 H such that \u03a6(V ) > 2 . Suppose that we draw n edges E i.i.d. from \u03c0|V and draw a data point x \u223c D. Then Hoeffding\u2019s inequality [21], combined with Lemma 4, tells us that there exist sequences n, \u03b4n \u2198 0 such that with probability at least 1\u2212 3\u03b4n, the following hold simultaneously:\n\u2022 \u03a6(V )\u2212 n \u2264 1n\u03c8(E) \u2264 \u03a6(V ) + n,\n\u2022 1n\u03c8(E + x ) \u2265\n\u03c0(V +x ) 2\n\u03c0(V )2 \u03a6(V + x )\u2212 n, and\n\u2022 1n\u03c8(E \u2212 x ) \u2265\n\u03c0(V \u2212x ) 2\n\u03c0(V )2 \u03a6(V \u2212 x )\u2212 n.\nFor n small enough, we have that \u03a6(V ) \u2212 n > 2 . Combining the above with Lemma 9, we have with probability at least \u03c4 \u2212 3\u03b4n,\nmax\n{ \u03c0(V +x ) 2\n\u03c0(V )2 \u03a6(V +x ),\n\u03c0(V \u2212x ) 2\n\u03c0(V )2 \u03a6(V \u2212x )\n} \u2212 n \u2264 1\nn max{\u03c8(E+x ), \u03c8(E\u2212x )}\n\u2264 (\n1\u2212 \u03c1 4dlog(1/ )e\n) \u03c8(E)\nn \u2264 (\n1\u2212 \u03c1 4dlog(1/ )e\n) (\u03a6(V ) + n).\nBy taking n\u2192\u221e, we have n, \u03b4n \u2198 0, giving us the lemma."}, {"heading": "6 Simulations", "text": "We compared DBAL against the baseline passive learner as well as two other generic active learning strategies: CAL and QBC. CAL proceeds by randomly sampling a data point and querying it if its label cannot be inferred from previously queried data points. QBC uses a prior distribution \u03c0 and maintains a version space V . Given a randomly sampled data point x, QBC samples two hypotheses h, h\u2032 \u223c \u03c0|V and queries x if h(x) 6= h\u2032(x).\nWe tested on two hypothesis classes: homogeneous, or through-the-origin, linear separators and k-sparse monotone disjunctions. In each of our simulations, we drew our target h\u2217 from the prior distribution. After each query, we estimated the average diameter of the version space. We repeated each simulation several times and plotted the average performance of each algorithm.\nHomogeneous linear separators The class of d-dimensional homogeneous linear separators can be identified with elements of the d-dimensional unit sphere. That is, a hypothesis h \u2208 Sd\u22121 acts on a data point x \u2208 Rd via the sign of their inner product:\nh(x) := sign(\u3008h, x\u3009).\nIn our simulations, both the prior distribution and the data distribution are uniform over the unit sphere. Although there is no known method to exactly sample uniformly from the version space, Gilad-Bachrach et al. [17] demonstrated that using samples generated by the hit-and-run Markov chain works well in practice. We adopted this approach for our sampling tasks.\nFigure 1 shows the results of our simulations on homogeneous linear separators.\nSparse monotone disjunctions A k-sparse monotone disjunction is a disjunction of k positive literals. Given a Boolean vector x \u2208 {0, 1}n, a monotone disjunction h classifies x as positive if and only if xi = 1 for some positive literal i in h.\nIn our simulations, each data point is a vector whose coordinates are i.i.d. Bernoulli random variables with parameter p. The prior distribution is uniform over all k-sparse monotone disjunctions. When k is constant, it is possible to sample from the prior restricted to the version space in expected polynomial time using rejection sampling.\nThe results of our simulations on k-sparse monotone disjunctions are in Figure 2."}, {"heading": "Acknowledgments", "text": "The authors are grateful to the NSF for support under grants IIS-1162581 and DGE-1144086. Part of this work was done at the Simons Institute for Theoretical Computer Science, Berkeley, as part of a program on the foundations of machine learning. CT additionally thanks Daniel Hsu and Stefanos Poulis for helpful discussions."}, {"heading": "Appendix: Proof Details", "text": "Remark from Section 3\nIn Section 3, the remark after the definition of average splitting stated that there exist hypothesis classes V for which there are many points which 1/4-split E for any E \u2282 ( V 2 ) but for which any x \u2208 X satisfies\nmax{\u03a6(V +x ),\u03a6(V \u2212x )} \u2248 \u03a6(V ).\nHere we formally prove this statement. Consider the hypothesis class of homogeneous linear separators and let V = {e1, . . . , en} \u2282 H where ek is the k-th unit coordinate vector. Let the data distribution be uniform over the n-sphere and the prior distribution \u03c0 be uniform over V . As a subset of the homogeneous linear separators, V has splitting index (1/4, ,\u0398( )) [12, Theorem 10].\nOn the other hand, for any i 6= j, d(hi, hj) = 1/2. This implies that\n\u03a6(V ) = Pr(h 6= h\u2032)Eh,h\u2032 [d(h, h\u2032) |h 6= h\u2032] = n\u2212 1\n2n .\nMoreover, any query x \u2208 X eliminates at most half the hypotheses in V in the worst case. Therefore, for all x \u2208 X ,\nmax{\u03a6(V +x ),\u03a6(V \u2212x )} \u2265 (n/2\u2212 1)\n2(n/2) = ( n\u2212 2 n\u2212 1 ) \u03a6(V ).\nProofs of Lemma 5 and Lemma 7\nThe proofs in this section rely crucially on two concentration inequalities. The first is due to Hoeffding [21].\nLemma 10 (Hoeffding 1963). Let X1, . . . , Xn be i.i.d. random variables taking values in [0, 1] and let X = \u2211 Xi and \u00b5 = E[X]. Then for t > 0,\nPr(X \u2212 \u00b5 \u2265 t) \u2264 exp ( \u22122t 2\nn ) Our other tool will be the following multiplicative Chernoff-Hoeffding bound due to Angluin and Valiant\n[1].\nLemma 11 (Angluin and Valiant 1977). Let X1, . . . , Xn be i.i.d. random variables taking values in [0, 1] and let X = \u2211 Xi and \u00b5 = E[X]. Then for 0 < \u03b2 < 1,\n(i) Pr(X \u2264 (1\u2212 \u03b2)\u00b5) \u2264 exp ( \u2212\u03b2\n2\u00b5 2\n) and\n(ii) Pr(X \u2265 (1 + \u03b2)\u00b5) \u2264 exp ( \u2212\u03b2\n2\u00b5 3\n) .\nWe now turn to the proof of Lemma 5.\nLemma 5. Let \u03c1, , \u03b40 > 0 be given. Suppose that version space V satisfies \u03a6(V ) > . In select, fix a round t and data point x \u2208 X that exactly \u03c1-average splits V (that is, max{\u03c0|V (V +x )2\u03a6(V +x ), \u03c0|V (V \u2212x )2\u03a6(V \u2212x )} = (1\u2212 \u03c1)\u03a6(V )). If mt \u2265 48\u03c1\u03022t log 4 \u03b40 and nt \u2265 max { 32 \u03c1\u03022t \u03a6\u0302t , 40 \u03a6\u03022t } log 4\u03b40 then with probability 1\u2212 \u03b40,\n(a) \u03a6\u0302t \u2265 (1\u2212 \u03c1\u0302t/4)\u03a6(V );\n(b) if \u03c1 \u2264 \u03c1\u0302t/2, then 1nt max {\u03c8(E + x ), \u03c8(E \u2212 x )} > (1\u2212 \u03c1\u0302t)\u03a6\u0302t; and\n(c) if \u03c1 \u2265 2\u03c1\u0302t, then 1nt max {\u03c8(E + x ), \u03c8(E \u2212 x )} \u2264 (1\u2212 \u03c1\u0302t)\u03a6\u0302t.\nProof. In round t, let \u03c1\u0302 := \u03c1\u0302t, \u03a6\u0302 := \u03a6\u0302t, m := mt, and n := nt.\nFor (a), recall \u03a6\u0302 = 1m\u03c8(E \u2032) for E\u2032 \u223c (\u03c0|V )2\u00d7m. By Lemma 11, we have for \u03b20 > 0\nPr ( (1\u2212 \u03b20)\u03a6(V ) \u2264 \u03a6\u0302 \u2264 (1 + \u03b20)\u03a6(V ) ) \u2265 1\u2212 2 exp ( \u2212m\u03b2 2 0\n3\n) .\nTaking m \u2265 3 \u03b220\nlog (\n4 \u03b40\n) , we have the above probability is at least 1\u2212 \u03b40/2. Let us condition on this event\noccurring. To see (b), say w.l.o.g. ( \u03c0(V +x ) \u03c0(V ) )2 \u03a6(V +x ) = (1\u2212 \u03c1)\u03a6(V ). Then, we have\nPr\n( 1\nn \u03c8(E+x ) \u2264 (1\u2212 \u03c1\u0302)\u03a6\u0302\n) \u2264 Pr ( 1\nn \u03c8(E+x ) \u2264 (1\u2212 \u03c1\u0302)(1 + \u03b20)\u03a6(V )\n) .\nTaking \u03b2 such that (1\u2212 \u03b2)(1\u2212 \u03c1) = (1\u2212 \u03c1\u0302)(1 + \u03b20), we have by Lemma 11 (i),\nPr\n( 1\nn \u03c8(E+x ) \u2264 (1\u2212 \u03c1\u0302)\u03a6\u0302\n) \u2264 Pr ( 1\nn \u03c8(E+x ) \u2264 (1\u2212 \u03b2)(1\u2212 \u03c1)\u03a6(V ) ) \u2264 exp ( \u2212n\u03b2\n2(1\u2212 \u03c1)\u03a6(V ) 2 ) \u2264 exp ( \u2212n(1\u2212 \u03c1)\u03a6\u0302\n2(1 + \u03b20) \u00b7 [ 1\u2212 (1\u2212 \u03c1\u0302)(1 + \u03b20) 1\u2212 \u03c1 ]2)\n\u2264 exp ( \u2212n(1\u2212 \u03c1\u0302/2)\u03a6\u0302 2(1 + \u03b20) \u00b7 [ 1\u2212 (1\u2212 \u03c1\u0302)(1 + \u03b20) 1\u2212 \u03c1\u0302/2 ]2) .\nTaking \u03b20 \u2264 \u03c1\u0302/4, the above is less than exp ( \u2212n\u03a6\u0302\u03c1\u0302 2\n32\n) . With n as in the lemma statement and combined\nwith our results on the concentration of \u03a6\u0302, we have that with probability 1\u2212 \u03b40\n1 n max\n{ \u03c8(E+x ), \u03c8(E \u2212 x ) } > (1\u2212 \u03c1\u0302)\u03a6\u0302.\nTo see (c), suppose now that w.l.o.g. ( \u03c0(V \u2212x ) \u03c0(V ) )2 \u03a6(V \u2212x ) \u2264 ( \u03c0(V +x ) \u03c0(V ) )2 \u03a6(V +x ) = (1 \u2212 \u03c1)\u03a6(V ). We need to\nconsider two cases.\nCase 1: \u03c1 \u2264 1/2. Taking \u03b2 such that (1 + \u03b2)(1\u2212 \u03c1) = (1\u2212 \u03c1\u0302)(1\u2212 \u03b20), we have by Lemma 11 (ii),\nPr\n( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1\u0302)\u03a6\u0302\n) \u2264 Pr ( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1\u0302)(1\u2212 \u03b20)\u03a6(V ) ) = Pr ( 1\nn \u03c8(E+x ) > (1 + \u03b2)(1\u2212 \u03c1)\u03a6(V ) ) \u2264 exp ( \u2212n\u03b2\n2(1\u2212 \u03c1)\u03a6(V ) 3 ) \u2264 exp ( \u2212n(1\u2212 \u03c1)\u03a6\u0302\n3(1 + \u03b20) \u00b7 [ (1\u2212 \u03c1\u0302)(1\u2212 \u03b20) 1\u2212 \u03c1 \u2212 1 ]2)\n\u2264 exp ( \u2212 n\u03a6\u0302 6(1 + \u03b20) \u00b7 [ (1\u2212 \u03c1\u0302)(1\u2212 \u03b20) 1\u2212 2\u03c1\u0302 \u2212 1 ]2) .\nTaking \u03b20 \u2264 \u03c1\u0302/4, the above is less than exp ( \u2212n\u03a6\u0302\u03c1\u0302 2\n12\n) . Because ( \u03c0(V \u2212x ) \u03c0(V ) )2 \u03a6(V \u2212x ) \u2264 ( \u03c0(V +x ) \u03c0(V ) )2 \u03a6(V +x ), we\nalso can say\nPr\n( 1\nn \u03c8(E\u2212x ) > (1\u2212 \u03c1\u0302)\u03a6\u0302\n) \u2264 exp ( \u2212n\u03a6\u0302\u03c1\u0302 2\n12\n) .\nCase 2: \u03c1 > 1/2. Taking \u03b20 \u2264 1/16, we have\nPr\n( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1\u0302)\u03a6\u0302\n) \u2264 Pr ( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1\u0302)(1\u2212 \u03b20)\u03a6(V ) ) = Pr ( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1)\u03a6(V ) + ((1\u2212 \u03c1\u0302)(1\u2212 \u03b20)\u2212 (1\u2212 \u03c1))\u03a6(V ) ) \u2264 Pr ( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1)\u03a6(V ) + (\u03c1\u2212 \u03c1\u0302\u2212 \u03b20)\u03a6(V ) ) \u2264 Pr ( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1)\u03a6(V ) + (\u03c1 2 \u2212 \u03b20 ) \u03a6(V ) ) \u2264 Pr ( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1)\u03a6(V ) +\n( 1\n4 \u2212 \u03b20\n) \u03a6(V ) ) \u2264 Pr ( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1)\u03a6(V ) + 1 4 \u2212 \u03b20 1 + \u03b20 \u03a6\u0302 ) \u2264 Pr ( 1\nn \u03c8(E+x ) > (1\u2212 \u03c1)\u03a6(V ) +\n3\n17 \u03a6\u0302\n)\nBy Lemma 10, the above is less than exp ( \u2212n\u03a6\u0302 2\n40\n) . Because ( \u03c0(V \u2212x ) \u03c0(V ) )2 \u03a6(V \u2212x ) \u2264 ( \u03c0(V +x ) \u03c0(V ) )2 \u03a6(V +x ), we also\ncan say\nPr\n( 1\nn \u03c8(E\u2212x ) > (1\u2212 \u03c1\u0302)\u03a6\u0302\n) \u2264 exp ( \u2212n\u03a6\u0302 2\n40\n) .\nRegardless of which case we are in, we have for n as in the lemma statement, with probability 1\u2212 \u03b40,\n1 n max\n{ \u03c8(E+x ), \u03c8(E \u2212 x ) } \u2264 (1\u2212 \u03c1\u0302)\u03a6\u0302.\nWe next provide the proof of Lemma 7.\nLemma 7. The following holds for DBAL:\n(a) Suppose that for all t = 1, 2, . . . ,K that \u03a6(Vt) > . Then the probability that the termination condition is ever true for any of those rounds is bounded above by K exp ( \u2212 n32 ) .\n(b) Suppose that for some t = 1, 2, . . . ,K that \u03a6(Vt) \u2264 /2. Then the probability that the termination condition is not true in that round is bounded above by K exp ( \u2212 n48 ) .\nProof. Recall that the termination condition from DBAL is 1n\u03c8(E) < 3 4 for E \u223c (\u03c0|V ) 2\u00d7n. Part (a) follows from plugging in \u03b2 = 14 into Lemma 11 (i) and taking a union bound over rounds 1, . . . ,K. Similarly, part (b) follows from plugging in \u03b2 = 14 into Lemma 11 (ii) and taking a union bound over rounds 1, . . . ,K."}], "references": [{"title": "Fast probabilistic algorithms for hamiltonian circuits and matchings", "author": ["Dana Angluin", "Leslie G Valiant"], "venue": "In Proceedings of the ninth annual ACM symposium on Theory of computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1977}, {"title": "Agnostic active learning", "author": ["Maria-Florina Balcan", "Alina Beygelzimer", "John Langford"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Margin based active learning", "author": ["Maria-Florina Balcan", "Andrei Broder", "Tong Zhang"], "venue": "In International Conference on Computational Learning Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Active and passive learning of linear separators under log-concave distributions", "author": ["Maria-Florina Balcan", "Phil Long"], "venue": "In Proceedings of the 26th Conference on Learning Theory,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Importance weighted active learning", "author": ["Alina Beygelzimer", "Sanjoy Dasgupta", "John Langford"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Minimax bounds for active learning", "author": ["Rui M Castro", "Robert D Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Learning unknown graphs", "author": ["Nicolo Cesa-Bianchi", "Claudio Gentile", "Fabio Vitale"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Worst-case analysis of selective sampling for linear classification", "author": ["Nicolo Cesa-Bianchi", "Claudio Gentile", "Luca Zaniboni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Improving generalization with active learning", "author": ["David Cohn", "Les Atlas", "Richard Ladner"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "An efficient graph based active learning algorithm with application to nonparametric classification", "author": ["Gautam Dasarathy", "Robert Nowak", "Xiaojin Zhu"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Analysis of a greedy active learning strategy", "author": ["Sanjoy Dasgupta"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Coarse sample complexity bounds for active learning", "author": ["Sanjoy Dasgupta"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Hierarchical sampling for active learning", "author": ["Sanjoy Dasgupta", "Daniel Hsu"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Analysis of perceptron-based active learning", "author": ["Sanjoy Dasgupta", "Adam Tauman Kalai", "Claire Monteleoni"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A general agnostic active learning algorithm", "author": ["Sanjoy Dasgupta", "Claire Monteleoni", "Daniel J Hsu"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Yoav Freund", "H Sebastian Seung", "Eli Shamir", "Naftali Tishby"], "venue": "Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Query by committee made real", "author": ["Ran Gilad-Bachrach", "Amir Navot", "Naftali Tishby"], "venue": "In Proceedings of the 18th International Conference on Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["Daniel Golovin", "Andreas Krause", "Debajyoti Ray"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Average-case active learning with costs", "author": ["Andrew Guillory", "Jeff Bilmes"], "venue": "In International Conference on Algorithmic Learning Theory,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "A bound on the label complexity of agnostic active learning", "author": ["Steve Hanneke"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["Wassily Hoeffding"], "venue": "Journal of the American statistical association,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1963}, {"title": "Hierarchical label queries with data-dependent partitions", "author": ["Samory Kpotufe", "Ruth Urner", "Shai Ben-David"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Beyond disagreement-based agnostic active learning", "author": ["Chicheng Zhang", "Kamalika Chaudhuri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}], "referenceMentions": [{"referenceID": 11, "context": "An exception to this general trend is the aggressive strategy of [12], whose label complexity is known to be optimal in its dependence on a key parameter called the splitting index.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "But what is the right notion of size? Dasgupta [12] pointed out that the diameter of the version space is what matters, where the distance between two classifiers is taken to be the fraction of points on which they make different predictions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "Dasgupta [12] pointed out that the label complexity of active learning depends on the underlying distribution, the amount of unlabeled data (since more data means greater potential for highly-informative points), and also the target classifier h\u2217.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": ") One scheme begins by building a neighborhood graph on the unlabeled data, and propagating queried labels along the edges of this graph [24, 7, 10].", "startOffset": 137, "endOffset": 148}, {"referenceID": 9, "context": ") One scheme begins by building a neighborhood graph on the unlabeled data, and propagating queried labels along the edges of this graph [24, 7, 10].", "startOffset": 137, "endOffset": 148}, {"referenceID": 12, "context": "Another starts with a hierarchical clustering of the data and moves down the tree, sampling at random until it finds clusters that are relatively pure in their labels [13].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "The label complexity of such methods have typically be given in terms of smoothness properties of the underlying data distribution [6, 22].", "startOffset": 131, "endOffset": 138}, {"referenceID": 21, "context": "The label complexity of such methods have typically be given in terms of smoothness properties of the underlying data distribution [6, 22].", "startOffset": 131, "endOffset": 138}, {"referenceID": 2, "context": "Another line of work has focused on active learning of linear separators, by querying points close to the current guess at the decision boundary [3, 14, 4].", "startOffset": 145, "endOffset": 155}, {"referenceID": 13, "context": "Another line of work has focused on active learning of linear separators, by querying points close to the current guess at the decision boundary [3, 14, 4].", "startOffset": 145, "endOffset": 155}, {"referenceID": 3, "context": "Another line of work has focused on active learning of linear separators, by querying points close to the current guess at the decision boundary [3, 14, 4].", "startOffset": 145, "endOffset": 155}, {"referenceID": 7, "context": "Interestingly, regret guarantees for online algorithms of this sort can be shown under far weaker conditions [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 8, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 14, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 4, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 1, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 22, "context": "Some of these schemes [9, 15, 5, 2, 23] are fairly mellow in the sense described earlier, using generalization bounds to gauge which labels can be inferred from those obtained so far.", "startOffset": 22, "endOffset": 39}, {"referenceID": 19, "context": "The label complexity of these methods can be bounded in terms of a quantity known as the disagreement coefficient [20].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "In the realizable case, the canonical such algorithm is that of [9], henceforth referred to as CAL.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16].", "startOffset": 108, "endOffset": 120}, {"referenceID": 18, "context": "These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16].", "startOffset": 108, "endOffset": 120}, {"referenceID": 17, "context": "These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16].", "startOffset": 108, "endOffset": 120}, {"referenceID": 15, "context": "These methods typically aim to shrink the mass of the version space under \u03c0, either greedily and explicitly [11, 19, 18] or implicitly [16].", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "This motivates using the diameter of the version space as a yardstick, which was first proposed in [12] and is taken up again here.", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "The following theorem, due to Dasgupta [12], bounds the sample complexity of active learning in terms of the splitting index.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "Dasgupta [12] derived the splitting indices for several hypothesis classes, including intervals and homogeneous linear separators.", "startOffset": 9, "endOffset": 13}, {"referenceID": 20, "context": "If we knew the value \u03c1 a priori, then Lemma 4 combined with standard concentration bounds [21, 1] would give us a relatively straightforward procedure to find a good query point: 1.", "startOffset": 90, "endOffset": 97}, {"referenceID": 0, "context": "If we knew the value \u03c1 a priori, then Lemma 4 combined with standard concentration bounds [21, 1] would give us a relatively straightforward procedure to find a good query point: 1.", "startOffset": 90, "endOffset": 97}, {"referenceID": 20, "context": "Then Hoeffding\u2019s inequality [21], combined with Lemma 4, tells us that there exist sequences n, \u03b4n \u2198 0 such that with probability at least 1\u2212 3\u03b4n, the following hold simultaneously:", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "[17] demonstrated that using samples generated by the hit-and-run Markov chain works well in practice.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "To date, the tightest upper and lower-bounds for the active learning of general concept classes have been in terms of a parameter of the learning problem called the splitting index. We provide, for the first time, an efficient algorithm that is able to realize this upper bound, and we empirically demonstrate its good performance.", "creator": "LaTeX with hyperref package"}}}