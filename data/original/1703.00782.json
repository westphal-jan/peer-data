{"id": "1703.00782", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Lock-Free Parallel Perceptron for Graph-based Dependency Parsing", "abstract": "Dependency parsing is an important NLP task. A popular approach for dependency parsing is structured perceptron. Still, graph-based dependency parsing has the time complexity of $O(n^3)$, and it suffers from slow training. To deal with this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can make full use of a multi-core computer which saves a lot of training time. Based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads, and with no loss at all in accuracy.", "histories": [["v1", "Thu, 2 Mar 2017 13:49:23 GMT  (29kb)", "http://arxiv.org/abs/1703.00782v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu sun", "shuming ma"], "accepted": false, "id": "1703.00782"}, "pdf": {"name": "1703.00782.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["shumingma}@pku.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n00 78\n2v 1\n[ cs\n.C L\n] 2\nM ar\n2 01\n7\ntask. A popular approach for dependency parsing is structured perceptron. Still, graph-based dependency parsing has the time complexity of O(n3), and it suffers from slow training. To deal with this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can make full use of a multicore computer which saves a lot of training time. Based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads, and with no loss at all in accuracy."}, {"heading": "1 Introduction", "text": "Dependency parsing is an important task in natural language processing. It tries to match head-child pairs for the words in a sentence and forms a directed graph (a dependency tree). Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006).\nStructured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order\nmodel while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters.\nRecently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing. Chen and Manning (2014) improved the performance of neural network dependency parsing algorithm while Le and Zuidema (2014) improved the parser with Inside-Outside Recursive Neural Network. However, those deep learning methods are very slow during training (Sun, 2016).\nTo address those issues, we hope to implement a simple and very fast dependency parser, which can at the same time achieve state-of-the-art accuracies. To reach this target, we propose a lock-free parallel algorithm called lock-free parallel perceptron. We use lock-free parallel perceptron to train the parameters for dependency parsing. Although lots of studies implemented perceptron for dependency parsing, rare studies try to implement lockfree parallel algorithms. McDonald et al. (2010) proposed a distributed perceptron algorithm. Nevertheless, this parallel method is not a lock-free version on shared memory systems. To the best of our knowledge, our proposal is the first lock-free parallel version of perceptron learning.\nOur contribution can be listed as follows:\n\u2022 The proposed method can achieve 8-fold faster speed of training than the baseline sys-\ntem when using 10 threads, and without additional memory cost.\n\u2022 We provide theoretical analysis of the parallel perceptron, and show that it is conver-\ngence even with the worst case of full delay. The theoretical analysis is for general lockfree parallel perceptron, not limited by this specific task of dependency parsing.\nAlgorithm 1 Lock-free parallel perceptron\n1: input: Training examples {(xi, yi)} n i=1\n2: initialize: \u03b1 = 0 3: repeat 4: for all Parallelized threads do 5: Get a random example (xi, yi) 6: y\u2032 = argmaxz\u2208GEN(x)\u03a6(x, y) \u00b7 \u03b1 7: if (y\u2032 6= y) then \u03b1 = \u03b1+\u03a6(x, y)\u2212 \u03a6(x, y\u2032) 8: end for 9: until Convergence 10: 11: return The averaged parameters \u03b1\u2217"}, {"heading": "2 Lock-Free Parallel Perceptron for Dependency Parsing", "text": "The dataset can be denoted as {(xi, yi)} n i=1 while xi is input and yi is correct output. GEN is a function which enumerates a set of candidates GEN(x) for input x. \u03a6(x, y) is the feature vector corresponding to the input output pair (x, y). Finally, the parameter vector is denoted as \u03b1.\nIn structured perceptron, the score of an input\noutput pair is calculated as follows:\ns(x, y) = \u03a6(x, y) \u00b7 \u03b1 (1)\nThe output of structured perceptron is to generate the structure y\u2032 with the highest score in the candidate set GEN(x).\nIn dependency parsing, the input x is a sentence while the output y is a dependency tree. An edge is denoted as (i, j) with a head i and its child j. Each edge has a feature representation denoted as f(i, j) and the score of edge can be written as follows:\ns(i, j) = \u03b1 \u00b7 f(i, j) (2)\nSince the dependency tree is composed of edges, the score are as follows:\ns(x, y) = \u2211\n(i,j)\u2208y\ns(i, j) = \u2211\n(i,j)\u2208y\n\u03b1 \u00b7 f(i, j) (3)\n\u03a6(x, y) = \u2211\n(i,j)\u2208y\nf(i, j) (4)\nThe proposed lock-free parallel perceptron is a variant of structured perceptron (Sun et al., 2009, 2013; Sun, 2015). We parallelize the decoding process of several examples and update the parameter vector on a shared memory system. In each step, parallel perceptron finds out the dependency tree y\u2032 with the highest score, and then updates the parameter vector immediately, without any lock of the shared memory. In typical parallel learning setting, the shared memory should be locked,\nso that no other threads can modify the model parameter when this thread is computing the update term. Hence, with the proposed method the learning can be fully parallelized. This is substantially different compared with the setting of McDonald et al. (2010), in which it is not lock-free parallel learning."}, {"heading": "3 Convergence Analysis of Lock-Free Parallel Perceptron", "text": "For lock-free parallel learning, it is very important to analyze the convergence properties, because in most cases lock-free learning leads to divergence of the training (i.e., the training fails). Here, we prove that lock-free parallel perceptron is convergent even with the worst case assumption. The challenge is that several threads may update and overwrite the parameter vector at the same time, so we have to prove the convergence.\nWe follow the definition in Collins\u2019s work (Collins, 2002). We write GEN(x) as all incorrect candidates generated by input x. We define that a training example is separable with margin \u03b4 > 0 if \u2203U with \u2016U\u2016 = 1 such that\n\u2200z \u2208 GEN(x), U \u00b7\u03a6(x, y)\u2212U \u00b7\u03a6(x, z) \u2265 \u03b4 (5)\nSince multiple threads are running at the same time in lock-free parallel perceptron training, the convergence speed is highly related to the delay of update. Lock-free learning has update delay, so that the update term may be applied on a \u201cold\u201d parameter vector, because this vector may have already be modified by other threads (because it is lock-free) and the current thread does not know that. Our analysis show that the perceptron learning is still convergent, even with the worst case that all of the k threads are delayed. To our knowledge, this is the first convergence analysis for lockfree parallel learning of perceptrons.\nWe first analyze the convergence of the worse case (full delay of update). Then, we analyze the convergence of optimal case (minimal delay). In experiments we will show that the real-world application is close to the optimal case of minimal delay."}, {"heading": "3.1 Worst Case Convergence", "text": "Suppose we have k threads and we use j to denote the j\u2019th thread, each thread updates the parameter vector as follows:\ny\u2032j = argmax z\u2208GEN(x) \u03a6j(x, y) \u00b7 \u03b1 (6)\nRecall that the update is as follows:\n\u03b1i+1 = \u03b1i +\u03a6j(x, y)\u2212 \u03a6j(x, y \u2032 j) (7)\nHere, y\u2032j and \u03a6j(x, y) are both corresponding to jth thread while \u03b1i is the parameter vector after ith time stamp.\nSince we adopt lock-free parallel setting, we suppose there are k perceptron updates in parallel in each time stamp. Then, after a time step, the overall parameters are updated as follows:\n\u03b1t+1 = \u03b1t + k\u2211\nj=1\n(\u03a6j(x, y)\u2212 \u03a6j(x, y \u2032 j)) (8)\nHence, it goes to:\nU \u00b7 \u03b1t+1 = U \u00b7 \u03b1t +\nk\u2211\nj=1\nU \u00b7 (\u03a6j(x, y)\u2212 \u03a6j(x, y \u2032 j))\n\u2265 U \u00b7 \u03b1t + k\u03b4\nwhere \u03b4 is the separable margin of data, following the same definition of Collins (2002). Since the initial parameter \u03b1 = 0, we will have that U \u00b7 \u03b1t+1 \u2265 tk\u03b4 after t time steps. Because U \u00b7 \u03b1t+1 \u2264 \u2016U\u2016\u2016\u03b1t+1\u2016, we can see that\n\u2016\u03b1t+1\u2016 \u2265 tk\u03b4 (9)\nOn the other hand, \u2016\u03b1t+1\u2016 can be written as:\n\u2016\u03b1t+1\u20162 = \u2016\u03b1t\u20162 + \u2016 k\u2211\nj=1\n(\u03a6j(x, y)\u2212 \u03a6j(x, y \u2032 j))\u2016 2\n+ 2\u03b1t \u00b7 ( k\u2211\nj=1\n(\u03a6j(x, y)\u2212 \u03a6j(x, y \u2032 j)))\n\u2264 \u2016\u03b1t\u20162 + k2R2\nwhere R is the same definition following Collins (2002) such that \u03a6(x, y) \u2212 \u03a6(x, y\u2032j) \u2264 R. The last inequality is based on the property of perceptron update such that the incorrect score is always higher than the correct score (the searched incorrect structure has the highest score) when an update happens. Thus, it goes to:\n\u2016\u03b1t+1\u20162 \u2264 tk2R2 (10)\nCombining Eq.10 and Eq.9, we have:\nt2k2\u03b42 \u2264 \u2016\u03b1t+1\u20162 \u2264 tk2R2 (11)\nHence, we have:\nt \u2264 R2/\u03b42 (12)\nThis proves that the lock-free parallel perceptron has bounded number of time steps before convergence even with the worst case of full delay, and the number of time steps is bounded by t \u2264 R2/\u03b42 in the worst case. The worst case means that the parallel perceptron is convergent even if the update is extremely delayed, such that k threads are updating based on the same old parameter vector."}, {"heading": "3.2 Optimal Case Convergence", "text": "In practice the worst case of extremely delayed update is not probable to happen, or at least not always happening. Thus, we expect that the real convergence speed should be much faster than this worst case bound. The optimal bound is as follows:\nt \u2264 R2/(k\u03b42) (13)\nThis bound is derived when the parallel update is not delayed (i.e., the update of each thread is based on a most recent parameter vector). As we can see, in the optimal case we can get k times speed up by using k threads for lock-free parallel perceptron training. This can achieve full acceleration of training by using parallel learning."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Dataset", "text": "Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) to evaluate our\nproposed approach. We follow the standard split of the corpus, using section 2-21 as training set, section 22 as development set, and section 23 as final test set. We implement two popular model of graph-based dependency parsing: first-order model and second-order model. We tune all of the hyper parameters in development set. The features in our model can be found in McDonald et al. (2005; 2006). Our baselines are traditional perceptron, MST-Parser (McDonald et al., 2005)1, and the locked version of parallel perceptron. All of the experiment is conducted on a computer with the Intel(R) Xeon(R) 3.0GHz CPU."}, {"heading": "4.2 Results", "text": "Table 2 shows that our lock-free method can achieve 8-fold faster speed than the baseline system, which is better speed up when compared with locked parallel perceptron. For both 1storder parsing and 2nd-order parsing, the results are consistent that the proposed lock-free method achieves the best rate of speed up. The results show that the lock-free parallel peceptron in real-\n1www.seas.upenn.edu/s\u0303trctlrn/MSTParser/MSTParser.html\nworld applications is near the optimal case theoretical analysis of low delay, rather than the worst case theoretical analysis of high delay.\nThe experimental results of accuracy are shown in Table 1. The baseline MSTParser (McDonald et al., 2005) is a popular system for dependency parsing. Table 1 shows that our method with 10 threads outperforms the system with single-thread. Our lock system is slightly better than MST-Parser mainly because we use more feature including distance based feature \u2013 our distance features are based on larger size of contextual window.\nFigure 1 shows that the lock-free parallel perceptron has no loss at all on parsing accuracy on both 1st-order and 2nd-order parsing setting, in spite of the substantial speed up of training.\nFigure 2 shows that the method can achieve near linear speed up, and with almost no extra memory cost."}, {"heading": "5 Conclusions", "text": "We propose lock-free parallel perceptron for graph-based dependency parsing. Our experiment\nshows that it can achieve more than 8-fold faster speed than the baseline when using 10 running threads, and with no loss in accuracy. We also provided convergence analysis for lock-free parallel perceptron, and show that it is convergent in the lock-free learning setting. The lock-free parallel perceptron can be directly used for other structured prediction NLP tasks."}, {"heading": "6 Acknowledgements", "text": "This work was supported in part by National Natural Science Foundation of China (No. 61673028), and National High Technology Research and Development Program of China (863 Program, No. 2015AA015404). Xu Sun is the corresponding author of this paper."}], "references": [{"title": "Very high accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics. pages 89\u201397.", "citeRegEx": "Bohnet.,? 2010", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Associa-", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Koby Crammer", "Yoram Singer."], "venue": "The Journal of Machine Learning Research 2:265\u2013292.", "citeRegEx": "Crammer and Singer.,? 2002", "shortCiteRegEx": "Crammer and Singer.", "year": 2002}, {"title": "Three new probabilistic models for dependency parsing: An exploration", "author": ["Jason Eisner."], "venue": "Proceedings of the 16th conference on Computational linguistics. pages 340\u2013345.", "citeRegEx": "Eisner.,? 1996", "shortCiteRegEx": "Eisner.", "year": 1996}, {"title": "Efficient thirdorder dependency parsers", "author": ["Terry Koo", "Michael Collins."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. pages 1\u201311.", "citeRegEx": "Koo and Collins.,? 2010", "shortCiteRegEx": "Koo and Collins.", "year": 2010}, {"title": "The insideoutside recursive neural network model for dependency parsing", "author": ["Phong Le", "Willem Zuidema."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 729\u2013739.", "citeRegEx": "Le and Zuidema.,? 2014", "shortCiteRegEx": "Le and Zuidema.", "year": 2014}, {"title": "Building a large annotated", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Online large-margin training of dependency parsers", "author": ["Ryan McDonald", "Koby Crammer", "Fernando Pereira."], "venue": "Proceedings of the 43rd annual meeting on association for computational linguistics. pages 91\u201398.", "citeRegEx": "McDonald et al\\.,? 2005", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Distributed training strategies for the structured perceptron", "author": ["Ryan McDonald", "Keith Hall", "Gideon Mann."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "McDonald et al\\.,? 2010", "shortCiteRegEx": "McDonald et al\\.", "year": 2010}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["Ryan T. McDonald", "Fernando C.N. Pereira."], "venue": "11st Conference of the European Chapter of the Association for Computational Linguistics.", "citeRegEx": "McDonald and Pereira.,? 2006", "shortCiteRegEx": "McDonald and Pereira.", "year": 2006}, {"title": "Towards shockingly easy structured classification: A search-based probabilistic online learning framework", "author": ["Xu Sun."], "venue": "Technical report, arXiv:1503.08381 .", "citeRegEx": "Sun.,? 2015", "shortCiteRegEx": "Sun.", "year": 2015}, {"title": "Asynchronous parallel learning for neural networks and structured models with dense features", "author": ["Xu Sun."], "venue": "COLING.", "citeRegEx": "Sun.,? 2016", "shortCiteRegEx": "Sun.", "year": 2016}, {"title": "Latent structured perceptrons for large-scale learning with hidden information", "author": ["Xu Sun", "Takuya Matsuzaki", "Wenjie Li."], "venue": "IEEE Trans. Knowl. Data Eng. 25(9):2063\u20132075.", "citeRegEx": "Sun et al\\.,? 2013", "shortCiteRegEx": "Sun et al\\.", "year": 2013}, {"title": "Latent variable perceptron algorithm for structured classification", "author": ["Xu Sun", "Takuya Matsuzaki", "Daisuke Okanohara", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Sun et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2009}, {"title": "Maxmargin parsing", "author": ["Ben Taskar", "Dan Klein", "Michael Collins", "Daphne Koller", "Christopher D Manning."], "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP). volume 1, page 3.", "citeRegEx": "Taskar et al\\.,? 2004", "shortCiteRegEx": "Taskar et al\\.", "year": 2004}, {"title": "A latent variable model for generative dependency parsing", "author": ["Ivan Titov", "James Henderson."], "venue": "Proceedings of the 10th International Conference on Parsing Technologies, pages 144\u2013155.", "citeRegEx": "Titov and Henderson.,? 2007", "shortCiteRegEx": "Titov and Henderson.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006).", "startOffset": 74, "endOffset": 116}, {"referenceID": 10, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006).", "startOffset": 74, "endOffset": 116}, {"referenceID": 3, "context": "The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004).", "startOffset": 204, "endOffset": 251}, {"referenceID": 15, "context": "The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004).", "startOffset": 204, "endOffset": 251}, {"referenceID": 12, "context": "However, those deep learning methods are very slow during training (Sun, 2016).", "startOffset": 67, "endOffset": 78}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al.", "startOffset": 75, "endOffset": 253}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing.", "startOffset": 75, "endOffset": 280}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al.", "startOffset": 75, "endOffset": 407}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model.", "startOffset": 75, "endOffset": 708}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model.", "startOffset": 75, "endOffset": 773}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters. Recently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing.", "startOffset": 75, "endOffset": 962}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters. Recently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing. Chen and Manning (2014) improved the performance of neural network dependency parsing algorithm while Le and Zuidema (2014) improved the parser with Inside-Outside Recursive Neural Network.", "startOffset": 75, "endOffset": 1065}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters. Recently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing. Chen and Manning (2014) improved the performance of neural network dependency parsing algorithm while Le and Zuidema (2014) improved the parser with Inside-Outside Recursive Neural Network.", "startOffset": 75, "endOffset": 1165}, {"referenceID": 0, "context": "Former researchers have proposed various models to deal with this problem (Bohnet, 2010; McDonald and Pereira, 2006). Structured perceptron is one of the most popular approaches for graph-based dependency parsing. It is first proposed by Collins (2002) and McDonald et al. (2005) first applied it to dependency parsing. The model of McDonald is decoded with an efficient algorithm proposed by Eisner (1996) and they trained the model with structured perceptron as well as its variant Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2002; Taskar et al., 2004). It proves that MIRA and structured perceptron are effective algorithms for graph-based dependency parsing. McDonald and Pereira (2006) extended it to a second-order model while Koo and Collins (2010) developed a third-order model. They all used perceptron style methods to learn the parameters. Recently, many models applied deep learning to dependency parsing. Titov and Henderson (2007) first proposed a neural network model for transition-based dependency parsing. Chen and Manning (2014) improved the performance of neural network dependency parsing algorithm while Le and Zuidema (2014) improved the parser with Inside-Outside Recursive Neural Network. However, those deep learning methods are very slow during training (Sun, 2016). To address those issues, we hope to implement a simple and very fast dependency parser, which can at the same time achieve state-of-the-art accuracies. To reach this target, we propose a lock-free parallel algorithm called lock-free parallel perceptron. We use lock-free parallel perceptron to train the parameters for dependency parsing. Although lots of studies implemented perceptron for dependency parsing, rare studies try to implement lockfree parallel algorithms. McDonald et al. (2010) proposed a distributed perceptron algorithm.", "startOffset": 75, "endOffset": 1805}, {"referenceID": 11, "context": "The proposed lock-free parallel perceptron is a variant of structured perceptron (Sun et al., 2009, 2013; Sun, 2015).", "startOffset": 81, "endOffset": 116}, {"referenceID": 8, "context": "This is substantially different compared with the setting of McDonald et al. (2010), in which it is not lock-free parallel learning.", "startOffset": 61, "endOffset": 84}, {"referenceID": 2, "context": "We follow the definition in Collins\u2019s work (Collins, 2002).", "startOffset": 43, "endOffset": 58}, {"referenceID": 2, "context": "where \u03b4 is the separable margin of data, following the same definition of Collins (2002). Since the initial parameter \u03b1 = 0, we will have that U \u00b7 \u03b1t+1 \u2265 tk\u03b4 after t time steps.", "startOffset": 74, "endOffset": 89}, {"referenceID": 2, "context": "where R is the same definition following Collins (2002) such that \u03a6(x, y) \u2212 \u03a6(x, y j) \u2264 R.", "startOffset": 41, "endOffset": 56}, {"referenceID": 7, "context": "Following prior work, we use English Penn TreeBank (PTB) (Marcus et al., 1993) to evaluate our", "startOffset": 57, "endOffset": 78}, {"referenceID": 8, "context": "Our baselines are traditional perceptron, MST-Parser (McDonald et al., 2005), and the locked version of parallel perceptron.", "startOffset": 53, "endOffset": 76}, {"referenceID": 8, "context": "The baseline MSTParser (McDonald et al., 2005) is a popular system for dependency parsing.", "startOffset": 23, "endOffset": 46}], "year": 2017, "abstractText": "Dependency parsing is an important NLP task. A popular approach for dependency parsing is structured perceptron. Still, graph-based dependency parsing has the time complexity of O(n3), and it suffers from slow training. To deal with this problem, we propose a parallel algorithm called parallel perceptron. The parallel algorithm can make full use of a multicore computer which saves a lot of training time. Based on experiments we observe that dependency parsing with parallel perceptron can achieve 8-fold faster training speed than traditional structured perceptron methods when using 10 threads, and with no loss at all in accuracy.", "creator": "LaTeX with hyperref package"}}}