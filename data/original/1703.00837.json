{"id": "1703.00837", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Meta Networks", "abstract": "Deep neural networks have been successfully applied in applications with a large amount of labeled data. However, there are major drawbacks of the neural networks that are related to rapid generalization with small data and continual learning of new concepts without forgetting. We present a novel meta learning method, Meta Networks (MetaNet), that acquires a meta-level knowledge across tasks and shifts its inductive bias via fast parameterization for the rapid generalization. When tested on the standard one-shot learning benchmarks, our MetaNet models achieved near human-level accuracy. We demonstrated several appealing properties of MetaNet relating to generalization and continual learning.", "histories": [["v1", "Thu, 2 Mar 2017 15:52:55 GMT  (250kb,D)", "http://arxiv.org/abs/1703.00837v1", "initial submission"], ["v2", "Thu, 8 Jun 2017 16:12:40 GMT  (254kb,D)", "http://arxiv.org/abs/1703.00837v2", "Accepted at ICML 2017 - rewrote: the main section; added: MetaNet algorithmic procedure; performed: Mini-ImageNet evaluation"]], "COMMENTS": "initial submission", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["tsendsuren munkhdalai", "hong yu"], "accepted": true, "id": "1703.00837"}, "pdf": {"name": "1703.00837.pdf", "metadata": {"source": "META", "title": "Meta Networks", "authors": ["Tsendsuren Munkhdalai", "Hong Yu"], "emails": ["suren.munkhdalai@umassmed.edu>."], "sections": [{"heading": "1. Introduction", "text": "Deep neural networks have shown great success in several application domains with a large amount of labeled data available. However, the availability of such large data from which training signals can be extracted have been prerequisite. Furthermore the standard deep neural networks lack an ability to incrementally learn new concepts on the fly, without forgetting the old knowledge.\nIn contrast, humans can rapidly learn and generalize from one or two examples of the same concept. Humans are also very good at incremental (i.e. continual) learning. These abilities are mostly explained by the meta learning (i.e. learning to learn) process in the brain (Harlow, 1949).\nPrevious work on meta learning has formulated the problem as a two-level learning, a slow learning of a meta-level model performing across tasks and a rapid learning of a base-level model acting within each task (Mitchell et al., 1993; Vilalta & Drissi, 2002). The goal of the meta-level learner is to acquire generic knowledge of different tasks. The knowledge can then be transferred to the base-level\n1University of Massachusetts, MA, USA. Correspondence to: Tsendsuren Munkhdalai <tsendsuren.munkhdalai@umassmed.edu>.\nMemory\nSlow weights Fast weights\nMeta weights\nSlow weights Fast weights\nInput Meta info Fast parameterization\nFast parameterization\nMemory access\nOutput\nMeta learner\nBase learner\nFigure 1. Overall architecture of Meta Networks.\nlearner to provide generalization in the context of a single task. The base and the meta-level models can be framed in a single learner (Schmidhuber, 1993) or in separate learners (Bengio et al., 1990; Hochreiter et al., 2001).\nIn this work we introduce a meta learning model called MetaNet (for Meta Networks) that supports meta-level continual learning by allowing neural networks to learn and to generalize new task or new concept from a single example on the fly. The overall architecture of MetaNet is shown in Figure 1. MetaNet consists of two main learning components, a base learner and a meta learner, and is equipped with an external memory. Learning occurs at two levels in separate spaces (i.e. meta space and task space). The base learner performs in the input task space whereas the meta learner operates in a task-agnostic meta space. By operating in the abstract meta space, the meta learner supports continual learning and performs meta knowledge acquisition across different tasks. To this end, the base learner first analyzes the input task. It then provides the meta learner with a meta-level information that explains the base learner\u2019s status in the current task space. Based on the meta information, the meta learner rapidly parameterizes both itself and the base learner so that the MetaNet model can recognize the new concepts of the input task. Specifically the training weights of MetaNet evolves at different timescales: standard slow weights updated through a learning algorithm (i.e. backpropagation), task-level fast weights ar X iv :1 70 3.\n00 83\n7v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\n7\nupdated within the scope of each task, and example-level fast weights updated for a specific input example. Finally MetaNet equipped with the external memory allows for such rapid learning and generalization.\nUnder the MetaNet framework, it is important to define types of the meta information which can be derived from the base learner. While other types of the meta information are also applicable, here we used the gradient of the base learner as meta information for an end-to-end training of MetaNet via backpropagation. We extensively studied its performance and characteristics on one-shot learning problems, under varying settings. Our proposed method not only improves the state-of-the-art results on standard benchmarks but also shows some interesting properties relating to generalization and continual learning."}, {"heading": "2. Related Work", "text": "Our work connects different threads of research in order to allow neural networks for rapid learning and generalization. The rapid learning and generalization refers to a one-shot learning scenario where a learner is introduced to a sequence of tasks, each with a single or few labeled examples of many classes. A key challenge in this setting is that the classes or concepts vary across the tasks. Due to this, one-shot learning problems have been widely addressed by generative models and metric learning methods. One notable success is reported by a probabilistic programming approach (Lake et al., 2015). They used specific knowledge of how pen strokes are composed to produce characters of different alphabets. Koch (2015) applied Siamese Networks to perform one-shot classification. Recently, Vinyals et al. (2016) unified the training and testing of a one-shot learner under the same procedure and developed an end-to-end differentiable nearest neighbor method for one-shot learning. Santoro et al. (2016) proposed a memory-based approach and trained Neural Turing Machines (Graves et al., 2014) for one-shot learning, although the meta-learner and the one-shot learner in this work are not separable explicitly. Their training procedure adapted the work of Hochreiter et al. (2001) in which they used LSTMs as the meta-level model. More recently an LSTMbased one-shot optimizer was proposed (Ravi & Larochell, 2017). By taking in the loss, the gradient and the parameters of the base learner, the meta optimizer was trained to update the parameters for one-shot classification.\nThere is a line of work on building meta optimizers (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017). As the main interest here is to train an optimization algorithm within the meta learning framework, the previous research have mainly focused on tasks with large datasets. In contrast, with the absent of large datasets, we emphasize the difficulties to optimize a neural network\nwith a large number of parameters to generalize with limited examples of new concept. Our work instead proposes a novel rapid parameterization approach by employing the meta information. By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets. The ideas of fast weights and generating parameters for one neural network with another have been explored individually in separate contexts. Hinton & Plaut (1987) suggested the usage of fast weights for rapid learning. Ba et al. (2016) recently used fast weights to replace soft attention mechanism. Fast weights have also been used to implement recurrent nets (Schmidhuber, 1992). Those usages of fast weights are well motivated by the fact that synapses have dynamics at many different time-scales.\nThe approach proposed by Gomez & Schmidhuber (2005) is more closely related to our work. They used recurrent nets to generate fast weights for a single-layer network controller. De Brabandere et al. (2016) used one network to generate slow filter weights for a convolutional neural net whereas more recently David Ha & Le (2017) generated slow weights for recurrent nets. Our MetaNet generates fast weights at two time-scales by operating in meta space. To integrate the fast weights with the slow weights, we propose a novel layer augmentation approach.\nFinally, we note that our MetaNet equipped with an external memory can be seen as a memory augmented neural network (MANN). MANNs have shown promising results on a range of tasks starting from small programming problems (Graves et al., 2014) to large-scale language tasks (Weston et al., 2015; Munkhdalai & Yu, 2017)."}, {"heading": "3. Meta Networks", "text": "The model consists of two main learning modules (Figure 1). The meta learner that operates across tasks is responsible for fast weight parameterization. These fast weights shifts the inductive bias of a neural network and are integrated into the base learner and the meta learner. Particularly the base learner parameterized by the fast weights is now able to generalize well for the new task. We propose a novel layer augmentation method to integrate the standard slow weights and the task or example specific fast weights in a neural net."}, {"heading": "3.1. Base Learner", "text": "The base learner denoted as b is a function or a neural net that maps an input example x (i.e. image) to an output y (i.e. label). However, unlike standard neural net b is parameterized by slow weightsW and example-level fast weights W \u2217. The slow weights are updated via a learning algorithm\nduring training whereas the fast weights are generated for every input.\nBy using some sort of meta information the base learner should be able to explain its setting in the current task space in order for the meta learner to generate the fast weights. We obtain such meta information in the form of the gradient by first performing a forward pass and then backpropagating as follows:\nLi = lossb(b(W,xi), yi) (1)\n\u2207i = \u2207WLi (2)\nwhere Li is the loss for input and label pair {xi, yi}Ni=1. N is the number of support examples in the task set which usually contains a single instance per class in one-shot setup. \u2207i is the loss gradient with respect to parameters W and is our meta information. Note that the loss function lossb is generic and for one-shot classification we use cross-entropy loss. Now the meta learner takes in the gradient information and generates the fast parameters W \u2217. We will describe this later.\nAssuming that the fast weights W \u2217j for input xj is defined, the base learner performs the one-shot classification as:\nP (yj |xj ,W,W \u2217j ) = b(W,W \u2217j , xj) (3)\nwhere {xj}Lj=1 is an example drawn from the current task set. To train MetaNet, we use another L number of labeled examples other than the support set and minimize the crossentropy loss."}, {"heading": "3.2. Meta Learner", "text": "The meta learner processes the meta information along with the task support set and generates the fast weights.\nParticularly it transforms the loss gradient {\u2207i}Ni=1 to a set of fast weights {W \u2217i }Ni=1:\nW \u2217i = m(Z,\u2207i) (4)\nwhere m is a neural network with parameter Z. The fast weights are then stored in a memory M = {W \u2217i }Ni=1. The memory M is indexed with task dependent representations R = {ri}Ni=1 of the support set {xi}Ni=1, obtained by a dynamic key embedding function u. The embedding function u is a neural net parameterized by slow weights Q and task-level fast weights Q\u2217:\nri = u(Q,Q \u2217, xi) (5)\nwhere the parameters Q and Q\u2217 are integrated using the layer augmentation method. Note that the fast weights Q\u2217 are generated per task basis to perform a task-specific embedding of input. Therefore Q\u2217 is defined as:\nLt = lossu(u(Q, xt), yt) (6)\n\u2207t = \u2207QLt (7)\nQ\u2217 = mLSTM (G,\u2207t) (8)\nwhere mLSTM is LSTM and G is the LSTM parameters. First, we sample T examples (possibly T = N ) {xt, yt}Tt=1 from the support set and obtain the loss gradient as meta information. Then LSTM observes the gradient corresponding to each sampled example and summarizes into the task specific parameter. Thus the order of LSTM input does not matter and alternatively we can take summation or average of the gradients and use a MLP. However, in our preliminary experiment we observed that the latter results in a poor convergence. In a similar way defined in the Equation 6-8, we can also parameterize the base learner with task-level fast weights. We will report an ablation experiment on different variation of MetaNet in Section 4.\nThe loss, lossu does not need to align with the base learner\u2019s loss, but it should be able to capture a good representation learning objective. We use cross-entropy loss when the support set has only a single example per class. However, when there are more than one examples per class available, contrastive loss (Chopra et al., 2005) is a natural choice for lossu since both positive and negative samples can be formed. In this case, we randomly draw T number of pairs to observe the gradients and the loss is\nLt = lossu(u(Q, x1,t), u(Q, x2,t), lt) (9)\nwhere\nlt = { 1, if y1,t = y2,t 0, otherwise\n(10)\nOnce the parameters are stored in the memory M and the memory index R is constructed, the meta learner parameterizes the base learner with the fast weights Wj\u2217. First it embeds the input xj in the task space:\nrj = u(Q,Q \u2217, xj) (11)\nand then reads the memory with soft attention:\naj = attention(R, rj) (12)\nWj\u2217 = softmax(aj)>M (13)\nwhere attention calculates similarity between the memory index and the input embedding and we use cosine similarity as attention.\nThe meta learner fully parameterize its embedding function and the base learner and thus the shape of Wj\u2217 and Qj\u2217 are same as Wj and Q. The embedding function u can be seen as a dynamic embedding network that changes its parameters within the scope of the current input task. The training parameters of MetaNet \u03b8 consists of the slow weights W and Q and the meta weights Z and G (i.e. \u03b8 = {W,Q,Z,G}) and jointly updated via a training algorithm such as backpropagation to minimize the crossentropy loss (Equation 3). Note that lossb and lossu in Equations 1 and 6 are calculated to obtain the gradient by using the support set during both training and testing for the rapid parameterization and these losses are not used for \u03b8 parameter updates."}, {"heading": "3.3. Layer Augmentation", "text": "A slow weight layer in the base learner is extended with its corresponding fast weights for rapid generalization. An example of the layer augmentation approach applied to a MLP with single hidden layer is shown in Figure 2. Input to each layer is first transformed by both slow and fast weights and then it is passed through a non-linearity (i.e. ReLU ) resulting in two separate activation vectors. Finally the activation vectors are aggregated by an element-wise vector addition. For the last softmax layer, we first aggregate two transformed inputs and then normalize for classification output.\nOverall the layer augmentation building block is similar to that of ResNet architecture (He et al., 2016). However, ResNet has an identity transformation of a preceding activation (i.e. shortcut connection) and a non-linearity after the element-wise additive aggregation while our layer augmented net has a fast weight mapping which captures the input task prior and a non-linearity before the aggregation. Intuitively, the fast and slow weights in the layer augmented neural net can be seen as feature detectors operating in two distinct numeric domains. The application of the non-linearity maps them into the same domain, which\nis [0,\u221e) in the case of ReLU so that the activations can be aggregated and processed further. Our aggregation function here is element-wise sum."}, {"heading": "4. Results", "text": "We carried out one-shot classification experiments on two different image datasets: Omniglot and MNIST. The Omniglot dataset consists of images across 1623 classes with only 20 images per class, from 50 different alphabets (Lake et al., 2015). It also comes with a standard split of 30 training and 20 evaluation alphabets. Following (Santoro et al., 2016), we augmented the data set through 90, 180 and 270 degrees rotations. The images are resized to 28 x 28 pixels for computational efficiency. We used the MNIST images as an out-of-domain data throughout our experiments. MNIST was also downscaled to the same size."}, {"heading": "4.1. Training Details", "text": "To train and test MetaNet on one-shot learning, we adapted the training procedure introduced by Vinyals et al. (2016). First we split the Omniglot data into training and test sets consisting of two disjoint classes. We then formulate a series of tasks from the training set. Each task has a support set of N images and N different classes, resulting an N-way one-shot classification problem. In addition to the support set, we also include L number of labeled examples in each task set to update the parameters \u03b8 during training. For testing, we follow the same procedure to form a set of test tasks from the disjoint classes. However, now MetaNet assigns class labels toL examples based only on the labeled support set of each test task.\nFor the one-shot benchmarks reported in the following sections, we used a CNN as base learner b. This CNN has 5 convolutional layers, each of which is a 3 x 3 convolution with 64 filters, followed by a ReLU non-linearity and a 2 x 2 max-pooling and additional two layers of a fully connected (FC) layer and a softmax layer. Another CNN with the same architecture is used define the dynamic embedding function u and we take the output of the FC layer as the task dependent representation r. For the networks mLSTM and m, we used a single-layer LSTM with 20 hidden units and a three-layer MLP with 20 hidden units and ReLU non-linearity. As in Andrychowicz et al. (2016), the parameters G and Z of mLSTM and m are shared across the coordinates of the gradients \u2207 and the gradients are normalized using the same preprocessing rule (with p = 7) before it is given to the meta learner. The MetaNet parameters \u03b8 is optimized with ADAM. The initial learning rate was set to 10\u22123. The model parameters \u03b8 were randomly initialized from the uniform distribution over [-0.1, 0.1)."}, {"heading": "4.2. One-shot Learning Test", "text": "In this section we will report three different groups of benchmark experiments: Omniglot previous split, MNIST as out-of-domain data and Omniglot standard split."}, {"heading": "4.2.1. OMNIGLOT PREVIOUS SPLIT", "text": "Following the previous setup Vinyals et al. (2016), we split the Omniglot classes into 1200 and 423 classes for training and testing. We performed 5, 10, 15 and 20-way oneshot classification and compared our performance against the state-of-the-art results. We also studied three variations of MetaNet as an ablation experiment in order to show how fast parameterization affects the network dynamics.\nIn Table 1, we compared the performance of our models with all published models (as baselines). The first group of methods are the previously published models. The next group is MetaNet variations. MetaNet is the main architecture described in Section 3. MetaNet- is a variant without task-level fast weights Q\u2217 in the embedding function u whereas MetaNet+ has additional task-level weights for the base learner in addition to W \u2217. Our MetaNet model improves the previous best results by 0.5% to 2% accuracy. As the number of classes increases (moving from 5-way\nto 20-way classification), overall the performance of the one-shot learners decreases. MetaNet\u2019s performance drop is relatively small and it is around 2% while the drop for the other models ranges from 3% to 15%. As a result, our model shows an absolute improvement of 2% on 20-way one-shot task.\nComparing different MetaNet variations, the additional task-level weights in the base learner did not seem to help and even this model (MetaNet+) has shown a decreasing result. MetaNet- however performed surprisingly well but still falls behind the MetaNet model as it lacks the dynamic embedding function. This performance gap even increases when we test them on the following out-of-the domain setting."}, {"heading": "4.2.2. MNIST AS OUT-OF-DOMAIN DATA", "text": "We treated MNIST images as a separate domain data. Particularly a model is trained on the Omniglot training set and evaluated on the MNIST test set in 10-way one-shot learning setup. We hypothesize that models with a high dynamic should perform well on this task.\nIn Figure 3, we plotted the results of this experiment. MetaNet- achieved 71.6% accuracy which was 0.6% and 3.2% lower than the other variants with fast weights. This is not surprising since MetaNet without dynamic embedding function lacks an ability to adapt its parameters to MNIST image representations. The standard MetaNet model achieved 74.8% while MetaNet+ obtained 72.3% while Matching Net (Vinyals et al., 2016) reported 72.0% accuracy in this setup. Again we did not observe improvement with MetaNet+ model here. The standard MetaNet model has shown the best performance by effectively adapting its bias to the new data distribution."}, {"heading": "4.2.3. OMNIGLOT STANDARD SPLIT", "text": "Omniglot data comes with a standard split of 30 training alphabets with 964 classes and 20 evaluation alphabets with 659 classes. We trained and tested only the standard MetaNet model in this setup. In order to best match the evaluation protocol of Lake et al. (2015), we form 400 tasks\n(trials) from the evaluation classes to test the model.\nIn table 2, we listed the MetaNet results along with the previous models and human performance. Our MetaNet outperformed the human performance by a slight margin, but underperformed the probabilistic programming approach. However, the performance gap is rather small between these top three baselines. In addition while the probabilistic programming performs slightly better than MetaNet, our model does not rely on any extra prior knowledge about how characters and strokes are composed. Comparing the results on two Omniglot splits in Tables 1 and 2, MetaNet showed decreasing performances on the standard split. The later setup seems to be slightly difficult as the number of classes in the training set is less (1200 vs 964) and test classes are bigger (423 vs 659)."}, {"heading": "4.3. Generalization Test", "text": "We conducted a set of experiments to test the generalization of MetaNet from multiple aspects. The first experiment is to test whether a MetaNet model skilled on N-way oneshot task could generalize to another K-way task (where N 6= K) without actually training on the second task. The second experiment is set to see if a meta learner trained for rapid parameterization of a base learner btrain could parameterize other base learner beval during evaluation. The last group of experiments we performed in the scope of MetaNet generalization test is to show that MetaNet supports a meta-level continual learning."}, {"heading": "4.3.1. N-WAY TRAINING AND K-WAY TESTING", "text": "We first trained MetaNet on N-way one-shot classification task and then tested it on K-way one-shot tasks. The number of training and test classes are varied (i.e. N 6= K). To handle this, we inserted a softmax layer into the base learner during evaluation and then augmented it with the fast weights generated by the meta learner. If the meta learner is generic enough, it should be able to parameterize the new softmax layer on the fly. The new layer weights remained fixed since no parameter update was performed for this layer. The K-way test tasks were formed from the 423 unseen classes in the test set.\nThe MetaNet models were trained on one of 5, 10, 15 and 20-way one-shot tasks and evaluated on the rest. In Table 3, we summarized the results. As a comparison we also included some results from Table 1, which reports accuracy of N-way train and test setting. The MetaNet model trained on 5-way tasks obtained 93.07% of 20-way test accuracy which is still a closer match to Matching Network and higher than Siamese Net trained 20-way tasks. An interesting finding is that when N is smaller than K, i.e. the model is trained on easier tasks than test ones, we observe a decreasing performance. Conversely the models skilled on harder tasks (i.e. N > K) achieved increasing performances when tested on the easier tasks and the performance is even higher than the ones that were applied to the tasks with the same level difficulty (i.e. N = K). For example, the model skilled on 20-way classification improved the 5-way one-shot baseline by 0.6% showing a ceiling performance in this setting. We also conducted a preliminary experiment on more extreme test-time classification. MetaNet trained on 10-way task achieved around 65% on 100-way one-shot classification task.\nThis flexibly in MetaNet is crucial because one-shot learning usually involves an online concept identification scenario. By looking at the results, we can empirically obtain a performance lower or upper bound. Particularly the test performance obtained on the tasks with the same level difficulty that the model was skilled on can be used as a performance lower or an upper bound depending on a scenario under which the model will be deployed in the future. For\nexample, for the MetaNet model that will deployed under the N > K scenario, we can obtain the performance lower bound by testing it on the N = K tasks."}, {"heading": "4.3.2. RAPID PARAMETERIZATION OF FIXED WEIGHT BASE LEARNER", "text": "We replaced the entire base learner with a new CNN during evaluation. The slow weights of this network remained fixed. The fast weights are generated by the meta learner that is trained to parameterize the old base learner and used to augmented the fixed slow weights.\nWe tested a small and a big CNN for the base learner. The small CNN has 32 filters and the big one has 128 filters. These CNNs consist of the same number of layers as the base learner (described in Section 4.1). In Figure 4, the test performances of these CNNs are compared. The base learner (target CNN) optimized along within the model performed better than the fixed weight CNNs. The performance difference between these models is large in earlier training iterations. However, as the meta learner sees more one-shot learning trials, their test accuracies of the base learners are to match. This results show that MetaNet effectively learns to parameterize a neural net with fixed weights."}, {"heading": "4.3.3. META-LEVEL CONTINUAL LEARNING", "text": "MetaNet operates in two spaces: input problem space and meta (gradient) space. If the meta space is problem independent, MetaNet should support meta-level continual learning or life-long learning. This experiment is set to examine this in the case of the loss gradient.\nFollowing the previous work on catastrophic forgetting in neural networks (Srivastava et al., 2013; Goodfellow et al., 2014; Kirkpatrick et al., 2016), we formulated two problems in a sequential manner. We first trained and tested"}, {"heading": "400 2800 5200 7600", "text": "the model on the Omniglot sets and then we switched and continued training on the MNIST data. After training on a number of MNIST one-shot tasks, we re-evaluated the model on the same Omniglot test set and compare the two performances. A decreasing performance indicates that the meta weights Z and G of the neural nets m and mLSTM are prone to catastrophic forgetting and MetaNet therefore does not support the continual learning. An increasing performance means MetaNet supporting a reverse transfer learning and the continual learning.\nWe allocated separate parameters for the weights W and Q when we switched the problems so the only meta weights were updated. We used two three-layer MLPs with 64 hidden units as the embedding function and the base learner. The MNIST image and classes were augmented by randomly permuting the pixels. We created 50 different random shuffles and thus the training set for the second oneshot problem consisted of 500 classes. We conducted multiple runs and increased the MNIST training trials by multiples of 400 (i.e. 400, 800, 1200...) in each run giving more time to MetaNet to adapt its meta weights on the second problem so MetaNet may forget the knowledge about Omniglot. Each run was repeated five times and we reported the average statistics. For every run, the network and the optimizer were reinitialized and the training started from scratch.\nIn Figure 5, we plotted the accuracy difference between two Omniglot test performances obtained before and after training on the MNIST task. The performance improvement (y-axis) after skilled on the MNIST tasks ranges from -1.7% to 1.24% depending on the training time (x-axis). The positive values indicate that the training on the second problem automatically improves the performance of the earlier task exhibiting the reverse transfer. MetaNet performs the reverse transfer. At the same time, it skilled on MNIST one-shot classification. The MNIST training accu-\nracy already reached over 72% after seeing 2400 MNIST trials. However, the reverse transfer happens up to a certain point (2400 trials). As we gave more time to train on MNIST, the meta weights started to forget the Omniglot information and from 2800 trials the Omniglot test performance dropped. Nevertheless, the biggest drop we observed by running the model up to 7600 trials, at which point the MNIST training accuracy reached over 90%, was only 1.7%."}, {"heading": "5. Discussion and Future Work", "text": "One-shot learning in combination with meta learning framework can be a useful approach to address certain neural network drawbacks relating to the rapid generalization with small data and the continual learning. We present a novel meta learning model, Meta Networks (MetaNet), that performs a generic knowledge acquisition in a meta space and shifts the parameters and the inductive bias via fast parameterization for the rapid generalization.\nUnder the MetaNet framework, an important consideration is the type of meta information that can be extracted from the model operating on a new task. We require that the meta information should be generic and problem independent. It should also be expressive enough to explain the model setting in the current task space. We explored the loss gradient in this work. As shown in the results, using the gradient as meta information seems to be a promising direction. We evaluated the MetaNet model on the image one-shot classification problems. MetaNet obtained the-state-of-the art results on several benchmarks and has shown to be very flexible in the online setups. It supported continual learning up to a certain point. It must be interesting to challenge a similar setup on two very different tasks, such as image classification versus text classification. We leave this for the future evaluation. It might of a long term interest to come up with a new type of meta information that is more robust and expressive. One could take inspiration from the meta learning process in the brain and ask whether the brain operates on some kind of meta information to generalize across tasks and acquire new skills.\nThe rapid parameterization approach presented here has been shown to be a good alternative to the direct optimization methods that learn to update network parameters for one-shot generalization. However, a problem this approach poses is the integration of slow and fast weights. As a solution to this, we presented a simple layer augmentation method. Although the layer augmentation worked reasonably well, this method is questionable when a neural net has many types of parameters operating in multiple different time-scales. An example of this is that the base learner equipped with three types of weights (slow, example-specific, and task-level weights) integrated under\nthe layer augmentation paradigm could not perform as well as a simpler one. Therefore, a potential extension would be to train MetaNet so it can discover its own augmentation schema for efficiency.\nIn this work, we applied the fast parameterization approach to CNNs and MLPs. The fast parameterization of sequential models such as RNNs have yet to be explored and this would have many useful applications of sequence modeling and language-based conversational tasks.\nIt was observed that neural nets with the fixed slow weight can perform well for new task inputs when augmented with the fast weights. When the slow weights are updated during training, it encodes the domain bias resulting in even better performance. However, one could expect a higher performance from the fixed weight network when aiming for one-shot generalization across very distinct domains.\nFinally, important components of MetaNet are the dynamic key embedding function and the external memory. Indeed, similar modules can also be found in the previous models such as Matching Net and memory based Siamese Net. The differences are that MetaNet learns to parameterize this embedding network as a response to the new task inputs yielding a dynamic attention and the MetaNet memory temporary stores neural net weights generated."}], "references": [{"title": "Learning to learn by gradient descent by gradient descent", "author": ["Andrychowicz", "Marcin", "Denil", "Misha", "Gomez", "Sergio", "Hoffman", "Matthew W", "Pfau", "David", "Schaul", "Tom", "de Freitas", "Nando"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Andrychowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andrychowicz et al\\.", "year": 2016}, {"title": "Using fast weights to attend to the recent past", "author": ["Ba", "Jimmy", "Hinton", "Geoffrey E", "Mnih", "Volodymyr", "Leibo", "Joel Z", "Ionescu", "Catalin"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "Ba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Learning a synaptic learning", "author": ["Bengio", "Yoshua", "Samy", "Cloutier", "Jocelyn"], "venue": "rule. Universite\u0301 de Montre\u0301al, De\u0301partement d\u2019informatique et de recherche ope\u0301rationnelle,", "citeRegEx": "Bengio et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1990}, {"title": "Learning a similarity metric discriminatively, with application to face verification", "author": ["Chopra", "Sumit", "Hadsell", "Raia", "LeCun", "Yann"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Chopra et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chopra et al\\.", "year": 2005}, {"title": "Dynamic filter networks", "author": ["De Brabandere", "Bert", "Jia", "Xu", "Tuytelaars", "Tinne", "Van Gool", "Luc"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Brabandere et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Brabandere et al\\.", "year": 2016}, {"title": "Evolving modular fast-weight networks for control", "author": ["Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Gomez et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gomez et al\\.", "year": 2005}, {"title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks", "author": ["Goodfellow", "Ian J", "Mirza", "Mehdi", "Xiao", "Da", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Neural turing machines", "author": ["Graves", "Alex", "Wayne", "Greg", "Danihelka", "Ivo"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "The formation of learning sets", "author": ["Harlow", "Harry F"], "venue": "Psychological review,", "citeRegEx": "Harlow and F.,? \\Q1949\\E", "shortCiteRegEx": "Harlow and F.", "year": 1949}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Using fast weights to deblur old memories", "author": ["Hinton", "Geoffrey E", "Plaut", "David C"], "venue": "In Proceedings of the ninth annual conference of the Cognitive Science Society,", "citeRegEx": "Hinton et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1987}, {"title": "Learning to learn using gradient descent", "author": ["Hochreiter", "Sepp", "Younger", "A Steven", "Conwell", "Peter R"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "Hochreiter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 2001}, {"title": "Learning to remember rare events", "author": ["Kaiser", "Lukasz", "Nachum", "Ofir", "Roy", "Aurko", "Bengio", "Samy"], "venue": "In ICLR", "citeRegEx": "Kaiser et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kaiser et al\\.", "year": 2017}, {"title": "Siamese neural networks for one-shot image recognition", "author": ["Koch", "Gregory"], "venue": "PhD thesis, University of Toronto,", "citeRegEx": "Koch and Gregory.,? \\Q2015\\E", "shortCiteRegEx": "Koch and Gregory.", "year": 2015}, {"title": "One-shot learning by inverting a compositional causal process", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan R", "Tenenbaum", "Josh"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Lake et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2013}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Lake", "Brenden M", "Salakhutdinov", "Ruslan", "Tenenbaum", "Joshua B"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Crafting papers on machine learning", "author": ["P. Langley"], "venue": "Proceedings of the 17th International Conference on Machine Learning (ICML", "citeRegEx": "Langley,? \\Q2000\\E", "shortCiteRegEx": "Langley", "year": 2000}, {"title": "Learning to optimize", "author": ["Li", "Ke", "Malik", "Jitendra"], "venue": "In ICLR 2017,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Explanationbased neural network learning for robot control", "author": ["Mitchell", "Tom M", "Thrun", "Sebastian B"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Mitchell et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 1993}, {"title": "Optimization as a model for few-shot learning", "author": ["Ravi", "Sachin", "Larochell", "Hugo"], "venue": "In ICLR", "citeRegEx": "Ravi et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2017}, {"title": "Meta-learning with memory-augmented neural networks", "author": ["Santoro", "Adam", "Bartunov", "Sergey", "Botvinick", "Matthew", "Wierstra", "Daan", "Lillicrap", "Timothy"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Santoro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santoro et al\\.", "year": 2016}, {"title": "Learning to control fast-weight memories: An alternative to dynamic recurrent networks", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1992}, {"title": "A neural network that embeds its own meta-levels", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1993\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1993}, {"title": "Compete to compute", "author": ["Srivastava", "Rupesh K", "Masci", "Jonathan", "Kazerounian", "Sohrob", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "A perspective view and survey of meta-learning", "author": ["Vilalta", "Ricardo", "Drissi", "Youssef"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Vilalta et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Vilalta et al\\.", "year": 2002}, {"title": "Matching networks for one shot learning", "author": ["Vinyals", "Oriol", "Blundell", "Charles", "Lillicrap", "Tim", "Wierstra", "Daan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "Memory networks", "author": ["Weston", "Jason", "Chopra", "Sumit", "Bordes", "Antoine"], "venue": "Proceedings Of The International Conference on Representation Learning (ICLR", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Fixed-weight on-line learning", "author": ["Younger", "A Steven", "Conwell", "Peter R", "Cotter", "Neil E"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Younger et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Younger et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 18, "context": "Previous work on meta learning has formulated the problem as a two-level learning, a slow learning of a meta-level model performing across tasks and a rapid learning of a base-level model acting within each task (Mitchell et al., 1993; Vilalta & Drissi, 2002).", "startOffset": 212, "endOffset": 259}, {"referenceID": 2, "context": "The base and the meta-level models can be framed in a single learner (Schmidhuber, 1993) or in separate learners (Bengio et al., 1990; Hochreiter et al., 2001).", "startOffset": 113, "endOffset": 159}, {"referenceID": 11, "context": "The base and the meta-level models can be framed in a single learner (Schmidhuber, 1993) or in separate learners (Bengio et al., 1990; Hochreiter et al., 2001).", "startOffset": 113, "endOffset": 159}, {"referenceID": 15, "context": "One notable success is reported by a probabilistic programming approach (Lake et al., 2015).", "startOffset": 72, "endOffset": 91}, {"referenceID": 7, "context": "(2016) proposed a memory-based approach and trained Neural Turing Machines (Graves et al., 2014) for one-shot learning, although the meta-learner and the one-shot learner in this work are not separable explicitly.", "startOffset": 75, "endOffset": 96}, {"referenceID": 12, "context": "One notable success is reported by a probabilistic programming approach (Lake et al., 2015). They used specific knowledge of how pen strokes are composed to produce characters of different alphabets. Koch (2015) applied Siamese Networks to perform one-shot classification.", "startOffset": 73, "endOffset": 212}, {"referenceID": 12, "context": "One notable success is reported by a probabilistic programming approach (Lake et al., 2015). They used specific knowledge of how pen strokes are composed to produce characters of different alphabets. Koch (2015) applied Siamese Networks to perform one-shot classification. Recently, Vinyals et al. (2016) unified the training and testing of a one-shot learner under the same procedure and developed an end-to-end differentiable nearest neighbor method for one-shot learning.", "startOffset": 73, "endOffset": 305}, {"referenceID": 12, "context": "One notable success is reported by a probabilistic programming approach (Lake et al., 2015). They used specific knowledge of how pen strokes are composed to produce characters of different alphabets. Koch (2015) applied Siamese Networks to perform one-shot classification. Recently, Vinyals et al. (2016) unified the training and testing of a one-shot learner under the same procedure and developed an end-to-end differentiable nearest neighbor method for one-shot learning. Santoro et al. (2016) proposed a memory-based approach and trained Neural Turing Machines (Graves et al.", "startOffset": 73, "endOffset": 497}, {"referenceID": 7, "context": "(2016) proposed a memory-based approach and trained Neural Turing Machines (Graves et al., 2014) for one-shot learning, although the meta-learner and the one-shot learner in this work are not separable explicitly. Their training procedure adapted the work of Hochreiter et al. (2001) in which they used LSTMs as the meta-level model.", "startOffset": 76, "endOffset": 284}, {"referenceID": 11, "context": "There is a line of work on building meta optimizers (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017).", "startOffset": 52, "endOffset": 122}, {"referenceID": 0, "context": "There is a line of work on building meta optimizers (Hochreiter et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2017).", "startOffset": 52, "endOffset": 122}, {"referenceID": 18, "context": "By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets.", "startOffset": 46, "endOffset": 142}, {"referenceID": 27, "context": "By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets.", "startOffset": 46, "endOffset": 142}, {"referenceID": 0, "context": "By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets.", "startOffset": 46, "endOffset": 142}, {"referenceID": 0, "context": ", 2001; Andrychowicz et al., 2016; Li & Malik, 2017). As the main interest here is to train an optimization algorithm within the meta learning framework, the previous research have mainly focused on tasks with large datasets. In contrast, with the absent of large datasets, we emphasize the difficulties to optimize a neural network with a large number of parameters to generalize with limited examples of new concept. Our work instead proposes a novel rapid parameterization approach by employing the meta information. By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets. The ideas of fast weights and generating parameters for one neural network with another have been explored individually in separate contexts. Hinton & Plaut (1987) suggested the usage of fast weights for rapid learning.", "startOffset": 8, "endOffset": 909}, {"referenceID": 0, "context": ", 2001; Andrychowicz et al., 2016; Li & Malik, 2017). As the main interest here is to train an optimization algorithm within the meta learning framework, the previous research have mainly focused on tasks with large datasets. In contrast, with the absent of large datasets, we emphasize the difficulties to optimize a neural network with a large number of parameters to generalize with limited examples of new concept. Our work instead proposes a novel rapid parameterization approach by employing the meta information. By following the success of the previous work (Mitchell et al., 1993; Younger et al., 1999; Andrychowicz et al., 2016; Ravi & Larochell, 2017), we studied the meta information in the form of the loss gradient of neural nets. The ideas of fast weights and generating parameters for one neural network with another have been explored individually in separate contexts. Hinton & Plaut (1987) suggested the usage of fast weights for rapid learning. Ba et al. (2016) recently used fast weights to replace soft attention mechanism.", "startOffset": 8, "endOffset": 982}, {"referenceID": 4, "context": "De Brabandere et al. (2016) used one network to generate slow filter weights for a convolutional neural net whereas more recently David Ha & Le (2017) generated slow weights for recurrent nets.", "startOffset": 3, "endOffset": 28}, {"referenceID": 4, "context": "De Brabandere et al. (2016) used one network to generate slow filter weights for a convolutional neural net whereas more recently David Ha & Le (2017) generated slow weights for recurrent nets.", "startOffset": 3, "endOffset": 151}, {"referenceID": 7, "context": "MANNs have shown promising results on a range of tasks starting from small programming problems (Graves et al., 2014) to large-scale language tasks (Weston et al.", "startOffset": 96, "endOffset": 117}, {"referenceID": 26, "context": ", 2014) to large-scale language tasks (Weston et al., 2015; Munkhdalai & Yu, 2017).", "startOffset": 38, "endOffset": 82}, {"referenceID": 3, "context": "However, when there are more than one examples per class available, contrastive loss (Chopra et al., 2005) is a natural choice for lossu since both positive and negative samples can be formed.", "startOffset": 85, "endOffset": 106}, {"referenceID": 9, "context": "Overall the layer augmentation building block is similar to that of ResNet architecture (He et al., 2016).", "startOffset": 88, "endOffset": 105}, {"referenceID": 15, "context": "The Omniglot dataset consists of images across 1623 classes with only 20 images per class, from 50 different alphabets (Lake et al., 2015).", "startOffset": 119, "endOffset": 138}, {"referenceID": 20, "context": "Following (Santoro et al., 2016), we augmented the data set through 90, 180 and 270 degrees rotations.", "startOffset": 10, "endOffset": 32}, {"referenceID": 25, "context": "To train and test MetaNet on one-shot learning, we adapted the training procedure introduced by Vinyals et al. (2016). First we split the Omniglot data into training and test sets consisting of two disjoint classes.", "startOffset": 96, "endOffset": 118}, {"referenceID": 0, "context": "As in Andrychowicz et al. (2016), the parameters G and Z of m and m are shared across the coordinates of the gradients \u2207 and the gradients are normalized using the same preprocessing rule (with p = 7) before it is given to the meta learner.", "startOffset": 6, "endOffset": 33}, {"referenceID": 12, "context": "Pixel kNN (Kaiser et al., 2017) 41.", "startOffset": 10, "endOffset": 31}, {"referenceID": 20, "context": "1 MANN (Santoro et al., 2016) 82.", "startOffset": 7, "endOffset": 29}, {"referenceID": 25, "context": "8 Matching Nets (Vinyals et al., 2016) 98.", "startOffset": 16, "endOffset": 38}, {"referenceID": 12, "context": "8 Siamese Net with Memory (Kaiser et al., 2017) 98.", "startOffset": 26, "endOffset": 47}, {"referenceID": 25, "context": "Following the previous setup Vinyals et al. (2016), we split the Omniglot classes into 1200 and 423 classes for training and testing.", "startOffset": 29, "endOffset": 51}, {"referenceID": 25, "context": "3% while Matching Net (Vinyals et al., 2016) reported 72.", "startOffset": 22, "endOffset": 44}, {"referenceID": 14, "context": "In order to best match the evaluation protocol of Lake et al. (2015), we form 400 tasks", "startOffset": 50, "endOffset": 69}, {"referenceID": 15, "context": "Human performance (Lake et al., 2015) - 95.", "startOffset": 18, "endOffset": 37}, {"referenceID": 14, "context": "Pixel kNN (Lake et al., 2013) - 21.", "startOffset": 10, "endOffset": 29}, {"referenceID": 14, "context": "7 Affine model (Lake et al., 2013) - 81.", "startOffset": 15, "endOffset": 34}, {"referenceID": 14, "context": "8 Deep Boltzmann Machines (Lake et al., 2013) - 62.", "startOffset": 26, "endOffset": 45}, {"referenceID": 15, "context": "0 Hierarchial Bayesian Program Learning (Lake et al., 2015) - 96.", "startOffset": 40, "endOffset": 59}, {"referenceID": 23, "context": "Following the previous work on catastrophic forgetting in neural networks (Srivastava et al., 2013; Goodfellow et al., 2014; Kirkpatrick et al., 2016), we formulated two problems in a sequential manner.", "startOffset": 74, "endOffset": 150}, {"referenceID": 6, "context": "Following the previous work on catastrophic forgetting in neural networks (Srivastava et al., 2013; Goodfellow et al., 2014; Kirkpatrick et al., 2016), we formulated two problems in a sequential manner.", "startOffset": 74, "endOffset": 150}], "year": 2017, "abstractText": "Deep neural networks have been successfully applied in applications with a large amount of labeled data. However, there are major drawbacks of the neural networks that are related to rapid generalization with small data and continual learning of new concepts without forgetting. We present a novel meta learning method, Meta Networks (MetaNet), that acquires a meta-level knowledge across tasks and shifts its inductive bias via fast parameterization for the rapid generalization. When tested on the standard oneshot learning benchmarks, our MetaNet models achieved near human-level accuracy. We demonstrated several appealing properties of MetaNet relating to generalization and continual learning.", "creator": "LaTeX with hyperref package"}}}