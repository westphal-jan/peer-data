{"id": "1703.00978", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Compositional Falsification of Cyber-Physical Systems with Machine Learning Components", "abstract": "Cyber-physical systems (CPS), such as automotive systems, are starting to include sophisticated machine learning (ML) components. Their correctness, therefore, depends on properties of the inner ML modules. While learning algorithms aim to generalize from examples, they are only as good as the examples provided, and recent efforts have shown that they can produce inconsistent output under small adversarial perturbations. This raises the question: can the output from learning components can lead to a failure of the entire CPS? In this work, we address this question by formulating it as a problem of falsifying signal temporal logic (STL) specifications for CPS with ML components. We propose a compositional falsification framework where a temporal logic falsifier and a machine learning analyzer cooperate with the aim of finding falsifying executions of the considered model. The efficacy of the proposed technique is shown on an automatic emergency braking system model with a perception component based on deep neural networks.", "histories": [["v1", "Thu, 2 Mar 2017 22:58:10 GMT  (9056kb,D)", "http://arxiv.org/abs/1703.00978v1", null]], "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["tommaso dreossi", "alexandre donz\\'e", "sanjit a seshia"], "accepted": false, "id": "1703.00978"}, "pdf": {"name": "1703.00978.pdf", "metadata": {"source": "CRF", "title": "Compositional Falsification of Cyber-Physical Systems with Machine Learning Components", "authors": ["Tommaso Dreossi", "Alexandre Donz\u00e9", "Sanjit A. Seshia"], "emails": ["dreossi@berkeley.edu", "sseshia@berkeley.edu", "alex.r.donze@gmail.com"], "sections": [{"heading": null, "text": "Keywords: Cyber-physical systems, machine learning, falsification, temporal logic"}, {"heading": "1 Introduction", "text": "Over the last decade, machine learning (ML) algorithms have achieved impressive results providing solutions to practical large-scale problems (see, e.g., [2,14,10,8]). Not surprisingly, ML is being used in cyber-physical systems (CPS) \u2014 systems that are integrations of computation with physical processes. For example, semiautonomous vehicles employ Adaptive Cruise Controllers (ACC) or Lane Keeping Assist Systems (LKAS) that rely heavily on image classifiers providing input to the software controlling electric and mechanical subsystems (see, e.g., [3]). The safety-critical nature of such systems involving ML raises the need for formal methods [18]. In particular, how do we systematically find bugs in such systems?\n? This work is funded in part by the DARPA BRASS program under agreement number FA8750-16-C-0043, NSF grants CNS-1646208 and CCF-1139138, and by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA. The second author did much of the work while affiliated with UC Berkeley.\nar X\niv :1\n70 3.\n00 97\n8v 1\n[ cs\n.S Y\n] 2\nM ar\n2 01\n7\nWe formulate this question as the falsification problem for CPS models with ML components (CPSML): given a formal specification \u03d5 in signal temporal logic (STL) [12], and a CPSML model M , find an input for which M does not satisfy \u03d5. A falsifying input generates a counterexample trace that reveals a bug. To solve this problem, multiple challenges must be tackled. First, the input space to be searched can be intractable. For instance, a simple model of a semi-autonomous car already involves several control signals (e.g., the angle of the acceleration pedal, steering angle) and other sensor input (e.g., images captured by a camera). Second, CPSML are often designed using languages (such as C, C++, or Simulink), for which clear semantics are not given, and involve third-party components that are opaque or poorly-specified. This obstructs the development of formal methods for the analysis of CPSML models and may force one to treat them as gray/black-boxes. Third, the formal verification of ML components is a difficult, and somewhat ill-posed problem due to the complexity of the underlying ML algorithms, large feature spaces, and the lack of consensus on a formal definition of correctness [18]. Hence, we need a technique to systematically analyze ML components within the context of a CPS.\nIn this paper, we propose a framework for the falsification of CPSML addressing the issues described above. Our technique is compositional in that it divides the search space for falsification into that of the ML component and of the remainder of the system, while establishing a connection between the two. The obtained subspaces are respectively analyzed by a temporal logic falsifier and an ML analyzer that cooperate. This cooperation mainly comprises a series of input space projections, leads to small subsets in which counterexamples are easier to find. Further, our technique can handle any machine learning technique, including the methods based on deep neural networks [8] that have proved effective in many recent applications. The proposed ML analyzer identifies sets of misclassifying features, i.e., inputs that \u201cfool\u201d the ML algorithm. The analysis is performed by considering subsets of parameterized features spaces that are used to approximate the ML components by simpler functions. The information gathered by the temporal logic falsifier and the ML analyzer together reduce the search space, providing an efficient approach to falsification for CPSML models.\nExample 1. As an illustrative example, let us consider a simple model of an Automatic Emergency Braking System (AEBS) as a closed-loop control system composed of a controller (automatic brake), a plant (car transmission), and a sensor (obstacle detector) (see Figure 1). The controller regulates the acceleration and braking of the plant using the velocity of the subject (ego) vehicle and the distance between it and an obstacle. The sensor used to detect the obstacle in-\ncludes a camera along with an image classifier. In general, this sensor can provide\nnoisy measurements due to incorrect image classifications which in turn can affect the correctness of the overall system.\nSuppose we want to verify whether the distance between the subject vehicle and a preceding obstacle is always larger than 5 meters. Such a verification requires the exploration of an intractable input space comprising the control inputs (e.g., acceleration and braking pedal angles) and the ML component\u2019s feature space (e.g., all the possible pictures observable by the camera). Note that feature space of RGB 1000\u00d7 600px pictures for an image classifier contains 2561000\u00d7600\u00d73 elements. ut\nAt first, the input space of the model described in Example 1 appears intractable. However, we can observe some interesting aspects of the relationship between the \u201cpure CPS\u201d input space and its ML feature space:\n1. Under the assumption of \u201cperfect ML components\u201d (i.e., all feature vectors are correctly classified), we can study the CPSML model on a lowerdimensional input space (the \u201cpure CPS\u201d one) and identify regions of values that satisfy the specification but might be affected by the malfunctioning of some ML modules;\n2. Instead of verifying the ML components on their whole feature spaces, we can focus only on those features related to the non-robust input values identified in the previous step, and\n3. If we are able to determine misclassifications on the restricted feature space, then we can relate them back to CPSML input space, thus focusing the falsification on a smaller input space.\nThese three observations constitute the core idea of the compositional falsification method proposed in this paper. Specifically, we use a temporal logic falsifier, Breach [4], in Steps (1) and (3) to partition a given input set into values that do and do not satisfy a given specification, and an ML analyzer in Step (2) to determine subsets of feature vectors that are misclassified by the ML components.\nThe proposed method, however, presents certain challenges that need to be addressed. First, we need to construct a validity domain of a specification against a CPSML model with (assumed) correct ML components. Second, we need a method to relate the non-robust input areas to the feature space of the ML modules. Third, we need to systematically analyze the ML components with the goal of finding feature vectors leading to misclassifications. We describe in detail in Sections 3 and 4 how we tackle these challenges.\nIn summary, the main contributions of this paper are:\n\u2022 A compositional framework for the falsification of temporal logic properties of CPSML models that works for any machine learning classifier. \u2022 A machine learning analyzer that identifies misclassifications leading to system-\nlevel property violations, based on two main ideas:\n- An input space parameterization used to abstract the feature space and relate it to the CPSML input space, and\n- A classifier approximation method used to identify misclassifications that can lead to unsafe executions of the CPSML.\nIn Sec. 5, we demonstrate the effectiveness of our approach on an Automatic Emergency Braking System (AEBS) involving an image classifier for obstacle detection based on deep neural networks using leading software packages Caffe [10] and TensorFlow [13].\nRelated Work\nThe verification of both CPS and ML algorithms have attracted several research efforts, and we focus here on the most closely related work. Techniques for the falsification of temporal logic specifications against CPS models have been implemented based on nonlinear optimization methods and stochastic search strategies (e.g., Breach [4], S-TaLiRo [1], RRT-REX [5], C2E2 [6]). While the verification of ML programs is less well-defined [18], recent efforts [19] show how even well trained neural networks can be sensitive to small adversarial perturbations, i.e., small intentional modifications that lead the network to misclassify the altered input with large confidence. Other efforts have tried to characterize the correctness of neural networks in terms of risk [21] (i.e., probability of misclassifying a given input) or robustness [7] (i.e., the minimal perturbation leading to a misclassification), while others proposed methods to generate pictures [16] or perturbations [15,9] in such a way to \u201cfool\u201d neural networks. To the best of our knowledge, our work is the first to address the verification of temporal logic properties of CPSML\u2014the combination of CPS and ML systems."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 CPSML Models", "text": "In this work, we consider models of cyber-physical systems with machine learning components (CPSML). We assume that a system model is given as a gray-box simulator defined as a tuple M = (S,U, sim), where S is a set of system states, U is a set of input values, and sim : S \u00d7 U \u00d7 T \u2192 S is a simulator that maps a state s(tk) \u2208 S and input value u(tk) \u2208 U at time tk \u2208 T to a new state s(tk+1) = sim(s(tk),u(tk), tk), where tk+1 = tk +\u2206k for a time-step \u2206k \u2208 Q>0.\nGiven an initial time t0 \u2208 T , an initial state s(t0) \u2208 S, a sequence of timesteps \u22060, . . . ,\u2206n \u2208 Q>0, and a sequence of input values u(t0), . . . ,u(tn) \u2208 U , a simulation trace of the model M = (S,U, sim) is a sequence:\n(t0, s(t0),u(t0)), (t1, s(t1),u(t1)), . . . , (tn, s(tn),u(tn))\nwhere s(tk+1) = sim(s(tk),u(tk), \u2206k) and tk+1 = tk +\u2206k for k = 0, . . . , n. The gray-box aspect of the CPSML model is that we assume some knowledge of the internal ML components. Specifically, these components, termed classifiers, are functions f : X \u2192 Y that assign to their input feature vector x \u2208 X a label y \u2208 Y , where X and Y are a feature and label space, respectively. Without loss of generality, we focus on binary classifiers whose label space is Y = {0, 1}. A ML algorithm selects a classifier using a training set {(x(1), y(1)), . . . , (x(m), y(m))} where the (x(i), y(i)) are labeled examples with x(i) \u2208 X and y(i) \u2208 Y , for i = 1, . . . ,m. The quality of a classifier\ncan be estimated on a test set of examples comparing the classifier predictions against the labels of the examples. Precisely, for a given test set T = {(x(1), y(1)), . . . , (x(l), y(l))}, the number of false positives fpf (T ) and false negatives fnf (T ) of a classifier f on T are defined as:\nfpf (T ) = | {x(i) \u2208 T | f(x(i)) = 1 and y(i) = 0} | fnf (T ) = | {x(i) \u2208 T | f(x(i)) = 0 and y(i) = 1} |\n(1)\nThe error rate of f on T is given by:\nerrf (T ) = (fpf (T ) + fnf (T ))/l (2)\nA low error rate implies good predictions of the classifier f on the test set T ."}, {"heading": "2.2 Signal Temporal Logic", "text": "We consider Signal Temporal Logic [12] (STL) as the language to specify properties to be verified against a CPSML model. STL is an extension of linear temporal logic (LTL) suitable for the specification of properties of CPS.\nA signal is a function s : D \u2192 S, with D \u2286 R\u22650 an interval and either S \u2286 B or S \u2286 R, where B = {>,\u22a5} and R is the set of reals. Signals defined on B are called booleans, while those on R are said real-valued. A trace w = {s1, . . . , sn} is a finite set of real-valued signals defined over the same interval D.\nLet \u03a3 = {\u03c31, . . . , \u03c3k} be a finite set of predicates \u03c3i : Rn \u2192 B, with \u03c3i \u2261 pi(x1, . . . , xn) C 0, C \u2208 {<,\u2264}, and pi : Rn \u2192 R a function in the variables x1, . . . , xn.\nAn STL formula is defined by the following grammar:\n\u03d5 := \u03c3 | \u00ac\u03d5 |\u03d5 \u2227 \u03d5 |\u03d5UI\u03d5 (3)\nwhere \u03c3 \u2208 \u03a3 is a predicate and I \u2282 R\u22650 is a closed non-singular interval. Other common temporal operators can be defined as syntactic abbreviations in the usual way, like for instance \u03d51 \u2228 \u03d52 := \u00ac(\u00ac\u03d51 \u2227 \u03d52), FI\u03d5 := >UI\u03d5, or GI\u03d5 := \u00acFI\u00ac\u03d5. Given a t \u2208 R\u22650, a shifted interval I is defined as t+I = {t+t\u2032 | t\u2032 \u2208 I}.\nDefinition 1 (Qualitative semantics). Let w be a trace, t \u2208 R\u22650, and \u03d5 be an STL formula. The qualitative semantics of \u03d5 is inductively defined as follows:\nw, t |= \u03c3 iff \u03c3(w(t)) is true w, t |= \u00ac\u03d5 iff w, t 6|= \u03d5\nw, t |= \u03d51 \u2227 \u03d52 iff w, t |= \u03d51 and w, t |= \u03d52 w, t |= \u03d51UI\u03d52 iff \u2203t\u2032 \u2208 t+ I s.t. w, t\u2032 |= \u03d52 and \u2200t\u2032\u2032 \u2208 [t, t\u2032], w, t\u2032\u2032 |= \u03d51\n(4)\nA trace w satisfies a formula \u03d5 if and only if w, 0 |= \u03d5, in short w |= \u03d5. For given signal w, time instant t \u2208 R\u22650, and STL formula \u03d5, the satisfaction signal X (w, t, \u03d5) is > if w, t |= \u03d5, \u22a5 otherwise.\nDefinition 2 (Quantitative semantics). Let w be a trace, t \u2208 R\u22650, and \u03d5 be an STL formula. The quantitative semantics of \u03d5 is defined as follows:\n\u03c1(p(x1, . . . , xn) C 0, w, t) = p(w(t)) with C \u2208 {<,\u2264} \u03c1(\u00ac\u03d5,w, t) = \u2212 \u03c1(\u03d5,w, t)\n\u03c1(\u03d51 \u2227 \u03d52, w, t) = min(\u03c1(\u03d51, w, t), \u03c1(\u03d52, w, t)) \u03c1(\u03d51UI\u03d52, w, t) = sup\nt\u2032\u2208t+I min(\u03c1(\u03d52, w, t \u2032), inf t\u2032\u2032[t,t\u2032] \u03c1(\u03d51, w, t \u2032\u2032))\n(5)\nThe robustness of a formula \u03d5 with respect to a trace w is the signal \u03c1(\u03d5,w, \u00b7)."}, {"heading": "3 Compositional Falsification Framework", "text": "In this section, we formalize the falsification problem for STL specifications against CPSML models, define our compositional falsification framework, and show its functionality on the AEBS system of Example 1.\nDefinition 3 (Falsification of CPSML). Given a model M = (S,U, sim) and an STL specification \u03d5, find an initial state s(t0) \u2208 S and a sequence of input values u = u(t0), . . . ,u(tn) \u2208 U such that the trace of states w = s(t0), . . . , s(tn) generated by the simulation of M from s(t0) \u2208 S under u does not satisfy \u03d5, i.e., w 6|= \u03d5. We refer to such (s(t0),u) as counterexamples for \u03d5. The problem of finding a counterexample is often called falsification problem.\nWe now present the compositional framework for the falsification of STL formulas against CPSML models. Intuitively, the proposed method decomposes a given model into two abstractions: a version of the CPSML model under the assumption of perfectly correct ML modules and its actual ML components. The two abstractions are separately analyzed, the first by a temporal logic falsifier that builds the validity domain with respect to the given specification, the second by an ML analyzer that identifies sets of feature vectors that are misclassified by the ML components. Finally, the results of the two analysis are composed and projected back to a targeted input subspace of the original CPSML model where counterexamples can be found by invoking a temporal logic falsifier. Let us formalize this procedure.\nLet M = (S,U, sim) be a CPSML model and \u03d5 be an STL specification. Let M \u2032 be a version of M with perfectly behaving ML components, that is, every feature vector of the ML feature spaces is correctly classified. Let us denote by ml the isolated ML components of the model M .\nUnder the assumption of correct ML components, the lower-dimensional input space of M \u2032 can be analyzed by constructing the validity domain of \u03d5, that is the partition of the input space into the sets U\u03d5 and U\u00ac\u03d5 that do and do not satisfy \u03d5, respectively. Note that considering the original model M , a possible misclassification of the ML components ml might affect the elements of U\u03d5 and U\u00ac\u03d5. In particular, we are interested in the elements of U\u03d5 that, due to misclassifications of ml, do not satisfy \u03d5 anymore. This corresponds to analyze the behavior of the ML components ml on the input set U\u03d5. We refer to this step as the ML analysis, that can be seen as the procedure of finding a subset Uml \u2286 U\u03d5\nof input values that are misclassified by the ML components ml. It is important to note that the input space of the CPS model M \u2032 and the feature spaces of the ML modules ml are different, thus the ML analyzer must adapt and relate the two different spaces. This important step will be clarified in Section 4.\nFinally, the intersection U\u03d5\u2229Uml of the subsets identified by the decomposed analysis of the CPS model and its ML components targets a small set of input values that are misclassified by the ML modules and are likely to falsify \u03d5. Thus, counterexamples in U\u03d5 \u2229 Uml \u2286 U can be determined by invoking a temporal logic falsifier on \u03d5 against M .\nAlgorithm 1 CPSML falsification scheme\n1: function CompFalsfy(M,\u03d5) . M CPSML, \u03d5 STL specification 2: [M \u2032,ml]\u2190Decompose(M) . M \u2032 exact ML, ml ML modules 3: [U\u03d5, U\u00ac\u03d5]\u2190ValidityDomain(M \u2032, U, \u03d5) . Validity domain of \u03d5 w.r.t. M \u2032 4: Uml \u2190 MLAnalysis(ml,U\u03d5) . Find misclassified feature vectors 5: Uml\u00ac\u03d5 \u2190Falsify(M,U\u03d5 \u2229 Uml, \u03d5) . Falsify on targeted input 6: return U\u00ac\u03d5 \u222a Uml\u00ac\u03d5 7: end function\nThe compositional falsification procedure is formalized in Algorithm 1. CompFalsfy receives as input a CPSML model M and an STL specification \u03d5, and returns a set of falsifying counterexamples. At first, the algorithm decomposes M into M \u2032 and ml, where M \u2032 is an abstract version of M with perfectly working ML modules, and ml are the ML components of M (Line 2). Then, the validity domain of \u03d5 with respect to the abstraction M \u2032 is computed by ValidityDomain (Line 3) and subsets of input that are misclassified by ml are identified by MLAnalysis (Line 4). Finally, the targeted input set U\u03d5 \u2229 Uml, consisting in the intersection of the sets identified by the decomposed analysis, is searched by a temporal logic falsifier on the original model M (Line 5) and a collection of counterexamples is returned.\nExample 2. Let us consider the model described in Example 1 and let us assume that the input space U of the model M consists of the initial velocity of the subject vehicle vel(0), the initial distance between the vehicle and the proceeding obstacle dist(0), and the set of pictures that can be captured by the camera. Let \u03d5 := G[0,T ](dist(t) \u2265 \u03c4) be a specification that requires the vehicle to be always farther than \u03c4 from the preceding obstacle. Instead of analyzing the whole input space U (including a vast number of pictures), we can adopt our compositional framework to target a specific subset of U . Let M \u2032 be the AEBS model with a perfectly working image classifier and ml be the actual classifier. We begin by computing the validity subsets U\u03d5 and U\u00ac\u03d5 of \u03d5 against M\n\u2032, considering only vel(0) and dist(0) and assuming exact distance measurements during the simulation. Next, we analyze only the image classifier ml on pictures of obstacles whose distances fall in U\u03d5, say in [dm, dM ] (see Figure 2). Our ML analyzer generates only pictures of obstacles whose distances are in [dm, dM ], finds possible sets of images that are misclassified, and returns the corresponding distances that,\nwhen projected back to U , yield the subset U\u03d5 \u2229 Uml. Finally, a temporal logic falsifier can be invoked over U\u03d5 \u2229Uml and a set of counterexamples is returned.\nut\nThis example illustrates how the compositional approach relies on tools, such as Breach [4], that compute validity domains and falsify STL specifications, as well as a ML analyzer. In the next section, we introduce our ML analyzer that identifies misclassifications of the ML component relevant to the overall CPSML input space."}, {"heading": "4 Machine Learning Analyzer", "text": "In this section, we define an ML analyzer that adapts the input of a model to its classifiers feature spaces and identifies subsets of feature vectors for which wrong labels are predicted. The analysis involves the construction of an approximation function used to study the original classifiers. In particular, given a classifier f : X \u2192 Y , the ML analyzer determines a simpler function f\u0303 : A \u2192 Y that approximates f on the abstract domain A. The abstract domain of the function f\u0303 is analyzed and clusters of misclassifying abstract elements are identified. The concretizations of such elements\nare subsets of features that are misclassified by the original classifier f ."}, {"heading": "4.1 Feature Space Abstraction", "text": "Let X\u0303 \u2286 X be a subset of the feature space of f : X \u2192 Y . Let \u2264 be a total order on a set A called the abstract set. An abstraction function is an injective function \u03b1 : X\u0303 \u2192 A that maps every feature vector s \u2208 X\u0303 to an abstract element \u03b1(s) \u2208 A. Conversely, the concretization function \u03b3 : A \u2192 X\u0303 maps every abstraction a \u2208 A to a feature \u03b3(a) \u2208 X\u0303.\nThe abstraction and concretization functions play a fundamental role in our falsification framework. First, they allow us to map the input space of the CPS model to the feature space of its classifiers. Second, the abstract space can be used to analyze the classifiers on a compact domain as opposite to intractable\nfeature spaces. These concepts are clarified in the following example, where a feature space of pictures is abstracted into a three-dimensional unit hyper-box.\nExample 3. Let X be the set of RGB pictures of size 1000 \u00d7 600, i.e., X = {0, . . . , 255}1000\u00d7600\u00d73. Suppose we are interested in analyzing an image classifier in the automotive context, i.e., on pictures of road scenarios rather than on the whole X. Suppose that we focus on the constrained feature space X\u0303 \u2286 X composed by the set of pictures of cars overlapped in different positions over a desert road background. We also consider the brightness level of the picture. The x and z positions of the car and the brightness level of the picture can be seen as the dimensions of an abstract set A. In this setting, we can define the abstraction and concretization functions \u03b1 and \u03b3 that relate the abstract set A = [0, 1]3 and X\u0303. For instance, the picture \u03b3(0, 0, 0) sees the car on the left, close to the observer, and low brightness; the picture \u03b3(1, 0, 0) places the car shifted to the right; on the other extreme, \u03b3(1, 1, 1) has the car on the right, far away from the observer, and with a high brightness level. Figure 3 depicts some car pictures of S\u0303 disposed accordingly to their position in the abstract domain A (the surrounding box).\nut"}, {"heading": "4.2 Approximation of Learning Components", "text": "We now describe how the feature space abstraction can be used to construct an approximation that helps the identification of misclassified feature vectors.\nGiven a classifier f : X \u2192 Y and a constrained feature space X\u0303 \u2286 X, we want to determine an approximated classifier f\u0303 : A\u2192 Y , such that errf\u0303 (T ) \u2264 , for some 0 \u2264 \u2264 1 and test set T = {(a(1), y(1)), . . . , (a(l), y(l))}, with y(i) = f(\u03b3(a(i))), for i = 1, . . . , l.\nIntuitively, the proposed approximation scheme samples elements from the abstract set, computes the labels of the concretized elements using the analyzed learning algorithm, and finally, interpolates the abstract elements and the corresponding labels in order to obtain an approximation function. The obtained approximation can be used to reason on the considered feature space and identify clusters of potentially misclassified feature vectors.\nAlgorithm 2 Approximation construction of classifier f : X \u2192 Y 1: function Approximation(A, \u03b3, ) . A abstract set (\u03b3 : A\u2192 X\u0303), 0 \u2264 \u2264 1 2: TI \u2190 \u2205 3: repeat 4: TI \u2190 TI\u222a sample(A, f) 5: f\u0303 \u2190 interpolate(TI) 6: TE \u2190 sample(A, f) 7: until errf\u0303 (TE) \u2264 8: return f\u0303 9: end function\nThe Approximation algorithm (Algorithm 2) formalizes the proposed approximation construction technique. It receives in input an abstract domain A for the concretization function \u03b3 : A \u2192 X\u0303, with X\u0303 \u2286 X, the error threshold 0 \u2264 \u2264 1, and returns a function f\u0303 : A \u2192 Y that approximates f on the constrained feature space X\u0303. The algorithm consists in a loop that iteratively improves the approximation f\u0303 . At every iteration, the algorithm populates the interpolation test set TI by sampling abstract features from A and computing the concretized labels accordingly to f (Line 4), i.e., sample(A, f)= {(a, y) | a \u2208 A\u0303, y = f(\u03b3(a))}, where A\u0303 \u2286 A is a finite subset of samples determined with some sampling method. Next, the algorithm interpolates the points of TI (Line 5). The result is a function f\u0303 : A \u2192 Y that simplifies the original classifier f on the concretized constrained feature space X\u0303. The approximation is evaluated on the test set TE . Note that at each iteration, TE changes while TI incrementally grows. The algorithm iterates until the error rate errf\u0303 (TE) is smaller than the desired threshold (Line 7). The technique with which the samples in TE and TI are selected strongly influences the accuracy of the approximation. In order to have a good coverage of the abstract set A, we propose the usage of low-discrepancy sampling methods that, differently from uniform random sampling, cover sets quickly and evenly. In this work, we use the Halton and lattice sequences, that are two common and easy to implement sampling methods. For details see, e.g., [17].\nExample 4. We now analyze two image classifiers: the Caffe [10] version of AlexNet [11] and the Inception-v3 model of Tensorflow [13], both trained on the ImageNet database.3 We sample 1000 points from the abstract domain defined in Example 3 using the lattice sampling techniques. These points encode\n3 http://image-net.org/\nthe x and z displacements of a car in a picture and its brightness level (see Figure 3). Figure 4 (a) depicts the sampled points with their concretized labels. The green circles indicate correct classifications, i.e., the classifier identified a car, the red circles denote misclassifications, i.e., no car detected. The linear interpolation of the obtained points leads to an approximation function. The error rates errf\u0303 (TE) of the obtained approximations (i.e., the discrepancies between the predictions of the original image classifiers and their approximations) computed on 300 randomly picked test cases are 0.0867 and 0.1733 for Caffe and Tensorflow, respectively. Figure 4 (b) shows the projections of the approximation functions for the brightness value 0.2. The more red a region, the larger the sets of pictures for which the neural networks do not detect a car. For illustrative purposes, we superimpose the projections of Figure 4 (b) over the background used for the picture generation. These illustrations show the regions of the concrete feature vectors in which a vehicle is misclassified. ut\nThe analysis of Example 4 on Caffe and Tensorflow provides useful insights. First, we observe that Tensorflow outperforms Caffe on the considered road pictures since it correctly classifies more pictures that Caffe. Second, we notice that Caffe tends to correctly classify pictures in which the x abstract component is either close to 0 or 1, i.e., pictures in which the car is not in the middle of the street, but on one of the two lanes. This suggests that the model might not have been trained enough with pictures of cars in the center of the road. Third, using the lattice method on Tensorflow, we were able to identify a corner case misclassification in a cluster of correct predictions (note the isolated red circle\nwith coordinates (0.1933, 0.0244, 0.4589)). All this information provides insights on the classifiers that can be useful in the hunt for counterexamples."}, {"heading": "5 Experimental Results", "text": ""}, {"heading": "5.1 Implementation Details", "text": "The presented falsification framework has been implemented in a Matlab toolbox publicly available at https://github.com/tommasodreossi/FalsifCPSML. The tool deals with Simulink models of CPSML and STL specifications. It consists of a temporal logic falsifier and an ML analyzer that interact to falsify the given STL specification against the decomposed Simulink model. As an STL falsifier, we chose the existing tool Breach [4], while the ML analyzer has been implemented from scratch. The ML analyzer implementation includes the feature space abstractor and the ML approximation algorithm (see Section 4). The feature space abstractor implements a picture generator that concretizes the abstracted feature vectors. The approximation algorithm, that computes an approximation of the analyzed ML component, gives to the user the possibility of selecting the sampling sequence method, interpolation technique, and setting the desired error rate. Our tool is interfaced with the deep learning frameworks Caffe [10] and Tensorflow [13]. Our tool has been tested on a desktop computer Dell XPS 8900, Intel (R) Core(TM) i7-6700 CPU 3.40GHz, DIMM RAM 16 GB 2132 MHz, GPU NVIDIA GeForce GTX TITAN X, with Ubuntu 14.04.5 LTS and Matlab R2016b."}, {"heading": "5.2 Case Studies", "text": "For the experimental evaluations, we consider a closed-loop Simulink model of a semi-autonomous vehicle with an Advanced Emergency Braking System (AEBS) [20] connected to an image classifier. The model mainly consists of a four-speed automatic transmission controller linked to an AEBS that automatically prevents collisions with preceding obstacles and alleviate the harshness of a crash when a collision is likely to happen (see Figure 5). The AEBS determines\na braking mode depending on the speed of the vehicle vs, the possible presence\nof a preceding obstacle, its velocity vp, and the longitudinal distance dist between the two. The distance dist is provided by radars having 30m of range. For obstacles farther than 30m, the camera, connected to an image classifier, alerts the AEBS that, in the case of detected obstacle, goes into warning mode.\nDepending on vs, vp, dist, and the presence of obstacles detected by the image classifier, the AEBS computes the time to collision and longitudinal safety indices, whose values determine a controlled mode among safe, warning, braking, and collision mitigation. In safe mode, the car does not need to brake. In warning mode, the driver should brake to avoid a collision. If this does not happen, the system goes into braking mode, where the automatic brake slows down the vehicle. Finally, in collision mitigation mode, the system, determining that a crash is unavoidable, triggers a full braking action aimed to minimize the damage.\nTo establish the correctness of the system and in particular of its AEBS controller, we formalize the STL specification G(\u00ac(dist(t)) \u2264 0), that requires dist(t) to always be positive, i.e., no collision happens. The input space is vs(0) \u2208 [0, 40] (mph), dist(0) \u2208 [0, 60] (m), and the set of all RGB pictures of size 1000\u00d7 600. The preceding vehicle is not moving, i.e., vp(t) = 0 (mph).\nAt first, we compute the validity domain of \u03d5 assuming that the radars are able to provide exact measurements for any distance dist(t) and the image classifier correctly detects the presence of a preceding vehicle. The computed validity domain is depicted in Figure 6: green for U\u03d5 and red for U\u00ac\u03d5. Next, we identify candidate counterexamples that belong to the satisfactory set (i.e., the inputs that satisfy the specification) but might be influenced by a misclassification of the image classifier. Since the AEBS relies on the classifier only for distances larger than 30m, we can focus on the subset of the input space with dist(0) \u2265 30. Specifically, we identify potential coun-\nterexamples by analyzing a pessimistic version of the model where the ML component always misclassifies the input pictures (see Figure 6, area with dashed boundary). From this sub-input space, we can identify candidate counterexamples, such as, for instance, (25, 40) (i.e., vs(0) = 25 and dist(0) = 40).\nNext, let us consider the Caffe image classifier and the ML analyzer presented in Section 4 that generates pictures from the abstract feature space A = [0, 1]3, where the dimensions of A determine the x and z displacements of a car and the brightness of a generated picture, respectively. The goal now is to determine an abstract feature ac \u2208 A related to the candidate counterexample (25, 40), that generates a picture that is misclassified by the ML component and might lead to\na violation of the specification \u03d5. The dist(0) component of uc = (25, 40) determines a precise z displacement a2 = 0.2 in the abstract picture. Now, we need to determine the values of the abstract x displacement and brightness. Looking at the interpolation projection of Figure 4 (b), we notice that the approximation function misclassifies pictures with abstract component a1 \u2208 [0.4, 0.5] and a3 = 0.2. Thus, it is reasonable to try to falsify the original model on the input element vs(0) = 25, dist(0) = 40, and concretized picture \u03b3(0.5, 0.2, 0.2). For this targeted input, the temporal logic falsifier computed a robustness value for \u03d5 of \u221224.60, meaning that a falsifying counterexample has been found. Other counterexamples found with the same technique are, e.g., (27, 45) or (31, 56) that, associated with the correspondent concretized pictures with a1 = 0.5 and a3 = 0.2, lead to the robustness values \u221223.86 and \u221224.38, respectively (see Figure 6, red crosses). Conversely, we also disproved some candidate counterexamples, such as (28, 50), (24, 35), or (25, 45), whose robustness values are 9.93, 7.40, and 7.67 (see Figure 6, green circles).\nFor experimental purposes, we try to falsify a counterexample in which we change the x position of the abstract feature so that the approximation function correctly classifies the picture. For instance, by altering the counterexample (27, 45) with \u03b3(0.5, 0.225, 0.2) to (27, 45) with \u03b3(1.0, 0.225, 0.2), we obtain a robusteness value of 9.09, that means that the AEBS is able to avoid the car for the same combination of velocity and distance of the counterexample, but different x position of the preceding vehicle. Another example, is the robustness value \u221224.38 of the falsifying input (31, 56) with \u03b3(0.5, 0.28, 0.2), that altered to \u03b3(0.0, 0.28, 0.2), changes to 12.41.\nFinally, we test Tensorflow on the corner case misclassification identified in Section 4.2 (i.e, the picture \u03b3(0.1933, 0.0244, 0.4589)). The distance dist(0) = 4.88 related to this abstract feature is below the activation threshold of the image classifier. Thus, the falsification points are exactly the same as those of the computed validity domain (i.e., dist(0) = 4.88 and vs(0) \u2208 [4, 40]). This study shows how a misclassification of the ML component might not affect the correctness of the CPSML model."}, {"heading": "6 Conclusion", "text": "We presented a compositional falsification framework for STL specifications against CPSML models based on the separate analysis of a CPS system and its ML components. We introduced an ML analyzer able to abstract feature spaces, approximate ML classifiers, and provide sets of misclassified feature vectors that can be used to drive the falsification process. We implemented our framework and showed its effectiveness for an autonomous driving controller using perception based on deep neural networks.\nThis work lays the basis for future advancements. We intend to improve our ML analyzer exploring the automatic generation of feature space abstractions from given training sets. Another direction is to integrate other techniques for generating misclassifications of ML components (e.g. [15,9]) into our approach. One could also apply our ML analyzer outside the falsification context, such as for controller synthesis. Finally, our compositional methodology could be extended to other, non-cyber-physical, systems that contain ML components."}], "references": [{"title": "S-taliro: A tool for temporal logic falsification for hybrid systems", "author": ["Y. Annpureddy", "C. Liu", "G.E. Fainekos", "S. Sankaranarayanan"], "venue": "In Tools and Algorithms for the Construction and Analysis of Systems, TACAS, pages 254\u2013257,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Selection of relevant features and examples in machine learning", "author": ["A.L. Blum", "P. Langley"], "venue": "Artificial intelligence, 97(1):245\u2013271,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "et al", "author": ["M. Bojarski", "D. Del Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang"], "venue": "End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Breach, a toolbox for verification and parameter synthesis of hybrid systems", "author": ["A. Donz\u00e9"], "venue": "In Computer Aided Verification, CAV, pages 167\u2013170,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient guiding strategies for testing of temporal properties of hybrid systems", "author": ["T. Dreossi", "T. Dang", "A. Donz\u00e9", "J. Kapinski", "X. Jin", "J. Deshmukh"], "venue": "In NASA Formal Methods, NFM, pages 127\u2013142,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "C2E2: a verification tool for stateflow models", "author": ["P.S. Duggirala", "S. Mitra", "M. Viswanathan", "M. Potok"], "venue": "In International Conference on Tools and Algorithms for the Construction and Analysis of Systems, pages 68\u201382. Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of classifiers\u2019 robustness to adversarial perturbations", "author": ["A. Fawzi", "O. Fawzi", "P. Frossard"], "venue": "arXiv preprint arXiv:1502.02590,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "et al", "author": ["G. Hinto"], "venue": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82\u201397,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Safety verification of deep neural networks", "author": ["X. Huang", "M. Kwiatkowska", "S. Wang", "M. Wu"], "venue": "CoRR, abs/1610.06940,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "In ACM Multimedia Conference, ACMMM, pages 675\u2013678,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in neural information processing systems, pages 1097\u20131105,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Monitoring temporal properties of continuous signals", "author": ["O. Maler", "D. Nickovic"], "venue": "In Formal Techniques, Modelling and Analysis of Timed and Fault-Tolerant Systems, pages 152\u2013166. Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "TensorFlow: Large-scale machine learning", "author": ["Mart\u0301\u0131n Abadi"], "venue": "on heterogeneous systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Machine learning: An artificial intelligence approach", "author": ["R.S. Michalski", "J.G. Carbonell", "T.M. Mitchell"], "venue": "Springer Science & Business Media,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In Computer Vision and Pattern Recognition, CVPR, pages 427\u2013436. IEEE,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Low-discrepancy and low-dispersion sequences", "author": ["H. Niederreiter"], "venue": "Journal of number theory, 30(1):51\u201370,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1988}, {"title": "Towards verified artificial intelligence", "author": ["S.A. Seshia", "D. Sadigh", "S.S. Sastry"], "venue": "CoRR, abs/1606.08514,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "arXiv:1312.6199,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Development and evaluations of advanced emergency braking system algorithm for the commercial vehicle", "author": ["L. Taeyoung", "Y. Kyongsu", "K. Jangseop", "L. Jaewan"], "venue": "In Enhanced Safety of Vehicles Conference, ESV, pages 11\u20130290,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Principles of risk minimization for learning theory", "author": ["V. Vapnik"], "venue": "In NIPS, pages 831\u2013838,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1991}], "referenceMentions": [{"referenceID": 1, "context": ", [2,14,10,8]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 13, "context": ", [2,14,10,8]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 9, "context": ", [2,14,10,8]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 7, "context": ", [2,14,10,8]).", "startOffset": 2, "endOffset": 13}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 16, "context": "The safety-critical nature of such systems involving ML raises the need for formal methods [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "We formulate this question as the falsification problem for CPS models with ML components (CPSML): given a formal specification \u03c6 in signal temporal logic (STL) [12], and a CPSML model M , find an input for which M does not satisfy \u03c6.", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "Third, the formal verification of ML components is a difficult, and somewhat ill-posed problem due to the complexity of the underlying ML algorithms, large feature spaces, and the lack of consensus on a formal definition of correctness [18].", "startOffset": 236, "endOffset": 240}, {"referenceID": 7, "context": "Further, our technique can handle any machine learning technique, including the methods based on deep neural networks [8] that have proved effective in many recent applications.", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "Specifically, we use a temporal logic falsifier, Breach [4], in Steps (1) and (3) to partition a given input set into values that do and do not satisfy a given specification, and an ML analyzer in Step (2) to determine subsets of feature vectors that are misclassified by the ML components.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "5, we demonstrate the effectiveness of our approach on an Automatic Emergency Braking System (AEBS) involving an image classifier for obstacle detection based on deep neural networks using leading software packages Caffe [10] and TensorFlow [13].", "startOffset": 221, "endOffset": 225}, {"referenceID": 12, "context": "5, we demonstrate the effectiveness of our approach on an Automatic Emergency Braking System (AEBS) involving an image classifier for obstacle detection based on deep neural networks using leading software packages Caffe [10] and TensorFlow [13].", "startOffset": 241, "endOffset": 245}, {"referenceID": 3, "context": ", Breach [4], S-TaLiRo [1], RRT-REX [5], C2E2 [6]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": ", Breach [4], S-TaLiRo [1], RRT-REX [5], C2E2 [6]).", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": ", Breach [4], S-TaLiRo [1], RRT-REX [5], C2E2 [6]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": ", Breach [4], S-TaLiRo [1], RRT-REX [5], C2E2 [6]).", "startOffset": 46, "endOffset": 49}, {"referenceID": 16, "context": "While the verification of ML programs is less well-defined [18], recent efforts [19] show how even well trained neural networks can be sensitive to small adversarial perturbations, i.", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "While the verification of ML programs is less well-defined [18], recent efforts [19] show how even well trained neural networks can be sensitive to small adversarial perturbations, i.", "startOffset": 80, "endOffset": 84}, {"referenceID": 19, "context": "Other efforts have tried to characterize the correctness of neural networks in terms of risk [21] (i.", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": ", probability of misclassifying a given input) or robustness [7] (i.", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": ", the minimal perturbation leading to a misclassification), while others proposed methods to generate pictures [16] or perturbations [15,9] in such a way to \u201cfool\u201d neural networks.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": ", the minimal perturbation leading to a misclassification), while others proposed methods to generate pictures [16] or perturbations [15,9] in such a way to \u201cfool\u201d neural networks.", "startOffset": 133, "endOffset": 139}, {"referenceID": 11, "context": "We consider Signal Temporal Logic [12] (STL) as the language to specify properties to be verified against a CPSML model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 3, "context": "This example illustrates how the compositional approach relies on tools, such as Breach [4], that compute validity domains and falsify STL specifications, as well as a ML analyzer.", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": "In this setting, we can define the abstraction and concretization functions \u03b1 and \u03b3 that relate the abstract set A = [0, 1] and X\u0303.", "startOffset": 117, "endOffset": 123}, {"referenceID": 15, "context": ", [17].", "startOffset": 2, "endOffset": 6}, {"referenceID": 9, "context": "We now analyze two image classifiers: the Caffe [10] version of AlexNet [11] and the Inception-v3 model of Tensorflow [13], both trained on the ImageNet database.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "We now analyze two image classifiers: the Caffe [10] version of AlexNet [11] and the Inception-v3 model of Tensorflow [13], both trained on the ImageNet database.", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "We now analyze two image classifiers: the Caffe [10] version of AlexNet [11] and the Inception-v3 model of Tensorflow [13], both trained on the ImageNet database.", "startOffset": 118, "endOffset": 122}, {"referenceID": 3, "context": "As an STL falsifier, we chose the existing tool Breach [4], while the ML analyzer has been implemented from scratch.", "startOffset": 55, "endOffset": 58}, {"referenceID": 9, "context": "Our tool is interfaced with the deep learning frameworks Caffe [10] and Tensorflow [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "Our tool is interfaced with the deep learning frameworks Caffe [10] and Tensorflow [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "For the experimental evaluations, we consider a closed-loop Simulink model of a semi-autonomous vehicle with an Advanced Emergency Braking System (AEBS) [20] connected to an image classifier.", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "Next, let us consider the Caffe image classifier and the ML analyzer presented in Section 4 that generates pictures from the abstract feature space A = [0, 1], where the dimensions of A determine the x and z displacements of a car and the brightness of a generated picture, respectively.", "startOffset": 152, "endOffset": 158}, {"referenceID": 3, "context": "88 and vs(0) \u2208 [4, 40]).", "startOffset": 15, "endOffset": 22}, {"referenceID": 8, "context": "[15,9]) into our approach.", "startOffset": 0, "endOffset": 6}], "year": 2017, "abstractText": "Cyber-physical systems (CPS), such as automotive systems, are starting to include sophisticated machine learning (ML) components. Their correctness, therefore, depends on properties of the inner ML modules. While learning algorithms aim to generalize from examples, they are only as good as the examples provided, and recent efforts have shown that they can produce inconsistent output under small adversarial perturbations. This raises the question: can the output from learning components can lead to a failure of the entire CPS? In this work, we address this question by formulating it as a problem of falsifying signal temporal logic (STL) specifications for CPS with ML components. We propose a compositional falsification framework where a temporal logic falsifier and a machine learning analyzer cooperate with the aim of finding falsifying executions of the considered model. The efficacy of the proposed technique is shown on an automatic emergency braking system model with a perception component based on deep neural networks.", "creator": "LaTeX with hyperref package"}}}