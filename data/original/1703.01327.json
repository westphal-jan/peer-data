{"id": "1703.01327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Multi-step Reinforcement Learning: A Unifying Algorithm", "abstract": "Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD($\\lambda$) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter $\\lambda$. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, $Q$-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called $Q(\\sigma)$ which unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, $\\sigma$, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). $Q(\\sigma)$ is generally applicable to both on- and off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of $\\sigma$, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance.", "histories": [["v1", "Fri, 3 Mar 2017 20:19:08 GMT  (452kb,D)", "http://arxiv.org/abs/1703.01327v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["kristopher de asis", "j fernando hernandez-garcia", "g zacharias holland", "richard s sutton"], "accepted": false, "id": "1703.01327"}, "pdf": {"name": "1703.01327.pdf", "metadata": {"source": "META", "title": "Multi-step Reinforcement Learning: A Unifying Algorithm", "authors": ["Kristopher De Asis", "Fernando Hernandez-Garcia", "Zacharias Holland", "Richard S. Sutton"], "emails": ["KLDEASIS@UALBERTA.CA", "JFHERNAN@UALBERTA.CA", "GHOLLAND@UALBERTA.CA", "RSUTTON@UALBERTA.CA"], "sections": [{"heading": "1. The Landscape of TD Algorithms", "text": "Temporal-difference (TD) methods (Sutton, 1988) are an important concept in reinforcement learning (RL) which combines ideas from Monte Carlo and dynamic programming methods. TD methods allow learning to occur directly from raw experience in the absence of a model of the environment\u2019s dynamics, like with Monte Carlo methods, while also allowing estimates to be updated based on other learned estimates without waiting for a final result, like with dynamic programming. The core concepts of TD methods provide a flexible framework for creating a variety of powerful algorithms that can be used for both prediction and control.\nThere are a number of TD control methods that have been proposed. Q-learning (Watkins, 1989) is arguably the most popular, and is considered an off-policy method because the policy generating the behaviour (the behaviour policy), and the policy that is being learned (the target policy) are different. Sarsa (Rummery & Niranjan, 1994; Sutton, 1996) is the classical on-policy control method, where the behaviour and target policies are the same. However, Sarsa can be extended to learn off-policy with the use of importance sampling (Precup, Sutton, & Singh, 2000). Expected Sarsa is an extension of Sarsa that, instead of using the action-value of the next state to update the value of the current state, uses the expectation of all the subsequent action-values of the current state with respect to the target policy. Expected Sarsa has been studied as a strictly on-policy method (van Seijen, van Hasselt, Whiteson, & Wiering, 2009), but in this paper we present a more general version that can be used for both on- and off-policy learning and that also subsumes Q-learning. All of these methods are often described in the simple one-step case, but they can also be extended across multiple time steps.\nThe TD(\u03bb) algorithm unifies one-step TD learning with Monte Carlo methods (Sutton, 1988). Through the use of eligibility traces, and the trace-decay parameter, \u03bb \u2208 [0, 1],\nar X\niv :1\n70 3.\n01 32\n7v 1\n[ cs\n.A I]\n3 M\nar 2\na spectrum of algorithms is created. At one end, \u03bb = 1, exists Monte Carlo methods, and at the other, \u03bb = 0, exists one-step TD learning. In the middle of the spectrum are intermediate methods which can perform better than the methods at either extreme (Sutton & Barto, 1998). The concept of eligibility traces can also be applied to TD control methods such as Sarsa and Q-learning, which can create more efficient learning and produce better performance (Rummery, 1995).\nMulti-step TD methods are usually thought of in terms of an average of many multi-step returns of differing lengths and are often associated with eligibility traces, as is the case with TD(\u03bb). However, it is also natural to think of them in terms of individual n-step returns with their associated nstep backup (Sutton & Barto, 1998). We refer to each of these individual backups as atomic backups, whereas the combination of several atomic backups of different lengths creates a compound backup.\nIn the existing literature, it is not clear how best to extend one-step Expected Sarsa to a multi-step algorithm. The Tree-backup algorithm was originally presented as a method to perform off-policy evaluation when the behaviour policy is non-Markov, non-stationary or completely unknown (Precup et al., 2000). In this paper, we re-present Tree-backup as a natural multi-step extension of Expected Sarsa. Instead of performing the updates with entirely sampled transitions as with multi-step Sarsa, Treebackup performs the update using the expected values of all the actions at each transition.\nQ(\u03c3) is an algorithm that was first proposed by Sutton and Barto (2017) which unifies and generalizes the existing multi-step TD control methods. The degree of sampling performed by the algorithm is controlled by the sampling parameter, \u03c3. At one extreme (\u03c3 = 1) exists Sarsa (full sampling), and at the other (\u03c3 = 0) exists Tree-backup (pure expectation). Intermediate values of \u03c3 create algorithms with a mixture of sampling and expectation. In this work we show that an intermediate value of \u03c3 can outperform the algorithms that exist at either extreme. In addition, we show that \u03c3 can be varied dynamically to produce even greater performance. We limit our discussion of Q(\u03c3) to the atomic multi-step case without eligibility traces, but a natural extension is to make use of compound backups and is an avenue for future research. Furthermore, Q(\u03c3) is generally applicable to both on- and off-policy learning, but for our initial empirical study we examined only on-policy prediction and control problems."}, {"heading": "2. MDPs and One-step Solution Methods", "text": "The sequential decision problem encountered in RL is often modeled as a Markov decision process (MDP). Under this\nframework, an agent and the environment interact over a sequence of discrete time steps t. At every time step, the agent receives information about the environment\u2019s state, St \u2208 S , where S is the set of all possible states. The agent uses this information to select an action, At, from the set of all possible actions A. Based on the behavior of the agent and the state of the environment, the agent receives a reward, Rt+1 \u2208 R \u2282 R, and moves to another state, St+1 \u2208 S, with a state-transition probability p(s\u2032|s, a) = P (St+1 = s \u2032|St = s,At = a), for a \u2208 A and s, s\u2032 \u2208 S.\nThe agent behaves according to a policy \u03c0(a|s), which is a probability distribution over the set S \u00d7 A. Through the process of policy iteration (Sutton & Barto, 1998), the agent learns the optimal policy, \u03c0\u2217, that maximizes the expected discounted return:\nGt = Rt+1+\u03b3Rt+2+\u03b3 2Rt+3+... = T\u22121\u2211 k=0 \u03b3kRt+1+k, (1)\nfor a discount factor \u03b3 \u2208 [0, 1], and T is infinity for continuing tasks, or the final time step in episodic tasks.\nTD algorithms strive to maximize the expected return by computing value functions that estimate the expected future rewards in terms of the elements of the environment and the actions of the agent. The state-value function is the expected return when the agent is in a state s and follows policy \u03c0, defined as v\u03c0(s) = E\u03c0[Gt|St = s]. For control, most of the time we focus on estimating the action-value function, which is the expected return when the agent takes an action a, in a state s, while following a policy \u03c0, and is defined as:\nq\u03c0(s, a) = E\u03c0[Gt|St = s,At = a]. (2)\nEquation 2 can be estimated iteratively by observing new rewards, bootstraping on old estimates of q\u03c0 , and using the update rule:\nQ(St, At)\u2190 Q(St, At) (3) + \u03b1[Rt+1 + \u03b3Q(St+1, At+1)\u2212Q(St, At)],\nwhere \u03b1 \u2208 (0, 1] is the step size parameter. Update rules are also known as backup operations because they transfer information back from future states to the current one. A common way to visualize backup operations is by using backup diagrams such as the ones depicted in Figure 1.\nTabular solution methods can be used when the state and action spaces are small enough so that it is possible to maintain the estimates of the value functions in an array or table. When the state space is large and/or continuous, approximate solution methods need to be used, which combine RL algorithms with some kind of function approximation scheme. For simplicity, we present the algorithmic ideas\nin this paper as tabular solution methods, but they can also be extended to use function approximation, and thus can serve also as approximate solution methods. In our experiments we study problems that require tabular solution methods and a problem that requires an approximate solution method using function approximation.\nThe term in brackets in (3):\n\u03b4St = Rt+1 + \u03b3Q(St+1, At+1)\u2212Q(St, At), (4)\nis also known as the TD error, denoted \u03b4t. TD control methods are characterized by their TD error; for example, the TD error in (4) corresponds to the classic on-policy method known as Sarsa.\nBecause learning requires a certain amount of exploration, behaving greedily with respect to the estimated optimal policy is often infeasible. Therefore, agents are often trained under -greedy policies for which the agent only chooses the optimal action with a probability (1 \u2212 ) and behaves randomly with probability , for \u2208 [0, 1]. Nevertheless, learning the optimal policy is possible if it is done off-policy. When the agent is learning off-policy, it behaves according to a behavior policy, \u00b5, while learning a target policy, \u03c0. This can be achieved by using another TD control method, Expected Sarsa. In contrast with Sarsa, Expected Sarsa behaves according the behavior policy, but updates its estimate by taking an expectation of Q(St, At) over the actions at time t, according to the target policy (van Seijen et al., 2009). For convenience, let the expected action-value be defined as:\nVt+1 = \u2211 a \u03c0(a|St+1)Q(St+1, a). (5)\nThen, the TD error of Expected Sarsa can be written as:\n\u03b4ESt = Rt+1 + \u03b3Vt+1 \u2212Q(St, At). (6)\nA special case of Expected Sarsa is Q-learning, where the estimate is updated according to the maximum of Q(St, a) over the actions (Watkins, 1989):\n\u03b4QLt = Rt+1 + \u03b3max a Q(St+1, a)\u2212Q(St, At) (7)\nQ-learning is the resulting algorithm when the target policy of Expected Sarsa is the greedy policy with respect to Q."}, {"heading": "3. Atomic Multi-Step Algorithms", "text": "The TD methods presented in the previous section can be generalized even further by bootstraping over longer time intervals. This has been shown to decrease the bias of the update at the cost of increasing the variance (Jaakkola, Jordan, & Singh, 1994). Nevertheless, in many cases it is possible to achieve better performance by choosing a value for\nthe backup length parameter, n, greater than one, which creates an atomic multi-step algorithm (Sutton & Barto, 1998).1 Just like how one-step methods are defined by their TD error, each atomic multi-step algorithm is characterized by its n-step return. For atomic multi-step Sarsa, the n-step return is:\nG (n) t = Rt+1 + \u03b3Rt+2 + \u03b3 2Rt+3 + ...+ \u03b3 n\u22121Rt+n\n+ \u03b3nQt+n\u22121(St+n, At+n),\n= n\u22121\u2211 k=0 \u03b3krt+1 + \u03b3 nQt+n\u22121(St+n, At+n), (8)\nwhere Qt+n\u22121 is the estimate of q\u03c0 at time t+ n\u2212 1, and the superscript (n) denotes the length of the backup. n-step Sarsa can be adapted for off-policy learning by introducing an importance sampling ratio term (Precup et al., 2000):\n\u03c1t+nt = min(t+n\u22121,T\u22121)\u220f k=t \u03c0(Ak|Sk) \u00b5(Ak|Sk) , (9)\nand multiplying it with the TD error to get the following update rule:\nQt+n(St, At)\u2190 Qt+n\u22121(St, At) (10)\n+ \u03b1\u03c1t+nt+1 [G (n) t \u2212Qt+n\u22121(St, At)].\nIn the update, the action-values for all other states remain the same \u2013 i.e. Qt+n(s, a) = Qt+n\u22121(s, a),\u2200 s 6= St, and a 6= At. This update rule is not only applicable for offpolicy n-step Sarsa, but is a generally useful form for other atomic multi-step algorithms as well.\n1Sutton and Barto (1998) refer to these as simple multi step algorithms.\nExpected Sarsa can also be generalized to a multi-step method by using the return:\nG (n) t = Rt+1 + \u03b3Rt+2 + \u03b3 2Rt+3 + ...+ \u03b3 nVt+n. (11)\nThe first n\u2212 1 states and actions are sampled according to the behaviour policy, as with n-step Sarsa, but the last state is backed up according to the expected action-value under the target policy. To make n-step Expected Sarsa entirely off-policy, an importance sampling ratio term can also be introduced, but it needs to omit the last time step. The resulting update would be the same as in (10), but would use \u03c1t+n\u22121t+1 and the n-step return for n-step Expected Sarsa from (11).\nA drawback to using importance sampling to learn offpolicy is that it can create high variance which must be compensated for by using small step sizes; this can slow learning (Precup et al., 2000). In the next section we present a method that is also a generalization of Expected Sarsa, but that can learn off-policy without importance sampling."}, {"heading": "4. Tree-backup", "text": "As shown in (11), the TD return of n-step Expected Sarsa is calculated by taking an expectation over the actions at the last step of the backup. However, it is possible to extend this idea to every time step of the backup by taking an expectation at every step (Precup et al., 2000). The resulting algorithm is a multi-step generalization of Expected Sarsa that is known as Tree-backup because of its characteristic backup diagram (Figure 1).\nBecause Expected Sarsa subsumes Q-learning, Treebackup can also be thought of as a multi-step generalization of Q-learning if the target policy is the greedy policy with respect to the action-value function.\nTree-backup has several advantages over n-step Expected Sarsa. Tree-backup has the capacity for learning off-policy without the need for importance sampling. This has the effect of reducing variance and speeding learning. Additionally, because an importance sampling ratio does not need to be computed, the behavior policy does not need to be stationary, Markov, or even known (Precup et al., 2000).\nEach branch of the tree represents an action, while the main branch represents the action taken at time t. The value of each of the branches is the value of Qt+n(St, At) for the corresponding t, whereas the value of each segment of the main branch is the reward at the corresponding time step. The n-step return is the sum of the values of each branch weighted by the product of the probabilities of the actions leading to the branch and multiplied by the corresponding power of the discount term. For clarity, it is easier to\npresent the n-step return of the Tree-backup algorithm in terms of the TD error of Expected Sarsa from (6):\nG (n) t = Qt\u22121(St, At) (12)\n+ min(t+n\u22121,T\u22121)\u2211 k=t \u03b4ESk k\u220f i=t+1 \u03b3\u03c0(Ai|Si).\nThis atomic version of multi-step Tree-backup was first presented by Sutton and Barto (2017)."}, {"heading": "5. TheQ(\u03c3) Algorithm", "text": "In the previous sections we have incrementally introduced several generalizations for the TD control methods Sarsa and Expected Sarsa, and in this section we present an algorithm that unifies them called Q(\u03c3).\nSarsa can be generalized to an atomic multi-step algorithm by using an n-step return, and n-step Sarsa generalizes to an off-policy algorithm through the use of importance sampling. In contrast, Expected Sarsa can learn off-policy without the need for importance sampling, and generalizes to the atomic multi-step algorithms: Tree-backup and nstep Expected Sarsa. All of the algorithms presented so far can be broadly categorized into two families: those that backup their actions as samples, like Sarsa; and those that consider an expectation over all actions in their backup, like Expected Sarsa and Tree-backup.2 In this section, we introduce a method to unify both families of algorithms by introducing a new parameter, \u03c3. The possibility of unifying Sarsa and Tree-backup was first suggested by Precup et al. (2000), and the first formulation of Q(\u03c3) was presented by Sutton and Barto (2017).\nThe intuition behindQ(\u03c3) is based on the idea that we have a choice to update the estimate of q\u03c0 based on one action sampled from the set of possible future actions, or based on the expectation over the possible future actions. For example, with n-step Sarsa, a sample is taken at every step of the backup, whereas with the Tree-backup algorithm, an expectation is taken instead. However, the choice of sampling or expectation does not have to remain constant for every step of the backup. Furthermore, the backup at a time step t could be based on a weighted average of both sampling and expectation. In order to implement this, the parameter, \u03c3t \u2208 [0, 1], is introduced to control the degree of sampling at each step of the backup. Thus, the TD error of Q(\u03c3) can be represented in terms of a weighted sum of\n2It may be useful to note that n-step Expected Sarsa belongs to both families since it samples every action, except the last one, for which it considers the expectation.\nthe TD errors of Sarsa and Expected Sarsa:\n\u03b4\u03c3t = \u03c3t+1\u03b4 S t + (1\u2212 \u03c3t+1)\u03b4ESt ,\n= Rt+1 + \u03b3[\u03c3t+1Qt(St+1, At+1) + (1\u2212 \u03c3t+1)Vt+1] \u2212Qt\u22121(St, At). (13)\nThe n-step return is then:\nG (n) t = Qt\u22121(St, At) (14)\n+ min(t+n\u22121,T\u22121)\u2211 k=t \u03b4\u03c3k k\u220f i=t+1 \u03b3[(1\u2212 \u03c3i)\u03c0(Ai|Si) + \u03c3i].\nMoreover, the importance sampling ratio from (9) can be modified to include \u03c3 as follows:\n\u03c1t+nt+1 = min(t+n\u22121,T\u22121)\u220f k=t+1 ( \u03c3k \u03c0(Ak|Sk) \u00b5(Ak|Sk) + 1\u2212 \u03c3k ) . (15)\nThe update rule for Q(\u03c3) can then be obtained by using G\n(n) t from (14) and \u03c1 t+n t+1 from (15), with the update rule from (10). Algorithm 1 shows the pseudocode for the onpolicy Q(\u03c3).3\nIt is important to note that every TD control method presented thus far can be obtained with Q(\u03c3) by varying the sampling parameter, \u03c3; when \u03c3 = 1, we obtain Sarsa, when \u03c3 = 0, we obtain Expected Sarsa and Tree-backup, and when \u03c3 = 1 for every step of the backup except for the last, where \u03c3 = 0, we obtain n-step Expected Sarsa. Intermediate values of \u03c3 between 0 and 1 create entirely new algorithms that exist somewhere between full sampling and pure expectation. Furthermore, \u03c3 does not need to remain constant throughout every episode or even at every time step during an episode or continuing task. \u03c3 could be varied dynamically as a function of time, of the current state, or of some measure of the learning progress. In particular, \u03c3 could also be varied as a function of the episode number, which we investigate in our experiments. There are potentially a variety of effective schemes for choosing and varying \u03c3, and would be a subject for further research."}, {"heading": "6. Experiments", "text": "In this section, we explore the performance of the different atomic multi-step algorithms that we have presented. First, we evaluate their performance on a prediction task which motivates the unification Sarsa and Expected Sarsa, and we also introduce the idea of dynamically varying \u03c3 to take advantage of each algorithm\u2019s particular performance characteristics. Then, on a gridworld navigation problem, we show that it is possible to improve performance with Q(\u03c3)\n3For an example of the psuedocode for general off-policy Q(\u03c3), see Sutton and Barto (2017).\nAlgorithm 1 On-policy n-step Q(\u03c3) for estimating q\u03c0 Initialize S0 6= terminal; select A0 according to \u03c0(.|S0) Store S0, A0, and Q(S0, A0) for t = 0, 1, 2, ..., T + n\u2212 1 do\nif t < T then Take action At; observe R and St+1 Store St+1 if St+1 is terminal then\nStore: \u03b4\u03c3t = R\u2212Q(St, At) else\nSelect At+1 according to \u03c0(.|St+1) and Store Store Q(St+1, At+1), \u03c3t+1, \u03c0(At+1|St+1) Store: \u03b4\u03c3t = R+ \u03b3[\u03c3t+1Q(St+1, At+1)\n+(1\u2212 \u03c3t+1)Vt+1]\u2212Q(St, At) end if\nby using an intermediate or dynamically varying value of \u03c3, and by increasing the length of the backup. Finally, we investigate the performance of Q(\u03c3) as an approximate solution method for a problem with a continuous state space."}, {"heading": "6.1. 19-State Random Walk", "text": "The 19-state random walk is a 1-dimensional environment where an agent randomly transitions to one of two neighboring states. There is a terminal state on each end of the environment, transitioning to one of them gives a reward of -1, and transitioning to the other gives a reward of 1. To compare algorithms that involve taking an expectation based on its policy, the task is formulated such that each state had two actions. Each action deterministically transitions to one of the two neighboring states, and the agent learns on-policy under an equiprobable random behavior policy. This differs from typical random walk setups where each state has one action that will randomly transition to either neighboring state (Sutton & Barto, 1998), but the resulting state values are identical.\nThis environment was treated as a prediction task where\na learning algorithm is to estimate a value function under its behavior policy. We conducted an experiment comparing various Q(\u03c3) algorithm instances, assessing different multi-step backup lengths, step sizes, and degrees of sampling. The root-mean-square (RMS) error between its estimated value function and the analytically computed values was measured after each episode. Each Q(\u03c3) instance and parameter setting ran for 50 episodes and the results are averaged across 100 runs.\nFigure 3 shows the results with n = 3 and \u03b1 = 0.4, which was found to be representative of the best parameter setting for each instance of Q(\u03c3) on this task. Sarsa (full sampling) had better initial performance but poor asymptotic performance, Tree-backup (no sampling) had poor initial performance but better asymptotic performance, and intermediate degrees of sampling traded off between the initial and asymptotic performances. This motivated the idea of dynamically decreasing \u03c3 from 1 (full sampling) towards 0 (pure expectation) to take advantage of the initial performance of Sarsa, and the asymptotic performance of Treebackup. To accomplish this we decreased \u03c3 by a factor of 0.95 after each episode. Q(\u03c3) with a dynamically varying \u03c3 outperformed all of the fixed degrees of sampling."}, {"heading": "6.2. Stochastic Windy Gridworld", "text": "The windy gridworld is a tabular navigation task in a standard gridworld which is described in (Sutton & Barto, 1998). There is a start state and a goal state, and there are\nfour possible moves: right, left, up, and down. When the agent moves into one of the middle columns of the gridworld, it is affected by an upward \u201cwind\u201d which shifts the resultant next state upwards by a number of cells and varies from column to column. Figure 4 shows the layout of the windy gridworld along with the strengths of the wind in each column. If the agent is at the edge of the world and selects a move that would cause it to leave the grid, or would be pushed off the world by the wind, it is simply replaced in the nearest state at the edge of the world. At each time step the agent receives a constant reward of -1 until the goal is reached.\nA variation of the windy gridworld is one where the results of choosing an action are not deterministic; let this be called the stochastic windy gridworld. The layout, actions, and wind strengths are the same as those in the windy gridworld described above, but at each time step, with a probability of 10%, the next state that results from picking any action is determined at random from the 8 states currently surrounding the agent.\nWe conducted an experiment on the stochastic windy gridworld which consisted of 1000 runs of 100 episodes each to evaluate the performance of various instances of Q(\u03c3) with different parameter combinations. All instances of the algorithms behaved and learned according to an -greedy policy, with = 0.1. As the performance measure, we compared the average return over the 100 episodes. The results are summarized in Figure 5.\nFor all the values of \u03c3 that we tested, choosing n = 4 resulted in the greatest performance; higher and lower values of n decreased the performance. Overall, Q(\u03c3) with a dynamic \u03c3 performed the best, while \u03c3 = 0.5 was a close second."}, {"heading": "6.3. Mountain Cliff", "text": "We implemented a variant of the classical episodic task, mountain car, as described by Sutton and Barto (1998). For this implementation, the rewards, actions and goal remained the same. However, if the agent ever ventured past the top of the leftmost mountain, it would fall off a cliff, be rewarded -100 and returned to a random initial location in the valley between the two hills. We named this environment mountain cliff. Both environments were tested and showed the same trend in the results. However, the results obtained in mountain cliff were more pronounced and more suitable for demonstration purposes.\nBecause the state space is continuous, we approximated q\u03c0 using tile coding function approximation. Specifically, we used version 3 of Sutton\u2019s tile coding software (n.d.) with 8 tilings, an asymmetric offset by consecutive odd numbers, and each tile taking over 1/8 fraction of the feature space, which gives a resolution of approximately 1.6%.\nFor each algorithm, we conducted 500 independent runs of 500 episodes each. All training was done on-policy under an -greedy policy with = 0.1 and \u03b3 = 1. We optimized for the average return after 500 episodes over different values of the step size parameter, \u03b1, and the backup length, n. The results correspond to the best-performing parameter combination for each algorithm. We omit n-step Expected Sarsa in the results because its performance was not much\ndifferent from n-step Sarsa\u2019s performance.\nFigure 7 shows the return per episode averaged over 500 runs. To smooth the results, we computed a right-centered moving average with a window of 100 successive episodes. As it can be observed, atomic multi-step Sarsa and Q(0.5) had fairly similar performance. Among the atomic multistep methods with static \u03c3, Tree-backup had the best performance. Nonetheless, Q(\u03c3) with dynamic \u03c3 outperformed all the algorithms that were using static \u03c3.\nIn order to gain more insight into the nature of the results, we summarized the average return after 50 and 500 episodes in Table 1. The standard error (SE), and lower (LB) and upper (UB) 95% confidence interval bounds are provided to validate the significance of the results. All the summaries were calculated based on 500 runs.\nThe average return after only 50 episodes could be interpreted as a measure of how fast the algorithm can learn, whereas the average return after 500 episodes shows how well an algorithm is capable of learning. As evidenced in the table, Q(0.5) obtained the best performance during the first 50 episodes, while Q(\u03c3) with dynamic \u03c3 was a close second. However, after 500 episodes, Q(\u03c3) with dynamic \u03c3 managed to outperform all the other algorithms."}, {"heading": "7. Discussion", "text": "From our experiments, it is evident that there is merit in unifying the space of algorithms with Q(\u03c3). In prediction tasks, such as the 19-state random walk, varying the degree of sampling results in a trade-off between initial and asymptotic performance. In control tasks, such as the stochastic windy gridworld, intermediate degrees of sampling are capable of achieving a higher per-episode aver-\nFigure 7. Mountain cliff results. The plot shows the performance of each atomic multi-step algorithm in terms of the average return per episode. A right-centered moving average with a window of 100 successive episodes was employed in order to smooth the results. Q(\u03c3) with dynamic \u03c3 had the best performance among all the algorithms.\nage return than either extreme, depending on the number of elapsed episodes.\nThese findings also extend to tasks with continuous state spaces, such as the mountain cliff. As evidenced by the results in Table 1, intermediate values of \u03c3 allow for a higher initial performance, whereas small values of \u03c3 allow for a better asymptotic performance. As shown in Figure 7 and Table 1, Q(\u03c3) with dynamic \u03c3 is able to exploit these two benefits by adjusting \u03c3 over time.\nMoreover, our experiments in the stochastic windy gridworld task demonstrated that it is possible to improve performance by choosing a higher value of the backup length parameter, n. When comparing algorithms with a backup length greater than one, we noticed that different families algorithm experienced a different effective length. For example, with no discounting (\u03b3 = 1), n-step Sarsa\u2019s effective backup length is equal to its backup length n. However, the effective backup length of Tree-backup can be less than n, depending on the stochasticity of \u03c0; this is a direct result from the product term in (12). Thus, for some algorithms the length of the backup, n, and the effective length are not equal. We did not explore this idea any further, but it could be subject for new research."}, {"heading": "8. Conclusions", "text": "In this paper we studied Q(\u03c3), which is a unifying algorithm for multi-step TD control methods. Q(\u03c3), through the use of the sampling parameter \u03c3, allows for continuous variation between updating based on full sampling and\nTable 1. Summaries of the initial and final performance for all the atomic multi-step algorithms in the mountain cliff environment. The standard error (SE), and lower (LB) and upper (UB) 95% confidence interval bounds are provided to validate the results. Q(0.5) had the best initial performance, whereas Dynamic Q(\u03c3) had the best final performance.\nAverage Return per Episode After 50 Episodes\nAlgorithm Mean SE LB UB\nQ(1), Sarsa -447.27 1.15 -449.17 -445.37 Q(0), Tree-backup -429.15 1.77 -432.06 -426.24 Q(0.5) -397.97 1.11 -399.80 -396.14 Dynamic \u03c3 -406.34 2.01 -409.65 -403.03\nAverage Return per Episode After 500 Episodes\nAlgorithm Mean SE LB UB\nQ(1), Sarsa -173.23 0.15 -173.48 -172.98 Q(0), Tree-backup -168.35 0.22 -168.71 -167.98 Q(0.5) -167.94 0.15 -168.19 -167.69 Dynamic \u03c3 -163.65 0.24 -164.05 -163.25\nupdating based on pure expectation. Our results on prediction and control problems showed that an intermediate fixed degree of sampling can outperform the methods that exist at the extremes (Sarsa and Tree-backup). In addition, we presented simple way of dynamically adjusting \u03c3 which outperformed any fixed degree of sampling.\nOur presentation of Q(\u03c3) was limited to the atomic multistep case without eligibility traces, we only conducted experiments on on-policy problems, and we only investigated one simple method for dynamically varying \u03c3. This leaves open several avenues for future research. First, Q(\u03c3) could be extended to use eligibility traces and compound backups. Second, the performance of Q(\u03c3) could be evaluated on off-policy problems. Third, other schemes for dynamically varying \u03c3 could be investigated \u2013 perhaps as a function of state, the recently observed rewards, or some measure of the learning progress."}, {"heading": "Acknowledgements", "text": "The authors thank Vincent Zhang, Harm van Seijen, Doina Precup, and Pierre-luc Bacon for insights and discussions contributing to the results presented in this paper, and the entire Reinforcement Learning and Artificial Intelligence research group for providing the environment to nurture and support this research. We gratefully acknowledge funding from Alberta Innovates \u2013 Technology Futures, Google Deepmind, and from the Natural Sciences and Engineering Research Council of Canada."}], "references": [{"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["T. Jaakkola", "M.I. Jordan", "S.P. Singh"], "venue": "Neural Computation", "citeRegEx": "Jaakkola et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1994}, {"title": "Eligibility traces for off-policy policy evaluation", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "In Proceedings of the 17th International Conference on Machine Learning,", "citeRegEx": "Precup et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Precup et al\\.", "year": 2000}, {"title": "Problem Solving with Reinforcement Learning", "author": ["G.A. Rummery"], "venue": "PhD Thesis,", "citeRegEx": "Rummery,? \\Q1995\\E", "shortCiteRegEx": "Rummery", "year": 1995}, {"title": "On-line qlearning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical Report CUED/F-INFENG/TR 166,", "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sutton,? \\Q1996\\E", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Reinforcement Learning: An Introduction (2nd ed.). Manuscript in preparation", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q2017\\E", "shortCiteRegEx": "Sutton and Barto", "year": 2017}, {"title": "A theoretical and empirical analysis of expected sarsa", "author": ["H. van Seijen", "H. van Hasselt", "S. Whiteson", "M. Wiering"], "venue": "In Proceedings of the IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Seijen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2009}, {"title": "Learning from Delayed Rewards", "author": ["Watkins", "C.J.C. H"], "venue": null, "citeRegEx": "Watkins and H.,? \\Q1989\\E", "shortCiteRegEx": "Watkins and H.", "year": 1989}], "referenceMentions": [{"referenceID": 4, "context": "Temporal-difference (TD) methods (Sutton, 1988) are an important concept in reinforcement learning (RL) which combines ideas from Monte Carlo and dynamic programming methods.", "startOffset": 33, "endOffset": 47}, {"referenceID": 5, "context": "Sarsa (Rummery & Niranjan, 1994; Sutton, 1996) is the classical on-policy control method, where the behaviour and target policies are the same.", "startOffset": 6, "endOffset": 46}, {"referenceID": 4, "context": "The TD(\u03bb) algorithm unifies one-step TD learning with Monte Carlo methods (Sutton, 1988).", "startOffset": 74, "endOffset": 88}, {"referenceID": 2, "context": "The concept of eligibility traces can also be applied to TD control methods such as Sarsa and Q-learning, which can create more efficient learning and produce better performance (Rummery, 1995).", "startOffset": 178, "endOffset": 193}, {"referenceID": 1, "context": "The Tree-backup algorithm was originally presented as a method to perform off-policy evaluation when the behaviour policy is non-Markov, non-stationary or completely unknown (Precup et al., 2000).", "startOffset": 174, "endOffset": 195}, {"referenceID": 4, "context": "Q(\u03c3) is an algorithm that was first proposed by Sutton and Barto (2017) which unifies and generalizes the existing multi-step TD control methods.", "startOffset": 48, "endOffset": 72}, {"referenceID": 1, "context": "n-step Sarsa can be adapted for off-policy learning by introducing an importance sampling ratio term (Precup et al., 2000):", "startOffset": 101, "endOffset": 122}, {"referenceID": 1, "context": "A drawback to using importance sampling to learn offpolicy is that it can create high variance which must be compensated for by using small step sizes; this can slow learning (Precup et al., 2000).", "startOffset": 175, "endOffset": 196}, {"referenceID": 1, "context": "However, it is possible to extend this idea to every time step of the backup by taking an expectation at every step (Precup et al., 2000).", "startOffset": 116, "endOffset": 137}, {"referenceID": 1, "context": "Additionally, because an importance sampling ratio does not need to be computed, the behavior policy does not need to be stationary, Markov, or even known (Precup et al., 2000).", "startOffset": 155, "endOffset": 176}, {"referenceID": 4, "context": "This atomic version of multi-step Tree-backup was first presented by Sutton and Barto (2017).", "startOffset": 69, "endOffset": 93}, {"referenceID": 1, "context": "The possibility of unifying Sarsa and Tree-backup was first suggested by Precup et al. (2000), and the first formulation of Q(\u03c3) was presented by Sutton and Barto (2017).", "startOffset": 73, "endOffset": 94}, {"referenceID": 1, "context": "The possibility of unifying Sarsa and Tree-backup was first suggested by Precup et al. (2000), and the first formulation of Q(\u03c3) was presented by Sutton and Barto (2017).", "startOffset": 73, "endOffset": 170}, {"referenceID": 4, "context": "For an example of the psuedocode for general off-policy Q(\u03c3), see Sutton and Barto (2017). Algorithm 1 On-policy n-step Q(\u03c3) for estimating q\u03c0 Initialize S0 6= terminal; select A0 according to \u03c0(.", "startOffset": 66, "endOffset": 90}, {"referenceID": 4, "context": "The windy gridworld as described by Sutton and Barto (1998). The start and goal states are denoted by S and G respectively.", "startOffset": 36, "endOffset": 60}, {"referenceID": 4, "context": "We implemented a variant of the classical episodic task, mountain car, as described by Sutton and Barto (1998). For this implementation, the rewards, actions and goal remained the same.", "startOffset": 87, "endOffset": 111}], "year": 2017, "abstractText": "Unifying seemingly disparate algorithmic ideas to produce better performing algorithms has been a longstanding goal in reinforcement learning. As a primary example, TD(\u03bb) elegantly unifies one-step TD prediction with Monte Carlo methods through the use of eligibility traces and the trace-decay parameter \u03bb. Currently, there are a multitude of algorithms that can be used to perform TD control, including Sarsa, Q-learning, and Expected Sarsa. These methods are often studied in the one-step case, but they can be extended across multiple time steps to achieve better performance. Each of these algorithms is seemingly distinct, and no one dominates the others for all problems. In this paper, we study a new multi-step action-value algorithm called Q(\u03c3) which unifies and generalizes these existing algorithms, while subsuming them as special cases. A new parameter, \u03c3, is introduced to allow the degree of sampling performed by the algorithm at each step during its backup to be continuously varied, with Sarsa existing at one extreme (full sampling), and Expected Sarsa existing at the other (pure expectation). Q(\u03c3) is generally applicable to both onand off-policy learning, but in this work we focus on experiments in the on-policy case. Our results show that an intermediate value of \u03c3, which results in a mixture of the existing algorithms, performs better than either extreme. The mixture can also be varied dynamically which can result in even greater performance. 1. The Landscape of TD Algorithms Temporal-difference (TD) methods (Sutton, 1988) are an important concept in reinforcement learning (RL) which combines ideas from Monte Carlo and dynamic programming methods. TD methods allow learning to occur directly from raw experience in the absence of a model of the environment\u2019s dynamics, like with Monte Carlo methods, while also allowing estimates to be updated based on other learned estimates without waiting for a final result, like with dynamic programming. The core concepts of TD methods provide a flexible framework for creating a variety of powerful algorithms that can be used for both prediction and control. There are a number of TD control methods that have been proposed. Q-learning (Watkins, 1989) is arguably the most popular, and is considered an off-policy method because the policy generating the behaviour (the behaviour policy), and the policy that is being learned (the target policy) are different. Sarsa (Rummery & Niranjan, 1994; Sutton, 1996) is the classical on-policy control method, where the behaviour and target policies are the same. However, Sarsa can be extended to learn off-policy with the use of importance sampling (Precup, Sutton, & Singh, 2000). Expected Sarsa is an extension of Sarsa that, instead of using the action-value of the next state to update the value of the current state, uses the expectation of all the subsequent action-values of the current state with respect to the target policy. Expected Sarsa has been studied as a strictly on-policy method (van Seijen, van Hasselt, Whiteson, & Wiering, 2009), but in this paper we present a more general version that can be used for both onand off-policy learning and that also subsumes Q-learning. All of these methods are often described in the simple one-step case, but they can also be extended across multiple time steps. The TD(\u03bb) algorithm unifies one-step TD learning with Monte Carlo methods (Sutton, 1988). Through the use of eligibility traces, and the trace-decay parameter, \u03bb \u2208 [0, 1], ar X iv :1 70 3. 01 32 7v 1 [ cs .A I] 3 M ar 2 01 7 Multi-step Reinforcement Learning: A Unifying Algorithm a spectrum of algorithms is created. At one end, \u03bb = 1, exists Monte Carlo methods, and at the other, \u03bb = 0, exists one-step TD learning. In the middle of the spectrum are intermediate methods which can perform better than the methods at either extreme (Sutton & Barto, 1998). The concept of eligibility traces can also be applied to TD control methods such as Sarsa and Q-learning, which can create more efficient learning and produce better performance (Rummery, 1995). Multi-step TD methods are usually thought of in terms of an average of many multi-step returns of differing lengths and are often associated with eligibility traces, as is the case with TD(\u03bb). However, it is also natural to think of them in terms of individual n-step returns with their associated nstep backup (Sutton & Barto, 1998). We refer to each of these individual backups as atomic backups, whereas the combination of several atomic backups of different lengths creates a compound backup. In the existing literature, it is not clear how best to extend one-step Expected Sarsa to a multi-step algorithm. The Tree-backup algorithm was originally presented as a method to perform off-policy evaluation when the behaviour policy is non-Markov, non-stationary or completely unknown (Precup et al., 2000). In this paper, we re-present Tree-backup as a natural multi-step extension of Expected Sarsa. Instead of performing the updates with entirely sampled transitions as with multi-step Sarsa, Treebackup performs the update using the expected values of all the actions at each transition. Q(\u03c3) is an algorithm that was first proposed by Sutton and Barto (2017) which unifies and generalizes the existing multi-step TD control methods. The degree of sampling performed by the algorithm is controlled by the sampling parameter, \u03c3. At one extreme (\u03c3 = 1) exists Sarsa (full sampling), and at the other (\u03c3 = 0) exists Tree-backup (pure expectation). Intermediate values of \u03c3 create algorithms with a mixture of sampling and expectation. In this work we show that an intermediate value of \u03c3 can outperform the algorithms that exist at either extreme. In addition, we show that \u03c3 can be varied dynamically to produce even greater performance. We limit our discussion of Q(\u03c3) to the atomic multi-step case without eligibility traces, but a natural extension is to make use of compound backups and is an avenue for future research. Furthermore, Q(\u03c3) is generally applicable to both onand off-policy learning, but for our initial empirical study we examined only on-policy prediction and control problems. 2. MDPs and One-step Solution Methods The sequential decision problem encountered in RL is often modeled as a Markov decision process (MDP). Under this framework, an agent and the environment interact over a sequence of discrete time steps t. At every time step, the agent receives information about the environment\u2019s state, St \u2208 S , where S is the set of all possible states. The agent uses this information to select an action, At, from the set of all possible actions A. Based on the behavior of the agent and the state of the environment, the agent receives a reward, Rt+1 \u2208 R \u2282 R, and moves to another state, St+1 \u2208 S, with a state-transition probability p(s\u2032|s, a) = P (St+1 = s |St = s,At = a), for a \u2208 A and s, s\u2032 \u2208 S. The agent behaves according to a policy \u03c0(a|s), which is a probability distribution over the set S \u00d7 A. Through the process of policy iteration (Sutton & Barto, 1998), the agent learns the optimal policy, \u03c0\u2217, that maximizes the expected discounted return: Gt = Rt+1+\u03b3Rt+2+\u03b3 Rt+3+... = T\u22121 \u2211", "creator": "LaTeX with hyperref package"}}}