{"id": "1703.01789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "Sample-level Deep Convolutional Neural Networks for Music Auto-tagging Using Raw Waveforms", "abstract": "Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolutional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. This allows the networks to hierarchically learn filters that are sensitive to log-scaled frequency, such as mel-frequency spectrogram that is widely used in music classification systems. It also helps learning high-level abstraction of music by increasing the depth of layers. We show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results that are com- parable to previous state-of-the-art performances for the Magnatagatune dataset and Million song dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features.", "histories": [["v1", "Mon, 6 Mar 2017 09:49:48 GMT  (3001kb,D)", "http://arxiv.org/abs/1703.01789v1", "7 pages, Sound and Music Computing Conference, 2017 submitted"], ["v2", "Mon, 22 May 2017 04:46:36 GMT  (6859kb,D)", "http://arxiv.org/abs/1703.01789v2", "7 pages, Sound and Music Computing Conference (SMC), 2017"]], "COMMENTS": "7 pages, Sound and Music Computing Conference, 2017 submitted", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.MM cs.NE", "authors": ["jongpil lee", "jiyoung park", "keunhyoung luke kim", "juhan nam"], "accepted": false, "id": "1703.01789"}, "pdf": {"name": "1703.01789.pdf", "metadata": {"source": "META", "title": "SAMPLE-LEVEL DEEP CONVOLUTIONAL NEURAL NETWORKS FOR MUSIC AUTO-TAGGING USING RAW WAVEFORMS", "authors": ["Jongpil Lee", "Jiyoung Park Keunhyoung", "Luke Kim", "Juhan Nam"], "emails": ["juhannam]@kaist.ac.kr"], "sections": [{"heading": "1. INTRODUCTION", "text": "In music information retrieval (MIR) tasks, raw waveforms of music signals are generally converted to a time-frequency representation and used as input to the system. The majority of MIR systems use a log-scaled representation in frequency such as mel-spectrograms and constant-Q transforms and then compress the amplitude with a log scale. The time-frequency representations are often transformed further into more compact forms of audio features depending on the task. All of these processes are designed based on acoustic knowledge or engineering efforts.\nRecent advances in deep learning, especially the development of deep convolutional neural networks (DCNN), made it possible to learn the entire hierarchical representations from the raw input data, thereby minimizing the input data processing by hands. This end-to-end hierarchical learning was attempted early in the image domain, particularly since the DCNN achieves break-through results in image classification [1]. These days, the method of stacking small filters (e.g. 3x3) is widely used after it has\nCopyright: c\u00a9 2017 Jongpil Lee et al. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution 3.0 Unported License, which\npermits unrestricted use, distribution, and reproduction in any medium, provided\nthe original author and source are credited.\nbeen found to be effective in learning more complex hierarchical filters while conserving receptive fields [2]. In the text domain, the language model typically consists of two steps: word embedding and word-level learning. While word embedding plays a very important role in language processing [3], it has limitations in that it is learned independently from the system. Recent work using CNNs that take character-level text as input showed that the end-toend learning approach can yield comparable results to the word-level learning system [4, 5]. In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310]. They reported that the performance can be similar to or even superior to that of the models using spectral-based features as input.\nThis end-to-end learning approach has been applied to music classification tasks as well [11, 12]. In particular, Dieleman and Schrauwen used raw waveforms as input of CNN models for music auto-tagging task and attempted to achieve comparable results to those using mel-spectrograms as input [11]. Unfortunately, they failed to do so and attributed the result to three reasons. First, their CNN models were not sufficiently expressive (e.g. a small number of layers and filters) to learn the complex structure of polyphonic music. Second, they could not find an appropriate non-linearity function that can replace the log-based amplitude compression in mel-spectrograms. Third, the bottom layer in the networks takes raw waveforms in frame-level which are typically several hundred samples long. The filters in the bottom layer should learn all possible phase variations of periodic waveforms which are likely to be prevalent in musical signals. The phase variations within a frame (i.e. time shift of periodic waveforms) are actually removed in mel-spectrograms.\nIn this paper, we address these issues with sample-level DCNN. What we mean by \u201csample-level\u201d is that the filter size in the bottom layer may go down to several samples long. We assume that this small granularity is analogous to pixel-level in image or character-level in text. We show the effectiveness of the sample-level DCNN in music auto-tagging task by decreasing strides of the first convolutional layer from frame-level to sample-level and accordingly increasing the depth of layers. Our experiments show that the depth of architecture with samplelevel filters is proportional to the accuracy and also the architecture achieves results comparable to previous stateof-the-art performances for the Magnatagatune dataset and the Million song dataset. In addition, we visualize filters learned in the sample-level DCNN.\nar X\niv :1\n70 3.\n01 78\n9v 1\n[ cs\n.S D\n] 6\nM ar\n2 01\n7"}, {"heading": "2. RELATED WORK", "text": "Since the waveform is one-dimensional data, previous work use a CNN that consists of one-dimensional convolution and pooling stages. While the convolution operation and filter length in upper layers are usually similar to those used in the image domain, the bottom layer that takes waveform directly conducted a special operation called strided convolution, which takes a large filter length and strides it as much as the filter length (or the half). This frame-level approach is comparable to sliding windows with 100% or 50% hop size in short-time Fourier transform. In many of previous work, the stride and filter length of the first convolution layer was set to 10-20 ms (160-320 samples at 16 kHz audio) [8, 10\u201312].\nIn this paper, we reduce the filter length and stride of the first convolution layer to sample-level, which can be as small as 2 samples. Accordingly, we increase the depth of layers in the CNN model. There are some works that use 0.6 ms (10 samples at 16 kHz audio) as a stride length [6, 7], but they used a CNN model only with three convolution layers, which is not sufficient to learn the complex structure of musical signals."}, {"heading": "3. LEARNING MODELS", "text": "Figure 1 illustrates three CNN-based models in the music auto-tagging task we compare in our experiments. In this section, we describe the three models in detail."}, {"heading": "3.1 Frame-level mel-spectrogram based model", "text": "This is the most common CNN model used in music autotagging. Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315]. We only used one-dimensional(1D) CNN model for experimental comparisons in our work because the performance gap between 1D and 2D models is not significant and 1D model can be directly compared to models using raw waveforms."}, {"heading": "3.2 Frame-level raw waveform based model", "text": "In the frame-level raw waveform-based model, a strided convolution layer is added beneath the bottom layer of the frame-level mel-spectrogram based model. The strided convolution layer learns a filter-bank represention that correspond to filter kernels in a time-frequency representation. In this model, once the raw waveforms pass through the first strided convolution layer, the output feature map has the same dimensions as the mel-spectrogram. This is because the stride, filter length, and number of filters of the first convolution layer correspond to the hop size, window size, and number of mel-bands in the mel-spectrogram, respectively. This configuration was used for music autotagging task in [11, 12] and so we used it as a baseline model."}, {"heading": "3.3 Sample-level raw waveform based model", "text": "As described in Section 1, the approach using the raw waveforms should be able to address log-scale amplitude compression and phase-invariance. Simple adding a strided convolution layer is not sufficient to overcome the problems. To improve this, we add multiple layers beneath the frame-level layer such that the first convolution layer can handle much smaller length of samples in filter length. For example, if the stride of the first convolution layer is reduced from 729 to 243, 3-size convolution layer and maxpooling layer are added to keep the output dimensions in the subsequent convolution layers equal. If we go down deep into sample-level along this framework, Six convolution layers (five pairs of 3-size convolution and maxpooling layer following one 3-size strided convolution layer). This is because the temporal dimensionality reduction occurs only through max-pooling and striding when zeropadding is applied on convolution layer to conserve input and output dimensions. We describe the configuration strategy of sample-level CNN model in the following section."}, {"heading": "3.4 Model Design", "text": "Since the length of an audio clip is variable in general, the following issues should be considered when configuring the temporal CNN architecture:\n\u2022 Convolution filter length and sub-sampling length.\n\u2022 The remaining temporal dimension after the last subsampling layer.\n\u2022 The segment length of audio corresponding to the input size of the network.\nFirst, we attempted a very small (sample-level) filter length in convolutional layers by referring to the VGG net [2]. However, unlike images with spatial dimensions such as 224 \u00d7 224, audio files contains, for example, 22050 samples per second. Total input size may be similar with images if we use 2.3 seconds of audio as input to the network (224 \u00d7 224 ' 22050 \u00d7 2.3). However, since we use one-dimentional convolution and sub-sampling for raw waveforms, the filter length and pooling length need to be varied. Therefore, we constructed several different DCNN models to verify the effects on music auto-tagging performance. As a sub-sampling method, max-pooling is generally used. Although sub-sampling using strided convolution has recently been proposed in generative model [9], our preliminary test showed that max-pooling was superior to the stride-style sub-sampling method. We assume that this result is due to the high density characteristics of music data and translation invariance characteristics, which is the advantage of using max-pooling in classification tasks. Thus, we used max-pooling as the basic sub-sampling method. In addition, to test the model performance with minimum layer setting, a pair of single convolution layer and maxpooling layer that share the same filter length and pooling length was defined as a basic building module of the DCNN.\nSecond, the remaining time dimension after the last subsampling layer represents the temporal compression ratio of the entire input audio. We set this value to one in all models in order to compare the performance according to the depth of layers and the stride of first convolution layer. The dimension of the fully connected layer that follows after the last sub-sampling layer is closely related to this value. By shortening this value to one, the parameter can be stored and the fully connected layer can be replaced by a convolution layer with filter length of one.\nThird, in music classification tasks, the input size of the network is an important parameter that determine the classification performance. In the mel-spectrogram based model, one song is generally segmented into 1-4 seconds [11], and then the predictions of all the segments in one song are averaged to make a song-level prediction. In the raw waveform based model, the learning ability according to the segmentation has been not sufficiently explored. Therefore, we conducted experiments to measure the effect of segmentation size on the raw waveform based model. The result shows that segmentation of 1-4 seconds also worked best in raw waveform based model and thus we followed this as our segmentation setting.\nConsidering these issues, we construct mn-DCNN models with different input sizes where m refers to the filter length and pooling length of intermediate convolution layer module, and n refers to the number of the modules (or depth), and evaluate several different values of m. An"}, {"heading": "39 model, 19683 frames", "text": ""}, {"heading": "59049 samples (2678 ms) as input", "text": "example of mn-DCNN models is shown in Table 1 where m is 3 and n is 9. According to the definition, the filter length and pooling length of the convolution layer are 3 other than the first strided convolution layer. If the hop size (stride length) of the first strided convolution layer is 3, the time-wise output dimension of the convolution layer becomes 19683 when the input of the network is 59049 samples. We call this \u201c39 model with 19683 frames and 59049 samples as input\u201d."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "In the section, we introduce the datasets used in our experiments and describe detailed experimental settings."}, {"heading": "4.1 Datasets", "text": "We evaluate the proposed model on two datasets, Magnatagatune dataset (MTT) [16] and Million song dataset (MSD) annotated with the Last.FM tags [17]. We primarily examined the proposed model on MTT and then veri-\nfied the effectiveness of our model on MSD which is much larger than MTT (MTT contains 170 hours long audio, and MSD contains 1955 hours long audio in total). We filtered out the tags and used most frequently labeled 50 tags in both datasets, following the previous work [11], [14,15] 1 . Also, all songs in the two datasets were trimmed to 29.1 second long. We used AUC (Area Under Receiver Operating Characteristic) as a primary evaluation metric for music auto-tagging.\n1 https://github.com/keunwoochoi/MSD_split_for_ tagging"}, {"heading": "4.2 Optimization", "text": "We trained the networks with the following settings: binary cross entropy loss with sigmoid activation on prediction layer is set to objectives. Batch normalization [18] and ReLU activation for every convolution layer is used. We should note that, in our experiments, batch normalization plays a vital role in raw waveform based deep learning. Dropout of 0.5 was applied to the output of the last convolution layer. We trained the models using stochastic gradient descent with 0.9 Nesterov momentum. The learning rate was initially set to 0.01 and decreased to a factor of 5 when the validation loss did not decrease more than 3 epochs. A total decrease of 4 times, the learning rate of the last training was 0.000016. Also, we used\nbatch size of 23 for MTT and 50 for MSD, respectively. In mel-spectrogram based model, we conducted the input normalization simply by dividing standard deviation after subtracting mean value of entire input data. On the other hand, we did not perform the input normalization on raw waveform based model."}, {"heading": "5. RESULTS", "text": "In this section, we examine proposed methods and finally compare them to previous state-of-the-art results.\n5.1 mn-DCNN models\nTable 2 shows the evaluation results for the mn-DCNN models on MTT for different input sizes, number of layers, filter length and stride of the first convolution layer. As described in Section 3.4, m refers to the filter length and pooling length of intermediate convolution layer module, and n refers to the number of the modules. In Table 2, we can first find that the accuracy is proportional to n in most m. Increasing n in our model with the same m\nvalue and input size means that the filter length and stride of the first convolution layer go down to sample-level (e.g. 2 or 3 size). When the first layer\u2019s filter length and pooling length reach the sample-level, the sample-level architectures are simply seen as models constructed with the same filter length and sub-sampling length in all convolution layers as depicted in Table 1. The best results were obtained when m was 3 and n was 9. Interestingly, the length of 3 corresponds to the 3-size spatial filters in the VGG net [2]. In addition, we can see that 1-3 seconds of audio as input length to the network is also a reasonable choice in raw waveform based model as in mel-spectrogram based model. As a result, we find that deep models (more than 10 layers) with 1-3s audio as input having a very small sample-level filter length and sub-sampling length is very effective at learning raw waveforms in the music auto-tagging task."}, {"heading": "5.2 Mel-spectrogram and raw waveforms", "text": "Based on the fact that the output size of the first convolution layer of the model that uses the raw waveform is equivalent to the mel-spectrogram size that shares filter length (window), stride (hop) and the number of filters (the number of mel-bands), we further validate the effectiveness of the proposed sample-level architecture by performing experiments presented in Table 3. The models used in the experiments follows our model configuration strategy described in Section 3.4. We added a convolution layer and a pooling layer module at the top module of the 3n\u22121 model to compress all time dimensions to 1 in the 3n model. In the mel-spectrogram experiments, 128 mel-bands are used to match the number of filters of first convolution layer of raw waveform based model. FFT size was set to 729 in all comparisons and magnitude compression is applied with a nonlinear curve, log(1+C|A|) where A is the magnitude and C is set to 10.\nThe results in Table 3 show that sample-level raw waveform based model achieves results comparable to the framelevel mel-spectrogram based model. We also found that using a smaller hop size (4 ms) than conventional approaches using hop size of 20 ms or so increased the performances significantly. However, if the hop size is less than 4 ms, the performance degrades. An interesting finding from the result of frame-level raw waveform based model is that when the filter length is larger than the stride, the accuracy was slightly lower than the models sharing filter length and stride. We interpret that this phenomenon is due to the learning ability of phase variances. As the filter length decreases, the extent that phase variance that the filter should learn is reduced."}, {"heading": "5.3 MSD result and the number of filters", "text": "We investigate the capacity of our sample-level architecture even further by evaluating the performance on MSD that is ten times larger than MTT. The result is shown in Table 4. While training the network on MSD, the number of filters in the convolution layers have been shown to affect performance. According to our preliminary test results, increasing the number of filters from 16 to 512 along\nthe layers was sufficient for MTT. However, the test on MSD shows that increasing the number of filters in the first convolution layer improves the performance. Therefore, we increased the number of filters in the first convolution layer from 16 to 128."}, {"heading": "5.4 Comparison to state-of-the-arts", "text": "In Table 4, we show the performance of the proposed architecture to previous state-of-the-arts on MTT and MSD. They show that our proposed sample-level architecture is highly effective compared to them."}, {"heading": "5.5 Visualization of learned filters", "text": "The technique of visualizing the filters learned at each layer allows better understanding of representation learning in the hierarchical network. However, previous works in music domain are limited to visualizing learned filters only on the first convolution layer [11, 12]. Especially the gradient ascent method has been proposed [20] for filter visualization and this technology has provided deeper understanding of what convolutional neural networks learn from images [21,22]. Thus, we applied the gradient ascent method to see how each layer of the proposed network hears the raw waveforms. The gradient ascent method is as follows. First, we generate random noise and back-propagate to the trained network. The loss is set to the target filter. Then, we add the bottom gradients to the input with gradient normalization. By repeating this process several times, we can obtain the waveforms that maximizes the target filter at the input. With the advantage that any dimension can be an input as long as it meets the sub-sampling dimension of the convolution layer because our sample-level DCNN consists solely of a single convolution layer, we could visualize the sorted spectrum in Figure 2 or the learned filters in Figure 3 more clearly using noise of 729 samples as input. The layer 1 shows the three distinctive filter bands which is possible with the filter length with 3 samples. The center frequency of the filter banks increases linearly in low fre-\nquency filter banks but it becomes non-linearly steeper in high frequency filter banks. This trend becomes stronger as the layer goes up. This nonlinearity was found in learned filters with a frame-level end-to-end learning [11] and also in perceptual pitch scales such as mel or bark."}, {"heading": "6. CONCLUSION AND FUTURE WORK", "text": "In this paper, we proposed sample-level DCNN models that take raw waveforms as input. Through our experiments, we showed that the deep architectures can improve the performance in music auto-tagging and they provide results that are comparable to previous state-of-the-art performances for the two datasets using raw waveforms as input. We also effectively visualized hierarchically learned filters. Future studies will analyze the learned filters more thoroughly by applying several visualization techniques. Furthermore, we can explore music style transfer at music or instrument level."}, {"heading": "7. REFERENCES", "text": "[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classification with deep convolutional neural networks,\u201d in Advances in neural information processing systems, 2012, pp. 1097\u20131105.\n[2] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks for large-scale image recognition,\u201d arXiv preprint arXiv:1409.1556, 2014.\n[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \u201cDistributed representations of words and phrases and their compositionality,\u201d in Advances in neural information processing systems, 2013, pp. 3111\u20133119.\n[4] X. Zhang, J. Zhao, and Y. LeCun, \u201cCharacter-level convolutional networks for text classification,\u201d in Advances in neural information processing systems, 2015, pp. 649\u2013657.\n[5] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, \u201cCharacter-aware neural language models,\u201d arXiv preprint arXiv:1508.06615, 2015.\n[6] D. Palaz, M. M. Doss, and R. Collobert, \u201cConvolutional neural networks-based continuous speech recognition using raw speech signal,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4295\u20134299.\n[7] D. Palaz, R. Collobert et al., \u201cAnalysis of cnn-based speech recognition system using raw speech as input,\u201d Idiap, Tech. Rep., 2015.\n[8] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2letter: an end-to-end convnet-based speech recognition system,\u201d arXiv preprint arXiv:1609.03193, 2016.\n[9] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d CoRR abs/1609.03499, 2016.\n[10] T. N. Sainath, R. J. Weiss, A. W. Senior, K. W. Wilson, and O. Vinyals, \u201cLearning the speech front-end with raw waveform cldnns.\u201d in INTERSPEECH, 2015, pp. 1\u20135.\n[11] S. Dieleman and B. Schrauwen, \u201cEnd-to-end learning for music audio,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.\n[12] D. Ardila, C. Resnick, A. Roberts, and D. Eck, \u201cAudio deepdream: Optimizing raw audio with convolutional networks.\u201d\n[13] J. Pons, T. Lidy, and X. Serra, \u201cExperimenting with musically motivated convolutional neural networks,\u201d in Content-Based Multimedia Indexing (CBMI), 2016 14th International Workshop on. IEEE, 2016, pp. 1\u2013 6.\n[14] K. Choi, G. Fazekas, and M. Sandler, \u201cAutomatic tagging using deep convolutional neural networks,\u201d in Proceedings of the 17th International Conference on Music Information Retrieval (ISMIR), 2016, pp. 805\u2013 811.\n[15] K. Choi, G. Fazekas, M. Sandler, and K. Cho, \u201cConvolutional recurrent neural networks for music classification,\u201d arXiv preprint arXiv:1609.04243, 2016.\n[16] E. Law, K. West, M. I. Mandel, M. Bay, and J. S. Downie, \u201cEvaluation of algorithms using games: The case of music tagging,\u201d in ISMIR, 2009, pp. 387\u2013392.\n[17] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere, \u201cThe million song dataset,\u201d in Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR), vol. 2, no. 9, 2011, pp. 591\u2013 596.\n[18] S. Ioffe and C. Szegedy, \u201cBatch normalization: Accelerating deep network training by reducing internal covariate shift,\u201d arXiv preprint arXiv:1502.03167, 2015.\n[19] J.-Y. Liu, S.-K. Jeng, and Y.-H. Yang, \u201cApplying topological persistence in convolutional neural network for music audio signals,\u201d arXiv preprint arXiv:1608.07373, 2016.\n[20] D. Erhan, Y. Bengio, A. Courville, and P. Vincent, \u201cVisualizing higher-layer features of a deep network,\u201d University of Montreal, vol. 1341, p. 3, 2009.\n[21] M. D. Zeiler and R. Fergus, \u201cVisualizing and understanding convolutional networks,\u201d in European conference on computer vision. Springer, 2014, pp. 818\u2013 833.\n[22] A. Nguyen, J. Yosinski, and J. Clune, \u201cDeep neural networks are easily fooled: High confidence predictions for unrecognizable images,\u201d in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 427\u2013436."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, 2013, pp. 3111\u20133119.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "Advances in neural information processing systems, 2015, pp. 649\u2013657.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "arXiv preprint arXiv:1508.06615, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks-based continuous speech recognition using raw speech signal", "author": ["D. Palaz", "M.M. Doss", "R. Collobert"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4295\u20134299.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input", "author": ["D. Palaz", "R. Collobert"], "venue": "Idiap, Tech. Rep., 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Wav2letter: an end-to-end convnet-based speech recognition system", "author": ["R. Collobert", "C. Puhrsch", "G. Synnaeve"], "venue": "arXiv preprint arXiv:1609.03193, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "CoRR abs/1609.03499, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Experimenting with musically motivated convolutional neural networks", "author": ["J. Pons", "T. Lidy", "X. Serra"], "venue": "Content-Based Multimedia Indexing (CBMI), 2016 14th International Workshop on. IEEE, 2016, pp. 1\u2013 6.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["K. Choi", "G. Fazekas", "M. Sandler"], "venue": "Proceedings of the 17th International Conference on Music Information Retrieval (ISMIR), 2016, pp. 805\u2013 811.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional recurrent neural networks for music classification", "author": ["K. Choi", "G. Fazekas", "M. Sandler", "K. Cho"], "venue": "arXiv preprint arXiv:1609.04243, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Evaluation of algorithms using games: The case of music tagging", "author": ["E. Law", "K. West", "M.I. Mandel", "M. Bay", "J.S. Downie"], "venue": "ISMIR, 2009, pp. 387\u2013392.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "The million song dataset", "author": ["T. Bertin-Mahieux", "D.P. Ellis", "B. Whitman", "P. Lamere"], "venue": "Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR), vol. 2, no. 9, 2011, pp. 591\u2013 596.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Applying topological persistence in convolutional neural network for music audio signals", "author": ["J.-Y. Liu", "S.-K. Jeng", "Y.-H. Yang"], "venue": "arXiv preprint arXiv:1608.07373, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing higher-layer features of a deep network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "University of Montreal, vol. 1341, p. 3, 2009.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European conference on computer vision. Springer, 2014, pp. 818\u2013 833.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 427\u2013436.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "This end-to-end hierarchical learning was attempted early in the image domain, particularly since the DCNN achieves break-through results in image classification [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 1, "context": "been found to be effective in learning more complex hierarchical filters while conserving receptive fields [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "While word embedding plays a very important role in language processing [3], it has limitations in that it is learned independently from the system.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Recent work using CNNs that take character-level text as input showed that the end-toend learning approach can yield comparable results to the word-level learning system [4, 5].", "startOffset": 170, "endOffset": 176}, {"referenceID": 4, "context": "Recent work using CNNs that take character-level text as input showed that the end-toend learning approach can yield comparable results to the word-level learning system [4, 5].", "startOffset": 170, "endOffset": 176}, {"referenceID": 5, "context": "In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310].", "startOffset": 111, "endOffset": 117}, {"referenceID": 6, "context": "In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310].", "startOffset": 111, "endOffset": 117}, {"referenceID": 7, "context": "In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310].", "startOffset": 111, "endOffset": 117}, {"referenceID": 8, "context": "In the audio domain, learning from raw audio has been explored mainly in the automatic speech recognition task [6\u201310].", "startOffset": 111, "endOffset": 117}, {"referenceID": 9, "context": "This end-to-end learning approach has been applied to music classification tasks as well [11, 12].", "startOffset": 89, "endOffset": 97}, {"referenceID": 9, "context": "In particular, Dieleman and Schrauwen used raw waveforms as input of CNN models for music auto-tagging task and attempted to achieve comparable results to those using mel-spectrograms as input [11].", "startOffset": 193, "endOffset": 197}, {"referenceID": 7, "context": "In many of previous work, the stride and filter length of the first convolution layer was set to 10-20 ms (160-320 samples at 16 kHz audio) [8, 10\u201312].", "startOffset": 140, "endOffset": 150}, {"referenceID": 9, "context": "In many of previous work, the stride and filter length of the first convolution layer was set to 10-20 ms (160-320 samples at 16 kHz audio) [8, 10\u201312].", "startOffset": 140, "endOffset": 150}, {"referenceID": 5, "context": "6 ms (10 samples at 16 kHz audio) as a stride length [6, 7], but they used a CNN model only with three convolution layers, which is not sufficient to learn the complex structure of musical signals.", "startOffset": 53, "endOffset": 59}, {"referenceID": 6, "context": "6 ms (10 samples at 16 kHz audio) as a stride length [6, 7], but they used a CNN model only with three convolution layers, which is not sufficient to learn the complex structure of musical signals.", "startOffset": 53, "endOffset": 59}, {"referenceID": 9, "context": "Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315].", "startOffset": 162, "endOffset": 172}, {"referenceID": 10, "context": "Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315].", "startOffset": 162, "endOffset": 172}, {"referenceID": 11, "context": "Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315].", "startOffset": 162, "endOffset": 172}, {"referenceID": 12, "context": "Since the time-frequency representation is two dimensional data, previous work regarded it as either twodimensional images or one-dimensional sequence of vectors [11,13\u201315].", "startOffset": 162, "endOffset": 172}, {"referenceID": 9, "context": "This configuration was used for music autotagging task in [11, 12] and so we used it as a baseline model.", "startOffset": 58, "endOffset": 66}, {"referenceID": 1, "context": "First, we attempted a very small (sample-level) filter length in convolutional layers by referring to the VGG net [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "Although sub-sampling using strided convolution has recently been proposed in generative model [9], our preliminary test showed that max-pooling was superior to the stride-style sub-sampling method.", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "In the mel-spectrogram based model, one song is generally segmented into 1-4 seconds [11], and then the predictions of all the segments in one song are averaged to make a song-level prediction.", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "We evaluate the proposed model on two datasets, Magnatagatune dataset (MTT) [16] and Million song dataset (MSD) annotated with the Last.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "FM tags [17].", "startOffset": 8, "endOffset": 12}, {"referenceID": 9, "context": "We filtered out the tags and used most frequently labeled 50 tags in both datasets, following the previous work [11], [14,15] 1 .", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "We filtered out the tags and used most frequently labeled 50 tags in both datasets, following the previous work [11], [14,15] 1 .", "startOffset": 118, "endOffset": 125}, {"referenceID": 12, "context": "We filtered out the tags and used most frequently labeled 50 tags in both datasets, following the previous work [11], [14,15] 1 .", "startOffset": 118, "endOffset": 125}, {"referenceID": 15, "context": "Batch normalization [18] and ReLU activation for every convolution layer is used.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Frame-level (mel-spectrogram) Persistent CNN [19] 0.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "9013 2D CNN [14] 0.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "851 CRNN [15] - 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "Frame-level (raw waveforms) 1D CNN [11] 0.", "startOffset": 35, "endOffset": 39}, {"referenceID": 1, "context": "Interestingly, the length of 3 corresponds to the 3-size spatial filters in the VGG net [2].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "However, previous works in music domain are limited to visualizing learned filters only on the first convolution layer [11, 12].", "startOffset": 119, "endOffset": 127}, {"referenceID": 17, "context": "Especially the gradient ascent method has been proposed [20] for filter visualization and this technology has provided deeper understanding of what convolutional neural networks learn from images [21,22].", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "Especially the gradient ascent method has been proposed [20] for filter visualization and this technology has provided deeper understanding of what convolutional neural networks learn from images [21,22].", "startOffset": 196, "endOffset": 203}, {"referenceID": 19, "context": "Especially the gradient ascent method has been proposed [20] for filter visualization and this technology has provided deeper understanding of what convolutional neural networks learn from images [21,22].", "startOffset": 196, "endOffset": 203}, {"referenceID": 9, "context": "This nonlinearity was found in learned filters with a frame-level end-to-end learning [11] and also in perceptual pitch scales such as mel or bark.", "startOffset": 86, "endOffset": 90}], "year": 2017, "abstractText": "Recently, the end-to-end approach that learns hierarchical representations from raw data using deep convolutional neural networks has been successfully explored in the image, text and speech domains. This approach was applied to musical signals as well but has been not fully explored yet. To this end, we propose sample-level deep convolutional neural networks which learn representations from very small grains of waveforms (e.g. 2 or 3 samples) beyond typical frame-level input representations. This allows the networks to hierarchically learn filters that are sensitive to log-scaled frequency, such as mel-frequency spectrogram that is widely used in music classification systems. It also helps learning high-level abstraction of music by increasing the depth of layers. We show how deep architectures with sample-level filters improve the accuracy in music auto-tagging and they provide results that are comparable to previous state-of-the-art performances for the Magnatagatune dataset and Million song dataset. In addition, we visualize filters learned in a sample-level DCNN in each layer to identify hierarchically learned features.", "creator": "LaTeX with hyperref package"}}}