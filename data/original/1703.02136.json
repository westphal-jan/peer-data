{"id": "1703.02136", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Mar-2017", "title": "English Conversational Telephone Speech Recognition by Humans and Machines", "abstract": "One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.", "histories": [["v1", "Mon, 6 Mar 2017 22:37:43 GMT  (106kb,D)", "http://arxiv.org/abs/1703.02136v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["george saon", "gakuto kurata", "tom sercu", "kartik audhkhasi", "samuel thomas", "dimitrios dimitriadis", "xiaodong cui", "bhuvana ramabhadran", "michael picheny", "lynn-li lim", "bergul roomi", "phil hall"], "accepted": false, "id": "1703.02136"}, "pdf": {"name": "1703.02136.pdf", "metadata": {"source": "CRF", "title": "English Conversational Telephone Speech Recognition by Humans and Machines", "authors": ["George Saon", "Gakuto Kurata", "Tom Sercu", "Kartik Audhkhasi", "Samuel Thomas", "Dimitrios Dimitriadis", "Xiaodong Cui", "Bhuvana Ramabhadran", "Michael Picheny", "Lynn-Li Lim", "Bergul Roomi", "Phil Hall"], "emails": [], "sections": [{"heading": null, "text": "rate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multitask learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models. Index Terms: LSTM, ResNet, dilated convolutions, conversational speech recognition"}, {"heading": "1. Introduction", "text": "With the performance of ASR systems inching ever closer to that of humans, it is important to benchmark human performance accurately. In [1], the authors claim a human word error rate (WER) of 5.9%/11.3% on the Switchboard/CallHome subsets (SWB/CH) of the NIST Hub5 2000 evaluation testset. When compared with [2] which quotes a WER of 4%, the 5.9% estimate seemed rather high (albeit measured on different data). This intriguing discrepancy prompted us to launch our own human transcription effort in order to confirm (or disconfirm) the estimates from [1]. The findings from this effort were doubly surprising. First, we were expecting the SWB measurement to be closer to the Lippmann estimate of 4% but could only get down to 5.1% for the best transcriber after quality checks.\nSecond, the same transcriber achieved a surprisingly low 6.8% WER for CallHome (we were expecting a much higher number based on the 11.3% estimate).\nFor comparison, our latest ASR system achieves 5.5%/10.3% WER on SWB/CH. This means that \u201chuman parity\u201d is attainable on this particular Switchboard subset (although not achieved yet) but is a distant dream for the CallHome task. What makes the Switchboard and CallHome testsets so different one might ask? The biggest problem with the SWB testset is that 36 out of 40 test speakers appear in the training data, some in as many as 8 different conversations [3], and our acoustic models are very good at memorizing speech patterns seen during training. The second problem is that the SWB and CH tasks differ in the style of conversational speech: SWB consists of conversations between strangers while CH consists of calls between friends and family members. Speaking style between strangers tends to be more formal whereas the CallHome style is more casual making CH a harder task. The training data collected by LDC under the Switchboard and Fisher protocols is almost entirely Switchboard-like meaning that testing on CallHome is a mismatched scenario for ASR systems. Since ASR systems are generally not robust to mismatched training and testing conditions, it comes as no surprise that the degradation in performance from SWB to CH for ASR systems is larger than that of expert transcribers.\nOn the system side, we have simplified and improved our acoustic models considerably and experimented with more sophisticated language models such as LSTM and WaveNet LMs. Most of the AM improvement comes from LSTMs that operate on multiple features or use a different training criterion such as speaker-adversarial multi-task learning. Additionally, replacing the VGG convolutional nets [4] that we had in our last year\u2019s system [5] with ResNets [6] turned out to be beneficial for performance. On the LM side, adding LSTM word and characterbased LMs resulted in substantial accuracy gains.\nThe rest of the paper is organized as follows: in section 2 we talk about the human transcription experiments; in section 3 we describe a series of system improvements pertaining to both acoustic and language modeling and in section 4 we summarize our findings."}, {"heading": "2. Human transcription experiments", "text": "These experiments were carried out by IBM\u2019s preferred speech transcription provider, Appen. The transcription protocol that was agreed upon was to have three independent transcribers provide transcripts which were quality checked by a fourth senior transcriber. All four transcribers are native US English\nar X\niv :1\n70 3.\n02 13\n6v 1\n[ cs\n.C L\n] 6\nM ar\n2 01\n7\nspeakers and were selected based on the quality of their work on past transcription projects. The transcribers were familiarized with the LDC transcription guidelines which cover hyphenations, spelled abbreviations, contractions, partial words, nonspeech sounds, etc.\nThe transcription time was estimated at 12-14 times realtime (xRT) for the first pass for Transcribers 1-3 and an additional 1.7-2xRT for the second quality checking pass (by Transcriber 4). Both passes involved listening to the audio multiple times: around 3-4 times for the first pass and 1-2 times for the second. After receiving the transcripts, the following filtering rules were aplied:\n\u2022 All non-speech markers were tagged as non-lexical items which are ignored during scoring. Examples of nonspeech markers are: [laughter], [breathing], [noise], {no speech}, etc.\n\u2022 Other markers such as \u2019...\u2019, \u2019\u2013\u2019, \u2019(( ))\u2019 were eliminated prior to scoring.\n\u2022 All partial words ending in \u2019-\u2019 were marked as nonlexical items.\n\u2022 All punctuation marks such as \u2019.\u2019, \u2019,\u2019, \u2019!\u2019 and \u2019?\u2019 were eliminated prior to scoring.\nIn order to use NIST\u2019s scoring tool sclite, we had to convert the transcripts into CTM files which have time-marked word boundary information. This was done by splitting the duration of the utterance uniformly across the number of words.\nIn Table 1 we show the error rates of the three transcribers before and after quality checking by the fourth transcriber as well as the human WER reported in [1]. Unsurprisingly, there is some variation among transcriber performance and the quality checking pass reduces the error rate across all transcribers.\nAdditionally, in Tables 2 and 3, we take a closer look at the most frequent substitution, deletion and insertion errors for our system output and the best human transcript after quality checking. While many of the errors look similar to those reported in [1], there is a glaring discrepancy in the frequency of top deletions for CallHome between our human transcript and theirs. This suggests that the very different estimates for the human error rate for CallHome (6.8% versus 11.3%) can be attributed to a much lower deletion rate for our best human transcript."}, {"heading": "3. System improvements", "text": "In this section we discuss the training data and testsets that were used as well as improvements in acoustic and language modeling. The training set for our acoustic models consists of 262\nhours of Switchboard 1 audio with transcripts provided by Mississippi State University, 1698 hours from the Fisher data collection and 15 hours of CallHome audio. In order to allay fears that we may be overfitting to the Hub5 2000 testsets by extensively testing on them, we have decided to report results on a variety of testsets. Since the RT\u201902, RT\u201903, RT\u201904 and DEV\u201904f testsets have not been used in more than a decade, we are fairly confident that performance improvements on these testsets are indicative of real progress. Statistics about all the testsets used in the experiments are given in Table 4.\nIn [7], we have shown that convolutional and nonconvolutional AMs have comparable performance and good complementarity. Hence, the strategy for our previous systems [8, 5] was to use a combination of recurrent and convolutional nets. For example, in last year\u2019s system we used a score fusion of three models which share the same decision tree: unfolded RNNs with maxout activations, LSTMs and VGG nets. This year, in order to simplify and enhance the overall architecture, we eliminated the maxout RNN, we improved the LSTMs and we replaced the VGG nets with residual nets (ResNets)."}, {"heading": "3.1. LSTM acoustic models", "text": "All models presented here share the following characteristics. Their architecture consists in 4-6 bidirectional layers with 1024 cells per layer (512 per direction), one linear bot-\ntleneck layer with 256 units and an output layer with 32K units corresponding to as many context-dependent HMM states (shown on the left side of Figure 1). Training is done on nonoverlapping subsequences of 21 frames where each frame consists of 40-dimensional FMLLR features to which we append 100-dimensional i-vectors. We group subsequences from different utterances into minibatches of size 128 for processing speed and reliable gradient estimates. The training consists of 14 passes of cross-entropy followed by 1 pass of SGD sequence training using the boosted MMI criterion [9] smoothed by adding the scaled gradient of the cross-entropy loss [10]. Implementation of the LSTM was done in Torch [11] with cuDNN v5.0 backend. Cross-entropy training for each model took about 2 weeks for 700M samples/epoch, on a single Nvidia K80 GPU device.\nThe first two improvements are fairly banal and consist in increasing the number of layers from 4 (like in our previous model [5]) to 6 and in realigning the training data with a 6-layer LSTM and retraining another LSTM. The effect of these steps is shown in the first three rows of Table 5 across all testsets.\nThe second set of experiments was centered around the use of speaker-adversarial multi-task learning (SA-MTL). In [12], the authors introduce domain-adversarial neural networks which are models that are trained to not distiguish between in-domain, labeled data and out-of-domain, unlabeled data. This is achieved by training a domain classifier in parallel with the main classifier and by subtracting the gradient component from the domain classifier when estimating the parameters of the main classifier. This idea has been successfully applied in speech by [13] in the context of noise robustness where the author proposes noise-adversarial MTL to suppress the effects of noise. Here, we experiment with training a speaker classifier in addition to the main CD-HMM state classifier in order to suppress the effects of speaker variability on ASR performance. Since i-vectors are a good low-dimensional representation of a speaker, we decided to train the speaker classifier to predict the i-vector inputs using an MSE loss function. The speaker classifier has one sigmoid layer and one hyperbolic tangent layer as shown in Figure 1.\nIf we denote by \u03b8, \u03b8c, \u03b8s the parameters of the common LSTM, the main classifier (weights of linear layer before softmax) and the speaker classifier, the SGD update is done according to:\n\u03b8\u0302c = \u03b8c \u2212 \u2202LCE(x) \u2202\u03b8c \u03b8\u0302s = \u03b8s \u2212 \u2202LMSE(x)\n\u2202\u03b8s \u03b8\u0302 = \u03b8 \u2212 ( \u2202LCE(x)\n\u2202\u03b8 \u2212 \u03bb\u2202LMSE(x) \u2202\u03b8 ) where x denotes a minibatch, LCE , LMSE denote respectively\nthe cross-entropy loss of the main classifier and the meansquared error loss of the i-vector classifier, \u03bb is a scaling parameter (typically set to 0.1), and is the learning rate. After the model is trained, the i-vector classifier branch is discarded at test time. As can be seen from Table 5 rows 3 and 4, we observe some small gains across all testsets which are also due in part to reestimating the VTLN warp factors and FMLLR transforms using an LSTM decoding output (old factors and transforms were based off a GMM decoding).\nLast but not least, the largest improvement in LSTM modeling was achieved through feature fusion. The thought process leading to this experiment was that we wanted to add utterance-level information to our models which were only looking at a window of 21 consecutive frames. One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features. This experiment worked quite well however, upon closer inspection, it turned out that the CTC model used a different set of input features: Logmel+\u2206+\u2206\u2206 instead of PLP followed by LDA and FMLLR. The question then naturally arose whether the gains came from CTC modeling or from the different input representations. To answer this question, we built an LSTM trained on fused FMLLR+i-vector+Logmel+\u2206+\u2206\u2206 features the standard way (without speaker-adversarial MTL). The WER improvement from adding the Logmel features, indicated in Table 5 rows 3 and 5, is the same as with CTC features meaning that the CTC modeling step was not needed. Finally, we note that the feature fusion LSTM compares favorably with other single acoustic models from the literature as mentioned in [17] (Table 4)."}, {"heading": "3.2. ResNet acoustic models", "text": "On the convolutional network acoustic modeling side, we trained residual networks with pre-activation identity shortcut connections. Residual Networks were introduced for image recognition in [18] and used in speech recognition in [1, 19]. The novelty of residual networks is to introduce shortcut connections between so-called \u201cblocks\u201d of convolutional layers, which was argued to improve the flow of information and gradients, and allows training even deeper CNNs without the optimization problem occuring without the residual connections.\nTable 6 shows four residual network model architectures\nand their performance on the testsets with small LM. We achieved best results with basic residual blocks without bottleneck, similar to the observations from [20] on CIFAR and SVHN experiments. However, bottleneck residual blocks could possibly be optimal with a larger computational budget. The input to our network are vtln-warped logmel features with 64 mel bins. We perform data-balancing according to [22] with exponent \u03b3 = 0.8. We use full pre-activation identity shortcut connections which keep a clean information path [21] without nonlinearity after addition. For batch normalization the statistics are accumulated per feature map and per frequency bin following [24].\nIn order to use residual networks for acoustic modeling, we need to adapt the residual blocks (see Figure 2), while taking efficient convolution on sequences into account. In ResNets for image classification the convolutional pathway only includes padded convolutions, so does not reduce the size of the feature maps. The addition with the shortcut pathway is trivial, since both feature maps have the same size. In contrast, for convolutions on sequences we can not pad along the time directions. Padding along the time direction would modify the values on the edges based on the input sliding window location, thus making efficient convolution over a full utterance impossible (see [23]). So we do not pad the convolutions in time and as a consequence, the convolutional pathway reduces the size of the feature maps along the time direction. In this case, we need to crop on the shortcut connection to match the size of the feature maps coming out of the convolutional pathway. It is important to note that this does not impact the ability to convolve the residual net over full utterances at once: since the values at the edges are computed the same as everywhere else, they are\nindependent of the position of the input window. Let us now consider how to use strided pooling and strided convolutions, and the relation to time-dilated convolutions. First off, in the frequency direction, similar as for images, convolutions are padded so they do not reduce the size. Rather, the size is reduced by a factor of 2 through convolutions with stride 2. In Table 6, the \u201cinitStride\u201d field on the first line of each stage indicates the (frequency x time) stride for the first block of that stage, where the number of feature maps is increased. This stride applies to both the first 3 \u00d7 3 convolution of the block, and the 1\u00d71 convolution in the projection shortcut. The output feature map size is indicated in the left column for each stage. Secondly, along the time direction, strided convolutions and strided pooling is optional, but was found to improve performance [24]. In Table 6, Stage 4, column (c) and (d), bold indicates striding in time. Note that, when adding time-strided conv and pool to an architecture, we need to increase the input context window to compensate for the additional size reduction. For residual networks, similar as for VGG-style networks, we indeed observe that time-strided time-pooling improves performance, see column (b) vs (d).\nWhen transitioning from cross-entropy (XE) to sequence training (ST), we want to modify our network to do dense prediction efficiently [24]. This means the intermediate states of the convolutional layers and output of the ResNet should maintain inputs full time-resolution, i.e. it should produce an output CD state distribution for each input frame. We can achieve this by using time-dilated convolutions according to the same recipe as in [24]: for each layer which originally strides in time with factor 2, set time-stride to 1 and dilate with factor 2 all consecutive convolutions, maxpooling and fully connected layers. This includes the projection shortcut in the first block of each stage, though dilation for these 1 \u00d7 1 convolutions is irrelevant. After these modifications, the residual net can be used for dense prediction on sequences.\nThe ResNet which we will use in further sections is in Table 6 (d). It has 12 residual blocks, 30 weight layers and 67.1 M parameters. We trained this model using Nesterov accelerated gradient with learningrate 0.03 and momentum 0.99. Implementation of the CNN was also done in Torch with cuDNN v5.0 backend. Cross-entropy training took about 80 days for 1.5 billion samples, on 2 Nvidia K80 GPU\u2019s (4 devices) with batch size 64 per GPU and full synchronization between every minibatch. We sequence trained this model for 200M frames with the boosted MMI criterion [9]."}, {"heading": "3.3. Model combination", "text": "In Table 7 we report the performance of the best individual models described in the previous paragraphs as well as the results after frame-level score fusion across all testsets. All decodings are done with an 85K word vocabulary and a 4-gram language model with 36M n-grams. We note that LSTMs and ResNets exhibit a strong complementarity which improves the WER for all testsets."}, {"heading": "3.4. Language modeling improvements", "text": "In addition to n-gram and model-M used in our previous system [5], we introduced LSTM-based as well as convolutionbased LMs in this paper.\nWe experimented with four LSTM LMs, namely WordLSTM, Char-LSTM, Word-LSTM-MTL, and Char-LSTM-MTL. The Word-LSTM had one word-embeddings layer, two LSTM layers, one fully-connected layer, and one softmax layer, as shown in Figure 3. The upper LSTM layer and the fullyconnected layer were wrapped by residual connections [6]. Dropout was only applied to the vertical dimension and not applied to the time dimension [25]. The Char-LSTM added an additional LSTM layer to estimate word-embeddings from character sequences as illustrated in Figure 4 [26]. Both Word-LSTM and Char-LSTM used the cross-entropy loss of predicting the next word given its history as objective function, similar to conventional LMs. In addition, we introduced multi-task learning (MTL) in Word-LSTM-MTL and Char-LSTM-MTL. We first clustered the vocabulary using Brown clustering [27]. When training Word-LSTM-MTL and Char-LSTM-MTL, weighted summation of cross-entropy of predicting next word given its history and next class given its history was used as objective function.\nInspired by the complementarity of convolutional and non-convolutional acoustic models, we experimented with a convolution-based LM in the form of dilated causal convolution as used in WAVENET [28]. The resulting model is called WordDCC and consists of word-embeddings layer, causal convolution layers with dilation, convolution layers, fully-connected layers, softmax layer, and residual connections. The actual number of layers and dilation/window sizes were determined using heldout data (Figure 5 has a simple configuration for illustration purposes).\nFor these five LMs, the training data and training procedures are common and described below:\n\u2022 We used the same vocabulary of 85K words from [5]. \u2022 We first train the LM with a corpus of 560M words\nconsisting of publicly available text data from LDC, including Switchboard, Fisher, Gigaword, and Brodcast News and Conversations. Then, starting from the trained model, we further train the LM with only the transcripts of the 1975 hours audio data used to train the acoustic model, consisting of 24M words.\n\u2022 We controlled the leaning rate by ADAM [29] and introduced a self-stabilization term to coordinate the layerwise learning rates [30].\n\u2022 For all models, we tuned the hyper-parameters based on the perplexity of the heldout data which is a subset of the acoustic transcripts. The approximate number of parameters for each model was 90M to 130M.\nWe first generated word lattices with the n-gram LM and our best acoustic model consisting of ResNet and two LSTMs. Then we rescored the word lattices with the model-M and generated n-best lists from the rescored lattice. Finally, we applied the four LSTM-based LMs and the convolution-based LM. Note that LM probabilities were linearly interpolated and the interpolation weights of all LMs were estimated using the heldout data.\nTable 8 shows WER on SWB and CH with various LM configurations. The LSTM-based LMs show significant improvements over the strong n-gram + model-M results. The Word-DCC also has a marginal improvement over the n-gram + model-M. The effect of multi-task learning was confirmed especially on CH. Among the five LSTM-based and convolutionbased LMs, word-LSTM-MTL achieved the best WER of 5.6% and 10.3% on SWB and CH respectively. By combining five LMs on top of n-gram + model-M, we achieved 5.5% and 10.3% WER for SWB and CH respectively. Lastly, we summarize the improvements due to the various language model rescoring steps across all testsets in Table 9. We noticed that the testset references have inconsistent transcription conventions with regards to spellings which are not followed by periods for SWB and CH (e.g. T V) and followed by periods for the other testsets (such as T. V.). The last line of Table 9 shows the WERs when periods are removed from both the references and system outputs by adding the filtering rules A. => A ... Z. => Z to the GLM file.\nMore details about the language modeling are given in a companion paper [31].\nbe\nable\nable\nto\nEmb.\nLSTM\nFC\nEmb.\nLSTM\nFC\nLSTM LSTM\nSoftMax SoftMax\nFigure 3: Word-LSTM e\nable\na\nto\nLSTM\nLSTM\nFC\nLSTM\nLSTM\nFC\nb b l e\nLSTM LSTMLSTMLSTM\nEmb. Emb.Emb. Emb.Emb.Emb.\nLSTM LSTM\nSoftmax Softmax\nFigure 4: Char-LSTM be able\nto\nEmb. Emb.\nConv.\nFC\nConv.\nSoftMax\nwanted to\nEmb. Emb.\nConv.\nFigure 5: Word-DCC"}, {"heading": "4. Conclusion", "text": "We have presented a set of acoustic and language modeling improvements to our English Switchboard system that resulted in a new record word error rate on this task. On the acoustic side, two things were instrumental in reaching this level of performance. The first one is a steady improvement in bidirectional LSTM modeling, chief among them being a simple feature fusion experiment. The second one is the replacement of VGG nets with residual nets which are a more effective architecture on the ImageNet classification task. When combined together, these recurrent and convolutional nets show good complementarity and enhanced accuracy on a variety of testsets. On the language modeling side, we exploited the same complementarity between recurrent and convolutional architectures by adding word and character-based LSTM LMs and a convolutional WaveNet LM.\nThe second and perhaps more important point made in this paper is that, unlike what was claimed in [1], we do not believe that human parity has been reached on this task. The reasons why we came to the opposite conclusion are twofold. First, the Hub5\u201900 SWB testset has a large overlap between training and test speakers which results in ASR systems having deceptively good performance. A more realistic level of ASR performance is the average WER across all testsets which is around 8% for our system. The second and more direct argument is that the human WER of expert transcribers that were asked to do a highquality job is simply lower than what was previously reported. On an optimistic note, this means that the future of research in conversational speech recognition looks bright for at least a few more years."}, {"heading": "5. References", "text": "[1] W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer,\nA. Stolcke, D. Yu, and G. Zweig, \u201cAchieving human parity in conversational speech recognition,\u201d arXiv preprint arXiv:1610.05256, 2016.\n[2] R. P. Lippmann, \u201cSpeech recognition by machines and humans,\u201d Speech communication, vol. 22, no. 1, pp. 1\u201315, 1997.\n[3] J. Fiscus, W. M. Fisher, A. F. Martin, M. A. Przybocki, and D. S. Pallett, \u201c2000 nist evaluation of conversational speech recognition over the telephone: English and mandarin performance results,\u201d in Proc. Speech Transcription Workshop. Citeseer, 2000, pp. 1\u20135.\n[4] K. Simonyan and A. Zisserman, \u201cVery deep convolu-\ntional networks for large-scale image recognition,\u201d CoRR arXiv:1409.1556, 2014.\n[5] G. Saon, T. Sercu, S. Rennie, and H.-K. Kuo, \u201cThe IBM 2016 English conversational speech recognition system,\u201d in Seventeenth Annual Conference of the International Speech Communication Association, 2016.\n[6] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. CVPR, 2016, pp. 770\u2013 778.\n[7] H. Soltau, G. Saon, and T. N. Sainath, \u201cJoint training of convolutional and non-convolutional neural networks,\u201d to Proc. ICASSP, 2014.\n[8] G. Saon, H.-K. Kuo, S. Rennie, and M. Picheny, \u201cThe IBM 2015 English conversational speech recognition system,\u201d in Sixteenth Annual Conference of the International Speech Communication Association, 2015.\n[9] D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran, G. Saon, and K. Visweswariah, \u201cBoosted MMI for model and feature-space discriminative training,\u201d in Proc. of ICASSP, 2008, pp. 4057\u20134060.\n[10] H. Su, G. Li, D. Yu, and F. Seide, \u201cError back propagation for sequence training of context-dependent deep networks for conversational speech transcription,\u201d Proc. ICASSP, 2013.\n[11] R. Collobert, K. Kavukcuoglu, and C. Farabet, \u201cTorch7: A matlab-like environment for machine learning,\u201d in BigLearn, NIPS Workshop, no. EPFL-CONF-192376, 2011.\n[12] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky, \u201cDomain-adversarial training of neural networks,\u201d Journal of Machine Learning Research, vol. 17, no. 59, pp. 1\u201335, 2016.\n[13] Y. Shinohara, \u201cAdversarial multi-task learning of deep neural networks for robust speech recognition,\u201d Interspeech 2016, pp. 2369\u20132372, 2016.\n[14] Y. Miao, M. Gowayyed, and F. Metze, \u201cEesen: End-to-end speech recognition using deep rnn models and wfst-based decoding,\u201d arXiv preprint arXiv:1507.08240, 2015.\n[15] H. Sak, A. Senior, K. Rao, O. Irsoy, A. Graves, F. Beaufays, and J. Schalkwyk, \u201cLearning acoustic frame labeling for speech recognition with recurrent neural networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.\n[16] H. Soltau, H. Liao, and H. Sak, \u201cNeural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition,\u201d arXiv preprint arXiv:1610.09975, 2016.\n[17] L. Hairong, Z. Zhu, X. Li, and S. Satheesh, \u201cGram-ctc: Automatic unit selection and target decomposition for sequence labelling,\u201d CoRR arXiv:1703.00096, 2017.\n[18] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d CoRR arXiv:1512.03385, 2015.\n[19] Y. Zhang, W. Chan, and N. Jaitly, \u201cVery deep convolutional networks for end-to-end speech recognition,\u201d Proc. ICASSP, 2017.\n[20] S. Zagoruyko and N. Komodakis, \u201cWide residual networks,\u201d arXiv preprint arXiv:1605.07146, 2016.\n[21] K. He, X. Zhang, S. Ren, and J. Sun, \u201cIdentity mappings in deep residual networks,\u201d arXiv preprint arXiv:1603.05027, 2016.\n[22] T. Sercu, C. Puhrsch, B. Kingsbury, and Y. LeCun, \u201cVery deep multilingual convolutional neural networks for lvcsr,\u201d Proc. ICASSP, 2016.\n[23] T. Sercu and V. Goel, \u201cAdvances in very deep convolutional neural networks for lvcsr,\u201d arXiv, 2016.\n[24] \u2014\u2014, \u201cDense prediction on sequences with time-dilated convolutions for speech recognition,\u201d NIPS End-to-end Learning for Speech and Audio Processing Workshop, 2016.\n[25] W. Zaremba, I. Sutskever, and O. Vinyals, \u201cRecurrent neural network regularization,\u201d arXiv preprint arXiv:1409.2329, 2014.\n[26] W. Ling, T. Lu\u0131\u0301s, L. Marujo, R. F. Astudillo, S. Amir, C. Dyer, A. W. Black, and I. Trancoso, \u201cFinding function in form: Compositional character models for open vocabulary word representation,\u201d arXiv preprint arXiv:1508.02096, 2015.\n[27] P. F. Brown, P. V. Desouza, R. L. Mercer, V. J. D. Pietra, and J. C. Lai, \u201cClass-based n-gram models of natural language,\u201d Computational Linguistics, vol. 18, no. 4, pp. 467\u2013479, 1992.\n[28] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWaveNet: A generative model for raw audio,\u201d arXiv preprint arXiv:1609.03499, 2016.\n[29] D. Kingma and J. Ba, \u201cADAM: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[30] P. Ghahremani and J. Droppo, \u201cSelf-stabilized deep neural network,\u201d in Proc. ICASSP, 2016, pp. 6645\u20136649.\n[31] G. Kurata, A. Sethy, B. Ramabhadran, and G. Saon, \u201cEmpirical exploration of LSTM and CNN language models for speech recognition,\u201d Submitted to Interspeech 2017, 2017."}], "references": [{"title": "Achieving human parity in conversational speech recognition", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "arXiv preprint arXiv:1610.05256, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech recognition by machines and humans", "author": ["R.P. Lippmann"], "venue": "Speech communication, vol. 22, no. 1, pp. 1\u201315, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "2000 nist evaluation of conversational speech recognition over the telephone: English and mandarin performance results", "author": ["J. Fiscus", "W.M. Fisher", "A.F. Martin", "M.A. Przybocki", "D.S. Pallett"], "venue": "Proc. Speech Transcription Workshop. Citeseer, 2000, pp. 1\u20135.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Very deep convolu-  tional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR arXiv:1409.1556, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBM 2016 English conversational speech recognition system", "author": ["G. Saon", "T. Sercu", "S. Rennie", "H.-K. Kuo"], "venue": "Seventeenth Annual Conference of the International Speech Communication Association, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR, 2016, pp. 770\u2013 778.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint training of convolutional and non-convolutional neural networks", "author": ["H. Soltau", "G. Saon", "T.N. Sainath"], "venue": "to Proc. ICASSP, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The IBM 2015 English conversational speech recognition system", "author": ["G. Saon", "H.-K. Kuo", "S. Rennie", "M. Picheny"], "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Boosted MMI for model and feature-space discriminative training", "author": ["D. Povey", "D. Kanevsky", "B. Kingsbury", "B. Ramabhadran", "G. Saon", "K. Visweswariah"], "venue": "Proc. of ICASSP, 2008, pp. 4057\u20134060.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "author": ["H. Su", "G. Li", "D. Yu", "F. Seide"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop, no. EPFL-CONF-192376, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1923}, {"title": "Domain-adversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "Journal of Machine Learning Research, vol. 17, no. 59, pp. 1\u201335, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Adversarial multi-task learning of deep neural networks for robust speech recognition", "author": ["Y. Shinohara"], "venue": "Interspeech 2016, pp. 2369\u20132372, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "arXiv preprint arXiv:1507.08240, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["H. Sak", "A. Senior", "K. Rao", "O. Irsoy", "A. Graves", "F. Beaufays", "J. Schalkwyk"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural speech recognizer: Acoustic-to-word lstm model for large vocabulary speech recognition", "author": ["H. Soltau", "H. Liao", "H. Sak"], "venue": "arXiv preprint arXiv:1610.09975, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Gram-ctc: Automatic unit selection and target decomposition for sequence labelling", "author": ["L. Hairong", "Z. Zhu", "X. Li", "S. Satheesh"], "venue": "CoRR arXiv:1703.00096, 2017.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR arXiv:1512.03385, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for end-to-end speech recognition", "author": ["Y. Zhang", "W. Chan", "N. Jaitly"], "venue": "Proc. ICASSP, 2017.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Wide residual networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "arXiv preprint arXiv:1605.07146, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1603.05027, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "Proc. ICASSP, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Advances in very deep convolutional neural networks for lvcsr", "author": ["T. Sercu", "V. Goel"], "venue": "arXiv, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Dense prediction on sequences with time-dilated convolutions for speech recognition", "author": ["\u2014\u2014"], "venue": "NIPS End-to-end Learning for Speech and Audio Processing Workshop, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "T. Lu\u0131\u0301s", "L. Marujo", "R.F. Astudillo", "S. Amir", "C. Dyer", "A.W. Black", "I. Trancoso"], "venue": "arXiv preprint arXiv:1508.02096, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Class-based n-gram models of natural language", "author": ["P.F. Brown", "P.V. Desouza", "R.L. Mercer", "V.J.D. Pietra", "J.C. Lai"], "venue": "Computational Linguistics, vol. 18, no. 4, pp. 467\u2013479, 1992.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "WaveNet: A generative model for raw audio", "author": ["A. v. d. Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "ADAM: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-stabilized deep neural network", "author": ["P. Ghahremani", "J. Droppo"], "venue": "Proc. ICASSP, 2016, pp. 6645\u20136649.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical exploration of LSTM and CNN language models for speech recognition", "author": ["G. Kurata", "A. Sethy", "B. Ramabhadran", "G. Saon"], "venue": "Submitted to Interspeech 2017, 2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "In [1], the authors claim a human word error rate (WER) of 5.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "When compared with [2] which quotes a WER of 4%, the 5.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "This intriguing discrepancy prompted us to launch our own human transcription effort in order to confirm (or disconfirm) the estimates from [1].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "What makes the Switchboard and CallHome testsets so different one might ask? The biggest problem with the SWB testset is that 36 out of 40 test speakers appear in the training data, some in as many as 8 different conversations [3], and our acoustic models are very good at memorizing speech patterns seen during training.", "startOffset": 227, "endOffset": 230}, {"referenceID": 3, "context": "Additionally, replacing the VGG convolutional nets [4] that we had in our last year\u2019s system [5] with ResNets [6] turned out to be beneficial for performance.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Additionally, replacing the VGG convolutional nets [4] that we had in our last year\u2019s system [5] with ResNets [6] turned out to be beneficial for performance.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Additionally, replacing the VGG convolutional nets [4] that we had in our last year\u2019s system [5] with ResNets [6] turned out to be beneficial for performance.", "startOffset": 110, "endOffset": 113}, {"referenceID": 0, "context": "In Table 1 we show the error rates of the three transcribers before and after quality checking by the fourth transcriber as well as the human WER reported in [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": "Human WER from [1] 5.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "Table 1: Word error rates on SWB and CH for human transcribers before and after quality checking contrasted with the human WER reported in [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "While many of the errors look similar to those reported in [1], there is a glaring discrepancy in the frequency of top deletions for CallHome between our human transcript and theirs.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "In [7], we have shown that convolutional and nonconvolutional AMs have comparable performance and good complementarity.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Hence, the strategy for our previous systems [8, 5] was to use a combination of recurrent and convolutional nets.", "startOffset": 45, "endOffset": 51}, {"referenceID": 4, "context": "Hence, the strategy for our previous systems [8, 5] was to use a combination of recurrent and convolutional nets.", "startOffset": 45, "endOffset": 51}, {"referenceID": 8, "context": "The training consists of 14 passes of cross-entropy followed by 1 pass of SGD sequence training using the boosted MMI criterion [9] smoothed by adding the scaled gradient of the cross-entropy loss [10].", "startOffset": 128, "endOffset": 131}, {"referenceID": 9, "context": "The training consists of 14 passes of cross-entropy followed by 1 pass of SGD sequence training using the boosted MMI criterion [9] smoothed by adding the scaled gradient of the cross-entropy loss [10].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Implementation of the LSTM was done in Torch [11] with cuDNN v5.", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "The first two improvements are fairly banal and consist in increasing the number of layers from 4 (like in our previous model [5]) to 6 and in realigning the training data with a 6-layer LSTM and retraining another LSTM.", "startOffset": 126, "endOffset": 129}, {"referenceID": 11, "context": "In [12], the authors introduce domain-adversarial neural networks which are models that are trained to not distiguish between in-domain, labeled data and out-of-domain, unlabeled data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "This idea has been successfully applied in speech by [13] in the context of noise robustness where the author proposes noise-adversarial MTL to suppress the effects of noise.", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 14, "context": "One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 15, "context": "One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 16, "context": "One possibility was to train an end-to-end LSTM using CTC as in [14, 15, 16, 17] and append the features from the last LSTM layer before the softmax to our existing features.", "startOffset": 64, "endOffset": 80}, {"referenceID": 16, "context": "Finally, we note that the feature fusion LSTM compares favorably with other single acoustic models from the literature as mentioned in [17] (Table 4).", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "Residual Networks were introduced for image recognition in [18] and used in speech recognition in [1, 19].", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Residual Networks were introduced for image recognition in [18] and used in speech recognition in [1, 19].", "startOffset": 98, "endOffset": 105}, {"referenceID": 18, "context": "Residual Networks were introduced for image recognition in [18] and used in speech recognition in [1, 19].", "startOffset": 98, "endOffset": 105}, {"referenceID": 19, "context": "We achieved best results with basic residual blocks without bottleneck, similar to the observations from [20] on CIFAR and SVHN experiments.", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "We perform data-balancing according to [22] with exponent \u03b3 = 0.", "startOffset": 39, "endOffset": 43}, {"referenceID": 20, "context": "We use full pre-activation identity shortcut connections which keep a clean information path [21] without nonlinearity after addition.", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "For batch normalization the statistics are accumulated per feature map and per frequency bin following [24].", "startOffset": 103, "endOffset": 107}, {"referenceID": 22, "context": "Padding along the time direction would modify the values on the edges based on the input sliding window location, thus making efficient convolution over a full utterance impossible (see [23]).", "startOffset": 186, "endOffset": 190}, {"referenceID": 23, "context": "Secondly, along the time direction, strided convolutions and strided pooling is optional, but was found to improve performance [24].", "startOffset": 127, "endOffset": 131}, {"referenceID": 23, "context": "When transitioning from cross-entropy (XE) to sequence training (ST), we want to modify our network to do dense prediction efficiently [24].", "startOffset": 135, "endOffset": 139}, {"referenceID": 23, "context": "We can achieve this by using time-dilated convolutions according to the same recipe as in [24]: for each layer which originally strides in time with factor 2, set time-stride to 1 and dilate with factor 2 all consecutive convolutions, maxpooling and fully connected layers.", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "We sequence trained this model for 200M frames with the boosted MMI criterion [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "In addition to n-gram and model-M used in our previous system [5], we introduced LSTM-based as well as convolutionbased LMs in this paper.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "The upper LSTM layer and the fullyconnected layer were wrapped by residual connections [6].", "startOffset": 87, "endOffset": 90}, {"referenceID": 24, "context": "Dropout was only applied to the vertical dimension and not applied to the time dimension [25].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "The Char-LSTM added an additional LSTM layer to estimate word-embeddings from character sequences as illustrated in Figure 4 [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 26, "context": "We first clustered the vocabulary using Brown clustering [27].", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "Inspired by the complementarity of convolutional and non-convolutional acoustic models, we experimented with a convolution-based LM in the form of dilated causal convolution as used in WAVENET [28].", "startOffset": 193, "endOffset": 197}, {"referenceID": 4, "context": "\u2022 We used the same vocabulary of 85K words from [5].", "startOffset": 48, "endOffset": 51}, {"referenceID": 28, "context": "\u2022 We controlled the leaning rate by ADAM [29] and introduced a self-stabilization term to coordinate the layerwise learning rates [30].", "startOffset": 41, "endOffset": 45}, {"referenceID": 29, "context": "\u2022 We controlled the leaning rate by ADAM [29] and introduced a self-stabilization term to coordinate the layerwise learning rates [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "More details about the language modeling are given in a companion paper [31].", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "The second and perhaps more important point made in this paper is that, unlike what was claimed in [1], we do not believe that human parity has been reached on this task.", "startOffset": 99, "endOffset": 102}], "year": 2017, "abstractText": "One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which at least at the writing of this paper is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multitask learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.", "creator": "LaTeX with hyperref package"}}}