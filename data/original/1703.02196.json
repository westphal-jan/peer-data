{"id": "1703.02196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Cooperative Epistemic Multi-Agent Planning for Implicit Coordination", "abstract": "Epistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities. Recently, Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. We extend the DEL-based epistemic planning framework to include perspective shifts, allowing us to define new notions of sequential and conditional planning with implicit coordination. With these, it is possible to solve planning tasks with joint goals in a decentralized manner without the agents having to negotiate about and commit to a joint policy at plan time. First we define the central planning notions and sketch the implementation of a planning system built on those notions. Afterwards we provide some case studies in order to evaluate the planner empirically and to show that the concept is useful for multi-agent systems in practice.", "histories": [["v1", "Tue, 7 Mar 2017 03:16:22 GMT  (36kb,D)", "http://arxiv.org/abs/1703.02196v1", "In Proceedings M4M9 2017,arXiv:1703.01736"]], "COMMENTS": "In Proceedings M4M9 2017,arXiv:1703.01736", "reviews": [], "SUBJECTS": "cs.AI cs.LO cs.MA", "authors": ["thorsten engesser", "thomas bolander", "robert mattm\\\"uller", "bernhard nebel"], "accepted": false, "id": "1703.02196"}, "pdf": {"name": "1703.02196.pdf", "metadata": {"source": "CRF", "title": "Cooperative Epistemic Multi-Agent Planning for Implicit Coordination", "authors": ["Sujata Ghosh", "Thorsten Engesser", "Thomas Bolander", "Robert Mattm\u00fcller", "Bernhard Nebel"], "emails": ["engesset@cs.uni-freiburg.de", "tobo@dtu.dk", "mattmuel@cs.uni-freiburg.de", "nebel@cs.uni-freiburg.de"], "sections": [{"heading": null, "text": "Sujata Ghosh and R. Ramanujam: M4M9 EPTCS 243, 2017, pp. 75\u201390, doi:10.4204/EPTCS.243.6\nc\u00a9 T. Engesser, T. Bolander, R. Mattmu\u0308ller & B. Nebel This work is licensed under the Creative Commons Attribution License.\nCooperative Epistemic Multi-Agent Planning for Implicit Coordination\nThorsten Engesser Institut fu\u0308r Informatik\nAlbert-Ludwigs-Universita\u0308t Freiburg, Germany\nengesset@cs.uni-freiburg.de\nThomas Bolander DTU Compute\nTechnical University of Denmark Copenhagen, Denmark\ntobo@dtu.dk\nRobert Mattmu\u0308ller Institut fu\u0308r Informatik\nAlbert-Ludwigs-Universita\u0308t Freiburg, Germany\nmattmuel@cs.uni-freiburg.de\nBernhard Nebel Institut fu\u0308r Informatik\nAlbert-Ludwigs-Universita\u0308t Freiburg, Germany\nnebel@cs.uni-freiburg.de\nEpistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities. Recently, Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. We extend the DEL-based epistemic planning framework to include perspective shifts, allowing us to define new notions of sequential and conditional planning with implicit coordination. With these, it is possible to solve planning tasks with joint goals in a decentralized manner without the agents having to negotiate about and commit to a joint policy at plan time. First we define the central planning notions and sketch the implementation of a planning system built on those notions. Afterwards we provide some case studies in order to evaluate the planner empirically and to show that the concept is useful for multi-agent systems in practice."}, {"heading": "1 Introduction", "text": "One important task in Multi-Agent Systems is to collaboratively reach a joint goal with multiple autonomous agents. The problem is particularly challenging in situations where the knowledge and capabilities required to reach the goal are distributed among the agents. Most existing approaches therefore apply some centralized coordinating instance from the outside, strictly separating the stages of communication and negotiation from the agents\u2019 internal planning and reasoning processes. In contrast, building upon the epistemic planning framework by Bolander and Andersen [8], we propose a decentralized planning notion in which each agent has to individually reason about the entire problem and autonomously decide when and how to (inter-)act. For this, both reasoning about the other agents\u2019 possible contributions and reasoning about their capabilities of performing the same reasoning is needed. We achieve our notion of implicitly coordinated plans by requiring all desired communicative abilities to be modeled as epistemic actions which then can be planned alongside their ontic counterparts, thus enabling the agents to perform observations and coordinate at run time. It captures the intuition that communication clearly constitutes an action by itself and, more subtly, that even a purely ontic action can play a communicative role (e.g. indirectly suggesting follow-up actions to another agent). Thus, for many problems our approach appears quite natural. On the practical side, the epistemic planning framework allows a very expressive way of defining both the agents\u2019 physical and communicative abilities.\nConsider the following example scenario. Bob would like to borrow the apartment of his friend Anne while she is away on vacation. Anne would be very happy to do him this favor. So they now have the\njoint goal of making sure that Bob can enter the apartment when he arrives. Anne will think about how to achieve the goal, and might come up with the following plan: Anne puts the key under the door mat; when Bob arrives, Bob takes the key from under the door mat; Bob opens the door with the key. Note that the plan does not only contain the actions required by Anne herself, but also the actions of Bob. These are the kind of multi-agent plans that this paper is about.\nHowever, the plan just presented does not count as an implicitly coordinated plan. When Bob arrives at the apartment, he will clearly not know that the key is under the door mat, unless Anne has told him, and this announcement was not part of the plan just presented. If Anne has the ability to take Bob\u2019s perspective (she has a Theory of Mind concerning Bob [29]), Anne should of course be able to foresee this problem, and realize that her plan can not be expected to be successful. An improved plan would then be: Anne puts the key under the door mat; Anne calls Bob to let him know where the key is; when Bob arrives, Bob takes the key from under the door mat; Bob opens the door with the key. This does qualify as an implicitly coordinated plan. Anne now knows that Bob will know that he can find the key under the door mat and hence will be able to reach the goal. Anne does not have to request or even coordinate the sub-plan for Bob (which is: take key under door mat; open door with key), as she knows he will himself be able to determine this sub-plan given the information she provides. This is an important aspect of implicit coordination: coordination happens implicitly as a consequence of observing the actions of others (including announcements), never explicitly through agreeing or committing to a specific plan. The essential contributions of this paper are to formally define this notion of implicitly coordinated plans as well as to document and benchmark an implemented epistemic planner that produces such plans.\nOur work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and Lo\u0308we, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13]. Andersen, Bolander, and Jensen [4] extended the approach to allow strong and weak conditional planning in the single-agent case. Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of \u201cnested\u201d [8] or \u201cshallow\u201d knowledge states [26, 27, 28]. Since compilation approaches to classical planning can only deal with bounded nesting of knowledge (or belief), similar to Bolander and Andersen [8], we use search in the space of epistemic states to find a solution. One of the important features that distinguishes our work from more traditional multi-agent planning [9] is the explicit notion of perspective shifts needed for agents to reason about the possible plan contributions of other agents\u2014and hence needed to achieve implicit coordination.\nOur concepts can be considered related to recent work in temporal epistemic logics [10, 19, 20], which addresses a question similar to ours, namely what groups of agents can jointly achieve under imperfect information. These approaches are based on concurrent epistemic game structures. Our approach is different in a number of ways, including: 1) As in classical planning, our actions and their effects are explicitly and compactly represented in an action description language (using the event models of DEL); 2) Instead of joint actions we have sequential action execution, where the order in which the agents act is not predefined; 3) None of the existing solution concepts considered in temporal epistemic logics capture the stepwise shifting perspective underlying our notion of implicitly coordinated plans.\nThe present paper is an extended and revised version of a paper presented at the Workshop on Distributed and Multi-Agent Planning (DMAP) 2015 (without archival proceedings). The present paper primarily offers an improved presentation: extended and improved introduction, improved motivation, better examples, improved formulation of definitions and theorems, simplified notation, and more discussions of related work. The technical content is essentially as in the original DMAP paper, except we now compare implicitly coordinated plans to standard sequential plans, we now formally derive the correct notion of an implicitly coordinated plan from the natural conditions it should satisfy, and we have added a proposition that gives a semantic characterisation of implicitly coordinated policies."}, {"heading": "2 Theoretical Background", "text": "To represent planning tasks as the \u2018apartment borrowing\u2019 example of the introduction, we need a formal framework where: (1) agents can reason about the knowledge and ignorance of other agents; (2) both fully and partially observable actions can be described in a compact way (Bob doesn\u2019t see Anne placing the key under the mat). Dynamic Epistemic Logic (DEL) satisfies these conditions. We first briefly recapitulate the foundations of DEL, following the conventions of Bolander and Andersen [8]. What is new in this exposition is mainly the parts on perspective shifts in Section 2.2.\nWe now define epistemic languages, epistemic states and epistemic actions. All of these are defined relative to a given finite set of agents A and a given finite set of atomic propositions P. To keep the exposition simple, we will not mention the dependency on A and P in the following."}, {"heading": "2.1 Epistemic Language and Epistemic States", "text": "Definition 1. The epistemic language LKC is \u03d5 ::= > | \u22a5 | p | \u00ac\u03d5 | \u03d5 \u2227\u03d5 | Ki\u03d5 |C\u03d5 where p \u2208 P and i \u2208 A.\nWe read Ki\u03d5 as \u201cagent i knows \u03d5\u201d and C\u03d5 as \u201cit is common knowledge that \u03d5\u201d.\nDefinition 2. An epistemic model isM= \u3008W,(\u223ci)i\u2208A,V \u3009 where\n\u2022 The domain W is a non-empty finite set of worlds.\n\u2022 \u223ci \u2286W \u00d7W is an equivalence relation called the indistinguishability relation for agent i.\n\u2022 V : P\u2192P(W ) assigns a valuation to each atomic proposition.\nFor Wd \u2286W , the pair (M,Wd) is called an epistemic state (or simply a state), and the worlds of Wd are called the designated worlds. A state is called global if Wd = {w} for some world w (called the actual world), and we then often write (M,w) instead of (M,{w}). We use Sgl to denote the set of global states. For any state s = (M,Wd), we let Globals(s) = {(M,w) | w \u2208Wd}. A state (M,Wd) is called a local state for agent i if Wd is closed under \u223ci. A local state for i is minimal if Wd is a minimal set closed under\u223ci. We use Smini to denote the set of minimal local states of i. Given a state s = (M,Wd), the associated local state of agent i, denoted si, is (M,{v | v\u223ci w and w \u2208Wd}).\nDefinition 3. Let (M,Wd) be a state withM= \u3008W,(\u223ci)i\u2208A,V \u3009. For i \u2208 A, p \u2208 P and \u03d5,\u03c8 \u2208 LKC, we define truth as follows (with the propositional cases being standard and hence left out):\n(M,Wd) |= \u03d5 iff (M,w) |= \u03d5 for all w \u2208Wd (M,w) |= Ki\u03d5 iff (M,w\u2032) |= \u03d5 for all w\u2032 \u223ci w (M,w) |=C\u03d5 iffM,w\u2032 |= \u03d5 for all w\u2032 \u223c\u2217 w\nwhere \u223c\u2217 is the transitive closure of \u22c3\ni\u2208A \u223ci.\nExample 1. LetA= {Anne,Bob} and P = {m}, where m is intended to express that the key is under the door mat. Consider the following global state s = (M,w), where the nodes represent worlds, the edges represent the indistinguishability relations (reflexive edges left out), and is used for designated worlds:\ns = (M,w) = w : m v :\nBob\nEach node is labeled by the name of the world, and the list of atomic propositions true at the world. The state represents a situation where the key is under the door mat (m is true at the actual world w), but Bob considers it possible that it isn\u2019t (m is not true at the world v indistinguishable from w by Bob). We can verify that in this state Anne knows that the key is under the mat, Bob doesn\u2019t, and Anne knows that he doesn\u2019t: s |= KAnnem\u2227\u00acKBobm\u2227KAnne\u00acKBobm. The fact that Bob does not know the key to be under the mat can also be expressed in terms of local states. Bob\u2019s local perspective on the state s is his associated local state sBob = (M,{w,v}). We have sBob 6|= m, signifying that from Bob\u2019s perspective, m can not be verified."}, {"heading": "2.2 Perspective Shifts", "text": "In general, given a global state s, the associated local state si will represent agent i\u2019s internal perspective on that state. Going from si to (si) j amounts to a perspective shift, where agent i\u2019s perspective on the perspective of agent j is taken. In Example 1, Anne\u2019s perspective on the state s is sAnne, which is s itself. Bob\u2019s perspective is sBob = (M,{w,v}). When Anne wants to reason about the available actions to Bob, e.g. whether he will be able to take the key from under the door mat or not, she will have to shift to his perspective, i.e. reason about what holds true in (sAnne)Bob, which is the same as sBob in this case. This type of perspective shift is going to be central in our notion of implicitly coordinated plans, since it is essential to the ability of an agent to reason about other agents\u2019 possible contributions to a plan from their own perspective. As the introductory example shows, this ability is essential: If Anne can not reason about Bob\u2019s contribution to the overall plan from his own perspective, she will not realize that she needs to call him to let him know where the key is.\nNote that any local state s induces a unique set of global states, Globals(s), and that we can hence choose to think of s as a compact representation of the \u201cbelief state\u201d Globals(s) over global states.\nWe have the following basic properties concerning perspective shifts (associated local states), where the third follows directly from the two first:\nProposition 1. Let s be a state, i \u2208 A and \u03d5 \u2208 LKC.\n1. si |= \u03d5 iff s |= Ki\u03d5 .\n2. If s is local for agent i then si = s.\n3. If s is local for agent i then s |= \u03d5 iff s |= Ki\u03d5 ."}, {"heading": "2.3 Dynamic Language and Epistemic Actions", "text": "To model actions, like announcing locations of keys and picking them up, we use the event models of DEL.\nDefinition 4. An event model is E = \u3008E,(\u223ci)i\u2208A,pre,post\u3009 where\n\u2022 The domain E is a non-empty finite set of events.\n\u2022 \u223ci \u2286 E\u00d7E is an equivalence relation called the indistinguishability relation for agent i.\n\u2022 pre : E\u2192LKC assigns a precondition to each event.\n\u2022 post : E \u2192LKC assigns a postcondition to each event. For all e \u2208 E, post(e) is a conjunction of literals (atomic propositions and their negations, including >).\nFor Ed \u2286 E, the pair (E ,Ed) is called an epistemic action (or simply an action), and the events in Ed are called the designated events. An action is called global if Ed = {e} for some event e, and we then often write (E ,e) instead of (E ,{e}). Similar to states, (E ,Ed) is called a local action for agent i when Ed is closed under \u223ci.\nEach event of an epistemic action represents a different possible outcome. By using multiple events e,e\u2032 \u2208 E that are indistinguishable (i.e. e \u223ci e\u2032), it is possible to obfuscate the outcomes for some agent i \u2208A, i.e. modeling partially observable actions. Using event models with |Ed |> 1, it is also possible to model sensing actions (where a priori, multiple outcomes are considered possible), and nondeterministic actions [8].\nThe product update is used to specify the successor state resulting from the application of an action in a state.\nDefinition 5. Let a state s = (M,Wd) and an action a = (E ,Ed) be given withM= \u3008W,(\u223ci)i\u2208A,V \u3009 and E = \u3008E,(\u223ci)i\u2208A,pre,post\u3009. Then the product update of s with a is s\u2297a = (\u3008W \u2032,(\u223c\u2032i)i\u2208A,V \u2032\u3009 ,W \u2032d) where\n\u2022 W \u2032 = {(w,e) \u2208W \u00d7E | M,w |= pre(e)}\n\u2022 \u223c\u2032i = {((w,e),(w\u2032,e\u2032)) \u2208W \u2032\u00d7W \u2032 | w\u223ci w\u2032 and e\u223ci e\u2032} \u2022 V \u2032(p) = {(w,e) \u2208W \u2032 | post(e) |= p or (M,w |= p and post(e) 6|= \u00acp)}\n\u2022 W \u2032d = {(w,e) \u2208W \u2032 | w \u2208Wd and e \u2208 Ed}.\nExample 2. Consider the following epistemic action try-take = (E ,{e, f}), using the same conventions as for epistemic models, except each event e is labeled by \u3008pre(e),post(e)\u3009:\ntry-take = (E ,{e, f}) = e : \u3008m,h\u2227\u00acm\u3009 f : \u3008\u00acm,>\u3009\nAnne\nIt represents the action of Bob attempting to take the key from under the mat. The event e = \u3008m,h\u2227\u00acm\u3009 represents that if the key is indeed under the mat (the precondition m is true), then the result will be that Bob holds the key and it is no longer under the mat (the postcondition h\u2227\u00acm becomes true). The event f = \u3008\u00acm,>\u3009 represents that if the key is not under the mat, nothing will happen (the postcondition is the trivial one, >). Note the indistinguishability edge for Anne: She is not there to see whether the action is successful or not. Note however that she is still is aware that either e or f happens, so the action represents the situation where she knows that he is attempting to take the key, but not necessarily whether he is successful (except if she already either knows m or knows \u00acm).\nLetting s denote the state from Example 1, we can calculate the result of executing try-take in s:\ns\u2297 try-take = (w,e) : h (v, f ) :\nNote that the result is for Bob to have the key and for this to be common knowledge among Anne and Bob (s\u2297 try-take |= Ch). So it seems that if we assume initially to be in the state s, and want to find a plan to achieve h, then the simple plan consisting only of the action try-take should work. It is, however, a bit more complicated than that. Let us assume that Bob does strong planning, that is, only looks for plans that are guaranteed to reach the goal. The problem is then that, from Bob\u2019s perspective, try-take can not be guaranteed to reach the goal. This is formally because we have:\nsBob\u2297 try-take = (w,e) : h (v, f ) :\nBoth worlds being designated, but distinguishable, means that Bob at plan time (before executing the action) considers them both as possible outcomes of try-take, but is aware that he will at run time (after having executed the action) know which one it is (see [8] for a more full discussion of the plan time/run time distinction). Since the world (v, f ) is designated, we have sBob\u2297 try-take 6|= h. So from Bob\u2019s perspective, try-take might fail to produce h and is hence not a strong plan to achieve h. Intuitively, it is of course simply because he does not, at plan time, know whether the key is under the mat or not.\nSince sAnne = s and s\u2297 try-take |= h, it might seem that try-take is still a strong plan to achieve h from the perspective of Anne. But in fact, it is not, at least not of the implicitly coordinated type we will define below. The issue is, try-take is still an action that Bob has to execute, but Anne knows that Bob does not know it to be succesful, and she can therefore not expect him to execute it. The idea is that when Anne comes up with a plan that involves actions of Bob, she should change her perspective to his, in order to find out what he can be expected to do. Technically speaking, it is because (sAnne)Bob\u2297 try-take 6|= h that the plan is not implicitly coordinated from the perspective of Anne.\nWe extend the language LKC into the dynamic language LDEL by adding a modality [(E ,e)] for each global action (E ,e). The truth conditions are extended with the following standard clause from DEL: (M,w) |= [(E ,e)]\u03d5 iff (M,w) |= pre(e) implies (M,w)\u2297 (E ,e) |= \u03d5 .\nWe define the following abbreviations:\n[(E ,Ed)]\u03d5 := \u2227 e\u2208Ed [(E ,e)]\u03d5 and \u3008(E ,Ed)\u3009\u03d5 := \u00ac[(E ,Ed)]\u00ac\u03d5.\nWe say that an action (E ,Ed) is applicable in a state (M,Wd) if for all w\u2208Wd there is an event e\u2208 Ed s.t. (M,w) |= pre(e). Intuitively, an action is applicable in a state if for each possible situation (designated world), at least one possible outcome (designated event) is specified.\nExample 3. Consider again the state s from Example 1 and the action try-take from Example 2. The action try-take is trivially seen to be applicable in the state s, since the designated event e has its precondition satisfied in the designated world w. The action try-take is also applicable in sBob, since (M,w) |= pre(e) and (M,v) |= pre( f ). This shows that try-take is applicable from the perspective of Bob. Intuitively, it is so because it is only an action for attempting to take the key. Even if the key is not under the mat, the action will specify an outcome (having the trivial postcondition >).\nLet s = (M,Wd) denote an epistemic state and a = (E ,Ed) an action. Andersen [3] shows that a is applicable in s iff s |= \u3008a\u3009>, and that s |= [a]\u03d5 iff s\u2297 a |= \u03d5 . We now define a further abbreviation: ((a))\u03d5 := \u3008a\u3009>\u2227 [a]\u03d5 . Hence:\ns |= ((a))\u03d5 iff a is applicable in s and s\u2297a |= \u03d5 (1)\nThus ((a))\u03d5 means that the application of a is possible and will (necessarily) lead to a state fulfilling \u03d5 ."}, {"heading": "3 Cooperative Planning", "text": "As mentioned in the introduction, in this paper we assume each action to be executable by a single agent, that is, we are not considering joint actions. In our \u2018apartment borrowing\u2019 example there are two agents, Anne and Bob. They are supposed to execute different parts of the plan to reach the goal of Bob getting access to the apartment. For instance, Anne is the one to put the key under the mat, and Bob is the one to take it from under the mat. To represent who performs which actions of a plan, we will introduce what we call an owner function (inspired by the approach of Lo\u0308we, Pacuit, and Witzel [22]). An owner function is defined to be a mapping \u03c9 : A\u2192A, mapping each action to the agent who can execute it. For\nany action a \u2208 A, we call \u03c9(a) the owner of a. Note that by defining the owner function this way, every action has a unique owner. This can be done without loss of generality, since we can always include any number of semantically equivalent actions in A, if we wish some action to be executable by several agents (e.g., if we want both Anne and Bob to be able to open and close the door). We can now define epistemic planning tasks.\nDefinition 6. A cooperative planning task (or simply a planning task) \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 consists of an initial epistemic state s0, a finite set of epistemic actions A, an owner function \u03c9 : A\u2192A, and a goal formula \u03b3 of LKC. Each a \u2208 A has to be local for \u03c9(a). When s0 is a global state, we call it a global planning task. When s0 is local for agent i, we call it a planning task for agent i. Given a planning task \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 and a state s, we define \u03a0(s) := \u3008s,A,\u03c9,\u03b3\u3009. Given a planning task \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009, the associated planning task for agent i is \u03a0i = \u03a0(si).\nGiven a multi-agent system facing a global planning task \u03a0, each individual agent i is facing the planning task \u03a0i (agent i cannot observe the global initial state s0 directly, only the associated local state s0i).\nIn the following, we will investigate various possible solution concepts for cooperative planning tasks. The solution to a planning task is called a plan. A plan can either be sequential (a sequence of actions) or conditional (a policy). We will first, in Section 3.1, consider the simplest possible type of solution, a standard sequential plan. Then, in Section 3.2, we are going to introduce the more complex notion of a sequential implicitly coordinated plan, and in Section 3.3 this will be generalized to implicitly coordinated policies."}, {"heading": "3.1 Standard Sequential Plans", "text": "The standard notion of a sequential plan in automated planning is as follows (see, e.g., [17]). An action sequence (a1, . . . ,an) is called a (standard sequential) plan if for every i\u2264 n, ai is applicable in the result of executing the action sequence (a1, . . . ,ai\u22121) in the initial state, and when executing the entire sequence (a1, . . . ,an) in the initial state, a state satisfying the goal formula is reached. Let us transfer this notion into the DEL-based setting of this paper. In our setting, the result of executing an action a in a state s is given as s\u2297 a. Hence, the above conditions for (a1, . . . ,an) being a plan can be expressed in the following way in our setting, where s0 denotes the initial state, and \u03b3 the goal formula: for every i \u2264 n, ai is applicable in s0\u2297a1\u2297\u00b7\u00b7 \u00b7\u2297ai\u22121, and s0\u2297a1\u2297\u00b7\u00b7 \u00b7\u2297an |= \u03b3 . Note that by equation (1) above, these conditions are equivalent to simply requiring s0 |= ((a1))((a2)) \u00b7 \u00b7 \u00b7((an))\u03b3 . Hence we get the following definition.\nDefinition 7. Let \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 be a planning task. A standard sequential plan for \u03a0 is a sequence (a1, . . . ,an) of actions from A satisfying s0 |= ((a1))((a2)) \u00b7 \u00b7 \u00b7((an))\u03b3 .\nThis solution concept is equivalent to the one considered in [8]. As the following example shows, it is not sufficiently strong to capture the notion of an \u2018implicitly coordinated plan\u2019 that we are after.\nExample 4. Consider again the scenario of Example 2 where the key is initially under the mat, Bob does not know this, and the goal is for Bob to have the key. The only action available in the scenario is for Bob to attempt to take the key from under the mat. Using the states and actions defined in Examples 1 and 2, we can express this scenario as a cooperative planning task \u03a0= \u3008s0,A,\u03c9,\u03b3\u3009where s0 = s, A= {try-take}, \u03c9(try-take) = Bob, \u03b3 = h. In Example 2, we informally concluded that the plan only consisting of trytake is a strong plan, since it is guaranteed to reach the goal, but that it is not a strong plan from the perspective of Bob. Given our formal definitions, we can now make this precise as follows:\n1. The sequence (try-take) is a standard sequential plan for \u03a0.\n2. The sequence (try-take) is not a standard sequential plan for \u03a0Bob.\nThe first item follows from the fact that try-take is applicable in s (Example 3), and that s\u2297 try-take |= h (Example 2). The second item follows from sBob\u2297 try-take 6|= h (Example 2).\nWe also have that (try-take) is a standard sequential plan for \u03a0Anne, since sAnne = s. This proves that the notion of a standard sequential plan is insufficient for our purposes. If Anne is faced with the planning task \u03a0Anne, she should not be allowed to consider (try-take) as a sufficient solution to the problem. She should be aware that the action try-take is to be executed by Bob, and from Bob\u2019s perspective, (try-take) is not a (strong) solution to the planning task (Item 2 above). So we need a way of explicitly integrating perspective-shifts into our notion of a solution to a planning task, and this is what we will do next."}, {"heading": "3.2 Implicitly Coordinated Sequential Plans", "text": "It follows from Definition 7 that (a1, . . . ,an) is a standard sequential plan for some planning task \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 iff a1, . . . ,an \u2208 A and the formula ((a1)) \u00b7 \u00b7 \u00b7((an))\u03b3 is true in s0. More generally, we can think of a planning notion as being defined via a mapping X that takes a plan \u03c0 and a planning task \u03a0 as parameters, and returns a logical formula X(\u03c0,\u03a0) such that, for all states s, s |= X(\u03c0,\u03a0) iff \u03c0 is a plan for \u03a0(s). In the case of standard sequential plans, it follows directly from Definition 7 that X would be defined like this:\nX((a1, . . . ,an),\u3008s0,A,\u03c9,\u03b3\u3009) = ((a1)) \u00b7 \u00b7 \u00b7((an))\u03b3 (2)\nfor all planning tasks \u3008s0,A,\u03c9,\u03b3\u3009 and all a1, . . . ,an \u2208 A. We now wish to define a similar mapping Y , so that s |= Y (\u03c0,\u03a0) iff \u03c0 is an implicitly coordinated plan for \u03a0(s). Our strategy is to list the natural conditions that Y should satisfy, and then derive the exact definition of Y (and hence implicitly coordinated plans) directly from those. First of all, the empty action sequence, denoted by \u03b5 , should be an implicitly coordinated plan iff it satisfies the goal formula, which is expressed by the following simple condition on Y :\nY (\u03b5,\u3008s0,A,\u03c9,\u03b3\u3009) = \u03b3. (3)\nFor non-empty action sequences, the \u2018apartment borrowing\u2019 example studied above gives us the following insights. If Anne is trying to come up with a plan where one of the steps is to be executed by Bob, then Anne has to make sure that Bob can himself verify his action to be applicable, and that he can himself verify that executing the action will lead to a state where the rest of the plan will succeed. More generally, for an action sequence (a1, . . . ,an) to be considered implicitly coordinated, the owner of the first action a1 has to know that a1 is applicable and will lead to a situation where (a2, . . . ,an) is again an implicitly coordinated plan. This leads us directly to the following condition on Y , for all planning tasks \u3008s0,A,\u03c9,\u03b3\u3009 and all a1, . . . ,an \u2208 A with n\u2265 1:\nY ((a1, . . . ,an),\u3008s0,A,\u03c9,\u03b3\u3009) = K\u03c9(a1)((a1))Y ((a2, . . . ,an),\u3008s0,A,\u03c9,\u03b3\u3009) (4)\nIt is now easy to see that any mapping Y satisfying (3) and (4) must necessarily be defined as follows, for all planning tasks \u3008s0,A,\u03c9,\u03b3\u3009 and all action sequences (a1, . . . ,an) \u2208 A:\nY ((a1, . . . ,an),\u3008s0,A,\u03c9,\u03b3\u3009) = K\u03c9(a1)((a1))K\u03c9(a2)((a2)) \u00b7 \u00b7 \u00b7K\u03c9(an)((an))\u03b3\nThis leads us directly to the following definition.\nDefinition 8. Let \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 be a cooperative planning task. An implicitly coordinated plan for \u03a0 is a sequence (a1, . . . ,an) of actions from A such that:\ns0 |= K\u03c9(a1)((a1))K\u03c9(a2)((a2)) \u00b7 \u00b7 \u00b7K\u03c9(an)((an))\u03b3 (5)\nIf (a1, . . . ,an) is an implicitly coordinated plan for \u03a0i, then it is said to be an implicitly coordinated plan for agent i to the planning task \u03a0.\nNote that the formula used to define implicitly coordinated plans above is uniquely determined by the natural conditions (3) and (4).\nThe following proposition gives a more structural, and semantic, characterization of implicitly coordinated plans. It becomes clear that such plans can be found by performing a breadth-first search over the set of successively applicable actions, shifting the perspective for each state transition to the owner of the respective action. Proposition 2. For a cooperative planning task \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009, a non-empty sequence (a1, . . . ,an) of actions from A is an implicitly coordinated plan for \u03a0 iff a1 is applicable in s \u03c9(a1) 0 and (a2, . . . ,an) is an implicitly coordinated plan for \u03a0(s\u03c9(a1)0 \u2297a1). The proposition can be seen as a semantic counterpart of (4), and is easily proven using (1), Proposition 1 and (5). Example 5. Consider again the planning task \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 of Example 4 with s0 = s, A = {try-take}, \u03c9(try-take) = Bob, \u03b3 = h. In Example 4 we concluded that (try-take) is a standard sequential plan for \u03a0. From Example 2, we have that sBob\u2297 try-take 6|= h, and hence s 6|= K\u03c9(try-take)((try-take))h (using (1) and Proposition 1). This shows that, as expected, (try-take) is not an implicitly coordinated plan for \u03a0.\nIn the introduction, we noted that the solution to this problem would be for Anne to make sure to announce the location of the key to Bob. Let us now treat this formally within the developed framework. We need to give Anne the possibility of announcing the location of the key, so we define a new planning task \u03a0\u2032 = \u3008s0,A\u2032,\u03c9,\u03b3\u3009 with A\u2032 = {try-take,announce}. Here announce is the action e : \u3008m,>\u3009 with \u03c9(announce) = Anne. In DEL, this action is known as a public announcement of m. It can now easily be formally verified that s |= KAnne((announce))KBob((try-take))h. In words: Anne knows that she can announce the location of the key, and that this will lead to a situation where Bob knows he can attempt to take the key, and he knows that he will be successful in this attempt. In other words, (announce, try-take) is indeed an implicitly coordinated plan to achieve that Bob has the key, consistent with our informal analysis in the introduction of the paper. Example 6. Consider a situation with agents A= {1,2,3} where a letter is to be passed from agent 1 to one of the other two agents, possibly via the third agent. Mutually exclusive propositions at-1,at-2,at-3\u2208 P are used to denote the current carrier of the letter, while for-1, for-2, for-3 \u2208 P denote the addressee. In our example, agent 1 has a letter for agent 3, so at-1 and for-3 are initially true.\ns0 = at-1, for-2 at-1, for-3\n2,3\nIn s0, all agents know that agent 1 has the letter (at-1), but agents 2 and 3 do not know who of them is the addressee (for-2 or for-3). We assume that agent 1 can only exchange letters with agent 2 and agent 2 can only exchange letters with agent 3. We thus define the four possible actions a12,a21,a23,a32, with ai j being the composite action of agent i publicly passing the letter to agent j and privately informing him about the correct addressee (the name of the addressee is on the envelope, but only visible to the receiver). I.e.\nai j = \u3008at-i\u2227 for-2,\u00acat-i\u2227at- j\u3009 \u3008at-i\u2227 for-3,\u00acat-i\u2227at- j\u3009\nA\\{ j}\nGiven that the joint goal is to pass a letter to its addressee, the global planning task then is \u03a0= \u3008s0,A,\u03c9,\u03b3\u3009 with A = {a12,a21,a23,a32}, \u03c9(ai j) = i for all i, j, and \u03b3 = \u2227 i\u2208{1,2,3} (for-i\u2192 at-i). Consider the action sequence (a12,a23): Agent 1 passes the letter to agent 2, and agent 2 passes it on to agent 3. It can now be verified that s01 |= K1((a12))K2((a23))\u03b3 and s0i 6|= K1((a12))K2((a23))\u03b3 for i = 2,3. Hence (a12,a23) is an implicitly coordinated plan for agent 1, but not for agents 2 and 3. This is because in the beginning agents 2 and 3 do not know to whom of them the letter is intended and hence cannot verify that (a12,a23) will lead to a goal state. However, after agent 1\u2019s execution of a12, agent 2 can distinguish between the possible addressees at run time, and find his subplan (a23), as contemplated by agent 1."}, {"heading": "3.3 Conditional Plans", "text": "Sequential plans are often not sufficient to solve a given epistemic planning task. In particular, as soon as branching on nondeterministic action outcomes or obtained observations becomes necessary, we need conditional plans to solve such a task. Unlike Andersen, Bolander, and Jensen [4], who represent conditional plans as action trees with branches depending on knowledge formula conditions, we represent them as policy functions (\u03c0i)i\u2208A, where each \u03c0i maps minimal local states of agent i into actions of agent i. We now define two different types of policies, joint policies and global policies, and later show them to be equivalent.\nDefinition 9 (Joint policy). Let \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 be a cooperative planning task. Then a joint policy (\u03c0i)i\u2208A consists of partial functions \u03c0i : Smini \u2192 A satisfying for all states s and actions a: if \u03c0i(s) = a then \u03c9(a) = i and a is applicable in s. Definition 10 (Global policy). Let \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 be a cooperative planning task. Then a global policy \u03c0 is a mapping \u03c0 : Sgl\u2192P(A) satisfying the requirements knowledge of preconditions (KOP), per-agent determinism (DET), and uniformity (UNIF):\n(KOP) For all s \u2208 Sgl, a \u2208 \u03c0(s): a is applicable in s\u03c9(a). (DET) For all s \u2208 Sgl,a,a\u2032 \u2208 \u03c0(s) with \u03c9(a) = \u03c9(a\u2032): a = a\u2032. (UNIF) For all s, t \u2208 Sgl,a \u2208 \u03c0(s) with s\u03c9(a) = t\u03c9(a): a \u2208 \u03c0(t). Proposition 3. Any joint policy (\u03c0i)i\u2208A induces a global policy \u03c0 given by\n\u03c0(s) = { \u03c0i(si) | i \u2208 A and \u03c0i(si) is defined } .\nConversely, any global policy \u03c0 induces a joint policy (\u03c0i)i\u2208A given by\n\u03c0i(si) = a for all (s,A\u2032) \u2208 \u03c0, a \u2208 A\u2032 with \u03c9(a) = i.\nFurthermore, the two mappings (\u03c0i)i\u2208A 7\u2192 \u03c0 (mapping joint policies to induced global policies) and \u03c0 7\u2192 (\u03c0i)i\u2208A (mapping global policies to induced joint policies) are each other\u2019s inverse.\nProof. First we prove that the induced mapping \u03c0 as defined above is a global policy. Condition (KOP): If a \u2208 \u03c0(s) then \u03c0i(si) = a for some i, and by definition of joint policy this implies a is applicable in si. Condition (DET): Assume a,a\u2032 \u2208 \u03c0(s) with \u03c9(a) = \u03c9(a\u2032). By definition of \u03c0 we have \u03c0i(si) = a and \u03c0 j(s j) = a\u2032 for some i, j. By definition of joint policy, \u03c9(a) = i and \u03c9(a\u2032) = j. Since \u03c9(a) = \u03c9(a\u2032) we get i = j and hence \u03c0i(si) = \u03c0 j(s j). This implies a = a\u2032. Condition (UNIF): Assume a \u2208 \u03c0(s) and s\u03c9(a) = t\u03c9(a). By definition of \u03c0 and joint policy, we get \u03c0i(si) = a for i = \u03c9(a). Thus si = t i, and\nsince \u03c0i(si) = a, we immediately get \u03c0i(t i) = a and hence a \u2208 \u03c0(t). We now prove that the induced mappings (\u03c0i)i\u2208A defined above form a joint policy. Constraint (KOP) ensures the applicability property as required by Definition 9, while the constraints (DET) and (UNIF) ensure the right-uniqueness of each partial function \u03c0i. It is easy to show that the two mappings (\u03c0i)i\u2208A 7\u2192 \u03c0 and \u03c0 7\u2192 (\u03c0i)i\u2208A are each other\u2019s inverse, using their definition.\nBy Proposition 3, we can identify joint and global policies, and will in the following move back and forth between the two. Notice that Definitions 9 and 10 allow a policy to distinguish between modally equivalent states. A more sophisticated definition avoiding this is possible, but is beyond the scope of this paper. Usually, a policy \u03c0 is only considered to be a solution to a planning task if it is closed in the sense that \u03c0 is defined for all non-goal states reachable following \u03c0 . Here, we want to distinguish between two different notions of closedness: one that refers to all states reachable from a centralized perspective, and one that refers to all states considered reachable when tracking perspective shifts. To that end, we distinguish between centralized and perspective-sensitive successor functions.\nWe take a successor function to be any function \u03c3 : Sgl\u00d7A\u2192P(Sgl). Successor functions are intended to map pairs of states s and actions a into the states \u03c3(s,a) that can result from executing a in s. Which states can result from executing a in s depend on whether we take the objective, centralized view, or whether we take the subjective view of an agent. An agent might subjectively consider more outcomes possible than are objectively possible. We define the centralized successor function as \u03c3cen(s,a) = Globals(s\u2297 a). It specifies the global states that are possible after the application of a in s. If closedness of a global policy \u03c0 based on the centralized successor function is required, then no execution of \u03c0 will ever lead to a non-goal state where \u03c0 is undefined. Like for sequential planning, we are again interested in the decentralized scenario where each agent has to plan and decide when and how to act by himself under incomplete knowledge. We achieve this by encoding the perspective shifts to the next agent to act in the perspective-sensitive successor function \u03c3ps(s,a) = Globals(s\u03c9(a)\u2297 a). Unlike \u03c3cen(s,a), \u03c3ps(s,a) considers a state s\u2032 to be a successor of s after application of a if agent \u03c9(a) considers s\u2032 possible after the application of a, not only if s\u2032 is actually possible from a global perspective. Thus, \u03c3cen(s,a) is always a (possibly strict) subset of \u03c3ps(s,a), and a policy \u03c0ps that is closed wrt. \u03c3ps(s,a) must be defined for at least the states for which a policy \u03c0cen that is closed wrt. \u03c3cen(s,a) must be defined. This corresponds to the intuition that solution existence for decentralized planning with implicit coordination is a stronger property than solution existence for centralized planning. For both successor functions, we can now formalize what a strong solution is that can be executed by the agents. Our notion satisfies the usual properties of strong plans [11], namely closedness, properness and acyclicity.\nDefinition 11 (Strong Policy). Let \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 be a cooperative planning task and \u03c3 a successor function. A global policy \u03c0 is called a strong policy for \u03a0 with respect to \u03c3 if\n(i) Finiteness: \u03c0 is finite. (ii) Foundedness: for all s \u2208 Globals(s0), (1) s |= \u03b3 , or (2) \u03c0(s) 6= /0.\n(iii) Closedness: for all (s,A\u2032) \u2208 \u03c0 , a \u2208 A\u2032,s\u2032 \u2208 \u03c3(s,a), (1) s\u2032 |= \u03b3 , or (2) \u03c0(s\u2032) 6= /0.\nNote that we do not explicitly require acyclicity, since this is already implied by a literal interpretation of the product update semantic that ensures unique new world names after each update. It then follows from (i) and (iii) that \u03c0 is proper. We call strong plans with respect to \u03c3cen centralized policies and strong plans with respect to \u03c3ps implicitly coordinated policies.\nAnalogous to Proposition 2, we want to give a semantic characterization of implicitly coordinated policies. For this, we first define a successor of a state s0 by following a policy \u03c0 to be a state s \u2208 Globals(s\u20320\u2297a) for arbitrary states s\u20320 \u2208 Globals(s0) and arbitrary actions a \u2208 \u03c0(s\u20320). We can then show that if \u03c0 is an implicitly coordinated policy for a planning task \u03a0, each successor state s of the initial\nstate s0 either will already be a goal state, or there will be some agent i \u2208 A who can find an implicitly coordinated policy for his own associated planning task prescribing an action for himself.\nProposition 4. Let \u03c0 be an implicitly coordinated policy for a planning task \u03a0 = \u3008s0,A,\u03c9,\u03b3\u3009 and let s be a non-goal successor state of s0 by following \u03c0 . Then there is an agent i \u2208 A such that \u03c0(s) contains at least one of agent i\u2019s actions and \u03c0 is an implicitly coordinated policy of \u03a0(si).\nProof. The existence of an action a \u2208 \u03c0(s) with some owner i follows directly from the closedness of implicitly coordinated policies. We need to show that \u03c0 is also implicitly coordinated for \u03a0(si). Finiteness and closedness of \u03c0 still hold for \u03a0(si), since \u03c0 was already finite and closed for \u03a0, and this does not change when replacing s0 with si. For foundedness of \u03c0 for \u03a0(si), we have to show that \u03c0 is defined and returns a nonempty set of actions for all global states s\u2032 \u2208 Globals(si). For s itself, we already known that a \u2208 \u03c0(s). By uniformity, since all such s\u2032 are indistinguishable from s for agent i, \u03c0 must assign the same action a to all states s\u2032 \u2208 Globals(si).\nExample 7. Consider again the letter passing problem introduced in Example 6. Let s0,2 and s0,3 denote the global states that are initially considered possible by agent 2.\ns0,2 = at-1, for-2 at-1, for-3\n2,3 s0,3 =\nat-1, for-2 at-1, for-3\n2,3\nWith s1,3 = s0,3\u2297a12, a policy for agent 2 is given by \u03c01 = {s0,3 7\u2192 a12,s0,2 7\u2192 a12} ,\u03c02 = {s1,3 7\u2192 a23} . After the contemplated application of a12 by agent 1 (in both cases), agent 2 can distinguish between s1,2 = s0,2\u2297 a12, where the goal is already reached and nothing has to be done, and s1,3, where agent 2 can apply a23, leading directly to the goal state s1,3\u2297a23. Thus, \u03c0 is an implicitly coordinated policy for \u03a02. While in the sequential case, agent 2 has to wait for the first action a12 of agent 1 to be able to find its subplan, it can find the policy (\u03c0i)i\u2208A in advance by explicitly planning for a run-time distinction.\nIn general, strong policies can be found by performing an AND-OR search, where AND branching corresponds to branching over different epistemic worlds and OR branching corresponds to branching over different actions. By considering modally equivalent states as duplicates and thereby transforming the procedure into a graph search, space and time requirements can be reduced, although great care has to be taken to deal with cycles correctly."}, {"heading": "4 Experiments", "text": "We implemented a planner that is capable of finding implicitly coordinated plans and policies1, and conducted two experiments: one small case study of the Russian card problem [12] intended to show how this problem can be modeled and solved from an individual perspective, and one experiment investigating the scaling behavior of our approach on private transportation problems in the style of Examples 6 and 7, using instances of increasing size."}, {"heading": "4.1 Russian Card Problem", "text": "In the Russian card problem, seven cards numbered 0, . . . ,6 are randomly dealt to three agents. Alice and Bob get three cards each, while Eve gets the single remaining card. Initially, each agent only knows its own cards. The task is now for Alice and Bob to inform each other about their respective cards using only public announcements, without revealing the holder of any single card to Eve. The problem\n1Our code can be downloaded at https://gkigit.informatik.uni-freiburg.de/tengesser/planner\nwas analyzed and solved from the global perspective by van Ditmarsch et al. [14], and a given protocol was verified from an individual perspective by A\u030agotnes et al. [1]. We want to solve the problem from the individual perspective of agent Alice and find an implicitly coordinated policy for her. To keep the problem computationally feasible, we impose some restrictions on the resulting policy, namely that the first action has to be Alice truthfully announcing five possible alternatives for her own hand, and that the second one has to be Bob announcing the card Eve is holding. Without loss of generality, we fix one specific initial hand for Alice, namely 012. From a plan for this initial hand, plans for all other initial hands can be obtained by renaming. For simplicity, we only generate applicable actions for Alice, i.e. announcements that include her true hand 012. This results in the planning task having a total of 46376 options for the first action, and 7 for the second action. Still, the initial state s0 consists of 140 worlds, one for each possible deal of cards. Agents can only distinguish worlds where their own hands differ. Alice\u2019s designated worlds in her associated local state of s0 are those four worlds in which she holds hand 012.\nOur planner needs approximately two hours and 600MB of memory to come up with a solution policy. In the solution, Alice first announces her hand to be one of 012, 034, 156, 236, and 245. It can be seen that each of the five hands other than the true hand 012 contains at least one of Alice\u2019s and one of Bob\u2019s cards, meaning that Bob will immediately be able to identify the complete deal. Also, Eve stays unaware of the individual cards of Alice and Bob since she will be able to rule out exactly two of the hands, with each of Alice and Bob\u2019s cards being present and absent in at least one of the remaining hands. Afterwards, Alice can wait for Bob to announce that Eve has either card 3, 4, 5 or 6."}, {"heading": "4.2 Mail Instances", "text": "Our second experiment concerns the letter passing problem from Examples 6 and 7. We generalized the scenario to allow an arbitrary number of agents with an arbitrary undirected neighborhood graph, indicating which agents are allowed to directly pass letters to each other. As neighborhood graphs, we used randomly generated Watts-Strogatz small-world networks [30], exhibiting characteristics that can also be found in social networks. Watts-Strogatz networks have three parameters: The number N of nodes (determining the number of agents in our setting), the average number K of neighbors per node (roughly determining the average branching factor of a search for a plan), and the probability \u03b2 of an edge being a \u201crandom shortcut\u201d instead of a \u201clocal connection\u201d (thereby influencing the shortest path lengths between agents). We only generate connected networks in order to guarantee plan existence.\nWe distinguish between the example domains MAILTELL and MAILCHECK. To guarantee plan existence, in both scenarios the actions are modeled such as to ensure that the letter position remains common knowledge among the agents in all reachable states. The mechanics of MAILTELL directly correspond to those given in Example 6. There is only one type of action, publicly passing the letter to a neighboring agent while privately informing him about the final addressee. This allows for sequential implicitly coordinated plans. In the resulting plans, letters are simply moved along a shortest path to the addressee. In contrast, in MAILCHECK, an agent that has the letter can only check if he himself is the addressee or not using a separate action (without learning the actual addressee if it is not him). To ensure plan existence in this scenario, we allow an agent to pass on the letter only if it is destined for someone else. Unlike in MAILTELL, conditional plans are required here. In a solution policy, the worst-case sequence of successively applied actions contains an action passing the letter to each agent at least once. As soon as the addressee has been reached, execution is stopped.\nExperiments were conducted for both scenarios with different parameters (Table 1). For finding sequential as well as conditional plans, our implementation uses breadth-first search over a regular graph\nand over an AND-OR graph, respectively. For each set of parameters, 100 trials were performed. For MAILTELL, direct path denotes the average shortest path length between sender and addressee, while for MAILCHECK, full path denotes the average length of a shortest path passing through all agents starting from the sender.\nWhile the shortest path length between sender and addressee grows very slowly with the number of agents (due to the shortcut connections in the network), the shortest path passing through all agents roughly corresponds to the number of agents. Since these measures directly correspond to the minimal plan lengths, the observed exponential growth of space and time requirements with respect to them (and to the base K) is unsurprising.\nNote also that in both scenarios, the number of agents determines the number of worlds (one for each possible addressee) in the initial state. Since the preconditions of the available actions are mutually exclusive, this constitutes an upper bound on the number of worlds per state throughout the search. Thus we get only a linear overhead in comparison to directly searching the networks for the relevant paths."}, {"heading": "5 Conclusion and Future Work", "text": "We introduced an interesting new cooperative, decentralized planning concept without the necessity of explicit coordination or negotiation. Instead, by modeling all possible communication directly as plannable actions and relying on the ability of the autonomous agents to put themselves into each other\u2019s shoes (using perspective shifts), some problems can be elegantly solved achieving implicit coordination between the agents. We briefly demonstrated an implementation of both the sequential and conditional solution algorithms and its performance on the Russian card problem and two letter passing problems.\nBased on the foundation this paper provides, a number of challenging problems needs to be addressed. First of all, concrete problems (e.g. epistemic versions of established multi-agent planning tasks) need to be formalized with a particular focus on the question of which kinds of communicative actions the agents would need to solve these problems in an implicitly coordinated way. As seen in the MAILTELL benchmark, the dynamic epistemic treatment of a problem does not necessarily lead to more than linear overhead. It will be interesting to identify classes of tractable problems and see how agents cope in a simulated environment. Another issue that is relevant in practice concerns the interplay of the single agents\u2019 individual plans. In our setting, the agents have to plan individually and decide autonomously when and how to act. Also, when it comes to action application, there is no predefined notion of agent precedence. This leads to the possibility of incompatible plans, and in consequence to the necessity for agents having to replan in some cases. While our notion of implicitly coordinated planning explicitly forbids the execution of actions leading to dead-end situations (i.e. non-goal states where there is no implicitly coordinated plan for any of the agents), replanning can still lead to livelocks. Both the conditions leading to livelocks and individually applicable strategies to avoid them need to be investigated."}], "references": [{"title": "A Translation-Based Approach to Contingent Planning", "author": ["Alexandre Albore", "Hector Palacios", "Hector Geffner"], "venue": "Proc. IJCAI", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Towards Theory-of-Mind agents using Automated Planning and Dynamic Epistemic Logic", "author": ["Mikkel Birkegaard Andersen"], "venue": "Ph.D. thesis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Undecidability in Epistemic Planning", "author": ["Guillaume Aucher", "Thomas Bolander"], "venue": "Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Symbolic Model Checking for Dynamic Epistemic Logic", "author": ["Johan van Benthem", "Jan van Eijck", "Malvin Gattinger", "Kaile Su"], "venue": "Proceedings of the 5th International Workshop on Logic, Rationality, and Interaction (LORI", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Epistemic planning for single and multi-agent systems", "author": ["Thomas Bolander", "Mikkel Birkegaard Andersen"], "venue": "Journal of Applied Non-Classical Logics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Continual planning and acting in dynamic multiagent environments", "author": ["Michael Brenner", "Bernhard Nebel"], "venue": "Autonomous Agents and Multi-Agent Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Comparing variants of strategic ability: how uncertainty and memory influence general properties of games", "author": ["Nils Bulling", "Wojciech Jamroga"], "venue": "Autonomous Agents and Multi-Agent Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Weak, Strong, and Strong Cyclic Planning via Symbolic Model Checking", "author": ["Alessandro Cimatti", "Marco Pistore", "Marco Roveri", "Paolo Traverso"], "venue": "Artificial Intelligence", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "The Russian Cards Problem", "author": ["Hans P. van Ditmarsch"], "venue": "Studia Logica 75(1),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Dynamic Epistemic Logic", "author": ["Hans P. van Ditmarsch", "Wiebe van der Hoek", "Barteld Kooi"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Distributed Problem Solving and Planning", "author": ["Edmund H. Durfee"], "venue": "ACAI", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Dynamic epistemic modelling. CWI. Software Engineering [SEN", "author": ["Jan van Eijck"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Automated Planning: Theory and Practice", "author": ["Malik Ghallab", "Dana S. Nau", "Paolo Traverso"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Strategic Planning through Model Checking of ATL Formulae", "author": ["Wojciech Jamroga"], "venue": "ICAISC, pp. 879\u2013884,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Constructive knowledge: what agents can achieve under imperfect information", "author": ["Wojciech Jamroga", "Thomas \u00c5gotnes"], "venue": "Journal of Applied Non-Classical Logics", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Agents that Know How to Play", "author": ["Wojciech Jamroga", "Wiebe van der Hoek"], "venue": "Fundam. Inform", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Beliefs in Multiagent Planning: From One Agent to Many", "author": ["Filippos Kominis", "Hector Geffner"], "venue": "Proc. ICAPS", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "DEL planning and some tractable cases", "author": ["Benedikt L\u00f6we", "Eric Pacuit", "Andreas Witzel"], "venue": "LORI", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Planning over Multi-Agent Epistemic States: A Classical Planning Approach", "author": ["Christian Muise", "Vaishak Belle", "Paolo Felli", "Sheila McIlraith", "Tim Miller", "Adrian R. Pearce", "Liz Sonenberg"], "venue": "Proc. AAAI", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Distributed Heuristic Forward Search for Multi-agent Planning", "author": ["Raz Nissim", "Ronen I. Brafman"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Compiling Uncertainty Away in Conformant Planning Problems with Bounded Width", "author": ["H\u00e9ctor Palacios", "Hector Geffner"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "A Knowledge-Based Approach to Planning with Incomplete Information and Sensing", "author": ["Ronald P.A. Petrick", "Fahiem Bacchus"], "venue": "Proc. AIPS", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Extending the Knowledge-Based Approach to Planning with Incomplete Information and Sensing", "author": ["Ronald P.A. Petrick", "Fahiem Bacchus"], "venue": "Proc. ICAPS", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Does the chimpanzee have a theory of mind", "author": ["David Premack", "Guy Woodruff"], "venue": "Behavioral and Brain Sciences", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1978}, {"title": "Collective dynamics of \u2019small-world", "author": ["Duncan J. Watts", "Steven H. Strogatz"], "venue": "networks. Nature 393(6684),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}], "referenceMentions": [{"referenceID": 4, "context": "In contrast, building upon the epistemic planning framework by Bolander and Andersen [8], we propose a decentralized planning notion in which each agent has to individually reason about the entire problem and autonomously decide when and how to (inter-)act.", "startOffset": 85, "endOffset": 88}, {"referenceID": 23, "context": "If Anne has the ability to take Bob\u2019s perspective (she has a Theory of Mind concerning Bob [29]), Anne should of course be able to foresee this problem, and realize that her plan can not be expected to be successful.", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "Our work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and L\u00f6we, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 4, "context": "Our work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and L\u00f6we, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13].", "startOffset": 157, "endOffset": 160}, {"referenceID": 17, "context": "Our work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and L\u00f6we, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 9, "context": "Our work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and L\u00f6we, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13].", "startOffset": 280, "endOffset": 284}, {"referenceID": 0, "context": "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of \u201cnested\u201d [8] or \u201cshallow\u201d knowledge states [26, 27, 28].", "startOffset": 112, "endOffset": 123}, {"referenceID": 16, "context": "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of \u201cnested\u201d [8] or \u201cshallow\u201d knowledge states [26, 27, 28].", "startOffset": 112, "endOffset": 123}, {"referenceID": 18, "context": "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of \u201cnested\u201d [8] or \u201cshallow\u201d knowledge states [26, 27, 28].", "startOffset": 112, "endOffset": 123}, {"referenceID": 4, "context": "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of \u201cnested\u201d [8] or \u201cshallow\u201d knowledge states [26, 27, 28].", "startOffset": 162, "endOffset": 165}, {"referenceID": 21, "context": "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of \u201cnested\u201d [8] or \u201cshallow\u201d knowledge states [26, 27, 28].", "startOffset": 196, "endOffset": 208}, {"referenceID": 22, "context": "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of \u201cnested\u201d [8] or \u201cshallow\u201d knowledge states [26, 27, 28].", "startOffset": 196, "endOffset": 208}, {"referenceID": 4, "context": "Since compilation approaches to classical planning can only deal with bounded nesting of knowledge (or belief), similar to Bolander and Andersen [8], we use search in the space of epistemic states to find a solution.", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "One of the important features that distinguishes our work from more traditional multi-agent planning [9] is the explicit notion of perspective shifts needed for agents to reason about the possible plan contributions of other agents\u2014and hence needed to achieve implicit coordination.", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "Our concepts can be considered related to recent work in temporal epistemic logics [10, 19, 20], which addresses a question similar to ours, namely what groups of agents can jointly achieve under imperfect information.", "startOffset": 83, "endOffset": 95}, {"referenceID": 14, "context": "Our concepts can be considered related to recent work in temporal epistemic logics [10, 19, 20], which addresses a question similar to ours, namely what groups of agents can jointly achieve under imperfect information.", "startOffset": 83, "endOffset": 95}, {"referenceID": 15, "context": "Our concepts can be considered related to recent work in temporal epistemic logics [10, 19, 20], which addresses a question similar to ours, namely what groups of agents can jointly achieve under imperfect information.", "startOffset": 83, "endOffset": 95}, {"referenceID": 4, "context": "We first briefly recapitulate the foundations of DEL, following the conventions of Bolander and Andersen [8].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "Using event models with |Ed |> 1, it is also possible to model sensing actions (where a priori, multiple outcomes are considered possible), and nondeterministic actions [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "Both worlds being designated, but distinguishable, means that Bob at plan time (before executing the action) considers them both as possible outcomes of try-take, but is aware that he will at run time (after having executed the action) know which one it is (see [8] for a more full discussion of the plan time/run time distinction).", "startOffset": 262, "endOffset": 265}, {"referenceID": 1, "context": "Andersen [3] shows that a is applicable in s iff s |= \u3008a\u3009>, and that s |= [a]\u03c6 iff s\u2297 a |= \u03c6 .", "startOffset": 9, "endOffset": 12}, {"referenceID": 17, "context": "To represent who performs which actions of a plan, we will introduce what we call an owner function (inspired by the approach of L\u00f6we, Pacuit, and Witzel [22]).", "startOffset": 154, "endOffset": 158}, {"referenceID": 12, "context": ", [17]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 4, "context": "This solution concept is equivalent to the one considered in [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "Our notion satisfies the usual properties of strong plans [11], namely closedness, properness and acyclicity.", "startOffset": 58, "endOffset": 62}, {"referenceID": 8, "context": "We implemented a planner that is capable of finding implicitly coordinated plans and policies1, and conducted two experiments: one small case study of the Russian card problem [12] intended to show how this problem can be modeled and solved from an individual perspective, and one experiment investigating the scaling behavior of our approach on private transportation problems in the style of Examples 6 and 7, using instances of increasing size.", "startOffset": 176, "endOffset": 180}, {"referenceID": 24, "context": "As neighborhood graphs, we used randomly generated Watts-Strogatz small-world networks [30], exhibiting characteristics that can also be found in social networks.", "startOffset": 87, "endOffset": 91}], "year": 2017, "abstractText": "Epistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities. Recently, Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. We extend the DEL-based epistemic planning framework to include perspective shifts, allowing us to define new notions of sequential and conditional planning with implicit coordination. With these, it is possible to solve planning tasks with joint goals in a decentralized manner without the agents having to negotiate about and commit to a joint policy at plan time. First we define the central planning notions and sketch the implementation of a planning system built on those notions. Afterwards we provide some case studies in order to evaluate the planner empirically and to show that the concept is useful for multi-agent systems in practice.", "creator": "LaTeX with hyperref package"}}}