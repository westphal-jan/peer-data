{"id": "1703.02375", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2017", "title": "Graph sketching-based Space-efficient Data Clustering", "abstract": "In this paper, we address the problem of recovering arbitrary-shaped data clusters from massive datasets. We present DBMSTClu a new density-based non-parametric method working on a limited number of linear measurements i.e. a sketched version of the similarity graph $G$ between the $N$ objects to cluster. Unlike $k$-means, $k$-medians or $k$-medoids algorithms, it does not fail at distinguishing clusters with particular structures. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. DBMSTClu as a graph-based technique relies on the similarity graph $G$ which costs theoretically $O(N^2)$ in memory. However, our algorithm follows the dynamic semi-streaming model by handling $G$ as a stream of edge weight updates and sketches it in one pass over the data into a compact structure requiring $O(\\operatorname{poly} \\operatorname{log} (N))$ space. Thanks to the property of the Minimum Spanning Tree (MST) for expressing the underlying structure of a graph, our algorithm successfully detects the right number of non-convex clusters by recovering an approximate MST from the graph sketch of $G$. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets.", "histories": [["v1", "Tue, 7 Mar 2017 13:43:45 GMT  (1467kb,D)", "https://arxiv.org/abs/1703.02375v1", null], ["v2", "Thu, 23 Mar 2017 13:11:52 GMT  (1624kb,D)", "http://arxiv.org/abs/1703.02375v2", null], ["v3", "Mon, 4 Sep 2017 07:58:04 GMT  (5526kb,D)", "http://arxiv.org/abs/1703.02375v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["anne morvan", "krzysztof choromanski", "c\\'edric gouy-pailler", "jamal atif"], "accepted": false, "id": "1703.02375"}, "pdf": {"name": "1703.02375.pdf", "metadata": {"source": "CRF", "title": "Graph sketching-based Space-efficient Data Clustering", "authors": ["Anne Morvan", "Krzysztof Choromanski", "C\u00e9dric Gouy-Pailler", "Jamal Atif"], "emails": ["anne.morvan@cea.fr."], "sections": [{"heading": "1 Introduction", "text": "Clustering is one of the principal data mining tasks consisting in grouping related objects in an unsupervised manner. It is expected that objects belonging to the same cluster are more similar to each other than to objects belonging to different clusters. There exists a variety of algorithms performing this task. Methods like k-means (Lloyd [1982]), k-medians (Jain and Dubes [1988]) or k-medoids (Kaufman and Rousseeuw [1987]) are useful unless the number and the shape of clusters are unknown which is unfortunately often the case in real-world applications. They are typically unable to find clusters with a non-convex shape. Although DBSCAN (Ester et al. [1996]) does not have these disadvantages, its resulting clustering still depends on the chosen parameter values.\nOne of the successful approaches relies on a graph representation of the data. Given a set of N data points {x1, . . . , xN}, a graph can be built based on the dissimilarity of data where points of the dataset are the vertices and weighted edges express distances between these objects. Besides, the dataset can be already a graph G modeling a network in many fields, such as bioinformatics - where gene-activation dependencies are described through a network - or social, computer, information, transportation network\n\u2217To whom correspondence should be adressed: anne.morvan@cea.fr. Partly supported by the Direction Ge\u0301ne\u0301rale de l\u2019Armement (French Ministry of Defense).\nar X\niv :1\n70 3.\n02 37\n5v 3\n[ cs\n.L G\n] 4\nanalysis. The clustering task consequently aims at detecting clusters as groups of nodes that are densely connected with each other and sparsely connected to vertices of other groups. In this context, Spectral Clustering (SC) (Nascimento and de Carvalho [2011]) is a popular tool to recover clusters with particular structures for which classical k-means algorithm fails. When dealing with large scale datasets, a main bottleneck of the technique is to perform the partial eigendecomposition of the associated graph Laplacian matrix, though. An other inherent difficulty is to handle the huge number of nodes and edges of the induced dissimilarity graph: storing all edges can cost up to O(N2) where N is the number of nodes. Over the last decade, it has been established that the dynamic streaming model (Muthukrishnan [2005]) associated with linear sketching techniques (Ahn et al. [2012a]) - also suitable for distributed processing -, is a good way for tackling this last issue. The addressed problem in this paper falls within the frame of storage limits allowing O(N polylog(N)) space complexity but not O(N2).\nContributions The new clustering algorithm DBMSTClu presented in this paper brings a solution to the following issues: 1) detecting arbitrary-shaped data clusters, 2) with no parameter, 3) in a spaceefficient manner by working on a limited number of linear measurements, a sketched version of the dissimilarity graph G. To deal with the space constraints, DBMSTClu follows the dynamic semi-streaming model by handling G as a stream of edge weight updates and sketches it in only one pass over the data into a compact structure taking O(N polylog(N)) space. Thanks to the property of the Minimum Spanning Tree (MST) for expressing the underlying structure of a graph, DBMSTClu automatically identifies the right number of non-convex clusters by recovering an approximate MST from the graph sketch of G.\nThe remaining of this paper is organized as follows. In Section 2 the related work about graph clustering, sketching and other space-efficient clustering algorithms is described. Section 3 gives fundamentals of the sketching technique that can be used to obtain an approximate MST of the dissimilarity graph. DBMSTClu (DB for Density-Based), the proposed MST-based algorithm for clustering is then explained in Section 4. Section 5 presents the experimental results comparing the proposed clustering algorithm to other existing methods. Finally, Section 6 concludes the work and discusses future directions."}, {"heading": "2 Related work", "text": "General graph clustering The approach of graph representation of the data leads to the extensive literature over graph clustering - related to graph partitioning and community detection - (Schaeffer [2007]). A widely used method to detect communities is the Edge Betweenness Clustering (Girvan and Newman [2002]) which is a hierarchical divisive algorithm removing iteratively edges with the highest edge betweenness. This method can take into account the edge weights but has a high time complexity: O(|E|2|V |) where |E| and |V | denote respectively the cardinality of the set of edges E and vertices V . From the clustering methods point of view, DenGraph (Falkowski et al. [2007]) proposes a graph version of DBSCAN which is able to deal with noise while work from Ailon et al. [2013] focuses on the problem of recovering clusters with considerably dissimilar sizes. Recent works include also approaches from convex optimization using low-rank decomposition of the adjacency matrix (Oymak and Hassibi [2011], Chen et al. [2012a,b, 2014a,b]). These methods bring theoretically guarantees about the exact recovery of the ground truth clustering for the Stochastic Block Model (Holland et al. [1983], Condon and Karp [2001], Rohe et al. [2011]) but demand to compute the eigendecomposition of a N \u00d7N matrix (resp. O(N3) and O(N2) for time and space complexity). Moreover they are restricted to unweighted graphs (weights in the work from Chen et al. [2014b] are about uncertainty of existence of an edge, not a distance between points).\nMST-based graph clustering The MST is known to help recognizing clusters with arbitrary shapes. Clustering algorithms from this family identify clusters by performing suitable cuts among the MST edges. The first one Standard Euclidean MST (SEMST) is from Zahn [1971] and given a number of expected clusters, consists in deleting the heaviest edges from the Euclidean MST of the considered graph but this completely fails when the intra-cluster distance is lower than the inter-clusters one. For decades since\nMST-based clustering methods (Asano et al. [1988], Grygorash et al. [2006]) have been developed and can be classified into the group of density-based methods. MSDR (Grygorash et al. [2006]) relies on the mean and the standard deviation of edge weights within clusters but will encourage clusters with points far from each other as soon as they are equally \u201dfar\u201d. Moreover, it does not handle clusters with less than three points. In practice, MST-based clustering algorithms have been successfully applied in bioinformatics (Xu et al. [2002]) and image color segmentation (Grygorash et al. [2006]).\nGraph sketching The work from Ahn et al. [2012a] is at the origin of graph sketches study by giving dynamic semi-streaming methods for recovering different properties of graphs (MST, connectivity, bipartiteness, etc.) and has lead to numerous works since, with among others Ahn et al. [2012b, 2013], Bhattacharya et al. [2015], Huang and Peng [2016], Bandyopadhyay et al. [2016].\nSpace-efficient clustering algorithms Streaming k-means (Ailon et al. [2009]) is a one-pass streaming method for the k-means problem but still fails to detect clusters with non-convex shape since only the centroid point of each cluster is stored. This is not the case of CURE algorithm (Guha et al. [2001]) which represents each cluster as a random sample of data points contained in it but this offline method has a prohibitive time complexity of O(N2 log(N)) not suitable for large datasets. More time-efficient, CluStream (Aggarwal et al. [2003]) and DenStream (Cao et al. [2006]) create microclusters based on local densities in an online fashion and aggregate them later to build bigger clusters in offline steps. Though, only DenStream can capture non-spherical clusters but needs parameters like DBSCAN from which it is inspired."}, {"heading": "3 Context, notations and graph sketching preprocessing step", "text": "Consider a dataset with N points. Either the underlying network already exists, or it is assumed that a dissimilarity graph G between points can be built where points of the dataset are the vertices and weighted edges express distances between these objects. For instance, this can be the Euclidean distance. In both cases, the graph on which the proposed DBMSTClu algorithm is applied should follow this definition:\nDefinition 3.1 (graph G = (V, E)) A graph G = (V, E) consists in a set of vertices or nodes V and a set of edges E \u2286 V \u00d7 V . No attributes are assigned to nodes or edges. The graph is undirected but weighted. The weight w on an edge between node i and j - if this edge exists - corresponds to the normalized predefined distance between i and j, s.t. 0 < w \u2264 1. |V | = N and |E| stand respectively for the cardinality of sets V and E. E = {e1, ..., eM} and for all edge ei is assigned a weight wi representing a distance between two vertices. In the sequel, E(G) is used to describe the set of edges of a graph G.\nOur streaming clustering algorithm is divided into two main steps: 1) From the stream of edge weights, a sketch of the graph is built and then an approximate MST is retrieved exclusively from it. 2) Freely of any parameter, the nodes clustering phase is finally performed from the recovered approximate MST. Note that the two steps are independent. The use of the graph sketching technique is motivated here to give a space-efficient trick to compute a MST (which is approximate) but another method can be used instead since DBMSTClu takes as input only a MST."}, {"heading": "3.1 Streaming graph sketching and recovery of an approximate MST", "text": "Processing data in the dynamic streaming model (Muthukrishnan [2005]) for graph sketching implies the following: 1) The graph should be handled as a stream s of edge weight updates: s = (a1, ... aj , ...) where aj is the j-th update in the stream corresponding to the tuple aj = (i, wold,i, \u2206wi) with i denoting the index of the edge to update, wold,i its previous weight and \u2206wi the update to perform. Thus, after reading aj in the stream, the i-th edge is assigned the new weight wi = wold,i + \u2206wi \u2265 0. 2) The method should make only one pass over this stream . 3) Edges can be both inserted or deleted (turnstile model),\ni.e weights can be increased or decreased (but have always to stay positive). So weights change regularly, as in social networks where individuals can be friends for some time then not anymore.\nAlgorithm from Ahn et al. [2012a] satisfies these conditions and is used here to produce in an online fashion a limited number of linear measurements summarizing edge weights of G, as new data aj are read from the stream s of edge weight updates. Its general principle is briefly described here. For a given small 1, G is seen as a set of unweighted subgraphs Gk containing all the edges with weight lower than (1 + 1)\nk, hence Gk \u2282 Gk+1. The Gk are embodied as N virtual vectors v(i) \u2208 {\u22121, 0, 1}M for i \u2208 [N ]1 expressing for each node the belonging to an existing edge: for j \u2208 [M ], 0 if node i is not in ej , 1 (resp. \u22121) if ej exists and i is its left (resp. right) node. All v(i) are described at L different \u201dlevels\u201d, i.e. L copies of the true vectors are made with some entries randomly set to zero s.t. the v(i),l get sparser as the corresponding level l \u2208 [L] increases. The v(i),l for each level are explicitly coded in memory by three counters: \u03c6 = \u2211M j=1 v (i),l j ; \u03b9 = \u2211M j=1 j v (i),l j ; \u03c4 = \u2211M j=1 v (i),l j z\nj mod p, with p a suitably large prime and z \u2208 Zp. The resulting compact data structure further named sketch enables to draw almost uniformly at random a nonzero weighted edge among Gk at any time among the levels vectors v\n(i),l which are 1-sparse (with exactly one nonzero coefficient) thanks to `0-sampling (Cormode and Firmani [2014]):\nDefinition 3.2 (`0-sampling) An ( , \u03b4) `0-sampler for a nonzero vector x \u2208 Rn fails with a probability at most \u03b4 or returns some i \u2208 [n] with probability (1\u00b1 ) 1| supp x| where suppx = {i \u2208 [n] | xi 6= 0}.\nThe sketch requires O(N polylog(N)) space (or more precisely O(N log3N)). It follows that the sketching is technically semi-streamed but in practice only one pass over the data is needed and the space cost is significantly lower than the theoretical O(N2) bound. Note that the time cost for each update of the sketch is polylog(N).\nIn the same work, Ahn et al. [2012a] also proposed an algorithm to compute approximately in a single-pass the weight of a MST by appropriate samplings from the sketch with O(N polylog(N)) time complexity. Here an extended method is applied for obtaining rather an approximate MST - and not simply its weight - by registering edges as they are sampled. Referring to the proof of Lemma 3.4 in Ahn et al. [2012a], the approach is simply justified by applying Kruskal\u2019s algorithm where edges with lower weights are first sampled.\nNote that the term MST is kept in the whole paper for the sake of simplicity, but the sketching technique so our algorithm also enables to recover a Minimum Spanning Forest if the initial graph is disconnected."}, {"heading": "4 The proposed MST-based graph clustering method: DBMST-", "text": "Clu"}, {"heading": "4.1 Principle", "text": "Let us consider a dataset with N points. After the sketching phase, an approximate MST further named T has been obtained with N \u2212 1 edges s.t. \u2200i \u2208 [N \u2212 1], 0 < wi \u2264 1. Our density-based clustering method DBMSTClu exclusively relies on this object by performing some cuts among the edges of the tree s.t. K \u2212 1 cuts result in K clusters. Note that independently of the technique used to obtain a MST (the sketching method is just a possible one), the space complexity of the algorithm is O(N) which is better than the O(N2) of SC. The time complexity of DBMSTClu is O(NK) which is clearly less than the O(N3) one implied by SC. After a cut, obtained clusters can be seen as subtrees of the initial T and the analysis of their qualities is only based on edges contained in those subtrees. For all clusters Ci, i \u2208 [K], we denote the corresponding subtree of T , Si. In the sequel, for instance the maximal edge of a cluster will refer to the edge with the maximum weight from the subtree associated to the cluster.\nOur algorithm is a parameter-free divisive top-down procedure (as opposed to an agglomerative clustering algorithm): it starts from one cluster containing the whole dataset and at each iteration, a cut\n1In the sequel, for a given integer a, [a] = {1, . . . , a}.\ncorresponding to the one maximizing some criterion is performed. The criterion used for identifying the best cut to do (if any should be made) at a given stage is a measure of the validity of the resulting clustering partition. This is a function of two positive quantities defined below: Dispersion and Separation of one cluster. The quality of a given cluster is then measured from Dispersion and Separation while the quality of the clustering partition results from the weighted average of all cluster validity indices. Finally, all those measures are based on the value of edge weights and the two latter ones lie between \u22121 and 1.\nDefinition 4.1 (Cluster Dispersion) The Dispersion of a cluster Ci (DISP) represented by the subtree Si of MST T is defined as the maximum edge weight of Si. If the cluster is a singleton (i.e. contains only one node), the associated Dispersion is set to 0. More formally, with Si the subtree of T representing Ci:\n\u2200i \u2208 [K], DISP(Ci) =\n{ max\nj, ej\u2208Si wj if |E(Si)| 6= 0 0 otherwise. (1)\nDefinition 4.2 (Cluster Separation) The Separation of a cluster Ci (SEP) is defined as the minimum distance between the nodes of Ci and the ones of all other clusters Cj , j 6= i, 1 \u2264 i, j,\u2264 K,K 6= 1 where K is the total number of clusters. In practice, it corresponds to the minimum weight among all already cut edges from T comprising a node from Ci. If K = 1, the Separation is set to 1. More formally, with Cuts(Ci) denoting edges incident to Ci,\n\u2200i \u2208 [K], SEP(Ci) =\n{ min\nj, ej\u2208Cuts(Ci) wj if K 6= 1 1 otherwise. (2)\nFig. 1 sums up introduced definitions. The higher the Separation, the farther is the cluster separated from the other clusters, while low values suggest that the cluster is close to the nearest one.\nDefinition 4.3 (Validity Index of a Cluster) The Validity Index of a cluster Ci, 1 \u2264 i \u2264 K is defined as:\nVC(Ci) = SEP(Ci)\u2212DISP(Ci)\nmax(SEP(Ci),DISP(Ci)) (3)\nThe Validity Index of a Cluster (illustration in Fig. 2) is defined s.t. \u22121 \u2264 VC(Ci) \u2264 1 where 1 stands for an optimal validity index and \u22121 for the worst. A division by zero (i.e. max(DISP(Ci),SEP(Ci)) = 0) can never happen because at least Separation is positive. When Dispersion is higher than Separation, \u22121 < VC(Ci) < 0. On the contrary, when Separation is higher than Dispersion, 0 < VC(Ci) < 1. So our clustering algorithm will naturally encourage clusters with a higher Separation over those with a higher Dispersion.\nDefinition 4.4 (Validity Index of a Clustering Partition) The Density-Based Validity Index of a Clustering partition \u03a0 = {Ci}, 1 \u2264 i \u2264 K, DBCVI(\u03a0) is defined as the weighted average of the Validity Indices of all clusters in the partition where N is the number of points in the dataset.\nDBCVI(\u03a0) = K\u2211 i=1 |Ci| N VC(Ci) (4)\nThe Validity Index of Clustering lies also between \u22121 and 1 where 1 stands for an optimal density-based clustering partition while \u22121 stands for the worst one.\nThe previously defined quantities are significantly distinct from the separation and sparseness defined in Moulavi et al. [2014]. Indeed, firstly, their quantities are not well defined for special cases when clusters have less than four nodes or a partition contains only one cluster. Secondly, differentiating internal and external nodes or edges like they do does not enable to properly recover easy clusters like the three convex blobs from Fig. 5. Moreover, our DBCVI differs from the Silhouette Coefficient from Rousseeuw [1987]. Although this is based on close concepts like tightness and also separation, the global coefficient is based on the average values of Silhouette coefficients of each point, while our computation of DBCVI begins at the cluster level.\nDBMSTClu is summarized in Algorithm 1. It starts from a partition with one cluster containing the whole dataset whereas the associated initial DBCVI is set to the worst possible value: \u22121. As long as there exists a cut which makes the DBCVI greater from (or equal to) the one of the current partition, a cut is greedily chosen by maximizing the obtained DBCVI among all the possible cuts. When no direct improvement is possible, the algorithm stops. It is guaranteed that the cut edge maximizes the DBCVI at each iteration since by construction, the greedy algorithm will try each possible cut. In practice, the algorithm stops after a reasonable number of cuts, getting trapped in a local maximum corresponding to a meaningful cluster partition. This prevents from obtaining a partition where all points are in singleton clusters. Indeed, such a result (K = N) is not desirable, although it is optimal in the sense of the DBCVI, since in this case, \u2200i \u2208 [K], DISP(Ci) = 0 and VC(Ci) = 1. Moreover, the non-parametric characteristic helps achieving stable partitions. In Algorithm 1, performCut applies the cut of the edge in parameter w.r.t. already formed clusters. getDBCV I computes the Validation Index of the Clustering partition in parameter."}, {"heading": "4.2 Quality of clusters", "text": "An analysis of the algorithm and the quality of the recovered clusters is given in this section. The main results are the following: 1) DBMSTClu differs significantly from the naive approach of SEMST by preferring cuts which do not necessarily correspond to the heaviest edge (Prop. 1 and 2). 2) As long as the current partition contains at least one cluster with a negative validity index, DBMSTClu will find a\nAlgorithm 1 Clustering algorithm DBMSTClu\n1: Input: T , the MST 2: splitDBCV I \u2190 \u22121.0; cut candidate list\u2190 [ edges(T ) ]; clusters = [ ] 3: while splitDBCV I < 1.0 do 4: temp cut\u2190 None; temp DBCV I \u2190 splitDBCV I 5: for each cut in cut candidate list do 6: newClusters\u2190 performCut(clusters, cut); 7: newDBCV I\u2190getDBCV I(newClusters, T ) 8: if newDBCV I \u2265 temp DBCV I then 9: temp cut\u2190 cut; temp DBCV I \u2190 newDBCV I\n10: if temp cut 6= None then 11: clusters\u2190 performCut(clusters, temp cut); splitDBCV I \u2190 temp DBCV I 12: remove(cut candidate list, temp cut) 13: else 14: break 15: return clusters, splitDBCV I\ncut improving the global index (Prop. 3). 3) Conditions are given to determine in advance if and which cut will be performed in a cluster with a positive validity index (Prop. 4 and 5 ). All are completely independent of the sketching phase. Prop. 1 and 2 rely on the two basic lemmas regarding the first cut in the MST:\nLemma 1 (Case of the edge with the highest weight) Let T be a MST of the dissimilarity data graph. If the first cut performed by DBMSTClu from E(T ) is the heaviest edge, then the resulting DBCVI is positive.\nProof 1 For the first cut, both separations of obtained clusters C1 and C2 are equal to the weight of the considered edge for cut. Here, this is the one with the highest weight. Thus, for i = 1, 2, DISP(Ci) \u2264 SEP(Ci) =\u21d2 VC(Ci) \u2265 0. Finally, the DBCVI of the partition, as a convex sum of two nonnegative quantities, is clearly positive.\nLemma 2 (Case of the edge with the lowest weight) Let T be a MST of the dissimilarity data graph. If the first cut done by DBMSTClu from E(T ) is the one with the lowest weight, then the resulting DBCVI is negative.\nProof 2 Same reasoning in the opposite case s.t. SEP(Ci)\u2212DISP(Ci) \u2264 0 for i \u2208 {1, 2}.\nProposition 1 (When the first cut is not the heaviest) Let T be a MST of the dissimilarity data graph with N nodes. Let us consider the following specific case: all edges have a weight equal to w except two edges e1 and e2 resp. with weight w1 and w2 s.t. w1 > w2 > w > 0. DBMSTClu does not cut any edge with weight w and cuts e2 instead of e1 as a first cut iff:\nw2 > 2n2w1 \u2212 n1 +\n\u221a n21 + 4w1(n 2 2w1 +N\n2 \u2212Nn1 \u2212 n22) 2(N \u2212 n1 + n2)\n(5)\nwhere n1 (resp. n2) is the number of nodes in the first cluster resulting from the cut of e1 (resp. e2). Otherwise, e1 gets cut.\nProof 3 Let DBCV I1 (resp. DBCV I2) be the DBCVI after cut of e1 (resp. e2). As w (resp. w1) is the minimum (resp. maximal) weight, the algorithm does not cut e since the resulting DBCVI would be negative (cf. Lemma 2) while DBCV I1 is guaranteed to be positive (cf. Lemma 1). So, the choice will be between e1 and e2 but e2 gets cut iff DBCV I2 > DBCV I1. DBCV I1 and DBCV I2 expressions are\nsimplified w.l.o.g. by scaling the weights by w s.t. w \u2190 1, w1 \u2190 w1/w, w2 \u2190 w2/w, hence w1 > w2 > 1. Then,\nDBCV I2 > DBCV I1 > 0 \u21d0\u21d2 n2 N ( w2 w1 \u2212 1) + (1\u2212 n2 N )(1\u2212 1 w2 )\n\u2212 n1 N (1\u2212 1 w1 ) + (1\u2212 n1 N )(1\u2212 w2 w1 ) > 0\n\u21d0\u21d2 w22 (N + n2 \u2212 n1)\ufe38 \ufe37\ufe37 \ufe38 a +w2 (n1 \u2212 2n2w1)\ufe38 \ufe37\ufe37 \ufe38 b + (n2 \u2212N)w1\ufe38 \ufe37\ufe37 \ufe38 c<0 > 0.\nClearly, \u2206 = b2 \u2212 4ac is positive and c/a is negative. But w2 > 0, then w2 > \u2212b+ \u221a b2\u22124ac 2a which gives the final result after some simplifications.\nThis proposition emphasizes that the algorithm is cleverer than simply cutting the heaviest edge first. Indeed, although w2 < w1, cutting e2 could be preferred over e1. Moreover, an edge with weight w s.t. w < w2 < w1 can never get cut at the first iteration. This proposition can seem very particular but in practice it really happens as an approximate MST with discrete rounded weights is used.\nProposition 2 (First cut on the heaviest edge in the middle) Let T be a MST of the dissimilarity data graph with N nodes. Let us consider the following specific case: all edges have a weight equal to w except two edges e1 and e2 resp. with weight w1 and w2 s.t. w1 > w2 > w > 0. Denote n1 (resp. n2) the number of nodes in the first cluster resulting from the cut of e1 (resp. e2). In the particular case where edge e1 with maximal weight w1 stands between two subtrees with the same number of points, i.e. n1 = N/2, e1 is always preferred over e2 as the first optimal cut.\nProof 4 A reductio ad absurdum is made by showing that cutting edge e2 i.e. DBCV I2 > DBCV I1 leads to the contradiction w1/w < 1. With the scaling process from Prop. 1\u2019proof:\nDBCV I1 = 1 2 (1\u2212 1 w1 ) + 1 2 (1\u2212 w2 w1 ) = 1\u2212 1 2w1 \u2212 w2 2w1 DBCV I2 = n2 N ( w2 w1 \u2212 1) + (1\u2212 n2 N )(1\u2212 1 w2 ) = 1\u2212 1 w2 + n2 N ( w2 w1 + 1\nw2 \u2212 2\ufe38 \ufe37\ufe37 \ufe38\n=A\n)\nThere is w2 > w = 1, so 1 w2 < 1. Besides w2 < w1 so w2 w1\n< 1 thus, A < 0. Let now consider w.l.o.g. that edge e2 is on the \u201dright side\u201d (right cluster/subtree) of e1 (similar proof if e2 is on the left side of e1). Hence, it is clear that for maximizing DBCV I2 as a function of n2, we need n2 = n1 + 1. Then,\nDBCV I2 > DBCV I1 \u21d0\u21d2 \u2212 1\nw2 + (\n1 2 + 1 N )( w2 w1 \u2212 2 + 1 w2 ) > \u2212 1 w1 \u2212 w2 w1\n\u21d0\u21d2 ( 1 2w1 + 1 Nw1 + 1 2w1 )w2 \u2212 1\u2212 2 N + 1 2w1 + (\u22121 + 1 2 + 1 N ) 1 w2 > 0 \u21d0\u21d2 (1 + 1 N\n)\ufe38 \ufe37\ufe37 \ufe38 a>0\nw22 + w2 ( 1\n2 \u2212 w1(1 +\n2\nN ))\ufe38 \ufe37\ufe37 \ufe38\nb<0\n+w1 ( 1 N \u2212 1\n2 )\ufe38 \ufe37\ufe37 \ufe38\nc<0\n> 0\nAs c/a < 0 and w2 > 0, w2 > N 2(N+1) [ w1(1+ 2 N )\u2212 1 2+ \u221a \u2206 ] with \u2206 = (w1(1+ 2 N )\u2212 1 2 ) 2+4(1+ 1N )( 1 2\u2212 1 N )w1. This inequality is incompatible with w1 > w2 since:\nw1 > w2 \u21d0\u21d2 w1 > N\n2(N + 1) [ w1(1 +\n2 N )\u2212 1 2 + \u221a \u2206 ] \u21d0\u21d2 w1 + 1 2 > \u221a \u2206\n\u21d0\u21d2 4 N w21 (1 + 1 N ) + 4 N w1(\u22121\u2212 1 N ) < 0 \u21d0\u21d2 w1 < 1 : ILLICIT\nIndeed, after the scaling process, w1 < 1 = w is not possible since by hypothesis, w1 > w. Finally, it is not allowed to cut e2, the only remaining possible edge to cut is e1.\nRemark 1 Let us consider the MST in Fig. 3 with N = 8, w1 = 1, w2 = w3 = 1\u2212 , and other weights set to . Clearly, it is not preferred to cut e1 in the middle since for = 0.1, DBCV I2 \u2248 0.27 > DBCV I1 = = 0.1. So, it is a counter-example to a possible generalization of Prop. 2 where there would be more than three possible distinct weights in T .\nb b b b b b b b w1w2\u03f5\u03f5 w3 \u03f5 \u03f5\nFigure 3: Counter-example for Remark 1\nThese last propositions hold for every iteration in the algorithm.\nProposition 3 (Fate of a cluster with a negative validity index) Let K = t+ 1 be the number of clusters in the clustering partition at iteration t. If for some i \u2208 [K] we have VC(Ci) < 0, then DBMSTClu will cut an edge at this stage.\nProof 5 Let i \u2208 [K] s.t. VC(Ci) < 0 i.e. SEP(Ci) < DISP(Ci). We denote wlsep the minimal weight outing cluster Ci and wmax the maximal weight in subtree Si of Ci i.e. SEP(Ci) def = wlsep and DISP(Ci) def = wmax. Hence, w l sep < wmax. By cutting the cluster Ci on the edge with weight wmax, we define C l i and C r i resp. the left and right resulting clusters. Arbitrarily we assume that the separation value (wlsep) comes from the left part of Ci, but the same reasoning would apply for the right side. Fig. 4 gives an illustration of this case. After a cut on the edge with weight wmax, separation of the left resulting cluster C l i remains the same since wlsep < wmax, i.e. SEP(C l i) = SEP(Ci) = w l sep. Then, DISP(C l i) \u2264 DISP(Ci) = wmax (there is equality if there is another edge in Cli with a weight equal to wmax). So, VC(C l i) \u2265 VC(Ci) since the dispersion decreases. Regarding the right side, SEP(Cri ) \u2265 SEP(Ci) (equality if there is an outing edge from Ci with weight equal to wlsep). Moreover, DISP(C r i ) \u2264 DISP(Ci) (equality if there is another edge with weight wmax). Hence VC(C r i ) \u2265 VC(Ci). So in both cases, the validation indices of both clusters are higher than VC(Ci) hence the DBCVI is improved. Finally, by cutting at least the heaviest edge in a cluster with a negative index, the global DBCVI is improved or at least equalized. Therefore, no cluster with a negative index will stay uncut.\nAs a consequence, at the end of the clustering algorithm, each cluster will have a nonnegative cluster validation index. So, at each step of the algorithm, the following bound holds for the final DBCVI:\nDBCV I \u2265 \u2211K i=1 |Ci| N max(VC(Ci), 0).\nProposition 4 (Fate of a cluster with a positive index I) Let T be a MST of the dissimilarity data graph and C a cluster s.t. VC(C) > 0 and SEP(C) = s. Among the edges of the subtree of C, DBMSTClu does not cut an edge e of weight w < s if both resulting clusters have at least one edge with weight greater than w.\nProof 6 Let us consider clusters C1 and C2 resulting from the cut of edge e. Assume that in the associated subtree of C1 (resp. C2), there is an edge e1 (resp. e2) with a weight w1 (resp. w2) higher than w s.t. without loss of generality, w1 > w2. Since VC(C) > 0, s > w1 > w2 > w. But cutting edge e implies that for i \u2208 {1, 2}, DISP(Ci) > SEP(Ci) = w, and thus VC(Ci) < 0. Cutting edge e would therefore mean to replace a cluster C s.t. VC(C) > 0 by two clusters s.t. for i \u2208 {1, 2}, VC(Ci) < 0 which obviously decreases the current DBCVI. Thus, e does not get cut at this step of the algorithm.\nProposition 5 (Fate of a cluster with a positive validity index II) Consider a partition with K clusters s.t. a given cluster Ci, i \u2208 [K] with VC(Ci) > 0 is in the setting of Fig. 4 i.e. if the heaviest edge e with weight wmax gets cut, it gives birth to the left (resp. right) cluster C l i (resp. C r i ) with n1 points (resp. n2) s.t. DISP(C l i) = d1, SEP(C l i) = w l sep, DISP(C r i ) = d2 and SEP(C r i ) = w r sep. We suppose w.l.o.g. that wlsep > w r sep Then, e gets cut iff:(\nn1d1+n2d2 n1+n2 ) wmax \u2264 wmax wrsep .\nProof 7 As VC(Ci) > 0, there is SEP(Ci) = w r sep > wmax. Then, the DBCVI before (K clusters) and after cut of wmax (K + 1 clusters) are:\nDBCV IK = K\u2211 j 6=i VC(Cj) + n1 + n2 N ( 1\u2212 wmax wrsep )\nDBCV IK+1 = K\u2211 j 6=i VC(Cj) + n1 N ( 1\u2212 d1 wmax ) + n2 N ( 1\u2212 d2 wmax ) DBMSTClu cuts wmax iff DBCV IK+1 \u2265 DBCV IK . So the result after simplification."}, {"heading": "5 Experiments", "text": "Synthetic datasets First experiments were performed on three classic synthetic datasets from the Euclidean space: three blobs, noisy circles and noisy moons. Each dataset contains 1000 data points in 20 dimensions: the first two dimensions are randomly drawn from predefined 2D-clusters, as shown in the following figures, while the other 18 dimensions are random Gaussian noise. The results of DBMSTClu are compared with two other algorithms: DBSCAN (Ester et al. [1996]) which competes with DBMSTClu regarding time and space complexities (resp. O(N2) in the worst case and O(N) when the distance matrix is not stored) and a naive approach of a MST-based algorithm. The latter called Standard Euclidean MST (SEMST) (Zahn [1971]) cuts the K \u2212 1 heaviest edges of the standard Euclidean MST given a targeted number of clusters K. For DBMSTClu, a k-NN graph is built from computing the Euclidean distance between data points (k = 550 for noisy circles and moons, k = 350 for the three blobs) in order to speed up the sketching phase by discarding useless heaviest edges. Note that k should not be considered as a parameter of the clustering algorithm since it is used only for speedup purposes. Then this k-NN graph is\npassed into the sketch phase to produce an approximate version of the exact MST. Fig. 5, 6 and 7 show the results when providing to DBMSTClu an approximate MST obtained from the sketch phase. The datasets were projected onto 2-dimensional spaces for helping results visualization. They were produced with a noise level such that SEMST fails and DBSCAN does not perform well without parameters optimization. In particular, for DBSCAN all the cross points correspond to noise. With the three blobs, each method performs well: they all manage to retrieve three clusters. Note that DBSCAN detects one point as noise which is considered as a member of the red cluster for DBMSTClu. With the concentric circles, SEMST does not cut on the consistent edges, hence leads to an isolated cluster with two points in blue. DBSCAN classifies the same points as noise while recovering the two circles well. Finally, DBMSTClu finds the two main clusters and also creates five singleton clusters which can be legitimately considered as noise. With noisy moons, while DBSCAN considers four outliers, DBMSTClu detects only three singletons. As theoretically proved above, experiments emphasize the fact that our algorithm is much more subtle than simply cutting the heaviest edges as the failure of SEMST shows. Moreover our algorithm exhibits an ability to detect outliers, which could be labeled as noise in a postprocessing phase. Another decisive advantage of our algorithm is the absence of any required parameters, contrarily to DBSCAN.\nTable 1 shows quantitative results of the experiments: the achieved silhouette coefficient, Adjusted Rand Index (ARI) and DBCVI for the three datasets (in order blobs, noisy circles and moons). For all the indices, the higher, the better. Silhouette coefficient (between \u22121 and 1) is used to measure a clustering partition without any external information. For DBSCAN it is computed by considering noise points as singletons. We see that this measure is not very suitable for non-convex clusters like noisy circles or moons. The ARI (between 0 and 1) measures the similarity between the experimental clustering partition and the known groundtruth. DBSCAN and DBMSTClu give similar almost optimal results. Finally, the obtained DBCVIs are consistent, since the best ones are reached for DBMSTClu.\nMushroom dataset DBMSTClu performances are also measured on the mushroom dataset. It contains 8124 records of 22 categorical attributes corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota family2. 118 binary attributes are created from the 22 categorical ones, then the complete graph (about 33 millions of edges) is built by computing the Hamming distance (i.e. the number of distinct bits) between points normalized by the dimension 118. Fig. 8 shows the results for the three methods. We see that if suitable parameters are given to SEMST and DBSCAN, they are able to find 23 clusters while DBMSTClu retrieves them without tuning any parameter. The corresponding DBCVI and silhouette coefficient are resp. 0.75 and 0.47."}, {"heading": "6 Conclusion", "text": "In this paper we introduced DBMSTClu a novel space-efficient Density-Based Clustering algorithm which works only on a Minimum Spanning Tree (MST) of the dissimilarity graph G: the spatial cost is O(N) with N the number of data points. What makes the method particularly interesting is the fact that DBMSTClu\n2https://archive.ics.uci.edu/ml/datasets/mushroom\nis non-parametric, unlike most existing clustering methods: it automatically determines the right number of non-convex clusters. Although the approach is fundamentally independent from the sketching phase, its robustness has been assessed by using as input an approximate MST of the sketched G rather than the exact MST itself. The graph sketch is computed dynamically on the fly as new edge weight updates are received in only one pass over the data. Hence, our algorithm adapts to the semi-streaming setting with O(N polylogN) space. Our approach shows promising results, as evidenced by the experimental part. Further work would consist in using this algorithm in privacy issues, as the lost information when sketching might be sufficient for ensuring data privacy. Moreover, as it is already the case for the graph sketching, it would be interesting to adapt both the MST recovery and DBMSTClu to the fully online setting, i.e. to be able to modify dynamically current MST and clustering partition as a new edge weight update from the stream is seen."}, {"heading": "Acknowledgements", "text": "We would like to thank Mario Lucic for the fruitful private conversation and for coauthoring with Krzysztof Choromanski the MSE sketching extension during his internship at Google."}], "references": [{"title": "A framework for clustering evolving data streams", "author": ["C.C. Aggarwal", "J. Han", "J. Wang", "P.S. Yu"], "venue": "In Proceedings of the 29th International Conference on Very Large Data Bases - Volume 29,", "citeRegEx": "Aggarwal et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Aggarwal et al\\.", "year": 2003}, {"title": "Analyzing graph structure via linear measurements", "author": ["K.J. Ahn", "S. Guha", "A. McGregor"], "venue": "In Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Graph sketches: Sparsification, spanners, and subgraphs", "author": ["K.J. Ahn", "S. Guha", "A. McGregor"], "venue": "In Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,", "citeRegEx": "Ahn et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2012}, {"title": "Spectral Sparsification in Dynamic Graph Streams, pages 1\u201310", "author": ["K.J. Ahn", "S. Guha", "A. McGregor"], "venue": null, "citeRegEx": "Ahn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ahn et al\\.", "year": 2013}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Ailon et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2009}, {"title": "Breaking the small cluster barrier of graph clustering", "author": ["N. Ailon", "Y. Chen", "H. Xu"], "venue": "CoRR, abs/1302.4549,", "citeRegEx": "Ailon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2013}, {"title": "Clustering algorithms based on minimum and maximum spanning trees", "author": ["T. Asano", "B. Bhattacharya", "M. Keil", "F. Yao"], "venue": "In Proceedings of the Fourth Annual Symposium on Computational Geometry,", "citeRegEx": "Asano et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Asano et al\\.", "year": 1988}, {"title": "Topological graph sketching for incremental and scalable analytics", "author": ["B. Bandyopadhyay", "D. Fuhry", "A. Chakrabarti", "S. Parthasarathy"], "venue": "In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management, CIKM", "citeRegEx": "Bandyopadhyay et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bandyopadhyay et al\\.", "year": 2016}, {"title": "Space- and time-efficient algorithm for maintaining dense subgraphs on one-pass dynamic streams", "author": ["S. Bhattacharya", "M. Henzinger", "D. Nanongkai", "C. Tsourakakis"], "venue": "In Proceedings of the Forty-seventh Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Bhattacharya et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhattacharya et al\\.", "year": 2015}, {"title": "Density-based clustering over an evolving data stream with noise", "author": ["F. Cao", "M. Ester", "W. Qian", "A. Zhou"], "venue": "SIAM Conference on Data Mining,", "citeRegEx": "Cao et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2006}, {"title": "Clustering sparse graphs", "author": ["Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Clustering sparse graphs", "author": ["Y. Chen", "S. Sanghavi", "H. Xu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Chen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "Clustering partially observed graphs via convex optimization", "author": ["Y. Chen", "A. Jalali", "S. Sanghavi", "H. Xu"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Weighted graph clustering with non-uniform uncertainties", "author": ["Y. Chen", "S.H. Lim", "H. Xu"], "venue": "In Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Random Struct. Algorithms,", "citeRegEx": "Condon and Karp.,? \\Q2001\\E", "shortCiteRegEx": "Condon and Karp.", "year": 2001}, {"title": "A unifying framework for l0-sampling algorithms", "author": ["G. Cormode", "D. Firmani"], "venue": "Distributed and Parallel Databases,", "citeRegEx": "Cormode and Firmani.,? \\Q2014\\E", "shortCiteRegEx": "Cormode and Firmani.", "year": 2014}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise. pages 226\u2013231", "author": ["M. Ester", "H. Kriegel", "J. Sander", "X. Xu"], "venue": null, "citeRegEx": "Ester et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Ester et al\\.", "year": 1996}, {"title": "Dengraph: A density-based community detection algorithm", "author": ["T. Falkowski", "A. Barth", "M. Spiliopoulou"], "venue": "In Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence,", "citeRegEx": "Falkowski et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Falkowski et al\\.", "year": 2007}, {"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M.E.J. Newman"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "Girvan and Newman.,? \\Q2002\\E", "shortCiteRegEx": "Girvan and Newman.", "year": 2002}, {"title": "Minimum spanning tree based clustering algorithms", "author": ["O. Grygorash", "Y. Zhou", "Z. Jorgensen"], "venue": "18th IEEE International Conference on Tools with Artificial Intelligence", "citeRegEx": "Grygorash et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Grygorash et al\\.", "year": 2006}, {"title": "Cure: An efficient clustering algorithm for large databases", "author": ["S. Guha", "R. Rastogi", "K. Shim"], "venue": "Inf. Syst.,", "citeRegEx": "Guha et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Guha et al\\.", "year": 2001}, {"title": "Stochastic blockmodels: First steps", "author": ["P.W. Holland", "K.B. Laskey", "S. Leinhardt"], "venue": "Social Networks,", "citeRegEx": "Holland et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Holland et al\\.", "year": 1983}, {"title": "Dynamic graph stream algorithms in $o(n)$", "author": ["Z. Huang", "P. Peng"], "venue": "space. CoRR,", "citeRegEx": "Huang and Peng.,? \\Q2016\\E", "shortCiteRegEx": "Huang and Peng.", "year": 2016}, {"title": "Algorithms for Clustering Data. Prentice-Hall, Inc", "author": ["A. Jain", "R. Dubes"], "venue": "Upper Saddle River, NJ,", "citeRegEx": "Jain and Dubes.,? \\Q1988\\E", "shortCiteRegEx": "Jain and Dubes.", "year": 1988}, {"title": "Clustering by means of medoids. Statistical Data Analysis Based on the L1\u2013Norm and Related Methods, pages 405\u2013416", "author": ["L. Kaufman", "P. Rousseeuw"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "Kaufman and Rousseeuw.,? \\Q1987\\E", "shortCiteRegEx": "Kaufman and Rousseeuw.", "year": 1987}, {"title": "Density-based clustering validation", "author": ["D. Moulavi", "P.A. Jaskowiak", "R.J.G.B. Campello", "A. Zimek", "J. Sander"], "venue": "In Proceedings of the 2014 SIAM International Conference on Data Mining,", "citeRegEx": "Moulavi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Moulavi et al\\.", "year": 2014}, {"title": "Data streams: Algorithms and applications", "author": ["S. Muthukrishnan"], "venue": "Found. Trends Theor. Comput. Sci.,", "citeRegEx": "Muthukrishnan.,? \\Q2005\\E", "shortCiteRegEx": "Muthukrishnan.", "year": 2005}, {"title": "Spectral methods for graph clustering - a survey", "author": ["M. Nascimento", "A. de Carvalho"], "venue": "European Journal of Operational Research,", "citeRegEx": "Nascimento and Carvalho.,? \\Q2011\\E", "shortCiteRegEx": "Nascimento and Carvalho.", "year": 2011}, {"title": "Finding dense clusters via \u201dlow rank + sparse", "author": ["S. Oymak", "B. Hassibi"], "venue": "decomposition. CoRR,", "citeRegEx": "Oymak and Hassibi.,? \\Q2011\\E", "shortCiteRegEx": "Oymak and Hassibi.", "year": 2011}, {"title": "Spectral clustering and the high-dimensional stochastic blockmodel", "author": ["K. Rohe", "S. Chatterjee", "B. Yu"], "venue": "Ann. Statist., 39(4):1878\u20131915,", "citeRegEx": "Rohe et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rohe et al\\.", "year": 2011}, {"title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis", "author": ["P.J. Rousseeuw"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "Rousseeuw.,? \\Q1987\\E", "shortCiteRegEx": "Rousseeuw.", "year": 1987}, {"title": "Survey: Graph clustering", "author": ["S.E. Schaeffer"], "venue": "Comput. Sci. Rev.,", "citeRegEx": "Schaeffer.,? \\Q2007\\E", "shortCiteRegEx": "Schaeffer.", "year": 2007}, {"title": "Clustering gene expression data using a graph-theoretic approach: an application of minimum spanning trees", "author": ["Y. Xu", "V. Olman", "D. Xu"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2002}, {"title": "Graph-theoretical methods for detecting and describing gestalt clusters", "author": ["C.T. Zahn"], "venue": "IEEE Trans. Comput.,", "citeRegEx": "Zahn.,? \\Q1971\\E", "shortCiteRegEx": "Zahn.", "year": 1971}], "referenceMentions": [{"referenceID": 22, "context": "Methods like k-means (Lloyd [1982]), k-medians (Jain and Dubes [1988]) or k-medoids (Kaufman and Rousseeuw [1987]) are useful unless the number and the shape of clusters are unknown which is unfortunately often the case in real-world applications.", "startOffset": 48, "endOffset": 70}, {"referenceID": 22, "context": "Methods like k-means (Lloyd [1982]), k-medians (Jain and Dubes [1988]) or k-medoids (Kaufman and Rousseeuw [1987]) are useful unless the number and the shape of clusters are unknown which is unfortunately often the case in real-world applications.", "startOffset": 48, "endOffset": 114}, {"referenceID": 16, "context": "Although DBSCAN (Ester et al. [1996]) does not have these disadvantages, its resulting clustering still depends on the chosen parameter values.", "startOffset": 17, "endOffset": 37}, {"referenceID": 23, "context": "Over the last decade, it has been established that the dynamic streaming model (Muthukrishnan [2005]) associated with linear sketching techniques (Ahn et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 1, "context": "Over the last decade, it has been established that the dynamic streaming model (Muthukrishnan [2005]) associated with linear sketching techniques (Ahn et al. [2012a]) - also suitable for distributed processing -, is a good way for tackling this last issue.", "startOffset": 147, "endOffset": 166}, {"referenceID": 19, "context": "General graph clustering The approach of graph representation of the data leads to the extensive literature over graph clustering - related to graph partitioning and community detection - (Schaeffer [2007]).", "startOffset": 189, "endOffset": 206}, {"referenceID": 10, "context": "A widely used method to detect communities is the Edge Betweenness Clustering (Girvan and Newman [2002]) which is a hierarchical divisive algorithm removing iteratively edges with the highest edge betweenness.", "startOffset": 79, "endOffset": 104}, {"referenceID": 10, "context": "From the clustering methods point of view, DenGraph (Falkowski et al. [2007]) proposes a graph version of DBSCAN which is able to deal with noise while work from Ailon et al.", "startOffset": 53, "endOffset": 77}, {"referenceID": 4, "context": "[2007]) proposes a graph version of DBSCAN which is able to deal with noise while work from Ailon et al. [2013] focuses on the problem of recovering clusters with considerably dissimilar sizes.", "startOffset": 92, "endOffset": 112}, {"referenceID": 4, "context": "[2007]) proposes a graph version of DBSCAN which is able to deal with noise while work from Ailon et al. [2013] focuses on the problem of recovering clusters with considerably dissimilar sizes. Recent works include also approaches from convex optimization using low-rank decomposition of the adjacency matrix (Oymak and Hassibi [2011], Chen et al.", "startOffset": 92, "endOffset": 335}, {"referenceID": 4, "context": "[2007]) proposes a graph version of DBSCAN which is able to deal with noise while work from Ailon et al. [2013] focuses on the problem of recovering clusters with considerably dissimilar sizes. Recent works include also approaches from convex optimization using low-rank decomposition of the adjacency matrix (Oymak and Hassibi [2011], Chen et al. [2012a,b, 2014a,b]). These methods bring theoretically guarantees about the exact recovery of the ground truth clustering for the Stochastic Block Model (Holland et al. [1983], Condon and Karp [2001], Rohe et al.", "startOffset": 92, "endOffset": 524}, {"referenceID": 4, "context": "[2007]) proposes a graph version of DBSCAN which is able to deal with noise while work from Ailon et al. [2013] focuses on the problem of recovering clusters with considerably dissimilar sizes. Recent works include also approaches from convex optimization using low-rank decomposition of the adjacency matrix (Oymak and Hassibi [2011], Chen et al. [2012a,b, 2014a,b]). These methods bring theoretically guarantees about the exact recovery of the ground truth clustering for the Stochastic Block Model (Holland et al. [1983], Condon and Karp [2001], Rohe et al.", "startOffset": 92, "endOffset": 548}, {"referenceID": 4, "context": "[2007]) proposes a graph version of DBSCAN which is able to deal with noise while work from Ailon et al. [2013] focuses on the problem of recovering clusters with considerably dissimilar sizes. Recent works include also approaches from convex optimization using low-rank decomposition of the adjacency matrix (Oymak and Hassibi [2011], Chen et al. [2012a,b, 2014a,b]). These methods bring theoretically guarantees about the exact recovery of the ground truth clustering for the Stochastic Block Model (Holland et al. [1983], Condon and Karp [2001], Rohe et al. [2011]) but demand to compute the eigendecomposition of a N \u00d7N matrix (resp.", "startOffset": 92, "endOffset": 568}, {"referenceID": 4, "context": "[2007]) proposes a graph version of DBSCAN which is able to deal with noise while work from Ailon et al. [2013] focuses on the problem of recovering clusters with considerably dissimilar sizes. Recent works include also approaches from convex optimization using low-rank decomposition of the adjacency matrix (Oymak and Hassibi [2011], Chen et al. [2012a,b, 2014a,b]). These methods bring theoretically guarantees about the exact recovery of the ground truth clustering for the Stochastic Block Model (Holland et al. [1983], Condon and Karp [2001], Rohe et al. [2011]) but demand to compute the eigendecomposition of a N \u00d7N matrix (resp. O(N) and O(N) for time and space complexity). Moreover they are restricted to unweighted graphs (weights in the work from Chen et al. [2014b] are about uncertainty of existence of an edge, not a distance between points).", "startOffset": 92, "endOffset": 780}, {"referenceID": 33, "context": "The first one Standard Euclidean MST (SEMST) is from Zahn [1971] and given a number of expected clusters, consists in deleting the heaviest edges from the Euclidean MST of the considered graph but this completely fails when the intra-cluster distance is lower than the inter-clusters one.", "startOffset": 53, "endOffset": 65}, {"referenceID": 6, "context": "MST-based clustering methods (Asano et al. [1988], Grygorash et al.", "startOffset": 30, "endOffset": 50}, {"referenceID": 6, "context": "MST-based clustering methods (Asano et al. [1988], Grygorash et al. [2006]) have been developed and can be classified into the group of density-based methods.", "startOffset": 30, "endOffset": 75}, {"referenceID": 6, "context": "MST-based clustering methods (Asano et al. [1988], Grygorash et al. [2006]) have been developed and can be classified into the group of density-based methods. MSDR (Grygorash et al. [2006]) relies on the mean and the standard deviation of edge weights within clusters but will encourage clusters with points far from each other as soon as they are equally \u201dfar\u201d.", "startOffset": 30, "endOffset": 189}, {"referenceID": 6, "context": "MST-based clustering methods (Asano et al. [1988], Grygorash et al. [2006]) have been developed and can be classified into the group of density-based methods. MSDR (Grygorash et al. [2006]) relies on the mean and the standard deviation of edge weights within clusters but will encourage clusters with points far from each other as soon as they are equally \u201dfar\u201d. Moreover, it does not handle clusters with less than three points. In practice, MST-based clustering algorithms have been successfully applied in bioinformatics (Xu et al. [2002]) and image color segmentation (Grygorash et al.", "startOffset": 30, "endOffset": 542}, {"referenceID": 6, "context": "MST-based clustering methods (Asano et al. [1988], Grygorash et al. [2006]) have been developed and can be classified into the group of density-based methods. MSDR (Grygorash et al. [2006]) relies on the mean and the standard deviation of edge weights within clusters but will encourage clusters with points far from each other as soon as they are equally \u201dfar\u201d. Moreover, it does not handle clusters with less than three points. In practice, MST-based clustering algorithms have been successfully applied in bioinformatics (Xu et al. [2002]) and image color segmentation (Grygorash et al. [2006]).", "startOffset": 30, "endOffset": 597}, {"referenceID": 1, "context": "Graph sketching The work from Ahn et al. [2012a] is at the origin of graph sketches study by giving dynamic semi-streaming methods for recovering different properties of graphs (MST, connectivity, bipartiteness, etc.", "startOffset": 30, "endOffset": 49}, {"referenceID": 1, "context": "Graph sketching The work from Ahn et al. [2012a] is at the origin of graph sketches study by giving dynamic semi-streaming methods for recovering different properties of graphs (MST, connectivity, bipartiteness, etc.) and has lead to numerous works since, with among others Ahn et al. [2012b, 2013], Bhattacharya et al. [2015], Huang and Peng [2016], Bandyopadhyay et al.", "startOffset": 30, "endOffset": 327}, {"referenceID": 1, "context": "Graph sketching The work from Ahn et al. [2012a] is at the origin of graph sketches study by giving dynamic semi-streaming methods for recovering different properties of graphs (MST, connectivity, bipartiteness, etc.) and has lead to numerous works since, with among others Ahn et al. [2012b, 2013], Bhattacharya et al. [2015], Huang and Peng [2016], Bandyopadhyay et al.", "startOffset": 30, "endOffset": 350}, {"referenceID": 1, "context": "Graph sketching The work from Ahn et al. [2012a] is at the origin of graph sketches study by giving dynamic semi-streaming methods for recovering different properties of graphs (MST, connectivity, bipartiteness, etc.) and has lead to numerous works since, with among others Ahn et al. [2012b, 2013], Bhattacharya et al. [2015], Huang and Peng [2016], Bandyopadhyay et al. [2016].", "startOffset": 30, "endOffset": 379}, {"referenceID": 3, "context": "Space-efficient clustering algorithms Streaming k-means (Ailon et al. [2009]) is a one-pass streaming method for the k-means problem but still fails to detect clusters with non-convex shape since only the centroid point of each cluster is stored.", "startOffset": 57, "endOffset": 77}, {"referenceID": 3, "context": "Space-efficient clustering algorithms Streaming k-means (Ailon et al. [2009]) is a one-pass streaming method for the k-means problem but still fails to detect clusters with non-convex shape since only the centroid point of each cluster is stored. This is not the case of CURE algorithm (Guha et al. [2001]) which represents each cluster as a random sample of data points contained in it but this offline method has a prohibitive time complexity of O(N log(N)) not suitable for large datasets.", "startOffset": 57, "endOffset": 306}, {"referenceID": 0, "context": "More time-efficient, CluStream (Aggarwal et al. [2003]) and DenStream (Cao et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 0, "context": "More time-efficient, CluStream (Aggarwal et al. [2003]) and DenStream (Cao et al. [2006]) create microclusters based on local densities in an online fashion and aggregate them later to build bigger clusters in offline steps.", "startOffset": 32, "endOffset": 89}, {"referenceID": 26, "context": "1 Streaming graph sketching and recovery of an approximate MST Processing data in the dynamic streaming model (Muthukrishnan [2005]) for graph sketching implies the following: 1) The graph should be handled as a stream s of edge weight updates: s = (a1, .", "startOffset": 111, "endOffset": 132}, {"referenceID": 1, "context": "Algorithm from Ahn et al. [2012a] satisfies these conditions and is used here to produce in an online fashion a limited number of linear measurements summarizing edge weights of G, as new data aj are read from the stream s of edge weight updates.", "startOffset": 15, "endOffset": 34}, {"referenceID": 1, "context": "Algorithm from Ahn et al. [2012a] satisfies these conditions and is used here to produce in an online fashion a limited number of linear measurements summarizing edge weights of G, as new data aj are read from the stream s of edge weight updates. Its general principle is briefly described here. For a given small 1, G is seen as a set of unweighted subgraphs Gk containing all the edges with weight lower than (1 + 1) , hence Gk \u2282 Gk+1. The Gk are embodied as N virtual vectors v \u2208 {\u22121, 0, 1} for i \u2208 [N ] expressing for each node the belonging to an existing edge: for j \u2208 [M ], 0 if node i is not in ej , 1 (resp. \u22121) if ej exists and i is its left (resp. right) node. All v are described at L different \u201dlevels\u201d, i.e. L copies of the true vectors are made with some entries randomly set to zero s.t. the v get sparser as the corresponding level l \u2208 [L] increases. The v for each level are explicitly coded in memory by three counters: \u03c6 = \u2211M j=1 v (i),l j ; \u03b9 = \u2211M j=1 j v (i),l j ; \u03c4 = \u2211M j=1 v (i),l j z j mod p, with p a suitably large prime and z \u2208 Zp. The resulting compact data structure further named sketch enables to draw almost uniformly at random a nonzero weighted edge among Gk at any time among the levels vectors v (i),l which are 1-sparse (with exactly one nonzero coefficient) thanks to `0-sampling (Cormode and Firmani [2014]): Definition 3.", "startOffset": 15, "endOffset": 1348}, {"referenceID": 1, "context": "Algorithm from Ahn et al. [2012a] satisfies these conditions and is used here to produce in an online fashion a limited number of linear measurements summarizing edge weights of G, as new data aj are read from the stream s of edge weight updates. Its general principle is briefly described here. For a given small 1, G is seen as a set of unweighted subgraphs Gk containing all the edges with weight lower than (1 + 1) , hence Gk \u2282 Gk+1. The Gk are embodied as N virtual vectors v \u2208 {\u22121, 0, 1} for i \u2208 [N ] expressing for each node the belonging to an existing edge: for j \u2208 [M ], 0 if node i is not in ej , 1 (resp. \u22121) if ej exists and i is its left (resp. right) node. All v are described at L different \u201dlevels\u201d, i.e. L copies of the true vectors are made with some entries randomly set to zero s.t. the v get sparser as the corresponding level l \u2208 [L] increases. The v for each level are explicitly coded in memory by three counters: \u03c6 = \u2211M j=1 v (i),l j ; \u03b9 = \u2211M j=1 j v (i),l j ; \u03c4 = \u2211M j=1 v (i),l j z j mod p, with p a suitably large prime and z \u2208 Zp. The resulting compact data structure further named sketch enables to draw almost uniformly at random a nonzero weighted edge among Gk at any time among the levels vectors v (i),l which are 1-sparse (with exactly one nonzero coefficient) thanks to `0-sampling (Cormode and Firmani [2014]): Definition 3.2 (`0-sampling) An ( , \u03b4) `0-sampler for a nonzero vector x \u2208 R fails with a probability at most \u03b4 or returns some i \u2208 [n] with probability (1\u00b1 ) 1 | supp x| where suppx = {i \u2208 [n] | xi 6= 0}. The sketch requires O(N polylog(N)) space (or more precisely O(N logN)). It follows that the sketching is technically semi-streamed but in practice only one pass over the data is needed and the space cost is significantly lower than the theoretical O(N) bound. Note that the time cost for each update of the sketch is polylog(N). In the same work, Ahn et al. [2012a] also proposed an algorithm to compute approximately in a single-pass the weight of a MST by appropriate samplings from the sketch with O(N polylog(N)) time complexity.", "startOffset": 15, "endOffset": 1923}, {"referenceID": 1, "context": "Algorithm from Ahn et al. [2012a] satisfies these conditions and is used here to produce in an online fashion a limited number of linear measurements summarizing edge weights of G, as new data aj are read from the stream s of edge weight updates. Its general principle is briefly described here. For a given small 1, G is seen as a set of unweighted subgraphs Gk containing all the edges with weight lower than (1 + 1) , hence Gk \u2282 Gk+1. The Gk are embodied as N virtual vectors v \u2208 {\u22121, 0, 1} for i \u2208 [N ] expressing for each node the belonging to an existing edge: for j \u2208 [M ], 0 if node i is not in ej , 1 (resp. \u22121) if ej exists and i is its left (resp. right) node. All v are described at L different \u201dlevels\u201d, i.e. L copies of the true vectors are made with some entries randomly set to zero s.t. the v get sparser as the corresponding level l \u2208 [L] increases. The v for each level are explicitly coded in memory by three counters: \u03c6 = \u2211M j=1 v (i),l j ; \u03b9 = \u2211M j=1 j v (i),l j ; \u03c4 = \u2211M j=1 v (i),l j z j mod p, with p a suitably large prime and z \u2208 Zp. The resulting compact data structure further named sketch enables to draw almost uniformly at random a nonzero weighted edge among Gk at any time among the levels vectors v (i),l which are 1-sparse (with exactly one nonzero coefficient) thanks to `0-sampling (Cormode and Firmani [2014]): Definition 3.2 (`0-sampling) An ( , \u03b4) `0-sampler for a nonzero vector x \u2208 R fails with a probability at most \u03b4 or returns some i \u2208 [n] with probability (1\u00b1 ) 1 | supp x| where suppx = {i \u2208 [n] | xi 6= 0}. The sketch requires O(N polylog(N)) space (or more precisely O(N logN)). It follows that the sketching is technically semi-streamed but in practice only one pass over the data is needed and the space cost is significantly lower than the theoretical O(N) bound. Note that the time cost for each update of the sketch is polylog(N). In the same work, Ahn et al. [2012a] also proposed an algorithm to compute approximately in a single-pass the weight of a MST by appropriate samplings from the sketch with O(N polylog(N)) time complexity. Here an extended method is applied for obtaining rather an approximate MST - and not simply its weight - by registering edges as they are sampled. Referring to the proof of Lemma 3.4 in Ahn et al. [2012a], the approach is simply justified by applying Kruskal\u2019s algorithm where edges with lower weights are first sampled.", "startOffset": 15, "endOffset": 2296}, {"referenceID": 25, "context": "The previously defined quantities are significantly distinct from the separation and sparseness defined in Moulavi et al. [2014]. Indeed, firstly, their quantities are not well defined for special cases when clusters have less than four nodes or a partition contains only one cluster.", "startOffset": 107, "endOffset": 129}, {"referenceID": 25, "context": "The previously defined quantities are significantly distinct from the separation and sparseness defined in Moulavi et al. [2014]. Indeed, firstly, their quantities are not well defined for special cases when clusters have less than four nodes or a partition contains only one cluster. Secondly, differentiating internal and external nodes or edges like they do does not enable to properly recover easy clusters like the three convex blobs from Fig. 5. Moreover, our DBCVI differs from the Silhouette Coefficient from Rousseeuw [1987]. Although this is based on close concepts like tightness and also separation, the global coefficient is based on the average values of Silhouette coefficients of each point, while our computation of DBCVI begins at the cluster level.", "startOffset": 107, "endOffset": 534}, {"referenceID": 16, "context": "The results of DBMSTClu are compared with two other algorithms: DBSCAN (Ester et al. [1996]) which competes with DBMSTClu regarding time and space complexities (resp.", "startOffset": 72, "endOffset": 92}, {"referenceID": 16, "context": "The results of DBMSTClu are compared with two other algorithms: DBSCAN (Ester et al. [1996]) which competes with DBMSTClu regarding time and space complexities (resp. O(N) in the worst case and O(N) when the distance matrix is not stored) and a naive approach of a MST-based algorithm. The latter called Standard Euclidean MST (SEMST) (Zahn [1971]) cuts the K \u2212 1 heaviest edges of the standard Euclidean MST given a targeted number of clusters K.", "startOffset": 72, "endOffset": 348}], "year": 2017, "abstractText": "In this paper, we address the problem of recovering arbitrary-shaped data clusters from datasets while facing high space constraints, as this is for instance the case in the Internet of Things environment when analysis algorithms are directly deployed on resources-limited mobile devices collecting the data. We present DBMSTClu a new density-based non-parametric method working on a limited number of linear measurements i.e. a sketched version of the dissimilarity graph G between the N objects to cluster. Unlike k-means, k-medians or k-medoids algorithms, it does not fail at distinguishing clusters with particular structures. No input parameter is needed contrarily to DBSCAN or the Spectral Clustering method. DBMSTClu as a graph-based technique relies on the dissimilarity graph G which costs theoretically O(N) in memory. However, our algorithm follows the dynamic semi-streaming model by handling G as a stream of edge weight updates and sketches it in one pass over the data into a compact structure requiring O(N polylog(N)) space. Thanks to the property of the Minimum Spanning Tree (MST) for expressing the underlying structure of a graph, our algorithm successfully detects the right number of non-convex clusters by recovering an approximate MST from the graph sketch of G. We provide theoretical guarantees on the quality of the clustering partition and also demonstrate its advantage over the existing state-of-the-art on several datasets.", "creator": "LaTeX with hyperref package"}}}