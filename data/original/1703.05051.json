{"id": "1703.05051", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Deep learning with convolutional neural networks for EEG decoding and visualization", "abstract": "Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end-to-end learning, i.e. learning from the raw data. Now, there is increasing interest in using deep ConvNets for end-to-end EEG analysis. However, little is known about many important aspects of how to design and train ConvNets for end-to-end EEG decoding, and there is still a lack of techniques to visualize the informative EEG features the ConvNets learn.", "histories": [["v1", "Wed, 15 Mar 2017 09:52:58 GMT  (8271kb,D)", "http://arxiv.org/abs/1703.05051v1", null], ["v2", "Mon, 10 Jul 2017 11:06:38 GMT  (8271kb,D)", "http://arxiv.org/abs/1703.05051v2", "A revised manuscript (with the new title) has been accepted at Human Brain Mapping, we will provide the URL once it is available online"], ["v3", "Mon, 7 Aug 2017 16:16:08 GMT  (8271kb,D)", "http://arxiv.org/abs/1703.05051v3", "A revised manuscript (with the new title) has been accepted at Human Brain Mapping, seethis http URL"], ["v4", "Tue, 8 Aug 2017 08:48:58 GMT  (8271kb,D)", "http://arxiv.org/abs/1703.05051v4", "A revised manuscript (with the new title) has been accepted at Human Brain Mapping, seethis http URL"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["robin tibor schirrmeister", "jost tobias springenberg", "lukas dominique josef fiederer", "martin glasstetter", "katharina eggensperger", "michael tangermann", "frank hutter", "wolfram burgard", "tonio ball"], "accepted": false, "id": "1703.05051"}, "pdf": {"name": "1703.05051.pdf", "metadata": {"source": "CRF", "title": "Deep learning with convolutional neural networks for brain mapping and decoding of movement-related information from the human EEG", "authors": ["Tibor Schirrmeister", "Jost Tobias Springenberg", "Lukas Dominique Josef Fiederer", "Martin Glasstetter", "Katharina Eggensperger", "Michael Tangermann", "Frank Hutter", "Wolfram Burgard"], "emails": ["tonio.ball@uniklinik-freiburg.de"], "sections": [{"heading": null, "text": "Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end-to-end learning, i.e. learning from the raw data. Now, there is increasing interest in using deep ConvNets for end-to-end EEG analysis. However, little is known about many important aspects of how to design and train ConvNets for end-to-end EEG decoding, and there is still a lack of techniques to visualize the informative EEG features the ConvNets learn.\nHere, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed movements from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching or surpassing that of the widely-used filter bank common spatial patterns (FBCSP) decoding algorithm. While FBCSP is designed to use spectral power modulations, the features used by ConvNets are not fixed a priori. Our novel methods for visualizing the learned features demonstrated that ConvNets indeed learned to use spectral power modulations in the alpha, beta and high gamma frequencies. These methods also proved useful as a technique for spatially mapping the learned features, revealing the topography of the causal contributions of features in different frequency bands to decoding the movement classes.\nOur study thus shows how to design and train ConvNets to decode movement-related information from the raw EEG without handcrafted features and highlights the potential of deep ConvNets combined with advanced visualization techniques for EEG-based brain mapping.\nTonio Ball 1 INTRODUCTION"}, {"heading": "1 Introduction", "text": "Machine-learning techniques allow to extract information from electroencephalographic (EEG) recordings of brain activity and therefore play a crucial role in several important EEG-based research and application areas. For example, machine-learning techniques are a central component of many EEGbased brain-computer interface (BCI) systems for clinical applications. Such systems already allowed, for example, persons with severe paralysis to communicate (Nijboer et al., 2008), to draw pictures (Mu\u0308n\u00dfinger et al., 2010), and to control telepresence robots (Tonin et al., 2011). Such systems may also facilitate stroke rehabilitation (Ramos-Murguialday et al., 2013) and may be used in the treatment of epilepsy (Gadhoumi et al., 2016) (for more examples of potential clinical applications, see Moghimi et al. (2013)). Furthermore, machine-learning techniques for the analysis of brain signals, including the EEG, are increasingly recognized as novel tools for neuroscientific inquiry (Das et al., 2010; Knops et al., 2009; Kurth-Nelson et al., 2016; Stansbury et al., 2013).\nHowever, despite many examples of impressive progress, there is still room for considerable improvement with respect to the accuracy of information extraction from the EEG and hence, an interest in transferring advances from the area of machine learning to the field of EEG decoding and BCI. A recent, prominent example of such an advance in machine learning is the application of convolutional neural networks (ConvNets), particularly in computer vision tasks. Thus, first studies have started to investigate the potential of ConvNets for brain-signal decoding ((Antoniades et al., 2016; Bashivan et al., 2016; Cecotti and Graser, 2011; Hajinoroozi et al., 2016; Lawhern et al., 2016; Liang et al., 2016; Manor et al., 2016; Manor and Geva, 2015; Page et al., 2016; Ren and Wu, 2014; Sakhavi et al., 2015; Shamwell et al., 2016; Stober, 2016; Stober et al., 2014; Sun et al., 2016; Tabar and Halici, 2017; Tang et al., 2017; Thodoroff et al., 2016; Wang et al., 2013), see Supplementary Section A.1 for more details on these studies). Still, several important methodological questions on EEG analysis with ConvNets remain, as detailed below and addressed in the present study.\nConvNets are artificial neural networks that can learn local patterns in data by using convolutions as their key component (also see Section 2.3). ConvNets vary in the number of convolutional layers, ranging from shallow architectures with just one convolutional layer such as in a successful speech recognition ConvNet (Abdel-Hamid et al., 2014) over deep ConvNets with multiple consecutive convolutional layers (Krizhevsky et al., 2012) to very deep architectures with more than 1000 layers as in the case of the recently developed residual networks (He et al., 2015). Deep ConvNets can first extract local, low-level features from the raw input and then increasingly more global and high level features in deeper layers. For example, deep ConvNets can learn to detect increasingly complex visual features (e.g., edges, simple shapes, complete objects) from raw images. Over the past years, deep ConvNets have become highly successful in many application areas, such as in computer vision and speech recognition, often outperforming previous state-of-the-art methods (we refer to LeCun et al. (2015) and Schmidhuber (2015) for recent reviews). For example, deep ConvNets reduced the error rates on the ImageNet image-recognition challenge, where 1.2 million images must be classified into 1000 different classes, from above 26% to below 4% within 4 years (He et al., 2015; Krizhevsky et al., 2012). ConvNets also reduced error rates in recognizing speech, e.g., from English news broadcasts (Sainath et al., 2015a; Sercu et al., 2016); however, in this field, hybrid models combining ConvNets with other machine-learning components, notably recurrent networks, and deep neural networks without convolutions are also competitive (Li and Wu, 2015; Sainath et al., 2015b; Sak et al., 2015). Deep ConvNets also contributed to the spectacular success of AlphaGo, an artificial intelligence that beat the world champion in the game of Go (Silver et al., 2016).\nAn attractive property of ConvNets that was leveraged in many previous applications is that\n1\nTonio Ball 1 INTRODUCTION\nthey are well suited for end-to-end learning, i.e., learning from the raw data without any a priori feature selection. End-to-end learning might be especially attractive in brain-signal decoding, as not all relevant features can be assumed to be known a priori. Hence, in the present study we have investigated how ConvNets of different architectures and designs can be used for end-to-end learning of EEG recorded in human subjects.\nThe EEG signal has characteristics that make it different from inputs that ConvNets have been most successful on, namely images. In contrast to two-dimensional static images, the EEG signal is a dynamic time series from electrode measurements obtained on the three-dimensional scalp surface. Also, the EEG signal has a comparatively low signal-to-noise ratio, i.e., sources that have no taskrelevant information often affect the EEG signal more strongly than the task-relevant sources. These properties could make learning features in an end-to-end fashion fundamentally more difficult for EEG signals than for images. Thus, the existing ConvNets architectures from the field of computer vision need to be adapted for EEG input and the resulting decoding accuracies rigorously evaluated against more traditional feature extraction methods. For that purpose, a well-defined baseline is crucial, i.e., a comparison against an implementation of a standard EEG decoding method validated on published results for that method. In light of this, in the present study we addressed two key questions:\n\u2022 What is the impact of ConvNet design choices (e.g., the overall network architecture or other design choices such as the type of non-linearity used) on the decoding accuracies?\n\u2022 What is the impact of ConvNet training strategies (e.g., training on entire trials or crops within trials) on decoding accuracies?\nTo address these questions, we created three ConvNets with different architectures, with the number of convolutional layers ranging from 2 layers in a \u201cshallow\u201d ConvNet over a 5-layer deep ConvNet up to a 31-layer residual network (ResNet). Additionally, we also created a hybrid ConvNet from the deep and shallow ConvNets. As described in detail in the methods section, these architectures were inspired both from existing \u201cnon-ConvNet\u201d EEG decoding methods, which we embedded in a ConvNet, as well as from previously published successful ConvNet solutions in the image processing domain (for example, the ResNet architecture recently won several image recognition competitions (He et al., 2015). All architectures were adapted to the specific requirements imposed by the analysis of multi-channel EEG data. To address whether these ConvNets can reach competitive decoding accuracies, we performed a statistical comparison of their decoding accuracies to those achieved with decoding based on filter bank common spatial patterns (FBCSP, Ang et al. (2008); Chin et al. (2009)), a method that is widely used in EEG decoding and has won several EEG decoding competitions such as BCI Competition IV 2a and 2b. We analyzed the offline decoding performance on two suitable motor decoding EEG datasets (see Section 2.7 for details). In all cases, we used only minimal preprocessing to conduct a fair end-to-end comparison of ConvNets and FBCSP.\nIn addition to the role of the overall network architecture, we systematically evaluated a range of important design choices. We focussed on alternatives resulting from recent advances in machinelearning research on deep ConvNets. Thus, we evaluated potential performance improvements by using dropout as a novel regularization strategy (Srivastava et al., 2014), intermediate normalization by batch normalization (Ioffe and Szegedy, 2015) or exponential linear units as a recently proposed activation function (Clevert et al., 2016). A comparable analysis of the role of deep ConvNet design choices in EEG decoding is currently lacking.\nIn addition to the global architecture and specific design choices which together define the \u201cstructure\u201dof ConvNets, another important topic that we address is how a given ConvNet should be trained\n2\nTonio Ball 1 INTRODUCTION\non the data. As with architecture and design, there are several different methodological options and choices with respect to the training process, such as the optimization algorithm (e.g., Adam (Kingma and Ba, 2014), Adagrad (Duchi et al., 2011), etc.), or the sampling of the training data. Here, we focused on the latter question of sampling the training data as there is usually, compared to current computer vision tasks with millions of samples, relatively little data available for EEG decoding. Therefore, we evaluated two sampling strategies, both for the deep and shallow ConvNets: training on whole trials or on multiple crops of the trial, i.e., on windows shifted through the trials. Using multiple crops holds promise as it increases the amount of training examples, which has been crucial to the success of deep ConvNets. Using multiple crops has become standard procedure for ConvNets for image recognition (see (He et al., 2015; Howard, 2013; Szegedy et al., 2015), but the usefulness of cropped training has not yet been examined in EEG decoding.\nIn addition to the problem of achieving good decoding accuracies, a growing corpus of research tackles the problem of understanding what ConvNets learn (see Yeager (2016) for a recent overview). This direction of research may be especially relevant for neuroscientists interested in using ConvNets \u2014 insofar as they want to understand what features in the brain signal discriminate the investigated classes. Here we present two novel methods for feature visualization that we used to gain insights into our ConvNet learned from the neuronal data.\nWe concentrated on EEG band power features as a target for visualizations. Based on a large body of literature on movement-related spectral power modulations (Chatrian et al., 1959; Pfurtscheller and Aranibar, 1977, 1978; Pfurtscheller and Berghold, 1989; Pfurtscheller et al., 1994; Toro et al., 1994), we had clear expectations which band power features should be discriminative for the different classes; thus our rationale was that visualizations of these band power features would be particularly useful to verify that the ConvNets are using actual brain signals. Also, since FBCSP uses these features too, they allowed us to directly compare visualizations for both approaches. Our first method can be used to show how much information about a specific feature is retained in the ConvNet in different layers, however it does not evaluate whether the feature causally affects the ConvNet outputs. Therefore, we designed our second method to directly investigate causal effects of the feature values on the ConvNet outputs. With both visualization methods, it is possible to derive topographic scalp maps that either show how much information about the band power in different frequency bands is retained in the outputs of the trained ConvNet or how much they causally affect the outputs of the trained ConvNet.\nAddressing the questions raised above, in summary the main contributions of this study are as follows:\n\u2022 We show for the first time that end-to-end deep ConvNets can reach accuracies at least in the same range as FBCSP for decoding movement-related information from EEG.\n\u2022 We evaluate a large number of ConvNet design choices on an EEG decoding task, and we show that recently developed methods from the field of deep learning such as batch normalization and exponential linear units are crucial for reaching high decoding accuracies.\n\u2022 We show that cropped training can increase the decoding accuracy of deep ConvNets and describe a computationally efficient training strategy to train ConvNets on a larger number of input crops per EEG trial.\n\u2022 We develop and apply novel visualizations that highly suggest that the deep ConvNets learn to use the band power in frequency bands relevant for motor decoding (alpha, beta, gamma) with meaningful spatial distributions.\n3\nTonio Ball 1 INTRODUCTION\nThus, in summary, the methods and findings described in this study pave the way for a widespread application of deep ConvNets for EEG decoding both in clinical applications and neuroscientific research.\n4\nTonio Ball 2 METHODS"}, {"heading": "2 Methods", "text": "We first provide basic definitions with respect to brain-signal decoding as a supervised classification problem used in the remaining Methods section. This is followed by the principles of both filter bank common spatial patterns (FBCSP), the established baseline decoding method referred to throughout the present study, and of convolutional neural networks (ConvNets). Next, we describe the ConvNets developed for this study in detail, including the design choices we evaluated. Afterwards, the training of the ConvNets, including two training strategies, are described. Then we present two novel visualizations of trained ConvNets in Section 2.6. Datasets and preprocessing descriptions follow in Section 2.7. Details about statistical evaluation, software and hardware can be found in Supplementary Sections A.8 and A.9."}, {"heading": "2.1 Definitions and notation", "text": "This section more formally defines how brain-signal decoding can be viewed as a supervised classification problem and includes the notation used to describe the methods."}, {"heading": "2.1.1 Input and labels", "text": "We assume that we are given one EEG data set per subject i. Each dataset is separated into labeled trials (time-segments of the original recording that each belong to one of several classes). Concretely, we are given datasets Di = {(X1, y1), ..., (XNi , yNi)} where Ni denotes the total number of recorded trials for subject i. The input matrix Xj \u2208 RE\u00b7T of trial j, 1 \u2264 j \u2264 Ni contains the preprocessed signals of E recorded electrodes and T discretized time steps recorded per trial.\nThe corresponding class label of trial j is denoted by yj . It takes values from a set of K class labels L that, in our case, correspond to the imagined or executed movements performed in each trial, e.g.: \u2200yj : yj \u2208 L = {l1 = \u201cHand (Left)\u201d, l2 = \u201cHand (Right)\u201d, l3 = \u201cFeet\u201d, l4 = \u201cRest\u201d}."}, {"heading": "2.1.2 Decoder", "text": "The decoder f is trained on these existing trials such that it is able to assign the correct label to new unseen trials. Concretely, we aim to train the decoder to assign the label yj to trial Xj using the output of a parametric classifier f(Xj ; \u03b8) : RE\u00b7T \u2192 L with parameters \u03b8.\nFor the rest of this manuscript we assume that the classifier f(Xj ; \u03b8) is represented by a standard machine-learning pipeline decomposed into two parts: A first part that extracts a (vector-valued) feature representation \u03c6(Xj ; \u03b8\u03c6) with parameters \u03b8\u03c6 \u2014 which could either be set manually (for hand designed features), or learned from the data; and a second part consisting of a classifier g with parameters \u03b8g that is trained using these features, i.e., f(X j ; \u03b8) = g ( \u03c6(Xj ; \u03b8\u03c6), \u03b8g ) .\nAs described in detail in the following sections, it is important to note that FBCSP and ConvNets differ in how they implement this framework: in short, FBCSP has separated feature extraction and classifier stages, while ConvNets learn both stages jointly."}, {"heading": "2.2 Filter bank common spatial patterns (FBCSP)", "text": "FBCSP (Ang et al., 2008; Chin et al., 2009) is a widely-used method to decode oscillatory EEG data, for example, with respect to movement-related information, i.e., the decoding problem we focus on in this study. FBCSP was the best-performing method for the BCI competition IV dataset 2a, which\n5\nTonio Ball 2 METHODS\nwe use in the present study (in the following called BCI Competition Dataset, see Section 2.7 for a short dataset description). FBCSP also won other similar EEG decoding competitions (Tangermann et al., 2012). Therefore, we consider FBCSP an adequate benchmark algorithm for the evaluation of the performance of ConvNets in the present study.\nIn the following, we explain the computational steps of FBCSP. We will refer to these steps when explaining our shallow ConvNet architecture (see Section 2.4.3), as it is inspired by these steps.\nIn a supervised manner, FBCSP computes spatial filters (linear combinations of EEG channels) that enhance class-discriminative band power features contained in the EEG. FBCSP extracts and uses these features \u03c6(Xj ; \u03b8\u03c6) (which correspond to the feature representation part in Section 2.1.2) to decode the label of a trial in several steps (we will refer back to these steps when we explain the shallow ConvNet):\n1. Bandpass filtering: Different bandpass filters are applied to separate the raw EEG signal into different frequency bands.\n2. Epoching: The continuous EEG signal is cut into trials as explained in Section 2.1.1.\n3. CSP computation: Per frequency band, the common spatial patterns (CSP) algorithm is applied to extract spatial filters. CSP aims to extract spatial filters that make the trials discriminable by the power of the spatially filtered trial signal (see Koles et al. (1990); Ramoser et al. (2000); Blankertz et al. (2008) for more details on the computation). The spatial filters correspond to the learned parameters \u03b8\u03c6 in FBCSP.\n4. Spatial filtering: The spatial filters computed in Step 2 are applied to the EEG signal.\n5. Feature construction: Feature vectors \u03c6(Xj ; \u03b8\u03c6) are constructed from the filtered signals: Specifically, feature vectors are the log-variance of the spatially filtered trial signal for each frequency band and for each spatial filter.\n6. Classification: A classifier is trained to predict per-trial labels based on the feature vectors.\nFor details on our FBCSP implementation, see Supplementary Section A.2."}, {"heading": "2.3 Convolutional neural networks", "text": "In the following sections, we first explain the basic ideas of ConvNets. We then describe architectural choices for ConvNets on EEG, including how to represent the EEG input for a ConvNet, the three different ConvNet architectures used in this study and several specific design choices that we evaluated for these architectures. Finally, we describe how to train the ConvNets, including the description of a trial-wise and a cropped training strategy for our EEG data."}, {"heading": "2.3.1 Basics", "text": "Generally, ConvNets combine two ideas useful for many learning tasks on natural signals, such as images and audio signals. These signals often have an inherent hierarchical structure (e.g., images typically consist of edges that together form simple shapes which again form larger, more complex shapes and so on). ConvNets can learn local non-linear features (through convolutions and nonlinearities) and represent higher-level features as compositions of lower level features (through multiple\n6\nTonio Ball 2 METHODS\nlayers of processing). In addition, many ConvNets use pooling layers which create a coarser intermediate feature representation and can make the ConvNet more translation-invariant. For further details see LeCun et al. (2015); Goodfellow et al. (2016); Schmidhuber (2015)."}, {"heading": "2.4 ConvNet architectures and design choices", "text": ""}, {"heading": "2.4.1 Input representation", "text": "The first important decision for applying ConvNets to EEG decoding is how to represent the input Xj \u2208 RE\u00b7T . One possibility would be to represent the EEG as a time series of topographically organized images, i.e., of the voltage distributions across the (flattened) scalp surface (this has been done for ConvNets that get power spectra as input (Bashivan et al., 2016). However, EEG signals are assumed to approximate a linear superposition of spatially global voltage patterns caused by multiple dipolar current sources in the brain (Nunez and Srinivasan, 2006). Unmixing of these global patterns using a number of spatial filters is therefore typically applied to the whole set of relevant electrodes as a basic step in many successful examples of EEG decoding (Ang et al., 2008; Blankertz et al., 2008; Rivet et al., 2009).\nIn this view, all relevant EEG modulations are global in nature, due to the physical origin of the non-invasive EEG and hence there would be no obvious hierarchical compositionality of local and global EEG modulations in space. In contrast, there is an abundance of evidence that the EEG is organized across multiple time scales, such as in nested oscillations (Canolty et al., 2006; Monto et al., 2008; Schack et al., 2002; Vanhatalo et al., 2004) involving both local and global modulations in time. In light of this, we designed ConvNets in a way that they can learn spatially global unmixing filters in the entrance layers, as well as temporal hierarchies of local and global modulations in the deeper architectures. To this end we represent the input as a 2D-array with the number of time steps as the width and the number of electrodes as the height. This approach also significantly reduced the input dimensionality compared with the \u201cEEG-as-an-image\u201d approach."}, {"heading": "2.4.2 Deep ConvNet for raw EEG signals", "text": "To tackle the task of EEG decoding we designed a deep ConvNet architecture inspired by successful architectures in computer vision, as for example described in Krizhevsky et al. (2012). The requirements for this architecture are as follows: We want a model that is able to extract a wide range of features and is not restricted to specific feature types (Hertel et al., 2015). We were interested in such a generic architecture for two reasons: 1) we aimed to uncover whether such a generic ConvNet designed only with minor expert knowledge can reach competitive accuracies, and, 2) to lend support to the idea that standard ConvNets can be used as a general-purpose tool for brain-signal decoding tasks. As an aside, keeping the architecture generic also increases the chances that ConvNets for brain decoding can directly profit from future methodological advances in deep learning.\nOur deep ConvNet had four convolution-max-pooling blocks, with a special first block designed to handle EEG input (see below), followed by three standard convolution-max-pooling blocks and a dense softmax classification layer (see Figure 1). The first convolutional block was split into two convolutional layers in order to better handle the large number of input channels \u2014 one input channel per electrode compared to three input channels (one per color) in rgb-images. The convolution was split into a first convolution across time and a second convolution across space (electrodes); each filter in these steps has weights for all electrodes (like a CSP spatial filter) and for the filters of the preceding temporal convolution (like any standard intermediate convolutional layer). Since there is\n7\nTonio Ball 2 METHODS\n8\nTonio Ball 2 METHODS\nSquare\nConvolution (temporal)\nConvolution (all\u00a0electrodes)\n40\u00a0Units\n40\u00a0Units\n534\n44 25 1\n510 40\n\u00a044 40\n44\nno activation function in between the two convolutions, they could in principle be combined into one layer. Using two layers however implicitly regularizes the overall convolution by forcing a separation of the linear transformation into a combination of two (temporal and spatial) convolutions. This splitted convolution was evaluated against a single-step convolution in our experiments (see Section 2.4.4).\nWe used exponential linear units (ELUs, f(x) = x for x > 0 and f(x) = ex \u2212 1 for x <= 0 (Clevert et al., 2016)) as activation functions (we also evaluated Rectified Linear Units (ReLUs, f(x) = max(x, 0)), as a less recently proposed alternative, see Section 2.4.4)."}, {"heading": "2.4.3 Shallow ConvNet for raw EEG signals", "text": "We also designed a more shallow architecture referred to as shallow ConvNet, inspired by the FBCSP pipeline (see Figure 2), specifically tailored to decode band power features. The transformations performed by the shallow ConvNet are similar to the transformations of FBCSP (see Section 2.2). Concretely, the first two layers of the shallow ConvNet perform a temporal and a spatial convolution, as in the deep ConvNet. These steps are analogous to the bandpass and CSP spatial filter steps in FBCSP. In contrast to the deep ConvNet, the temporal convolution of the shallow ConvNet had a larger kernel size (25 vs 10), allowing a larger range of transformations in this layer (smaller kernel sizes for the shallow ConvNet led to lower accuracies in preliminary experiments). After the two convolutions of the shallow ConvNet, a squaring nonlinearity, a mean pooling layer and a logarithmic activation function followed; together these steps are analogous to the trial log-variance computation in FBCSP (we note that these steps were not used in the deep ConvNet). In contrast to FBCSP, the\n9\nTonio Ball 2 METHODS\nshallow ConvNet embeds all the computational steps in a single network, and thus all steps can be optimized jointly (see Section 2.5). Also, due to having several pooling regions within one trial, the shallow ConvNet can learn a temporal structure of the band power changes within the trial, which was shown to help classification in prior work (Sakhavi et al., 2015)."}, {"heading": "2.4.4 Design choices for deep and shallow ConvNet", "text": "For both architectures described above we evaluated several design choices. We evaluated architectural choices which we expect to have a potentially large impact on the decoding accuracies and/or from which we hoped to gain insights into the behavior of the ConvNets. Thus, for the deep ConvNet, we compared the design aspects listed in Table 1.\nIn the following, we give additional details for some of these aspects. Batch normalization standardizes intermediate outputs of the network to zero mean and unit variance for a batch of training examples (Ioffe and Szegedy, 2015). This is meant to facilitate the optimization by keeping the inputs of layers closer to a normal distribution during training. We applied batch normalization, as recommended in the original paper (Ioffe and Szegedy, 2015), to the output of convolutional layers before the nonlinearity. Dropout randomly sets some inputs for a layer to zero in each training update. It is meant to prevent co-adaption of different units and can be seen as analogous to training an ensemble of networks. We drop out the inputs to all convolutional layers after the first with a probability of 0.5. Finally, our new tied loss function is designed to further regularize our cropped training (see Section 2.5.4 for an explanation).\n10\nTonio Ball 2 METHODS\nWe evaluated the same design aspects for the shallow ConvNet, except for the following differences:\n\u2022 The baseline methods for the activation function and pooling mode were chosen as \u201csquaring nonlinearity\u201d and \u201cmean pooling\u201d, motivation is given in Section 2.4.3.\n\u2022 We did not include factorized temporal convolutions into the comparison, as the longer kernel lengths of the shallow ConvNet make these convolutions less similar to other successful ConvNets anyways.\n\u2022 We additionally compared the logarithmic nonlinearity after the pooling layer with a square root nonlinearity to check if the logarithmic activation inspired by FBCSP is better than traditional L2-pooling."}, {"heading": "2.4.5 Hybrid ConvNet", "text": "Besides the individual design choices for the deep and shallow ConvNet, a natural question to ask is whether both ConvNets can be combined into a single ConvNet. Such a hybrid ConvNet could profit from the more specific feature extraction of the shallow ConvNet as well as from the more generic feature extraction of the deep ConvNet. Therefore, we also created a hybrid ConvNet by fusing both networks after the final layer. Concretely, we replaced the four-filter softmax classification layers of both ConvNets by 60- and 40-filter ELU layers for the deep and shallow ConvNet respectively. The resulting 100 feature maps were concatenated and used as the input to a new softmax classification layer. We retrained the whole hybrid ConvNet from scratch and did not use any pretrained deep or shallow ConvNet parameters."}, {"heading": "2.4.6 Residual ConvNet for raw EEG signals", "text": "In addition to the shallow and deep ConvNets, we evaluated another network architecture: Residual networks (ResNets), a ConvNet architecture that recently won several benchmarks in the computer vision field (He et al., 2015). ResNets typically have a very large number of layers and we wanted to investigate whether similar networks with more layers also result in good performance in EEG decoding. ResNets add the input of a convolutional layer to the output of the same layer, to the effect that the convolutional layer only has to learn to output a residual that changes the previous layers output (hence the name residual network). This allows ResNets to be successfully trained with a much larger number of layers than traditional convolutional networks (He et al., 2015). Our residual blocks are the same as described in the original paper (see Figure 3).\nOur ResNet used exponential linear unit activation functions (Clevert et al., 2016) throughout the network (same as the deep ConvNet) and also starts with a splitted temporal and spatial convolution (same as the deep and shallow ConvNets), followed by 14 residual blocks, mean pooling and a final softmax dense classification layer (for further details, see Supplementary Section A.3)."}, {"heading": "2.5 ConvNet training", "text": "In this section, we first give a definition of how ConvNets are trained in general. Second, we describe two ways of extracting training inputs and training labels from the EEG data, which result in a trialwise and a cropped training strategy.\n11\nTonio Ball 2 METHODS\nx\nF(x) F(x) + x"}, {"heading": "2.5.1 Definition", "text": "To train a ConvNet, all parameters (all weights and biases) of the ConvNet are trained jointly. Formally, in our supervised classification setting, the ConvNet computes a function from input data to one real number per class, f(Xj ; \u03b8) : RE\u00b7T \u2192 RK , where \u03b8 are the parameters of the function, E the number of electrodes, T the number of timesteps and K the number of possible output labels. To use ConvNets for classification, the output is typically transformed to conditional probabilities of a label lk given the input X j using the softmax function: p(lk|f(Xj ; \u03b8)) = exp(fk(X\nj ;\u03b8))\u2211K k=1 exp(fk(X j ;\u03b8)) .\nIn our case, since we train per subject, the softmax output gives us a subject-specific conditional distribution over the K classes. Now we can train the entire ConvNet to assign high probabilities to the correct labels by minimizing the sum of the per-example losses:\n\u03b8\u2217 = arg min \u03b8\nN\u2211\nj=1\nloss ( yj , p ( lk|fk(Xj ; \u03b8) )) (1)\n, where\nloss ( yj , p ( lk|fk(Xj ; \u03b8) )) = K\u2211\nk=1\n\u2212log ( p ( lk|fk(Xj ; \u03b8) )) \u00b7 \u03b4(yj = lk) (2)\nis the negative log likelihood of the labels. As is common for training ConvNets, the parameters are optimized via mini-batch stochastic gradient descent using analytical gradients computed via backpropagation (see LeCun et al. (2015) for an explanation in the context of ConvNets and Section 2.5.5 in this manuscript for details on the optimizer used in this study).\nThis ConvNet training description is connected to our general EEG decoding definitions from Section 2.1 as follows. The function that the ConvNet computes can be viewed as consisting of a feature extraction function and a classifier function: The feature extraction function \u03c6(Xj ; \u03b8\u03c6) with parameters \u03b8\u03c6 is computed by all layers up to the penultimate layer. The classification function g ( \u03c6(Xj ; \u03b8\u03c6), \u03b8g ) with parameters \u03b8g, which uses the output of the feature extraction function as input, is computed by the final classification layer. In this view, one key advantage of ConvNets becomes clear: With the joint optimization of both functions, a ConvNet learns both, a descriptive feature representation for the task as well as a discriminative classifier. This is especially useful with large datasets, where it is more likely that the ConvNet learns to extract useful features and does\n12\nTonio Ball 2 METHODS\nnot just overfit to noise patterns. For EEG data, learning features can be especially valuable since there may be unknown discriminative features or at least discriminative features that are not used by more traditional feature extraction methods such as FBCSP."}, {"heading": "2.5.2 Input and labels", "text": "In this study, we evaluated two ways of defining the input examples and target labels that the ConvNet is trained on. First, a trial-wise strategy that uses whole trials as input and per-trial labels as targets. Second, a cropped training strategy that uses crops, i.e., sliding time windows within the trial as input and per-crop labels as targets (where the label of a crop is identical to the label of the trial the crop was extracted from)."}, {"heading": "2.5.3 Trial-wise training", "text": "The standard trial-wise training strategy uses the whole duration of the trial and is therefore similar to how FBCSP is trained. For each trial, the trial signal is used as input and the corresponding trial label as target to train the ConvNet. In our study, for both datasets we had 4.5-second trials (from 500 ms before trial start cue until trial end cue, as that worked best in preliminary experiments) as the input to the network. This led to 288 training examples per subject for the BCI Competition Dataset and about 880 training examples per subject on the High-Gamma Dataset after their respective train-test split."}, {"heading": "2.5.4 Cropped training", "text": "The cropped training strategy uses crops, i.e., sliding input windows within the trial, which leads to many more training examples for the network than the trial-wise training strategy. We adapted this strategy from convolutional neural networks for object recognition in images, where using multiple crops of the input image is a standard procedure to increase decoding accuracy (see for example He et al. (2015) and Szegedy et al. (2015)).\nIn our study, we used crops of about 2 seconds as the input. We adopt a cropping approach, which leads to the largest possible number of crops by creating one crop per sample (by sample, we mean a timestep in our EEG trial time series). More formally, given an original trial Xj \u2208 RE\u00b7T with E electrodes and T timesteps, we create a set of crops with crop size T \u2032 as timeslices of the trial: Cj = {Xj1..E,t..t+T |t \u2208 1..T \u2212 T \u2032}. All of these T \u2212 T \u2032 crops are new training data examples for our decoder and will get the same label yj as the original trial.\nThis aggressive cropping has the aim to force the ConvNet into using features that are present in all crops of the trial, since the ConvNet can no longer use the differences between crops and the global temporal structure of the features in the complete trial. We collected crops starting from 0.5 seconds before trial start (first crop from 0.5 seconds before to 1.5 seconds after trial start), with the last crop ending 4 seconds after the trial start (which coincides with the trial end, so the last crop starts 2 seconds before the trial and continues to the trial end). Overall, this resulted in 625 crops and therefore 625 label predictions per trial. The mean of these 625 predictions is used as the final prediction for the trial during the test phase. During training, we compute a loss for each prediction. Therefore, cropped training increases our training set size by a factor of 625, albeit with highly correlated training examples. Since our crops are smaller than the trials, the ConvNet input size is also smaller (from about 1000 input samples to about 500 input samples for the 250 Hz sampling rate), while all other hyperparameters stay the same.\n13\nTonio Ball 2 METHODS\n1 2 3 4 5 6 7\n1 2 3 4 5 2 3 4 5 6 3 4 5 6 7\n3 5 7 9 5 7 9 11 7 9 11 13\n8 16 12 20 16 24\n24 32 40\nInput\nSplit\u00a0Crops\nConvolution \u00a0(Size\u00a02)\nConvolution \u00a0(Size\u00a02,\u00a0Stride\u00a02)\nDense\u00a0Linear\u00a0Projection\n(a) Na\u0308\u0131ve implementation by first splitting the trial into crops and passing the crops through the ConvNet independently.\n1 2 3 4 5 6 7\n3 5 7 9 11 13\n8 12 16 20 24\n8 16 24 12 20 NaN\n24 40 32 NaN\n24 32 40\nInput\nConvolution (Size\u00a02)\nConvolution (Size\u00a02)\nSplit\u00a0Stride\u00a0Offsets (Stride\u00a02)\nConvolution (Size\u00a02)\nInterleave\n(b) Optimized implementation, computing the outputs for each crop in a single forward pass. Strides in the original ConvNet are handled by separating intermediate results that correspond to different stride offsets, see the split stride offsets step. NaNs are only needed to pad all intermediate outputs to the same size and are removed in the end. The split stride step can simply be repeated in case of further layers with stride. We interleave the outputs only after the final predictions, also in the case of ConvNets with more layers.\nFigure 4: Multiple-crop prediction used for cropped training. In this toy example, a trial with the sample values 1,2,3,4,5,6,7 is cut into three crops of length 5 and these crops are passed through a convolutional network with two convolutional layers and one dense layer. The convolutional layers both have kernel size 2, the second one additionally uses a stride of 2. Filters for both layers and the final dense layer have values 1,1. Red indicates intermediate outputs that were computed multiple times in the na\u0308\u0131ve implementation. Note that both implementations result in the same final outputs.\n14\nTonio Ball 2 METHODS\nTo reduce the computational load from the increased training set size, we decoded a group of neighboring crops together and reused intermediate convolution outputs. This idea has been used in the same way to speed up ConvNets that make predictions for each pixel in an image (Giusti et al., 2013; Nasse et al., 2009; Sermanet et al., 2014; Shelhamer et al., 2016). In a nutshell, this method works by providing the ConvNet with an input that contains several crops and computing the predictions for all crops in a single forward pass (see Figure 4 for an explanation). This cropped training method leads to a new hyperparameter: the number of crops that are processed at the same time. The larger this number of crops, the larger the speedup one can get (upper bounded by the size of one crop, see Giusti et al. (2013) for a more detailed speedup analysis on images), at the cost of increased memory consumption. A larger number of crops that are processed at the same time during training also implies parameter updates from gradients computed on a larger number of crops from the same trial during mini-batch stochastic gradient descent, with the risk of less stable training. However, we did not observe substantial accuracy decreases when enlarging the number of simultaneously processed crops (this stability was also observed for images (Shelhamer et al., 2016)) and in the final implementation we processed about 500 crops in one pass, which corresponds to passing the ConvNet an input of about 1000 samples, twice the 500 samples of one crop. Note that this method only results in exactly the same predictions as the na\u0308\u0131ve method when using valid convolutions (i.e., no padding). For padded convolutions (which we use in the residual network described in Section 2.4.6), the method no longer results in the same predictions, so it cannot be used to speed up predictions for individual samples anymore. However, it can still be used if one is only interested in the average prediction for a trial as we are in this study.\nTo further regularize ConvNets trained with cropped training, we designed a new objective function, which penalizes discrepancies between predictions of neighboring crops. In this tied sample loss function, we added the cross-entropy of two neighboring predictions to the usual loss of of negative log likelihood of the labels. So, denoting the prediction p ( lk|fk(Xjt..t+T \u2032 ; \u03b8) ) for crop Xjt..t+T \u2032 from time step t to t+T \u2032 by pf,k(X j t..t+T \u2032), the loss now also depends on the prediction for the next crop pf,k(X j t..t+T \u2032+1) and changes from equation 2 to:\nloss ( yj , pf,k(X j t..t+T \u2032) ) = K\u2211\nk=1\n\u2212log ( pf,k(X j t..t+T \u2032) ) \u00b7 \u03b4(yj = lk) +\nK\u2211\nk=1\n\u2212log ( pf,k(X j t..t+T \u2032) ) \u00b7 pf,k(Xjt..t+T \u2032+1)\n(3)\nThis is meant to make the ConvNet focus on features which are stable for several neighboring input crops."}, {"heading": "2.5.5 Optimization and early stopping", "text": "As optimization method, we used Adam (Kingma and Ba, 2014) together with a specific early stopping method, since this consistently yielded good accuracies in our experiments. For details on Adam and our early stopping strategy, see Supplementary Section A.4.\n15\nTonio Ball 2 METHODS"}, {"heading": "2.6 Visualization", "text": ""}, {"heading": "2.6.1 Correlating Input Features and Unit Outputs: Network Correlation Maps", "text": "As described in the Introduction, currently there is a great interest in understanding how ConvNets learn to solve different tasks. To this end, methods to visualize functional aspects of ConvNets can be helpful and the development of such methods is an active area of research. Here, we wanted to delineate what brain-signal features the ConvNets used and in which layers they extracted these features. The most obvious restriction on possible features is that units in individual layers of the ConvNet can only extract features from samples that they have \u201cseen\u201d, i.e., from their so-called receptive field (see Figure 5). A way to further narrow down the possibly used features is to use domain-specific prior knowledge and to investigate whether known class-discriminative features are learned by the ConvNet. Then it is possible to compute a feature value for all receptive fields of all individual units for each of these class-discriminative features and to measure how much this feature affects the unit output, for example by computing the correlation between feature values and unit outputs.\nIn this spirit, we propose input-feature unit-output correlation maps as a method to visualize how networks learn spectral amplitude features. It is known that the amplitudes, for example of the alpha, beta and gamma bands, provide class-discriminative information for motor tasks (Ball et al., 2008; Pfurtscheller, 1981; Pfurtscheller and Aranibar, 1979). Therefore, we used the mean envelope values for several frequency bands as feature values. We correlated these values inside a receptive field of a unit, as a measure of its total spectral amplitude, with the corresponding unit outputs to gain insight into how much these amplitude features are used by the ConvNet. Positive\n16\nTonio Ball 2 METHODS\nor negative correlations that systematically deviate from those found in an untrained net imply that the ConvNet learned to create representations that contain more information about these features than before training.\nA limitation of this approach is that it does not distinguish between correlation and causation (i.e., whether the change in envelope caused the change in the unit output, or whether another feature, itself correlated to the unit output, caused the change). Therefore, we propose a second visualization method, where we perturbed the amplitude of existing inputs and observed the change in predictions of the ConvNets. This complements the first visualization and we refer to this method as input-perturbation network-prediction correlation map. By using artificial perturbations of the data, they provide insights in whether changes in specific feature amplitudes cause the network to change its outputs. For details on the computation of both NCM methods and a ConvNetindependent visualization, see Supplementary Section A.5."}, {"heading": "2.7 Data sets and preprocessing", "text": "We evaluated decoding accuracies on two EEG datasets, a smaller public dataset (BCI Competition IV dataset 2a) for comparing to previously published accuracies and a larger new dataset acquired in our lab for evaluating the decoding methods with a larger number of training trials (approx. 880 trials per subject, compared to 288 trials in the public set). For details on the datasets, see Supplementary Section A.6."}, {"heading": "2.7.1 EEG preprocessing and evaluating different frequency bands", "text": "We only minimally preprocessed the datasets to allow the ConvNets to learn any further transformations themselves. In addition to the full-bandwidth (0\u2013fend-Hz) dataset, we analyzed data high-pass filtered above 4 Hz (which we call 4\u2013fend-Hz dataset). Filtering was done with a causal 3rd order Butterworth filter. We included the 4\u2013fend-Hz dataset since the highpass filter should make it less probable that either the networks or FBCSP would use class-discriminative eye movement artifacts to decode the behavior classes, as eye movements generate most power in such low frequencies (Gratton, 1998). We included this analysis as for the BCI Competition Dataset, special care to avoid decoding eye-related signals was requested from the publishers of the dataset (Brunner et al., 2008). For details on other preprocessing steps, see Supplementary Section A.7.\n17"}, {"heading": "3 Results", "text": ""}, {"heading": "3.1 Validation of FBCSP baseline", "text": "Result 1 FBCSP baseline reached same results as previously reported in the literature\nAs a first step before moving to the evaluation of ConvNet decoding, we validated our FBCSP implementation, as this was the baseline we compared the ConvNets results against. To validate our FBCSP implementation, we compared its accuracies to those published in the literature for the BCI competition IV dataset 2a (called BCI Competition Dataset in the following) (Sakhavi et al., 2015). Using the same 0.5\u20132.5 s (relative to trial onset) time window, we reached an accuracy of 67.6%, statistically not significantly different from theirs (67.0%, p=0.73, Wilcoxon signed-rank test). Note however, that we used the full trial window for later experiments with convolutional networks, i.e., from 0.5\u20134 seconds. This yielded a slightly better accuracy of 67.8%, which was still not statistically significantly different from the original results on the 0.5\u20132.5 s window (p=0.73). For all later comparisons, we use the 0.5\u20134 seconds time window on all datasets."}, {"heading": "3.2 Architectures and design choices", "text": "Result 2 ConvNets reached FBCSP accuracies\nBoth the deep the shallow ConvNets, with appropriate design choices (see Result 5), reached similar accuracies as FBCSP-based decoding, with small but statistically significant advantages for the ConvNets in some settings. For the mean of all subjects of both datasets, accuracies of the shallow ConvNet on 0\u2013fend Hz and for the deep ConvNet on 4\u2013fend Hz were not statistically significantly different from FBCSP (see Figure 6 and Table 2). The deep ConvNet on 0\u2013fend Hz and the shallow ConvNet on 4\u2014fend Hz reached slightly higher (1.9% and 3.3% higher respectively) accuracies that were also statistically significantly different (p<0.05, Wilcoxon signed-rank test). Note that all results in this section were obtained with cropped training, for a comparison of cropped and trial-wise training, see Section 3.3.\nResult 3 Confusion matrices for all decoding approaches were similar\n18\nTonio Ball 3 RESULTS\n19\nConfusion matrices for the High-Gamma Dataset on 0\u2013fend Hz were very similar for FBCSP and both ConvNets (see Figure 7). The majority of all mistakes were due to discriminating between Hand (L) / Hand (R) and Feet / Rest, see Table 3. Seven entries of the confusion matrix had a statistically significant difference (p<0.05, Wilcoxon signed-rank test) between the deep and the shallow ConvNet, in all of them the deep ConvNet performed better. Only two differences between the deep ConvNet and FBCSP were statistically significant (p<0.05), none for the shallow ConvNet and FBCSP. Confusion matrices for the BCI Competition Dataset showed a larger variability and hence a less consistent pattern, possibly because of the much smaller number of trials.\nResult 4 Hybrid ConvNets performed slightly, but statistically insignificantly, worse than deep ConvNets\nThe hybrid ConvNet performed similar, but slightly worse than the deep ConvNet, i.e., 83.8% vs 84.0% (p>0.5, Wilcoxon signed-rank test) on the 0\u2013fend-Hz dataset, 82.1% vs 83.1% (p>0.9) on the 4\u2013fend-Hz dataset. In both cases, the hybrid ConvNet\u2019s accuracy was also not statistically significantly different from FBCSP (83.8% vs 82.1%, p>0.4 on 0\u2013fend Hz , 82.1% vs 81.9%, p>0.7 on 4\u2013fend Hz).\nResult 5 ConvNet design choices substantially affected decoding accuracies\nIn the following, results for all design choices are reported for all subjects from both datasets. For an overview of the different design choices investigated, and the motivation behind these choices, we refer to Section 2.4.4.\nBatch normalization and dropout significantly increased accuracies.This became especially clear when omitting both simultaneously (see Figure 8a). Batch normalization provided a larger accuracy increase for the shallow ConvNet, whereas dropout provided a larger increase for the deep ConvNet. For both networks and for both frequency bands, the only statistically significant accuracy differences were accuracy decreases after removing dropout for the deep ConvNet on 0\u2013fend-Hz data or removing batch normalization and dropout for both networks and frequency ranges (p<0.05, Wilcoxon signedrank test). Usage of tied loss did not affect the accuracies very much, never yielding statistically significant differences (p>0.05). Splitting the first layer into two convolutions had the strongest accuracy increase on the 0\u2013fend-Hz data for the shallow ConvNet, where it is also the only statistically significant difference (p<0.01).\n20\nTonio Ball 3 RESULTS\n21\nTonio Ball 3 RESULTS\nNo\u00a0 Batc\nh\u00a0N orm\nNo\u00a0 Dro\npou t\nNo\u00a0 Dro\npou t\u00a0No\n\u00a0Bat ch\u00a0N\norm\nNo\u00a0 Spli\nt\u00a01s t\u00a0La\nyer\nNo\u00a0 Tied\n\u00a0Los s\n80\n60\n40\n20\n0\n20\n40\nA cc\nur ac\ny\u00a0 D\niff er\nen ce\n\u00a0[% ]\n* ** * *** * **\nDeep\u00a00 fend\u00a0Hz Shallow\u00a00 fend\u00a0Hz Deep\u00a04 fend\u00a0Hz Shallow\u00a04 fend\u00a0Hz\n(a) Impact of design choices applicable to both ConvNets. Shown are the effects from the removal of one aspect from the architecture on decoding accuracies. All statistically significant differences were accuracy decreases. Notably, there was a clear negative effect of removing both dropout and batch normalization, seen in both ConvNets\u2019 accuracies and for both frequency ranges.\nSqr t\u00a0ins\ntead \u00a0of\u00a0l og Elu\u00a0 Mea n\nElu\u00a0 Max ReL U\nLog (Me\nan(S qua\nre)) \u00a0firs\nt\nLog (Ma\nx(Sq uare\n))\u00a0fir st\n6x1 +6x\n1\u00a0co nvs\n40\n30\n20\n10\n0\n10\n20\n30\nA cc\nur ac\ny\u00a0 D\niff er\nen ce\n\u00a0[% ]\n* ** * ** ** ** ***\nShallow\u00a00 fend\u00a0Hz Shallow\u00a04 fend\u00a0Hz Deep\u00a00 fend\u00a0Hz Deep\u00a04 fend\u00a0Hz\n(b) Impact of different types of nonlinearities, pooling modes and filter sizes. Results are given independently for the deep ConvNet and the shallow ConvNet. As before, all statistically significant differences were from accuracy decreases. Notably, replacing ELU by ReLU as nonlinearity led to decreases on both frequency ranges, which were both statistically significant.\nFigure 8: Impact of ConvNet design choices on decoding accuracy. Accuracy differences of baseline and design choices on x-axis for the 0\u2013fend-Hz and 4\u2013fend-Hz datasets. Each small marker represents accuracy difference for one subject, each larger marker represents mean accuracy difference across all subjects of both datasets. Bars: standard error of the differences across subjects. Stars indicate statistically significant differences to baseline (Wilcoxon signed-rank test, p<0.05: *, p<0.01: **, p<0.001=***)\n22\nFor the deep ConvNet, using ReLU instead of ELU as nonlinearity in all layers worsened performance (p<0.01, see Figure 8b on the right side). Replacing the 10x1 convolutions by 6x1+6x1 convolutions did not statistically significantly affect the performance (p>0.4).\nResult 6 Recent deep learning advances substantially increased accuracies\nFigure 9 clearly shows that only recent advances in deep learning methods together (by which we mean the combination of batch normalization, dropout and ELUs) allowed our deep ConvNet to be competitive with FBCSP. Without these recent advances, the deep ConvNet had statistically significantly worse accuracies than FBCSP for both 0\u2013fend-Hz and 4\u2013fend-Hz data (p<0.001, Wilcoxon signed-rank test). The shallow ConvNet was less strongly affected, with no statistically significant accuracy difference to FBCSP (p>0.2).\nResult 7 Residual network performed worse than deep ConvNet\nResidual networks had consistently worse accuracies than the deep ConvNet as seen in Table 4. All accuracies were lower and the difference was statistically significant for both frequency ranges on the combined dataset."}, {"heading": "3.3 Training Strategy", "text": "Result 8 Cropped training strategy improved deep ConvNet on higher frequencies\nCropped training increased accuracies statistically significantly for the deep ConvNet on the 4\u2013 fend-Hz data (p<1e-5, Wilcoxon signed-rank test). In all other settings (0\u2013fend-Hz data, shallow ConvNet), the accuracy differences were not statistically significant (p>0.1) and showed a lot of variation between subjects.\nResult 9 Training ConvNets took substantially longer than FBCSP\nFBCSP was substantially faster to train than the ConvNets with cropped training, by a factor of 27\u201345 on the BCI Competition Dataset and a factor of 5\u20139 on the High-Gamma Dataset. Training times are end-to-end, i.e., include the loading and preprocessing of the data. These times are only\n23\nTonio Ball 3 RESULTS\n24\nTonio Ball 3 RESULTS\nmeant to give a rough estimate of the training times as there were differences in the computing environment between ConvNets training and FBCSP training. Most importantly, FBCSP was trained on CPU, while the networks were trained on GPUs (see Section A.9). Longer relative training times for FBCSP on the High-Gamma Dataset can be explained by the larger number of frequency bands we use on the High-Gamma Dataset. Online application of the trained ConvNets does not suffer from the same speed disadvantage compared to FBCSP; the fast prediction speed of trained ConvNets make them well suited for decoding in real-time BCI applications."}, {"heading": "3.4 Visualization", "text": "Result 10 Band power topographies show event-related \u201cdesynchronization/synchronization\u201d typical for motor tasks\nBefore moving to ConvNet visualization, we examined the spectral amplitude changes associated with the different movement classes in the alpha, beta and gamma frequency bands, finding the expected overall scalp topographies (see Figure 11). For example, for the alpha (7\u201313 Hz) frequency band, there was a class-related power decrease (anti-correlation in the class-envelope correlations) in the left and right pericentral regions with respect to the hand classes, stronger contralaterally to the side of the hand movement , i.e., the regions with pronounced power decreases lie around\n25\nTonio Ball 3 RESULTS\nEnvelope\u00a0class\u00a0correlation\u00a0maps\nthe primary sensorimotor hand representation areas. For the feet class, there was a power decrease located around the vertex, i.e., approx. above the primary motor foot area. As expected, opposite changes (power increases) with a similar topography were visible for the gamma band (71\u201391 Hz).\nResult 11 Input-feature unit-output correlation maps show learning progression through the ConvNet layers\nWe used our input-feature unit-output correlation mapping technique to examine the question how correlations between EEG power and the behavioral classes are learnt by the network. Figure 12 shows the input-feature unit-output correlation maps for all four conv-pooling-blocks of the deep ConvNet, for the group of subjects of the High-Gamma Dataset. As a comparison, the Figure also contains the correlation between the power and the classes themselves as described in Section A.5.1. The differences of the absolute correlations show which regions were more correlated with the unit outputs of the trained ConvNet than with the unit outputs of the untrained ConvNet; these correlations are naturally undirected. Overall, the input-feature unit-output correlation maps became more similar to the power-class correlation maps with increasing layer depth. This gradual progression was also reflected in an increasing correlation of the unit outputs with the class labels\n26\nTonio Ball 3 RESULTS\nwith increasing depth of the layer (see Figure 13).\nResult 12 Input-perturbation network-prediction correlation maps show causal effect of spatially localized band power features on ConvNet predictions\nWe show three visualizations extracted from input-perturbation network-prediction correlations, the first two to show the frequency profile of the causal effects, the third to show their topography.\nThus, first, we computed the mean across electrodes for each class separately to show correlations between classes and frequency bands. We see plausible results, for example, for the rest class, positive correlations in the alpha and beta bands and negative correlations in the gamma band (see Figure 14).\nThen, second, by taking the mean of the absolute values both over all classes and electrodes, we computed a general frequency profile. This showed clear peaks in the alpha, beta and gamma bands (see Figure 15). Similar peaks were seen in the means of the CSP binary decoding accuracies for the same frequency range.\nThirdly, scalp maps of the input-perturbation effects on network predictions for the different frequency bands, as shown in Figure 16, show spatial distributions expected for motor tasks in the alpha, beta and \u2014 for the first time for such a non-invasive EEG decoding visualization \u2014 for the high gamma band. These scalp maps directly reflect the behavior of the ConvNets and one needs to\n27\nTonio Ball 3 RESULTS\n0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 105 110 115 120\nFrequency\u00a0[Hz]\nHand\u00a0(L) Hand\u00a0(R)\nFeet Rest 0.0004\n0.0000 0.0004\nC or\nre la\ntio n\nFigure 14: Input-perturbation network-prediction correlations for all frequencies for the deep ConvNet, per class. Plausible correlations, e.g., rest positively, other classes negatively correlated with the amplitude changes in frequency range from 20 Hz to 30 Hz.\nbe careful when making inferences about the data from them. For example, the positive correlation on the right side of the scalp for the Hand (R) class in the alpha band only means the ConvNet increased its prediction when the amplitude at these electrodes was increased independently of other frequency bands and electrodes. It does not imply that there was an increase of amplitude for the right hand class in the data. Rather, this correlation could be explained by the ConvNet reducing common noise between both locations, for more explanations of these effects in case of linear models see Haufe et al. (2014). Nevertheless, for the first time in non-invasive EEG, these maps clearly revealed the global somatotopic organization of causal contributions of motor cortical gamma band activity to decoding right and left hand as well as foot movements.\nIn summary, our visualization methods proved useful to map the spatial distribution of the features learned by the ConvNets to perform single-trial decoding of the different movement classes and in different physiologically important frequency bands.\n28\nTonio Ball 3 RESULTS\n29\nTonio Ball 4 DISCUSSION"}, {"heading": "4 Discussion", "text": "This study systematically evaluated ConvNet of different architectures and with different design choices against a validated baseline method, i.e. FBCSP. This study shows that ConvNets allow accurate motor decoding from EEG, that recent deep-learning techniques are critical to boost ConvNet performance, and that a cropped ConvNet training strategy can further increase decoding performance. Thus, ConvNets can achieve successful end-to-end learning from EEG with just minimal preprocessing. This study also demonstrates that novel ConvNets visualization offer new possibilities in brain mapping of informative EEG features."}, {"heading": "4.1 Architectures and design choices", "text": ""}, {"heading": "4.1.1 ConvNets vs. FBCSP", "text": "Our results demonstrate that deep and shallow ConvNets, with appropriate design choices, are able to \u2014 at least \u2014 reach the accuracies of FBCSP for motor decoding from EEG (see Result 2). In our main comparison for the combined datasets (see Table 2), the accuracies of both deep and shallow ConvNets are very close and slightly higher than the accuracies of FBCSP. As filter bank common spatial patterns is the de facto standard for motor decoding from EEG recordings, this strongly implies ConvNets are also a suitable method for motor decoding. While we have shown deep ConvNets to be competitive with standard FBCSP, a lot of variants of FBCSP exist. For example, many regularized variants of CSP exist that can be used inside FBCSP (Lotte and Guan, 2011; Samek, 2014); a comparison to these could further show the exact tradeoff between the more generic ConvNets and the more domain-specific FBCSP."}, {"heading": "4.1.2 Role of recent deep learning advances", "text": "Success depends on using recent developments in deep learning. The accuracy increase that we demonstrate when using batch normalization, dropout and exponential linear units implies that general advances in deep learning can also improve brain-signal decoding. The improvement from using these techniques replicates recent findings in computer vision and other fields. In our study, improvements were most pronounced for the deep ConvNet on 4\u2013fendHz data (see Result 6), indicating that the networks can easily overfit in this setting, where band power features are likely dominant. This is consistent with our observation that cropped training, which combats overfitting by increasing the number of training examples, also drastically increased accuracies on 4\u2013fendHz data (see Result 8). There seemed to be some further gains when combining both batch normalization and dropout, albeit with some variation across architectures and frequency bands. This improvement was not clear from the start as batch normalization can in some cases remove the need for dropout (Ioffe and Szegedy, 2015), however this improvement was also found in another study using ConvNets to decode EEG data (Lawhern et al., 2016). The smaller improvement batch normalization yielded for the deep ConvNet is consistent with the claim that ELUs already allow fast learning (Clevert et al., 2016). However, all of these findings are limited by the fact that there can be interactions between these methods and with all other hyperparameters. As of yet, we also do not have a clear explanation for the large difference in accuracies obtained with ReLUs compared to ELUs; a recent study on computer vision tasks did not find these differences (Mishkin et al., 2016). Mathematically and empirically analyzing the behavior of ELUs and ReLUs for oscillatory signals and typical EEG noise might shed some light on plausible reasons.\n30\nTonio Ball 4 DISCUSSION"}, {"heading": "4.1.3 ConvNet architectures and interactions with discriminative features", "text": "Another finding of our study was that the shallow ConvNets performed as good as the deep ConvNets, in contrast to the hybrid and residual architectures (see Results 2, 4 and 7). These observations could possibly be better understood by investigating more closely what discriminative features there are in the EEG data and what architectures can hence best use them. For example, it would be interesting to study the effect of more layers when the networks use mostly EEG band power features, phaserelated features, or a combination thereof (c.f. Hammer et al. (2013), for the role of power and phase in motor decoding) and whether there are features for which a deeper hierarchical representation could be beneficial.\nWe observed that squaring was important for the shallow but not for the deep ConvNet (see Result 5). The worse performance of the shallow ConvNet with ELU instead of squaring may be explained as follows. Squaring naturally allows the network to more easily extract band power features: In combination with the approximately zero-mean input, the network would already capture the signal\u2019s variance by squaring. To see this, assume that the two bandpass-filter-like and spatial-filterlike convolutional layers extract an oscillatory source in a specific frequency band; the squaring and mean pooling then directly computes the variance of this source in the pool regions. With ELUs instead of squaring, the positive parts of the oscillation would remain unchanged while the negative ones would be suppressed; the mean of the pool region would still be larger for larger amplitudes of the oscillation, but less strongly so than for the square activation. The effects of ELU and squaring for the deep ConvNet are less straightforward to analyze, since the pooling regions in our deep ConvNet were much smaller than for the shallow ConvNet (3 vs 75 samples) and might thus not cover a large enough time span to compute a very robust and informative variance average."}, {"heading": "4.1.4 Possibilities for substantial decoding accuracy improvements?", "text": "In the analyses presented here, ConvNets did not improve accuracies over FBCSP by a large margin. Significant improvements, if present, were never larger than 3.5 percent on the combined dataset with a lot of variation per subject (see Result 2). However, the deep ConvNets as used here may have learned features different from FBCSP, which could explain their higher accuracies in the lower frequencies where band power features may be less important (Hammer et al., 2013). Nevertheless, ConvNets failed to clearly outperform FBCSP in our experiments. Several reasons might contribute to this: the datasets might still not be large enough to reveal the full potential of deeper convolutional networks in EEG decoding; or the class-discriminative features might not have enough hierarchical structure which deeper ConvNets could exploit. The dataset-size issue could be solved by either creating larger datasets or also by using transfer learning approaches across subjects and/or other datasets. Further analysis of the data itself and of the convolutional networks might help to shed light whether there are features with a lot of hierarchical structure. Finally, recurrent networks could exploit signal changes that happen on longer timescales, e.g., electrodes slowly losing scalp contact over the course of a session, changes of the electrode cap position or nonstationarities in the brain signals. Thus, there is clearly still a large potential for methodological improvement in ConvNet-based EEG decoding.\nThese methodological improvements might also come from further methodological advances in deep learning, such as newer forms of hyperparameter optimization, in case these advances also translate to even better EEG decoding accuracies. As discussed above, recent advances like dropout, batch normalization and exponential linear units can substantially improve the performance of EEG decoding with ConvNets, especially for our deep architecture. Therefore, using other recent tech-\n31\nTonio Ball 4 DISCUSSION\nniques, such as newer forms of hyperparameter optimization (Domhan et al., 2015; Klein et al., 2016; Springenberg et al., 2016) hold promise to further increase accuracies of ConvNets for brain-signal decoding. Furthermore, as the field is still evolving at a fast pace, new techniques can be expected to be developed and might then also benefit brain-signal decoders using convolutional neural networks.\nHowever, methodological improvements may also happen in the broad field of \u201cnon-ConvNet\u201d approaches. Obviously, currently no final verdict is possible about an \u201coptimal\u201d method for EEG decoding, if there is a single best method for the large variety of EEG decoding problems at all. The findings of the present study however support that ConvNet-based decoding is a contender in this competition."}, {"heading": "4.1.5 Further potential advantages of ConvNets for brain-signal decoding", "text": "Besides the decoding performance, there are also other potential advantages of using deep ConvNets for brain-signal decoding. First, several use cases desirable for brain-signal decoding are very easy to do with deep ConvNets iteratively trained in an end-to-end fashion: Deep ConvNets can be applied to other types of tasks such as as workload estimation, error- or event-related potential decoding (as others have started (Lawhern et al., 2016)) or even other types of recordings such as MEG or ECoG. Also, ConvNets, due to their iterative training, have a natural way of pretraining and finetuning; for example a ConvNet can be pretrained on data from the past or data from other subjects and then be finetuned with new data from a new subject. Finetuning can be as simple as continuing the iterative training process on the new data, possibly with a smaller learning rate and this finetuning can also be used to perform supervised online adaptation. Second, due to their joint optimization, single ConvNets can be building blocks for more sophisticated setups of multiple ConvNets. One recent example attempts to create ConvNets that are robust to changes in the input distribution (Ganin et al., 2016). This could be used to alleviate the long-standing EEG decoding problem of changes in the EEG signal distribution from one session to another."}, {"heading": "4.2 Training strategy", "text": ""}, {"heading": "4.2.1 Cropped training effect on accuracies", "text": "We observed that cropped training was necessary for the deep ConvNet to reach competitive accuracies on the dataset excluding very low frequencies (see Result 8). The large increase in accuracy with cropped training for the deep network on the 4\u2013fend-Hz data might indicate a large number of training examples is necessary to learn to extract band power features. This makes sense as the shifted neighboring windows may contain the same, but shifted, oscillatory signals. These shifts could prevent the network from overfitting on phase information within the trial, which is less important in the higher than the lower frequencies (Hammer et al., 2013). This could also explain why other studies on ConvNets for brain-signal decoding, which did not use cropped training, but where band power might be the most discriminative feature, have used fairly shallow architectures and sometimes found them to be superior to deeper versions (Stober et al., 2014)."}, {"heading": "4.2.2 Suitability for online decoding", "text": "Our cropped training strategy appears particularly well-applicable for online brain-signal decoding. As described above, it may offer performance advantages compared with conventional (non-cropped) training. Additionally, cropped training allows for a useful calibration of the trade-off between\n32\nTonio Ball 4 DISCUSSION\ndecoding delay and decoding accuracy in online settings. The duration from trial start until the last sample of the first crop should roughly correspond to the minimum time needed to decode a control signal. Hence, smaller crops can allow less delay \u2014 the first small crop could end at an early sample within the trial without containing too many timesteps from before the trial that could otherwise disturb the training process. Conversely, larger crops that still contain mostly timesteps from within the trial imply a larger delay until a control signal is decoded while possibly increasing the decoding accuracy due to more information contained in the larger crops. These intuitions should be confirmed in online experiments."}, {"heading": "4.3 Visualization", "text": ""}, {"heading": "4.3.1 Insights from current visualizations", "text": "In addition to exploring how ConvNets can be successfully used to decode information from the EEG, we have also developed and tested two complementary methods to visualize what ConvNets learn from the EEG data. So far, the literature on using ConvNets for brain-signal decoding has, for example, visualized weights or outputs of ConvNet layers (Bashivan et al., 2016; Santana et al., 2014; Stober, 2016; Yang et al., 2015), determined inputs that maximally activate specific convolutional filters (Bashivan et al., 2016), or described attempts at synthesizing the preferred input of a convolutional filter (Bashivan et al., 2016) (see Supplementary Section A.1 for a more extensive overview). Here, we applied both a correlative and a causally interpretable visualization method to visualize the frequencies and spatial distribution of band power features used by the networks. The visualizations showed plausible spatial distributions for motor tasks in the alpha, beta and gamma bands (see Section 3.4). The input-feature unit-output and the input-perturbation network-prediction correlation maps together clearly showed that the deep ConvNet learned to extract and use band power features with specific spatial distributions. Hence, while the computation of power was built into both the FBCSP and shallow ConvNet, our deep ConvNets successfully learned to perform the computation of band power features from the raw input in an end-to-end manner. Our network correlation maps can readily show spatial distributions per subject and for the whole group of subjects. Thus, our network correlation maps proved useful as a technique for spatially mapping the features learned by the ConvNets to perform single-trial decoding."}, {"heading": "4.3.2 Feature discovery through more sophisticated visualizations?", "text": "One limitation of the visualizations presented here is that so far, we only designed them to show how ConvNets use known band power features. However, it could be even more interesting to investigate whether novel or so-far unknown features are used and to characterize them. This could be especially informative for tasks where the discriminative features are less well known than for motor decoding, e.g. for less-investigated tasks such as decoding of task performance (Meinel et al., 2016). But even for the data used in this study, our results show hints that deep ConvNets used different features than shallow ConvNets as well as the FBCSP-based decoding, since there are statistically significant differences between their confusion matrices (see Result 3). This further strengthens the motivation to explore what features the deep ConvNet exploits, for example using visualizations that show what parts of a trial are relevant for the classification decision or what a specific convolutional filter/unit output encodes. Newer visualization methods such as layer-wise relevance propagation (Bach et al., 2015; Montavon et al., 2017), inverting convolutional networks\n33\nTonio Ball 4 DISCUSSION\nwith convolutional networks (Dosovitskiy and Brox, 2016) or synthesizing preferred inputs of units (Nguyen et al., 2016) could be promising next steps in that direction."}, {"heading": "4.4 Conclusion", "text": "In conclusion, ConvNets are not only a novel, promising tool in the EEG decoding toolbox, but combined with innovative visualization techniques, they may also open up new windows for EEGbased brain mapping.\n34\nTonio Ball 4 DISCUSSION"}, {"heading": "Acknowledgements", "text": "This work was supported by the BrainLinks-BrainTools Cluster of Excellence (DFG grant EXC1086) and by the Federal Ministry of Education and Research (BMBF, grant Motor-BIC 13GW0053D).\nConflicts of interest\nThe authors declare that there is no conflict of interest regarding the publication of this paper.\n35\nTonio Ball REFERENCES"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Deep learning with convolutional neural networks (deep ConvNets) has revolutionized computer vision through end-to-end learning, i.e. learning from the raw data. Now, there is increasing interest in using deep ConvNets for end-to-end EEG analysis. However, little is known about many important aspects of how to design and train ConvNets for end-to-end EEG decoding, and there is still a lack of techniques to visualize the informative EEG features the ConvNets learn. Here, we studied deep ConvNets with a range of different architectures, designed for decoding imagined or executed movements from raw EEG. Our results show that recent advances from the machine learning field, including batch normalization and exponential linear units, together with a cropped training strategy, boosted the deep ConvNets decoding performance, reaching or surpassing that of the widely-used filter bank common spatial patterns (FBCSP) decoding algorithm. While FBCSP is designed to use spectral power modulations, the features used by ConvNets are not fixed a priori. Our novel methods for visualizing the learned features demonstrated that ConvNets indeed learned to use spectral power modulations in the alpha, beta and high gamma frequencies. These methods also proved useful as a technique for spatially mapping the learned features, revealing the topography of the causal contributions of features in different frequency bands to decoding the movement classes. Our study thus shows how to design and train ConvNets to decode movement-related information from the raw EEG without handcrafted features and highlights the potential of deep ConvNets combined with advanced visualization techniques for EEG-based brain mapping. Tonio Ball", "creator": "LaTeX with hyperref package"}}}