{"id": "1703.05486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Using Reinforcement Learning for Demand Response of Domestic Hot Water Buffers: a Real-Life Demonstration", "abstract": "This paper demonstrates a data-driven control approach for demand response in real-life residential buildings. The objective is to optimally schedule the heating cycles of the Domestic Hot Water (DHW) buffer to maximize the self-consumption of the local photovoltaic (PV) production. A model-based reinforcement learning technique is used to tackle the underlying sequential decision-making problem. The proposed algorithm learns the stochastic occupant behavior, predicts the PV production and takes into account the dynamics of the system. A real-life experiment with six residential buildings is performed using this algorithm. The results show that the self-consumption of the PV production is significantly increased, compared to the default thermostat control.", "histories": [["v1", "Thu, 16 Mar 2017 06:42:07 GMT  (61kb,D)", "http://arxiv.org/abs/1703.05486v1", "Submitted to IEEE ISGT Europe 2017"]], "COMMENTS": "Submitted to IEEE ISGT Europe 2017", "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["oscar de somer", "ana soares", "tristan kuijpers", "koen vossen", "koen vanthournout", "fred spiessens"], "accepted": false, "id": "1703.05486"}, "pdf": {"name": "1703.05486.pdf", "metadata": {"source": "CRF", "title": "Using Reinforcement Learning for Demand Response of Domestic Hot Water Buffers: a Real-Life Demonstration", "authors": ["Oscar De Somer", "Ana Soares", "Tristan Kuijpers", "Koen Vossen", "Koen Vanthournout", "Fred Spiessens"], "emails": [], "sections": [{"heading": null, "text": "Reinforcement Learning, Demand Response, Domestic Hot Water, Field Experiment March 1, 2017\nI. INTRODUCTION\nResidential demand response (DR) has received considerable attention in the recent literature. It can enable consumers with flexible loads to adapt their consumption profile in response to an external signal. The increasing amount of installed PV has a significant impact on the existing power system. It accelerates transformer aging and may cause voltage problems on the distribution feeder [1]. Due to these problems, the tariffs for injecting energy in the grid are decreasing, and are even zero in some countries.\nResidential DHW buffers offer the possibility to store thermal energy without impacting the user comfort. Since the heating system is used for a double purpose and an insulated buffer is inherently cheaper than electrochemical energy storage, the investment costs are lower compared to battery systems. Besides that, it does not induce as much energy losses as with grid connected battery systems [2].\nSeveral research projects investigate the potential and implementation of residential demand response. Field tests in the residential sector have been conducted in multiple countries in order to assess the response to various input signals and the resultant flexibility [3], [4], [5].\nThis project comprises the control of thermostatically controlled loads (TCL), which is a stochastic sequential optimization problem. Model predictive control (MPC) and reinforcement learning (RL) are two candidate approaches to\nsolve such problems. MPC was originally designed to exploit an explicitly formulated model of the process and solve in a receding horizon manner a series of open-loop deterministic optimal control problems. RL was designed to infer closedloop policies for stochastic optimal control problems from a sample of trajectories gathered from interaction with the real system or from simulations [6]. RL has already been applied in several related test cases with promising results [5], [7], [8].\nThis paper describes the implementation and the results of a demand response application. In the context of the Rennovates project [9], houses in social districts in the Netherlands are renovated and equipped with a smart heat pump and a PV installation. The objective is to optimally schedule the heating cycles of the DHW buffer to maximize the self-consumption of the local PV production. To do so, a model-based RL approach is employed. The main contribution of this paper is the reallife implementation of a PV self-consumption algorithm using non-battery residential devices.\nSection II discusses the demonstration set-up. After that, section III presents the formal notation that is used in this work and its implementation in the real-life set-up. In section IV the control strategy is described and section V discusses the results. Conclusions and future research directions are finally outlined in section VI."}, {"heading": "II. SET-UP", "text": "In its initial configuration, the field experiment consists of six renovated houses, however, more houses will be added in a later phase. In each house, a smart heat pump is used for space heating and for heating the DHW buffer. The insulation of the houses and the number of PV panels are dimensioned such that the annual energy production exceeds its consumption. The following three subsections will give a concise description of the DHW buffer, the available sensors, and the controls."}, {"heading": "A. DHW Buffer", "text": "The DHW buffer has a volume of 200 liters. The buffer remains properly stratified when water is tapped, but gets mixed when a heating cycle is started. Since this mixing creates an unpredictable temperature distribution, only complete charging\nar X\niv :1\n70 3.\n05 48\n6v 1\n[ cs\n.S Y\n] 1\n6 M\nar 2\ncycles are allowed, i.e., the heating process is only stopped when the temperature distribution in the buffer is assumed to be uniform. This considerably simplifies the problem of estimating the energy content of the buffer."}, {"heading": "B. Sensors", "text": "A collection of sensors provide input for the control algorithm. The most important for this use case are: a temperature sensor in the middle of the buffer, a flow meter to measure the volume of hot water tapped, and an electricity meter of the heat pump."}, {"heading": "C. Controls", "text": "The charging cycles of the DHW buffer can be controlled by two control commands on the smart heat pump. As first, the target temperature of the buffer can be set (Tset). This means that when the temperature, measured by the sensor in the middle of the buffer, decreases below Tset, the heat pump starts charging to Tset. In order to ensure the user comfort limits, the minimum allowed target temperature is Tmin = 45\n\u25e6C, and maximum Tmax = 55\u25e6C. Secondly, a control command is available to force start the heating process. This control can be used when a significant portion of hot water is tapped but the cold water front is still below the temperature sensor in the middle of the buffer. In this case, the sensor does not measure the cold water yet, thus the heating would not start automatically."}, {"heading": "III. MARKOV DECISION PROCESSES", "text": "This section introduces the notation used in the remainder of this work. In RL, a problem is usually formulated as a Markov Decision Process (MDP) [10], [11]. An MDP is defined by its state space X , its action space U , and a transition function f :\nxk+1 = f(xk, uk, wk), k = 0, 1, . . . , T \u2212 1, (1)\nwhich describes the dynamics from xk to xk+1, under control action uk \u2208 U , and subject to random disturbance wk \u2208 W . The number of control periods in one optimization horizon is represented by T . The disturbance wk is generated by a conditional probability distribution pw(\u00b7|x). The transition from k to k + 1, is associated with a cost ck:\nck = \u03c1(xk, uk, wk), k = 0, 1, . . . , T \u2212 1, (2)\nit corresponds to a cost for injecting energy in the grid. The aim is to find the control policy h : X \u2192 U that minimizes the expected sum of costs over the considered time period:\nJh(xt) = E( T\u22121\u2211 k=t \u03c1(xk, h(xk), wk)). (3)\nTypically, this policy is characterized by a state-action value function or Q-function:\nQh(x, u) = E w\u223cpw(\u00b7|x) [\u03c1(x, u, w) + Jh(f(x, u, w))], (4)\nwhich estimates the expected cost when choosing action u in state x, and following policy h thereafter. Given a Q-function,\nthe policy is calculated by choosing an action that minimizes the expected cost in a given state:\nh(x) \u2208 argmin u\u2208U Qh(x, u). (5)\nThe next four paragraphs describe the state, action, transition function, and cost function in detail."}, {"heading": "A. State Description", "text": "The relevant state space X of a domestic water heater can be broken down in a time-related component, a controllable component and a non-controllable exogenous component [8], [7].\n1) Time feature: The time-related component represents the discrete feature space that contains relevant timing information. Using this information the learning algorithm should be able to capture the dynamics of the system.\nFor residential consumers, the behavior comprises a daily and a weekly pattern. Therefore, the most relevant timing information can be captured by the hour of the day: Xht = {1, . . . , 24} and the day in the week: Xdt = {1, 2} .\nNote that the day in the week is either a weekday (xdt = 1) or a weekend day (xdt = 2). By only differentiating between those two options, the main weekly pattern can emerge from a minimal number of costly data samples.\n2) Boiler representation: The controllable component describes the state of the DHW boiler, which is influenced by control actions. The state of the boiler corresponds to the temperature distribution over the height of the buffer. However, as described in section II, only one temperature sensor in the middle of the buffer is used.\nThe most important information derived from this temperature distribution is the thermal energy content of the boiler. A heuristic approach to estimate the energy content in a stratified buffer can be obtained with a 2-layer model. In this case, the boiler model has only two separate temperature layers. An upper layer that contains heated water, with a temperature set to be equal to the temperature measured by the sensor in the middle of the buffer: Thot. The lower layer contains cold input water, with a temperature approximated by the feeding water temperature Tin.\nWhen the boiler is uniformly charged, the boundary between the two layers is completely at the bottom. The boundary moves upward according to the volume of hot water tapped by the consumers since the last heating cycle Vtapped, as described in section II.\nUsing this model the energy content of the buffer can be estimated as follows:\nE = cp(Thot(Vbuffer \u2212 Vtapped)\u2212 TinVtapped), (6)\nwith cp the heat capacity of water and Vbuffer the volume of the DHW buffer. This model assumes that temperature distribution in the hot layer is uniform. Therefore, it is important to reset Vtapped to zero regularly by fully recharging to ensure the model is correct. The field experiments show that this happens typically once a day.\nTin is not directly measured by the system. This temperature slowly changes over time according to the outdoor temperature. An estimate of this temperature can be made by using the minimum temperature measured over the past month. The field experiments show that at least once a month more than 100 liters of water is tapped before the heat pump turns on. On such occasion the cold water front passes the temperature sensor in the middle of the buffer, which makes the input temperature directly measurable by this sensor.\nThe accuracy of this model decreases with increasing Vtapped. Since all the houses use the same set-up, the heuristic model was validated by measuring the real energy content in an identical lab set-up. Six temperature sensors were placed under the insulation material of the buffer. Those real energy measurements can then be used to fit a regression model that learns the 2-layer model error in function of Vtapped.\nThe remainder of this work will use state of charge (SoC), defined as:\nSoC = (E \u2212 Emin)/(Emax \u2212 Emin), (7)\nwhere Emax is defined as the energy content of the buffer when it is uniformly charged at Tmax:\nEmax = cpTmaxVbuffer, (8)\nand Emin is defined as the energy content of the buffer when half of it is equal to Tin and the other half equal to Tmin:\nEmin = cp(TinVbuffer/2 + TminVbuffer/2). (9)\n3) Exogenous information: The non-controllable component of the state space contains exogenous information of the system. In this case, the exogenous component only consists of a forecast of the PV production. It has no direct influence on the dynamics, but determines the cost function \u03c1.\nA data driven approach is used to obtain a PV power forecast. It takes as input two data streams. The first is the historic PV power values, where the amount of historic data is a parameter. The second is the weather data forecast at the physical location of the site. This data stream comes from forecast.io [12], an open source weather forecasting API, that contains weather features pex.\nThe forecaster is implemented to represent the function:\nf : (pt\u2212n, . . . , pt\u22121, pex)\u2192 (pt, . . . , pt+k), (10)\nwhere n \u2212 1 is the number of historic power values taken into account, and k is the amount of future prediction. A deep neural network is trained on a set of samples of the form (x, f(x)) coming from the historic data (i.e. also taking historic weather data into account). To perform the forecast, all the required data is collected and used as input for the neural network. This gives the PV power prediction."}, {"heading": "B. Action", "text": "As described in section II, the target temperature of the boiler can be set and a heating cycle can be forced to start. The considered actions are to either delay the reheating by setting\nthe target temperature to Tmin, or to start the heat pump and charge the buffer uniformly to a certain temperature. In the remainder of this work the action space will consist of three options, i.e., u \u2208 {0, 1, 2}, with: \u2022 u = 0: Delay heating \u2022 u = 1: Start heating now until SoC corresponding to\nbuffer with uniform temperature equal to Tmin \u2022 u = 2: Start heating now until SoC = 1"}, {"heading": "C. Transition Function", "text": "This work proposes a model-based reinforcement learning approach. That means that a model of the transition function is used to generate data as input for the control strategy described in section IV. This model is learned using historical measurement data. It needs to describe the evolution in SoC over time, contain the stochastic user behavior and incorporate the losses to the environment. On top of that, there is backup controller (BUC) present in the system that avoids comfort limits violations. This BUC should also be contained in the model.\n1) Tap water model: The user behavior is captured by constructing an approximation of the conditional probability distribution pv(\u00b7|t), that can be used to generate samples vi of the water tapped at a given time step. It is vi that represents wk from (1). This is done by binning the historical tap water consumption data points into bins corresponding to their timerelated component of the state space. For the time feature discussed in section III-A, all the data points from the same hour in the day and same week day are gathered in the same bin. Given a certain timestamp, samples are generated by sampling randomly from the bin corresponding to this timestamp. If enough historical data is available, this generator will be a good representation of the underlying stochasticity. In this work, the last two months of consumption data are used for training the model.\nA disadvantage of the discussed model is that it neglects the inter time step dependency of the tap water consumption. However, auto-correlation plots show only minor correlation with previous time steps. In future work it will be verified whether generating complete trajectories that take into account past consumption data has a positive influence on the results.\n2) Standing losses: Besides the decrease in SoC by the water tapped from the buffer, there are standing losses to the environment. These standing losses are approximated by a regression function only dependent of the SoC, that is learned from historical data.\n3) Backup controller: As last, the transition function should contain the BUC, which corresponds to the automatically started heating cycles when the SoC decreases below zero. This is modeled by increasing the SoC back to the SoC corresponding to a buffer with a uniform temperature equal to Tmin. Note that, the effect of taking action 1 is equal to action 0 when the SoC is lower or equal to zero."}, {"heading": "D. Cost Function", "text": "In this work the objective is to maximize the selfconsumption of the PV production. Hence, the cost function is\ndefined as: \u03c1(xk, uk, wk) = Pinj\u2206t, where Pinj is the average power injected in the grid during the time interval \u2206t, which is defined as the PV production minus the PV production covered by the heat pump consumption corresponding to the chosen action. A forecast of the PV production is available as described in section III-A3. Obviously, this cost function can easily be adapted to offer more flexible demand response options. Note that non-controllable loads are ignored here since they are very hard to predict for an individual house.\nThe power consumption of the heat pump depends on the chosen action and the SoC of the buffer. However, the power profile can not be modulated, only the duration of the consumption can be elongated. This means that given the SoC of the boiler and the target temperature, the power profile can be deduced. Fig. 1 shows an example of the power profiles for two actions."}, {"heading": "IV. CONTROL STRATEGY", "text": "The central idea behind batch reinforcement learning is to estimate the Q-function (4) based on past observations. Those observations are represented by 4-tuples (x, u, x\u2032, c), containing the state, action, next state and corresponding cost.\nIn this work, first a model is learned to generate observations and then this model is used to build a training set to learn the policy. This approach reduces the number of interactions with the system. Note that neither the dynamics of the system nor the cost function are given in an analytical form. The optimal behavior in this strongly stochastic environment should be learned by interacting with the model [13]. In order to simplify the estimation of the Q-function, a separate approximation Q\u0302N is made at every time step. Another simplification is made by iterating backwards over the time steps, i.e., start at the end of the optimization horizon, set the Q-function equal to zero there and run backward in time. Using this strategy the Q-function contains all the information about the future costs after one sweep over the optimization horizon.\nAlgorithm 1 describes the calculation of the sequence of Q\u0302N -functions. As described in section III-C1, only a timestamp is needed to generate tap water samples. For each in the considered period, a grid is created over the state-action\nAlgorithm 1 Model based fitted Q-iteration [13] Require: Grid X \u00d7 U over state-action space, transition\nfunction f and cost function \u03c1 Q\u0302T+1 \u2190 0 for N = T, \u00b7 \u00b7 \u00b7 , 1 do\n// Build training set T S = { (il, ol) }|F| l=1\n: l\u2190 0 for \u2200(x, u) \u2208 X \u00d7 U do\nl\u2190 l + 1 il \u2190 (x, u) Get L tap water samples { vi }L i=1 for time step k\nol \u2190\nL\u2211 i=1\n[ \u03c1(x, u, vi) + min\nu\u2208U Q\u0302N+1\n( f(x, u, vi), u )] L\nend for Use regression algorithm to fit Q\u0302N from T S\nend for\nspace. For each grid point, L samples are generated from the tap water model. Those samples are used to calculate one output ol of the training set, with the grid point as input il. As can be seen, the random component in the cost function \u03c1 and the transition function f is replaced by a sample from the tap water model v \u223c pv(\u00b7|t). A regression algorithm is used to generalize this information to any unseen state-action pair, i.e., a function approximation is used to fit this training set in order to get an approximation Q\u0302N of QN over the whole state-action space. In this case, an ensemble of extremely randomized trees is used as supervised learning algorithm [14]."}, {"heading": "V. RESULTS", "text": "This section presents the results of the proposed algorithm in a real-life demonstration. The experiment considers six houses from the same district over a period of four months. A time step of one hour is used to calculate the Q\u0302-functions. The state space is partitioned in 25 equidistant grid points and at every grid point 200 samples of the tap water model are taken. Every hour the policy, for a receding horizon of 24 hours, is retrained with the latest data. Every five minutes the best action is chosen.\nThe next two subsections investigate the learned control policy and some typical active control behavior will be discussed. Besides that, the performance of the learning algorithm is compared with the default scenario using static thermostat control."}, {"heading": "A. Control Policy", "text": "Fig. 2 shows an example of the learned control policy over a period of 28 hours. It shows the best action, according to the policy, over the state space. It can be seen that the policy tries to avoid charging in the period close before the highest PV production. Depending on the state of charge and the time, it charges during the night to the minimum temperature to avoid a charging cycle induced by the backup controller right before noon. When the PV production comes close to its maximum\nit starts heating the boiler. Preferably, first to the minimum temperature (u = 1) and then to the maximum temperature (u = 2). When the PV production starts declining, it will charge to its maximum temperature if possible (u = 2). An example of a real trajectory on a winter day is shown in Fig. 3.\nA more in-depth analysis of the policy can be obtained by evaluating the Q\u0302-functions over the state space for every action. Fig. 4 presents the evaluated Q\u0302-functions for the last 12 hours of the same day as in Fig. 2. It shows what the best SoC is at every time step and which action should be chosen."}, {"heading": "B. Performance Indicators", "text": "To compare the PV self-consumption of the proposed control algorithm with the default thermostat control, performance indicators were calculated for both cases. Over a period stretching from September 2016 to January 2017 the scenarios were alternated each week. All the houses are controlled by the same algorithm. By switching the scenarios weekly, the seasonal influence of meteorological conditions was minimized.\nTable I shows some performance indicators for the six houses combined. It can be seen that the percentage of PV production captured by the heating of DHW almost triples by using the active control algorithm. The second indicator represents the share of PV production captured by the total electricity consumption. Non-controllable loads have been neglected in the control strategy since they are hard to predict for an individual house. However, in order to verify the practical importance of the algorithm, they are taken into account in PV captured by total. The increase in PV self-consumption of more than 20%, clearly demonstrates the effectiveness of the described approach. The four remaining indicators serve to compare certain important variables of the two scenarios. It can be observed that the electricity consumption is higher during the PV self-consumption test period, compared to when the thermostat was used. This difference is mainly caused by the increased space heating consumption. However, the table shows that the PV captured by space heating (SH) only increases with 1.1 percentage point. Taking into account this difference, there is still an increase of more than 20% in total PV self-consumption. Fig. 5 shows a qualitative representation of the results. It can be seen that the DHW consumption is clearly shifted towards the PV production.\nAs for now, the results only span a limited period of time (September-January). However, the experiment is still running and future results will provide a more elaborated and valid investigation over a longer time period covering all seasons."}, {"heading": "VI. CONCLUSION", "text": "This work has presented the implementation of a data-driven control approach for demand response in real-life residential buildings. The objective was to optimally schedule the heating cycles of the DHW buffer to maximize the self-consumption of the local PV production. A model-based reinforcement learning algorithm was deployed to tackle the underlying sequential decision making problem. The approach to learn stochastic occupant behavior, predict the PV production and take into account the dynamics of the system is discussed. A real-life experiment, using the proposed approach over a period of four months, with six residential buildings is analyzed. Each house is equipped with a smart heat pump and PV panels, dimensioned to achieve annual energy neutrality. The results show that the deployed algorithm significantly increases PV self-consumption compared to thermostat control.\nIn future work, more houses will be added to the experiment and the results will be evaluated over a longer period. Besides\nthat, the influence of the assumptions made will be assessed by using different transition function models and it will be compared with a model-free approach."}, {"heading": "ACKNOWLEDGMENT", "text": "The research leading to these results has received funding from the European Commission in the H2020 Programme - EU.2.1.5.3 under grant agreement nr.680603 - REnnovates."}], "references": [{"title": "LV distribution network feeders in Belgium and power quality issues due to increasing PV penetration levels", "author": ["C. Gonzalez", "J. Geuns", "S. Weckx", "T. Wijnhoven", "P. Vingerhoets", "T. De Rybel", "J. Driesen"], "venue": "IEEE PES Innovative Smart Grid Technologies Conference Europe, pp. 1\u20138, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "The impacts of storing solar energy in the home to reduce reliance on the utility", "author": ["R.L. Fares", "M.E. Webber"], "venue": "Nature Energy, vol. 2, p. 17001, 1 2017.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2017}, {"title": "Linear breakthrough project: Large-scale implementation of smart grid technologies in distribution grids", "author": ["B. Dupont", "P. Vingerhoets", "P. Tant", "K. Vanthournout", "W. Cardinaels", "T.D. Rybel", "E. Peeters", "R. Belmans"], "venue": "2012 3rd IEEE PES Innovative Smart Grid Technologies Europe (ISGT Europe), Oct 2012, pp. 1\u20138.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalizable occupant-driven optimization model for domestic hot water production in NZEB", "author": ["H. Kazmi", "S. D\u2019Oca", "C. Delmastro", "S. Lodeweyckx", "S.P. Corgnati"], "venue": "Applied Energy, vol. 175, pp. 1\u201315, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Beyond theory: Experimental results of a self-learning air conditioning unit", "author": ["T. Leurs", "B.J. Claessens", "F. Ruelens", "S. Weckx", "G. Deconinck"], "venue": "IEEE International Energy Conference, ENERGYCON, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning versus model predictive control: A comparison on a power system problem", "author": ["D. Ernst", "M. Glavic", "F. Capitanescu", "L. Wehenkel"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, vol. 39, no. 2, pp. 517\u2013529, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Residential demand response of thermostatically controlled loads using batch reinforcement learning", "author": ["F. Ruelens", "B.J. Claessens", "S. Vandael", "B.D. Schutter", "R. Babuka", "R. Belmans"], "venue": "IEEE Transactions on Smart Grid, vol. PP, no. 99, pp. 1\u201311, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Residential demand response applications using batch reinforcement learning", "author": ["F. Ruelens", "B. Claessens", "S. Vandael", "B. De Schutter", "R. Babuska", "R. Belmans"], "venue": "arXiv preprint arXiv:1504.02125, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["R. Sutton", "A. Barto"], "venue": "1998.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Tree-Based Batch Mode Reinforcement Learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research, vol. 6, no. 1, pp. 503\u2013556, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Approximate value iteration in the reinforcement learning context. application to electrical power system control", "author": ["D. Ernst", "M. Glavic", "P. Geurts", "L. Wehenkel"], "venue": "International Journal of Emerging Electric Power Systems, vol. 3, no. 1, pp. 1066\u20131, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine learning, vol. 63, no. 1, pp. 3\u201342, 2006. 6", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "It accelerates transformer aging and may cause voltage problems on the distribution feeder [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Besides that, it does not induce as much energy losses as with grid connected battery systems [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Field tests in the residential sector have been conducted in multiple countries in order to assess the response to various input signals and the resultant flexibility [3], [4], [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "Field tests in the residential sector have been conducted in multiple countries in order to assess the response to various input signals and the resultant flexibility [3], [4], [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "Field tests in the residential sector have been conducted in multiple countries in order to assess the response to various input signals and the resultant flexibility [3], [4], [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 5, "context": "RL was designed to infer closedloop policies for stochastic optimal control problems from a sample of trajectories gathered from interaction with the real system or from simulations [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 4, "context": "RL has already been applied in several related test cases with promising results [5], [7], [8].", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "RL has already been applied in several related test cases with promising results [5], [7], [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 7, "context": "RL has already been applied in several related test cases with promising results [5], [7], [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "In RL, a problem is usually formulated as a Markov Decision Process (MDP) [10], [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "In RL, a problem is usually formulated as a Markov Decision Process (MDP) [10], [11].", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "The relevant state space X of a domestic water heater can be broken down in a time-related component, a controllable component and a non-controllable exogenous component [8], [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 6, "context": "The relevant state space X of a domestic water heater can be broken down in a time-related component, a controllable component and a non-controllable exogenous component [8], [7].", "startOffset": 175, "endOffset": 178}, {"referenceID": 10, "context": "The optimal behavior in this strongly stochastic environment should be learned by interacting with the model [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 10, "context": "For each in the considered period, a grid is created over the state-action Algorithm 1 Model based fitted Q-iteration [13] Require: Grid X \u00d7 U over state-action space, transition function f and cost function \u03c1 Q\u0302T+1 \u2190 0 for N = T, \u00b7 \u00b7 \u00b7 , 1 do // Build training set T S = { (i, o) }|F| l=1 : l\u2190 0 for \u2200(x, u) \u2208 X \u00d7 U do l\u2190 l + 1 i \u2190 (x, u)", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "In this case, an ensemble of extremely randomized trees is used as supervised learning algorithm [14].", "startOffset": 97, "endOffset": 101}], "year": 2017, "abstractText": "This paper demonstrates a data-driven control approach for demand response in real-life residential buildings. The objective is to optimally schedule the heating cycles of the Domestic Hot Water (DHW) buffer to maximize the selfconsumption of the local photovoltaic (PV) production. A modelbased reinforcement learning technique is used to tackle the underlying sequential decision-making problem. The proposed algorithm learns the stochastic occupant behavior, predicts the PV production and takes into account the dynamics of the system. A real-life experiment with six residential buildings is performed using this algorithm. The results show that the self-consumption of the PV production is significantly increased, compared to the default thermostat control. Reinforcement Learning, Demand Response, Domestic Hot Water, Field Experiment March 1, 2017", "creator": "LaTeX with hyperref package"}}}