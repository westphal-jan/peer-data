{"id": "1703.05706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Mar-2017", "title": "Improving Document Clustering by Eliminating Unnatural Language", "abstract": "Technical documents contain a fair amount of unnatural language, such as tables, formulas, pseudo-codes, etc. Unnatural language can be an important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 15%. Our corpus and tool are publicly available.", "histories": [["v1", "Thu, 16 Mar 2017 16:32:06 GMT  (1134kb,D)", "https://arxiv.org/abs/1703.05706v1", null], ["v2", "Fri, 17 Mar 2017 04:03:36 GMT  (1134kb,D)", "http://arxiv.org/abs/1703.05706v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["myungha jang", "jinho d choi", "james allan"], "accepted": false, "id": "1703.05706"}, "pdf": {"name": "1703.05706.pdf", "metadata": {"source": "CRF", "title": "Improving Document Clustering by Eliminating Unnatural Language", "authors": ["Myungha Jang", "Jinho D. Choi", "James Allan"], "emails": ["mhjang@cs.umass.edu,", "jinho.choi@emory.edu,", "allan@cs.umass.edu"], "sections": [{"heading": "1 Introduction", "text": "Technical documents typically include meta components such as figures, tables, mathematical formulas, and pseudo-code to effectively communicate complex ideas and results. Let us define the term unnatural language as blocks of lines consist of only such components, as opposed to the body text that are natural language.\nThere are many great NLP tools available as the field has been advanced. However, these tools are\nmostly built for input text that are natural language. As many of our tools for NLP can be badly confused by unnatural language, it is necessary to distinguish unnatural language blocks from natural language blocks, or else unnatural language blocks will cause confusion for natural language processing. Once we salvage natural language blocks from the documents, we can exploit NLP tools much better as they are intended for. This phenomenon is emphasized in technical documents that have higher ratio of unnatural language compared to non-technical documents such as essays and novels.\nDocument layout analysis aiming to identify document format by classifying blocks into text, figures, tables, and such has been a long-studied problem (O\u2019Gorman, 1993; Simon et al., 1997). Most previous work have focused on image-based documents, PDF and OCR formats, and used geometric analysis on the pages using the visual cues from its layout. This was a clearly important problem in many applications in NLP and IR.\nThis work was particularly motivated while we attempted to cluster teaching documents (e.g., lecture slides and reading materials from courses) in technical topics. We discovered that unnatural language blocks introduced significant noise for clustering, causing spurious matches between documents. For example, code consists of reserved programming keywords and variable names. Two documents can contain two very different code from one another but their cosine similarity is computed high because they share many same terms by programming convention (Figure 1). (Kohlhase and Sucan, 2006) similarly recognized this problem by explaining main challenges\nar X\niv :1\n70 3.\n05 70\n6v 2\n[ cs\n.I R\n] 1\n7 M\nar 2\nof semantic search for mathematical formula: (1) Mathematical notation is context-dependent; without human\u2019s capability to understand the formula from the context, formulas are just noise. (2) Identical presentations can stand for multiple distinct mathematical objects.\nThis paper proposes a new approach for identifying unnatural language blocks in plain text into four types of categories,(1) TABLE (2) CODE (3) MATHEMATICAL FORMULA, and (4) MISCELLANEOUS (MISC.). Text are extracted from technical documents in the PDF, PPT, and HTML formats with little to no explicit visual layout information preserved. We focus on technical documents because they have a significant amount of unnatural language blocks (26.3% and 16% in our two corpora). Specifically, we focus on documents in slide formats, which have been relatively under-explored.\nWe further study how removal of unnatural language improves NLP tasks, document similarity and document clustering. Our experiments show that clustering on documents with unnatural language removed consistently showed higher accuracy on many of the settings than on original documents, with the maximum improvements up to 15% and 11% in two datasets, while it never significantly hurts the original clustering."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Table Extraction", "text": "Various efforts have been made for table extraction using semi-supervised learning on the patterns of table layouts within ASCII text documents (Ng et al.,\n1999) web documents (Pinto et al., 2003; Lerman et al., 2001; Zanibbi et al., 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al., 2007). Existing techniques exploit the graphical features such as primitive geometry shapes, symbols, and lines to detect table borders. (Khusro et al., 2015) introduces and compares the state-of-the-art table extraction techniques from PDF articles. However, there does not appear to be any work that has attempted to process plain text extracted from richer formats, where table layouts are unpreserved."}, {"heading": "2.2 Formula Extraction", "text": "Lin et al. (2011) categorized existing approaches for mathematical formulas detection by \u2018characterbased\u2019 and \u2018layout-based\u2019 with respect to key features. (Chan and Yeung, 2000) provides a comprehensive survey of mathematical formula extraction using various layout features available from imagebased documents. Since we have no access to layout information, character-based approaches are more relevant to our work. They use features of mathematical symbols, operators, and positions and their character sizes (Suzuki et al., 2003; Kacem et al., 2001)."}, {"heading": "2.3 Code Extraction", "text": "Tuarob et al. (2013) proposed 3 pseudo-code extraction methods: a rule based, a machine learning, and a combined method. Their rule based approach finds the presence of pseudo-code captions using keyword matching. The machine learning approach detects a box surrounding a sparse region and clas-\nsifies whether the box is pseudo-code or not. They extracted four groups of features: font-style based, context based, content based, and structure based."}, {"heading": "3 Problem Definition", "text": "Input to our task is the plain text extracted from PDF or PPT documents. The goal is to assign a class label to each line in that plain text, identifying it as natural language (regular text) or one of the four types of non-natural language block components, table, code, formula, or miscellaneous text. In this work, we focus on these four specific types because our observations lead us to believe they are the most frequently occurring components in PPT lecture slides and PDF articles. Figures are also a very frequent component but we do not consider them because they are commonly pictures or drawings and cannot be easily extracted to text. In this section, we briefly discuss the characteristics of each component and challenges in their identification from the raw text."}, {"heading": "3.1 Table", "text": "Tables are prevalent in almost every domain of technical documents. Tables are usually conveyed by its two-dimensional layout and its column and/or row headings (Khusro et al., 2015). Figure 2 shows a table in an original PDF document and the same table as it appears the text extracted by Apache Tika1. Ta-\n1https://tika.apache.org/\nbles frequently have multiple cells merged for layout, which makes them particularly difficult to distinguish as table once they are converted to flat text."}, {"heading": "3.2 Mathematical Formula", "text": "Mathematical formulas exist in two ways: isolated formulas on their own lines or as formulas embedded within a line of text. In this work, we treat both types as formula component. Because not all math symbols can be matched to Unicode characters and because the extraction software may not convert them correctly, the extracted text tends to contain more oddly formatted or even completely wrong characters. Superscripts and subscripts are no longer distinguishable and the original visual layout (e.g., math symbols over multiple lines such as \u03a0 and \u2211 ) is lost."}, {"heading": "3.3 Code", "text": "Articles in Computer Science or related fields often contain pseudo-code or actual program code to illustrate their algorithm. Similar to mathematical formulas, they exist both isolated and embedded, though most code components are isolated code blocks. As in formula component, we treat both types of code blocks as code components. We assume that even indents, one of the strong code visual cues, are not preserved in the extracted text although some extraction tool saves them, not to limit ourselves to the detailed performances of text extraction tools."}, {"heading": "3.4 Miscellaneous Non-text (Misc.)", "text": "In addition to the components mentioned above, there are other types of non-natural language blocks that are left during conversion to text and that may provide spurious sub-topic matches between documents. To allow for those, we denote those components as miscellaneous text. One example of miscellaneous text is the text and caption that are part of the diagrams in slides. Figure 3 shows an example of miscellaneous text that lost its structure and meaning while being converted to text without the original diagram."}, {"heading": "4 Corpus", "text": ""}, {"heading": "4.1 Data Collection", "text": "We collected 1,561 lecture slides from various Computer Science and Electrical Engineering courses that are available online, and 5,898 academic papers from several years of ACL/EMNLP archive2. We divided the dataset for several purposes; training the classification model, training word embedding model for feature extraction, and clustering for extrinsic evaluation. The details of the dataset we used are summarized in Table 1. We make the data publicly available for download at http://people.cs.umass. edu/~mhjang/publications.html.\nFor classification, we constructed three dataset using two different data sources: (1) lecture slides (2) ACL papers, (3) combining both. We chose these two types of data sources because they have different ratios of unnatural language components, hence complementary to each other for the coverage. Table 2 shows the ratio of the four components from each annotated dataset. For example, 1.4% of lines in TSLIDES are annotated as part of table.\n2https://aclweb.org/anthology"}, {"heading": "4.2 Text Extraction", "text": "We extracted plain text from our datasets using an open-source software package, Apache Tika. The package is available for text extraction from various formats including PDF, PPT, and HTML."}, {"heading": "4.3 Annotation", "text": "To train a statistical model, we need ground-truth data. We created annotation guidelines for the 4 types of non-natural language components and annotated 35 lectures slides (7,943 lines) and 35 ACL proceeding papers (25,686 lines). We developed a simple annotation tool to support the task and also to enforce that annotators follow certain rules3. We hired four undergraduate annotators who have knowledge of the Computer Science domain for this task."}, {"heading": "5 Features", "text": "We find line-based prediction has more advantage over token-based prediction because it allows us to observe the syntactic structure of the line, how statistically common the grammar structure is, and how layout patterns compare to neighboring lines. We in-\n3The guidelines and the tool are available at http://people.cs.umass.edu/~mhjang/ publication.html\ntroduce five sets of features used to train our classifier and discuss each feature\u2019s impact on the accuracy."}, {"heading": "5.1 N-gram (N)", "text": "Unigrams and bigrams of each line are included as features."}, {"heading": "5.2 Parsing Features (P)", "text": "Unnatural languages are not liklely to form any grammar structure. When we attempt to parse the unnatural language line, the resultant parsing tree would form unusual syntactic structure. To capture this insight, we parsed each line using the dependency parser in ClearNLP (Choi and McCallum, 2013) and extracted features such as the set of dependency labels, the ratio of each POS tag, and POS tags of each dependent-head pair from each parse tree."}, {"heading": "5.3 Table String Layout (T)", "text": "Text extracted from tables loses its visual layout as a table but still preserves implicit layout through its string patterns. Tables tend to convey the same type of data along the same column or row. For example, if a column in a table reports numbers, it is more likely to contain numeral tokens in the same location of the lines of the table in parallel. Hence, a block of lines will more likely be a table if they share the same pattern. We encode each line by replacing each token as either S (String) or N (Numeral). We then compute the edit distance among neighboring lines weighted by language modeling probability computed from table corpus (Equation 1, 2).\nPtable(li) \u221d Ptable(li|li\u22121) = TableLanguageModel(li)\u00b7\neditDistance(encode(li), encode(li\u22121)) (1)\nTableLanguageModel(li)\n= \u03a0nj (P (encode(ti,j+1)|encode(ti,j)) (2)\nwhere li refers to a i-th line in a document, ti,j refers to a j-th token in li."}, {"heading": "5.4 Word Embedding Feature (E)", "text": "We train word embeddings using TWORD2V EC using WORD2VEC (Mikolov et al., 2013). The training corpus contained 278,719 words. Since we do a linebased prediction, we need a vector that represents\nthe line, not each word. We consider three ways of computing line embedding vector: (1) by averaging the vector of the words, (2) by computing paragraph vector introduced in (Le and Mikolov, 2014), (3) using both."}, {"heading": "5.5 Sequential Feature (S)", "text": "The sequential nature of the lines is also an important feature because the component most likely occurs over a block of contiguous lines. We train two models. The first model uses the annotation for the previous line\u2019s class. We then train another model using the previous line\u2019s predicted label, which is the output of the first model."}, {"heading": "6 Classification Experiments", "text": "We used the Liblinear Support Vector Machine (SVM) (Chang and Lin, 2011) classifier for training and ran 5-fold cross-validation for evaluation. To improve the robustness of structured prediction, we adopted a learning to search algorithm known as DAGGER to SVM (Ross et al., 2010). We first introduce two baselines to compare the accuracy against our statistical model."}, {"heading": "6.1 Baselines", "text": "Since no existing work is directly applicable to our scenario, we consider two straightforward baselines.\n\u2022 Weighted Random (W-Random) This assigns the random component class to each line. Instead of uniform random prediction, we made more educated guesses using the ratio of components known from the annotated dataset (Table 2).\n\u2022 Component Language Modeling (CLM) Among the five language models of five component class (the four non-textual components and text component) generated from the annotations, we predict the component for each line by assigning the component whose language model gives the highest probability to the line."}, {"heading": "6.2 Classification Result", "text": "We first conducted single-domain classification. Annotations within each dataset, TSLIDES and TACL are split for training and testing using 5-fold cross\nvalidation scheme. Table 3 reports F1-score for prediction of the four components in the two dataset using our method as well as baselines.\nTable 4: Multi-domain classification improves the singledomain classification in Table 3. Identification of categories with particularly low accuracy in each datasets (TABLE and FORMULA in TSLIDES and CODE in TACL) are improved to be as good as the other categories.\nProposed method dramatically increased the prediction accuracies for all of the components against the baselines. CLM baseline showed the highest accuracy on CODE among the four categories in both datasets. Because pseudo-codes use more controlled vocabulary (e.g., reserved words, common variable names), the language itself becomes distinctive characteristics. We also include the numbers reported by Tuarob et al. (2013) for comparison. Since their dataset was 258 PDF scholarly articles, TACL is more comparable dataset than TSLIDES , but our training set is much smaller than their dataset. However, their number reported on Table 3 is not directly comparable to other numbers because the numbers are on different datasets.\nIn TSLIDES , the classification F1-score for FORMULA is relatively low as 29.09% compared to the other components in the same dataset, and also compared to the FORMULA prediction in TACL (80.98%). This is due to too small amount of training data (only 0.5% of FORMULA in TACL), which is overcome in\nTSLIDES that contain 5% of FORMULA training data (refer to Table 2).\nIn the proposed method, classification of CODE and MISC was significantly improved in TSLIDES (around 90%), while that of TABLE and FORMULA was improved in TACL (over 80%). This shows the complementary nature between two datasets, which suggests that a combined dataset of two, Tcombined, would improve classification performance. Table 4 shows the multi-domain classification result using Tcombined, in which all four categories are identified with F1-score higher than 80%."}, {"heading": "6.3 Feature Analysis", "text": "We conducted feature analysis to understand the impact of single feature and their combination. We started from single features and incrementally combined them to observe the performance (Figure 5). Features are added in a greedy fashion that a feature that gives the higher accuracy when used alone is added first.\nWe first compare the three ways of computing sentence vector features mentioned in Section 5 (Figure 4). When we experiment with only embedding fea-\ntures, averaging word vectors performed 9-12 times better than paragraph vectors. When both features were used, there are some gains in CODE and MISC. components and losses in TABLE and FORMULA. However, when we experiment with all the other features in addition to embedding features, losses were covered by the other features such that ultimately combined vectors give overall the highest performances.\nN-gram (N) features was the most powerful feature with 68% of F1-score when used alone. The next useful features are parsing feature (P), table layout (T), and embedding features (E) in order for TABLE, while embedding vectors were more effective than parsing feature for CODE (Figure 5)."}, {"heading": "7 Removal Effects of Unnatural Language on NLP tools", "text": "We observe how removal of unnatural language from document affects the performance of two NLP tools, document similarity and document clustering. For the set of experiments, we prepared a gold standard clustering for each dataset, CDSA and COS ."}, {"heading": "7.1 Document Similarity", "text": "If two documents are similar, they must be topically relevant to each other. A good similarity measure should reflect that; two topically relevant documents should have a high similarity score. To test whether the computed similarity reflects the actual topic relevance better once the unnatural language is removed, we conducted regression analysis.\nWe converted the gold standard clustering to pairwise binary relevance. If two documents are in the same ground-truth cluster, they are relevant, and otherwise irrelevant. We then fitted a log-linear model in R for predicting binary relevance from the cosine similarity of document pairs.\nRegression models fitted in R are evaluated using AIC (Akaike, 1974). The AIC is a measure used as a means for model selection, which measures the relative quality of statistical models learned from the given data. When AIC is smaller, the goodness of fit is better, and the smaller the complexity of the model is, having fewer parameters to represent the model. Table 5 shows that AIC was reduced by 53 and 118 respectively on the models trained with documents whose unnatural language blocks are removed, compared to the original documents. Since AIC does not provide a test for a model, AIC does not suggest anything about the quality of the model in an absolute sense, but relative quality. From this result, we can conclude that cosine similarity can fit a better model that predicts documents\u2019 topic relevance with significance after unnatural language blocks have been removed."}, {"heading": "7.2 Document Clustering", "text": "Comparing general clustering performance on two document sets is tricky because clustering performance varies by many factors, e.g., clustering algorithm, similarity function, document representation, and parameters. To make a safe claim that clustering quality of one set of documents is better than of the other, clustering on one set should consistently outperform the other under many different settings. To validate this, we perform clustering experiments with multiple settings such as varying document vector size and and different initialization schemes.\nIn this experiment, we consider seeded K-means clustering algorithm (Basu et al., 2002) for teaching documents. In our application scenario, users initially submit a topic list (e.g., syllabus) of the course.\nThen lecture slides are grouped into the given topic cluster. Depending on users\u2019 interaction level, we consider a semi-interactive scenario where users only provide a topic list, and a fully-interactive setting where users not only provide a topic list but also provide an answer document for each topic cluster, more specifying the intended topic.\nIn a semi-interactive setting, topic keywords are sparse seed as they usually consist of two or three words. Therefore, we expand the topic keywords by finding the top-1 document retrieved from the keywords and use it as seed. For experiments, we simulate the fully-interactive setting; instead of having an actual user to pick an answer document, we use an answer document randomly chosen from a gold cluster. The seeded K-means clustering algorithm with three interactive seeding schemes is described in Algorithm 1.\nWe can consider a simulated setting more realistic when the selected document is suggested to the user as the top or a near-top choice. In our dataset, 60% of the selected documents were ranked in top 10 in CDSA, and 13% of the selected documents were ranked in top 10 in COS , which implies that the simulated setting in CDSA was more realistic than in CDSA. For top-1 document seeding, 64% and 78% of document seeds matched with the gold standard in CDSA and COS , respectively.\nFigure 6 shows the clustering result of original documents (Doriginal) and documents whose unnatu-\nData: Set of document vectors D = {d1, ...dn}, di \u2208 RT , set of seed vectors S = {s1, ...sk}, user-provided topic keywords vector T = {t1, ...tk} Result: Disjoint K partitioning of D into Ckl=1 Seed Initialization:\n(1) Topic-keywords seeding: si = ti,\n(2) Top-1 document seeding:\nsi = dj argmaxj(COSINESIMILARITY(ti, dj))\n(3) User-selected document seeding:\nsi =DOCSELECTEDBYUSER(ti) while convergence do\nK-means clustering document selection process\nend Algorithm 1: Seeded K-means with User Interaction\nral language blocks removed (Dremoved), with three different seeding schemes over two lecture slides datasets. In CDSA, Dremoved consistently outperformed with all three seeding schemes. The clustering performed the best with Dremoved when top1 document was used as seed. Overall, in CDSA, clustering was improved 94% of the times with the maximum absolute gain of 14.7% and the average absolute gain of 4.6%. The average absolute loss was 0.8% when 6% of the times it hurt. In COS , clustering was improved 73% of the times with the maximum absolute gain of 11.4% and the average absolute gain of 3.9%. The average absolute loss was 1.7%. Our results suggest that removal of unnatural language blocks can significantly improve clustering most of the times with bigger gain than occasional losses."}, {"heading": "8 Conclusion", "text": "In this paper, we argued that unnatural language should be distinguished from natural language in technical documents for NLP tools to work effectively. We presented an approach to the identification of four types of unnatural language blocks from plain text, which is not dependent a document format. Pro-\nposed method extracts five sets of line-based textual features, and showed above 82% F1-score for the four categories of unnatural language. We showed how existing NLP tools can work better on documents if we remove unnatural language from documents. Specifically, we demonstrated removing unnatural language improved document clustering in many settings by up to 15% and 11% at best, while not significantly hurting the original clustering in any setting."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS-0910884, and in part by NSF grant #IIS1217281. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. The authors thank Kenneth W. Church for providing valuable comments and advice."}], "references": [{"title": "Semi-supervised clustering by seeding", "author": ["Basu et al.2002] Sugato Basu", "Arindam Banerjee", "Raymond J. Mooney"], "venue": "In Proceedings of the Nineteenth International Conference on Machine Learning,", "citeRegEx": "Basu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2002}, {"title": "Mathematical expression recognition: A survey", "author": ["Chan", "Yeung2000] Kam-Fai Chan", "Dit-Yan Yeung"], "venue": null, "citeRegEx": "Chan et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2000}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "Lin2011] Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Transition-based dependency parsing with selectional branching", "author": ["Choi", "McCallum2013] Jinho D. Choi", "Andrew McCallum"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Choi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2013}, {"title": "Looking beyond text: Extracting figures, tables and captions from computer science papers", "author": ["Clark", "Divvala2015] Christopher Clark", "Santosh Divvala"], "venue": "In AAAI Workshops", "citeRegEx": "Clark et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2015}, {"title": "Automatic extraction of printed mathematical formulas using fuzzy logic and propagation of context", "author": ["Kacem et al.2001] Afef Kacem", "Abdel Bela\u00efd", "Mohamed Ben Ahmed"], "venue": "IJDAR,", "citeRegEx": "Kacem et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kacem et al\\.", "year": 2001}, {"title": "On methods and tools of table detection, extraction and annotation in pdf documents", "author": ["Khusro et al.2015] Shah Khusro", "Asima Latif", "Irfan Ullah"], "venue": "J. Inf. Sci.,", "citeRegEx": "Khusro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Khusro et al\\.", "year": 2015}, {"title": "A search engine for mathematical formulae", "author": ["Kohlhase", "Sucan2006] Michael Kohlhase", "Ioan Sucan"], "venue": "AISC, volume 4120 of Lecture Notes in Computer Science,", "citeRegEx": "Kohlhase et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kohlhase et al\\.", "year": 2006}, {"title": "Distributed representations of sentences and documents. CoRR, abs/1405.4053", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Automatic data extraction from lists and tables in web sources", "author": ["Craig Knoblock", "Steven Minton"], "venue": "Proceedings of the workshop on Advances in Text Extraction and Mining (IJCAI-2001),", "citeRegEx": "Lerman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lerman et al\\.", "year": 2001}, {"title": "Mathematical Formula Identification in PDF Documents", "author": ["Lin et al.2011] Xiaoyan Lin", "Liangcai Gao", "Zhi Tang", "Xiaofan Lin", "Xuan Hu"], "venue": "In International Conference on Document Analysis and Recognition,", "citeRegEx": "Lin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2011}, {"title": "TableSeer: automatic table metadata extraction and searching in digital libraries", "author": ["Liu et al.2007] Ying Liu", "Kun Bai", "Prasenjit Mitra", "C. Lee Giles"], "venue": "In Joint Conference on Digital Library,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning to recognize tables in free text", "author": ["Ng et al.1999] Hwee Tou Ng", "Chung Yong Lim", "Jessica Li Teng Koo"], "venue": "In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics,", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "The document spectrum for page layout analysis", "author": ["L. O\u2019Gorman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "O.Gorman.,? \\Q1993\\E", "shortCiteRegEx": "O.Gorman.", "year": 1993}, {"title": "Table extraction using conditional random fields", "author": ["Pinto et al.2003] David Pinto", "Andrew McCallum", "Xing Wei", "W. Bruce Croft"], "venue": "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,", "citeRegEx": "Pinto et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pinto et al\\.", "year": 2003}, {"title": "No-regret reductions for imitation learning and structured prediction. CoRR, abs/1011.0686", "author": ["Ross et al.2010] St\u00e9phane Ross", "Geoffrey J. Gordon", "J. Andrew Bagnell"], "venue": null, "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A fast algorithm", "author": ["Simon et al.1997] Anik\u00f3 Simon", "Jean-Christophe Pret", "A. Peter Johnson"], "venue": null, "citeRegEx": "Simon et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Simon et al\\.", "year": 1997}, {"title": "Infty- an integrated ocr system for mathematical documents", "author": ["Fumikazu Tamari", "Ryoji Fukuda", "Seiichi Uchida", "Toshihiro Kanahori"], "venue": "In Proceedings of ACM Symposium on Document Engineering", "citeRegEx": "Suzuki et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2003}, {"title": "Automatic detection of pseudocodes in scholarly documents using machine learning", "author": ["Sumit Bhatia", "Prasenjit Mitra", "C. Lee Giles"], "venue": "In Proceedings of the 2013 12th International Conference on Document Analysis and", "citeRegEx": "Tuarob et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Tuarob et al\\.", "year": 2013}, {"title": "A survey of table recognition: Models, observations, transformations, and inferences", "author": ["Dorothea Blostein", "R. Cordy"], "venue": "Int. J. Doc. Anal. Recognit.,", "citeRegEx": "Zanibbi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zanibbi et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 14, "context": "Document layout analysis aiming to identify document format by classifying blocks into text, figures, tables, and such has been a long-studied problem (O\u2019Gorman, 1993; Simon et al., 1997).", "startOffset": 151, "endOffset": 187}, {"referenceID": 17, "context": "Document layout analysis aiming to identify document format by classifying blocks into text, figures, tables, and such has been a long-studied problem (O\u2019Gorman, 1993; Simon et al., 1997).", "startOffset": 151, "endOffset": 187}, {"referenceID": 13, "context": "Various efforts have been made for table extraction using semi-supervised learning on the patterns of table layouts within ASCII text documents (Ng et al., 1999) web documents (Pinto et al.", "startOffset": 144, "endOffset": 161}, {"referenceID": 15, "context": ", 1999) web documents (Pinto et al., 2003; Lerman et al., 2001; Zanibbi et al., 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al.", "startOffset": 22, "endOffset": 85}, {"referenceID": 9, "context": ", 1999) web documents (Pinto et al., 2003; Lerman et al., 2001; Zanibbi et al., 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al.", "startOffset": 22, "endOffset": 85}, {"referenceID": 20, "context": ", 1999) web documents (Pinto et al., 2003; Lerman et al., 2001; Zanibbi et al., 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al.", "startOffset": 22, "endOffset": 85}, {"referenceID": 11, "context": ", 2004) PDF and OCR image documents (Clark and Divvala, 2015; Liu et al., 2007).", "startOffset": 36, "endOffset": 79}, {"referenceID": 6, "context": "(Khusro et al., 2015) introduces and compares the state-of-the-art table extraction techniques from PDF articles.", "startOffset": 0, "endOffset": 21}, {"referenceID": 18, "context": "They use features of mathematical symbols, operators, and positions and their character sizes (Suzuki et al., 2003; Kacem et al., 2001).", "startOffset": 94, "endOffset": 135}, {"referenceID": 5, "context": "They use features of mathematical symbols, operators, and positions and their character sizes (Suzuki et al., 2003; Kacem et al., 2001).", "startOffset": 94, "endOffset": 135}, {"referenceID": 6, "context": "Tables are usually conveyed by its two-dimensional layout and its column and/or row headings (Khusro et al., 2015).", "startOffset": 93, "endOffset": 114}, {"referenceID": 12, "context": "We train word embeddings using TWORD2V EC using WORD2VEC (Mikolov et al., 2013).", "startOffset": 57, "endOffset": 79}, {"referenceID": 16, "context": "To improve the robustness of structured prediction, we adopted a learning to search algorithm known as DAGGER to SVM (Ross et al., 2010).", "startOffset": 117, "endOffset": 136}, {"referenceID": 19, "context": "PC-CB (Tuarob et al., 2013)* N/A 75.", "startOffset": 6, "endOffset": 27}, {"referenceID": 19, "context": "We also include the numbers reported by Tuarob et al. (2013) for comparison.", "startOffset": 40, "endOffset": 61}, {"referenceID": 0, "context": "In this experiment, we consider seeded K-means clustering algorithm (Basu et al., 2002) for teaching documents.", "startOffset": 68, "endOffset": 87}], "year": 2017, "abstractText": "Technical documents contain a fair amount of unnatural language, such as tables, formulas, pseudo-codes, etc. Unnatural language can be an important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 15%. Our corpus and tool are publicly available.", "creator": "LaTeX with hyperref package"}}}