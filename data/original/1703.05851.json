{"id": "1703.05851", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture", "abstract": "In this paper, we propose to use a set of simple, uniform in architecture LSTM-based models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, cross-sentence, and document creation time relations. A \"double-checking\" technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-of-the-art methods by a large margin.", "histories": [["v1", "Fri, 17 Mar 2017 00:02:42 GMT  (327kb,D)", "http://arxiv.org/abs/1703.05851v1", null], ["v2", "Thu, 5 Oct 2017 21:38:08 GMT  (298kb,D)", "http://arxiv.org/abs/1703.05851v2", "EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["yuanliang meng", "anna rumshisky", "alexey romanov"], "accepted": true, "id": "1703.05851"}, "pdf": {"name": "1703.05851.pdf", "metadata": {"source": "CRF", "title": "Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture", "authors": ["Yuanliang Meng", "Anna Rumshisky", "Alexey Romanov"], "emails": ["ymeng@cs.uml.edu", "arum@cs.uml.edu", "aromanov@cs.uml.edu"], "sections": [{"heading": "1 Introduction", "text": "Recovering temporal information from text is essential to many text processing tasks that require deep language understanding, such as answering questions about the timeline of events or automatically producing text summaries. This work presents intermediate results of an effort to build a temporal reasoning framework with contemporary deep learning techniques.\nUntil recently, there has been remarkably few attempts to evaluate temporal information extraction (TemporalIE) methods in context of downstream applications that require reasoning over the temporal representation. One recent effort to conduct such evaluation was SemEval2015 Task 5, a.k.a. QA-TempEval (Llorens et al., 2015a), which used question answering (QA) as the target application. QA-TempEval evaluated systems producing TimeML (Pustejovsky et al., 2003) annotation based on how well their output could\nbe used in QA. We believe that application-based evaluation of TemporalIE should eventually completely replace the intrinsic evaluation if we are to make progress, and therefore we evaluated our techniques using QA-TempEval setup.\nDespite the recent advances produced by multilayer neural network architectures in a variety of areas, the research community is still struggling to make neural architectures work for linguistic tasks that require long-distance dependencies (such as discourse parsing or coreference resolution). Our goal was to see if a relatively simple architecture with minimal capacity for retaining information was able to incorporate the information required to identify temporal relations in text.\nSpecifically, we use several simple LSTMbased components to recover ordering relations between temporally relevant entities (events and temporal expressions). These components are fairly uniform in their architecture, they rely on dependency relations recovered with a very small number of mature, widely available processing tools, and require minimal engineering otherwise. To our knowledge, this is the first attempt to apply such simplified techniques to the TemporalIE task, and we demonstrate this streamlined architecture is able to outperform state-of-the-art results on a temporal QA task with a large margin."}, {"heading": "2 Related Work", "text": "A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al., 2014b; Mirza, 2016). The best methods used by TemporalIE systems to date tend to rely on highly engineered task-\nar X\niv :1\n70 3.\n05 85\n1v 1\n[ cs\n.I R\n] 1\n7 M\nar 2\n01 7\nspecific models using traditional statistical learning, typically used in succession (Sun et al., 2013; Chambers et al., 2014a). For example, in a recent QA-TempEval shared task, the participants routinely used a series of classifiers (such as support vector machine (SVM) or hidden Markov chain SVM) or hybrid methods combining hand crafted rules and SVM, as was used by the top system in that challenge (Mirza and Minard, 2015). While our method also relies on decomposing the temporal relation extraction task into subtasks, we use essentially the same simple LSTM-based architecture for different components, that consume a highly simplified representation of the input.\nWhile there has not been much work applying deep learning techniques to TemporalIE, some relevant work has been done on a similar (but typically more local) task of relation extraction. Convolutional neural networks (Zeng et al., 2014) and recurrent neural networks both have been used for argument relation classification and similar tasks (Zhang and Wang, 2015; Xu et al., 2015; Vu et al., 2016). We take inspiration from some of this work, including specifically the approach proposed proposed by Xu et al. (2015)."}, {"heading": "3 Dataset", "text": "We used QA-TempEval (SemEval 2015 Task 5)1 data and evaluation methods in our experiments. The training set contains 276 annotated TimeML files, mostly containing news articles from major agencies or Wikinews from late 1990s to early 2000s. The test set contains unannotated files in three genres: 10 news articles composed in 2014, 10 Wikipedia articles about world history, and 8 blogs entries from early 2000s.\nIn QA-TempEval, evaluation is done with a QA toolkit which has yes/no questions about two events, or an event and a TIMEX. Since the test set contains unannotated files, QA is the only way to measure the performance. Some statistics of test data and its QA set can be found in Table 5. QA toolkit is also provided for 25 files from training data which we used as our validation set."}, {"heading": "4 Timex and Event Extraction", "text": "The first task in our TemporalIE pipeline (TEA) is to identify time expressions (TIMEX) and events\n1http://alt.qcri.org/semeval2015/ task5/\nin text. We utilized the HeidelTime package (Stro\u0308tgen and Gertz, 2013) to identify TIMEXes.\nWe trained a neural network model to identify event mentions. Contrary to common practice in TemporalIE, our TLINK models do not rely on event attributes, and therefore we did not attempt to identify them.\nOur preprocessing includes tokenization, part of speech tagging, and dependency tree parsing, which is done with NewsReader (Agerri et al., 2014) for preprocessing. Every token is represented with a set of features. The features used to identify events are listed in Table 1. Event extractor does not use syntactic dependencies, but they are used later in TLINK classification.\nThe extraction model uses long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) which is a popular RNN architecture to process time series. It has gating mechanisms to control forgetting, remembering and outputting values in memory, and has achieved the best results in many tasks. The extraction model has two components, as shown in Figure 2 (right). One component is an LSTM layer which takes word embeddings as input. The other component takes 4 token-level features as input. These components produce hidden representations which are concatenated, and jointly connected to an output layer which performs binary classification. For each token, we use the preceding 4 tokens and successive 4 tokens as its surrounding context. Then the word embeddings of the whole context are used as input to an LSTM layer. If a word is near the edge of a sentence, zero padding is applied. We only use the token-level features of the target token, and ignore those from the context words. The 4 features are all binary, as shown in Table 1. Since the vast majority of event mentions in the training data are single words, we only mark single words as event mentions."}, {"heading": "5 TLINK Classification", "text": "Our TLINK classifier consists of four components: an LSTM-based model for intra-sentence entity relations, an LSTM-based model for cross-\nsentence relations, another LSTM-based model for relations with document creation time, and a rule-based component for time expression pairs. The four models perform TLINK classifications independently, and the combined results are fed into a pruning module to remove the conflicting TLINKs."}, {"heading": "5.1 Intra-Sentence Model", "text": "A TLINK extraction model should be able learn the patterns that indicate temporal relations, such as phrases with temporal prepositions or clauses with specific conjunctions. This suggests such models may benefit from encoding syntactic relations, rather than a linear sequences of lexical items.\nFollowing an idea used by Xu et al. (2015) in relation extraction, we use the shortest path between entities in a dependency tree to capture the essen-\ntial context. Using the NewsReader pipeline, we identify the shortest path, and use the word embeddings for all tokens in the path as input to a neural network. Similar to Xu et al. (2015), we use two branches, where the left branch processes the path from the source entity to the least common ancestor (LCA), and the right branch processes the path from the target entity to the LCA. However, we use a much simpler model that uses only word embeddings, and not POS tags, grammatical relations themselves, or WordNet hypernyms.\nFor example, for the sentence \u201cTheir marriage ended before the war\u201d, given an event pair (marriage, war), the left branch of the model will receive the sequence (marriage, ended), while the right branch will receive (war, before, ended). The LSTM layer then processes the appropriate sequence of word embeddings in the respective branches. This is followed by a max pooling layer is on top of each branch. Finally, the results from the max pooling layers of both branches are concatenated as a hidden layer and followed by softmax to yield class labels and scores. The model architecture is shown in Figure 2 (left).\nWe also augment the training data by flipping every pair, i.e. if (e1, e2) \u2192 BEFORE, it is guaranteed that (e2, e1) \u2192 AFTER is also present."}, {"heading": "5.2 Cross-Sentence Model", "text": "TLINKs between the entities in consecutive sentences can often also be identified without any external context or prior knowledge. For example, the ordering relations may be indicated by discourse connectives, such as then or meanwhile, or the events may follow natural orders, potentially encoded in their word embeddings.\nTo recover such relations, we use a model similar to the one used for intra-sentence relations, as described in section 5.1. Since there is no common root between entities in different sentences, we use the path between an entity and the sentence root to construct input data. A sentence root is often the main verb, or a conjunction."}, {"heading": "5.3 Time Expressions Model", "text": "Time expressions explicitly signify a time point or an interval of time. Once labeled, the temporal relations between timex pairs can be identified using rule-based techniques. Without the timex entities serving as \u201chubs\u201d, many events would be isolated from each other.\nIn this component, we focus on the DATE class of timex tags. The TIME class tags which contain more information are converted to DATE. Every DATE value is mapped to a tuple of real values (START, END). The \u201cvalue\u201d attribute of timex tags follows the ISO-8601 standard so the mapping is straightforward. We set the minimum time interval to be a day. Practically, such a treatment suffices for our data. After mapping DATE values to tuples of real numbers, we can define 5 relations between timex entities T1 = (start1, end1) and T2 = (start2, end2) as follows:\nT1 \u00d7 T2 \u2192  BEFORE if end1 < start2 AFTER if start1 > end2 INCLUDES if start1 < start2 and end1 > end2 IS INCLUDED if start1 > start2 and end1 < end2 SIMULTANEOUS if start1 = start2\nand end1 = end2\n(1)\nThe TLINKs from training data contain more types of relations than the five described in Equation 1. But relations such as IBEFORE (\u201cimmediately before\u201d), IAFTER(\u201cimmediately after\u201d) and IDENTITY are generally used on event pairs, not timex pairs. The QA system also should not target questions on timex pairs. The purpose here is to use the timex relations to link the otherwise isolated events."}, {"heading": "5.4 Relations to DCT", "text": "In a written document, the document creation time (DCT) naturally serves as the \u201ccurrent time\u201d. Identifying DCT and linking it with events and time expressions in the text is a major step of understanding the content, and for our purpose, it is\nalso a component of TLINK classification. The relation between DCT and other time expressions is just a special case of timex TLINK classification, which has been discussed in section 5.3. In this section we discuss how to identify temporal relation between an event and DCT.\nThe assumption here is an event and its local context can often show its relation to DCT. From a linguistics point of view, this is largely true. English requires verbs to be inflected for tense in a finite clause, and uses auxiliaries to express different aspects.\nThe model we used is similar to the one in section 5.2, but only one branch should be enough because only one entity is mentionedd in text. However we still use two branches in our implementation. The extra branch processes the top-down sequence of words embeddings, from the sentence root to the entity.\nAgain, we found the training files usually do not contain many TLINKS between events and DCT, even if the temporal relations are clear. In spite of that, we hope the patterns are relatively easy to learn even if training data does not appear to be abundant."}, {"heading": "5.5 Pruning TLINKs", "text": "The four models deal with TLINKs in different conditions. They only produce pairwise relations and do not overlap, and thus will not produce conflicts if we only look at the output TLINK labels alone. Nevertheless some temporal relations are transitive in nature, so the deduced relations from given TLINKs can be in conflict.\nMost conflicts are potentially from two types of relations. One is the BEFORE/AFTER relation, and the other is the INCLUDES/IS INCLUDED relation. Naturally, we can convert TLINKs of opposite relations and put them all together. If we use a directed graph to represent the BEFORE relations between all entities, it should be acyclic.Sun (Sun, 2014) proposed a strategy that \u201cprefers the edges that can be inferred by other edges in the graph and remove the ones that are least so\u201d. Another strategy (Chambers et al., 2014a), based on results from separate classifiers (\u201csieves\u201d in their term), is to rank results according to their precisions. High-ranking results overwrite low-ranking ones.\nWe follow the same idea of purging the weak TLINKs and retaining the strong ones. Given a di-\nrected graph, our approach is to remove the edges to break cycles, so that the sum of weights from the removed edges is minimal. This problem is formulated as follows: given a directed weighted graph, find the maximum acyclic subgraph. This problem is actually an extension of the minimum feedback arc set problem and is NP-hard (Karp, 1972). We therefore adopt a heuristic-based approach instead, which we apply separately to the graphs induced by BEFORE/AFTER and INCLUDE/IS INCLUDED relations.2 The softmax layer provides a probability score for each relation class, which we use to represent the strength of a link. Since TLINKs between TIMEX pairs are generated by rules, we assume them to be reliable and assign them a score of 1.\nFor a given relation (e.g., BEFORE), we incrementally build a directed graph with all edges representing that relation. We add event vertices to this graph in a random order. Then for each event, we add all edges associated with it. If this creates a cycle, we remove the edges one by one until there is no cycle, keeping track of the sum of scores associated with the removed edges. We choose the order in which the edges are removed to minimize that value.3 The algorithm is shown below.\nX \u2190 EVENTS; V \u2190 TIMEXes; E \u2190 TIMEX pairs; Initialize G\u2190< V,E >; for x\u2208 X do\nV \u2032 \u2190 V + {x}; C \u2190 {(x, v) \u222a (v, x)|v \u2208 V } ; E\u2032 \u2190 E \u222a C ; G\u2032 \u2190< V \u2032, E\u2032 > ; if cycle exists(G\u2019) then\nfor Ci \u2208 \u03c0(C) do scorei = 0; while Ci 6= \u03c6 & cycle exists(G \u222a Ci)\ndo c\u2190 Ci.pop(); scorei+ = weight(c);\nend end\nend G\u2190 G \u222a Ci s.t. i = argmin(scorei);\nend Algorithm 1: Algorithm to prune edges. \u03c0(C) denotes permutations of C, where C is a list of weighted edges.\n2We found that ENDS and BEGINS TLINKs are too infrequent to warrant a separate treatment.\n3By \u201cremoving\u201d an edge we mean resetting the relation to NO-LINK. Another possibility may be to set the relation associated with the edge to the one with the second highest probability score, however this may create additional cycles.\nNote that in many cases the greedy approach which would first attempt to drop the edges with the smallest weights suffices, however, in our experiments, we found that attempting all edge permutations produces better overall performance. Practically, we found the vertices do not have high degrees, so permuting the candidates does not slow down the program. We add the events to the graph in a random order, except that we initialize the graph with all TIMEX relations (each with weight 1.0). In preliminary experiments, we found no evidence that adding events in the narrative order or adding the events associated with the most TLINKs first produces an improvement over the random-order strategy."}, {"heading": "6 Experiments", "text": "Each model involves a number of hyperparameters. It is impossible to perform a full grid search, but we refer to previous research as starting points, and vary the hyperparameters to see the effects.\nWe use the word2vec-GoogleNews-vectors4 for all models requiring word embeddings. It was pretrained on Google News corpus (3 billion running words), with the word2vec model. It contains 3 million English words and each word vector has a dimensionality of 300. Our neural network models are written in Keras on top of Theano. Training and testing run on a GPU-enabled work station."}, {"heading": "6.1 Timex and Event Annotation", "text": "Table 2 shows the performance of HeidelTime. We only tag positions, and partial overlap is considered wrong.\nThe LSTM layer of the event extraction model contains 128 LSTM units. The hidden layer on top of that has 30 neurons. The input layer corresponding to the 4 token features is connected with a hidden layer with 3 neurons. The combined hidden layer is then connected with a single-neuron output layer. We set a dropout rate 0.5 on input layer, and another drop out rate 0.5 on the hidden layer before output.\n4https://github.com/mmihaltz/ word2vec-GoogleNews-vectors\nThe evaluation rate is also shown in Table 2. We intentionally boost the recall even if the precision has to sacrifice. In order to answer questions about temporal relations, it is not particularly harmful to have redundant events, but missing an event makes it impossible to answer any question related to it."}, {"heading": "6.2 Intra-Sentence Model", "text": "We identify 12 classes of temporal relations, plus a NO-LINK class. Since we have not addressed event coreference, we combined SIMULTANEOUS and IDENTITY. For training, we downsampled NO-LINK class to 50% of the number of positive instances. In addition to the usual class imbalance issues, downsampling also addresses the fact that TimeML-style annotation is de-facto sparse, with only a tiny fraction of positive instances annotated.\nThe LSTM layer of the intra-sentence model contains 256 LSTM units on each branch. The hidden layer on top of that has 100 neurons. We set a dropout rate 0.6 on input layer, and another drop out rate 0.5 on the hidden layer before output."}, {"heading": "6.2.1 Double-checking", "text": "We introduce a technique to boost the recall of positive classes and reducing misclassifications among opposite classes. Since entity pairs are always classified in both orders, if both results are positive (not NO-LINK), we adopt the label with a higher probability score, as assigned by the softmax classifier. We call this technique \u201cdoublechecking\u201d. One effect of this is to reduce the errors that are fundamentally harmful e.g. BEFORE misclassified as AFTER, and vice versa. In addition to that, we also allow a positive class to have the \u201cveto power\u201d against NO-LINK class. For instance, if our model predicts (e1, e2) \u2192 NO \u2212 LINK but (e2, e1) \u2192 AFTER, then we adopt the result (e2, e1)\u2192 AFTER.\nAs illustrated in Table 3, downsampling the NO-LINK cases can largely boost the recall of\npositive classes. Using the double-check technique not only further boost recalls, but also reduce the misclassifications between opposite classes. This is not the final checkpoint for conflicting classifications. As described in section 5.5, we have a pruning algorithm to resolve conflicts at the end. Results these configurations produce in the QA-based evaluation will be shown in the next section."}, {"heading": "6.3 Cross-Sentence Model", "text": "The LSTM layer of the cross-sentence model contains 256 LSTM units on each branch. The hidden layer on top of that has 100 neurons. We set a dropout rate 0.6 on input layer, and another drop out rate 0.5 on the hidden layer before output.\nThe training and evaluation procedures are very similar to what we did for intra-sentence models. Now the vast majority of entity pairs have no TLINKs explicitly marked in training data. Unlike the intra-sentence scenario, however, a NO-LINK label is truly adequate in most cases. From the two sentences alone, it is often true that a reader cannot interpret the temporal relations between to the entities across sentence boundary. That being said, there are still many false negs and it is not feasible to evaluate the systems in any straightforward way. We found that downsampling NO-LINK instances to match the number of all positive instances (ratio=1) yields desirable results. Since positive instances are very sparse in the training data and validation data, the ratio should not be too low otherwise overfitting would be a concern."}, {"heading": "6.4 DCT Model", "text": "The hyperparameters for the DCT model are the same the ones we use for intra-sentence and crosssentence models. Again, the training files do not sufficiently annotate TLINKs with DCT even if the relations are clear, so there are many false negatives. We downsample the NO-LINK instances so that they are 4 times more than positive instances."}, {"heading": "7 Results and Discussion", "text": "This section presents the QA-based evaluation results. We will first present the results from validation data, with different model configurations. We will then show the results on test data."}, {"heading": "7.1 Evaluation on Validation data", "text": "The objective of the evaluation here is model selection. It is impossible to show all the intermediate results, so we will selectively highlight the ones of interest.\nThe QA toolkit contains 79 yes-no questions about temporal relations of entities in the validation data. Originally only 6 questions have \u201cno\u201d as the correct answer, and 1 question has \u201cunknown\u201d. After investigating the questions and answers, however, we found some errors and typos. After fixing the errors, there are 7 no-questions and 72 yes-questions in total. All evaluations are performed after fixing the bugs. We report results in two decimal digits because the previous publication (Llorens et al., 2015a) did so.\nThe evaluation tool draws answers from the annotations only. If an entity (event or timex) involved in a question is not annotated, or the TLINK cannot be found, then the answer will be \u201cunknown\u201d. The question will then be counted as not answered. There is no way for participants to give any answer directly, other than delivering the annotations. The program generates Timegraphs to infer relations from the annotated TLINKs. As a result, relations without explicit TLINK labels can still be counted in, if they can be inferred from the annotations. The measures are defined in below: coverage = #answered#questions , precision = #correct #answered\nrecall = #correct#questions , f1 = 2\u00d7precision\u00d7recall precision+recall\nIn Table 4, the four systems above the horizontal line are given by the task organizer. Among them,\nThe top two are systems annotated by human experts. As we can see, the precisions are very high, both above 0.9. Our automated systems cannot reach that precision. In spite of the lower precisions, automated systems can have much higher coverages i.e. answer a lot more questions.\nAs a starting point, we evaluated the validation files in their original form. The precision is good, but the coverage is very low. This supports our claim that the TLINKs provided by the training/validation files are not complete. We also tried using the event and timex tags from the validation data, but performing TLINK classification with our system. Now the coverage raises to 64, and the overall F1 score reaches 0.52. The TEA-initial system uses our own annotators. The performance is similar, with a slight improvement in precision. This result shows our event and timex tags work well, not inferior to the ones provided by training data. The double-check technique boosts the coverage a lot, probably because we allow positive results to veto NO-LINKs. Combining doublecheck with the pruning technique yields the best results, with F1 score 0.58. Now 42 out of 79 questions can be answered correctly."}, {"heading": "7.2 Evaluation on Test Data", "text": "The method to evaluate results on test data is basically the same as on validation data. Table 5 shows the the statistics of test data. As we can see, the vast majority of the questions should be answered with yes. Generally speaking, it is much more difficult to validate a specific relation (answer yes) than to reject it (answer no) when we have as many as 12 types of relations in addition to the vague NO-LINK class. dist- means questions involving entities that are in the same sentence or in consecutive sentences. dist+ means the entities are farther away.\nThe organizer had two ways to evaluate systems. The first one is exactly the same as we did on validation data. The second one used a so-called Time Expression Reasoner (TREFL) to add relations between timexes, and evaluate the augmented results. The goal of such an extra run\nis to \u201canalyze how a general time expression reasoner could improve results\u201d. Our model already includes a component to handle timex relations, so we will compare our results with other systems\u2019 in both methods.\nThe results are shown in Table 6. The hlt-fbk systems were participants with top performances. Among them, hlt-fbk-ev2-trel2 was the overall winner of TempEval task in 2015. ClearTK, CAEVO, TIPSEMB and TIPSem were some offthe-shelf systems provided by the organizer for reference. These systems were not optimized for the task (Llorens et al., 2015a).\nFor news and Wikipedia genres, our system outperforms all other systems by a large margin. For blogs genre, however, the advantage of our system is unclear. Remember our training set contains news articles only. While the trained model works well on Wikipedia dataset too, blog dataset is fundamentally different in a couple of ways: (1) Each blog article is very short. (2) The style of writing in blogs is much more informal, with non-\nstandard spellings and punctuations. (3) Blogs are written in first person, and the content is usually personal stories and feelings.\nThe hlt-fbk-ev2-trel2 system implements event coreference, which seems to contribute to their relatively good performance. Our model has not integrated event coreference yet, yet performs better. It is possible that the deep learning mechanisms capture elements of co-references.\nThe extra evaluation with TREFL has a postprocessing to add TLINKs between timex entities. Our model already employs such a strategy, so this post-processing does not help. In fact, it drags down the scores a little.\nTable 7 summarizes the results over all genres before and after applying TREFL. To save space, the table only includes the top participant hlt-fbk-ev2-trel2, which had the best scores in 2015. As we can see, TEA generally has significantly higher scores than others. As mentioned before, the performance is largely dragged down by the blog dataset. If we only consider news and Wikipedia datasets, which are more appropriate for the trained models, TEA actually answered 101 out of 229 questions correctly, which is 44% in percentage."}, {"heading": "8 Conclusion", "text": "We have proposed a method that takes a relatively simple LSTM-based architecture which uses shortest dependency paths as input and redeploys it in a set of subtasks needed for extraction of temporal relations from text. We also introduce (1) a \u201cdouble-checking\u201d technique which reverses pairs in classification, thus boosting the recall of positives and reducing misclassifications among opposite classes and (2) an efficient pruning algorithm for resolving conflicting TLINKs. In a QA-based evaluation, our proposed method which does not address event coreference explicitly outperforms state-of-the-art methods which do address it by a large margin."}], "references": [{"title": "Ixa pipeline: Efficient and ready to use multilingual nlp tools", "author": ["Rodrigo Agerri", "Josu Bermudez", "German Rigau."], "venue": "Proceedings of the 9th Language Resources and Evaluation Conference (LREC2014). pages 26\u201331. http://ixa2.si.ehu.es/ixa-", "citeRegEx": "Agerri et al\\.,? 2014", "shortCiteRegEx": "Agerri et al\\.", "year": 2014}, {"title": "Semeval-2015 task 6: Clinical tempeval", "author": ["Steven Bethard", "Leon Derczynski", "James Pustejovsky", "Marc Verhagen."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Association for Computational Lin-", "citeRegEx": "Bethard et al\\.,? 2015", "shortCiteRegEx": "Bethard et al\\.", "year": 2015}, {"title": "Dense event ordering with a multipass architecture", "author": ["Nathanael Chambers", "Taylor Cassidy", "Steven Bethard."], "venue": "Transactions of the Association for Computational Linguistics 2:273\u2013284.", "citeRegEx": "Chambers et al\\.,? 2014a", "shortCiteRegEx": "Chambers et al\\.", "year": 2014}, {"title": "Dense event ordering with a multi-pass architecture", "author": ["Nathanael Chambers", "Taylor Cassidy", "Bill McDowell", "Steven Bethard."], "venue": "Transactions of the Association for Computational Linguistics 2:273\u2013 284.", "citeRegEx": "Chambers et al\\.,? 2014b", "shortCiteRegEx": "Chambers et al\\.", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Reducibility among combinatorial problems", "author": ["Richard Karp."], "venue": "Complexity of Computer Computations, Proc. Sympos.. pages 85\u2013103.", "citeRegEx": "Karp.,? 1972", "shortCiteRegEx": "Karp.", "year": 1972}, {"title": "Semeval-2015 task 5: Qa tempeval - evaluating temporal information understanding with question answering", "author": ["Hector Llorens", "Nathanael Chambers", "Naushad UzZaman", "Nasrin Mostafazadeh", "James Allen", "James Pustejovsky."], "venue": "Proceedings of", "citeRegEx": "Llorens et al\\.,? 2015a", "shortCiteRegEx": "Llorens et al\\.", "year": 2015}, {"title": "Semeval-2015 task 5: Qa tempeval-evaluating temporal information understanding with question answering", "author": ["Hector Llorens", "Nathanael Chambers", "Naushad UzZaman", "Nasrin Mostafazadeh", "James Allen", "James Pustejovsky."], "venue": "Proceed-", "citeRegEx": "Llorens et al\\.,? 2015b", "shortCiteRegEx": "Llorens et al\\.", "year": 2015}, {"title": "Semeval-2015 task 4: Timeline: Cross-document event ordering", "author": ["Anne-Lyse Minard", "Manuela Speranza", "Eneko Agirre", "Itziar Aldabe", "Marieke van Erp", "Bernardo Magnini", "German Rigau", "Rub\u00e9n Urizar", "Fondazione Bruno Kessler."], "venue": "In", "citeRegEx": "Minard et al\\.,? 2015", "shortCiteRegEx": "Minard et al\\.", "year": 2015}, {"title": "Extracting temporal and causal relations between events", "author": ["Paramita Mirza."], "venue": "CoRR abs/1604.08120. http://arxiv.org/abs/1604.08120.", "citeRegEx": "Mirza.,? 2016", "shortCiteRegEx": "Mirza.", "year": 2016}, {"title": "Timeml: Robust specification", "author": ["ham Katz"], "venue": null, "citeRegEx": "Katz.,? \\Q2003\\E", "shortCiteRegEx": "Katz.", "year": 2003}, {"title": "Time Well Tell: Temporal Reason", "author": [], "venue": null, "citeRegEx": "Sun.,? \\Q2014\\E", "shortCiteRegEx": "Sun.", "year": 2014}, {"title": "Classifying relations via long", "author": ["Zhi Jin"], "venue": null, "citeRegEx": "Jin.,? \\Q2015\\E", "shortCiteRegEx": "Jin.", "year": 2015}, {"title": "Relation classification via", "author": ["Jun Zhao"], "venue": null, "citeRegEx": "Zhao.,? \\Q2014\\E", "shortCiteRegEx": "Zhao.", "year": 2014}, {"title": "Relation classification via recurrent neural network", "author": ["Dongxu Zhang", "Dong Wang."], "venue": "CoRR abs/1508.01006. http://arxiv.org/abs/1508.01006.", "citeRegEx": "Zhang and Wang.,? 2015", "shortCiteRegEx": "Zhang and Wang.", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "QA-TempEval (Llorens et al., 2015a), which used question answering (QA) as the target application.", "startOffset": 12, "endOffset": 35}, {"referenceID": 1, "context": "A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al.", "startOffset": 149, "endOffset": 284}, {"referenceID": 7, "context": "A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al.", "startOffset": 149, "endOffset": 284}, {"referenceID": 8, "context": "A multitude of TemporalIE systems have been developed over the past decade both in response to the series of shared tasks organized by the community (Verhagen et al., 2007, 2010; UzZaman et al., 2012; Sun et al., 2013; Bethard et al., 2015; Llorens et al., 2015b; Minard et al., 2015) and in standalone efforts (Chambers et al.", "startOffset": 149, "endOffset": 284}, {"referenceID": 3, "context": ", 2015) and in standalone efforts (Chambers et al., 2014b; Mirza, 2016).", "startOffset": 34, "endOffset": 71}, {"referenceID": 9, "context": ", 2015) and in standalone efforts (Chambers et al., 2014b; Mirza, 2016).", "startOffset": 34, "endOffset": 71}, {"referenceID": 2, "context": "specific models using traditional statistical learning, typically used in succession (Sun et al., 2013; Chambers et al., 2014a).", "startOffset": 85, "endOffset": 127}, {"referenceID": 14, "context": ", 2014) and recurrent neural networks both have been used for argument relation classification and similar tasks (Zhang and Wang, 2015; Xu et al., 2015; Vu et al., 2016).", "startOffset": 113, "endOffset": 169}, {"referenceID": 14, "context": ", 2014) and recurrent neural networks both have been used for argument relation classification and similar tasks (Zhang and Wang, 2015; Xu et al., 2015; Vu et al., 2016). We take inspiration from some of this work, including specifically the approach proposed proposed by Xu et al. (2015).", "startOffset": 114, "endOffset": 289}, {"referenceID": 0, "context": "Our preprocessing includes tokenization, part of speech tagging, and dependency tree parsing, which is done with NewsReader (Agerri et al., 2014) for preprocessing.", "startOffset": 124, "endOffset": 145}, {"referenceID": 4, "context": "The extraction model uses long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) which is a popular RNN architecture to process time series.", "startOffset": 56, "endOffset": 90}, {"referenceID": 11, "context": "Sun (Sun, 2014) proposed a strategy that \u201cprefers the edges that can be inferred by other edges in the graph and remove the ones that are least so\u201d.", "startOffset": 4, "endOffset": 15}, {"referenceID": 2, "context": "Another strategy (Chambers et al., 2014a), based on results from separate classifiers (\u201csieves\u201d in their term), is to rank results according to their precisions.", "startOffset": 17, "endOffset": 41}, {"referenceID": 5, "context": "This problem is actually an extension of the minimum feedback arc set problem and is NP-hard (Karp, 1972).", "startOffset": 93, "endOffset": 105}, {"referenceID": 6, "context": "We report results in two decimal digits because the previous publication (Llorens et al., 2015a) did so.", "startOffset": 73, "endOffset": 96}, {"referenceID": 6, "context": "Adapted from (Llorens et al., 2015a) Table 1.", "startOffset": 13, "endOffset": 36}, {"referenceID": 6, "context": "These systems were not optimized for the task (Llorens et al., 2015a).", "startOffset": 46, "endOffset": 69}], "year": 2017, "abstractText": "In this paper, we propose to use a set of simple, uniform in architecture LSTMbased models to recover different kinds of temporal relations from text. Using the shortest dependency path between entities as input, the same architecture is used to extract intra-sentence, crosssentence, and document creation time relations. A \u201cdouble-checking\u201d technique reverses entity pairs in classification, boosting the recall of positive cases and reducing misclassifications between opposite classes. An efficient pruning algorithm resolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our proposed technique outperforms state-ofthe-art methods by a large margin.", "creator": "LaTeX with hyperref package"}}}