{"id": "1703.05916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Mar-2017", "title": "Construction of a Japanese Word Similarity Dataset", "abstract": "An evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.", "histories": [["v1", "Fri, 17 Mar 2017 07:53:03 GMT  (36kb,D)", "http://arxiv.org/abs/1703.05916v1", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yuya sakaizawa", "mamoru komachi"], "accepted": false, "id": "1703.05916"}, "pdf": {"name": "1703.05916.pdf", "metadata": {"source": "CRF", "title": "Construction of a Japanese Word Similarity Dataset", "authors": ["Yuya Sakaizawa"], "emails": ["sakaizawa-yuya@ed.tmu.ac.jp", "komachi@tmu.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Traditionally, a word is represented as a sparse vector indicating the word itself (one-hot vector) or the context of the word (distributional vector). However, both the one-hot notation and distributional notation suffer from data sparseness since dimensions of the word vector do not interact with each other. Distributed word representation addresses the data sparseness problem by constructing a dense vector of a fixed length, wherein contexts are shared (or distributed) across dimensions. Distributed word representation is known to improve the performance of many NLP applications such as machine translation (Chen and Guo, 2015) and sentiment analysis (Tai et al., 2015) to name a few. The task to learn a distributed representation is called representation learning.\nHowever, evaluating the quality of learned distributed word representation itself is not straightforward. In language modeling, perplexity or\ncross-entropy is widely accepted as a de facto standard for intrinsic evaluation. In contrast, distributed word representations include the additive (or compositional) property of the vectors, which cannot be assessed by perplexity. Moreover, perplexity makes little use of infrequent words; thus, it is not appropriate for evaluating distributed presentations that try to represent them.\nTherefore, a word similarity task and/or a word analogy task are generally used to evaluate distributed word representations in the NLP literature. The former judges whether distributed word representations improve modeling contexts, and the latter estimates how well the learned representations achieve the additive property. However, such resources other than for English (e.g., Japanese) seldom exist. In addition, most of these datasets comprise high-frequency nouns so that they tend not to include other parts of speech. Therefore, previous data fail to evaluate word representations of other parts of speech, including content words such as verbs and adjectives.\nTo address the problem of the lack of a dataset for evaluating Japanese distributed word representation, we propose to build a dataset for evaluating the word similarity task of Japanese. Although our goal is to entirely analyze Japanese distributed word representations, it is not limited to evaluating Japanese distributed word representations.\nThe main contributions of our work are as follows:\n\u2022 To the best of our knowledge, it is the first work that constructs a Japanese word similarity dataset.\n\u2022 The dataset contains various parts of speech and includes rare words in addition to common words.\nar X\niv :1\n70 3.\n05 91\n6v 1\n[ cs\n.C L\n] 1\n7 M\nar 2\n01 7"}, {"heading": "2 Related Work", "text": "In general, distributed word representations are evaluated using a word similarity task. For instance, WordSim353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) have been used to evaluate word similarities in English. Moreover, Baker et al. (2014) built a verb similarity dataset (VSD) based on WordSim353 because there is no dataset of verbs in the word-similarity task.\nIn addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models. However, low-frequency words and unknown words are often comprise high-frequency morphemes (e.g., unkingly \u2192 un + king + ly). Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words (Luong et al., 2013; Soricut and Och, 2015)."}, {"heading": "3 Construction of a Japanese Word Similarity Dataset", "text": "What makes a pair of words similar? Most of the previous datasets do not concretely define the similarity of word pairs. The difference in the similarity of word pairs originates from each annotator\u2019s mind, resulting in different scales of a word. Thus, we propose to use an example-based approach (Table 2) to control the variance of the similarity rat-\nings. We remove the context of word when we extracted the word. So, we consider that an ambiguous word has high variance of the similarity, but we can get low variance of the similarity when the word is monosemous.\nFor this study, we constructed a Japanese word similarity dataset1. We followed the procedure used to construct the Stanford Rare Word Similarity Dataset (RW) (Luong et al., 2013).\nWe extracted Japanese word pairs from the Evaluation Dataset of Japanese Lexical Simplification (Kodaira et al., 2016). It targeted content words (nouns, verbs, adjectives, adverbs). It included 10 contexts about target words annotated with their lexical substitutions and rankings. Figure 1 shows an example of the dataset. A word in square brackets in the text is represented as a target word of simplification. A target word is not only recorded in the lemma form but also in the conjugated form. We built a Japanese similarity dataset from this dataset using the following procedure.\nWord selection: First, paraphrase candidates were extracted from this dataset. Because the construction process of the simplification dataset was divided into a paraphrase acquisition phase and a simplification ranking phase, we simply discarded the simplification rankings from the dataset to obtain paraphrase candidates. Table 1 shows the frequency of extracted words in the Japanese Wikipedia as of May 2015. As shown in the table, low-frequency words are included in the dataset.\nPair construction: Because extracted words are annotated with their paraphrase candidates, we picked up each pair from the candidate as a word pair. Consequently, we acquired 5,051 verb pairs, 4,033 adjective pairs, 1,528 noun pairs and 902 adverb pairs. To balance the numbers of verb and adjective pairs with other parts of speech, we extracted samples at random for verbs and adjectives. Finally, we obtained 1,464 verb pairs and 960 adjective pairs.\n1https://github.com/tmu-nlp/ JapaneseWordSimilarityDataset\nWe observed that the similarity of the pairs extracted from the dataset of Kodaira et al. (2016) was low without providing contexts; thus, we did not augment the dataset by inserting pseudonegative instances from WordNet\u2019s synsets, as was done in the RW corpus. nyther reason why we did not employ the synset from the Japanese WordNet (Isahara et al., 2008) was because its quality was not as good as the English WordNet except for concrete nouns2.\nHuman judgment: We opted to use the crowdsourcing service (Lancers3) to hire native Japanese speakers. We asked annotators to assign the degree of similarity for each pair using the same 10-point scale4. We used only those annotators who were able to complete at least 95% of their previous assignments correctly. We collected similarity rating for each word pair from ten annotators and defined the average of their annotations as the similarity of the pairs.\nAlthough (Kodaira et al., 2016) gave the annotators the context during annotation, we removed the context and gave only pairs to annotators. We did so because the previous datasets such as VSD and RW did not present any context during annotation5. To improve the quality of the annotation, we presented an example of the degree of similarity of the pairs during annotation (Table 2). Consequently, we collected 4,851 pairs overall. Table 4 shows an example of a pair from our dataset. Inter-annotator agreements (IAA) of each POS are shown in Table 3. The inter-annotator agreement\n2It might be because it was translated from the English WordNet. This is why we decided not to translate the existing English word similarity dataset to create a Japanese version.\n3http://www.lancers.jp 4In a crowdsourcing request, we indicated that a similarity of pairs with different notations, such as \u201cwrite\uff08\u66f8\u3044\u305f\uff09\u201d and \u201cwrite\uff08\u304b\u3044\u305f\uff09\u201d is 10.\n5Another reason why we did not do so is because the SCWS has a very high variance even though it is annotated with contexts (Table 5).\nis the average Spearman\u2019s \u03c1 between a single annotator and the average of all others."}, {"heading": "4 Discussion", "text": ""}, {"heading": "4.1 Comparison to Other Datasets", "text": "Table 5 shows how several resources vary. WordSim353 comprises high-frequency words and so the variance tends to be low. In contrast, RW includes low-frequency words, unknown words, and complex words composed of several morphemes; thus, the variance is large. VSD has many polysemic words, which increase the variance. Despite the fact that our dataset, similar to the VSD and RW datasets, contains low-frequency and ambiguous words, its variance is 3.00. The variance level is low compared with the other corpora. We considered that the examples of the similarity in the task request reduced the variance level.\nWe did not expect SCWS to have the largest variance in the datasets shown in Table 5 because it gave the context to annotators during annotation. At the beginning, we thought the context would serve to remove the ambiguity and clarify the meaning of word; however after looking into the dataset, we determined that the construction procedure used several extraordinary annotators. It is crucial to filter insincere annotators and provide straightforward instructions to improve the quality of the similarity annotation like we did.\nTo gain better similarity, each dataset should utilize the reliability score to exclude extraordinary annotators. For example, for SCWS, an annotator rating the similarity of pair of \u201cCD\u201d and \u201caglow\u201d assigned a rating of 10. We assumed it was a typo or misunderstanding regarding the words. To address this problem, such an annotation should be removed before calculating the true similarity. All the datasets except for RW simply calculated the average of the similarity, but datasets created using crowdsourcing should consider the reliability of the annotator."}, {"heading": "4.2 Analysis", "text": "We present examples of a pair with high variance of similarity as shown below:\nAspect of relatedness. (e.g., a pairing of \u201cfast\uff08\u901f\u3044\uff09\u201d and \u201cearly\uff08\u65e9\u3044\uff09\u201d.)\nAlthough they are similar in meaning with respect to the time, they have nothing in common with respect to speed; Annotator A assigned a rating of 10, but Annotator B assigned a rating of 1.\nAnother example, the pairing of \u201cbe eager\uff08\u61c7 \u9858\u3059\u308b\uff09\u201d and \u201crequest\uff08\u983c\u3080\uff09\u201d. Even though the act indicated by the two verbs is the same, there are some cases where they express different degrees of feeling. Compared with \u201crequest\u201d, \u201ceager\u201d indicates a stronger feeling. There were two annotators who emphasized the similarity of the act itself rather than the different degrees of feeling, and vice versa. In this case, Annotator A assigned a rating of 9, but Annotator B assigned a rating of 2.\nAlthough we asked annotators to rate the pairs based on semantic similarity, it is not straightforward to put paraphrase candidates onto a single scale considering all the attributes of the words. This limitation might be relaxed if we would ask annotators to refer to a thesaurus or an ontology such as the Japanese Lexicon.\nComparing spell6. (e.g., a pairing of \u201cslogan\uff08\u30b9\u30ed\u30fc\u30ac\u30f3\uff09\u201d and \u201cslogan\uff08\u6a19\u8a9e\uff09\u201d.)\nIn Japanese, we can write a word using hiragana, katakana, or kanji characters; however be-\n6We indicated these pair\u2019s similarity is 10. However, some annotators ignored this instruction. It would be necessary to clean the spellings of paraphrase candidates before requesting similarity annotation.\ncause hiragana and katakana represent only the pronunciation of a word, annotators might think of different words. In this case, Annotator A assigned a rating of 8, but Annotator B assigned a rating of 0. Similarly, we confirmed the same thing in other parts of speech. Especially, nouns have several word pairs with different spellings, so we considered the IAAs that were low.\nFrequency or time expressions. (e.g., a pairing of \u201coften\uff08\u3057\u3070\u3057\u3070\uff09\u201d and \u201cfrequently\uff08\u3057\u304d \u308a\u306b\uff09\u201d.)\nWe confirmed that the variance becomes larger among adverbs expressing frequency. This is due to the difference in the frequency of words that annotators imagines. In this case, Annotator A assigned a rating of 9, but Annotator B assigned a rating of 0. Similarly, we confirmed the same thing among adverbs expressing time."}, {"heading": "5 Conclusion", "text": "In this study, we constructed the first Japanese word similarity dataset. It contains various parts of speech and includes rare words in addition to common words. Crowdsourced annotators assigned similarity to word pairs during the word similarity task. We gave examples of similarity in the task request sent to annotators, so that we reduced the variance of each word pair. However, we did not restrict the attributes of words, such as the level of feeling, during annotation. Error analysis revealed that the notion of similarity should be carefully defined when constructing a similarity dataset.\nAs a future work, we plan to construct a word analogy dataset in Japanese by translating an English dataset to Japanese. We hope that a Japanese database will facilitate research in Japanese distributed representations."}], "references": [{"title": "An Unsupervised Model for Instance Level Subcategorization Acquisition", "author": ["Simon Baker", "Roi Reichart", "Anna Korhonen."], "venue": "EMNLP.", "citeRegEx": "Baker et al\\.,? 2014", "shortCiteRegEx": "Baker et al\\.", "year": 2014}, {"title": "Representation Based Translation Evaluation Metrics", "author": ["Boxing Chen", "Hongyu Guo."], "venue": "ACL.", "citeRegEx": "Chen and Guo.,? 2015", "shortCiteRegEx": "Chen and Guo.", "year": 2015}, {"title": "Investigations on word senses and word usages", "author": ["Katrin Erk", "Diana McCarthy", "Nicholas Gaylord."], "venue": "ACL.", "citeRegEx": "Erk et al\\.,? 2009", "shortCiteRegEx": "Erk et al\\.", "year": 2009}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "ACM.", "citeRegEx": "Finkelstein et al\\.,? 2002", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2002}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "ACL.", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Development of the Japanese WordNet", "author": ["Hitoshi Isahara", "Fransis Bond", "Kiyotaka Uchimoto", "Masao Utiyama", "Kyoko Kanzaki."], "venue": "LREC.", "citeRegEx": "Isahara et al\\.,? 2008", "shortCiteRegEx": "Isahara et al\\.", "year": 2008}, {"title": "Controlled and Balanced Dataset for Japanese Lexical Simplification", "author": ["Tomonori Kodaira", "Tomoyuki Kajiwara", "Mamoru Komachi."], "venue": "ACL Student Research Workshop.", "citeRegEx": "Kodaira et al\\.,? 2016", "shortCiteRegEx": "Kodaira et al\\.", "year": 2016}, {"title": "Better Word Representations with Recursive Neural Networks for Morphology", "author": ["Minh-Thang Luong", "Richard Socher", "Christopher D. Manning."], "venue": "CoNLL.", "citeRegEx": "Luong et al\\.,? 2013", "shortCiteRegEx": "Luong et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "WordNet: A Lexical Database for English", "author": ["George A. Miller."], "venue": "Commun. ACM .", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Contextual Correlates of Semantic Similarity", "author": ["George A Miller", "Walter G Charles."], "venue": "Language and Cognitive Processes .", "citeRegEx": "Miller and Charles.,? 1991", "shortCiteRegEx": "Miller and Charles.", "year": 1991}, {"title": "Contextual Correlates of Synonymy", "author": ["Herbert Rubenstein", "John B. Goodenough."], "venue": "Commun. ACM .", "citeRegEx": "Rubenstein and Goodenough.,? 1965", "shortCiteRegEx": "Rubenstein and Goodenough.", "year": 1965}, {"title": "Unsupervised Morphology Induction Using Word Embeddings", "author": ["Radu Soricut", "Franz Och."], "venue": "NAACL.", "citeRegEx": "Soricut and Och.,? 2015", "shortCiteRegEx": "Soricut and Och.", "year": 2015}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning."], "venue": "ACL.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Distributed word representation is known to improve the performance of many NLP applications such as machine translation (Chen and Guo, 2015) and sentiment analysis (Tai et al.", "startOffset": 121, "endOffset": 141}, {"referenceID": 13, "context": "Distributed word representation is known to improve the performance of many NLP applications such as machine translation (Chen and Guo, 2015) and sentiment analysis (Tai et al., 2015) to name a few.", "startOffset": 165, "endOffset": 183}, {"referenceID": 6, "context": "Figure 1: An example of the dataset from a previous study (Kodaira et al., 2016).", "startOffset": 58, "endOffset": 80}, {"referenceID": 3, "context": "For instance, WordSim353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 25, "endOffset": 51}, {"referenceID": 10, "context": ", 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 12, "endOffset": 38}, {"referenceID": 11, "context": ", 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al.", "startOffset": 43, "endOffset": 76}, {"referenceID": 4, "context": ", 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) have been used to evaluate word similarities in English.", "startOffset": 87, "endOffset": 107}, {"referenceID": 0, "context": "Moreover, Baker et al. (2014) built a verb similarity dataset (VSD) based on WordSim353 because there is no dataset of verbs in the word-similarity task.", "startOffset": 10, "endOffset": 30}, {"referenceID": 7, "context": "Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words (Luong et al., 2013; Soricut and Och, 2015).", "startOffset": 149, "endOffset": 192}, {"referenceID": 12, "context": "Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words (Luong et al., 2013; Soricut and Och, 2015).", "startOffset": 149, "endOffset": 192}, {"referenceID": 7, "context": "We followed the procedure used to construct the Stanford Rare Word Similarity Dataset (RW) (Luong et al., 2013).", "startOffset": 91, "endOffset": 111}, {"referenceID": 6, "context": "We extracted Japanese word pairs from the Evaluation Dataset of Japanese Lexical Simplification (Kodaira et al., 2016).", "startOffset": 96, "endOffset": 118}, {"referenceID": 5, "context": "nyther reason why we did not employ the synset from the Japanese WordNet (Isahara et al., 2008) was because its quality was not as good as the English WordNet except for concrete nouns2.", "startOffset": 73, "endOffset": 95}, {"referenceID": 5, "context": "We observed that the similarity of the pairs extracted from the dataset of Kodaira et al. (2016) was low without providing contexts; thus, we did not augment the dataset by inserting pseudonegative instances from WordNet\u2019s synsets, as was done in the RW corpus.", "startOffset": 75, "endOffset": 97}, {"referenceID": 6, "context": "Although (Kodaira et al., 2016) gave the annotators the context during annotation, we removed the context and gave only pairs to annotators.", "startOffset": 9, "endOffset": 31}], "year": 2017, "abstractText": "An evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.", "creator": "LaTeX with hyperref package"}}}