{"id": "1703.06283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2017", "title": "Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters", "abstract": "As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such \"in-the-tail\" data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (around 1000 images). To explore large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right \"priors\" or parameters for synthesis: we would like realistic data with realistic poses and object configurations. Inspired by Generative Adversarial Networks, we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem Synthetic Imposters. We demonstrate that this pipeline allows one to generate realistic training data by making use of rendering/animation engines. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Synthetic Imposters can also be used for \"in-the-tail\" validation at test-time, a notoriously difficult challenge for real-world deployment.", "histories": [["v1", "Sat, 18 Mar 2017 10:52:53 GMT  (1970kb)", "http://arxiv.org/abs/1703.06283v1", "To appear in CVPR 2017"], ["v2", "Mon, 10 Apr 2017 14:59:25 GMT  (3308kb,D)", "http://arxiv.org/abs/1703.06283v2", "To appear in CVPR 2017"]], "COMMENTS": "To appear in CVPR 2017", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["shiyu huang", "deva ramanan"], "accepted": false, "id": "1703.06283"}, "pdf": {"name": "1703.06283.pdf", "metadata": {"source": "CRF", "title": "Recognition in-the-Tail: Training Detectors for Unusual Pedestrians with Synthetic Imposters", "authors": ["Shiyu Huang", "Deva Ramanan"], "emails": ["huangsy13@mails.tsinghua.edu.cn", "deva@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n06 28\n3v 1\n[ cs\n.C V\n] 1\n8 M\nAs autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such \u201cinthe-tail\u201d data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (\u2248 1000 images). To explore largescale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right \u201cpriors\u201d or parameters for synthesis: we would like realistic data with realistic poses and object configurations. Inspired by Generative Adversarial Networks, we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem Synthetic Imposters. We demonstrate that this pipeline allows one to generate realistic training data by making use of rendering/animation engines. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Synthetic Imposters can also be used for \u201cin-the-tail\u201d validation at test-time, a notoriously difficult challenge for real-world deployment."}, {"heading": "1. Introduction", "text": "As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on \u201ceveryday\u201d scenes of people engaged in typical walking\nposes on sidewalks [9, 6, 12, 11, 40]. However, perhaps the most important operating point for a deployable system is its behaviour in dangerous, unexpected scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways.\nPrecarious Pedestrian Dataset: Such \u201cin-the-tail\u201d data is notoriously hard to observe, making both training and evaluation of existing systems difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian Dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (\u2248 1000 images). To explore large-scale data-driven learning, we explore the use of synthetic data generated by a game engine. Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34]. Particularly attractive are approaches that combine a large amount of synthetic training data with a small amount of real data (that may have been difficult to acquire and/or label).\nChallenges in synthesis: We see two primary difficulties with the use of synthetic training data. The first is that not all data is created \u201cequal\u201d: when combining synthetic data with real data, synthesizing common scenes may not be particularly useful since they will likely already appear in the training set. Hence we argue that the real power of synthetic data is generating examples \u201cin-the-tail\u201d, which would otherwise have been hard to collect. The second difficulty arises in building good generative models of images, a notoriously difficult problem. Rather than building generative pixel-level models, we make use of state-of-the-art rendering/animation engines that contain an immense amount of knowledge (about physics, light transfer, etc.). The challenge of generative synthesis then lies in constructing the right \u201cpriors\u201d, or scene-parameters, to render/animate. In our case, these correspond to body poses and spatial configurations of people and other objects (e.g., light) in the scene.\nSynthetic imposters: We address both concerns with a novel variant of Generative Adversarial Networks (GANs) [14], a method for synthesizing data from latent noise vectors. Because defining a loss for the \u201crealness\u201d of a synthetic image is difficult, GANs ingeniously define an implicit loss by learning a second-stage (deep) discriminator that is trained to discriminate between real and synthetic images. Both the synthesizer and discriminator are deep networks that are jointly trained using adversarial targets. In our case, we do not wish to learn a generative model because we plan to make use of highly tuned rendering/animation packages. Specifically, we define a rendering pipeline that takes an input a fixed dimensional latent vector capturing camera parameters and object attributes (such as position, attribute, pose, etc.). We demonstrate that one can use rejection sampling to construct a set of latent vectors (and their associated renderings) that maximally confuse the discriminator. We call such images \u201csynthetic imposters\u201d, and demonstrate that they act as a rich but realistic set of augmented data to train detectors for precarious pedestrians. Specifically, we use our imposter set to define a simple pipeline for transferring knowledge from synthetic to real images.\nRPN+: We use our dataset of real+imposter images to train a suite of contemporary detectors. We find surprisingly good results with a (to our knowledge) novel variant of region proposal network (RPN) [41] tuned for particular objects (precarious people) rather than a general class of objectness detections. Instead of classifying a sparse set of proposed windows (as nearly all contemporary object detection systems based on RCNN do [31]), this network returns a dense heatmap of pedestrian detections, along with regressed bounding box location for each pixel location in the heatmap. We call this detector RPN+. Our experiments show that our RPN+, trained on real+imposter data, outper-\nforms other detectors trained only on real data.\nValidation: Interestingly, we also demonstrate that our Synthetic Imposter Dataset can be used to rank algorithms, suggesting that our pipeline can also be used for \u201cin-thetail\u201d validation at test-time, a notoriously difficult challenge for real-world deployment.\nContributions: The contribution of our work is as follows: (1) a novel dataset of pedestrians in dangerous situations (Precarious Pedestrians) (2) a general architecture for creating realistic synthetic data \u201cin-the-tail\u201d, for which limited real data can be collected and (3) demonstration of our overall pipeline for the task of pedestrian detection using a novel detector. Our datasets and code can be found here: https://github.com/huangshiyu13/RPNplus."}, {"heading": "2. Related work", "text": "Synthetic Dataset. Synthetic datasets have been used to train and evaluate the performance of computer vision algorithms. Some kinds of ground truth are hard to obtain from hand-labelling, such as optical flow, but easy to access via computer. Adam et al. [20] used a 3D game engine to generate synthetic data and learned a physical intuition to predict the falling progress of block tower. Mayer et al. [23] gave out a benchmark of synthetic data. They designed a computer program to generate synthetic images with disparity, optical flow and disparity change, and evaluate the FlowNet[13]. Richter et al. [32] used synthetic data to improve image segmentation performance. But they just used frames output by a video game and had no ability to control what kind of scenes they could generate. We design the source code ourselves and we have the highest authority to the synthetic data. German Ros et al. [34] used Unity Development Platform to generate a synthetic urban scene dataset. 3D Models for Detection. Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33]. Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19]. Marin et\nal. [22] used a game engine to generate synthetic training data. Pishchulin et al. [28] used 8 HD cameras to scan human body and built real 3D human models. Then they used synthetic data and some labelled real data to train pedestrian detectors. Hattori et al. [17] used 3D modelling software to build a special scene and randomly put 3D models on a special background for pedestrian detection. All of these works just ignored data-shift problem. We should find out the differences between synthetic data and real data, and analyse them in order to make better use of synthetic data. Domain Adaptation for Detectors. Domain Adaptation is the standard strategy to deal with data-shift problem. Large synthetic dataset can be used to bootstrap detectors and then adapted to real data by moving to the target domain distribution. Sun and Saenko [37] used 3D models to train detectors for real objects. Their adaption method is based on the work of Hariharan et al. [16] and can be used only for feature-based detectors. Vazquez et al. [38] used synthetic pedestrian data to generate robust real-world detectors. Their detectors are also feature-based and their method can\u2019t be transplanted to RCNN-based detectors. To the best of our knowledge, we are the first one to study how to transfer RCNN-based detectors from synthetic data to real data. Generative Adversarial Nets. GAN [14] is a deep network which can generate fake images from latent code and train a neural network to discriminate whether one image is real or generated. Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code. Our synthetic data generator can also generate images from vectors. GAN generates images with low resolution and can\u2019t make sure that generated images contain semantic information. However, our generator can generate high resolution images with reasonable semantic information."}, {"heading": "3. Dataset", "text": ""}, {"heading": "3.1. Synthetic Dataset", "text": "With the ongoing development of computer graphics, virtual worlds are becoming increasingly reaslitic. Numerous software packages reduce the barrier for reaslitic content generation. In this project, we use Unity 3D as our basic tool to build synthetic data generator.\nFigure 2 shows 3D human models we use in this project. There are 20 different 3D human models, which contain women, men, cyclist and skate boys. Each 3D model will have some associated animations like jumping, talking, running, cheering and applause, so that we can get kinds of human gestures. We also design a scene showed in Figure 2 to capture 2D images. Unlike Javier et al. [22] who used a single virtual urban city, our synthetic data can achieve the diversity of background by rendering real images. We collect 1726 background images from INRIA dataset [6] and the Internet.\nGenerate Initial Set of Synthetic Data. Firstly, the generator randomly generates some values. These values will be constrained within a certain range and used as the parameters for rendering image. To make all the things easily described as parameters, each background image is given an index and so do the 3D models. Each model has been attached with several animations and each animation will last for certain time, so the type of animations are described as an integer and the time of each animation is described as a floating number. In brief, the needed parameters include the number of the 3D models, their type indexes, their positions, their euclidean angles, their animations, the background image\u2019s index, the light intensity and the light\u2019s an-\ngle. The position will not only decide the location of the 3D model on images but also decide the size(the farther the distance between 3D models and camera, the smaller the 3D models will be). All these parameters will be formed as vectors(a vector will be expressed as z). So, each vector corresponds to one scene. Secondly, the generator uses these vectors to render the scene and captures images at the same time. At last, for each pixel on the image, the generator will judge which kind of types(i.e. 3D models\u2019 types, background) it belongs. The generator can calculate pixel-wise ground truth, but we only use 2D bounding boxes in this project. Table 1 shows the constraints of the parameters. In addition, following conditions should also be met: the maximum overlap between two 3Dmodels is 20%(to avoid congestion) and the location of 3D models should be within the background image. Unlike Hironori et al. [17], we don\u2019t set a special mode to generate synthetic data, and our parameters are consecutive and randomly generated. When we want to add more training data or get a new test dataset, we just need to keep running the generator without changing any setting."}, {"heading": "3.2. Precarious Dataset", "text": "We collect Precarious Dataset from Google Images and Baidu Images. We also add some selected images from MPII Dataset [2]. Finally, there are 951 reasonable images in Precarious Dataset. We then label bounding boxes for each image manually. Precarious Dataset has kinds of scenes and people, such as kids running on the road, people watching their phone when crossing the street, people sliding down, motorcyclists doing dangerous movements, overloaded motorcycle and people with umbrellas. After a rough statistics, there are about four hundred cyclists, fifty motor-bikers, five hundred pedestrians in Precarious Dataset. Figure 6 shows some examples in Precarious Dataset. Compared with Caltech Dataset showed in Fig-\nure 6, Precarious Dataset contains more dangerous situations, and these situations will be a very difficult task for people detectors. Figure 5 shows the comparison of Precarious Dataset and Caltech Dataset in terms of human types."}, {"heading": "3.3. Data Pre-Processing", "text": "Our original synthetic data and precarious data are different in size. For simplicity, we resize all images to a standard size, which is resolution of 960\u00d7720. At first, we keep the aspect ratio of original images and resize them to a maximum width of 720 pixels or a maximum height of 540 pixels. Secondly, we randomly put resized images on a black background, whose resolution is 960\u00d7720. The final images will be in the same size. Because the original image size of Caltech Dataset is 640\u00d7480 and has the same aspect\nratio with target size, we directly resize it to 960\u00d7720. All the image data with standardized resolution can be trained and tested with our detectors."}, {"heading": "4. Proposed Method", "text": "In this section, we will introduce a novel framework named selection model, which can transfer our RCNNbased detector from synthetic data to real data."}, {"heading": "4.1. Selecting Imposters", "text": "Notation: We begin with the definitions of terminologies. For clarity, Table 2 show frequently used notations in this paper. Vectors are denoted by lower-case (x), which will represent images in this paper. A domain D consists of two components: a feature space X and a marginal probability distribution P (x), where x \u2208 X . Given a special domainD, a task T consists of two components: a label space Y and an predictive function f(x), i.e. T = {Y, f(x)}, where y \u2208 Y , and f(x) = Q(y|x) represents the conditional probability distribution.\nIn this paper, we will consider the synthetic data as source domain DS = (XS , PS(xS)) and real data as target domain DT = (XT , PT (xT )) under the assumptions that XS = XT ,YS = YT , PS(xS) 6= PT (xT ), QS(yS |xS) 6= QT (yT |xT ). Our question is how to transfer detectors trained on source domain DS to target domain DT .\nWe define imposter domain DI as the transitional zone between DS and DT . DI is a subdomain of DS , which has similar data distribution with target domain, i.e. PI(xI) \u2248 PT (xT ). However, the conditional distribution QI(yI |xI) andQT (yT |xT ) doesn\u2019t need to equal.\nFramework: We propose a general framework to transfer parameters of detectors from source domain to target domain. The framework is divided into three steps. Step One: Pre-training. In the first step, we use synthetic data to train a detector. This detector is base on source domainDS and can only work on source task TS .\nAlgorithm 1 Selection Model\nInput: Source domain dataset XS and target domain datasetXT Output: Imposter datasetXI \u2282 XS Initialize: Set XI = \u2205. Discriminatively train a CNN selector S(xi) using (xi, ti), where xi \u2208 XS \u22c3 XT , ti = 1 if xi \u2208 XS and ti = 0 if xi \u2208 XT for k = 1 to maxChooseNumber do Select xi fromXS satisfying xi = argminxi |S(x)| LetXI = XI \u22c3 {xi},XS = XS/{xi} end for\nStep Two: Marginal Distribution Adaptation. We then mix imposters chosen from synthetic data with real training data to refine the detector. This is a data augmentation for small set of real data, i.e. source domain DT is expanded by imposter domain DI . We can call this step as marginal distribution adaptation. Since PI(xI) \u2248 PT (xT ) and QI(yI |xI) 6= QT (yT |xT ), DI has similar data distribution with DT , but their conditional distributions are different and data-shift problem still exists. Step Three: Conditional Distribution Adaptation. In the last step, only the real training data will be used to refine the detector. This step can be called as conditional distribution adaptation.\nSelector: We design a selector to choose imposters from synthetic data. Vector z is the parameters input to generator. Also the generator can be formed as a function G(z). The synthetic image is x = G(z). The selector can be formed as S(x; \u03b8), which takes the image data as input and outputs features. The goal of our selector is to maximize a value function V (S,G):\nmax S V (S,G) = \u2016Ex\u223cp(x)S(x) \u2212 Ez\u223cp(z)S(G(z))\u2016 (1)\nThe selector S(x; \u03b8) maximizes the distance between source domain and target domain, and it will be easy to find out source data which is similar to target data. Algorithm 1 shows the process of how to train and test with selector. Our selector is based on VGG16 and takes an image as input and outputs features with length of 128. The features then are fed into a fully connected layer and a softmax layer to get the likelihood of its classes(synthetic and real). In this project, we first sample 1049 synthetic images from the synthetic dataset and give them a label 1, which means it is a synthetic image. Also we label all the precarious data as 0. 2000 images are used as the training data. To accelerate training time, we down-sample these images to 384\u00d7288 pixels. After training the selector, we use it to test on synthetic dataset and choose out the most realistic images it thinks. All the chosen images will form imposter dataset. Figure 3 shows some images from imposter dataset.\nMost feature based domain adaptation methods want to find a shared feature space between source domain and target domain and it is difficult to observe the difference of two domains. However, our selector would like to maximize the distance betweenDS and DT . If selector can\u2019t discriminate DS and DT , this means these two domains are in similar data distribution. So we can define the similarity of two domains using Hausdorff distance dS(XS ,XT ) based on our selector S(x; \u03b8):\ndS(XS ,XT ) = max{d \u2032 S(XS ,XT ), d \u2032 S(XT ,XS)} (2)\nd \u2032\nS(XS ,XT ) = sup xS\u2208XS inf xT\u2208XT \u2016S(xS)\u2212 S(xT )\u2016 (3)"}, {"heading": "4.2. RPN+", "text": "RPN [41] is the state-of-the-art pedestrian detector on Caltech dataset. Our detection model called RPN+ is based on RPN. Figure 4 shows the architecture of RPN+. RPN+ is a fully convolutional neural network implementedwith TensorFlow. We concatenate several layers on different stages in order to improve the ability of locating people in different resolutions. We use 9 anchors(reference boxes with 3 scales and aspect ratios) at each sliding position. Default boxes will be matched to ground truth with jaccard overlap higher than 50%. One box will be treat as negative box if its jaccard overlap with ground truth is lower than 20%. To accelerate training time, we adopt a pre-trained VGG-16 model as network initialization. We also freeze the first two convolutional layers when training."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Evaluation Metrics", "text": "Evaluation method of Caltech pedestrian dataset [10] is used as our 2D bounding box evaluation. The standard overlap is 50%, but we also include the results of 70% overlap criteria. Log-average miss rate is adopted as the comprehensive evaluation index. Baselines: We compare proposed detectors with following baseline approaches on precarious test dataset. ACF: An aggregate channel features detector[8] trained on Caltech pedestrian dataset. LDCF: A LDCF detector [26] trained on Caltech Pedestrian dataset. HOG+Cascade: A cascade of boosted classifiers working with HOG features[42]. HARR+Cascade: A cascade of boosted classifiers working with haar-like features[39, 21]. RPN/BF: The RPN detection model trained with boosted forest[41].\nRPN+: Our RPN+ people detection model trained on Caltech pedestrian dataset. This detector is only trained on Caltech pedestrian dataset and not trained with selection model. Ours: Our RPN+ people detection model trained with proposed selection model. This detector is trained on synthetic data and precarious training data. Datasets: We use three different datasets for evaluation of the various pedestrian detection system above. We evaluate on our novel Precarious Pedestrian dataset, a standard pedestrian benchmark dataset (Caltech), and our Synthetic Imposter Testset."}, {"heading": "5.2. Evaluation for Detectors", "text": "We draw ROC curves of different detectors with 50% and 70% overlap criteria, which are both showed in Figure 6. The curves show that our proposed detector has a significantly superior to other detectors. Table 3 shows the log-average miss rate of all detectors. Our proposed\nmethod outperforms all baselines with log-average miss rate of 42.47%. Our RPN+ detector trained on Caltech pedestrian dataset achieves the second best result with logaverage miss rate of 58.82%. Figure 9 shows the selected visualized results of the proposed detector and RPN+ detector trained only on Caltech. These examples illustrate our method can improve the ability of localizing people and is more flexible with kinds of aspect ratios.\nWe also test our RPN+ on Caltech Dataset, and Figure 7 shows the results of different detectors on Caltech Dataset. All the detectors are trained on Caltech Dataset. For reference, our RPN model would currently rank 6th out of 68 entries on the Caltech Dataset leaderboard."}, {"heading": "5.3. Effect of the Selector and Imposter Number", "text": "In this section, we do some experiments to show how our selector can improve the detection performance. Detection models in the three steps of selection model are tested on precarious dataset. And we also train a model without the second step(marginal distribution adaptation), which means we directly refine the model trained on synthetic dataset. Additionally, we also train a model only based on precarious training dataset. Table 4 shows the re-\nsults of different training strategies. The model which finishes all the three steps achieves the best result, which is 42.47%, and the model refined on imposter and real training data achieves the second best result, which is 45.97%. Comparing the results of NoStep2(no marginal distribution adaptation) and Step3(our proposed method), it illustrates that without imposters, there will be decrease of about 6% on the detection performance. Comparing the results of OnlyRealData(purely trained on precarious dataset) and NoStep2, it illustrates that without synthetic data, there will be decrease of about 24% on the performance. Experiments show that the model trained only on synthetic data(Step1) dose worst on precarious dataset.\nTo show how the selector will influence the performance, we select three models of selector from different training period. We use these three selectors to choose three impostor datasets. And then these impostor datasets are mixed with precarious training dataset to refine the pretrained model. Figure 10 shows that selector at 800th epoch achieves the best result i.e. 45.98%. This means the more accurate the selector is, the better the detection result will be.\nWe also investigate that how will the performance change when the amount of imposters changes. Figure 10 show the results of models from the second step of of our algorithm when using different amount of imposters."}, {"heading": "5.4. Evaluation on Synthetic Imposter Dataset", "text": "We finally explore how detectors will perform on synthetic imposter dataset. The RPN+ and ACF detectors are trained on corresponding training dataset for test dataset(e.g., detector which will test on real data will trained only on real training data). Figure 8 and Figure 10 show that the performance on both real test data and synthetic test data has the same ranking order. We think the synthetic data can be used as test dataset for evaluating the detectors when lacking real test data."}, {"heading": "6. Conclusion", "text": "We have explored methods for analyzing \u201cin-the-tail\u201d urban scenes, which represent important modes of operations for autonomous vehicles. Motivated by the fact\nthat rare but dangerous scenes are exactly the scenarios on which visual recognition should excel, we first analyze existing datasets and illustrate that they do not contain sufficient rare scenarios (because they naturally focus on common or typical urban scenes). To address this gap, we have collected our own dataset of Precarious People, which we will release to spur further research on this important (but under explored) problem. Precarious scenes are challenging because little data is available for both evaluation and training. To address this challenge, we propose the use of synthetic data generated with a game engine. However, it is challenging to ensure that the synthesized data matches the statistics of real precarious scenarios. Inspired by generative adversarial networks, we\nintroduce the use of a discriminative classifier (trained to discriminate real vs synthetic data) to implicitly specify this distribution. We then use the synthesized data that fooled the discriminator (the \u201csynthetic imposters\u201d) to both train and evaluate state-of-the-art, robust pedestrian detection systems.\nAcknowledgements. We thank Yinpeng Dong and Mingsheng Long for helpful discussions."}], "references": [{"title": "A local basis representation for estimating human pose from cluttered images", "author": ["A. Agarwal", "B. Triggs"], "venue": "In Asian Conference on Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "A database-based framework for gesture recognition", "author": ["V. Athitsos", "H. Wang", "A. Stefan"], "venue": "Personal and Ubiquitous Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Model-based validation approaches and matching techniques for automotive vision based pedestrian detection", "author": ["A. Broggi", "A. Fascioli", "P. Grisleri", "T. Graf", "M. Meinecke"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["X. Chen", "Y. Duan", "R. Houthooft", "J. Schulman", "I. Sutskever", "P. Abbeel"], "venue": "arXiv preprint arXiv:1606.03657,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Deep generative image models using a laplacian pyramid of adversarial networks", "author": ["E.L. Denton", "S. Chintala", "R. Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Fast feature pyramids for object detection", "author": ["P. Doll\u00e1r", "R. Appel", "S. Belongie", "P. Perona"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Pedestrian detection: A benchmark", "author": ["P. Doll\u00e1r", "C. Wojek", "B. Schiele", "P. Perona"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Pedestrian detection: An evaluation of the state of the art", "author": ["P. Doll\u00e1r", "C. Wojek", "B. Schiele", "P. Perona"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Monocular pedestrian detection: Survey and experiments", "author": ["M. Enzweiler", "D.M. Gavrila"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A mobile vision system for robust multi-person tracking", "author": ["A. Ess", "B. Leibe", "K. Schindler", "L. van Gool"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": "arXiv preprint arXiv:1504.06852,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D.Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Inferring 3d structure with a statistical image-based shape model", "author": ["K. Grauman", "G. Shakhnarovich", "T. Darrell"], "venue": "In Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Discriminative decorrelation for clustering and classification", "author": ["B. Hariharan", "J. Malik", "D. Ramanan"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning scene-specific pedestrian detectors without real data", "author": ["H. Hattori", "V. Naresh Boddeti", "K.M. Kitani", "T. Kanade"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Analysis by synthesis: 3d object recognition by object reconstruction", "author": ["M. Hejrati", "D. Ramanan"], "venue": "In 2014 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Unsupervised feature learning for 3d scene labeling", "author": ["K. Lai", "L. Bo", "D. Fox"], "venue": "In 2014 IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning physical intuition of block towers by example", "author": ["A. Lerer", "S. Gross", "R. Fergus"], "venue": "arXiv preprint arXiv:1603.01312,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "An extended set of haar-like features for rapid object detection", "author": ["R. Lienhart", "J. Maydt"], "venue": "In Image Processing", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Learning appearance in virtual scenarios for pedestrian detection", "author": ["J. Marin", "D. V\u00e1zquez", "D. Ger\u00f3nimo", "A.M. L\u00f3pez"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation", "author": ["N. Mayer", "E. Ilg", "P. H\u00e4usser", "P. Fischer", "D. Cremers", "A. Dosovitskiy", "T. Brox"], "venue": "arXiv preprint arXiv:1512.02134,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Conditional generative adversarial nets", "author": ["M. Mirza", "S. Osindero"], "venue": "arXiv preprint arXiv:1411.1784,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "3d pose-by-detection of vehicles via discriminatively reduced ensembles of correlation filters", "author": ["Y. Movshovitz-Attias", "Y. Sheikh", "V.N. Boddeti", "Z. Wei"], "venue": "In BMVC,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Local decorrelation for improved pedestrian detection", "author": ["W. Nam", "P. Doll\u00e1r", "J.H. Han"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Teaching 3d geometry to deformable part models", "author": ["B. Pepik", "M. Stark", "P. Gehler", "B. Schiele"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Learning people detection models from few training samples", "author": ["L. Pishchulin", "A. Jain", "C. Wojek", "M. Andriluka", "T. Thorm\u00e4hlen", "B. Schiele"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Nearest neighbor search methods for handshape recognition", "author": ["M. Potamias", "V. Athitsos"], "venue": "In Proceedings of the 1st international conference on PErvasive Technologies Related to Assistive Environments,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Playing for data: Ground truth from computer games", "author": ["S.R. Richter", "V. Vineet", "S. Roth", "V. Koltun"], "venue": "arXiv preprint arXiv:1608.02192,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Hands in action: real-time 3d reconstruction of hands in interaction with objects", "author": ["J. Romero", "H. Kjellstr\u00f6m", "D. Kragic"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes", "author": ["G. Ros", "L. Sellart", "J. Materzynska", "D. Vazquez", "A.M. Lopez"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["T. Salimans", "I. Goodfellow", "W. Zaremba", "V. Cheung", "A. Radford", "X. Chen"], "venue": "arXiv preprint arXiv:1606.03498,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "Data-driven scene understanding from 3d models", "author": ["S. Satkin", "J. Lin", "M. Hebert"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "From virtual to reality: Fast adaptation of virtual object detectors to real domains", "author": ["B. Sun", "K. Saenko"], "venue": "In BMVC,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Virtual and real world adaptation for pedestrian detection", "author": ["D. Vazquez", "A.M. Lopez", "J. Marin", "D. Ponsa", "D. Geronimo"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M. Jones"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}, {"title": "Multi-cue onboard pedestrian detection", "author": ["C.Wojek", "S.Walk", "B. Schiele"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Is faster r-cnn doing well for pedestrian detection", "author": ["L. Zhang", "L. Lin", "X. Liang", "K. He"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Fast human detection using a cascade of histograms of oriented gradients", "author": ["Q. Zhu", "M.-C. Yeh", "K.-T. Cheng", "S. Avidan"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}], "referenceMentions": [{"referenceID": 6, "context": "poses on sidewalks [9, 6, 12, 11, 40].", "startOffset": 19, "endOffset": 37}, {"referenceID": 9, "context": "poses on sidewalks [9, 6, 12, 11, 40].", "startOffset": 19, "endOffset": 37}, {"referenceID": 8, "context": "poses on sidewalks [9, 6, 12, 11, 40].", "startOffset": 19, "endOffset": 37}, {"referenceID": 37, "context": "poses on sidewalks [9, 6, 12, 11, 40].", "startOffset": 19, "endOffset": 37}, {"referenceID": 17, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 20, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 10, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 14, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 29, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 31, "context": "Synthetic training data is an actively explored topic because it provides a potentially infinite well of annotated data for training data-hungry architectures [20, 23, 13, 17, 32, 34].", "startOffset": 159, "endOffset": 183}, {"referenceID": 11, "context": "Synthetic imposters: We address both concerns with a novel variant of Generative Adversarial Networks (GANs) [14], a method for synthesizing data from latent noise vectors.", "startOffset": 109, "endOffset": 113}, {"referenceID": 38, "context": "We find surprisingly good results with a (to our knowledge) novel variant of region proposal network (RPN) [41] tuned for particular objects (precarious people) rather than a general class of objectness detections.", "startOffset": 107, "endOffset": 111}, {"referenceID": 28, "context": "Instead of classifying a sparse set of proposed windows (as nearly all contemporary object detection systems based on RCNN do [31]), this network returns a dense heatmap of pedestrian detections, along with regressed bounding box location for each pixel location in the heatmap.", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "[20] used a 3D game engine to generate synthetic data and learned a physical intuition to predict the falling progress of block tower.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] gave out a benchmark of synthetic data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "They designed a computer program to generate synthetic images with disparity, optical flow and disparity change, and evaluate the FlowNet[13].", "startOffset": 137, "endOffset": 141}, {"referenceID": 29, "context": "[32] used synthetic data to improve image segmentation performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[34] used Unity Development Platform to generate a synthetic urban scene dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 2, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 0, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 19, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 26, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 1, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 30, "context": "Previous work with 3D computer graphics models has widely been used for modeling kinds of human body shapes [15, 4, 1, 22, 29, 3, 33].", "startOffset": 108, "endOffset": 133}, {"referenceID": 24, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 61, "endOffset": 73}, {"referenceID": 22, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 61, "endOffset": 73}, {"referenceID": 15, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 61, "endOffset": 73}, {"referenceID": 33, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 98, "endOffset": 106}, {"referenceID": 16, "context": "Moreover, 3D simulation can also been used for car detection [27, 25, 18] and scene understanding [36, 19].", "startOffset": 98, "endOffset": 106}, {"referenceID": 19, "context": "[22] used a game engine to generate synthetic training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] used 8 HD cameras to scan human body and built real 3D human models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] used 3D modelling software to build a special scene and randomly put 3D models on a special background for pedestrian detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Sun and Saenko [37] used 3D models to train detectors for real objects.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "[16] and can be used only for feature-based detectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[38] used synthetic pedestrian data to generate robust real-world detectors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "GAN [14] is a deep network which can generate fake images from latent code and train a neural network to discriminate whether one image is real or generated.", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 4, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 27, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 32, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 3, "context": "Recent works [24, 7, 30, 35, 5] on GAN used neural networks to generate realistic data from latent code.", "startOffset": 13, "endOffset": 31}, {"referenceID": 2, "context": "Number of 3D models [4, 8] Index of background images [0, 1726) Index of 3D models [0, 20) Position of 3D models Within the field of vision Index of Animations [0,maxnumber) Time of animation [0, 1] Model\u2019s angle on the x axis [\u221290, 90] Model\u2019s angle on the y axis [\u2212180, 180] Model\u2019s angle on the z axis [\u221290, 90] Light intensity [0.", "startOffset": 20, "endOffset": 26}, {"referenceID": 5, "context": "Number of 3D models [4, 8] Index of background images [0, 1726) Index of 3D models [0, 20) Position of 3D models Within the field of vision Index of Animations [0,maxnumber) Time of animation [0, 1] Model\u2019s angle on the x axis [\u221290, 90] Model\u2019s angle on the y axis [\u2212180, 180] Model\u2019s angle on the z axis [\u221290, 90] Light intensity [0.", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "Number of 3D models [4, 8] Index of background images [0, 1726) Index of 3D models [0, 20) Position of 3D models Within the field of vision Index of Animations [0,maxnumber) Time of animation [0, 1] Model\u2019s angle on the x axis [\u221290, 90] Model\u2019s angle on the y axis [\u2212180, 180] Model\u2019s angle on the z axis [\u221290, 90] Light intensity [0.", "startOffset": 192, "endOffset": 198}, {"referenceID": 19, "context": "[22] who used a single virtual urban city, our synthetic data can achieve the diversity of background by rendering real images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17], we don\u2019t set a special mode to generate synthetic data, and our parameters are consecutive and randomly generated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "RPN [41] is the state-of-the-art pedestrian detector on Caltech dataset.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "Evaluation method of Caltech pedestrian dataset [10] is used as our 2D bounding box evaluation.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "ACF: An aggregate channel features detector[8] trained on Caltech pedestrian dataset.", "startOffset": 43, "endOffset": 46}, {"referenceID": 23, "context": "LDCF: A LDCF detector [26] trained on Caltech Pedestrian dataset.", "startOffset": 22, "endOffset": 26}, {"referenceID": 39, "context": "HOG+Cascade: A cascade of boosted classifiers working with HOG features[42].", "startOffset": 71, "endOffset": 75}, {"referenceID": 36, "context": "HARR+Cascade: A cascade of boosted classifiers working with haar-like features[39, 21].", "startOffset": 78, "endOffset": 86}, {"referenceID": 18, "context": "HARR+Cascade: A cascade of boosted classifiers working with haar-like features[39, 21].", "startOffset": 78, "endOffset": 86}, {"referenceID": 38, "context": "RPN/BF: The RPN detection model trained with boosted forest[41].", "startOffset": 59, "endOffset": 63}, {"referenceID": 36, "context": "HAAR [39, 21] 95.", "startOffset": 5, "endOffset": 13}, {"referenceID": 18, "context": "HAAR [39, 21] 95.", "startOffset": 5, "endOffset": 13}, {"referenceID": 39, "context": "4% HOG [42] 89.", "startOffset": 7, "endOffset": 11}, {"referenceID": 5, "context": "28% ACF[8] 73.", "startOffset": 7, "endOffset": 10}, {"referenceID": 23, "context": "50% LDCF [26] 71.", "startOffset": 9, "endOffset": 13}, {"referenceID": 38, "context": "37% RPN/BF [41] 54.", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "As autonomous vehicles become an every-day reality, high-accuracy pedestrian detection is of paramount practical importance. Pedestrian detection is a highly researched topic with mature methods, but most datasets focus on common scenes of people engaged in typical walking poses on sidewalks. But performance is most crucial for dangerous scenarios, such as children playing in the street or people using bicycles/skateboards in unexpected ways. Such \u201cinthe-tail\u201d data is notoriously hard to observe, making both training and testing difficult. To analyze this problem, we have collected a novel annotated dataset of dangerous scenarios called the Precarious Pedestrian dataset. Even given a dedicated collection effort, it is relatively small by contemporary standards (\u2248 1000 images). To explore largescale data-driven learning, we explore the use of synthetic data generated by a game engine. A significant challenge is selected the right \u201cpriors\u201d or parameters for synthesis: we would like realistic data with realistic poses and object configurations. Inspired by Generative Adversarial Networks, we generate a massive amount of synthetic data and train a discriminative classifier to select a realistic subset, which we deem Synthetic Imposters. We demonstrate that this pipeline allows one to generate realistic training data by making use of rendering/animation engines. Interestingly, we also demonstrate that such data can be used to rank algorithms, suggesting that Synthetic Imposters can also be used for \u201cin-the-tail\u201d validation at test-time, a notoriously difficult challenge for real-world deployment.", "creator": "LaTeX with hyperref package"}}}