{"id": "1703.06490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "Generating Multi-label Discrete Patient Records using Generative Adversarial Networks", "abstract": "Access to electronic health records (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic EHRs. Based on an input EHR dataset, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic EHR datasets that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and medical expert review.", "histories": [["v1", "Sun, 19 Mar 2017 18:56:37 GMT  (2988kb,D)", "http://arxiv.org/abs/1703.06490v1", null], ["v2", "Sat, 17 Jun 2017 08:51:01 GMT  (3513kb,D)", "http://arxiv.org/abs/1703.06490v2", "Accepted at Machine Learning in Health Care (MLHC) 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["edward choi", "siddharth biswal", "bradley malin", "jon duke", "walter f stewart", "jimeng sun"], "accepted": false, "id": "1703.06490"}, "pdf": {"name": "1703.06490.pdf", "metadata": {"source": "CRF", "title": "Generating Multi-label Discrete Electronic Health Records using Generative Adversarial Networks", "authors": ["Edward Choi", "Siddharth Biswal", "Bradley Malin", "Jon Duke", "Walter F. Stewart", "Jimeng Sun"], "emails": ["mp2893@gatech.edu,", "sbiswal7@gatech.edu,", "bradley.malin@vanderbilt.edu,", "jon.duke@gatech.edu,", "stewarwf@sutterhealth.org,", "jsun@cc.gatech.edu"], "sections": [{"heading": null, "text": "In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic EHRs. Based on an input EHR dataset, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic EHR datasets that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and medical expert review."}, {"heading": "1 Introduction", "text": "The adoption of electronic healthcare records (EHR) by healthcare providers, and the large quantity and quality of data now generated, has led to an explosion in computational health. However, the wide adoption of EHR systems does not automatically lead to easy access to EHR data for researchers. One of the reasons for limited access is because EHR data are composed of personally identifiable information, which in combination with potentially sensitive medical data, induces privacy concerns. As a result, access to such data for secondary purposes (e.g., research) is highly regulated, as well as controlled by provider groups that are at risk if data are misused or breached. The review process by legal departments and institutional review boards can take months, with no guarantee of access (Hodge Jr et al., 1999). This process limits timely opportunities to use data and may slow advances in biomedical knowledge and patient care (Gostin et al., 2009).\nHealthcare institutions often aim to mitigate privacy risks through the practice of de-identification (for Civil Rights, 2013). Typically, de-identification is accomplished through the perturbation of potentially identifiable attributes (e.g., dates of birth) via generalization, suppression or randomization (El Emam et al., 2015). However, this approach to privacy protection is not impregnable to re-identification attack (El Emam et al., 2011b). An alternative approach to de-identification is to generate synthetic data (McLachlan et al., 2016; Buczak et al., 2010; Lombardo and Moniz, 2008). However, realizing this approach in practice has been challenging because the resulting synthetic data are often not sufficiently realistic for machine learning tasks. Since many machine learning models on EHR data are using aggregated discrete features derived from longitudinal EHR records, we concentrate our effort on generating such aggregated data in this work. Although it is ultimately desirable to generate longitudinal event sequences, in this work we focus on generating high-dimensional discrete variables, which is an important and challenging problem on its own.\nGenerative adversarial networks (GANs) have recently demonstrated impressive performance in generating high-quality synthetic images (Goodfellow et al., 2014; Radford et al., 2015; Goodfellow, 2016). A GAN consists\nar X\niv :1\n70 3.\n06 49\n0v 1\n[ cs\n.L G\n] 1\n9 M\nar 2\nof two components: a generator that attempts to generate realistic, but fake, images and a discriminator that aims to distinguish between the generated fake images and the real images. By playing an adversarial game against each other, the generator can learn the distribution of the real samples provided that both the generator and the discriminator are sufficiently expressive. Empirically, a GAN outperforms other popular generative models such as variational autoencoders (VAE) (Kingma and Welling, 2013) and PixelRNN/PixelCNN (van den Oord et al., 2016a,b) on the quality of images (i.e., fake compared to real) and on processing speed (Goodfellow, 2016). However, a GAN cannot learn the distribution of discrete variables in its original form.\nOn that premise, we propose medGAN, a neural network model that generates high-dimensional discrete variables representing events documented in patients\u2019 EHRs (e.g., diagnosis of a certain disease or treatment of a certain medication). Using an EHR source data, medGAN is designed to learn the distribution of discrete features, such as diagnosis or medication codes via a combination of an autoencoder and the adversarial framework. In this setting, the autoencoder is applied to overcome the original GAN\u2019s inability to generate discrete samples. The specific contributions of this work are as follows: \u2022 We propose medGAN, an efficient algorithm to generate high-dimensional multi-hot discrete samples\nby combining an autoencoder with GAN. In particular, medGAN can handle both binary and count variables. \u2022 medGAN translates input EHR data to a program that can generate arbitrarily large volume of high-quality,\nhigh-dimensional synthetic patient data. \u2022 We propose a simple, yet effective, method called minibatch averaging to cope with the situation where\nGAN learns to generate samples of low diversity, the \u201cmode collapse\u201d problem, which outperforms previous methods such as minibatch discrimination. \u2022 We demonstrate the close-to-real-data performance of medGAN using real EHR datasets on diverse tasks including distribution statistics, classification performance and medical expert review."}, {"heading": "2 Related work", "text": "We first discuss existing methods to generate synthetic EHR data, followed by recent advances in generative adversarial networks (GAN) and specific works on generating discrete variables using GAN. Synthetic Data Generation for Health Data: De-identification of EHR data is the prominent technical method for protecting patient privacy when sharing EHR data for research (Johnson et al., 2016). However, deidentification does not guarantee that a system is devoid of risk. In certain circumstances, re-identification of patients can be accomplished through residual distinguishable patterns in various features (e.g., demographics (Sweeney, 1997; El Emam et al., 2011a), diagnoses (Loukides et al., 2010), lab tests (Atreya et al., 2013), visits across healthcare providers (Malin and Sweeney, 2004), and genomic variants (Erlich and Narayanan, 2014)) To mitigate re-identification vulnerabilities, researchers in the statistical disclosure control community have investigated how to generate synthetic datasets. Yet, historically, these approaches have been limited to summary statistics for only several variables at a time (e.g., (Dreschsler, 2011; Reiter, 2002). For instance, McLachlan et al. (2016) used clinical practice guidelines and health incidence statistics with state transition machine to generate synthetic patient datasets.\nThere is some, but limited, work on synthetic data generation in the healthcare domain and, the majority that has, tend to be disease specific. For example, Buczak et al. (2010) generated EHR to explore questions related to the outbreak of specific illnesses, where care patterns in the source EHR were applied to generate synthetic datasets. Many of these methods often rely heavily upon domain-specific knowledge along with actual data to generate synthetic EHRs (Lombardo and Moniz, 2008). More recently, and most related to our work, a privacy-preserving patient data generator was proposed based on a perturbed Gibbs sampler (Park et al., 2013). Still, this approach can only handle binary variables and its utility was assessed only on a small, low-dimensional dataset. By contrast, our proposed medGAN directly captures general EHR data without focusing on specific diseases, which makes it suitable for diverse applications. GAN and its Applications: Attempts to advance GAN (Goodfellow et al., 2014) include, but are not limited to, using convolutional neural networks to improve image processing capacity (Radford et al., 2015), extending GAN to a conditional architecture for higher quality image generation (Mirza and Osindero, 2014;\nDenton et al., 2015; Odena et al., 2016), and text-to-image generation (Reed et al., 2016). We, in particular, pay attention to the recent studies that attempted to handle discrete variables using GAN.\nOne way to generate discrete variables with GAN is to invoke reinforcement learning. SeqGAN (Yu et al., 2016) trains GAN with REINFORCE (Williams, 1992) and Monte-Carlo search to generate word sequences. Although REINFORCE enables unbiased estimation of gradients of the model via sampling, the estimates come with a high variance. Moreover, SeqGAN focuses on sampling one word (i.e. one-hot) at each timestep, whereas our goal is to generate multi-label binary/count variables. Alternatively, one could use specialized distributions, such as the Gumbel-softmax (Jang et al., 2016; Kusner and Herna\u0301ndez-Lobato, 2016), a concrete distribution (Maddison et al., 2016) or a soft-argmax function (Zhang et al., 2016) to approximate the gradient of the model from discrete samples. However, since these approaches focus on the softmax distribution, they cannot be directly invoked for multi-label discrete variables, especially in the count variable case. Another way to handle discrete variables is to generate distributed representations, then decode them into discrete outputs. For example, Glover (2016) generated document embeddings with GAN, but did not attempt to generate actual documents.\nTo handle high-dimensional multi-label discrete variables, we propose medGAN to efficiently generate discrete samples by generating the distributed representations of patient records with GAN, then decoding them into actual discrete records with an autoencoder."}, {"heading": "3 Method", "text": "We first describe the structure of EHR data and the related mathematical notations, followed by details on medGAN."}, {"heading": "3.1 Description of EHR Data and Notations", "text": "We assume there are |C| number of discrete variables (e.g., diagnosis, medication or procedure codes) in the EHR data that can be expressed as a fixed-size vector x \u2208 Z|C|+ where the value of the ith dimension indicates the number of occurrences (i.e., counts) of the i-th variable in the patient record. In addition to the count variables, a visit record can also be represented as a binary vector x \u2208 {0, 1}|C| where the ith dimension indicates the absence or occurrence of the ith variable in the patient record. Note that we can also represent demographic information, such as age and gender, as count and binary variables, respectively.\nLearning the distribution of count variables is generally more difficult than learning the distribution of binary variables. This is because the model needs to learn more than simple co-occurrence relations between the various dimensions. Moreover, in EHR data, certain codes tend to occur much more frequently (e.g., essential hypertension) than others. This is problematic because it can skew a distribution among different dimensions."}, {"heading": "3.2 Preliminary: Generative Adversarial Network", "text": "In a GAN, the generator G(z; \u03b8g), accepts the random prior z \u2208 Rr and generates synthetic samples G(z) \u2208 Rd, while the discriminator D(x; \u03b8d) determines whether a given sample is real or fake. The optimal discriminator D\u2217 would perfectly distinguish real samples from fake samples. The optimal generator G\u2217 would generate fake samples that are indistinguishable from the real samples so that D is forced to make random guesses. Formally, D and G play the following minimax game with the value function V (G,D):\nmin G max D V (G,D) = Ex\u223cpdata [logD(x)]\n+ Ez\u223cpz [log(1\u2212D(G(z)))]\nwhere pdata is the distribution of the real samples and pz is the distribution of the random prior, for which N (0, 1) is generally used. Both G and D iterate in optimizing the respective parameters \u03b8g and \u03b8d as follows,\n\u03b8d \u2190 \u03b8d + \u03b1\u2207\u03b8d 1\nm m\u2211 i=1 logD(xi) + log(1\u2212D(G(zi)))\n\u03b8g \u2190 \u03b8g \u2212 \u03b1\u2207\u03b8g 1\nm m\u2211 i=1 log(1\u2212D(G(zi)))\nwhere m is the size of the minibatch and \u03b1 the step size. In practice, however, G can be trained to maximize log(D(G(z)) instead of minimizing log(1\u2212D(G(z)) to provide stronger gradients in the early stage of the training(Goodfellow et al., 2014) as follows,\n\u03b8g \u2190 \u03b8g + \u03b1\u2207\u03b8g 1\nm m\u2211 i=1 logD(G(zi)) (1)\nIn the remainder of this paper we use Eq.(1) as it showed significantly more stable performance. We also assume throughout the paper that both D and G are implemented with feedforward neural networks.\n3.3 medGAN\nSince the generator G is trained by the error signal from the discriminator D via backpropagation, the original GAN cannot directly learn the distribution of discrete patient records x \u2208 Z|C|+ . We overcome this limitation by leveraging the autoencoder. Autoencoders are trained to project given samples to a lower dimensional space, then project them back to the original space. Such a mechanism leads the autoencoder to learn salient features of the samples and has been successfully used in certain applications, like image processing (Goodfellow et al., 2016; Vincent et al., 2008). We apply the autoencoder to learn the salient features of discrete variables that can be used to decode the continuous output of G. This allows the gradient flow from D to the decoder Dec to enable the end-to-end fine-tuning. As depicted by Figure 1, an autoencoder consists of an encoder Enc(x; \u03b8enc) that compresses the input x \u2208 Z|C|+ to Enc(x) \u2208 Rh, and a decoder Dec(Enc(x); \u03b8dec) decompresses Enc(x) to Dec(Enc(x)) as the reconstruction of the original input x. The objective of the autoencoder is to minimize the reconstruction error:\n1 m m\u2211 i=0 ||xi \u2212 x\u2032i||22 (2)\n1 m m\u2211 i=0 xi logx \u2032 i + (1\u2212 xi) log(1\u2212 x\u2032i) (3) where x\u2032i = Dec(Enc(xi))\nwhere m is the size of the mini-batch. We use the mean squared loss (Eq.(2)) for count variables and cross entropy loss (Eq.(3)) for binary variables. For count variables, we use rectified linear units (ReLU) as the activation function in both Enc and Dec. For binary variables, we use tanh activation for Enc and the sigmoid activation for Dec 1\nWith the autoencoder, we can allow GAN to generate distributed representation of patient records (i.e., the output of the encoder Enc), rather than generating discrete records directly. As the generator G and the encoder Enc both generate similar continuous values, the decoder Dec can pick up the right signals to convert synthetic continuous samples G(z) \u2208 Rh to discrete samples Dec(G(z)) \u2208 Z|C|+ . The discriminator D is trained to determine whether the given input is a synthetic sample Dec(G(z)) or a real sample x. The architecture of the proposed model medGAN is depicted in Figure 1. medGAN is trained in a similar fashion as the original GAN as follows,\n\u03b8d \u2190 \u03b8d + \u03b1\u2207\u03b8d 1\nm m\u2211 i=1 logD(xi) + log(1\u2212D(xzi))\n\u03b8g,dec \u2190 \u03b8g,dec + \u03b1\u2207\u03b8g,dec 1\nm m\u2211 i=1 logD(xzi)\nwhere xzi = Dec(G(zi))\nNote that we fine-tune the pre-trained parameters of the decoder \u03b8dec while optimizing for G. Therefore the generator G can be viewed as a neural network with an extra hidden layer pre-trained to map continuous samples to discrete samples. We used ReLU for all of G\u2019s activation functions, except for the output layer, where we used the tanh function2. For D, we used ReLU for all activation functions except for the output layer, where we used the sigmoid function for binary classification."}, {"heading": "3.4 Minibatch Averaging", "text": "Since the objective of the generator G is to produce samples that can fool the discriminator D, G could learn to map different random priors z to the same synthetic output, rather than producing diverse synthetic outputs. This problem is denoted as mode collapse, which arises most likely due to the GAN\u2019s optimization strategy often solving the max-min problem instead of the min-max problem (Goodfellow, 2016). Some methods have been proposed to cope with mode collapse such as minibatch discrimination and unrolled GANs, but they require some fine-tuning of the hyperparameters, or scalability has not been addressed (Salimans et al., 2016; Metz et al., 2016).\nmedGAN offers a simple and efficient method to cope with mode collapse when generating discrete outputs. Our method, minibatch averaging, is motivated by the philosophy behind minibatch discrimination; allow the discriminator D to view the minibatch of real samples x1,x2, . . . and the minibatch of the fake samples G(z1), G(z2), . . ., respectively, while classifying a real sample and a fake sample. Given a sample to discriminate, minibatch discrimination calculates the distance between the sample and all samples in the minibatch in the latent space. Minibatch averaging, by contrast, provides the average of the minibatch samples to D, modifying the objective as follows:\n\u03b8d \u2190 \u03b8d + \u03b1\u2207\u03b8d 1\nm m\u2211 i=1 logD(xi, x\u0304) + log(1\u2212D(xzi , x\u0304z))\n\u03b8g,dec \u2190 \u03b8g,dec + \u03b1\u2207\u03b8g,dec 1\nm m\u2211 i=1 logD(xzi , x\u0304z)\nwhere x\u0304 = 1\nm m\u2211 i=1 xi, xzi = Dec(G(zi)), x\u0304z = 1 m m\u2211 i=1 xzi\nSpecifically, the average of the minibatch is concatenated on the sample and provided to the discriminator D.\n1We considered the denoising autoencoder (dAE) (Vincent et al., 2008) but there was no visible performance improvement. 2Note that we also used tanh activation for the encoder Enc for consistency.\nBinary variables: When processing binary variables x \u2208 {0, 1}|C|, the average of minibatch samples x\u0304 and x\u0304z are equivalent to the maximum likelihood estimate of the Bernoulli success probability p\u0302k of each dimension k. This information makes it easier for D to ascertain whether a given sample is real or fake, if p\u0302k\u2019s of fake samples are considerably different from those of real samples. This is especially likely if mode collapse occurs because the p\u0302k\u2019s for most dimensions of the fake samples become dichotomized (either 0 or 1), whereas the p\u0302k\u2019s of real samples generally take on a value between 0 and 1. Therefore, if G wants to fool D, it will have to generate more diverse examples within the minibatch Dec(G(z1, z2, . . .)). For EHR data generation, the binary variables can be applied to present whether diseases or treatments are present or not. Count variables: Count variables are a more accurate representation of clinical events they can indicate the number of times a diagnosis was made or a certain medication was prescribed over multiple hospital visits. In this case of count variables x \u2208 Z|C|+ , the average of minibatch samples x\u0304 and x\u0304z can be viewed as the estimate of the binomial distribution mean np\u0302k of each dimension k, where n is the number of hospital visits. Hence minibatch averaging for the count variables also provides helpful statistics to the discriminator D, guiding the generator G to generate more diverse and realistic samples. Minibatch averaging works surprisingly well and does not require additional parameters as minibatch discrimination, therefore has minimal impact to the training time. It is worth mentioning that, for both binary and count variables, a bigger minibatch than usual is recommended to properly capture the statistics of the real data. We use 1,000 records for a minibatch in this work."}, {"heading": "3.5 Enhanced Generator Training", "text": "Similar to image processing GANs, we observed that balancing the power of D and G in the multi-label discrete variable setting was quite challenging (Goodfellow, 2016). Empirically, we observed that training medGAN with minibatch averaging demonstrated D consistently overpowering G after several iterations. While G still managed to learn under such situation, the performance seemed suboptimal, and updating \u03b8g and \u03b8dec more often than \u03b8d in each iteration only degraded performance. Considering the importance of an optimal D (Goodfellow, 2016), we chose not to limit the discriminative power of D, but rather improve the learning efficiency of G by applying batch normalization (Ioffe and Szegedy, 2015) and shortcut connection (He et al., 2016). G\u2019s kth layer is now formulated as follows:\nxk = ReLU(BNk(Wkxk\u22121)) + xk\u22121\nwhere ReLU is the rectified linear unit, BNk is the batch normalization at the k-th layer, Wk is the weight matrix of the k-th layer, and xk\u22121 is the input from the previous layer. The righthand side of Figure 1 depicts the first two layers of G. Note that we do not incporate the bias variable into each layer because batch normalization negates the necessity of the bias term. Additionally, Batch normalization and shortcut connections could be applied to the discriminator D, but the experiments showed that D was consistently overpowering G without such techniques, and we empirically found that a simple feedforward network was sufficient for D.\nAlgorithm 1 describes the overall optimization process of medGAN. Note that \u03b8d is updated k times per iteration, while \u03b8g and \u03b8dec are updated once per iteration to ensure optimality of D. However, typically, a larger k has not shown a clear improvement (Goodfellow, 2016). And we set k = 2 in our experiments."}, {"heading": "4 Experiments", "text": "We evaluated medGAN with two distinct EHR datasets. First, we describe the datasets and baseline models. Next, we report on quantitative evaluation using both binary and count variables. Finally, we perform a qualitative analysis with medical expert review. The source code of medGAN is publicly available at https://github.com/mp2893/medgan.\nAlgorithm 1 medGAN Optimization\nDataset Sutter PAMF MIMIC-III\n# of patients 258,559 46,520 # of unique codes 615 1071 Avg. # of codes per patient 38.37 11.27 Max # of codes for a patient 198 90 Min # of codes for a patient 1 1"}, {"heading": "4.1 Experimental Setup", "text": "Source data: The datasets in this study were from: 1) the Sutter Palo Alto Medical Foundation (PAMF), which consists of 10-years of longitudinal medical records of 258K patients with age 50 to 90 and 2) the MIMICIII dataset (Johnson et al., 2016; Goldberger et al., 2000), which is a publicly available dataset consisting of the medical records of 46K intensive care unit (ICU) patients over 11 years. From the PAMF dataset, we extracted diagnoses, medications and and procedure codes, which were then respectively grouped by Clinical Classifications Software for ICD-93, Generic Product Identifier Drug Group4 and Clinical Classifications Software for CPT5 into a total of 615 codes. From the MIMIC-III, we extracted diagnosis codes and grouped them by generalizing up to their 3-digit ICD9 codes, yielding a total of 1071 codes. A summary of the datasets are in Table 1. Finally, we aggregate a patient\u2019s longitudinal record into a single fixed-size vector x \u2208 Z|C|+ , where |C| equals 615 and 1071 for PAMF and MIMIC-III, respectively. Models for comparison: To assess the effectiveness of our methods, we tested multiple versions of medGAN: \u2022 Basic: We use the same architecture as medGAN with the standard training strategy, but do not pre-train\nthe autoencoder.\n3https://www.hcup-us.ahrq.gov/toolssoftware/ccs/ccs.jsp 4http://www.wolterskluwercdi.com/drug-data/medi-span-electronic-drug-file/ 5https://www.hcup-us.ahrq.gov/toolssoftware/ccs svcsproc/ ccssvcproc.jsp\n\u2022 Pre-train: We pre-train autoencoder in addition to GAN. \u2022 PD: We pre-train the autoencoder and use minibatch discrimination (Salimans et al., 2016). \u2022 PA: We pre-train the autoencoder and use minibatch averaging. \u2022 Full medGAN: We pre-train the autoencoder and use minibatch averaging. We also use batch normalization\nand shortcut connection for the generator G. We also compare the performance of medGAN with several popular generative methods as below. \u2022 Random Noise (RN): Given a real patient record x, we invert the binary value of each code (i.e.,\ndimension) with probability 0.1. This is not strictly a generative method but a simple implementation of a privacy protection method based on randomization. \u2022 Independent Sampling (IS): For the binary variable case, we calculate the Bernoulli success probability of each code in the real dataset, based on which we sample binary values to generate the synthetic dataset. For the count variable case, we use the kernel density estimator (KDE) for each code then sample from that distribution. \u2022 Stacked RBM (DBN): We train a stacked Restricted Boltzmann Machines (Hinton and Salakhutdinov, 2006) and generate synthetic samples using Gibbs sampling. This was used as a baseline for binary variables only, as it is a binary network. \u2022 Variational Autoencoder (VAE): We train a variational autoencoder (Kingma and Welling, 2013)\nwhere the encoder and the decoder are constructed with feed-forward neural networks. Implementation details: We implemented medGAN with TensorFlow 0.12 (Team, 2015). For training models, we used Adam (Kingma and Ba, 2014) with a mini-batch of 100 patients on a machine equipped with Intel Xeon E5-2630, 256GB RAM, four Nvidia Pascal Titan X\u2019s and CUDA 8.0. The hyperparameter details are provided in Appendix .1."}, {"heading": "4.2 Quantitative Evaluation for Binary Variables", "text": "We evaluate the model performance for binary variables first, then proceed to the more challenging count variables. For all evaluations, we divide the dataset into a training set R \u2208 {0, 1}N\u00d7|C| and a test set T \u2208 {0, 1}n\u00d7|C| by 4:1 ratio. We use R to train the models, then generate synthetic samples S \u2208 {0, 1}N\u00d7|C| to use it for various tasks. For medGAN and VAE, we round the continuous values to the nearest integer values.\n\u2022 Dimension-wise probability: A basic sanity check to confirm the model has learned each dimension\u2019s distribution correctly. We use the training set R to train the models, then generate the same number\nof synthetic samples S. Using the R and S, we compare the Bernoulli success probability pk of each dimension k.\n\u2022 Dimension-wise prediction: A task to indirectly measure how well the model captures the interdimensional relationship of the real samples. After training the models with R to generate S, we choose one dimension k to be the label yRk \u2208 {0, 1}N and ySk \u2208 {0, 1}N . The remaining R\\k \u2208 {0, 1}N\u00d7|C|\u22121 and S\\k \u2208 {0, 1}N\u00d7|C|\u22121 are used as features to train two logistic regression classifiers LRRk and LRSk to predict respectively yRk and ySk . Then we use the model LRRk and LRSk to predict label yTk \u2208 {0, 1}n of the test set T .We can assume that the closer the performance of LRSk to that of LRRk , the better the quality of the synthetic dataset S. We use F1-score to measure the prediction performance, with the threshold set to 0.5.\nTo relieve the reader of repetitive experiment results, we present evaluation results using Sutter PAMF in this section and provide the results from MIMIC-III in Appendix .3."}, {"heading": "4.2.1 Dimensions-wise probability", "text": "The dimension-wise probability performance increased as we used more advanced version of medGAN, where the full medGAN shows the best performance as depicted by figure 2a. Note that minibatch averaging significantly increases the performance. Since minibatch averaging provides Bernoulli success probability information of real data to the model during training, it is natural that the generator learns to generate synthetic data that follow the similar distribution. Minibatch discrimination does not seem to improve the results, most likely due to the discrete nature of the datasets. Improving the learning efficiency of the generator G with batch normalization and shortcut connection clearly helped improving the results on both datasets.\nFigure 2b compares the dimension-wise probability performance of baseline models with medGAN. Independent sampling (IS) naturally shows great performance as expected. DBN, given its stochastic binary nature, shows comparable performance as medGAN. VAE, although slightly inferior to DBN and medGAN, seems to capture the dimension-wise distribution relatively well, showing specific weakness at processing codes with low probability. Overall, we can see that medGAN can clearly capture the independent distribution of each code."}, {"heading": "4.2.2 Dimensions-wise prediction", "text": "Figure 3a shows the dimension-wise prediction performance of various versions of medGAN. The full medGAN again shows the best performance as it did in the dimension-wise probability task. Although the advanced versions of medGAN do not seem to dramatically increase the performance as they did for the previous task, this is due to the complex nature of inter-dimensional relationship compared to the independent dimension-wise probability.\nFigure 3b shows the dimension-wise prediction performance of baseline models compared to medGAN. As expected, IS is incapable of capturing the inter-dimensional relationship, given its naive sampling method. VAE shows similar behavior as it did in the previous task, showing weakness at predicting codes with low occurrence probability. Again, DBN shows comparable, if not slightly better performance to medGAN, which seems to come from its binary nature."}, {"heading": "4.3 Quantitative Evaluation for Count Variables", "text": "In order to evaluate for count variables, we use Sutter heart failure (HF) dataset, which is a subset of Sutter PAMF, consisting of 30,738 patients whose records were taken for exactly 18 months. Note that each patient\u2019s total hospital visits within the 18 months period can vary, which is a perfect test case for count variables. Detailed process of constructing Sutter HF dataset is provided in Appendix .2. Again, we aggregate the dataset into a fixed-size vector and divide it into R \u2208 ZN\u00d7|C|+ and T \u2208 Z n\u00d7|C| + in 4:1 ratio, Since we have confirmed the superior performance of full medGAN for binary variables, we focus on the comparison with baseline models in this section. Note that, to generate count variables, we replaced all activation functions in both VAE and medGAN (except the discriminator\u2019s output) to ReLU. We also use kernel density estimator with Gaussian kernel (bandwidth=0.75) to perform the independent sampling (IS) baseline.\nFor count variables, we conduct similar quantitative evaluations as binary variables with slight modifications. We first calculate dimension-wise average count instead of dimension-wise probability. For dimension-wise prediction, we use the binary labels yRk \u2208 {0, 1}N and ySk \u2208 {0, 1}N as before, but we train the logistic regression classifier with count samples R\\k \u2208 Z N\u00d7|C|\u22121 + and S\\k \u2208 Z N\u00d7|C|\u22121 + . The classifiers use count features as oppose to binary features while the evaluation metric is still F1-score."}, {"heading": "4.3.1 Dimensions-wise average count", "text": "Figure 5 shows the performance of baseline models and medGAN. The discontinuous behavior of VAE is due to its extremely low-variance synthetic samples. We found that, on average, VAE\u2019s synthetic samples had nine orders of magnitude smaller standard deviation than medGAN\u2019s synthetic samples. medGAN, on the other hand, shows good performance with the simple substitution of activation functions.\nFigure 4 shows the count histograms of five most frequent codes from Sutter HF dataset, where the top row was plotted with the real data and the bottom row with medGAN\u2019s synthetic data. We can see that medGAN\u2019s synthetic counterpart has very similar distribution as the real data. This tells us that medGAN is\nnot just trying to match the average count of codes (i.e. binomial distribution mean), but learns the actual distribution of the data."}, {"heading": "4.3.2 Dimensions-wise prediction", "text": "Figure 6 shows the performance of baseline models and medGAN. We can clearly see that medGAN shows superior performance. The experiments on count variables is especially interesting, as medGAN seems to make a smooth transition from binary variables to count variables, with just a replacement of the activation function. We also speculate that the medGAN\u2019s dimension-wise prediction performance will increase with more training data, as Sutter HF dataset consists of only 30K samples."}, {"heading": "4.4 Qualitative Evaluation", "text": "To conduct a qualitative evaluation of medGAN, we use Sutter HF dataset to train medGAN and generate synthetic samples with count variables. We then randomly pick 50 records from real data and 50 records from synthetic data, shuffle the order, present them to a medical doctor (specialized in internal medicine) who is asked to score how realistic each record is using scale 1 to 10 (10 being most realistic). Here the human doctor is served as the role of discriminator to provide the quality assessment of the synthetic data generated by medGAN.\nThe results of this assessment is shown in Figure 7. The findings suggest that medGAN\u2019s synthetic data are generally indistinguishable to a human doctor - except for several outliers. In those cases, the fake records identified by the doctor either lacked appropriate medication codes, or had both male-related codes (e.g. prostate cancer) and female-related codes (e.g. menopausal disorders) in the same record. The former issue also existed in some of the real records due to missing data, but the latter issue demonstrates a current limitation in medGAN which could potentially be alleviated by domain specific heuristics. In addition to medGAN\u2019s impressive performance in statistical aspects, this medical review lends credibility to the qualitative aspect of medGAN."}, {"heading": "5 Conclusion", "text": "In this work, we proposed medGAN, which uses generative adversarial framework to learn the distribution of real-world multi-label discrete electronic health records (EHR). Through rigorous evaluation using two datasets, medGAN showed impressive results for both binary variables and count variables. Considering the\ndifficulty accessibility of EHRs, we expect medGAN to make a contribution for healthcare research. As we have shown the versatility of GAN framework, we also expect this will encourage other researchers to apply GAN framework on various datasets and tasks. For future directions, we plan to explore the sequential version of medGAN, and also try to include other modalities such as lab measures, patient demographics, and free-text medical notes."}], "references": [{"title": "Reducing patient re-identification", "author": ["R.V. Atreya", "J.C. Smith", "A.B. McCoy", "B. Malin", "R.A. Miller"], "venue": null, "citeRegEx": "Atreya et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Atreya et al\\.", "year": 2013}, {"title": "Data-driven approach for creating synthetic electronic", "author": ["Anna Buczak", "Steven Babin", "Linda Moniz"], "venue": null, "citeRegEx": "Buczak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Buczak et al\\.", "year": 2010}, {"title": "A systematic review of re-identification attacks", "author": ["K. El Emam", "E. Jonker", "L. Arbuckle", "B. Malin"], "venue": null, "citeRegEx": "Emam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Emam et al\\.", "year": 2011}, {"title": "Anonymising and sharing individual patient", "author": ["El Emam", "S. Rodgers", "B. Malin"], "venue": "on health data. PLoS ONE 6,", "citeRegEx": "Emam et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Emam et al\\.", "year": 2011}, {"title": "Routes for breaching and protecting genetic privacy", "author": ["Erlich", "A. Narayanan"], "venue": "Medical Journal", "citeRegEx": "Erlich and Narayanan.,? \\Q2015\\E", "shortCiteRegEx": "Erlich and Narayanan.", "year": 2015}, {"title": "Modeling documents with Generative Adversarial Networks", "author": ["Department of Health", "Human Services. John Glover."], "venue": "arXiv:1612.09122 (2016). Ary Goldberger and others. 2000. Physiobank, physiotoolkit, and physionet components of a new research", "citeRegEx": "Health and Glover.,? 2016", "shortCiteRegEx": "Health and Glover.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Courville", "Yoshua Bengio."], "venue": "NIPS. 2672\u20132680. Lawrence Gostin, Laura Levit, Sharyl Nass, and others. 2009. Beyond the HIPAA Privacy Rule: Enhancing", "citeRegEx": "Courville and Bengio.,? 2014", "shortCiteRegEx": "Courville and Bengio.", "year": 2014}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey Hinton", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Hinton and Salakhutdinov.,? \\Q2006\\E", "shortCiteRegEx": "Hinton and Salakhutdinov.", "year": 2006}, {"title": "Legal issues concerning electronic health", "author": ["Hodge Jr.", "Lawrence O Gostin", "Peter Jacobson"], "venue": "Science 313,", "citeRegEx": "Jr et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Jr et al\\.", "year": 2006}, {"title": "Categorical Reparameterization with Gumbel-Softmax", "author": ["Eric Jang", "Shixiang Gu", "Ben Poole."], "venue": "arXiv:1611.01144 (2016).", "citeRegEx": "Jang et al\\.,? 2016", "shortCiteRegEx": "Jang et al\\.", "year": 2016}, {"title": "MIMIC-III, a freely accessible critical care database", "author": ["Alistair Johnson", "others."], "venue": "Scientific Data 3 (2016).", "citeRegEx": "Johnson and others.,? 2016", "shortCiteRegEx": "Johnson and others.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv:1412.6980 (2014).", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik Kingma", "Max Welling."], "venue": "arXiv:1312.6114 (2013).", "citeRegEx": "Kingma and Welling.,? 2013", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution", "author": ["Matt J Kusner", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato."], "venue": "arXiv:1611.04051 (2016).", "citeRegEx": "Kusner and Hern\u00e1ndez.Lobato.,? 2016", "shortCiteRegEx": "Kusner and Hern\u00e1ndez.Lobato.", "year": 2016}, {"title": "TA Method for Generation and Distribution", "author": ["Joseph S Lombardo", "Linda J Moniz."], "venue": "Johns Hopkins APL Technical Digest 27, 4 (2008), 356.", "citeRegEx": "Lombardo and Moniz.,? 2008", "shortCiteRegEx": "Lombardo and Moniz.", "year": 2008}, {"title": "The disclosure of diagnosis codes can breach research participants\u2019 privacy", "author": ["G. Loukides", "J.C. Denny", "B. Malin."], "venue": "J Am Med Inform Assoc 17, 3 (2010), 322\u2013327.", "citeRegEx": "Loukides et al\\.,? 2010", "shortCiteRegEx": "Loukides et al\\.", "year": 2010}, {"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "author": ["Chris Maddison", "Andriy Mnih", "Yee Whye Teh."], "venue": "arXiv:1611.00712 (2016).", "citeRegEx": "Maddison et al\\.,? 2016", "shortCiteRegEx": "Maddison et al\\.", "year": 2016}, {"title": "How (not) to protect genomic data privacy in a distributed network: using trail re-identification to evaluate and design anonymity protection systems", "author": ["B. Malin", "L. Sweeney."], "venue": "Journal of Biomedical Informatics 37, 3 (2004), 179\u2013192.", "citeRegEx": "Malin and Sweeney.,? 2004", "shortCiteRegEx": "Malin and Sweeney.", "year": 2004}, {"title": "Using the CareMap with Health Incidents Statistics for Generating the Realistic Synthetic Electronic Healthcare Record", "author": ["Scott McLachlan", "Kudakwashe Dube", "Thomas Gallagher."], "venue": "Healthcare Informatics (ICHI), 2016 IEEE International Conference on. IEEE, 439\u2013448.", "citeRegEx": "McLachlan et al\\.,? 2016", "shortCiteRegEx": "McLachlan et al\\.", "year": 2016}, {"title": "Unrolled Generative Adversarial Networks", "author": ["Luke Metz", "Ben Poole", "David Pfau", "Jascha Sohl-Dickstein."], "venue": "arXiv:1611.02163 (2016).", "citeRegEx": "Metz et al\\.,? 2016", "shortCiteRegEx": "Metz et al\\.", "year": 2016}, {"title": "Conditional generative adversarial nets", "author": ["Mehdi Mirza", "Simon Osindero."], "venue": "arXiv:1411.1784 (2014).", "citeRegEx": "Mirza and Osindero.,? 2014", "shortCiteRegEx": "Mirza and Osindero.", "year": 2014}, {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "author": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens."], "venue": "arXiv:1610.09585 (2016).", "citeRegEx": "Odena et al\\.,? 2016", "shortCiteRegEx": "Odena et al\\.", "year": 2016}, {"title": "Perturbed gibbs samplers for generating large-scale privacy-safe synthetic health data", "author": ["Yubin Park", "Joydeep Ghosh", "Mallikarjun Shankar."], "venue": "Healthcare Informatics (ICHI), 2013 IEEE International Conference on. IEEE, 493\u2013498.", "citeRegEx": "Park et al\\.,? 2013", "shortCiteRegEx": "Park et al\\.", "year": 2013}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala."], "venue": "arXiv:1511.06434 (2015).", "citeRegEx": "Radford et al\\.,? 2015", "shortCiteRegEx": "Radford et al\\.", "year": 2015}, {"title": "Generative adversarial text to image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee."], "venue": "ICML.", "citeRegEx": "Reed et al\\.,? 2016", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Satisfying disclosure restrictions with synthetic datasets", "author": ["J. Reiter."], "venue": "Journal of Official Statistics 18, 4 (2002), 531\u2013543.", "citeRegEx": "Reiter.,? 2002", "shortCiteRegEx": "Reiter.", "year": 2002}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen."], "venue": "NIPS. 2226\u20132234.", "citeRegEx": "Salimans et al\\.,? 2016", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Weaving technology and policy together to maintain confidentiality", "author": ["L. Sweeney."], "venue": "Journal of Law, Medicine, and Ethics 25, 2-3 (1997), 98\u2013110. 14", "citeRegEx": "Sweeney.,? 1997", "shortCiteRegEx": "Sweeney.", "year": 1997}, {"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems", "author": ["TensorFlow Team."], "venue": "(2015). http://tensorflow.org/ Software available from tensorflow.org.", "citeRegEx": "Team.,? 2015", "shortCiteRegEx": "Team.", "year": 2015}, {"title": "Pixel Recurrent Neural Networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu."], "venue": "arXiv:1601.06759 (2016).", "citeRegEx": "Oord et al\\.,? 2016a", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "ICML. 1096\u20131103.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald Williams."], "venue": "Machine learning 8, 3-4 (1992), 229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient", "author": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu."], "venue": "arXiv:1609.05473 (2016).", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Generating Text via Adversarial Training", "author": ["Yizhe Zhang", "Zhe Gan", "Lawrence Carin."], "venue": "NIPS Workshop on Adversarial Training (2016).", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "An alternative approach to de-identification is to generate synthetic data (McLachlan et al., 2016; Buczak et al., 2010; Lombardo and Moniz, 2008).", "startOffset": 75, "endOffset": 146}, {"referenceID": 1, "context": "An alternative approach to de-identification is to generate synthetic data (McLachlan et al., 2016; Buczak et al., 2010; Lombardo and Moniz, 2008).", "startOffset": 75, "endOffset": 146}, {"referenceID": 14, "context": "An alternative approach to de-identification is to generate synthetic data (McLachlan et al., 2016; Buczak et al., 2010; Lombardo and Moniz, 2008).", "startOffset": 75, "endOffset": 146}, {"referenceID": 23, "context": "Generative adversarial networks (GANs) have recently demonstrated impressive performance in generating high-quality synthetic images (Goodfellow et al., 2014; Radford et al., 2015; Goodfellow, 2016).", "startOffset": 133, "endOffset": 198}, {"referenceID": 12, "context": "Empirically, a GAN outperforms other popular generative models such as variational autoencoders (VAE) (Kingma and Welling, 2013) and PixelRNN/PixelCNN (van den Oord et al.", "startOffset": 102, "endOffset": 128}, {"referenceID": 27, "context": ", demographics (Sweeney, 1997; El Emam et al., 2011a), diagnoses (Loukides et al.", "startOffset": 15, "endOffset": 53}, {"referenceID": 15, "context": ", 2011a), diagnoses (Loukides et al., 2010), lab tests (Atreya et al.", "startOffset": 20, "endOffset": 43}, {"referenceID": 0, "context": ", 2010), lab tests (Atreya et al., 2013), visits across healthcare providers (Malin and Sweeney, 2004), and genomic variants (Erlich and Narayanan, 2014)) To mitigate re-identification vulnerabilities, researchers in the statistical disclosure control community have investigated how to generate synthetic datasets.", "startOffset": 19, "endOffset": 40}, {"referenceID": 17, "context": ", 2013), visits across healthcare providers (Malin and Sweeney, 2004), and genomic variants (Erlich and Narayanan, 2014)) To mitigate re-identification vulnerabilities, researchers in the statistical disclosure control community have investigated how to generate synthetic datasets.", "startOffset": 44, "endOffset": 69}, {"referenceID": 25, "context": ", (Dreschsler, 2011; Reiter, 2002).", "startOffset": 2, "endOffset": 34}, {"referenceID": 14, "context": "Many of these methods often rely heavily upon domain-specific knowledge along with actual data to generate synthetic EHRs (Lombardo and Moniz, 2008).", "startOffset": 122, "endOffset": 148}, {"referenceID": 22, "context": "More recently, and most related to our work, a privacy-preserving patient data generator was proposed based on a perturbed Gibbs sampler (Park et al., 2013).", "startOffset": 137, "endOffset": 156}, {"referenceID": 23, "context": ", 2014) include, but are not limited to, using convolutional neural networks to improve image processing capacity (Radford et al., 2015), extending GAN to a conditional architecture for higher quality image generation (Mirza and Osindero, 2014;", "startOffset": 114, "endOffset": 136}, {"referenceID": 0, "context": ", 2010), lab tests (Atreya et al., 2013), visits across healthcare providers (Malin and Sweeney, 2004), and genomic variants (Erlich and Narayanan, 2014)) To mitigate re-identification vulnerabilities, researchers in the statistical disclosure control community have investigated how to generate synthetic datasets. Yet, historically, these approaches have been limited to summary statistics for only several variables at a time (e.g., (Dreschsler, 2011; Reiter, 2002). For instance, McLachlan et al. (2016) used clinical practice guidelines and health incidence statistics with state transition machine to generate synthetic patient datasets.", "startOffset": 20, "endOffset": 508}, {"referenceID": 0, "context": ", 2010), lab tests (Atreya et al., 2013), visits across healthcare providers (Malin and Sweeney, 2004), and genomic variants (Erlich and Narayanan, 2014)) To mitigate re-identification vulnerabilities, researchers in the statistical disclosure control community have investigated how to generate synthetic datasets. Yet, historically, these approaches have been limited to summary statistics for only several variables at a time (e.g., (Dreschsler, 2011; Reiter, 2002). For instance, McLachlan et al. (2016) used clinical practice guidelines and health incidence statistics with state transition machine to generate synthetic patient datasets. There is some, but limited, work on synthetic data generation in the healthcare domain and, the majority that has, tend to be disease specific. For example, Buczak et al. (2010) generated EHR to explore questions related to the outbreak of specific illnesses, where care patterns in the source EHR were applied to generate synthetic datasets.", "startOffset": 20, "endOffset": 822}, {"referenceID": 24, "context": ", 2016), and text-to-image generation (Reed et al., 2016).", "startOffset": 38, "endOffset": 57}, {"referenceID": 32, "context": "SeqGAN (Yu et al., 2016) trains GAN with REINFORCE (Williams, 1992) and Monte-Carlo search to generate word sequences.", "startOffset": 7, "endOffset": 24}, {"referenceID": 31, "context": ", 2016) trains GAN with REINFORCE (Williams, 1992) and Monte-Carlo search to generate word sequences.", "startOffset": 34, "endOffset": 50}, {"referenceID": 9, "context": "Alternatively, one could use specialized distributions, such as the Gumbel-softmax (Jang et al., 2016; Kusner and Hern\u00e1ndez-Lobato, 2016), a concrete distribution (Maddison et al.", "startOffset": 83, "endOffset": 137}, {"referenceID": 13, "context": "Alternatively, one could use specialized distributions, such as the Gumbel-softmax (Jang et al., 2016; Kusner and Hern\u00e1ndez-Lobato, 2016), a concrete distribution (Maddison et al.", "startOffset": 83, "endOffset": 137}, {"referenceID": 16, "context": ", 2016; Kusner and Hern\u00e1ndez-Lobato, 2016), a concrete distribution (Maddison et al., 2016) or a soft-argmax function (Zhang et al.", "startOffset": 68, "endOffset": 91}, {"referenceID": 33, "context": ", 2016) or a soft-argmax function (Zhang et al., 2016) to approximate the gradient of the model from discrete samples.", "startOffset": 34, "endOffset": 54}, {"referenceID": 9, "context": "Alternatively, one could use specialized distributions, such as the Gumbel-softmax (Jang et al., 2016; Kusner and Hern\u00e1ndez-Lobato, 2016), a concrete distribution (Maddison et al., 2016) or a soft-argmax function (Zhang et al., 2016) to approximate the gradient of the model from discrete samples. However, since these approaches focus on the softmax distribution, they cannot be directly invoked for multi-label discrete variables, especially in the count variable case. Another way to handle discrete variables is to generate distributed representations, then decode them into discrete outputs. For example, Glover (2016) generated document embeddings with GAN, but did not attempt to generate actual documents.", "startOffset": 84, "endOffset": 624}, {"referenceID": 30, "context": "Such a mechanism leads the autoencoder to learn salient features of the samples and has been successfully used in certain applications, like image processing (Goodfellow et al., 2016; Vincent et al., 2008).", "startOffset": 158, "endOffset": 205}, {"referenceID": 26, "context": "Some methods have been proposed to cope with mode collapse such as minibatch discrimination and unrolled GANs, but they require some fine-tuning of the hyperparameters, or scalability has not been addressed (Salimans et al., 2016; Metz et al., 2016).", "startOffset": 207, "endOffset": 249}, {"referenceID": 19, "context": "Some methods have been proposed to cope with mode collapse such as minibatch discrimination and unrolled GANs, but they require some fine-tuning of the hyperparameters, or scalability has not been addressed (Salimans et al., 2016; Metz et al., 2016).", "startOffset": 207, "endOffset": 249}, {"referenceID": 30, "context": "1We considered the denoising autoencoder (dAE) (Vincent et al., 2008) but there was no visible performance improvement.", "startOffset": 47, "endOffset": 69}, {"referenceID": 26, "context": "\u2022 PD: We pre-train the autoencoder and use minibatch discrimination (Salimans et al., 2016).", "startOffset": 68, "endOffset": 91}, {"referenceID": 7, "context": "\u2022 Stacked RBM (DBN): We train a stacked Restricted Boltzmann Machines (Hinton and Salakhutdinov, 2006) and generate synthetic samples using Gibbs sampling.", "startOffset": 70, "endOffset": 102}, {"referenceID": 12, "context": "\u2022 Variational Autoencoder (VAE): We train a variational autoencoder (Kingma and Welling, 2013) where the encoder and the decoder are constructed with feed-forward neural networks.", "startOffset": 68, "endOffset": 94}, {"referenceID": 28, "context": "12 (Team, 2015).", "startOffset": 3, "endOffset": 15}, {"referenceID": 11, "context": "For training models, we used Adam (Kingma and Ba, 2014) with a mini-batch of 100 patients on a machine equipped with Intel Xeon E5-2630, 256GB RAM, four Nvidia Pascal Titan X\u2019s and CUDA 8.", "startOffset": 34, "endOffset": 55}], "year": 2017, "abstractText": "Access to electronic health records (EHR) data has motivated computational advances in medical research. However, various concerns, particularly over privacy, can limit access to and collaborative use of EHR data. Sharing synthetic EHR data could mitigate risk. In this paper, we propose a new approach, medical Generative Adversarial Network (medGAN), to generate realistic synthetic EHRs. Based on an input EHR dataset, medGAN can generate high-dimensional discrete variables (e.g., binary and count features) via a combination of an autoencoder and generative adversarial networks. We also propose minibatch averaging to efficiently avoid mode collapse, and increase the learning efficiency with batch normalization and shortcut connections. To demonstrate feasibility, we showed that medGAN generates synthetic EHR datasets that achieve comparable performance to real data on many experiments including distribution statistics, predictive modeling tasks and medical expert review.", "creator": "LaTeX with hyperref package"}}}