{"id": "1703.06501", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Mar-2017", "title": "M\\'etodos de Otimiza\\c{c}\\~ao Combinat\\'oria Aplicados ao Problema de Compress\\~ao MultiFrases", "abstract": "The Internet has led to a dramatic increase in the amount of available information. In this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for Multi-Sentences Compression (MSC), method which reduces the sentence length without removing core information, the insertion of optimization methods requires further study to improve the performance of MSC. This article describes a method for MSC using Combinatorial Optimization and Graph Theory to generate more informative sentences while maintaining their grammaticality. An experiment led on a corpus of 40 clusters of sentences shows that our system has achieved a very good quality and is better than the state-of-the-art.", "histories": [["v1", "Sun, 19 Mar 2017 19:56:25 GMT  (50kb,D)", "http://arxiv.org/abs/1703.06501v1", "12 pages, 1 figure, 3 tables (paper in Portuguese), Preprint of XLVIII Simp\\'osio Brasileiro de Pesquisa Operacional, 2016, Vit\\'oria, ES, (Brazil)"]], "COMMENTS": "12 pages, 1 figure, 3 tables (paper in Portuguese), Preprint of XLVIII Simp\\'osio Brasileiro de Pesquisa Operacional, 2016, Vit\\'oria, ES, (Brazil)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["elvys linhares pontes", "thiago gouveia da silva", "r\\'ea carneiro linhares", "juan-manuel torres-moreno", "st\\'ephane huet"], "accepted": false, "id": "1703.06501"}, "pdf": {"name": "1703.06501.pdf", "metadata": {"source": "CRF", "title": "Me\u0301todos de Otimizac\u0327a\u0303o Combinato\u0301ria Aplicados ao Problema de Compressa\u0303o MultiFrases\u2217", "authors": ["Elvys Linhares Pontes", "Thiago Gouveia da Silva", "Andr\u00e9a Carneiro Linhares", "Juan-Manuel Torres-Moreno", "St\u00e9phane Huet"], "emails": ["elvys.linhares-pontes@alumni.univ-avignon.com,", "thiago.gouveia@ifpb.edu.br"], "sections": [{"heading": null, "text": "this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for MultiSentences Compression (MSC), method which reduces the sentence length without removing core information, the insertion of optimization methods requires further study to improve the performance of MSC. This article describes a method for MSC using Combinatorial Optimization and Graph Theory to generate more informative sentences while maintaining their grammaticality. An experiment led on a corpus of 40 clusters of sentences shows that our system has achieved a very good quality and is better than the state-of-the-art.\nKEYWORDS. Combinatorial Optimization, Multi-Sentences Compression, Word Graph.\n\u2217Preprint of XLVIII Simpo\u0301sio Brasileiro de Pesquisa Operacional, 2016.\nar X\niv :1\n1. Introduc\u0327a\u0303o\nO aumento da quantidade de dispositivos eletro\u0302nicos (smartphones, tablets, etc) e da Internet mo\u0301vel tornaram o acesso a\u0300 informac\u0327a\u0303o fa\u0301cil e ra\u0301pido. Atrave\u0301s da Internet e\u0301 poss\u0131\u0301vel ter acesso aos acontecimentos de todo o mundo a partir de diferentes sites, blogs e portais. Pa\u0301ginas como a Wikipe\u0301dia e portais de not\u0131\u0301cias fornecem informac\u0327o\u0303es detalhadas sobre diversas tema\u0301ticas, entretanto os textos sa\u0303o longos e possuem muitas informac\u0327o\u0303es irrelevantes. Uma soluc\u0327a\u0303o para esse problema e\u0301 a gerac\u0327a\u0303o de resumos contendo as principais informac\u0327o\u0303es do documento e sem redunda\u0302ncias (Linhares Pontes et al., 2014). Vista a vasta quantidade e o fa\u0301cil acesso a\u0300s informac\u0327o\u0303es, e\u0301 poss\u0131\u0301vel automatizar a ana\u0301lise e gerac\u0327a\u0303o de resumos a partir da ana\u0301lise estat\u0131\u0301stica, morfolo\u0301gica e sinta\u0301tica das frases (Torres-Moreno, 2014).\nO Processamento da Linguagem Natural (PLN) concerne a\u0300 aplicac\u0327a\u0303o de sistemas e te\u0301cnicas de informa\u0301tica para analisar a linguagem humana. Dentre as diversas aplicac\u0327o\u0303es do PLN (traduc\u0327a\u0303o automa\u0301tica, compressa\u0303o textual, etc.), a Sumarizac\u0327a\u0303o Automa\u0301tica de Textos (SAT) consiste em resumir um ou mais textos automaticamente. O sistema sumarizador identifica os dados relevantes e cria um resumo a partir das principais informac\u0327o\u0303es (Linhares Pontes et al., 2015). A CMF e\u0301 um dos me\u0301todos utilizados na SAT para gerar resumos, que utiliza um conjunto de frases para gerar uma u\u0301nica frase de tamanho reduzido gramaticalmente correta e informativa (Filippova, 2010; Boudin e Morin, 2013).\nNeste artigo, apresentamos um me\u0301todo baseado na Teoria dos Grafos e na Otimizac\u0327a\u0303o Combinato\u0301ria para modelar um documento como um Grafo de Palavras (GP) (Filippova, 2010) e gerar a CMF com uma melhor qualidade informativa.\nA sec\u0327a\u0303o 2 descreve o problema e os trabalhos relacionados a\u0300 CMF. Detalhamos a abordagem e a modelagem matema\u0301tica nas sec\u0327o\u0303es 3 e 4, respectivamente. O corpus, as ferramentas utilizadas e os resultados obtidos sa\u0303o discutidos na sec\u0327a\u0303o 5. Finalmente, as concluso\u0303es e os comenta\u0301rios finais sa\u0303o expostos na sec\u0327a\u0303o 6."}, {"heading": "2. Compressa\u0303o MultiFrases", "text": "A Compressa\u0303o MultiFrases (CMF) consiste em produzir uma frase de tamanho reduzido gramaticalmente correta a partir de um conjunto de frases oriundas de um documento, preservando-se as principais informac\u0327o\u0303es desse conjunto. Uma compressa\u0303o pode ter diferentes valores de Taxa de Compressa\u0303o (TC), entretanto quanto menor a TC maior sera\u0301 a reduc\u0327a\u0303o das informac\u0327o\u0303es nele contidas. Seja D o documento analisado composto pelas frases {f1, f2, . . . , fn} e fraseCMF a compressa\u0303o desse documento, a TC e\u0301 definida por:\nTC = ||fraseCMF ||\u2211n\ni=1 ||fi|| n\n. (1)\nonde ||fi|| e\u0301 o tamanho da frase fi (quantidade de palavras). Dessa forma, os principais desafios da CMF sa\u0303o a selec\u0327a\u0303o dos conteu\u0301dos informativos e a legibilidade da frase produzida.\nDentre as diversas abordagens feitas sobre a CMF, algumas baseiam-se em analisadores sinta\u0301ticos para a produc\u0327a\u0303o de compresso\u0303es gramaticais. Por exemplo, Barzilay e McKeown (2005) desenvolveram uma te\u0301cnica de gerac\u0327a\u0303o text-to-text em que cada frase\ndo texto e\u0301 representada como uma a\u0301rvore de depende\u0302ncia. De forma geral, essa te\u0301cnica alinha e combina estas a\u0301rvores para gerar a fusa\u0303o das frases analisadas. Outra abordagem poss\u0131\u0301vel e\u0301 descrita por Filippova (2010), que gerou compresso\u0303es de frases de boa qualidade utilizando uma simples modelagem baseada na Teoria dos Grafos e uma lista de stopwords1. Boudin e Morin (2013) geraram a CMF mais informativas a partir da ana\u0301lise da releva\u0302ncia das frases geradas pelo me\u0301todo de Filippova.\nVisto que os trabalhos apresentados utilizaram uma modelagem simples e obtiveram resultados de boa qualidade, este trabalho baseia-se na mesma modelagem utilizada por Filippova e me\u0301todos de otimizac\u0327a\u0303o combinato\u0301ria para aumentar a informatividade da CMF. As subsec\u0327o\u0303es 2.1 e 2.2 descrevem os me\u0301todos utilizados por Filippova (2010) e Boudin e Morin (2013), respectivamente."}, {"heading": "2.1. Filippova", "text": "Filippova (2010) modelou um documento D composto por frases similares como um Grafo de Palavras (GP). O GP e\u0301 um grafo direcionadoGP = (V,A), onde V e\u0301 o conjunto de ve\u0301rtices (palavras) e A e\u0301 o conjunto de arcos (relac\u0327a\u0303o de adjace\u0302ncia). Dessa forma, dado um documento D de frases similares {f1, f2, . . . , fn}, o GP e\u0301 constru\u0131\u0301do a partir da adic\u0327a\u0303o dessas frases no grafo. A Figura 1 ilustra o GP descrito por Filippova das seguintes frases:\na) George Solita\u0301rio, a u\u0301ltima tartaruga gigante Pinta Island do mundo, faleceu. b) A tartaruga gigante conhecida como George Solita\u0301rio morreu na segunda no Par-\nque Nacional de Galapagos, Equador. c) Ele tinha apenas cem anos de vida, mas a u\u0301ltima tartaruga gigante Pinta conhecida,\nGeorge Solita\u0301rio, faleceu. d) George Solita\u0301rio, a u\u0301ltima tartaruga gigante da sua espe\u0301cie, morreu.\nInicialmente, o GP e\u0301 composto pela primeira frase (a) e pelos ve\u0301rtices -comec\u0327oe -fim-. Uma palavra e\u0301 representada por um ve\u0301rtice existente somente se ela possuir a\n1Stopwords sa\u0303o palavras comuns sem releva\u0302ncia informativa para uma frase. Ex: artigos, preposic\u0327o\u0303es, etc.\nmesma forma minu\u0301scula, mesma Part-Of-Speech (POS)2, e se na\u0303o existir outra palavra dessa mesma frase que ja\u0301 tenha sido mapeada nesse ve\u0301rtice. Um novo ve\u0301rtice e\u0301 criado caso na\u0303o seja encontrado um ve\u0301rtice com suas caracter\u0131\u0301sticas no GP. Dessa forma, cada frase representa um caminho simples entre os ve\u0301rtices -comec\u0327o- e -fim-.\nAs frases sa\u0303o analisadas e adicionadas individualmente ao GP. Para cada frase analisada, as palavras sa\u0303o inseridas na seguinte ordem:\n1. Palavras que na\u0303o sejam stopwords e para os quais na\u0303o existam nenhum candidato no grafo ou mapeamento na\u0303o amb\u0131\u0301guo; 2. Palavras que na\u0303o sejam stopwords e para os quais existam va\u0301rios candidatos poss\u0131\u0301veis no grafo ou que ocorram mais de uma vez na mesma frase; 3. Stopwords.\nNos grupos 2 e 3, o mapeamento das palavras e\u0301 amb\u0131\u0301guo, pois ha\u0301 mais de uma palavra no grafo que referencia a mesma palavra/POS. Nesse caso, as palavras predecessoras e posterioras sa\u0303o analisadas para verificar o contexto da palavra e escolher o mapeamento correto. Caso uma dessas palavras na\u0303o possua o mesmo contexto das existentes no grafo, um novo ve\u0301rtice e\u0301 criado para representa\u0301-la.\nTendo adicionado os ve\u0301rtices, os pesos dos arcos representam o n\u0131\u0301vel de coesa\u0303o entre as palavras de dois ve\u0301rtices a partir da freque\u0302ncia e da posic\u0327a\u0303o dessas palavras nas frases, conforme as Equac\u0327o\u0303es 2 e 3:\nw(ei,j) = coesa\u0303o(ei,j)\nfreq(i)\u00d7 freq(j) , (2)\ncoesa\u0303o(ei,j) = freq(i) + freq(j)\u2211 f\u2208D dist(f, i, j) \u22121 , (3)\ndist(f, i, j) = { pos(f, i)\u2212 pos(f, j) se pos(f, i) < pos(f, j) 0 caso contra\u0301rio (4)\nonde freq(i) e\u0301 a freque\u0302ncia da palavra mapeada no ve\u0301rtice i e a func\u0327a\u0303o pos(f, i) retorna a posic\u0327a\u0303o da palavra i na frase f .\nA partir do GP, o sistema calcula os 50 menores caminhos3 que tenham no m\u0131\u0301nimo oito palavras e ao menos um verbo. Por fim, o sistema normaliza os scores (dista\u0302ncias do caminhos) das frases geradas a partir do comprimento das mesmas e seleciona a frase com o menor score normalizado como a melhor CMF."}, {"heading": "2.2. Boudin e Morin", "text": "Boudin e Morin (2013) (BM) propuseram um me\u0301todo para melhor avaliar a qualidade de uma frase e gerar compresso\u0303es mais informativas a partir da abordagem descrita por Filippova (sec\u0327a\u0303o 2.1). BM utilizaram a mesma metodologia de Filippova para gerar os 200 menores caminhos, que tenham no m\u0131\u0301nimo oito palavras e ao menos um verbo, do\n2POS e\u0301 a classe gramatical de uma palavra numa frase. 3Ressaltando que cada caminho no GP representa uma frase.\nGP. Ao inve\u0301s de realizar uma simples normalizac\u0327a\u0303o dos valores de cada frase como Filippova, BM mensuraram a releva\u0302ncia da frase gerada (caminho c no GP) a partir das keyphrases4 e o comprimento das frases, conforme Equac\u0327o\u0303es 5 e 6:\nscore(c) =\n\u2211 i,j\u2208caminho(c)w(i,j)\n||c|| \u00d7 \u2211 k\u2208c scorekp(k) , (5)\nscorekp(k) = \u2211 w\u2208k TextRank(w) ||k||+ 1 , (6)\nonde w(i,j) e\u0301 o score entre os ve\u0301rtices i e j descrito na Equac\u0327a\u0303o 2, o algoritmo TextRank (Mihalcea e Tarau, 2004) que calcula a releva\u0302ncia de uma palavra w no GP a partir das suas palavras predecessoras e posteriores e scorekp(k) e\u0301 a releva\u0302ncia da keyphrase k presente no caminho c. Por fim, a frase com o menor score e\u0301 a escolhida para a compressa\u0303o do texto."}, {"heading": "3. Nova modelagem do problema", "text": "Os me\u0301todos de Filippova e BM calculam os menores caminhos do GP analisando somente o n\u0131\u0301vel de coesa\u0303o entre duas palavras vizinhas no texto. Apo\u0301s a gerac\u0327a\u0303o dos caminhos, os scores de cada caminho sa\u0303o normalizados para escolher o \u201cmenor\u201d deles. Entretanto, duas palavras possuindo uma forte coesa\u0303o na\u0303o significa que as mesmas possuam uma boa informatividade. Por mais que a normalizac\u0327a\u0303o ou reana\u0301lise das frases seja eficiente, esses me\u0301todos esta\u0303o sempre limitados a\u0300s frases geradas pela ana\u0301lise do n\u0131\u0301vel de coesa\u0303o. Portanto, a gerac\u0327a\u0303o de 50 ou 200 dos menores caminhos (frases) na\u0303o garante a existe\u0302ncia de uma frase com boa informatividade. Por isso, propomos um me\u0301todo para analisar concomitantemente a coesa\u0303o e a releva\u0302ncia das palavras a fim de gerar uma compressa\u0303o mais informativa de um documento.\nO me\u0301todo aqui exposto visa calcular o caminho mais curto analisando a coesa\u0303o das palavras e bonificando os caminhos que possuam palavras-chaves e 3-grams5 frequentes do texto. Inicialmente, utiliza-se a mesma abordagem de Filippova (sec\u0327a\u0303o 2.1) para modelar um documento D como um GP e calcular a coesa\u0303o das palavras. Ale\u0301m de considerar a coesa\u0303o, analisamos as palavras-chaves e os 3-grams do documento para gerar uma CMF mais informativa. As palavras-chaves auxiliam a gerac\u0327a\u0303o de caminhos com as principais informac\u0327o\u0303es descritas no texto. Como o documento D e\u0301 composto por frases similares, consideramos que o documento possui somente uma tema\u0301tica. A Latent Dirichlet Allocation (LDA) e\u0301 um me\u0301todo para analisar as frases de um texto e identificar o conjunto de palavras que representam as tema\u0301ticas nele abordadas (Blei et al., 2003). Configura-se o me\u0301todo LDA para identificar o conjunto de palavras que representa uma u\u0301nica tema\u0301tica do documento. Finalmente, esse conjunto de palavras constitui as palavras-chaves do documento D.\nUma outra considerac\u0327a\u0303o sobre o documento analisado e\u0301 que a presenc\u0327a de uma palavra em diferentes frases aumenta sua releva\u0302ncia para a CMF (vale salientar que consideramos a releva\u0302ncia dos stopwords igual a zero). A partir da ponderac\u0327a\u0303o dos 2-grams\n4Keyphrases sa\u0303o as palavras que representam o conteu\u0301do principal do texto. 53-gram e\u0301 formado por 3 palavras vizinhas.\n(Equac\u0327a\u0303o 2), consideramos que a releva\u0302ncia de um 3-gram e\u0301 baseado na releva\u0302ncia dos dois 2-grams que o formam, como descrito na Equac\u0327a\u0303o 7:\n3-gram(i, j, k) = qt3(i, j, k) maxa,b,c\u2208GP qt3(a, b, c) \u00d7 w(ei,j) + w(ej,k) 2 , (7)\nonde qt3(i, j, k) e\u0301 quantidade de 3-grams composto pelas palavras dos ve\u0301rtices i, j e k no documento. Os 3-grams auxiliam a gerac\u0327a\u0303o de CMF com estruturas importantes para o texto e incrementam a qualidade gramatical das frases geradas.\nO nosso sistema calcula os 50 menores caminhos do GP que possuam ao menos 8 palavras, baseado na coesa\u0303o, palavras-chaves e 3-grams (Equac\u0327a\u0303o 9). Contrariamente ao me\u0301todo de Filippova, as frases podem ter score negativo, pois reduzimos o valor do caminho composto por palavras-chaves e 3-grams. Dessa forma, normalizamos os scores dos caminhos (frases) baseado na func\u0327a\u0303o exponencial para obter um score maior que zero, conforme a Equac\u0327a\u0303o 8:\nscorenorm(f) = escoreopt(f)\n||f || , (8)\nonde scoreopt(f) e\u0301 o valor do caminho para gerar a frase f a partir da Equac\u0327a\u0303o 9. Finalmente, selecionamos a frase com menor score normalizado e contendo, ao menos, um verbo como a melhor compressa\u0303o das frases do documento.\nPara exemplificar nosso me\u0301todo, simplificamos sua ana\u0301lise e utilizamos o texto modelado na Figura 1. Nessa figura, existem diversos caminhos poss\u0131\u0301veis entre os ve\u0301rtices -comec\u0327o- e -fim-. A partir das palavras-chaves \u201cGeorge\u201d, \u201cgigante\u201d, \u201csolita\u0301rio\u201d \u201ctartaruga\u201d e \u201cu\u0301ltima\u201d, nosso me\u0301todo gerou a compressa\u0303o \u201ca tartaruga gigante conhecida george solita\u0301rio morreu\u201d. Dentre as 5 palavras-chaves analisadas, foi gerada uma compressa\u0303o contendo 4 delas e com as principais informac\u0327o\u0303es das frases."}, {"heading": "4. Modelo Matema\u0301tico Proposto", "text": "Formalmente, o GP utilizado pode ser representado como segue: seja GP = (V,A) um grafo orientado simples no qual V e\u0301 o conjunto de ve\u0301rtices (palavras), A o conjunto de arcos (2-grams) e bij e\u0301 o peso do arco (i, j) \u2208 A (coesa\u0303o das palavras dos ve\u0301rtices i e j, Equac\u0327a\u0303o 2). Sem perda de generalidade, considere v0 como o ve\u0301rtice -comec\u0327o- e adicione um arco auxiliar do ve\u0301rtice -fim- para v0. Adicionalmente, cada ve\u0301rtice possui uma cor indicando se o mesmo e\u0301 uma palavra-chave. Denotamos K como o conjunto de cores em que cada palavra-chave do documento representa uma cor diferente. A cor 0 (na\u0303o palavras-chaves) possui o custo c0 = 0 e as palavras-chaves possuem o mesmo custo ck = 1 (para k > 0 e k \u2208 K). O conjunto T e\u0301 composto pelos 3-grams do documento com uma freque\u0302ncia maior que 1. Cada 3-gram t = (a, b, c) \u2208 T possui o custo dt = 3-gram(a, b, c) (Equac\u0327a\u0303o 7) normalizados entre 0 e 1.\nExistem va\u0301rios algoritmos com complexidade polinomial para encontrar o menor caminho em um grafo. Contudo, a restric\u0327a\u0303o de que o caminho deve possuir um nu\u0301mero m\u0131\u0301nimo Pmin de ve\u0301rtices (o nu\u0301mero m\u0131\u0301nimo de palavras da compressa\u0303o) torna o problema NP-Hard. De fato, encontrar o menor caminho no GP descrito implica encontrar um\nciclo com in\u0131\u0301cio e fim em v0, e caso Pmin seja igual a |V |, o problema corresponde ao Problema do Caixeiro Viajante (PCV). Nesse caso, como o PCV e\u0301 um caso especial do problema descrito, ele tambe\u0301m sera\u0301 NP-Hard.\nO modelo matema\u0301tico proposto para resoluc\u0327a\u0303o do problema apresentado define cinco grupos de varia\u0301veis de decisa\u0303o:\n\u2022 xij , \u2200(i, j) \u2208 A, indicando se o arco (i, j) faz parte da soluc\u0327a\u0303o; \u2022 yv, \u2200v \u2208 V , indicando se o ve\u0301rtice (a palavra) v faz parte da soluc\u0327a\u0303o; \u2022 zt, \u2200t \u2208 T , indicando se o 3-gram t faz parte da soluc\u0327a\u0303o; \u2022 wk, \u2200k \u2208 K, indicando que alguma palavra com a cor (palavra-chave) k foi uti-\nlizada na soluc\u0327a\u0303o; e \u2022 uv, \u2200v \u2208 V , varia\u0301veis auxiliares para eliminac\u0327a\u0303o de sub-ciclos da soluc\u0327a\u0303o.\nO processo de encontrar as 50 melhores soluc\u0327o\u0303es se deu pela proibic\u0327a\u0303o das soluc\u0327o\u0303es encontradas e reexecuc\u0327a\u0303o do modelo. Optamos por essa estrate\u0301gia em virtude da simetria gerada pela te\u0301cnica de eliminac\u0327a\u0303o de sub-ciclos que utilizamos. A formulac\u0327a\u0303o e\u0301 apresentada nas expresso\u0303es (9) a (22).\nMinimize ( \u03b1 \u2211\n(i,j)\u2208A bi,j \u00b7 xi,j \u2212 \u03b2 \u2211 k\u2208K ck \u00b7 wk \u2212 \u03b3 \u2211 t\u2208T dk \u00b7 zt )\n(9)\ns.a. \u2211 v\u2208V\nyv \u2265 Pmin, (10)\u2211 v\u2208V (k) yv \u2265 wk, \u2200k \u2208 K, (11)\n2zt \u2264 xij + xjl, \u2200t = (i, j, l) \u2208 T, (12)\u2211 i\u2208\u03b4\u2212(v)\nxiv = yv \u2200v \u2208 V, (13)\u2211 i\u2208\u03b4+(v) xvi = yv \u2200v \u2208 V, (14)\ny0 = 1, (15)\nu0 = 1, (16)\nui \u2212 uj + 1 \u2264M \u2212M \u00b7 xij \u2200(i, j) \u2208 A, j 6= 0, (17)\nxij \u2208 {0, 1}, \u2200(i, j) \u2208 A, (18)\nzl \u2208 {0, 1}, \u2200t \u2208 T, (19)\nyv \u2208 {0, 1}, \u2200v \u2208 V, (20)\nwk \u2208 [0, 1], \u2200k \u2208 K, (21)\nuv \u2208 [1, |V |], \u2200v \u2208 V. (22)\nA func\u0327a\u0303o objetiva do programa (9) maximiza a qualidade da compressa\u0303o gerada. As varia\u0301veis \u03b1, \u03b2 e \u03b3 controlam, respectivamente, a releva\u0302ncia da coesa\u0303o, das palavraschaves e dos 3-grams na gerac\u0327a\u0303o da compressa\u0303o. A restric\u0327a\u0303o (10) limita o nu\u0301mero de ve\u0301rtices (palavras) utilizadas na soluc\u0327a\u0303o. O conjunto de restric\u0327o\u0303es (11) faz a corresponde\u0302ncia entre as varia\u0301veis de cores (palavras-chaves) e de ve\u0301rtices (palavras), sendo V (k) o conjunto de todos os ve\u0301rtices com a cor k (uma palavra-chave pode ser representada por mais de um ve\u0301rtice). O conjunto de restric\u0327o\u0303es (12) faz a corresponde\u0302ncia entre as varia\u0301veis de 3-grams e de arcos (2-grams). As igualdades (13) e (14) obrigam que para cada palavra usada na soluc\u0327a\u0303o exista um arco ativo interior (entrando) e um exterior (saindo), respectivamente. A igualdade (15) forc\u0327a que o ve\u0301rtice zero seja usado na soluc\u0327a\u0303o. Por fim, as restric\u0327o\u0303es (16) e (17) sa\u0303o responsa\u0301veis pela eliminac\u0327a\u0303o de sub-ciclos enquanto as expresso\u0303es (18)-(22) definem o dom\u0131\u0301nio das varia\u0301veis.\nComo discutido em Pataki (2003), existem duas formas cla\u0301ssicas de evitar ciclos em problemas derivados do PCV. A primeira consiste na criac\u0327a\u0303o de um conjunto exponencial de cortes garantindo que para todo subconjunto de ve\u0301rtices S \u2282 V , S 6= \u2205, haja exatamente |S| \u2212 1 arcos ativos (mais detalhes em Lenstra et al. (1985)). A segunda, conhecida como formulac\u0327a\u0303o Miller\u2013Tucker\u2013Zemlin (MTZ) utiliza um conjunto auxiliar de varia\u0301veis, uma para cada ve\u0301rtice, de modo a evitar que um ve\u0301rtice seja visitado mais de uma vez no ciclo e um conjunto de arcos-restric\u0327o\u0303es. Mais informac\u0327o\u0303es sobre a formulac\u0327a\u0303o MTZ podem ser obtidas em O\u0308ncan et al. (2009).\nNeste trabalho, optamos por eliminar sub-ciclos utilizando o me\u0301todo MTZ, uma vez que sua implementac\u0327a\u0303o e\u0301 mais simples. Para tal, utilizamos uma varia\u0301vel auxiliar uv para cada ve\u0301rtice v \u2208 V , e o conjunto de arcos-restric\u0327o\u0303es definido em (17). Nesse grupo de restric\u0327o\u0303es, M representa um nu\u0301mero grande o suficiente, podendo ser utilizado o valor M = |V |."}, {"heading": "5. Experimentos computacionais", "text": "O desempenho do sistema proposto foi analisado a partir de diversos valores dos para\u0302metros (\u03b2 e \u03b3) associados a\u0300 func\u0327a\u0303o objetivo. Os testes foram realizados num computador com processador i5 2.6 GHz e 6 GB de memoria RAM no sistema operacional Ubuntu 14.04 de 64 bits. Os algoritmos foram implementados utilizando a linguagem de programac\u0327a\u0303o Python e as bibliotecas takahe6 e gensim7. O modelo matema\u0301tico foi implementado na linguagem C++ com a biblioteca Concert e o solver utilizado foi o CPLEX 12.6."}, {"heading": "5.1. Corpus e ferramentas utilizadas", "text": "Para avaliar a qualidade dos sistemas, utilizamos o corpus publicado por Boudin e Morin (2013). Esse corpus conte\u0301m 618 frases (me\u0301dia de 33 palavras por frase) divididas em 40 clusters de not\u0131\u0301cias em France\u0302s extra\u0131\u0301dos do Google News8. A taxa de redunda\u0302ncia de um corpus e\u0301 obtida pela divisa\u0303o da quantidade de palavras u\u0301nicas pela quantidade de palavras de cada cluster. A taxa de redunda\u0302ncia do corpus que utilizamos e\u0301 38,8%. Cada palavra do corpus e\u0301 acompanhada por sua POS. Para cada cluster, ha\u0301 3 frases comprimidas por profissionais. Dividimos o corpus em duas partes de 20 clusters. A primeira parte e\u0301 utilizada como corpus de aprendizado e a outra parte como corpus de teste. As frases do\n6Site: http://www.florianboudin.org/publications.html 7Site: https://radimrehurek.com/gensim/models/ldamodel.html 8Site: https://news.google.fr\ncorpus de aprendizado tem o tamanho me\u0301dio de 34,1 palavras e as frases do corpus de teste tem um tamanho me\u0301dio de 31,6 palavras.\nAs caracter\u0131\u0301sticas mais importante da CMF sa\u0303o a informatividade e gramaticalidade das frases. A informatividade representa a porcentagem das principais informac\u0327o\u0303es transmitidas no texto. Como consideramos que as compresso\u0303es de refere\u0302ncia possuem as informac\u0327o\u0303es mais importantes, avaliamos a informatividade de uma compressa\u0303o baseada nas informac\u0327o\u0303es em comum entre a mesma e as compresso\u0303es de refere\u0302ncia usando o sistema ROUGE (Lin, 2004). Utilizamos as me\u0301tricas de cobertura ROUGE-1 e ROUGE-2, que analisam os 1-grams e 2-grams, respectivamente, das compresso\u0303es de refere\u0302ncias presentes nas compresso\u0303es geradas pelos sistemas, para estimar a informatividade das compresso\u0303es geradas.\nDevido a complexidade da ana\u0301lise gramatical de uma frase, foi utilizado uma avaliac\u0327a\u0303o manual para estimar a qualidade das compresso\u0303es propostas por nosso sistema. Como a avaliac\u0327a\u0303o humana e\u0301 lenta, utilizamos essa te\u0301cnica somente para o corpus de teste. Para o corpus de aprendizado, decidimos avaliar somente a qualidade informativa (coberturas ROUGE-1 e ROUGE-2) e a TC devido ser invia\u0301vel a ana\u0301lise manual da quantidade de testes do nosso sistema."}, {"heading": "5.2. Resultados", "text": "Nomeamos nosso sistema como GP+OPT e utilizamos os sistemas de Filippova e de BM como baselines. Testamos o GP+OPT utilizando 1, 3, 5, 7 e 9 palavras-chaves9 (PC) obtidas a partir do me\u0301todo LDA. Como o GP+OPT utiliza como base o me\u0301todo de Filippova, tornamos fixo o \u03b1 = 1.0 (priorizando a coesa\u0303o das compresso\u0303es geradas) e variamos \u03b2 e \u03b3 de tal forma que:\n\u03b2 + \u03b3 < 1.0; \u03b2, \u03b3 = 0.0, 0.1, ..., 0.8, 0.9. (23)\nTodos os sistemas geraram a compressa\u0303o de um documento em tempo via\u0301vel (menos de 6 segundos). Devido a\u0300 grande quantidade de testes gerados para o corpus de aprendizado, selecionamos os resultados que generalizam o funcionamento do GP+OPT. A Tabela 1 descreve a qualidade e a TC das compresso\u0303es. Essa tabela e\u0301 dividida em 4 partes. A primeira descreve os resultados das baselines e as demais partes descrevem os resultados do nosso sistema. A primeira parte da tabela comprova que o po\u0301s-tratamento utilizado por BM (ana\u0301lise da releva\u0302ncia das keyphrases) e\u0301 melhor que a simples normalizac\u0327a\u0303o dos scores das frases realizada por Filippova. O aumento da releva\u0302ncia dos 3-grams na nossa modelagem melhora a informatividade da compressa\u0303o sem aumentar bruscamente a TC, pois os 3-grams favorecem a utilizac\u0327a\u0303o de 2-grams frequentes no texto (segunda parte da Tabela 1). Ale\u0301m disso, os 3-grams podem melhorar a qualidade gramatical, pois eles adicionam conjuntos de palavras gramaticalmente corretos a\u0300 compressa\u0303o.\nApesar do aumento da releva\u0302ncia das palavras-chaves gerar compresso\u0303es com uma maior TC, as compresso\u0303es sa\u0303o mais informativas (a terceira parte da tabela) e proporcionam as melhores compresso\u0303es (linhas em negrito da Tabela 1). Dentre os melhores resultados (u\u0301ltima parte da Tabela), escolhemos a versa\u0303o do nosso sistema com PC=9,\n9Visto que o texto e\u0301 composto de frases similares sobre um mesmo to\u0301pico, consideramos que 9 palavras e\u0301 a quantidade ma\u0301xima de palavras-chaves para representar um to\u0301pico.\n\u03b2=0.8 e \u03b3=0.1, pois essa configurac\u0327a\u0303o prioriza as palavras-chaves e tenta adicionar 3- grams a\u0300s compresso\u0303es.\nSelecionado a melhor configurac\u0327a\u0303o do nosso sistema, validamos a qualidade dos sistemas utilizando o corpus de teste (Tabela 2). Similar aos resultados do corpus de aprendizado, o me\u0301todo de BM foi melhor que o me\u0301todo de Filippova para as me\u0301tricas ROUGE-1 e ROUGE-2. GP+OPT obteve resultados bem superiores que as baselines comprovando que a ana\u0301lise da coesa\u0303o juntamente com as palavras-chaves e 3-grams auxiliam a gerac\u0327a\u0303o de melhores compresso\u0303es. Apesar dos valores da TC do nosso sistema terem sido maiores que os valores da TC das baselines10, a TC do sistema GP+OPT ficou pro\u0301xima da TC das compresso\u0303es dos profissionais (TC = 59%).\nCom o intuito de melhor analisar a qualidade informativa e gramatical das compresso\u0303es, 5 franceses avaliaram as compresso\u0303es geradas por cada sistema e notificaram a qualidade gramatical e informativa para o corpus de teste (Tabela 3). Nosso sistema gerou estatisticamente compresso\u0303es mais informativas que as baselines. Apesar da me\u0301dia da gramaticalidade do nosso sistema ter sido inferior a dos demais sistemas, na\u0303o podemos confirmar qual sistema e\u0301 estatisticamente melhor para a gramaticalidade devido ao fato dos intervalos de confianc\u0327a da gramaticalidade dos sistemas se cruzarem. Portanto, nosso\n10A diferenc\u0327a do tamanho me\u0301dio das frases entre os sistemas GP+OPT e Filippova foi 3,7 palavras.\nsistema pode gerar compresso\u0303es com qualidade gramatical igual a\u0300s compresso\u0303es geradas pelos me\u0301todos de Filippova ou de BM.\nDesse modo, pode-se afirmar que o GP+OPT apresentou melhores resultados que as baselines gerando compresso\u0303es mais informativas e com uma boa qualidade gramatical."}, {"heading": "6. Considerac\u0327o\u0303es Finais e Proposta de Trabalhos Futuros", "text": "A CMF gera frases de boa qualidade sendo uma ferramenta interessante para a SAT. A ana\u0301lise concomitante da coesa\u0303o, palavras-chaves e 3-grams identificaram as informac\u0327o\u0303es principais do documento. Apesar do nosso sistema ter gerado compresso\u0303es com uma TC maior que as baselines, a informatividade foi consideravelmente melhor. A ana\u0301lise manual dos franceses comprovou que nosso me\u0301todo gerou compresso\u0303es mais informativas e mantendo uma boa qualidade gramatical.\nOs pro\u0301ximos trabalhos visam criar um corpus similar ao de BM para o idioma Portugue\u0302s e testar o desempenho do nosso sistema para diferentes idiomas. Ale\u0301m disso, pretende-se adaptar o sistema para escolher a releva\u0302ncia das palavras-chaves e dos 3- grams baseados no tamanho e no vocabula\u0301rio do documento. Finalmente, objetiva-se implementar diferentes me\u0301todos para a obtenc\u0327a\u0303o de palavras-chaves, a fim de avaliar o impacto de cada um na qualidade da gerac\u0327a\u0303o da CMF.\nAgradecimentos\nEste trabalho foi financiado parcialmente pelo projeto europeu CHISTERA-AMIS ANR15-CHR2-0001."}], "references": [{"title": "Sentence fusion for multidocument news summarization", "author": ["Barzilay", "K.R.R. e McKeown"], "venue": "Computational Linguistics, v. 31,", "citeRegEx": "Barzilay and McKeown,? \\Q2005\\E", "shortCiteRegEx": "Barzilay and McKeown", "year": 2005}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "Ng", "M.I.A.Y. e Jordan"], "venue": "Journal Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Keyphrase extraction for n-best reranking in multi-sentence compression", "author": ["Boudin", "E.F. e Morin"], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Boudin and Morin,? \\Q2013\\E", "shortCiteRegEx": "Boudin and Morin", "year": 2013}, {"title": "Multi-sentence compression: Finding shortest paths in word", "author": ["K. Filippova"], "venue": "COLING, p", "citeRegEx": "Filippova,? \\Q2010\\E", "shortCiteRegEx": "Filippova", "year": 2010}, {"title": "The traveling salesman problem: a guided tour of combinatorial optimization", "author": ["J.K. Lenstra", "A.R. Kan", "Lawler", "D.E.L. e Shmoys"], "venue": null, "citeRegEx": "Lenstra et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Lenstra et al\\.", "year": 1985}, {"title": "ROUGE: A package for automatic evaluation of summaries", "author": ["Lin", "C.-Y"], "venue": "ACL workshop on Text Summarization Branches Out,", "citeRegEx": "Lin and C..Y.,? \\Q2004\\E", "shortCiteRegEx": "Lin and C..Y.", "year": 2004}, {"title": "Sasi: sumarizador autom\u00e1tico de documentos baseado no problema do subconjunto independente de v\u00e9rtices", "author": ["E. Linhares Pontes", "Linhares", "A.C. e Torres-Moreno", "J.-M"], "venue": "Anais do Simpo\u0301sio Brasileiro de Pesquisa Operacional,", "citeRegEx": "Pontes et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pontes et al\\.", "year": 2014}, {"title": "Lia-rag: a system based on graphs and divergence of probabilities applied to speech-to-text summarization", "author": ["E. Linhares Pontes", "Linhares", "A.C. e Torres-Moreno", "J.-M"], "venue": "CCCS (Call Centre Conversation Summarization) Multiling 2015 Workshop,", "citeRegEx": "Pontes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pontes et al\\.", "year": 2015}, {"title": "TextRank: Bringing order into texts", "author": ["Mihalcea", "P.R. e Tarau"], "venue": "Proceedings of EMNLP04and the 2004 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mihalcea and Tarau,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea and Tarau", "year": 2004}, {"title": "A comparative analysis of several asymmetric traveling salesman problem formulations", "author": ["T. \u00d6ncan", "Alt\u0131nel", "G.\u0130.K. e Laporte"], "venue": "Computers & Operations Research, v. 36,", "citeRegEx": "\u00d6ncan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "\u00d6ncan et al\\.", "year": 2009}, {"title": "Teaching integer programming formulations using the traveling salesman problem", "author": ["G. Pataki"], "venue": "SIAM review, v. 45,", "citeRegEx": "Pataki,? \\Q2003\\E", "shortCiteRegEx": "Pataki", "year": 2003}, {"title": "Automatic Text Summarization", "author": ["Torres-Moreno", "J.-M"], "venue": "John Wiley & Sons. ISBN 9781-84821-668-6,", "citeRegEx": "Torres.Moreno and J..M.,? \\Q2014\\E", "shortCiteRegEx": "Torres.Moreno and J..M.", "year": 2014}], "referenceMentions": [{"referenceID": 3, "context": "A CMF \u00e9 um dos m\u00e9todos utilizados na SAT para gerar resumos, que utiliza um conjunto de frases para gerar uma \u00fanica frase de tamanho reduzido gramaticalmente correta e informativa (Filippova, 2010; Boudin e Morin, 2013).", "startOffset": 180, "endOffset": 219}, {"referenceID": 3, "context": "Neste artigo, apresentamos um m\u00e9todo baseado na Teoria dos Grafos e na Otimiza\u00e7\u00e3o Combinat\u00f3ria para modelar um documento como um Grafo de Palavras (GP) (Filippova, 2010) e gerar a CMF com uma melhor qualidade informativa.", "startOffset": 152, "endOffset": 169}, {"referenceID": 3, "context": "Outra abordagem poss\u0131\u0301vel \u00e9 descrita por Filippova (2010), que gerou compress\u00f5es de frases de boa qualidade utilizando uma simples modelagem baseada na Teoria dos Grafos e uma lista de stopwords1.", "startOffset": 41, "endOffset": 58}, {"referenceID": 3, "context": "Outra abordagem poss\u0131\u0301vel \u00e9 descrita por Filippova (2010), que gerou compress\u00f5es de frases de boa qualidade utilizando uma simples modelagem baseada na Teoria dos Grafos e uma lista de stopwords1. Boudin e Morin (2013) geraram a CMF mais informativas a partir da an\u00e1lise da relev\u00e2ncia das frases geradas pelo m\u00e9todo de Filippova.", "startOffset": 41, "endOffset": 219}, {"referenceID": 3, "context": "Visto que os trabalhos apresentados utilizaram uma modelagem simples e obtiveram resultados de boa qualidade, este trabalho baseia-se na mesma modelagem utilizada por Filippova e m\u00e9todos de otimiza\u00e7\u00e3o combinat\u00f3ria para aumentar a informatividade da CMF. As subse\u00e7\u00f5es 2.1 e 2.2 descrevem os m\u00e9todos utilizados por Filippova (2010) e Boudin e Morin (2013), respectivamente.", "startOffset": 167, "endOffset": 330}, {"referenceID": 3, "context": "Visto que os trabalhos apresentados utilizaram uma modelagem simples e obtiveram resultados de boa qualidade, este trabalho baseia-se na mesma modelagem utilizada por Filippova e m\u00e9todos de otimiza\u00e7\u00e3o combinat\u00f3ria para aumentar a informatividade da CMF. As subse\u00e7\u00f5es 2.1 e 2.2 descrevem os m\u00e9todos utilizados por Filippova (2010) e Boudin e Morin (2013), respectivamente.", "startOffset": 167, "endOffset": 354}, {"referenceID": 3, "context": "Grafo de palavras gerado a partir das frases a-d e um poss\u0131\u0301vel caminho representando a compress\u00e3o (Filippova, 2010).", "startOffset": 99, "endOffset": 116}, {"referenceID": 1, "context": "A Latent Dirichlet Allocation (LDA) \u00e9 um m\u00e9todo para analisar as frases de um texto e identificar o conjunto de palavras que representam as tem\u00e1ticas nele abordadas (Blei et al., 2003).", "startOffset": 165, "endOffset": 184}, {"referenceID": 8, "context": "Como discutido em Pataki (2003), existem duas formas cl\u00e1ssicas de evitar ciclos em problemas derivados do PCV.", "startOffset": 18, "endOffset": 32}, {"referenceID": 4, "context": "A primeira consiste na cria\u00e7\u00e3o de um conjunto exponencial de cortes garantindo que para todo subconjunto de v\u00e9rtices S \u2282 V , S 6= \u2205, haja exatamente |S| \u2212 1 arcos ativos (mais detalhes em Lenstra et al. (1985)).", "startOffset": 188, "endOffset": 210}, {"referenceID": 4, "context": "A primeira consiste na cria\u00e7\u00e3o de um conjunto exponencial de cortes garantindo que para todo subconjunto de v\u00e9rtices S \u2282 V , S 6= \u2205, haja exatamente |S| \u2212 1 arcos ativos (mais detalhes em Lenstra et al. (1985)). A segunda, conhecida como formula\u00e7\u00e3o Miller\u2013Tucker\u2013Zemlin (MTZ) utiliza um conjunto auxiliar de vari\u00e1veis, uma para cada v\u00e9rtice, de modo a evitar que um v\u00e9rtice seja visitado mais de uma vez no ciclo e um conjunto de arcos-restri\u00e7\u00f5es. Mais informa\u00e7\u00f5es sobre a formula\u00e7\u00e3o MTZ podem ser obtidas em \u00d6ncan et al. (2009).", "startOffset": 188, "endOffset": 529}, {"referenceID": 3, "context": "Sistemas ROUGE-1 ROUGE-2 TC Filippova (2010) 0,58769 0,43063 51,9% Boudin e Morin (2013) 0,62364 0,45467 55,8% GP+OPT PC=9 \u03b2=0.", "startOffset": 28, "endOffset": 45}, {"referenceID": 3, "context": "Sistemas ROUGE-1 ROUGE-2 TC Filippova (2010) 0,58769 0,43063 51,9% Boudin e Morin (2013) 0,62364 0,45467 55,8% GP+OPT PC=9 \u03b2=0.", "startOffset": 28, "endOffset": 89}, {"referenceID": 3, "context": "Sistemas ROUGE-1 ROUGE-2 TC Filippova (2010) 0,58455 0,43939 51,1% Boudin e Morin (2013) 0,62116 0,45734 55,2% GP+OPT PC=9 \u03b2=0.", "startOffset": 28, "endOffset": 45}, {"referenceID": 3, "context": "Sistemas ROUGE-1 ROUGE-2 TC Filippova (2010) 0,58455 0,43939 51,1% Boudin e Morin (2013) 0,62116 0,45734 55,2% GP+OPT PC=9 \u03b2=0.", "startOffset": 28, "endOffset": 89}, {"referenceID": 3, "context": "Sistemas Gramaticalidade Informatividade Filippova (2010) 4,2 \u00b1 0,18 2,86 \u00b1 0,32 Boudin e Morin (2013) 3,99 \u00b1 0,21 3,31 \u00b1 0,32 GP+OPT PC=9 \u03b2=0.", "startOffset": 41, "endOffset": 58}, {"referenceID": 3, "context": "Sistemas Gramaticalidade Informatividade Filippova (2010) 4,2 \u00b1 0,18 2,86 \u00b1 0,32 Boudin e Morin (2013) 3,99 \u00b1 0,21 3,31 \u00b1 0,32 GP+OPT PC=9 \u03b2=0.", "startOffset": 41, "endOffset": 103}], "year": 2017, "abstractText": "The Internet has led to a dramatic increase in the amount of available information. In this context, reading and understanding this flow of information have become costly tasks. In the last years, to assist people to understand textual data, various Natural Language Processing (NLP) applications based on Combinatorial Optimization have been devised. However, for MultiSentences Compression (MSC), method which reduces the sentence length without removing core information, the insertion of optimization methods requires further study to improve the performance of MSC. This article describes a method for MSC using Combinatorial Optimization and Graph Theory to generate more informative sentences while maintaining their grammaticality. An experiment led on a corpus of 40 clusters of sentences shows that our system has achieved a very good quality and is better than the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}