{"id": "1703.08068", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2017", "title": "Sequential Recurrent Neural Networks for Language Modeling", "abstract": "Feedforward Neural Network (FNN)-based language models estimate the probability of the next word based on the history of the last N words, whereas Recurrent Neural Networks (RNN) perform the same task based only on the last word and some context information that cycles in the network. This paper presents a novel approach, which bridges the gap between these two categories of networks. In particular, we propose an architecture which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network. The context integration is performed using an additional word-dependent weight matrix that is also learned during the training. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.", "histories": [["v1", "Thu, 23 Mar 2017 13:48:45 GMT  (168kb,D)", "http://arxiv.org/abs/1703.08068v1", "published (INTERSPEECH 2016), 5 pages, 3 figures, 4 tables"]], "COMMENTS": "published (INTERSPEECH 2016), 5 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["youssef oualil", "clayton greenberg", "mittul singh", "dietrich klakow"], "accepted": false, "id": "1703.08068"}, "pdf": {"name": "1703.08068.pdf", "metadata": {"source": "CRF", "title": "Sequential recurrent neural networks for language modeling", "authors": ["Youssef Oualil", "Clayton Greenberg", "Mittul Singh", "Dietrich Klakow"], "emails": ["firstname.lastname@lsv.uni-saarland.de"], "sections": [{"heading": "1. Introduction", "text": "A high quality Language Model (LM) is considered to be an integral component of many systems for language technology applications, such as speech recognition [1], machine translation [2], etc. The goal of an LM is to identify probable sequences of predefined linguistic units, which are typically words. Semantic and syntactic properties of the language, encoded by the LM, guide these predictions.\nIntrinsically, the performance of an LM can be evaluated based upon its ability to predict the next word given its context. The most common approach to build such models is the word count-based method, which is commonly known as N - gram language modeling [3, 4]. By simply enumerating all possibilities over a short span of words and assigning probabilities to them directly, N -grams were difficult to outperform for a very long time.\nThe introduction of neural networks for language modeling led to a significant improvement over these standard models. This was mainly due to the continuous word representations they provide, which typically overcome the exponential growth of parameters that N -gram models require to enumerate possibilities. Bengio et al. [5] proposed a Feedforward Neural Network (FNN) for language modeling, as an alternative to\nThis research was funded by the German Research Foundation (DFG) as part of SFB 1102.\nN -grams, to estimate the probability of a given word sequence while considering a fixed context (word history) size. This approach was very successful and has been shown to outperform a mixture of different other models [6], and to significantly improve speech recognition performance [7].\nIn order to overcome the fixed context size constraint and to capture long range dependencies known to be present in language, Mikolov et al. [8, 9] proposed a Recurrent Neural Network (RNN) which allows context information to cycle in the network. Another recurrence-based network architecture, Long-Short Term Memory (LSTM) [10], addresses some learning issues from the original RNN and explicitly controls the longevity of context information in the network.\nContrary to FNN, recurrent models such as RNN and LSTM predict the next word based only on the current word and the context representation. Therefore, they lose information about word position rather quickly and cannot model short range dependencies as well as FNN and N -grams. For example, English has position-dependent patterns such as \u201che \u2217 he\u201d (\u201che said he\u201d, \u201che mentioned he\u201d, . . . ). The position of \u201che\u201d is essential for making the right prediction in this case, and the recurrent models are not designed to encode that. Rather, they are better for smooth incremental updates and hence for longer range dependencies.\nThis paper proposes a novel approach that models short range dependencies like FNN and long range dependencies like RNN. In particular, the hidden layers combine explicit encoding of the local context and a recurrent architecture, which allows the context information to sequentially evolve in the network at the projection layer. In the first step, the word representation are enhanced using the context information. This step maps the word representations from a universal embedding space into a context-based space. Then, the system performs the next word prediction as it is typically done in FNN. The learning of the network weights uses the Back-Propagation Through Time (BPTT) algorithm similarly to RNN. The main difference here is the additional network error resulting from the additional sequential connections. This paper also shows that learning of word-dependent sequential connections can substantially improve the performance of the proposed network.\nWe proceed as follows. Section 2 presents a brief overview of FNN and RNN models. Section 3 introduces the proposed architecture which combines these two models. Then, Section 4 evaluates the proposed network in comparison to different stateof-the-art language models for perplexity on the PTB and the LTCB corpus. Finally, we conclude in Section 5.\nar X\niv :1\n70 3.\n08 06\n8v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\n01 7"}, {"heading": "2. Neural Network Language Models", "text": "The goal of a language model is to estimate the probability distribution p(wT1 ) of word sequences wT1 = w1, \u00b7 \u00b7 \u00b7 , wT . Using the chain rule, this distribution can be expressed as\np(wT1 ) = T\u220f t=1 p(wt|wt\u221211 ) (1)\nThe rest of this section shows how FNN and RNN are used to approximate this probability distribution."}, {"heading": "2.1. Feedforward Neural Networks", "text": "Similarly to N -gram models, FNN uses the Markov assumption of order N-1 to approximate (1) according to\np(wT1 ) \u2248 T\u220f\nt=1\np(wt|wt\u22121t\u2212N+1) (2)\nSubsequently, each of the terms involved in this product, i.e, p(wt|wt\u22121t\u2212N+1), is estimated, separately, in a single bottom-up evaluation of the network according to\nPt\u2212i = Xt\u2212i \u00b7 U , i = N \u2212 1, \u00b7 \u00b7 \u00b7 , 1 (3)\nHt = f ( N\u22121\u2211 i=1 Pt\u2212i \u00b7 Vi ) (4)\nOt = g (Ht \u00b7W ) (5) Xt\u2212i is a one-hot encoding of the word wt\u2212i, whereas the rows of U encode the continuous word representations (i.e, embeddings). Thus, Pt\u2212i is the continuous representation of the word wt\u2212i. W and V = [V1, \u00b7 \u00b7 \u00b7 , VN\u22121] are the network connection weights, which are learned during training in addition to U . Moreover, f(\u00b7) is an activation function, whereas g(\u00b7) is the softmax function. Figure (1a) shows an example of an FNN with a fixed context size N \u2212 1 = 3 with a single hidden layer."}, {"heading": "2.2. Recurrent Neural Networks", "text": "An RNN attempts to capture the complete history in a context vector ht, which represents the state of the network and evolves in time. Therefore, it approximates (1) according to\np(wT1 ) \u2248 T\u220f\nt=1\np(wt|wt\u22121, ht\u22121) = T\u220f\nt=1\np(wt|ht) (6)\nRNN evaluates this distribution similarly to FNN. The main difference occurs in Equations (3) and (4) which are combined into\nHt = f (Xt\u22121 \u00b7 U +Ht\u22121 \u00b7 V ) (7)\nFigure (1b) shows an example of a standard RNN. The next Section will show how an RNN can be extended to explicitly model short range dependencies through additional sequential connections."}, {"heading": "3. Sequential Recurrent Neural Network", "text": "The main difference between an RNN and an FNN is the context representation. More precisely, The context layer Ht of an FNN is estimated based on a fixed context size i.e, the last N \u2212 1 words, whereas in an RNN, Ht is constantly updated (at each time iteration) using only the last word and context at time t\u22121."}, {"heading": "3.1. The proposed Neural Architecture", "text": "We propose in this paper an architecture which captures short range dependencies over the last N \u2212 1 word positions as it is done in FNN, and the long range context through recurrence, similarly to RNN. The design of this structure is motivated by the inefficiency of RNN to model position dependent patterns, which are particularly frequent in conversational speech. RNN loses information about word position quickly and therefore cannot efficiently model short range dependencies. FNN and N-gram models, however, are designed as position-dependent models, which deal only with short-term context. Extending RNN structure to explicitly represent the short term history as it is done in FNN will 1) help improve the modeling of short range context, as it will 2) allow the network to capture any residual/additional context information that may be present in the past i = t\u2212N+1, \u00b7 \u00b7 \u00b7 , t\u22122 time iterations but which may have been lost during the last context update, which is based only on the last word at t \u2212 1 (See illustration in Figure 2). In the worst case scenario, the context information will be simply redundant and is expected not to harm the performance. The rest of this Section introduces the mathematical formulation of this approach.\nThe proposed Sequential Recurrent Neural Network (SRNN) approximates (1) according to\np(wT1 )\u2248 T\u220f\nt=1\np(wt|wt\u22121t\u2212N+1, ht\u2212N+1)= T\u220f\nt=1\np(wt|htt\u2212N+2) (8)\nThe proposed architecture to estimate (8) explicitly represents the history over the last N \u2212 1 word positions as it is done in FNN to approximate (2) while it enhances the actual word representations using the recurrent context information, which propagates sequentially within the network. Furthermore, restricting the context to a 1-word history window (N=2) in (8) leads to the RNN approximation in (6). Therefore, the proposed approach can be seen as an extension of the standard RNN to explicitly model and capture short range context.\nThe additional sequential connections allow the context information to propagate from the past to the future within the network. These connections can be defined as a Word-Independent (WI) recurrence vector, which fixes the amount of context information allowed to propagate in the network, as they can be designed as Word-Dependent (WD) vectors. In this case, each word will have its own context weight vector, which will typically learn which context \u201cneurons\u201d are relevant for that particular word and therefore scales each context unit accordingly.\nThe network evaluation is performed similarly to FNN, the main difference occurs in Equation (3), which becomes in the case of the word-indepdent model\nPt\u2212i = fs(Xt\u2212i \u00b7U+C Pt\u2212i\u22121), i = N\u22121, \u00b7 \u00b7 \u00b7 , 1 (9)\nas it becomes in the case of the word-dependent model\nPt\u2212i=fs(Xt\u2212i \u00b7U+Cwt\u2212i Pt\u2212i\u22121), i = N\u22121, \u00b7 \u00b7 \u00b7 , 1(10)\nwhere fs(\u00b7) is an activation function and is the elementwise product operator. C is the word-independent recurrence weight vector, whereas Cwt\u2212i is the word-dependent context weight corresponding to the word wt\u2212i. Figure (3) shows an example of an SRNN with three additional sequential connections (N \u2212 1 = 3) and a single hidden layer.\nThe proposed SRNN model is a general architecture that includes different networks. In particular, setting C = [0, \u00b7 \u00b7 \u00b7 , 0] and fs(x)=x results in the classical FNN architecture, whereas setting N = 2 leads to a standard RNN with a diagonal recurrence matrix and an additional non-recurrent layer. Moreover, setting C to a fixed value in [0, 1] and fs(x) = x leads to the Fixed-size Ordinally-Forgetting Encoding (FOFE) [11] architecture, which was proposed to uniquely encode word sequences.\nThe proposed model replaces the universal word embeddings at the projection layer of an FNN by context-dependent word embeddings. More particularly, both Equations (9) and (10) show that each word representation is enhanced using the context information before proceeding to the next word prediction. Therefore, we can see this particular step as a transformation from the universal embedding space into a contextdependent space with a better discrimination of words."}, {"heading": "3.2. SRNN Training", "text": "The parameters to train for an SRNN are the word embeddings U , the project-to-hidden connection weights V = [V1, \u00b7 \u00b7 \u00b7 , VN\u22121], the hidden-to-output connection weights W and the context weight vector C for the WI model, or C = [C\u1d401 , \u00b7 \u00b7 \u00b7 , C \u1d40 K ] \u1d40 (K is the vocabulary size) for the WD model.\nIn this case, each word w in the vocabulary will be characterized by two learnable vectors, namely, the continuous representation (embedding) Uw and the context weight Cw.\nSimilarly to RNN, the parameter learning of an SRNN architecture follows the standard Back-Propagation Through Time (BPTT) algorithm. The main difference occurs at the projection layer, where the additional error vectors resulting from the sequential connections should be taken into account (See example or error propagation in Figure 3) before unfolding the network in time."}, {"heading": "4. Experiments and Results", "text": ""}, {"heading": "4.1. Experimental Setup", "text": "We evaluated the proposed architecture on two different benchmark tasks. The first set of experiments was conducted on the Penn Treebank (PTB) corpus using the standard division, e.g. [9, 11]: sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing. The vocabulary was limited to the most 10k frequent words while the remaining words were all mapped to the token <unk>. In order to evaluate how the proposed approach scales to large corpora, we run a set of experiments on the Large Text Compression Benchmark (LTCB) [12]. This corpus is based on the enwik9 dataset which contains the first 109 bytes of enwiki20060303-pages-articles.xml. We adopted the same trainingtest-validation data split and preprocessing from [11]. All but the 80k most frequent words were replaced by <unk>. Details about the sizes of these two corpora and the percentage of OutOf-Vocabulary (OOV) words that were mapped to <unk> can be found in Table 1.\nThe proposed approach (SRNN) is compared to different systems including the N -gram Kneser-Ney (KN) model and different feedforward and recurrent neural architectures. For feedforward networks, the baseline systems include 1) the FNN-based LM [5] as well as the 2) Fixed-size Ordinally Forgetting Encoding (FOFE) approach, which was implemented as a feedforward sentence-based model [11]. The FOFE results were obtained using the FOFE toolkit [11]. The results are reported for different context sizes (N-1=1,2 and 4) and different numbers of hidden layers (1 or 2). Regarding recurrent models, we compare the proposed approach to 3) the full RNN (without classes) [9], 4) to a deep RNN [13], which investigates different ways of adding hidden layers to RNN, and finally 5) to the LSTM architecture [10], which explicitly regulates the amount of information that propagates in the network."}, {"heading": "4.2. PTB Experiments", "text": "For the PTB experiments, the FNN, FOFE and SRNN architectures have similar configurations. That is, the hidden layer(s) size is 400 with all hidden units using the Rectified Linear Unit (ReLu) i.e., f(x) = max(0, x), as an activation function, whereas the word representation (embedding) size was set to 200 for FNN, FOFE and LSTM and 100 for SRNN. The latter uses fs = tanh(\u00b7) as sequential activation function. The hidden layer size of RNN and LSTM were set to 400 and follow the original configuration proposed in [9] and [10], respectively.\nWe also use the same learning setup adopted in [11]. Namely, we use the stochastic gradient descent algorithm with a minibatch size of 200, the learning rate is initialized to 0.4, the momentum is set to 0.9, the weight decay is fixed to 4.10\u22125 and the training is done in epochs. The weights initialization follows the normalized initialization proposed in [14]. Similarly to [8], the learning rate is halved when no significant improvement in the log-likelihood of the validation data is observed. Then, we continue with seven more epochs while halving the learning rate after each epoch. The BPTT was set to 5 time steps. In the tables below, WI-SRNN refers to the word-independent SRNN model proposed in (9), whereas WD-SRNN refers to the worddependent model in (10). For both models, the context connection weights, C, were randomly initialized in [0, 1]. In order to compare to the FOFE approach, we also report results where C is reduced to a scalar forgetting factor that is fixed at 0.7. This is denoted as WI-SRNN\u2217 in the tables below. We report the results in terms of perplexity (PPL), Number of model Parameters (NoP) and the training speed, which is defined as the number of words processed per second (w/s) on a GTX TITAN X GPU.\nTable 2 shows the LMs evaluation on the PTB test set. We can clearly see that the proposed approach outperforms all other models using the lowest Number of model Parameters (NoP) among all configurations. This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18]. More particularly, SRNN with two hidden layers achieves a comparable performance to a mixture of RNNs [19]. We can also conclude that the explicit modeling of short range dependencies through sequential connections improves the performance. More precisely, the results show that increasing the history window (1, 2 and 4) improves the performance for all SRNN models. Table 2 also shows that using a fixed scalar forgetting factor (WI-SRNN\u2217) leads to a slight improvement over the FOFE approach, which is mainly due to the additional non-linear activation function fs. Furthermore, the word-dependent (WD-SRNN) model slightly outperforms the word-independent model (WI-SRNN) but with a non-negligible increase in the number of parameters. Regarding the training speed, we can conclude that training an SRNN model requires approximately twice the time needed for FFN and RNN, whereas it needs less time compared to LSTM."}, {"heading": "4.3. LTCB Experiments", "text": "The LTCB experiments use the same PTB setup with minor changes. The results shown in Table 3 follow the same experimental setup used in [11]. More precisely, these results were obtained without usage of momentum or weight decay whereas the mini-batch size was set to 400. The FNN and FOFE architectures contain 2 hidden layers of size 600 (or 400) whereas RNN and SRNN have a single hidden layer of size 600. In order to compare to [11], the forgetting factor C of WI-SRNN\u2217 is fixed at 0.6.\nThe LTCB results shown in Table 3 generally confirm the PTB conclusions. In particular, we can see that SRNN models outperform all other models while requiring comparable or fewer model parameters. Moreover, the WI-SRNN\u2217 model with a single hidden layer slightly outperforms FOFE (2 hidden layers). These results, however, show a more significant improvement for the WD-SRNN model and for the increased window size (from 1 to 4) compared to the improvement obtained on the PTB. This is mainly due to the large amount of LTCB training data, which allows us to train richer WD context vectors.\nTable 4 shows some word examples with their top 5 cosine similarities for word embeddings Uw and Euclidean distance for context weights Cw. These examples show a general trend, not valid for every example, that the embeddings capture semantic (conceptual) similarities and the context weights model syntactic (functional) similarities."}, {"heading": "5. Conclusion and Future Work", "text": "We have presented a sequential recurrent neural network which captures short range dependencies using short history windows, and models long range context through recurrent connections. Experiments on PTB and LTCB corpora have shown that this architecture substantially outperforms many state-of-the-art neural systems, due to its successful combination of the motivating features of its feedforward and recurrent predecessors. Further gains could be made by more optimally controlling the amount of information evolving in the network, as it is done in LSTM, and by more thoroughly addressing long range dependencies. These will be investigated in future work."}, {"heading": "6. References", "text": "[1] S. Katz, \u201cEstimation of probabilities from sparse data for the lan-\nguage model component of a speech recognizer,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 3, pp. 400\u2013401, Mar. 1987.\n[2] P. F. Brown, J. Cocke, S. A. D. Pietra, V. J. D. Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin, \u201cA statistical approach to machine translation,\u201d Comput. Linguist., vol. 16, no. 2, pp. 79\u201385, Jun. 1990.\n[3] R. Rosenfeld, \u201cTwo decades of statistical language modeling: Where do we go from here?\u201d in Proceedings of the IEEE, vol. 88, 2000, pp. 1270\u20131278.\n[4] R. Kneser and H. Ney, \u201cImproved backing-off for m-gram language modeling,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Detroit, Michigan, USA, May 1995, pp. 181\u2013184.\n[5] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, \u201cA neural probabilistic language model,\u201d J. Mach. Learn. Res., vol. 3, pp. 1137\u20131155, Mar. 2003.\n[6] J. Goodman, \u201cA bit of progress in language modeling, extended version,\u201d Microsoft Research, Tech. Rep. MSR-TR-200172, 2001.\n[7] H. Schwenk and J. Gauvain, \u201cTraining neural network language models on very large corpora,\u201d in Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP), Oct. 2005, pp. 201\u2013208.\n[8] T. Mikolov, M. Karafia\u0301t, L. Burget, J. Cernocky\u0301, and S. Khudanpur, \u201cRecurrent neural network based language model,\u201d in 11th Annual Conference of the International Speech Communication Association (INTERSPEECH), Makuhari, Chiba, Japan, Sep. 2010, pp. 1045\u20131048.\n[9] T. Mikolov, S. Kombrink, L. Burget, J. ernock, and S. Khudanpur, \u201cExtensions of recurrent neural network language model,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), May 2011, pp. 5528\u20135531.\n[10] M. Sundermeyer, R. Schlu\u0308ter, and H. Ney, \u201cLSTM neural networks for language modeling,\u201d in 13th Annual Conference of the International Speech Communication Association (INTERSPEECH), Portland, OR, USA, Sep. 2012, pp. 194\u2013197.\n[11] S. Zhang, H. Jiang, M. Xu, J. Hou, and L. Dai, \u201cThe fixedsize ordinally-forgetting encoding method for neural network language models,\u201d in 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL, vol. 2, July 2015, pp. 495\u2013 500.\n[12] M. Mahoney, \u201cLarge text compression benchmark,\u201d 2011. [Online]. Available: http://mattmahoney.net/dc/textdata.html\n[13] R. Pascanu, C\u0327. Gu\u0308lc\u0327ehre, K. Cho, and Y. Bengio, \u201cHow to construct deep recurrent neural networks,\u201d CoRR, vol. abs/1312.6026, 2013.\n[14] X. Glorot and Y. Bengio, \u201cUnderstanding the difficulty of training deep feedforward neural networks,\u201d in Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), Chia Laguna Resort, Sardinia, Italy, May 2010, pp. 249\u2013256.\n[15] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Cernocky\u0301, \u201cStrategies for training large scale neural network language models,\u201d in IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), Waikoloa, HI, USA, Dec. 11-15, 2011, pp. 196\u2013201.\n[16] P. Xu and F. Jelinek, \u201cRandom forests and the data sparseness problem in language modeling,\u201d Computer Speech & Language, vol. 21, no. 1, pp. 105\u2013152, 2007.\n[17] D. Filimonov and M. P. Harper, \u201cA joint language model with fine-grain syntactic tags,\u201d in Conference on Empirical Methods in Natural Language Processing (EMNLP), A meeting of SIGDAT, a Special Interest Group of the ACL, Singapore, Aug. 2009, pp. 1114\u20131123.\n[18] A. Emami and F. Jelinek, \u201cExact training of a neural syntactic language model,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Montreal, Quebec, Canada, May 2004, pp. 245\u2013248.\n[19] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, and J. Cernocky\u0301, \u201cEmpirical evaluation and combination of advanced language modeling techniques,\u201d in 12th Annual Conference of the International Speech Communication Association (INTERSPEECH), Florence, Italy, Aug. 27-31, 2011, pp. 605\u2013608."}], "references": [{"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 3, pp. 400\u2013401, Mar. 1987.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "A statistical approach to machine translation", "author": ["P.F. Brown", "J. Cocke", "S.A.D. Pietra", "V.J.D. Pietra", "F. Jelinek", "J.D. Lafferty", "R.L. Mercer", "P.S. Roossin"], "venue": "Comput. Linguist., vol. 16, no. 2, pp. 79\u201385, Jun. 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Two decades of statistical language modeling: Where do we go from here?", "author": ["R. Rosenfeld"], "venue": "Proceedings of the IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Detroit, Michigan, USA, May 1995, pp. 181\u2013184.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1137\u20131155, Mar. 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "A bit of progress in language modeling, extended version", "author": ["J. Goodman"], "venue": "Microsoft Research, Tech. Rep. MSR-TR-2001- 72, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Training neural network language models on very large corpora", "author": ["H. Schwenk", "J. Gauvain"], "venue": "Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (EMNLP), Oct. 2005, pp. 201\u2013208.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u00fd", "S. Khudanpur"], "venue": "11th Annual Conference of the International Speech Communication Association (INTERSPEECH), Makuhari, Chiba, Japan, Sep. 2010, pp. 1045\u20131048.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. ernock", "S. Khudanpur"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), May 2011, pp. 5528\u20135531.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "LSTM neural networks for language modeling", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney"], "venue": "13th Annual Conference of the International Speech Communication Association (INTER- SPEECH), Portland, OR, USA, Sep. 2012, pp. 194\u2013197.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "The fixedsize ordinally-forgetting encoding method for neural network language models", "author": ["S. Zhang", "H. Jiang", "M. Xu", "J. Hou", "L. Dai"], "venue": "53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing ACL, vol. 2, July 2015, pp. 495\u2013 500.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Large text compression benchmark", "author": ["M. Mahoney"], "venue": "2011. [Online]. Available: http://mattmahoney.net/dc/textdata.html", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "How to construct deep recurrent neural networks", "author": ["R. Pascanu", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, vol. abs/1312.6026, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS), Chia Laguna Resort, Sardinia, Italy, May 2010, pp. 249\u2013256.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Strategies for training large scale neural network language models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. Cernock\u00fd"], "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), Waikoloa, HI, USA, Dec. 11-15, 2011, pp. 196\u2013201.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Random forests and the data sparseness problem in language modeling", "author": ["P. Xu", "F. Jelinek"], "venue": "Computer Speech & Language, vol. 21, no. 1, pp. 105\u2013152, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A joint language model with fine-grain syntactic tags", "author": ["D. Filimonov", "M.P. Harper"], "venue": "Conference on Empirical Methods in Natural Language Processing (EMNLP), A meeting of SIGDAT, a Special Interest Group of the ACL, Singapore, Aug. 2009, pp. 1114\u20131123.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Exact training of a neural syntactic language model", "author": ["A. Emami", "F. Jelinek"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Montreal, Quebec, Canada, May 2004, pp. 245\u2013248.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernock\u00fd"], "venue": "12th Annual Conference of the International Speech Communication Association (INTERSPEECH), Florence, Italy, Aug. 27-31, 2011, pp. 605\u2013608.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "A high quality Language Model (LM) is considered to be an integral component of many systems for language technology applications, such as speech recognition [1], machine translation [2], etc.", "startOffset": 158, "endOffset": 161}, {"referenceID": 1, "context": "A high quality Language Model (LM) is considered to be an integral component of many systems for language technology applications, such as speech recognition [1], machine translation [2], etc.", "startOffset": 183, "endOffset": 186}, {"referenceID": 2, "context": "The most common approach to build such models is the word count-based method, which is commonly known as N gram language modeling [3, 4].", "startOffset": 130, "endOffset": 136}, {"referenceID": 3, "context": "The most common approach to build such models is the word count-based method, which is commonly known as N gram language modeling [3, 4].", "startOffset": 130, "endOffset": 136}, {"referenceID": 4, "context": "[5] proposed a Feedforward Neural Network (FNN) for language modeling, as an alternative to", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "This approach was very successful and has been shown to outperform a mixture of different other models [6], and to significantly improve speech recognition performance [7].", "startOffset": 103, "endOffset": 106}, {"referenceID": 6, "context": "This approach was very successful and has been shown to outperform a mixture of different other models [6], and to significantly improve speech recognition performance [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 7, "context": "[8, 9] proposed a Recurrent Neural Network (RNN) which allows context information to cycle in the network.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[8, 9] proposed a Recurrent Neural Network (RNN) which allows context information to cycle in the network.", "startOffset": 0, "endOffset": 6}, {"referenceID": 9, "context": "Another recurrence-based network architecture, Long-Short Term Memory (LSTM) [10], addresses some learning issues from the original RNN and explicitly controls the longevity of context information in the network.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "Moreover, setting C to a fixed value in [0, 1] and fs(x) = x leads to the Fixed-size Ordinally-Forgetting Encoding (FOFE) [11] architecture, which was proposed to uniquely encode word sequences.", "startOffset": 40, "endOffset": 46}, {"referenceID": 10, "context": "Moreover, setting C to a fixed value in [0, 1] and fs(x) = x leads to the Fixed-size Ordinally-Forgetting Encoding (FOFE) [11] architecture, which was proposed to uniquely encode word sequences.", "startOffset": 122, "endOffset": 126}, {"referenceID": 8, "context": "[9, 11]: sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing.", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[9, 11]: sections 0-20 are used for training while sections 21-22 and 23-24 are used for validation and testing.", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "In order to evaluate how the proposed approach scales to large corpora, we run a set of experiments on the Large Text Compression Benchmark (LTCB) [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "We adopted the same trainingtest-validation data split and preprocessing from [11].", "startOffset": 78, "endOffset": 82}, {"referenceID": 4, "context": "For feedforward networks, the baseline systems include 1) the FNN-based LM [5] as well as the 2) Fixed-size Ordinally Forgetting Encoding (FOFE) approach, which was implemented as a feedforward sentence-based model [11].", "startOffset": 75, "endOffset": 78}, {"referenceID": 10, "context": "For feedforward networks, the baseline systems include 1) the FNN-based LM [5] as well as the 2) Fixed-size Ordinally Forgetting Encoding (FOFE) approach, which was implemented as a feedforward sentence-based model [11].", "startOffset": 215, "endOffset": 219}, {"referenceID": 10, "context": "The FOFE results were obtained using the FOFE toolkit [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "Regarding recurrent models, we compare the proposed approach to 3) the full RNN (without classes) [9], 4) to a deep RNN [13], which investigates different ways of adding hidden layers to RNN, and finally 5) to the LSTM architecture [10], which explicitly regulates the amount of information that propagates in the network.", "startOffset": 98, "endOffset": 101}, {"referenceID": 12, "context": "Regarding recurrent models, we compare the proposed approach to 3) the full RNN (without classes) [9], 4) to a deep RNN [13], which investigates different ways of adding hidden layers to RNN, and finally 5) to the LSTM architecture [10], which explicitly regulates the amount of information that propagates in the network.", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Regarding recurrent models, we compare the proposed approach to 3) the full RNN (without classes) [9], 4) to a deep RNN [13], which investigates different ways of adding hidden layers to RNN, and finally 5) to the LSTM architecture [10], which explicitly regulates the amount of information that propagates in the network.", "startOffset": 232, "endOffset": 236}, {"referenceID": 8, "context": "The hidden layer size of RNN and LSTM were set to 400 and follow the original configuration proposed in [9] and [10], respectively.", "startOffset": 104, "endOffset": 107}, {"referenceID": 9, "context": "The hidden layer size of RNN and LSTM were set to 400 and follow the original configuration proposed in [9] and [10], respectively.", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "We also use the same learning setup adopted in [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "The weights initialization follows the normalized initialization proposed in [14].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "Similarly to [8], the learning rate is halved when no significant improvement in the log-likelihood of the validation data is observed.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "For both models, the context connection weights, C, were randomly initialized in [0, 1].", "startOffset": 81, "endOffset": 87}, {"referenceID": 14, "context": "This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "This also includes other models that were reported in the literature, such as RNN with maximum entropy [15], random forest LM [16], structured LM [17] and syntactic neural network LM [18].", "startOffset": 183, "endOffset": 187}, {"referenceID": 18, "context": "More particularly, SRNN with two hidden layers achieves a comparable performance to a mixture of RNNs [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "The results shown in Table 3 follow the same experimental setup used in [11].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "In order to compare to [11], the forgetting factor C of WI-SRNN\u2217 is fixed at 0.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "Feedforward Neural Network (FNN)-based language models estimate the probability of the next word based on the history of the last N words, whereas Recurrent Neural Networks (RNN) perform the same task based only on the last word and some context information that cycles in the network. This paper presents a novel approach, which bridges the gap between these two categories of networks. In particular, we propose an architecture which takes advantage of the explicit, sequential enumeration of the word history in FNN structure while enhancing each word representation at the projection layer through recurrent context information that evolves in the network. The context integration is performed using an additional word-dependent weight matrix that is also learned during the training. Extensive experiments conducted on the Penn Treebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a significant reduction of the perplexity when compared to state-of-the-art feedforward as well as recurrent neural network architectures.", "creator": "LaTeX with hyperref package"}}}