{"id": "1703.08524", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2017", "title": "Joint Modeling of Event Sequence and Time Series with Attentional Twin Recurrent Neural Networks", "abstract": "A variety of real-world processes (over networks) produce sequences of data whose complex temporal dynamics need to be studied. More especially, the event timestamps can carry important information about the underlying network dynamics, which otherwise are not available from the time-series evenly sampled from continuous signals. Moreover, in most complex processes, event sequences and evenly-sampled times series data can interact with each other, which renders joint modeling of those two sources of data necessary. To tackle the above problems, in this paper, we utilize the rich framework of (temporal) point processes to model event data and timely update its intensity function by the synergic twin Recurrent Neural Networks (RNNs). In the proposed architecture, the intensity function is synergistically modulated by one RNN with asynchronous events as input and another RNN with time series as input. Furthermore, to enhance the interpretability of the model, the attention mechanism for the neural point process is introduced. The whole model with event type and timestamp prediction output layers can be trained end-to-end and allows a black-box treatment for modeling the intensity. We substantiate the superiority of our model in synthetic data and three real-world benchmark datasets.", "histories": [["v1", "Fri, 24 Mar 2017 17:29:14 GMT  (2457kb,D)", "http://arxiv.org/abs/1703.08524v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuai xiao", "junchi yan", "mehrdad farajtabar", "le song", "xiaokang yang", "hongyuan zha"], "accepted": false, "id": "1703.08524"}, "pdf": {"name": "1703.08524.pdf", "metadata": {"source": "CRF", "title": "Joint Modeling of Event Sequence and Time Series with Attentional Twin Recurrent Neural Networks", "authors": ["Shuai Xiao", "Junchi Yan", "Mehrdad Farajtabar", "Le Song", "Xiaokang Yang", "Hongyuan Zha"], "emails": ["benjaminforever@sjtu.edu.cn,", "xkyang@sjtu.edu.cn", "yanesta13@163.com"], "sections": [{"heading": null, "text": "Index Terms\u2014Recurrent Neural Networks, Temporal Point Process, Relational Mining, Interpretable Attention Models.\nF"}, {"heading": "1 INTRODUCTION", "text": "E VENT sequences are becoming increasingly available ina variety of applications. Such event sequences, which are asynchronously generated with random timestamps, are ubiquitous in areas such as e-commerce, social networks, electronic health data, and equipment failures. The event data can carry rich information not only about the event attribute (e.g., type, participator) but also the timestamp {zi, ti}Ni=1 indicating when the event takes place. A major line of research [1] has been devoted to studying event sequence, especially exploring the timestamp information to model the underlying dynamics of the system, whereby point process has been a powerful and elegant framework in this direction.\nBeing treated as a random variable when the event is stochastically generated in an asynchronous manner, the timestamp makes the event sequence of point processes fundamentally different from the time series [2] evenlysampled from continuous signals because the asynchronous timestamps reflect the network dynamic while the time for time-series is deterministic..\n\u2022 S. Xiao and X. Yang are with the Department of Electrical Engineering, Shanghai Jiao Tong University, China. E-mail: benjaminforever@sjtu.edu.cn, xkyang@sjtu.edu.cn \u2022 J. Yan (correspondence author) is with the Department of Electrical Engineering, Shanghai Jiao Tong University, China. He is also secondarily affiliated with IBM Research - China. E-mail: yanesta13@163.com \u2022 M. Farajtabar, H. Zha and L. Song are with School of Computational Science and Engineering, College of Computing, Georgia Institute of Technology, Atlanta, Georgia, 30332, USA. E-mail: mehrdad@gatech.edu,{zha,lsong}@cc.gatech.edu\nHowever these time series data, when available, provide timely updates of background environment where events occur in the temporal point process, such as temperature for computing servers or blood pressure for patients. Many complex systems posses such time series data regularly recorded along with the point processes data.\nWhile there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time. To better understand the dynamics of point processes, there is an urgent need for joint models of the two processes, which are largely inexistent to date. There are related efforts in linking the time series and event sequence to each other. In fact, one popular way to convert a time series to an event sequence is by detecting multiple events (e.g., based on thresholding the stock price series [11]) from the series data. On the other hand, statistical aggregation (e.g., total number of counts) is often performed on each time interval with equal length to extract aligned time series data from the event sequences. However such a coarse treatment can lead to the key information loss about the actual behavior of the process, or at least in a too early stage.\nRecent progresses on modeling point process includes mathematical reformulations and optimization techniques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowledge to capture the characters of the dataset in their study. One major limitation of those model is that the specified form of point process limits its capability to capture the dynamic of data. Moreover, it may suffer from misspecification, for which the model is not suitable for the data.\nar X\niv :1\n70 3.\n08 52\n4v 1\n[ cs\n.L G\n] 2\n4 M\nar 2\n01 7\n2\n\u00a9 2013 IBM Research-China\nRecent works e.g., [13] start to turn to non-parametric form to fit the structure of a point process, but their method is under the Hawkes process formulation, which runs the risk of unknown model complexity and can be inappropriate for point processes that disobey the self or mutual-exciting rule assumed by the Hawkes model [19]. In another recent work [6], the authors proposed a semi-parametric pesudo point process model, which assumes a time-decaying influence between events and the background of intensity is constant. Besides, the event type is regarded as the mark associated with a univariate process.\nIn this paper, we view the conditional intensity of a point process as a nonlinear mapping from the joint embedding of time series and past event data to the predicted transient occurrence intensity of future events with different types. Such a nonlinear mapping is expected to be complex and flexible enough to model various characters of real event data for its application utility, e.g., failure prediction, social network analysis, and disease network mining as will be empirically studied in the experiment part of this paper. To overcome the disadvantages associated with the explicit parametric form of intensity function we bypass direct modeling of the intensity function and directly model the next event time and dimension. Neural networks are our choice to model this nonparametric mapping.\nWe utilize the state-of-the-art deep learning techniques to efficiently and flexibly model the intensity function of the point processes. Instead of predefining the form of point process, we turn to the synergic multi-RNNs (specifically twin-RNNs in this paper) as a natural way to encode such nonlinear and dynamic mapping, in an effort for modeling an end-to-end nonlinear intensity mapping without any prior knowledge. To further improve its interpretability, we infuse the attention model to improve its capability for both prediction and relational mining among event dimensions.\nKey idea and highlights. Our model interprets the conditional intensity function of a point process as a nonlinear mapping, which is synergetically established by a composite neural network with two RNNs as its building blocks. As illustrated in Fig. 1, time series (top row) and event sequence (bottom row) are distinct to each other. The underlying\nrationale is that time series is more suitable to carry the synchronously and regularly updated (i.e. in a fixed pace) or constant profile features. In contrast, the event sequence can compactly catch event driven, more abrupt information, which can affect the conditional intensity function over longer period of time.\nMore specifically, We first argue that many conditional intensity functions can be viewed as an integration of two effects: i) spontaneous background component inherently affected by the internal (time-varying) attributes of the individual; ii) effects from history events. Meanwhile, most information in the real world can also be covered by continuously updated features like age, temperature, and asynchronous event data such as clinical records, failures. This motivates us to devise a general approach. Then, we use one RNN whose units are aligned with the time points of a time series, and another RNN whose units are aligned with events. The time series RNN can timely update the intensity function while the event sequence RNN is used to efficiently capture the long-range dependency over history. They can interact with each other through synergic nonlinear mapping. This allows fitting arbitrary dynamics of point process which otherwise will be difficult or often impossible to be specified by a parameterized model restricted to certain assumptions.\nAs an extension to the conference version [20]1, the overall highlights of this paper are:\ni) To the best of our knowledge, this is the first work to jointly interpret and instantiate the conditional intensity function with fused time series and event sequence RNNs. This opens up the room for connecting the neural network techniques to traditional point process that emphasizes more on specific model driven by domain knowledge. The introduction of a full RNN treatment lessen the efforts for the design of (semi-)parametric point process model and its complex learning algorithms which often call for special tricks e.g. [22] that prohibiting the wide use for practitioners. In contrast, neural networks and specifically RNNs, are becoming off-the-shelf tools and getting widely used recently.\nii) We model the genuine multi-dimensional point process through recurrent neural networks. Previous work [6] use the RNN to model so-called pseudo multi-dimensional point process [21]. Actually, they regard the event sequence as a univariate point process and treat the dimension as the mark associated with events. Consequently, there exists only one intensity function for all the processes instead of one per each dimension. On the contrary, in our work the\n1. The main extensions include: i) in contrast to [20] where the socalled pseudo multi-dimensional point process [21] is adopted which involves only a single intensity function for all types of event sequence, here we separately model the intensity functions for each event type leading to the so-called genuine multi-dimensional point process via recurrent neural networks; ii) based on the resulting multi-dimensional point process model, we incorporate a new attention mechanism to improve the interpretability of the prediction model. This expands the dependency model in RNN from the recent hidden variable hj to a set of recent ones which further improves its modeling capability; iii) a more thorough analysis and verification via devised simulation experiments; iv) performing more experiments from both social network data and healthcare data. Note that the added attention model enables the relation discovery capability as also verified in both simulation based and real-world data. While the conference paper [20] only deals with event prediction rather than relation mining.\n3 process is the result of the superposition of sub-processes for each dimension. In this way, we can separate the parameters of each dimension as well as capture their interactions. This leads to more effective simulation and learning algorithm.\niii) To improve the interpretability, it is also perhaps the first time, to our knowledge, an attention based RNN model for point process is proposed. For multi-dimensional point process, our proposed attention mechanism allows each dimensional has its own attention parameters. One typical resulting utility involves decision support and causality analysis [23].\niv) Our model is simple and general and can be end-toend trained. We target three empirical application domains to demonstrate the superiority of the proposed method, namely, predictive maintenance, social network analysis and disease relation mining. The state-of-the-art performance on relational mining, event type and timestamp prediction corroborates its suitability to real-world applications.\nThe organization of this paper is as follows: related work is reviewed in Section 2. Section 3 presents the main approach, and Section 4 describes the empirical studies involving three different application scenarios related to both prediction and mining. Section 5 concludes this paper."}, {"heading": "2 RELATED WORK AND MOTIVATION", "text": "We review the related concepts and work in this section, which is mainly focused on Recurrent Neural Networks (RNNs) and their applications in time series and sequence data, respectively. Then we discuss existing point process methods and their connection to RNNs. All these observations motivate the work of this paper.\nRecurrent neural network. The building block of our model is the Recurrent Neural Networks (RNNs) [24], [25] and its modern variants e.g., Long Short-Term Memory (LSTM) units [26], [27] and Gated Recurrent Units (GRU) [28]. RNNs are dynamical systems whose next state and output depend on the present network state and input, which are more general models than the feed-forward networks. RNNs have long been explored in perceptual applications for many decades, however it can be very difficult for training RNNs to learn long-range dynamics in part due to the vanishing and exploding gradients problem. LSTMs provide a solution by incorporating memory units that allow the network to learn when to forget previous hidden states and when to update hidden states given new information. Recently, RNNs and LSTMs have been successfully applied in large-scale vision [29], speech [30] and language [31] problems.\nRNNs for series and event data. From application perspective, we consider two main scenarios in this paper: i) RNNs for synchronized series with evenly spaced interval e.g., time series or indexed sequence with pure order information e.g., language; ii) asynchronous sequence with timestamp e.g., event data.\ni) Synchronized series: RNNs have been a long time a natural tool for standard time series modeling and prediction [32], [33], whereby the indexed series data point is fed as input to an (unfold) RNN. In a broader sense, video frames can also be treated as time series and RNN\nare widely used in recent visual analytics works [34] and so for speech [30]. RNNs are also intensively adopted for sequence modeling tasks [28] when only order information is considered.\nii) Asynchronous event: In contrast, event sequence with timestamp about their occurrence, which are asynchronously and randomly distributed over the continuous time space, is another typical input type for RNNs [6], [35] (despite its title for \u2019time series\u2019). One key differentiation against the first scenario is that the timestamp or time duration between events (together with other features) is taken as input to the RNNs. By doing so, (long-range) event dependency can be effectively encoded.\nInterpretability and attention model. Prediction accuracy and model interpretability are two goals of many successful predictive methods. Existing works often have to suffer the tradeoff between the two by either picking complex black box models such as deep neural network or relying on traditional models with better interpretation such as Logistic Regression often with less accuracy compared with state-of-the-art deep neural network models. Despite the promising gain in accuracy, RNNs are relatively difficult to interpret. There have been several attempts to interpret RNNs [35], [36], [37]. However, they either compute the attention score by the same function regardless of the affected point\u2019s dimension [35], or only consider the hidden state of the decoder for sequence prediction [36], [37]. As for multi-dimensional point process, past events shall influence the intensity function differently for each dimension. As a result, we explicitly assign different attention function for each dimension which is modeled by respective intensity functions, thus leading to an infectivity matrix based attention mechanism which will be detailed later in this paper.\nPoint processes. Point process is a mathematically rich and principled framework for modeling event data [1]. It is a random process whose realization consists of a list of discrete events localized in time. The dynamics of the point process can be well captured by its conditional intensity function whose definition is briefly reviewed here: for a short time window [t, t + dt), \u03bb(t) represents the rate for the occurrence of a new event conditioned on the history Ht = {zi, ti|ti < t}:\n\u03bb(t) = lim \u2206t\u21920 E(N(t+ \u2206t)\u2212N(t)|Ht) \u2206t = E(dN(t)|Ht) dt ,\nwhere E(dN(t)|Ht) is the expectation of the number of events happened in the interval (t, t + dt] given the historical observations Ht. The conditional intensity function has played a central role in point processes and many popular processes vary on how it is parameterized. Some typical examples include:\n1) Poisson process [38]: the homogeneous Poisson process has a very simple form for its intensity function: \u03bbd(t) = \u03bbd. Poisson process and its time-varying generalization are both assumed to be independent of the history.\n2) Reinforced Poisson processes [15], [39]: the model captures the \u2018rich-get-richer\u2019 mechanism characterized by a compact intensity function, which is recently used for popularity prediction [15].\n3) Hawkes process [19]: Recently, Hawkes process has received a wide attention in network cascades modeling [5],\n4\n[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44]. As an illustration example intensively used in this paper, we particularly write out its intensity function is:\n\u03bbd = \u00b5d(t) + \u2211 i:ti<t \u03b3did(t\u2212 ti)\n= \u00b5d(t) + \u2211 i:ti<t adidexp(\u2212w(t\u2212 ti)),\nwhere Adid = {adid} is the infectivity matrix, indicating the directional influence strength from dimension di to d. It explicitly uses a triggering term to model the excitation effect from history events where the parameter w denotes the decaying bandwidth. The model is originally motivated to analyze the earthquake and its aftershocks [45].\n4) Reactive point process [16]: it can be regarded as a generalization of the Hawkes process by adding a selfinhibiting term to account for the inhibiting effects from history events.\n5) Self-correcting process [46]: its background part increases steadily, while it is decreased by a constant e\u2212\u03b1 < 1 every time a new event appears.\nWe summarize the above forms in Table 1. It tries to separate the spontaneous background component and history event effect explicitly. This also motivates us to design an RNN model that can flexibly model various point process forms without model specification."}, {"heading": "3 NETWORK STRUCTURE AND LEARNING", "text": "In this section, we will present the proposed network structure along with the learning algorithm for modeling the behavior of dynamic events."}, {"heading": "3.1 Brief on RNN as building block", "text": "Taking a sequence {x}Tt=1 as input, the RNN generates the hidden states {h}Tt=1, also known high-level representation of inputs [24], [25]:\nht = f(Wxt + Hht\u22121 + b),\nwhere xt is the profile associated with each event, and f is a non-linear function, and W,H,b are parameters to be learned. One common choice for non-linear function f is Sigmoid or tanh, who suffers from vanishing-gradients problem [47] and poor long-range dependency modeling capability. In contrast, we implement our RNN with Long Short Term Memory (LSTM) [26], [27] for its popularity\n\u00a9 2013 IBM Research-China\n22 , te jj te ,\neh1 eh 2 e jh\nzv\nz jc\nz z1  zz2 zz j\n11, te\n22 , tz jj tz ,11, tz\nFig. 2: For a sequence of events from 1 to j (in a recent time window), vz is the learned feature vector for dimension z, \u03b1zzi is the influence strength from dimension zi to z, and c z j is the new representation vector of Htj for z.\nand capability for capturing long-range dependency. In fact, other RNN variants e.g. Gated Recurrent Units (GRU) [28] can also be alternative choices, while the analysis of the consequence of this particular choice is not the focus of our paper. To make the presented paper self-contained, we reiterate the formulation of LSTM as follows:\nit = \u03c3(Wixt + Uiht\u22121 + Vict\u22121 + bi),\nft = \u03c3(Wfxt + Ufht\u22121 + Vfct\u22121 + bf ),\nct = ftct\u22121 + it tanh(Wcxt + Ucht\u22121 + bc), ot = \u03c3(Woxt + Uoht\u22121 + Voct + bo),\nht = ot tanh(ct),\nwhere denotes element-wise multiplication and the recurrent activation \u03c3 is the Logistic Sigmoid function. Unlike standard RNNs, the Long Short Term Memory (LSTM) architecture uses memory cells to store and output information, allowing it to better discover long-range temporal relationships. Specifically, i, f , c, and c are respectively the input gate, forget gate, output gate, and cell activation vectors2. By default, the value stored in the LSTM cell c is maintained unless it is added to by the input gate i or diminished by the forget gate f . The output gate o controls the emission of the memory from the LSTM cell. Compactly, we represent the LSTM system via the following equation:\nht = LSTM(xt,ht\u22121)"}, {"heading": "3.2 Infectivity matrix based attention mechanism", "text": "Now we give further formal notational definitions used in this paper. Time series data is denoted by {yt}Tt=1, e.g. a patient\u2019s temperature and blood pressure recorded in different dimensions of the vector yt. Event data is represented as {zi, ti}Ni=1, where zi \u2208 Z is the dimension representing a categorical information (e.g a mark or type or an agent), where\n2. In subsection 3.1 we slightly abuse the notations and in fact their effects are only valid in this subsection and have no relation to other notations used in the other parts of this paper.\n5 \u00a9 2013 IBM Research-China Time series LSTM Event dependency LSTM Event embedding layerTime series input Event sequence input Synergic layer Event dimension prediction layer Event timestamp prediction layer y c s t u Classification loss layer Regression loss layer Neural Attention layer e z,t eh yh\nFig. 3: Our network can be trained end-to-end. Event is embedded in low-dimensional space and then pass through attention module. Time series and event sequence are connected to an synergic mapping layer that fuses the information from two LSTMs. Then output layer consists of dimension and timestamp.\nZ is the finite set of all event types, and ti is occurrence time. The former can timely affect the transient occurrence intensity of events and the latter can often abruptly cause jump and transition of states of agents and capture longrange event dependency [48].\nAs shown in Fig. 3, for our proposed network, these two sources of data are fed separately to two RNNs (we call it a twin RNN structure in this paper) whose outputs are further combined to serve as the inputs of subsequent layers. For event sequence {zi, ti}Ni=1 with length N , we can generate a hidden variable sequence {hi}Ni=1 as the highlevel representation of input sequence in RNN. To predict the dimension (e.g. event type) zj+1 and time tj+1 for the (j + 1)-th event, the history Htj = {zi, ti} j i=1, prior to that event should be utilized. The most recent hj is often regarded as a compressed representation of Htj .\nOne may argue the necessity for involving a twin RNN structure, since the instantaneous time series data can be sampled and fed into a single RNN along with the event data when an event occurs. However, there are particular advantages for adopting such a twin-RNN structure. Empirically, it has been shown in some recent study [34] that using separate RNN for each time series data e.g., video and time series sensors can lead to better prediction accuracy than a single RNN fed with the combination of the multiple time series data. More importantly, the events can occur in arbitrary timestamp i.e., they are asynchronous while the time series data is often sampled with equal time interval being a synchronous sequence. This inherent difference inspires us to model these two different sequence data via separate RNN as their dynamics can be rather varying.\nHowever, there are still two limitations for the above approach: i) The prediction capability or model expressive-\nness is limited: in fact only the recently updated hidden variable hj is used for prediction regardless of the length of input sequence. ii) The interpretability is limited. As we compress all information into a fixed vector hj , it is hard to infer which event contributes most to the prediction. For example in the problem of multi-dimensional Hawkes process learning, one important goal is to uncover the hidden network structure, infectivity matrix A, from real-world event sequences, such as the influence strength between users in social network [5], [14], or progression relationship between event types [17]. Uncovering the hidden structure is also stressed in causal analysis [23], which gives much evidence for prediction result. This calls for particular mechanisms to improve its flexibility and interpretability.\nIn this work, we devise a temporal attention mechanism to enable interpretable prediction models for point processes. Events from a certain dimension may have higher influence on some dimensions. We exploit this observation to make the trained neural network model more interpretable and expressive. To achieve this, we first expand the representation ofHtj to be a set of vectors {hi} j i=1 instead of only the most recent hj , referred as context vectors. Each of them is localized to its respective preceding event of interest from inputs {zi, ti}ji=1. Inspired by the Hawkes process, the influence strength \u03b1zzi from zi to z is introduced and it is modeled by:\n\u03b1zzi = fatt(hi,vz) (1)\nwhere vz is the feature vector to be learned for the particular prediction dimension z and fatt is the score function which gives the influence strength from zi to z. Once the influence strength \u03b1zzi are computed, we can generate the representation vector czj for the next layer:\nczj = \u03c6({hi} j i=1, {\u03b1 z zi} j i=1), (2)\nwhere \u03c6 is the attention function, which computes the final representation ofHtj . Here we choose the widely used soft attention mechanism [36], whose influence from former events is in an additive form [5]:\nczj = S\u2211 i=1 \u03b1zzihi (3)\nNote that hard attention [37] only assigns 0 or 1 to the influence strength \u03b1zzi , which is too rough to capture fine-grained influence. Since the cost is differentiable with respect to the parameters, we can easily train the network end-to-end using backpropagation.\nAfter the model is trained i.e. the parameter vz is fixed, for each testing event sequence k and its computed {{\u03b1zzi} j i=1}k by Eq.1, we define the infectivity matrix to reflect the mutual influence among dimensions as Azi,z = \u3008\u03b1zzi\u3009, zi, z \u2208 Z, where \u3008\u00b7\u3009 represents the average of all {\u03b1zzi}k divided by k.\nThe attention mechanism is depicted in Fig. 2. This attention mechanism can allow the network to refer back to the preceding hidden variables {hj}ji=1, instead of forcing it to encode all information into the recent hj . It can retrieve from internal memory and choose what to attend to.\nFinally we point out that in the context of point process, our attention mechanism is different from the existing\n6 work [35], [36], [37] in that they only consider one-way effect over the sequence. In another word, their approaches are current state agnostic. This leads to a vector representation (similar to the role of the vector vz used in Eq.1) for the weight variables \u03b1 instead of a two-way infectivity matrix A. Moreover, to make the model tractable, we use a parameterized form by Eq.1 to represent the two-way weights."}, {"heading": "3.3 Network structure", "text": "Now we give the full description of our network whose overview is depicted in Fig. 3. The dashed part in the right of figure is illustrated in detail in Fig. 2. For time series data e.g., temperature, blood pressure, they are sampled evenly over time. We use {yt}Tt=1 to indicate the dense feature vector sampled at different timestamps. Those signals are expected to reflect the states of each dimension and drive the occurrence intensity of events. Hence we have:\nhyt = LSTMy(yt,h y t\u22121)\nFor event sequence {zi, ti}Ni=1, we can generate hidden states through LSTM, which can capture long-range dependency of events. First we project the dimension zi to a lowdimensional embedding vector space. Then the embedding vector combined with timestamps is fed to LSTM. We use the following equation to represent the process:\nei = Wemzi, hei = LSTMz({ei, ti},hei\u22121),\nwhere ei denotes the embedding vector of input zi and Wem the embedding matrix to learn.\nFor dimension z, its final representation of Htj is czj , which is obtained through the attention mechanism introduced in Eq. 3. For the score function of Eq. 1, we specify it by:\nfatt(h e i ,vz) = { 0, if |tanh(hei \u2217 vz)| < |tanh(hei \u2217 vz)|, otherwise\n(4) When the context vector hei is similar to vz , the attention function fatt produces a large score. Otherwise a small one. In an extreme case, the score is zero when the context vector is orthogonal to feature vector vz of dimension z. To promote sparsity of infectivity matrix, we threshold the score with a minus operation. The threshold can control the degree of sparsity which is set to 0.01 throughout this paper. Note the form of Eq. 4 is also used in [36], [37] to model the one-way attention weight \u03b1i = fatt(hi,v). From this, it is clear that our model for the attention is two-way between dimension i to z as shown in Eq. 1.\nTo jointly model event sequence and time series, we combine them into one synergic layer as illustrated in Fig. 3:\nszj = fsyn(Wf [h y tj , c z j ] + bf ), (5)\nwhere [hytj , c d j ] is the concatenation of the two vectors. The synergic layer can be any function, coupling two data sources together. Here we use the Sigmoid function. As a result, we obtain a representation szj for the output dimension z. We can use this representation to compute the intensity for each dimension and then simulate its next occurrence time and its dimension jointly. Here we take a more efficient\napproach by firstly predicting the next event\u2019s dimension and then further predicting the occurrence timestamp based on the predicted event dimension. Note that the intensity function is modeled implicitly within the neural network architecture and we directly model the timing and dimension of the events. In this way, we overcome the expensive computation cost from explicit parametric form of intensity function.\nTo predict the next event\u2019s dimension uj+1, we apply the Softmax operation to those representations {szj}Zz=1 where Z is the number of event dimensions.\nuj+1 = softMax(wus1j , . . . ,wus Z j ) (6)\nwhere wu are model parameters to learn. The optimal dimension z\u2217j+1 (as the prediction result) is computed by selecting the corresponding maximum element in uj+1:\nz\u2217j+1 = argmax d uj+1 (7)\nAfter we obtain the optimal dimension z\u2217j+1, we use the\nrepresentation s z\u2217j+1 j to derive occurrence time following:\nt\u2217j+1 = wss z\u2217j+1 j + bs, (8)\nwhere ws are model parameters for learning."}, {"heading": "3.4 End-to-end learning", "text": "The likelihood of observing a sequence {zi, ti}Ni=1 along with time series signals {yt}Tt=1 can be expressed as follows:\nL ( {zi, ti}Ni=1 ) = N\u22121\u2211 j=1 {bzj+1 log(u zj+1 j+1 ) + log ( f(tj+1|Htj ) ) }\n(9) where the weight parameters b are set as the inverse of the sample count in that dimension against the total size of samples, to weight more on those dimensions with fewer training samples. This is in line with the importance weighting policy for skewed data in machine learning [49].\nFor the second term, the underlying rationale is that we not only encourage correct prediction of the coming event dimension, but also require the corresponding timestamp of the event to be close to the ground truth. We adopt a Gaussian penalty function:\nf(tj+1|Htj ) = 1\u221a 2\u03c0\u03c3 exp\n( \u2212(tj+1 \u2212 t\u2217j+1)2\n2\u03c32 ) As shown in Fig. 3, the output t\u2217j+1 from the timestamp prediction layer is fed to the classification loss layer to compute the above penalty given the actual timestamp tj+1. We adopt RMSprop gradients [50] which have been shown to work well on training deep networks to learn these parameters.\nBy directly optimizing the loss function, we learn the prediction model in an end-to-end manner without the need for sophisticated or carefully designed algorithms (e.g., Majorization-Minimization techniques [5], [51]) used in generative Point process models. Moreover, as pointed out by recent work [18], another limitation for the generative point process model is that they are aimed to maximize the joint probability of all observed events via a maximum likelihood estimator, which is not tailored to the prediction task.\n7"}, {"heading": "4 EXPERIMENTS AND DISCUSSION", "text": "We evaluate the proposed approach on both synthetic and real-world datasets, from which three popular application scenarios are covered: social network analysis, electronic health records (EHR) mining, and proactive machine maintenance. The first two scenarios involve public available benchmark dataset: MemeTracker and MIMIC, while the latter involves a private ATM maintenance dataset from a commercial bank headquartered in North America.\nThe code is based on Theano running on a Linux server with 32G memory, 2 CPUs with 6 cores for each: Intel(R) Xeon(R) CPU E5-2603 v3@1.60GHz. We also use 4 GPU:GeForce GTX TITAN X with 12G memory backed by CUDA and MKL for acceleration."}, {"heading": "4.1 Baselines and evaluation metrics", "text": "We compare the proposed method to the following algorithms and state-of-the-art methods:\n1) Logistic model: We use Logistic regression for event timestamp prediction and an independent Logistic classification model for event type prediction. To make sure the proposed method and the logistic model use the same amount of information, the predictor features in the regression are comprised of the concatenation of feature vectors for sub-windows of all active time series RNN.\n2) Hawkes Process: To enable multi-type event prediction, we use a Multi-dimensional Hawkes process [7], [14]. The Full, Sparse, LowRankSparse indicate the different types of Hawkes Process model. The full model has no regularization on infectivity matrix while the Sparse and LowRankSparse ones have sparse and lowrank-sparse regularization, respectively. In the following, Hawkes process indicates LowRankSparse model if not explicitly mentioned. The inputs are comprised of event sequences with dimensions and timestamps.\n3) Recurrent Marked Temporal Point Processes (RMTPP): [6] uses a neural network to model the event dependency flexibly. The inputs are event sequences with continuous signals sampled when they happen. The method can only sample features of transient time series when the events happen and use partially parametric form for the base intensity and a time-decaying influence from former event to the current one. Another difference is that it assumes an independent distribution between time and marks and predicts the dimension and time independently given the learned representation of the history.\n4) TRPP: This method uses time series and event sequences to collaboratively model the intensity function of point process. The model uses RNN to model the non-linear mapping from history to the predicted marker and time, and treats RNN as a block box without much interpretability. Here we rename it as Twin Recurrent Point Processes (TRPP). This method is the one presented in the conference version of this paper [20].\n5) ERPP: We also include TRPP\u2019s degraded version by only keeping the event sequence as input and removing the time series RNN, which is termed by Event Recurrent Point Processes (ERPP). Including the term \u2018event\u2019 also helps distinguish it from the existing term RPP: Reinforced Poisson Processes [15], [39].\n6) Markov Chain (MC): The Markov chain refers to the sequence of random variables, with the Markov property that future state only depends on the current state and is conditionally independent of the history. The order of Markov chain indicates how many recent states on which the future state depends. As this model can only learn the transition probability of dimensions, we use it to predict the dimensions without taking the time into account. The optimal order of Markov chain is determined by the performance on separate validation dataset.\n7) Continuous Time Markov Chain (CTMC): The CTMC is a special type of semi-Markov model, which models the continuous transition among dimensions as a Markov process. It predicts the next dimension with the earliest transition time, therefore, it can jointly predict time and dimension.\n8) Homogeneous Poisson Process: This method implements the most basic point process model in which the intensity function is constant and events occur independently. It can estimate interval-event gaps.\n9) Self-correcting Process: When the occurrence of an event decrease the probability of other events, we are facing a variant of point process called self-correcting processes. Its intensity function is shown in Table 1 and it can estimate the inter-event time.\nInline with the above TRPP and ERPP methods, we term our model Attentional Twin Recurrent Point Processes (ATRPP). To further study the effect of the time series RNN, we also evaluate the baseline version without using this channel, which is termed as Attentional Event Recurrent Point Processes (AERPP).\nEvaluation metrics. We use several common metrics for performance evaluation. For the next event dimension prediction, we adopt Precision, Recall, F1 Score and Confusion matrix. For event time prediction, we use the Mean Absolute Error (MAE) which measures the absolute difference between the predicted time point and the actual one. For the infectivity matrix, we use RankCorr [14] to measure whether the relative order of the estimated influence strength is correctly recovered, when the true infectivity matrix is available. The RankCorr is defined as the averaged Kendall rank correlation coefficient3 between each row of ground-truth and estimated infectivity matrix. RelErr measures the relative deviation between estimated a\u2217ij and and ground-truth aij , which is defined as the average of |a\u2217ij\u2212aij |\naij , i, j \u2208 Z."}, {"heading": "4.2 Experiments on synthetic data", "text": "The test on synthetic data provides a quantitative way for evaluating the performance of compared methods.\nSynthetic data generation. We use simulated data with known ground-truth to quantitatively verify our model by aforementioned metrics. Specifically, we generate cascades from multi-dimensional hawkes process via the Thinning algorithm [52]. We choose Z = 20 for the number of event dimensions. The background intensity term is set uniformly at random: \u00b5d \u223c U(0, 0.01). Mutual influence is set similarly to aij \u223c U(0, 0.1). Half of the elements in the infectivity matrix are set to 0 by random in order to mimic\n3. https://en.wikipedia.org/wiki/Kendall rank correlation coefficient\n8\nthe sparsity of influence between dimensions in many realworld problems. For the stability of the simulation process, the matrix is scaled such that its spectral radius is no larger than one. The decaying parameter in the Hawkes process is set tow = 0.01. The detailed description of these parameters can be found in Sec. 1 for the Hawkes process. We simulate 5000 independent cascades, 3000 for training, 1000 for validating, and 1000 for testing, respectively. To generate time series signals, y, with some explanation capability to the background intensity, we sample from y = \u00b5d+nd for all dimensions d, where, nd is a Gaussian noise, nd \u223c U(0, 0.001).\nExperimental results. The performance on synthetic data is shown in Fig. 4. The relative error of infectivity matrix, RankCorr is demonstrated to verify the capability of uncovering hidden network structure among those dimensions i.e., the nodes in the network. The accuracy of time and dimension prediction is shown in order to compare the predictive performance. Our model achieves a better prediction performance, and meanwhile uncovers the infectivity matrix better than the alternatives. The self-correcting process suffers from model misspecification and performs worse than other point process models as the events are self-exciting not self-correcting. Our non-parametric model can learn from data and generalize well without prior knowledge of data."}, {"heading": "4.3 Predictive machine maintenance", "text": "Predictive maintenance is a sound testbed for our model. It involves equipment risk prediction to allow for proactive scheduling of corrective maintenance. Such an early identification of potential concerns helps deploy limited resources more efficiently and cost effectively, reduce operations costs and maximize equipment uptime. Predictive maintenance is adopted in a wide variety of applications such as fire inspection, data center and electrical grid management e.g. [16]. For its practical importance in different scenarios and relative rich event data for modeling, we target our model to a real-world dataset of more than 1,000 automated teller machines (ATMs) from a global bank headquartered in North America.\nWe have no prior knowledge on the dynamics of the complex system and the task can involve arbitrarily working schedules and heterogeneous mix of conditions. It takes much cost or even impractical to devise specialized models.\nThe studied dataset is comprised of the event logs involving error reporting and failure tickets, which is originally collected from 1,554 ATMs. The event log of error records includes device identity, timestamp, message\ncontent, priority, code, and action. A ticket (TIKT) means that maintenance will be conducted. Statistics of the data is presented in Table 2. The error type indicates which component encounters an error: 1) printer (PRT), 2) cash dispenser module (CNG), 3) Internet data center (IDC), 4) communication part (COMM), 5) printer monitor (LMTP), 6) miscellaneous e.g., hip card module, usb (MISC). The time series here consists of features: i) the inventory information: ATM type, age, operations, temperatures; ii) event frequency for each event in the recent an hour interval. The event types and their occurrence time from the ATMs are an event sequences. Therefore, there are 1554 sequences in total, which are randomly divided into training (50%), validating (20%) and testing (30%) portions.\nTable 3 shows the averaged performance of the proposed method compared to the alternatives. The confusion matrix for the seven event types are shown in Fig. 5 by all methods. Not surprisingly, for both event type and timestamp prediction, our main approach, i.e., ATRPP outperforms by a notable margin. ATRPP report 0.634 F1 score and 3.92 MAE while AERPP reaches 0.617 F1 score and 3.98 MAE. Obviously, this verifies that synergically modeling event se-\n9 (a) ATRPP (b) AERPP (c) Hawkes\n(d) TRPP (e) ERPP (f) RMTPP\n(g) CTMC (h) Markov Chain (i) Logistic\nFig. 5: Confusion matrices over different event dimensions on the ATM maintenance dataset.\nquence and time series can boost the performance of predicting future events and time.Interestingly, all point process based models obtain better results on this task which suggests they are more promising compared to classical classification models. Indeed, our methodology provides an endto-end learning mechanism without any pre-assumption in modeling point process. All these empirical results on realworld tasks suggest the efficacy of our approach, especially in capturing the temporal dynamics of events data.\nVisualization of influence pattern. We visualize the infectivity matrix of ATRPP as in Fig. 6. Each node denotes one dimension which here represents one type of events. The directed edge means the influence strength from source node to destination node. The size of nodes and depth of color is proportional to the weighted degree of nodes, which indicate the total influence of the node has on others. The width of edges is is proportional to the strength of influence. Self-loop edges are located at the right\nof nodes without an arrow. Note this setting applies to the subsequent two experiments. As is shown, it\u2019s obvious that TIKT (maintenance) have a strong influence over all types of errors (breakdown) as maintenance can greatly decrease the probability of breakdown of machines. Also, self-loop edge of TIKT node is too small to see which indicates that maintenance has low correlation itself. All types of errors have self-loop, indicating a recurrent pattern of errors. The breakdown of communication module (COMM) often leads to disfunction of cash dispenser module (CNG), printer (PRT) and internet data center (IDC). Besides, internet data center (IDC) problems influence cash dispenser module (CNG) and printer (PRT) weakly."}, {"heading": "4.4 Social network analysis", "text": "In line with the previous works for information diffusion tracking [14], [42], [53], the public dataset MemeTracker4 is\n4. http://memetracker.org\n10\nused in this paper, which contains more than 172 million news articles or blog posts from various online media. The information, such as ideas, products and user behaviors, propagates over sites in a subtle way. For example, when Mark Zuckerberg posted \u201dI just killed a pig and a goat\u201d, the meme appeared on the theguardian, Fortune, Business Insider one after the other and it became viral. This cascade can be regarded as a realization of an information diffusion over the network. By looking at the observed diffusion history, we want to know through which site and when a particular meme is likely to spread. Besides, we want to uncover the hidden diffusion structure from those meme cascades, which is useful in other social network analysis applications, e.g., marketing by information maximization [54]. From online articles, we collect more than 1 million meme cascades over different websites. For each meme cascade, we have the timestamp when sites mention a specific meme. For the experiments here, we use the top 500 media sites with the largest number of documents and select the meme cascades diffuse over them as done in previous works [14], [42]. As a result, we obtain around 31 million meme cascades, which are randomly split into training (50%), validating (%20) and testing (%30) parts. The event sequences are the meme cascades, which contain the website (dimension) and the timestamp. We count the times that a meme is mentioned during an hour over all the websites and use it as the timeseries to reflect the hotness of the meme and the activity of websites.. The time interval of meme is shown in Fig. 9(a).\nAs the ground truth of network is unknown, we proceed by following the protocol as designed and adopted in [14], [55], [56]. We create a graph G, for which each node is a website. If a post on site u has a hyperlink pointed to site v, then we create a directed edge (u, v) with weight 1. If multiple hyperlinks exist between two sites, then the weight is accumulated. We use the graph as the ground truth and compare it with the inferred infectivity matrix from meme cascades. The prediction performance is evaluated by Accuracy@k, which evaluates whether the true label is within the top k predicted dimensions.\nThe prediction performance is shown in Table 4 from\nTABLE 4: Prediction evaluation by accuracy and MAE (mean absolute error) on MemeTracker dataset.\nmodel accuracy@10 accuracy@5 MAE\nPoisson \u2014\u2013 \u2014\u2013 1.63 SelfCorrecting \u2014\u2013 \u2014\u2013 1.70 Markov Chain 0.563 0.472 \u2014\u2013 CTMC 0.513 0.453 1.69 Logistic 0.463 0.416 1.72 Hawkes 0.623 0.563 1.68 RMTPP 0.679 0.589 1.55 TRPP 0.681 0.592 1.52 ERPP 0.673 0.586 1.56 ATRPP 0.694 0.598 1.43 AERPP 0.678 0.589 1.45\nFig. 7: Examples of detected media communities over the inferred diffusion network by our method ATRPP.\n11\nwhich one can observe our model outperforms the alternatives. The rank correlation is shown in Fig. 8. Our models ATRPP, AERPP can better uncover the infectivity matrix than the competitive methods in terms of the correlation rank metric.\nIn order to visualize the learned network, we use community detection algorithm [57] with resolution 0.9 [58] over learned directed network of ATRPP, which renders 18 communities. The resolution parameter controls the resolution of detected communities. It is set to lower values to get more communities (smaller ones), and is set higher to get fewer communities (bigger ones). Therefore, the communities can vary from the macroscale in which all nodes belong to the same community, to the microscale in which every node forms its own community. Fig. 7 shows some examples of those communities. Some media domains, like liverpooldailypost.com dominate in the cluster and what they publish usually spread to others and get viral."}, {"heading": "4.5 Electronic health records mining", "text": "Uncovering the disease progression relation is important in healthcare analytics, which helps take preventive measures before fatal diseases happen. MIMIC-III (Medical Information Mart for Intensive Care III) is a large, publicly available dataset5, which contains de-identified health-related data during 2001 to 2012 for more than 40,000 patients. It includes information such as demographics, vital sign\n5. https://mimic.physionet.org\nmeasurements, diagnoses and procedures. For each visit, a patient receives multiple diagnoses, with one as the primary one. We filter out 937 patients and 75 diseases. The age, weight, heart rate and blood pressure are used as time series signals of patients. The distribution of time intervals between every two visits is shown in Fig. 9(b). We have used the sequences of 600 patients to train, 100 to evaluate and the rest for test.\nSimilar to MemeTracker, community detection algorithm [57] with resolution 0.9 is applied on the learned directed network from ATRPP. The results show cohesion within communities, which demonstrates the effectiveness of our attention mechanism. Note that some edges are too small to be visible. Specifically, Fig. 10(a) is about liver diseases. The node of alcohol cirrhosis has a large self-loop edge, which means this disease generally repeats many times. Besides, it has a large edge towards complications of transplanted kidney and hepatic encephalopathy, which means alcohol cirrhosis has a high probability of developing into the these two diseases. Fig. 10(b) is about respiratory diseases. Similarly, mechanical complication of tracheostomy and pseudomonal pneumonia have large self-loop edges, indicating they often relapse. Mechanical complication of tracheostomy has an edge towards pseudomonal pneumonia while the reverse, interestingly, does not exist. Fig. 10(c) shows the graph for alcohol-related diseases. Obsessed in alcohol has impact on reaction to indwelling urinary catheter regarding urinary system and hemorrhage of gastrointestinal tract regarding gastrointestinal system. Fig. 10(d) is about heart and blood related diseases. Three different parts of body form a progression line, consisting of diseases of trachea and bronchus (respiratory disease), paroxysmal ventricular tachycardia (heart rate disturbance) and acute vascular insufficiency of intestine (intestinal disease). The other line is septicemia leads to acute myocardial infarction of other anterior wall (heart attack), which results in paroxysmal ventricular tachycardia (heart rate disturbance). Fig. 10(e) is about blood metabolic diseases. Hemorrhage complicating a procedure leads to pulmonary embolism and infarction and complications due to renal dialysis device, implant, and graft. Diabetes with ketoacidosis results in urinary tract infection. For the observed strong association as stated above, we conjecture it might be due to either causal or correlation relationship, which can provide supporting evidence and implication for clinical staff and is subject to\n12\nfurther analysis by health practitioners. Table 5 reports the predictive performance of various models. ATRPP outperforms alternatives in both disease types and time prediction. Here first-order Markov Chain outperforms higher-order models, the reason might be due to the fact that visits of patients are sparse and there is not enough data to train the higher order Markov chains."}, {"heading": "5 CONCLUSION", "text": "We conclude this paper with Fig. 11 and identify our proposed method as a recurrent point process model. To elaborate, Hawkes process uses a full explicit parametric model and RMTPP misses the dense time series features to model time-varying base intensity, assumes a partially para-\nmetric form for it and model the pseudo multi-dimensional \u00a9 2013 IBM Research-China\n13\npoint process. We make a further step by proposing an interpretable model which is simple and general and can be trained end-to-end. Most importantly, our model can uncover the subtle network structure and provide interpretable evidence for predicting result. The extensive experiments in this paper have clearly suggested its superior performance in synthetic and real-world data, even when we have no domain knowledge on the problem at hand. This is in contrast to existing point process models where an assumption about the dynamics is often needed to be specified beforehand."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Robert Chen with Emory University School of Medicine, and for helpful discussions and suggestions on the study of the computational experimental results on the MIMIC dataset. We are also thankful to Changsheng Li who shared us the ATM log data from IBM to allow us to perform the predictive maintenance study on real-world data."}], "references": [{"title": "Survival and event history analysis: a process point of view", "author": ["O. Aalen", "O. Borgan", "H. Gjessing"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Identifying and labeling search tasks via query-based hawkes processes", "author": ["L. Li", "H. Deng", "A. Dong", "Y. Chang", "H. Zha"], "venue": "KDD, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "From micro to macro: Uncovering and predicting information cascading process with behavioral dynamics", "author": ["L. Yu", "P. Cui", "F. Wang", "C. Song", "S. Yang"], "venue": "2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Coevolve: A joint point process model for information diffusion and network co-evolution", "author": ["M. Farajtabar", "Y. Wang", "M.G. Rodriguez", "S. Li", "H. Zha", "L. Song"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1954\u20131962.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent marked temporal point processes: Embedding event history to vectore", "author": ["N. Du", "H. Dai", "R. Trivedi", "U. Upadhyay", "M. Gomez-Rodriguez", "L. Song"], "venue": "KDD, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Shaping social activity by incentivizing users", "author": ["M. Farajtabar", "N. Du", "M.G. Rodriguez", "I. Valera", "H. Zha", "L. Song"], "venue": "Advances in neural information processing systems, 2014, pp. 2474\u20132482.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The analysis of time series: an introduction", "author": ["C. Chatfield"], "venue": "CRC press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Event detection from time series data", "author": ["V. Guralnik", "J. Srivastava"], "venue": "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 1999, pp. 33\u201342.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Hawkes processes in finance", "author": ["E. Bacry", "I. Mastromatteo", "J.-F. Muzy"], "venue": "Market Microstructure and Liquidity, vol. 1, no. 01, p. 1550005, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A nonparametric em algorithm for multiscale hawkes processes", "author": ["E. Lewis", "E. Mohler"], "venue": "Journal of Nonparametric Statistics, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning triggering kernels for multi-dimensional hawkes processes.", "author": ["K. Zhou", "H. Zha", "L. Song"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Modeling and predicting popularity dynamics via reinforced poisson processes", "author": ["H. Shen", "D. Wang", "C. Song", "A. Barab\u00e1si"], "venue": "AAAI, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Reactive point processes: A new approach to predicting power failures in underground electrical systems", "author": ["S. Ertekin", "C. Rudin", "T.H. McCormick"], "venue": "The Annals of Applied Statistics, vol. 9, no. 1, pp. 122\u2013144, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Constructing disease network and temporal progression model via context-sensitive hawkes process", "author": ["E. Choi", "N. Du", "R. Chen", "L. Song", "J. Sun"], "venue": "ICDM. IEEE, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Patient flow prediction via discriminative learning of mutually-correcting processes", "author": ["H. Xu", "W. Wu", "S. Nemati", "H. Zha"], "venue": "TKDE, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectra of some self-exciting and mutually exciting point processes", "author": ["A.G. Hawkes"], "venue": "Biometrika, 1971.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1971}, {"title": "Modeling the intensity function of point process via recurrent neural networks", "author": ["X. Shuai", "J. Yan", "X. Yang", "H. Zha", "S. Chu"], "venue": "AAAI, 2017.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Multivariate hawkes processes", "author": ["T.J. Liniger"], "venue": "PhD thesis, Swiss Federal Institute Of Technology, Zurich, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards effective prioritizing water pipe replacement and rehabilitation", "author": ["J. Yan", "Y. Wang", "K. Zhou", "J. Huang", "C.H. Tian", "H.Y. Zha", "W.S. Dong"], "venue": "IJCAI, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning granger causality for hawkes processes", "author": ["H. Xu", "M. Farajtabar", "H. Zha"], "venue": "ICML, 2016.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, vol. 14, pp. 179\u2013211, 1990.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "On the difficulty of training recurrent neural networks", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1997}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv:1308.0850, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv:1412.3555, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Rezende", "D. Wierstra"], "venue": "ICML, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards end-toend speech recognition with recurrent neural networks", "author": ["A. Graves", "A. rahman Mohamed", "G. Hinton"], "venue": "ICML, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le."], "venue": "NIPS, 2014.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent neural networks and robust time series prediction", "author": ["J.T. Connor", "R.D. Martin", "L.E. Atlas"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 240\u2013254, 1994.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1994}, {"title": "Cooperative coevolution of elman recurrent neural networks for chaotic time series prediction", "author": ["R. Chandra", "M. Zhang"], "venue": "Neurocomputing, vol. 86, pp. 116\u2013123, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Recurrent neural networks for driver activity anticipation via sensory-fusion architecture", "author": ["A. Jain", "A. Singh", "H.S. Koppula", "S. Soh", "A. Saxena"], "venue": "ICRA, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Retain: An interpretable predictive model for healthcare using reverse time attention mechanism", "author": ["E. Choi", "M.T. Bahadori", "J. Sun", "J. Kulas", "A. Schuetz", "W. Stewart"], "venue": "NIPS, 2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville"], "venue": "ICML, 2015.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["K. Cho", "A. Courville", "Y. Bengio"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 1875\u20131886, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1875}, {"title": "A survey of random processes with reinforcement", "author": ["R. Pemantle"], "venue": "Probability Survey, vol. 4, no. 0, pp. 1\u201379, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Netcodec: Community detection from individual activities", "author": ["L. Tran", "M. Farajtabar", "L. Song", "H. Zha"], "venue": "Proceedings of the 2015 SIAM International Conference on Data Mining. SIAM, 2015, pp. 91\u201399.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Selfexciting point process models of insurgency in iraq", "author": ["E. Lewis", "G. Mohler", "P.J. Brantingham", "A. Bertozzi"], "venue": "UCLA CAM Reports 10, vol. 38, 2010.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Multistage campaigning in social networks", "author": ["M. Farajtabar", "X. Ye", "S. Harati", "L. Song", "H. Zha"], "venue": "Advances in Neural Information Processing Systems, 2016, pp. 4718\u20134726.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent poisson factorization for temporal recommendation", "author": ["S.A. Hosseini", "K. Alizadeh", "A. Khodadadi", "A. Arabzadeh", "M. Farajtabar", "H. Zha", "H.R. Rabiee"], "venue": "arXiv preprint arXiv:1703.01442, 2017.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "Distilling information reliability and source trustworthiness from digital traces", "author": ["B. Tabibian", "I. Valera", "M. Farajtabar", "L. Song", "B. Sch\u00f6lkopf", "M. Gomez-Rodriguez"], "venue": "arXiv preprint arXiv:1610.07472, 2016.  14", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Statistical models for earthquake occurrences and residual analysis for point processes", "author": ["Y. Ogata"], "venue": "J. Amer. Statist. Assoc., vol. 83, no. 401, pp. 9\u201327, 1988.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1988}, {"title": "A self-correcting pint process", "author": ["V. Isham", "M. Westcott"], "venue": "Advances in Applied Probability, vol. 37, pp. 629\u2013646, 1979.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1979}, {"title": "On the difficulty of training recurrent neural networks.", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "ICML (3),", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "A cluster process representation of a self-exciting process", "author": ["A.G. Hawkes", "D. Oakes"], "venue": "Journal of Applied Probability, 1974.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1974}, {"title": "Classifying skewed data: Importance weighting to optimize average recall.", "author": ["A. Rosenberg"], "venue": "INTERSPEECH,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "Rmsprop and equilibrated adaptive learning rates for non-convex optimization", "author": ["Y.N. Dauphin", "H. de Vries", "J. Chung", "Y. Bengio"], "venue": "arXiv:1502.04390, 2015.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards effective prioritizing water pipe replacement and rehabilitation", "author": ["J. Yan", "Y. Wang", "K. Zhou", "J. Huang", "C.H. Tian", "H.Y. Zha", "W.S. Dong"], "venue": "IJCAI, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "On lewis\u2019 simulation method for point processes", "author": ["Y. Ogata"], "venue": "IEEE Transactions on Information Theory, vol. 27, no. 1, pp. 23\u201331, 1981.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1981}, {"title": "Uncovering the structure and temporal dynamics of information propagation", "author": ["M.G. Rodriguez", "J. Leskovec", "D. Balduzzi", "B. Sch\u00f6lkopf"], "venue": "Network Science, vol. 2, no. 01, pp. 26\u201365, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Influence maximization in dynamic social networks", "author": ["H. Zhuang", "Y. Sun", "J. Tang", "J. Zhang", "X. Sun"], "venue": "2013.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2013}, {"title": "Inferring networks of diffusion and influence", "author": ["M. Gomez Rodriguez", "J. Leskovec", "A. Krause"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2010, pp. 1019\u20131028.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Uncovering the temporal dynamics of diffusion networks", "author": ["M. Gomez Rodriguez", "D. Balduzzi", "B. Sch\u00f6lkopf", "G.T. Scheffer"], "venue": "28th International Conference on Machine Learning (ICML 2011). International Machine Learning Society, 2011, pp. 561\u2013568.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of statistical mechanics: theory and experiment, vol. 2008, no. 10, p. P10008, 2008.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2008}, {"title": "Laplacian dynamics and multiscale modular structure in networks", "author": ["R. Lambiotte", "J.-C. Delvenne", "M. Barahona"], "venue": "arXiv preprint arXiv:0812.1770, 2008.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "A major line of research [1] has been devoted to studying event sequence, especially exploring the timestamp information to model the underlying dynamics of the system, whereby point process has been a powerful and elegant framework", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "While there have been many recent works on modeling continuous-time point processes [3], [4], [5], [6], [7] and time series [8], [9], [10], most of them treat these two processes independently and separately, ignoring the influence one may have on the other over time.", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": ", based on thresholding the stock price series [11]) from the series data.", "startOffset": 47, "endOffset": 51}, {"referenceID": 3, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 7, "endOffset": 10}, {"referenceID": 9, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 18, "endOffset": 22}, {"referenceID": 11, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "niques [5], [12], [13], [14] and novel parametric forms [15], [16], [17], [18] as carefully designed by human prior knowl-", "startOffset": 74, "endOffset": 78}, {"referenceID": 10, "context": ", [13] start to turn to non-parametric form to fit the structure of a point process, but their method is under the Hawkes process formulation, which runs the risk of unknown model complexity and can be inappropriate for point processes that disobey the self or mutual-exciting rule assumed by the Hawkes model [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [13] start to turn to non-parametric form to fit the structure of a point process, but their method is under the Hawkes process formulation, which runs the risk of unknown model complexity and can be inappropriate for point processes that disobey the self or mutual-exciting rule assumed by the Hawkes model [19].", "startOffset": 310, "endOffset": 314}, {"referenceID": 4, "context": "In another recent work [6], the authors proposed a semi-parametric pesudo point process model, which assumes a time-decaying influence between events and the background of intensity is constant.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "As an extension to the conference version [20]1, the overall highlights of this paper are:", "startOffset": 42, "endOffset": 46}, {"referenceID": 18, "context": "[22] that prohibiting the wide use for practitioners.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Previous work [6] use the RNN to model so-called pseudo multi-dimensional point process [21].", "startOffset": 14, "endOffset": 17}, {"referenceID": 17, "context": "Previous work [6] use the RNN to model so-called pseudo multi-dimensional point process [21].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "The main extensions include: i) in contrast to [20] where the socalled pseudo multi-dimensional point process [21] is adopted which involves only a single intensity function for all types of event sequence, here we separately model the intensity functions for each event type leading to the so-called genuine multi-dimensional point process via recurrent neural networks; ii) based on the resulting multi-dimensional point process model, we incorporate a new attention mechanism to improve the interpretability of the prediction model.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "The main extensions include: i) in contrast to [20] where the socalled pseudo multi-dimensional point process [21] is adopted which involves only a single intensity function for all types of event sequence, here we separately model the intensity functions for each event type leading to the so-called genuine multi-dimensional point process via recurrent neural networks; ii) based on the resulting multi-dimensional point process model, we incorporate a new attention mechanism to improve the interpretability of the prediction model.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "While the conference paper [20] only deals with event prediction rather than relation mining.", "startOffset": 27, "endOffset": 31}, {"referenceID": 19, "context": "One typical resulting utility involves decision support and causality analysis [23].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "The building block of our model is the Recurrent Neural Networks (RNNs) [24], [25] and its modern variants e.", "startOffset": 72, "endOffset": 76}, {"referenceID": 21, "context": "The building block of our model is the Recurrent Neural Networks (RNNs) [24], [25] and its modern variants e.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": ", Long Short-Term Memory (LSTM) units [26], [27] and Gated Recurrent Units (GRU) [28].", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": ", Long Short-Term Memory (LSTM) units [26], [27] and Gated Recurrent Units (GRU) [28].", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": ", Long Short-Term Memory (LSTM) units [26], [27] and Gated Recurrent Units (GRU) [28].", "startOffset": 81, "endOffset": 85}, {"referenceID": 25, "context": "Recently, RNNs and LSTMs have been successfully applied in large-scale vision [29], speech [30] and language [31] problems.", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "Recently, RNNs and LSTMs have been successfully applied in large-scale vision [29], speech [30] and language [31] problems.", "startOffset": 91, "endOffset": 95}, {"referenceID": 27, "context": "Recently, RNNs and LSTMs have been successfully applied in large-scale vision [29], speech [30] and language [31] problems.", "startOffset": 109, "endOffset": 113}, {"referenceID": 28, "context": "i) Synchronized series: RNNs have been a long time a natural tool for standard time series modeling and prediction [32], [33], whereby the indexed series data point", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "i) Synchronized series: RNNs have been a long time a natural tool for standard time series modeling and prediction [32], [33], whereby the indexed series data point", "startOffset": 121, "endOffset": 125}, {"referenceID": 30, "context": "In a broader sense, video frames can also be treated as time series and RNN are widely used in recent visual analytics works [34] and so for speech [30].", "startOffset": 125, "endOffset": 129}, {"referenceID": 26, "context": "In a broader sense, video frames can also be treated as time series and RNN are widely used in recent visual analytics works [34] and so for speech [30].", "startOffset": 148, "endOffset": 152}, {"referenceID": 24, "context": "RNNs are also intensively adopted for sequence modeling tasks [28] when only order information is considered.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "ii) Asynchronous event: In contrast, event sequence with timestamp about their occurrence, which are asynchronously and randomly distributed over the continuous time space, is another typical input type for RNNs [6], [35] (despite its title for \u2019time series\u2019).", "startOffset": 212, "endOffset": 215}, {"referenceID": 31, "context": "ii) Asynchronous event: In contrast, event sequence with timestamp about their occurrence, which are asynchronously and randomly distributed over the continuous time space, is another typical input type for RNNs [6], [35] (despite its title for \u2019time series\u2019).", "startOffset": 217, "endOffset": 221}, {"referenceID": 31, "context": "There have been several attempts to interpret RNNs [35], [36], [37].", "startOffset": 51, "endOffset": 55}, {"referenceID": 32, "context": "There have been several attempts to interpret RNNs [35], [36], [37].", "startOffset": 57, "endOffset": 61}, {"referenceID": 33, "context": "There have been several attempts to interpret RNNs [35], [36], [37].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "However, they either compute the attention score by the same function regardless of the affected point\u2019s dimension [35], or only consider the hidden state of the decoder for sequence prediction [36], [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 32, "context": "However, they either compute the attention score by the same function regardless of the affected point\u2019s dimension [35], or only consider the hidden state of the decoder for sequence prediction [36], [37].", "startOffset": 194, "endOffset": 198}, {"referenceID": 33, "context": "However, they either compute the attention score by the same function regardless of the affected point\u2019s dimension [35], or only consider the hidden state of the decoder for sequence prediction [36], [37].", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "Point process is a mathematically rich and principled framework for modeling event data [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "2) Reinforced Poisson processes [15], [39]: the model cap-", "startOffset": 32, "endOffset": 36}, {"referenceID": 34, "context": "2) Reinforced Poisson processes [15], [39]: the model cap-", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "tures the \u2018rich-get-richer\u2019 mechanism characterized by a compact intensity function, which is recently used for popularity prediction [15].", "startOffset": 134, "endOffset": 138}, {"referenceID": 15, "context": "3) Hawkes process [19]: Recently, Hawkes process has received a wide attention in network cascades modeling [5],", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "3) Hawkes process [19]: Recently, Hawkes process has received a wide attention in network cascades modeling [5],", "startOffset": 108, "endOffset": 111}, {"referenceID": 35, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 69, "endOffset": 72}, {"referenceID": 36, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 86, "endOffset": 90}, {"referenceID": 37, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 141, "endOffset": 145}, {"referenceID": 38, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 170, "endOffset": 174}, {"referenceID": 39, "context": "[14], community structure [40], viral diffusion and activity shaping [7], criminology [41], optimization and intervention in social networks [42], recommendation systems [43], and verification of crowd generated data [44].", "startOffset": 217, "endOffset": 221}, {"referenceID": 40, "context": "The model is originally motivated to analyze the earthquake and its aftershocks [45].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "4) Reactive point process [16]: it can be regarded as a generalization of the Hawkes process by adding a selfinhibiting term to account for the inhibiting effects from history events.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "5) Self-correcting process [46]: its background part increases steadily, while it is decreased by a constant e\u2212\u03b1 < 1 every time a new event appears.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "Taking a sequence {x}t=1 as input, the RNN generates the hidden states {h}t=1, also known high-level representation of inputs [24], [25]:", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "Taking a sequence {x}t=1 as input, the RNN generates the hidden states {h}t=1, also known high-level representation of inputs [24], [25]:", "startOffset": 132, "endOffset": 136}, {"referenceID": 42, "context": "problem [47] and poor long-range dependency modeling capability.", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "In contrast, we implement our RNN with Long Short Term Memory (LSTM) [26], [27] for its popularity \u00a9 2013 IBM Research-China 2 2 , t e j j t e , e h1 e h 2 e j h z v z j c", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "In contrast, we implement our RNN with Long Short Term Memory (LSTM) [26], [27] for its popularity \u00a9 2013 IBM Research-China 2 2 , t e j j t e , e h1 e h 2 e j h z v z j c", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "Gated Recurrent Units (GRU) [28] can also be alternative choices, while the analysis of the consequence of this particular choice is not the focus of our paper.", "startOffset": 28, "endOffset": 32}, {"referenceID": 43, "context": "The former can timely affect the transient occurrence intensity of events and the latter can often abruptly cause jump and transition of states of agents and capture longrange event dependency [48].", "startOffset": 193, "endOffset": 197}, {"referenceID": 30, "context": "Empirically, it has been shown in some recent study [34] that using separate RNN for each time series data e.", "startOffset": 52, "endOffset": 56}, {"referenceID": 3, "context": "For example in the problem of multi-dimensional Hawkes process learning, one important goal is to uncover the hidden network structure, infectivity matrix A, from real-world event sequences, such as the influence strength between users in social network [5], [14], or progression relationship between event types [17].", "startOffset": 254, "endOffset": 257}, {"referenceID": 13, "context": "For example in the problem of multi-dimensional Hawkes process learning, one important goal is to uncover the hidden network structure, infectivity matrix A, from real-world event sequences, such as the influence strength between users in social network [5], [14], or progression relationship between event types [17].", "startOffset": 313, "endOffset": 317}, {"referenceID": 19, "context": "Uncovering the hidden structure is also stressed in causal analysis [23], which gives much evidence for prediction result.", "startOffset": 68, "endOffset": 72}, {"referenceID": 32, "context": "Here we choose the widely used soft attention mechanism [36], whose influence from former events is in an additive form [5]:", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "Here we choose the widely used soft attention mechanism [36], whose influence from former events is in an additive form [5]:", "startOffset": 120, "endOffset": 123}, {"referenceID": 33, "context": "Note that hard attention [37] only assigns 0 or 1 to the influence strength \u03b1 zi , which is too rough to capture fine-grained influence.", "startOffset": 25, "endOffset": 29}, {"referenceID": 31, "context": "work [35], [36], [37] in that they only consider one-way effect over the sequence.", "startOffset": 5, "endOffset": 9}, {"referenceID": 32, "context": "work [35], [36], [37] in that they only consider one-way effect over the sequence.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "work [35], [36], [37] in that they only consider one-way effect over the sequence.", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "4 is also used in [36], [37] to model the one-way attention weight \u03b1i = fatt(hi,v).", "startOffset": 18, "endOffset": 22}, {"referenceID": 33, "context": "4 is also used in [36], [37] to model the one-way attention weight \u03b1i = fatt(hi,v).", "startOffset": 24, "endOffset": 28}, {"referenceID": 44, "context": "This is in line with the importance weighting policy for skewed data in machine learning [49].", "startOffset": 89, "endOffset": 93}, {"referenceID": 45, "context": "We adopt RMSprop gradients [50] which have been shown", "startOffset": 27, "endOffset": 31}, {"referenceID": 3, "context": ", Majorization-Minimization techniques [5], [51]) used in generative Point process models.", "startOffset": 39, "endOffset": 42}, {"referenceID": 46, "context": ", Majorization-Minimization techniques [5], [51]) used in generative Point process models.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "recent work [18], another limitation for the generative point process model is that they are aimed to maximize the joint", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "2) Hawkes Process: To enable multi-type event prediction, we use a Multi-dimensional Hawkes process [7], [14].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "3) Recurrent Marked Temporal Point Processes (RMTPP): [6] uses a neural network to model the event dependency flexibly.", "startOffset": 54, "endOffset": 57}, {"referenceID": 16, "context": "This method is the one presented in the conference version of this paper [20].", "startOffset": 73, "endOffset": 77}, {"referenceID": 11, "context": "Poisson Processes [15], [39].", "startOffset": 18, "endOffset": 22}, {"referenceID": 34, "context": "Poisson Processes [15], [39].", "startOffset": 24, "endOffset": 28}, {"referenceID": 47, "context": "from multi-dimensional hawkes process via the Thinning algorithm [52].", "startOffset": 65, "endOffset": 69}, {"referenceID": 12, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "In line with the previous works for information diffusion tracking [14], [42], [53], the public dataset MemeTracker4 is", "startOffset": 73, "endOffset": 77}, {"referenceID": 48, "context": "In line with the previous works for information diffusion tracking [14], [42], [53], the public dataset MemeTracker4 is", "startOffset": 79, "endOffset": 83}, {"referenceID": 49, "context": ", marketing by information maximization [54].", "startOffset": 40, "endOffset": 44}, {"referenceID": 37, "context": "For the experiments here, we use the top 500 media sites with the largest number of documents and select the meme cascades diffuse over them as done in previous works [14], [42].", "startOffset": 173, "endOffset": 177}, {"referenceID": 50, "context": "As the ground truth of network is unknown, we proceed by following the protocol as designed and adopted in [14], [55], [56].", "startOffset": 113, "endOffset": 117}, {"referenceID": 51, "context": "As the ground truth of network is unknown, we proceed by following the protocol as designed and adopted in [14], [55], [56].", "startOffset": 119, "endOffset": 123}, {"referenceID": 52, "context": "In order to visualize the learned network, we use community detection algorithm [57] with resolution 0.", "startOffset": 80, "endOffset": 84}, {"referenceID": 53, "context": "9 [58] over learned directed network of ATRPP, which renders 18 communities.", "startOffset": 2, "endOffset": 6}, {"referenceID": 52, "context": "Similar to MemeTracker, community detection algorithm [57] with resolution 0.", "startOffset": 54, "endOffset": 58}, {"referenceID": 4, "context": "bodiments of the first two blocks can be referred to [14] and [6] respectively.", "startOffset": 62, "endOffset": 65}, {"referenceID": 16, "context": "our conference version [20] and this extended journal paper (from left to right).", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "A variety of real-world processes (over networks) produce sequences of data whose complex temporal dynamics need to be studied. More especially, the event timestamps can carry important information about the underlying network dynamics, which otherwise are not available from the time-series evenly sampled from continuous signals. Moreover, in most complex processes, event sequences and evenly-sampled times series data can interact with each other, which renders joint modeling of those two sources of data necessary. To tackle the above problems, in this paper, we utilize the rich framework of (temporal) point processes to model event data and timely update its intensity function by the synergic twin Recurrent Neural Networks (RNNs). In the proposed architecture, the intensity function is synergistically modulated by one RNN with asynchronous events as input and another RNN with time series as input. Furthermore, to enhance the interpretability of the model, the attention mechanism for the neural point process is introduced. The whole model with event type and timestamp prediction output layers can be trained end-to-end and allows a black-box treatment for modeling the intensity. We substantiate the superiority of our model in synthetic data and three real-world benchmark datasets.", "creator": "LaTeX with hyperref package"}}}