{"id": "1703.08864", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2017", "title": "Learning Simpler Language Models with the Differential State Framework", "abstract": "Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks like language modeling. Existing architectures that address the issue are often complex and costly to train. The Delta Recurrent Neural Network (Delta-RNN) framework is a simple and high-performing design that unifies previously proposed gated neural models. The Delta-RNN models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. This requires hardly any more parameters than a classical simple recurrent network. The models outperform popular complex architectures, such as the Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU) and achieve state-of-the art performance in language modeling at character and word levels and yield comparable performance at the subword level.", "histories": [["v1", "Sun, 26 Mar 2017 20:02:44 GMT  (120kb,D)", "http://arxiv.org/abs/1703.08864v1", null], ["v2", "Thu, 1 Jun 2017 21:19:31 GMT  (125kb,D)", "http://arxiv.org/abs/1703.08864v2", "Updated benchmark experimental results (as well as content). Regularized versions of the Delta-RNN described and evaluated"], ["v3", "Mon, 5 Jun 2017 14:40:47 GMT  (124kb,D)", "http://arxiv.org/abs/1703.08864v3", "Now distinguishing framework from its implementation (the Delta-RNN). Updated benchmark experimental results (as well as content). Regularized versions of the Delta-RNN described and evaluated"], ["v4", "Sun, 16 Jul 2017 22:27:29 GMT  (115kb)", "http://arxiv.org/abs/1703.08864v4", "Edits/revisions applied throughout document"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander g ororbia ii", "tomas mikolov", "david reitter"], "accepted": false, "id": "1703.08864"}, "pdf": {"name": "1703.08864.pdf", "metadata": {"source": "CRF", "title": "Learning Simpler Language Models with the Delta Recurrent Neural Network Framework", "authors": ["Alexander G. Ororbia II", "Tomas Mikolov", "David Reitter"], "emails": [], "sections": [{"heading": null, "text": "Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks like language modeling. Existing architectures that address the issue are often complex and costly to train. The Delta Recurrent Neural Network (Delta-RNN) framework is a simple and highperforming design that unifies previously proposed gated neural models. The DeltaRNN models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. This requires hardly any more parameters than a classical simple recurrent network. The models outperform popular complex architectures, such as the Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU) and achieve state-of-the art performance in language modeling at character and word levels and yield comparable performance at the subword level."}, {"heading": "1 Introduction", "text": "Recurrent neural networks are increasingly popular models for sequential data. The simple recurrent neural network (RNN) architecture (Elman, 1990) is, however, not\nar X\niv :1\n70 3.\n08 86\n4v 1\n[ cs\nsuitable for capturing longer-distance dependencies. Architectures that address this shortcoming included the Long Short-Term Memory (LSTM, Hochreiter and Schmidhuber 1997a), the Gated Recurrent Unit (GRU, Chung et al. 2014, 2015), and the structurally constrained recurrent network (SCRN, Mikolov et al. 2014). While these can capture some longer-term patterns (20 to 50 words), their structural complexity makes it difficult to understand what is going on inside. One exception is the SCRN architecture, which is by design simple to understand. It shows that the memory acquired by the complex LSTM models on language tasks does correlate strongly with simple weighted bagsof-words. This demystifies the abilities of the LSTM model to a degree: while some authors have suggested that the LSTM understands the language and even the thoughts being expressed in sentences (Choudhury, 2015), it is arguable whether this could be said about a model that performs equally well and is based on representations that are essentially equivalent to a bag of words.\nOne property of recurrent architectures that allows for the formation of longer-term memory is the self-connectedness of the basic units: this is most explicitly shown in the SCRN model, where one hidden layer contains neurons that do not have other recurrent connections except to themselves. Still, this architecture has several drawbacks: one has to choose the size of the fully connected and self connected recurrent layers, and the model is not capable of modeling non-linearities in the longer memory part.\nIn this work, we aim to increase representational efficiency, i.e., the ratio of performance to acquired parameters. We simplify the model architecture further and develop several variants under the Delta-RNN framework, where the hidden layer state of the next time step is a function of its current state and the delta change computed by the model. We do not present the Delta-RNN architecture as a model of human memory for language. However, we point out its conceptual origins in Surprisal Theory (Boston et al., 2008; Hale, 2001; Levy, 2008), which posits that the human language processor develops complex expectations of future words, phrases and syntactic choices, and that these expectations and deviations from them (surprisal) guide language processing, e.g., in reading comprehension. How complex the models are (in the human language processor) that form the expectation is an open question. The cognitive literature has approached this with existing parsing algorithms, probabilistic context-free grammars or n-gram language models. We take a connectionist perspective. The Delta-RNN architecture proposes to not just generatively develop expectations and compare them with actual\nstate changes caused by observing new input; it explicitly maintains gates as a form of high-level error correction and interpolation. The model will be evaluated as a language model, and we will not attempt to simulate human performance such as in situations with garden-path sentences that need to be reanalyzed because of costly initial mis-analysis."}, {"heading": "2 The Delta Recurrent Neural Network", "text": "In this section, we will describe the proposed Delta-RNN framework as well as several concrete implementations one can derive from it."}, {"heading": "2.1 General Framework", "text": "The most general formulation of our architecture distinguishes two forms of the hidden state. The first is a fast state, which is generally a function of the data at the current time-step and a filtration (or summary function of past states). The second is a slow state, or data-independent state. This concept can be specifically viewed as a composition of two general functions, formally defined as follows:\nht = q\u0398(xt,Mt\u22121)\n= f\u03c8[g\u03b8(xt,Mt\u22121),Mt\u22121] (1)\nwhere \u0398 = {\u03b8, \u03c8} are the parameters of the state-machine and Mt\u22121 is the previous latent information the model is conditioned on. In the case of most gated architectures, Mt\u22121 = ht\u22121, but in some others, as in the SCRN or the LSTM, Mt\u22121 = {ht\u22121, ct\u22121}1 or could even include information such as de-coupled memory, and in general will be updated as symbols are iteratively processed. We define g\u03b8(\u00b7) to be any, possibly complicated, function that maps the previous hidden state and the currently encountered data point (e.g. a word, subword, or character token) to a real-valued vector of fixed dimensions using parameters \u03b8. f\u03c8(\u00b7), on the other hand, is defined to be the outer function that uses parameters \u03c8 to integrate the fast-state, as calculated by g\u03b8(\u00b7), and the slowly-moving, currently un-transformed state ht\u22121. In the sub-sections that follow, we will describe simple formulations of these two core functions and, later in Section 3,\n1ct refers to the \u201ccell-state\u201d as in (Hochreiter and Schmidhuber, 1997b).\nwe will show how currently popular architectures, like the LSTM and various simplifications, are instantiations of this framework. The specific structure of Equation 1 was chosen to highlight that we hypothesize the reason behind the success of gated neural architectures is largely because they have been treating the next-step prediction tasks, like language modeling, as an interaction between two functions. One inner function focuses on integrating observed samples with a current filtration to create a new datadependent hidden representation (or state \u201cproposal\u2019)\u2019 while an outer function focuses on computing the difference, or \u201cdelta\u201d, between the impression of the sub-sequence observed so far (i.e., ht\u22121) with the newly formed impression. For example, as a sentence is iteratively processed, there might not be much new information (or \u201csuprisal\u201d) in a token\u2019s mapped hidden representation (especially if it is a frequently encountered token), thus requiring less change to the iteratively inferred global representation of the sentence.2 However, encountering a new or rare token (especially an unexpected) might bias the outer function to allow the newly formed hidden impression to more strongly influence the overall impression of the sentence, which will be useful when predicting what the next token/symbol might be. In Section 5, we will present a small demonstration using one of the trained word-models to illustrate the intuition just described.\nIn the sub-sections to follow, we will describe the ways we chose to formulate g\u03b8(\u00b7) and f\u03c8(\u00b7) in the experiments of our paper. The process we followed for developing the concrete implementations of g\u03b8(\u00b7) and f\u03c8(\u00b7) involved starting from the simplest possible form using the fewest (if any) possible parameters to compose each function and testing it in preliminary experiments to verify its usefulness.\nIt is important to note that Equation 1 is still general enough to allow for future design or more clever or efficient functions that might improve the performance and long-term memory capabilities of the Delta-RNN. More importantly, one might view the parameters \u03c8 that f\u03c8(\u00b7) uses as possibly encapsulating structures that can be used to store explicit memory-vectors, as is the case in stacked-based RNNs (Das et al., 1992; Joulin and Mikolov, 2015) or linked-list (Joulin and Mikolov, 2015) data structures.\n2One way to extract a \u201csentence representation\u201d from a temporal neural language model would be to simply to take the last hidden state calculated upon reaching a symbol such as punctuation (e.g., period or exclamation point). This is sometimes referred to as encoding variable-length sentences or paragraphs to a real-valued vector of fixed dimensionality."}, {"heading": "2.2 Forms of the Outer Function", "text": "Keeping g\u03b8(\u00b7) as general as possible, here we will describe several ways one could design f\u03c8(\u00b7), the function meant to decide how new and old hidden representations can be combined at each time step. We will strive to introduce as few additional parameters as necessary and experimental results will confirm the effectiveness of our simple designs.\nOne form that f\u03c8(\u00b7) could take is a simple weighted summation, as follows:\nht = f\u03c8[g\u03b8(xt,ht\u22121),ht\u22121]\n= \u03a6(\u03b3[g\u03b8(xt,ht\u22121) + \u03b2ht\u22121) (2)\nwhere \u03a6(\u00b7) is an element-wise activation applied to the final summation and \u03b3 and \u03b2 are bias vectors meant to weight the fast and slow states respectively. In Equation 2, if \u03b3 = \u03b2 = 1, no additional parameters have been introduced making the outer function simply a rigid summation operator followed by a non-linearity. However, one will notice that ht\u22121 is transmitted across a set of fixed identity connections in addition to being transformed by g\u03b8(\u00b7). While \u03b3 and \u03b2 could be chosen to be hyper-parameters and tuned externally (as sort of per-dimension scalar multipliers), it might prove to be more effective to allow the model to learn these coefficients. If we introduce a vector of parameters r, we can choose the fast and slow weights to be \u03b3 = (1\u2212 r) and \u03b2 = (r), creating a simple, learn-able interpolation mechanism. Adding these negligibly few additional parameters to compose an interpolation mechanism yields the state-model:\nht = \u03a6((1\u2212 r)\u2297 g\u03b8(xt,ht\u22121) + r\u2297 ht\u22121). (3)\nIncorporating this interpolation mechanism can be interpreted as giving the DeltaRNN a flexible mechanism for mixing various dimensions of its longer-term memory with its more localized memory. Interpolation, especially through a simple gating mechanism, can be an effective way to allow the model to learn how to turn on/off latent dimensions, potentially yielding improved generalization performance, such as was empirically shown by Serban et al. (2016).\nBeyond fixing r to some vector of pre-initialized values, there two simple ways to\nparametrize r:\nr = 1/(1 + exp(\u2212br)), or (4) r = 1/(1 + exp(\u2212[Wxt + br])) (5)\nwhere both forms only introduce an additional set of learnable bias parameters, however Equation 5 allows the data at time step t to interact with the gate and thus takes into account additional information from the input distribution when mixing stable and local states together. Unlike Serban et al. (2016), we constrain the rates to lie to in the range [0, 1] by using the logistic link function, \u03c3(v) = 1/(1 + exp(\u2212v)), which will transform the biases into rates much like the rates of the SCRN. We crucially choose to share W in this particular mechanism for two reasons: 1) we avoid adding yet another matrix of input to hidden parameters and, much to our advantage, reuse the computation of the linear pre-activation termWxt, and 2) additionally coupling the data pre-activation to the gating mechanism will serve as further regularization of the input-to-hidden parameters as two error signals, \u2202r \u2202W and \u2202zt \u2202W\n, are used in the calculation of the partial derivative \u2202L(yt,xt+1)\n\u2202W (yt is the output of the model at t). Figure 1 depicts the architecture using the simple late-integration mechanism."}, {"heading": "2.3 Forms of the Inner Function", "text": "When a concrete form of the inner function g\u03b8(\u00b7) is chosen, we can fully specify the Delta-RNN architecture. We will also show, in Section 3, how many other commonlyused RNN architectures can, in fact, be treated as special cases of this general framework defined under Equation 1.\nStarting from Equation 3, if we fix \u03b3 = 1 and \u03b2 = 0, we can recover the classical Elman RNN, where g\u03b8(xt,ht\u22121) is a linear combination of the projection of the current data point and the projection of the previous hidden state, followed by a non-linearity \u03c6(\u00b7). However, if we also set \u03b2 = 1, we obtain a naive way to compute a delta change of states. Specifically, the simple-RNN\u2019s hidden state, where \u03a6(v) = v (the identity function), is:\nht = \u03b3 \u2297 g\u03b8(xt,ht\u22121) + \u03b2 \u2297 ht\u22121 ht = 1\u2297 \u03c6(V ht\u22121 +Wxt + b) + 0\u2297 ht\u22121\nht = \u03c6(V ht\u22121 +Wxt + b) (6)\nwhere ht is the hidden layer state at time t, xt is the input vector, and \u03b8 = {W,V } contains the weight matrices. In contrast, the simple Delta-RNN, where instead \u03c6(v) = v, we have:\nht = \u03a6(V ht\u22121 +Wxt + b + ht\u22121). (7)\nThus, the state can be implicitly stable, assuming W and V are initialized with small values and \u03c6(\u00b7) allows this by being partially linear. For example we can choose \u03c6(\u00b7) to be the linear rectifier (or initialize the model so to start out in the linear regime of the hyperbolic tangent). In this case, the Delta-RNN network does not need to learn anything to maintain the state constant over time.\nPreliminary experimentation with this simple form (Equation 7) often yielded unsatisfactory performance. This further motivated the development of the simple interpolation mechanism presented in Equation 3. However, depending on how one chooses the non-linearities, \u03c6(\u00b7) and \u03a6(\u00b7), one can create different types of interpolation. Using an Elman RNN for g\u03b8(xt,ht\u22121) as in Equation 6, substituting into Equation 3 can create what we propose as the \u201clate-integration\u201d state model:\nzt = g\u03b8(xt,ht\u22121)\n= \u03c6(V ht\u22121 +Wxt + b), and, (8) ht = \u03a6((1\u2212 r)\u2297 zt + r\u2297 ht\u22121). (9)\nwhere \u03a6(\u00b7) could be any choice of activation function, including the identity function. This form of interpolation allows for a more direct propagation pathway for error since gradient information, once transmitted through the interpolation gate, has two pathways: through the non-linearity of the local state (through g\u03b8(xt,ht\u22121)) and the pathway composed of implicit identity connections.3\nWhen using a simple Elman RNN, we have essentially described a first-order DeltaRNN. However, historically, second-order recurrent neural architectures have been shown to be powerful models in tasks such as grammatical inference (Giles et al., 1991) and noisy time-series prediction (Giles et al., 2001) as well as incredibly useful in rule-extraction when treated as finite-state automata (Giles et al., 1992; Goudreau et al.,\n3Late-integration might remind the reader of the phrase \u201clate fusion\u201d, as in the context of Wang and Cho (2015). However, Wang and Cho was focused on merging the information from an external bag-of-words context vector with the standard cell state of the LSTM.\n1994). Very recently, (Wu et al., 2016) showed that the gating effect between the statedriven component and data-driven components of a layer\u2019s pre-activations facilitated better propagation of gradient signals as opposed to the usual linear combination. A second-order version of g\u03b8(xt,ht\u22121) would be highly desirable, not only because it further mitigates the vanishing gradient problem that plagues back-propagation through time (used in calculating parameter gradients of neural architectures), but because the form introduces negligibly few additional parameters. We do note that the second-order form we use, like in Wu et al. (2016), is a rank-1 matrix approximation of the actual tensor used in Giles et al. (1992); Goudreau et al. (1994).\nWe can take the late-integration model, Equation 9, and replace zt with (Giles et al.,\n1991):\nzt = \u03c6(V ht\u22121 \u2297Wxt + b) (10)\nor a more general form (Wu et al., 2016):\nd1t = \u03b1\u2297 Vddt\u22121 \u2297Wxt d2t = \u03b21 \u2297 Vddt\u22121 + \u03b22 \u2297Wxt\nzt = \u03c6(d 1 t + d 2 t + b), (11)\nwhere we note that zt can be a function of any arbitrary incoming set of information signals that are gated by the last known state. The Delta-RNN will ultimately combine this data-driven signal zt with its slow-moving state. More importantly, observe that\neven in the most general form (Equation 11), only a few further bias vector parameters, \u03b1, \u03b21, and \u03b22 are required.\nAssuming a single hidden layer language model, with H hidden units and V input units (where V corresponds to cardinality of the symbol dictionary), a full late-integration Delta-RNN that employs a second-order g\u03b8(xt,ht\u22121) (Equation 11), has only ((H \u2217H)+ 2(H \u2217 V ) + 5H + V ) parameters 4, which is only slightly larger than a classical RNN with only ((H \u2217H) + 2(H \u2217 V ) +H + V ) parameters. This stands in stark contrast to the sheer number of parameters required to train commonly-used complex architectures such as the LSTM (with peephole connections), with (4(H \u2217H) + 8(H \u2217 V ) + 4H + V ) parameters, and the GRU, with (3(H \u2217H) + 4(H \u2217 V ) + 3H + V ) parameters."}, {"heading": "2.4 Learning under the Delta-RNN", "text": "Let w1, . . . , wN be a variable-length sequence of N symbols (such as words that would compose a sentence). In general, the distribution over the variables follows the graphical model:\nP\u03b8(w1, . . . , wT ) = T\u220f t=1 P\u0398(wt|w<t), (12)\nwhere \u0398 = {\u03c6, \u03b8} = {V,W,R,b,br, \u03b1, \u03b21, \u03b22} are the model parameters (of a full Delta-RNN).\nNo matter how the hidden state ht is calculated, in this paper, it will ultimately be\nfed into a maximum-entropy classifier 5 defined as:\nP (w,ht) = P\u0398(w|ht) = exp (wTRht)\u2211 w\u2032 exp (w TRht) , (13)\nTo learn parameters for any of our models, we optimize with respect to the sequence negative log likelihood:\nL = \u2212 N\u2211 i=1 T\u2211 t=1 logP\u0398(wt|h), (14)\nModel parameters, \u0398 = {\u03b8, \u03c8}, of the Delta-RNN are learned under an empirical risk minimization framework. We employ back-propagation of errors (or rather, reversemode automatic differentiation with respect to this negative log likelihood objective\n45H counts the hidden bias, the full interpolation mechanism rt (Equation 5), and the second-order biases, {\u03b1, \u03b21, \u03b22}.\n5Note that the bias term has been omitted for clarity.\nfunction) to calculate gradients and update the parameters using the method of steepest gradient descent. For all experiments conducted in this paper, we found that the ADAM adaptive learning rate scheme (Kingma and Ba, 2014) followed by a Polyak average (Polyak and Juditsky, 1992) yielded the most consistent and near-optimal performance. We therefore we use this set-up for optimization of parameters for all models (including baselines), unless otherwise mentioned. For all experiments, we unroll computation graphs T steps in time (where T varies across experiments/tasks), and, in order to approximate full back-propagation through time, we carry over the last hidden from the previous mini-batch (within a full sequence). More importantly, we found that by furthermore using the derivative of the loss with respect to the last hidden state, we can improve the approximation and thus perform one step of iterative inference 6 to update the last hidden state carried over. We thus use this proposed improved approximation for all models in the experiments.\nIn the experiments of this paper, we compare our proposed models against a wide variety of un-regularized baselines (to focus on the impact that the architecture has on performance, keeping this separate from regularization). These baselines include the LSTM, GRU, and SCRN as well as computationally more efficient formulations of each, such as the MGU. The goal is to see if our proposed Delta-RNN is a suitable replacement for complex gated architectures and can capture longer term patterns in sequential text data."}, {"heading": "3 Related Work: Recovering Previous Models", "text": "A contribution of this work is that our general framework, presented in Section 2.1, offers a way to unify previous proposals for gated neural architectures (especially for use in next-step prediction tasks like language modeling) and explore directions of improvement. Since we will ultimately compare our proposed Delta-RNN of Section 2.3 to these architectures, we will next present how to derive several key architectures from our general form, such as the Gated Recurrent Unit and the Long Short Term Memory. More importantly, we will introduce them in the same notation / design as the Delta-RNN and highlight the differences between previous work and our own through\n6We searched the step-size \u03bb over the values {0.05, 0.1, 0.15} for all experiments in this paper.\nthe lens of f\u03c8(\u00b7) and g\u03b8(xt,Mt\u22121). Simple models, largely based on the original Elman RNN (Elman, 1990), have often been shown to perform quite well in language modeling tasks (Mikolov et al., 2010, 2011). The structurally constrained recurrent network (SCRN, Mikolov et al. 2014), an important predecessor and inspiration for this work, showed that one fruitful path to learning longer-term dependencies was to impose a hard constraint on how quickly the values of hidden units could change design units, yielding more \u201cstable\u201d long-term memory. The SCRN itself is very similar to a combination of RNN architectures of (Jordan, 1986; Mozer, 1993). The key element of its design is the constraint that part of recurrent weight matrix must be close to the identity, a constraint that is also satisfied by the Delta-RNN. These identity connections (and corresponding context units that use them) allow for improved information travel over many time-steps and can even be viewed as an exponential trace memory (Mozer, 1993), Residual networks, though feedforward in nature, also share a similar motivation, as in He et al. (2016). Unlike the SCRN, the proposed Delta-RNN does not require a separation of the slow and fast moving units, but instead models this slower time-scale through implicitly stable states.\nThe Long Short Term Memory (LSTM, Hochreiter and Schmidhuber 1997a) is arguably the currently most popular and often-used gated neural architecture, especially in the domain of Natural Language Processing. Starting from our general form, Equation 1, we can see how the LSTM can be deconstructed, where setting ct = g\u03b8(xt,Mt\u22121), yields:\nht = f\u03c8[g\u03b8(xt,Mt\u22121),Mt\u22121] ht = rt \u2297 \u03a6(ct), where, (15)\nrt = \u03c3(Wrxt + Vrht\u22121 + Urct + br) (16)\nwhere Mt\u22121 = {ht\u22121, ct\u22121}, noting that ct\u22121 is the cell-state designed to act as the constant error carousal in mitigating the problem of vanishing gradients when using back-propagation through time. A great deal of recent work has attempted to improve the training of the LSTM, often by increasing its complexity, such as through the introduction of so-called \u201cpeephole connections\u201d (Gers and Schmidhuber, 2000). To compute ct = g\u03b8(xt,Mt\u22121), using peephole connections, we use the following set of\nequations:\nct = ft \u2297 ct\u22121 + it \u2297 zt, where,\nzt = \u03a6(Wzxt + Vzht\u22121 + bz),\nit = \u03c3(Wixt + Viht\u22121 + Uict\u22121 + bi),\nft = \u03c3(Wfxt + Vfht\u22121 + Ufct\u22121 + bf ).\nThe Gated Recurrent Unit (GRU, Chung et al. 2014, 2015) can be viewed as one of the more successful attempts to simplify the LSTM. We see that f\u03c8(\u00b7) and g\u03b8(\u00b7) are still quite complex, requiring many intermediate computations to reach an output. In the case of the outer mixing function, f\u03c8(\u00b7), we see that:\nht = \u03a6(\u03b3[g\u03b8(xt,ht\u22121) + \u03b2ht\u22121)\n\u03b3 = rt and \u03b2 = (1\u2212 rt), where, (17)\nrt = \u03c3(Vrht\u22121 +Wrxt + br) (18)\nnoting that the state gate rt is also a function of the RNN\u2019s previous hidden state and introduces parameters specialized for r. In contrast, the Delta-RNN does not use an extra set of input-to-hidden weights, and more directly, the pre-activation of the input projection can be reused for the interpolation gate. The inner function of the GRU, g\u03b8(xt,ht\u22121), is defined as:\ng\u03b8(xt,ht\u22121) = \u03c6(Vh(qt \u2297 ht\u22121) +Whxt + bh)\nqt = \u03c3(Vqht\u22121 +Wqxt + bq)\nwhere \u03c6() is generally set to be the hyperbolic tangent activation function. A mutated architecture (MUT, Jozefowicz et al. 2015) was an attempt to simplify the GRU somewhat, as, much like the Delta-RNN, its interpolation mechanism is not a function of the previous hidden state but is still largely as parameter-heavy as the GRU, only shedding a single extra parameter matrix, especially since its interpolation mechanism retains a specialized parameter matrix to transform the data. The Delta-RNN, on the other hand, shares this with its primary calculation of the data\u2019s pre-activation values. The Minimally Gated Unit (MGU, Zhou et al. 2016) is yet a further attempt to reduce the complexity of the GRU by merging its reset and update gates into a single forget gate, essentially using the same outer function under the GRU defined in Equation 18, but simplifying the inner\nfunction g\u03b8(xt,ht\u22121) to be quite close to the Elman-RNN but conditioned on the forget gate as follows:\ng\u03b8(xt,ht\u22121) = \u03c6(Vh(rt \u2297 ht\u22121) +Whxt + bh).\nWhile the MGU certainly does reduce the number of parameters, viewing it from the perspective of our general Delta-RNN framework, one can see that it still largely uses a g\u03b8(xt,ht\u22121) that is rather limited (only the capabilities of the Elman-RNN). The most effective version of our Delta-RNN emerged from the insight that a more powerful g\u03b8(xt,ht\u22121) could be obtained by merely increasing its order (only requiring a few more bias parameters) and nesting it within a non-linear interpolation mechanism that will compute the delta-states. Our framework is general enough to also allow designers to incorporate functions that augment the general state-engine with an external memory to create architectures that can exploit the strengths of models with decoupled memory architectures (Weston et al., 2014,?; Sukhbaatar et al., 2015; Graves et al., 2016; ?) or data-structures that server as memory (Sun et al., 1998; Joulin and Mikolov, 2015).\nA final related, but important, strand of work uses depth (i.e., number of processing layers) to directly model various time-scales, as emulated in models such as the hierarchical/multi-resolutional recurrent neural network (HM-RNN) (Chung et al., 2016). Since the Delta-RNN is designed to allow its interpolation gate r to be driven by the data, it is possible that the model might already be learning how to make use of boundary information (word boundaries at the character/sub-word level, sentence boundaries as marked by punctuation at the word-level). The HM-RNN, however, more directly attacks this problem by modifying an LSTM to learn how to manipulate its states when certain types of symbols are encountered. (This is different from models like the Clockwork RNN that require explicit boundary information (Koutnik et al., 2014).) One way to take advantage of the ideas behind the HM-RNN would be to manipulate the Delta-RNN framework to incorporate the explicit modeling of time-scales through layer depth (each layer is responsible for modeling a different time-scale0 as well as investigate how the HM-RNN\u2019s performance would change when built from modifying a Delta-RNN instead of an LSTM."}, {"heading": "4 Experimental Results", "text": "Language modeling is an incredibly important next-step prediction task, with applications in downstream tasks in speech recognition, parsing, and information retrieval. As such, we will focus this paper on experiments on this task domain to gauge the efficacy of our Delta-RNN framework, noting that the Delta-RNN framework might prove useful in other tasks including machine translation (Bahdanau et al., 2014) and light chunking (Turian et al., 2009). Beyond improving language modeling performance, the sentence (and document) representations iteratively inferred by our architectures might also prove useful in composing higher-level representations of text corpora, a subject we will investigate in future work."}, {"heading": "4.1 Datasets", "text": ""}, {"heading": "4.1 The Penn Treebank Corpus", "text": "The Penn Treebank corpus (Marcus et al., 1993) is often used to benchmark both word and character-level models via perplexity or bits-per-character, and thus we start here. 7 The corpus contains 42,068 sentences (971,657 tokens, average token-length of about 4.727 characters) of varying length (the range is from 3 to 84 tokens, at the word-level)."}, {"heading": "4.2 The IMDB Corpus", "text": "The large sentiment analysis corpus (Maas et al., 2011) is often used to benchmark algorithms for predicting the positive or negative tonality of documents. However, we opt to use this large corpus (training consists of 149,714 documents, 1,875,523 sentences, 40,765,697 tokens, average token-length is about 3.4291415 characters) to evaluate our proposed Delta-RNN as a language model. The IMDB data-set serves as a case when the context extends beyond the sentence-level in the form of actual documents."}, {"heading": "4.2 Word & Character-Level Benchmark", "text": "The first set of experiments allow us to examine our proposed Delta-RNN models against reported state-of-the-art models. These reported measures have been on traditional word\n7To be directly comparable with previously reported results, we make use of the specific pre-processed train/valid/test splits found at http://www.fit.vutbr.cz/\u223cimikolov/rnnlm/.\nand character-level language modeling tasks\u2013we measure the per-symbol perplexity of models. For the word-level models, we calculate the per-word perplexity (PPL) using\nthe measure PPL = exp [ \u2212 (1/N) \u2211N i=1 \u2211T t=1 logP\u0398(wt|h) ] . For the character-level models, we report the standard bits-per-character (BPC), which can be calculated from\nthe log likelihood using the formula: BPC = \u22121/(N log(2)) \u2211N\ni=1 \u2211T t=1 logP\u0398(wt|h).\nOver 20 epochs, word-level models were updated every 30 steps (i.e., unfolding length was 30) with mini-batches of 20 (padded) sequences. A simple gridsearch was performed to tune the learning rate, \u03bb = {0.002, 0.001, 0.0005, 0.0002}, as well as the size of the hidden layer H = {500, 1000, 1500}. Parameters (nonbiases) were initialized from zero-mean Gaussian distributions with variance tuned,\n\u03c3 = {0.1, 0.01, 0.005, 0.001}8. The character-level models, on the other hand, were updated every 50 steps using mini-batches of 20 samples over 50 epochs. The parameter initializations and grid-search for the learning rate and hidden layer size were the same as for the word models, with the exception of the hidden layer size, which was searched over H = {500, 1000, 1500, 2000}9. A simple learning rate decay schedule was employed: if the validation loss did not decrease after a single epoch, the learning rate was halved (unless a lower bound on the value had been reached).\nThe standard vocabulary for the word-level models contains 10K unique words (including an unknown token for out-of-vocabulary symbols and an end-of-sequence token) and the standard vocabulary for the character-level models includes 49 unique characters (including a symbol for spaces). We use a special \u201cnull\u201d token (or zero-vector) to mark the start of a sequence. Results for the word-level models are reported in Table 1 and results for the character-level models are reported in Table 1."}, {"heading": "4.3 Sub-word Language Modeling", "text": "We chose to measure the negative log likelihood of the various architectures in the task of subword modeling. Subwords are particularly appealing not only in that the input distribution is of lower dimensionality but, as evidenced by the positive results of Mikolov et al. (2012), sub-word/character hybrid language models improve over the performance of pure character-level models. Sub-word models also enjoy the advantage held by character-level models when it comes to handling out-of-vocabulary words, avoiding the need for an \u201cunknown\u201d token. Research in psycholinguistics has long suggested that even human infants are sensitive to word boundaries at an early stage (e.g., Aslin et al. 1998), and that morphologically complex words enjoy dedicated\n8We also experimented with other initializations, most notably the identity matrix for the recurrent weight parameters as in (Le et al., 2015). We found that this initialization often worsened performance. For the activation functions of the first-order models, we experimented with the linear rectifier, the parametrized linear rectifier, and even our own proposed parametrized smoothened linear rectifier, but found such activations lead to less-than-satisfactory results. The results of this inquiry is documented in the code that will accompany the paper. 9Note that H = 2000 would yield nearly 4 million parameters, which was our upper bound on total number of parameters allowed for experiments in order to be commensurable with the work of Wu et al. (2016), which actually used H = 2048 for all Penn Treebank models.\nprocessing mechanisms (Baayen and Schreuder, 2006). Subword-level language models may approximate such an architecture. Consistency in subword formation is critical in order to obtain meaningful results (Mikolov et al., 2012). Thus, we design our sub-word algorithm to partition a word according to the following scheme:\n1. Split on vowels (using a predefined list)\n2. Link/merge each vowel with a consonant to the immediate right if applicable\n3. Merge straggling single characters to subwords on the immediate right unless a\nsubword of shorter character length is to the left.\nThis simple partitioning scheme was designed to ensure that no subword was shorter than two characters in length. Future work will entail designing a more realistic subword partitioning algorithm. Subwords below a certain frequency were discarded, and combined with 26 single characters to create the final dictionary. For Penn Treebank, this yields a vocabulary of 2405 symbols was created (2,378 subwords + 26 characters + 1 end-token). For the IMDB corpus, after replacing all emoticons and special non-word symbols with special tokens, creates a dictionary of 1926 symbols (1899 subwords + 26 single characters + 1 end-token). Results for all sub-word models are reported in Tables 2.\nSpecifically, we test our implementations of the LSTM 10 (with peephole connections as described in Graves 2013), the GRU, the MGU, the SCRN, as well as a classical Elman network, of both 1st and 2nd-order (Giles et al., 1991; Wu et al., 2016).11 Subword\n10We experimented with initializing the forget gate biases of all LSTMs with values searched over {1, 2, 3} since previous work has shown this can improve model performance. 11We will publicly release code to build and train the architectures in this paper upon publication.\nmodels were trained in a similar fashion as the character-level models, updated every 50 steps of mini-batches of 20 samples but over 30 epochs. Learning rates were tuned in the same fashion as the word-level models, and the same parameter initialization schemes were explored. The notable difference between this experiment and the previous ones is that we fix the number of parameters for each model to be equivalent to that of an LSTM with 100 hidden units for PTB and 50 hiddens units for IMDB. This ensures a controlled, fair comparison across models and allows us to evaluate if the Delta-RNN can learn similarly to models with more complicated processing elements (an LSTM cell versus a GRU cell versus a Delta-RNN unit). We are currently running larger versions of the models depicted in the table above to determine if the results hold at scale."}, {"heading": "5 Discussion", "text": "With respect to the word and character-level benchmarks, we see that the Delta-RNN outperforms all previous, un-regularized state-of-the-art models. As shown in Table 2, we further trained a second-order, word-level RNN (MI-RNN) to complete the comparison, and remark that the second-order connections appear to be quite useful in general, outperforming the SCRN and coming close to that of the LSTM. This extends the results of Wu et al. (2016) to the word-level. However, the Delta-RNN, which also makes use of second-order units within its inner function, ultimately offers the best performance and performs better than the LSTM in all experiments. In both Penn Treebank and IMDB subword language modeling experiments, the Delta-RNN is competitive with complex architectures such as the GRU and the MGU. In both cases, the Delta-RNN nearly reaches the same performance as the best performing baseline model in either data-set (i.e., it nearly reaches the same performance as the GRU on Penn Treebank and the MGU on IMDB). Surprisingly, on IMDB, a simple Elman network is quite performant, even outperforming the MI-RNN. We argue that this might be the result of constraining all neural architectures to only a small number of parameters for such a large data-set, of which we intend to investigate further.\nThe Delta-RNN is far more efficient than a complex LSTM. Moreover, it appears to learn how to make appropriate use of its interpolation mechanism to decide how and when\nto update its hidden state in the presence of new data.12 Given our derivations in Section 3, one could argue that nearly all previously proposed gated neural architectures are essentially trying do the same thing under the Delta-RNN framework. The key advantage of the Delta-RNN is that it is simply an architecture that offers this functionality directly and cheaply (in terms of required parameters)\nIt is important to contrast these results to those that use some form of regularization.\nZaremba et al. (2014) reported that a single LSTM (for word-level Penn Treebank) can reach a PPL of \u223c 80, but this was achieved via dropout regularization (Srivastava et al., 2014). There is a strong relationship between using dropout and training an ensemble of models. Thus, one can argue that a single model trained with dropout actually is not a single model, but an implicit ensemble (see also Srivastava et al. 2014). An ensemble of twenty simple RNNs and cache models did previously reach PPL as low as 72, while a single RNN model gives only 124 (Mikolov, 2012). Zaremba et al. (2014) trained an ensemble of 38 LSTMs regularized with dropout, each with 100x times more parameters than the RNNs used in (Mikolov, 2012), achieving PPL 68. This is arguably a small improvement over 72, and seems to strengthen our claim that dropout is an implicit model ensemble and thus should not be used when one wants to report the performance of a single model. However, the Delta-RNN is amenable to regularization, including drop-out. Results could be even further improved if maximal ensemble performance is the goal.\nWhat is the lesson to be learned from the Delta-RNN framework? First, and foremost, we can obtain strong performance in language modeling with a simpler, more efficient (in terms of number of parameters), and thus faster architecture. Second, the Delta-RNN is designed from the interpretation that the computation of the next hidden state is the result of a composition of two functions. One inner function decides how to \u201cpropose\u201d a new hidden state while the outer function decides how to use this new proposal in updating the previously calculated state. The data-driven interpolation mechanism is used by the model to decide how much impact the newly proposed state has in updating what is likely to be a slowly changing representation. The SCRN, which could be viewed as the predecessor to the Delta-RNN framework, was designed with the idea\n12At greater computational cost, a somewhat lower perplexity for an LSTM may be attainable, such as the perplexity of 107 reported by Sundermeyer (2016) (see Table 1). However, this requires many more training epochs and precludes batch training.\nthat some constrained units could serve as a sort of cache meant to capture longer-term dependencies. Like the SCRN, the Delta-RNN is designed to help mitigate the problem of vanishing gradients, and through the interpolation mechanism, has multiple pathways through which the gradient might be carried, boosting the error signal\u2019s longevity down the propagation path through time. However, the SCRN combines the slow-moving and fast-changing hidden states through a simple summation and thus cannot model non-linear interactions between its shorter and longer term memories, furthermore requiring tuning of the sizes of these separated layers. On the other hand, the Delta-RNN, which does not require special tuning of an additional hidden layer, can non-linearly combine the two types of states in a data-dependent fashion, possibly allowing the model to exploit boundary information from text, which is quite powerful in the case of documents. The key intuition is that the gating mechanism only allows the state proposal to effect the maintained memory state only if the currently observed data-point carries any useful information, warranting a comparison, albeit indirect, to Surprisal Theory. This information proves useful in iteratively forming a sentence impression that will help to better predict the words that come later.\nWith respect to the last point made, we briefly examine the evolution of a trained Delta-RNN\u2019s hidden state across several sample sentences. The first two sentences are hand-created (constrained to use only the vocabulary of Penn Treebank) while the last one is sampled from the Penn Treebank training split. Since the Delta-RNN iteratively processes symbols of an ordered sequence, we measure the L1 norm across consecutive pairs of hidden states. We report the (min-max) normalized L1 scores in Figure 2 and observe that, in accordance with our initial intuition, we can see that the L1 norm is lower for high-frequency words (indicating a smaller delta) such as \u201cthe\u201d or \u201cof\u201d or \u201cis\u201d, which are words that are generally less informative about the general subject or topic of a sentence/document. As this simple qualitative demonstration illustrates, the Delta-RNN does appear to learn what to do with its internal state in the presence of low-information content symbols/tokens."}, {"heading": "6 Conclusions", "text": "We present a different way of viewing computation in recurrent networks. Instead of recomputing the whole state from scratch at every time step, the Delta-RNN only\nlearns how to update the current state. This seems to be better suited for many types of problems, especially those that involve longer term patterns where part of the recurrent network\u2019s state should be constant most of the time. Comparison to the currently widely popular LSTM and GRU architectures shows that the Delta-RNN can achieve similar or better performance on language modeling tasks, while being conceptually much simpler and with far less parameters. Comparison to Structurally Constrained Recurrent Network (SCRN), which shares many of the main ideas and motivation, shows better accuracy and a simpler model architecture (since, in the SCRN, tuning the sizes of two separate hidden layers is required, and this model cannot learn non-linear interactions within its longer memory).\nFuture work includes larger-scale language modeling experiments to test the efficacy of the Delta-RNN framework as well as architectural variants that employ decoupled memory. In addition, we intend to explore how useful the Delta-RNN might be in other tasks that the architectures such as the LSTM currently hold state-of-the-art performance in. Finally, it would be useful to explore if Delta-RNN\u2019s simpler, faster design can speed up the performance of grander architectures, such as the Differentiable Neural Computer Graves et al. (2016)."}, {"heading": "Acknowledgments", "text": "We thank C. Lee Giles and Prasenjit Mitra for their advice. We thank NVIDIA for providing GPU hardware that supported this paper. A.O. was funded by a NACMESloan scholarship; D.R. acknowledges funding from NSF IIS-1459300."}], "references": [{"title": "Computation of conditional", "author": ["R.N. Aslin", "J.R. Saffran", "E.L. Newport"], "venue": null, "citeRegEx": "Aslin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Aslin et al\\.", "year": 1998}, {"title": "probability statistics by 8-month-old infants", "author": ["R.H. Baayen", "R. Schreuder"], "venue": "Psychological Science,", "citeRegEx": "Baayen and Schreuder,? \\Q2006\\E", "shortCiteRegEx": "Baayen and Schreuder", "year": 2006}, {"title": "Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus", "author": ["M.F. Boston", "J. Hale", "R. Kliegl", "U. Patil", "S. Vasishth"], "venue": "Journal of Eye Movement Research, 2(1).", "citeRegEx": "Boston et al\\.,? 2008", "shortCiteRegEx": "Boston et al\\.", "year": 2008}, {"title": "Thought vectors: Bringing common sense to artificial intelligence", "author": ["V. Choudhury"], "venue": null, "citeRegEx": "Choudhury,? \\Q2015\\E", "shortCiteRegEx": "Choudhury", "year": 2015}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["J. Chung", "S. Ahn", "Y. Bengio"], "venue": "arXiv preprint arXiv:1609.01704.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Gated feedback recurrent neural networks", "author": ["J. Chung", "C. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1502.02367.", "citeRegEx": "Chung et al\\.,? 2015", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Learning context-free grammars: Capabilities and limitations of a recurrent neural network with an external stack memory", "author": ["S. Das", "C.L. Giles", "Sun", "G.-Z."], "venue": "Proceedings of the 14th Annual Conference of the Cognitive Science Society, page 14.", "citeRegEx": "Das et al\\.,? 1992", "shortCiteRegEx": "Das et al\\.", "year": 1992}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, 14(2):179\u2013211.", "citeRegEx": "Elman,? 1990", "shortCiteRegEx": "Elman", "year": 1990}, {"title": "Recurrent nets that time and count", "author": ["F.A. Gers", "J. Schmidhuber"], "venue": "Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks, volume 3, pages 189\u2013194. IEEE.", "citeRegEx": "Gers and Schmidhuber,? 2000", "shortCiteRegEx": "Gers and Schmidhuber", "year": 2000}, {"title": "Secondorder recurrent neural networks for grammatical inference", "author": ["C.L. Giles", "D. Chen", "C. Miller", "H. Chen", "G. Sun", "Y. Lee"], "venue": "International Joint Conference on Neural Networks, volume 2, pages 273\u2013281.", "citeRegEx": "Giles et al\\.,? 1991", "shortCiteRegEx": "Giles et al\\.", "year": 1991}, {"title": "Noisy time series prediction using recurrent neural networks and grammatical inference", "author": ["C.L. Giles", "S. Lawrence", "A.C. Tsoi"], "venue": "Machine Learning, 44(1-2):161\u2013 183.", "citeRegEx": "Giles et al\\.,? 2001", "shortCiteRegEx": "Giles et al\\.", "year": 2001}, {"title": "Learning and extracting finite state automata with second-order recurrent neural networks", "author": ["C.L. Giles", "C.B. Miller", "D. Chen", "Chen", "H.-H.", "Sun", "G.-Z.", "Lee", "Y.-C."], "venue": "Neural Computation, 4(3):393\u2013405. 23", "citeRegEx": "Giles et al\\.,? 1992", "shortCiteRegEx": "Giles et al\\.", "year": 1992}, {"title": "First-order versus second-order single-layer recurrent neural networks", "author": ["M.W. Goudreau", "C.L. Giles", "S.T. Chakradhar", "D. Chen"], "venue": "IEEE Transactions on Neural Networks, 5(3):511\u2013513.", "citeRegEx": "Goudreau et al\\.,? 1994", "shortCiteRegEx": "Goudreau et al\\.", "year": 1994}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves,? 2013", "shortCiteRegEx": "Graves", "year": 2013}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska-Barwi\u0144ska", "S.G. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J Agapiou"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2016}, {"title": "Noisy activation functions", "author": ["C. Gulcehre", "M. Moczulski", "M. Denil", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.00391.", "citeRegEx": "Gulcehre et al\\.,? 2016", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "A probabilistic earley parser as a psycholinguistic model", "author": ["J. Hale"], "venue": "Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, NAACL \u201901, pages 1\u20138, Stroudsburg, PA, USA.", "citeRegEx": "Hale,? 2001", "shortCiteRegEx": "Hale", "year": 2001}, {"title": "Identity mappings in deep residual networks", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "European Conference on Computer Vision, pages 630\u2013645. Springer.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997a", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Lstm can solve hard long time lag problems", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, pages 473\u2013479.", "citeRegEx": "Hochreiter and Schmidhuber,? 1997b", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Variable computation in recurrent neural networks", "author": ["Y. Jernite", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": "arXiv preprint arXiv:1611.06188.", "citeRegEx": "Jernite et al\\.,? 2016", "shortCiteRegEx": "Jernite et al\\.", "year": 2016}, {"title": "Attractor dynamics and parallellism in a connectionist sequential machine", "author": ["M.I. Jordan"], "venue": null, "citeRegEx": "Jordan,? \\Q1986\\E", "shortCiteRegEx": "Jordan", "year": 1986}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["A. Joulin", "T. Mikolov"], "venue": "Advances in Neural Information Processing Systems, pages 190\u2013198.", "citeRegEx": "Joulin and Mikolov,? 2015", "shortCiteRegEx": "Joulin and Mikolov", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["R. Jozefowicz", "W. Zaremba", "I. Sutskever"], "venue": "37. 24", "citeRegEx": "Jozefowicz et al\\.,? 2015", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba,? 2014", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "A clockwork rnn", "author": ["J. Koutnik", "K. Greff", "F. Gomez", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1402.3511.", "citeRegEx": "Koutnik et al\\.,? 2014", "shortCiteRegEx": "Koutnik et al\\.", "year": 2014}, {"title": "Zoneout: Regularizing rnns by randomly preserving hidden activations", "author": ["D. Krueger", "T. Maharaj", "J. Kram\u00e1r", "M. Pezeshki", "N. Ballas", "N.R. Ke", "A. Goyal", "Y. Bengio", "H. Larochelle", "A Courville"], "venue": "arXiv preprint arXiv:1606.01305", "citeRegEx": "Krueger et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krueger et al\\.", "year": 2016}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Q.V. Le", "N. Jaitly", "G.E. Hinton"], "venue": "arXiv preprint arXiv:1504.00941.", "citeRegEx": "Le et al\\.,? 2015", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Expectation-based syntactic comprehension", "author": ["R. Levy"], "venue": "Cognition, 106(3):1126 \u2013 1177.", "citeRegEx": "Levy,? 2008", "shortCiteRegEx": "Levy", "year": 2008}, {"title": "Learning word vectors for sentiment analysis", "author": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACLHLT2011, pages 142\u2013150, Portland, Oregon, USA. Association for Computational Linguistics.", "citeRegEx": "Maas et al\\.,? 2011", "shortCiteRegEx": "Maas et al\\.", "year": 2011}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics, 19(2):313\u2013 330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Statistical Language Models Based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis.", "citeRegEx": "Mikolov,? 2012", "shortCiteRegEx": "Mikolov", "year": 2012}, {"title": "Learning longer memory in recurrent neural networks", "author": ["T. Mikolov", "A. Joulin", "S. Chopra", "M. Mathieu", "M. Ranzato"], "venue": "arXiv preprint arXiv:1412.7753.", "citeRegEx": "Mikolov et al\\.,? 2014", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Interspeech, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. \u010cernock\u1ef3", "S. Khudanpur"], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5528\u20135531. IEEE. 25", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Subword language modeling with neural networks", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "Le", "H.-S.", "S. Kombrink", "J. Cernocky"], "venue": "preprint.", "citeRegEx": "Mikolov et al\\.,? 2012", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Neural net architectures for temporal sequence processing", "author": ["M.C. Mozer"], "venue": "Santa Fe Institute Studies in the Sciences of Complexity, volume 15, pages 243\u2013243. Addison-Wesley Publishing Co.", "citeRegEx": "Mozer,? 1993", "shortCiteRegEx": "Mozer", "year": 1993}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 30(4):838\u2013855.", "citeRegEx": "Polyak and Juditsky,? 1992", "shortCiteRegEx": "Polyak and Juditsky", "year": 1992}, {"title": "Multimodal variational encoder-decoders", "author": ["I.V. Serban", "I. Ororbia", "G. Alexander", "J. Pineau", "A. Courville"], "venue": "arXiv preprint arXiv:1612.00377.", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "A. Szlam", "J. Weston", "R. Fergus"], "venue": "arXiv:1503.08895 [cs].", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "The neural network pushdown automaton: Architecture, dynamics and training", "author": ["Sun", "G.-Z.", "C.L. Giles", "Chen", "H.-H."], "venue": "Adaptive processing of sequences and data structures, pages 296\u2013345. Springer.", "citeRegEx": "Sun et al\\.,? 1998", "shortCiteRegEx": "Sun et al\\.", "year": 1998}, {"title": "Improvements in Language and Translation Modeling", "author": ["M. Sundermeyer"], "venue": "PhD thesis, RWTH Aachen University.", "citeRegEx": "Sundermeyer,? 2016", "shortCiteRegEx": "Sundermeyer", "year": 2016}, {"title": "Quadratic features and deep architectures for chunking", "author": ["J. Turian", "J. Bergstra", "Y. Bengio"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 245\u2013248. Association for Computational Linguistics.", "citeRegEx": "Turian et al\\.,? 2009", "shortCiteRegEx": "Turian et al\\.", "year": 2009}, {"title": "Larger-context language modelling", "author": ["T. Wang", "K. Cho"], "venue": "arXiv preprint arXiv:1511.03729.", "citeRegEx": "Wang and Cho,? 2015", "shortCiteRegEx": "Wang and Cho", "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv:1410.3916 [cs, stat]. 26", "citeRegEx": "Weston et al\\.,? 2014", "shortCiteRegEx": "Weston et al\\.", "year": 2014}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Y. Wu", "S. Zhang", "Y. Zhang", "Y. Bengio", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Minimal gated unit for recurrent neural networks", "author": ["Zhou", "G.-B.", "J. Wu", "Zhang", "C.-L.", "Zhou", "Z.-H."], "venue": "International Journal of Automation and Computing, 13(3):226\u2013234. 27", "citeRegEx": "Zhou et al\\.,? 2016", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "The simple recurrent neural network (RNN) architecture (Elman, 1990) is, however, not ar X iv :1 70 3.", "startOffset": 55, "endOffset": 68}, {"referenceID": 3, "context": "This demystifies the abilities of the LSTM model to a degree: while some authors have suggested that the LSTM understands the language and even the thoughts being expressed in sentences (Choudhury, 2015), it is arguable whether this could be said about a model that performs equally well and is based on representations that are essentially equivalent to a bag of words.", "startOffset": 186, "endOffset": 203}, {"referenceID": 2, "context": "However, we point out its conceptual origins in Surprisal Theory (Boston et al., 2008; Hale, 2001; Levy, 2008), which posits that the human language processor develops complex expectations of future words, phrases and syntactic choices, and that these expectations and deviations from them (surprisal) guide language processing, e.", "startOffset": 65, "endOffset": 110}, {"referenceID": 17, "context": "However, we point out its conceptual origins in Surprisal Theory (Boston et al., 2008; Hale, 2001; Levy, 2008), which posits that the human language processor develops complex expectations of future words, phrases and syntactic choices, and that these expectations and deviations from them (surprisal) guide language processing, e.", "startOffset": 65, "endOffset": 110}, {"referenceID": 29, "context": "However, we point out its conceptual origins in Surprisal Theory (Boston et al., 2008; Hale, 2001; Levy, 2008), which posits that the human language processor develops complex expectations of future words, phrases and syntactic choices, and that these expectations and deviations from them (surprisal) guide language processing, e.", "startOffset": 65, "endOffset": 110}, {"referenceID": 20, "context": "1ct refers to the \u201ccell-state\u201d as in (Hochreiter and Schmidhuber, 1997b).", "startOffset": 37, "endOffset": 72}, {"referenceID": 7, "context": "More importantly, one might view the parameters \u03c8 that f\u03c8(\u00b7) uses as possibly encapsulating structures that can be used to store explicit memory-vectors, as is the case in stacked-based RNNs (Das et al., 1992; Joulin and Mikolov, 2015) or linked-list (Joulin and Mikolov, 2015) data structures.", "startOffset": 191, "endOffset": 235}, {"referenceID": 23, "context": "More importantly, one might view the parameters \u03c8 that f\u03c8(\u00b7) uses as possibly encapsulating structures that can be used to store explicit memory-vectors, as is the case in stacked-based RNNs (Das et al., 1992; Joulin and Mikolov, 2015) or linked-list (Joulin and Mikolov, 2015) data structures.", "startOffset": 191, "endOffset": 235}, {"referenceID": 23, "context": ", 1992; Joulin and Mikolov, 2015) or linked-list (Joulin and Mikolov, 2015) data structures.", "startOffset": 49, "endOffset": 75}, {"referenceID": 39, "context": "Interpolation, especially through a simple gating mechanism, can be an effective way to allow the model to learn how to turn on/off latent dimensions, potentially yielding improved generalization performance, such as was empirically shown by Serban et al. (2016). Beyond fixing r to some vector of pre-initialized values, there two simple ways to", "startOffset": 242, "endOffset": 263}, {"referenceID": 39, "context": "Unlike Serban et al. (2016), we constrain the rates to lie to in the range [0, 1] by using the logistic link function, \u03c3(v) = 1/(1 + exp(\u2212v)), which will transform the biases into rates much like the rates of the SCRN.", "startOffset": 7, "endOffset": 28}, {"referenceID": 10, "context": "However, historically, second-order recurrent neural architectures have been shown to be powerful models in tasks such as grammatical inference (Giles et al., 1991) and noisy time-series prediction (Giles et al.", "startOffset": 144, "endOffset": 164}, {"referenceID": 11, "context": ", 1991) and noisy time-series prediction (Giles et al., 2001) as well as incredibly useful in rule-extraction when treated as finite-state automata (Giles et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 8, "context": "3 When using a simple Elman RNN, we have essentially described a first-order DeltaRNN. However, historically, second-order recurrent neural architectures have been shown to be powerful models in tasks such as grammatical inference (Giles et al., 1991) and noisy time-series prediction (Giles et al., 2001) as well as incredibly useful in rule-extraction when treated as finite-state automata (Giles et al., 1992; Goudreau et al., 3Late-integration might remind the reader of the phrase \u201clate fusion\u201d, as in the context of Wang and Cho (2015). However, Wang and Cho was focused on merging the information from an external bag-of-words context vector with the standard cell state of the LSTM.", "startOffset": 22, "endOffset": 542}, {"referenceID": 47, "context": "Very recently, (Wu et al., 2016) showed that the gating effect between the statedriven component and data-driven components of a layer\u2019s pre-activations facilitated better propagation of gradient signals as opposed to the usual linear combination.", "startOffset": 15, "endOffset": 32}, {"referenceID": 10, "context": "We can take the late-integration model, Equation 9, and replace zt with (Giles et al., 1991):", "startOffset": 72, "endOffset": 92}, {"referenceID": 43, "context": "Very recently, (Wu et al., 2016) showed that the gating effect between the statedriven component and data-driven components of a layer\u2019s pre-activations facilitated better propagation of gradient signals as opposed to the usual linear combination. A second-order version of g\u03b8(xt,ht\u22121) would be highly desirable, not only because it further mitigates the vanishing gradient problem that plagues back-propagation through time (used in calculating parameter gradients of neural architectures), but because the form introduces negligibly few additional parameters. We do note that the second-order form we use, like in Wu et al. (2016), is a rank-1 matrix approximation of the actual tensor used in Giles et al.", "startOffset": 16, "endOffset": 633}, {"referenceID": 10, "context": "(2016), is a rank-1 matrix approximation of the actual tensor used in Giles et al. (1992); Goudreau et al.", "startOffset": 70, "endOffset": 90}, {"referenceID": 10, "context": "(2016), is a rank-1 matrix approximation of the actual tensor used in Giles et al. (1992); Goudreau et al. (1994). We can take the late-integration model, Equation 9, and replace zt with (Giles et al.", "startOffset": 70, "endOffset": 114}, {"referenceID": 47, "context": "or a more general form (Wu et al., 2016):", "startOffset": 23, "endOffset": 40}, {"referenceID": 25, "context": "For all experiments conducted in this paper, we found that the ADAM adaptive learning rate scheme (Kingma and Ba, 2014) followed by a Polyak average (Polyak and Juditsky, 1992) yielded the most consistent and near-optimal performance.", "startOffset": 98, "endOffset": 119}, {"referenceID": 38, "context": "For all experiments conducted in this paper, we found that the ADAM adaptive learning rate scheme (Kingma and Ba, 2014) followed by a Polyak average (Polyak and Juditsky, 1992) yielded the most consistent and near-optimal performance.", "startOffset": 149, "endOffset": 176}, {"referenceID": 8, "context": "Simple models, largely based on the original Elman RNN (Elman, 1990), have often been shown to perform quite well in language modeling tasks (Mikolov et al.", "startOffset": 55, "endOffset": 68}, {"referenceID": 22, "context": "The SCRN itself is very similar to a combination of RNN architectures of (Jordan, 1986; Mozer, 1993).", "startOffset": 73, "endOffset": 100}, {"referenceID": 37, "context": "The SCRN itself is very similar to a combination of RNN architectures of (Jordan, 1986; Mozer, 1993).", "startOffset": 73, "endOffset": 100}, {"referenceID": 37, "context": "These identity connections (and corresponding context units that use them) allow for improved information travel over many time-steps and can even be viewed as an exponential trace memory (Mozer, 1993), Residual networks, though feedforward in nature, also share a similar motivation, as in He et al.", "startOffset": 188, "endOffset": 201}, {"referenceID": 8, "context": "Simple models, largely based on the original Elman RNN (Elman, 1990), have often been shown to perform quite well in language modeling tasks (Mikolov et al., 2010, 2011). The structurally constrained recurrent network (SCRN, Mikolov et al. 2014), an important predecessor and inspiration for this work, showed that one fruitful path to learning longer-term dependencies was to impose a hard constraint on how quickly the values of hidden units could change design units, yielding more \u201cstable\u201d long-term memory. The SCRN itself is very similar to a combination of RNN architectures of (Jordan, 1986; Mozer, 1993). The key element of its design is the constraint that part of recurrent weight matrix must be close to the identity, a constraint that is also satisfied by the Delta-RNN. These identity connections (and corresponding context units that use them) allow for improved information travel over many time-steps and can even be viewed as an exponential trace memory (Mozer, 1993), Residual networks, though feedforward in nature, also share a similar motivation, as in He et al. (2016). Unlike the SCRN, the proposed Delta-RNN does not require a separation of the slow and fast moving units, but instead models this slower time-scale through implicitly stable states.", "startOffset": 45, "endOffset": 1092}, {"referenceID": 9, "context": "A great deal of recent work has attempted to improve the training of the LSTM, often by increasing its complexity, such as through the introduction of so-called \u201cpeephole connections\u201d (Gers and Schmidhuber, 2000).", "startOffset": 184, "endOffset": 212}, {"referenceID": 42, "context": ", 2016; ?) or data-structures that server as memory (Sun et al., 1998; Joulin and Mikolov, 2015).", "startOffset": 52, "endOffset": 96}, {"referenceID": 23, "context": ", 2016; ?) or data-structures that server as memory (Sun et al., 1998; Joulin and Mikolov, 2015).", "startOffset": 52, "endOffset": 96}, {"referenceID": 4, "context": ", number of processing layers) to directly model various time-scales, as emulated in models such as the hierarchical/multi-resolutional recurrent neural network (HM-RNN) (Chung et al., 2016).", "startOffset": 170, "endOffset": 190}, {"referenceID": 26, "context": "(This is different from models like the Clockwork RNN that require explicit boundary information (Koutnik et al., 2014).", "startOffset": 97, "endOffset": 119}, {"referenceID": 44, "context": ", 2014) and light chunking (Turian et al., 2009).", "startOffset": 27, "endOffset": 48}, {"referenceID": 31, "context": "The Penn Treebank corpus (Marcus et al., 1993) is often used to benchmark both word and character-level models via perplexity or bits-per-character, and thus we start here.", "startOffset": 25, "endOffset": 46}, {"referenceID": 30, "context": "The large sentiment analysis corpus (Maas et al., 2011) is often used to benchmark algorithms for predicting the positive or negative tonality of documents.", "startOffset": 36, "endOffset": 55}, {"referenceID": 33, "context": "Penn Treebank: Word Models PPL N-Gram (Mikolov et al., 2014) 141 NNLM (Mikolov, 2012) 140.", "startOffset": 38, "endOffset": 60}, {"referenceID": 32, "context": ", 2014) 141 NNLM (Mikolov, 2012) 140.", "startOffset": 17, "endOffset": 32}, {"referenceID": 33, "context": "2 N-Gram+cache (Mikolov et al., 2014) 125 RNN (Gulcehre et al.", "startOffset": 15, "endOffset": 37}, {"referenceID": 16, "context": ", 2014) 125 RNN (Gulcehre et al., 2016) 129 RNN (Mikolov, 2012) 124.", "startOffset": 16, "endOffset": 39}, {"referenceID": 32, "context": ", 2016) 129 RNN (Mikolov, 2012) 124.", "startOffset": 16, "endOffset": 31}, {"referenceID": 33, "context": "7 LSTM (Mikolov et al., 2014) 115 SCRN (Mikolov et al.", "startOffset": 7, "endOffset": 29}, {"referenceID": 33, "context": ", 2014) 115 SCRN (Mikolov et al., 2014) 115 LSTM (Sundermeyer, 2016) 107 MI-RNN (Wu et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 43, "context": ", 2014) 115 LSTM (Sundermeyer, 2016) 107 MI-RNN (Wu et al.", "startOffset": 17, "endOffset": 36}, {"referenceID": 36, "context": "Penn Treebank: Character Models BPC N-discount N-gram (Mikolov et al., 2012) 1.", "startOffset": 54, "endOffset": 76}, {"referenceID": 27, "context": "48 RNN+stabilization (Krueger et al., 2016) 1.", "startOffset": 21, "endOffset": 43}, {"referenceID": 47, "context": "48 linear MI-RNN (Wu et al., 2016) 1.", "startOffset": 17, "endOffset": 34}, {"referenceID": 26, "context": "48 Clockwork RNN (Koutnik et al., 2014) 1.", "startOffset": 17, "endOffset": 39}, {"referenceID": 36, "context": "46 RNN (Mikolov et al., 2012) 1.", "startOffset": 7, "endOffset": 29}, {"referenceID": 21, "context": "42 GRU (Jernite et al., 2016) 1.", "startOffset": 7, "endOffset": 29}, {"referenceID": 36, "context": "42 HF-MRNN (Mikolov et al., 2012) 1.", "startOffset": 11, "endOffset": 33}, {"referenceID": 47, "context": "41 MI-RNN (Wu et al., 2016) 1.", "startOffset": 10, "endOffset": 27}, {"referenceID": 36, "context": "39 Max-Ent N-gram (Mikolov et al., 2012) 1.", "startOffset": 18, "endOffset": 40}, {"referenceID": 27, "context": "37 LSTM (Krueger et al., 2016) 1.", "startOffset": 8, "endOffset": 30}, {"referenceID": 31, "context": "Subwords are particularly appealing not only in that the input distribution is of lower dimensionality but, as evidenced by the positive results of Mikolov et al. (2012), sub-word/character hybrid language models improve over the performance of pure character-level models.", "startOffset": 148, "endOffset": 170}, {"referenceID": 28, "context": "8We also experimented with other initializations, most notably the identity matrix for the recurrent weight parameters as in (Le et al., 2015).", "startOffset": 125, "endOffset": 142}, {"referenceID": 28, "context": "8We also experimented with other initializations, most notably the identity matrix for the recurrent weight parameters as in (Le et al., 2015). We found that this initialization often worsened performance. For the activation functions of the first-order models, we experimented with the linear rectifier, the parametrized linear rectifier, and even our own proposed parametrized smoothened linear rectifier, but found such activations lead to less-than-satisfactory results. The results of this inquiry is documented in the code that will accompany the paper. 9Note that H = 2000 would yield nearly 4 million parameters, which was our upper bound on total number of parameters allowed for experiments in order to be commensurable with the work of Wu et al. (2016), which actually used H = 2048 for all Penn Treebank models.", "startOffset": 126, "endOffset": 764}, {"referenceID": 1, "context": "processing mechanisms (Baayen and Schreuder, 2006).", "startOffset": 22, "endOffset": 50}, {"referenceID": 36, "context": "Consistency in subword formation is critical in order to obtain meaningful results (Mikolov et al., 2012).", "startOffset": 83, "endOffset": 105}, {"referenceID": 10, "context": "Specifically, we test our implementations of the LSTM 10 (with peephole connections as described in Graves 2013), the GRU, the MGU, the SCRN, as well as a classical Elman network, of both 1st and 2nd-order (Giles et al., 1991; Wu et al., 2016).", "startOffset": 206, "endOffset": 243}, {"referenceID": 47, "context": "Specifically, we test our implementations of the LSTM 10 (with peephole connections as described in Graves 2013), the GRU, the MGU, the SCRN, as well as a classical Elman network, of both 1st and 2nd-order (Giles et al., 1991; Wu et al., 2016).", "startOffset": 206, "endOffset": 243}, {"referenceID": 46, "context": "This extends the results of Wu et al. (2016) to the word-level.", "startOffset": 28, "endOffset": 45}, {"referenceID": 40, "context": "(2014) reported that a single LSTM (for word-level Penn Treebank) can reach a PPL of \u223c 80, but this was achieved via dropout regularization (Srivastava et al., 2014).", "startOffset": 140, "endOffset": 165}, {"referenceID": 32, "context": "An ensemble of twenty simple RNNs and cache models did previously reach PPL as low as 72, while a single RNN model gives only 124 (Mikolov, 2012).", "startOffset": 130, "endOffset": 145}, {"referenceID": 32, "context": "(2014) trained an ensemble of 38 LSTMs regularized with dropout, each with 100x times more parameters than the RNNs used in (Mikolov, 2012), achieving PPL 68.", "startOffset": 124, "endOffset": 139}, {"referenceID": 46, "context": "Zaremba et al. (2014) reported that a single LSTM (for word-level Penn Treebank) can reach a PPL of \u223c 80, but this was achieved via dropout regularization (Srivastava et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 32, "context": "An ensemble of twenty simple RNNs and cache models did previously reach PPL as low as 72, while a single RNN model gives only 124 (Mikolov, 2012). Zaremba et al. (2014) trained an ensemble of 38 LSTMs regularized with dropout, each with 100x times more parameters than the RNNs used in (Mikolov, 2012), achieving PPL 68.", "startOffset": 131, "endOffset": 169}, {"referenceID": 43, "context": "12At greater computational cost, a somewhat lower perplexity for an LSTM may be attainable, such as the perplexity of 107 reported by Sundermeyer (2016) (see Table 1).", "startOffset": 134, "endOffset": 153}, {"referenceID": 14, "context": "Finally, it would be useful to explore if Delta-RNN\u2019s simpler, faster design can speed up the performance of grander architectures, such as the Differentiable Neural Computer Graves et al. (2016).", "startOffset": 175, "endOffset": 196}], "year": 2017, "abstractText": "Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks like language modeling. Existing architectures that address the issue are often complex and costly to train. The Delta Recurrent Neural Network (Delta-RNN) framework is a simple and highperforming design that unifies previously proposed gated neural models. The DeltaRNN models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. This requires hardly any more parameters than a classical simple recurrent network. The models outperform popular complex architectures, such as the Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU) and achieve state-of-the art performance in language modeling at character and word levels and yield comparable performance at the subword level.", "creator": "LaTeX with hyperref package"}}}