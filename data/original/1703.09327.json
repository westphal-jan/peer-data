{"id": "1703.09327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "Iterative Noise Injection for Scalable Imitation Learning", "abstract": "In Imitation Learning, a supervisor's policy is observed and the intended behavior is learned. A known problem with this approach is covariate shift, which occurs because the agent visits different states than the supervisor. Rolling out the current agent's policy, an on-policy method, allows for collecting data along a distribution similar to the updated agent's policy. However this approach can become less effective as the demonstrations are collected in very large batch sizes, which reduces the relevance of data collected in previous iterations. In this paper, we propose to alleviate the covariate shift via the injection of artificial noise into the supervisor's policy. We prove an improved bound on the loss due to the covariate shift, and introduce an algorithm that leverages our analysis to estimate the level of $\\epsilon$-greedy noise to inject. In a driving simulator domain where an agent learns an image-to-action deep network policy, our algorithm Dart achieves a better performance than DAgger with 75% fewer demonstrations.", "histories": [["v1", "Mon, 27 Mar 2017 22:26:16 GMT  (923kb,D)", "http://arxiv.org/abs/1703.09327v1", null], ["v2", "Wed, 18 Oct 2017 03:52:18 GMT  (4585kb,D)", "http://arxiv.org/abs/1703.09327v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["michael laskey", "jonathan lee", "wesley hsieh", "richard liaw", "jeffrey mahler", "roy fox", "ken goldberg"], "accepted": false, "id": "1703.09327"}, "pdf": {"name": "1703.09327.pdf", "metadata": {"source": "META", "title": "Iterative Noise Injection for Scalable Imitation Learning", "authors": ["Michael Laskey", "Jonathan Lee", "Wesley Hsieh", "Richard Liaw", "Jeffrey Mahler", "Roy Fox", "Ken Goldberg"], "emails": ["<laskeymd@berkeley.edu>."], "sections": [{"heading": "1. Introduction", "text": "In Imitation Learning (IL), an agent learns to mimic a supervisor on a task involving sequential decisions (Argall et al., 2009). The generality of this approach has led to a wide range of applications: parsing sentence structure (Ballesteros et al., 2016), playing video games (Guo et al., 2014), robotic manipulation (Laskey et al., 2016b) and self-driving cars (Pomerleau, 1989).\nSimilar to Reinforcement Learning, IL has two types of algorithms: Off-Policy and On-Policy. In Off-Policy IL, an agent observes demonstrations from a supervisor and tries to recover the behavior via supervised learning (Pomerleau, 1989). Off-Policy algorithms are traditionaly referred to as supervised learning or Behavior Cloning (Ross & Bagnell, 2010; Syed & Schapire, 2010). An issue with Off-Policy\n1University of California, Berkeley. Correspondence to: Michael Laskey <laskeymd@berkeley.edu>.\nalgorithms is that the agent may not perfectly agree with the supervisor during training. These small errors can compound during execution of the agent\u2019s policy because the agent may deviate greatly from the states visited by the supervisor (Ross & Bagnell, 2010) and causing the agent to receive non-i.i.d data.\nOn-Policy methods, such as DAgger, attempt to remedy this by sampling trajectories using the current agent\u2019s policy (Ross et al., 2010) and querying the supervisor at each state for the correct action. On-Policy methods have been shown to empirically converge faster than existing OffPolicy methods (Guo et al., 2014).\nWhile the potential for IL algorithms is growing, there remains a challenge of collecting data from supervisors in an efficient manner (Laskey et al., 2016a). Parallelization of data collection is one potential way to achieve this efficiency. For example, in autonomous car applications one could have a large number of human drivers collecting data in parallel on the road. Parallelization can pose a challenge for current On-Policy algorithms when training on large batch updates.\nInuitively, if the agent\u2019s policy is significantly perturbed during an update, then it would visit different states than the previous agent, thus making the data collected at each iteration potentially uninformative. Similar results have been shown in On-Policy Reinforcement Learning and active learning settings (Schulman et al., 2015; Settles & Craven, 2008).\nWe conjecture that when the current agent\u2019s policy becomes a poor predictor, it is better to consider a distribution concentrated on the supervisor\u2019s policy (i.e Off-Policy). The goal of this work is to design this distribution. We begin with a new analysis on the Total Variation Distance between the agent\u2019s and supervisor\u2019s distributions. In relating Total Variation Distance to Hellinger Distance (Le Cam & Yang, 2012), we can bound the covariate shift linearly in the time horizon as a function of the percentage of trajectories they agree on.\nWe then extend this analysis to a less greedy version of OffPolicy, where unbiased noise is injected into the supervisor\u2019s demonstrations. Noise injection can be viewed as placing a more conservative prior over the states the trained agent will likely visit. Leveraging our theoretical analysis, we develop\nar X\niv :1\n70 3.\n09 32\n7v 1\n[ cs\n.L G\n] 2\n7 M\nar 2\n01 7\nan algorithm, Dart, that iteratively determines the level of noise to inject into the supervisor via minimizing our bound on the Total Variation Distance. Our algorithm specifically considers the -greedy noise distribution.\nWe test our hypothesis in a grid-world and driving simulator domains. Results suggest that by adding noise to the actions executed by the supervisor\u2019s policy we can reduce the covariate shift and enable sample efficient batch learning. Specifically, results on a driving simulator, where an imageto-action policy is trained, show that Dart can have similar performance to DAgger with 75% fewer demonstrations when 100 demonstrations are collected in a parallelized manner."}, {"heading": "2. Problem Statement", "text": "The objective of Imitation Learning is to learn a policy that matches what the supervisor demonstrated.\nModeling Choices and Assumptions: We model the system dynamics as Markovian and stochastic. We model the initial state as sampled from a distribution over the state space. We assume a known state space and set of actions. We also assume access to a robot or simulator, such that we can sample from the state sequences induced by a sequence of action. Lastly, we assume access to a supervisor who can, given a state, provide an action, which might be noisy.\nPolicies and State Densities. We denote by S the set consisting of observable states for an agent. We assume S is a finite set of states. We furthermore consider an action set A of size K = |A|. We model dynamics as Markovian, such that the probability of state st+1 \u2208 S can be determined from the previous state st \u2208 S and action at \u2208 A:\np(st+1|a0, s0, . . . at, st) = p(st+1|at, st).\nWe assume a probability density over initial states p(s0). The environment of a task is thus defined as a specific instance of action and state spaces, initial state distribution, and dynamics.\nGiven a time horizon T \u2208 N, a trajectory \u03be is a finite sequence of T pairs of states visited and corresponding control inputs at these states, \u03be = (s0, a0, s1, a1, . . . , sT ), where st \u2208 S and at \u2208 A for each t.\nA policy is a measurable function \u03c0 : S \u2192 A from states to control inputs. We consider policies \u03c0\u03b8 : S \u2192 A parametrized by some \u03b8 \u2208 \u0398. Under our assumptions, any such policy \u03c0\u03b8 induces a probability density over the set of trajectories of length T :\np(\u03be|\u03c0\u03b8) = p(s0) T\u22121\u220f t=0 \u03c0\u03b8(at|st)p(st+1|at, st)\nThe term \u03c0\u03b8(at|st) indicates stochasticity in the applied\npolicy and we consider this to be a user-defined distribution in which the deterministic output of the policy is a sufficient statistic. A distribution we consider is -greedy, in which with probability a random control is applied instead of \u03c0\u03b8(st).\nWhile we do not assume knowledge of the distributions corresponding to p(st+1|st, at) or p(s0), we assume that we have a real robot or a simulator. Therefore, when \u2018rolling out\u2019 trajectories under a policy \u03c0\u03b8, we utilize the robot or the simulator to sample the resulting stochastic trajectories rather than estimating p(\u03be|\u03c0\u03b8) itself.\nObjective. The objective of policy learning is to find a policy that maximizes some known reward function R(\u03be) =\u2211T\u22121 t=0 r(st, at) of a trajectory \u03be. The reward r : S \u00d7A \u2192 R is typically user defined and task specific. For example in the task of grasping, the reward can be a binary measure of success.\nDefining a reward function that provides enough information for efficient learning can be challenging (Abbeel & Ng, 2004). Thus in Imitation Learning, we do not assume access to a reward function but instead a supervisor, \u03c0\u03b8\u2217 , where \u03b8\u2217 may not be contained in \u0398. We assume the supervisor achieves a desired level of performance on the task, although it may not be optimal.\nWe measure the difference between controls using a surrogate loss l : A \u00d7 A \u2192 R (Ross et al., 2010; Ross & Bagnell, 2010). The surrogate loss we consider is an indicator on the action l(a1, a2) = \u03b4a1 6=a2 , which is 1 when the actions disagree and 0 otherwise. We measure total loss along a trajectory with respect to two policies \u03c0\u03b81 and \u03c0\u03b82 by J(\u03b81, \u03b82|\u03be) = \u2211T\u22121 t=0 l(\u03c0\u03b81(st), \u03c0\u03b82(st)). We note our choice of surrogate loss implies 0 \u2264 J(\u03b81, \u03b82|\u03be) \u2264 T .\nThe objective of Imitation Learning is to minimize the expected surrogate loss along the distribution induced by the agent\u2019s policy.\nmin \u03b8 Ep(\u03be|\u03c0\u03b8)J(\u03b8, \u03b8\n\u2217|\u03be) (1)\nIn Eq. 1, the distribution of trajectories and the cumulative surrogate loss are coupled, which makes this a challenging optimization problem. The field of Imitation Learning has considered two types of algorithmic solutions to this objective, off-policy learning and on-policy learning. We review each approach in the following section. We denote Ep(\u03be|\u03c0\u03b8) = E\u03b8 in the rest of the paper."}, {"heading": "3. Off and On-Policy Imitation Learning", "text": "An approach to optimize Eq. 1 is to train an agent on trajectories sampled from a different distribution p(\u03be|\u03c0\u03b8\u0302). If p(\u03be|\u03c0\u03b8\u0302) is close to the learned agent\u2019s distribution p(\u03be|\u03c0\u03b8)\n, one may expect the agent\u2019s performance to be similar. Algorithms designed to select p(\u03be|\u03c0\u03b8\u0302) can be viewed as placing a prior over the learned agent\u2019s distribution p(\u03be|\u03c0\u03b8). The questions of how to best select the prior, \u03c0\u03b8\u0302 ,and how close the distributions need to be has led to two different classes of algorithms (Syed & Schapire, 2010; Daume\u0301 et al., 2009)"}, {"heading": "3.1. Off-Policy IL", "text": "In Off-Policy learning, supervisor demonstrations are sampled from the distribution p(\u03be|\u03c0\u03b8\u2217). Then for a finite sample of N demonstrations the following minimization is performed.\nmin \u03b8 N\u2211 i=1 J(\u03b8, \u03b8\u2217|\u03bei) \u03bei \u223c p(\u03be|\u03c0\u03b8\u2217) (2)\nEq. 2 can be solved as a standard supervised learning problem (Pomerleau, 1989). It is important to note that during optimization of \u03b8 our current indicator loss function l should be replaced with a smooth classification calibrated loss function such as the Hinge Loss (Bartlett et al., 2006).\nWhile there is reason to believe that under a consistent policy representation and infinite data, p(\u03be|\u03c0\u03b8\u2217) would be very close to p(\u03be|\u03c0\u03b8). In practice, small training errors can accumulate during exeuction of the agent\u2019s policy. Thus, causing the agent to deviate significantly from the supervisor\u2019s demonstrations. This notion of a distribution mismatch, or covariate shift, has led to both theoretical and empirical evidence (Ross et al., 2010; Guo et al., 2014) that suggests off-policy learning is not a robust algorithm.\nWhile off-policy algorithms suffer in performance, they can have a large number of implementation advantages. As noted in the active learning literature, methods that decouple data collection from the current model can potentially lead to easier parallelization in data collection. Furthermore, the choice of learning model and hyper-parameters do not have an impact on the data collected, which is advantageous when the model needs to be changed at a later date (Settles & Craven, 2008).\nIn Sec. 4, we offer a new theoretical analysis of this inherent problem that suggests not all types of training errors are equal. We then extend this analysis, in Sec. 5, to show how adding stochasticity to the supervisor\u2019s policy can help alleviate the covariate shift. We then empirically show that we can achieve strong performance and the advantages of off-policy methods."}, {"heading": "3.2. On-Policy IL", "text": "Similar to Policy Gradient algorithms in Reinforcement Learning, on-policy IL methods sample trajectories from the agent\u2019s current distribution and update the model based on the data received (Schulman et al., 2015; Daume\u0301 et al.,\nAlgorithm 1 DAgger Input: \u03b2 Initialize: \u03b80 for n = 0 to N \u2212 1 do\nfor m = 1 to M do \u03ben,m \u223c p(\u03be|\u03c0\u03b8n , \u03b2n) end for \u03b8n+1 = arg min\n\u03b8\n\u2211n i=0 \u2211M j=1 J(\u03b8, \u03b8 \u2217|\u03bei,j)\nend for\n2009)\nA common on-policy algorithm, known as DAgger, is shown in Alg. 1. DAgger operates forN iterations where at each iteration M trajectories are sampled from the distribution p(\u03be|\u03c0\u03b8n , \u03b2n). The distribution p(\u03be|\u03c0\u03b8n , \u03b2n) is an exponentially decaying stochastic mixing of the supervisor\u2019s distribution and the current agent\u2019s policy. Specifically it sets \u03c0\u03b8(a|s, \u03b2n) = \u03b2n\u03c0\u03b8\u2217(a|s) + (1\u2212 \u03b2n)\u03c0\u03b8(a|s), where \u03b2 \u2208 [0, 1]. After collection of M trajectories, the agent\u2019s policy is retrained on the aggregate dataset.\nA large number of extensions to DAgger have been proposed, such as modifying the supervisor\u2019s policy to be easier to learn (He et al., 2012; Levine et al., 2015) or querying the supervisor for only informative states (Kim & Pineau, 2013; Laskey et al., 2016c).\nIn order to understand how similar p(\u03be|\u03c0\u03b8n) is to p(\u03be|\u03c0\u03b8n+1). Ross et al. proposed analyzing this algorithm in the context of online convex optimization, which considers analyzing the rate of decay of \u03b3N , defined as\n1\nN \u2223\u2223\u2223\u2223\u2223 N\u22121\u2211 n=0 E\u03b8nJ(\u03b8n, \u03b8 \u2217|\u03be)\u2212min \u03b8 N\u22121\u2211 n=0 E\u03b8nJ(\u03b8, \u03b8 \u2217|\u03be) \u2223\u2223\u2223\u2223\u2223 \u2264 \u03b3N The motivation to consider bounding the average loss over the different agent policies is that the policy with the lowest expected loss (i.e. the returned agent\u2019s policy) after N iterations would also be bounded. Ross et al. additionally consider the effect of \u03b2 in the analysis, however we are specifically interested in \u03b3N . Under the assumption that E\u03b8nJ(\u03b8, \u03b8\n\u2217|\u03be) is convex with respect to \u03b8 then results exists on how fast \u03b3N decays (Shalev-Shwartz, 2011).\nConvergence of \u03b3N \u2192 0 is propotional to a bound on the maximum gradient (i.e. \u2016\u2207\u03b8nE\u03b8nJ(\u03b8, \u03b8\u2217|\u03be)\u2016 \u2264 G) (Hazan et al., 2006) and the number of iterations, N . These terms suggest that on-policy methods could become less effective when N is set small. Furthermore, large changes to expressive models could need more iterations to converge.\nThis can be problematic in situations where we want to train computationally expensive deep neural networks with large\namounts of data collected in batches. Ideally, we would want N to be small to reduce computationally expensive retraining and set M large for situations where parrallized data collection is possible. This raises a key question: is the previous agent\u2019s policy a good prior under these potentially aggressive changes to the current agent?"}, {"heading": "4. The Covariate Shift Problem", "text": ""}, {"heading": "4.1. Bounding the Mismatch", "text": "In statistical learning, the notion of a training and test distributions being different on the input space is known as covariate shift. Under a significant shift between the two distributions, it is unlikely for a learned model to generalize to the shifted test distribution (Quionero-Candela et al., 2009).\nRoss et al. applied the intuition of covariate shift to OffPolicy Imitation Learning, to bound the expected number of errors the agent performs during execution for the binary classification case.\nTheorem 4.1 (Ross and Bagnell 2010) Denote by \u03c0\u03b8 a policy found using Off-Policy IL. The following inequality holds:\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217 J(\u03b8, \u03b8\u2217|\u03be)| \u2264\nmin(TE\u03b8\u2217J(\u03b8, \u03b8 \u2217|\u03be), T )\nTheroem 4.1 suggests Off-Policy IL can perform quite poorly. For example, if E\u03b8\u2217J(\u03b8, \u03b8\u2217|\u03be) = 1 then E\u03b8J(\u03b8, \u03b8\n\u2217|\u03be) \u2264 T , which implies the agent can incur near maximum error with relatively small error on the supervisor\u2019s distribution. A similar result for the more general classification setting was proved in (Syed & Schapire, 2010). This result has been a motivation for many to perform onpolicy methods (Ross et al., 2010; Le et al., 2016; Levine et al., 2015).\nWhile the problem of covariate shift can lead to arbitrarily poor performance in general, in IL, the problem can be less severe than previously thought. We will now examine the distribution mismatch between the two distributions p(\u03be|\u03c0\u03b81) and p(\u03be|\u03c0\u03b82). The difference between these two distributions corresponds to amount of error the covariate shift can create.\nWe will assume that probability of controls in these distributions \u03c0\u03b8(at|st) is the following for all timesteps in a trajectory.\n\u03c0\u03b8(at|st) = { 1 if \u03c0\u03b8(st) = at 0 if \u03c0\u03b8(st) 6= at\nThe chosen distribution is a deterministic agent in which the intended control of the agent is always applied.\nLemma 4.2 For two deterministic agents with distributions p(\u03be|\u03c0\u03b81) and p(\u03be|\u03c0\u03b82) over trajectories the following inequality holds:\n||p(\u03be|\u03c0\u03b82)\u2212 p(\u03be|\u03c0\u03b81)||TV \u2264 \u221a 1\u2212 (E\u03b82d0(\u03b81, \u03b82|\u03be)) 2\nwhere d0(\u03b81, \u03b82|\u03be) = \u03b4J(\u03b81,\u03b82|\u03be)=0 =\u220fT\u22121 t=0 \u03b4\u03c0\u03b81 (st)=\u03c0\u03b82 (st) or the indicator that the two agents apply the same controls for all states in the trajectory. [See Appendix for Proof]\nLemma 4.2 demonstrates that the distribution mismatch for the two agents can be bounded via the expected fraction of the trajectories on which they match perfectly. The bound is tight in the sense that it has a unique maximum, which is equivalent to the total variation maximum. We will now use Lemma 4.2 to provide new insights into Off-Policy IL."}, {"heading": "4.2. The Effect on Off-Policy IL", "text": "In Off-Policy IL, the two distributions would correspond to the supervisor\u2019s policy, \u03c0\u03b8\u2217 and the agent\u2019s \u03c0\u03b8. Under the assumption of the supervisor and agent both having deterministic distributions over controls, we can show the following.\nTheorem 4.3 Denote by \u03c0\u03b8 a policy, with a deterministic supervisor. The following inequality holds:\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217 J(\u03b8, \u03b8\u2217|\u03be)| \u2264\nT \u221a 1\u2212 (E\u03b8\u2217 d0(\u03b8, \u03b8\u2217|\u03be))2\n[See Appendix for Proof]\nTheorem 4.3 is interesting because it suggests some type of errors are worse than others. For example consider the situation where there exist M possible trajectories each with time horizon T = 100. The supervisor\u2019s distribution assigns uniform probability for each trajectory occurring (i.e. p(\u03be|\u03c0\u03b8\u2217) = 1M \u2200\u03be). The agent incurs maximum error, T , on 1% of trajectories and is in perfect agreement on the remaining 99% of trajectories, in this case E\u03b8\u2217J(\u03b8, \u03b8\u2217, \u03be) = 1 and E\u03b8\u2217 d0(\u03b8, \u03b8\u2217|\u03be)) = 0.99. Prior analysis would indicate E\u03b8 J(\u03b8, \u03b8\u2217, \u03be) \u2264 100, however Theorem 4.3 shows E\u03b8 J(\u03b8, \u03b8 \u2217, \u03be) \u2264 15.\nWe note a limitation of this analysis is that if an agent has a large number of possible trajectories with a few expected errors on each trajectory, Theorem 4.3 could still yield a pessimistic result. In Sec. 5, we demonstrate how to derive\na soft form of agreement via adding stochasticity to the currently deterministic policy \u03c0\u03b8(at|st) to help alleviate this issue."}, {"heading": "5. Stochastic Off Policy IL", "text": ""}, {"heading": "5.1. Stochastic Supervisor", "text": "In the previous section, Lemma 4.2 illustrated how the expected perfect agreement between the supervisor and the agent can control the total variation distance of their distributions. However, for long time horizon tasks perfect agreement on even a single trajectory may be hard to achieve in practice.\nA reason for this limitation is the deterministic distribution defined on \u03c0\u03b8\u2217(at|st). Under this distribution, any deviation in action between the supervisor and agent along a trajectory could result in very different states being visited. We can overcome this by adding stochasicity to the supervisor\u2019s distribution. Thus, even if the supervisor and agent deviate along a trajectory, with some probability the supervisor could still follow the unknown agent\u2019s policy.\nOne choice of stochasicity to consider is the -greedy probability distribution, which is defined as:\n\u03c0\u03b8(at|st, ) =  1\u2212 if \u03c0\u03b8(st) = at K \u2212 1 if \u03c0\u03b8(st) 6= at\nA reason to consider the -greedy distribution is that it places a uniform prior over all controls \u03c0\u03b8\u2217(s). The choice to weight all other controls equally can be thought of as a conservative prior on the distribution of the learned agent\u2019s policy. To indicate that the supervisor\u2019s distribution is also controlled by , we will denote it as p(\u03be|\u03c0\u03b8\u2217 , ). The effect that this distribution has on Off-Policy Imitation Learning can be shown with the following theorem.\nTheorem 5.1 Denote by \u03c0\u03b8 a policy with an -greedy strategy. The following inequality holds:\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217, J(\u03b8, \u03b8\u2217|\u03be)| \u2264\nT \u221a 1\u2212 (E\u03b8 d (\u03b8, \u03b8\u2217|\u03be))2\nwhere,\nd (\u03b8, \u03b8 \u2217|\u03be) =\n(\nK \u2212 1\n) 1 2J(\u03b8,\u03b8 \u2217|\u03be)\n(1\u2212 ) 12 (T\u2212J(\u03b8,\u03b8 \u2217|\u03be))\n[See Appendix for Proof]\nThe expression E\u03b8 d (\u03b8, \u03b8\u2217|\u03be) in Theorem 5.2 can be considered a soft form of perfect agreement. In this expression,\nevery trajectory that is possible under the agent\u2019s distribution is weighted by the probability of the supervisor applying those actions. In Theorem 4.3 only the subset of trajectories that were in perfect agreement could be considered. We empirically show how noise can lead to better performance over the deterministic supervisor in Sec. 6.\nWhile Theorem 5.2 only applies to -greedy distribution, we can extend it to the more general setting of arbitrary noise distributions with the following Corollary. We note that in this work, we will only focus on the -greedy distribution.\nCorollary 5.2 Denote by \u03c0\u03b8 a policy. The following inequality holds:\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217 J(\u03b8, \u03b8\u2217|\u03be)| \u2264\nT \u221a\u221a\u221a\u221a1\u2212(E\u03b8 T\u22121\u220f t=0 \u221a \u03c0\u03b8\u2217(at|st) )2\nWhile noise injection can reduce the covariate shift, one question is how to best set the level of noise. Intuitively, if the supervisor and agent match perfectly then adding stochasticity to the supervisor\u2019s distribution would cause the total variation distance to grow. Likewise, if the supervisor and agent are in very large disagreement, choosing = K\u22121K and performing random actions is sensible. In the next section, we will use Theorem 5.2 to derive an algorithm to set ."}, {"heading": "5.2. Optimizing the Mismatch", "text": "One technique to choose the correct noise level is to gridsearch over possible values of noise, however in tasks that require real-world execution of the policy this may be difficult. Thus, we will leverage the prior analysis to propose an algorithm that iteratively optimizes the bound in Theorem 5.2 to select . We will write the objective as:\n\u2217 = argmax E\u03b8 d (\u03b8, \u03b8 \u2217|\u03be) (3)\nThe goal of this objective is to maximize a measure of the probability the supervisor overlaps the agent\u2019s distribution. In practice, we will not have access to the true expectation in Eq. 3. Thus, we need to generate trajectory drawn from via C trajectory samples. While one approach is to first estimate 1 C \u2211C c=1 d (\u03b8, \u03b8\n\u2217|\u03bec) , we found that the sampled polynomial terms in d (\u03b8, \u03b8\u2217|\u03be) can cause very high variance when computing this estimate. Thus, we will consider a biased estimate derived from the first-order taylor expansion around the first moment for the variable X = J(\u03b8, \u03b8\u2217|\u03be) (Esmaili, 2006).\nAlgorithm 2 Dart Input: 0 for n = 0 to N \u2212 1 do\nfor m = 1 to M do \u03ben,m \u223c p(\u03be|\u03c0\u03b8\u2217 , n) end for \u03b8n+1 = arg min\n\u03b8\n\u2211n i=0 \u2211M j=1 J(\u03b8, \u03b8 \u2217|\u03bei,j)\nfor c = 1 to C do \u03ben,c \u223c p(\u03be|\u03c0\u03b8n+1) end for J\u0302 = 1C \u2211C c=1 J(\u03b8n+1, \u03b8\n\u2217|\u03ben,c) n+1 = J\u0302 T\nend for\n\u2248 (\nK \u2212 1\n) 1 2E\u03b8 J(\u03b8,\u03b8 \u2217|\u03be)\n(1\u2212 ) 1 2 (T\u2212E\u03b8J(\u03b8,\u03b8 \u2217|\u03be)) (4)\nAn advantage of this term is that the sample estimate is only the surrogate loss of the agent\u2019s policy. E\u03b8 J(\u03b8, \u03b8\u2217|\u03be) is a potentially tamer quantity to estimate than the original polynomial quantities. We note that while this is a biased estimate of the original bound, we have found it to empirically perform well in selecting .\nWhen optimizing Eq. 4 with repsect to , we can solve for the closed form solution, which has the following form\n\u2217 \u2248 E\u03b8 J(\u03b8, \u03b8 \u2217|\u03be)\nT\nWe note this closed-form solution is for the approximation\u2019s objective not the original objective.\nSince the quantity requires knowing the current agent\u2019s performance, we propose optimizing it in an iterative algorithm, called Dart, defined in Algorithm 2. Dart operates for N iterations. At each iteration a batch size of M demonstrations are sampled from p(\u03be|\u03c0\u03b8\u2217 , n), where n is the current estimate of the best . The algorithm then optimizes for \u03b8n+1 on the aggregate dataset, similar to DAgger. Dart then evaluates the current agent policies via sampling C times from p(\u03be|\u03c0\u03b8n) and sets n+1 = 1C \u2211C c=1 J(\u03b8n+1,\u03b8 \u2217|\u03ben,c) T to compute the level of noise to inject.\nWe note that Dart does require the additional overhead of policy evaluation at each iteration. While this can be an issue, Dart is intended for large batch updates where the data needed to train is much larger than for evaluation (i.e. M >> C). Furthermore, it is common practice to evaluate the performance of the agent\u2019s policy after each update (Laskey et al., 2016a). In these situations, our method would need no additional samples.\nA key question of this algorithm is how increasingM affects performance. The insight of Dart is that as M becomes large the current agent\u2019s policy will become a worse prior for the final agent\u2019s distribution. Thus, the prior to sample from may be concentrated around the policy the agent is trying to learn (i.e. the supervisors). Dart encapsulates this intuition by setting as function of the agent\u2019s current error E\u03b8n J(\u03b8n, \u03b8 \u2217|\u03be)."}, {"heading": "6. Experiments", "text": "In the experiments, we consider two domains: Grid-World and a driving simulator. The Grid-World domain has low initial state variance and a low-dimensional state space. The Driving simulator has high initial state variance and state representation based on a high-dimensional images."}, {"heading": "6.1. Grid-World Domain", "text": "In Grid-World, we have an agent that is trying to reach a goal state, where it receives +10 reward. The agent receives\u221210 reward if it touches a penalty state. The agent has a state space of (x, y) coordinates and a set of actions consisting of { Left, Right, Forward, Backward, Stay}. The grid size for the environment is 30\u00d7 30. A state is randomly marked as penalty state with probability 8%. For the transition dynamics, p(st+1|st, at), the agent goes to an adjacent state different from the one desired uniformly at random with probability 0.16. The time horizon for the policy is T = 70. The agent must learn to be robust to the noise in the dynamics, reach the goal state and then stay there.\nWe use Value Iteration to compute an optimal supervisor. We run all trials over 50 randomly generated environments. We report normalized performance, where 1.0 represents matching the expected cumulative reward of the deterministic supervisor."}, {"heading": "6.1.1. BATCH LEARNING WITH DART", "text": "In this experiment, we will try to iteratively optimize for the best value of and explore how varying the batch size M affects the two on-policy and off-policy algorithms. We will optimize over batch sizes M = 10 and M = 20. Our policy class is a Linear SVM trained in Sci-Kit Learn (Pedregosa et al., 2011) .\nWe compare the deterministic off-policy approach (i.e. supervised learning) and DAgger. We also compare a stochastic mixing of the supervisor and current robot\u2019s policy, where \u03b2 = 0.5, i.e. with 50% probability the agent\u2019s action is taken instead of the supervisor. This comparison is useful to test the need for the more conservative -greedy distribution. We refer to this method as \u201950/50\u2019.\nFor Dart, we report two initializations 0 = 0.4 and\n0 = 0.5. For DAgger, we set \u03b2 via grid-searching over {0.0, 0.1, ..., 0.9}. We found the best \u03b2 is the indicator function \u03b2n = \u03b4n=0, which corresponds to not sampling from the supervisor after the first iteration. We initialized \u201950/50\u2019, DAgger and Supervised Learning with one demonstration of the deterministic supervisor\u2019s policy. Dart is initialized with a demonstration from the stochastic supervisor corresponding to 0.\nOur results, shown in Fig. 1, suggest that as the batch size grows DAgger\u2019s performance decreases, however our offpolicy algorithm remains unaffected. We attribute this to the fact that the current agent\u2019s distribution, p(\u03be|\u03c0\u03b8n) , becomes a worse prior for p(\u03be|\u03c0\u03b8N ) as the batch size grows. By sampling from p(\u03be|\u03c0\u03b8\u2217 , n), we are able to better predict the distribution the agent will converge to.\nThe traditional off-policy approach (i.e. supervised learning) does poorly in this domain, which is consistent with prior literature (Ross et al., 2010). Finally the stochastic mixing method \u201950/50\u2019 performs poorly compared to both DAgger and Dart.\nWe note that Dart does require an additional 5 samples per iteration to evaluate the policy. In this experiment, we are specifically interested in how batch size affects the methods, thus we removed this potential confound. In Sec 6.2, we will show the overhead of policy evaluation is relatively small for tasks with high initial state variance."}, {"heading": "6.1.2. DART\u2019S SENSITIVITY TO INITIALIZATION", "text": "We will now examine how sensitive Dart is to the selection of 0. We also plot the performance of the chosen 0 if it is held fixed at each iteration. Ideally, we would want Dart to be robust to this parameter and lead the agent to converge to a good policy. We chose a batch size of M = 10 and used a\nLinear SVM as the policy class.\nThe results, shown in Fig. 2, suggests that Dart is robust the selection of 0 as the number of iterations increases. For example, 0 = 0.8 has 0.25 performance when left unchanged for all iterations, but with Dart\u2019s adaptivity it converges to 0.61. We note that there does exist one fixed 0 = 0.5, that converges to value 0.1 higher in normalized performance, than Dart when left unchanged. Thus, if an application allows for exhaustive grid-search of the parameters, it may be more beneficial than an adaptive algorithm."}, {"heading": "6.2. Driving Simulator", "text": "In order to test our algorithms in domains with high state variance, we developed a driving simulator, in which the agent must learn to drive around other vehicles. The other vehicles are randomly placed on the road at each iteration, thus the agent must learn a policy that generalizes to random vehicle configurations.\nEach car has a non-holonomic bicycle car dynamics model with an internal state space of position, acceleration and velocity. The position specifies both translation and rotation: {x, y, \u03c1}. The agent\u2019s car has initial state variance uniform over \u03c1 \u2208 [\u221230, 30]. The other cars\u2019 have uniform initial state variance over translation, but are always driving forward (i.e. \u03c1 = 0.0). There are 5 other vehicles, per track. The agent drives twice as fast as the other vehicles, thus it must learn to navigate around them.\nThe state space of the simulator is gray-scale 8-bit images of the workspace, in the set S = [0, 255]300\u00d7300. The images are centered around the agent\u2019s car at all times. Examples of the simulator can be seen in Fig. 3. The action space of the car is the selection of a steering angle for the car A = {\u221230\u25e6,\u221215\u25e6, 0\u25e6, 15\u25e6, 30\u25e6}. We measure reward in terms of the number of time-steps the agent is able to travel before\ncrashing, which is defined as going off-road or colliding with another vehicle. We terminate the environment if the agent successfully navigates T = 100 time-steps.\nThe supervisor is a cost-based search planner that has access to the internal dynamics of the game engine and a lower dimensional state space. The cost function is weighted to ensure that the planner tries to find collision-free paths that keep the car near the center of the road. The supervisor is able to travel 70 time-steps on average before crashing.\nOur neural net architecture consists of a convolutional layer, with 5 filters 7x7 dimensions, a fully connected layer that maps to a 60 dimensional hidden state space and a final fully connected layer that maps to the controls space. Each layer is seperated by ReLU non-linearities. The architecture design was inspired by that used in (Laskey et al., 2016b). The network was trained using TensorFlow on a GeForce 1080. We used momentum for optimization with learning rate 0.9.\nEach roll-out of a policy takes 5 seconds, where the majority of the computation comes from the supervisor\u2019s planner. In order to produce a large amount of data, we parallelized each roll-out on a 4 core Intel Core i7 CPU. Due to the time requirements of training a network and the ability to parallelize data collection, we are interested in working with a large batch size of data (i.e. M = 100) for 4 iterations. We chose C = 10 for the policy evaluation step of Dart.\nIn this experiment, we compare DAgger, Supervised Learning, \u201950/50\u2019 and Dart. DAgger\u2019s \u03b2 term is set via gridsearch over a smaller batch size of M = 20 for N = 4 iterations. We found that \u03b2n = \u03b4n=0 performed best. We set Dart\u2019s initial parameter 0 = 0.5 because we expected high approximation error. Also, because each rollout has a potentially different time horizon Tc, we compute J\u0302 = \u2211C c=1 100 J(\u03b8n,\u03b8 \u2217|\u03bec) Tc , which re-weights the samples by the maximum fixed time-horizon of T = 100. We also compared against DAgger-50, which is DAgger with a batch size of M = 50 instead of M = 100.\nIn Fig. 4, we plot the performance of the five methods in terms of distance traveled. Each method is averaged over 20 trials of the algorithm and the policy is evaluated on\n20 roll-outs at each iteration. Dart is robust to the large batch sizes, which results in more robust policy earlier on. Interestingly, as the batch size decreases between DAgger50 and DAgger there is a performance increase. A limitation of this performance gain is that it limits the amount of data that can be collected in parallel."}, {"heading": "7. Conclusion & Future Work", "text": "We contributed a new analysis of off-policy methods that bounds the error caused by covariate shift. We then use this analysis to demonstrate how injecting noise into the supervisor\u2019s policy can reduce the covaraite shift. Finally, we contributed an algorithm, Dart, that enables robustness to the original choice of and accelerates learning.\nOur analysis in Theroem 4.3 illustrates that expected cumulative surrogate loss is not always a good indicator for agent\u2019s performance. The proposed idea of perfect agreement offers new insights to the covariate shift problem by examining error on the trajectory level. In future work, we hope to determine the rate of decay on this quantity using Rademacher Complexity analysis.\nWhile Dart is able to perform well on the domains presented, it may be challenging for large or continuous action spaces. In lage action spaces the problem of predicting the learned agent\u2019s policy will naturally become harder. In future work, we will examine distributions over continuous actions that exhibit concentration around the supervisor\u2019s policy, for example Gaussian distributions. We will also examine alternative ways to set by leveraging work on sample complexity analysis (Bartlett & Mendelson, 2002)\nto estimate how the agent\u2019s loss will decay ahead of time."}, {"heading": "8. Appendix", "text": ""}, {"heading": "8.1. Additional Experiments", "text": "In Grid World, we have a robot that is trying to reach a goal state, at which it receives +10 reward. The robot receives \u221210 reward if it touches a penalty state. The robot also receives a \u22120.02 penalty for every blank state. The robot must learn to be robust to the noise in the dynamics, reach the goal state and then stay there. The robot has a state space of (x, y) coordinates and a set of actions consisting of {Left, Right, Forward, Backward, Stay} state. The grid size for the environment is 30\u00d7 30. 8% of randomly drawn states are marked as a penalties, while only one is a goal state. For the transition dynamics, p(st+1|st, at), the robot goes to an adjacent state different from the one desired uniformly at random with probability 0.16. The time horizon for the policy is T = 70.\nWe use Value Iteration to compute an optimal supervisor. We run all trials over 100 randomly generated environments. We report normalized performance, where 1.0 represents matching the expected cumulative reward of the deterministic supervisor. The robot\u2019s policy is represented as a Linear SVM."}, {"heading": "6.1.3. SWEEPING", "text": "In this experiment, we are interested in what setting of is the best for different types of agent policies classes. Thus, we perform a grid-search over a discretized . We choice a discretization of 0.1 and searched between [0, 1]. We consider two policy classes a Linear SVM and a Decision Tree with depth 4. The reason for two different classes is too see how varying test error effects the best . Each model is trained in Sci-Kit Learn (Pedregosa et al., 2011).\nThe Results shown in Fig. 5, shows performance over a range of parameters. We observed for the Linear SVM, who had on average E\u03b8J(\u03b8, \u03b8\u2217|\u03be) = 0.2 , the best are 0.5 and 0.6. However for the Decision Tree who on average had E\u03b8J(\u03b8, \u03b8\u2217|\u03be) = 0.04 test error, the best noise term was between 0.1 and 0.2. Thus suggesting, the better an agent is able to learn the data the lower the noise term needs to be. Our analysis in Theorem 5.2 also illustrates such a relationship."}, {"heading": "6.2. Theoretical Analysis", "text": "Lemma 4.2 For two deterministic agents with distributions on trajectories p(\u03be|\u03c0\u03b81) and p(\u03be|\u03c0\u03b82) the following inequality holds:\n||p(\u03be|\u03c0\u03b82)\u2212 p(\u03be|\u03c0\u03b81)||TV \u2264 \u221a 1\u2212 (E\u03b82d0(\u03b81, \u03b82|\u03be)) 2\nwhere d0(\u03b81, \u03b82|\u03be) = \u03b4J(\u03b81,\u03b82|\u03be)=0 =\u220fT\u22121 t=0 \u03b4\u03c0\u03b81 (st)=\u03c0\u03b82 (st) or the indicator that the two agents apply the same controls for all states in the trajectory.\nproof 4.2 We begin our proof by restating Le Cam\u2019s Inequality (Le Cam & Yang, 2012), which bounds the Total Variational Distance by the Hellinger Distance, H (p(\u03be|\u03c0\u03b81)||p(\u03be|\u03c0\u03b82)) on two distributions\n||p(\u03be|\u03c0\u03b81)\u2212 p(\u03be|\u03c0\u03b82)||TV \u2264 H (p(\u03be|\u03c0\u03b81)||p(\u03be|\u03c0\u03b82)) \u221a 1\u2212 H 2 (p(\u03be|\u03c0\u03b81)||p(\u03be|\u03c0\u03b82))\n4\nWe will write the Hellinger distance squared as a function of an expectation over p(\u03be|\u03c0\u03b82). We note there is J possible trajectories in the environment.\nH2 (p(\u03be|\u03c0\u03b81)||p(\u03be|\u03c0\u03b82)) =\n= J\u22121\u2211 j=0 (\u221a p(\u03bej |\u03c0\u03b81)\u2212 \u221a p(\u03bej |\u03c0\u03b82) )2\n= J\u22121\u2211 j=0 p(\u03bej |\u03c0\u03b81) + J\u22121\u2211 j=0 p(\u03bej |\u03c0\u03b82) \u2212 2 J\u22121\u2211 j=0 \u221a p(\u03bej |\u03c0\u03b81)p(\u03bej |\u03c0\u03b82)\n= 2 1\u2212 J\u22121\u2211 j=0 \u221a p(\u03bej |\u03c0\u03b81)p(\u03bej |\u03c0\u03b82)  (5) We will now apply the definition of the Markov chain to reduce the product term into an expectation.\nJ\u22121\u2211 j=0 \u221a p(\u03bej |\u03c0\u03b81)p(\u03bej |\u03c0\u03b82)\n= J\u22121\u2211 j=0 p(s0) T\u22121\u220f t=0 p(st+1|at, st) \u221a\u221a\u221a\u221aT\u22121\u220f t=0 \u03c0\u03b82(at|st)\u03c0\u03b81(at|st)\nBecause the agents are deterministic we can conclude that\u220fT\u22121 t=0 \u03c0\u03b8(at|st)\u03c0\u03b8\u2217(at|st) will be either 1 or 0 and the inner product term will only be 1 when they both agree on the given trajectory. Thus, we can rewrite it as follows:\nE\u03b82 \u221a\u221a\u221a\u221aT\u22121\u220f t=0 \u03b4\u03c0\u03b81 (st)=\u03c0\u03b82 (st)\nCorollary 4.2 For a deterministic agent with trajectory distribution p(\u03be|\u03c0\u03b81) and a stochastic agent with trajectory distribution p(\u03be|\u03c0\u03b82):\nH2 (p(\u03be|\u03c0\u03b81)||p(\u03be|\u03c0\u03b82)) =\n2 1\u2212 E\u03b81 \u221a\u221a\u221a\u221aT\u22121\u220f\nt=0\n\u03c0\u03b82(at|st, )  proof 4.2 Note that the steps of the proof of Lemma 6.2 up to Equation 5 apply to stochasic policies, giving:\nH2 (p(\u03be|\u03c0\u03b81)||p(\u03be|\u03c0\u03b82)) =\n2 1\u2212 J\u22121\u2211 j=0 \u221a p(\u03bej |\u03c0\u03b81)p(\u03bej |\u03c0\u03b82)  We can then relate the sum over trajectories to the expectation as follows:\nJ\u22121\u2211 j=0 \u221a p(\u03bej |\u03c0\u03b81)p(\u03bej |\u03c0\u03b82)\n= J\u22121\u2211 j=0 p(s0) T\u22121\u220f t=0 p(st+1|at, st) \u221a\u221a\u221a\u221aT\u22121\u220f t=0 \u03c0\u03b81(at|st)\u03c0\u03b82(at|st)\n= J\u22121\u2211 j=0 p(s0) T\u22121\u220f t=0 p(st+1|at, st) \u221a\u221a\u221a\u221aT\u22121\u220f t=0 \u03b4at=\u03c0\u03b81 (st)\u03c0\u03b82(at|st)\n= J\u22121\u2211 j=0 p(s0) T\u22121\u220f t=0 p(st+1|at, st)\u03b4at=\u03c0\u03b81 (st) \u221a\u221a\u221a\u221aT\u22121\u220f t=0 \u03c0\u03b82(at|st)\n= E\u03b81 \u221a\u221a\u221a\u221aT\u22121\u220f t=0 \u03c0\u03b82(at|st, )\nLemma 4.3 Let P and Q be any distribution on X . Let f : X \u2192 [0, B]. Then\n|EP [f(x)]\u2212 EQ[f(x)]| \u2264 B||P \u2212Q||TV\nproof 4.3\n| \u2211 x p(x)f(x)\u2212 \u2211 x q(x)f(x)|\n= | \u2211 x (p(x)\u2212 q(x))f(x)|\n= | \u2211 x (p(x)\u2212 q(x))|(f(x)\u2212 B 2 )\n+ B 2 ( \u2211 x p(x)\u2212 q(x))|\n\u2264 \u2211 x |p(x)\u2212 q(x)||f(x)\u2212 B 2 |\n\u2264 B 2 \u2211 x |p(x)\u2212 q(x)|\n\u2264 B||P \u2212Q||TV\nThe last line applies the definition of total variational distance, which is ||P \u2212Q||TV = 12 \u2211 x |p(x)\u2212 q(x)|.\nTheorem 4.3 Denote \u03c0\u03b8 a policy found using Off-Policy IL, with a deterministic supervisor, the following inequality holds:\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217 J(\u03b8, \u03b8\u2217|\u03be)| \u2264\nT \u221a 1\u2212 (E\u03b8\u2217 d0(\u03b8, \u03b8\u2217|\u03be))2\nproof 4.3 We first bound the difference between expectations using Lemma 4.3.\n|E\u03b8J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217J(\u03b8, \u03b8\u2217|\u03be)| = \u2264 T ||p(\u03be|\u03c0\u03b8)\u2212 p(\u03be|\u03c0\u03b8\u2217)||TV\nWe then apply Lemma 4.2 to obtain the proof.\nTheorem 5.1 Denote \u03c0\u03b8 a policy found using Off-Policy IL with an -greedy strategy, the following inequality holds:\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217, J(\u03b8, \u03b8\u2217|\u03be)| \u2264\nT \u221a 1\u2212 (E\u03b8 d (\u03b8, \u03b8\u2217|\u03be))2\nwhere,\nd (\u03b8, \u03b8 \u2217|\u03be) =\n(\nK \u2212 1\n) 1 2J(\u03b8,\u03b8 \u2217|\u03be)\n(1\u2212 ) 12 (T\u2212J(\u03b8,\u03b8 \u2217|\u03be))\nproof 5.1 We first bound the difference between expectations using Lemma 4.3.\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217, J(\u03b8, \u03b8\u2217|\u03be)| = \u2264 T ||p(\u03be|\u03c0\u03b8)\u2212 p(\u03be|\u03c0\u03b8\u2217 , )||TV\nWe now apply Le Cam\u2019s Inequality to bound the total variational distance.\n||p(\u03be|\u03c0\u03b8)\u2212 p(\u03be|\u03c0\u03b8\u2217 , )||TV \u2264 H(p(\u03be|\u03c0\u03b8)||p(\u03be|\u03c0\u03b8\u2217 , )) \u221a 1\u2212 H 2(p(\u03be|\u03c0\u03b8)||p(\u03be|\u03c0\u03b8\u2217 , ))\n4\nSimilar to Lemma 4.3, we are interested deriving an expression for the Hellinger distance. We use the result of Corollary 4.2, where \u03b81 is the deterministic learning agent\u2019s policy and \u03b82 is the noise injected supervisor.\nH2 (p(\u03be|\u03c0\u03b8)||p(\u03be|\u03c0\u03b8\u2217 , )) =\n2 1\u2212 E\u03b8 \u221a\u221a\u221a\u221aT\u22121\u220f\nt=0\n\u03c0\u03b8\u2217(at|st, )  We can write the product in terms of the surrogate loss:\nT\u22121\u220f t=0 \u03c0\u03b8\u2217(at|st, ) =\n(\nK \u2212 1\n) 1 2J(\u03b8,\u03b8 \u2217|\u03be)\n(1\u2212 ) 12 (T\u2212J(\u03b8,\u03b8 \u2217|\u03be))\nbecause when the deterministic agent and deterministic supervisor agree, we know the supervisor did not take the action uniformly at random, and T \u2212 J(\u03b8, \u03b8\u2217|\u03be) counts the number of states for which they agree. Combining these terms and simplifying yields the proof.\nCorollary 5.2 Denote by \u03c0\u03b8 a policy. The following inequality holds:\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217 J(\u03b8, \u03b8\u2217|\u03be)| \u2264\nT \u221a\u221a\u221a\u221a1\u2212(E\u03b8 T\u22121\u220f t=0 \u221a \u03c0\u03b8\u2217(at|st) )2\nproof 5.3 We first bound the difference between expectations using Lemma 4.3.\n|E\u03b8 J(\u03b8, \u03b8\u2217|\u03be)\u2212 E\u03b8\u2217, J(\u03b8, \u03b8\u2217|\u03be)| = \u2264 T ||p(\u03be|\u03c0\u03b8)\u2212 p(\u03be|\u03c0\u03b8\u2217 , )||TV\nWe now apply Le Cam\u2019s Inequality to bound the total variational distance.\n||p(\u03be|\u03c0\u03b8)\u2212 p(\u03be|\u03c0\u03b8\u2217 , )||TV \u2264 H(p(\u03be|\u03c0\u03b8)||p(\u03be|\u03c0\u03b8\u2217 , )) \u221a 1\u2212 H 2(p(\u03be|\u03c0\u03b8)||p(\u03be|\u03c0\u03b8\u2217 , ))\n4\nSimilar to Lemma 4.3, we are interested deriving an expression for the Hellinger distance. We use the result of Corollary 4.2, where \u03b81 is the deterministic learning agent\u2019s policy and \u03b82 is the noise injected supervisor.\nH2 (p(\u03be|\u03c0\u03b8)||p(\u03be|\u03c0\u03b8\u2217 , )) =\n2 1\u2212 E\u03b8 \u221a\u221a\u221a\u221aT\u22121\u220f\nt=0\n\u03c0\u03b8\u2217(at|st, )  We can then simplify to yield the result."}], "references": [{"title": "Apprenticeship learning via reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the twentyfirst international conference on Machine learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["Argall", "Brenna D", "Chernova", "Sonia", "Veloso", "Manuela", "Browning", "Brett"], "venue": "Robotics and autonomous systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "Training with exploration improves a greedy stack-lstm parser", "author": ["Ballesteros", "Miguel", "Goldberg", "Yoav", "Dyer", "Chris", "Smith", "Noah A"], "venue": "arXiv preprint arXiv:1603.03793,", "citeRegEx": "Ballesteros et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Bartlett", "Peter L", "Mendelson", "Shahar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2002}, {"title": "Convexity, classification, and risk bounds", "author": ["Bartlett", "Peter L", "Jordan", "Michael I", "McAuliffe", "Jon D"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Searchbased structured prediction", "author": ["Daum\u00e9", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2009}, {"title": "Probability models in engineering and science", "author": ["Esmaili", "Ali"], "venue": null, "citeRegEx": "Esmaili and Ali.,? \\Q2006\\E", "shortCiteRegEx": "Esmaili and Ali.", "year": 2006}, {"title": "Deep learning for realtime atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In NIPS,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Hazan", "Elad", "Kalai", "Adam", "Kale", "Satyen", "Agarwal", "Amit"], "venue": "In Lecture Notes in Computer Science,", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "Imitation learning by coaching", "author": ["He", "Eisner", "Jason", "Daume", "Hal"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "He et al\\.,? \\Q2012\\E", "shortCiteRegEx": "He et al\\.", "year": 2012}, {"title": "Maximum mean discrepancy imitation learning", "author": ["Kim", "Beomjoon", "Pineau", "Joelle"], "venue": "In Robotics Science and Systems,", "citeRegEx": "Kim et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2013}, {"title": "Comparing human-centric and robotcentric sampling for robot deep learning from demonstrations", "author": ["Laskey", "Michael", "Chuck", "Caleb", "Lee", "Jonathan", "Mahler", "Jeffrey", "Krishnan", "Sanjay", "Jamieson", "Kevin", "Dragan", "Anca", "Goldberg", "Ken"], "venue": "arXiv preprint arXiv:1610.00850,", "citeRegEx": "Laskey et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Laskey et al\\.", "year": 2016}, {"title": "Smooth imitation learning for online sequence prediction", "author": ["Le", "Hoang M", "Kang", "Andrew", "Yue", "Yisong", "Carr", "Peter"], "venue": "arXiv preprint arXiv:1606.00968,", "citeRegEx": "Le et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Le et al\\.", "year": 2016}, {"title": "Asymptotics in statistics: some basic concepts", "author": ["Le Cam", "Lucien", "Yang", "Grace Lo"], "venue": "Springer Science & Business Media,", "citeRegEx": "Cam et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cam et al\\.", "year": 2012}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "Levine et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2015}, {"title": "Alvinn: An autonomous land vehicle in a neural network", "author": ["Pomerleau", "Dean A"], "venue": "Technical report, Carnegie-Mellon University,", "citeRegEx": "Pomerleau and A.,? \\Q1989\\E", "shortCiteRegEx": "Pomerleau and A.", "year": 1989}, {"title": "Dataset shift in machine learning", "author": ["Quionero-Candela", "Joaquin", "Sugiyama", "Masashi", "Schwaighofer", "Anton", "Lawrence", "Neil D"], "venue": null, "citeRegEx": "Quionero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quionero.Candela et al\\.", "year": 2009}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "Drew"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "J Andrew"], "venue": "arXiv preprint arXiv:1011.0686,", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael", "Moritz", "Philipp"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "An analysis of active learning strategies for sequence labeling tasks. In Proceedings of the conference on empirical methods in natural language processing, pp. 1070\u20131079", "author": ["Settles", "Burr", "Craven", "Mark"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Settles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Settles et al\\.", "year": 2008}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": null, "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2011}, {"title": "A reduction from apprenticeship learning to classification", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Syed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "In Imitation Learning (IL), an agent learns to mimic a supervisor on a task involving sequential decisions (Argall et al., 2009).", "startOffset": 107, "endOffset": 128}, {"referenceID": 2, "context": "The generality of this approach has led to a wide range of applications: parsing sentence structure (Ballesteros et al., 2016), playing video games (Guo et al.", "startOffset": 100, "endOffset": 126}, {"referenceID": 7, "context": ", 2016), playing video games (Guo et al., 2014), robotic manipulation (Laskey et al.", "startOffset": 29, "endOffset": 47}, {"referenceID": 17, "context": "On-Policy methods, such as DAgger, attempt to remedy this by sampling trajectories using the current agent\u2019s policy (Ross et al., 2010) and querying the supervisor at each state for the correct action.", "startOffset": 116, "endOffset": 135}, {"referenceID": 7, "context": "On-Policy methods have been shown to empirically converge faster than existing OffPolicy methods (Guo et al., 2014).", "startOffset": 97, "endOffset": 115}, {"referenceID": 19, "context": "Similar results have been shown in On-Policy Reinforcement Learning and active learning settings (Schulman et al., 2015; Settles & Craven, 2008).", "startOffset": 97, "endOffset": 144}, {"referenceID": 17, "context": "We measure the difference between controls using a surrogate loss l : A \u00d7 A \u2192 R (Ross et al., 2010; Ross & Bagnell, 2010).", "startOffset": 80, "endOffset": 121}, {"referenceID": 5, "context": "The questions of how to best select the prior, \u03c0\u03b8\u0302 ,and how close the distributions need to be has led to two different classes of algorithms (Syed & Schapire, 2010; Daum\u00e9 et al., 2009)", "startOffset": 142, "endOffset": 185}, {"referenceID": 4, "context": "It is important to note that during optimization of \u03b8 our current indicator loss function l should be replaced with a smooth classification calibrated loss function such as the Hinge Loss (Bartlett et al., 2006).", "startOffset": 188, "endOffset": 211}, {"referenceID": 17, "context": "This notion of a distribution mismatch, or covariate shift, has led to both theoretical and empirical evidence (Ross et al., 2010; Guo et al., 2014) that suggests off-policy learning is not a robust algorithm.", "startOffset": 111, "endOffset": 148}, {"referenceID": 7, "context": "This notion of a distribution mismatch, or covariate shift, has led to both theoretical and empirical evidence (Ross et al., 2010; Guo et al., 2014) that suggests off-policy learning is not a robust algorithm.", "startOffset": 111, "endOffset": 148}, {"referenceID": 9, "context": "A large number of extensions to DAgger have been proposed, such as modifying the supervisor\u2019s policy to be easier to learn (He et al., 2012; Levine et al., 2015) or querying the supervisor for only informative states (Kim & Pineau, 2013; Laskey et al.", "startOffset": 123, "endOffset": 161}, {"referenceID": 14, "context": "A large number of extensions to DAgger have been proposed, such as modifying the supervisor\u2019s policy to be easier to learn (He et al., 2012; Levine et al., 2015) or querying the supervisor for only informative states (Kim & Pineau, 2013; Laskey et al.", "startOffset": 123, "endOffset": 161}, {"referenceID": 8, "context": "\u2016\u2207\u03b8nE\u03b8nJ(\u03b8, \u03b8\u2217|\u03be)\u2016 \u2264 G) (Hazan et al., 2006) and the number of iterations, N .", "startOffset": 24, "endOffset": 44}, {"referenceID": 16, "context": "Under a significant shift between the two distributions, it is unlikely for a learned model to generalize to the shifted test distribution (Quionero-Candela et al., 2009).", "startOffset": 139, "endOffset": 170}, {"referenceID": 17, "context": "This result has been a motivation for many to perform onpolicy methods (Ross et al., 2010; Le et al., 2016; Levine et al., 2015).", "startOffset": 71, "endOffset": 128}, {"referenceID": 12, "context": "This result has been a motivation for many to perform onpolicy methods (Ross et al., 2010; Le et al., 2016; Levine et al., 2015).", "startOffset": 71, "endOffset": 128}, {"referenceID": 14, "context": "This result has been a motivation for many to perform onpolicy methods (Ross et al., 2010; Le et al., 2016; Levine et al., 2015).", "startOffset": 71, "endOffset": 128}, {"referenceID": 17, "context": "supervised learning) does poorly in this domain, which is consistent with prior literature (Ross et al., 2010).", "startOffset": 91, "endOffset": 110}], "year": 2017, "abstractText": "In Imitation Learning, a supervisor\u2019s policy is observed and the intended behavior is learned. A known problem with this approach is covariate shift, which occurs because the agent visits different states than the supervisor. Rolling out the current agent\u2019s policy, an on-policy method, allows for collecting data along a distribution similar to the updated agent\u2019s policy. However this approach can become less effective as the demonstrations are collected in very large batch sizes, which reduces the relevance of data collected in previous iterations. In this paper, we propose to alleviate the covariate shift via the injection of artificial noise into the supervisor\u2019s policy. We prove an improved bound on the loss due to the covariate shift, and introduce an algorithm that leverages our analysis to estimate the level of -greedy noise to inject. In a driving simulator domain where an agent learns an image-to-action deep network policy, our algorithm Dart achieves a better performance than DAgger with 75% fewer demonstrations.", "creator": "LaTeX with hyperref package"}}}