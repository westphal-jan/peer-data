{"id": "1703.10724", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "N-gram Language Modeling using Recurrent Neural Network Estimation", "abstract": "We investigate the effective memory depth of RNN models by using them for n-gram language model (LM) smoothing.", "histories": [["v1", "Fri, 31 Mar 2017 01:21:40 GMT  (25kb)", "https://arxiv.org/abs/1703.10724v1", "10 pages, including references"], ["v2", "Tue, 20 Jun 2017 01:22:18 GMT  (25kb)", "http://arxiv.org/abs/1703.10724v2", "10 pages, including references"]], "COMMENTS": "10 pages, including references", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ciprian chelba", "mohammad norouzi", "samy bengio"], "accepted": false, "id": "1703.10724"}, "pdf": {"name": "1703.10724.pdf", "metadata": {"source": "CRF", "title": "N-GRAM LANGUAGE MODELING USING RECURRENT NEURAL NETWORK ESTIMATION", "authors": ["Ciprian Chelba", "Mohammad Norouzi", "Samy Bengio"], "emails": ["ciprianchelba@google.com", "mnorouzi@google.com", "bengio@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n10 72\n4v 2\n[ cs\n.C L\n] 2\n0 Ju\nn 20\n17 Google Tech Report\nExperiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found the LSTM cell with dropout to be the best model for encoding the n-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM n-gram matches the LSTM LM performance for n = 9 and slightly outperforms it for n = 13. When allowing dependencies across sentence boundaries, the LSTM 13-gram almost matches the perplexity of the unlimited history LSTM LM.\nLSTM n-gram smoothing also has the desirable property of improving with increasing n-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low n-gram orders.\nExperiments on the One Billion Words benchmark show that the results hold at larger scale: while LSTM smoothing for short n-gram contexts does not provide significant advantages over classic N-gram models, it becomes effective with long contexts (n > 5); depending on the task and amount of data it can match fully recurrent LSTM models at about n = 13. This may have implications when modeling short-format text, e.g. voice search/query LMs.\nBuilding LSTM n-gram LMs may be appealing for some practical situations: the state in a n-gram LM can be succinctly represented with (n\u2212 1) \u2217 4 bytes storing the identity of the words in the context and batches of n-gram contexts can be processed in parallel. On the downside, the n-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM."}, {"heading": "1 INTRODUCTION", "text": "A statistical language model (LM) estimates the prior probability values P (W ) for strings of words W in a vocabulary V whose size is usually in the tens or hundreds of thousands. Typically the string W is broken into sentences, or other segments such as utterances in automatic speech recognition which are assumed to be conditionally independent; the independence assumption has certain advantages in practice but is not strictly necessary.\nApplying the chain rule to a sentenceW = w1, w2, . . . , wn we get:\nP (W ) =\nn\u220f\nk=1\nP (wk|w1, w2, . . . , wk\u22121) (1)\nSince the parameter space of P (wk|w1, w2, . . . , wk\u22121) is too large, the language model is forced to put the context Wk\u22121 = w1, w2, . . . , wk\u22121 into an equivalence class determined by a function \u03a6(Wk\u22121). As a result:\nP (W ) \u223c=\nn\u220f\nk=1\nP (wk|\u03a6(Wk\u22121)) (2)\nResearch in languagemodeling consists of finding appropriate equivalence classifiers\u03a6 andmethods to estimate P (wk|\u03a6(Wk\u22121))."}, {"heading": "1.1 PERPLEXITY AS A MEASURE OF LANGUAGE MODEL QUALITY", "text": "A commonly used quality measure for a given model M is related to the entropy of the underlying source and was introduced under the name of perplexity (PPL) Jelinek (1997):\nPPL(W,M) = exp(\u2212 1\nN\nN\u2211\nk=1\nln [PM (wk|Wk\u22121)]) (3)\nTo give intuitive meaning to perplexity, it represents the average number of guesses the model needs to make in order to ascertain the identity of the next word, when running over the test word string W = w1 . . . wN from left to right. It can be easily shown that the perplexity of a language model that uses the uniform probability distribution over words in the vocabulary V equals the size of the vocabulary; a good language model should of course have lower perplexity, and thus the vocabulary size is an upper bound on the perplexity of a given language model.\nVery likely, not all words in the test data are part of the language model vocabulary. It is common practice to map all words that are out-of-vocabulary to a distinguished unknown word symbol, and report the out-of-vocabulary (OOV) rate on test data\u2014the rate at which one encounters OOV words in the test sequenceW\u2014as yet another languagemodel performancemetric besides perplexity. Usually the unknown word is assumed to be part of the language model vocabulary\u2014open vocabulary language models\u2014and its occurrences are counted in the language model perplexity calculation in Eq. (3). A situation less common in practice is that of closed vocabulary language models where all words in the test data will always be part of the vocabulary V ."}, {"heading": "1.2 SMOOTHING", "text": "Since the language model is meant to assign non-zero probability to unseen strings of words (or equivalently, ensure that the cross-entropy of the model over an arbitrary test string is not infinite), a desirable property is that:\nP (wk|\u03a6(Wk\u22121)) > \u01eb > 0, \u2200wk,Wk\u22121 (4)\nalso known as the smoothing requirement.\nThere are currently two dominant approaches for building LMs:\n1.2.1 n-GRAM LANGUAGE MODELS\nThe most widespread paradigm in language modeling makes a Markov assumption and uses the (n\u2212 1)-gram equivalence classification, that is, defines\n\u03a6(Wk\u22121) . = wk\u2212n+1, wk\u2212n+2, . . . , wk\u22121 = h (5)\nA large body of work has accumulated over the years on various smoothing methods for n-gram LMs. The two most popular smoothing techniques are probably Kneser & Ney (1995) and Katz (1987), both making use of back-off to balance the specificity of long contexts with the reliability of estimates in shorter n-gram contexts. Goodman (2001) provides an excellent overview that is highly recommended to any practitioner of language modeling.\nApproaches that depart from the nested features used in back-off n-gram LMs have shown excellent results at the cost of increasing the number of features and parameters stored by the model, e.g. Pelemans et al. (2016)."}, {"heading": "1.2.2 NEURAL LANGUAGE MODELS", "text": "Neural networks (NNLM) have emerged in recent years as an alternative to estimating and storing n-gram LMs. Words (or some other modeling unit) are represented using an embedding vector E(w) \u2208 Rd. A simple NNLM architecture makes the Markov assumption and feeds the concatenated embedding vectors for the words in the n-gram context to one or more layers each consisting\nof an affine transform followed by a non-linearity (typically tanh); the output of the last such layer is then fed to the output layer consisting again of an affine transform but this time followed by an exponential non-linearity that is normalized to guarantee a proper probability over the vocabulary. This is commonly named a feed-forward architecture for an n-gram LM (FF-NNLM), first introduced by Bengio et al. (2001).\nAn alternative is the recurrent NNLM architecture that feeds the embedding of each word E(wk) one at a time, advancing the state S \u2208 Rs of a recurrent cell and producing a new output U \u2208 Ru:\n[Sk, Uk] = RNN(Sk\u22121, E(wk)) (6)\nS0 = 0\nThis provides a representation for the contextWk\u22121 that can be directly plugged into Eq. 2:\n\u03a6(Wk\u22121) = Uk\u22121(Wk\u22121) (7)\nSimilar to the FF-NNLM architecture, the output U of the recurrent cell is then fed to a soft-max layer consisting of an affine transformO followed by an exponential non-linearity properly normalized over the vocabulary.\nThe recurrent cell RNN(\u00b7) can consist of one or more simple affine/non-linearity layers, often called a vanilla RNN architecture, see Mikolov et al. (2010). The LSTM cell due to Hochreiter & Schmidhuber (1997) has proven very effective at modeling long range dependencies and has become the state-of-the-art architecture for language modeling using RNNs, see J\u00f3zefowicz et al. (2016).\nIn this work we approximate unlimited history (R)NN models with n-gram models in an attempt to identify the order n at which they become equivalent from a perplexity point of view. This is a promising direction in a few ways:\n\u2022 the training data can be reduced to n-gram sufficient statistics, and the target distribution presented to the NN n-gram LM in a given context can be a multinomial pmf instead of the one-hot encoding used in on-line training for (R)NN LMs;\n\u2022 unlike many LSTM LM implementations, back-propagation through time for the LSTM n-gram need not be truncated at the begining of segments used to batch the training data;\n\u2022 the state in a n-gram LM can be succinctly represented with (n \u2212 1) \u2217 4 bytes storing the identity of the words in the context; this is in stark contrast with the state S \u2208 Rs for an RNN LM, where s = 1024 or higher, making the n-gram LM much easier to use in decoders such as for ASR/SMT;\n\u2022 similar to Brants et al. (2007), batches of n-gram contexts can be processed in parallel to estimate a sharded (R)NN n-gram model; this is particularly attractive because it allows scaling both the amount of training data and the NNLM size significantly (100X)."}, {"heading": "2 METHOD", "text": "As mentioned in the previous section, the Markov assumption made by n-gram models allows us to present to the NN multinomial training targets specifying the full distribution in a given n-gram context instead of the usual one-hot target specifying the predicted word occurring in a given context instance. In addition, when using multinomial targets we can either weight each training sample by the context count or simply present each context token encountered in the training data along with the conditional multinomial pmf computed from the entire training set.\nWe thus have three main training regimes:\n\u2022 context-weighted multinomial targets\n\u2022 multinomial targets (context count count(h) = 1)\n\u2022 one-hot targets (context count count(h) = 1, word count count(h,w) = 1)\nThe loss function optimized in training is the cross-entropy between the model pmf P (w|h; \u03b8) in some n-gram context h and the relative frequency f(w|h; T ) in the training data T (or development D, or test data E) is computed as:\nH(P, T ) = \u22121/T \u2211\nh\ncount(h) \u2211\nw\nf(w|h; T ) logP (w|h; \u03b8) (8)\nwhere T is the length of the training data T and P (\u00b7; \u03b8) is the n-gram model being evaluated/trained as parameterized by \u03b8.\nThe baseline back-off n-gram models (Katz, interpolated Kneser-Ney) are trained by making a sentence independence assumption. As a result n-gram contexts at the beginning of the sentence are padded to the left to reach the full context length. The same n-gram counting strategy is used when preparing the data for the various NN n-gram LMs that we experimented with. Since RNN LMs are normally trained and evaluated without making this independence assumption by passing the LM state across sentence boundaries, we also evaluated the impact of resetting the RNN LM state at sentence beginning.\nWe next detail the various flavors or NN LM implementations we experimented with.\nFor all NN LMs we represent context words using an embedding vector E(w) \u2208 Rd. Unless otherwise stated, all models are trained to minimize the cross-entropy on training data in Eq. (8), using Adagrad (Duchi et al. (2011)) and gradient norm clipping (Pascanu et al. (2012)); the model parameters are initialized by sampling from a truncated normal distribution of zero mean and a given standard deviation.\nTraining proceeds for a fixed number of epochs for every given point on the grid of hyper-parameters explored for a given model type; the best performingmodel (parameter values) on development data D is retained as the final one to be evaluated on test data E in order to report the model perplexity.\nAll models were implemented using TensorFlow, see Abadi & et al. (2015b).\n2.1 FEED FORWARD n-GRAM LM\nEach word w in the n-gram context h = wk\u2212n+1 . . . wk\u22121 is embedded using the mapping E(w); the resulting vectors are concatenated to form a d \u00b7 (n\u2212 1) dimensional vector that is first fed into a dropout layer Srivastava et al. (2014) and then into an affine layer followed by a tanh non-linearity. The output of this so-called \u201chidden\u201d layer is again fed into a dropout layer and then followed by an affine layer O whose output is of the same dimensionality as the vocabulary. An exponential \u201csoft-max\u201d layer converts the activations produced by the last affine layer into probabilities over the vocabulary.\nTo summarize:\nX = concat(E(wk\u2212n+1), . . . , E(wk\u22121))\nD(X) = dropout(X ;Pkeep)\nY = tanh(H \u00b7D(X) +Hbias)\nD(Y ) = dropout(Y ;Pkeep)\nP (\u00b7|wk\u2212n+1 . . . wk\u22121) = exp(O \u00b7D(Y ) +Obias) (9)\nThe parameters of the model are the embedding matrix E \u2208 Rd\u00d7V , the keep probability for dropout layers Pkeep, the affine input layer parameterized by H \u2208 R s\u00d7(n\u22121)\u00b7d, Hbias \u2208 R s and the output one parameterized by O \u2208 RV\u00d7s, Obias \u2208 R V .\nThe hyper-parameters controlling the training are: number of training epochs, n-gram order, dimensionality of the model parameters d, s, keep probability value, gradient norm clipping value, standard deviation for the initializer and the Adagrad learning rate and initial accumulator value.\n2.2 \u201cVANILLA\u201d RECURRENT n-GRAM LM\nEach word w in the n-gram context h = wk\u2212n+1 . . . wk\u22121 is embedded using the mapping E(w) followed by dropout and then fed in left-to-right order into the RNN cell in Eq. (6). The final\noutput of the RNN cell is then fed first into a dropout layer and then into an affine layer followed by exponential \u201csoft-max\u201d.\nAssuming that we encode the context h = wk\u2212n+1 . . . wk\u22121 with an RNN cell defined as follows (using the running index l = k \u2212 n + 1 . . . k to traverse the context and a dropout layer on the embeddingD(E(wl)) = dropout(E(wl), Pkeep)):\n[Sl, Ul] = tanh(R \u00b7 [Sl\u22121, D(E(wl))] +Rbias)\nSk\u2212n = 0 (10)\nwe pick the last outputUk\u22121 and feed it into a dropout layer followed by an affine layer and soft-max output:\nD(Uk\u22121) = dropout(Uk\u22121;Pkeep)\nP (\u00b7|wk\u2212n+1 . . . wk\u22121) = exp(O \u00b7D(Uk\u22121) +Obias) (11)\nThe parameters of the model are the embedding matrix E \u2208 Rd\u00d7V , the keep probability for dropout layers Pkeep, the RNN affine layer parameterized by R \u2208 R (d+s)\u00d72\u00b7s) and Rbias \u2208 R 2\u00b7s and the output one parameterized by O \u2208 RV\u00d7s and Obias \u2208 R V . Note that we choose to use the same dimensionality s for both S,U \u2208 Rs.\nThe hyper-parameters controlling the training are the same as in the previous section.\n2.3 LSTM RECURRENT n-GRAM LM\nFinally, we replace the \u201cvanilla\u201d RNN cell defined above with a multilayer LSTM cell with dropout. Since this was the most effective model, we experimented with a few options:\n\u2022 forward context encoding: context words h = wk\u2212n+1 . . . wk\u22121 are fed in left-to-right order in the LSTM cell; the LSTM cell output after the last context word wk\u22121 is then fed into the output layer;\n\u2022 reverse context encoding: context words h = wk\u2212n+1 . . . wk\u22121 are fed in left-to-right order in the LSTM cell; the LSTM cell output after the first context word wk\u2212n+1 is then fed into the output layer;\n\u2022 stacked output for either of the above: we concatenate the output vectors along the way and feed that into the output layer;\n\u2022 bidirectional context encoding: we encode the context twice, forward and reverse order respectively, using two separate LSTM cells; the two outputs are then concatenated and fed to the output layer;\n\u2022 forward context encoding with incremental loss with/out exponential decay as a function of the context length\nThe last item above deserves a more detailed explanation. It is possible that the LSTM encoder would benefit from incremental error back-propagation along the n-gram context instead of just one back-propagation step at the end of the context. As such, we modify the loss function to be the cumulative cross-entropy between the relative frequency and the model output distribution at each step in the for loop feeding the n-gram context into the LSTM cell instead of just the last one. Thus amounts to targetting a mix of 1 . . . n-gram target distributions; to have better control over the contribution of different n-gram orders to the loss function, we weigh each loss function by an exponential term exp(\u2212decay \u00b7(n\u22121\u2212 l)). The decay > 0 value controls how fast the contribution to the loss function from lower n-gram orders decays; note that the highest order l = n \u2212 1 has weight 1.0 so a very large value decay = \u221e restores the regular training loss function. For this training regime we only implemented one-hot targets: the amount of data that needs to be fed to the TensorFlow graph would increase significantly for incremental multinomial targets.\nThe hyper-parameters controlling the training are: number of training epochs, n-gram order, embedding dimensionality d, LSTM cell output dimensionality s and number of layers, keep probability value, gradient norm clipping value, standard deviation for the initializer. To match the fully recurrent LSTM LM implemented by the UPenn Treebank TensorFlow tutorial, we estimated all of our\nLSTM n-gram models using gradient descent with variable learning rate: initially the learning rate is constant for a few iterations after which it follows a linear decay schedule. The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively.\nPerhaps a bit of a technicality but it is worth pointing out a major difference between error backpropagation through time (BPTT) as implemented in either of the above and the error backpropagation in the LSTM/RNN n-gram LM: Abadi & et al. (2015a) and J\u00f3zefowicz (2016) implement BPTT by segmenting the training data into non-overlapping segments (of length 35 or 20, respectively)1. The error BPTT does not cross the left boundary of such segments, whereas the LSTM state is of course copied forward. As a result, the first word in a segment is not really contributing to training, and the immediately following ones have a limited effect. This is in contrast to error back-propagation for the LSTM/RNN n-gram LM: the n-gram window slides over the training/test data, and error back-propagation covers the entire n-gram context; the LSTM cell state and output computed for a given n-gram context are discarded once the output distribution is computed."}, {"heading": "3 EXPERIMENTS", "text": ""}, {"heading": "3.1 UPENN TREEBANK CORPUS", "text": "For our initial set of experiments we used the same data set as in Abadi & et al. (2015a), with exactly the same training/validation/test set partition and vocabulary. The training data consists of about one million words, and the vocabulary contains ten thousand words; the validation/test data contains 73760/82430 words, respectively (including the end-of-sentence token). The out-of-vocabulary rate on validation/test data is 5.0/5.8%, respectively.\nAs an initial batch of experiments we trained and evaluated back-off n-gram models using Katz and interpolated Kneser-Ney smoothing. We also used the medium setting in Abadi & et al. (2015a) as an LSTM/RNN LM baseline; since the baseline n-gram models are trained under a sentence independence assumption, we also ran the LSTM/RNN LM baseline by resetting the LSTM state at each sentence beginning. The results are presented in Table 1.\nAs expected Kneser-Ney (KN) is better than Katz, and it does not improve with the n-gram order past a certain value, in this case n = 5. This behavior is due to the fact that the n-gram hit ratio on test data (number of test n-grams that were observed in training) decreases dramatically with the n-gram order: the percentage of covered n-grams2 for n = 1 . . . 9 is 100, 81, 42, 18, 8.6, 5.0, 3.3, 2.5, 2.0, respectively.\nThe medium setting for the LSTM LM in Abadi & et al. (2015a) performs significantly better than the KN baseline. Resetting the state at sentence beginning degrades PPL significantly by 13% relative.\n1We have evaluated the impact of reducing the segment length dramatically, e.g. 4 instead of 35. Much to our surprise, the LSTM PPL increased modestly, from 84 to 88, see the before last row in Table 1; for the One Billion Words experiments using a segment of length 5 did not change PPL at all.\n2For the hit ratio calculation the n-grams are not padded to the left of sentence beginning; if we are to count hit ratios using padded n-grams, the values are: 100, 81, 44.7, 24.0, 16.5, 13.7, 12.5, 11.8, 11.5, respectively.\nWe then trained and evaluated various NN-smoothed n-gram LMs, as described in Section 2. The results are presented in Table 2. The best model among the ones considered is by far the LSTM n-gram. The most significant experimental result is that the LSTM n-gram can match and even outperform the fully recurrent LSTM LM as we increase the order n: n = 9 matches the LSTM LM performance, decreasing the LM perplexity by 34% relative over the Kneser-Ney baseline. LSTM n-gram smoothing also has the desirable property of improving with the n-gram order, unlike the Katz or Kneser-Ney back-off estimators, which can be credited to better feature extraction from the n-gram context.\nMultinomial targets can slightly outperform the one-hot ones although the difference is shrinking as we increase the n-gram order. Weighting the contribution of each context to the loss function by its count did not work; we suspect this is because on-line training does not work well with the Zipf distribution on context counts.\nAmong the various flavors of LSTM models we experimented with, the forward context encoding performs best. The incremental LSTM n-gram with a fairly large decay (decay = 2.0) is slightly better but we do not consider the difference to be statistically significant (it also entails significantly more computation, we need to perform n\u2212 1 back-propagation steps for each input n-gram).\nTo compare with the LSTM RNN LM that does not reset state at sentence beginning we also trained LSTM n-gram models (forward context encoding only) that straddle the sentence beginning. The results are presented in Table 3. Again, we notice that for a large enough order the LSTM n-gram LM comes very close to matching the fully recurrent LSTM baseline."}, {"heading": "3.2 ONE BILLION WORDS BENCHMARK", "text": "In a second set of experiments we used the corpus in Chelba et al. (2013), the same as in J\u00f3zefowicz et al. (2016). For the baseline LSTM model we used the single machine implementation provided by J\u00f3zefowicz (2016); the LSTM n-gram variant was implemented as a minor tweak on this codebase and is thus different from the one used in the UPenn Treebank experiments in Section 3.1.\nWe experimented with the LSTM configuration in Table 3 of J\u00f3zefowicz et al. (2016) for both baseline LSTM and n-gram variant, which are also the default settings in J\u00f3zefowicz (2016): embedding and projection layer dimensionality was 128, one layer with state dimensionality of 2048. Training used Adagrad with gradient clipping by global norm (10.0) and droput (probability 0.1); back-propagation at the output soft-max layer is done using importance sampling as described in J\u00f3zefowicz et al. (2016) with a set 8192 \u201cnegative\u201d samples. An additional set of experiments investigated the benefits of adding one more layer to both baseline and n-gram LSTM.\nThe results are presented in Tables 4-5; unlike the UPenn Treebank experiments, we did not tune the hyper-parameters for the n-gram LSTM and instead just used the same ones as for the LSTM baseline; as a result the perplexity values for the n-gram LSTM may be slightly suboptimal.\nSimilar to the UPenn Treebank experiments, we examined the effect of resetting state at sentence boundaries. As expected PPL dit not change significantly because the sentences in the training and test data were randomized, see Chelba et al. (2013); in fact modeling the sentence independence explicitly is slightly beneficial.\nWe observe that on large amounts of data LSTM smoothing for short n-gram contexts does not provide significant advantages over classic back-off n-gram models. This may have implications for short-format text, e.g. voice search/query LMs. On the other hand, LSTM smoothing becomes very effective with long contexts (n > 5) approaching the fully recurrent LSTM model perplexity at about n = 13.\nTraining times are significantly different between the LSTM baseline and the n-gram variant, with the latter being about an order of magnitude slower due to the fact that the LSTM state is recomputed and discarded for every new training sample."}, {"heading": "4 CONCLUSIONS AND FUTURE WORK", "text": "We investigated the effective memory depth of (R)NN models by using them for word-level n-gram LM smoothing. The LSTM cell with dropout was by far the best (R)NN model for encoding the n-gram state.\nWhen preserving the sentence independence assumption the LSTM n-gram matches the LSTM LM performance for n = 9 and slightly outperforms it for n = 13. When allowing dependencies across sentence boundaries, the LSTM 13-gram almost matches the perplexity of the unlimited history LSTM LM.\nWe can thus conclude that the memory of LSTM LMs seems to be about 9-13 previous words which is not a trivial depth but not that large either.\nCompared to standard n-gram smoothing methods LSTMs have excellent statistical properties: they improvewith the n-gram order well beyond the point where Katz or Kneser-Ney back-off smoothing methods saturate, proving that they are able to extract richer features from the same context. Using multinomial targets in training is only slightly beneficial in this setting, although the advantage over one-hot diminishes with increasing n-gram order.\nExperiments on the One Billion Words benchmark confirm that n-gram LSTMs can match the performance of fully recurrent LSTMs at larger amounts of data.\nBuilding LSTM n-gram LMs is attractive due to the fact that the state in a n-gram LM can be succinctly represented on 4 \u00b7 (n\u2212 1) bytes storing the identity of the context words. This is in stark contrast with the stateH \u2208 Rh for an RNN LM, where h = 1024 or higher, making the n-gram LM easier to use in decoders such as for ASR/SMT. The LM requests in the decoder can be batched, making the RNN LM operation more efficient on GPUs.\nOn the downside, the LSTM encoding for the n-gram context is discarded and cannot be re-used; caching it for frequent LM states is possible."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to thank Oriol Vinyals and Rafa\u0142 J\u00f3zefowicz for support with the baseline implementation of LSTM LMs for UPenn Treebank in Abadi & et al. (2015a) and One Billion Words Benchmark in J\u00f3zefowicz (2016), respectively. We would also like to thank Maxim Krikun for thorough code reviews and useful discussions."}], "references": [{"title": "Recurrent neural networks tutorial (language modeling), 2015a", "author": ["M. Abadi"], "venue": "URL https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html. UPenn Treebank language modeling", "citeRegEx": "Abadi,? \\Q2015\\E", "shortCiteRegEx": "Abadi", "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015b", "author": ["M. Abadi"], "venue": "URL http://tensorflow.org/. Software available from tensorflow.org", "citeRegEx": "Abadi,? \\Q2015\\E", "shortCiteRegEx": "Abadi", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2001}, {"title": "Large language models in machine translation", "author": ["T. Brants", "A.C. Popat", "P. Xu", "F.J. Och", "J. Dean"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "Brants et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2007}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn"], "venue": "CoRR, abs/1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "A bit of progress in language modeling, extended version", "author": ["Joshua Goodman"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Goodman.,? \\Q2001\\E", "shortCiteRegEx": "Goodman.", "year": 2001}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Information Extraction From Speech And Text, chapter 8, pp. 141\u2013142", "author": ["Frederick Jelinek"], "venue": null, "citeRegEx": "Jelinek.,? \\Q1997\\E", "shortCiteRegEx": "Jelinek.", "year": 1997}, {"title": "Single machine implementation of LSTM language model on One Billion Words benchmark using synchronized gradient updates, 2016", "author": ["Rafal J\u00f3zefowicz"], "venue": "URL https://github.com/rafaljozefowicz/lm", "citeRegEx": "J\u00f3zefowicz.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz.", "year": 2016}, {"title": "Exploring the limits of language modeling", "author": ["Rafal J\u00f3zefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "CoRR, abs/1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Estimation of probabilities from sparse data for the language model component of a speech recognizer", "author": ["S. Katz"], "venue": "In IEEE Transactions on Acoustics, Speech and Signal Processing,", "citeRegEx": "Katz.,? \\Q1987\\E", "shortCiteRegEx": "Katz.", "year": 1987}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Kneser and Ney.,? \\Q1995\\E", "shortCiteRegEx": "Kneser and Ney.", "year": 1995}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "CoRR, abs/1211.5063,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Sparse non-negativematrix languagemodeling", "author": ["Joris Pelemans", "Noam Shazeer", "Ciprian Chelba"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "Pelemans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pelemans et al\\.", "year": 2016}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "A commonly used quality measure for a given model M is related to the entropy of the underlying source and was introduced under the name of perplexity (PPL) Jelinek (1997):", "startOffset": 157, "endOffset": 172}, {"referenceID": 10, "context": "The two most popular smoothing techniques are probably Kneser & Ney (1995) and Katz (1987), both making use of back-off to balance the specificity of long contexts with the reliability of estimates in shorter n-gram contexts.", "startOffset": 79, "endOffset": 91}, {"referenceID": 6, "context": "Goodman (2001) provides an excellent overview that is highly recommended to any practitioner of language modeling.", "startOffset": 0, "endOffset": 15}, {"referenceID": 6, "context": "Goodman (2001) provides an excellent overview that is highly recommended to any practitioner of language modeling. Approaches that depart from the nested features used in back-off n-gram LMs have shown excellent results at the cost of increasing the number of features and parameters stored by the model, e.g. Pelemans et al. (2016).", "startOffset": 0, "endOffset": 333}, {"referenceID": 2, "context": "This is commonly named a feed-forward architecture for an n-gram LM (FF-NNLM), first introduced by Bengio et al. (2001). An alternative is the recurrent NNLM architecture that feeds the embedding of each word E(wk) one at a time, advancing the state S \u2208 R of a recurrent cell and producing a new output U \u2208 R: [Sk, Uk] = RNN(Sk\u22121, E(wk)) (6) S0 = 0", "startOffset": 99, "endOffset": 120}, {"referenceID": 11, "context": "The recurrent cell RNN(\u00b7) can consist of one or more simple affine/non-linearity layers, often called a vanilla RNN architecture, see Mikolov et al. (2010). The LSTM cell due to Hochreiter & Schmidhuber (1997) has proven very effective at modeling long range dependencies and has become the state-of-the-art architecture for language modeling using RNNs, see J\u00f3zefowicz et al.", "startOffset": 134, "endOffset": 156}, {"referenceID": 11, "context": "The recurrent cell RNN(\u00b7) can consist of one or more simple affine/non-linearity layers, often called a vanilla RNN architecture, see Mikolov et al. (2010). The LSTM cell due to Hochreiter & Schmidhuber (1997) has proven very effective at modeling long range dependencies and has become the state-of-the-art architecture for language modeling using RNNs, see J\u00f3zefowicz et al.", "startOffset": 134, "endOffset": 210}, {"referenceID": 9, "context": "The LSTM cell due to Hochreiter & Schmidhuber (1997) has proven very effective at modeling long range dependencies and has become the state-of-the-art architecture for language modeling using RNNs, see J\u00f3zefowicz et al. (2016). In this work we approximate unlimited history (R)NN models with n-gram models in an attempt to identify the order n at which they become equivalent from a perplexity point of view.", "startOffset": 202, "endOffset": 227}, {"referenceID": 3, "context": "\u2022 the training data can be reduced to n-gram sufficient statistics, and the target distribution presented to the NN n-gram LM in a given context can be a multinomial pmf instead of the one-hot encoding used in on-line training for (R)NN LMs; \u2022 unlike many LSTM LM implementations, back-propagation through time for the LSTM n-gram need not be truncated at the begining of segments used to batch the training data; \u2022 the state in a n-gram LM can be succinctly represented with (n \u2212 1) \u2217 4 bytes storing the identity of the words in the context; this is in stark contrast with the state S \u2208 R for an RNN LM, where s = 1024 or higher, making the n-gram LM much easier to use in decoders such as for ASR/SMT; \u2022 similar to Brants et al. (2007), batches of n-gram contexts can be processed in parallel to estimate a sharded (R)NN n-gram model; this is particularly attractive because it allows scaling both the amount of training data and the NNLM size significantly (100X).", "startOffset": 718, "endOffset": 739}, {"referenceID": 5, "context": "(8), using Adagrad (Duchi et al. (2011)) and gradient norm clipping (Pascanu et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 5, "context": "(8), using Adagrad (Duchi et al. (2011)) and gradient norm clipping (Pascanu et al. (2012)); the model parameters are initialized by sampling from a truncated normal distribution of zero mean and a given standard deviation.", "startOffset": 20, "endOffset": 91}, {"referenceID": 0, "context": "All models were implemented using TensorFlow, see Abadi & et al. (2015b).", "startOffset": 50, "endOffset": 73}, {"referenceID": 16, "context": "wk\u22121 is embedded using the mapping E(w); the resulting vectors are concatenated to form a d \u00b7 (n\u2212 1) dimensional vector that is first fed into a dropout layer Srivastava et al. (2014) and then into an affine layer followed by a tanh non-linearity.", "startOffset": 159, "endOffset": 184}, {"referenceID": 0, "context": "The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively.", "startOffset": 141, "endOffset": 164}, {"referenceID": 0, "context": "The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively.", "startOffset": 141, "endOffset": 207}, {"referenceID": 0, "context": "The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively. Perhaps a bit of a technicality but it is worth pointing out a major difference between error backpropagation through time (BPTT) as implemented in either of the above and the error backpropagation in the LSTM/RNN n-gram LM: Abadi & et al. (2015a) and J\u00f3zefowicz (2016) implement BPTT by segmenting the training data into non-overlapping segments (of length 35 or 20, respectively).", "startOffset": 141, "endOffset": 470}, {"referenceID": 0, "context": "The hyper-parameters controlling this schedule were not optimized but rather we used the same values as in the RNN LM tutorial provided with Abadi & et al. (2015a) or the implementation in J\u00f3zefowicz (2016), respectively. Perhaps a bit of a technicality but it is worth pointing out a major difference between error backpropagation through time (BPTT) as implemented in either of the above and the error backpropagation in the LSTM/RNN n-gram LM: Abadi & et al. (2015a) and J\u00f3zefowicz (2016) implement BPTT by segmenting the training data into non-overlapping segments (of length 35 or 20, respectively).", "startOffset": 141, "endOffset": 492}, {"referenceID": 0, "context": "For our initial set of experiments we used the same data set as in Abadi & et al. (2015a), with exactly the same training/validation/test set partition and vocabulary.", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "For our initial set of experiments we used the same data set as in Abadi & et al. (2015a), with exactly the same training/validation/test set partition and vocabulary. The training data consists of about one million words, and the vocabulary contains ten thousand words; the validation/test data contains 73760/82430 words, respectively (including the end-of-sentence token). The out-of-vocabulary rate on validation/test data is 5.0/5.8%, respectively. As an initial batch of experiments we trained and evaluated back-off n-gram models using Katz and interpolated Kneser-Ney smoothing. We also used the medium setting in Abadi & et al. (2015a) as an LSTM/RNN LM baseline; since the baseline n-gram models are trained under a sentence independence assumption, we also ran the LSTM/RNN LM baseline by resetting the LSTM state at each sentence beginning.", "startOffset": 67, "endOffset": 645}, {"referenceID": 0, "context": "The medium setting for the LSTM LM in Abadi & et al. (2015a) performs significantly better than the KN baseline.", "startOffset": 38, "endOffset": 61}, {"referenceID": 4, "context": "In a second set of experiments we used the corpus in Chelba et al. (2013), the same as in J\u00f3zefowicz et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 4, "context": "In a second set of experiments we used the corpus in Chelba et al. (2013), the same as in J\u00f3zefowicz et al. (2016). For the baseline LSTM model we used the single machine implementation provided by J\u00f3zefowicz (2016); the LSTM n-gram variant was implemented as a minor tweak on this codebase and is thus different from the one used in the UPenn Treebank experiments in Section 3.", "startOffset": 53, "endOffset": 115}, {"referenceID": 4, "context": "In a second set of experiments we used the corpus in Chelba et al. (2013), the same as in J\u00f3zefowicz et al. (2016). For the baseline LSTM model we used the single machine implementation provided by J\u00f3zefowicz (2016); the LSTM n-gram variant was implemented as a minor tweak on this codebase and is thus different from the one used in the UPenn Treebank experiments in Section 3.", "startOffset": 53, "endOffset": 216}, {"referenceID": 9, "context": "We experimented with the LSTM configuration in Table 3 of J\u00f3zefowicz et al. (2016) for both baseline LSTM and n-gram variant, which are also the default settings in J\u00f3zefowicz (2016): embedding and projection layer dimensionality was 128, one layer with state dimensionality of 2048.", "startOffset": 58, "endOffset": 83}, {"referenceID": 9, "context": "We experimented with the LSTM configuration in Table 3 of J\u00f3zefowicz et al. (2016) for both baseline LSTM and n-gram variant, which are also the default settings in J\u00f3zefowicz (2016): embedding and projection layer dimensionality was 128, one layer with state dimensionality of 2048.", "startOffset": 58, "endOffset": 183}, {"referenceID": 9, "context": "We experimented with the LSTM configuration in Table 3 of J\u00f3zefowicz et al. (2016) for both baseline LSTM and n-gram variant, which are also the default settings in J\u00f3zefowicz (2016): embedding and projection layer dimensionality was 128, one layer with state dimensionality of 2048. Training used Adagrad with gradient clipping by global norm (10.0) and droput (probability 0.1); back-propagation at the output soft-max layer is done using importance sampling as described in J\u00f3zefowicz et al. (2016) with a set 8192 \u201cnegative\u201d samples.", "startOffset": 58, "endOffset": 502}, {"referenceID": 4, "context": "As expected PPL dit not change significantly because the sentences in the training and test data were randomized, see Chelba et al. (2013); in fact modeling the sentence independence explicitly is slightly beneficial.", "startOffset": 118, "endOffset": 139}, {"referenceID": 0, "context": "We would like to thank Oriol Vinyals and Rafa\u0142 J\u00f3zefowicz for support with the baseline implementation of LSTM LMs for UPenn Treebank in Abadi & et al. (2015a) and One Billion Words Benchmark in J\u00f3zefowicz (2016), respectively.", "startOffset": 137, "endOffset": 160}, {"referenceID": 0, "context": "We would like to thank Oriol Vinyals and Rafa\u0142 J\u00f3zefowicz for support with the baseline implementation of LSTM LMs for UPenn Treebank in Abadi & et al. (2015a) and One Billion Words Benchmark in J\u00f3zefowicz (2016), respectively.", "startOffset": 137, "endOffset": 213}], "year": 2017, "abstractText": "We investigate the effective memory depth of RNN models by using them for n-gram language model (LM) smoothing. Experiments on a small corpus (UPenn Treebank, one million words of training data and 10k vocabulary) have found the LSTM cell with dropout to be the best model for encoding the n-gram state when compared with feed-forward and vanilla RNN models. When preserving the sentence independence assumption the LSTM n-gram matches the LSTM LM performance for n = 9 and slightly outperforms it for n = 13. When allowing dependencies across sentence boundaries, the LSTM 13-gram almost matches the perplexity of the unlimited history LSTM LM. LSTM n-gram smoothing also has the desirable property of improving with increasing n-gram order, unlike the Katz or Kneser-Ney back-off estimators. Using multinomial distributions as targets in training instead of the usual one-hot target is only slightly beneficial for low n-gram orders. Experiments on the One Billion Words benchmark show that the results hold at larger scale: while LSTM smoothing for short n-gram contexts does not provide significant advantages over classic N-gram models, it becomes effective with long contexts (n > 5); depending on the task and amount of data it can match fully recurrent LSTM models at about n = 13. This may have implications when modeling short-format text, e.g. voice search/query LMs. Building LSTM n-gram LMs may be appealing for some practical situations: the state in a n-gram LM can be succinctly represented with (n\u2212 1) \u2217 4 bytes storing the identity of the words in the context and batches of n-gram contexts can be processed in parallel. On the downside, the n-gram context encoding computed by the LSTM is discarded, making the model more expensive than a regular recurrent LSTM LM.", "creator": "LaTeX with hyperref package"}}}