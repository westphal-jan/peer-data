{"id": "1703.10757", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "Diabetic Retinopathy Detection via Deep Convolutional Networks for Discriminative Localization and Visual Explanation", "abstract": "We proposed a deep learning method for interpretable diabetic retinopathy (DR) detection. The visual-interpretable feature of the proposed method is achieved by adding the regression activation map (RAM) after the global averaging pooling layer of the convolutional networks (CNN). With RAM, the proposed model can localize the discriminative regions of an retina image to show the specific region of interest in terms of its severity level. We believe this advantage of the proposed deep learning model is highly desired for DR detection because in practice, users are not only interested with high prediction performance, but also keen to understand the insights of DR detection and why the adopted learning model works. In the experiments conducted on a large scale of retina image dataset, we show that the proposed CNN model can achieve high performance on DR detection compared with the state-of-the-art while achieving the merits of providing the RAM to highlight the salient regions of the input image.", "histories": [["v1", "Fri, 31 Mar 2017 05:10:56 GMT  (9185kb,D)", "http://arxiv.org/abs/1703.10757v1", null], ["v2", "Mon, 3 Apr 2017 16:44:23 GMT  (9185kb,D)", "http://arxiv.org/abs/1703.10757v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["zhiguang wang", "jianbo yang"], "accepted": false, "id": "1703.10757"}, "pdf": {"name": "1703.10757.pdf", "metadata": {"source": "CRF", "title": "Diabetic Retinopathy Detection via Deep Convolutional Networks for Disciminative Localization and Visual Explanation", "authors": ["Zhiguang Wang", "Jianbo Yang"], "emails": ["jianbo.yang}@ge.com"], "sections": [{"heading": "1. Introduction", "text": "Diabetes is an widespread disease in the world, and up to 2014 around 422 million people worldwide have this disease1. Diabetic retinopathy (DR) is an eye disease caused by the long-standing diabetes. Basically, DR affects blood vessels in the light-sensitive tissue (i.e. retina). It becomes the leading cause of vision impairment and blindness for working-age adults in the world today[15], and around half of Americans with diabetes have this disease to some extent. A widely-known challenge for DR is that it has no early warning sign, even for diabetic macular edema. Thus, it is highly desired that DR can be detected in time. Unfortunately, in practice the current DR detection solution is nearly infeasible to meet this requirement. Specifically, the\n\u2217The authors contribute equally to this work. 1http://www.who.int/mediacentre/factsheets/fs312/en/\ncurrent solution requires a well-trained clinician to manually evaluate digital color fundus photographs of retina, and DR is identified by locating the lesions associated with vascular abnormalities due to diabetes. Though this current solution is effective, it is time-consuming and highly relies on the expertise of well-training practitioners. To solve this issue, in the past few years considerable efforts have been put on developing an automated solution for DR detection.\nMost of previous automated solutions consists of two parts: feature extraction and detection/prediction algorithm [13, 15, 16, 20]. Feature extraction is the main focus as standard machine learning algorithms can be directly used as the detection/prediction algorithm. This type of approaches are effective to some extent but also suffer from several shortcomings. First, as reviewed in Section 2, the extracted features are all hand-crafted features. Thus, these features highly depend on the parameters of the used feature extraction tools and they are sensitive to the quality of fundus photography, like object view, exposedness, artifacts, noise, out-of-focus, etc. Second, feature extraction is a solo task rather than embedded into the whole DR detection framework. The above-mentioned features extraction methods can be considered as the universal image feature extraction methods that are applicable to most computer vision tasks, and they are not dedicated to the specific task, e.g., DR detection task considered in this paper. It is worth noting that color fundus photography is more challenging than the standard scene or object images that most image feature extraction methods were developed based on, since the key signals are often tiny in fundus photography and they often look indiscriminating from noise and artifacts. Thus, these two challenges make it highly desirable to develop a systematical feature representation approach to effectively characterize the nature of features particularly related to the DR detection task.\nRecently, the convolutional neural networks (CNN) has achieved tremendous success in computer vision area. It can model high-level abstractions in data relative to specific prediction task [4, 5, 8, 9, 21]. In CNN, a multiple layers\nar X\niv :1\n70 3.\n10 75\n7v 1\n[ cs\n.C V\n] 3\n1 M\nar 2\nnetwork is built up for automating feature design. Specifically, each layer in deep architecture performs a non-linear transformation on the outputs of the previous layer, so that the data are represented by a hierarchy of features from lowlevel to high-level. The key attribute of the CNN is conducting different processing units (e.g. convolution, pooling, sigmoid/hyperbolic tangent squashing, rectifier and normalization ) alternatively. Such a variety of processing units can yield an effective nonlinear representation of local salience of the signals. Then, the deep architecture allows multiple layers of these processing units to be stacked, so that this deep learning model can characterize the salience of signals in different scales. Also, in CNN, feature extraction and prediction algorithm are unified as a single model. Thus, the extracted features own more discriminative power, since the entire CNN model is trained under the supervision of output labels. Briefly speaking, the features extracted by the CNN are task dependent and non-handcrafted.\nIn this paper, we also adopt CNN as the key predictive algorithm, but aim to develop a more efficient CNN architecture that is particularly useful for large-scale dataset. Specifically, the CNN we built has no fully connected layer and only have convolutional and pooling layers. This setting significantly reduces the number of parameters (fullyconnected layers often bring more parameters than convolutional layers in the conventional CNN) and provides better conditions for interpretability of neural network as presented below. We show in experiments that with less parameters and no fully-connected layers the proposed CNN architecture can achieve the comparative prediction performance. The key advantage of the proposed network structure is that it can provide a regression activation maps (RAM) of input image to show the contribution score of each pixel of input image for DR detection task. This RAM output, to some extent, somehow mitigates the well-known uninterpretable shortcoming of CNN as a black box method. We believe that this RAM output make the proposed solution more self-explained and can motivate the practitioners to trace the cause of the disease for every patient."}, {"heading": "2. Related Work", "text": "The two-step (i.e., feature extraction and prediction) automated DR detection approaches dominated the field of DR detection for many years. Given color fundus photography, this type of approaches often extracted visual features from the images on the parts of blood vessels, fovea and optic disc [13, 20]. The generic feature extraction methods developed in computer vision area were widely used here, e.g, hough transform, gabor filters and intensity variations. With the extracted features, an object detection or object registration algorithm like support vector machines and k-NN were used to identify and localize exudates and hemorrhages [15, 16]. As mentioned before, this type of\napproaches are not as effective as the recent deep learning approaches, such as [2, 10, 14, 18]. All these deep learning approaches adopted the standard architecture like AlexNet and GoogLeNet to build their CNN, and based on the experimental results these deep learning approaches significantly outperform the traditional two-step approaches. Moreover, the recent DR detection competition held in Kaggle [1]2 witnessed that all top solutions adopted CNN as the key algorithm. However, all these CNN approaches require complex neural network structures, and it is hard for practitioners to understand the insight of CNN and clearly explain that which region of the color fundus photography is the main cause of the disease.\nUnderstanding the insights of CNN has always been a pain point, though CNN yields excellent predictive performance. It is well-known that deriving theoretical results is quite challenging due to the nonliner and non-convex nature of CNN. To mitigate this issue, considerable efforts have been put on visualizing the CNN. A deconvolutional networks approach was proposed to visualize activated pattern in each hidden unit [22]. This method is limited as it is hard to summarize all hidden units\u2019s patterns into one pattern, and also only the hidden neurons in the hidden layers are analyzed though the networks considered also contain the fully-connected layers. The work [3, 12, 23] and the reference therein include the objection location task besides the conventional object classification problem, so their CNN can predict the label of an image and also identify the region of the object related to the class label. Though this type of CNNs can predict the location of the object of interest, it still cannot reveal the insight of CNN. Recently, [6, 11] have presented the methods to invert the representation of images in each layer of the CNN. However, these approaches can only indicate what information is preserved in each layer of the CNN.\nThe most work most related to our method is [24] in which class activation map is proposed to characterize the weighted activation maps after global average pooling or global maximum pooling layer. This idea has recently been generalized to time series analysis to localize the significant regions in the raw data [19]. In this paper, we extend the method [24] from a classification to a regression setting and shed light on DR detection problem."}, {"heading": "3. Regression Activation Maps (RAM)", "text": "Inspired by [24], we present in this section the idea of generating the RAM of an input image to localize the discriminative region towards the regression outcomes. It is known that the convolutional units of each layers of CNN act as visual concept detectors to identify low-level concepts like textures or materials, to high-level concepts\n2https://www.kaggle.com/c/diabetic-retinopathy-detection\nlike objects or scenes. Deeper into the network, the units become increasingly discriminative. However, the fullyconnected layers will make it difficult to identify the importance of different units for identifying the output labels (regression values, in our networks). Instead, using GAP and the linear output unit, we can directly visualize the region of interest (ROI) that are most discriminative for a given regression value. As we use regression for the purpose of classification, each single RAM obtained for each single image explicitly depict the ROI on different clinical level.\nThe network architectures of our convolutional nets are shown in Table 1. Since we consider regression problem the output layer has one neuron outputting a real value. The key difference between our neural network and conventional neural networks like AlexNet [7] and GoogLeNet [17] lie in that our network uses global averaging pooling (GAP) layer to connect the last convolutional layer and the output layer, instead of using fully-connected layers. The idea of GAP layer is that each neuron in GAP obtains the spatial average of the feature maps from the last convolutional layer so that the value of each neuron in GAP reflects its contribution to the final prediction. Specifically, supposing the last convolutional layer contains K feature maps {gk(i, j)|\u2200i, j}, k = 1, ...,K and (i, j) is the spatial coordinate locating an entry in the feature map k. In the GAP layer, each feature map gk(i, j) in the last convolutional layer is mapped into a scaler tk by the function tk = \u2211 i,j gk(i, j). Then, the weighted sum of the output of\nthe GAP layer y\u0302 = \u2211K\nk=1 tkwk is the value of the neuron in the output layer, where y\u0302 is the predicted label and wk is the weight of neuron k for the output of the global averaging pooling layer.\nGiven the network structure in Table 1, the regression activation maps (RAM) is defined as below:\nG(i, j) = K\u2211 k=1 gk(i, j)wk\nThus, RAM is essentially a weighted sum of the feature maps in the last convolutional layer. The weights herein are the connections between the outputs of the global averaging pooling layer and the neuron in the output layer. Therefore, the findal prediction can also expressed as:\ny\u0302 = K\u2211 k=1 wk \u2211 i,j gk(i, j) = \u2211 i,j G(i, j)\nIntuitively, RAM contains the immediate information for prediction (feature maps in the last convolutional layer and weights before the final regression output), and also maintain the correspondence between last convolutional feature map and input images. Therefore, RAM can localize the discriminative region towards the regression outcomes. The illustration of the adopted neural network structure and RAM are show in Figure 1."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Datasets", "text": "The color retina images are downloaded from the Kaggle website3. The training dataset contains 35126 high resolution images under a variety of imaging conditions. These retina images were obtained from a group of subjects, and for each subject two images were obtained for left and right eyes, respectively. The labels were provided by clinicians who rated the presence of diabetic retinopathy in each image by a scale of \u201c0, 1, 2, 3, 4\u201d, which represent \u201cno DR\u201d, \u201cmild\u201d, \u201cmoderate\u201d, \u201csevere\u201d, \u201cproliferative DR\u201d respectively. As mentioned in the description of the dataset, the images in the dataset come from different models and types of camera, which can affect the visual appearance of left\n3https://www.kaggle.com/c/diabetic-retinopathy-detection/data\nvs. right. The samples images are shown in Fig 2. Also, the dataset doesn\u2019t have the equal distributions among the 5 scales. As one can expect, normal data with label \u201c0\u201d is the biggest class in the whole dataset, while \u201cproliferative DR\u201d data is the smallest class. Fig 3 shows counts of images for different scales in the training dataset."}, {"heading": "4.2. Benchmark method", "text": "We consider the publicly-available method [2] as the benchmark method, which was ranked as the second place in the Kaggle competition. This method crops away all background and resize the images to squares of 128, 256 and 512 pixels. The interested readers may refer to [2] for more detailed settings of the baseline method. We summarize the main features of the baseline methods as follow:\nResampling First, sample all classes such that all classes\n0 1 2 3 4 Scales\n0\n5000\n10000\n15000\n20000\n25000\nC o u n ts\ni n t\nra in\nin g d\na ta\nInitialization and pretraining Orthogonal initialization is used to initialize weights and biases. First, train smaller networks on 128 pixel images. Then, use the trained weights to (partially) initialize medium networks for training on 256 pixel images. Finally, use the trained weights of medium networks to (partially) initialize large network for training on 512 pixel images.\nData augmentation The common image transformation like translation, stretching, rotation, flipping and color augmentation are used for data augmentation.\nFeature blending Use the last pooling layer of the convolutional networks as extracted features, and then blend all these extracted features from 50 networks outputs (with different augmentation). Then, a fully-connected neural network is used on the blended features as the final predictor."}, {"heading": "4.3. Experimental Settings", "text": "We trained our convolutional neural network in Table 1 on a single Tesla-P100 GPU. For nonlinearity, we use leaky (0.01) rectifier units following each convolutional layer. The networks are trained with Nesterov momentum with fixed schedule over 250 epochs. For the nets on 256 and 128 pixel images, we stop training after 200 epochs. L2 weight decay with factor 0.0005 are applied to all layers. As we treat the problem as a regression problem, the loss function is mean squared error.The convolutional networks have untied biases. Batch size is fixed at 32 for all networks. 4. Following the evaluation setting [1], the quadratic weighted Kappa score is adopted as the performance metric of prediction. Specifically, the predicted regression values are discretized at the thresholds (0.5, 1.5, 2.5, 3.5) to obtain integer levels for computing the Kappa scores and making submissions. All the features mentioned in Section 4.2 were also adopted in our model training."}, {"heading": "4.4. Performance of Kappa Score", "text": "Following [2], we split 35126 images into training and validation datasets in a ratio of 9 to 1 for local evaluation purpose, and we also submit our prediction results on the test dataset to Kaggle to obtain the Kappa score. Table 2 summarizes the performance of both benchmark and our approach on the test dataset. By simply replacing the fullyconnected layer with the global average pooling layer, our networks achieved very competitive Kappa score compared with the benchmark while reducing the parameter size by about 21.8% and speed-up the training by 11.8%-13.1%. As mentioned in Section 4.2, feature blending strategy was used. Thus, the final Kappa score is an average of six per patient blends for the two convolutional network architectures and three different sets of trained weights.\nConsidering that the key signals in the retina images like Micro-aneurysms are very small with respect to the retina, we also evaluate the performance of the our model with respect to different size of input images. On the validation set, our networks achieve the Kappa scores of around 0.70\n4Codes are available at https://github.com/cauchyturing/kaggle diabetic RAM.\nfor 256 pixel images, 0.80 for 512 pixel images and 0.81 for 768 pixel images on both Net-5 and Net-4 settings without feature blending. This observation suggests that the larger input images the better prediction performance but there is no much gain when the image size is greater than 512. Taking computation cost into consideration, we limit the input image size to be not greater than 512."}, {"heading": "4.5. Discriminative Localization by RAM", "text": ""}, {"heading": "4.5.1 Network settings for RAM", "text": "To generate RAM, we used Net-5 with the 128 and 256 pixel images as the input. We also removed several convolutional layers of Net-5 for each input size to increase the resolution of RAM, since the localization ability of RAM can be significantly improved when the last convolutional layer before GAP had a higher spatial resolution [24]. Specifically, we made the following modifications: For Net-5 on the 128 pixel images, we removed the layers after Conv-11 and all the strides excepts the Maxpool-8, which resulting in a mapping resolution of 54\u00d754. For Net-5 on the 256 pixel images, we removed the layers after Conv-15 and last two max pooling layer, which resulted in a mapping resolution of 56 \u00d7 56. Each of these networks were then fine-tuned 5 on the training data."}, {"heading": "4.5.2 Fusion of multiple RAMs", "text": "Figure 4(a) and (b) show the RAMs from the input images of size 128 and 256 pixels respectively and their corresponding Kappa scores 5.06 and 4.63. We noticed that these RAMs reflect different ROIs that may both have contributions to the final Kappa score prediction. Thus, we consider\n5Train from the scratch has the similar performance but a little bit slower.\nthe fusion of multiple RAMs generated from the different scales of input images. The fusion of the RAMs (a) and (b), i.e., the average of the values in RAM matrices, is plotted in Figure 4(c). By referring to the original image shown in Figure 4(d), we argue that Figure 4(c) can better capture the ROI of original image. We also found the similar phonomania from other examples. So we conclude that the fusion of different RAMs from various resolutions is simple and effective to depict the comprehensive ROI, so the fused RAM is only reported in the following analysis."}, {"heading": "4.5.3 Analysis on RAM", "text": "For the mild-conditioned patients, RAM learned to discover the narrowing of the retinal arteries associated with reduced retinal blood flow (Figure 5(d)), where the vessel shows dark red. The dysfunction of the neurons of the inner retina, followed in later stages (moderate) by changes in the function of the outer retina are captured in Figure 5(c), as such dysfunction protects the retina from many substances in the blood (including toxins and immune cells), leading to the leaking of blood constituents into the retinal neuropile. When the patients belong to the next stage (severe), as the basement membrane of the retinal blood vessels thickens, capillaries degenerate and lose cells leading to loss of blood flow and progressive ischemia and microscopic aneurysms which appear as balloon-like structures jutting out from the capillary walls. RAM, as shown in Figure 5(b), learned to converge its focus on the border where the balloon-like structures occurs. As the disease progresses to the proliferative stage, the lack of oxygen in the retina causes fragile, new, blood vessels to grow along the retina and in the clear, gel-like vitreous humour that fills the inside of the eye. In Figure 5(a), RAM shows the model put its attention on the grey dots scattering around, which undoubtedly\ndemonstrate the proliferative stage. We also note that if the patient has no DR and the score predicted by the model is smaller than 0.5, then the RAM uniformly shows the dotlike focus near the pupil (5(e)).\nWe also observed that the proposed model may have the conservative diagnostics behavior, which means that the predicted regression value is often smaller than the ground truth. The examples of this case are plotted in Figure 6. One plausible reason is that the training dataset has considerably large number of normal images as shown in Figure 3 so the learned model might be biased to the \u201c0\u201d class. Other reasons could be due to image quality and the sensitivity on the ambiguous features between different levels. Specifically, in Figure 6(a), though our model ignores the proliferate on the top left corner, it can capture the leaking of blood in the region of retinal neuropile. To identify leaking of blood in retinal neuropile may even be challenging for the clinicians. In (b) and (c), the original images contain very limited information about the balloon-like jitter and the blood leaking due to partial exposedness, so the RAM are scattered on the dard red vessels which are caused by the reduced retinal blood flow.\nBased on the above analysis, RAM provides the reasonable transparency on our deep learning model to see why and how it makes the decision. The visual explanation of RAM may assist the clinicians to quickly identify the pathogenesis of disease."}, {"heading": "5. Conclusions", "text": "Practically, clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. In this work, we provided a deep learning model that includes regression activation maps layer (RAM). The RAM layer can provide the robust interpretability of the proposed detection model by monitoring the pathogenesis so that the proposed model can be taken as an assistant for clinicians. With this feature, the proposed model can still yield the competitive performance of DR detection, compared with the state-of-the-art methods. In future, we would consider to extend the proposed method to other medical application problems."}], "references": [{"title": "Kaggle Diabetic Retinopathy Detection", "author": ["M. Antony", "S. Brggemann"], "venue": "Team o O solution,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Self-taught object alization with deep networks", "author": ["L. Bazzani", "A. Bergamo", "D. Anguelov", "L. Torresani"], "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A tutorial survey of architectures, algorithms, and applications for deep learning", "author": ["L. Deng"], "venue": "APSIPA Transactions on Signal and Information Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Inverting visual representations with convolutional networks", "author": ["A. Dosovitskiy", "T.Brox"], "venue": "In CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Efficient backprop. Neural Networks: Tricks of the trade, pages", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Transformed representations for convolutional neural networks in diabetic retinopathy screening", "author": ["G. Lim", "M.L. Lee", "W. Hsu", "T.Y. Wong"], "venue": "In AAAI Workshop on Modern Artificial Intelligence for Health Analytics (MAIHA),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Understanding deep image representations by inverting them", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "In CVPR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Mapping the human retina", "author": ["A. Pinz", "S. Bernogger", "P. Datlinger", "A. Kruger"], "venue": "IEEE Transactions on Medical Imaging,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Convolutional neural networks for diabetic retinopathy", "author": ["H. Pratta", "F. Coenenb", "D.M. Broadbentc", "S.P. Hardinga", "Y. Zheng"], "venue": "In International Conference On Medical Imaging Understanding and Analysis (MIUA),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Case for automated detection of diabetic retinopathy", "author": ["N. Silberman", "K. Ahrlich", "R. Fergus", "L. Subramanian"], "venue": "In AAAI Spring Symposium: Artificial Intelligence for Development. AAAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Automatic exudate detection from non-dilated diabetic retinopathy retinal images using fuzzy c-means clustering", "author": ["A. Sopharak", "B. Uyyanonvara", "S. Barman"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Hierarchical retinal blood vessel segmentation based on feature and ensemble learning", "author": ["S. Wang", "Y. Yin", "G. Cao", "B. Wei", "Y. Zheng", "G. Yang"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Time series classification from scratch with deep neural networks: A strong baseline", "author": ["Z. Wang", "W. Yan", "T. Oates"], "venue": "arXiv preprint arXiv:1611.06455,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "On the adaptive detection of blood vessels in retinal images", "author": ["D. Wu", "M. Zhang", "J.-C. Liu", "W. Bauman"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Deep convolutional neural networks on multichannel time series for human activity recognition", "author": ["J.B. Yang", "M.N. Nguyen", "P.P. San", "X.L. Li", "S. Krishnaswamy"], "venue": "In IJCAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "\u00c0. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "In ICLR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Learning deep features for discriminative localization", "author": ["B. Zhou", "A. Khosla", "\u00c0. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "It becomes the leading cause of vision impairment and blindness for working-age adults in the world today[15], and around half of Americans with diabetes have this disease to some extent.", "startOffset": 105, "endOffset": 109}, {"referenceID": 11, "context": "Most of previous automated solutions consists of two parts: feature extraction and detection/prediction algorithm [13, 15, 16, 20].", "startOffset": 114, "endOffset": 130}, {"referenceID": 13, "context": "Most of previous automated solutions consists of two parts: feature extraction and detection/prediction algorithm [13, 15, 16, 20].", "startOffset": 114, "endOffset": 130}, {"referenceID": 14, "context": "Most of previous automated solutions consists of two parts: feature extraction and detection/prediction algorithm [13, 15, 16, 20].", "startOffset": 114, "endOffset": 130}, {"referenceID": 18, "context": "Most of previous automated solutions consists of two parts: feature extraction and detection/prediction algorithm [13, 15, 16, 20].", "startOffset": 114, "endOffset": 130}, {"referenceID": 2, "context": "It can model high-level abstractions in data relative to specific prediction task [4, 5, 8, 9, 21].", "startOffset": 82, "endOffset": 98}, {"referenceID": 3, "context": "It can model high-level abstractions in data relative to specific prediction task [4, 5, 8, 9, 21].", "startOffset": 82, "endOffset": 98}, {"referenceID": 6, "context": "It can model high-level abstractions in data relative to specific prediction task [4, 5, 8, 9, 21].", "startOffset": 82, "endOffset": 98}, {"referenceID": 7, "context": "It can model high-level abstractions in data relative to specific prediction task [4, 5, 8, 9, 21].", "startOffset": 82, "endOffset": 98}, {"referenceID": 19, "context": "It can model high-level abstractions in data relative to specific prediction task [4, 5, 8, 9, 21].", "startOffset": 82, "endOffset": 98}, {"referenceID": 11, "context": "Given color fundus photography, this type of approaches often extracted visual features from the images on the parts of blood vessels, fovea and optic disc [13, 20].", "startOffset": 156, "endOffset": 164}, {"referenceID": 18, "context": "Given color fundus photography, this type of approaches often extracted visual features from the images on the parts of blood vessels, fovea and optic disc [13, 20].", "startOffset": 156, "endOffset": 164}, {"referenceID": 13, "context": "With the extracted features, an object detection or object registration algorithm like support vector machines and k-NN were used to identify and localize exudates and hemorrhages [15, 16].", "startOffset": 180, "endOffset": 188}, {"referenceID": 14, "context": "With the extracted features, an object detection or object registration algorithm like support vector machines and k-NN were used to identify and localize exudates and hemorrhages [15, 16].", "startOffset": 180, "endOffset": 188}, {"referenceID": 0, "context": "As mentioned before, this type of approaches are not as effective as the recent deep learning approaches, such as [2, 10, 14, 18].", "startOffset": 114, "endOffset": 129}, {"referenceID": 8, "context": "As mentioned before, this type of approaches are not as effective as the recent deep learning approaches, such as [2, 10, 14, 18].", "startOffset": 114, "endOffset": 129}, {"referenceID": 12, "context": "As mentioned before, this type of approaches are not as effective as the recent deep learning approaches, such as [2, 10, 14, 18].", "startOffset": 114, "endOffset": 129}, {"referenceID": 16, "context": "As mentioned before, this type of approaches are not as effective as the recent deep learning approaches, such as [2, 10, 14, 18].", "startOffset": 114, "endOffset": 129}, {"referenceID": 20, "context": "A deconvolutional networks approach was proposed to visualize activated pattern in each hidden unit [22].", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "The work [3, 12, 23] and the reference therein include the objection location task besides the conventional object classification problem, so their CNN can predict the label of an image and also identify the region of the object related to the class label.", "startOffset": 9, "endOffset": 20}, {"referenceID": 10, "context": "The work [3, 12, 23] and the reference therein include the objection location task besides the conventional object classification problem, so their CNN can predict the label of an image and also identify the region of the object related to the class label.", "startOffset": 9, "endOffset": 20}, {"referenceID": 21, "context": "The work [3, 12, 23] and the reference therein include the objection location task besides the conventional object classification problem, so their CNN can predict the label of an image and also identify the region of the object related to the class label.", "startOffset": 9, "endOffset": 20}, {"referenceID": 4, "context": "Recently, [6, 11] have presented the methods to invert the representation of images in each layer of the CNN.", "startOffset": 10, "endOffset": 17}, {"referenceID": 9, "context": "Recently, [6, 11] have presented the methods to invert the representation of images in each layer of the CNN.", "startOffset": 10, "endOffset": 17}, {"referenceID": 22, "context": "The most work most related to our method is [24] in which class activation map is proposed to characterize the weighted activation maps after global average pooling or global maximum pooling layer.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "This idea has recently been generalized to time series analysis to localize the significant regions in the raw data [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 22, "context": "In this paper, we extend the method [24] from a classification to a regression setting and shed light on DR detection problem.", "startOffset": 36, "endOffset": 40}, {"referenceID": 22, "context": "Inspired by [24], we present in this section the idea of generating the RAM of an input image to localize the discriminative region towards the regression outcomes.", "startOffset": 12, "endOffset": 16}, {"referenceID": 5, "context": "The key difference between our neural network and conventional neural networks like AlexNet [7] and GoogLeNet [17] lie in that our network uses global averaging pooling (GAP) layer to connect the last convolutional layer and the output layer, instead of using fully-connected layers.", "startOffset": 92, "endOffset": 95}, {"referenceID": 15, "context": "The key difference between our neural network and conventional neural networks like AlexNet [7] and GoogLeNet [17] lie in that our network uses global averaging pooling (GAP) layer to connect the last convolutional layer and the output layer, instead of using fully-connected layers.", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "We consider the publicly-available method [2] as the benchmark method, which was ranked as the second place in the Kaggle competition.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "The interested readers may refer to [2] for more detailed settings of the baseline method.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "Following [2], we split 35126 images into training and validation datasets in a ratio of 9 to 1 for local evaluation purpose, and we also submit our prediction results on the test dataset to Kaggle to obtain the Kappa score.", "startOffset": 10, "endOffset": 13}, {"referenceID": 22, "context": "We also removed several convolutional layers of Net-5 for each input size to increase the resolution of RAM, since the localization ability of RAM can be significantly improved when the last convolutional layer before GAP had a higher spatial resolution [24].", "startOffset": 254, "endOffset": 258}], "year": 2017, "abstractText": "We proposed a deep learning method for interpretable diabetic retinopathy (DR) detection. The visualinterpretable feature of the proposed method is achieved by adding the regression activation map (RAM) after the global averaging pooling layer of the convolutional networks (CNN). With RAM, the proposed model can localize the discriminative regions of an retina image to show the specific region of interest in terms of its severity level. We believe this advantage of the proposed deep learning model is highly desired for DR detection because in practice, users are not only interested with high prediction performance, but also keen to understand the insights of DR detection and why the adopted learning model works. In the experiments conducted on a large scale of retina image dataset, we show that the proposed CNN model can achieve high performance on DR detection compared with the state-ofthe-art while achieving the merits of providing the RAM to highlight the salient regions of the input image.", "creator": "LaTeX with hyperref package"}}}