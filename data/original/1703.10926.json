{"id": "1703.10926", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "EMULATOR vs REAL PHONE: Android Malware Detection Using Machine Learning", "abstract": "The Android operating system has become the most popular operating system for smartphones and tablets leading to a rapid rise in malware. Sophisticated Android malware employ detection avoidance techniques in order to hide their malicious activities from analysis tools. These include a wide range of anti-emulator techniques, where the malware programs attempt to hide their malicious activities by detecting the emulator. For this reason, countermeasures against antiemulation are becoming increasingly important in Android malware detection. Analysis and detection based on real devices can alleviate the problems of anti-emulation as well as improve the effectiveness of dynamic analysis. Hence, in this paper we present an investigation of machine learning based malware detection using dynamic analysis on real devices. A tool is implemented to automatically extract dynamic features from Android phones and through several experiments, a comparative analysis of emulator based vs. device based detection by means of several machine learning algorithms is undertaken. Our study shows that several features could be extracted more effectively from the on-device dynamic analysis compared to emulators. It was also found that approximately 24% more apps were successfully analysed on the phone. Furthermore, all of the studied machine learning based detection performed better when applied to features extracted from the on-device dynamic analysis.", "histories": [["v1", "Fri, 31 Mar 2017 14:59:15 GMT  (3787kb,D)", "http://arxiv.org/abs/1703.10926v1", "IWSPA 2017 Proceedings of the 3rd ACM International Workshop on Security and Privacy Analytics, co-located with CODASPY'17, Scottsdale, Arizona, USA - March 24 - 24, 2017, pages 65-72"]], "COMMENTS": "IWSPA 2017 Proceedings of the 3rd ACM International Workshop on Security and Privacy Analytics, co-located with CODASPY'17, Scottsdale, Arizona, USA - March 24 - 24, 2017, pages 65-72", "reviews": [], "SUBJECTS": "cs.CR cs.AI", "authors": ["mohammed k alzaylaee", "suleiman y yerima", "sakir sezer"], "accepted": false, "id": "1703.10926"}, "pdf": {"name": "1703.10926.pdf", "metadata": {"source": "CRF", "title": "EMULATOR vs REAL PHONE: Android Malware Detection Using Machine Learning", "authors": ["Mohammed K. Alzaylaee", "Suleiman Y. Yerima", "Sakir Sezer"], "emails": ["malzaylaee01@qub.ac.uk", "s.yerima@qub.ac.uk", "s.sezer@qub.ac.uk"], "sections": [{"heading": null, "text": "Keywords Android; Malware; Malware detection; Anti-analysis; Antiemulation; Machine Learning; Device-based detection"}, {"heading": "1. INTRODUCTION", "text": "The Google Android operating system (OS) is the leading OS in the market with nearly 80% market share compared to iOS, Blackberry, Windows, and Symbian mobile. Over 1 billion Android devices have been sold with an estimated 65 billion app downloads from Google Play [1]. Moreover, it is reported that more than 1.5 billion Android devices will be shipped worldwide by 2020 [2]. This has led to malware developers increasingly targeting Android devices. According to a report from McAfee, there are more than 12 million Android malware samples with nearly 2.5 million new Android malware samples discovered every year [24].\nThe rapid increase in malware numbers targeting Android devices has highlighted the need for efficient detection mechanisms to detect zero-day malware. In contrast with other\nmobile operating systems, Android allows users to download applications from third party stores, many of which do not have any mechanisms or tools to check the submitted apps for malware. The Google play store uses a tool called Bouncer to screen submitted applications. However it has been previously demonstrated that the Bouncer dynamic analysis process can be bypassed by means of some simple anti-emulator techniques [26].\nAndroid malware can be found in a variety of applications such as banking apps, gaming apps, media player apps etc. These malware-infected apps may access phone data to collect privacy sensitive information, root the phone, dial premium rate numbers or send text messages to premium rate numbers without the user approval etc. Many Android malware families employ detection avoidance techniques in order to hide their malicious activities and evade anti-virus software. Commonly used detection avoidance methods by Android malware include a wide range of anti-emulator techniques, where the malware programs attempt to hide their malicious activities when being analysed in an emulator. For this reason, countermeasures against anti-emulation are becoming increasingly important in Android malware detection.\nSeveral approaches for anti-emulation (or anti-virtualization) have been discussed in previous work. The paper [30] discusses some methods that could be employed in order to detect the run-time environment thereby hindering dynamic analysis. Some malware applications, for example, will detect the emulator through the use of Android APIs. For instance, if the Telephony Manager API method TelephonyManager.getDeviceId() returns 000000000000000, it means the run-time environment is an emulator rather than a real device, because no real phone will return 0s as the device identifier. This is one of the emulator detection methods used by the Pincer family of Android malware [3].\nThe emulator can also be detected through the networking environment which is different from that of a real phone, or the underlying QEMU can be detected. Morpheus [22] has also exposed more than 10,000 detection heuristics based on some artifacts that can be used to detect the run-time analysis environments. These artifacts can be used in malware samples to hide the malicious activities accordingly. Dynamic analysis tools that rely on emulators (or virtual devices) such as Dynalog [14] attempt to address the problem by changing properties of the environment to emulate a real phone as much as possible and to incorporate several behaviours to mimic a real phone. However, these methods\nIWSPA 2017 Proceedings of the 3rd ACM International Workshop on Security and Privacy Analytics, co-located with CODASPY\u201917,\npages 65-72, Scottsdale, Arizona, USA - March 24 - 24, 2017\nar X\niv :1\n70 3.\n10 92\n6v 1\n[ cs\n.C R\n] 3\n1 M\nar 2\n01 7\nwhilst useful, have been shown to be insufficient to completely tackle anti-emulation [21], [22], [31] .\nSeveral dynamic analysis tools such as TaintDroid [20], DroidBox [4], CopperDroid [29], Andrubis [17], AppsPlayground [27] have been proposed. Similarly, some online based tools are available for Android malware analysis such as SandDroid [5], CopperDroid [6], TraceDroid [7], and NVISO ApkScan [8]. All of these dynamic approaches can be evaded by malware that use anti-emulation. Since the analysis is done in a virtualized environment.\nMutti et al. [25], have demonstrated the feasibility of device-based dynamic analysis to alleviate the problems of anti-emulation. We have also found that phones generally provide more effective analysis environment due to incomplete nature of emulators. Many apps nowadays have functionality that utilize device hardware features such as sensors, WiFi, GPS, etc. and these require emulation in the sandbox. Therefore, devices should provide more effective analysis environment. Hence, we have designed and implemented a python-based tool to enable dynamic analysis using real phones to automatically extract dynamic features and potentially mitigate anti-emulation detection. Furthermore, in order to validate this approach, we undertake a comparative analysis of emulator vs device based detection by means of several machine learning algorithms. We examine the performance of these algorithms in both environments after investigating the effectiveness of obtaining the run-time features within both environments. The experiments were performed using 1222 malware samples from the Android malware genome project [9] and 1222 benign samples from Intel Security (McAfee Labs).\nThe rest of the paper is structured as follows. Section II describes the runtime analysis process for feature extraction from the phone and emulator, Section III details the methodology and experiments undertaken for the evaluation. The results and the discussions of results will be given in Section IV, followed by related work in Section V. Section VI will present conclusions and future work."}, {"heading": "2. PHONE BASED DYNAMIC ANALYSIS AND FEATURE EXTRACTION", "text": "In order to apply machine learning to the classification and detection of malicious applications, a platform is needed to extract features from the applications. These features will be used by the machine learning algorithm to classify the application. Prior to that, the algorithm must be trained with several instances of clean and malicious applications. This process is known as supervised learning. Since our aim is to perform experiments to compare emulator based detection with device based detection we need to extract features for the supervised learning from both environments. For the emulator based learning, we utilized the DynaLog dynamic analysis framework described in [14].\nThe framework is designed to automatically accept a large number of applications, launch them serially in an emulator, log several dynamic behaviours (features) and extract them for further processing. DynaLog components include an emulator-based analysis sandbox, an APK instrumentation module, Behaviour/features logging and extraction, App trigger/exerciser and log parsing and processing scripts [14]. DynaLog provides the ability to instrument each application with the necessary API calls to be monitored, logged\nand extracted from the emulator during the run-time analysis. The instrumentation module was built using APIMonitor [10].\nDynaLog currently relies on Monkey, an application exerciser tool, which allows the application to be triggered with thousands of random events for this experiment. These random events include \u201dswipes\u201d, \u201dpresses\u201d, \u201dtouch screens\u201d etc, to ensure that most of the activities has been traversed to cover as much code as possible. In order to enable dynamic analysis and feature extraction from a real phone, the DynaLog framework was extended with a python-based tool to enable the following:\n\u2022 At the start, push a list of contacts to the device SD card and then import them (using adb shell command) to populate the phone\u2019s contact list.\n\u2022 Discover and uninstall all third-party applications prior to installing the app under analysis (or from which features are to be extracted). This capability was implemented using package manager within an adb shell as illustrated in Fig. 1.\n\u2022 Check whether the phone is in airplane mode or not. If it is in airplane mode, turn it (airplane mode) off. This is because with airplane mode switched on, many phone features such as WiFi, 3G/4G connectivity will be unavailable, which could affect the dynamic analysis. Also, Monkey (the app exerciser tool) was found to sometimes temper with the analysis by turning on the airplane mode due to its randomly sending events such as \u201dtouch screens\u201d, \u201dpresses\u201d and \u201dswipes\u201d.\n\u2022 Check the battery level of the phone. If the level is very low i.e. battery has discharged to a low level, put the analysis on hold (sleep) until the phone has re-charged to an appropriate level.\n\u2022 Outgoing call dialling using adb shell.\n\u2022 Outgoing sms messages using adb shell.\n\u2022 Populate the phone SD card with other assets such as folders containing dummy files: image files, pdf, text files etc.\nEach of the apps is installed and run for 300 seconds on the emulator (and then on the phone for the second experiment) and the behaviours are logged and parsed through scripts that extract the desired features. The features used in the experiments include API calls and Intents (signalling critical events). The API calls signatures provided during the instrumentation phase allows the APIs to be logged and extracted from the phone (or the emulator) via adb logcat as the app is run. For malicious apps that incorporate anti-emulator techniques it is expected that the API call that could lead to the exposure of their malicious behaviour would not be logged as the malware will attempt to hide this behaviour. The analysis process is shown in Fig. 2."}, {"heading": "3. METHODOLOGY AND EXPERIMENTS", "text": "This section describes the methodology of the experiments which were performed to evaluate the environmental effectiveness of detecting and extracting the features as well as analysing the performance of the machine learning algorithms on both emulator and real phone."}, {"heading": "3.1 Dataset", "text": "The dataset used for the experiments consists of a total of 2444 Android applications. Of these, 1222 were malware samples obtained from 49 families of the Android malware genome project [9]. The rest were 1222 benign samples obtained from Intel Security (McAfee Labs)."}, {"heading": "3.2 Environmental configurations", "text": "The two environments under which the experiments were performed had the following configurations. The first was the phone which was an Elephone P6000 brand smartphone equipped with Android 5.0 Lollipop, 1.3 GHz CPU, 16 GB internal memory, and 32 GB of external SD card storage. In addition, a sim card with call credit was installed in the phone to enable outgoing calls and 3G data usage.\nThe emulator environment (an Android virtual device) was created in a Santoku Linux VirtualBox based machine. The emulator was configured with 2 MB of memory, and an 8 GB of external SD card memory. The Android version in the emulator was 4.1.2 Jelly Bean (API level 16). Note that, the features are app dependent so the difference in Android versions will have no impact. The emulator was enhanced as described in [14] by adding contact numbers, images, .pdf files, and .txt files as well as changing the default IMEI, IMSI, sim serial number and phone numbers to mimic a real phone as much as possible. The emulator is also restored after each application run to ensure that all third party apps are removed."}, {"heading": "3.3 Features extraction", "text": "After all the apps are run and processed in both analyses environments, the outputs are pre-processed into a file of feature vectors representing the features extracted from each application. Within the vector, each feature is represented by 0 or 1 denoting presence or absence of the feature. The files were converted to ARFF format and fed into WEKA machine learning tool for further processing. Initially, 178 features were extracted for both the phone and emulator environments. These were then ranked using the InfoGain (information gain) feature ranking algorithm in WEKA. The top 100 ranked features were then used for the experiments to compare the detection performance between the two environments using several machine learning algorithms. The\ntop 10 ranked features (based on InfoGain) in both sets (phone and emulator) are shown in Table 1."}, {"heading": "3.4 Machine learning classifiers", "text": "In the experiments, the features were divided into five different sets (20, 40, 60, 80 and 100 top ranked using InfoGain) in order to compare the performance of emulator and phone based detection using machine learning algorithms. The following algorithms were used in the experiments: Support Vector Machine (SVM-linear), Naive Bayes (NB), Simple Logistic (SL), Multilayer Perceptron (MLP), Partial Decision Trees (PART), Random Forest (RF), and J48 Decision Tree."}, {"heading": "3.5 Metrics", "text": "Five metrics were used for the performance emulation of the detection approaches. These include: true positive rate (TPR), true negative ratio (TNR), false positive ratio (FPR), false negative ratio (FNR), and weighted average F-measure. The definition of these metrics are as follows:\nTPR = TP\nTP + FN (1)\nTNR = TN\nTP + FN (2)\nFPR = FP\nTP + FN (3)\nFNR = FN\nTP + FN (4)\nF \u2212measure = 2 \u2217 recall \u2217 precision recall + precision\n(5)\nTrue positives (TP) is defined as the number of malware samples that are correctly classified, whereas the false negatives is defined as the number of malware samples that are incorrectly classified. True negatives (TN) is defined as the number of benign samples that are correctly classified, while false positives (FP) is defined as the number of benign samples that are incorrectly classified. The F-measures is the accuracy metric which incorporates both the recall and the precision."}, {"heading": "4. RESULTS AND DISCUSSIONS", "text": ""}, {"heading": "4.1 Experiment 1: Emulator vs Device analysis and feature extraction", "text": "In order to validate our phone-based machine learning detection, we analysed the effectiveness of running the samples and extracting features from both phone and emulator environments. Out of the 1222 malware samples used, 1205 were successfully run on a real phone compared to only 939 successfully run on an emulator. From the benign samples, 1097 out of 1222 were successfully examined using a real phone versus 786 from the emulator for the same dataset. Therefore, 98.6% malware sample were successfully analysed using the phone, while only 76.84% were successfully analysed when using the emulator. Also, 90% of the benign samples were successfully analysed using the phone versus 64.27% using the emulator. That gives us a total of 94.3% successfully analysed using the phone compared to only 70.5% using the emulator as illustrated in Table 2 .\nThe phone-based experiments were performed using a Santoku Linux-based VM. During the study we discovered that the use of USB 2.0 or 3.0 was essential. In our initial experiments where the default USB 1.0 was utilized to attach the phone, only 480 out of the 1222 benign samples were able to run. This was due to the fact that samples > 1 MB in size took longer to analyse and hence experienced a \u2019time-out\u2019 error with the use of the slower USB 1.0 standard. We also noticed that apps that did not have any Activities crashed and could not be analysed on the phone or emulator. This accounted for a small percentage of the apps that failed to run successfully.\nFig. 3 and Fig. 4 show the top-10 extracted features from malware and benign dataset respectively. Fig. 3 shows that more malware features are able to be extracted from the phone based analysis vs. the emulator based analysis from the same sample set. The feature \u201dTimerTask;-><init>\u201d, for example, was logged from 813 malware applications using the phone, while it was only logged from 633 malware applications using the emulator. Similarly, the feature \u201dintent.BOOT COMPLETED\u201d in Fig. 3, has been extracted from 662 malware applications using the phone whereas only 501 were extracted from the same sample set using the emulator.\nSimilar findings appear with the benign samples as shown in Fig. 4. More samples were able to be analysed using the phone. With some features, the difference between phone and emulator extraction were > 200. The more features we can extract during the dynamic analysis the better the result of the detection mechanism is likely to be. There were some features that were extracted exclusively from the phone but not with the emulator. These are shown in Table 3. The System.loadLibrary feature (found in 209 apps) is the API call associated with native code. The reason it is not logged when the emulator is used could be due to the lack of ability to run native code on the emulator. Overall,\nthe phone based analysis shows a much higher efficiency of detecting and extracting features for analysis of the apps or training machine learning classifiers."}, {"heading": "4.2 Experiment 2: Emulator vs Device Machine learning detection comparison", "text": "Table 4 and Table 5 present the performance evaluation results of the different machine learning algorithms (for the top 100 features). The results demonstrates that using the phone-based features for the dynamic analysis approach is more effective for detecting and classifying the applications compared to the emulator. The results shown were obtained from testing on 33% of the samples while 66% were used for training the model. Table 5 shows that higher detection rates were obtained from the phone-based features for all the algorithms (top 100 training features). TPR of > 90% were obtained with all but the N.B classifier with the phonebased features. Whereas, NB, SL, PART, and J48 all had < 90% TPR with the emulator-based features.\nAs mentioned earlier, 939/1222 malware and 786/1222 benign samples executed successfully on the emulator (i.e. total of 1725 out of 2444 samples). Out of these 1725 samples, 12 did not execute successfully on the phone. Thus there were 1713 (939 malware and 774 benign) overlapping apps that executed successfully on both the emulator and the phone. Another set of experiments were performed with only the apps that executed in BOTH the emulator and phone successfully. The results (for top 100 features) using 66% training and 33% testing split are shown in Tables 6 and 7 respectively.\nThe results show with the exception of RF, all of the tested algorithms in the data collected from emulator recorded an F-measure of < 0.9, whereas with the phone, only NB, PART and J48 recorded < 0.9 F-measure. Again, the results indicate better performance with the phone-based features.\nFig. 5 shows the results from top 20, 40, 60, 80 and 100 information-gain ranked features for the first experiment. It depicts the F-measures obtained for the various machine learning algorithms trained on both phone and emulator data. From the figure it is clear that the overall detection performance is better with the phone than the emulator. It is only in the case of MLP-80 features that the emulator records a better f-measure performance.\nFig. 6 shows the results from top 20, 40, 60, 80 and 100 information-gain ranked features for the second experiment, where only apps that executed successfully in BOTH environments were used. In all the cases, the phone-based experiments showed better performance except in the following: J48-60 features, SVM-20 features, RF-100 features and SL-20 features. Thus, we can conclude that the phonebased method still surpassed the emulator-based method in overall performance prediction."}, {"heading": "4.3 Comparison with other works", "text": "In this subsection, we compare our results with those obtained from the DroidDolphin [31] and STREAM [15] dynamic analysis frameworks. DroidDolphin is a dynamic analysis framework for detecting Android malware applications which leverages the technologies of GUI-based testing, big data analysis, and machine learning. STREAM is also\na dynamic analysis framework based on Andromaly which enables rapid large-scale validation of mobile malware machine learning classifiers. DroidDolphin used 1000 to 64000 balanced malware and benign Android applications.\nIn the STREAM approach, the testing set used 24 benign and 23 malware applications while the training set consisted of 408 benign and 1330 malware applications. Both used split training/testing set (T.S.) and 10 fold cross-validation (C.V.) methods. Table 8 shows the comparison between our phone results (100 features) and the STREAM results, while Table 9 shows the results obtained from DroidDolphin.\nFrom the DroidDolphin results, it is obvious that the detection accuracy is increasing as the number of the training samples are increased. The accuracy rate starts from 83% for the training set with 1000 applications and increased gradually to 92.50% with 64k applications. Table 8 shows that despite the difference in the testing set numbers between our work and STREAM, our phone based RF, SL, J48 and MLP perform significantly better for T.S. accuracy. In the case of C.V. accuracy, S.L performs better with our phone results, while the RF, J48, and MLP results were close to those of STREAM. The C.V. accuracy of RF, SL, J48 and MLP from our phone results showed better performance than all the DroidDolphin C.V. results.\nFor the T.S. results, our phone SL, J48 and MLP were better than DroidDolphin T.S. results except for the 32k/32k training/testing split T.S. results. The T.S. results from our phone based RF experiments showed better accuracy than all of the DroidDolphin T.S. results. Therefore, based on the encouraging results, we would continue our analysis using the real phones with larger numbers of training samples in future work."}, {"heading": "5. RELATED WORK", "text": "Once a new malware application is discovered in the wild, it should be run in a closed environment in order to understand its behaviour. Researchers and malware analysts rely heavily on emulators or virtual devices due to the fact that it is a comparatively low cost analysis environment. Emulators are also more attractive for automated mass analysis commonly employed with machine learning. Hence, most previous machine learning based detection with dynamic analysis rely on feature extraction using tools running on emulator environments. Contrary to previous machine learning based dynamic detection work, we attempt to utilize real phones (devices) for automated feature extraction in order to avoid the problem of anti-emulator techniques being employed by Android malware to evade detection.\nSome previous machine learning based Android malware detection works such as [16], , [33], [13], [32], have considered API calls and Intents in their studies. However, unlike our work, these are based on static feature extraction and thus could be affected by obfuscation. Marvin [23] applies a machine learning approach to the extracted features from a combination of static and dynamic analysis techniques in order to improve the detection performance. Shabtai et al [28] presented a dynamic framework called Andromaly which applies several different machine learning algorithms, including random forest, naive Bayes, multilayer perceptron, Bayes net, logistic, and J48 to classify the Android applications. However, they assessed their performances on four self-written malware applications. MADAM [19] is also a dynamic analysis framework that uses machine learning to classify Android apps. MADAM extracted 13 features at\nthe user and kernel level. However, their experiments were only performed on an emulator with a small dataset. Crowdroid [18] is a cloud-based machine learning framework for Android malware detection. Crowdroid features were collected based on Strace from only two self-written malware samples. Most of these previous works utilize dynamic features extracted from emulator-based analysis. By contrast, in this paper our work is based on dynamically extracted features from real device and we perform a comparative analysis between emulator and phone based machine learning approaches.\nBareDroid [25] proposed a system designed to make baremetal analysis of Android applications feasible. It presented analysis with malware samples from Android.HeHe [11], OBAD [12], and Android Pincer.A [3] families. Their work highlighted the anti-emulator capabilities of malware which can be solved by using real devices. Glassbox [21] also presented a dynamic analysis platform for analysing Android malware on real devices. However, unlike the work presented in this paper, these studies have not addressed machine learning based detection on real devices. Different from the previous studies, this paper presents a comparative analysis of machine learning based detection between real devices and emulators and investigates the effectiveness of run-time feature extraction in both environments."}, {"heading": "6. CONCLUSIONS", "text": "In this paper we presented an investigation of machine learning based malware detection using dynamic analysis on real Android devices. We implemented a tool to automatically extract dynamic features from Android phones and through several experiments we performed a comparative analysis of emulator based vs. device based detection by means of Random Forest, Naive Bayes, Multilayer Perceptron, Simple Logistics, J48 decision tree, PART, and SVM (linear) algorithms. Our experiments showed that several features were extracted more effectively from the phone than the emulator using the same dataset. Furthermore, 23.8% more apps were fully analyzed on the phone compared to emulator. This shows that for more efficient analysis the phone is definitely a better environment as far more apps crash when being analysed on the emulator. The results of our phone-based analysis obtained up to 0.926 F-measure and 93.1% TPR and 92% FPR with the Random Forest classifier and in general, phone-based results were better than emulator based results. Thus we conclude that as an incentive to reduce the impact of malware anti-emulation and environmental shortcomings of emulators which affect analysis efficiency, it is important to develop more effective machine learning device based detection solutions. Hence future work will aim to investigate more effective, larger scale device based machine learning solutions using larger sample datasets. Future work could also investigate alternative set of dynamic features to those utilized in this study."}, {"heading": "7. REFERENCES", "text": "[1] Smartphone OS market share worldwide 2009-2015 |\nStatistic, Statista https://www.statista.com/statistics/263453/ global-market-share-held-by-smartphone-operating-systems.\n[2] Global smartphone shipments by OS 2016 and 2020 | Statistic, https://www.statista.com/statistics/309448/ global-smartphone-shipments-forecast-operating-system/.\n[3] F-Secure, Android Pincer A, https: //www.f-secure.com/weblog/archives/00002538.html.\n[4] DroidBox, Google Archive https://code.google.com/archive/p/droidbox/#!\n[5] SandDroid, Hu.Wenjun, http://sanddroid.xjtu.edu.cn/.\n[6] CopperDroid, http://copperdroid.isg.rhul.ac.uk/copperdroid/.\n[7] Tracedroid, http://tracedroid.few.vu.nl/.\n[8] NVISO ApkScan - Scan Android applications for malware, https://apkscan.nviso.be/.\n[9] Android Malware Genome Project, Yajin Zhou and Xuxian Jiang, http://www.malgenomeproject.org/.\n[10] APIMonitor, https: //github.com/pjlantz/droidbox/wiki/APIMonitor.\n[11] FireEye, Android.HeHe, https: //www.fireeye.com/blog/threat-research/2014/01/ android-hehe-malware-now-disconnects-phone-calls. html.\n[12] Contagio mobile mini-dump, OBAD, http://contagiominidump.blogspot.co.uk/search/ label/Backdoor.AndroidOS.Obad.a.\n[13] Y. Aafer, W. Du, and H. Yin. DroidAPIMiner: Mining API-Level Features for Robust Malware Detection in Android. Security and Privacy in Communication Networks, 127:86\u2013103, 2013.\n[14] M. K. Alzaylaee, S. Y. Yerima, and S. Sezer. DynaLog: An automated dynamic analysis framework for characterizing android applications. 2016 International Conference on Cyber Security and Protection of Digital Services, Cyber Security 2016, 2016.\n[15] B. Amos, H. Turner, and J. White. Applying machine learning classifiers to dynamic android malware detection at scale. 2013 9th International Wireless Communications and Mobile Computing Conference, IWCMC 2013, pages 1666\u20131671, 2013.\n[16] D. Arp, M. Spreitzenbarth, H. Malte, H. Gascon, and K. Rieck. Drebin: Effective and Explainable Detection of Android Malware in Your Pocket. Symposium on Network and Distributed System Security (NDSS), (February):23\u201326, 2014.\n[17] U. Bayer, I. Habibi, D. Balzarotti, E. Kirda, and C. Kruegel. A view on current malware behaviors. Proceedings of the 2nd USENIX conference on Large-scale exploits and emergent threats: botnets, spyware, worms, and more, page 8, 2009.\n[18] I. Burguera, U. Zurutuza, and S. Nadjm-Tehrani. Crowdroid: Behavior-Based Malware Detection System for Android. Proceedings of the 1st ACM workshop on Security and privacy in smartphones and mobile devices - SPSM \u201911, page 15, 2011.\n[19] G. Dini, F. Martinelli, A. Saracino, and D. Sgandurra. MADAM: A multi-level anomaly detector for android malware. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 7531 LNCS:240\u2013253, 2012.\n[20] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung, P. McDaniel, and A. N. Sheth. TaintDroid: An Information-Flow Tracking System for Realtime\nPrivacy Monitoring on Smartphones. Osdi \u201910, 49:1\u20136, 2010.\n[21] P. Irolla and E. Filiol. Glassbox: Dynamic Analysis Platform for Malware Android Applications on Real Devices. 2016.\n[22] Y. Jing, Z. Zhao, G.-J. Ahn, and H. Hu. Morpheus: Automatically Generating Heuristics to Detect Android Emulators. Proceedings of the 30th Annual Computer Security Applications Conference on - ACSAC \u201914, pages 216\u2013225, 2014.\n[23] M. Lindorfer and M. Neugschwandtner. Marvin: Efficient and comprehensive mobile app classification through static and dynamic analysis. Computer Software and Applications Conference (COMPSAC), 2015 IEEE 39th Annual, 2:422\u2013433, 2015.\n[24] McAfee Labs. McAfee Labs Threats Predictions Report. (March):34\u201335, 2016.\n[25] S. Mutti, Y. Fratantonio, A. Bianchi, L. Invernizzi, J. Corbetta, D. Kirat, C. Kruegel, and G. Vigna. BareDroid. Proceedings of the 31st Annual Computer Security Applications Conference on - ACSAC 2015, pages 71\u201380, 2015.\n[26] J. Oberheide and C. Miller. Dissecting the Android Bouncer. Summercon 2012, 2012.\n[27] V. Rastogi, Y. Chen, and W. Enck. AppsPlayground : Automatic Security Analysis of Smartphone Applications. CODASPY \u201913 (3rd ACM conference on Data and Application Security and Privac), pages 209\u2013220, 2013.\n[28] A. Shabtai, U. Kanonov, Y. Elovici, C. Glezer, and Y. Weiss. \u201dAndromaly\u201d: A behavioral malware detection framework for android devices. Journal of Intelligent Information Systems, 38(1):161\u2013190, 2012.\n[29] K. Tam, S. J. Khan, A. Fattori, and L. Cavallaro. CopperDroid: Automatic Reconstruction of Android Malware Behaviors. Ndss, (February):8\u201311, 2015.\n[30] T. Vidas and N. Christin. Evading android runtime analysis via sandbox detection. ASIA CCS \u201914 (9th ACM symposium on Information, computer and communications security), pages 447\u2013458, 2014.\n[31] W.-C. Wu and S.-H. Hung. Droiddolphin: A dynamic android malware detection framework using big data and machine learning. In Proceedings of the 2014 Conference on Research in Adaptive and Convergent Systems, RACS \u201914, pages 247\u2013252, New York, NY, USA, 2014. ACM.\n[32] S. Y. Yerima, S. Sezer, and I. Muttik. Android Malware Detection : an Eigenspace Analysis Approach. Science and Information Conference, pages 1\u20137, 2015.\n[33] S. Y. Yerima, S. Sezer, and I. Muttik. High accuracy android malware detection using ensemble learning. IET Information Security, 9(6):313\u2013320, 2015."}], "references": [{"title": "DroidAPIMiner: Mining API-Level Features for Robust Malware Detection in Android", "author": ["Y. Aafer", "W. Du", "H. Yin"], "venue": "Security and Privacy in Communication Networks, 127:86\u2013103", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "DynaLog: An automated dynamic analysis framework for characterizing android applications", "author": ["M.K. Alzaylaee", "S.Y. Yerima", "S. Sezer"], "venue": "2016 International Conference on Cyber Security and Protection of Digital Services, Cyber Security 2016", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Applying machine learning classifiers to dynamic android malware detection at scale", "author": ["B. Amos", "H. Turner", "J. White"], "venue": "2013 9th International Wireless Communications and Mobile Computing Conference, IWCMC 2013, pages 1666\u20131671", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Drebin: Effective and Explainable Detection of Android Malware in Your Pocket", "author": ["D. Arp", "M. Spreitzenbarth", "H. Malte", "H. Gascon", "K. Rieck"], "venue": "Symposium on Network and Distributed System Security (NDSS), (February):23\u201326", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A view on current malware behaviors", "author": ["U. Bayer", "I. Habibi", "D. Balzarotti", "E. Kirda", "C. Kruegel"], "venue": "Proceedings of the 2nd USENIX conference on Large-scale exploits and emergent threats: botnets, spyware, worms, and more, page 8", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Crowdroid: Behavior-Based Malware Detection System for Android", "author": ["I. Burguera", "U. Zurutuza", "S. Nadjm-Tehrani"], "venue": "Proceedings of the 1st ACM workshop on Security and privacy in smartphones and mobile devices - SPSM \u201911, page 15", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "MADAM: A multi-level anomaly detector for android malware", "author": ["G. Dini", "F. Martinelli", "A. Saracino", "D. Sgandurra"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 7531 LNCS:240\u2013253", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "TaintDroid: An Information-Flow Tracking System for Realtime  Privacy Monitoring on Smartphones", "author": ["W. Enck", "P. Gilbert", "B.-G. Chun", "L.P. Cox", "J. Jung", "P. McDaniel", "A.N. Sheth"], "venue": "Osdi \u201910, 49:1\u20136", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Glassbox: Dynamic Analysis Platform for Malware", "author": ["P. Irolla", "E. Filiol"], "venue": "Android Applications on Real Devices", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Morpheus: Automatically Generating Heuristics to Detect Android Emulators", "author": ["Y. Jing", "Z. Zhao", "G.-J. Ahn", "H. Hu"], "venue": "Proceedings of the 30th Annual Computer Security Applications Conference on - ACSAC \u201914, pages 216\u2013225", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Marvin: Efficient and comprehensive mobile app classification through static and dynamic analysis", "author": ["M. Lindorfer", "M. Neugschwandtner"], "venue": "Computer Software and Applications Conference (COMPSAC), 2015 IEEE 39th Annual, 2:422\u2013433", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "BareDroid", "author": ["S. Mutti", "Y. Fratantonio", "A. Bianchi", "L. Invernizzi", "J. Corbetta", "D. Kirat", "C. Kruegel", "G. Vigna"], "venue": "Proceedings of the 31st Annual Computer Security Applications Conference on - ACSAC 2015, pages 71\u201380", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Dissecting the Android Bouncer", "author": ["J. Oberheide", "C. Miller"], "venue": "Summercon 2012", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "AppsPlayground : Automatic Security Analysis of Smartphone Applications", "author": ["V. Rastogi", "Y. Chen", "W. Enck"], "venue": "CODASPY \u201913 (3rd ACM conference on Data and Application Security and Privac), pages 209\u2013220", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Andromaly\u201d: A behavioral malware detection framework for android devices", "author": ["A. Shabtai", "U. Kanonov", "Y. Elovici", "C. Glezer", "Y. Weiss"], "venue": "Journal of Intelligent Information Systems, 38(1):161\u2013190", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "CopperDroid: Automatic Reconstruction of Android Malware Behaviors", "author": ["K. Tam", "S.J. Khan", "A. Fattori", "L. Cavallaro"], "venue": "Ndss, (February):8\u201311", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Evading android runtime analysis via sandbox detection", "author": ["T. Vidas", "N. Christin"], "venue": "ASIA CCS \u201914 (9th ACM symposium on Information, computer and communications security), pages 447\u2013458", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Droiddolphin: A dynamic android malware detection framework using big data and machine learning", "author": ["W.-C. Wu", "S.-H. Hung"], "venue": "Proceedings of the 2014 Conference on Research in Adaptive and Convergent Systems, RACS \u201914, pages 247\u2013252, New York, NY, USA", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Android Malware Detection : an Eigenspace Analysis Approach", "author": ["S.Y. Yerima", "S. Sezer", "I. Muttik"], "venue": "Science and Information Conference, pages 1\u20137", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "High accuracy android malware detection using ensemble learning", "author": ["S.Y. Yerima", "S. Sezer", "I. Muttik"], "venue": "IET Information Security, 9(6):313\u2013320", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "However it has been previously demonstrated that the Bouncer dynamic analysis process can be bypassed by means of some simple anti-emulator techniques [26].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "The paper [30] discusses some methods that could be employed in order to detect the run-time environment thereby hindering dynamic analysis.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "Morpheus [22] has also exposed more than 10,000 detection heuristics based on some artifacts that can be used to detect the run-time analysis environments.", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "Dynamic analysis tools that rely on emulators (or virtual devices) such as Dynalog [14] attempt to address the problem by changing properties of the environment to emulate a real phone as much as possible and to incorporate several behaviours to mimic a real phone.", "startOffset": 83, "endOffset": 87}, {"referenceID": 8, "context": "whilst useful, have been shown to be insufficient to completely tackle anti-emulation [21], [22], [31] .", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "whilst useful, have been shown to be insufficient to completely tackle anti-emulation [21], [22], [31] .", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "whilst useful, have been shown to be insufficient to completely tackle anti-emulation [21], [22], [31] .", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": "Several dynamic analysis tools such as TaintDroid [20], DroidBox [4], CopperDroid [29], Andrubis [17], AppsPlayground [27] have been proposed.", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Several dynamic analysis tools such as TaintDroid [20], DroidBox [4], CopperDroid [29], Andrubis [17], AppsPlayground [27] have been proposed.", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "Several dynamic analysis tools such as TaintDroid [20], DroidBox [4], CopperDroid [29], Andrubis [17], AppsPlayground [27] have been proposed.", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "Several dynamic analysis tools such as TaintDroid [20], DroidBox [4], CopperDroid [29], Andrubis [17], AppsPlayground [27] have been proposed.", "startOffset": 118, "endOffset": 122}, {"referenceID": 11, "context": "[25], have demonstrated the feasibility of device-based dynamic analysis to alleviate the problems of anti-emulation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "For the emulator based learning, we utilized the DynaLog dynamic analysis framework described in [14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "DynaLog components include an emulator-based analysis sandbox, an APK instrumentation module, Behaviour/features logging and extraction, App trigger/exerciser and log parsing and processing scripts [14].", "startOffset": 198, "endOffset": 202}, {"referenceID": 1, "context": "The emulator was enhanced as described in [14] by adding contact numbers, images, .", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "In this subsection, we compare our results with those obtained from the DroidDolphin [31] and STREAM [15] dynamic analysis frameworks.", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "In this subsection, we compare our results with those obtained from the DroidDolphin [31] and STREAM [15] dynamic analysis frameworks.", "startOffset": 101, "endOffset": 105}, {"referenceID": 3, "context": "Some previous machine learning based Android malware detection works such as [16], , [33], [13], [32], have considered API calls and Intents in their studies.", "startOffset": 77, "endOffset": 81}, {"referenceID": 19, "context": "Some previous machine learning based Android malware detection works such as [16], , [33], [13], [32], have considered API calls and Intents in their studies.", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "Some previous machine learning based Android malware detection works such as [16], , [33], [13], [32], have considered API calls and Intents in their studies.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "Some previous machine learning based Android malware detection works such as [16], , [33], [13], [32], have considered API calls and Intents in their studies.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Marvin [23] applies a machine learning approach to the extracted features from a combination of static and dynamic analysis techniques in order to improve the detection performance.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "Shabtai et al [28] presented a dynamic framework called Andromaly which applies several different machine learning algorithms, including random forest, naive Bayes, multilayer perceptron, Bayes net, logistic, and J48 to classify the Android applications.", "startOffset": 14, "endOffset": 18}, {"referenceID": 6, "context": "MADAM [19] is also a dynamic analysis framework that uses machine learning to classify Android apps.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "Crowdroid [18] is a cloud-based machine learning framework for Android malware detection.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "BareDroid [25] proposed a system designed to make baremetal analysis of Android applications feasible.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "Glassbox [21] also presented a dynamic analysis platform for analysing Android malware on real devices.", "startOffset": 9, "endOffset": 13}], "year": 2017, "abstractText": "The Android operating system has become the most popular operating system for smartphones and tablets leading to a rapid rise in malware. Sophisticated Android malware employ detection avoidance techniques in order to hide their malicious activities from analysis tools. These include a wide range of anti-emulator techniques, where the malware programs attempt to hide their malicious activities by detecting the emulator. For this reason, countermeasures against antiemulation are becoming increasingly important in Android malware detection. Analysis and detection based on real devices can alleviate the problems of anti-emulation as well as improve the effectiveness of dynamic analysis. Hence, in this paper we present an investigation of machine learning based malware detection using dynamic analysis on real devices. A tool is implemented to automatically extract dynamic features from Android phones and through several experiments, a comparative analysis of emulator based vs. device based detection by means of several machine learning algorithms is undertaken. Our study shows that several features could be extracted more effectively from the on-device dynamic analysis compared to emulators. It was also found that approximately 24% more apps were successfully analysed on the phone. Furthermore, all of the studied machine learning based detection performed better when applied to features extracted from the on-device dynamic analysis.", "creator": "LaTeX with hyperref package"}}}