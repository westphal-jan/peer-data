{"id": "1703.10931", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Mar-2017", "title": "Sentence Simplification with Deep Reinforcement Learning", "abstract": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model brings significant improvements over the state of the art.", "histories": [["v1", "Fri, 31 Mar 2017 15:05:45 GMT  (38kb,D)", "http://arxiv.org/abs/1703.10931v1", null], ["v2", "Sun, 16 Jul 2017 02:28:14 GMT  (45kb,D)", "http://arxiv.org/abs/1703.10931v2", "to appear in EMNLP 2017"]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["xingxing zhang", "mirella lapata"], "accepted": true, "id": "1703.10931"}, "pdf": {"name": "1703.10931.pdf", "metadata": {"source": "CRF", "title": "Sentence Simplification with Deep Reinforcement Learning", "authors": ["Xingxing Zhang", "Mirella Lapata"], "emails": ["x.zhang@ed.ac.uk", "mlap@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model brings significant improvements over the state of the art.1"}, {"heading": "1 Introduction", "text": "The main goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning. The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individuals alike (Siddharthan, 2014; Shardlow, 2014). For instance, a simplification component could be used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al., 2004) and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014). Automatic simplification would also benefit people with low-literacy skills (Watanabe et al., 2009), such as children and non-native speakers as well as individuals with\n1Our code and data are publicly available at http:// anonymized.url.\nautism (Evans et al., 2014), aphasia (Carroll et al., 1999), or dyslexia (Rello et al., 2013).\nThe most prevalent rewrite operations which give rise to simplified text include substituting rare words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014). Earlier work focused on individual aspects of the simplification problem. For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).\nRecent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation. Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia. For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting). Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications. Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs. During inference, the K-best outputs of the PBMT model are reranked according to their dis-similarity to the (complex) input senar X iv :1 70 3. 10 93\n1v 1\n[ cs\n.C L\n] 3\n1 M\nar 2\n01 7\ntence. The hybrid model developed in Narayan and Gardent (2014) also operates in two phases. Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007). The resulting sentences are further simplified by a model similar to Wubben et al. (2012). Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output.\nIn this paper we propose a simplification model which draws on insights from neural machine translation, a new approach to machine translation based purely on neural networks (Bahdanau et al., 2015; Sutskever et al., 2014). Central to this approach is an encoder-decoder architecture implemented by recurrent neural networks. The encoder reads the source sequence into a list of continuousspace representations from which the decoder generates the target sequence. Our model uses the encoder-decoder architecture as its backbone coupled with a REINFORFCE-style (Williams, 1992) training algorithm. The model explores the space of possible simplifications while learning to maximize an expected reward function that encourages outputs which meet simplification-specific constraints. Reinforcement learning has been previously applied to extractive summarization (Ryang and Abekawa, 2012), information extraction (Narasimhan et al., 2016), dialogue generation (Li et al., 2016), machine translation and image caption generation (Ranzato et al., 2016). To train our network, we use a policy gradients method (Ranzato et al., 2016; Li et al., 2016) which we adapt to the simplification problem so as to optimize a reward function which jointly reflects the grammaticality and simplicity of the output, while preserving the meaning of the input.\nWe evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b). We experimentally show that the reinforcement learning framework is key to successful generation of simplified text bringing significant improvements over the state of the art across datasets and evaluation metrics."}, {"heading": "2 Attention-based Encoder-Decoder Model", "text": "Given a (complex) source sentence X = (x1,x2, . . . ,x|X |), our model learns to predict its simplified target Y = (y1,y2, . . . ,y|Y |). Inferring the target Y given the source X is a typical sequence to sequence learning problem, which can be modeled with attention-based encoder-decoder models (Bahdanau et al., 2015; Luong et al., 2015). Sentence simplification is slightly different from related sequence transduction tasks (e.g., compression) in that it can involve splitting operations. For example, a long source sentence (Because he had to work at night to support his family, Paco often fell asleep in class.) can be simplified as several shorter sentences (Paco had to make money for his family. Paco worked at night. He often went to sleep in class.). Nevertheless, we still view the target as a sequence, i.e., two or more short sequences concatenated with full stops. In the following we describe a basic encoder-decoder model for sentence simplification and then explain how to embed it in a reinforcement learning framework.\nAs the name implies, the encoder-decoder model has two parts (see left Figure 1, left side). The encoder transforms the source sentence X into a sequence of hidden states (hS1,h S 2, . . . ,h S |X |) with a Recurrent Neural Network (RNN), while the decoder uses another RNN to generate one word yt+1 at a time in the simplified target Y . In this paper we use RNNs with Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber 1997) hidden units for both the encoder and decoder. Generation is conditioned on all previously generated words y1:t and a dynamically created context vector ct , which encodes the source sentence:\nP(Y |X) = |Y |\n\u220f t=1 P(yt |y1:t\u22121,X) (1)\nP(yt+1|y1:t ,X) = softmax(g(hTt ,ct)) (2)\nwhere g(\u00b7) is a one-hidden-layer neural network with the following parametrization:\ng(hTt ,ct) = Wo tanh(Uhh T t +Whct) (3)\nwhere Wo \u2208 R|V |\u00d7d , Uh \u2208 Rd\u00d7d , and Wh \u2208 Rd\u00d7d ; |V | is the output vocabulary size and d the hidden unit size. hTt is the hidden state of the decoder\nLSTM which summarizes y1:t , i.e., what has been generated so far:\nhTt = LSTM(yt ,h T t\u22121) (4)\nThe dynamic context vector ct is essentially the weighted sum of the source sentence hidden states, with the weights being determined by an attention mechanism (\u03b1ti is the attention score):\nct = |X |\n\u2211 i=1 \u03b1tihSi (5)\n\u03b1ti = exp(hTt \u00b7hSi )\n\u2211i exp(hTt \u00b7hSi ) (6)\nwhere \u00b7 is the dot product between two vectors. We use the dot product here mainly for efficiency reasons; alternative ways to compute attention scores have been proposed in the literature and we refer the interested reader to Luong et al. (2015). The model sketched above is usually trained by minimizing the negative log-likelihood of the training source-target pairs."}, {"heading": "3 Reinforcement Learning for Sentence Simplification", "text": "Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence simplification. Although a number of rewrite operations (e.g., copying, deletion, substitution, word reordering) can be used to simplify text, copying is by far the most common. We empirically found that 73% of the target words are copied from the source in the Newsela dataset. This number further increases to 83% when considering Wikipedia-based datasets (we provide details on these datasets in Section 4.1). As a result, a generic encoder-decoder model would learn to copy all too well at the expense of other rewrite operations, often parroting back the source or only making few changes.\nIn order to encourage a wider variety of rewrite operations while preserving grammaticality and the meaning of the source, we employ a reinforcement learning framework (see Figure 1 for an overview). We view the encoder-decoder model as an agent which first reads the source sentence X ; then at each step, the agent takes an action y\u0302t \u2208 V (where V is the output vocabulary) according to\nsome policy PRL(y\u0302t |y\u03021:t\u22121,X) (see Equation (2)). The agent continues to take actions until it produces an End Of Sentence (EOS) token yielding the action sequence Y\u0302 = (y\u03021, y\u03022, . . . , y\u0302|Y\u0302 |), which is also the simplified output of our model. A reward r is then received and we use the REINFORCE algorithm (Williams, 1992) to update the agent. In the following, we first introduce the reward used in our model and then present the details of the REINFORCE algorithm."}, {"heading": "3.1 Reward", "text": "The reward r(Y\u0302 ) for sequence Y\u0302 is the weighted sum of the three components aimed at capturing key aspects of the target output, namely simplicity, relevance, and fluency:\nr(Y\u0302 ) = \u03bbS rS +\u03bbR rR +\u03bbF rF (7)\nwhere \u03bbS,\u03bbR,\u03bbF \u2208 [0,1]; r(Y\u0302 ) is a shorthand for r(X ,Y,Y\u0302 ) where X is the source, Y the reference (or target), and Y\u0302 the system output. rS, rR, and rF are shorthands for simplicity rS(X ,Y,Y\u0302 ), relevance rR(X ,Y\u0302 ), and fluency rF(Y\u0302 ). We provide details for each reward summand below.\nSimplicity To encourage the model to apply a wide range of simplification operations, we use SARI (Xu et al., 2016), a recently proposed metric which compares System output Against References and against the Input sentence. SARI is the arithmetic average of n-gram precision and recall of three rewrite operations: addition, copying, and deletion. It rewards addition operations where system output was not in the input but occurred in the references. Analogously, it rewards words retained/deleted in both the system output and the references. In experimental evaluation Xu et al. (2016) demonstrate that SARI correlates well with human judgments of simplicity, whilst correctly rewarding systems that both make changes and simplify the input.\nOne caveat with using SARI as a reward is the fact that it relies on the availability of multiple references which are rare for sentence simplification. Xu et al. (2016) provide 8 reference simplifications for 2,350 sentences, but these are primarily for system tuning and evaluation rather than training. Most available simplification datasets (see Section 4.1 for details) consist of a single reference for each source sentence. Moreover, they are unavoidably noisy as they are mostly constructed automatically, e.g., by aligning sentences from the\nordinary and simple English Wikipedias. When relying solely on a single reference, SARI will try to reward accidental n-grams that should never have occurred in it. To countenance the effect of noise, we apply SARI(X ,Y\u0302 ,Y ) in the expected direction, with X as the source, Y\u0302 the system output, and Y the reference as well as in the reverse direction with Y as the system output and Y\u0302 as the reference. Assuming our system can produce reasonably good simplifications, by swapping the output and the reference, reverse SARI can be used to estimate how good a reference is with respect to the system output. Our first reward is therefore the linear interpolation of SARI and reverse SARI:\nrS = \u03b2 SARI(X ,Y\u0302 ,Y ) +(1\u2212\u03b2)SARI(X ,Y,Y\u0302 )\n(8)\nRelevance While the simplicity-based reward rS tries to encourage the model to make changes, the relevance reward rR ensures that the generated sentences preserve the meaning of the source. We use an LSTM sentence encoder to convert the source X and the predicted target Y\u0302 into two vectors qX and qY\u0302 . The relevance reward rR is simply the cosine similarity between these two vectors:\nrR = cos(qX ,qY\u0302 ) = qX \u00b7qY\u0302 ||qX || ||qY\u0302 ||\n(9)\nwhere \u00b7 is the dot product between qX and qY\u0302 . We use a sequence auto-encoder (SAE; Dai and Le 2015) to train the LSTM sentence encoder on both the complex and simple sentences. Specifically, the SAE uses sentence X = (x1, . . . ,x|X |) to infer itself via an encoder-decoder model (without an attention mechanism). Firstly, an encoder\nLSTM converts X into a sequence of hidden states (h1, . . . ,h|X |). Then, we use h|X | to initialize the hidden state of the decoder LSTM and recover/generate X one word at a time.\nFluency Xu et al. (2016) observe that SARI has relatively low correlation with fluency compared to other metrics such as BLEU (Papineni et al., 2002). The fluency reward rF explicitly models the well-formedness of the generated sentences. It is the normalized sentence probability assigned by an LSTM language model trained on simple sentences:\nrF = exp ( 1 |Y\u0302 | |Y\u0302 | \u2211 i=1 logPLM(y\u0302i|y\u03020:i\u22121) ) (10)\nWe take the exponential of Y\u0302 \u2019s perplexity to ensure that rF \u2208 [0,1] as is the case with rS and rR."}, {"heading": "3.2 The REINFORCE Algorithm", "text": "The goal of the REINFORCE algorithm is to find an agent that maximizes the expected reward. The training loss for one sequence is its negative expected reward:\nL(\u03b8) =\u2212E(y\u03021,...,y\u0302|Y\u0302 |)\u223cPRL(\u00b7|X)[r(y\u03021, . . . , y\u0302|Y\u0302 |)] (11)\nwhere PRL is our policy, i.e., the distribution produced by the encoder-decoder model (see Equation(2)) and r(\u00b7) is the reward function of an action sequence Y\u0302 = (y\u03021, . . . , y\u0302|Y\u0302 |), i.e., a generated simplification. Unfortunately, computing the expectation term is prohibitive, since there is an infinite number of possible action sequences. In practice, we approximate this expectation with a single sample from the distribution of actions implemented by the encoder-decoder model. We refer\nthe reader to Williams (1992) for the full derivation of the gradients. The gradient of L(\u03b8) is:\n\u2207L(\u03b8)\u2248 \u2211|Y\u0302 |t=1 \u2207 logPRL(y\u0302t |y\u03021:t\u22121,X)[r(y\u03021:|Y\u0302 |)\u2212bt ]\n(12) To reduce the variance of gradients, we also introduce a baseline linear regression model bt to estimate the expected future reward at time t (Ranzato et al., 2016). bt takes the concatenation of hTt and ct as input and outputs a real value as the expected reward. The parameters of the regressor are trained by minimizing mean squared error. We do not back-propagate this error to hTt or ct during training (Ranzato et al., 2016)."}, {"heading": "3.3 Learning", "text": "Presented in its original form, the REINFORCE algorithm starts learning with a random policy. This assumption can make model training challenging for generation tasks like ours with large vocabularies (i.e., action spaces). We address this issue by pre-training our agent (i.e., the encoderdecoder model) with a negative log-likelihood objective (see Section 2), making sure it can produce reasonable simplifications, thereby starting off with a policy which is better than random. We follow prior work (Ranzato et al., 2016) in adopting a curriculum learning strategy. In the beginning of training, we give little freedom to our agent allowing it to predict the last few words for each target sentence. For every sequence, we use negative log-likelihood to train the first L (initially L = 24 ) tokens and apply the reinforcement learning algorithm to the (L+1)th tokens onwards. After every two epochs, we set L = L\u22123 and the training terminates when L equals 0."}, {"heading": "3.4 Lexical Simplification", "text": "Lexical substitution, the replacement of complex words with simpler alternatives, is an integral part of sentence simplification. Although very intuitive, the task is challenging since the substitution must take place in context, preserving both the original meaning and grammaticality of the sentence being simplified. The simplification model presented so far learns lexical substitutions only indirectly. More importantly, there is no guarantee that lexical simplifications when they occur will be meaning preserving; words may often be replaced with substitutes which seem natural in their context but do not reflect the content of the source.\nIn this section we propose a model that explicitly learns lexical simplifications and discuss how it can be integrated with the sentence simplification framework presented in the previous sections.\nWe learn lexical simplification probabilities automatically from parallel corpora containing complex and simple sentences. We use an encoderdecoder model trained on such corpora to obtain probabilistic word alignment scores, i.e., the attention scores \u03b1t in Equation (6). Let X = (x1,x2, . . . ,x|X |) denote a source sentence and Y = (y1,y2, . . . ,y|Y |) a target sentence. We convert X into |X | hidden states (v1,v2, . . . ,v|X |) with an LSTM. Note that vt \u2208 Rd\u00d71 corresponds to the context dependent representation of xt . Let \u03b1t denote the alignment scores \u03b1t1,\u03b1t2, . . . ,\u03b1t|X |. The lexical simplification probability of yt given the source sentence and the alignment scores is:\nPLS(yt |X ,\u03b1t) = softmax(Wl st) (13)\nwhere Wl \u2208 R|V |\u00d7d (|V | is the output vocabulary size) and st the representation of the source:\nst = |X |\n\u2211 i=1 \u03b1tivi (14)\nThe above model encourages lexical substitutions, however, without taking into account what has been generated so far (i.e., y1:t\u22121). As a result, fluency could be compromised. We avoid this by integrating the lexical simplification model with our reinforcement learning trained model (Section 3) using linear interpolation, where \u03b7 \u2208 [0,1]:\nP(yt |y1:t\u22121,X) =(1\u2212\u03b7)PRL(yt |y1:t\u22121,X) +\u03b7PLS(yt |X ,\u03b1t) (15)"}, {"heading": "4 Experimental Setup", "text": "In this section we present our experimental setup for assessing the performance of the simplification model described above. We give details on the datasets we used, model training, evaluation protocol, and the systems used for comparison with our approach."}, {"heading": "4.1 Datasets", "text": "To compare with the state of the art and assess whether our model performs well across the board, we conducted experiments on three existing simplification datasets which we describe below.\nWikiSmall This parallel corpus constructed by Zhu et al. (2010) has been previously used as a benchmark for evaluating automatic text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010). It contains automatically aligned complex and simple sentences from the ordinary English Wikipedia and its simple version. The test set consists of 100 complex-simple sentence pairs. The training set contains 89,042 sentence pairs (after removing duplicates and sentences in the test set). We randomly sampled 205 sentence pairs for development and used the remaining sentences for training. The simplification system put forward in Narayan and Gardent (2014) represents the current state of the art on this dataset.\nWikiLarge We also constructed a larger Wikipedia corpus by combining previously created simplification corpora. Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu\u2019s (2010) WikiSmall dataset described above. We used the development and test sets created in Xu et al. (2016). These are complex sentences taken from WikiSmall paired with simplifications provided by Amazon Mechanical Turk workers. The dataset contains 8 (reference) simplifications for 2,359 sentences partitioned into 2,000 for development and 359 for testing. After removing duplicates and sentences in dev and test sets, the resulting training set contains 296,402 sentence pairs. Xu et al. (2016) report results on this test with a paraphrase-based model outperforming related SMT-based models trained on monolingual data.\nNewsela Xu et al. (2015b) argue that Wikipedia is a suboptimal simplification data resource for at least three reasons: a) the automatic sentence alignment unavoidably introduces errors; b) a large number of purported simplifications are not actually simpler sentences; and c) models trained on Wikipedia generalize poorly to other text genres. The authors move on to introduce, Newsela2, a new dataset consisting of 1,130 news articles, each re-written four times for children at different grade levels by professional editors. The data is by nature a parallel corpus: within each document, there are multiple aligned complex-simple\n2Newsela is a company that produces reading materials for pre-college classroom use.\nsentence pairs at different levels of simplicity. 0 is the most complex level and 4 is simplest. We removed sentence pairs corresponding to levels 0\u20131, 1\u20132, and 2\u20133, since they were too similar to each other. The first 1,070 documents were used for training (94,208 sentence pairs), the next 30 documents for development (1,129 sentence pairs) and the last 30 documents for testing (1,076 sentence pairs). We are not aware of any published results on this dataset."}, {"heading": "4.2 Training Details", "text": "We trained all our models on an Nvidia GPU card with 4G RAM. We used the same hyperparameters across datasets. We first trained an encoder-decoder model, and then performed reinforcement learning training (Section 3). Lastly, we train the lexical simplification model (Section 3.4). Encoder-decoder parameters were uniformly initialized to [\u22120.1,0.1]. We used Adam (Kingma and Ba, 2014) to optimize the model with learning rate 0.001; the first momentum coefficient was set to 0.9 and the second momentum coefficient to 0.999. The gradient was rescaled when the norm exceeded 5 (Pascanu et al., 2013). Both the encoder and decoder LSTMs have two layers with 256 hidden neurons in each layer. We regularized all LSTMs with a dropout rate of 0.2 (Srivastava et al., 2014; Zaremba et al., 2014). We initialized the encoder and decoder word embedding matrices with pre-trained 300 dimensional Glove vectors (Pennington et al., 2014).\nDuring reinforcement training, we used plain stochastic gradient descent with a learning rate of 0.01. We set \u03b2 = 0.1, \u03bbS = 1, \u03bbR = 0.25 and \u03bbF = 0.5.3 Training details for the lexical simplification model are identical to the encoder-decoder model except that the word embedding matrices were randomly initialized. The weight of the lexical simplification model in the final prediction was set to \u03b7 = 0.1. To reduce the size of the vocabulary, named entities were tagged with the Stanford CoreNLP toolkit (Manning et al., 2014) and anonymized with a NE@N token, where NE \u2208 {PER,LOC,ORG,MISC} and N is the N-th NE type. At test time, we de-anonymize NE@N tokens in generated sentences by looking them up in their corresponding source sentences.\n3Weights were tuned on the development set of the Newsela dataset and kept fixed for the other two datasets."}, {"heading": "4.3 Evaluation", "text": "Following previous work (Woodsend and Lapata, 2011; Xu et al., 2016) we evaluated system output automatically. We used BLEU4 (Papineni et al., 2002) to assess the degree to which generated simplifications differed from the gold standard references and the Flesch-Kincaid Grade Level index5 (FKGL) to measure the readability of the output (lower FKGL implies simpler output)6. In addition, we used SARI (Xu et al., 2016), which evaluates the quality of the output by comparing it against the source and the references7.\nWe also evaluated the generated simplifications by eliciting human judgments. Specifically, we invited 26 annotators (all self-reported native English speakers) to assess the output of our systems (and comparison systems). They were asked to rate the simplifications on three dimensions: Grammaticality (is the output grammatical and well formed?), Meaning (does the output preserve the meaning of the original sentence?) and Simplicity (is the output simpler than the original sentence?). All ratings were obtained using a five point Likert scale."}, {"heading": "5 Results", "text": "The top blocks in Tables 1 and 2 summarize our results on the Newsela dataset. We compared sev-\n4With the default mtevalv13a.pl settings. 5See Kincaid et al. (1975) for more details on FKGL. 6We used the implementation of FKGL from https:// github.com/mmautner/readability. 7We used the implementation of SARI in Xu et al. (2016).\neral configurations of our own model: EncDecA, a straightforward adaption of the attention-based encoder-decoder model to the simplification task (Section 2); EncDecA-RF, the deep reinforcement learning model introduced in Section 3, where a pre-trained EncDecA is used to initialize the reinforcement model; and EncDecA-RF-LS, a linear combination of EncDecA-RF and the lexical simplification model presented in Section 3.4; Neuralnetwork models were further compared against a strong baseline, the simplification model introduced in Wubben et al. (2012): PBMT-R, is a monolingual phrase-based machine translation system with a reranking post-processing step.8\nAs shown in Table 1, all encoder-decoder based models obtain higher BLEU, lower FKGL and higher SARI compared to PBMT-R. Amongst the encoder-decoder models, EncDecA-RF obtains the lowest FKGL and highest SARI scores. This is perhaps not surprising, since SARI is part of our reward. Integrating lexical simplification (EncDecA-RF-LS) yields better BLEU, but slightly worse FKGL and SARI compared to EncDecA-RF.\nThe results of our human evaluation study are presented in Table 2. We elicited judgments for 100 randomly sampled test sentences. Aside from comparing system output (PBMT-R, EncDecA, EncDecA-RF, and EncDecA-RF-LS) we also elicited ratings for the gold standard Reference sentences as an upper bound. We report\n8We made a good-faith effort to re-implement their system following closely the details in Wubben et al. (2012).\nresults for Grammaticality, Meaning, and Simplicity individually and in combination (All is the average rating of the three dimensions). As can be seen, all encoder-decoder based models (EncDecA, EncDecA-RF and EncDecA-RF-LS) outperform PBMT-R on Grammaticality, Simplicity and overall, while they obtain worse ratings on Meaning. Interestingly, our models are closest to the human references on the Meaning dimension. Table 3 shows example output of PBMT-R and our models on the Newsela dataset.\nBoth EncDecA-RF and EncDecA-RF-LS are significantly better on Simplicity than EncDecA and PBMT-R (p < 0.01, using a student t-test), which indicates that our reinforcement learning based model is effective at creating simpler output. Combined ratings for EncDecA-RF-LS are significantly different compared to the other models (p < 0.05) and the Reference. Integration of the lexical simplification model boosts performance as ratings increase across all dimensions. All neural network models fare well on Grammaticality, which may not be entirely surprising given the recent success of LSTM models in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015).\nThe middle blocks in Tables 1 and 2 report results on the WikiSmall dataset. Besides PBMT-R (Wubben et al., 2012) and different versions of our model, we also compared against Narayan and Gardent (2014) whose model (Hybrid) is state of the art on this dataset.9 FKGL and SARI follow a similar pattern as on the Newsela dataset. BLEU scores for PBMT-R, Hybrid, and EncDecA are much higher compared to EncDecA-RF and EncDecA-RF-LS. Hybrid obtains best BLEU and SARI scores, while EncDecA-RF and EncDecARF-LS do very well on FKGL.\nIn human evaluation, we elicited judgments on the entire WikiSmall test set (100 sentences). We compared our final model EncDecA-RF-LS with PBMT-R, Hybrid, and gold standard Reference simplifications. EncDecA-RF-LS is significantly better on Simplicity than PBMT-R and Hybrid (p < 0.01). It performs on par with PBMT-R on Grammaticality and worse on Meaning (but still closer to the human Reference than PBMTR or Hybrid). When combining all ratings (All in Table 2), EncDecA-RF-LS is significantly better\n9We are grateful to Shashi Narayan for providing us with the output of his system.\nthan PBMT-R (p < 0.05) and Hybrid (p < 0.01). The bottom blocks in Tables 1 and 2 report results on the WikiLarge dataset. We compare our models with PBMT-R (Wubben et al., 2012) and SBMR-SARI (Xu et al., 2016)10, a syntax-based translation model trained on the PPDB dataset (Ganitkevitch et al., 2013) and tuned with SARI. PPDB, which contains 106 million sentence pairs with 2 billion words, is much larger than our WkiLarge dataset. The FKGL follows a similar patten as in the previous datasets. Our models and PBMT-R are best in terms of BLEU while SBMTSARI outperforms all other systems on SARI. In human evaluation, we again elicited judgments for 100 randomly sampled test sentences. EncDecARF-LS is significantly better than SBMT-SARI and PBMT-R on all dimensions including \u201dAll\u201d (p< 0.05) except for Meaning. We used more data to train PBMT-R and maybe that is why PBMT-R performs better than Xu et al. (2016) reported.\n10We are grateful to Wei Xu for making their system output publicly available."}, {"heading": "6 Conclusions", "text": "In this paper, we developed a deep reinforcement learning based text simplification model, which can jointly model simplicity, grammaticality, and semantic fidelity to the input. We also proposed a lexical simplification model that further boosts performance. Overall, we find that reinforcement learning offers a great means to inject prior knowledge to the simplification task achieving state-ofthe-art results across three datasets (see Table 2). In the future, we would like to explicitly incorporate sentence splitting into our model. We are also interested in simplifying entire documents (rather than individual sentences)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR 2015.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Text simplification for informationseeking applications", "author": ["Beata Beigman Klebanov", "Kevin Knight", "Daniel Marcu."], "venue": "Proceedings of ODBASE. Springer, Agia Napa, Cyprus, volume 3290 of Lecture Notes in Computer Science, pages 735\u2013747.", "citeRegEx": "Klebanov et al\\.,? 2004", "shortCiteRegEx": "Klebanov et al\\.", "year": 2004}, {"title": "Simplifying text for languageimpaired readers", "author": ["J. Carroll", "G. Minnen", "D. Pearce", "Y. Canning", "S. Devlin", "J Tait."], "venue": "Proceedings of the 9th EACL. Bergen, Norway, pages 269\u2013270.", "citeRegEx": "Carroll et al\\.,? 1999", "shortCiteRegEx": "Carroll et al\\.", "year": 1999}, {"title": "Motivations and methods for text simplification", "author": ["R. Chandrasekar", "C. Doran", "B. Srinivas."], "venue": "Proceedings of the 16th COLING. Copenhagen, Denmark, pages 1041\u20131044.", "citeRegEx": "Chandrasekar et al\\.,? 1996", "shortCiteRegEx": "Chandrasekar et al\\.", "year": 1996}, {"title": "Abstractive sentence summarization with attentive recurrent neural networks", "author": ["Sumit Chopra", "Michael Auli", "Alexander M. Rush."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-", "citeRegEx": "Chopra et al\\.,? 2016", "shortCiteRegEx": "Chopra et al\\.", "year": 2016}, {"title": "Linguistically motivated large-scale nlp with c&c and boxer", "author": ["James Curran", "Stephen Clark", "Johan Bos."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and", "citeRegEx": "Curran et al\\.,? 2007", "shortCiteRegEx": "Curran et al\\.", "year": 2007}, {"title": "Semi-supervised sequence learning", "author": ["Andrew M Dai", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems. pages 3079\u20133087.", "citeRegEx": "Dai and Le.,? 2015", "shortCiteRegEx": "Dai and Le.", "year": 2015}, {"title": "Simplifying Natural Language for Aphasic Readers", "author": ["Siobhan Devlin."], "venue": "Ph.D. thesis, University of Sunderland.", "citeRegEx": "Devlin.,? 1999", "shortCiteRegEx": "Devlin.", "year": 1999}, {"title": "An evaluation of syntactic simplification rules for people with autism", "author": ["Richard Evans", "Constantin Or asan", "Iustin Dornescu."], "venue": "Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR).", "citeRegEx": "Evans et al\\.,? 2014", "shortCiteRegEx": "Evans et al\\.", "year": 2014}, {"title": "PPDB: The paraphrase database", "author": ["Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch."], "venue": "Proceedings of NAACLHLT . Association for Computational Linguistics, Atlanta, Georgia, pages 758\u2013764.", "citeRegEx": "Ganitkevitch et al\\.,? 2013", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text simplification for reading assistance: A project note", "author": ["Kentaro Inui", "Atsushi Fujita", "Tetsuro Takahashi", "Ryu Iida", "Tomoya Iwakura."], "venue": "Proceedings of the Second International Workshop on Paraphrasing. Association for Computa-", "citeRegEx": "Inui et al\\.,? 2003", "shortCiteRegEx": "Inui et al\\.", "year": 2003}, {"title": "Montreal neural machine translation systems for wmt15", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the Tenth Workshop on Statistical Machine Translation. pages 134\u2013140.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Verb paraphrase based on case frame alignment", "author": ["Nobuhiro Kaji", "Daisuke Kawahara", "Sadao Kurohashi", "Satoshi Sato."], "venue": "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Philadelphia, Pennsylvania, USA, pages", "citeRegEx": "Kaji et al\\.,? 2002", "shortCiteRegEx": "Kaji et al\\.", "year": 2002}, {"title": "Improving text simplification language modeling using unsimplified text data", "author": ["David Kauchak."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Com-", "citeRegEx": "Kauchak.,? 2013", "shortCiteRegEx": "Kauchak.", "year": 2013}, {"title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel", "author": ["J Peter Kincaid", "Robert P Fishburne Jr", "Richard L Rogers", "Brad S Chissom."], "venue": "Technical report, DTIC", "citeRegEx": "Kincaid et al\\.,? 1975", "shortCiteRegEx": "Kincaid et al\\.", "year": 1975}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky", "Michel Galley", "Jianfeng Gao."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic", "author": ["Jing Zhu"], "venue": null, "citeRegEx": "Zhu.,? \\Q2002\\E", "shortCiteRegEx": "Zhu.", "year": 2002}, {"title": "A survey of automated text simplification", "author": ["Matthew Shardlow."], "venue": "International Journal of Advanced Computer Science and Applications pages 581\u2013701. Special Issue on Natural Language Processing.", "citeRegEx": "Shardlow.,? 2014", "shortCiteRegEx": "Shardlow.", "year": 2014}, {"title": "Syntactic simplification and text cohesion", "author": ["Advaith Siddharthan."], "venue": "research on language and computation. Research on Language and Computation 4(1):77\u2013109.", "citeRegEx": "Siddharthan.,? 2004", "shortCiteRegEx": "Siddharthan.", "year": 2004}, {"title": "A survey of research on text simplification", "author": ["Advaith Siddharthan."], "venue": "International Journal of Applied Linguistics 165(2):259\u2013298.", "citeRegEx": "Siddharthan.,? 2014", "shortCiteRegEx": "Siddharthan.", "year": 2014}, {"title": "Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies", "author": ["David A Smith", "Jason Eisner."], "venue": "Proceedings of the Workshop on Statistical Machine Translation. Association for Computational Linguistics, pages", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in Neural Information Processing Systems, Curran Associates, Inc., pages 3104\u2013 3112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Sentence simplification for semantic role labeling", "author": ["D. Vickrey", "D. Koller."], "venue": "Proceedings of ACL-08: HLT . Columbus, OH, pages 344\u2013352.", "citeRegEx": "Vickrey and Koller.,? 2008", "shortCiteRegEx": "Vickrey and Koller.", "year": 2008}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming", "author": ["Kristian Woodsend", "Mirella Lapata."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Woodsend and Lapata.,? 2011", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2011}, {"title": "Text rewriting improves semantic role labeling", "author": ["Kristian Woodsend", "Mirella Lapata."], "venue": "Journal of Artificial Intelligence Research 51:133\u2013164.", "citeRegEx": "Woodsend and Lapata.,? 2014", "shortCiteRegEx": "Woodsend and Lapata.", "year": 2014}, {"title": "Sentence simplification by monolingual machine translation", "author": ["Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Asso-", "citeRegEx": "Wubben et al\\.,? 2012", "shortCiteRegEx": "Wubben et al\\.", "year": 2012}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044", "citeRegEx": "Xu et al\\.,? 2015a", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Problems in current text simplification research: New data can help", "author": ["Wei Xu", "Chris Callison-Burch", "Courtney Napoles."], "venue": "Transactions of the Association for Computational Linguistics 3:283\u2013297.", "citeRegEx": "Xu et al\\.,? 2015b", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Optimizing statistical machine translation for text simplification", "author": ["Wei Xu", "Courtney Napoles", "Ellie Pavlick", "Quanze Chen", "Chris Callison-Burch."], "venue": "Transactions of the Association for Computational Linguistics 4:401\u2013415.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "A syntaxbased statistical translation model", "author": ["Kenji Yamada", "Kevin Knight."], "venue": "Proceedings of the 39th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 523\u2013530.", "citeRegEx": "Yamada and Knight.,? 2001", "shortCiteRegEx": "Yamada and Knight.", "year": 2001}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "A monolingual tree-based translation model for sentence simplification", "author": ["Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych."], "venue": "Proceedings of the 23rd international conference on computational linguistics. Association for Computational Linguistics,", "citeRegEx": "Zhu et al\\.,? 2010", "shortCiteRegEx": "Zhu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 21, "context": "The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individuals alike (Siddharthan, 2014; Shardlow, 2014).", "startOffset": 153, "endOffset": 188}, {"referenceID": 19, "context": "The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individuals alike (Siddharthan, 2014; Shardlow, 2014).", "startOffset": 153, "endOffset": 188}, {"referenceID": 3, "context": "For instance, a simplification component could be used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al.", "startOffset": 117, "endOffset": 144}, {"referenceID": 25, "context": ", 2004) and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014).", "startOffset": 35, "endOffset": 88}, {"referenceID": 28, "context": ", 2004) and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014).", "startOffset": 35, "endOffset": 88}, {"referenceID": 8, "context": "autism (Evans et al., 2014), aphasia (Carroll et al.", "startOffset": 7, "endOffset": 27}, {"referenceID": 2, "context": ", 2014), aphasia (Carroll et al., 1999), or dyslexia (Rello et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 21, "context": "and deleting elements of the original text (Siddharthan, 2014).", "startOffset": 43, "endOffset": 62}, {"referenceID": 2, "context": "splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al.", "startOffset": 10, "endOffset": 104}, {"referenceID": 3, "context": "splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al.", "startOffset": 10, "endOffset": 104}, {"referenceID": 25, "context": "splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al.", "startOffset": 10, "endOffset": 104}, {"referenceID": 20, "context": "splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al.", "startOffset": 10, "endOffset": 104}, {"referenceID": 7, "context": ", 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).", "startOffset": 180, "endOffset": 232}, {"referenceID": 11, "context": ", 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).", "startOffset": 180, "endOffset": 232}, {"referenceID": 13, "context": ", 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).", "startOffset": 180, "endOffset": 232}, {"referenceID": 18, "context": "For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally per-", "startOffset": 13, "endOffset": 31}, {"referenceID": 18, "context": "For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally per-", "startOffset": 13, "endOffset": 134}, {"referenceID": 22, "context": "Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications.", "startOffset": 98, "endOffset": 122}, {"referenceID": 26, "context": "Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications.", "startOffset": 0, "endOffset": 27}, {"referenceID": 22, "context": "Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications. Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs.", "startOffset": 99, "endOffset": 232}, {"referenceID": 5, "context": "Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007).", "startOffset": 144, "endOffset": 165}, {"referenceID": 9, "context": "(2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output.", "startOffset": 90, "endOffset": 117}, {"referenceID": 29, "context": "Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al.", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "translation, a new approach to machine translation based purely on neural networks (Bahdanau et al., 2015; Sutskever et al., 2014).", "startOffset": 83, "endOffset": 130}, {"referenceID": 24, "context": "translation, a new approach to machine translation based purely on neural networks (Bahdanau et al., 2015; Sutskever et al., 2014).", "startOffset": 83, "endOffset": 130}, {"referenceID": 26, "context": "Our model uses the encoder-decoder architecture as its backbone coupled with a REINFORFCE-style (Williams, 1992) training algorithm.", "startOffset": 96, "endOffset": 112}, {"referenceID": 17, "context": ", 2016), dialogue generation (Li et al., 2016), machine translation and image caption generation (Ranzato et al.", "startOffset": 29, "endOffset": 46}, {"referenceID": 17, "context": "To train our network, we use a policy gradients method (Ranzato et al., 2016; Li et al., 2016) which we adapt to the simplification problem so as to optimize a reward function which jointly reflects the grammaticality and simplicity of the output, while preserving the meaning of the input.", "startOffset": 55, "endOffset": 94}, {"referenceID": 35, "context": "We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al.", "startOffset": 98, "endOffset": 143}, {"referenceID": 27, "context": "We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al.", "startOffset": 98, "endOffset": 143}, {"referenceID": 31, "context": ", 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b).", "startOffset": 68, "endOffset": 86}, {"referenceID": 0, "context": "which can be modeled with attention-based encoder-decoder models (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 65, "endOffset": 108}, {"referenceID": 12, "context": "Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence", "startOffset": 71, "endOffset": 129}, {"referenceID": 4, "context": "Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence", "startOffset": 71, "endOffset": 129}, {"referenceID": 30, "context": "Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence", "startOffset": 71, "endOffset": 129}, {"referenceID": 26, "context": "A reward r is then received and we use the REINFORCE algorithm (Williams, 1992) to update the agent.", "startOffset": 63, "endOffset": 79}, {"referenceID": 32, "context": "Simplicity To encourage the model to apply a wide range of simplification operations, we use SARI (Xu et al., 2016), a recently proposed metric which compares System output Against References and against the Input sentence.", "startOffset": 98, "endOffset": 115}, {"referenceID": 30, "context": "In experimental evaluation Xu et al. (2016) demonstrate that SARI correlates well with human judgments of simplicity, whilst correctly rewarding systems that both make changes and simplify the input.", "startOffset": 27, "endOffset": 44}, {"referenceID": 30, "context": "In experimental evaluation Xu et al. (2016) demonstrate that SARI correlates well with human judgments of simplicity, whilst correctly rewarding systems that both make changes and simplify the input. One caveat with using SARI as a reward is the fact that it relies on the availability of multiple references which are rare for sentence simplification. Xu et al. (2016) provide 8 reference simplifications for 2,350 sentences, but these are primarily for system tuning and evaluation rather than training.", "startOffset": 27, "endOffset": 370}, {"referenceID": 30, "context": "Fluency Xu et al. (2016) observe that SARI has relatively low correlation with fluency compared to other metrics such as BLEU (Papineni et al.", "startOffset": 8, "endOffset": 25}, {"referenceID": 26, "context": "the reader to Williams (1992) for the full derivation of the gradients.", "startOffset": 14, "endOffset": 30}, {"referenceID": 29, "context": "(2010) has been previously used as a benchmark for evaluating automatic text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).", "startOffset": 100, "endOffset": 193}, {"referenceID": 27, "context": "(2010) has been previously used as a benchmark for evaluating automatic text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).", "startOffset": 100, "endOffset": 193}, {"referenceID": 35, "context": "(2010) has been previously used as a benchmark for evaluating automatic text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).", "startOffset": 100, "endOffset": 193}, {"referenceID": 14, "context": "Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu\u2019s (2010) WikiSmall dataset described above.", "startOffset": 58, "endOffset": 73}, {"referenceID": 14, "context": "Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu\u2019s (2010) WikiSmall dataset described above.", "startOffset": 58, "endOffset": 144}, {"referenceID": 14, "context": "Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu\u2019s (2010) WikiSmall dataset described above.", "startOffset": 58, "endOffset": 162}, {"referenceID": 30, "context": "Xu et al. (2016) report results on this test with a paraphrase-based model outperforming related SMT-based models trained on monolingual data.", "startOffset": 0, "endOffset": 17}, {"referenceID": 30, "context": "Newsela Xu et al. (2015b) argue that Wikipedia is a suboptimal simplification data resource for at least three reasons: a) the automatic sentence alignment unavoidably introduces errors; b) a large number of purported simplifications are not actually simpler sentences; and c) models trained on Wikipedia generalize poorly to other text genres.", "startOffset": 8, "endOffset": 26}, {"referenceID": 16, "context": "We used Adam (Kingma and Ba, 2014) to optimize the model with learning rate 0.", "startOffset": 13, "endOffset": 34}, {"referenceID": 23, "context": "2 (Srivastava et al., 2014; Zaremba et al., 2014).", "startOffset": 2, "endOffset": 49}, {"referenceID": 34, "context": "2 (Srivastava et al., 2014; Zaremba et al., 2014).", "startOffset": 2, "endOffset": 49}, {"referenceID": 27, "context": "Following previous work (Woodsend and Lapata, 2011; Xu et al., 2016) we evaluated system output automatically.", "startOffset": 24, "endOffset": 68}, {"referenceID": 32, "context": "Following previous work (Woodsend and Lapata, 2011; Xu et al., 2016) we evaluated system output automatically.", "startOffset": 24, "endOffset": 68}, {"referenceID": 32, "context": "In addition, we used SARI (Xu et al., 2016), which evaluates the quality of the output by comparing it against the source and the references7.", "startOffset": 26, "endOffset": 43}, {"referenceID": 15, "context": "5See Kincaid et al. (1975) for more details on FKGL.", "startOffset": 5, "endOffset": 27}, {"referenceID": 15, "context": "5See Kincaid et al. (1975) for more details on FKGL. 6We used the implementation of FKGL from https:// github.com/mmautner/readability. 7We used the implementation of SARI in Xu et al. (2016). Newsela Grammar Mean Simple All", "startOffset": 5, "endOffset": 192}, {"referenceID": 29, "context": "a strong baseline, the simplification model introduced in Wubben et al. (2012): PBMT-R, is a monolingual phrase-based machine translation system with a reranking post-processing step.", "startOffset": 58, "endOffset": 79}, {"referenceID": 29, "context": "8We made a good-faith effort to re-implement their system following closely the details in Wubben et al. (2012).", "startOffset": 91, "endOffset": 112}, {"referenceID": 34, "context": "ral network models fare well on Grammaticality, which may not be entirely surprising given the recent success of LSTM models in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015).", "startOffset": 177, "endOffset": 218}, {"referenceID": 12, "context": "ral network models fare well on Grammaticality, which may not be entirely surprising given the recent success of LSTM models in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015).", "startOffset": 177, "endOffset": 218}, {"referenceID": 29, "context": "Besides PBMT-R (Wubben et al., 2012) and different versions of our model, we also compared against Narayan and", "startOffset": 15, "endOffset": 36}, {"referenceID": 29, "context": "We compare our models with PBMT-R (Wubben et al., 2012) and SBMR-SARI (Xu et al.", "startOffset": 34, "endOffset": 55}, {"referenceID": 32, "context": ", 2012) and SBMR-SARI (Xu et al., 2016)10, a syntax-based translation model trained on the PPDB dataset (Ganitkevitch et al.", "startOffset": 22, "endOffset": 39}, {"referenceID": 9, "context": ", 2016)10, a syntax-based translation model trained on the PPDB dataset (Ganitkevitch et al., 2013) and tuned with SARI.", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": ", 2016)10, a syntax-based translation model trained on the PPDB dataset (Ganitkevitch et al., 2013) and tuned with SARI. PPDB, which contains 106 million sentence pairs with 2 billion words, is much larger than our WkiLarge dataset. The FKGL follows a similar patten as in the previous datasets. Our models and PBMT-R are best in terms of BLEU while SBMTSARI outperforms all other systems on SARI. In human evaluation, we again elicited judgments for 100 randomly sampled test sentences. EncDecARF-LS is significantly better than SBMT-SARI and PBMT-R on all dimensions including \u201dAll\u201d (p< 0.05) except for Meaning. We used more data to train PBMT-R and maybe that is why PBMT-R performs better than Xu et al. (2016) reported.", "startOffset": 73, "endOffset": 716}], "year": 2017, "abstractText": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model brings significant improvements over the state of the art.1", "creator": "LaTeX with hyperref package"}}}