{"id": "1704.00514", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "Multi-Task Learning of Keyphrase Boundary Classification", "abstract": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.", "histories": [["v1", "Mon, 3 Apr 2017 10:25:22 GMT  (19kb)", "https://arxiv.org/abs/1704.00514v1", "ACL 2017"], ["v2", "Wed, 26 Apr 2017 16:48:49 GMT  (25kb)", "http://arxiv.org/abs/1704.00514v2", "ACL 2017"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI stat.ML", "authors": ["isabelle augenstein", "anders s\u00f8gaard"], "accepted": true, "id": "1704.00514"}, "pdf": {"name": "1704.00514.pdf", "metadata": {"source": "CRF", "title": "Multi-Task Learning of Keyphrase Boundary Classification", "authors": ["Isabelle Augenstein", "Anders S\u00f8gaard"], "emails": ["i.augenstein@ucl.ac.uk", "soegaard@di.ku.dk"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n00 51\n4v 2\n[ cs\n.C L\n] 2\n6 A\npr 2\n01 7\nis the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases."}, {"heading": "1 Introduction", "text": "The scientific keyphrase boundary classification (KBC) task consists of a) determining keyphrase boundaries, and b) labelling keyphrases with their types according to a predefined schema. KBC is motivated by the need to efficiently search scientific literature, which can be summarised by their keyphrases. Several companies are working on keyphrase-based recommender systems for scientific literature or search interfaces where scientific articles decorate graphs, in which nodes are keyphrases. Such keyphrases must be dynamically retrieved from the articles, because important scientific concepts emerge on a daily basis, and the most recent concepts are typically the ones of interest to scientists.\nKBC is not a common task in NLP, and there are only few small annotated datasets for inducing supervised KBC models, made\n\u22c6Both authors contributed equally\navailable recently (QasemiZadeh and Schumann, 2016; Augenstein et al., 2017). Typical KBC approaches therefore rely on hand-crafted gazetteers (Hasan and Ng, 2014) or reduce the task to extracting a list of keyphrases for each document (Kim et al., 2010) instead of identifying mentions of keyphrases in sentences. For related more common NLP tasks such as named entity recognition and identification of multi-word expressions, neural sequence labelling methods have been shown to be useful (Lample et al., 2016). In order to overcome the small data problem, we study using more widely available data for tasks related to KBC and exploit their synergies in a deep multi-task learning setup.\nMulti-task learning has become popular within natural language processing and machine learning over the last few years; in particular, hard parameter sharing of hidden layers in deep learning models. This approach to multi-task learning has three advantages: a) It significantly reduces Rademacher complexity (Baxter, 2000; Maurer, 2007), i.e., the risk of over-fitting, b) it is spaceefficient, reducing the number of parameters, and c) it is easy to implement.\nThis paper shows how hard parameter sharing can be used to improve gazetteer-free keyphrase boundary classification models, by exploiting different syntactically and semantically annotated corpora, as well as more readily available data such as hyperlinks.\nContributions We study the so far widely underexplored, though in practice important task of scientific keyphrase boundary classification, for which only a small amount of training data is available. We overcome this by identifying good auxiliary tasks and cast it as a multi-task learning problem. We evaluate our models across two new, manually annotated corpora of scientific arti-\ncles and outperform single-task approaches by up to 9.64% F1, mostly due to better performance for long keyphrases."}, {"heading": "2 Keyphrase Boundary Classification", "text": "Consider the following sentence from a scientific paper:\n(1) We find that simple interpolation methods,\nlike log-linear and linear interpolation, improve the performance but fall short of the performance of an oracle.\nThis sentence occurs in the ACL RD-TEC 2.0 corpus. Here, interpolation methods and loglinear and linear interpolation are annotated as technical keyphrases, performance as a keyphrase related to measurements, and oracle is a keyphrase labelled as miscellaneous. Below, we are interested in predicting the boundaries and the types of all keyphrases."}, {"heading": "3 Multi-Task Learning", "text": "Multi-task learning is an approach to learning, in which generalisation is improved by taking advantage of the inductive bias in training signals of related tasks. When abundant labelled data is available for an auxiliary task, but little data for the target task, multi-task learning can act as a form of semi-supervised learning combined with a distant supervision signal. Inducing a model from only the sparse target task data may lead to overfitting to random noise in the data, but relying on auxiliary data helps the model generalise, making it easier to abstract away from noise, as well as leveraging the marginal distribution of auxiliary input data. From a representation learning perspective, auxiliary tasks can be used to induce representations that may be beneficial for the target task. Caruana (1993) also suggests that the auxiliary task can help focus attention in the induction of the target task model. Finally, multi-task learning can be cast as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007).\nHere, we follow the probably most common approach to multi-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks. We assume T different training set, D1, \u00b7 \u00b7 \u00b7 ,DT ,\nwhere each Dt contains pairs of input-output sequences (w1:n, y t 1:n ), wi \u2208 V , y t i \u2208 Lt. The input vocabulary V is shared across tasks, but the output vocabularies (tagset) Lt are task dependent. At each step in the training process we choose a random task t, followed by a random training instance (w1:n, y t 1:n ) \u2208 Dt. We use the tagger to predict the labels y\u0302t i , suffer a loss with respect to the true labels yt i and update the model parameters. The parameters are trained jointly for a sentence, i.e. cross-entropy loss over each sentence is employed. Each task is associated with an independent classification function, but all tasks share the hidden layers. Note that for our experiments, we only consider one auxiliary task at a time."}, {"heading": "4 Experiments", "text": "Experimental Setup We perform experiments for both keyphrase boundary identification (unlabelled), and keyphrase boundary identification and classification (labelled). Metrics measured are token-level precision, recall and F1, which are micro-average results across keyphrase types. Types are defined by the two datasets studied.\nAuxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task at a time. Note that the datasets for the auxiliary tasks are not annotated with keyphrase boundary identification or classification labels.\nDatasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.0 dataset (QasemiZadeh and Schumann, 2016). The SemEval 2017 dataset is annotated with three keyphrase types, the ACL RD-TEC dataset with seven. For the former, we test on the development portion of the dataset, as the test set is not released yet. We randomly split ACL RD-TEC into a\ntraining and test set, reserving 1/3 for testing. Key dataset characteristics are summarised in Table 1. One important observation is that the SemEval 2017 dataset contains a significantly higher proportion of long keyphrases than the ACL dataset.\nModels Our single- and multi-task networks are three-layer, bi-directional LSTMs (Graves and Schmidhuber, 2005) with pre-trained SENNA embeddings.1 For the multi-task networks, we follow the training procedure outlined in Section 3. The dimensionality of the embeddings is 50, and we follow S\u00f8gaard and Goldberg (2016) in using the same dimensionality for the hidden layers. We add a dropout of 0.1 to the input and train these architectures with momentum SGD with initial learning rate of 0.001 and momentum of 0.9 for 10 epochs.\nBaselines Our baselines are Finkel et al. (2005)2 and Lample et al. (2016)3, in order to compare to a lexicalised and a state-of-the-art neural method. We use the implementations released by the authors and re-train models on our data."}, {"heading": "5 Results and Analysis", "text": "Results for SemEval 2017 Task 10 corpus are presented in Table 2, and for the ACL RD-TEC corpus in Table 3. For the SemEval corpus, all five labelled multi-task learning models outperform both examples of previous work, as well as our singletask BiLSTM baseline, by some margin. For ACL\n1 http://ronan.collobert.com/senna/ 2http://nlp.stanford.edu/software/\nCRF-NER.shtml 3 https://github.com/clab/\nstack-lstm-ner\nRD-TEC, three of out five multi-task learning labelled labelled perform better than the single-task BiLSTM baseline.\nOn the SemEval corpus, the F1 error reduction of of the best labelled model over the Stanford tagger is 9.64%. The lexicalised Finkel et al. (2005) model shows a surprisingly competitive performance on the ACL RD-TEC corpus, where it is only 2 points in F1 behind our best performing labelled model and on par with our best-performing unlabelled model. Results with Lample et al. (2016), on the other hand, are lower than the Finkel et al. (2005) baseline. This might be due to the model having a large set of parameters to model state transitions which poses a difficulty for small training datasets.\nOverall, multi-task models show bigger improvements over baselines for the SemEval corpus, and all models achieve better results on ACL RD-TEC. Statistics shown in Table 1 help to explain this. Most noticeably, the SemEval dataset contains a significantly higher proportion of long keyphrases than the ACL dataset. Interestingly, ACL RD-TEC contains a large proportion of keyphrases which only appear once in the training set (singletons), significantly fewer keyphrases and more keyphrase type, but that does not seem to impact results as much as a high proportion of long keyphrases.\nAll models struggle with semantically vague or broad keyphrases (e.g. \u2018items\u2019, \u2018scope\u2019, \u2018key\u2019) and long keyphrases, especially those containing clauses (e.g. \u2018complete characterisation of the oxide particles\u2019, \u2018earley deduction proof procedure for definite clauses\u2019). The multi-task models generally outperform the BiLSTM baseline for long\nphrases (e.g. \u2018language-independent system for automatic discovery of text in parallel translation\u2019, \u2018honeycomb network of graphite bricks\u2019). Being able to recognise long keyphrases correctly is part of the reason our multi-task models outperform the baselines, especially on the SemEval dataset, which contains many such long keyphrases."}, {"heading": "6 Related Work", "text": "Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected layers (S\u00f8gaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). S\u00f8gaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on heterogeneous tasks such as the ones considered here. Hard parameter sharing has been studied for several tasks, including CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017). Shar-\ning of information can further be achieved by extending LSTMs with an external memory shared across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and auto-encoding, and in Rei (2017) for different sequence labelling tasks and language modelling.\nBoundaryClassification KBC is very similar to named entity recognition (NER), though arguably harder. Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016), currently state-of-the-art for NER, stack CRFs on top of recurrent neural networks. We leave exploring such models in combination with multi-task learning for future work.\nKeyphrase detection methods specific to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014). However, previous methods relied on corpora annotated for type-level identification, not\nfor mention-level identification (Kim et al., 2010; Sterckx et al., 2016). While most applications rely on extracting keyphrases (as types), this has the unfortunate consequence that previous work ignores acronyms and other short-hand forms referring to methods, metrics, etc. Further, relying on gazetteers makes overfitting likely, obtaining lower scores on out-of-gazetteer keyphrases."}, {"heading": "7 Conclusions and Future Work", "text": "We present a new state of the art for keyphrase boundary classification, using data from related, auxiliary tasks; in particular, super-sense tagging and identification of multi-word expressions. Deep multi-task learning improves significantly on previous approaches to KBC, with error reductions of up to 9.64%, mostly due to better identification and labelling of long keyphrases.\nIn future work, we want to explore alternative multi-task learning regimes to hard parameter sharing and experiment with additional auxiliary tasks. The auxiliary tasks considered here are standard NLP tasks, hyperlink prediction aside. Other tasks may be more directly relevant such as predicting the layout of calls for papers for scientific conferences, or predicting hashtags in tweets by scientists, since both data sources contain scientific keyphrases."}, {"heading": "Acknowledgments", "text": "Wewould like to thank Elsevier for supporting this work."}], "references": [{"title": "SemEval-2017 Task 10 : Extracting Keyphrases and Relations from Scientific Publications", "author": ["Isabelle Augenstein", "Mrinal Das", "Sebastian Riedel", "Lakshmi Vikraman", "Andrew McCallum."], "venue": "Proceedings of SemEval, to appear.", "citeRegEx": "Augenstein et al\\.,? 2017", "shortCiteRegEx": "Augenstein et al\\.", "year": 2017}, {"title": "A model of inductive bias learning", "author": ["Jonathan Baxter."], "venue": "Journal of Artificial Intelligence Research 12:149\u2013198.", "citeRegEx": "Baxter.,? 2000", "shortCiteRegEx": "Baxter.", "year": 2000}, {"title": "Improving historical spelling normalization with bi-directional LSTMs and multi-task learning", "author": ["Marcel Bollman andAnders S\u00f8gaard."], "venue": "Proceedings of COLING.", "citeRegEx": "S\u00f8gaard.,? 2016", "shortCiteRegEx": "S\u00f8gaard.", "year": 2016}, {"title": "Multitask Learning: A Knowledge-Based Source of Inductive Bias", "author": ["Rich Caruana."], "venue": "Proceedings of ICML.", "citeRegEx": "Caruana.,? 1993", "shortCiteRegEx": "Caruana.", "year": 1993}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011b", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Frame-semantic parsing", "author": ["Dipanjan Das", "Desai Chen", "Andre Martins", "Nathan Schneider", "Noah Smith."], "venue": "Computational linguistics 40(1):9\u201356.", "citeRegEx": "Das et al\\.,? 2014", "shortCiteRegEx": "Das et al\\.", "year": 2014}, {"title": "Multi-Task Learning for Multiple Language Translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of ACL.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Jenny Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of ACL.", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Framewise Phoneme Classification with Bidirectional LSTM and other Neural Network Architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Automatic Keyphrase Extraction: A Survey of the State of the Art", "author": ["Kazi Saidul Hasan", "Vincent Ng."], "venue": "Proceedings of ACL.", "citeRegEx": "Hasan and Ng.,? 2014", "shortCiteRegEx": "Hasan and Ng.", "year": 2014}, {"title": "More or less supervised supersense tagging of Twitter", "author": ["Anders Johannsen", "Dirk Hovy", "H\u00e9ctor Mart\u0131\u0301nez", "Barbara Plank", "Anders S\u00f8gaard"], "venue": "In Proceedings of *SEM", "citeRegEx": "Johannsen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Johannsen et al\\.", "year": 2014}, {"title": "SemEval-2010 Task 5 : Automatic Keyphrase Extraction from Scientific Articles", "author": ["Su Nam Kim", "Olena Medelyan", "Min-Yen Kan", "Timothy Baldwin."], "venue": "Proceedings of SemEval.", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Neural Architectures for Named Entity Recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "Proceedings of NAACL-HLT. pages 260\u2013270.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Finegrained Opinion Mining with Recurrent Neural Networks and Word Embeddings", "author": ["Pengfei Liu", "Shafiq Joty", "Helen Meng."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Deep Multi-Task Learning with Shared Memory for Text Classification", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Multi-task Sequence to Sequence Learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "Proceedings of ICLR.", "citeRegEx": "Luong et al\\.,? 2016", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "When is multitask learning effective? Semantic sequence prediction under varying data conditions", "author": ["H\u00e9ctor Mart\u0131\u0301nez Alonso", "Barbara Plank"], "venue": "In Proceedings of EACL", "citeRegEx": "Alonso and Plank.,? \\Q2017\\E", "shortCiteRegEx": "Alonso and Plank.", "year": 2017}, {"title": "Bounds for Linear Multi Task Learning", "author": ["Andreas Maurer."], "venue": "Journal of Machine Learning Research 7:117\u2013139.", "citeRegEx": "Maurer.,? 2007", "shortCiteRegEx": "Maurer.", "year": 2007}, {"title": "The ACL RD-TEC 2.0: A Language Resource for Evaluating Term Extraction and Entity Recognition Methods", "author": ["Behrang QasemiZadeh", "Anne-Kathrin Schumann"], "venue": "In Proceedings of LREC", "citeRegEx": "QasemiZadeh and Schumann.,? \\Q2016\\E", "shortCiteRegEx": "QasemiZadeh and Schumann.", "year": 2016}, {"title": "Semi-supervisedMultitask Learning for Sequence Labeling", "author": ["Marek Rei."], "venue": "Proceedings of ACL, to appear.", "citeRegEx": "Rei.,? 2017", "shortCiteRegEx": "Rei.", "year": 2017}, {"title": "A Corpus and Model Integrating Multiword Expressions and Supersenses", "author": ["Nathan Schneider", "Noah Smith."], "venue": "Proceedings of NAACL-HLT .", "citeRegEx": "Schneider and Smith.,? 2015", "shortCiteRegEx": "Schneider and Smith.", "year": 2015}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of ACL.", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing", "author": ["Valentin Spitkovsky", "Daniel Jurafsky", "Hiyan Alshawi."], "venue": "Proceedings of ACL.", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Supervised Keyphrase Extraction as Positive Unlabeled Learning", "author": ["Lucas Sterckx", "Cornelia Caragea", "Thomas Demeester", "Chris Develder."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Sterckx et al\\.,? 2016", "shortCiteRegEx": "Sterckx et al\\.", "year": 2016}, {"title": "Online Segment to Segment Neural Transduction", "author": ["Lei Yu", "Jan Buys", "Phil Blunsom."], "venue": "Proceedings of EMNLP.", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 18, "context": "\u22c6Both authors contributed equally available recently (QasemiZadeh and Schumann, 2016; Augenstein et al., 2017).", "startOffset": 53, "endOffset": 110}, {"referenceID": 0, "context": "\u22c6Both authors contributed equally available recently (QasemiZadeh and Schumann, 2016; Augenstein et al., 2017).", "startOffset": 53, "endOffset": 110}, {"referenceID": 9, "context": "Typical KBC approaches therefore rely on hand-crafted gazetteers (Hasan and Ng, 2014) or reduce the task to extracting a list of keyphrases for each document (Kim et al.", "startOffset": 65, "endOffset": 85}, {"referenceID": 11, "context": "Typical KBC approaches therefore rely on hand-crafted gazetteers (Hasan and Ng, 2014) or reduce the task to extracting a list of keyphrases for each document (Kim et al., 2010) instead of identifying mentions of keyphrases in sentences.", "startOffset": 158, "endOffset": 176}, {"referenceID": 12, "context": "For related more common NLP tasks such as named entity recognition and identification of multi-word expressions, neural sequence labelling methods have been shown to be useful (Lample et al., 2016).", "startOffset": 176, "endOffset": 197}, {"referenceID": 1, "context": "This approach to multi-task learning has three advantages: a) It significantly reduces Rademacher complexity (Baxter, 2000; Maurer, 2007), i.", "startOffset": 109, "endOffset": 137}, {"referenceID": 17, "context": "This approach to multi-task learning has three advantages: a) It significantly reduces Rademacher complexity (Baxter, 2000; Maurer, 2007), i.", "startOffset": 109, "endOffset": 137}, {"referenceID": 1, "context": "be cast as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007).", "startOffset": 136, "endOffset": 164}, {"referenceID": 17, "context": "be cast as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007).", "startOffset": 136, "endOffset": 164}, {"referenceID": 1, "context": "be cast as a regulariser as studies show reductions in Rademacher complexity in multi-task architectures over single-task architectures (Baxter, 2000; Maurer, 2007). Here, we follow the probably most common approach to multi-task learning, known as hard parameter sharing. This was introduced in Caruana (1993) in the context of deep neural networks, in which hidden layers can be shared among tasks.", "startOffset": 137, "endOffset": 311}, {"referenceID": 20, "context": "(2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al.", "startOffset": 79, "endOffset": 106}, {"referenceID": 2, "context": "Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.", "startOffset": 150, "endOffset": 178}, {"referenceID": 2, "context": "Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al.", "startOffset": 150, "endOffset": 316}, {"referenceID": 2, "context": "Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al.", "startOffset": 150, "endOffset": 391}, {"referenceID": 2, "context": "Auxiliary tasks We experiment with five auxiliary tasks: (1) syntactic chunking using annotations extracted from the English Penn Treebank, following S\u00f8gaard and Goldberg (2016); (2) frame target annotations from FrameNet 1.5 (corresponding to the target identification and classification tasks in Das et al. (2014)); (3) hyperlink prediction using the dataset from Spitkovsky et al. (2010), (4) identification of multi-word expressions using the Streusle corpus (Schneider and Smith, 2015); and (5) semantic super-sense tagging using the Semcor dataset, following Johannsen et al. (2014). We train our models on the main task with one auxiliary task", "startOffset": 150, "endOffset": 589}, {"referenceID": 0, "context": "Datasets We evaluate on the SemEval 2017 Task 10 dataset (Augenstein et al., 2017) and the the ACL RD-TEC 2.", "startOffset": 57, "endOffset": 82}, {"referenceID": 18, "context": "0 dataset (QasemiZadeh and Schumann, 2016).", "startOffset": 10, "endOffset": 42}, {"referenceID": 8, "context": "Models Our single- and multi-task networks are three-layer, bi-directional LSTMs (Graves and Schmidhuber, 2005) with pre-trained SENNA embeddings.", "startOffset": 81, "endOffset": 111}, {"referenceID": 2, "context": "The dimensionality of the embeddings is 50, and we follow S\u00f8gaard and Goldberg (2016) in using the same dimensionality for the hidden layers.", "startOffset": 58, "endOffset": 86}, {"referenceID": 7, "context": "Baselines Our baselines are Finkel et al. (2005) and Lample et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 7, "context": "Baselines Our baselines are Finkel et al. (2005) and Lample et al. (2016), in order to compare to a lexicalised and a state-of-the-art neural method.", "startOffset": 28, "endOffset": 74}, {"referenceID": 7, "context": "The lexicalised Finkel et al. (2005) model shows a surprisingly competitive performance on the ACL RD-TEC corpus, where it is only 2 points in F1 behind our best performing labelled model and on par with our best-performing unlabelled model.", "startOffset": 16, "endOffset": 37}, {"referenceID": 7, "context": "The lexicalised Finkel et al. (2005) model shows a surprisingly competitive performance on the ACL RD-TEC corpus, where it is only 2 points in F1 behind our best performing labelled model and on par with our best-performing unlabelled model. Results with Lample et al. (2016), on the other hand, are lower than the Finkel et al.", "startOffset": 16, "endOffset": 276}, {"referenceID": 7, "context": "The lexicalised Finkel et al. (2005) model shows a surprisingly competitive performance on the ACL RD-TEC corpus, where it is only 2 points in F1 behind our best performing labelled model and on par with our best-performing unlabelled model. Results with Lample et al. (2016), on the other hand, are lower than the Finkel et al. (2005) baseline.", "startOffset": 16, "endOffset": 336}, {"referenceID": 3, "context": "Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al.", "startOffset": 72, "endOffset": 87}, {"referenceID": 3, "context": "Multi-Task Learning Hard sharing of all hidden layers was introduced in Caruana (1993), and popularised in NLP by Collobert et al. (2011a). Several variants have been introduced, including hard sharing of selected", "startOffset": 72, "endOffset": 139}, {"referenceID": 21, "context": "layers (S\u00f8gaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al.", "startOffset": 7, "endOffset": 35}, {"referenceID": 13, "context": "layers (S\u00f8gaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015).", "startOffset": 79, "endOffset": 97}, {"referenceID": 2, "context": "layers (S\u00f8gaard and Goldberg, 2016) and sharing of parts (subspaces) of layers (Liu et al., 2015). S\u00f8gaard and Goldberg (2016) show that hard parameter sharing is an effective regulariser, also on heterogeneous tasks such as the ones considered here.", "startOffset": 8, "endOffset": 127}, {"referenceID": 21, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al.", "startOffset": 22, "endOffset": 50}, {"referenceID": 6, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017).", "startOffset": 127, "endOffset": 166}, {"referenceID": 15, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017).", "startOffset": 127, "endOffset": 166}, {"referenceID": 14, "context": "Sharing of information can further be achieved by extending LSTMs with an external memory shared across tasks (Liu et al., 2016).", "startOffset": 110, "endOffset": 128}, {"referenceID": 2, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017). Sharing of information can further be achieved by extending LSTMs with an external memory shared across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and auto-encoding, and in Rei (2017) for different sequence labelling tasks and language modelling.", "startOffset": 23, "endOffset": 525}, {"referenceID": 2, "context": "ing CCG super tagging (S\u00f8gaard and Goldberg, 2016), text normalisation (Bollman and S\u00f8gaard, 2016), neural machine translation (Dong et al., 2015; Luong et al., 2016), and super-sense tagging (Mart\u0131\u0301nez Alonso and Plank, 2017). Sharing of information can further be achieved by extending LSTMs with an external memory shared across tasks (Liu et al., 2016). A further instance of multi-task learning is to optimise a supervised training objective jointly with an unsupervised training objective, as shown in Yu et al. (2016) for natural language generation and auto-encoding, and in Rei (2017) for different sequence labelling tasks and language modelling.", "startOffset": 23, "endOffset": 594}, {"referenceID": 4, "context": "Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al.", "startOffset": 49, "endOffset": 74}, {"referenceID": 4, "context": "Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step.", "startOffset": 49, "endOffset": 96}, {"referenceID": 4, "context": "Deep neural networks have been applied to NER in Collobert et al. (2011b); Lample et al. (2016). Other successful methods rely on conditional random fields, thereby modelling the probability of each output label conditioned on the label at the previous time step. Lample et al. (2016),", "startOffset": 49, "endOffset": 285}, {"referenceID": 9, "context": "Keyphrase detection methods specific to the scientific domain often use keyphrase gazetteers as features or exploit citation graphs (Hasan and Ng, 2014).", "startOffset": 132, "endOffset": 152}, {"referenceID": 11, "context": "for mention-level identification (Kim et al., 2010; Sterckx et al., 2016).", "startOffset": 33, "endOffset": 73}, {"referenceID": 23, "context": "for mention-level identification (Kim et al., 2010; Sterckx et al., 2016).", "startOffset": 33, "endOffset": 73}], "year": 2017, "abstractText": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.", "creator": "LaTeX with hyperref package"}}}