{"id": "1704.00708", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2017", "title": "No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis", "abstract": "In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.", "histories": [["v1", "Mon, 3 Apr 2017 17:49:02 GMT  (50kb)", "http://arxiv.org/abs/1704.00708v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.OC stat.ML", "authors": ["rong ge 0001", "chi jin", "yi zheng"], "accepted": true, "id": "1704.00708"}, "pdf": {"name": "1704.00708.pdf", "metadata": {"source": "CRF", "title": "No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis", "authors": ["Rong Ge", "Chi Jin", "Yi Zheng"], "emails": ["rongge@cs.duke.edu", "chijin@cs.berkeley.edu", "sheng.zheng@duke.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n00 70\n8v 1\n[ cs\n.L G\n] 3\nA pr\n2 01\nIn this paper we develop a new framework that captures the common landscape underlying the common nonconvex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no highorder saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA."}, {"heading": "1 Introduction", "text": "Non-convex optimization is one of the most powerful tools in machine learning. Many popular approaches, from traditional ones such as matrix factorization [Hotelling, 1933] to modern deep learning [Bengio, 2009] rely on optimizing non-convex functions. In practice, these functions are optimized using simple algorithms such as alternating minimization or gradient descent. Why such simple algorithms work is still a mystery for many important problems.\nOne way to understand the success of non-convex optimization is to study the optimization landscape: for the objective function, where are the possible locations of global optima, local optima and saddle points. Recently, a line of works showed that several natural problems including tensor decomposition [Ge et al., 2015], dictionary learning [Sun et al., 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al. [2016], Jin et al. [2017]) that are guaranteed to find a local minimum for many non-convex functions, such problems can be efficiently solved by basic optimization algorithms such as stochastic gradient descent.\nIn this paper we focus on optimization problems that look for low rank matrices using partial or corrupted observations. Such problems are studied extensively [Fazel, 2002, Rennie and Srebro, 2005, Cande\u0300s and Recht, 2009] and has many applications in recommendation systems [Koren, 2009], see survey by Davenport and Romberg [2016]. These optimization problems can be formalized as follows:\nmin M\u2208Rd1\u00d7d2 f(M), (1)\ns.t. rank(M) = r.\nHere M is an d1 \u00d7 d2 matrix and f is a convex function of M. The non-convexity of this problem stems from the low rank constraint. Several interesting problems, such as matrix sensing [Recht et al., 2010], matrix completion [Cande\u0300s and Recht, 2009] and robust PCA [Cande\u0300s et al., 2011] can all be framed as optimization problems of this form(see Section 3).\n\u2217Duke University. Email: rongge@cs.duke.edu \u2020University of California, Berkeley. Email: chijin@cs.berkeley.edu \u2021Duke University. Email: sheng.zheng@duke.edu\nIn practice, Burer and Monteiro [2003] heuristic is often used \u2013 replaceM with an explicit low rank representation\nM = UV\u22a4, whereU \u2208 Rd1\u00d7r andV \u2208 Rd2\u00d7r. The new optimization problem becomes\nmin U\u2208Rd1\u00d7r,V\u2208Rd2\u00d7r\nf(UV\u22a4) +Q(U,V). (2)\nHere Q(U,V) is a (optional) regularizer. Despite the objective being non-convex, for all the problems mentioned above, simple iterative updates from random or even arbitrary initial point find the optimal solution in practice. It is then natural to ask: Can we characterize the similarities between the optimization landscape of these problems? We show this is indeed possible:\nTheorem 1 (informal). The objective function of matrix sensing, matrix completion and robust PCA have similar optimization landscape. In particular, for all these problems, 1) all local minima are also globally optimal; 2) any saddle point has at least one strictly negative eigenvalue in its Hessian.\nMore precise theorem statements appear in Section 3. Note that there were several cases (matrix sensing [Bhojanapalli et al.,\n2016, Park et al., 2016], symmetric matrix completion [Ge et al., 2016]) where similar results on the optimization landscape were known. However the techniques in previous works are tailored to the specific problems and hard to generalize. Our framework captures and simplifies all these previous results, and also gives new results on asymmetric matrix completion and robust PCA.\nThe key observation in our analysis is that for matrix sensing, matrix completion, and robust PCA (when fixing sparse estimate), function f (in Equation (1)) is a quadratic function over the matrix M. Hence the Hessian H of f with respect to M is a constant. More importantly, the Hessian H in all above problems has similar properties (that it approximately preserves norm, similar to the RIP properties used in matrix sensing [Recht et al., 2010]), which allows their optimization landscapes to be characterized in a unified way. Specifically, our framework gives principled way of defining a direction of improvement for all points that are not globally optimal.\nAnother crucial property of our framework is the interaction between the regularizer and the HessianH. Intuitively, the regularizer makes sure the solution is in a nice region B (e.g. set of incoherent matrices for matrix completion), and only within B the Hessian has the norm preserving property. On the other hand, regularizer should not be too large to severely distort the landscape. This interaction is crucial for matrix completion, and is also very useful in handling noise and perturbations. In Section 4, we discuss ideas required to apply this framework to matrix sensing, matrix completion and robust PCA.\nUsing this framework, we also give a way to reduce asymmetric matrix problems to symmetric PSD problems (where the desired matrix is of the formUU\u22a4). See Section 5 for more details. In addition to the results of no spurious local minima, our framework also implies that any saddle point has at least one strictly negative eigenvalue in its Hessian. Formally, we proved all above problems satisfy a robust version of this claim \u2014 strict saddle property (see Definition 2), which is one of crucial sufficient conditions to admit efficient optimization algorithms, and thus following corollary (see Section 6 for more details).\nCorollary 2 (informal). For matrix sensing, matrix completion and robust PCA, simple local search algorithms can find the desired low rank matrixUV\u22a4 = M\u22c6 from an arbitrary starting point in polynomial time with high probability.\nFor simplicity, we present most results in the noiseless setting, but our results can also be generalized to handle\nnoise. As an example, we show how to do this for matrix sensing in Section C."}, {"heading": "1.1 Related Works", "text": "The landscape of low rank matrix problems have recently received a lot of attention. Ge et al. [2016] showed symmetric matrix completion has no spurious local minimum. At the same time, Bhojanapalli et al. [2016] proved similar result for symmetric matrix sensing. Park et al. [2016] extended the matrix sensing result to asymmetric case. All of these works guarantee global convergence to the correct solution.\nThere has been a lot of work on the local convergence analysis for various algorithms and problems. For matrix sensing or matrix completion, the works [Keshavan et al., 2010a,b, Hardt and Wootters, 2014, Hardt, 2014, Jain et al., 2013, Chen and Wainwright, 2015, Sun and Luo, 2015, Zhao et al., 2015, Zheng and Lafferty, 2016, Tu et al., 2015]\nshowed that given a good enough initialization, many simple local search algorithms, including gradient descent and alternating least squares, succeed. Particularly, several works (e.g. Sun and Luo [2015], Zheng and Lafferty [2016]) accomplished this by showing a geometric property which is very similar to strong convexity holds in the neighborhood of optimal solution. For robust PCA, there are also many analysis for local convergence [Lin et al., 2010, Netrapalli et al., 2014, Yi et al., 2016, Zhang et al., 2017].\nSeveral works also try to unify the analysis for similar problems. Bhojanapalli et al. [2015] gave a framework for local analysis for these low rank problems. Belkin et al. [2014] showed a framework of learning basis functions, which generalizes tensor decompositions. Their techniques imply the optimization landscape for all such problems are very similar. For problems looking for a symmetric PSD matrix, Li and Tang [2016] showed for objective similar to (2) (but in the symmetric setting), restricted smoothness/strong convexity on the function f suffices for local analysis. However, their framework does not address the interaction between regularizer and the function f , hence cannot be directly applied to problems such as matrix completion or robust PCA.\nOrganization We will first introduce notations and basic optimality conditions in Section 2. Then Section 3 introduces the problems and our results. For simplicity, we present our framework for the symmetric case in Section 4, and briefly discuss how to reduce asymmetric problem to symmetric problem in Section 5. We discuss how our geometric result implies efficient algorithms in Section 6. We then show how our geometric results imply fast runtime of popular local search algorithms in Section 6. For clean presentation, many proofs are deferred to appendix ."}, {"heading": "2 Preliminaries", "text": "In this section we introduce notations and basic optimality conditions."}, {"heading": "2.1 Notations", "text": "We use bold letters for matrices and vectors. For a vector v we use \u2016v\u2016 to denote its \u21132 norm. For a matrix M we use \u2016M\u2016 to denote its spectral norm, and \u2016M\u2016F to denote its Frobenius norm. For vectors we use \u3008u,v\u3009 to denote inner-product, and for matrices we use \u3008M,N\u3009 = \u2211i,j MijNij to denote the trace of MN\u22a4. We will always use M\u22c6 to denote the optimal low rank solution. Further, we use \u03c3\u22c61 to denote its largest singular value, \u03c3 \u22c6 r to denote its r-th singular value and \u03ba\u22c6 = \u03c3\u22c61/\u03c3 \u22c6 r be the condition number.\nWe use \u2207f to denote the gradient and \u22072f to denote its Hessian. Since function f can often be applied to both M (as in (1)) and U,V (as in (2)), we use \u2207f(M) to denote gradient with respect to M and \u2207f(U,V) to denote gradient with respect to U,V. Similar notation is used for Hessian. The Hessian \u22072f(M) is a crucial object in our framework. It can be interpreted as a linear operator on matrices. This linear operator can be viewed as a d1d2 \u00d7 d1d2 matrix (or (\nd+1 2\n) \u00d7 ( d+1 2 ) matrix in the symmetric case) that applies to the vectorized version of matrices. We use\nthe notation M : H : N to denote the quadratic form \u3008M,H(N)\u3009. Similarly, the Hessian of objective (2) is a linear operator on a pair of matricesU,V, which we usually denote as\u22072f(U,V)."}, {"heading": "2.2 Optimality Conditions", "text": "Local Optimality Suppose we are optimizing a function f(x) with no constraints on x. In order for a point x to be a local minimum, it must satisfy the first and second order necessary conditions. That is, we must have \u2207f(x) = 0 and\u22072f(x) 0.\nDefinition 1 (Optimality Condition). Suppose x is a local minimum of f(x), then we have\n\u2207f(x) = 0, \u22072f(x) 0.\nIntuitively, if one of these conditions is violated, then it is possible to find a direction that decreases the function value. Ge et al. [2015] characterized the following strict-saddle property, which is a quantitative version of the optimality conditions, and can lead to efficient algorithms to find local minima.\nDefinition 2. We say function f(\u00b7) is (\u03b8, \u03b3, \u03b6)-strict saddle. That is, for any x, at least one of followings holds:\n1. \u2016\u2207f(x)\u2016 \u2265 \u03b8. 2. \u03bbmin(\u22072f(x)) \u2264 \u2212\u03b3. 3. x is \u03b6-close to X \u22c6 \u2013 the set of local minima.\nIntuitively, this definition says for any point x, it either violates one of the optimality conditions significantly (first two cases), or is close to a local minima. Note that \u03b6 and \u03b8 are often closely related. For a function with strict-saddle property, it is possible to efficiently find a point near a local minimum.\nLocal vs. Global However, of course finding a local minimum is not sufficient in many case. In this paper we are also going to prove that all local minima are also globally optimal, and they correspond to the desired solutions."}, {"heading": "3 Low Rank Problems and Our Results", "text": "In this section we introduce matrix sensing, matrix completion and robust PCA. For each problem we give the results obtained by our framework. The proof ideas are illustrated later in Sections 4 and 5."}, {"heading": "3.1 Matrix Sensing", "text": "Matrix sensing [Recht et al., 2010] is a generalization of compressed sensing [Candes et al., 2006]. In the matrix sensing problem, there is an unknown low rank matrix M\u22c6 \u2208 Rd1\u00d7d2 . We make linear observations on this matrix: let A1,A2, ...,Am \u2208 Rd1\u00d7d2 be m sensing matrices, the algorithm is given {Ai}\u2019s and the corresponding bi = \u3008Ai,M\u22c6\u3009. The goal is now to find the unknown matrix M\u22c6. In order to find M\u22c6, we need to solve the following nonconvex optimization problem\nmin M\u2208Rd1\u00d7d2 ,rank(M)=r\nf(M) = 1\n2m\nm \u2211\ni=1\n(\u3008M,Ai\u3009 \u2212 bi)2.\nWe can transform this constraint problem to an unconstraint problem by expressing M as M = UV\u22a4 where U \u2208 Rd1\u00d7r andV \u2208 Rd2\u00d7r. We also need an additional regularizer (common for all asymmetric problems):\nmin U,V\n1\n2m\nm \u2211\ni=1\n(\u3008UV\u22a4,Ai\u3009 \u2212 bi)2 + 1 8 \u2016U\u22a4U\u2212V\u22a4V\u20162F . (3)\nThe regularizer has been widely used in previous works [Zheng and Lafferty, 2016, Park et al., 2016]. In Section 5 we show how this regularizer can be viewed as a way to deal with the additional invariants in asymmetric case, and reduce the asymmetric case to the symmetric case. A crucial concept in standard sensing literature is Restrict Isometry Property (RIP), which is defined as follows:\nDefinition 3. A group of sensing matrices {A1, ..,Am} satisfies the (r, \u03b4)-RIP condition, if for every matrix M of rank at most r,\n(1 \u2212 \u03b4)\u2016M\u20162F \u2264 1\nm\nm \u2211\ni=1\n\u3008Ai,M\u30092 \u2264 (1 + \u03b4)\u2016M\u20162F .\nIntuitively, RIP says operator 1 m \u2211m i=1\u3008Ai, \u00b7\u30092 approximately perserve norms for all low rank matrices. When the sensing matrices are chosen to be i.i.d. matrices with independent Gaussian entries, if m \u2265 c(d1 + d2)r for large enough constant c, the sensing matrices satisfy the (2r, 120 )-RIP condition [Candes and Plan, 2011]. Using our framework we can show:\nTheorem 3. When measurements {Ai} satisfy (2r, 120 )-RIP, for matrix sensing objective (3) we have 1) all local minima satisfy UV\u22a4 = M\u22c6 2) the function is (\u01eb,\u2126(\u03c3\u22c6r ), O(\n\u01eb \u03c3\u22c6r ))-strict saddle.\nThis in particular says 1) no spurious local minima existsl; 2) whenever at some point (U,V) so that the gradient is small and the Hessian does not have significant negative eigenvalue, then the distance to global optimal (see Definition 6 and Definition 7) is guaranteed to be small. Such a point can be found efficiently (see Section 6)."}, {"heading": "3.2 Matrix Completion", "text": "Matrix completion is a popular technique in recommendation systems and collaborative filtering [Koren, 2009, Rennie and Srebro, 2005]. In this problem, again we have an unknown low rank matrix M\u22c6. We observe each entry of the matrix M\u22c6 independently with probability p. Let \u2126 \u2282 [d1] \u00d7 [d2] be a set of observed entries. For any matrix M, we use M\u2126 to denote the matrix whose entries outside of \u2126 are set to 0. That is, [M\u2126]i,j = Mi,j if (i, j) \u2208 \u2126, and [M\u2126]i,j = 0 otherwise. We further use \u2016M\u2016\u2126 to denote \u2016M\u2126\u2016F . Matrix completion can be viewed as a special case of matrix sensing, where the sensing matrices only have one nonzero entry. However such matrices do not satisfy the RIP condition.\nIn order to solve matrix completion, we try to optimize the following:\nmin M\u2208Rd1\u00d7d2 ,rank(M)=r\n1\n2p \u2016M\u2212M\u22c6\u20162\u2126.\nA well-known problem in matrix completion is that when the true matrix M\u22c6 is very sparse, then we are very likely to observe only 0 entries, and has no chance to learn the other entries ofM\u22c6. To avoid this case, previous works have assumed following incoherence condition:\nDefinition 4. A rank r matrix M \u2208 Rd1\u00d7d2 is \u00b5-incoherent, if for the rank-r SVD XDY\u22a4 of M, we have for all i \u2208 [d1], j \u2208 [d2]\n\u2016e\u22a4i X\u2016 \u2264 \u221a \u00b5r/d2, \u2016e\u22a4j Y\u2016 \u2264 \u221a \u00b5r/d1.\nWe assume the unknown optimal low rank matrixM\u22c6 is \u00b5-incoherent. In the non-convex program,we try to make sure the decompositionUV\u22a4 is also incoherent by adding a regularizer\nQ(U,V) = \u03bb1\nd1 \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b11)4+ + \u03bb2 d2 \u2211\nj=1\n(\u2016e\u22a4j V\u2016 \u2212 \u03b12)4+.\nHere \u03bb1, \u03bb2, \u03b11, \u03b12 are parameters that we choose later, (x)+ = max{x, 0}. Using this regularizer, we can now transform the objective function to the unconstraint form\nmin U,V\n1\n2p \u2016UV\u22a4 \u2212M\u22c6\u20162\u2126 +\n1 8 \u2016U\u22a4U\u2212V\u22a4V\u20162F +Q(U,V). (4)\nUsing the framework, we can show following:\nTheorem 4. Let d = max{d1, d2}, when sample rate p \u2265 \u2126(\u00b5 4r6(\u03ba\u22c6)6 log d min{d1,d2} ), choose \u03b1 2 1 = \u0398( \u00b5r\u03c3\u22c6 1 d1 ), \u03b122 = \u0398( \u00b5r\u03c3\u22c6 1 d2 ) and \u03bb1 = \u0398( d1\n\u00b5r\u03ba\u22c6 ), \u03bb2 = \u0398( d2 \u00b5r\u03ba\u22c6 ). With probability at least 1\u2212 1/poly(d), for Objective Function (4) we have 1) all local minima satisfy UV\u22a4 = M\u22c6 2) The objective is (\u01eb,\u2126(\u03c3\u22c6r ), O(\n\u01eb \u03c3\u22c6r ))-strict saddle for polynomially small \u01eb."}, {"heading": "3.3 Robust PCA", "text": "Robust PCA [Cande\u0300s et al., 2011] is a generalization to the standard Principled Component Analysis. In Robust PCA, we are given an observation matrix Mo, which is an true underlying matrix M \u22c6 corrupted by a sparse noise S\u22c6 (Mo = M \u22c6 + S\u22c6). In some sense the goal is to decompose the matrixM into these two components. There are many models on how many entries can be perturbed, and how they are distributed. In this paper we work in the setting where M\u22c6 is \u00b5-incoherent, and the rows/columns of S\u22c6 can have at most \u03b1-fraction non-zero entries.\nIn order to express robust PCA as an optimization problem, we need constraints on bothM and S:\nmin 1\n2 \u2016M+ S\u2212Mo\u20162F . (5)\ns.t. rank(M) \u2264 r,S is sparse.\nThere can be several ways to specify the sparsity of S. In this paper we restrict attention to the following set:\nS\u03b1 = { S \u2208 Rd1\u00d7d2 | S has at most \u03b1-fraction non-zero entries each column/row, and \u2016S\u2016\u221e \u2264 2 \u00b5r\u03c3\u22c61\u221a d1d2 } .\nAssuming the true sparse matrix S\u22c6 is in S\u03b1. Note that the infinite norm requirement on S\u22c6 is without loss of generality, because by incoherence M\u22c6 cannot have entries with absolute value more than \u00b5r\u03c3\u22c6 1\u221a\nd1d2 . Any entry larger\nthan that is obviously in the support of S\u22c6 and can be truncated.\nIn objective function, we allow S to be \u03b3 times denser (in S\u03b3\u03b1) where \u03b3 is a parameter we choose later. Now the constraint optimization problem can be tranformed to the unconstraint problem\nmin U,V\nf(U,V) + 1\n8 \u2016U\u22a4U\u2212V\u22a4V\u20162F , (6)\nf(U,V) := min S\u2208S\u03b3\u03b1\n1 2 \u2016UV\u22a4 + S\u2212Mo\u20162F .\nOf course, we can also think of this as a joint minimization problem ofU,V,S. However we choose to present it this way in order to allow extension of the strict-saddle condition. Since f(U,V) is not twice-differetiable w.r.tU,V, it does not admit Hessian matrix, so we use the following generalized version of strict-saddle\nDefinition 5. We say function f(\u00b7) is (\u03b8, \u03b3, \u03b6)-pseudo strict saddle if for any x, at least one of followings holds:\n1. \u2016\u2207f(x)\u2016 \u2265 \u03b8.\n2. \u2203gx(\u00b7) so that \u2200y, gx(y) \u2265 f(y); gx(x) = f(x); \u03bbmin(\u22072gx(x)) \u2264 \u2212\u03b3.\n3. x is \u03b6-close to X \u22c6 \u2013 the set of local minima.\nNote that in this definition, the upperbound in 2 can be viewed as similar to the idea of subgradient. For functions with non-differentiable points, subgradient is defined so that it still offers a lowerbound for the function. In our case this is very similar \u2013 although Hessian is not defined, we can use a smooth function that upperbounds the current function (upper-bound is required for minimization). In the case of robust PCA the upperbound is obtained by a fixed S. Using this formalization we can prove\nTheorem 5. There is an absolute constant c > 0, if \u03b3 > c, and \u03b3\u03b1 \u00b7\u00b5r \u00b7 (\u03ba\u22c6)5 \u2264 1 c holds, for objective function Eq.(6) we have 1) all local minima satisfies UV\u22a4 = M\u22c6; 2) objective function is (\u01eb,\u2126(\u03c3\u22c6r ), O( \u01eb \u221a \u03ba\u22c6\n\u03c3\u22c6r ))-pseudo strict saddle\nfor polynomially small \u01eb."}, {"heading": "4 Framework for Symmetric Positive Definite Problems", "text": "In this section we describe our framework in the simpler setting where the desired matrix is positive semidefinite. In particular, suppose the true matrixM\u22c6 we are looking for can be written asM\u22c6 = U\u22c6(U\u22c6)\u22a4 whereU\u22c6 \u2208 Rd\u00d7r. For objective functions that is quadratic overM, we denote its Hessian asH and we can write the objective as\nmin M\u2208Rd\u00d7dsym ,rank(M)=r\n1 2 (M \u2212M\u22c6) : H : (M\u2212M\u22c6), (7)\nWe call this objective function f(M). Via Burer-Monteiro factorization, the corresponding unconstraint optimization problem, with regularizationQ can be written as\nmin U\u2208Rn\u00d7r\n1 2 (UU\u22a4 \u2212M\u22c6) : H : (UU\u22a4 \u2212M\u22c6) +Q(U). (8)\nIn this section, we also denote f(U) as objective function with respect to parameterU, abuse the notation of f(M) previously defined overM.\nDirection of Improvement The optimality condition (Definition 1) implies if the gradient is non-zero, or if we can find a negative direction of the Hessian (that is a direction v, so that v\u22a4\u22072f(x)v < 0), then the point is not a local minimum. A common technique in characterizing the optimization landscape is therefore trying to explicitly find this negative direction. We call this the direction of improvement. Different works [Bhojanapalli et al., 2016, Ge et al., 2016] have chosen very different directions of improvement.\nIn our framework, we show it suffices to choose a single direction\u2206 as the direction of improvement. Intuitively, this direction should bring us close to the true solution U\u22c6 from the current point U. Due to rotational symmetry (U and UR behave the same for the objective if R is a rotation matrix), we need to carefully define the difference betweenU andU\u22c6. Definition 6. Given matricesU,U\u22c6 \u2208 Rd\u00d7r, define their difference\u2206 = U \u2212U\u22c6R, where R \u2208 Rr\u00d7r is chosen as R = argminZ\u22a4Z=ZZ\u22a4=I \u2016U\u2212U\u22c6Z\u20162F .\nNote that this definition tries to \u201calign\u201d U and U\u22c6 before taking their difference, and therefore is invariant under rotations. In particular, this definition has the nice property that as long as M = UU\u22a4 is close to M\u22c6 = U\u22c6(U\u22c6)\u22a4, we have\u2206 is small (we defer the proof to Appendix): Lemma 6. Given matrices U,U\u22c6 \u2208 Rd\u00d7r, let M = UU\u22a4 and M\u22c6 = U\u22c6(U\u22c6)\u22a4, and let \u2206 be defined as in Definition 6, then we have \u2016\u2206\u2206\u22a4\u20162F \u2264 2\u2016M\u2212M\u22c6\u20162F , and \u03c3\u22c6r\u2016\u2206\u20162F \u2264 12(\u221a2\u22121)\u2016M\u2212M\n\u22c6\u20162F . Now we can state the main Lemma:\nLemma 7 (Main). For the objective (8), let\u2206 be defined as in Definition 6 andM = UU\u22a4. Then, for anyU \u2208 Rd\u00d7r, we have\n\u2206 : \u22072f(U) : \u2206 =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M\u2212M\u22c6) + 4\u3008\u2207f(U),\u2206\u3009+ [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009] (9)\nTo see why this lemma is useful, let us look at the simplest case where Q(U) = 0 and H is identity. In this case, if gradient is zero, by Eq. (9)\n\u2206 : \u22072f(U) : \u2206 = \u2016\u2206\u2206\u20162F \u2212 3\u2016M\u2212M\u22c6\u20162F By Lemma 6 this is no more than \u2212\u2016M \u2212M\u22c6\u20162F . Therefore, all stationary point with M 6= M\u2217 must be saddle\npoints, and we immediately conclude all local minimum satisfiesUU\u22a4 = M\u22c6!\nInteraction with Regularizer For problems such as matrix completion, the Hessian H does not preserve the norm for all low rank matrices. In these cases we need to use additional regularizer. In particular, conceptually we need the following steps:\n1. Show that the regularizerQ ensures for anyU such that\u2207f(U) = 0,U \u2208 B for some set B. 2. Show that whenever U \u2208 B, the Hessian operator H behaves similarly as identity: for some c > 0 we have:\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M \u2212M\u22c6) < \u2212c\u2016\u2206\u20162F . 3. Show that the regularizer does not contribute a large positive term to\u2206 : \u22072f(U) : \u2206. This means we show an\nupperbound for 4\u3008\u2207f(U),\u2206\u3009 + [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009]. Interestingly, these steps are not just useful for handling regularizers. Any deviation to the original model (such as noise, or if the optimal matrix is not exactly low rank) can be viewed as an additional \u201cregularizer\u201d functionQ(U) and argued in the same framework. See e.g. Section C."}, {"heading": "4.1 Matrix Sensing", "text": "Matrix sensing is the ideal setting for this framework. For symmetric matrix sensing, the objective function is\nmin U\u2208Rd\u00d7r\n1\n2m\nm \u2211\ni=1\n(\u3008Ai,UU\u22a4\u3009 \u2212 bi)2. (10)\nRecall that matrices {Ai : i = 1, 2, ...,m} are known sensing matrices, and bi = \u3008Ai,M\u22c6\u3009 is the result of i-th observation. The intended solution is the unknown low rank matrix M\u22c6 = U\u22c6(U\u22c6)\u22a4. For any low rank matrix M, the Hessian operator satisfies\nM : H : M = m \u2211\ni=1\n\u3008Ai,M\u30092.\nTherefore if the sensing matrices satisfy the RIP property (Definition 3), the Hessian operator is close to identity for all low rank matrices! In the symmetric case there is no regularizer, so the landscape for symmetric matrix sensing follows immediately from our main Lemma 7.\nTheorem 8. When measurement {Ai} satisfies (2r, 110 )-RIP, for matrix sensing objective (10) we have 1) all local minimaU satisfy UU\u22a4 = M\u22c6; 2) the function is (\u01eb,\u2126(\u03c3\u22c6r ), O(\n\u01eb \u03c3\u22c6r ))-strict saddle.\nProof. For pointU with small gradient satisfying \u2016\u2207f(U)\u2016F \u2264 \u01eb, by (2r, \u03b42r)-RIP property:\n\u2206 : \u22072f(U) : \u2206 =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M \u2212M\u22c6) + 4\u3008\u2207f(U),\u2206\u3009 \u2264(1 + \u03b42r)\u2016\u2206\u2206\u22a4\u20162F \u2212 3(1\u2212 \u03b42r)\u2016M\u2212M\u22c6\u20162F + 4\u01eb\u2016\u2206\u2016F \u2264\u2212 (1\u2212 5\u03b42r)\u2016M\u2212M\u22c6\u20162F + 4\u01eb\u2016\u2206\u2016F \u2264\u2212 0.4\u03c3\u22c6r\u2016\u2206\u20162F + 4\u01eb\u2016\u2206\u2016F\nThe second last inequality is due to Lemma 6 that \u2016\u2206\u2206\u22a4\u20162F \u2264 2\u2016M\u2212M\u22c6\u20162F, and last inequality is due to \u03b42r = 110 and second part of Lemma 6. This means if U is not close to U\u22c6, that is, if \u2016\u2206\u2016F \u2265 20\u01eb\u03c3\u22c6r , we have \u2206 : \u2207 2f(U) : \u2206 \u2264 \u22120.2\u03c3\u22c6r\u2016\u2206\u20162F. This proves (\u01eb, 0.2\u03c3\u22c6r , 20\u01eb\u03c3\u22c6r )-strict saddle property. Take \u01eb = 0, we know all stationary points with \u2016\u2206\u2016F 6= 0 are saddle points. This means all local minima are global minima (satisfyingUU\u22a4 = M\u22c6), which finishes the proof."}, {"heading": "4.2 Matrix Completion", "text": "For matrix completion, we need to ensure the incoherence condition (Definition 4). In order to do that, we add a regularizer Q(U) that penalize the objective function when some row of U is too large. We choose the same regularizer as Ge et al. [2016]: Q(U) = \u03bb \u2211d\ni=1(\u2016Ui\u2016 \u2212 \u03b1)4+. The objective is then\nmin U\u2208Rd\u00d7r\n1\n2p \u2016M\u22c6 \u2212UU\u22a4\u20162\u2126 +Q(U). (11)\nUsing our framework, we first need to show that the regularizer ensures all rows ofU are small (step 1).\nLemma 9. There exists an absolute constant c, when sample rate p \u2265 \u2126(\u00b5r d log d), \u03b12 = \u0398(\n\u00b5r\u03c3\u22c6 1\nd ) and \u03bb = \u0398( d \u00b5r\u03ba\u22c6 ),\nwe have for any pointsU with \u2016\u2207f(U)\u2016F \u2264 \u01eb for polynomially small \u01eb, with probability at least 1\u2212 1/poly(d):\nmax i\n\u2016e\u22a4i U\u20162 \u2264 O ( (\u00b5r)1.5\u03ba\u22c6\u03c3\u22c61 d )\nThis is a slightly stronger version of Lemma 4.7 in Ge et al. [2016]. Next we show under this regularizer, we can\nstill select the direction\u2206, and the first part of Equation (9) is significantly negative when\u2206 is large (step 2):\nLemma 10. When sample rate p \u2265 \u2126(\u00b5 3r4(\u03ba\u22c6)4 log d\nd ), by choosing \u03b12 = \u0398(\n\u00b5r\u03c3\u22c6 1\nd ) and \u03bb = \u0398( d \u00b5r\u03ba\u22c6 ) with probability\nat least 1\u2212 1/poly(d), for allU with \u2016\u2207f(U)\u2016F \u2264 \u01eb for polynomially small \u01eb we have\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M \u2212M\u22c6) \u2264 \u22120.3\u03c3\u22c6r\u2016\u2206\u20162F This lemma follows from several standard concentration inequalities, and is made possible because of the incoher-\nence bound we proved in the previous lemma.\nFinally we show the additional regularizer related term in Equation (9) is bounded (step 3).\nLemma 11. By choosing \u03b12 = \u0398( \u00b5r\u03c3\u22c6 1\nd ) and \u03bb\u03b12 \u2264 O(\u03c3\u22c6r ), we have:\n1 4 [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009] \u2264 0.1\u03c3\u22c6r\u2016\u2206\u20162F\nCombining these three lemmas, it is easy to see\nTheorem 12. When sample rate p \u2265 \u2126(\u00b5 3r4(\u03ba\u22c6)4 log d\nd ), by choosing \u03b12 = \u0398(\n\u00b5r\u03c3\u22c6 1\nd ) and \u03bb = \u0398( d \u00b5r\u03ba\u22c6 ). Then with\nprobability at least 1\u22121/poly(d), for matrix completion objective (11)we have 1) all local minima satisfyUU\u22a4 = M\u22c6 2) the function is (\u01eb,\u2126(\u03c3\u22c6r ), O(\n\u01eb \u03c3\u22c6r ))-strict saddle for polynomially small \u01eb.\nNotice that our proof is different from Ge et al. [2016], as we focus on the direction \u2206 for both first and second order conditions while they need to select different directions for the Hessian. The framework allowed us to get a simpler proof, generalize to asymmetric case and also improved the dependencies on rank."}, {"heading": "4.3 Robust PCA", "text": "In the robust PCA problem, for any given matrix M the objective function try to find the optimal sparse perturbation S. In the symmetric PSD case, recall we observeMo = M \u22c6 + S\u22c6, we define the set of sparse matrices to be\nS\u03b1 = { S \u2208 Rd\u00d7d | S has at most \u03b1-fraction non-zero entries each column/row, and \u2016S\u2016\u221e \u2264 2 \u00b5r\u03c3\u22c61 d } .\nNote the projection onto set S\u03b1 be computed in polynomial time (using a max flow algorithm). We assume S\u22c6 \u2208 S\u03b1, the objective can be written as\nmin U f(U), where f(U) := min S\u2208S\u03b3\u03b1\n1 2 \u2016UU\u22a4 + S\u2212Mo\u20162F . (12)\nHere \u03b3 is a slack parameter that we choose later. Note that now the objective function f(U) is not quadratic, so we cannot use the framework directly. However, if we fix S, then fS(U) := 1 2\u2016UU\u22a4 + S \u2212Mo\u20162F is a quadratic function with Hessian equal to identity. We can still apply our framework to this function. In this case, since the Hessian is identity for all matrices, we can skip the first step. The problem becomes a matrix factorization problem:\nmin U\u2208Rd\u00d7r\n1 2 \u2016A\u2212UU\u22a4\u20162F . (13)\nThe difference here is that the matrix A (which is M\u22c6 + S\u22c6 \u2212 S) is not equal to M\u22c6 and is in general not low rank. We can use the framework to analyze this problem (and treat the residueA\u2212M\u22c6 as the \u201cregularizer\u201dQ(U)).\nLemma 13. Let A \u2208 Rd\u00d7d be a symmetric PSD matrix, and matrix factorization objective to be:\nf(U) = \u2016UU\u22a4 \u2212A\u20162F where \u03c3r(A) \u2265 15\u03c3r+1(A). then 1) all local minima satisfies UU\u22a4 = Pr(A) (best rank-r approximation), 2) objective is (\u01eb,\u2126(\u03c3\u22c6r ), O(\n\u01eb \u03c3\u22c6r ))-strict saddle.\nTo deal with the case S not fixed (but as minimizer of Eq.(12)), we letU\u2020(U\u2020)\u22a4 be the best rank r-approximation ofM\u22c6 + S\u22c6 \u2212 S. The next lemma shows whenU is close toU\u2020 up to some rotation,U will actually be already close toU\u22c6 up to some rotation.\nLemma 14. There is an absolute constant c, assume \u03b3 > c, and \u03b3\u03b1 \u00b7\u00b5r \u00b7 (\u03ba\u22c6)5 \u2264 1 c . LetU\u2020(U\u2020)\u22a4 be the best rank rapproximation ofM\u22c6+S\u22c6\u2212S, where S is the minimizer as in Eq.(12). AssumeminR\u22a4R=RR\u22a4=I \u2016U\u2212U\u2020R\u2016F \u2264 \u01eb. Let \u2206 be defined as in Definition 6, then \u2016\u2206\u2016F \u2264 O(\u01eb \u221a \u03ba\u22c6) for polynomially small \u01eb.\nThe proof of Lemma 14 is inspired by Yi et al. [2016] and uses the property of the optimally chosen sparse set S.\nCombining these two lemmas we get our main result:\nTheorem 15. There is an absolute constant c, if \u03b3 > c, and \u03b3\u03b1 \u00b7 \u00b5r \u00b7 (\u03ba\u22c6)5 \u2264 1 c holds, for objective function Eq.(12) we have 1) all local minima satisfies UU\u22a4 = M\u22c6; 2) objective function is (\u01eb,\u2126(\u03c3\u22c6r ), O( \u01eb \u221a \u03ba\u22c6\n\u03c3\u22c6r ))-pseudo strict saddle\nfor polynomially small \u01eb."}, {"heading": "5 Handling Asymmetric Matrices", "text": "In this section we show how to reduce problems on asymmetric matrices to problems on symmetric PSD matrices.\nLetM\u22c6 = U\u22c6V\u22c6\u22a4, andM = UV\u22a4, and objective function:\nf(U,V) = 2(M\u2212M\u22c6) : H0 : (M \u2212M\u22c6) + 1 2 \u2016U\u22a4U\u2212V\u22a4V\u20162F +Q0(U,V)\nNote this is a scaled version of objectives introduced in Sec.3 (multiplied by 4), and scaling will not change the property of local minima, global minima and saddle points.\nWe view the problem as if it is trying to find a (d1 + d2) \u00d7 r matrix, whose first d1 rows are equal to U, and last d2 rows are equal toV.\nDefinition 7. SupposeM\u22c6 is the optimal solution, and its SVD isX\u22c6D\u22c6Y\u22c6\u22a4. LetU\u22c6 = X\u22c6(D\u22c6) 1 2 ,V\u22c6 = Y\u22c6(D\u22c6) 1 2 , M = UV\u22a4 is the current point, we reduce the problem into a symmetric case using following notations.\nW =\n(\nU V\n)\n,W\u22c6 =\n(\nU\u22c6 V\u22c6\n)\n,N = WW\u22a4,N\u22c6 = W\u22c6W\u22c6\u22a4 (14)\nFurther,\u2206 is defined to be the difference betweenW andW\u22c6 up to rotation as in Definition 6.\nWe will also transform the Hessian operators to operate on (d1 + d2) \u00d7 r matrices. In particular, define Hessian H1,G such that for all W we have:\nN : H1 : N = M : H0 : M N : G : N = \u2016U\u22a4U\u2212V\u22a4V\u20162F\nNow, let Q(W) = Q(U,V), and we can rewrite the objective function f(W) as\n1 2 [(N\u2212N\u22c6) : 4H1 : (N\u2212N\u22c6) +N : G : N] +Q(W) (15)\nWe knowH0 perserves the norm of low rank matricesM. To reduce asymmetric problems to symmetric problem, intuitively, we also hopeH0 to approximately preserve the norm ofN. However this is impossible as by definition,H0 only acts on M, which is the off-diagonal blocks of N. We can expect N : H0 : N to be close to the norm of UV\u22a4, but for all matrices U,V with the same UV\u22a4, the matrix N can have very different norms. The easiest example is to consider U = diag(1/\u01eb, \u01eb) and V = diag(\u01eb, 1/\u01eb): while UV\u22a4 = I no matter what \u01eb is, the norm of N is of order 1/\u01eb2 and can change drastically. The regularizer is exactly there to handle this case: the Hessian G of the regularizer will be related to the norm of the diagonal components, therefore allowing the full Hessian H = 4H1 + G to still be approximately identity.\nNow we can formalize the reduction as the following main Lemma:\nLemma 16. For the objective (15), let \u2206,N,N\u22c6 be defined as in Definition 7. Then, for any W \u2208 R(d1+d2)\u00d7r, we have\n\u2206 : \u22072f(W) : \u2206 \u2264\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(N\u2212N\u22c6) : H : (N\u2212N\u22c6) + 4\u3008\u2207f(W),\u2206\u3009+ [\u2206 : \u22072Q(W) : \u2206\u2212 4\u3008\u2207Q(W),\u2206\u3009] (16)\nwhere H = 4H1 + G. Further, if H0 satisfiesM : H0 : M \u2208 (1\u00b1 \u03b4)\u2016M\u20162F for some matrix M = UV\u22a4, let W and N be defined as in (14), thenN : H : N \u2208 (1\u00b1 2\u03b4)\u2016N\u20162F .\nIntuitively, this lemma shows the same direction of improvement works as before, and the regularizer is exactly\nwhat it requires to maintain the norm-preserving property of the Hessian.\nBelow we prove Theorem 3, which show for matrix sensing 1) all local minima satisfy M = M\u22c6; 2) strict saddle property is satisfied. Other proofs are deferred to appendix.\nProof of Theorem 3. In this case, M : H0 : M = 1m \u2211m i=1\u3008Ai,M\u30092 and regularization Q(W) = 0. Since H0 is (2r, 1/20)-RIP, by Lemma 16, we haveH = 4H1 + G satisfying (2r, 1/10)-RIP.\nSimilar to the symmetric case, for point W with small gradient satisfying \u2016\u2207f(W)\u2016F \u2264 \u01eb, by (2r, 1/10)-RIP property ofH (let \u03b4 = 1/10) we have\n\u2206 : \u22072f(U) : \u2206 =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(N\u2212N\u22c6) : H : (N\u2212N\u22c6) + 4\u3008\u2207f(U),\u2206\u3009 \u2264(1 + \u03b4)\u2016\u2206\u2206\u22a4\u20162F \u2212 3(1\u2212 \u03b4)\u2016N\u2212N\u22c6\u20162F + 4\u01eb\u2016\u2206\u2016F \u2264\u2212 (1\u2212 5\u03b4)\u2016N\u2212N\u22c6\u20162F + 4\u01eb\u2016\u2206\u2016F \u2264\u2212 0.4\u03c3\u22c6r\u2016\u2206\u20162F + 4\u01eb\u2016\u2206\u2016F\nThe second last inequality is due to Lemma 6 that \u2016\u2206\u2206\u22a4\u20162F \u2264 2\u2016N\u2212N\u22c6\u20162F, and last inequality is due to \u03b4 = 110 and second part of Lemma 6. This means if W is not close to W\u22c6, that is, if \u2016\u2206\u2016F \u2265 20\u01eb\u03c3\u22c6r , we have \u2206 : \u2207 2f(W) : \u2206 \u2264 \u22120.2\u03c3\u22c6r\u2016\u2206\u20162F. This proves (\u01eb, 0.2\u03c3\u22c6r , 20\u01eb\u03c3\u22c6r )-strict saddle property. Take \u01eb = 0, we know all stationary points with \u2016\u2206\u2016F 6= 0 are saddle points. This means all local minima satisfy WW\u22a4 = N\u22c6, which in particular implies UV\u22a4 = M\u22c6 becauseM\u22c6 is a submatrix ofN\u22c6."}, {"heading": "6 Runtime", "text": "In this section we give the precise statement of Corollary 2: the runtime of algorithms implied by the geometric properties we prove.\nIn order to translate the geometric result into runtime guarantees, many algorithms require additional smoothness\nconditions. We say a function f(x) is l-smooth if for all x,y,\n\u2016\u2207f(y)\u2212\u2207f(x)\u2016 \u2264 l\u2016y\u2212 x\u2016.\nThis is a standard assumption in optimization. In order to avoid saddle points, say a function f(x) is \u03c1-Hessian Lipschitz if for all x,y \u2016\u22072f(y) \u2212\u22072f(x)\u2016 \u2264 \u03c1\u2016y \u2212 x\u2016. We call an optimization algorithm saddle-avoiding if the algorithm is able to find a point with small gradient and almost positive semidefinite Hessian.\nDefinition 8. A local search algorithm is called saddle-avoiding, if for a function f : Rd \u2192 R that is l-smooth and \u03c1-Lipschitz Hessian, given a point x such that either \u2016\u2207f(x)\u2016 \u2265 \u01eb or \u03bbmin(\u22072f(x)) \u2264 \u2212\u221a\u03c1\u01eb, can find a point y in poly(1/\u01eb, d, l, \u03c1) iterations such that f(y) \u2264 f(x)\u2212 \u03b4 where \u03b4 = poly(d, l, \u03c1, \u01eb).\nAs a immediate corollary, we know such algorithms can find a pointx such that \u2016\u2207f(x)\u2016 \u2264 \u01eb and \u03bbmin(\u22072f(x)) \u2265 \u2212\u221a\u03c1\u01eb in poly(1/\u01eb, d, l, \u03c1) iterations.\nExisting results show many algorithms are saddle-avoiding, including cubic regularization [Nesterov and Polyak, 2006], stochastic gradient descent [Ge et al., 2015], trust-region algorithms [Sun et al., 2015b]. The most recent algorithms [Jin et al., 2017, Carmon et al., 2016, Agarwal et al., 2016] are more efficient: in particular the number of iterations only depend poly-logarithmic on dimension d. Now we are ready to formally state Corollary 2.\nCorollary 17. LetR be the Frobenius norm of the initial pointsU0,V0, a saddle-avoiding local search algorithm can find a point \u01eb-close to global optimal for matrix sensing (10)(3), matrix completion (11)(4) in poly(R, 1/\u01eb, d, \u03c3\u22c61 , 1/\u03c3 \u22c6 r ) iterations. For robust PCA (12)(6), alternating between a saddle-avoiding local search algorithm and computing optimal S \u2208 S\u03b3\u03b1 will find a point \u01eb-close to global optimal in poly(R, 1/\u01eb, d, \u03c3\u22c61 , 1/\u03c3\u22c6r ) iterations.\nThis corollary states the existence of simple local search algorithms which can efficiently optimizing non-convex objectives of matrix sensing, matrix completion and robust PCA in polynomial time. The proof essentially follows from the guarantees of the saddle-avoiding algorithm and the strict-saddle properties we prove. We will sketch the proof in Section D.\nTowards faster convergence For many low-rank matrices problems, in the neighborhood of local minima, objective function satisfies conditions similar to strong convexity [Zheng and Lafferty, 2016, Bhojanapalli et al., 2016] (more precisely, the (\u03b1, \u03b2)-regularity condition as Assumption A3.b in [Jin et al., 2017]). Jin et al. [2017] showed a principle way of how to combine these strong local structures with saddle-avoiding algorithm to give global linear convergence. Therefore, it is likely that some saddle-avoiding algorithms (such as perturbed gradient descent) can achieve linear convergence for these problems."}, {"heading": "7 Conclusions", "text": "In this paper we give a framework that explains the recent success in understanding optimization landscape for low rank matrix problems. Our framework connects and simplifies the existing proofs, and generalizes to new settings such as asymmetric matrix completion and robust PCA. The key observation is when the Hessian operator preserves the norm of certain matrices, one can use the same directions of improvement to prove similar optimization landscape. We show the regularizer 14\u2016U\u22a4U \u2212 V\u22a4V\u20162F is exactly what it requires to maintain this norm preserving property in the asymmetric case.Our analysis also allows the interaction between regularizer and Hessian to handle difficult settings such as.\nFor low rank matrix problems, there are generalizations such as weighted matrix factorization[Li et al., 2016] and 1-bit matrix sensing[Davenport et al., 2014] where the Hessian operator may behave differently as the settings we can analyze. How to characterize the optimization landscape in these settings is still an open problem.\nIn order to get general ways of understanding optimization landscapes for more generally, there are still many open problems. In particular, how can we decide whether two problems are similar enough to share the same optimization landscape? A minimum requirement is that the non-convex problem should have the same symmetry structure \u2013 the set of equivalent global optimum should be the same. In this work, we show if the problems come from convex objective functions with similar Hessian properties, then they have the same optimization landscape. We hope this serves as a first step towards general tools for understanding optimization landscape for groups of problems."}, {"heading": "A Proofs for Symmetric Positive Definite Problems", "text": "In this section we provide the missing proofs for the symmetric matrix problems. First we prove Lemma 6 which connects difference in the matrixU and the difference in the matrixM.\nLemma 6. Given matrices U,U\u22c6 \u2208 Rd\u00d7r, let M = UU\u22a4 and M\u22c6 = U\u22c6(U\u22c6)\u22a4, and let \u2206 be defined as in Definition 6, then we have \u2016\u2206\u2206\u22a4\u20162F \u2264 2\u2016M\u2212M\u22c6\u20162F , and \u03c3\u22c6r\u2016\u2206\u20162F \u2264 12(\u221a2\u22121)\u2016M\u2212M \u22c6\u20162F .\nProof. Recall in Definition 6,\u2206 = U\u2212U\u22c6RU where\nRU = argmin R\u22a4R=RR\u22a4=I\n\u2016U\u2212U\u22c6R\u20162F.\nWe first prove following claim, which will used in many places across this proof:\nU\u22a4U\u22c6RU is a symmetric PSD matrix. (17)\nThis because by expanding the Frobenius norm, and letting the SVD ofU\u22c6\u22a4U beADB\u22a4, we have:\nargmin R:RR\u22a4=R\u22a4R=I \u2016U\u2212U\u22c6R\u20162F = argmin R:RR\u22a4=R\u22a4R=I \u2212\u3008U,U\u22c6R\u3009\n= argmin R:RR\u22a4=R\u22a4R=I \u2212tr(U\u22a4U\u22c6R) = argmin R:RR\u22a4=R\u22a4R=I \u2212tr(DA\u22a4RB)\nSince A,B,R \u2208 Rr\u00d7r are all orthonormal matrix, we know A\u22a4RB is also orthonormal matrix. Moreover for any orthonormal matrix T, we have:\ntr(DT) = \u2211\ni\nDiiTii \u2264 \u2211\ni\nDii\nThe last inequality is becauseDii is singular value thus non-negative, andT is orthonormal, thusTii \u2264 1. This means the maximum of tr(DT) is achieved whenT = I, i.e., the minimum of \u2212tr(DA\u22a4RB) is achieved whenR = AB\u22a4. Therefore,U\u22a4U\u22c6RU = BDA\u22a4AB\u22a4 = BDB\u22a4 is symmetric PSD matrix.\nWith Eq.(17), the remaining of proof directly follows from the results by substituting (U,Y) in Lemma 40 and 41 with (U\u22c6RU,U).\nNow we are ready to prove the main lemma.\nLemma 7 (Main). For the objective (8), let\u2206 be defined as in Definition 6 andM = UU\u22a4. Then, for anyU \u2208 Rd\u00d7r, we have\n\u2206 : \u22072f(U) : \u2206 =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M\u2212M\u22c6) + 4\u3008\u2207f(U),\u2206\u3009+ [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009]\nProof. Recall the objective function is:\nf(U) = 1\n2 (UU\u22a4 \u2212M\u22c6) : H : (UU\u22a4 \u2212M\u22c6) +Q(U)\nand letM = UU\u22a4. Calculating gradient and Hessian, we have for any Z \u2208 Rd\u00d7r:\n\u3008\u2207f(U),Z\u3009 =(M \u2212M\u22c6) : H : (UZ\u22a4 + ZU\u22a4) + \u3008\u2207Q(U),Z\u3009 (18) Z : \u22072f(U) : Z =(UZ\u22a4 + ZU\u22a4) : H : (UZ\u22a4 + ZU\u22a4) + 2(M\u2212M\u22c6) : H : ZZ\u22a4 + Z : \u22072Q(U) : Z\nLet Z = \u2206 = U\u2212U\u22c6R as in Definition 6 and noteM\u2212M\u22c6 +\u2206\u2206\u22a4 = U\u2206\u22a4 +\u2206U\u22a4, then\n\u2206 : \u22072f(U) : \u2206 =(U\u2206\u22a4 +\u2206U\u22a4) : H : (U\u2206\u22a4 +\u2206U\u22a4) + 2(M\u2212M\u22c6) : H : \u2206\u2206\u22a4 +\u2206 : \u22072Q(U) : \u2206 =(M\u2212M\u22c6 +\u2206\u2206\u22a4) : H : (M\u2212M\u22c6 +\u2206\u2206\u22a4) + 2(M\u2212M\u22c6) : H : \u2206\u2206\u22a4 +\u2206 : \u22072Q(U) : \u2206 =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M\u2212M\u22c6) + 4(M\u2212M\u22c6) : H : (M\u2212M\u22c6 +\u2206\u2206\u22a4) + \u2206 : \u22072Q(U) : \u2206\n=\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M\u2212M\u22c6) + 4\u3008\u2207f(U),\u2206\u3009 + [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009]\nwhere in last line, we use the calculation of gradient\u2207f(U) in Eq.18. This finishes the proof.\nIn the subsequent subsections we will prove the guarantees for matrix completion and robust PCA. The proof of\nmatrix sensing is already given in Section 4.\nA.1 Matrix Completion\nFor matrix completion, the crucial component of the proof is the interaction between regularizer and the Hessian. We first state the properties (gradient and Hessian) of the regularizerQ here:\nLemma 18. The gradient and the hessian of regularizationQ(U) = \u03bb \u2211d i=1(\u2016e\u22a4i U\u2016 \u2212 \u03b1)4+ is:\n\u3008\u2207Q(U),Z\u3009 =4\u03bb d \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)3+ e\u22a4i UZ \u22a4ei \u2016eiU\u2016\n(19)\nZ : \u22072Q(U) : Z =4\u03bb d \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)3+ \u2016e\u22a4i U\u20162\u2016e\u22a4i Z\u20162 \u2212 (e\u22a4i UZ\u22a4ei)\n\u2016e\u22a4i U\u20163\n+ 12\u03bb\nd \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)2+ ( e\u22a4i UZ \u22a4ei\n\u2016eiU\u2016\n)2\n(20)\nProof. This Lemma follows from direct calculation using linear algebra and calculus.\nIn the first step of our framework, we hope to show that the regularizer forces the matrixU to not have large rows. This is formalized and proved below (the Lemma is similar to Lemma 4.7 in Ge et al. [2016], but we get a stronger guarantee here):\nLemma 9. There exists an absolute constant c, when sample rate p \u2265 \u2126(\u00b5r d log d), \u03b12 = \u0398(\n\u00b5r\u03c3\u22c6 1\nd ) and \u03bb = \u0398( d \u00b5r\u03ba\u22c6 ),\nwe have for any pointsU with \u2016\u2207f(U)\u2016F \u2264 \u01eb for polynomially small \u01eb, with probability at least 1\u2212 1/poly(d):\nmax i\n\u2016e\u22a4i U\u20162 \u2264 O ( (\u00b5r)1.5\u03ba\u22c6\u03c3\u22c61 d )\nProof. Recall the calculation of gradient:\n\u2207f(U) =2 p (M\u2212M\u22c6)\u2126U+\u2207Q(U)\nwhere by Lemma 18, the gradient of regularizer is:\n\u2207Q(U) = 4\u03bb d \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)3+ eie\n\u22a4 i U \u2016eiU\u20162\nLet i\u22c6 = argmaxi \u2016e\u22a4i U\u2016 be the row index with maximum 2-norm. If \u2016e\u22a4i\u22c6U\u2016 < 2\u03b1, by the choice of \u03b1 in Lemma 9 we immediately prove the lemma. In case of \u2016e\u22a4i\u22c6U\u2016 \u2265 2\u03b1, consider gradient along ei\u22c6e\u22a4i\u22c6U direction. Since \u2016\u2207f(U)\u2016F \u2264 \u01eb, we have \u3008\u2207f(U), ei\u22c6e\u22a4i\u22c6U\u3009 = \u3008e\u22a4i\u22c6\u2207f(U), e\u22a4i\u22c6U\u3009 \u2264 \u01eb\u2016e\u22a4i\u22c6U\u2016. Therefore, with 1\u22121/poly(d) probability, following holds:\n\u01eb\u2016e\u22a4i\u22c6U\u2016 \u2265\u3008e\u22a4i\u22c6\u2207f(U), e\u22a4i\u22c6U\u3009 = \u3008e\u22a4i\u22c6 [ 2 p (UU\u22a4 \u2212M\u22c6)\u2126U+\u2207Q(U)], e\u22a4i\u22c6U\u3009\n\u22654\u03bb(\u2016e\u22a4i\u22c6U\u2016 \u2212 \u03b1)3+\u2016e\u22a4i\u22c6U\u2016 \u2212 2 p \u3008e\u22a4i\u22c6(M\u22c6)\u2126, e\u22a4i\u22c6(UU\u22a4)\u2126\u3009 \u2265\u03bb 2 \u2016e\u22a4i\u22c6U\u20164 \u2212 2 1\u221a p \u2016e\u22a4i\u22c6(M\u22c6)\u2126\u2016 \u00b7 1\u221a p \u2016e\u22a4i\u22c6(UU\u22a4)\u2126\u2016\n\u2265\u03bb 2 \u2016e\u22a4i\u22c6U\u20164 \u2212 2\n\u221a 1 + 0.01\u2016e\u22a4i\u22c6M\u22c6\u2016 \u00b7O( \u221a d)\u2016UU\u22a4\u2016\u221e\n\u2265\u03bb 2 \u2016e\u22a4i\u22c6U\u20164 \u2212O( \u221a \u00b5r\u03c3\u22c61)\u2016e\u22a4i\u22c6U\u20162\nwhere second inequality use the fact \u3008e\u22a4i\u22c6(UU\u22a4)\u2126U, e\u22a4i\u22c6U\u3009 = \u2016e\u22a4i\u22c6(UU\u22a4)\u2126\u20162 \u2265 0; third inequality is by CauchySwartz; second last inequality is by Lemma 35 and our choice p \u2265 \u2126(\u00b5r\nd log d) with large enough constant, we have\n1\u221a p \u2016e\u22a4i\u22c6(M\u22c6)\u2126\u2016 \u2264 \u221a 1 + 0.01\u2016e\u22a4i\u22c6M\u22c6\u2016, and by Lemma 39 we have 1\u221ap\u2016e\u22a4i\u22c6(UU\u22a4)\u2126\u2016 \u2264 O( \u221a d)\u2016UU\u22a4\u2016\u221e; the last inequality is because \u2016e\u22a4i\u22c6M\u22c6\u2016 \u2264 \u221a \u00b5r d \u03c3\u22c61 asM\n\u22c6 is \u00b5-incoherent. Rearrange terms, we have:\n\u2016e\u22a4i\u22c6U\u20163 \u2264 O( \u221a \u00b5r\u03c3\u22c61 \u03bb )\u2016e\u22a4i\u22c6U\u2016+ 2\u01eb \u03bb\nBy choosing \u01eb small enough to satisfy ( \u01eb \u03bb )\n2 3 \u2264 \u221a \u00b5r\u00b7\u03c3\u22c6 1\n\u03bb , this gives:\nmax i\n\u2016e\u22a4i U\u20162 \u2264 c \u00b7max { \u03b12, \u221a \u00b5r \u00b7 \u03c3\u22c61 \u03bb }\nFinally, substituting our choice of \u03b12 and \u03bb, we finished the proof.\nIn the second step, we need to prove that the sum of Hessian H related terms in Equation (9) is significantly negative whenU andU\u22c6 are not close.\nLemma 10. When sample rate p \u2265 \u2126(\u00b5 3r4(\u03ba\u22c6)4 log d\nd ), by choosing \u03b12 = \u0398(\n\u00b5r\u03c3\u22c6 1\nd ) and \u03bb = \u0398( d \u00b5r\u03ba\u22c6 ) with probability\nat least 1\u2212 1/poly(d), for allU with \u2016\u2207f(U)\u2016F \u2264 \u01eb for polynomially small \u01eb, we have\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M \u2212M\u22c6) \u2264 \u22120.3\u03c3\u22c6r\u2016\u2206\u20162F Proof. The key problem here is for the matrix\u2206, it captures the difference betweenU andU\u22c6 and could have norms concentrate in very few columns. Note that when\u2206 is not incoherent, Hessian will still perserve norm for matrices like \u2206U\u22a4 Lemma 35, but not necessarily perserve norm for matrices like\u2206\u2206\u22a4. Therefore, we use different concentration lemmas in different regimes (divided according to whether\u2206 is small or large).\nFirst by our choice of \u03b1, \u03bb and Lemma 9, we know with 1 \u2212 1/poly(d) probabilty, the maximum 2-norm of any row ofU will be small:\nmax i\n\u2016e\u22a4i U\u20162 \u2264 O ( (\u00b5r)1.5\u03ba\u22c6\u03c3\u22c61 d )\nCase 1: \u2016\u2206\u20162F \u2264 \u03c3\u22c6r/4. In this case,\u2206 is small, and\u2206\u2206\u22a4 is even smaller. AlthoughH does not perserve norm for\u2206\u2206\u22a4 very well, it will only contribute a very small factor to overall summation. Specifically, by our choice of p, we will have Lemma 35 holds with small constant \u03b4 and thus:\n1 p \u2016U\u22c6\u2206\u22a4\u20162\u2126 \u2265 (1 \u2212 \u03b4)\u2016U\u22c6\u2206\u22a4\u20162F \u2265 (1\u2212 \u03b4)\u03c3\u22c6r\u2016\u2206\u20162F\nOn the other hand, by Lemma 37, we know:\n1 p \u2016\u2206\u2206\u22a4\u20162\u2126 \u2264\u2016\u2206\u20164F +O(\n\u221a\nd p \u00b7 (\u00b5r) 1.5\u03ba\u22c6\u03c3\u22c61 d )\u2016\u2206\u20162F \u2264 \u2016\u2206\u20164F + \u03c3\u22c6r 4 \u2016\u2206\u20162F \u2264 \u03c3\u22c6r 2 \u2016\u2206\u20162F\nthis gives the summation:\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M\u2212M\u22c6) =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(U\u22c6\u2206\u22a4 +\u2206U\u22c6\u22a4 +\u2206\u2206\u22a4) : H : (U\u22c6\u2206\u22a4 +\u2206U\u22c6\u22a4 +\u2206\u2206\u22a4) \u2264\u2212 12(U\u22c6\u2206\u22a4 : H : \u2206\u2206\u22a4 +U\u22c6\u2206\u22a4 : H : U\u22c6\u2206\u22a4)\n\u2264\u2212 12 p (\u2016U\u22c6\u2206\u22a4\u20162\u2126 \u2212 \u2016U\u22c6\u2206\u22a4\u2016\u2126\u2016\u2206\u2206\u22a4\u2016\u2126) = \u2212 12 p \u2016U\u22c6\u2206\u22a4\u2016\u2126(\u2016U\u22c6\u2206\u22a4\u2016\u2126 \u2212 \u2016\u2206\u2206\u22a4\u2016\u2126) \u2264\u2212 12 \u221a 1\u2212 \u03b4( \u221a 1\u2212 \u03b4 \u2212 \u221a\n2/3)\u03c3\u22c6r\u2016\u2206\u20162F \u2264 \u22121.2\u03c3\u22c6r\u2016\u2206\u20162F\nThe last inequality is by choosing p \u2265 \u2126(\u00b5 3r4(\u03ba\u22c6)4 log d\nd ) with large enough constant factor, we have small \u03b4.\nCase 2: \u2016\u2206\u20162F \u2265 \u03c3\u22c6r/4. In this case \u2206 is large, by Lemma 38 with high probability, our choice of p gives:\n1 p \u2016\u2206\u2206\u22a4\u20162\u2126 \u2264\u2016\u2206\u2206\u22a4\u20162F +O\n(\ndr log d\np \u2016\u2206\u2206\u22a4\u20162\u221e +\n\u221a\ndr log d\np \u2016\u2206\u2206\u22a4\u2016F\u2016\u2206\u2206\u22a4\u2016\u221e\n)\n\u2264\u2016\u2206\u2206\u22a4\u20162F +O ( dr log d\np \u00b7 (\u00b5r)\n3(\u03ba\u22c6\u03c3\u22c61) 2\nd2 +\n\u221a\ndr log d p \u00b7 (\u00b5r) 3(\u03ba\u22c6\u03c3\u22c61) 2 d2 \u2016\u2206\u20162F\n)\n\u2264\u2016\u2206\u2206\u22a4\u20162F + (\u03c3\u22c6r ) 2\n80 + \u03c3\u22c6r 20 \u2016\u2206\u20162F \u2264 \u2016\u2206\u2206\u22a4\u20162F + 0.1\u03c3\u22c6r\u2016\u2206\u20162F\nAgain by Lemma 38 with high probability\n1 p \u2016M\u2212M\u22c6\u20162\u2126 \u2265\u2016M\u2212M\u22c6\u20162F \u2212O\n(\ndr log d\np \u2016M\u2212M\u22c6\u20162\u221e +\n\u221a\ndr log d\np \u2016M\u2212M\u22c6\u2016F\u2016M\u2212M\u22c6\u2016\u221e\n)\n\u2265\u2016M\u2212M\u22c6\u20162F \u2212O ( dr log d\np \u00b7 (\u00b5r)\n3(\u03ba\u22c6\u03c3\u22c61) 2\nd2 +\n\u221a\ndr log d p \u00b7 (\u00b5r) 3(\u03ba\u22c6\u03c3\u22c61) 2 d2 \u2016M\u2212M\u22c6\u2016F\n)\n\u2265\u2016M\u2212M\u22c6\u20162F \u2212 (\u03c3\u22c6r ) 2 80 \u2212 \u03c3 \u22c6 r 20 \u2016M\u2212M\u22c6\u2016F \u2265 0.95\u2016M\u2212M\u22c6\u20162F \u2212 0.1\u03c3\u22c6r\u2016\u2206\u20162F\nThis gives:\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M\u2212M\u22c6) \u2264\u2016\u2206\u2206\u22a4\u20162F + 0.1\u03c3\u22c6r\u2016\u2206\u20162F \u2212 3(0.95\u2016M\u2212M\u22c6\u20162F \u2212 0.1\u03c3\u22c6r\u2016\u2206\u20162F) \u2264\u2212 0.85\u2016M\u2212M\u22c6\u20162F + 0.4\u03c3\u22c6r\u2016\u2206\u20162F \u2264 \u22120.3\u03c3\u22c6r\u2016\u2206\u20162F\nwhere the last step is by Lemma 6. This finishes the proof.\nFinally, as in step 3 of our framework, we need to bound the contribution from the regularizer to Equation (9).\nLemma 11. By choosing \u03b12 = \u0398( \u00b5r\u03c3\u22c6 1\nd ) and \u03bb\u03b12 \u2264 O(\u03c3\u22c6r ), we have:\n1 4 [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009] \u2264 0.1\u03c3\u22c6r\u2016\u2206\u20162F\nProof. By Lemma 18, the contribution from the regularizer to Equation (9) can be calculated as follows:\n1 4 [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009] =\u03bb\nd \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)3+ \u2016e\u22a4i U\u20162\u2016e\u22a4i \u2206\u20162 \u2212 (e\u22a4i U\u2206\u22a4ei)2\n\u2016e\u22a4i U\u20163\n+ 3\u03bb\nd \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)2+ ( e\u22a4i U\u2206 \u22a4ei\n\u2016e\u22a4i U\u2016\n)2\n\u2212 4\u03bb d \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)3+ e\u22a4i U\u2206 \u22a4ei \u2016e\u22a4i U\u2016\nDenote three terms in RHS to be A1, A2, A3. Since \u2016e\u22a4i U\u2212 e\u22a4i \u2206\u2016 = \u2016e\u22a4i U\u22c6R\u2016 = \u2016e\u22a4i U\u22c6\u2016 \u2264 \u221a \u00b5 r d \u03c3\u22c61 . By choosing \u03b1 > C \u221a\n\u00b5 r d \u03c3\u22c61 for some large constant C. Thus, when \u2016e\u22a4i U\u2016 \u2212 \u03b1 > 0, we have e\u22a4i U \u2248 e\u22a4i \u2206, strictly\nspeaking:\ne\u22a4i U\u2206 \u22a4ei = e \u22a4 i U(U\u2212U\u22c6R)\u22a4ei \u2265 \u2016e\u22a4i U\u20162 \u2212 \u2016e\u22a4i U\u2016\u2016e\u22a4i U\u22c6\u2016 \u2265 (1 \u2212\n1 C )\u2016e\u22a4i U\u20162\nand\n\u2016e\u22a4i U\u2016\u2016e\u22a4i \u2206\u2016 \u2264 \u2016e\u22a4i U\u2016(\u2016e\u22a4i U\u2016+ \u2016e\u22a4i U\u22c6\u2016) \u2264 (1 + 1 C )\u2016e\u22a4i U\u20162\nNow we bound the summation A1 +A2 +A3 by seperately boundingA1 + 0.1A3 and A2 + 0.9A3. First, we have:\nA1 + 0.1A3 \u2264 \u03bb d \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)3+\u2016e\u22a4i U\u2016 ( (1 + 1 C )2 \u2212 (1\u2212 1 C )2 \u2212 0.4(1\u2212 1 C ) ) < 0\nThen, the remaining part is:\nA2 + 0.9A3 = 3\u03bb\nd \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b1)2+ e\u22a4i U\u2206 \u22a4ei \u2016e\u22a4i U\u2016 [ e\u22a4i U\u2206 \u22a4ei \u2016e\u22a4i U\u2016 \u2212 1.2(\u2016e\u22a4i U\u2016 \u2212 \u03b1)+ ]\nWe further denote i-th summand in RHS as A (i) 2 + 0.9A (i) 3 , and decompose this term as A2 + 0.9A3 = \u2211d i=1(A (i) 2 + 0.9A (i) 3 ).\nCase 1: for i such that \u2016e\u22a4i U\u2016 \u2265 9\u03b1, and C \u2265 100, we have:\nA (i) 2 + 0.9A (i) 3 \u2264 3\u03bb(\u2016e\u22a4i U\u2016 \u2212 \u03b1)2+\ne\u22a4i U\u2206 \u22a4ei\n\u2016e\u22a4i U\u2016\n[\n(1 + 1\nC )\u2016e\u22a4i U\u2016 \u2212 1.2(\u2016e\u22a4i U\u2016 \u2212 \u03b1)+\n]\n\u2264 0\nThis is because in the above product, we have (\u2016e\u22a4i U\u2016 \u2212 \u03b1)2+ > 0, e \u22a4 i U\u2206 \u22a4ei \u2016e\u22a4 i U\u2016 \u2265 (1 \u2212 1 C )\u2016e\u22a4i U\u2016 \u2265 0 and [\n(1 + 1 C )\u2016e\u22a4i U\u2016 \u2212 1.2(\u2016e\u22a4i U\u2016 \u2212 \u03b1)+ ] \u2264 0.\nCase 2: for i such that \u03b1 < \u2016e\u22a4i U\u2016 < 9\u03b1, we call this set I = {i | \u03b1 < \u2016e\u22a4i U\u2016 < 9\u03b1}: \u2211\ni\u2208I A\n(i) 2 + 0.9A (i) 3 \u2264 3 \u00b7 104 \u00d7 \u03bb|I|\u03b14\nIn sum, this proves there exists some large constant c2 so the regularization term:\n1 4 [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009] \u2264 c2\u03bb|I|\u03b14\nFinally, by the property of set I:\n\u03c3\u22c6r\u2016\u2206\u20162F \u2265 \u03c3\u22c6r \u2211 i\u2208I \u2016e\u22a4i \u2206\u20162 \u2265 \u03c3\u22c6r |I|\u03b12\nTherefore, as long as \u03bb\u03b12 \u2264 \u03c3\u22c6r/c3 for some large absolute constant c3 (which is satisfied by our choice of \u03bb), we have\n[\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009] \u2264 0.1\u03c3\u22c6r\u2016\u2206\u20162F\nCombining these lemmas, we are now ready to prove the main theorem for symmetric matrix completion.\nTheorem 12. When sample rate p \u2265 \u2126(\u00b5 3r4(\u03ba\u22c6)4 log d\nd ), by choosing \u03b12 = \u0398(\n\u00b5r\u03c3\u22c6 1\nd ) and \u03bb = \u0398( d \u00b5r\u03ba\u22c6 ). Then with\nprobability at least 1\u22121/poly(d), for matrix completion objective (11)we have 1) all local minima satisfyUU\u22a4 = M\u22c6 2) the function is (\u01eb,\u2126(\u03c3\u22c6r ), O(\n\u01eb \u03c3\u22c6r ))-strict saddle for polynomially small \u01eb.\nProof. By Lemma 10, we know\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M \u2212M\u22c6) \u2264 \u22120.3\u03c3\u22c6r\u2016\u2206\u20162F On the other hand, by Lemma 11, we have the regularization term:\n[\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009] \u2264 0.1\u03c3\u22c6r\u2016\u2206\u20162F\nThis means for pointU with small gradient satisfying \u2016\u2207f(U)\u2016F \u2264 \u01eb:\n\u2206 : \u22072f(U) : \u2206 \u2264 \u22120.2\u03c3\u22c6r\u2016\u2206\u20162F + 4\u01eb\u2016\u2206\u2016F\nThat is, if U is close to U\u22c6 (i.e. if \u2016\u2206\u2016F \u2265 40\u01eb\u03c3\u22c6r ), we have \u2206 : \u2207 2f(U) : \u2206 \u2264 \u22120.1\u03c3\u22c6r\u2016\u2206\u20162F. This proves (\u01eb, 0.1\u03c3\u22c6r , 40\u01eb \u03c3\u22c6r\n)-strict saddle property. Take \u01eb = 0, we know all stationary points with \u2016\u2206\u2016F 6= 0 are saddle points. This means all local minima are global minima (satisfyingUU\u22a4 = M\u22c6), which finishes the proof.\nA.2 Robust PCA\nFor robust PCA, the first crucial step is to analyze the matrix factorization problemwhen target matrix is not necessarily low rank (that happens if we fix S).\nLemma 13. Let A \u2208 Rd\u00d7d be a symmetric PSD matrix, and matrix factorization objective to be:\nf(U) = \u2016UU\u22a4 \u2212A\u20162F where \u03c3r(A) \u2265 15\u03c3r+1(A). then 1) all local minima satisfies UU\u22a4 = Pr(A) (best rank-r approximation), 2) objective is (\u01eb,\u2126(\u03c3\u22c6r ), O(\n\u01eb \u03c3\u22c6r ))-strict saddle.\nProof. Denote M\u22c6 = Pr(A) to be the top r part and S = A \u2212M\u22c6 to be the remaining part. In our framework, we can also view this remaining part as regularization term. That is:\nM : H : M = \u2016M\u20162F and Q(U) = \u3008M\u22c6 \u2212UU\u22a4,S\u3009+ 1\n2 \u2016S\u20162F\nMoreover, since the eigenspace ofM\u22c6 is perpendicular to S, we have:\n[\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009] = \u22122\u3008\u2206\u2206\u22a4,S\u3009+ 4\u3008U\u2206+\u2206U,S\u3009 = 6\u3008UU\u22a4,S\u3009 \u2264 6\u2016S\u2016\u2016\u2206\u20162F\nThe last step is because supposeXDX\u22a4 is the SVD ofS, then \u3008UU\u22a4,S\u3009 \u2264 \u2016D\u2016\u2016X\u22a4U\u20162F = \u2016D\u2016\u2016X\u22a4(U\u2212U\u22c6)\u20162F \u2264 \u2016S\u2016\u2016\u2206\u20162F.\nTherefore, for pointU with small gradient satisfying \u2016\u2207f(U)\u2016F \u2264 \u01eb:\n\u2206 : \u22072f(U) : \u2206 =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M \u2212M\u22c6) + 4\u3008\u2207f(U),\u2206\u3009 + [\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009]\n\u2264\u2016\u2206\u2206\u22a4\u20162F \u2212 3\u2016M\u2212M\u22c6\u20162F + 4\u01eb\u2016\u2206\u2016F + 6\u2016S\u2016\u2016\u2206\u20162F \u2264\u2212 \u2016M\u2212M\u22c6\u20162F + 4\u01eb\u2016\u2206\u2016F + 6\u2016S\u2016\u2016\u2206\u20162F \u2264 \u22120.4\u03c3\u22c6r\u2016\u2206\u20162F + 4\u01eb\u2016\u2206\u2016F\nThe second last inequality is due to Lemma 40 that \u2016\u2206\u2206\u22a4\u20162F \u2264 2\u2016M\u2212M\u22c6\u20162F, and last inequality is due to Lemma 41 and \u2016S\u2016 = \u03bbr+1(A) \u2264 \u03bbr(A)/15. This means ifU is close toU\u22c6, that is, if \u2016\u2206\u2016F \u2265 20\u01eb\u03c3\u22c6r , we have\u2206 : \u2207 2f(U) : \u2206 \u2264 \u22120.2\u03c3\u22c6r\u2016\u2206\u20162F. This proves (\u01eb, 0.2\u03c3\u22c6r , 20\u01eb\u03c3\u22c6r )-strict saddle property. Take \u01eb = 0, we know all stationary points with \u2016\u2206\u2016F 6= 0 are saddle points. This means all local minima are global minima (satisfyingUU\u22a4 = M\u22c6), which finishes the proof.\nNext we need to show that if U is close to the best rank-r approximation of M\u22c6 + S\u22c6 \u2212 S, then it also must be close to the trueU\u22c6. The proofs of this lemma for symmetric robust PCA is almost directly followed by the arguments for asymmetric versions. Therefore we do not repeat the proofs here.\nLemma 14. There is an absolute constant c, assume \u03b3 > c, and \u03b3\u03b1 \u00b7\u00b5r \u00b7 (\u03ba\u22c6)5 \u2264 1 c . LetU\u2020(U\u2020)\u22a4 be the best rank rapproximation ofM\u22c6+S\u22c6\u2212S, where S is the minimizer as in Eq.(12). AssumeminR\u22a4R=RR\u22a4=I \u2016U\u2212U\u2020R\u2016F \u2264 \u01eb. Let \u2206 be defined as in Definition 6, then \u2016\u2206\u2016F \u2264 O(\u01eb \u221a \u03ba\u22c6) for polynomially small \u01eb.\nProof. The proof follows from the same argument as the proof of Lemma 24.\nCombining these two lemmas, it is not hard to show to main result for Robust PCA.\nTheorem 15. There is an absolute constant c, if \u03b3 > c, and \u03b3\u03b1 \u00b7 \u00b5r \u00b7 (\u03ba\u22c6)5 \u2264 1 c holds, for objective function Eq.(12) we have 1) all local minima satisfies UU\u22a4 = M\u22c6; 2) objective function is (\u01eb,\u2126(\u03c3\u22c6r ), O( \u01eb \u221a \u03ba\u22c6\n\u03c3\u22c6r ))-pseudo strict saddle\nfor polynomially small \u01eb.\nProof. Recall objective function:\nf(U) = 1\n2 min S\u2208S\u03b3\u03b1 \u2016UU\u22a4 + S\u2212M\u22c6 \u2212 S\u22c6\u20162F\nConsider pointU with small gradient satisfying \u2016\u2207f(U)\u2016F \u2264 \u01eb. Let\nSU = argmin S\u2208S\u03b3\u03b1\n\u2016UU\u22a4 + S\u2212M\u22c6 \u2212 S\u22c6\u20162F\nand function fU(U\u0303) = \u2016U\u0303U\u0303\u22a4 + SU \u2212M\u22c6 \u2212 S\u22c6\u20162F , then, we know for all U\u0303, we have fU(U\u0303) \u2265 f(U\u0303) and fU(U) = f(U). Since fU(U\u0303) is matrix factorization objective where by Lemma 28:\n\u2016S\u22c6 \u2212 SU\u2016 \u2264 2\u03b3\u03b1 \u00b7 2 \u00b5r\u03c3\u22c61 d\n\u2264 0.01\u03c3\u22c6r \u03c3r(M\n\u22c6 + S\u22c6 \u2212 SU) \u2265 \u03c3\u22c6r \u2212 \u2016S\u22c6 \u2212 SU\u2016 \u2265 0.99\u03c3\u22c6r \u03c3r+1(M \u22c6 + S\u22c6 \u2212 SU) \u2264 \u2016S\u22c6 \u2212 SU\u2016 \u2264 0.01\u03c3\u22c6r\nThis gives \u03c3r(M \u22c6 + S\u22c6 \u2212 SU) \u2265 15\u03c3r+1(M\u22c6 + S\u22c6 \u2212 SU). Given \u2016\u2207fU(U)\u2016F = \u2016\u2207f(U)\u2016F \u2264 \u01eb, by Lemma 23, we know either \u03bbmin(\u22072fU(U)) \u2264 \u22120.2\u03c3\u22c6r or minR\u22a4R=RR\u22a4=I \u2016U\u2212U\u2020R\u2016F \u2264 20\u01eb\u03c3\u22c6r where U \u2020(U\u2020)\u22a4 is the best rank r-approximation of M\u22c6 + S\u22c6 \u2212 SU. By Lemma 24, we immediately have \u2016\u2206\u2016F \u2264 10 3\u01eb \u221a \u03ba\u22c6\n\u03c3\u22c6r , which proves\n(\u01eb,\u2126(\u03c3\u22c6r ), O( \u01eb \u221a \u03ba\u22c6\n\u03c3\u22c6r ))-pseudo strict saddle. By taking \u01eb = 0, we proved all local minima satisfies UU\u22a4 = M\u22c6."}, {"heading": "B Proofs for Asymmetric Problems", "text": "In this section we give proofs for the asymmetric settings. In particular, we first prove the main lemma, which gives the crucial reduction from asymmetric case to symmetric case.\nLemma 16. For the objective (15), let \u2206,N be defined as in Definition 7. Then, for anyW \u2208 R(d1+d2)\u00d7r, we have\n\u2206 : \u22072f(W) : \u2206 \u2264\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(N\u2212N\u22c6) : H : (N\u2212N\u22c6) + 4\u3008\u2207f(W),\u2206\u3009+ [\u2206 : \u22072Q(W) : \u2206\u2212 4\u3008\u2207Q(W),\u2206\u3009]\nFurther, if H0 satisfies M : H0 : M \u2208 (1 \u00b1 \u03b4)\u2016M\u20162F for some matrix M = UV\u22a4, let W and N be defined as in (14), thenN : H : N \u2208 (1\u00b1 2\u03b4)\u2016N\u20162F . Proof. Recall the objective function is (N = WW\u22a4):\nf(W) = 1\n2 [(N\u2212N\u22c6) : 4H1 : (N\u2212N\u22c6) +N : G : N] +Q(W)\nCalculating gradient and Hessian, we have for any Z \u2208 Rd\u00d7r:\n\u3008\u2207f(W),Z\u3009 =(N\u2212N\u22c6) : 4H1 : (WZ\u22a4 + ZW\u22a4) +N : G : (WZ\u22a4 + ZW\u22a4) + \u3008\u2207Q(W),Z\u3009 Z : \u22072f(W) : Z =(WZ\u22a4 + ZW\u22a4) : (4H1 + G) : (WZ\u22a4 + ZW\u22a4) + 2(N\u2212N\u22c6) : 4H1 : ZZ\u22a4\n+ 2N : G : \u2206\u2206\u22a4 + Z : \u22072Q(W) : Z\nLet Z = \u2206 = W\u2212W\u22c6R as in Definition 7, and noteN\u2212N\u22c6 +\u2206\u2206\u22a4 = W\u2206\u22a4 +\u2206W\u22a4 andN\u22c6 : G : N\u22c6 = N\u22c6 : G : W\u22c6W\u22a4 = 0 due to U\u22c6\u22a4U\u22c6 = V\u22c6\u22a4V\u22c6. LetH = 4H1 + G, then\n\u3008\u2207f(W),\u2206\u3009 =(N\u2212N\u22c6) : H : (W\u2206\u22a4 +\u2206W\u22a4) +N\u22c6 : G : (W\u2206\u22a4 +\u2206W\u22a4) + \u3008\u2207Q(W),\u2206\u3009 =(N\u2212N\u22c6) : H : (W\u2206\u22a4 +\u2206W\u22a4) + 2N\u22c6 : G : N+ \u3008\u2207Q(W),\u2206\u3009 (21)\nWhere the last equality is use the fact N\u22c6 : G : W\u22c6W\u22a4 = N\u22c6 : G : WW\u22c6\u22a4 = 0. For Hessian along\u2206 direction:\n\u2206 : \u22072f(W) : \u2206 =(W\u2206\u22a4 +\u2206W\u22a4) : H : (W\u2206\u22a4 +\u2206W\u22a4) + 2(N\u2212N\u22c6) : 4H1 : \u2206\u2206\u22a4\n+ 2N : G : \u2206\u2206\u22a4 +\u2206 : \u22072Q(W) : \u2206 (22)\nFor first term of Eq.(22): since\u2206\u2206\u22a4+(N\u2212N\u22c6) = W\u2206\u22a4+\u2206W\u22a4 and (a+ b)2 = a2+2b(a+ b)\u2212 b2 and Eq.(21), we have:\n(W\u2206\u22a4 +\u2206W\u22a4) : H : (W\u2206\u22a4 +\u2206W\u22a4) =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 + 2(N\u2212N\u22c6) : H : (W\u2206\u22a4 +\u2206W\u22a4)\u2212 (N\u2212N\u22c6) : H : (N\u2212N\u22c6) =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 (N\u2212N\u22c6) : H : (N\u2212N\u22c6) + 2\u3008\u2207f(W),\u2206\u3009 \u2212 4N\u22c6 : G : N\u2212 2\u3008\u2207Q(W),\u2206\u3009\nFor the sum of second and third terms of Eq.(22):\n2(N\u2212N\u22c6) : 4H1 : \u2206\u2206\u22a4 + 2N : G : \u2206\u2206\u22a4\n=2(N\u2212N\u22c6) : H : \u2206\u2206\u22a4 + 2N\u22c6 : G : N =\u2212 2(N\u2212N\u22c6) : H : (N\u2212N\u22c6) + 2(N\u2212N\u22c6) : H : (W\u2206\u22a4 +\u2206W\u22a4) + 2N\u22c6 : G : N =\u2212 2(N\u2212N\u22c6) : H : (N\u2212N\u22c6) + 2\u3008\u2207f(W),\u2206\u3009 \u2212 2N\u22c6 : G : N\u2212 2\u3008\u2207Q(W),\u2206\u3009\nIn sum, we have:\n\u2206 : \u22072f(W) : \u2206 =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(N\u2212N\u22c6) : H : (N\u2212N\u22c6)\u2212 6N\u22c6 : G : N + 4\u3008\u2207f(W),\u2206\u3009+ [\u2206 : \u22072Q(W) : \u2206\u2212 4\u3008\u2207Q(W),\u2206\u3009]\ncleary the third term is always non-positive. this gives:\n\u2206 : \u22072f(W) : \u2206 \u2264\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(N\u2212N\u22c6) : H : (N\u2212N\u22c6) + 4\u3008\u2207f(W),\u2206\u3009 + [\u2206 : \u22072Q(W) : \u2206\u2212 4\u3008\u2207Q(W),\u2206\u3009]\nThe remaining claims directly follows from Lemma 19.\nLemma 19. Let A =\n(\nA0 Ac A\u22a4c A1\n)\n\u2208 R(d1+d2)\u00d7(d1+d2) be a symmetric matrix, if forH0 we have:\n(1\u2212 \u03b4)\u2016Ac\u20162F \u2264 Ac : H0 : Ac \u2264 (1 + \u03b4)\u2016Ac\u20162F\nThen, we have: (1\u2212 2\u03b4)\u2016A\u20162F \u2264 A : H : A \u2264 (1 + 2\u03b4)\u2016A\u20162F Proof. By calculation,\nA : H : A = 4Ac : H0 : Ac + ( \u2016A1\u20162F + \u2016A2\u20162F \u2212 2\u2016Ac\u20162F ) )\nThe lemma easily follows.\nIn the remainder of this section we prove the main theorems for matrix completion and robust PCA.\nB.1 Matrix Completion\nAcross this section, we denote\nQ1(U) = \u03bb1\nd1 \u2211\ni=1\n(\u2016e\u22a4i U\u2016 \u2212 \u03b11)4+ and Q2(V) = \u03bb2 d2 \u2211\nj=1\n(\u2016e\u22a4j V\u2016 \u2212 \u03b12)4+\nand clearlyQ(W) = Q1(U) +Q2(V). We always denote d = max{d1, d2} We proceed in three steps analogous to the symmetric setting. First we show the regularizer again implies rows of U,V cannot be too large (similar to Lemma 9).\nLemma 20. Let d = max{d1, d2}, there is an absolute constant c, when sample rate p \u2265 \u2126( \u00b5r log dmin{d1,d2} ), and \u03b121 = \u0398( \u00b5r\u03c3\u22c6 1 d1 ), \u03b122 = \u0398( \u00b5r\u03c3\u22c6 1 d2 ), \u03bb1 = \u0398( d1 \u00b5r\u03ba\u22c6 ), \u03bb2 = \u0398( d2 \u00b5r\u03ba\u22c6 ), we have for any pointsW with \u2016\u2207f(W)\u2016F \u2264 \u01eb with polynomially small \u01eb. with probability at least 1\u2212 1/poly(d):\nmax i\n\u2016e\u22a4i U\u20162 \u2264 O ( \u00b52r2.5(\u03ba\u22c6)2\u03c3\u22c61 d1 ) and max j \u2016e\u22a4j V\u20162 \u2264 O ( \u00b52r2.5(\u03ba\u22c6)2\u03c3\u22c61 d2 )\nProof. In this proof, by symmetry, W.L.O.G, we can assume \u221a d1 maxi \u2016e\u22a4i U\u2016 \u2265 \u221a d2 maxj \u2016e\u22a4j V\u2016. We know gradient can be calculated as:\n\u2207f(W) =4 p\n(\n(M\u2212M\u22c6)\u2126V (M\u2212M\u22c6)\u22a4\u2126U\n)\n+\n( U(U\u22a4U\u2212V\u22a4V) V(V\u22a4V \u2212U\u22a4U) ) +\u2207Q(W)\nWhere:\n\u2207Q(W) = 4\u03bb1 d1 \u2211\ni=1\n(\u2016e\u22a4i W\u2016 \u2212 \u03b11)3+ eie\n\u22a4 i W\n\u2016eiW\u20162 + 4\u03bb2\nd2 \u2211\ni=d1+1\n(\u2016e\u22a4i W\u2016 \u2212 \u03b12)3+ eie\n\u22a4 i W \u2016eiW\u20162\nClearly, we have \u3008\u2207Q(W),W\u3009 \u2265 0, therefore, for any pointsW with small gradient \u2016\u2207f(W)\u2016F \u2264 \u01eb, we have:\n\u01eb\u2016W\u2016F \u2265\u3008\u2207f(W),W\u3009\n=\u2016U\u22a4U\u2212V\u22a4V\u20162F + 4\np \u3008(M \u2212M\u22c6)\u2126,M\u3009+ \u3008\u2207Q(W),W\u3009\n\u2265\u2016U\u22a4U\u2212V\u22a4V\u20162F \u2212 4\np \u3008(M\u22c6)\u2126, (M)\u2126\u3009\n\u2265\u2016U\u22a4U\u2212V\u22a4V\u20162F \u2212 4 \u00b7 1\u221a p \u2016M\u22c6\u2016\u2126 \u00b7 1\u221a p \u2016M\u2016\u2126 \u2265\u2016U\u22a4U\u2212V\u22a4V\u20162F \u2212O( \u221a\nd1d2)\u2016M\u22c6\u2016F\u2016M\u2016\u221e where last inequality is by Lemma 35 and Lemma 39. Let i\u22c6 = argmaxi \u2016e\u22a4i U\u2016, and j\u22c6 = argmaxj \u2016e\u22a4j V\u2016. By assumption, we know \u221a d1\u2016e\u22a4i\u22c6U\u2016 \u2265 \u221a d2\u2016e\u22a4j\u22c6V\u2016 and due to \u2016M\u22c6\u2016F \u2264 \u221a r\u03c3\u22c61 and \u2016M\u2016\u221e \u2264 \u2016e\u22a4i\u22c6U\u2016\u2016e\u22a4j\u22c6V\u2016, this gives:\n\u2016U\u22a4U\u2212V\u22a4V\u20162F \u2264 O(d1\u03c3\u22c61 \u221a r)\u2016e\u22a4i\u22c6U\u20162 +O(\u01ebd)\u2016e\u22a4i\u22c6U\u2016 (23)\nIn case \u2016e\u22a4i\u22c6U\u2016 \u2265 2\u03b1i, consider \u3008e\u22a4i\u22c6\u2207f(W), e\u22a4i\u22c6W\u3009:\n\u01eb\u2016e\u22a4i\u22c6U\u2016 \u2264\u3008e\u22a4i\u22c6\u2207f(W), e\u22a4i\u22c6W\u3009\n=\u3008e\u22a4i\u22c6 [ 4\np (M \u2212M\u22c6)\u2126V +U(U\u22a4U\u2212V\u22a4V) +\u2207Q1(U)\n]\n, e\u22a4i\u22c6U\u3009\n\u22654\u03bb1(\u2016e\u22a4i\u22c6U\u2016 \u2212 \u03b11)3+\u2016e\u22a4i\u22c6U\u2016 \u2212 4 p \u3008e\u22a4i\u22c6(M\u22c6)\u2126, e\u22a4i\u22c6(M)\u2126\u3009 \u2212 \u2016U\u22a4U\u2212V\u22a4V\u2016F\u2016e\u22a4i\u22c6U\u20162 \u2265\u03bb1 2 \u2016e\u22a4i\u22c6U\u20164 \u2212 4 1\u221a p \u2016e\u22a4i\u22c6(M\u22c6)\u2126\u2016 \u00b7 1\u221a p \u2016e\u22a4i\u22c6(M)\u2126\u2016 \u2212 \u2016U\u22a4U\u2212V\u22a4V\u2016F\u2016e\u22a4i\u22c6U\u20162\n\u2265\u03bb1 2 \u2016e\u22a4i\u22c6U\u20164 \u2212O(1)\u2016e\u22a4i\u22c6M\u22c6\u2016 \u00b7 \u221a d2\u2016M\u2016\u221e \u2212 \u2016U\u22a4U\u2212V\u22a4V\u2016F\u2016e\u22a4i\u22c6U\u20162 \u2265\u03bb1 2 \u2016e\u22a4i\u22c6U\u20164 \u2212 \u221a \u00b5r\u03c3\u22c61\u2016e\u22a4i\u22c6U\u20162 \u2212 \u2016U\u22a4U\u2212V\u22a4V\u2016F\u2016e\u22a4i\u22c6U\u20162\nwhere second last inequality is by Lemma 35 and Lemma 39. Substitute in Eq.(23), we have:\n\u03bb1\u2016e\u22a4i\u22c6U\u20163 \u2264 O( \u221a \u00b5r\u03c3\u22c61)\u2016e\u22a4i\u22c6U\u2016+O( \u221a d\u03c3\u22c61 \u00b7 r 1 4 )\u2016e\u22a4i\u22c6U\u20162 + \u01eb+O( \u221a \u01ebd)\u2016e\u22a4i\u22c6U\u20161.5\nby choosing \u01eb to be polynomially small, we have: \u221a\nd2 d1 max j \u2016e\u22a4j V\u2016 \u2264 max i \u2016e\u22a4i U\u20162 \u2264 cmax { \u03b121, \u221a \u00b5r \u00b7 \u03c3\u22c61 \u03bb1 , d1\u03c3 \u22c6 1 \u221a r \u03bb21 }\nFinally, substituting our choice of \u03b12 and \u03bb, we finished the proof.\nNext we show the HessianH related terms in Eq.(16) is negative whenW 6= W\u22c6. This is analogous to Lemma 10.\nLemma 21. Let d = max{d1, d2}, when sample rate p \u2265 \u2126(\u00b5 4r6(\u03ba\u22c6)6 log d min{d1,d2} ), by choosing \u03b1 2 1 = \u0398( \u00b5r\u03c3\u22c6 1 d1 ), \u03b122 = \u0398( \u00b5r\u03c3\u22c6 1\nd2 ) and \u03bb1 = \u0398( d1 \u00b5r\u03ba\u22c6 ), \u03bb2 = \u0398( d2 \u00b5r\u03ba\u22c6 ). Then with probability at least 1 \u2212 1/poly(d), for all W with\n\u2016\u2207f(W)\u2016F \u2264 \u01eb for polynomially small \u01eb:\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M \u2212M\u22c6) \u2264 \u22120.3\u03c3\u22c6r\u2016\u2206\u20162F Proof. Again the idea is similar, we divide into cases according to the norm of \u2206 and use different concentration inequalities. By our choice of \u03b1, \u03bb and Lemma 20, we known when \u01eb is polynomially small, with high probability:\nmax i\n\u2016e\u22a4i U\u20162 \u2264 O ( \u00b52r2.5(\u03ba\u22c6)2\u03c3\u22c61 d1 ) and max j \u2016e\u22a4j V\u20162 \u2264 O ( \u00b52r2.5(\u03ba\u22c6)2\u03c3\u22c61 d2 )\nIn this proof, we denote\u2206 = (\u2206\u22a4U,\u2206 \u22a4 V) \u22a4, clearly, we have \u2016\u2206U\u2016F \u2264 \u2016\u2206\u2016F and \u2016\u2206V\u2016F \u2264 \u2016\u2206\u2016F. Case 1: \u2016\u2206\u20162F \u2264 \u03c3\u22c6r/40. By Lemma 35 and Lemma 19, we know: W\u22c6\u2206\u22a4 : H : W\u22c6\u2206\u22a4 \u2265 (1\u2212 2\u03b4)\u2016W\u22c6\u2206\u22a4\u20162F \u2265 (1\u2212 2\u03b4)\u03c3\u22c6r\u2016\u2206\u20162F On the other hand, by Lemma 37 and our choice of p, we have:\n1 p \u2016\u2206U\u2206\u22a4V\u20162\u2126 \u2264(1 + \u03b4)\u2016\u2206U\u20162F\u2016\u2206V\u20162F +O(\n\u221a\nd p \u00b7 \u00b5 2r2.5(\u03ba\u22c6)2\u03c3\u22c61\u221a d1d2 )\u2016\u2206U\u2016F\u2016\u2206V\u2016F\n\u2264(1 + \u03b4)\u2016\u2206\u20164F + \u03c3\u22c6r 4 \u2016\u2206\u20162F \u2264 \u03c3\u22c6r ( 2 9 + \u03b4 60 )\u2016\u2206\u20162F \u2264 \u03c3\u22c6r 20 \u2016\u2206\u20162F\nThus by \u2016\u2206U\u20162F \u2264 \u2016\u2206\u20162F \u2264 \u03c3\u22c6r/40 and \u2016\u2206V\u20162F \u2264 \u2016\u2206\u20162F \u2264 \u03c3\u22c6r/40,\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 = 4 p \u2016\u2206U\u2206\u22a4V\u20162\u2126 + ( \u2016\u2206U\u2206\u22a4U\u20162F + \u2016\u2206V\u2206\u22a4V\u20162F \u2212 2\u2016\u2206U\u2206\u22a4V\u20162F ) ) \u2264 1 4 \u03c3\u22c6r\u2016\u2206\u20162F\nThis gives:\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(N\u2212N\u22c6) : H : (N\u2212N\u22c6) =\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(W\u22c6\u2206\u22a4 +\u2206W\u22c6\u22a4 +\u2206\u2206\u22a4) : H : (W\u22c6\u2206\u22a4 +\u2206W\u22c6\u22a4 +\u2206\u2206\u22a4) \u2264\u2212 12(W\u22c6\u2206\u22a4 : H : \u2206\u2206\u22a4 +W\u22c6\u2206\u22a4 : H : W\u22c6\u2206\u22a4)\n\u2264\u2212 12 p\n\u221a W\u22c6\u2206\u22a4 : H : W\u22c6\u2206\u22a4( \u221a W\u22c6\u2206\u22a4 : H : W\u22c6\u2206\u22a4 \u2212 \u221a \u2206\u2206\u22a4 : H : \u2206\u2206\u22a4)\n\u2264\u2212 12 \u221a 1\u2212 2\u03b4( \u221a 1\u2212 2\u03b4 \u2212 \u221a\n1/4)\u03c3\u22c6r\u2016\u2206\u20162F \u2264 \u22121.2\u03c3\u22c6r\u2016\u2206\u20162F The last inequality is by choosing large enough p, we have small \u03b4.\nCase 2: \u2016\u2206\u20162F \u2265 \u03c3\u22c6r/40, by Lemma 38 with high probability, our choice of p gives:\n1 p \u2016\u2206U\u2206\u22a4V\u20162\u2126 \u2264\u2016\u2206U\u2206\u22a4V\u20162F +O\n(\ndr log d\np \u2016\u2206U\u2206\u22a4V\u20162\u221e +\n\u221a\ndr log d\np \u2016\u2206U\u2206\u22a4V\u2016F\u2016\u2206U\u2206\u22a4V\u2016\u221e\n)\n\u2264\u2016\u2206U\u2206\u22a4V\u20162F +O ( dr log d\np \u00b7 \u00b5\n4r5(\u03ba\u22c6)4(\u03c3\u22c61) 2\nd1d2 +\n\u221a\ndr log d p \u00b7 \u00b5 4r5(\u03ba\u22c6)4(\u03c3\u22c61) 2 d1d2 \u2016\u2206\u20162F\n)\n\u2264\u2016\u2206U\u2206\u22a4V\u20162F + (\u03c3\u22c6r ) 2\n1000 + \u03c3\u22c6r 1000\n\u2016\u2206\u20162F \u2264 \u2016\u2206U\u2206\u22a4V\u20162F + 0.01\u03c3\u22c6r\u2016\u2206\u20162F Again by Lemma 38 with high probability\n1 p \u2016M\u2212M\u22c6\u20162\u2126 \u2265\u2016M\u2212M\u22c6\u20162F \u2212O\n(\ndr log d\np \u2016M\u2212M\u22c6\u20162\u221e +\n\u221a\ndr log d\np \u2016M\u2212M\u22c6\u2016F\u2016M\u2212M\u22c6\u2016\u221e\n)\n\u2265\u2016M\u2212M\u22c6\u20162F \u2212O ( dr log d\np \u00b7 \u00b5\n4r5(\u03ba\u22c6)4(\u03c3\u22c61) 2\nd1d2 +\n\u221a\ndr log d p \u00b7 \u00b5 4r5(\u03ba\u22c6)4(\u03c3\u22c61) 2 d1d2 \u2016M\u2212M\u22c6\u2016F\n)\n\u2265\u2016M\u2212M\u22c6\u20162F \u2212 (\u03c3\u22c6r ) 2 1000 \u2212 \u03c3 \u22c6 r 1000 \u2016M\u2212M\u22c6\u2016F \u2265 0.99\u2016M\u2212M\u22c6\u20162F \u2212 0.01\u03c3\u22c6r\u2016\u2206\u20162F\nThen by simple calculation, this gives:\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(N\u2212N\u22c6) : H : (N\u2212N\u22c6) \u2264\u2016\u2206\u2206\u22a4\u20162F + 0.04\u03c3\u22c6r\u2016\u2206\u20162F \u2212 3(0.98\u2016N\u2212N\u22c6\u20162F \u2212 0.04\u03c3\u22c6r\u2016\u2206\u20162F) \u2264\u2212 0.94\u2016N\u2212N\u22c6\u20162F + 0.12\u03c3\u22c6r\u2016\u2206\u20162F \u2264 \u22120.3\u03c3\u22c6r\u2016\u2206\u20162F\nwhere the last step is by Lemma 6. This finishes the proof.\nFinally we bound the contribution from regularizer (analogous to Lemma 11). In fact since our regularizers are\nvery similar we can directly use the same calculation.\nLemma 22. By choosing \u03b121 = \u0398( \u00b5r\u03c3\u22c6 1 d1 ), \u03b121 = \u0398( \u00b5r\u03c3\u22c6 1 d1 ), \u03bb1\u03b1 2 1 \u2264 O(\u03c3\u22c6r ) and \u03bb2\u03b122 \u2264 O(\u03c3\u22c6r ), we have:\n1 4 [\u2206 : \u22072Q(W) : \u2206\u2212 4\u3008\u2207Q(W),\u2206\u3009] \u2264 0.1\u03c3\u22c6r\u2016\u2206\u20162F\nProof. By same calculation as the proof of Lemma 11, we can show:\n1 4 [\u2206U : \u22072Q1(U) : \u2206U \u2212 4\u3008\u2207Q1(U),\u2206U\u3009] \u2264 0.1\u03c3\u22c6r\u2016\u2206U\u20162F 1 4 [\u2206V : \u22072Q2(V) : \u2206V \u2212 4\u3008\u2207Q2(V),\u2206V\u3009] \u2264 0.1\u03c3\u22c6r\u2016\u2206V\u20162F\nGivenQ(W) = Q1(U) +Q2(V), the lemma follows.\nCombining three lemmas, our main result for asymmetric matrix completion easily follows.\nTheorem 4. Let d = max{d1, d2}, when sample rate p \u2265 \u2126(\u00b5 4r6(\u03ba\u22c6)6 log d min{d1,d2} ), choose \u03b1 2 1 = \u0398( \u00b5r\u03c3\u22c6 1 d1 ), \u03b122 = \u0398( \u00b5r\u03c3\u22c6 1 d2 ) and \u03bb1 = \u0398( d1\n\u00b5r\u03ba\u22c6 ), \u03bb2 = \u0398( d2 \u00b5r\u03ba\u22c6 ). With probability at least 1\u2212 1/poly(d), for Objective Function (4) we have 1) all local minima satisfy UV\u22a4 = M\u22c6 2) The objective is (\u01eb,\u2126(\u03c3\u22c6r ), O(\n\u01eb \u03c3\u22c6r ))-strict saddle for polynomially small \u01eb.\nProof. Same argument as the proof Theorem 12 by combining Lemma 20, 21 and 22.\nB.2 Robust PCA\nFor robust PCA, again a crucial step is to analyze the matrix factorization problem. We prove the following Lemma (analogous to Lemma 13).\nLemma 23. Let matrix factorization objective to be (U \u2208 Rd1\u00d7r,V \u2208 Rd2\u00d7r):\nf(W) = 2\u2016UV\u22a4 \u2212A\u20162F + 1 2 \u2016U\u22a4U\u2212V\u22a4V\u20162F\nand \u03c3r(A) \u2265 30\u03c3r+1(A). then 1) all local minima satisfies UV\u22a4 is the top-r SVD of matrix A; 2) objective is (\u01eb, 0.2\u03c3\u22c6r ,\n20\u01eb \u03c3\u22c6r )-strict saddle\nProof. DenoteM\u22c6 = Pr(A) to be the top-r SVD ofA, and S = A\u2212M\u22c6 to be the remaining part. In our framework, we can also view this remaining part as regularization term. That is:\nM : H0 : M = \u2016M\u20162F and Q(W) = 4\u3008M\u22c6 \u2212UV\u22a4,S\u3009+ 2\u2016S\u20162F Moreover, since the eigenspace ofM\u22c6 is perpendicular to S, we have:\n[\u2206 : \u22072Q(W) : \u2206\u2212 4\u3008\u2207Q(W),\u2206\u3009] =\u2212 8\u3008\u2206U\u2206\u22a4V,S\u3009+ 16\u3008U\u2206\u22a4V +\u2206UV\u22a4,S\u3009 =24\u3008UV\u22a4,S\u3009 \u2264 24\u2016S\u2016\u2016\u2206U\u2016F\u2016\u2206V\u2016F \u2264 12\u2016S\u2016\u2016\u2206\u20162F\nThe last step is because supposeXDY\u22a4 is the SVD of S, then\n\u3008UV\u22a4,S\u3009 \u2264 \u2016D\u2016\u2016X\u22a4U\u2016F\u2016Y\u22a4V\u2016F = \u2016D\u2016\u2016X\u22a4(U\u2212U\u22c6)\u2016F\u2016Y\u22a4(V \u2212V\u22c6)\u2016F \u2264 \u2016S\u2016\u2016\u2206U\u2016F\u2016\u2206V\u2016F\nUsing Lemma 19, the remaining argument is the same as Lemma 13.\nNext we prove when W is close to the optimal solution of the matrix factorization problem, it must also be close to the trueW\u22c6. The proof of this lemma uses several crucial properties in choice of S and the sparse set S\u03b3\u03b1. This will require several supporting lemmas which we prove after the main theorem. Our proof is inspired by Yi et al. [2016].\nLemma 24. There is an absolute constant c, assume \u03b3 > c, and \u03b3\u03b1 \u00b7 \u00b5r \u00b7 (\u03ba\u22c6)5 \u2264 1 c . LetX\u2020D\u2020Y\u2020\u22a4 be the best rank r-approximation of M\u22c6 + S\u22c6 \u2212 SW, where SW = argminS\u2208S\u03b3\u03b1 \u2016UV\u22a4 + S\u2212M\u22c6 \u2212 S\u22c6\u20162F . Let U\u2020 = X\u2020(D\u2020) 1 2 , V\u2020 = Y\u2020(D\u2020) 1\n2 . Assume minR\u22a4R=RR\u22a4=I \u2016W \u2212W\u2020R\u2016F \u2264 \u01eb. Let \u2206 be defined as in Definition 7, then \u2016\u2206\u2016F \u2264 O(\u01eb \u221a \u03ba\u22c6) for polynomially small \u01eb.\nProof. By assumption, we have \u2016W \u2212W\u2020\u2016F \u2264 \u01eb, we also haveW in the neighborhood ofW\u22c6. First, we know\n\u2016N\u2020 \u2212N\u2016F =\u2016W\u2020W\u2020\u22a4 \u2212WW\u22a4\u2016F \u2264 (\u2016W\u2020\u2016+ \u2016W\u2016)\u2016W \u2212W\u2020\u2016F \u2264(2\u2016W\u2020\u2016+ \u2016W \u2212W\u2020\u2016F)\u2016W \u2212W\u2020\u2016F \u2264 3\u01eb \u221a \u03c3\u22c61\nWhere the last step is due to \u2016U\u2020\u2016 = \u2016V\u2020\u2016 \u2264 \u221a \u2016M\u22c6 + S\u22c6 \u2212 SW\u2016 \u2264 \u221a \u2016M\u22c6\u2016+ \u2016S\u22c6 \u2212 SW\u2016 \u2264 \u221a 1.01\u03c3\u22c61 ,\n\u2016W\u2020\u2016 \u2264 \u2016U\u2020\u2016 + \u2016V\u2020\u2016 and \u2016U\u2212U\u2020\u2016F \u2264 20\u01eb\u03c3\u22c6r \u2264 0.5 \u221a \u03c3\u22c61 by our choice of \u01eb. Then by Lemma 25 and Lemma 42 we have: \u2016N\u2020 \u2212N\u22c6\u2016F \u2264 2\u2016M\u2020 \u2212M\u22c6\u2016F \u2264 4\u2016SW \u2212 S\u22c6\u2016F By triangle inequality, this gives:\n\u2016N\u2212N\u22c6\u2016F \u2264 \u2016N\u2020 \u2212N\u2016F + \u2016N\u2020 \u2212N\u22c6\u2016F \u2264 4\u2016SW \u2212 S\u22c6\u2016F + 3\u01eb \u221a \u03c3\u22c61 (24)\nOn the other hand, by Lemma 27, we know matrixM\u2020 is 4\u00b5(\u03ba\u22c6)4-incoherent. Thus for any i \u2208 [d1]:\n\u2016e\u22a4i U\u2016 \u2264 \u2016e\u22a4i (U\u2212U\u2020)\u2016+ \u2016e\u22a4i U\u2020\u2016 \u2264 \u2016W \u2212W\u2020\u2016F + 2(\u03ba\u22c6)2 \u221a 1.01\u00b5r\u03c3\u22c61 d1 \u2264 3(\u03ba\u22c6)2 \u221a \u00b5r\u03c3\u22c61 d1\nBy symmetry, we also have for any j \u2208 [d2], \u2016e\u22a4j V\u2016 \u2264 3(\u03ba\u22c6)2 \u221a \u00b5r\u03c3\u22c6 1\nd1 . Then, by Lemma 30:\n\u2016SW \u2212 S\u22c6\u20162F \u2264 2\u2016M\u2212M\u22c6\u20162\u2126\u22c6\u222a\u2126 + 8\n\u03b3 \u2212 1\u2016M\u2212M \u22c6\u20162F\nby Lemma 26: \u2016M\u2212M\u22c6\u20162\u2126 \u2264 36\u03b3\u03b1\u00b5(\u03ba\u22c6)4r\u2016M\u22c6\u2016(\u2016\u2206U\u20162F + \u2016\u2206V\u20162F) \u2264 0.04\u03c3\u22c6r\u2016\u2206\u20162F Similarly, we also have \u2016M\u2212M\u22c6\u20162\u2126\u22c6 \u2264 0.04\u03c3\u22c6r\u2016\u2206\u20162F Clearly, we have \u2016M\u2212M\u22c6\u2016F \u2264 \u2016N\u2212N\u22c6\u2016F. By Lemma 41, we also have \u03c3\u22c6r\u2016\u2206\u20162F \u2264 12(\u221a2\u22121)\u2016N\u2212N \u22c6\u20162F. Given our choice of \u03b3, this gives:\n\u2016SW \u2212 S\u22c6\u20162F \u2264 2\u2016M\u2212M\u22c6\u20162\u2126\u22c6\u222a\u2126 + 8\n\u03b3 \u2212 1\u2016M\u2212M \u22c6\u20162F \u2264\n1\n25 \u2016N\u2212N\u22c6\u20162F (25)\nFinally, combineing Eq.(24) and Eq.(25), we have:\n\u2016N\u22c6 \u2212N\u2016F \u2212 60\u01eb \u221a \u03ba\u22c6\u221a\n\u03c3\u22c6r \u2264 4 5 \u2016N\u22c6 \u2212N\u2016F\nBy Lemma 41, we know:\n\u2016\u2206\u2016F \u2264 1\u221a \u03c3\u22c6r\n\u221a\n1\n2( \u221a 2\u2212 1)\n\u2016N\u22c6 \u2212N\u2016F \u2264 \u221a\n1\n2( \u221a 2\u2212 1)\n\u00b7 5 \u00b7 3\u01eb \u221a \u03ba\u22c6 \u2264 20\u01eb \u221a \u03ba\u22c6\nThis finishes the proof.\nNow we are ready to prove the main theorem:\nTheorem 5. There is an absolute constant c, if \u03b3 > c, and \u03b3\u03b1 \u00b7 \u00b5r \u00b7 (\u03ba\u22c6)5 \u2264 1 c holds, for objective function Eq.(6) we have 1) all local minima satisfiesUV\u22a4 = M\u22c6; 2) objective function is (\u01eb,\u2126(\u03c3\u22c6r ), O( \u01eb \u221a \u03ba\u22c6\n\u03c3\u22c6r ))-pseudo strict saddle for\npolynomially small \u01eb.\nProof. Recall scaled version (multiplied by 4) of objective function Eq.(6) is:\nf(W) = 2 min S\u2208S\u03b3\u03b1\n\u2016UV\u22a4 + S\u2212M\u22c6 \u2212 S\u22c6\u20162F + 1 2 \u2016U\u22a4U\u2212V\u22a4V\u20162F\nConsider pointW with small gradient satisfying \u2016\u2207f(W)\u2016F \u2264 \u01eb. Let\nSW = argmin S\u2208S\u03b3\u03b1\n\u2016UV\u22a4 + S\u2212M\u22c6 \u2212 S\u22c6\u20162F\nand function fW(W\u0303) = 2\u2016U\u0303V\u0303\u22a4 + SW \u2212M\u22c6 \u2212 S\u22c6\u20162F + 12\u2016U\u0303\u22a4U\u0303\u2212 V\u0303\u22a4V\u0303\u20162F, then, we know for all W\u0303, we have fW(W\u0303) \u2265 f(W\u0303) and fW(W) = f(W). Since fW(W\u0303) is matrix factorization objective where by Lemma 28:\n\u2016S\u22c6 \u2212 SW\u2016 \u2264 2\u03b3\u03b1 \u00b7 2 \u00b5r\u03c3\u22c61 d\n\u2264 0.01\u03c3\u22c6r \u03c3r(M\n\u22c6 + S\u22c6 \u2212 SW) \u2265 \u03c3\u22c6r \u2212 \u2016S\u22c6 \u2212 SW\u2016 \u2265 0.99\u03c3\u22c6r \u03c3r+1(M \u22c6 + S\u22c6 \u2212 SW) \u2264 \u2016S\u22c6 \u2212 SW\u2016 \u2264 0.01\u03c3\u22c6r\nThis gives \u03c3r(M \u22c6+S\u22c6\u2212SW) \u2265 15\u03c3r+1(M\u22c6+S\u22c6\u2212SW). Given \u2016\u2207fW(W)\u2016F = \u2016\u2207f(W)\u2016F \u2264 \u01eb, by Lemma 23, we know either \u03bbmin(\u22072fW(W)) \u2264 \u22120.2\u03c3\u22c6r or minR\u22a4R=RR\u22a4=I \u2016W \u2212W\u2020R\u2016F \u2264 20\u01eb\u03c3\u22c6r where U \u2020 = X\u2020(D\u2020) 1 2 , V\u2020 = Y\u2020(D\u2020) 1\n2 andX\u2020D\u2020Y\u2020\u22a4 is the best rank r-approximation ofM\u22c6+S\u22c6\u2212SW. By Lemma 24, we immediately have \u2016\u2206\u2016F \u2264 10 3\u01eb \u221a \u03ba\u22c6\n\u03c3\u22c6r , which finishes the proof.\nB.3 Supporting Lemmas for robust PCA\nIn the proof of Lemma 24, we used several supporting lemmas. We now prove them one by one. The first is a classical result from matrix perturbations.\nLemma 25. Let M\u2020 be the top-r SVD of matrixM\u22c6 + S \u2208 Rd1\u00d7d2 whereM\u22c6 is rank r. Then we have:\n\u2016M\u2020 \u2212M\u22c6\u2016F \u2264 2\u2016S\u2016F\nProof. By triangle inequality:\n\u2016M\u22c6 \u2212M\u2020\u2016F \u2264\u2016M\u22c6 + S\u2212M\u2020\u2016F + \u2016S\u2016F\nFor the second term, by the factM\u22c6 is rank r, the definition ofM\u2020 and Weyl\u2019s inequality:\n\u2016M\u22c6 + S\u2212M\u2020\u20162F = d \u2211\ni=r+1\n\u03c32i (M \u22c6 + S) \u2264\nd \u2211\ni=r+1\n(\u03c3r+1(M \u22c6) + \u03c3i\u2212r(S)) 2 = d \u2211\ni=r+1\n\u03c32i\u2212r(S) \u2264 \u2016S\u20162F\nThis finishes the proof.\nNext we show how to bound the norm of matrixM\u2212M\u22c6 restricted to a sparse set \u2126.\nLemma 26. Let M\u22c6 = U\u22c6V\u22c6\u22a4 are both \u00b5-incoherent matrix, M = UV\u22a4 satisfies maxi \u2016e\u22a4i U\u20162 \u2264 \u00b5r\u2016M \u22c6\u2016\nd1 and\nmaxj \u2016e\u22a4j V\u20162 \u2264 \u00b5r\u2016M \u22c6\u2016 d2 and \u2126 has at most \u03b1 fraction of non-zero entries in each row/column. Then:\n\u2016M\u2212M\u22c6\u20162\u2126 \u2264 4\u03b1\u00b5r\u2016M\u22c6\u2016(\u2016\u2206U\u20162F + \u2016\u2206V\u20162F)\nwhere \u2206U = U\u2212U\u22c6, \u2206V = V \u2212V\u22c6.\nProof. Then for any (i, j), since M,M\u22c6 are both \u00b5-incoherent, we have:\n|(M \u2212M\u22c6)(i,j)| =|(U\u2206\u22a4V +\u2206UV\u22c6\u22a4)(i,j)| \u2264 \u2016e\u22a4i U\u2016\u2016e\u22a4j \u2206V\u2016+ \u2016e\u22a4i \u2206U\u2016\u2016e\u22a4j V\u22c6\u2016\n\u2264 \u221a \u2016M\u22c6\u2016\u00b5r d1 \u2016e\u22a4j \u2206V\u2016+ \u221a \u2016M\u22c6\u2016\u00b5r d2 \u2016e\u22a4i \u2206U\u2016\nTherefore,\n\u2016M\u22c6 \u2212M\u20162\u2126 \u22642 \u2211 (i,j)\u2208\u2126 \u2016M\u22c6\u2016\u00b5r d1 \u2016e\u22a4j \u2206V\u20162 + \u2016M\u22c6\u2016 \u00b5r d2 \u2016e\u22a4i \u2206U\u20162\n\u22644\u03b1\u00b5r\u2016M\u22c6\u2016(\u2016\u2206U\u20162F + \u2016\u2206V\u20162F)\nNext, we show for any fixed sparse estimator S, if M\u22c6 is incoherent, the top-r SVD ofM\u22c6 + S\u22c6 \u2212 S will also be incoherent. Thus, sparse matrix will not interfere incoherence in this sense.\nLemma 27. For any S \u2208 S\u03b3\u03b1, let M\u2020 be the top-r SVD of M\u22c6 + S\u22c6 \u2212 S, and the SVD of M\u2020 to be XDY\u22a4, if \u03b3\u03b1 \u00b7 \u00b5r \u00b7 \u03ba\u22c6 \u2264 11000 , then we have:\nmax i\n\u2016e\u22a4i X\u20162 \u2264 4 \u00b5r(\u03ba\u22c6)4\nd1 and max j \u2016e\u22a4j Y\u20162 \u2264 4\n\u00b5r(\u03ba\u22c6)4\nd2\nwhere condition number \u03ba\u22c6 = \u03c31(M \u22c6)/\u03c3r(M \u22c6).\nProof. SinceXDY\u22a4 is the top r SVD ofM\u22c6 + S\u22c6 \u2212 S, we have:\n(M\u22c6 + S\u22c6 \u2212 S)(M\u22c6 + S\u22c6 \u2212 S)\u22a4X = XD2\nTherefore, for any i \u2208 [d], because S,S\u22c6 has at most \u03b3\u03b1 fraction non-zero entries in each row, and \u2016S\u22c6\u2016\u221e \u2264 2 \u00b5r\u03c3 \u22c6 1\u221a d1d2 and S \u2208 S\u03b3\u03b1, we have:\n\u03c32r(D)\u2016e\u22a4i X\u2016 \u2264\u2016e\u22a4i XD2\u2016 = \u2016e\u22a4i (M\u22c6 + S\u22c6 \u2212 S)(M\u22c6 + S\u22c6 \u2212 S)\u22a4X\u2016 \u2264(\u2016e\u22a4i M\u22c6\u2016+ \u2016e\u22a4i (S\u22c6 \u2212 S)\u2016)\u2016M\u22c6 + S\u22c6 \u2212 S\u2016\n\u2264 [\u221a \u00b5r\nd1 \u03c3\u22c61 +\n\u221a\n\u03b3\u03b1d2\n]\n(\u2016S\u22c6\u2016\u221e + \u2016S\u2016\u221e)\u2016D\u2016\n\u2264 \u221a \u00b5r\nd1 \u03c3\u22c61 (1 + 5\n\u221a \u03b3\u03b1 \u00b7 \u00b5r) \u2016D\u2016 \u2264 1.2 \u221a \u00b5r\nd1 \u03c3\u22c61 \u00b7 \u2016D\u2016\nOn the other hand, by Lemma 28 and Weyl\u2019s inequality, we also have\n\u2016S\u22c6 \u2212 S\u2016 \u2264\u2016S\u22c6\u2016+ \u2016S\u2016 \u2264 \u03b3\u03b1 \u221a d1d2(\u2016S\u22c6\u2016\u221e + \u2016S\u2016\u221e) \u2264 0.1\u03c3\u22c6r \u2016D\u2016 \u2264\u2016M\u2016+ \u2016S\u22c6 \u2212 S\u2016 \u2264 1.1\u03c3\u22c61\n\u03c3r(D) \u2265\u03c3r(M\u22c6)\u2212 \u2016S\u22c6 \u2212 S\u2016 \u2265 0.9\u03c3\u22c6r Therefore, in sum, we have:\n\u2016e\u22a4i X\u2016 \u2264 1.2 \u221a \u00b5r\nd1 \u03c3\u22c61 \u00b7 \u2016D\u2016 \u03c32r (D) \u2264 2 \u221a \u00b5r d1 (\u03ba\u22c6)2\nBy symmetry, we can also prove it for \u2016e\u22a4j Y\u2016 with any j \u2208 [d2].\nWe also need to upper bound the spectral norm of sparse matrix S.\nLemma 28. For any sparse matrix S that can only has at most \u03b1 fraction non-zero entries in each row/column, we have:\n\u2016S\u2016 \u2264 \u03b1 \u221a d1d2\u2016S\u2016\u221e\nProof. Let \u2126 be the support of matrix S, and \u03b2 = \u221a\nd1 d2 , we have:\n\u2016S\u2016 = sup (x,y):\u2016x\u2016=1,\u2016y\u2016=1\nx\u22a4Sy = \u2211\n(i,j)\u2208\u2126 xi(S)ijyj \u2264\n1\n2\n\u2211\n(i,j)\u2208\u2126 \u2016S\u2016\u221e(\u03b2x2i +\n1 \u03b2 y2j ) \u2264 \u03b1 \u221a d1d2\u2016S\u2016\u221e\nThen, we show a crucial property of the optimal S \u2208 S\u03b3\u03b1:\nLemma 29. For any matrix A, let Sopt = argminS\u2208S\u03b3\u03b1 \u2016A\u2212 S\u20162F, and \u2126 be the support of S, then for any (i, j) \u2208 [d1]\u00d7 [d2]\u2212 \u2126, we have:\n|A(i,j)| \u2264 |A(\u03b3\u03b1d1)(i,\u00b7) |+ |A (\u03b3\u03b1d2) (\u00b7,j) |\nwhereA (k) (i,\u00b7) is the k-th largest element (in terms of absolute value) in i-th row ofA andA (k) (\u00b7,j) the k-th largest element (in terms of absolute value) in j-th column of A.\nProof. Assume the contradiction that in optimal solution S there is a pair (i, j) \u2208 [d1]\u00d7 [d2]\u2212 \u2126 such that\n|A(i,j)| > |A(\u03b3\u03b1d1)(i,\u00b7) |+ |A (\u03b3\u03b1d2) (\u00b7,j) |\nIf row i has exactly \u03b3\u03b1d1 elements in\u2126, let e1 = (i, j \u2032) be the smallest entry in row i (j\u2032 = argminz:(i,z)\u2208\u2126 |A(i,z)|),\nclearly A(i,j\u2032) \u2264 A(\u03b3\u03b1d1)(i,\u00b7) . If row i has fewer elements we just let e1 be empty. Similarly, if column j has exactly \u03b3\u03b1d2 elements in \u2126, let e2 = (i\n\u2032, j) be the smallest entry in the column (i\u2032 = argminz:(z,j)\u2208\u2126 |A(z,j)|), we also have A(i\u2032,j) \u2264 A(\u03b3\u03b1d2)(\u00b7,j) .\nNow we can add (i, j) to \u2126, and remove e1 and e2. Call the resulting matrix S \u2032. This clearly does not violate\nthe support constraint. Let q(x) = (x \u2212 max{0, x \u2212 2 \u00b5r\u03c3 \u22c6 1\u221a\nd1d2 })2, this function is monotone for x > 0 and satisfies\nq(x) + q(y) \u2264 q(x+ y). Now the difference we get from changing S to S\u2032 is\n\u2016A\u2212 S\u2032\u20162F = \u2016A\u2212 S\u20162F \u2212 q(|A(i,j)|) + q(|Ae1 |) + q(|Ae2 |) < \u2016A\u2212 S\u20162F .\nThis contradicts with the fact that S was the optimal. Therefore there cannot be such an entry (i, j).\nFinally, by using above lemmas, we can show how to bound the difference between optimal S and true sparse\nmatrix S\u22c6, which is a key step in Lemma 24.\nLemma 30. For any matrix A \u2208 Rd1\u00d7d2 , let S\u22c6 \u2208 S\u03b1 and\nSopt = argmin S\u2208S\u03b3\u03b1 \u2016S\u2212 S\u22c6 +A\u20162F\nLet \u2126 be the support of Sopt and \u2126\u22c6 be the support of S\u22c6, we will have:\n\u2016Sopt \u2212 S\u22c6\u20162F \u2264 2\u2016A\u20162\u2126\u22c6\u222a\u2126 + 8\n\u03b3 \u2212 1\u2016A\u2016 2 F\nProof. Clearly, the support of Sopt \u2212 S\u22c6 must be a subset of \u2126 \u222a\u2126\u22c6. Therefore, we have:\n\u2016Sopt \u2212 S\u22c6\u20162F = \u2016Sopt \u2212 S\u22c6\u20162\u2126 + \u2016Sopt \u2212 S\u22c6\u20162\u2126\u22c6\u2212\u2126\nFor the first term, since Sopt is defined as minimizer over S\u03b3\u03b1, we know for (i, j) \u2208 \u2126:\n(Sopt)(i,j) = max\n{\nmin\n{\n(S\u22c6 \u2212A)(i,j), 2 \u00b5r\u03c3\u22c61\u221a d1d2\n}\n, \u2212 2 \u00b5r\u03c3 \u22c6 1\u221a\nd1d2\n}\nBy assumption we know S\u22c6 \u2208 S\u03b1 thus \u2016S\u22c6\u2016\u221e \u2264 2 \u00b5r\u03c3 \u22c6 1\u221a\nd1d2 , this gives |(Sopt \u2212 S\u22c6)(i,j)| \u2264 |(A)(i,j)| thus:\n\u2016Sopt \u2212 S\u22c6\u20162\u2126 \u2264 \u2016A\u20162\u2126 For the second term, by triangle inequality, we have:\n\u2016Sopt \u2212 S\u22c6\u2016\u2126\u22c6\u2212\u2126 = \u2016S\u22c6\u2016\u2126\u22c6\u2212\u2126 \u2264 \u2016S\u22c6 \u2212A\u2016\u2126\u22c6\u2212\u2126 + \u2016A\u2016\u2126\u22c6\u2212\u2126 By Lemma 29, we have for any (i, j) \u2208 \u2126\u22c6 \u2212 \u2126:\n|(S\u22c6 \u2212A)(i,j)|2 \u22642|(S\u22c6 \u2212A)(\u03b3\u03b1d1)(i,\u00b7) |2 + 2|(S\u22c6 \u2212A) (\u03b3\u03b1d2) (\u00b7,j) |2\n\u22642|(A)((\u03b3\u22121)\u03b1)(i,\u00b7) |2 + 2|(A) ((\u03b3\u22121)\u03b1) (\u00b7,j) |2 \u2264 2 (\u03b3 \u2212 1)\u03b1 ( \u2016e\u22a4i A\u20162 + \u2016Aej\u20162 )\nwhere the second inequality used the fact that S\u22c6 has at most \u03b1 non-zero entries each row/column. Then, we have:\n\u2016S\u22c6 \u2212A\u20162\u2126\u22c6\u2212\u2126 \u2264 \u2211\n(i,j)\u2208\u2126\u22c6\u2212\u2126\n2\n(\u03b3 \u2212 1)\u03b1 ( \u2016e\u22a4i A\u20162 + \u2016Aej\u20162 )\n\u2264 4 \u03b3 \u2212 1\u2016A\u2016 2 F\nTherefore, in conclusion, we have:\n\u2016Sopt \u2212 S\u22c6\u20162F \u2264\u2016Sopt \u2212 S\u22c6\u20162\u2126 + \u2016Sopt \u2212 S\u22c6\u20162\u2126\u22c6\u2212\u2126 \u2264\u2016A\u20162\u2126 + 2\u2016A\u20162\u2126\u22c6\u2212\u2126 + 2\u2016S\u22c6 \u2212A\u20162\u2126\u22c6\u2212\u2126 \u22642\u2016A\u20162\u2126\u22c6\u222a\u2126 + 8\n\u03b3 \u2212 1\u2016A\u2016 2 F"}, {"heading": "C Matrix Sensing with Noise", "text": "In this section we demonstrate how to handle noise using our framework. The key idea here is to consider the noise as a perturbation to the original objective function and use Q(U) (originally the regularizer) to also capture the noise.\nC.1 Symmetric case\nHere, we assume in each observation, instead of observing the exact value bi = \u3008Ai,M\u22c6\u3009we observe bi = \u3008Ai,M\u22c6\u3009+ ni. Here ni is i.i.d N (0, \u03c32). Recall the objective function in Section 4, we now have:\nf(M) = 1\nm\nm \u2211\ni=1\n(\u3008M\u2212M\u22c6,Ai\u3009+ ni)2 (26)\nDefine Q(U) = f(UU\u22a4) \u2212 12 (UU\u22a4 \u2212M\u22c6) : H : (UU\u22a4 \u2212 M\u22c6) to be the perturbation, we can write out the non-convex objective\nmin U\u2208Rd\u00d7r\n1 2 (UU\u22a4 \u2212M\u22c6) : H : (UU\u22a4 \u2212M\u22c6) +Q(U). (27)\nNow we can use the same framework. Again, since for matrix sensing we have the RIP property, we do not need\nthe first step to restrict to special low rank matrices. Using our approach we can get\nTheorem 31. For objective Equation (27), suppose the sensing matrices {Ai}\u2019s satisfy (2r, 1/10)-RIP, with high probability all points satisfy first and second order optimality condition must satisfy\n\u2016UU\u22a4 \u2212M\u22c6\u2016F \u2264 O(\u03c3 \u221a dr logm\nm ).\nProof. Using the same proof as Theorem 8, we know\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(N\u2212N\u22c6) : H : (N\u2212N\u22c6) \u2265 \u22120.5\u2016N\u2212N\u22c6\u20162F . We then bound the contribution fromQ.\nQ(U) =\u2212 2 m\nm \u2211\ni=1\n(\u3008M \u2212M\u22c6,Ai\u3009ni) + 1\nm\nm \u2211\ni=1\n(ni) 2\n\u3008\u2207Q(U),\u2206\u3009 =\u2212 2 m\nm \u2211\ni=1\n(\u3008U\u2206\u22a4 +\u2206U\u22a4,Ai\u3009ni)\n\u2206 : \u22072Q(U) : \u2206 =\u2212 4 m\nm \u2211\ni=1\n(\u3008\u2206\u2206\u22a4,Ai\u3009ni)\nTherefore,\n[\u2206 : \u22072Q(U) : \u2206\u2212 4\u3008\u2207Q(U),\u2206\u3009]\n\u2264\u2212 4 m\nm \u2211\ni=1\n(\u3008\u2206\u2206\u22a4,Ai\u3009ni) + 8\nm\nm \u2211\ni=1\n(\u3008U\u2206\u22a4 +\u2206U\u22a4,Ai\u3009ni)\n= 4\nm\nm \u2211\ni=1\n(\u3008M \u2212M\u22c6,Ai\u3009 \u00b7 ni) + 4\nm\nm \u2211\ni=1\n(\u3008U\u2206\u22a4 +\u2206U\u22a4,Ai\u3009 \u00b7 ni).\nHere the last step follows from M \u2212 M\u22c6 + \u2206\u2206\u22a4 = U\u2206\u22a4 + \u2206U\u22a4. Intuitively, ni is random and should not have large correlation with any fixed vector. We formalize this in Lemma 34. Using this lemma, we know\n| 4 m\nm \u2211\ni=1\n(\u3008M \u2212M\u22c6,Ai\u3009ni)| \u2264 4\u03c3 \u221a dr logm\nm \u2016M\u2212M\u22c6\u2016F\n| 4 m\nm \u2211\ni=1\n(\u3008U\u2206\u22a4 +\u2206U\u22a4,Ai\u3009ni| \u2264 4\u03c3 \u221a dr logm\nm \u2016U\u2206\u22a4 +\u2206U\u22a4\u2016F\n\u2264 4(1 + \u221a 2)\u03c3 \u221a dr\nm \u2016M\u2212M\u22c6\u2016F\nHere the last inequality follows from \u2016U\u2206\u22a4 +\u2206U\u22a4\u2016F \u2264 \u2016M\u2212M\u22c6\u2016F + \u2016\u2206\u2206\u22a4\u2016F \u2264 (1 + \u221a 2)\u2016M\u2212M\u22c6\u2016F (by Lemma 40). Now using the main Lemma 7, we know\n\u2206 : \u22072f(U) : \u2206 \u2264\u2212 1 2 \u2016M\u2212M\u22c6\u20162F\n+ (8 + 4 \u221a 2)\u03c3 \u221a dr logm\nm \u2016M\u2212M\u22c6\u2016F .\nIf the current point satisfy the second order optimality condition we must have\n\u2016M\u2212M\u22c6\u2016F \u2264 O(\u03c3 \u221a dr logm\nm ).\nNote that this bound matches the intuitive bound from the VC-dimension of rank-r matrices.\nC.2 Asymmetric case\nFor the asymmetric case, the proof is again almost identical. We use the same noise model where the observation bi = \u3008Ai,M\u22c6\u3009 + ni where ni \u223c N(0, \u03c32). We also use the same notations as in Definition 7. Let Q(W) = Q(U,V) = 2\nm \u2211m i=1[(\u3008M\u2212M\u22c6,Ai\u3009+ ni)2 \u2212 (\u3008M \u2212M\u22c6,Ai\u3009)2], we have the objective function\nf(U,V) = 1\n2 (N\u2212N\u22c6) : H : (N\u2212N\u22c6) +Q(W). (28)\nAgain by bounding the gradient and Hessian for Q(W) we get the following\nTheorem 32. For objective Equation (28), suppose the sensing matrices {Ai}\u2019s satisfy (2r, 1/20)-RIP, let d = d1+d2, with high probability all points satisfy first and second order optimality condition must satisfy\n\u2016UV\u22a4 \u2212M\u22c6\u2016F \u2264 O(\u03c3 \u221a dr logm\nm ).\nProof. Again using the same proof as Theorem 3, we know\n\u2206\u2206\u22a4 : H : \u2206\u2206\u22a4 \u2212 3(M\u2212M\u22c6) : H : (M \u2212M\u22c6) \u2265 \u22120.5\u2016M\u2212M\u22c6\u20162F .\nWe then bound the contribution fromQ.\nQ(W) =\u2212 8 m\nm \u2211\ni=1\n(\u3008M\u2212M\u22c6,Ai\u3009ni) + 4\nm\nm \u2211\ni=1\n(ni) 2\n\u3008\u2207Q(W),\u2206\u3009 =\u2212 8 m\nm \u2211\ni=1\n(\u3008U\u2206\u22a4V +\u2206UV\u22a4,Ai\u3009ni)\n\u2206 : \u22072Q(W) : \u2206 =\u2212 16 m\nm \u2211\ni=1\n(\u3008\u2206U\u2206\u22a4V ,Ai\u3009ni)\nLetBi be the (d1 + d2)\u00d7 (d1 + d2) matrix whose diagonal blocks are 0, and off diagonal blocks are equal toAi andA\u22a4i respectively, we have\n[\u2206 : \u22072Q(W) : \u2206\u2212 4\u3008\u2207Q(W),\u2206\u3009]\n\u2264\u2212 16 m\nm \u2211\ni=1\n(\u3008\u2206\u2206\u22a4,Bi\u3009ni) + 32\nm\nm \u2211\ni=1\n(\u3008W\u2206\u22a4 +\u2206W\u22a4,Bi\u3009ni)\n= 16\nm\nm \u2211\ni=1\n(\u3008N\u2212N\u22c6,Bi\u3009ni) + 16\nm\nm \u2211\ni=1\n(\u3008W\u2206\u22a4 +\u2206W\u22a4,Bi\u3009ni).\nNow we can use Lemma 34 again to bounding the noise terms:\n| 8 m\nm \u2211\ni=1\n(\u3008N\u2212N\u22c6,Bi\u3009ni)| \u2264 8\u03c3 \u221a dr logm\nm \u2016M\u2212M\u22c6\u2016F\n| 8 m\nm \u2211\ni=1\n(\u3008W\u2206\u22a4 +\u2206W\u22a4,Bi\u3009ni| \u2264 8\u03c3 \u221a dr logm\nm \u2016U\u2206\u22a4V +\u2206UV\u22a4\u2016F\n\u2264 8\u03c3 \u221a dr logm\nm \u2016W\u2206\u22a4 +\u2206W\u22a4\u2016F\n\u2264 8(1 + \u221a 2)\u03c3 \u221a dr\nm \u2016N\u2212N\u22c6\u2016).\nTherefore the Hessian at \u2206 direction is equal to:\n\u2206 : \u22072f(W) : \u2206 \u2264\u2212 1 2 \u2016N\u2212N\u22c6\u20162F + (16 + 8\n\u221a 2)\u03c3 \u221a dr logm\nm \u2016N\u2212N\u22c6\u2016F .\nWhen the point satisfies the second order optimality condition we have\n\u2016N\u2212N\u22c6\u2016F \u2264 O(\u03c3 \u221a dr logm\nm ).\nIn particular,M\u2212M\u22c6 is a submatrix ofN\u2212N\u22c6, therefore \u2016M\u2212M\u22c6\u2016F \u2264 O(\u03c3 \u221a dr logm m )."}, {"heading": "D Proof Sketch for Running Time", "text": "In this section we sketch the proof for Corollary 17.\nCorollary 17. LetR be the Frobenius norm of the initial pointsU0,V0, a saddle-avoiding local search algorithm can find a point \u01eb-close to global optimal for matrix sensing (10)(3), matrix completion (11)(4) in poly(R, 1/\u01eb, d, \u03c3\u22c61 , 1/\u03c3 \u22c6 r ) iterations. For robust PCA (12)(6), alternating between a saddle-avoiding local search algorithm and computing optimal S \u2208 S\u03b3\u03b1 will find a point \u01eb-close to global optimal in poly(R, 1/\u01eb, d, \u03c3\u22c61 , 1/\u03c3\u22c6r ) iterations.\nThe full proof require some additional analysis depending on the particular algorithm used, and is highly dependent\non the detailed proofs of the guarantees, so we only give a proof sketch here.\nOur geometric results show that for small enough \u01eb\u2032, the objective functions are (\u01eb\u2032, \u03b3, C\u01eb\u2032) strict-saddle where \u03b3 and C may depend polynomially on (\u03c3\u22c61 , \u03c3 \u22c6 r ). Choose \u01eb\n\u2032 = \u01eb/C, we know for each point, either it has a gradient at least \u01eb\u2032, or the Hessian has an eigenvalue smaller than \u2212\u03b3, or \u2016\u2206\u2016F \u2264 \u01eb. In the first two cases, by Definition 8 we know saddle-avoiding algorithm can decrease the function value by an inverse polynomial factor in polynomial time. By the radius of the initial solution, the difference in function value between the original solution and optimal solution is bounded by poly(R, \u03c3\u22c61 , d), so after a polynomial number of iterations we can no longer decrease function value and must be in the third case (where \u2016\u2206\u2016F \u2264 \u01eb).\nSmoothness andHessian Lipschitz The objective funcitons we work with are mostly polynomials thus both smooth and Hessian Lipschitz. The regularizerswe add also tried to make sure at least both smoothness and Hessians Lipschitz are satisfied. However, the objective functions are still not very smooth or Hessian-Lipschitz especially in the region when then the norm of (U,V) is very large. This is because the polynomials are of degree more than 2 and in general the smoothness and Hessian-Lipschitzness parameters (l, \u03c1) depend on the norm of the current point (U,V). It is not hard to show that when the solution is constrained into a ball of radiusR, the parameters l, \u03c1 are all poly(R). Therefore to complete the proof we need to show that the intermediate steps of the algorithms cannot escape from a large ball. In fact, for all the known algorithms, on our objective functions the following is true\nLemma 33. For current saddle avoiding algorithms (including cubic regularization [Nesterov and Polyak, 2006], perturbed gradient descent [Jin et al., 2017] ) There exists a radiusR that is polynomial in problem parameters, such that if initially \u2016U\u2016F + \u2016V\u2016F = R0 \u2264 R, then with high probability all the iterations will have \u2016U\u2016F + \u2016V\u2016F \u2264 2R.\nThe proof of this lemma is mostly calculations (and observing the fact that when U,V are both very large, the gradient will essentially point to 0), as an example this is done for matrix factorization in [Jin et al., 2017]. We omit the proof in this paper.\nNote that our geometric results for matrix sensing does not depend on the dimension. If we can prove a bound on R that is independent of the dimension d, by recent result in [Jin et al., 2017], we can get algorithms whose number of iterations depend only on log d for matrix sensing.\nHandling Robust-PCA For robust PCA, the objective function is only pseudo strict-saddle (see Definition 5).\nIn order to turn the geometric property to an algorithm, the first observation is that the optimal S forU,V can be found in polynomial time: The problem of finding the optimal S can be formulated as a weighted bipartite matching problem where one part corresponds to the rows, the other part corresponds to the columns, and the value corresponds to the improvement in objective function when we add (i, j) into the support. According to the definition of S, each row/column can be matched a limited number of times. This problem can be solved by converting it to max-flow, and standard analysis shows that there exists an optimal integral solution.\nNext we view the robust PCA objective function of form f(U,V) = minS\u2208S\u03b3\u03b1 g(U,V;S). We show that alternating between saddle-avoiding local search and optimizing S over S\u03b3\u03b1 will allow us to get the desired guarantee. For a point U,V, if it is not close enough to the global optimal solution, we can fix the optimal S for U,V and study g(U,V;S). First, we know for this optimal choice of S, the gradient of g(U,V;S) over (U,V) is the same as gradient of f(U,V). Then, by Theorem 15 / Theorem 5, we know either the gradient of g(U,V;S) is large or the Hessian of g(U,V;S) has an eigenvalue at most \u2212\u2126(\u03c3\u22c6r ). By the guarantee of saddle-avoiding algorithms in polynomial number of steps we can findU\u2032,V\u2032 such that the objective function g(U\u2032,V\u2032;S)will decrease by a inverse polynomial. After that, replacing S with S\u2032 (optimal forU\u2032,V\u2032) cannot increase function value, so in polynomial time we found a new point such that f(U\u2032,V\u2032) \u2264 f(U,V) \u2212 \u03b4 where \u03b4 is at least an inverse polynomial. This procedure cannot be repeated by more than polynomial number of times (because the function value cannot decrease below the optimal value), so the algorithm finds an approximate optimal point in polynomial time."}, {"heading": "E Concentrations", "text": "In this section we summarize the concentration inequalities we use for different problems.\nE.1 Matrix Sensing\nDefinition 9 (Restrict Isometry Property). Measurement A ({Ai}) satisfies (r, \u03b4r)-Restrict Isometry Property (RIP) if for any matrixX with rank r, we have:\n(1\u2212 \u03b4r)\u2016X\u20162F \u2264 1\nm\nm \u2211\ni=1\n\u3008Ai,X\u30092 \u2264 (1 + \u03b4r)\u2016X\u20162F\nIn the case of Gaussian measurement, standard analysis shows whenm = O(dr \u03b42 ), we haveA satisfying (r, \u03b4)-RIP\ncondition with probability at least 1\u2212 e\u2126(d). (Candes and Plan [2011], Theorem 2.3) We need the follow inequality for handling noise.\nLemma 34. Suppose the set of sensing matrices A1,A2, ...,Am satisfy the (2r, \u03b4)-RIP condition, let n1, n2, ..., nm be iid. Gaussian N(0, \u03c32), then with high probability for any matrixM of rank at most r, we have\n| 1 m\nm \u2211\ni=1\nni\u3008Ai,M\u3009| \u2264 O ( \u03c3 \u221a dr logm\nm \u2016M\u2016F\n)\n.\nProof. Since the LHS is linear inM we focus on matrices with \u2016M\u2016F = 1. Let X be an \u01eb-net for rank-r matrices with Frobenius norm 1. By standard constructions we know log |X | \u2264 dr log(dr/\u01eb). We will set \u01eb = 1/m so log(dr/\u01eb) = O(logm) (m is at least dr for RIP condition). Now, for any matrixM \u2208 X , we know 1\nm \u2211m i=1 ni\u3008Ai,M is just a Gaussian random variable with variance at most \u03c32(1 + \u03b4)/m.\nTherefore, the probability that it is larger than \u03c3 \u221a\ndr logm m is at most exp(\u2212C\u2032dr logm). When C is a large enough constant we can apply union bound, and we know for everyM \u2208 X ,\n| 1 m\nm \u2211\ni=1\nni\u3008Ai,M\u3009| \u2264 O ( \u03c3 \u221a dr logm\nm \u2016M\u2016F\n)\n.\nOn the other hand, with high probability the norm of the vector n \u2208 Rm is O(\u03c3\u221am). Suppose M is not in X , let M\u2032 be the closest matrix in X , let zi = \u3008Ai,M \u2212 M\u2032\u3009, then we know the norm of zi is at most 1+\u03b4m (again by RIP property). Now we know\n| 1 m\nm \u2211\ni=1\nni\u3008Ai,M\u3009| \u2264 | 1\nm\nm \u2211\ni=1\nni\u3008Ai,M\u2032\u3009|+ \u3008z,n\u3009 \u2264 O ( \u03c3 \u221a dr logm\nm \u2016M\u2016F\n)\n.\nE.2 Matrix Completion\nFor matrix completion, we need different concentration inequalities for different kinds of matrices. The first kind of matrix lies in a tangent space and is proved in Candes and Recht [2012].\nLemma 35. Candes and Recht [2012] Let subspace\nT = {M \u2208 Rd1\u00d7d2 |M = U\u22c6X\u22a4 +YV\u22c6\u22a4, for someX \u2208 Rd1\u00d7r,Y \u2208 Rd2\u00d7r}.\nfor any \u03b4 > 0, as long as sample rate p \u2265 \u2126( \u00b5r \u03b42d log d), we will have:\n\u20161 p PT P\u2126PT \u2212 PT \u2016 \u2264 \u03b4\nFor arbitrary low rank matrix, we use the following lemma which comes from graph theory.\nLemma 36. Suppose \u2126 \u2282 [d1] \u00d7 [d2] is the set of edges of a random bipartite graph with (d1, d2) nodes, where any pair of nodes on different side is connected with probability p. Let d = max d1, d2,then there exists universal constant c1, c2, for any \u03b4 > 0 so that if p \u2265 c1 log dmin{d1,d2} , then with probability at least 1\u2212 d \u22124, we have for any x,y \u2208 Rd:\n1\np\n\u2211\n(i,j)\u2208\u2126 xiyj \u2264 \u2016x\u20161\u2016y\u20161 + c2\n\u221a\nd p \u2016x\u20162\u2016y\u20162\nProof. Let A be the adjacency matrix of the graph. Clearly E[A] = pJ where J is the all 1\u2019s matrix. Let Z = A\u2212E[A]. The matrix Z has independent entries with expectation 0 and variance p(1\u2212 p). By randommatrix theory, we know when p \u2265 c1 log dmin{d1,d2} , with probability at least 1 \u2212 d \u22124, we have \u2016Z\u2016 = \u2016A\u2212 E[A]\u2016 \u2264 c2 \u221a pd [Lata\u0142a, 2005]1. Now for any vectors x,y simultaneously, we have\n1\np\n\u2211\n(i,j)\u2208\u2126 xiyj =\n1 p x\u22a4Ay = 1 p x\u22a4(pJ+ Z)y\n\u2264\u3008x,1\u3009\u3008y,1\u3009 + c2\n\u221a\nd p \u2016x\u20162\u2016y\u20162 \u2264 \u2016x\u20161\u2016y\u20161 + c2\n\u221a\nd p \u2016x\u20162\u2016y\u20162.\nAbove lemma immediately implies following:\nLemma 37. Let d = max{d1, d2}. There exists universal constant c1, c2, for any \u03b4 > 0 so that if p \u2265 c1 log dmin{d1,d2} , then with probability at least 1\u2212 12d\u22124, we have for any matricesX,Y \u2208 Rd\u00d7r:\n1 p \u2016XY\u22a4\u20162\u2126\u0304 \u2264 \u2016X\u20162F\u2016Y\u20162F + c2\n\u221a\nd p \u2016X\u2016F\u2016Y\u2016F \u00b7max i \u2016e\u22a4i X\u2016 \u00b7max j \u2016e\u22a4j Y\u2016\n1The high probability result follows directly from Talagrand\u2019s inequality.\nProof. 1\np \u2016XY\u22a4\u20162\u2126 =\n1\np\n\u2211\n(i,j)\u2208\u2126 \u2016e\u22a4i X\u20162\u2016e\u22a4j Y\u20162\nThe remaining follows from Lemma 36.\nOn the other hand, for all low-rank matrices we also have the following (which is tighter for incoherent matrices).\nLemma 38. Ge et al. [2016] Let d = max{d1, d2}, then with at least probability 1\u2212 e\u2126(d) over random choice of \u2126, we have for any rank 2r matricesA \u2208 Rd1\u00d7d2:\n\u2223 \u2223 \u2223 \u2223 1\np \u2016P\u2126(A)\u20162\u2126 \u2212 \u2016A\u20162F\n\u2223 \u2223 \u2223 \u2223 \u2264 O(dr log d p \u2016A\u20162\u221e + \u221a dr log d p \u2016A\u2016F\u2016A\u2016\u221e)\nAlthough Ge et al. [2016] stated the symmetric version, and we need the asymmetric version here, the proof in\nGe et al. [2016] works directly. In fact, they first proved the asymmetric case in the proof.\nFinally, for a matrix with each entry randomly sampled independently with small probability p, next lemma says with high probablity, no row can have too many non-zero entries.\nLemma 39. Let \u2126i denote the support of \u2126 on i-th row, let d = max{d1, d2}. Assume pd2 \u2265 log(2d), then with at least probability 1\u2212 1/poly(d) over random choice of \u2126, we have for all i \u2208 [d1] simultaneously:\n|\u2126i| \u2264 O(pd2)\nProof. This follows directly from Chernoff bound and union bound."}, {"heading": "F Auxiliary Inequalities", "text": "In this section, we provide some frequently used lemmas regarding matrices. Our first two lemmas lower bound \u2016UU\u22a4 \u2212YY\u22a4\u20162F by \u2016(U\u2212Y)(U \u2212Y)\u22a4\u20162F and \u2016U\u2212Y\u20162F.\nLemma 40. Let U andY be two d\u00d7 r matrices. Further let U\u22a4Y = Y\u22a4U be a PSD matrix. Then,\n\u2016(U\u2212Y)(U \u2212Y)\u22a4\u20162F \u2264 2\u2016UU\u22a4 \u2212YY\u22a4\u20162F\nProof. To prove this, we let \u2206 = U\u2212Y, and expand:\n\u2016UU\u22a4 \u2212YY\u22a4\u20162F =\u2016U\u2206\u22a4 +\u2206U\u22a4 \u2212\u2206\u2206\u22a4\u20162F =tr(2U\u22a4U\u2206\u22a4\u2206+ (\u2206\u22a4\u2206)2 + 2(U\u22a4\u2206)2 \u2212 4U\u22a4\u2206\u2206\u22a4\u2206)\n=tr(2U\u22a4(U\u2212\u2206)\u2206\u22a4\u2206+ ( 1\u221a 2 \u2206\u22a4\u2206\u2212\n\u221a 2U\u22a4\u2206)2 + 1\n2 (\u2206\u22a4\u2206)2)\n\u2265tr(2U\u22a4Y\u2206\u22a4\u2206+ 1 2 (\u2206\u22a4\u2206)2) \u2265 1 2 \u2016\u2206\u2206\u22a4\u20162F\nThe last inequality is due toU\u22a4Y is a PSD matrix.\nLemma 41. Let U andY be two d\u00d7 r matrices. Further let U\u22a4Y = Y\u22a4U be a PSD matrix. Then,\n\u03c3min(U \u22a4U)\u2016U\u2212Y\u20162F \u2264 \u2016(U\u2212Y)U\u22a4\u20162F \u2264\n1\n2( \u221a 2\u2212 1) \u2016UU\u22a4 \u2212YY\u22a4\u20162F\nProof. The left inequality is basic, we only need to prove right inequality. To prove this, we let \u2206 = U \u2212 Y, and expand:\n\u2016UU\u22a4 \u2212YY\u22a4\u20162F =\u2016U\u2206\u22a4 +\u2206U\u22a4 \u2212\u2206\u2206\u22a4\u20162F =tr(2U\u22a4U\u2206\u22a4\u2206+ (\u2206\u22a4\u2206)2 + 2(U\u22a4\u2206)2 \u2212 4U\u22a4\u2206\u2206\u22a4\u2206) =tr((4\u2212 2 \u221a 2)U\u22a4(U \u2212\u2206)\u2206\u22a4\u2206+ (\u2206\u22a4\u2206\u2212 \u221a 2U\u22a4\u2206)2 + 2( \u221a 2\u2212 1)U\u22a4U\u2206\u22a4\u2206)\n\u2265tr((4\u2212 2 \u221a 2)U\u22a4Y\u2206\u22a4\u2206+ 2( \u221a 2\u2212 1)U\u22a4U\u2206\u22a4\u2206) \u2265 2( \u221a 2\u2212 1)\u2016U\u2206\u22a4\u20162F\nThe last inequality is due toU\u22a4Y is a PSD matrix.\nNext we show the difference between matrices formed by swapping sigular spaces of M1 and M2 can be upper\nbounded by the difference betweenM1 andM2.\nLemma 42. Let M1,M2 \u2208 Rd1\u00d7d2 be two arbitrary matrices whose SVDs are U1D1V\u22a41 and U2D2V\u22a42 . Then we have: \u2016U1D1U\u22a41 \u2212U2D2U\u22a42 \u20162F + \u2016V1D1V\u22a41 \u2212V2D2V\u22a42 \u20162F \u2264 2\u2016M1 \u2212M2\u20162F Proof. Expand the Frobenius Norm out, we have LHS:\n\u2016U1D1U\u22a41 \u2212U2D2U\u22a42 \u20162F + \u2016V1D1V\u22a41 \u2212V2D2V\u22a42 \u20162F =2tr(D21 +D 2 2 \u2212U1D1U\u22a41 U2D2U\u22a42 \u2212V1D1V\u22a41 V2D2V\u22a42 )\nOn the other hand, we also have RHS:\n2\u2016M1 \u2212M2\u20162F = 2\u2016U1D1V\u22a41 \u2212U2D2V\u22a42 \u20162F =2tr(D21 +D 2 2 \u2212U1D1V\u22a41 V2D2U\u22a42 \u2212U2D2V\u22a42 V1D1U\u22a41 )\nLetA = D 1 2 1 U \u22a4 1 U2D 1 2 2 andB = D 1 2 1 V \u22a4 1 V2D 1 2 2 . We know to prove the lemma, we only need to show tr(AA \u22a4+\nBB\u22a4) \u2265 tr(AB\u22a4 +BA\u22a4). This is true because \u2016A\u2212B\u20162F \u2265 0, which finishes the proof."}], "references": [{"title": "Basis learning as an algorithmic primitive", "author": ["2016. Mikhail Belkin", "Luis Rademacher", "James Voss"], "venue": null, "citeRegEx": "Belkin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2016}, {"title": "Global optimality of local search for low rank matrix", "author": ["Srinadh Bhojanapalli", "Behnam Neyshabur", "Nathan Srebro"], "venue": null, "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "Tight oracle inequalities for low-rank matrix recovery from a minimal number", "author": ["Emmanuel J Candes", "Yaniv Plan"], "venue": null, "citeRegEx": "Candes and Plan.,? \\Q2012\\E", "shortCiteRegEx": "Candes and Plan.", "year": 2012}, {"title": "Stable signal recovery from incomplete and inaccurate", "author": ["Emmanuel J Candes", "Justin K Romberg", "Terence Tao"], "venue": null, "citeRegEx": "Candes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2009}, {"title": "Matrix rank minimization with applications", "author": ["Maryam Fazel"], "venue": "PhD thesis,", "citeRegEx": "Fazel.,? \\Q2002\\E", "shortCiteRegEx": "Fazel.", "year": 2002}, {"title": "Matrix completion has no spurious local minimum", "author": ["Rong Ge", "Jason D Lee", "Tengyu Ma"], "venue": null, "citeRegEx": "Ge et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ge et al\\.", "year": 2015}, {"title": "Understanding alternating minimization for matrix completion", "author": ["Moritz Hardt"], "venue": "Information Processing Systems,", "citeRegEx": "Hardt.,? \\Q2014\\E", "shortCiteRegEx": "Hardt.", "year": 2014}, {"title": "Low-rank matrix completion using alternating minimization", "author": ["Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q1933\\E", "shortCiteRegEx": "Jain et al\\.", "year": 1933}, {"title": "How to escape saddle points efficiently", "author": ["2013. Chi Jin", "Rong Ge", "Praneeth Netrapalli", "ShamMKakade", "andMichael I Jordan"], "venue": "Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "citeRegEx": "Jin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2013}, {"title": "Matrix completion from a few entries", "author": ["2017. Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": null, "citeRegEx": "Keshavan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2017}, {"title": "Matrix completion from noisy entries", "author": ["Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh"], "venue": "Theory, IEEE Transactions on,", "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Keshavan et al\\.", "year": 2010}, {"title": "The nonconvex geometry of low-rank matrix optimizations with general objective", "author": ["Qiuwei Li", "Gongguo Tang"], "venue": null, "citeRegEx": "Li and Tang.,? \\Q2005\\E", "shortCiteRegEx": "Li and Tang.", "year": 2005}, {"title": "Non-squarematrix sensing without", "author": ["2014. Dohyung Park", "Anastasios Kyrillidis", "Constantine Caramanis", "Sujay Sanghavi"], "venue": "pca. In Advances in Neural Information Processing Systems,", "citeRegEx": "Park et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Park et al\\.", "year": 2014}, {"title": "Complete dictionary recovery over the sphere I: Overview and the geometric", "author": ["Ju Sun", "Qing Qu", "John Wright"], "venue": "Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Sun et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2005}, {"title": "When are nonconvex problems not scary", "author": ["2015a. Ju Sun", "Qing Qu", "John Wright"], "venue": "arXiv preprint arXiv:1510.06096,", "citeRegEx": "Sun et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Guaranteed matrix completion via nonconvex factorization", "author": ["2015b. Ruoyu Sun", "Zhi-Quan Luo"], "venue": null, "citeRegEx": "Sun and Luo.,? \\Q2015\\E", "shortCiteRegEx": "Sun and Luo.", "year": 2015}, {"title": "Low-rank solutions of linear matrix equations", "author": ["Stephen Tu", "Ross Boczar", "Mahdi Soltanolkotabi", "Benjamin Recht"], "venue": "Science (FOCS),", "citeRegEx": "Tu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2015}, {"title": "A nonconvex free lunch for low-rank plus sparse matrix recovery", "author": ["Xiao Zhang", "Lingxiao Wang", "Quanquan Gu"], "venue": "arXiv preprint arXiv:1702.06525,", "citeRegEx": "Zhang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}, {"title": "A nonconvex optimization framework for low rank matrix estimation", "author": ["Tuo Zhao", "Zhaoran Wang", "Han Liu"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent", "author": ["Qinqing Zheng", "John Lafferty"], "venue": "arXiv preprint arXiv:1605.07051,", "citeRegEx": "Zheng and Lafferty.,? \\Q2016\\E", "shortCiteRegEx": "Zheng and Lafferty.", "year": 2016}, {"title": "Lemma follows from direct calculation using linear algebra and calculus. In the first step of our framework, we hope to show that the regularizer forces the matrixU to not have large rows. This is formalized and proved below (the Lemma is similar to Lemma", "author": ["Ge"], "venue": null, "citeRegEx": "Ge,? \\Q2016\\E", "shortCiteRegEx": "Ge", "year": 2016}, {"title": "2016] stated the symmetric version, and we need the asymmetric version here, the proof in Ge et al", "author": ["Although Ge"], "venue": null, "citeRegEx": "Ge,? \\Q2016\\E", "shortCiteRegEx": "Ge", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "Recently, a line of works showed that several natural problems including tensor decomposition [Ge et al., 2015], dictionary learning [Sun et al.", "startOffset": 94, "endOffset": 111}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al.", "startOffset": 26, "endOffset": 250}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al.", "startOffset": 26, "endOffset": 272}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al. [2016], Jin et al.", "startOffset": 26, "endOffset": 295}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al. [2016], Jin et al. [2017]) that are guaranteed to find a local minimum for many non-convex functions, such problems can be efficiently solved by basic optimization algorithms such as stochastic gradient descent.", "startOffset": 26, "endOffset": 314}, {"referenceID": 1, "context": ", 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016] have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results (e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al. [2016], Jin et al. [2017]) that are guaranteed to find a local minimum for many non-convex functions, such problems can be efficiently solved by basic optimization algorithms such as stochastic gradient descent. In this paper we focus on optimization problems that look for low rank matrices using partial or corrupted observations. Such problems are studied extensively [Fazel, 2002, Rennie and Srebro, 2005, Cand\u00e8s and Recht, 2009] and has many applications in recommendation systems [Koren, 2009], see survey by Davenport and Romberg [2016]. These optimization problems can be formalized as follows: min M\u2208Rd1\u00d7d2 f(M), (1) s.", "startOffset": 26, "endOffset": 832}, {"referenceID": 4, "context": "Ge et al. [2016] showed symmetric matrix completion has no spurious local minimum.", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "At the same time, Bhojanapalli et al. [2016] proved similar result for symmetric matrix sensing.", "startOffset": 18, "endOffset": 45}, {"referenceID": 1, "context": "At the same time, Bhojanapalli et al. [2016] proved similar result for symmetric matrix sensing. Park et al. [2016] extended the matrix sensing result to asymmetric case.", "startOffset": 18, "endOffset": 116}, {"referenceID": 12, "context": "Sun and Luo [2015], Zheng and Lafferty [2016]) accomplished this by showing a geometric property which is very similar to strong convexity holds in the neighborhood of optimal solution.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Sun and Luo [2015], Zheng and Lafferty [2016]) accomplished this by showing a geometric property which is very similar to strong convexity holds in the neighborhood of optimal solution.", "startOffset": 0, "endOffset": 46}, {"referenceID": 0, "context": "Bhojanapalli et al. [2015] gave a framework for local analysis for these low rank problems.", "startOffset": 0, "endOffset": 27}, {"referenceID": 0, "context": "Belkin et al. [2014] showed a framework of learning basis functions, which generalizes tensor decompositions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Belkin et al. [2014] showed a framework of learning basis functions, which generalizes tensor decompositions. Their techniques imply the optimization landscape for all such problems are very similar. For problems looking for a symmetric PSD matrix, Li and Tang [2016] showed for objective similar to (2) (but in the symmetric setting), restricted smoothness/strong convexity on the function f suffices for local analysis.", "startOffset": 0, "endOffset": 268}, {"referenceID": 5, "context": "Ge et al. [2015] characterized the following strict-saddle property, which is a quantitative version of the optimality conditions, and can lead to efficient algorithms to find local minima.", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "We choose the same regularizer as Ge et al. [2016]: Q(U) = \u03bb \u2211d i=1(\u2016Ui\u2016 \u2212 \u03b1)+.", "startOffset": 34, "endOffset": 51}, {"referenceID": 5, "context": "7 in Ge et al. [2016]. Next we show under this regularizer, we can still select the direction\u2206, and the first part of Equation (9) is significantly negative when\u2206 is large (step 2):", "startOffset": 5, "endOffset": 22}, {"referenceID": 5, "context": "Notice that our proof is different from Ge et al. [2016], as we focus on the direction \u2206 for both first and second order conditions while they need to select different directions for the Hessian.", "startOffset": 40, "endOffset": 57}, {"referenceID": 5, "context": "Existing results show many algorithms are saddle-avoiding, including cubic regularization [Nesterov and Polyak, 2006], stochastic gradient descent [Ge et al., 2015], trust-region algorithms [Sun et al.", "startOffset": 147, "endOffset": 164}, {"referenceID": 1, "context": "Towards faster convergence For many low-rank matrices problems, in the neighborhood of local minima, objective function satisfies conditions similar to strong convexity [Zheng and Lafferty, 2016, Bhojanapalli et al., 2016] (more precisely, the (\u03b1, \u03b2)-regularity condition as Assumption A3.b in [Jin et al., 2017]). Jin et al. [2017] showed a principle way of how to combine these strong local structures with saddle-avoiding algorithm to give global linear convergence.", "startOffset": 196, "endOffset": 333}, {"referenceID": 5, "context": "7 in Ge et al. [2016], but we get a stronger guarantee here): Lemma 9.", "startOffset": 5, "endOffset": 22}, {"referenceID": 2, "context": "(Candes and Plan [2011], Theorem 2.", "startOffset": 1, "endOffset": 24}, {"referenceID": 5, "context": "Ge et al. [2016] Let d = max{d1, d2}, then with at least probability 1\u2212 e over random choice of \u03a9, we have for any rank 2r matricesA \u2208 Rd1\u00d7d2:", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Although Ge et al. [2016] stated the symmetric version, and we need the asymmetric version here, the proof in Ge et al.", "startOffset": 9, "endOffset": 26}, {"referenceID": 5, "context": "Although Ge et al. [2016] stated the symmetric version, and we need the asymmetric version here, the proof in Ge et al. [2016] works directly.", "startOffset": 9, "endOffset": 127}], "year": 2017, "abstractText": "In this paper we develop a new framework that captures the common landscape underlying the common nonconvex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no highorder saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.", "creator": "LaTeX with hyperref package"}}}