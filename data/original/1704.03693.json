{"id": "1704.03693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2017", "title": "Trainable Referring Expression Generation using Overspecification Preferences", "abstract": "Referring expression generation (REG) models that use speaker-dependent information require a considerable amount of training data produced by every individual speaker, or may otherwise perform poorly. In this work we present a simple REG experiment that allows the use of larger training data sets by grouping speakers according to their overspecification preferences. Intrinsic evaluation shows that this method generally outperforms the personalised method found in previous work.", "histories": [["v1", "Wed, 12 Apr 2017 10:47:10 GMT  (9kb)", "http://arxiv.org/abs/1704.03693v1", "8 pages"]], "COMMENTS": "8 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["thiago castro ferreira", "ivandre paraboni"], "accepted": false, "id": "1704.03693"}, "pdf": {"name": "1704.03693.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n03 69\n3v 1\n[ cs\n.C L\n] 1\n2 A\npr 2\n01 7\ninformation require a considerable amount of training data produced by every individual speaker, or may otherwise perform poorly. In this work we present a simple REG experiment that allows the use of larger training data sets by grouping speakers according to their overspecification preferences. Intrinsic evaluation shows that this method generally outperforms the personalised method found in previous work."}, {"heading": "1 Introduction", "text": "In natural language generation systems, referring expression generation (REG) is the microplanning task responsible for generating descriptions of discourse objects [1]. REG includes the well-known content selection task for definite description generation, which is the focus of the present work.\nExisting work in computational REG and related fields have identified a wide range of factors that may drive content selection. To a considerable extent, however, content selection is known to be influenced by human variation [2]. In other words, under identical circumstances (i.e., in the same referential context), different speakers will often produce different descriptions.\nDifferences across speakers may be observed in at least two aspects of referential behaviour: (a) in the choice of attributes (e.g., \u2018the large ball\u2019 vs, \u2018the red ball\u2019) and (b) in the level of referential overspecification (e.g., \u2018the ball\u2019 vs. \u2018the red ball\u2019 in a context in which there is only one ball.) In this work we will focus on the issue of overspecification (b), discussing how preferences of this kind may be taken into account in trainable, speaker-dependent REG.\nExisting REG algorithms as in [3,4] usually pay regard to human variation by computing personalised features from a training set of descriptions produced by each individual speaker. This highly personalised training method may of course be considered an ideal account of human variation but, in practice, will only be effective if every speaker in the domain is represented by a sufficiently large number of training instances.\nAs an alternative to standard speaker-dependent REG, in this work we describe a simple experiment in which a machine-learning REG model is trained on descriptions produced by a group of speakers with similar referential behaviour (i.e., as opposed to using only the descriptions produced by each speaker individually.) By allowing the\nuse of larger training datasets in this way, we would like to show that this method may outperform the use of personalised information addressed in previous work, an improvement that may be particularly useful when the availability of training examples produced by every speaker is limited."}, {"heading": "2 Related Work", "text": "Existing methods for speaker-dependent REG generally consist of computing the relevant features for each individual speaker. In what follows we summarise a number of studies that follow this method - which is equivalent to our Speaker baseline method to be discussed in Section 3.2.\nIn [3], the Incremental algorithm [5] and a number of extensions of the Full Brevity algorithm [6] are evaluated on TUNA data [7]. In the case of the Incremental algorithm, human variation is accounted for by computing individual preference lists based on the attribute frequency of each individual speaker as observed in the training data. In the case of Full Brevity, all possible descriptions for a given referent are computed, and the description that most closely resembles those produced by the speaker is selected using a nearest neighbour approach.\nThe work in [8] also makes use of the Full Brevity [6] and Incremental [5] algorithms to generate TUNA descriptions. In the case of Full Brevity, human variation is accounted for by selecting the set of attributes computed by the algorithm according to the frequency and recency estimate use for each individual speaker. In the case of the Incremental algorithm, human variation is implemented as in [3], that is, by computing individual preference lists for each speaker.\nThe work in [2] makes use of decision-tree induction to predict content patterns (i.e., full attribute sets representing actual referring expressions) from GRE3D3/7 data [9,10]. Human variation is accounted for by modelling speaker identifiers as machine learning features.\nFinally, the work in [11,12,4] presents a SVM-based approach to speaker-dependent REG tested on GRE3D3/7 and Stars/Stars2 [13,14] data. Once again, human variation is accounted for by computing individual preference lists from the subset of descriptions produced by each individual speaker. As this approach will be taken as the basis of our current work, further details will be discussed in the next section."}, {"heading": "3 Experimental Setup", "text": "We designed an experiment to compare two training methods for speaker-dependent REG. Both methods are based on the REGmodel described in Section 3.1. The methods themselves are described in Section 3.2."}, {"heading": "3.1 Basic REG model", "text": "Our experiment makes use of a speaker-dependent REG model adapted from [4]. Given a set D of domain objects, a set A of referential attributes, a set R of spatial relations between object pairs, and a target object t \u2208 D to be identified, content selection is implemented with the aid of a set of classifiers Catom = {c (1), c(2), ..., c(|A|)},\nin which c(i) \u2208 Catom predicts whether a (i) \u2208 A should be selected or not, and a multi-class classifier Crel predicts the kind of relation (r \u2208 R) that may hold between the target t and the nearest landmark lm. R includes the special no-relation property to denote situations in which no relation between a certain object pair is predicted. When a relation to a landmark object lm exists, we also consider a set of classifiers Clmatom = {c (1), c(2), ..., c(|A|)} to describe lm.\nPart of the input to the classifiers consists of feature vectors extracted from the referential context. These features - hereby called context features - are based on the ones proposed in [2], and are intended to model target and landmark properties (if any), and similarities between objects. More specifically, context features represent the size of the target and its nearest landmark, the relation between the two objects (horizontal or vertical) and the number of distractors that share a certain property (e.g., type, colour etc.) with each of them.\nIn order to model human variation, we also consider two kinds of speaker-dependent feature: those that model personal information about the speakers, and those that model their content selection preferences. Speaker\u2019s personal features consist of a unique speaker identifier as in [2], gender and age bracket. Speaker\u2019s preferences consist of lists of preferred attributes for reference to target and landmark objects sorted by frequency.\nAttributes and relations of the main target t and nearby landmark lm are combined\nto form a description L according to Algorithm 1.\n1 Algorithm getDescription(t, L,D, H) 2 L[t] \u2190 {} 3 H \u2190 H \u222a t 4 level \u2190 |H | 5 Pratom \u2190 getPredictions(level) 6 Prrel \u2190 getRelationPrediction(level) 7 for Ai \u2208 Pratom do 8 if Pratom[Ai] == 1 then 9 L[t] \u2190 L[t] \u222a \u3008Ai, value(t, Ai)\u3009\n10 end 11 end 12 if Prrel 6= no-relation then 13 lm \u2190 value(t, P rrel) 14 if lm 6= null and lm /\u2208 H then 15 L[t] \u2190 L[t] \u222a \u3008rel, lm\u3009 16 L \u2190 getDescription(lm,L,D,H)\n17 end 18 end 19 return L Algorithm 1: Classification-based REG\nThe input to the algorithm is a target t and a domain D. The algorithm also makes use of a history list H to prevent self-reference (e.g., \u2018the ball next to a box that is next\nto a ball that...\u2019) and the initially empty list L representing the output description (to be built recursively).\nAn auxiliary function level is assumed to return 1 when t corresponds to the main target, 2 when t corresponds to the first landmark object, and so on. This information is taken into account to invoke the appropriate set of classifiers, which are implemented by the auxiliary functions getPredictions and getRelationPrediction. The former is assumed to invoke the set of binary classifiers for every attribute of t, and the latter invokes the multivalue prediction for the relation class.\nContent selection proper is performed by selecting all atomic attributes of the target t that were predicted by the corresponding binary classifiers. If a relation between t and its nearest distractor lm has been predicted, the relation is included in L and the algorithm is called recursively to describe lm as well. As in [4], all classifiers are built using Support Vector Machines (SVMs) with a Gaussian Kernel. For the relation prediction, we use an \u201cone-against-one\u201d multi-class method."}, {"heading": "3.2 Training methods", "text": "We consider two training methods for the basic REG model described in the previous section: a baseline method called Speaker, and our proposed Profile method.\nIn the Speaker method, classifiers are trained on the set of referring expressions produced by each individual speaker as in [3]. This method will effectively create personalised REG models, and may in principle be considered ideal for the purpose of modelling human variation in REG. In order to be successful, however, the method requires a sufficiently large number of descriptions produced by every single speaker, which may not always be available.\nAs an alternative to the standard Speaker approach, we propose a training method based on the simple observation - made by [2] and others - that some speakers follow a consistent pattern in reference production, whereas others do not. More specifically, in the present method - hereby called Profile - speakers are divided into three general categories: those that always produced overspecified descriptions, those that always produced minimally distinguishing descriptions, and those that do not follow a consistent pattern. Knowing in advance the category of a particular speaker, the REG model will be trained on the subset of descriptions produced by that category only. This will effectively allow us to use more training data than in the Speakermethod, and it should improve the overall results of the REG model."}, {"heading": "3.3 Evaluation", "text": "The Speaker and Profile training methods were compared against each other using six REG datasets: TUNA-Furniture and TUNA-People - only descriptions to single objects were considered -, GRE3D3, GRE3D7, Stars [13] and Stars2 [14].\nAll models were built using cross-validation with a balanced number of referring expressions per participant within each fold. For TUNA and Stars, descriptions were divided into six folds each. For GRE3D3/7 and Stars2, descriptions were divided into ten folds each.\nOptimal values for the SVM C parameter and for the Gaussian kernel \u03b3 parameter were obtained using grid-search.We testedC values of 1, 10, 100 and 1000 and gamma values of 1, 0.1, 0.01, and 0.001 in a validation set before testing the models. Given k folds in a cross-validation iteration, k\u2212 2 folds were used as training data, one fold was used to estimate the optimal values of C and \u03b3, and the remaining fold was used to test the model.\nWe measured Dice coefficients [15] to assess the similarity between each description generated by the model and the corpus description. We also computed the overall REG Accuracy by counting the number of exact matches between each description pair."}, {"heading": "4 Results", "text": "Table 1 presents the results of the REG model using the Speaker and Profile training methods on each of the test domains.\nOverall results suggest that the Profile training method outperforms Speaker in\nterms of Dice (WilcoxonW=3188296.5, p<.01) and Accuracy (Chi-Square\u03c72 =104.28, p<.01). The main exception is the Stars corpus, in which the Profilemodel failed to accurately predict the use of relational properties that are ubiquitous in this domain. More work will be required to shed light on this particular issue.\nResults based on Dice coefficients were also confirmed in four individual domains:\nTUNA-People (W=17969, p<.01), GRE3D3 (W=21483, p<.01), GRE3D7 (W=759954, p<.01) and Stars2 (W=100727, p<.01). In the case of TUNA-Furniture and Stars the difference between the two methods was not significant.\nRegardingAccuracy, results were also confirmed in four individual domains. TUNA-\nPeople (\u03c72 =19.61, p<.01), GRE3D3 (\u03c72 = 61.71, p<0.01), GRE3D7 (\u03c72 =64.61,p<0.01) and Stars2 (\u03c72 =27.97,p<0.01). In the case of TUNA-Furniture the difference between the two methods was not significant, and in the case of Stars a significant effect in the opposite direction was observed (\u03c72 =9.38, p<0.01).\nGiven that speakers are grouped according to their overspecification preferences, it is interesting to observe whether our output descriptions actually correspond to the expected level of information. To this end, Table 2 shows how often the Speaker and Profile methods were able to reproduce the level of referential specification found in each corpus, that is, how often each method correctly produced underspecified, overspecified and minimally distinguishing descriptions. Results show that predictions made by the Profilemethod generally outperform those made by the Speakermethod. The exception is, once again the Stars domain as discussed above.\nFinally, Table 3 shows Precision, Recall and F1-measures obtained by both methods according to reference type (minimally distinguishing, overspecified and underspecified). Results show that both models generally make accurate predictions regarding the generation of overspecified descriptions, which make the majority of our data. For underspecified and minimally distinguishing descriptions, on the other hand, results remain much lower due to data sparsity."}, {"heading": "5 Final remarks", "text": "This paper presented an experiment in machine-learningREG that takes speaker-dependent information into account, and which makes use of a simple training method based on speaker profiles to circumnavigate the issue of data sparsity. By grouping speakers according to their overspecification preferences, we were able to sketch a speakerdependent REG model that was shown to outperform the standard use of individual speaker\u2019s information proposed in previous work.\nDespite the overall positive results of this initial experiment, we may of course ask which alternative training methods may be considered for the task. More specifically, since using more training data - as we did by considering groups of similar speakers - has improved results, it may be the case that by simply training our REG models on the data provided by all speakers, we could improve results even further. Although we presently do not seek to validate this claim (which in any case would defeat the purpose of using speaker-dependent information in REG), there is plenty of evidence to suggest that this would not be the case. Studies such as in [3,16], for instance, have consistently shown that using individual training datasets for each speaker outperforms speaker-independentREG and, in particular, the work in [4] has shown that SVM-based REG models generally produce best results when trained on personalised datasets.\nFinally, we notice that the present experiment has focused on a single aspect of referential behaviour, namely, on the issue of overspecification preferences across speakers. As future work, we would like not only to refine the current method (e.g., by distinguishing between target and landmark overspecification preferences, amongmany other options.) but also to consider the issue of attribute choice (e.g., by grouping speakers according to their preferred referential attributes.)"}, {"heading": "Acknowledgements", "text": "This work has been supported by the National Council of Scientific and Technological Development from Brazil (CNPq) and FAPESP."}], "references": [{"title": "Computational generation of referring expressions: A survey", "author": ["E. Krahmer", "K. van Deemter"], "venue": "Computational Linguistics 38(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Speaker-dependent variation in content selection for referring expression generation", "author": ["J. Viethen", "R. Dale"], "venue": "Australasian Language Technology Association Workshop 2010, Melbourne, Australia", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "The fingerprint of human referring expressions and their surface realization with graph transducers", "author": ["B. Bohnet"], "venue": "Fifth International Natural Language Generation Conference, Stroudsburg, PA, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Generating natural language descriptions using speakerdependent information", "author": ["T.C. Ferreira", "I. Paraboni"], "venue": "Natural Language Engineering", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Computational interpretations of the Gricean maxims in the generation of referring expressions", "author": ["R. Dale", "E. Reiter"], "venue": "Cognitive Science 19(2)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Cooking up referring expressions", "author": ["R. Dale"], "venue": "Proc. ACL-1989, Stroudsburg, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Evaluating algorithms for the generation of referring expressions using a balanced corpus", "author": ["A. Gatt", "I. van der Sluis", "K. van Deemter"], "venue": "Proceedings of ENLG-07.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Trainable speaker-based referring expression generation", "author": ["G.D. Fabbrizio", "A. Stent", "S. Bangalore"], "venue": "12th Conference on Computational Natural Language Learning, Manchester, UK, Association for Computational Linguistics", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Referring expression generation through attribute-based heuristics", "author": ["R. Dale", "J. Viethen"], "venue": "Proceedings of ENLG-2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "GRE3D7: A corpus of distinguishing descriptions for objects in visual scenes", "author": ["J. Viethen", "R. Dale"], "venue": "Proceedings of UCNLG+Eval-2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Classification-based referring expression generation", "author": ["T.C. Ferreira", "I. Paraboni"], "venue": "Computational Linguistics and Intelligent Text Processing, Lecture Notes in Computer Science 8403, Kathmandu, Nepal, Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Referring expression generation: taking speakers\u2019 preferences into account", "author": ["T.C. Ferreira", "I. Paraboni"], "venue": "Text, Speech and Dialogue, Lecture Notes in Artificial Intelligence 8655, Brno, Czech Republic, Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Generating relational descriptions involving mutual disambiguation", "author": ["C.V.M. Teixeira", "I. Paraboni", "A.S.R. da Silva", "A.K. Yamasaki"], "venue": "LNCS 8403", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Stars2: a corpus of object descriptions in a visual domain", "author": ["I. Paraboni", "M. Galindo", "D. Iacovelli"], "venue": "Language Resources and Evaluation", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Measures of the amount of ecologic association between species", "author": ["L.R. Dice"], "venue": "Ecology 26(3)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1945}, {"title": "Speaker-dependent variation in content selection for referring expression generation", "author": ["J. Viethen", "R. Dale"], "venue": "Australasian Language Technology Association Workshop 2010, Melbourne, Australia", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "In natural language generation systems, referring expression generation (REG) is the microplanning task responsible for generating descriptions of discourse objects [1].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "To a considerable extent, however, content selection is known to be influenced by human variation [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 2, "context": "Existing REG algorithms as in [3,4] usually pay regard to human variation by computing personalised features from a training set of descriptions produced by each individual speaker.", "startOffset": 30, "endOffset": 35}, {"referenceID": 3, "context": "Existing REG algorithms as in [3,4] usually pay regard to human variation by computing personalised features from a training set of descriptions produced by each individual speaker.", "startOffset": 30, "endOffset": 35}, {"referenceID": 2, "context": "In [3], the Incremental algorithm [5] and a number of extensions of the Full Brevity algorithm [6] are evaluated on TUNA data [7].", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [3], the Incremental algorithm [5] and a number of extensions of the Full Brevity algorithm [6] are evaluated on TUNA data [7].", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "In [3], the Incremental algorithm [5] and a number of extensions of the Full Brevity algorithm [6] are evaluated on TUNA data [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "In [3], the Incremental algorithm [5] and a number of extensions of the Full Brevity algorithm [6] are evaluated on TUNA data [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 7, "context": "The work in [8] also makes use of the Full Brevity [6] and Incremental [5] algorithms to generate TUNA descriptions.", "startOffset": 12, "endOffset": 15}, {"referenceID": 5, "context": "The work in [8] also makes use of the Full Brevity [6] and Incremental [5] algorithms to generate TUNA descriptions.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "The work in [8] also makes use of the Full Brevity [6] and Incremental [5] algorithms to generate TUNA descriptions.", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "In the case of the Incremental algorithm, human variation is implemented as in [3], that is, by computing individual preference lists for each speaker.", "startOffset": 79, "endOffset": 82}, {"referenceID": 1, "context": "The work in [2] makes use of decision-tree induction to predict content patterns (i.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": ", full attribute sets representing actual referring expressions) from GRE3D3/7 data [9,10].", "startOffset": 84, "endOffset": 90}, {"referenceID": 9, "context": ", full attribute sets representing actual referring expressions) from GRE3D3/7 data [9,10].", "startOffset": 84, "endOffset": 90}, {"referenceID": 10, "context": "Finally, the work in [11,12,4] presents a SVM-based approach to speaker-dependent REG tested on GRE3D3/7 and Stars/Stars2 [13,14] data.", "startOffset": 21, "endOffset": 30}, {"referenceID": 11, "context": "Finally, the work in [11,12,4] presents a SVM-based approach to speaker-dependent REG tested on GRE3D3/7 and Stars/Stars2 [13,14] data.", "startOffset": 21, "endOffset": 30}, {"referenceID": 3, "context": "Finally, the work in [11,12,4] presents a SVM-based approach to speaker-dependent REG tested on GRE3D3/7 and Stars/Stars2 [13,14] data.", "startOffset": 21, "endOffset": 30}, {"referenceID": 12, "context": "Finally, the work in [11,12,4] presents a SVM-based approach to speaker-dependent REG tested on GRE3D3/7 and Stars/Stars2 [13,14] data.", "startOffset": 122, "endOffset": 129}, {"referenceID": 13, "context": "Finally, the work in [11,12,4] presents a SVM-based approach to speaker-dependent REG tested on GRE3D3/7 and Stars/Stars2 [13,14] data.", "startOffset": 122, "endOffset": 129}, {"referenceID": 3, "context": "Our experiment makes use of a speaker-dependent REG model adapted from [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "These features - hereby called context features - are based on the ones proposed in [2], and are intended to model target and landmark properties (if any), and similarities between objects.", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Speaker\u2019s personal features consist of a unique speaker identifier as in [2], gender and age bracket.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "As in [4], all classifiers are built using Support Vector Machines (SVMs) with a Gaussian Kernel.", "startOffset": 6, "endOffset": 9}, {"referenceID": 2, "context": "In the Speaker method, classifiers are trained on the set of referring expressions produced by each individual speaker as in [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "As an alternative to the standard Speaker approach, we propose a training method based on the simple observation - made by [2] and others - that some speakers follow a consistent pattern in reference production, whereas others do not.", "startOffset": 123, "endOffset": 126}, {"referenceID": 12, "context": "The Speaker and Profile training methods were compared against each other using six REG datasets: TUNA-Furniture and TUNA-People - only descriptions to single objects were considered -, GRE3D3, GRE3D7, Stars [13] and Stars2 [14].", "startOffset": 208, "endOffset": 212}, {"referenceID": 13, "context": "The Speaker and Profile training methods were compared against each other using six REG datasets: TUNA-Furniture and TUNA-People - only descriptions to single objects were considered -, GRE3D3, GRE3D7, Stars [13] and Stars2 [14].", "startOffset": 224, "endOffset": 228}, {"referenceID": 14, "context": "We measured Dice coefficients [15] to assess the similarity between each description generated by the model and the corpus description.", "startOffset": 30, "endOffset": 34}, {"referenceID": 2, "context": "Studies such as in [3,16], for instance, have consistently shown that using individual training datasets for each speaker outperforms speaker-independentREG and, in particular, the work in [4] has shown that SVM-based REG models generally produce best results when trained on personalised datasets.", "startOffset": 19, "endOffset": 25}, {"referenceID": 15, "context": "Studies such as in [3,16], for instance, have consistently shown that using individual training datasets for each speaker outperforms speaker-independentREG and, in particular, the work in [4] has shown that SVM-based REG models generally produce best results when trained on personalised datasets.", "startOffset": 19, "endOffset": 25}, {"referenceID": 3, "context": "Studies such as in [3,16], for instance, have consistently shown that using individual training datasets for each speaker outperforms speaker-independentREG and, in particular, the work in [4] has shown that SVM-based REG models generally produce best results when trained on personalised datasets.", "startOffset": 189, "endOffset": 192}], "year": 2017, "abstractText": "Referring expression generation (REG)models that use speaker-dependent information require a considerable amount of training data produced by every individual speaker, or may otherwise perform poorly. In this work we present a simple REG experiment that allows the use of larger training data sets by grouping speakers according to their overspecification preferences. Intrinsic evaluation shows that this method generally outperforms the personalised method found in previous work.", "creator": "LaTeX with hyperref package"}}}