{"id": "1704.03956", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Incremental Skip-gram Model with Negative Sampling", "abstract": "This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SNGS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SNGS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.", "histories": [["v1", "Thu, 13 Apr 2017 00:36:33 GMT  (167kb,D)", "http://arxiv.org/abs/1704.03956v1", null], ["v2", "Sat, 15 Apr 2017 07:15:00 GMT  (167kb,D)", "http://arxiv.org/abs/1704.03956v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["nobuhiro kaji", "hayato kobayashi"], "accepted": true, "id": "1704.03956"}, "pdf": {"name": "1704.03956.pdf", "metadata": {"source": "CRF", "title": "Incremental Skip-gram Model with Negative Sampling", "authors": ["Nobuhiro Kaji", "Hayato Kobayashi"], "emails": ["nkaji@yahoo-corp.jp", "hakobaya@yahoo-corp.jp"], "sections": [{"heading": "1 Introduction", "text": "Existing methods of neural word embeddings are typically designed to go through the entire training data multiple times. For example, negative sampling (Mikolov et al., 2013b) needs to precompute the noise distribution from the entire training data before performing Stochastic Gradient Descent (SGD). It thus needs to go through the training data at least twice. Similarly, hierarchical soft-max (Mikolov et al., 2013b) has to determine the tree structure and GloVe (Pennington et al., 2014) has to count co-occurrence frequencies before performing SGD.\nThe fact that those existing methods are multipass algorithms means that they cannot perform incremental model update when additional training data is provided. Instead, they have to re-train the model on the old and new training data from scratch.\nHowever, the re-training is obviously inefficient since it has to process the entire training data received thus far whenever new training data is\nprovided. This is especially problematic when the amount of the new training data is relatively smaller than the old one. One such a situation is that the embedding model is updated on a small amount of training data that includes newly emerged words to instantly add them to the vocabulary set. Another situation is learning word embeddings from ever-evolving data such as news articles and microbologs (Peng et al., 2017). In such a situation, the model is periodically updated on newly generated data (e.g., once in a week or month).\nThis paper investigates an incremental training method of word embeddings with a focus on the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) for its popularity. We present a simple incremental extension of SGNS, referred to as incremental SGNS, and provide a thorough theoretical analysis to demonstrate its validity. Our analysis reveals that, under a mild assumption, the optimal solution of incremental SGNS agrees with the original SGNS when the training data size is infinitely large. See Section 4 for the formal and strict statement. Additionally, we present techniques for the efficient implementation of incremental SGNS.\nThree experiments were conducted to assess the correctness of the theoretical analysis as well as the practical usefulness of incremental SGNS. The first experiment empirically investigates the validity of the theoretical analysis result. The second experiment compares the word embeddings learned by incremental SGNS and the original SGNS across five benchmark datasets, and demonstrates that those word embeddings are of comparable quality. The last experiment explores the training time of incremental SGNS, demonstrating that it is able to save much training time by avoiding expensive re-training when additional training data is provided. ar X\niv :1\n70 4.\n03 95\n6v 1\n[ cs\n.C L\n] 1\n3 A\npr 2\n01 7"}, {"heading": "2 SGNS Overview", "text": "As a preliminary, this section provides a brief overview of SGNS.\nGiven a word sequence, w1, w2, . . . , wn, for training, the skip-gram model seeks to minimize the following objective to learn word embeddings:\nLSG = \u2212 1\nn n\u2211 i=1 \u2211 |j|\u2264c j 6=0 log p(wi+j | wi),\nwhere wi is a target word and wi+j is a context word within a window of size c. p(wi+j | wi) represents the probability thatwi+j appears within the neighbor of wi, and is defined as\np(wi+j | wi) = exp(twi \u00b7 cwi+j )\u2211 w\u2208W exp(twi \u00b7 cw) , (1)\nwhere tw and cw are w\u2019s embeddings when it behaves as a target and context, respectively. W represents the vocabulary set.\nSince it is too expensive to optimize the above objective, Mikolov et al. (2013b) proposed negative sampling to speed up skip-gram training. This approximates Eq. (1) using sigmoid functions and k randomly-sampled words, called negative samples. The resulting objective is given as\nLSGNS =\u2212 1\nn n\u2211 i=1 \u2211 |j|\u2264c j 6=0 \u03c8+wi,wi+j+kEv\u223cq(v)[\u03c8 \u2212 wi,v],\nwhere \u03c8+w,v = log \u03c3(tw \u00b7 cv), \u03c8\u2212w,v = log \u03c3(\u2212tw \u00b7 cv), and \u03c3(x) is the sigmoid function. The negative sample v is drawn from a smoothed unigram probability distribution referred to as noise distribution: q(v) \u221d f(v)\u03b1, where f(v) represents the frequency of a word v in the training data and \u03b1 is a smoothing parameter (0 < \u03b1 \u2264 1).\nThe objective is optimized by SGD. Given a target-context word pair (wi and wi+j) and k negative samples (v1, v2, . . . , vk) drawn from the noise distribution, the gradient of \u2212\u03c8+wi,wi+j \u2212 kEv\u223cq(v)[\u03c8\u2212wi,v] \u2248 \u2212\u03c8 + wi,wi+j \u2212 \u2211k k\u2032=1 \u03c8 \u2212 wi,vk\u2032 is computed. Then, the gradient descent is performed to update twi , cwi+j , and cv1 , . . . , cvk .\nSGNS training needs to scan the entire training data multiple times because it has to precompute the noise distribution q(v) before performing SGD. This makes it difficult to perform incremental model update when additional training data is provided."}, {"heading": "3 Incremental SGNS", "text": "This section explores incremental training of SGNS. The incremental training algorithm (Section 3.1), its efficient implementation (Section 3.2), and the computational complexity (Section 3.3) are discussed in turn."}, {"heading": "3.1 Algorithm", "text": "Algorithm 1 presents incremental SGNS, which goes through the training data in a single-pass to update word embeddings incrementally. Unlike the original SGNS, it does not pre-compute the noise distribution. Instead, it reads the training data word by word1 to incrementally update the word frequency distribution and the noise distribution while performing SGD. Hereafter, the original SGNS (c.f., Section 2) is referred to as batch SGNS to emphasize that the noise distribution is computed in a batch fashion.\nThe learning rate for SGD is adjusted by using AdaGrad (Duchi et al., 2011). Although the linear decay function has widely been used for training batch SGNS (Mikolov, 2013), adaptive methods such as AdaGrad are more suitable for the incremental training since the amount of training data is unknown in advance or can increase unboundedly.\nIt is straightforward to extend the incremental SGNS to the min-batch setting by reading a subset of the training data (or min-batch), rather than a single word, at a time to update the noise distribution and perform SGD (Algorithm 2). Although this paper primarily focuses on the incremental SGNS, the min-batch algorithm is also important in practical terms because it is easier to be multithreaded.\nAlternatives to Algorithms 2 might be possible. Other possible approaches include computing the noise distribution separately on each subset of the training data, fixing the noise distribution after computing it on the first (possibly large) subset, and so on. We exclude such alternatives from our investigation because it is considered difficult to provide them with theoretical justification."}, {"heading": "3.2 Efficient implementation", "text": "Although the incremental SGNS is conceptually simple, implementation issues are involved.\n1In practice, Algorithm 1 buffers a sequence of words wi\u2212c, . . . , wi+c (rather than a single word wi) at each step, as it requires an access to the context words wi+j in line 7. This is not a practical problem because the window size c is usually small and independent from the training data size n.\nAlgorithm 1 Incremental SGNS 1: f(w)\u2190 0 for all w \u2208 W 2: for i = 1, . . . , n do 3: f(wi)\u2190 f(wi) + 1 4: q(w)\u2190 f(w)\n\u03b1\u2211 w\u2032\u2208W f(w\n\u2032)\u03b1 for all w \u2208 W 5: for j = \u2212c, . . . ,\u22121, 1, . . . , c do 6: draw k negative samples from q(w): v1, . . . , vk 7: use SGD to update twi , cwi+j , and cv1 , . . . , cvk 8: end for 9: end for\nAlgorithm 2 Mini-batch SGNS 1: for each subset D of the training data do 2: update the noise distribution using D 3: perform SGD over D 4: end for"}, {"heading": "3.2.1 Dynamic vocabulary", "text": "One problem that arises when training incremental SGNS is how to maintain the vocabulary set. Since new words emerge endlessly in the training data, the vocabulary set can grow unboundedly and exhaust a memory.\nWe address this problem by dynamically changing the vocabulary set. The Misra-Gries algorithm (Misra and Gries, 1982) is used to approximately keep track of top-m frequent words during training, and those words are used as the dynamic vocabulary set. This method allows the maximum vocabulary size to be explicitly limited tom, while being able to dynamically change the vocabulary set."}, {"heading": "3.2.2 Adaptive unigram table", "text": "Another problem is how to generate negative samples efficiently. Since k negative samples per target-context pair have to be generated by the noise distribution, the sampling speed has a significant effect on the overall training efficiency.\nLet us first examine how negative samples are generated in batch SGNS. In a popular implementation (Mikolov, 2013), a word array (referred to as a unigram table) is constructed such that the number of a word w in it is proportional to q(w). See Table 1 for an example. Using the unigram table, negative samples can be efficiently generated by sampling the table elements uniformly at random. It takes only O(1) time to generate one negative sample.\nThe above method assumes that the noise distribution is fixed and thus cannot be used directly for the incremental training. One simple solution is to reconstruct the unigram table whenever new training data is provided. However, such a method\nAlgorithm 3 Adaptive unigram table. 1: f(w)\u2190 0 for all w \u2208 W 2: z \u2190 0 3: for i = 1, . . . , n do 4: f(wi)\u2190 f(wi) + 1 5: F \u2190 f(wi)\u03b1 \u2212 (f(wi)\u2212 1)\u03b1 6: z \u2190 z + F 7: if |T | < \u03c4 then 8: add F copies of wi to T 9: else 10: for j = 1, . . . , \u03c4 do 11: T [j]\u2190 wi with probability Fz 12: end for 13: end if 14: end for\nis not effective for the incremental SGNS, because the unigram table reconstruction requires O(|W|) time.2\nWe propose a reservoir-based algorithm for efficiently updating the unigram table (Vitter, 1985; Efraimidis, 2015) (Algorithm 3). The algorithm incrementally update the unigram table T while limiting its maximum size to \u03c4 . In case |T | < \u03c4 , it can be easily confirmed that the number of a word w in T is f(w)\u03b1(\u221d q(w)). In case |T | = \u03c4 , since z = \u2211 w\u2208W f(w)\n\u03b1 is equal to the normalization factor of the noise distribution, it can be proven by induction that, for all j, T [j] is a word w with probability q(w). See (Vitter, 1985; Efraimidis, 2015) for reference.\nNote on implementation In line 8, F copies of wi are added to T . When F is not an integer, the copies are generated so that their expected number becomes F . Specifically, dF e copies are added to T with probability F \u2212 bF c, and bF c copies are added otherwise.\nThe loop from line 10 to 12 becomes expensive if implemented straightforwardly because the maximum table size \u03c4 is typically set large (e.g., \u03c4 = 108 in word2vec (Mikolov, 2013)). For acceleration, instead of checking all the elements in the unigram table, randomly chosen \u03c4Fz elements are substituted with wi. Note that \u03c4Fz is the\n2This overhead is amortized in mini-batch SGNS if the mini-batch size is sufficiently large. Our discussion here is dedicated to efficiently perform the incremental training irrespective of the mini-batch size.\nexpected number of table elements to be substituted in the original algorithm. This approximation achieves great speed-up because we usually have F z. In fact, it can be proven that it takes O(1) time when \u03b1 = 1.0. See Appendix3 A for more discussions."}, {"heading": "3.3 Computational complexity", "text": "Both incremental and batch SGNS have the same space complexity, which is independent of the training data size n. Both require O(|W|) space to store the word embeddings and the word frequency counts, and O(|T |) space to store the unigram table.\nThe two algorithms also have the same time complexity, and it is linearly dependent on the training data size n, O(n). Although incremental SGNS requires extra time for updating the dynamic vocabulary and adaptive unigram table, these costs are practically negligible, as will be demonstrated in Section 5.3."}, {"heading": "4 Theoretical Analysis", "text": "Although the extension from batch to incremental SGNS is simple and intuitive, it is not readily clear whether incremental SGNS can learn word embeddings as well as the batch counterpart. To answer this question, in this section we examine incremental SGNS from a theoretical point of view.\nThe analysis begins by examining the difference between the objectives optimized by batch and incremental SGNS (Section 4.1). Then, probabilistic properties of their difference are investigated to demonstrate the relationship between batch and incremental SGNS (Sections 4.2 and 4.3). We shortly touch the min-batch SGNS at the end of this section (Section 4.4)."}, {"heading": "4.1 Objective difference", "text": "As discussed in Section 2, batch SGNS optimizes the following objective:\nLB(\u03b8)=\u2212 1\nn n\u2211 i=1 \u2211 |j|\u2264c j 6=0 \u03c8+wi,wi+j+kEv\u223cqn(v)[\u03c8 \u2212 wi,v],\nwhere \u03b8 = (t1, t2, . . . , t|W|, c1, c2, . . . , c|W|) collectively represents the model parameter4 (i.e., word embeddings) and qn(v) represents the noise\n3The appendices are in the supplementary material. 4We treat words as integers and thusW={1, 2, . . . |W|}.\ndistribution. Note that the noise distribution is represented in a different notation than Section 2 to make its dependence on the whole training data explicit. The function qi(v) is defined as qi(v) =\nfi(v) \u03b1\u2211\nv\u2032\u2208W fi(v \u2032)\u03b1 , where fi(v) represents the word frequency in the first i words of the training data. In contrast, incremental SGNS computes the gradient of \u2212\u03c8+wi,wi+j \u2212 kEv\u223cqi(v)[\u03c8 \u2212 wi,v] at each step to perform gradient descent. Note that the noise distribution does not depend on n but rather on i. Because it can be seen as a sample approximation of the gradient of\nLI(\u03b8) = \u2212 1\nn n\u2211 i=1 \u2211 |j|\u2264c j 6=0 \u03c8+wi,wi+j+kEv\u223cqi(v)[\u03c8 \u2212 wi,v],\nincremental SGNS can be interpreted as optimizing LI(\u03b8) with SGD.\nSince the expectation terms in the objectives can be rewritten as Ev\u223cqi(v)[\u03c8 \u2212 wi,v] =\u2211\nv\u2208W qi(v)\u03c8 \u2212 wi,v, the difference between the two\nobjectives can be given as\n\u2206L(\u03b8) = LB(\u03b8)\u2212 LI(\u03b8)\n= 1\nn n\u2211 i=1 \u2211 |j|\u2264c j 6=0 k \u2211 v\u2208W (qi(v)\u2212qn(v))\u03c8\u2212wi,v\n= 2ck\nn n\u2211 i=1 \u2211 v\u2208W (qi(v)\u2212 qn(v))\u03c8\u2212wi,v\n= 2ck\nn \u2211 w,v\u2208W n\u2211 i=1 \u03b4wi,w(qi(v)\u2212 qn(v))\u03c8\u2212w,v\nwhere \u03b4w,v = \u03b4(w = v) is the delta function."}, {"heading": "4.2 Unsmoothed case", "text": "Let us begin by examining the objective difference \u2206L(\u03b8) in the unsmoothed case, \u03b1 = 1.0.\nThe technical difficulty in analyzing \u2206L(\u03b8) is that it is dependent on the word order in the training data. We address this difficulty by assuming that the words in the training data are generated from some stationary distribution and by investigating the probabilistic property of \u2206L(\u03b8). We below introduce some definitions and notations as the preparation of the analysis.\nDefinition 1. Let Xi,w be a random variable that represents \u03b4wi,w. It takes 1 when the i-th word in the training data is w \u2208 W and 0 otherwise.\nRemind that we assume that the words in the training data are generated from a stationary distribution. This assumption means that the expectation and (co)variance of Xi,w do not depend on the index i. Hereafter, they are respectively denoted as E[Xi,w] = \u00b5w and V[Xi,w,Xj,v] = \u03c1w,v. Definition 2. Let Yi,w be a random variable that represents qi(w) when \u03b1 = 1.0. It is given as Yi,w = 1 i \u2211i i\u2032=1 Xi\u2032,w."}, {"heading": "4.2.1 Convergence of the first and second order moments of \u2206L(\u03b8)", "text": "It can be shown that the first order moment of \u2206L(\u03b8) has an analytical form. Theorem 1. The first order moment of \u2206L(\u03b8) is given as\nE[\u2206L(\u03b8)] = 2ck(Hn \u2212 1) n \u2211 w,v\u2208W \u03c1w,v\u03c8 \u2212 w,v,\nwhere Hn is the n-th harmonic number.\nSketch of proof. Notice that E[\u2206L(\u03b8)] can be written as\n2ck\nn \u2211 w,v\u2208W n\u2211 i=1 ( E[Xi,wYi,v]\u2212 E[Xi,wYn,v] ) \u03c8\u2212w,v.\nBecause we have, for any i and j such that i \u2264 j, E[Xi,wYj,v] = j\u2211\nj\u2032=1\nE[Xi,w Xj\u2032,v j ] = \u00b5w\u00b5v + \u03c1w,v j ,\nplugging this into E[\u2206L(\u03b8)] proves the theorem. See Appendix B.1 for the complete proof.\nTheorem 1 readily gives the convergence property of the first order moment of \u2206L(\u03b8): Theorem 2. The first-order moment of \u2206L(\u03b8) decreases in the order of O( log(n)n ):\nE[\u2206L(\u03b8)] = O ( log(n)\nn\n) ,\nand thus converges to zero in the limit of infinity:\nlim n\u2192\u221e\nE[\u2206L(\u03b8)] = 0.\nProof. We have Hn = O(log(n)) from the upper integral bound, and thus Theorem 1 gives the proof.\nA similar result to Theorem 2 can be obtained for the second order moment of \u2206L(\u03b8) as well.\nTheorem 3. The second-order moment of \u2206L(\u03b8) decreases in the order of O( log(n)n ):\nE[\u2206L(\u03b8)2] = O ( log(n)\nn\n) ,\nand thus converges to zero in the limit of infinity:\nlim n\u2192\u221e\nE[\u2206L(\u03b8)2] = 0.\nProof. Omitted. See Appendix B.2."}, {"heading": "4.2.2 Main result", "text": "The above theorems reveal the relationship between the optimal solutions of the two objectives, as stated in the next lemma.\nLemma 4. Let \u03b8\u2217 and \u03b8\u0302 be the optimal solutions of LB(\u03b8) and LI(\u03b8), respectively: \u03b8\u2217 = arg min\u03b8 LB(\u03b8) and \u03b8\u0302 = arg min\u03b8 LI(\u03b8). Then,\nlim n\u2192\u221e\nE[LB(\u03b8\u0302)\u2212 LB(\u03b8\u2217)] = 0, (2)\nlim n\u2192\u221e\nV[LB(\u03b8\u0302)\u2212 LB(\u03b8\u2217)] = 0. (3)\nProof. The proof is made by the squeeze theorem. Let l = LB(\u03b8\u0302) \u2212 LB(\u03b8\u2217). The optimality of \u03b8\u2217 gives 0 \u2264 l. Also, the optimality of \u03b8\u0302 gives\nl = LB(\u03b8\u0302)\u2212 LI(\u03b8\u2217) + LI(\u03b8\u2217)\u2212 LB(\u03b8\u2217) \u2264 LB(\u03b8\u0302)\u2212 LI(\u03b8\u0302) + LI(\u03b8\u2217)\u2212 LB(\u03b8\u2217) = \u2206L(\u03b8\u0302)\u2212\u2206L(\u03b8\u2217).\nWe thus have 0 \u2264 E[l] \u2264 E[\u2206L(\u03b8\u0302) \u2212 \u2206L(\u03b8\u2217)]. Since Theorem 2 implies that the right hand side converges to zero when n \u2192 \u221e, the squeeze theorem gives Eq. (2). Next, we have\nV[l]=E[l2]\u2212E[l]2\u2264E[l2]\u2264E[(\u2206L(\u03b8\u0302)\u2212\u2206L(\u03b8\u2217))2] \u2264E[(\u2206L(\u03b8\u0302)\u2212\u2206L(\u03b8\u2217))2+(\u2206L(\u03b8\u0302)+\u2206L(\u03b8\u2217))2] =2E[\u2206L(\u03b8\u0302)2] + 2E[\u2206L(\u03b8\u2217)2]. (4)\nTheorem 3 suggests that Eq. (4) converges to zero when n \u2192 \u221e. Also, the non-negativity of the variance gives 0 \u2264 V[l]. Therefore, the squeeze theorem gives Eq. (3).\nWe are now ready to provide the main result of the analysis. The next theorem shows the convergence of LB(\u03b8\u0302). Theorem 5. LB(\u03b8\u0302) converges in probability to LB(\u03b8\u2217):\n\u2200 > 0, lim n\u2192\u221e Pr\n[ |LB(\u03b8\u0302)\u2212 LB(\u03b8\u2217)| \u2265 ] = 0.\nProof. Let again l = LB(\u03b8\u0302) \u2212 LB(\u03b8\u2217). Then, Chebyshev\u2019s inequality gives, for any 1 > 0,\nlim n\u2192\u221e V[l] 21 \u2265 lim n\u2192\u221e Pr\n[ |l \u2212 E[l]| \u2265 1 ] .\nRemember that Eq. (2) means that for any 2 > 0, there exists n\u2032 such that if n\u2032 \u2264 n then |E[l]| < 2. Therefore, we have\nlim n\u2192\u221e V[l] 21 \u2265 lim n\u2192\u221e Pr\n[ |l| \u2265 1 + 2 ] \u2265 0.\nThe arbitrary property of 1 and 2 allows 1 + 2 to be rewritten as . Also, Eq. (3) implies that limn\u2192\u221e\nV[l] 21 = 0. This completes the proof.\nInformally, this theorem can be interpreted as suggesting that the optimal solutions of batch and incremental SGNS agree when n is infinitely large."}, {"heading": "4.3 Smoothed case", "text": "We next examine the smoothed case (0 < \u03b1 < 1). In this case, the noise distribution can be represented by using the ones in the unsmoothed case:\nqi(w) \u221d fi(w) \u03b1\u2211 w\u2032\u2208W fi(w \u2032)\u03b1 =\n(fi(w) Fi )\u03b1\u2211 w\u2032\u2208W (fi(w\u2032) Fi\n)\u03b1 where Fi = \u2211 w\u2032\u2208W fi(w\n\u2032) and fi(w)Fi corresponds to the noise distribution in the unsmoothed case.\nDefinition 3. Let Zi,w be a random variable that represents qi(w). Then, it is given as\nZi,w = gw(Yi,1,Yi,2, . . . ,Yi,|W|)\nwhere gw(x1, x2, . . . , x|W|) = x\u03b1w\u2211\nw\u2032\u2208W x \u03b1 w\u2032\n.\nBecause Zi,w is no longer a linear combination of Xi,w, it becomes difficult to derive similar proofs to the unsmoothed case. To address this difficulty, Zi,w is approximated by the first-order Taylor expansion around\nE[(Yi,1,Yi,2, . . . ,Yi,|W|)] = (\u00b51, \u00b52, . . . , \u00b5|W|).\nThe first-order Taylor approximation gives Zi,w \u2248 gw(\u00b5) + \u2211 v\u2208W Mw,v(Yi,v \u2212 gv(\u00b5))\nwhere Mw,v = \u2202gw(x) \u2202xv |x=\u00b5. Consequently, it can be shown that the first and second order moments of \u2206L(\u03b8) have the order ofO( log(n)n ) in the smoothed case as well. See Appendix D for the details."}, {"heading": "4.4 Mini-batch SGNS", "text": "The same analysis result can also be obtained for the min-batch SGNS. We can prove Theorems 2 and 3 in the mini-batch case as well (see Appendix C for the proof). The other part of the analysis remains the same."}, {"heading": "5 Experiments", "text": "Three experiments were conducted to investigate the correctness of the theoretical analysis (Section 5.1) and the practical usefulness of incremental SGNS (Sections 5.2 and 5.3). The experimental settings are detailed in Appendix E."}, {"heading": "5.1 Validation of theorems", "text": "An empirical experiment was conducted to validate the result of the theoretical analysis. Since it is difficult to assess the main result in Section 4.2.2 directly, the theorems in Sections 4.2.1, from which the main result is readily derived, were investigated. Specifically, the first and second order moments of \u2206L(\u03b8) were computed on datasets of increasing sizes to empirically investigate the convergence property.\nDatasets of various sizes were constructed from the English Gigaword corpus (Napoles et al., 2012). The datasets made up of n words were constructed by randomly sampling sentences from the Gigaword corpus. The value of n was varied over {103, 104, 105, 106, 107}. 10, 000 different datasets were created for each size n to compute the first and second order moments.\nFigure 1 (top left) shows log-log plots of the first order moments of \u2206L(\u03b8) computed on the different sized datasets when \u03b1 = 1.0. The crosses and circles represent the empirical values and theoretical values obtained by Theorem 1, respectively. Figure 1 (top right) similarly illustrates the second order moments of \u2206L(\u03b8). Since Theorem 3 suggests that the second order moment decreases in the order of O( log(n)n ), the graph y \u221d log(x) x is also shown. The graph was fitted to the empirical data by minimizing the squared error. The top left figure demonstrates that the empirical values of the first order moments fit the theoretical result very well, providing a strong empirical evidence for the correctness of Theorem 1. In addition, the two figures show that the first and second order moments decrease almost in the order of O( log(n)n ), converging to zero as the data size increases. This result validates Theorems 2 and 3.\nFigures 1 (bottom left) and (bottom right) show similar results when \u03b1 = 0.75. Since we do not have theoretical estimates of the first order moment when \u03b1 6= 1.0, the graphs y \u221d log(n)n are shown in both figures. From these, we can again observe that the first and second order moments decrease almost in the order of O( log(n)n ). This indicates the validity of the investigation in Section 4.3. The relatively larger deviations from the graphs y \u221d log(n)n , compared with the top right figure, are considered to be attributed to the firstorder Taylor approximation."}, {"heading": "5.2 Quality of word embeddings", "text": "The next experiment investigates the quality of the word embeddings learned by incremental SGNS through comparison with the batch counterparts.\nThe Gigaword corpus was used for the training. For the comparison, both our own implementation of batch SGNS as well as WORD2VEC (Mikolov et al., 2013c) were used (denoted as batch and w2v). The training configurations of the three methods were set the same as much as possible, although it is impossible to do so perfectly. For example, incremental SGNS (denoted as incremental) utilized the dynamic vocabulary (c.f., Section 3.2.1) and thus we set the maximum vocabulary sizem to control the vocabulary size. On the other hand, we set a frequency threshold to determine the vocabulary size of w2v. We set m = 240k for incremental, while setting the frequency threshold to 100 for w2v. This yields vocabulary sets of comparable sizes: 220, 389 and 246, 134.\nThe learned word embeddings were assessed on five benchmark datasets commonly used in\nthe literature (Levy et al., 2015): WordSim353 (Agirre et al., 2009), MEN (Bruni et al., 2013), SimLex-999 (Hill et al., 2015), the MSR analogy dataset (Mikolov et al., 2013c), the Google analogy dataset (Mikolov et al., 2013a). The former three are for a semantic similarity task, and the remaining two are for a word analogy task. As evaluation measures, Spearman\u2019s \u03c1 and prediction accuracy were used in the two tasks, respectively.\nFigures 2 (a) and (b) represent the results on the similarity datasets and the analogy datasets. We see that the three methods (incremental, batch, and w2v) perform equally well on all of the datasets. This indicates that incremental SGNS can learn as good word embeddings as the batch counterparts, while being able to perform incremental model update. Although incremental performs slightly better than the batch methods in some datasets, the difference seems to be a product of chance.\nThe figures also show the results of incremental SGNS when the maximum vocabulary size m was reduced to 150k and 100k (incremental-150k and incremental-100k). The resulting vocabulary sizes were 135, 447 and 86, 993, respectively. We see that incremental-150k and incremental-100k perform comparatively well with incremental, although relatively large performance drops are observed in some datasets (MEN and MSR). This demonstrates that the Misra-Gries algorithm can effectively control the vocabulary size."}, {"heading": "5.3 Update time", "text": "The last experiment investigates how much time incremental SGNS can save by avoiding retraining when updating the word embeddings.\nIn this experiment, incremental was first trained on the initial training data of size5 n1 and then updated on the new training data of size n2 to measure the update time. For comparison, batch and w2v were re-trained on the combination of the initial and new training data. We fixed n1 = 107 and varied n2 over {1\u00d7106, 2\u00d7106, . . . , 5\u00d7106}. Figure 2 (c) compares the update time of the three methods across various values of n2. We see that incremental significantly reduces the update time. It achieves 10 and 7.3 times speed-up compared with batch and w2v (when n2 = 106). This represents the advantage of the incremental algorithm, as well as the time efficiency of the dynamic\n5The number of sentences here.\nvocabulary and adaptive unigram table. We note that batch is slower than w2v because it uses AdaGrad, which maintains different learning rates for different dimensions of the parameter, while w2v uses the same learning rate for all dimensions."}, {"heading": "6 Related Work", "text": "Word representations based on distributional semantics have been common (Turney and Pantel, 2010; Baroni and Lenci, 2010). The distributional methods typically begin by constructing a word-context matrix and then applying dimension reduction techniques such as SVD to obtain high-quality word meaning representations. Although some investigated incremental updating of the word-context matrix (Yin et al., 2015; Goyal and Daume III, 2011), they did not explore the reduced representations. On the other hand, neural word embeddings have recently gained much popularity as an alternative. However, most previous studies have not investigated incremental strategies (Mikolov et al., 2013a,b; Pennington et al., 2014).\nVery recently, Peng et al. (2017) proposed an incremental learning method of hierarchical softmax. Because hierarchical soft-max and negative sampling have different advantages (Peng et al., 2017), the incremental SGNS and their method are complementary to each other. Also, their updating method needs to scan not only new but also old training data, and thus is not an incremental algorithm in a strict sense. As a consequence, it potentially incurs the same time complexity as the re-training strategy. Additional drawback is that it has to retain the old training data and thus wastes space, while incremental SGNS can discard old training examples after processing them.\nThere are publicly available implementations for training SGNS, one of the most popular being\nWORD2VEC (Mikolov, 2013). However, it does not support an incremental training method. GENSIM (R\u030cehu\u030ar\u030cek and Sojka, 2010) also offers SGNS training. Although GENSIM allows the incremental updating of SGNS models, it is done in an adhoc manner. In GENSIM, the vocabulary set as well as the unigram table are fixed once trained, meaning that new words cannot be added. Also, they do not provide any theoretical accounts for the validity of their training method."}, {"heading": "7 Conclusion and Future Work", "text": "This work proposed incremental SGNS and provided thorough theoretical analysis to demonstrate its validity. We also conducted experiments to empirically demonstrate its effectiveness. Although the incremental model update is often required in practical machine learning applications, only a little attention has been paid to learning word embeddings incrementally. We consider that incremental SGNS successfully addresses this situation and serves as an useful tool for practitioners.\nThe success of this work suggests several research directions to be explored in the future. One possibility is to explore extending other embedding methods such as GloVe (Pennington et al., 2014) to incremental algorithms. Such studies would further extend the potential of word embedding methods."}, {"heading": "A Note on Adaptive Unigram Table", "text": "Algorithm 4 illustrates the efficient implementation of the adaptive unigram table (c.f., Section 3.2.2). In line 8 and 10, F and \u03c4Fz are not always integers and therefore they are probabilistically converted into integers as explained in the paper.\nTime complexity of Algorithm 4 is O(1) per update in case of \u03b1 = 1.0. When |T | < \u03c4 , the update (line 8) takes O(1) time since we always have F = 1. When \u03c4 \u2264 |T |, we have \u03c4 \u2264 z and consequently \u03c4F z \u2264 1. This means that the update (line 10\u201313) takes O(1) time.\nEven if \u03b1 6= 1.0, the value of z becomes sufficiently large in practice, and thus the update becomes efficient as demonstrated in the experiment.\nAlgorithm 4 Adaptive unigram table. 1: f(w)\u2190 0 for all w \u2208 W 2: z \u2190 0 3: for i = 1, . . . , n do 4: f(wi)\u2190 f(wi) + 1 5: F \u2190 f(wi)\u03b1 \u2212 (f(wi)\u2212 1)\u03b1 6: z \u2190 z + F 7: if |T | < \u03c4 then 8: add F copies of wi to T 9: else 10: for t = 1, . . . , \u03c4Fz do 11: j is randomly drawn from [1, |T |] 12: T [j]\u2190 wi 13: end for 14: end if 15: end for"}, {"heading": "B Complete Proofs", "text": "This appendix provides complete proofs of Theorems 1, 3, and 5.\nB.1 Proof of Theorem 1\nProof. The first order moment of \u2206L(\u03b8) can be rewritten as\nE[\u2206L(\u03b8)] = E [ 2ck\nn \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 \u03b4wi,w(qi(v)\u2212 qn(v))\u03c8\u2212w,v ]\n= 2ck\nn \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 E[\u03b4wi,w(qi(v)\u2212 qn(v))\u03c8\u2212w,v]\n= 2ck\nn \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 E[Xi,w(Yi,v \u2212Yn,v)\u03c8\u2212w,v]\n= 2ck\nn \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 ( E[Xi,wYi,v]\u2212 E[Xi,wYn,v] ) \u03c8\u2212w,v.\nHere, for any i and j such that i \u2264 j, we have\nE[Xi,wYj,v] = E[Xi,w 1\nj j\u2211 j\u2032=1 Xj\u2032,v] = 1 j j\u2211 j\u2032=1 E[Xi,wXj\u2032,v]\n= 1\nj j\u2211 j\u2032=1 ( E[Xi,w]E[Xj\u2032,v] + V[Xi,w,Xj\u2032,v] ) = \u00b5w\u00b5v + 1\nj \u03c1w,v.\nTherefore, we have\nE[\u2206L(\u03b8)] = 2ck n \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 ( \u00b5w\u00b5v + 1 i \u03c1w,v \u2212 \u00b5w\u00b5v \u2212 1 n \u03c1w,v ) \u03c8\u2212w,v\n= 2ck(Hn \u2212 1)\nn\n\u2211 w\u2208W \u2211 v\u2208W \u03c1w,v\u03c8 \u2212 w,v.\nB.2 Proof of Theorem 3 To prove Theorem 3, we begin by examining the upper- and lower-bounds of E[Xi,wYj,vYk,v] in the following Lemma, and then make use of the bounds to evaluate the order of the second order moment of \u2206L(\u03b8). Lemma 6. For any j and k such that j \u2264 k, we have\nE[Xi,wYj,vYk,v] \u2264 (jk \u2212 2j \u2212 k + 2)\u00b5w\u00b52v + 2j + k \u2212 2\njk ,\nE[Xi,wYj,vYk,v] \u2265 (jk \u2212 2j \u2212 k + 2)\u00b5w\u00b52v\njk .\nProof. We have\nE[Xi,wYj,vYk,v] = E[Xi,w ( 1\nj j\u2211 l=1 Xl,v )( 1 k k\u2211 m=1 Xm,v ) ]\n= j\u2211 l=1 k\u2211 m=1 E[Xi,wXl,vXm,v] jk .\nTo prove the lemma, we rewrite the expression by splitting the set of (l,m) into two subsets. Let S(j,k)i (j \u2264 k) be a set of (l,m) such that Xi,w, Xl,v, and Xm,v are independent from each other (i.e., i, l, and m are all different), and let S\u0304(j,k)i be its complementary set:\nS(j,k)i = {(l,m) \u2208 {1, 2, . . . , j} \u00d7 {1, 2, . . . , k} | i 6= l \u2227 l 6= m \u2227m 6= i}, S\u0304(j,k)i = {1, 2, . . . , j} \u00d7 {1, 2, . . . , k} \\ S (j,k) i .\nThen, E[Xi,wYj,vYk,v] is upper-bounded as\nE[Xi,wYj,vYk,v] = \u2211\n(l,m)\u2208S(j,k)i\nE[Xi,w]E[Xl,v]E[Xm,v] jk\n+ \u2211\n(l,m)\u2208S\u0304(j,k)i\nE[Xi,wXl,vXm,v] jk\n\u2264 \u2211\n(l,m)\u2208S(j,k)i\n\u00b5w\u00b5 2 v\njk + \u2211 (l,m)\u2208S\u0304(j,k)i 1 jk\n= |S(j,k)i |\u00b5w\u00b52v + |S\u0304 (j,k) i |\njk ,\nwhere the inequality holds because Xi,w, Xl,v, and Xm,v are binary random variables and thus E[Xi,wXl,vXm,v] \u2264 1. Here, we have |S\u0304 (j,k) i | = 2j+ k\u2212 2, because S\u0304 (j,k) i includes j elements such that l = m and also includes k \u2212 1 and j \u2212 1 elements such that i = l 6= m and i = m 6= l, respectively. And we consequently have |S(j,k)i | = jk \u2212 |S\u0304 (j,k) i | = jk \u2212 2j \u2212 k + 2. Therefore, the upper-bound can be rewritten as\nE[Xi,wYj,vYk,v] \u2264 (jk \u2212 2j \u2212 k + 2)\u00b5w\u00b52v + 2j + k \u2212 2\njk .\nSimilarly, by making use of 0 \u2264 E[Xi,wXl,vXm,v], the lower-bound can be derived:\nE[Xi,wYj,vYk,v] = \u2211\n(l,m)\u2208S(j,k)i\nE[Xi,w]E[Xl,v]E[Xm,v] jk\n+ \u2211\n(l,m)\u2208S\u0304(j,k)i\nE[Xi,wXl,vXm,v] jk\n\u2265 \u2211\n(l,m)\u2208S(j,k)i\n\u00b5w\u00b5 2 v\njk + \u2211 (l,m)\u2208S\u0304(j,k)i 0 jk\n= |S(j,k)i |\u00b5w\u00b52v\njk = (jk \u2212 2j \u2212 k + 2)\u00b5w\u00b52v jk .\nMaking use the above Lemma, we can prove Theorem 3.\nProof. The upper-bound of E[\u2206L(\u03b8)2] is examined to prove the theorem. Let \u03a8i,n,w,v = \u03b4wi,w(qi(v)\u2212 qn(v))\u03c8 \u2212 w,v. Making use of Jensen\u2019s inequality, we have\nE[\u2206L(\u03b8)2] = E [ 4c2k2\nn2 (\u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 \u03a8i,n,w,v )2]\n= E [ 4c2k2\nn2 |W|4n2 (\u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 1 |W|2n \u03a8i,n,w,v )2]\n\u2264 E [ 4c2k2\nn2 |W|4n2 \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 1 |W|2n \u03a8i,n,w,v\n]\n= 4c2k2|W|2\nn\n\u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 E[\u03a82i,n,w,v].\nFurthermore, the term E[\u03a82i,n,w,v] is upper-bounded as\nE[\u03a82i,n,w,v] = E[\u03b42wi,v(qi(v)\u2212 qn(v)) 2(\u03c8\u2212w,v) 2]\n= E[\u03b4wi,v(qi(v)\u2212 qn(v))2(\u03c8\u2212w,v)2] = E[Xi,w(Yi,v \u2212Yn,v)2](\u03c8\u2212w,v)2 = (E[Xi,wY2i,v]\u2212 2E[Xi,wYi,vYn,v] + E[Xi,wY2n,v])(\u03c8\u2212w,v)2\n\u2264 { 1\ni2\n( (i2 \u2212 3i+ 2)\u00b5w\u00b52v + 3i\u2212 2 ) \u2212 2 1\nin (in\u2212 2i\u2212 n+ 2)\u00b5w\u00b52v\n+ 1\nn2\n( (n2 \u2212 3n+ 2)\u00b5w\u00b52v + 3n\u2212 2 )} (\u03c8\u2212w,v) 2\n= { (2\u00b5w\u00b5 2 v \u2212 2) 1\ni2 + (\u2212\u00b5w\u00b52v \u2212\n4 n \u00b5w\u00b5 2 v + 3) 1 i\n+ (2\u00b5w\u00b5 2 v \u2212 2)\n1\nn2 + (\u00b5w\u00b5\n2 v + 3)\n1\nn\n} (\u03c8\u2212w,v) 2,\nwhere the above Lemma is used to derive the inequality. Therefore, we have\nn\u2211 i=1 E[\u03a82i,n,w,v] \u2264 n\u2211 i=1 { (2\u00b5w\u00b5 2 v \u2212 2) 1 i2 + (\u2212\u00b5w\u00b52v \u2212 4 n \u00b5w\u00b5 2 v + 3) 1 i\n+ (2\u00b5w\u00b5 2 v \u2212 2)\n1\nn2 + (\u00b5w\u00b5\n2 v + 3)\n1\nn\n} (\u03c8\u2212w,v) 2\n= { (2\u00b5w\u00b5 2 v \u2212 2)Hn,2 + (\u2212\u00b5w\u00b52v \u2212 4\nn \u00b5w\u00b5\n2 v + 3)Hn\n+ (2\u00b5w\u00b5 2 v \u2212 2)\n1 n + (\u00b5w\u00b5 2 v + 3)\n} (\u03c8\u2212w,v) 2,\nwhereHn,2 represents the generalized harmonic number of order n of 2. SinceHn,2 \u2264 Hn = O(log(n)), we have \u2211n i=1 E[\u03a82i,n,w,v] = O(log(n)) and consequently E[\u2206L(\u03b8)2] = O( log(n) n ).\nB.3 Proof of Theorem 5\nProof. The proof is made by the squeeze theorem. Let l = LB(\u03b8\u0302) \u2212 LB(\u03b8\u2217). Then, Chebyshev\u2019s inequality gives, for any 1 > 0,\nlim n\u2192\u221e V[l] 21 \u2265 lim n\u2192\u221e Pr\n[ |l \u2212 E[l]| \u2265 1 ] = lim\nn\u2192\u221e Pr\n[ l \u2212 E[l] \u2264 \u2212 1 ] + Pr [ 1 \u2264 l \u2212 E[l] ] = lim\nn\u2192\u221e Pr\n[ l \u2264 E[l]\u2212 1 ] + Pr [ E[l] + 1 \u2264 l ] .\nRemind that Eq. (2) in Lemma 4 means that for any 2 > 0, there exists n\u2032 such that if n\u2032 \u2264 n then |E[l]| < 2. Therefore we have\nlim n\u2192\u221e V[l] 21 \u2265 lim n\u2192\u221e Pr\n[ l \u2264 E[l]\u2212 1 ] + Pr [ E[l] + 1 \u2264 l ] \u2265 lim\nn\u2192\u221e Pr\n[ l \u2264 \u2212 2 \u2212 1 ] + Pr [ 2 + 1 \u2264 l ] = lim\nn\u2192\u221e Pr\n[ |l| \u2265 1 + 2 ] \u2265 0.\nThe arbitrary property of 1 and 2 allows 1 + 2 to be rewritten as . Also, Eq. (3) in Lemma 4 implies that limn\u2192\u221e\nV[l] 21 = 0. Therefore, the squeeze theorem gives the proof."}, {"heading": "C Theoretical Analysis of Mini-batch SGNS", "text": "This appendix demonstrates that Theorems 2 and 3 also hold for the mini-batch SGNS, that is, the first and second order moments of \u2206L(\u03b8) are in the order of O( log(n)n ). We here investigate the mini-batch setting in which M words, as opposed to a single word in the case of incremental SGNS, are processed at a time.\nDefinition 4. Let Y(M)i,w be a random variable that represents qi(w) when \u03b1 = 1.0 and the mini-batch size is M . Then, it is given as\nY (M) i,w = Yb(i,M),w\nwhere b(i,M) = d iM e \u00d7M . Note that we always have Y (M) n,w = Yn,w and i \u2264 b(i,M).\nWe first examine the first order moment of \u2206L(\u03b8) by taking a similar step as the proof of Theorem 1. The first order moment of \u2206L(\u03b8) is given as\nE[\u2206L(\u03b8)] = 2ck n \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 ( E[Xi,wY (M) j,v ]\u2212 E[Xi,wY (M) n,v ] ) \u03c8\u2212w,v\n= 2ck\nn \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 ( E[Xi,wY (M) j,v ]\u2212 E[Xi,wYn,v] ) \u03c8\u2212w,v\n= 2ck\nn \u2211 w\u2208W \u2211 v\u2208W \u03c1w,v\u03c8 \u2212 w,v ( n\u2211 i=1\n1\nb(i,M) \u2212 n\u2211 i=1 1 n ) .\nBecause we have\nn\u2211 i=1\n1\nb(i,M) \u2264 n\u2211 i=1 1 i = Hn = O(log(n)), (5)\nwe have E[\u2206L(\u03b8)] = O( log(n)n ). Next, we investigate the second order moment of E[\u2206L(\u03b8)]. Analogously to the last inequality of the proof of Theorem 3, we have\nn\u2211 i=1 E[\u03a82i,n,w,v] \u2264 n\u2211 i=1 { (2\u00b5w\u00b5 2 v \u2212 2)\n1\nb(i,M)2 + (\u2212\u00b5w\u00b52v \u2212\n4 n \u00b5w\u00b5 2 v + 3)\n1\nb(i,M)\n+ (2\u00b5w\u00b5 2 v \u2212 2)\n1\nn2 + (\u00b5w\u00b5\n2 v + 3)\n1\nn\n} (\u03c8\u2212w,v) 2.\nSince we have\nn\u2211 i=1\n1\nb(i,M)2 \u2264 n\u2211 i=1 1 i2 = Hn,2 = O(log(n)), (6)\nit can be proven that E[\u2206L(\u03b8)2] = O( log(n)n )."}, {"heading": "D Theoretical Analysis in Smoothed Case", "text": "This appendix investigates the convergence of the first and second order moment of \u2206L(\u03b8) in the smoothed case.\nD.1 Convergence of the first order moment of \u2206L(\u03b8)\nThe first order moment of \u2206L(\u03b8) in the smoothed case is given as\nE[\u2206L(\u03b8)] = 2ck n \u2211 w\u2208W \u2211 v\u2208W n\u2211 i=1 ( E[Xi,wZi,v]\u2212 E[Xi,wZn,v] ) \u03c8\u2212w,v.\nLet us investigate E[Xi,wZj,v] as we did E[Xi,wYj,v] in the unsmoothed case. Let \u03c6w = gw(\u00b5) \u2212\u2211 v\u2208WMw,vgv(\u00b5). Then, for any i and j such that i \u2264 j, we have\nE[Xi,wZj,v] \u2248 E[Xi,w ( gv(\u00b5) + \u2211 v\u2032\u2208W Mv,v\u2032(Yj,v\u2032 \u2212 gv\u2032(\u00b5)) ) ]\n= E[Xi,w( \u2211 v\u2032\u2208W Mv,v\u2032Yj,v\u2032 + \u03c6v)]\n= \u2211 v\u2032\u2208W Mv,v\u2032E[Xi,wYj,v\u2032 ] + \u03c6vE[Xi,w]\n= \u2211 v\u2032\u2208W Mv,v\u2032(\u00b5w\u00b5v\u2032 + 1 j \u03c1w,v\u2032) + \u00b5w\u03c6v\n= \u2211 v\u2032\u2208W Mv,v\u2032\u00b5w\u00b5v\u2032 + \u00b5w\u03c6v + 1 j \u2211 v\u2032\u2208W Mv,v\u2032\u03c1w,v\u2032 .\nTherefore, plugging the above equation into E[\u2206L(\u03b8)] yields E[\u2206L(\u03b8)] \u2248 O( log(n)n ).\nD.2 Convergence of the second order moment of \u2206L(\u03b8) Next, let us examine the convergence of the second order moment of \u2206L(\u03b8). This can be confirmed by inspecting E[Xi,wZj,vZk,v] and then E[\u03a82i,n,w,v] analogously to the unsmoothed case.\nFor any i, j, and k such that i \u2264 j \u2264 k, we have\nE[Xi,wZj,vZk,v] \u2248 E[Xi,w (\u2211 v\u2032\u2208W Mv,v\u2032Yj,v\u2032 + \u03c6v )( \u2211 v\u2032\u2032\u2208W Mv,v\u2032\u2032Yk,v\u2032\u2032 + \u03c6v ) ]\n= \u2211 v\u2032\u2208W \u2211 v\u2032\u2032\u2208W Mv,v\u2032Mv,v\u2032\u2032E[Xi,wYj,v\u2032Yk,v\u2032\u2032 ]\n+ \u2211 v\u2032\u2208W Mv,v\u2032\u03c6vE[Xi,wYj,v\u2032 ] + \u2211 v\u2032\u2032\u2208W Mv,v\u2032\u2032\u03c6vE[Xi,wYk,v\u2032\u2032 ] + \u03c62vE[Xi,w]\n= \u2211 v\u2032\u2208W \u2211 v\u2032\u2032\u2208W Mv,v\u2032Mv,v\u2032\u2032E[Xi,wYj,v\u2032Yk,v\u2032\u2032 ]\n+ \u2211 v\u2032\u2208W Mv,v\u2032\u03c6v(\u00b5w\u00b5v\u2032 + 1 j \u03c1w,v\u2032)\n+ \u2211 v\u2032\u2032\u2208W Mv,v\u2032\u2032\u03c6v(\u00b5w\u00b5v\u2032\u2032 + 1 k \u03a3w,v\u2032\u2032) + \u00b5w\u03c6 2 v.\nTherefore, we have\nE[\u03a82i,n,w,v] = E[Xi,w(Zi,v \u2212 Zn,v)2]\u03c82w,v \u2248 \u2211 v\u2032\u2208W \u2211 v\u2032\u2032\u2208W Mv,v\u2032Mv,v\u2032\u2032 ( E[Xi,wYi,v\u2032Yi,v\u2032\u2032 ]\n\u2212 2E[Xi,wYi,v\u2032Yn,v\u2032\u2032 ] + E[Xi,wYn,v\u2032Yn,v\u2032\u2032 ] ) \u03c82w,v.\nUsing similar bounds to Lemma 3, we also have \u2211n\ni=1 E[\u03a82i,n,w,v] \u2248 O(log(n)) and consequently E[\u2206L(\u03b8)2] \u2248 O( log(n)n )."}, {"heading": "E Experimental Configurations", "text": "This appendix details the experimental configurations that are not described in the paper.\nE.1 Verification of theorems The vocabulary set in the Gigaword corpus was reduced to 1000 by converting infrequent words into the same special tokens because it is expensive to evaluate the expectation terms in \u2206L(\u03b8) for a large vocabulary set.\nThe parameter \u03b8 was set to 100-dimensional vectors each element of which is drawn from [\u22120.5, 0.5] uniformly at random. In preliminary experiments we confirmed that the result is not sensitive to the choice of the parameter value. Note that the same parameter value was used for all n. We set c and k as c = 5 and k = 5.\nThe mean \u00b5w and covariances \u03c1w,v are required to compute the theoretical value of the first order moment. They were given as the maximum likelihood estimations from the entire Gigaword corpus.\nE.2 Quality of word embeddings Table 2 summarizes the training configurations. Those parameter values were used for both incremental and batch SGNS. The learning rate was set to 0.1 for incremental and batch, which use AdaGrad to adjust the learning rate. On the other hand, the learning rate of w2v, which uses linear decay function to adjust the learning rate, was set as the default value of 0.025.\nIn the word similarity and the analogy tasks, we use tw + cw as an embedding of the word w (Pennington et al., 2014; Levy et al., 2015). The analogy task was performed by using 3CosMul (Levy et al., 2015).\nE.3 Update time The experiment was conducted on Intel R\u00a9 Xeon R\u00a9 2GHz CPU. The update time was averaged over five trials."}], "references": [{"title": "Multimodal distributional semantics", "author": ["E. Bruni", "N.K. Tran", "M. Baroni."], "venue": "Journal of Artificial Intelligence Research 49:1\u201349.", "citeRegEx": "Bruni et al\\.,? 2013", "shortCiteRegEx": "Bruni et al\\.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Weighted random sampling over data streams", "author": ["Pavlos S. Efraimidis."], "venue": "ArXiv:1012.0256.", "citeRegEx": "Efraimidis.,? 2015", "shortCiteRegEx": "Efraimidis.", "year": 2015}, {"title": "Approximate scalable bounded space sketch for large data nlp", "author": ["Amit Goyal", "Hal Daume III."], "venue": "Proceedings of EMNLP. pages 250\u2013261. http://www.aclweb.org/anthology/D11-1023.", "citeRegEx": "Goyal and III.,? 2011", "shortCiteRegEx": "Goyal and III.", "year": 2011}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics 41:665\u2013695. http://aclweb.org/anthology/J/J15/J15-4004.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225. https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Workshop at ICLR.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in NIPS. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-Tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of NAACL. pages 746\u2013751. http://www.aclweb.org/anthology/N13-1090.", "citeRegEx": "Mikolov et al\\.,? 2013c", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Finding repeated elements", "author": ["Jayadev Misra", "David Gries."], "venue": "Science of Computer Programming 2(2):143\u2013152.", "citeRegEx": "Misra and Gries.,? 1982", "shortCiteRegEx": "Misra and Gries.", "year": 1982}, {"title": "Annotated english gigaword ldc2012t21", "author": ["Courtney Napoles", "Matthew Gormley", "Benjamin Van Durme"], "venue": null, "citeRegEx": "Napoles et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Napoles et al\\.", "year": 2012}, {"title": "Incrementally learning the hierarchical softmax function for neural language models", "author": ["Hao Peng", "Jianxin Li", "Yangqiu Song", "Yaopeng Liu."], "venue": "Proceedings of AAAI (to appear).", "citeRegEx": "Peng et al\\.,? 2017", "shortCiteRegEx": "Peng et al\\.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of EMNLP. pages 1532\u20131543. http://www.aclweb.org/anthology/D141162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Software framework for topic modelling with large corpora", "author": ["Radim \u0158eh\u016f\u0159ek", "Petr Sojka."], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. pages 45\u201350.", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka.,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka.", "year": 2010}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Peter D. Turney", "Patrick Pantel."], "venue": "Journal of Artificial Intelligence Research 37:141\u2013188.", "citeRegEx": "Turney and Pantel.,? 2010", "shortCiteRegEx": "Turney and Pantel.", "year": 2010}, {"title": "Random sampling with a reservoir", "author": ["Jeffrey S. Vitter."], "venue": "ACM Transactions on Mathematical Software 11:37\u201357.", "citeRegEx": "Vitter.,? 1985", "shortCiteRegEx": "Vitter.", "year": 1985}, {"title": "Online updating of word representations for part-of-speech tagging", "author": ["Wenpeng Yin", "Tobias Schnabel", "Hinrich Sch\u00fctze."], "venue": "Proceedings of EMNLP. pages 1329\u20131334. http://aclweb.org/anthology/D15-1155.", "citeRegEx": "Yin et al\\.,? 2015", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "For example, negative sampling (Mikolov et al., 2013b) needs to precompute the noise distribution from the entire training data before performing Stochastic Gradient Descent (SGD).", "startOffset": 31, "endOffset": 54}, {"referenceID": 7, "context": "Similarly, hierarchical soft-max (Mikolov et al., 2013b) has to determine the tree structure and GloVe (Pennington et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 12, "context": ", 2013b) has to determine the tree structure and GloVe (Pennington et al., 2014) has to count co-occurrence frequencies before performing SGD.", "startOffset": 55, "endOffset": 80}, {"referenceID": 11, "context": "Another situation is learning word embeddings from ever-evolving data such as news articles and microbologs (Peng et al., 2017).", "startOffset": 108, "endOffset": 127}, {"referenceID": 7, "context": "This paper investigates an incremental training method of word embeddings with a focus on the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013b) for its popularity.", "startOffset": 140, "endOffset": 163}, {"referenceID": 6, "context": "Since it is too expensive to optimize the above objective, Mikolov et al. (2013b) proposed negative sampling to speed up skip-gram training.", "startOffset": 59, "endOffset": 82}, {"referenceID": 1, "context": "The learning rate for SGD is adjusted by using AdaGrad (Duchi et al., 2011).", "startOffset": 55, "endOffset": 75}, {"referenceID": 9, "context": "The Misra-Gries algorithm (Misra and Gries, 1982) is used to approximately keep track of top-m frequent words during training, and those words are used as the dynamic vocabulary set.", "startOffset": 26, "endOffset": 49}, {"referenceID": 15, "context": "2 We propose a reservoir-based algorithm for efficiently updating the unigram table (Vitter, 1985; Efraimidis, 2015) (Algorithm 3).", "startOffset": 84, "endOffset": 116}, {"referenceID": 2, "context": "2 We propose a reservoir-based algorithm for efficiently updating the unigram table (Vitter, 1985; Efraimidis, 2015) (Algorithm 3).", "startOffset": 84, "endOffset": 116}, {"referenceID": 15, "context": "See (Vitter, 1985; Efraimidis, 2015) for reference.", "startOffset": 4, "endOffset": 36}, {"referenceID": 2, "context": "See (Vitter, 1985; Efraimidis, 2015) for reference.", "startOffset": 4, "endOffset": 36}, {"referenceID": 10, "context": "Datasets of various sizes were constructed from the English Gigaword corpus (Napoles et al., 2012).", "startOffset": 76, "endOffset": 98}, {"referenceID": 8, "context": "For the comparison, both our own implementation of batch SGNS as well as WORD2VEC (Mikolov et al., 2013c) were used (denoted as batch and w2v).", "startOffset": 82, "endOffset": 105}, {"referenceID": 5, "context": "The learned word embeddings were assessed on five benchmark datasets commonly used in the literature (Levy et al., 2015): WordSim353 (Agirre et al.", "startOffset": 101, "endOffset": 120}, {"referenceID": 0, "context": ", 2009), MEN (Bruni et al., 2013), SimLex-999 (Hill et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 4, "context": ", 2013), SimLex-999 (Hill et al., 2015), the MSR analogy dataset (Mikolov et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 8, "context": ", 2015), the MSR analogy dataset (Mikolov et al., 2013c), the Google analogy dataset (Mikolov et al.", "startOffset": 33, "endOffset": 56}, {"referenceID": 6, "context": ", 2013c), the Google analogy dataset (Mikolov et al., 2013a).", "startOffset": 37, "endOffset": 60}, {"referenceID": 14, "context": "Word representations based on distributional semantics have been common (Turney and Pantel, 2010; Baroni and Lenci, 2010).", "startOffset": 72, "endOffset": 121}, {"referenceID": 16, "context": "Although some investigated incremental updating of the word-context matrix (Yin et al., 2015; Goyal and Daume III, 2011), they did not explore the reduced representations.", "startOffset": 75, "endOffset": 120}, {"referenceID": 12, "context": "However, most previous studies have not investigated incremental strategies (Mikolov et al., 2013a,b; Pennington et al., 2014).", "startOffset": 76, "endOffset": 126}, {"referenceID": 11, "context": "Because hierarchical soft-max and negative sampling have different advantages (Peng et al., 2017), the incremental SGNS and their method are complementary to each other.", "startOffset": 78, "endOffset": 97}, {"referenceID": 13, "context": "GENSIM (\u0158eh\u016f\u0159ek and Sojka, 2010) also offers SGNS training.", "startOffset": 7, "endOffset": 32}, {"referenceID": 6, "context": "However, most previous studies have not investigated incremental strategies (Mikolov et al., 2013a,b; Pennington et al., 2014). Very recently, Peng et al. (2017) proposed an incremental learning method of hierarchical softmax.", "startOffset": 77, "endOffset": 162}], "year": 2017, "abstractText": "This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SNGS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SNGS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.", "creator": "TeX"}}}