{"id": "1704.04521", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation", "abstract": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In NMTs, words that are out of vocabulary are represented by a single unknown token. In this paper, we propose a method that enables NMT to translate patent sentences comprising a large vocabulary of technical terms. We train an NMT system on bilingual data wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Further, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using SMT. We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens. Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.", "histories": [["v1", "Fri, 14 Apr 2017 19:36:54 GMT  (2383kb,D)", "http://arxiv.org/abs/1704.04521v1", "WAT 2016. arXiv admin note: substantial text overlap witharXiv:1704.04520"]], "COMMENTS": "WAT 2016. arXiv admin note: substantial text overlap witharXiv:1704.04520", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zi long", "takehito utsuro", "tomoharu mitsuhashi", "mikio yamamoto"], "accepted": false, "id": "1704.04521"}, "pdf": {"name": "1704.04521.pdf", "metadata": {"source": "CRF", "title": "Translation of Patent Sentences with a Large Vocabulary of Technical Terms Using Neural Machine Translation", "authors": ["Zi Long", "Takehito Utsuro", "Tomoharu Mitsuhashi", "Mikio Yamamoto"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms.\nThere have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed to replace out-of-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and unknown words as sequences of subword units. Luong and Manning (2016) provided a character-level\nar X\niv :1\n70 4.\n04 52\n1v 1\n[ cs\n.C L\n] 1\n4 A\npr 2\n01 7\nand word-level hybrid NMT model to achieve an open vocabulary, and Costa-jussa\u0300 and Fonollosa (2016) proposed a NMT system based on character-based embeddings.\nHowever, these previous approaches have limitations when translating patent sentences. This is because their methods only focus on addressing the problem of unknown words even though the words are parts of technical terms. It is obvious that a technical term should be considered as one word that comprises components that always have different meanings and translations when they are used alone. An example is shown in Figure1, wherein Japanese word \u201c \u201d(bridge) should be translated to Chinese word \u201c \u201d when included in technical term \u201cbridge interface\u201d; however, it is always translated as \u201c \u201d.\nIn this paper, we propose a method that enables NMT to translate patent sentences with a large vocabulary of technical terms. We use an NMT model similar to that used by Sutskever et al. (2014), which uses a deep long short-term memories (LSTM) (Hochreiter and Schmidhuber, 1997) to encode the input sentence and a separate deep LSTM to output the translation. We train the NMT model on a bilingual corpus in which the technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Similar to Sutskever et al. (2014), we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using statistical machine translation (SMT). We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT and NMT scores of the translated sentences that have been rescored with the technical term tokens. Our experiments on Japanese-Chinese patent sentences show that our proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over a traditional SMT system and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique."}, {"heading": "2 Japanese-Chinese Patent Documents", "text": "Japanese-Chinese parallel patent documents were collected from the Japanese patent documents published by the Japanese Patent Office (JPO) during 2004-2012 and the Chinese patent documents published by the State Intellectual Property Office of the People\u2019s Republic of China (SIPO) during 2005- 2010. From the collected documents, we extracted 312,492 patent families, and the method of Utiyama and Isahara (2007) was applied1 to the text of the extracted patent families to align the Japanese and Chinese sentences. The Japanese sentences were segmented into a sequence of morphemes using the Japanese morphological analyzer MeCab2 with the morpheme lexicon IPAdic,3 and the Chinese sentences were segmented into a sequence of words using the Chinese morphological analyzer Stanford Word Segment (Tseng et al., 2005) trained using the Chinese Penn Treebank. In this study, JapaneseChinese parallel patent sentence pairs were ordered in descending order of sentence-alignment score\n1 Herein, we used a Japanese-Chinese translation lexicon comprising around 170,000 Chinese entries. 2 http://mecab.sourceforge.net/ 3 http://sourceforge.jp/projects/ipadic/\nand we used the topmost 2.8M pairs, whose Japanese sentences contain fewer than 40 morphemes and Chinese sentences contain fewer than 40 words.4"}, {"heading": "3 Neural Machine Translation (NMT)", "text": "NMT uses a single neural network trained jointly to maximize the translation performance (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a). Given a source sentence x = (x1, . . . , xN ) and target sentence y = (y1, . . . , yM ), an NMT system uses a neural network to parameterize the conditional distributions\np(yl | y<l,x)\nfor 1 \u2264 l \u2264 M . Consequently, it becomes possible to compute and maximize the log probability of the target sentence given the source sentence\nlog p(y | x) = M\u2211 l=1 log p(yl|y<l,x) (1)\nIn this paper, we use an NMT model similar to that used by Sutskever et al. (2014). It uses two separate deep LSTMs to encode the input sequence and output the translation. The encoder, which is implemented as a recurrent neural network, reads the source sentence one word at a time and then encodes it into a large vector that represents the entire source sentence. The decoder, another recurrent neural network, generates a translation on the basis of the encoded vector one word at a time.\nOne important difference between our NMT model and the one used by Sutskever et al. (2014) is that we added an attention mechanism. Recently, Bahdanau et al. (2015) proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences. Luong et al. (2015a) proposed an attention mechanism for different scoring functions in order to compare the source and target hidden states as well as different strategies for placing the attention. In this paper, we utilize the attention mechanism proposed by Bahdanau et al. (2015), wherein each output target word is predicted on the basis of not only a recurrent hidden state and the previously predicted word but also a context vector computed as the weighted sum of the hidden states."}, {"heading": "4 NMT with a Large Technical Term Vocabulary", "text": ""}, {"heading": "4.1 NMT Training after Replacing Technical Term Pairs with Tokens", "text": "Figure 2 illustrates the procedure of the training model with parallel patent sentence pairs, wherein technical terms are replaced with technical term tokens \u201cTT1\u201d, \u201cTT2\u201d, . . ..\nIn the step 1 of Figure 2, we align the Japanese technical terms, which are automatically extracted from the Japanese sentences, with their Chinese translations in the Chinese sentences.5 Here, we introduce the following two steps to identify technical term pairs in the bilingual Japanese-Chinese corpus:\n1. According to the approach proposed by Dong et al. (2015), we identify Japanese-Chinese technical term pairs using an SMT phrase translation table. Given a parallel sentence pair \u3008SJ , SC\u3009 containing a Japanese technical term tJ , the Chinese translation candidates collected from the phrase translation table are matched against the Chinese sentence SC of the parallel sentence pair. Of those found in SC , tC with the largest translation probability P (tC | tJ) is selected, and the bilingual technical term pair \u3008tJ , tC\u3009 is identified.\n4 In this paper, we focus on the task of translating patent sentences with a large vocabulary of technical terms using the NMT system, where we ignore the translation task of patent sentences that are longer than 40 morphemes in Japanese side or longer than 40 words in Chinese side.\n5 In this work, we approximately regard all the Japanese compound nouns as Japanese technical terms. These Japanese compound nouns are automatically extracted by simply concatenating a sequence of morphemes whose parts of speech are either nouns, prefixes, suffixes, unknown words, numbers, or alphabetical characters. Here, morpheme sequences starting or ending with certain prefixes are inappropriate as Japanese technical terms and are excluded. The sequences that include symbols or numbers are also excluded. In Chinese side, on the other hand, we regard Chinese translations of extracted Japanese compound nouns as Chinese technical terms, where we do not regard other Chinese phrases as technical terms.\n2. For the Japanese technical terms whose Chinese translations are not included in the results of Step 1, we then use an approach based on SMT word alignment. Given a parallel sentence pair \u3008SJ , SC\u3009 containing a Japanese technical term tJ , a sequence of Chinese words is selected using SMT word alignment, and we use the Chinese translation tC for the Japanese technical term tJ .6\nAs shown in the step 2 of Figure 2, in each of Japanese-Chinese parallel patent sentence pairs, occurrences of technical term pairs \u3008t 1J , t1C\u3009, \u3008t2J , t2C\u3009, . . ., \u3008tkJ , tkC\u3009 are then replaced with technical term tokens \u3008TT1, TT1\u3009, \u3008TT2, TT2\u3009, . . ., \u3008TTk, TTk\u3009. Technical term pairs \u3008t1J , t1C\u3009, \u3008t2J , t2C\u3009, . . ., \u3008tkJ , tkC\u3009 are numbered in the order of occurrence of Japanese technical terms t iJ (i = 1, 2, . . . , k) in each Japanese sentence SJ . Here, note that in all the parallel sentence pairs \u3008SJ , SC\u3009, technical term tokens \u201cTT1\u201d, \u201cTT2\u201d, . . . that are identical throughout all the parallel sentence pairs are used in this procedure. Therefore, for example, in all the Japanese patent sentences SJ , the Japanese technical term t 1J which appears earlier than other Japanese technical terms in SJ is replaced with TT1. We then train the NMT system on a bilingual corpus, in which the technical term pairs is replaced by \u201cTTi\u201d (i = 1, 2, . . .) tokens, and obtain an NMT model in which the technical terms are represented as technical term tokens.7"}, {"heading": "4.2 NMT Decoding and SMT Technical Term Translation", "text": "Figure 3 illustrates the procedure for producing Chinese translations via decoding the Japanese sentence using the method proposed in this paper. In the step 1 of Figure 3, when given an input Japanese sentence, we first automatically extract the technical terms and replace them with the technical term tokens \u201cTTi\u201d (i = 1, 2, . . .). Consequently, we have an input sentence in which the technical term tokens \u201cTTi\u201d (i = 1, 2, . . .) represent the positions of the technical terms and a list of extracted Japanese technical terms. Next, as shown in the step 2-N of Figure 3, the source Japanese sentence with technical term tokens is translated using the NMT model trained according to the procedure described in Section 4.1, whereas the extracted Japanese technical terms are translated using an SMT phrase translation table in the step 2-S of Figure 3.8 Finally, in the step 3, we replace the technical term tokens \u201cTTi\u201d (i = 1, 2, . . .) of the sentence translation with SMT the technical term translations.\n6 We discard discontinuous sequences and only use continuous ones. 7 We treat the NMT system as a black box, and the strategy we present in this paper could be applied to any NMT system (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a). 8 We use the translation with the highest probability in the phrase translation table. When an input Japanese technical term has multiple translations with the same highest probability or has no translation in the phrase translation table, we apply a"}, {"heading": "4.3 NMT Rescoring of 1,000-best SMT Translations", "text": "As shown in the step 1 of Figure 4, similar to the approach of NMT rescoring provided in Sutskever et al.(2014), we first obtain 1,000-best translation list of the given Japanese sentence using the SMT system. Next, in the step 2, we then replace the technical terms in the translation sentences with technical term tokens \u201cTTi\u201d (i = 1, 2, 3, . . .), which must be the same with the tokens of their source Japanese technical terms in the input Japanese sentence. The technique used for aligning Japanese technical terms with their Chinese translations is the same as that described in Section 4.1. In the step 3 of Figure 4, the 1,000- best translations, in which technical terms are represented as tokens, are rescored using the NMT model trained according to the procedure described in Section 4.1. Given a Japanese sentence SJ and its 1,000- best Chinese translations S nC (n = 1, 2, . . . , 1, 000) translated by the SMT system, NMT score of each translation sentence pair \u3008SJ , SnC\u3009 is computed as the log probability log p(SnC | SJ) of Equation (1). Finally, we rerank the 1,000-best translation list on the basis of the average SMT and NMT scores and output the translation with the highest final score."}, {"heading": "5 Evaluation", "text": ""}, {"heading": "5.1 Training and Test Sets", "text": "We evaluated the effectiveness of the proposed NMT system in translating the Japanese-Chinese parallel patent sentences described in Section 2. Among the 2.8M parallel sentence pairs, we randomly extracted 1,000 sentence pairs for the test set and 1,000 sentence pairs for the development set; the remaining sentence pairs were used for the training set.\nAccording to the procedure of Section 4.1, from the Japanese-Chinese sentence pairs of the training set, we collected 6.5M occurrences of technical term pairs, which are 1.3M types of technical term pairs with 800K unique types of Japanese technical terms and 1.0M unique types of Chinese technical terms. Out of the total 6.5M occurrences of technical term pairs, 6.2M were replaced with technical term tokens\ncompositional translation generation approach, wherein Chinese translation is generated compositionally from the constituents of Japanese technical terms.\nusing the phrase translation table, while the remaining 300K were replaced with technical term tokens using the word alignment.9 We limited both the Japanese vocabulary (the source language) and the Chinese vocabulary (the target language) to 40K most frequently used words.\nWithin the total 1,000 Japanese patent sentences in the test set, 2,244 occurrences of Japanese technical terms were identified, which correspond to 1,857 types."}, {"heading": "5.2 Training Details", "text": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for a phrase-based SMT models.\nFor the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Sutskever et al. (2014). We used a deep LSTM neural network comprising three layers, with 512 cells in each layer, and a 512-dimensional word embedding. Similar to Sutskever et al. (2014), we reversed the words in the source sentences and ensure that all sentences in a minibatch are roughly the same length. Further training details are given below:\n\u2022 All of the LSTM\u2019s parameter were initialized with a uniform distribution ranging between -0.06 and 0.06.\n\u2022 We set the size of a minibatch to 128.\n\u2022 We used the stochastic gradient descent, beginning at a learning rate of 0.5. We computed the perplexity of the development set using the currently produced NMT model after every 1,500 minibatches were trained and multiplied the learning rate by 0.99 when the perplexity did not decrease with respect to the last three perplexities. We trained our model for a total of 10 epoches.\n\u2022 Similar to Sutskever et al. (2014), we rescaled the normalized gradient to ensure that its norm does not exceed 5.\nWe implement the NMT system using TensorFlow,10 an open source library for numerical computation. The training time was around two days when using the described parameters on an 1-GPU machine.\n9 There are also Japanese technical terms (3% of all the extracted terms) for which Chinese translations can be identified using neither the SMT phrase translation table nor the SMT word alignment.\n10 https://www.tensorflow.org/"}, {"heading": "5.3 Evaluation Results", "text": "We calculated automatic evaluation scores for the translation results using two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). As shown in Table 1, we report the evaluation scores, on the basis of the translations by Moses (Koehn et al., 2007), as the baseline SMT11 and the scores based on translations produced by the equivalent NMT system without our proposed approach as the baseline NMT. As shown in Table 1, the two versions of the proposed NMT systems clearly improve the translation quality when compared with the baselines. When compared with the baseline SMT, the performance gain of the proposed system is approximately 3.1 BLEU points if translations are produced by the proposed NMT system of Section 4.3 or 2.3 RIBES points if translations are produced by the proposed NMT system of Section 4.2. When compared with the result of decoding with the baseline NMT, the proposed NMT system of Section 4.2 achieved performance gains of 0.8 RIBES points. When compared with the result of reranking with the baseline NMT, the proposed NMT system of Section 4.3 can still achieve performance gains of 0.6 BLEU points. Moreover, when the output translations produced by NMT decoding and SMT technical term translation described in Section 4.2 with the output translations produced by decoding with the baseline NMT, the number of unknown tokens included in output translations reduced from 191 to 92. About 90% of remaining unknown tokens correspond to numbers, English words, abbreviations, and symbols.12\nIn this study, we also conducted two types of human evaluation according to the work of Nakazawa et al. (2015): pairwise evaluation and JPO adequacy evaluation. During the procedure of pairwise evaluation, we compare each of translations produced by the baseline SMT with that produced by the two versions of the proposed NMT systems, and judge which translation is better, or whether they are with comparable quality. The score of pairwise evaluation is defined by the following formula, where W\n11 We train the SMT system on the same training set and tune it with development set. 12 In addition to the two versions of the proposed NMT systems presented in Section 4, we evaluated a modified version of the propsed NMT system, where we introduce another type of token corresponding to unknown compound nouns and integrate this type of token with the technical term token in the procedure of training the NMT model. We achieved a slightly improved translation performance, BLEU/RIBES scores of 55.6/90.9 for the proposed NMT system of Section 4.2 and those of 55.7/89.5 for the proposed NMT system of Section 4.3.\nis the number of better translations compared to the baseline SMT, L the number of worse translations compared to the baseline SMT, and T the number of translations having their quality comparable to those produced by the baseline SMT:\nscore = 100\u00d7 W \u2212 L W + L+ T\nThe score of pairwise evaluation ranges from \u2212100 to 100. In the JPO adequacy evaluation, Chinese translations are evaluated according to the quality evaluation criterion for translated patent documents proposed by the Japanese Patent Office (JPO).13 The JPO adequacy criterion judges whether or not the technical factors and their relationships included in Japanese patent sentences are correctly translated into Chinese, and score Chinese translations on the basis of the percentage of correctly translated information, where the score of 5 means all of those information are translated correctly, while that of 1 means most of those information are not translated correctly. The score of the JPO adequacy evaluation is defined as the average over the whole test sentences. Unlike the study conducted Nakazawa et al. (Nakazawa et al., 2015), we randomly selected 200 sentence pairs from the test set for human evaluation, and both human evaluations were conducted using only one judgement. Table 2 shows the results of the human evaluation for the baseline SMT, the baseline NMT, and the proposed NMT system. We observed that the proposed system achieved the best performance for both pairwise evaluation and JPO adequacy evaluation when we replaced technical term tokens with SMT technical term translations after decoding the source sentence with technical term tokens.\nThroughout Figure 5\u223cFigure 7, we show an identical source Japanese sentence and each of its translations produced by the two versions of the proposed NMT systems, compared with translations produced by the three baselines, respectively. Figure 5 shows an example of correct translation produced by the proposed system in comparison to that produced by the baseline SMT. In this example, our model correctly translates the Japanese sentence into Chinese, whereas the translation by the baseline SMT is a translation error with several erroneous syntactic structures. As shown in Figure 6, the second example highlights that the proposed NMT system of Section 4.2 can correctly translate the Japanese technical term \u201c \u201d(laminated wafer) to the Chinese technical term \u201c \u201d. The translation by the baseline NMT is a translation error because of not only the erroneously translated unknown token but also the Chinese word \u201c \u201d, which is not appropriate as a component of a Chinese technical term. Another example is shown in Figure 7, where we compare the translation of a reranking SMT 1,000-best translation produced by the proposed NMT system with that produced by reranking with the baseline NMT. It is interesting to observe that compared with the baseline NMT, we obtain a better translation when we rerank the 1,000-best SMT translations using the proposed NMT system, in which technical term\n13 https://www.jpo.go.jp/shiryou/toushin/chousa/pdf/tokkyohonyaku_hyouka/01.pdf (in Japanese)\ntokens represent technical terms. It is mainly because the correct Chinese translation \u201c \u201d(wafter) of Japanese word \u201c \u201d is out of the 40K NMT vocabulary (Chinese), causing reranking with the baseline NMT to produce the translation with an erroneous construction of \u201cnoun phrase of noun phrase of noun phrase\u201d. As shown in Figure 7, the proposed NMT system of Section 4.3 produced the translation with a correct construction, mainly because Chinese word \u201c \u201d(wafter) is a part of Chinese technical term \u201c \u201d(laminated wafter) and is replaced with a technical term token and then rescored by the NMT model (with technical term tokens \u201cTT1\u201d, \u201cTT2\u201d, . . .)."}, {"heading": "6 Conclusion", "text": "In this paper, we proposed an NMT method capable of translating patent sentences with a large vocabulary of technical terms. We trained an NMT system on a bilingual corpus, wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except the technical terms. Similar to Sutskever et al. (2014), we used it as a decoder to translate the source sentences with technical term tokens and replace the tokens with technical terms translated using SMT. We also used it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of NMT rescoring of translated sentences with technical term tokens. For the translation of Japanese patent sentences, we observed that our proposed NMT system performs better than the phrase-based SMT system as well as the equivalent NMT system without our proposed approach.\nOne of our important future works is to evaluate our proposed method in the NMT system proposed by Bahdanau et al. (2015), which introduced a bidirectional recurrent neural network as encoder and is the state-of-the-art of pure NMT system recently. However, the NMT system proposed by Bahdanau et\nal. (2015) also has a limitation in addressing out-of-vocabulary words. Our proposed NMT system is expected to improve the translation performance of patent sentences by applying approach of Bahdanau et al. (2015). Another important future work is to quantitatively compare our study with the work of Luong et al. (2015b). In the work of Luong et al. (2015b), they replace low frequency single words and translate them in a post-processing Step using a dictionary, while we propose to replace the whole technical terms and post-translate them with phrase translation table of SMT system. Therefore, our proposed NMT system is expected to be appropriate to translate patent documents which contain many technical terms comprised of multiple words and should be translated together. We will also evaluate the present study by reranking the n-best translations produced by the proposed NMT system on the basis of their SMT rescoring. Next, we will rerank translations from both the n-best SMT translations and n-best NMT translations. As shown in Section 5.3, the decoding approach of our proposed NMT system achieved the best RIBES performance and human evaluation scores in our experiments, whereas the reranking approach achieved the best performance with respect to BLEU. A translation with the highest average SMT and NMT scores of the n-best translations produced by NMT and SMT, respectively, is expected to be an effective translation."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau et al.2015] D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proc. 3rd ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merri\u00ebnboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Proc. EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Character-based neural machine translation", "author": ["Costa-juss\u00e0", "Fonollosa2016] M.R. Costa-juss\u00e0", "J.A.R. Fonollosa"], "venue": "In Proc. 54th ACL,", "citeRegEx": "Costa.juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.juss\u00e0 et al\\.", "year": 2016}, {"title": "Collecting bilingual technical terms from Japanese-Chinese patent families by SVM", "author": ["Dong et al.2015] L. Dong", "Z. Long", "T. Utsuro", "T. Mitsuhashi", "M. Yamamoto"], "venue": "In Proc. PACLING,", "citeRegEx": "Dong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Automatic evaluation of translation quality for distant language pairs", "author": ["Isozaki et al.2010] H. Isozaki", "T. Hirao", "K. Duh", "K. Sudoh", "H. Tsukada"], "venue": "In Proc. EMNLP,", "citeRegEx": "Isozaki et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Isozaki et al\\.", "year": 2010}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean et al.2014] S. Jean", "K. Cho", "Y. Bengio", "R. Memisevic"], "venue": "In Proc. 28th NIPS,", "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Recurrent continous translation models", "author": ["Kalchbrenner", "Blunsom2013] N. Kalchbrenner", "P. Blunsom"], "venue": "In Proc. EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "In Proc. 45th ACL, Companion Volume,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Towards zero unknown word in neural machine translation", "author": ["X. Li", "J. Zhang", "C. Zong"], "venue": "In Proc. 25th IJCAI,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Luong", "Manning2016] M. Luong", "C.D. Manning"], "venue": "In Proc. 54th ACL,", "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015a] M. Luong", "H. Pham", "C.D. Manning"], "venue": "In Proc. EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong et al.2015b] M. Luong", "I. Sutskever", "O. Vinyals", "Q.V. Le", "W. Zaremba"], "venue": "In Proc. 53rd ACL,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Overview of the 2nd workshop on asian translation", "author": ["Nakazawa et al.2015] T. Nakazawa", "H. Mino", "I. Goto", "G. Neubig", "S. Kurohashi", "E. Sumita"], "venue": "In Proc. 2nd WAT,", "citeRegEx": "Nakazawa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nakazawa et al\\.", "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Papineni et al.2002] K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "In Proc. 40th ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Neural machine translation of rare words with subword units", "author": ["Sennrich et al.2016] R. Sennrich", "B. Haddow", "A. Birch"], "venue": "In Proc. 54th ACL,", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural machine translation", "author": ["O. Vinyals", "Q.V. Le"], "venue": "In Proc. 28th NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A conditional random field word segmenter for Sighan bakeoff", "author": ["Tseng et al.2005] H. Tseng", "P. Chang", "G. Andrew", "D. Jurafsky", "C. Manning"], "venue": "In Proc. 4th SIGHAN Workshop on Chinese Language Processing,", "citeRegEx": "Tseng et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tseng et al\\.", "year": 2005}, {"title": "A Japanese-English patent parallel corpus", "author": ["Utiyama", "Isahara2007] M. Utiyama", "H. Isahara"], "venue": "In Proc. MT Summit XI,", "citeRegEx": "Utiyama et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Utiyama et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 16, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b).", "startOffset": 112, "endOffset": 270}, {"referenceID": 1, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b).", "startOffset": 112, "endOffset": 270}, {"referenceID": 0, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b).", "startOffset": 112, "endOffset": 270}, {"referenceID": 6, "context": "Neural machine translation (NMT), a new approach to solving machine translation, has achieved promising results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b).", "startOffset": 112, "endOffset": 270}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system.", "startOffset": 8, "endOffset": 1076}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy.", "startOffset": 8, "endOffset": 1205}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed to replace out-of-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data.", "startOffset": 8, "endOffset": 1456}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed to replace out-of-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and unknown words as sequences of subword units.", "startOffset": 8, "endOffset": 1614}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015; Jean et al., 2014; Luong et al., 2015a; Luong et al., 2015b). An NMT system builds a simple large neural network that reads the entire input source sentence and generates an output translation. The entire neural network is jointly trained to maximize the conditional probability of a correct translation of a source sentence with a bilingual corpus. Although NMT offers many advantages over traditional phrase-based approaches, such as a small memory footprint and simple decoder implementation, conventional NMT is limited when it comes to larger vocabularies. This is because the training complexity and decoding complexity proportionally increase with the number of target words. Words that are out of vocabulary are represented by a single unknown token in translations, as illustrated in Figure 1. The problem becomes more serious when translating patent documents, which contain several newly introduced technical terms. There have been a number of related studies that address the vocabulary limitation of NMT systems. Jean el al. (2014) provided an efficient approximation to the softmax to accommodate a very large vocabulary in an NMT system. Luong et al. (2015b) proposed annotating the occurrences of a target unknown word token with positional information to track its alignments, after which they replace the tokens with their translations using simple word dictionary lookup or identity copy. Li et al. (2016) proposed to replace out-of-vocabulary words with similar in-vocabulary words based on a similarity model learnt from monolingual data. Sennrich et al. (2016) introduced an effective approach based on encoding rare and unknown words as sequences of subword units. Luong and Manning (2016) provided a character-level ar X iv :1 70 4.", "startOffset": 8, "endOffset": 1744}, {"referenceID": 16, "context": "We use an NMT model similar to that used by Sutskever et al. (2014), which uses a deep long short-term memories (LSTM) (Hochreiter and Schmidhuber, 1997) to encode the input sentence and a separate deep LSTM to output the translation.", "startOffset": 44, "endOffset": 68}, {"referenceID": 16, "context": "We use an NMT model similar to that used by Sutskever et al. (2014), which uses a deep long short-term memories (LSTM) (Hochreiter and Schmidhuber, 1997) to encode the input sentence and a separate deep LSTM to output the translation. We train the NMT model on a bilingual corpus in which the technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Similar to Sutskever et al. (2014), we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using statistical machine translation (SMT).", "startOffset": 44, "endOffset": 466}, {"referenceID": 17, "context": "The Japanese sentences were segmented into a sequence of morphemes using the Japanese morphological analyzer MeCab2 with the morpheme lexicon IPAdic,3 and the Chinese sentences were segmented into a sequence of words using the Chinese morphological analyzer Stanford Word Segment (Tseng et al., 2005) trained using the Chinese Penn Treebank.", "startOffset": 280, "endOffset": 300}, {"referenceID": 16, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 89, "endOffset": 207}, {"referenceID": 1, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 89, "endOffset": 207}, {"referenceID": 0, "context": "NMT uses a single neural network trained jointly to maximize the translation performance (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 89, "endOffset": 207}, {"referenceID": 12, "context": "In this paper, we use an NMT model similar to that used by Sutskever et al. (2014). It uses two separate deep LSTMs to encode the input sequence and output the translation.", "startOffset": 59, "endOffset": 83}, {"referenceID": 12, "context": "In this paper, we use an NMT model similar to that used by Sutskever et al. (2014). It uses two separate deep LSTMs to encode the input sequence and output the translation. The encoder, which is implemented as a recurrent neural network, reads the source sentence one word at a time and then encodes it into a large vector that represents the entire source sentence. The decoder, another recurrent neural network, generates a translation on the basis of the encoded vector one word at a time. One important difference between our NMT model and the one used by Sutskever et al. (2014) is that we added an attention mechanism.", "startOffset": 59, "endOffset": 584}, {"referenceID": 0, "context": "Recently, Bahdanau et al. (2015) proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences.", "startOffset": 10, "endOffset": 33}, {"referenceID": 0, "context": "Recently, Bahdanau et al. (2015) proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences. Luong et al. (2015a) proposed an attention mechanism for different scoring functions in order to compare the source and target hidden states as well as different strategies for placing the attention.", "startOffset": 10, "endOffset": 163}, {"referenceID": 0, "context": "Recently, Bahdanau et al. (2015) proposed an attention mechanism, a form of random access memory, to help NMT cope with long input sequences. Luong et al. (2015a) proposed an attention mechanism for different scoring functions in order to compare the source and target hidden states as well as different strategies for placing the attention. In this paper, we utilize the attention mechanism proposed by Bahdanau et al. (2015), wherein each output target word is predicted on the basis of not only a recurrent hidden state and the previously predicted word but also a context vector computed as the weighted sum of the hidden states.", "startOffset": 10, "endOffset": 427}, {"referenceID": 3, "context": "According to the approach proposed by Dong et al. (2015), we identify Japanese-Chinese technical term pairs using an SMT phrase translation table.", "startOffset": 38, "endOffset": 57}, {"referenceID": 16, "context": "7 We treat the NMT system as a black box, and the strategy we present in this paper could be applied to any NMT system (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 119, "endOffset": 237}, {"referenceID": 1, "context": "7 We treat the NMT system as a black box, and the strategy we present in this paper could be applied to any NMT system (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 119, "endOffset": 237}, {"referenceID": 0, "context": "7 We treat the NMT system as a black box, and the strategy we present in this paper could be applied to any NMT system (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 119, "endOffset": 237}, {"referenceID": 16, "context": "As shown in the step 1 of Figure 4, similar to the approach of NMT rescoring provided in Sutskever et al.(2014), we first obtain 1,000-best translation list of the given Japanese sentence using the SMT system.", "startOffset": 89, "endOffset": 112}, {"referenceID": 8, "context": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for a phrase-based SMT models.", "startOffset": 112, "endOffset": 132}, {"referenceID": 8, "context": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for a phrase-based SMT models. For the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Sutskever et al. (2014). We used a deep LSTM neural network comprising three layers, with 512 cells in each layer, and a 512-dimensional word embedding.", "startOffset": 113, "endOffset": 309}, {"referenceID": 8, "context": "For the training of the SMT model, including the word alignment and the phrase translation table, we used Moses (Koehn et al., 2007), a toolkit for a phrase-based SMT models. For the training of the NMT model, our training procedure and hyperparameter choices were similar to those of Sutskever et al. (2014). We used a deep LSTM neural network comprising three layers, with 512 cells in each layer, and a 512-dimensional word embedding. Similar to Sutskever et al. (2014), we reversed the words in the source sentences and ensure that all sentences in a minibatch are roughly the same length.", "startOffset": 113, "endOffset": 473}, {"referenceID": 16, "context": "\u2022 Similar to Sutskever et al. (2014), we rescaled the normalized gradient to ensure that its norm does not exceed 5.", "startOffset": 13, "endOffset": 37}, {"referenceID": 8, "context": "Baseline SMT (Koehn et al., 2007) 52.", "startOffset": 13, "endOffset": 33}, {"referenceID": 8, "context": "Baseline SMT (Koehn et al., 2007) 3.", "startOffset": 13, "endOffset": 33}, {"referenceID": 14, "context": "We calculated automatic evaluation scores for the translation results using two popular metrics: BLEU (Papineni et al., 2002) and RIBES (Isozaki et al.", "startOffset": 102, "endOffset": 125}, {"referenceID": 5, "context": ", 2002) and RIBES (Isozaki et al., 2010).", "startOffset": 18, "endOffset": 40}, {"referenceID": 8, "context": "As shown in Table 1, we report the evaluation scores, on the basis of the translations by Moses (Koehn et al., 2007), as the baseline SMT11 and the scores based on translations produced by the equivalent NMT system without our proposed approach as the baseline NMT.", "startOffset": 96, "endOffset": 116}, {"referenceID": 13, "context": "In this study, we also conducted two types of human evaluation according to the work of Nakazawa et al. (2015): pairwise evaluation and JPO adequacy evaluation.", "startOffset": 88, "endOffset": 111}, {"referenceID": 13, "context": "(Nakazawa et al., 2015), we randomly selected 200 sentence pairs from the test set for human evaluation, and both human evaluations were conducted using only one judgement.", "startOffset": 0, "endOffset": 23}, {"referenceID": 16, "context": "Similar to Sutskever et al. (2014), we used it as a decoder to translate the source sentences with technical term tokens and replace the tokens with technical terms translated using SMT.", "startOffset": 11, "endOffset": 35}, {"referenceID": 0, "context": "One of our important future works is to evaluate our proposed method in the NMT system proposed by Bahdanau et al. (2015), which introduced a bidirectional recurrent neural network as encoder and is the state-of-the-art of pure NMT system recently.", "startOffset": 99, "endOffset": 122}, {"referenceID": 0, "context": "Our proposed NMT system is expected to improve the translation performance of patent sentences by applying approach of Bahdanau et al. (2015). Another important future work is to quantitatively compare our study with the work of Luong et al.", "startOffset": 119, "endOffset": 142}, {"referenceID": 0, "context": "Our proposed NMT system is expected to improve the translation performance of patent sentences by applying approach of Bahdanau et al. (2015). Another important future work is to quantitatively compare our study with the work of Luong et al. (2015b). In the work of Luong et al.", "startOffset": 119, "endOffset": 250}, {"referenceID": 0, "context": "Our proposed NMT system is expected to improve the translation performance of patent sentences by applying approach of Bahdanau et al. (2015). Another important future work is to quantitatively compare our study with the work of Luong et al. (2015b). In the work of Luong et al. (2015b), they replace low frequency single words and translate them in a post-processing Step using a dictionary, while we propose to replace the whole technical terms and post-translate them with phrase translation table of SMT system.", "startOffset": 119, "endOffset": 287}], "year": 2017, "abstractText": "Neural machine translation (NMT), a new approach to machine translation, has achieved promising results comparable to those of traditional approaches such as statistical machine translation (SMT). Despite its recent success, NMT cannot handle a larger vocabulary because training complexity and decoding complexity proportionally increase with the number of target words. This problem becomes even more serious when translating patent documents, which contain many technical terms that are observed infrequently. In NMTs, words that are out of vocabulary are represented by a single unknown token. In this paper, we propose a method that enables NMT to translate patent sentences comprising a large vocabulary of technical terms. We train an NMT system on bilingual data wherein technical terms are replaced with technical term tokens; this allows it to translate most of the source sentences except technical terms. Further, we use it as a decoder to translate source sentences with technical term tokens and replace the tokens with technical term translations using SMT. We also use it to rerank the 1,000-best SMT translations on the basis of the average of the SMT score and that of the NMT rescoring of the translated sentences with technical term tokens. Our experiments on Japanese-Chinese patent sentences show that the proposed NMT system achieves a substantial improvement of up to 3.1 BLEU points and 2.3 RIBES points over traditional SMT systems and an improvement of approximately 0.6 BLEU points and 0.8 RIBES points over an equivalent NMT system without our proposed technique.", "creator": "LaTeX with hyperref package"}}}