{"id": "1704.04587", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "Deep Learning for Photoacoustic Tomography from Sparse Data", "abstract": "The development of fast and accurate image reconstruction algorithms is a central aspect of computed tomography. In this paper we investigate this issue for the sparse data problem of photoacoustic tomography (PAT). We develop direct and highly efficient reconstruction algorithms based on deep-learning. In this approach image reconstruction is performed with a deep convolutional neural network (CNN), whose weights are adjusted prior to the actual image reconstruction based on a set of training data. Our results demonstrate that the proposed deep learning approach reconstructs images with a quality komparable to state of the art iterative approaches from sparse data. At the same time, the numerically complexity of our approach is much smaller and the image reconstruction is performed in a fraction of the time required by iterative methods.", "histories": [["v1", "Sat, 15 Apr 2017 05:33:32 GMT  (508kb,D)", "http://arxiv.org/abs/1704.04587v1", "13 pages, 8 figures"], ["v2", "Fri, 18 Aug 2017 06:22:48 GMT  (557kb,D)", "http://arxiv.org/abs/1704.04587v2", "20 pages, 7 figures"]], "COMMENTS": "13 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["stephan antholzer", "markus haltmeier", "johannes schwab"], "accepted": false, "id": "1704.04587"}, "pdf": {"name": "1704.04587.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Photoacoustic Tomography from Sparse Data", "authors": ["Stephan Antholzer", "Markus Haltmeier"], "emails": ["markus.haltmeier@uibk.ac.at"], "sections": [{"heading": null, "text": "tion algorithms is a central aspect of computed tomography. In this paper we investigate this issue for the sparse data problem of photoacoustic tomography (PAT). We develop direct and highly efficient reconstruction algorithms based on deep-learning. In this approach image reconstruction is performed with a deep convolutional neural network (CNN), whose weights are adjusted prior to the actual image reconstruction based on a set of training data. Our results demonstrate that the proposed deep learning approach reconstructs images with a quality comparable to state of the art iterative approaches from sparse data. At the same time, the numerically complexity of our approach is much smaller and the image reconstruction is performed in a fraction of the time required by iterative methods.\nIndex Terms\u2014Deep learning, convolutional neural networks, photoacoustic tomography, sparse data, image reconstruction, inverse problems.\nI. INTRODUCTION\nDeep learning is a rapidly emerging research area that has significantly improved performance of many pattern recognition and machine learning applications [1], [2]. Deep learning algorithms make use of special artificial neural network designs for representing a nonlinear input to output map together with optimization procedures for adjusting the weights of the network during the training phase. Deep learning techniques\nS. Antholzer, M. Haltmeier and J. Schwab are with the Department of Mathematics, University of Innsbruck, Technikestra\u00dfe 13, A-6020 Innsbruck, Austria. Corresponding E-mail: markus.haltmeier@uibk.ac.at\nare currently the state of the art for visual object recognition, natural language understanding or applications in other domains such as drug discovery or biomedical image analysis [3]\u2013[10].\nDespite its success in various scientific disciplines, in image reconstruction deep learning research appeared only very recently (see [11]\u2013[17]). In this paper, we a develop a deep learning framework for image reconstruction in photoacoustic tomography (PAT). To concentrate on the main ideas we restrict ourselves to the sparse data problem in PAT in a circular measurement geometry. With minor modifications our approach can be extended to an arbitrary measurement geometry in arbitrary dimension."}, {"heading": "A. PAT and the sparse sampling problem", "text": "PAT is a non-invasive coupled-physics biomedical imaging technology which beneficial combines the high contrast of pure optical imaging with the high spatial resolution of ultrasound imaging [18]\u2013[21]. It is based on the generation of acoustic waves by illuminating a semi-transparent biological or medical object with short optical pulses. The induced time dependent acoustic waves are measured outside of the sample with acoustic detectors, and the measured data are used to recover an image of the interior (see Fig. I.1). As in many other imaging modalities, high spatial resolution in PAT can be achieved by measuring the acoustic data with high spatial and temporal sampling rate [22], [23]. While temporal samples can be easily collected at or above the Nyquist rate, the spatial ar X\niv :1\n70 4.\n04 58\n7v 1\n[ cs\n.C V\n] 1\n5 A\npr 2\n2 sampling density is usually limited [24]\u2013[29]. In fact, each spatial measurement requires a separate sensor and high quality detectors are often costly. Moving the detector elements around can increase the number of sensor locations but it is time consuming and introduces motion artifacts. Therefore, in actual\napplications, the number of sensors locations is usually small compared to the desired resolution which yields to a so called sparse data problem.\nApplying standard algorithms to sparse data yields low-quality images containing severe undersampling artifacts. To some extent, these artifacts can be reduced by using iterative image reconstruction algorithms [30]\u2013[36] which allow to include prior knowledge such as smoothness, sparsity or total variation (TV) constraints [37]\u2013[42]. These algorithms tend to be time consuming as the forward and adjoint problem have to be applied repeatedly. Further, iterative algorithms have their own limitations. For example, the reconstruction quality strongly depends on the used a-priori knowledge about the objects to be recovered (for example, sparsity of the gradient in TV minimization). Such assumptions are often not strictly satisfied in real world scenarios which again limits the theoretically achievable reconstruction quality. To overcome these limitation in this paper we develop a new reconstruction approach based on deep learning that comes with the following properties: (i) it has low numerical complexity; (ii) no explicit knowledge of prior information on the object class is required; and (iii) it yields a reconstruction quality comparable to state of the art methods for sparse data.\nFig. I.2. ILLUSTRATION OF THE PROPOSED DEEP LEARNING APPROACH. In the first step apply a standard linear reconstruction method to the sparse data. In a second step a deep convolutional neural network is applied to the intermediate reconstruction which outputs an almost artifact free image."}, {"heading": "B. Proposed deep learning approach", "text": "Our reconstruction approach for the sparse data problem in PAT uses deep convolutional neural networks in combination with any simple linear reconstruction method as preprocessing step. Essentially, it consists of the following two steps (see Fig. I.2):\n(1) In the first step, a standard linear PAT image reconstruc-\ntion algorithm is applied, which yields an approximation of the original object including under-sampled artifacts. (2) In the second step, a deep convolutional neural network\n(CNN) is applied. The parameters of the network are trained to map the intermediate reconstruction from the first step to an artifact free final image.\nThe first step can be implemented by any standard linear reconstruction algorithm including filtered backprojection (FBP) [43]\u2013[49], Fourier methods [50]\u2013[54], or time reversal [55]\u2013 [58]. In fact, all these methods can be implemented efficiently using at most O(D3) floating point operations (FLOPS) for reconstructing a high resolution image on an D \u00d7 D grid. The CNN applied in the second step depends on weights that are adjusted using a set of training data to achieve artifact removal. The weights in the CNN are learned during the so called training phase which is performed prior to the actual image reconstruction [2]. In our current implementation we use the U-net architecture originally designed in [59] for image segmentation. Application of the trained CNN for image reconstruction is fast. In fact, one application of the CNN requires O(FLD2) FLOPS, where F is the number of\n3 channels in the first convolution and L describes the depth of the network. Typically, FL will be in the order of D and the number of FLOPS for evaluating the CNN is comparable to the effort of performing an FBP reconstruction. On the other hand, iterative reconstruction are much slower as they repeatedly apply implementations of the forward operator and its adjoint.\nTo the best of our knowledge this is the first paper using deep learning or neural networks for PAT. Related approaches applying CNNs for different medical imaging technologies including computed tomography (CT) and magnetic resonance imaging (MRI) appeared recently in [11]\u2013[17]. The author of [15] shares his opinions on deep learning for image reconstruction. In [14] deep learning is applied for imaging problems where the normal operator is shift invariant; PAT does not belong to this class. A different learning approach for addressing the limited view problem in PAT is proposed in [60]."}, {"heading": "C. Outline", "text": "The rest of this paper is organized as follows. In Sec. II we review PAT and discuss the sparse sampling problem. In Sec. III we describe the proposed deep learning approach. For that purpose we discuss neural networks and present CNNs and the U-net actually implemented in our approach. Details on the numerical implementation and numerical results are presented in Sec. IV. The paper concludes with a short summary and outlook given in Sec. V."}, {"heading": "II. PHOTOACOUSTIC TOMOGRAPHY", "text": "As illustrated in Fig. I.1, PAT is based by generating an acoustic wave using a short optical pulse. Let us denote by h : Rd \u2192 R the initial pressure distribution which provides diagnostic information about the patient and which is the quantity of interest in PAT. Of practical relevance are the cases d = 2, 3 (see [20], [61], [62]). For keeping the presentation simple and focusing on the main ideas we only consider the case of d = 2 spatial dimensions in the following. Amon others, the two dimensional case arises in PAT with so called\nintegrating line detectors [20], [47]. The extension to three spatial dimensions is straight forward. Further, we restrict ourselves to the case of a circular measurement geometry."}, {"heading": "A. PAT in circular measurement geometry", "text": "In two spatial dimensions, the induced pressure in PAT satisfies the following initial value problem for the 2D wave equation \u22022t p(x, t)\u2212\u2206p(x, t) = 0 for (x, t) \u2208 R2 \u00d7 (0,\u221e) p(x, 0) = h(x) for x \u2208 R2\n\u2202tp(x, 0) = 0 for x \u2208 R2 , (II.1) where we assume a constant sound speed rescaled to one. In the circular measurement geometry the initial pressure h is assumed to vanish outside the disc BR := {x \u2208 R2 | \u2016x\u2016 < R}. The goal is to recover h from measurements of p made on the boundary \u2202BR.\nIn a complete data situation, PAT in a circular measurement geometry consist in recovering the function h from data\n(Ph)(z, t) := p(z, t) for (z, t) \u2208 \u2202BR \u00d7 [0, T ] , (II.2)\nwhere p denotes the solution of (II.1) with initial data h and T is the final measurement time. Several efficient methods for recovering h from complete data Ph are well investigated (see, for example, [43]\u2013[58]). As an example we mention the FBP formula derived in [45],\nh(r) = \u2212 1 \u03c0R \u222b S1 \u222b \u221e |r\u2212z| (\u2202ttPh)(z, t)\u221a t2 \u2212 |r \u2212 z|2 dtdS(z) . (II.3) Note that (II.3) requires data for all t > 0; see [45, Theorem 1.4] for a related FBP formula that only uses data for t < 2R. For the numerical results in this paper we truncate (II.3) at t = 2R, in which situation all singularities are contained in the inversion and the truncation error is small."}, {"heading": "B. Discretization and sparse sampling", "text": "In practical applications, the acoustic pressure Ph can only be measured with a finite number of acoustic detectors. The standard sampling scheme for PAT in circular geometry assumes uniformly sampled values\np[m, \u00b7 ] := Ph (zm, \u00b7 ) , for m = 1, . . . ,M , (II.4)\n4\nwith zm := R cos (2\u03c0(m\u2212 1)/M) R sin (2\u03c0(m\u2212 1)/M)  . (II.5) Here p[i, \u00b7 ] : [0, T ] \u2192 R is the signal corresponding to the mth detector, and M is the total number of detector locations. Of course, in practice also the signals p[m, \u00b7 ] have to be represented by discrete samples. However, temporal samples can easily be collected at a high sampling rate compared to the spatial sampling, where each sample requires a separate sensor.\nIn the case a sufficiently large number of detectors is used, according to Shannon sampling theory implementations of full data methods yield almost artifact free reconstructions (for a detailed analysis of sampling in PAT see [22]). As the fabrication of an array of detectors is demanding, experiments using integrating line detectors are often carried out using a single line detector, scanned on circular paths using scanning stages [63], [64], which is very time consuming. Recently, systems using arrays of 64 parallel line detectors have been demonstrated [25], [26]. To keep production costs low and to allow fast imaging the number M will typically be kept much smaller than advised by Shannon sampling theory and one deals with highly under-sampled data.\nDue to the high frequency information contained in time, there is still hope to recover high resolution images form spatially under-sampled data. For example, iterative algorithms, using TV minimization yield good reconstruction results from\nundersampled data (see [24], [28], [42], [65]). However such algorithms are quite time consuming as they require evaluating the forward and adjoint problem repeatedly (for TV typically at least several hundreds of times). Moreover the reconstruction quality depends on strong a-priori assumptions on the class of objects to be reconstructed. The deep learning approach proposed in this paper requires a much smaller numerical effort compared to iterative methods and further does not require explicit prior knowledge about the object to be recovered."}, {"heading": "III. DEEP LEARNING FOR PAT IMAGE RECONSTRUCTION", "text": "Suppose that sparsely sampled data of the form (II.4), (II.5) are at our disposal. As illustrated in Fig. I.2 in our deep learning approach we first apply a linear reconstruction procedure to the sparsely sampled data (p[m, \u00b7 ])Mm=1 which outputs a discrete image X \u2208 RD\u00d7D . According to Shannon sampling theory an aliasing free reconstruction requires M \u2265 \u03c0D detector positions [22]. However, in practical applications we will have M D, in which case severe undersampling artifacts appear in the reconstructed image. To reduce these artifacts we apply a CNN to the intermediate reconstruction which outputs an almost artifact free reconstruction Y \u2208 RD\u00d7D . How to implement such an approach is described in the following.\nA. Image restoration by neural networks\nThe task of removing artifacts can be formulated as supervised machine learning problem. In that context, the aim is finding a restoration function \u03a6: RD\u00d7D \u2192 RD\u00d7D that maps the input image X \u2208 RD\u00d7D (containing undersampling artifacts) to the output image Y \u2208 RD\u00d7D which should be almost artifact free. For constructing such a function \u03a6, one assumes that a family of training data T := (Xn,Yn)Nn=1 are given. Any training example (Xn,Yn) consist of an input image Xn and a corresponding artifact-free output image Yn. The restoration function is constructed in such a way that the training error\nE(T ; \u03a6) := N\u2211\nn=1\nd(\u03a6(Xn),Yn) (III.1)\nis minimized, where d : RD\u00d7D \u00d7 RD\u00d7D \u2192 R measures the error made by \u03a6 on the training examples.\n5 Particular powerful supervised machine learning methods are based on neural networks (NNs). In our context the restoration function is taken in the form\n\u03a6W = (\u03c3L \u25e6WL) \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 (\u03c31 \u25e6W1) , (III.2)\nwhere any factor \u03c3` \u25e6 W` is the composition of a linear transformation (or matrix) W` \u2208 RD`+1\u00d7D` and a nonlinearity \u03c3` : R \u2192 R that is applied component-wise. Here L denotes the number of processing layers, \u03c3` are so called activation functions and W := (W1, . . . ,WL) is the weight vector. Neural networks can be graphically illustrated in the form of layers as illustrated in Fig. III.1. Between any neighboring layer in the NN the factor \u03c3` \u25e6W` maps the variables of layer ` to the variables of layer `+ 1. The variables in the first layer are the entries of the input vector X and the variables in the last layer are the entries of the output vector Y. Note that in our situation we use equal dimension D := D1 = DL+1 of the input and output layer. Approximation properties of NNs have been analyzed, for example, in [66], [67].\nThe entries of the weight vector W are called weights and are the variable parameters in the NN. They are adjusted during the training phase prior to the actual restoration process. This is commonly implemented using gradient descent methods to\nminimize the training set error [2], [68]\nE(T ,W) := E(T ,\u03a6W) = N\u2211\nn=1\nd (\u03a6W(Xn),Yn) (III.3)\nThe standard gradient method uses the update rule W(k+1) = W(k)\u2212\u03b7\u2207E(T ,W(k)), where\u2207E denotes the gradient of the error function in the second component and W(k) the weight vector in the kth iteration. In the context of neural networks the update term is also known as error backpropagation. If the number of training examples is large then the gradient method becomes notoriously slow. In such a situation a popular acceleration is the stochastic gradient descent algorithm [2], [68]. Here for each iteration a small subset T (k) of the whole training set is chosen randomly at any iteration and the weights are adjusted using the modified update formula W(k+1) = W(k) \u2212 \u03b7\u2207E(T (k),W(k)) for the kth iteration. In the context of image reconstruction similar acceleration strategies are known as ART or Kaczmarz type reconstruction methods [23], [69], [70]. The number of elements in T (k) is called batch size and \u03b7 is referred to as the lerning rate. To stabilize the iterative process it is common to add a so-called momentum term \u03b2 (W(k) \u2212W(k\u22121)) with some nonnegative parameter \u03b2 in the update of the kth iteration."}, {"heading": "B. CNNs and the U-net", "text": "In our application the inputs and outputs are high dimensional vectors. Such large scale problems require special network designs, where the weight matrices are not taken as arbitrary matrices but take a special form reducing its effective dimensionality. When the input is an image, convolutional neural networks (CNNs) use such special network designs that are widely and successfully used in various imaging applications [68], [71]. A main property of CNNs is the invariance with respect to certain transformations of the input. In CNNs the weight matrices are block diagonal, where each block corresponds to a convolution with a filter of small support and the number of blocks corresponds to the number of different filters (or channels) used in each layer. Each block is therefore a sparse band matrix, where the non-zero entries of the band matrices determine the filters of the convolution. CNNs are currently\n6 extensively used in image processing and image classification, since they outperform most comparable algorithms [2]. They are also the method of choice for the present paper.\nThere are various CNN designs that can differ in the number of layers, the form of the activation functions and the particular form of the weight matrices W`. In this paper we use a particular CNN based on the so called U-net introduced in [59]. It has been originally designed for biomedical image segmentation and recently been used for low dose CT in [13], [14]. The U-net is based on the so-called fully convolutional network of [72]. Such network architectures employ multichannel filtering which means that the weight matrix in every layer consists of a family of multiple convolution filters followed by the rectified linear unit (ReLU) as activation function. The rectified linear unit is defined by ReLU(x) := max {x, 0}. As shown in [13], the residual images X \u2212 Y often have a simpler structure and are more accessible to the U-net than the original outputs. Therefore, learning the residuals and subtracting them from the inputs after the last layer is more effective than directly training for Y. Such an approach is followed in our implementation. The resulting deep neural network architecture is illustrated in Fig. III.2; see also [13], [14], [59]."}, {"heading": "C. PAT using FBP combined with the U-net", "text": "We are now ready to present an instance of a deep learning approach for PAT from sparse data, that uses the FBP algorithm as linear preprocessing step followed by the U-net for removing undersampling artifacts. Recall that we have given sparsely sampled data (p[m, \u00b7 ])Mm=1 of the form (II.4), (II.5). A discrete high resolution approximation Y \u2208 RD\u00d7D with D M of the original object is then reconstructed as follows.\n(1) Apply the FBP algorithm to p which yields an reconstruc-\ntion X \u2208 RD\u00d7D containing undersampling artifacts.\n(2) Apply the U-net to X which yields an image Y \u2208 RD\u00d7D\nwith significantly reduced undersampling artifacts.\nNote the first step could be replaced by another linear reconstruction methods such as time reversal and the second step by\na different CNN; see Fig. I.2. Such alternative implementations will be investigated in future studies.\nA crucial ingredient in the above deep learning method step are the weights in the U-net which have to be trained on an appropriate training data set. For that purpose we construct training data T = (Xn,Yn)Nn=1 by first creating certain phantoms Yn. We then simulate sparse data by numerically implementing the well known solution formula for the wave equation and then subsequently construct Xn by applying the FBP algorithm to the sparse data. The trained U-net maps a reconstruction with undersampling artifact to a reconstruction image with removed artifact. For training we apply the stochastic gradient algorithm for minimizing the training set error (III.1), where we take the error measure d corresponding\nto the `1-norm \u2016Y\u20161 = \u2211D\ni1,i2=1 |Y[i1, i2]|."}, {"heading": "IV. NUMERICAL REALIZATION", "text": "In this section we give details on the numerical implementation of the deep learning approach and present reconstruction results under various scenarios."}, {"heading": "A. Data generation and network training", "text": "For all numerical results presented below we use D = 128 for the image size and take R = 1 for the radius of the measurement curve. For the sparse data in (II.4) we use M = 30 detector locations and discretize the pressure signals p[m, \u00b7 ] with 300 uniform samples in the time interval [0, 2]. In our initial studies we generate simple phantoms consisting of indicator functions of ellipses with support in the unit cube [\u22121, 1]2 \u2286 R2. For that purpose we randomly generate solid ellipses E by sampling independent uniformly distributed random variables. The centers are selected uniformly in (\u22120.5, 0.5) and the minor and major axes uniformly in (0.1, 0.2).\nFor the training of the network we generate two different data sets, each consisting of N = 1000 training pairs (Xn,Yn) N n=1. Because the convolutional network works on local features of the image, such a number of training data\n7 Fig. III.2. ARCHITECTURE OF THE USED CNN. The number written above each layer denotes the number of convolution kernels (channels), which is equal to number of images in each layer. The numbers B1, . . . , B5 denote the dimension of the images (the block sizes in the weight matrices), which stays constant in every row. The long yellow arrows indicate direct connections with subsequent concatenation or summation for the upmost arrow.\nturned out to be sufficient for accurate training of sums of indicator functions of ellipses. One training data corresponds to simulated data without noise and for the second data set we added random noise to the simulated pressure data. The output data Yn consist of the sum of indicator functions \u03c7E of ellipses generated randomly as described above that are sampled on the 128 \u00d7 128 imaging grid. The number of ellipses in each training example is also taken randomly according to the uniform distribution on {1, . . . , 5}. The input images are generated numerically by first computing the sparse pressure data using the solution formula for the wave equation and then applying the FBP algorithm to obtain Xn. These intermediate reconstructions contain undersampling artifacts for whose removal the network is trained for. For actual training we use the stochastic gradient descent algorithm with a batch size of one for minimizing (III.3). We train for 60 epochs which means we make 60 sweep through the whole training dataset. We take \u03b7 = 10\u22123 for the learning rate, include a momentum parameter \u03b2 = 0.99, and use the mean absolute error in (III.3). The weights in the jth layer are initialized by sampling the uniform distribution on [\u2212H`, H`] where H` := \u221a 6/ \u221a D` +D`+1 and D` is the size of the input in layer `. This initializer is due to Glorot [73]. We use F = 32 channels for the first convolution and the total number of layers is L = 19.\n8\nFig. IV.1. RESULTS FOR SIMULATED DATA. Top left: Superposition of 5 ellipses as test phantom. Top right: Corresponding simulated wave data (time against detector location). Bottom left: Result of FBP algorithm. Bottom right: Reconstruction using the proposed two step approach. The phantom and reconstructions are displayed using the same colormap."}, {"heading": "B. Numerical results", "text": "We first test the network trained above on a test set of 50 pairs (X,Y) that are generated according to the random model for the training data described above. For such superpositions of random ellipse phantoms, the trained network is in all tested case able to almost completely eliminate the sparse data artifacts in the test images. Fig. IV.1 illustrates such result for one of the test phantoms. The top row shows the actual phantom Y and the sparse data p. The bottom row in Fig. IV.1 shows the result of the FPB algorithm (left) which contains severe undersampling artifacts and the result of applying the CNN (right) which is almost artifact free. The actual relative `2-reconstruction error \u2016YCNN \u2212 Y\u20162/\u2016Y\u20162 of the CNN reconstruction is 0.0087 which is much smaller than the relative error of FBP reconstruction which is 0.1811.\nIn order to test the stability with respect to noise we also test the network on reconstructions coming from noisy data. For that purpose we added Gaussian noise with a standard\ndeviation equal to 2% of the maximal value to the simulated data. Reconstruction results are shown in Fig. IV.2. There we show two reconstruction results with a trained network. In one case we used the network trained on the exact data (bottom left) and the other case the network trained on the noisy data (bottom right). The reconstructions using each of the networks are again almost artifact free.\nWe also compared our trained network to penalized TV mini-\n9 mization [41], [42]\n1 2 \u2016p\u2212 P(Y)\u201622 + \u03bb\u2016Y\u2016TV \u2192 min Y . (IV.1)\nHere P is a discretization of the PAT forward operator using M detector locations and D spatial discretization points and \u2016Y\u2016TV is the discrete total variation. For the presented results we take \u03bb = 0.002 and used the lagged diffusivity algorithm [74] with 20 outer and 20 inner iterations for numerically minimizing (IV.1). TV minimization exploits the sparsity of the gradient as prior information and is therefore especially well suited for reconstructing sums of indicator functions and can be seen as state of the art approach for reconstructing such type of objects. As can be seen from the results in Fig. IV.3, TV minimization in fact gives very accurate results. Nevertheless, the deep learning approach yields comparable results in both cases; the relative `2 reconstruction errors for CNN reconstructions are even smaller than the errors for the corresponding TV results (see Tab. I).\nIn order to investigate limitations of the presented we also applied the deep learning to test data, where the training data is not adequate. The results are illustrated in Fig. IV.4, which shows a test phantom similar to the Shepp Logan phantom and its reconstructions using FBP, the CNN improved version and total variation minimization. As expected, the network is not able to remove all the artifacts this time. However, despite the test object has features not appearing in the training data, still artifacts are removed by the network trained on the ellipse phantoms. We see these initial results quite encouraging; future work will be done to extensively test the framework using a variety of training and test data sets.\nThe relative `2-reconstruction errors for numerical results presented above are summarized in Tab. I."}, {"heading": "C. Comparison of computational efforts", "text": "It is worth mentioning that evaluating the CNN requires a much smaller number of FLOPS than a variational or iterative minimization scheme. Application of the trained CNN for image reconstruction is then fast. In fact, one application of the CNN requires O(FLD2), where F is the number of channels for the first convolution and L describes the depth of the network. Typically, FL will be in the order of D and the number of FLOPS for evaluating the evaluation CNN is comparable to the effort of performing evaluation of the forward operator or the adjoint. On the other hand, for computing the minimizer of (IV.1) we have to repeatedly evaluate the forward operator and its adjoint. In our example we evaluated both operations 400 times, and therefore the neural network approach is faster by orders of magnitude.\nFor training and evaluation of the U-net we use the Keras software (see https://keras.io/) which is a high-level application\n10\nprogramming interface written in Python. Keras runs on top of the open-source software library for machine intelligence TensorFlow (see https://www.tensorflow.org/). These software packages allow an efficient and simple implementation of the modified U-net according to Fig. III.2. The filtered backprojection and the TV-minimization were implemented in MATLAB. We performed our computations using an Intel Core i7-6850K CPU and a Nvidia Geforce 1080 Ti GPU. The Training time for our CNN was 16 seconds per epoch, which in our case is 16 minutes over all. The evaluation time for one image is 15 milliseconds for the reconstruction by filtered backprojection and 5 milliseconds for applying the CNN. The computation time for the TV-minimization algorithm has been 25 seconds. In summary, the total reconstruction time using the two-stage deep learning approach is 20 milliseconds, which is over 1000 times faster than the time required for the TV minimization algorithm."}, {"heading": "V. CONCLUSION", "text": "In this paper we developed a deep learning approach for PAT from sparse data. In our approach we first apply a linear standard reconstruction algorithm to the sparsely sampled data and subsequently apply a CNN trained on a set of training data. Evaluation of the CNN has a similar numerical effort as the standard direct FBP algorithms. Our deep learning approach has been shown to offer as reconstruction quality similar to state of the art iterative algorithms and at the same time requires a computational effort similar to direct algorithms such as FBP. The presented numerical results can be seen as a prove of principle that the proposed deep learning approach is feasible and highly promising for image reconstruction in PAT.\nAs we have demonstrated for the sparse data problem it already offers a reconstruction quality comparable to state of the art iterative algorithms. In future studies we will investigate and test the approach under various real-world scenarios including different measurement geometries, realistic phantoms and increased discretization sizes. Especially, we will also train CNNs on real world data. It is an interesting line of future research using other CNNs that may outperform the currently\nimplemented U-net. We further work on the theoretical analysis of our proposal providing insight why it works that well, and how to steer the network design for further improving its performance."}], "references": [{"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493\u20132537, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "2014, arXiv:1409.0473.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural nets as a method for quantitative structure\u2013 activity relationships", "author": ["J. Ma", "R.P. Sheridan", "A. Liaw", "G.E. Dahl", "V. Svetnik"], "venue": "Journal of chemical information and modeling, vol. 55, no. 2, pp. 263\u2013274, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "2015, arXiv:1502.03167.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep image: Scaling up image recognition", "author": ["R. Wu", "S. Yan", "Y. Shan", "Q. Dang", "G. Sun"], "venue": "2015, arXiv:1501.02876.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on deep learning in medical image analysis", "author": ["G. Litjens", "T. Kooi", "B.E. Bejnordi", "A.A. Setio", "F. Ciompi", "M. Ghafoorian", "J. van der Laak", "B. van Ginneken", "C.I. S\u00e1nchez"], "venue": "2017.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Guest editorial deep learning in medical imaging: Overview and future promise of an exciting new technique", "author": ["H. Greenspan", "B. van Ginneken", "R.M. Summers"], "venue": "IEEE Trans. Med. Imaging, vol. 35, no. 5, pp. 1153\u20131159, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Low-dose CT via convolutional neural network", "author": ["H. Chen", "Y. Zhang", "W. Zhang", "P. Liao", "K. Li", "J. Zhou", "G. Wang"], "venue": "Biomed. Opt. Express, vol. 8, no. 2, pp. 679\u2013694, 2017.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Accelerating magnetic resonance imaging via  11 deep learning", "author": ["S. Wang", "Z. Su", "L. Ying", "X. Peng", "S. Zhu", "F. Liang", "D. Feng", "D. Liang"], "venue": "Biomedical Imaging (ISBI), 2016 IEEE 13th International Symposium on. IEEE, 2016, pp. 514\u2013 517.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for compressed sensing CT reconstruction via persistent homology analysis", "author": ["Y. Han", "J.J. Yoo", "J.C. Ye"], "venue": "2016, http://arxiv.org/abs/1611.06391.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep convolutional neural network for inverse problems in imaging", "author": ["K.H. Jin", "M.T. McCann", "E. Froustey", "M. Unser"], "venue": "arXiv:1611.03679, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A perspective on deep imaging", "author": ["G. Wang"], "venue": "IEEE Access, vol. 4, pp. 8914\u20138924, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning computed tomography", "author": ["T. W\u00fcrfl", "F.C. Ghesu", "V. Christlein", "A. Maier"], "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2016, pp. 432\u2013440.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Image prediction for limited-angle tomography via deep learning with convolutional neural network", "author": ["H. Zhang", "L. Li", "K. Qiao", "L. Wang", "B. Yan", "L. Li", "G. Hu"], "venue": "arXiv:1607.08707, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Biomedical photoacoustic imaging", "author": ["P. Beard"], "venue": "Interface focus, vol. 1, no. 4, pp. 602\u2013631, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Photoacoustic ultrasound (paus) \u2013 reconstruction tomography", "author": ["R. Kruger", "P. Lui", "Y. Fang", "R. Appledorn"], "venue": "Med. Phys., vol. 22, no. 10, pp. 1605\u20131609, 1995.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Photoacoustic tomography using a Mach-Zehnder interferometer as an acoustic line detector", "author": ["G. Paltauf", "R. Nuster", "M. Haltmeier", "P. Burgholzer"], "venue": "Appl. Opt., vol. 46, no. 16, pp. 3352\u20133358, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Multiscale photoacoustic microscopy and computed tomography", "author": ["L.V. Wang"], "venue": "Nat. Photon, vol. 3, no. 9, pp. 503\u2013509, 2009.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Sampling conditions for the circular radon transform", "author": ["M. Haltmeier"], "venue": "IEEE Trans. Image Process., vol. 25, no. 6, pp. 2910\u20132919, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "The Mathematics of Computerized Tomographytte, ser", "author": ["F. Natterer"], "venue": "Classics in Applied Mathematics. Philadelphia: SIAM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Accelerated high-resolution photoacoustic tomography via compressed sensing", "author": ["S. Arridge", "P. Beard", "M. Betcke", "B. Cox", "N. Huynh", "F. Lucka", "O. Ogunlade", "E. Zhang"], "venue": "Phys. Med. Biol., vol. 61, no. 24, p. 8908, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Pho-  toacoustic projection imaging using a 64-channel fiber optic detector array", "author": ["J. Bauer-Marschallinger", "K. Felbermayer", "K.-D. Bouchal", "I.A. Veres", "H. Gr\u00fcn", "P. Burgholzer", "T. Berer"], "venue": "Proc. SPIE, vol. 9323, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "64-line-sensor array: fast imaging system for photoacoustic tomography", "author": ["S. Gratt", "R. Nuster", "G. Wurzinger", "M. Bugl", "G. Paltauf"], "venue": "Proc. SPIE, vol. 8943, p. 894365, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Acoustic inversion in optoacoustic tomography: A review", "author": ["A. Rosenthal", "V. Ntziachristos", "D. Razansky"], "venue": "Current medical imaging reviews, vol. 9, no. 4, pp. 318\u2013336, 2013.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Compressed sensing in photoacoustic tomography in vivo", "author": ["Z. Guo", "C. Li", "L. Song", "L.V. Wang"], "venue": "J. Biomed. Opt., vol. 15, no. 2, pp. 021 311\u2013021 311, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel compressed sensing scheme for photoacoustic tomography", "author": ["M. Sandbichler", "F. Krahmer", "T. Berer", "P. Burgholzer", "M. Haltmeier"], "venue": "SIAM J. Appl. Math., vol. 75, no. 6, pp. 2475\u20132494, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "On the adjoint operator in photoacoustic tomography", "author": ["S.R. Arridge", "M.M. Betcke", "B.T. Cox", "F. Lucka", "B.E. Treeby"], "venue": "Inverse Probl., vol. 32, no. 11, p. 115012 (19pp), 2016.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "A direct method for photoacoustic tomography with inhomogeneous sound speed", "author": ["Z. Belhachmi", "T. Glatz", "O. Scherzer"], "venue": "Inverse Probl., vol. 32, no. 4, p. 045005, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Analysis of iterative methods in photoacoustic tomography with variable sound speed", "author": ["M. Haltmeier", "L.V. Nguyen"], "venue": "SIAM J. Imaging Sci., 2017, to appear, arXiv:1611.07563.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Full-wave iterative image reconstruction in photoacoustic tomography with acoustically inhomogeneous media", "author": ["C. Huang", "K. Wang", "L. Nie", "L.V.M.A. Wang", "Anastasio"], "venue": "IEEE Trans. Med. Imag., vol. 32, no. 6, pp. 1097\u20131110, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "A multi-grid iterative method for photoacoustic tomography", "author": ["A. Javaherian", "S. Holman"], "venue": "IEEE Trans. Med. Imag., vol. 36, no. 3, pp. 696\u2013706, 2017.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2017}, {"title": "A galerkin least squares approach for photoacoustic tomography", "author": ["J. Schwab", "S. Pereverzyev Jr", "M. Haltmeier"], "venue": "2016, arXiv:1612.08094.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Investigation of iterative image reconstruction in threedimensional optoacoustic tomography", "author": ["K. Wang", "R. Su", "A.A. Oraevsky", "M.A. Anastasio"], "venue": "Phys. Med. Biol., vol. 57, no. 17, p. 5399, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Comm. Pure Appl. Math., vol. 57, no. 11, pp. 1413\u20131457, 2004.  12", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient regularization with wavelet sparsity constraints in PAT", "author": ["J. Frikel", "M. Haltmeier"], "venue": "arXiv:1703.08240, 2017.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "Sparsity in inverse geophysical problems", "author": ["M. Grasmair", "M. Haltmeier", "O. Scherzer"], "venue": "Handbook of Geomathematics, W. Freeden, M. Nashed, and T. Sonar, Eds. Springer Berlin Heidelberg, 2010, pp. 763\u2013784.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "The application of compressed sensing for photo-acoustic tomography", "author": ["J. Provost", "F. Lesage"], "venue": "IEEE Trans. Med. Imag., vol. 28, no. 4, pp. 585\u2013594, 2009.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Analysis of bounded variation penalty methods for ill-posed problems", "author": ["R. Acar", "C.R. Vogel"], "venue": "Inverse Probl., vol. 10, no. 6, pp. 1217\u20131229, 1994.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1994}, {"title": "Variational methods in imaging, ser. Applied Mathematical Sciences", "author": ["O. Scherzer", "M. Grasmair", "H. Grossauer", "M. Haltmeier", "F. Lenzen"], "venue": "New York: Springer,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Explicit inversion formulae for the spherical mean Radon transform", "author": ["L.A. Kunyansky"], "venue": "Inverse Probl., vol. 23, no. 1, pp. 373\u2013383, 2007.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "Determining a function from its mean values over a family of spheres", "author": ["D. Finch", "S.K. Patch", "Rakesh"], "venue": "SIAM J. Math. Anal., vol. 35, no. 5, pp. 1213\u20131240, 2004.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2004}, {"title": "Inversion of spherical means and the wave equation in even dimensions", "author": ["D. Finch", "M. Haltmeier", "Rakesh"], "venue": "SIAM J. Appl. Math., vol. 68, no. 2, pp. 392\u2013412, 2007.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2007}, {"title": "Inversion of circular means and the wave equation on convex planar domains", "author": ["M. Haltmeier"], "venue": "Comput. Math. Appl., vol. 65, no. 7, pp. 1025\u20131036, 2013.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}, {"title": "Temporal back-projection algorithms for photoacoustic tomography with integrating line detectors", "author": ["P. Burgholzer", "J. Bauer-Marschallinger", "H. Gr\u00fcn", "M. Haltmeier", "G. Paltauf"], "venue": "Inverse Probl., vol. 23, no. 6, pp. S65\u2013S80, 2007.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal back-projection algorithm for photoacoustic computed tomography", "author": ["M. Xu", "L.V. Wang"], "venue": "Phys. Rev. E, vol. 71, no. 1, p. 016706, 2005.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2005}, {"title": "Universal inversion formulas for recovering a function from spherical means", "author": ["M. Haltmeier"], "venue": "SIAM J. Math. Anal., vol. 46, no. 1, pp. 214\u2013232, 2014.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Uniqueness of reconstruction and an inversion procedure for thermoacoustic and photoacoustic tomography with variable sound speed", "author": ["M. Agranovsky", "P. Kuchment"], "venue": "Inverse Probl., vol. 23, no. 5, pp. 2089\u20132102, 2007.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "A series solution and a fast algorithm for the inversion of the spherical mean Radon transform", "author": ["L.A. Kunyansky"], "venue": "Inverse Probl., vol. 23, no. 6, pp. S11\u2013S20, 2007.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2007}, {"title": "Fourier reconstruction in optoacoustic imaging using truncated regularized inverse k-space interpolation", "author": ["M. Jaeger", "S. Sch\u00fcpbach", "A. Gertsch", "M. Kitz", "M. Frenz"], "venue": "Inverse Probl., vol. 23, pp. S51\u2013S63, 2007.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "A reconstruction algorithm for photoacoustic imaging based on the nonuniform FFT", "author": ["M. Haltmeier", "O. Scherzer", "G. Zangerl"], "venue": "IEEE Trans. Med. Imag., vol. 28, no. 11, pp. 1727\u2013 1735, November 2009.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2009}, {"title": "Exact frequency-domain reconstruction for thermoacoustic tomography\u2013II: Cylindrical geometry", "author": ["Y. Xu", "M. Xu", "L.V. Wang"], "venue": "IEEE Trans. Med. Imag., vol. 21, pp. 829\u2013833, 2002.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2002}, {"title": "Exact and approximate imaging methods for photoacoustic tomography using an arbitrary detection surface", "author": ["P. Burgholzer", "G.J. Matt", "M. Haltmeier", "G. Paltauf"], "venue": "Phys. Rev. E, vol. 75, no. 4, p. 046706, 2007.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}, {"title": "k-wave: Matlab toolbox for the simulation and reconstruction of photoacoustic wave-fields", "author": ["B.E. Treeby", "B.T. Cox"], "venue": "J. Biomed. Opt., vol. 15, p. 021314, 2010.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Reconstruction and time reversal in thermoacoustic tomography in acoustically homogeneous and inhomogeneous media", "author": ["Y. Hristova", "P. Kuchment", "L. Nguyen"], "venue": "Inverse Probl., vol. 24, no. 5, p. 055006 (25pp), 2008.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2008}, {"title": "Thermoacoustic tomography with variable sound speed", "author": ["P. Stefanov", "G. Uhlmann"], "venue": "Inverse Probl., vol. 25, no. 7, pp. 075 011, 16, 2009.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2009}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["O. Ronneberge", "P. Fischer", "T. Brox"], "venue": "CoRR, 2015. [Online]. Available: http://arxiv.org/abs/1505.04597", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Operator learning approach for the limited view problem in photoacoustic tomography", "author": ["F. Dreier", "M. Haltmeier", "S. Pereverzyev", "Jr."], "venue": "2017, in preparation.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2017}, {"title": "Mathematics of photoacoustic and thermoacoustic tomography", "author": ["P. Kuchment", "L. Kunyansky"], "venue": "Handbook of Mathematical Methods in Imaging. Springer, 2011, pp. 817\u2013 865.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Photoacoustic imaging in biomedicine", "author": ["M. Xu", "L.V. Wang"], "venue": "Rev. Sci. Instruments, vol. 77, no. 4, p. 041101 (22pp), 2006.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2006}, {"title": "Photoacoustic microtomography using optical interferometric detection", "author": ["R. Nuster", "M. Holotta", "C. Kremser", "H. Grossauer", "P. Burgholzer", "G. Paltauf"], "venue": "J. Biomed. Optics, vol. 15, no. 2, pp. 021 307\u2013021 307\u20136, 2010.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2010}, {"title": "Paltauf,  13 \u201cThree-dimensional photoacoustic imaging using fiber-based line detectors,", "author": ["H. Gr\u00fcn", "T. Berer", "P. Burgholzer", "R. Nuster"], "venue": "J. Biomed. Optics,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2010}, {"title": "In vivo optical-resolution photoacoustic computed tomography with compressed sensing", "author": ["J. Meng", "L.V. Wang", "D. Liang", "L. Song"], "venue": "Optics letters, vol. 37, no. 22, pp. 4573\u2013 4575, 2012.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2012}, {"title": "On the approximate realization of continuous mappings by neural networks", "author": ["K. Funahashi"], "venue": "Neural Netw., vol. 2, no. 3, pp. 183\u2013192, 1989.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1989}, {"title": "Approximation capabilities of multilayer feedforward networks", "author": ["K. Hornik"], "venue": "Neural Netw., vol. 4, no. 2, pp. 251\u2013257, Mar. 1991.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1991}, {"title": "On steepest-descent-Kaczmarz methods for regularizing systems of nonlinear ill-posed equations", "author": ["A. De Cezaro", "M. Haltmeier", "A. Leit\u00e3o", "O. Scherzer"], "venue": "Appl. Math. Comp., vol. 202, no. 2, pp. 596\u2013607, 2008.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2008}, {"title": "Algebraic reconstruction techniques (art) for three-dimensional electron microscopy and x-ray photography", "author": ["R. Gordon", "R. Bender", "G.T. Herman"], "venue": "J. Theor. Biol., vol. 29, no. 3, pp. 471\u2013481, 1970.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 1970}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Comput., vol. 1, no. 4, pp. 541\u2013551, 1989.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 1989}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431\u20133440.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks.", "author": ["X. Glorot", "Y. Bengio"], "venue": "in Aistats, vol", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2010}, {"title": "Iterative methods for total variation denoising", "author": ["C. Vogel", "M. Oman"], "venue": "SIAM J. Sci. Comput., vol. 17, pp. 227\u2013238, 1996.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning is a rapidly emerging research area that has significantly improved performance of many pattern recognition and machine learning applications [1], [2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 1, "context": "at are currently the state of the art for visual object recognition, natural language understanding or applications in other domains such as drug discovery or biomedical image analysis [3]\u2013[10].", "startOffset": 185, "endOffset": 188}, {"referenceID": 8, "context": "at are currently the state of the art for visual object recognition, natural language understanding or applications in other domains such as drug discovery or biomedical image analysis [3]\u2013[10].", "startOffset": 189, "endOffset": 193}, {"referenceID": 9, "context": "Despite its success in various scientific disciplines, in image reconstruction deep learning research appeared only very recently (see [11]\u2013[17]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "Despite its success in various scientific disciplines, in image reconstruction deep learning research appeared only very recently (see [11]\u2013[17]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "PAT is a non-invasive coupled-physics biomedical imaging technology which beneficial combines the high contrast of pure optical imaging with the high spatial resolution of ultrasound imaging [18]\u2013[21].", "startOffset": 191, "endOffset": 195}, {"referenceID": 19, "context": "PAT is a non-invasive coupled-physics biomedical imaging technology which beneficial combines the high contrast of pure optical imaging with the high spatial resolution of ultrasound imaging [18]\u2013[21].", "startOffset": 196, "endOffset": 200}, {"referenceID": 20, "context": "As in many other imaging modalities, high spatial resolution in PAT can be achieved by measuring the acoustic data with high spatial and temporal sampling rate [22], [23].", "startOffset": 160, "endOffset": 164}, {"referenceID": 21, "context": "As in many other imaging modalities, high spatial resolution in PAT can be achieved by measuring the acoustic data with high spatial and temporal sampling rate [22], [23].", "startOffset": 166, "endOffset": 170}, {"referenceID": 22, "context": "sampling density is usually limited [24]\u2013[29].", "startOffset": 36, "endOffset": 40}, {"referenceID": 27, "context": "sampling density is usually limited [24]\u2013[29].", "startOffset": 41, "endOffset": 45}, {"referenceID": 28, "context": "To some extent, these artifacts can be reduced by using iterative image reconstruction algorithms [30]\u2013[36] which allow to include prior knowledge such as smoothness, sparsity or total variation (TV) constraints [37]\u2013[42].", "startOffset": 98, "endOffset": 102}, {"referenceID": 34, "context": "To some extent, these artifacts can be reduced by using iterative image reconstruction algorithms [30]\u2013[36] which allow to include prior knowledge such as smoothness, sparsity or total variation (TV) constraints [37]\u2013[42].", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "To some extent, these artifacts can be reduced by using iterative image reconstruction algorithms [30]\u2013[36] which allow to include prior knowledge such as smoothness, sparsity or total variation (TV) constraints [37]\u2013[42].", "startOffset": 212, "endOffset": 216}, {"referenceID": 40, "context": "To some extent, these artifacts can be reduced by using iterative image reconstruction algorithms [30]\u2013[36] which allow to include prior knowledge such as smoothness, sparsity or total variation (TV) constraints [37]\u2013[42].", "startOffset": 217, "endOffset": 221}, {"referenceID": 41, "context": "The first step can be implemented by any standard linear reconstruction algorithm including filtered backprojection (FBP) [43]\u2013[49], Fourier methods [50]\u2013[54], or time reversal [55]\u2013 [58].", "startOffset": 122, "endOffset": 126}, {"referenceID": 47, "context": "The first step can be implemented by any standard linear reconstruction algorithm including filtered backprojection (FBP) [43]\u2013[49], Fourier methods [50]\u2013[54], or time reversal [55]\u2013 [58].", "startOffset": 127, "endOffset": 131}, {"referenceID": 48, "context": "The first step can be implemented by any standard linear reconstruction algorithm including filtered backprojection (FBP) [43]\u2013[49], Fourier methods [50]\u2013[54], or time reversal [55]\u2013 [58].", "startOffset": 149, "endOffset": 153}, {"referenceID": 52, "context": "The first step can be implemented by any standard linear reconstruction algorithm including filtered backprojection (FBP) [43]\u2013[49], Fourier methods [50]\u2013[54], or time reversal [55]\u2013 [58].", "startOffset": 154, "endOffset": 158}, {"referenceID": 53, "context": "The first step can be implemented by any standard linear reconstruction algorithm including filtered backprojection (FBP) [43]\u2013[49], Fourier methods [50]\u2013[54], or time reversal [55]\u2013 [58].", "startOffset": 177, "endOffset": 181}, {"referenceID": 56, "context": "The first step can be implemented by any standard linear reconstruction algorithm including filtered backprojection (FBP) [43]\u2013[49], Fourier methods [50]\u2013[54], or time reversal [55]\u2013 [58].", "startOffset": 183, "endOffset": 187}, {"referenceID": 57, "context": "In our current implementation we use the U-net architecture originally designed in [59] for image segmentation.", "startOffset": 83, "endOffset": 87}, {"referenceID": 9, "context": "Related approaches applying CNNs for different medical imaging technologies including computed tomography (CT) and magnetic resonance imaging (MRI) appeared recently in [11]\u2013[17].", "startOffset": 169, "endOffset": 173}, {"referenceID": 15, "context": "Related approaches applying CNNs for different medical imaging technologies including computed tomography (CT) and magnetic resonance imaging (MRI) appeared recently in [11]\u2013[17].", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": "The author of [15] shares his opinions on deep learning for image reconstruction.", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "In [14] deep learning is applied for imaging problems where the normal operator is shift invariant; PAT does not belong to this class.", "startOffset": 3, "endOffset": 7}, {"referenceID": 58, "context": "A different learning approach for addressing the limited view problem in PAT is proposed in [60].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "Of practical relevance are the cases d = 2, 3 (see [20], [61], [62]).", "startOffset": 51, "endOffset": 55}, {"referenceID": 59, "context": "Of practical relevance are the cases d = 2, 3 (see [20], [61], [62]).", "startOffset": 57, "endOffset": 61}, {"referenceID": 60, "context": "Of practical relevance are the cases d = 2, 3 (see [20], [61], [62]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "Amon others, the two dimensional case arises in PAT with so called integrating line detectors [20], [47].", "startOffset": 94, "endOffset": 98}, {"referenceID": 45, "context": "Amon others, the two dimensional case arises in PAT with so called integrating line detectors [20], [47].", "startOffset": 100, "endOffset": 104}, {"referenceID": 41, "context": "Several efficient methods for recovering h from complete data Ph are well investigated (see, for example, [43]\u2013[58]).", "startOffset": 106, "endOffset": 110}, {"referenceID": 56, "context": "Several efficient methods for recovering h from complete data Ph are well investigated (see, for example, [43]\u2013[58]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 43, "context": "As an example we mention the FBP formula derived in [45],", "startOffset": 52, "endOffset": 56}, {"referenceID": 20, "context": "In the case a sufficiently large number of detectors is used, according to Shannon sampling theory implementations of full data methods yield almost artifact free reconstructions (for a detailed analysis of sampling in PAT see [22]).", "startOffset": 227, "endOffset": 231}, {"referenceID": 61, "context": "As the fabrication of an array of detectors is demanding, experiments using integrating line detectors are often carried out using a single line detector, scanned on circular paths using scanning stages [63], [64], which is very time consuming.", "startOffset": 203, "endOffset": 207}, {"referenceID": 62, "context": "As the fabrication of an array of detectors is demanding, experiments using integrating line detectors are often carried out using a single line detector, scanned on circular paths using scanning stages [63], [64], which is very time consuming.", "startOffset": 209, "endOffset": 213}, {"referenceID": 23, "context": "Recently, systems using arrays of 64 parallel line detectors have been demonstrated [25], [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 24, "context": "Recently, systems using arrays of 64 parallel line detectors have been demonstrated [25], [26].", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "For example, iterative algorithms, using TV minimization yield good reconstruction results from undersampled data (see [24], [28], [42], [65]).", "startOffset": 119, "endOffset": 123}, {"referenceID": 26, "context": "For example, iterative algorithms, using TV minimization yield good reconstruction results from undersampled data (see [24], [28], [42], [65]).", "startOffset": 125, "endOffset": 129}, {"referenceID": 40, "context": "For example, iterative algorithms, using TV minimization yield good reconstruction results from undersampled data (see [24], [28], [42], [65]).", "startOffset": 131, "endOffset": 135}, {"referenceID": 63, "context": "For example, iterative algorithms, using TV minimization yield good reconstruction results from undersampled data (see [24], [28], [42], [65]).", "startOffset": 137, "endOffset": 141}, {"referenceID": 20, "context": "According to Shannon sampling theory an aliasing free reconstruction requires M \u2265 \u03c0D detector positions [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 64, "context": "Approximation properties of NNs have been analyzed, for example, in [66], [67].", "startOffset": 68, "endOffset": 72}, {"referenceID": 65, "context": "Approximation properties of NNs have been analyzed, for example, in [66], [67].", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "In the context of image reconstruction similar acceleration strategies are known as ART or Kaczmarz type reconstruction methods [23], [69], [70].", "startOffset": 128, "endOffset": 132}, {"referenceID": 66, "context": "In the context of image reconstruction similar acceleration strategies are known as ART or Kaczmarz type reconstruction methods [23], [69], [70].", "startOffset": 134, "endOffset": 138}, {"referenceID": 67, "context": "In the context of image reconstruction similar acceleration strategies are known as ART or Kaczmarz type reconstruction methods [23], [69], [70].", "startOffset": 140, "endOffset": 144}, {"referenceID": 68, "context": "When the input is an image, convolutional neural networks (CNNs) use such special network designs that are widely and successfully used in various imaging applications [68], [71].", "startOffset": 174, "endOffset": 178}, {"referenceID": 57, "context": "In this paper we use a particular CNN based on the so called U-net introduced in [59].", "startOffset": 81, "endOffset": 85}, {"referenceID": 11, "context": "It has been originally designed for biomedical image segmentation and recently been used for low dose CT in [13], [14].", "startOffset": 108, "endOffset": 112}, {"referenceID": 12, "context": "It has been originally designed for biomedical image segmentation and recently been used for low dose CT in [13], [14].", "startOffset": 114, "endOffset": 118}, {"referenceID": 69, "context": "The U-net is based on the so-called fully convolutional network of [72].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "As shown in [13], the residual images X \u2212 Y often have a simpler structure and are more accessible to the U-net than the original outputs.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "2; see also [13], [14], [59].", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "2; see also [13], [14], [59].", "startOffset": 18, "endOffset": 22}, {"referenceID": 57, "context": "2; see also [13], [14], [59].", "startOffset": 24, "endOffset": 28}, {"referenceID": 70, "context": "This initializer is due to Glorot [73].", "startOffset": 34, "endOffset": 38}, {"referenceID": 39, "context": "mization [41], [42]", "startOffset": 9, "endOffset": 13}, {"referenceID": 40, "context": "mization [41], [42]", "startOffset": 15, "endOffset": 19}, {"referenceID": 71, "context": "002 and used the lagged diffusivity algorithm [74] with 20 outer and 20 inner iterations for numerically minimizing (IV.", "startOffset": 46, "endOffset": 50}], "year": 2017, "abstractText": "The development of fast and accurate image reconstruction algorithms is a central aspect of computed tomography. In this paper we investigate this issue for the sparse data problem of photoacoustic tomography (PAT). We develop direct and highly efficient reconstruction algorithms based on deep-learning. In this approach image reconstruction is performed with a deep convolutional neural network (CNN), whose weights are adjusted prior to the actual image reconstruction based on a set of training data. Our results demonstrate that the proposed deep learning approach reconstructs images with a quality comparable to state of the art iterative approaches from sparse data. At the same time, the numerically complexity of our approach is much smaller and the image reconstruction is performed in a fraction of the time required by iterative methods.", "creator": "LaTeX with hyperref package"}}}