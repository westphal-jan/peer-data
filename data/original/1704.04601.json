{"id": "1704.04601", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2017", "title": "MUSE: Modularizing Unsupervised Sense Embeddings", "abstract": "This paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts. Prior work about learning multi-sense embeddings suffered from either ambiguity of different-level embeddings or inefficient sense selection. The proposed modular framework, MUSE, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with linear-time sense selection. We leverage reinforcement learning to enable joint training on the proposed modules, and introduce various exploration techniques on sense selection for better robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on synonym selection as well as on contextual word similarities in terms of MaxSimC.", "histories": [["v1", "Sat, 15 Apr 2017 07:36:49 GMT  (466kb,D)", "http://arxiv.org/abs/1704.04601v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guang-he lee", "yun-nung chen"], "accepted": true, "id": "1704.04601"}, "pdf": {"name": "1704.04601.pdf", "metadata": {"source": "CRF", "title": "MUSE: Modularizing Unsupervised Sense Embeddings", "authors": ["Guang-He Lee", "Yun-Nung Chen"], "emails": ["yvchen}@csie.ntu.edu.tw"], "sections": [{"heading": "1 Introduction", "text": "Recently, deep learning methodologies have dominated several research areas in natural language processing (NLP), such as machine translation, language understanding, and dialogue systems. However, most of applications usually utilize word-level embeddings to obtain semantics. Considering that natural language is highly ambiguous, the standard word embeddings may suffer from polysemy issues. Neelakantan et al. (2014) pointed out that, due to triangle inequality in vector space, if one word has two different senses but is restricted to one embedding, the sum of the distances between the word and its synonym\nin each sense would upper-bound the distance between the respective synonyms, which may be mutually irrelevant, in embedding space1. Due to the theoretical inability to account for polysemy using a single embedding representation per word, multi-sense word representations are proposed to address the ambiguity issue using multiple embedding representations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012).\nThis paper focuses on unsupervised learning from an unannotated corpus. There are two key mechanisms for a multi-sense word representation system in such scenario: 1) a sense selection (decoding) mechanism infers the most probable sense for a word given its context and 2) a sense representation mechanism learns to embed word senses in a continuous space.\nUnder this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016). However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (Neelakantan et al., 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al., 2016).\nUnlike the prior work, this paper proposes MUSE2\u2014a novel modularization framework incorporating sense selection and representation learning models, which implements flexible modules to optimize distinct mechanisms. Specifically, MUSE enables linear time sense identity decoding\n1d(rock, stone) + d(rock, shake) \u2265 d(stone, shake) 2The trained models and code will be released after the\npaper gets accepted.\nar X\niv :1\n70 4.\n04 60\n1v 1\n[ cs\n.C L\n] 1\n5 A\npr 2\n01 7\nwith a sense selection module and purely senselevel representation learning with a sense representation module. With the modular design, we propose a novel joint learning algorithm on the modules by connecting to a reinforcement learning scenario. We further employ a word sense exploration mechanism to improve robustness in the learning algorithm. Our contributions are fourfold: \u2022 MUSE is the first system that maintains\npurely sense-level representation learning with linear-time sense decoding. \u2022 We are among the first to propose a joint\nlearning algorithm for modularized unsupervised sense embedding learning. \u2022 We introduce a sense exploration mechanism\nfor the sense selection module to achieve better flexibility and robustness. \u2022 Our experimental results show the state-of-\nthe-art performance for synonym selection and contextual word similarities in terms of MaxSimC."}, {"heading": "2 Related Work", "text": "There are three dominant types of approaches for learning multi-sense word representations in the literature: 1) clustering methods, 2) probabilistic modeling methods, and 3) lexical ontology based methods. Our reinforcement learning based approach can be loosely connected to clustering methods and probabilistic modeling methods.\nReisinger and Mooney (2010) first proposed multi-sense word representations on the vector space based on clustering techniques. With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014). Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). Ka\u030ageba\u0308ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning.\nInstead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training. Li and Jurafsky (2015) exploited Chinese Restaurant Process to infer the sense identity. Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b). Despite reasonable modeling on sense selection, all above methods mixed wordlevel and sense-level tokens during representation learning\u2014unable to conduct representation learning in the pure sense level due to the complicated computation in their EM algorithms.\nRecently, Qiu et al. (2016) proposed an EM algorithm to learn purely sense-level representations, where the computational cost is high when decoding the sense identity sequence, because it takes exponential time to search all sense combination within a context window. Our modular design addresses such drawback, where the sense selection module decodes a sense sequence with linear-time complexity, while the sense representation module remains representation learning in the pure sense level.\nUnlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Schu\u0308tze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; S\u030custer et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus. Also, some prior work proposed to learn topical embeddings and word embeddings jointly in order to consider the contexts (Liu et al., 2015a,b), whereas this paper focuses on learning multi-sense word embeddings."}, {"heading": "3 Proposed Approach: MUSE", "text": "This work proposes a framework to modularize two key mechanisms for multi-sense word representations: a sense selection module and a sense representation module. The sense selection module decides which sense to use given a text context, whereas the sense representation module learns meaningful representations based on its statistical characteristics. Unlike prior work that must compromise between efficient sense selection and\npurely sense-level representation learning, the proposed modularized framework is capable of performing efficient sense selection and learning sense-level representations simultaneously.\nTo learn sense-level representations, a sense selection model should be first established for sense identity decoding. On the other hand, the sense embeddings should guide the sense selection model when decoding a sense identity sequence. Therefore, these two modules should be tangled. This indicates that a naive two-stage algorithm or two separate learning algorithms proposed by prior work are not optimal.\nBy connecting the proposed formulation with reinforcement learning literature, we design a novel joint training algorithm. Besides, taking advantage of the form of reinforcement learning, we are among the first to investigate various exploration techniques in sense selection for unsupervised sense embedding learning."}, {"heading": "3.1 Model Architecture", "text": "Our model architecture is illustrated in Figure 1, where there are two modules in optimization."}, {"heading": "3.1.1 Sense Selection Module", "text": "Formally speaking, given a corpus C, vocabulary V , and the t-th word wi = Ct \u2208 V , we would like to find the most probable sense zik \u2208 Zi, where Zi is the set of senses in word wi. Assuming that a word sense is determined by the local context, we exploit a local context C\u0304t = {Ct\u2212m, \u00b7 \u00b7 \u00b7 , Ct+m} for sense selection according to the Markov assumption, where m is the size of a context window. Then we can either formulate a probabilistic\npolicy \u03c0(zik | C\u0304t) about sense selection or estimate the individual fitness q(zik | C\u0304t) for each sense identity.\nTo ensure efficiency, here we exploit a linear neural architecture that takes word-level input tokens and outputs sense-level identities. The architecture is similar to continuous bag-of-words (CBOW) (Mikolov et al., 2013a). Specifically, given a word embedding matrix P , the local context can be modeled as the summation of word embeddings from its context C\u0304t. The output can be formulated with a 3-mode tensorQ, whose dimensions denote words, senses, and latent variables. Then we can model \u03c0(zik | C\u0304t) or q(zik | C\u0304t) correspondingly. Here we model \u03c0(\u00b7) as a categorical distribution using a softmax layer:\n\u03c0(zik | C\u0304t) = exp(QTik \u2211 j\u2208C\u0304t Pj)\u2211\nk\u2032\u2208Zi exp(Q T ik\u2032 \u2211 j\u2208C\u0304t Pj) .\n(1) On the other hand, the likelihood of selecting distinct sense identities, q(zik | C\u0304t), is modeled as a Bernoulli distribution with a sigmoid function \u03c3(\u00b7):\nq(zik | C\u0304t) = \u03c3(QTik \u2211 j\u2208C\u0304t Pj). (2)\nDifferent modeling approaches require different learning methods, especially for the unsupervised setting. We leave the corresponding learning algorithms in \u00a7 3.2. Finally, with a built sense selection module, we can apply any selection algorithm such as a greedy selection strategy to infer the sense identity zik given a word wi with its context Ct.\nWe note that modularized model allows sense selection to enjoy efficiency by leveraging wordlevel tokens, while remaining purely sense-level tokens in the representation module. Specifically, if n denotes maxk |Zk|, decoding L words takes O(nL) senses to be searched due to independent sense selection. The prior work using a single model with purely sense-level tokens (Qiu et al., 2016) requires exponential time to calculate the collocation energy for every possible combination of sense identities within a context window, O(n2m), for a single target sense. Further, Qiu et al. (2016) took an additional sequence decoding step with quadratic time complexity O(n4mL), based on an exponential number n2m in the base unit. It demonstrates the achievement about sense inference efficiency in our proposed model."}, {"heading": "3.1.2 Sense Representation Module", "text": "The semantic representation learning is typically formulated as a maximum likelihood estimation (MLE) problem for collocation likelihood. In this paper, we use the skip-gram formulation (Mikolov et al., 2013b) considering that it requires less training time, where only two sense identities are required for stochastic training. Other popular candidates, like GloVe (Pennington et al., 2014) and CBOW (Mikolov et al., 2013a), require more sense identities to be selected as input and thus not suitable for our scenario. For example, GloVe (Pennington et al., 2014) takes computationally expensive collocation counting statistics for each token in a corpus as input, which requires sense selection for every occurrence of the target word across the whole corpus for a single optimization step.\nTo learn the representations, we first create input sense representation matrix U and collocation estimation matrix V as the learning targets. Given a target word wi and collocated word wj with corresponding local contexts, we map them to their sense identities as zik and zjl by the sense selection module, and maximize the sense collocation log likelihood logL(\u00b7). A natural choice of the likelihood function is formulated as a categorical distribution over all possible collocated senses given the target sense zik:\nlogL(zjl | zik) = log exp(UTzikVzjl)\u2211 zuv exp(UTzikVzuv) . (3)\nInstead of enumerating all possible collocated senses which is computationally expensive, we\nuse the skip-gram objective (4) (Mikolov et al., 2013b) to approximate (3) as shown in the green block of Figure 1.\nlog L\u0304(zjl |zik) = log \u03c3(UTzikVzjl) (4)\n+ M\u2211 v=1 Ezuv\u223cpneg(z)[log \u03c3(\u2212U T zik Vzuv)],\nwhere pneg(z) is the distribution over all senses for negative samples. In our experiment with |Zi| senses for word wi, we use (1/|Zi|) word-level unigram as sense-level unigram for efficiency and the 3/4-th power trick in Mikolov et al. (2013b).\nWe note that our modular framework can easily maintain purely sense-level tokens with an arbitrary representation learning model. In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process."}, {"heading": "3.2 Learning", "text": "Without supervised signal for the proposed modules, it is desirable to connect two modules in a way where they can improve each other by their own estimations. On one hand, forwarding the prediction of the sense selection module to the representation module is trivial. On the other hand, we cast the estimated collocation likelihood as a reward signal for the selected sense for effective learning. Based on different modeling methods ((1) or (2)) in the sense selection module, we connect the model to respective learning algorithms.\nThe learning algorithm can be viewed as a reinforcement learning algorithm solving a onestep Markov Decision Process (MDP) (Sutton and Barto, 1998), where the state, action, and reward correspond to context C\u0304t, sense zik, and collocation log likelihood log L\u0304(\u00b7) respectively. Note that we focus on stochastic optimization for a practical learning setting. In the following sections, we refer (1) to policy distribution and refer (2) to Qvalue estimation in the reinforcement learning literature."}, {"heading": "3.2.1 Policy Gradient Method", "text": "Because (1) fits a valid probability distribution, an intuitive way is to optimize the expectation of re-\nsulting collocation likelihood among each sense. In addition, as the skip-gram formulation in (4) is unidirectional (L\u0304(zik | zjl) 6= L\u0304(zjl | zik)), we perform one-side optimization for the target sense zik to stabilize model training3. That is, for the target word wi and the collocated word wj given respective contexts C\u0304t and C\u0304t\u2032 (0 < |t\u2212 t\u2032| \u2264 m), we first draw a sense zjl for wj from the policy \u03c0(\u00b7 | C\u0304t\u2032) and optimize the expected collocation likelihood for the target sense zik as follows,\nmaxEzik\u223c\u03c0(\u00b7|C\u0304t)[log L\u0304(zjl | zik)]. (5)\nThe objective is differentiable and supports stochastic optimization (Lei et al., 2016), which uses a stochastic sample zik for optimization.\nHowever, there are two possible disadvantages in this formulation. First, because the policy assumes the probability distribution in (1), optimizing the selected sense must affect the estimation of the other senses. Second, if applying stochastic gradient ascent to optimizing (5), it would always lower the probability estimation for the selected sense zik even if the model accurately selects the right sense. The detailed proof is in Appendix A."}, {"heading": "3.2.2 Value-Based Method", "text": "To address the above issues, we apply the Qlearning algorithm (Mnih et al., 2013). Instead of maintaining a probabilistic policy for sense selection, Q-learning estimates the Q-value (resulting collocation log likelihood) for each sense candidate directly and independently. Thus, the estimation of unselected senses may not be influenced by the selected one. Note that in one-step MDP, the reward is equivalent to the Q-value, so we will use reward and Q-value interchangeably, hereinafter, based on the context.\nWe further follow the convention of recent neural reinforcement learning by reducing the reward range to aid model training (Mnih et al., 2013). Specifically, we replace the log likelihood log L\u0304(\u00b7) \u2208 (\u2212 inf, 0] with the likelihood L\u0304(\u00b7) \u2208 [0, 1] as the reward function. Due to the monotonous operation in log(), the relative ordering of the reward remains the same.\nIn addition, we exploit the probabilistic nature of likelihood for Q-learning. To elaborate, as Q-learning is used to approximate the Q-value for each action in typical reinforcement learning,\n3We observe about 4% performance drop by optimizing input selection zik and output selection zjl simultaneously.\nmost literature adopted square loss to characterize the discrepancy between the target and estimated Q-values (Mnih et al., 2013). In our setting where the Q-value/reward is a likelihood function, our model exploits cross-entropy loss to better capture the characteristics of probability distribution.\nFurthermore, given that the collocation likelihood in (4) is an approximation to the original categorical distribution with a softmax function shown in (3) (Mikolov et al., 2013b), we revise the formulation by omitting the negative sampling term. The resulting formulation L\u0302(\u00b7) is a Bernoulli distribution indicating whether zjl collocates or not given zik:\nL\u0302(zjl | zik) = \u03c3(UTzikVzjl). (6)\nThere are three advantages about using L\u0302(\u00b7) instead of approximated L\u0304(\u00b7) and original L(\u00b7). First, regarding the variance of estimation, L\u0302(\u00b7) better captures L(\u00b7) than L\u0304(\u00b7) because L\u0304(\u00b7) involves sampling:\nV ar(L\u0304(\u00b7)) \u2265 V ar(L\u0302(\u00b7)) = V ar(L(\u00b7)) = 0.\nSecond, regarding the relative ordering of estimation, for any two collocated senses zjl and zjl\u2032 with a target sense zik, the following equivalence holds:\nL(zjl | zik) < L(zjl\u2032 | zik) \u21d4 L\u0304(zjl | zik) < L\u0304(zjl\u2032 | zik) \u21d4 L\u0302(zjl | zik) < L\u0302(zjl\u2032 | zik)\nThird, for collocation computation, L(\u00b7) requires all sense identities and L\u0304(\u00b7) requires (M+1) sense identities, whereas L\u0302(\u00b7) only requires 1 sense identity. In sum, the proposed L\u0302(\u00b7) approximates L(\u00b7) with no variance, no \u201cbias\u201d (in terms of relative ordering), and significantly less computation.\nFinally, because both target distribution L\u0302(\u00b7) and estimated distribution q(\u00b7) are Bernoulli distributions, we follow the last section to conduct one-side optimization by fixing a collocated sense zjl and optimize the selected sense zik with cross entropy as\nmin H(L\u0302(zik, zjl), q(zik, C\u0304t)) (7) = min \u2212L\u0302(zik, zjl) log q(zik, C\u0304t) \u2212 (1\u2212 L\u0302(zik, zjl)) log(1\u2212 q(zik, C\u0304t))."}, {"heading": "3.2.3 Joint Training", "text": "To jointly train sense selection and sense representation modules, we first select a pair of the collocated senses, zik and zjl, based on the sense selection module with any selecting strategy (e.g.\nAlgorithm 1: Learning Algorithm for wi = Ct \u2208 C do\nsample wj = Ct\u2032(0 < |t\u2032 \u2212 t| \u2264 m); zik = select(Ct, wi); zjl = select(Ct\u2032 , wj); optimize U, V by (4) for the sense\nrepresentation module; optimize P,Q by (5) or (7) for the sense selection module;\ngreedy), and then optimize the sense representation module and the sense selection module using the above derivations. Algorithm 1 describes the proposed MUSE model training procedure.\nThe major distinction between our modular framework and two-stage clusteringrepresentation learning framework (Neelakantan et al., 2014; Vu and Parker, 2016) is that we establish a reward signal from the sense representation to the sense selection module to enable immediate and joint optimization."}, {"heading": "3.3 Sense Selection Strategy", "text": "Given a fitness estimation for each sense, exploiting the greedy sense is the most popular strategy for clustering algorithms (Neelakantan et al., 2014; Ka\u030ageba\u0308ck et al., 2015) and hard-EM algorithms (Qiu et al., 2016; Jauhar et al., 2015) in literature. However, there are two incentives to conduct exploration. First, in the early training stage when the fitness is not well estimated, it is desirable to explore underestimated senses. Second, due to high ambiguity in natural language, sometimes multiple senses in a word would fit in the same context. The dilemma between exploring sub-optimal choices and exploiting the optimal choice is called exploration-exploitation trade-off in reinforcement learning (Sutton and Barto, 1998).\nWe introduce exploration mechanisms for sense selection for both policy gradient and Q-learning. For policy gradient, we sample the policy distribution to approximate the expectation in (5). Because of the flexible formulation of Q-learning, the following classic exploration mechanisms are applied to sense selection: \u2022 Greedy: selects the sense with the largest Q-\nvalue (no exploration). \u2022 -Greedy: selects a random sense with\nprobability, and adopts the greedy strategy\notherwise (Mnih et al., 2013). \u2022 Boltzmann: samples the sense based on the\nBoltzmann distribution modeled by Q-value. We directly use (1) as the Boltzmann distribution for simplicity.\nWe note that Q-learning with Boltzmann sampling yields the same sampling process as policy gradient but different optimization objective. To our best knowledge, we are among the first to explore several exploration strategies for unsupervised sense embedding learning.\nIn the following sections, MUSE-Policy denotes the proposed MUSE model with policy learning and MUSE-Greedy denotes the model using corresponding sense selection strategy for Qlearning."}, {"heading": "4 Experiments", "text": "We evaluate our proposed MUSE models in both quantitative and qualitative experiments."}, {"heading": "4.1 Experimental Setup", "text": "Our model is trained on the April 2010 Wikipedia dump (Shaoul and Westbury, 2010), which contains approximately 1 billion tokens. For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al. (2014). For preprocessing, we convert all words to their lower cases, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al., 2014), and remove all sentences with less than 10 tokens. The number of senses per word in Q is set to 3 for fair comparison with prior work (Neelakantan et al., 2014). We refer implementation details to Appendix B."}, {"heading": "4.2 Experiment 1: Contextual Word Similarity", "text": "To evaluate the quality of the learned sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford\u2019s Contextual Word Similarities (SCWS) data (Huang et al., 2012). Specifically, given a list of word pairs with corresponding contexts, S = {(wi, C\u0304t, wj , C\u0304t\u2032)}, we calculate the Spearman\u2019s rank correlation \u03c1 between human-judged similarity and model similarity estimations. Two major contextual similarity estimations are introduced by Reisinger and Mooney (2010): AvgSimC and MaxSimC. AvgSimC is a\nsoft measurement that addresses the contextual information with a probability estimation:\nAvgSimC(wi, C\u0304t, wj , C\u0304t\u2032) = |Zi|\u2211 k=1 |Zj |\u2211 l=1 \u03c0(zik|C\u0304t)\u03c0(zjl|C\u0304t\u2032)d(zik, zjl),\nwhere d(zik, zjl) refers to the cosine similarity between Uzik and Uzjl . AvgSimC weights the similarity measurement of each sense pair zik and zjl by their probability estimations. On the other hand, MaxSimC is a hard measurement that only considers on the most probable senses:\nMaxSimC(wi, C\u0304t, wj , C\u0304t\u2032) = d(zik, zjl),\nzik = arg max zik\u2032 \u03c0(zik\u2032 |C\u0304t), zjl = arg max zjl\u2032 \u03c0(zjl\u2032 |C\u0304t\u2032).\nThe baselines for comparison include classic clustering methods (Huang et al., 2012; Neelakantan et al., 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al. (2016) used more recent Wikipedia dumps. The embedding sizes of all baselines are 300, except 50 in Huang et al. (2012). For every competitor with multiple settings, we report the best performance in each similarity measurement setting and show in Table 1.\nOur MUSE model achieves the state-of-the-art performance on MaxSimC, demonstrating superior quality on independent sense embeddings. On the other hand, MUSE achieves comparable performance with the best competitor in terms of\n4We run Li and Jurafsky (2015)\u2019s released code on our corpus for fair comparison.\nAvgSimC (68.7 vs. 69.3), while MUSE outperforms the same competitor significantly in terms of MaxSimC (67.9 vs. 60.1). The results demonstrate not only the high quality of sense representations but also accurate sense selection.\nFrom the application perspective, MaxSimC refers to a typical scenario using single embedding per word, while AvgSimC employs multiple sense vectors simultaneously per word, which not only brings computational overhead but changes existing neural architecture for NLP. Hence, we argue that MaxSimC better characterize practical usage of a sense representation system than AvgSimC.\nAmong various learning methods for MUSE, policy gradient performs worst, echoing our argument in \u00a7 3.2.1. On the other hand, the superior performance of Boltzmann sampling and - Greedy over Greedy selection demonstrates the effectiveness of exploration.\nFinally, replacing L\u0304(\u00b7) with L\u0302(\u00b7) as the reward signal yields 2.3 times speedup for MUSE-Greedy and 1.3 times speedup for MUSEBoltzmann to reach 67.0 in MaxSimC, which demonstrates the efficacy of proposed approximation L\u0302(\u00b7) over typical L\u0304(\u00b7) in terms of convergence."}, {"heading": "4.3 Experiment 2: Synonym Selection", "text": "We further evaluate our model on synonym selection using multi-sense word representations (Jauhar et al., 2015). Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed. In the datasets, each question consists of a question word wQ and four answer candidates {wA, wB, wC , wD}, and the goal is to select the most semantically synonymous choice among the four candidates. For example, in the TOEFL-80 dataset, a question shows {(Q) enormously, (A) appropriately, (B) uniquely, (C) tremendously, (D) decidedly}, and the answer is (C). For multi-sense representations system, it selects the synonym of the question word wQ using the maximum senselevel cosine similarity as a proxy of the semantic similarity (Jauhar et al., 2015).\nOur model is compared with the following baselines: 1) conventional word embeddings: global context vectors (Huang et al., 2012) and skipgram (Mikolov et al., 2013b); 2) applying supervised word sense disambiguation using the\nIMS system and then applying skip-gram on disambiguated corpus (IMS+SG) (Zhong and Ng, 2010); 3) unsupervised sense embeddings: EM algorithm (Jauhar et al., 2015), multi-sense skipgram (MSSG) (Neelakantan et al., 2014), Chinese Restaurant Process (CPR) (Li and Jurafsky, 2015), and the MUSE models; 4) supervised sense embeddings with WordNet: retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG) (Jauhar et al., 2015).\nAmong unsupervised sense embedding approaches, CRP and MSSG refer to the baselines with highest MaxSimC and AvgSimC in Table 1 respectively. Here we report the setting for baselines based on the best average performance or in this task. We also show the performance of supervised sense embeddings as an upperbound of unsupervised methods due to the usage of additional supervised information from WordNet.\nThe results are shown in Table 3, where our MUSE- -Greedy and MUSE-Boltzmann signifi-\ncantly outperforms all unsupervised sense embeddings methods, echoing the superior quality of our sense vectors in last section. MUSE-Boltzmann also outperforms the supervised sense embeddings except 1 setting without any supervised signal during training. Finally, the MUSE methods with proper exploration outperform all unsupervised baselines consistently, demonstrating the importance of exploration."}, {"heading": "4.4 Qualitative Analysis", "text": "We further conduct qualitative analysis to check the semantic meanings of different senses learned by MUSE with k-nearest neighbors (k-NN) using sense representations. In addition, we provide contexts in the training corpus where the sense will be selected to validate the sense selection module. Table 2 shows the results. The learned sense embeddings of the words \u201ctie\u201d, \u201cblackberry\u201d, and \u201chead\u201d clearly correspond to correct senses under different contexts."}, {"heading": "5 Conclusion", "text": "This paper propose a novel modularized framework for unsupervised sense representation learning, which supports not only flexible design of modular tasks but also joint optimization among modules. The proposed model is the first work to achieve purely sense-level representation learning with linear-time sense selection, and achieves the state-of-the-art performance on synonym selection and on benchmark contextual word similarity task in terms of MaxSimC. In the future, we plan to investigate reinforcement learning methods to incorporate multi-sense word representations for downstream NLP tasks."}, {"heading": "A Doubly Stochastic Gradient", "text": "To derive doubly stochastic gradient for equation (5), we first denote (5) as J(\u0398) with \u0398 = {P,Q} and resolve the expectation form as:\nJ(\u03b8) = Ezik\u223c\u03c0(\u00b7|C\u0304t)[log L\u0304(zjl | zik)] = \u2211 k \u03c0(zik | C\u0304t) log L\u0304(zjl|zik).\nDenote \u0398 = {P,Q} as the parameter set for policy \u03c0. The gradient with respect to \u0398 should be:\n\u2202J(\u03b8)\n\u2202\u0398\n= \u2202\n\u2202\u0398 \u2211 k \u03c0(zik | C\u0304t) log L\u0304(zjl|zik)\n= \u2211 k log L\u0304(zjl|zik) \u2202\u03c0(zik | C\u0304t) \u2202\u0398\n= \u2211 k log L\u0304(zjl|zik)( \u2202 log \u03c0(zik | C\u0304t) \u2202\u0398 )(\u03c0(zik | C\u0304t)) = Ezik\u223c\u03c0(\u00b7|C\u0304t)[log L\u0304(zjl | zik) \u2202 log \u03c0(zik | C\u0304t)\n\u2202\u0398 ]\nAccordingly, if we conduct typical stochastic gradient ascent training on J(\u0398) with respect to \u0398 from samples zik with a learning rate \u03b7, the update formula will be:\n\u0398 = \u0398 + \u03b7 log L\u0304(zjl | zik) \u2202 log \u03c0(zik | C\u0304t)\n\u2202\u0398 .\nHowever, the collocation log likelihood should always be non-positive: log L\u0304(zjl | zik) \u2264 0. Therefore, as long as the collocation log likelihood log L\u0304(zjl | zik) is negative, the update formula is to minimize the likelihood of choosing zik, despite the fact that zik may be good choices. On the other hand, if the log likelihood reaches 0, according to (4), it indicates:\nlog L\u0304(zjl | zik) = 0 \u21d2 L\u0304(zjl | zik) = 1 \u21d2 UTzikVzjl \u2192\u221e, U T zik Vzuv \u2192\u221e, \u2200zuv,\nwhich leads to computational overflow from an infinity value.\nB Implementation Details\nIn the experiments, the context window size is set to 5 (|C\u0304t| = 11). Subsampling technique introduced by word2vec (Mikolov et al., 2013b) is\napplied to accelerate the training process. The learning rate is set to 0.025. The embedding dimension is 300. We initialize Q and V as zeros, and P and U from uniform distribution [\u2212 \u221a 1/100, \u221a 1/100] such that each embedding has unit length in expectation (Lei et al., 2015). Our model uses 25 negative senses for negative sampling in (4). We use = 5% for -Greedy sense selection strategy\nIn optimization, we conduct mini-batch training with 2048 batch size using the following procedure: 1) select senses in the batch; 2) optimize U, V using stochastic training within the batch for efficiency; 3) optimize P,Q using mini-batch training for robustness."}], "references": [{"title": "Breaking sticks and ambi", "author": ["Sergey Bartunov", "Dmitry Kondrashkin", "Anton Osokin", "Dmitry Vetrov"], "venue": null, "citeRegEx": "Bartunov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bartunov et al\\.", "year": 2016}, {"title": "Improving distributed representation of word sense via wordnet gloss composition and context clustering", "author": ["Tao Chen", "Ruifeng Xu", "Yulan He", "Xuan Wang."], "venue": "Association for Computational Linguistics.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "A unified model for word sense representation and disambiguation", "author": ["Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun."], "venue": "EMNLP. Citeseer, pages 1025\u2013 1035.", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Retrofitting sense-specific word vectors using parallel text", "author": ["Allyson Ettinger", "Philip Resnik", "Marine Carpuat."], "venue": "Proceedings of NAACL-HLT . pages 1378\u20131383.", "citeRegEx": "Ettinger et al\\.,? 2016", "shortCiteRegEx": "Ettinger et al\\.", "year": 2016}, {"title": "Learning sense-specific word embeddings by exploiting bilingual resources", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "COLING. pages 497\u2013507.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Huang et al\\.,? 2012", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Sensembed: Learning sense embeddings for word and relational similarity", "author": ["Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli."], "venue": "ACL (1). pages 95\u2013105.", "citeRegEx": "Iacobacci et al\\.,? 2015", "shortCiteRegEx": "Iacobacci et al\\.", "year": 2015}, {"title": "Roget\u2019s thesaurus and semantic similarity", "author": ["Mario Jarmasz", "Stan Szpakowicz."], "venue": "Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003:111.", "citeRegEx": "Jarmasz and Szpakowicz.,? 2004", "shortCiteRegEx": "Jarmasz and Szpakowicz.", "year": 2004}, {"title": "Ontologically grounded multi-sense representation learning for semantic vector space models", "author": ["Sujay Kumar Jauhar", "Chris Dyer", "Eduard H Hovy."], "venue": "HLT-NAACL. pages 683\u2013693.", "citeRegEx": "Jauhar et al\\.,? 2015", "shortCiteRegEx": "Jauhar et al\\.", "year": 2015}, {"title": "Neural context embeddings for automatic discovery of word senses", "author": ["Mikael K\u00e5geb\u00e4ck", "Fredrik Johansson", "Richard Johansson", "Devdatt Dubhashi."], "venue": "Proceedings of NAACL-HLT . pages 25\u201332.", "citeRegEx": "K\u00e5geb\u00e4ck et al\\.,? 2015", "shortCiteRegEx": "K\u00e5geb\u00e4ck et al\\.", "year": 2015}, {"title": "A solution to plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K Landauer", "Susan T Dumais."], "venue": "Psychological review 104(2):211.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Molding cnns for text: non-linear, non-consecutive convolutions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "EMNLP .", "citeRegEx": "Lei et al\\.,? 2015", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Rationalizing neural predictions", "author": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .", "citeRegEx": "Lei et al\\.,? 2016", "shortCiteRegEx": "Lei et al\\.", "year": 2016}, {"title": "Do multi-sense embeddings improve natural language understanding", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Li and Jurafsky.,? \\Q2015\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2015}, {"title": "Learning context-sensitive word embeddings with neural tensor skip-gram model", "author": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "venue": "IJCAI. pages 1284\u20131290.", "citeRegEx": "Liu et al\\.,? 2015a", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Topical word embeddings", "author": ["Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun."], "venue": "AAAI. pages 2418\u20132424.", "citeRegEx": "Liu et al\\.,? 2015b", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Association for Computational Linguistics", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of Workshop at ICLR .", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller."], "venue": "Communications of the ACM 38(11):39\u2013", "citeRegEx": "Miller.,? 1995", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Playing atari with deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller."], "venue": "NIPS Deep Learning Workshop .", "citeRegEx": "Mnih et al\\.,? 2013", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Efficient nonparametric estimation of multiple embeddings per word in vector space", "author": ["Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language", "citeRegEx": "Neelakantan et al\\.,? 2014", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2014}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "volume 14, pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "De-conflated semantic representations", "author": ["Mohammad Taher Pilehvar", "Nigel Collier."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .", "citeRegEx": "Pilehvar and Collier.,? 2016", "shortCiteRegEx": "Pilehvar and Collier.", "year": 2016}, {"title": "Contextdependent sense embedding", "author": ["Lin Qiu", "Kewei Tu", "Yong Yu."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Qiu et al\\.,? 2016", "shortCiteRegEx": "Qiu et al\\.", "year": 2016}, {"title": "Multi-prototype vector-space models of word meaning", "author": ["Joseph Reisinger", "Raymond J Mooney."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Reisinger and Mooney.,? 2010", "shortCiteRegEx": "Reisinger and Mooney.", "year": 2010}, {"title": "Autoextend: Extending word embeddings to embeddings for synsets and lexemes", "author": ["Sascha Rothe", "Hinrich Sch\u00fctze."], "venue": "arXiv preprint arXiv:1507.01127 .", "citeRegEx": "Rothe and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Rothe and Sch\u00fctze.", "year": 2015}, {"title": "The westbury lab wikipedia", "author": ["Cyrus Shaoul", "Chris Westbury"], "venue": null, "citeRegEx": "Shaoul and Westbury.,? \\Q2010\\E", "shortCiteRegEx": "Shaoul and Westbury.", "year": 2010}, {"title": "Bilingual learning of multi-sense embeddings with discrete autoencoders", "author": ["Simon \u0160uster", "Ivan Titov", "Gertjan van Noord."], "venue": "NAACL-HLT 2016 .", "citeRegEx": "\u0160uster et al\\.,? 2016", "shortCiteRegEx": "\u0160uster et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto."], "venue": "MIT press Cambridge.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "A probabilistic model for learning multi-prototype word embeddings", "author": ["Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu."], "venue": "COLING. pages 151\u2013160.", "citeRegEx": "Tian et al\\.,? 2014", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Mining the web for synonyms: Pmi-ir versus lsa on toefl", "author": ["Peter D Turney."], "venue": "European Conference on Machine Learning. Springer, pages 491\u2013502.", "citeRegEx": "Turney.,? 2001", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "K-embeddings: Learning conceptual embeddings for words using context", "author": ["Thuy Vu", "D Stott Parker."], "venue": "Proceedings of NAACL-HLT . pages 1262\u20131267.", "citeRegEx": "Vu and Parker.,? 2016", "shortCiteRegEx": "Vu and Parker.", "year": 2016}, {"title": "It makes sense: A wide-coverage word sense disambiguation system for free text", "author": ["Zhi Zhong", "Hwee Tou Ng."], "venue": "Proceedings of the ACL 2010 System Demonstrations. Association for Computational Linguistics, pages 78\u201383.", "citeRegEx": "Zhong and Ng.,? 2010", "shortCiteRegEx": "Zhong and Ng.", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Due to the theoretical inability to account for polysemy using a single embedding representation per word, multi-sense word representations are proposed to address the ambiguity issue using multiple embedding representations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012).", "startOffset": 256, "endOffset": 304}, {"referenceID": 5, "context": "Due to the theoretical inability to account for polysemy using a single embedding representation per word, multi-sense word representations are proposed to address the ambiguity issue using multiple embedding representations for different senses in a word (Reisinger and Mooney, 2010; Huang et al., 2012).", "startOffset": 256, "endOffset": 304}, {"referenceID": 20, "context": "Neelakantan et al. (2014) pointed out that, due to triangle inequality in vector space, if one word has two different senses but is restricted to one embedding, the sum of the distances between the word and its synonym in each sense would upper-bound the distance between the respective synonyms, which may be mutually irrelevant, in embedding space1.", "startOffset": 0, "endOffset": 26}, {"referenceID": 21, "context": "Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016).", "startOffset": 96, "endOffset": 163}, {"referenceID": 13, "context": "Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016).", "startOffset": 96, "endOffset": 163}, {"referenceID": 24, "context": "Under this framework, prior work focused on designing a single model to deliver both mechanisms (Neelakantan et al., 2014; Li and Jurafsky, 2015; Qiu et al., 2016).", "startOffset": 96, "endOffset": 163}, {"referenceID": 21, "context": "However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (Neelakantan et al., 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al.", "startOffset": 229, "endOffset": 278}, {"referenceID": 13, "context": "However, the previously proposed models introduce side-effects: 1) mixing word-level and sense-level tokens achieves efficient sense selection but introduces ambiguous word-level tokens during the representation learning process (Neelakantan et al., 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al.", "startOffset": 229, "endOffset": 278}, {"referenceID": 24, "context": ", 2014; Li and Jurafsky, 2015), and 2) pure sense-level tokens prevent ambiguity from word-level tokens but require exponential time complexity when decoding a sense sequence (Qiu et al., 2016).", "startOffset": 175, "endOffset": 193}, {"referenceID": 5, "context": "With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014).", "startOffset": 130, "endOffset": 176}, {"referenceID": 21, "context": "With the power of deep learning, some work exploited neural networks to learn embeddings with sense selection based on clustering (Huang et al., 2012; Neelakantan et al., 2014).", "startOffset": 130, "endOffset": 176}, {"referenceID": 19, "context": "(2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995).", "startOffset": 94, "endOffset": 108}, {"referenceID": 18, "context": "(2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b).", "startOffset": 76, "endOffset": 99}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995).", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure.", "startOffset": 0, "endOffset": 145}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure.", "startOffset": 0, "endOffset": 170}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering.", "startOffset": 0, "endOffset": 291}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning. Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al.", "startOffset": 0, "endOffset": 823}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning. Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training.", "startOffset": 0, "endOffset": 848}, {"referenceID": 0, "context": "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model using WordNet (Miller, 1995). K\u00e5geb\u00e4ck et al. (2015) and Vu and Parker (2016) further leveraged a weighting mechanism and interactive process in the clustering procedure. Moreover, Guo et al. (2014) leveraged bilingual resources for clustering. However, most of the above approaches separated the clustering procedure and the representation learning procedure without a joint objective, which may suffer from the error propagation issue. Instead, the proposed approach, MUSE, enables joint training on sense selection and representation learning. Instead of clustering, probabilistic modeling methods have been applied for learning multisense embeddings in order to make the sense selection more flexible, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training. Li and Jurafsky (2015) exploited Chinese Restaurant Process to infer the sense identity.", "startOffset": 0, "endOffset": 922}, {"referenceID": 0, "context": "Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al.", "startOffset": 13, "endOffset": 36}, {"referenceID": 24, "context": "Recently, Qiu et al. (2016) proposed an EM algorithm to learn purely sense-level representations, where the computational cost is high when decoding the sense identity sequence, because it takes exponential time to search all sense combination within a context window.", "startOffset": 10, "endOffset": 28}, {"referenceID": 23, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 26, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 8, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 1, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 6, "context": "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Pilehvar and Collier, 2016; Rothe and Sch\u00fctze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.", "startOffset": 94, "endOffset": 211}, {"referenceID": 4, "context": ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; \u0160uster et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus.", "startOffset": 26, "endOffset": 88}, {"referenceID": 3, "context": ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; \u0160uster et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus.", "startOffset": 26, "endOffset": 88}, {"referenceID": 28, "context": ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; \u0160uster et al., 2016), which may be unavailable in some language, our model can be trained using only an unlabeled corpus.", "startOffset": 26, "endOffset": 88}, {"referenceID": 17, "context": "The architecture is similar to continuous bag-of-words (CBOW) (Mikolov et al., 2013a).", "startOffset": 62, "endOffset": 85}, {"referenceID": 24, "context": "The prior work using a single model with purely sense-level tokens (Qiu et al., 2016) requires exponential time to calculate the collocation energy for every possible combination of sense identities within a context window, O(n2m), for a single target sense.", "startOffset": 67, "endOffset": 85}, {"referenceID": 24, "context": "The prior work using a single model with purely sense-level tokens (Qiu et al., 2016) requires exponential time to calculate the collocation energy for every possible combination of sense identities within a context window, O(n2m), for a single target sense. Further, Qiu et al. (2016) took an additional sequence decoding step with quadratic time complexity O(n4mL), based on an exponential number n2m in the base unit.", "startOffset": 68, "endOffset": 286}, {"referenceID": 18, "context": "In this paper, we use the skip-gram formulation (Mikolov et al., 2013b) considering that it requires less training time, where only two sense identities are required for stochastic training.", "startOffset": 48, "endOffset": 71}, {"referenceID": 22, "context": "Other popular candidates, like GloVe (Pennington et al., 2014) and CBOW (Mikolov et al.", "startOffset": 37, "endOffset": 62}, {"referenceID": 17, "context": ", 2014) and CBOW (Mikolov et al., 2013a), require more sense identities to be selected as input and thus not suitable for our scenario.", "startOffset": 17, "endOffset": 40}, {"referenceID": 22, "context": "For example, GloVe (Pennington et al., 2014) takes computationally expensive collocation counting statistics for each token in a corpus as input, which requires sense selection for every occurrence of the target word across the whole corpus for a single optimization step.", "startOffset": 19, "endOffset": 44}, {"referenceID": 18, "context": "Instead of enumerating all possible collocated senses which is computationally expensive, we use the skip-gram objective (4) (Mikolov et al., 2013b) to approximate (3) as shown in the green block of Figure 1.", "startOffset": 125, "endOffset": 148}, {"referenceID": 30, "context": "In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process.", "startOffset": 60, "endOffset": 146}, {"referenceID": 8, "context": "In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process.", "startOffset": 60, "endOffset": 146}, {"referenceID": 13, "context": "In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process.", "startOffset": 60, "endOffset": 146}, {"referenceID": 0, "context": "In contrast, most related work using probabilistic modeling (Tian et al., 2014; Jauhar et al., 2015; Li and Jurafsky, 2015; Bartunov et al., 2016) binded sense representations with the sense selection mechanism, so efficient sense selection by leveraging wordlevel tokens can be achieved only at the cost of mixing word-level and sense-level tokens in their representation learning process.", "startOffset": 60, "endOffset": 146}, {"referenceID": 14, "context": "In our experiment with |Zi| senses for word wi, we use (1/|Zi|) word-level unigram as sense-level unigram for efficiency and the 3/4-th power trick in Mikolov et al. (2013b). We note that our modular framework can easily maintain purely sense-level tokens with an arbitrary representation learning model.", "startOffset": 151, "endOffset": 174}, {"referenceID": 29, "context": "The learning algorithm can be viewed as a reinforcement learning algorithm solving a onestep Markov Decision Process (MDP) (Sutton and Barto, 1998), where the state, action, and reward correspond to context C\u0304t, sense zik, and collocation log likelihood log L\u0304(\u00b7) respectively.", "startOffset": 123, "endOffset": 147}, {"referenceID": 12, "context": "The objective is differentiable and supports stochastic optimization (Lei et al., 2016), which uses a stochastic sample zik for optimization.", "startOffset": 69, "endOffset": 87}, {"referenceID": 20, "context": "To address the above issues, we apply the Qlearning algorithm (Mnih et al., 2013).", "startOffset": 62, "endOffset": 81}, {"referenceID": 20, "context": "We further follow the convention of recent neural reinforcement learning by reducing the reward range to aid model training (Mnih et al., 2013).", "startOffset": 124, "endOffset": 143}, {"referenceID": 20, "context": "most literature adopted square loss to characterize the discrepancy between the target and estimated Q-values (Mnih et al., 2013).", "startOffset": 110, "endOffset": 129}, {"referenceID": 18, "context": "Furthermore, given that the collocation likelihood in (4) is an approximation to the original categorical distribution with a softmax function shown in (3) (Mikolov et al., 2013b), we revise the formulation by omitting the negative sampling term.", "startOffset": 156, "endOffset": 179}, {"referenceID": 21, "context": "The major distinction between our modular framework and two-stage clusteringrepresentation learning framework (Neelakantan et al., 2014; Vu and Parker, 2016) is that we establish a reward signal from the sense representation to the sense selection module to enable immediate and joint optimization.", "startOffset": 110, "endOffset": 157}, {"referenceID": 32, "context": "The major distinction between our modular framework and two-stage clusteringrepresentation learning framework (Neelakantan et al., 2014; Vu and Parker, 2016) is that we establish a reward signal from the sense representation to the sense selection module to enable immediate and joint optimization.", "startOffset": 110, "endOffset": 157}, {"referenceID": 21, "context": "Given a fitness estimation for each sense, exploiting the greedy sense is the most popular strategy for clustering algorithms (Neelakantan et al., 2014; K\u00e5geb\u00e4ck et al., 2015) and hard-EM algorithms (Qiu et al.", "startOffset": 126, "endOffset": 175}, {"referenceID": 9, "context": "Given a fitness estimation for each sense, exploiting the greedy sense is the most popular strategy for clustering algorithms (Neelakantan et al., 2014; K\u00e5geb\u00e4ck et al., 2015) and hard-EM algorithms (Qiu et al.", "startOffset": 126, "endOffset": 175}, {"referenceID": 24, "context": ", 2015) and hard-EM algorithms (Qiu et al., 2016; Jauhar et al., 2015) in literature.", "startOffset": 31, "endOffset": 70}, {"referenceID": 8, "context": ", 2015) and hard-EM algorithms (Qiu et al., 2016; Jauhar et al., 2015) in literature.", "startOffset": 31, "endOffset": 70}, {"referenceID": 29, "context": "The dilemma between exploring sub-optimal choices and exploiting the optimal choice is called exploration-exploitation trade-off in reinforcement learning (Sutton and Barto, 1998).", "startOffset": 155, "endOffset": 179}, {"referenceID": 20, "context": "\u2022 -Greedy: selects a random sense with probability, and adopts the greedy strategy otherwise (Mnih et al., 2013).", "startOffset": 93, "endOffset": 112}, {"referenceID": 27, "context": "Our model is trained on the April 2010 Wikipedia dump (Shaoul and Westbury, 2010), which contains approximately 1 billion tokens.", "startOffset": 54, "endOffset": 81}, {"referenceID": 16, "context": "For preprocessing, we convert all words to their lower cases, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al., 2014), and remove all sentences with less than 10 tokens.", "startOffset": 127, "endOffset": 149}, {"referenceID": 21, "context": "The number of senses per word in Q is set to 3 for fair comparison with prior work (Neelakantan et al., 2014).", "startOffset": 83, "endOffset": 109}, {"referenceID": 5, "context": "For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 5, "context": "For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al. (2014). For preprocessing, we convert all words to their lower cases, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al.", "startOffset": 57, "endOffset": 107}, {"referenceID": 5, "context": "To evaluate the quality of the learned sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford\u2019s Contextual Word Similarities (SCWS) data (Huang et al., 2012).", "startOffset": 248, "endOffset": 268}, {"referenceID": 5, "context": "To evaluate the quality of the learned sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford\u2019s Contextual Word Similarities (SCWS) data (Huang et al., 2012). Specifically, given a list of word pairs with corresponding contexts, S = {(wi, C\u0304t, wj , C\u0304t\u2032)}, we calculate the Spearman\u2019s rank correlation \u03c1 between human-judged similarity and model similarity estimations. Two major contextual similarity estimations are introduced by Reisinger and Mooney (2010): AvgSimC and MaxSimC.", "startOffset": 249, "endOffset": 571}, {"referenceID": 0, "context": "8 Bartunov et al. (2016) 53.", "startOffset": 2, "endOffset": 25}, {"referenceID": 0, "context": "8 Bartunov et al. (2016) 53.8 61.2 Qiu et al. (2016) 64.", "startOffset": 2, "endOffset": 53}, {"referenceID": 5, "context": "The baselines for comparison include classic clustering methods (Huang et al., 2012; Neelakantan et al., 2014), EM algorithms (Tian et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 21, "context": "The baselines for comparison include classic clustering methods (Huang et al., 2012; Neelakantan et al., 2014), EM algorithms (Tian et al.", "startOffset": 64, "endOffset": 110}, {"referenceID": 30, "context": ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al.", "startOffset": 23, "endOffset": 83}, {"referenceID": 24, "context": ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al.", "startOffset": 23, "endOffset": 83}, {"referenceID": 0, "context": ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al.", "startOffset": 23, "endOffset": 83}, {"referenceID": 13, "context": ", 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al.", "startOffset": 40, "endOffset": 63}, {"referenceID": 0, "context": ", 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al. (2016) used more recent Wikipedia dumps.", "startOffset": 8, "endOffset": 166}, {"referenceID": 0, "context": ", 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015)4, where all approaches are trained on the same corpus except Qiu et al. (2016) used more recent Wikipedia dumps. The embedding sizes of all baselines are 300, except 50 in Huang et al. (2012). For every competitor with multiple settings, we report the best performance in each similarity measurement setting and show in Table 1.", "startOffset": 8, "endOffset": 279}, {"referenceID": 13, "context": "We run Li and Jurafsky (2015)\u2019s released code on our corpus for fair comparison.", "startOffset": 7, "endOffset": 30}, {"referenceID": 8, "context": "We further evaluate our model on synonym selection using multi-sense word representations (Jauhar et al., 2015).", "startOffset": 90, "endOffset": 111}, {"referenceID": 31, "context": "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.", "startOffset": 50, "endOffset": 64}, {"referenceID": 7, "context": "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.", "startOffset": 73, "endOffset": 103}, {"referenceID": 10, "context": "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.", "startOffset": 118, "endOffset": 145}, {"referenceID": 8, "context": "For multi-sense representations system, it selects the synonym of the question word wQ using the maximum senselevel cosine similarity as a proxy of the semantic similarity (Jauhar et al., 2015).", "startOffset": 172, "endOffset": 193}, {"referenceID": 5, "context": "Our model is compared with the following baselines: 1) conventional word embeddings: global context vectors (Huang et al., 2012) and skipgram (Mikolov et al.", "startOffset": 108, "endOffset": 128}, {"referenceID": 18, "context": ", 2012) and skipgram (Mikolov et al., 2013b); 2) applying supervised word sense disambiguation using the", "startOffset": 21, "endOffset": 44}, {"referenceID": 33, "context": "IMS system and then applying skip-gram on disambiguated corpus (IMS+SG) (Zhong and Ng, 2010); 3) unsupervised sense embeddings: EM algorithm (Jauhar et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "IMS system and then applying skip-gram on disambiguated corpus (IMS+SG) (Zhong and Ng, 2010); 3) unsupervised sense embeddings: EM algorithm (Jauhar et al., 2015), multi-sense skipgram (MSSG) (Neelakantan et al.", "startOffset": 141, "endOffset": 162}, {"referenceID": 21, "context": ", 2015), multi-sense skipgram (MSSG) (Neelakantan et al., 2014), Chinese Restaurant Process (CPR) (Li and Jurafsky, 2015), and the MUSE models; 4) supervised sense embeddings with WordNet: retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG) (Jauhar et al.", "startOffset": 37, "endOffset": 63}, {"referenceID": 13, "context": ", 2014), Chinese Restaurant Process (CPR) (Li and Jurafsky, 2015), and the MUSE models; 4) supervised sense embeddings with WordNet: retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG) (Jauhar et al.", "startOffset": 42, "endOffset": 65}, {"referenceID": 8, "context": ", 2014), Chinese Restaurant Process (CPR) (Li and Jurafsky, 2015), and the MUSE models; 4) supervised sense embeddings with WordNet: retrofitting global context vectors (Retro-GC) and retrofitting skip-gram (Retro-SG) (Jauhar et al., 2015).", "startOffset": 218, "endOffset": 239}], "year": 2017, "abstractText": "This paper proposes to address the word sense ambiguity issue in an unsupervised manner, where word sense representations are learned along a word sense selection mechanism given contexts. Prior work about learning multi-sense embeddings suffered from either ambiguity of different-level embeddings or inefficient sense selection. The proposed modular framework, MUSE, implements flexible modules to optimize distinct mechanisms, achieving the first purely sense-level representation learning system with lineartime sense selection. We leverage reinforcement learning to enable joint training on the proposed modules, and introduce various exploration techniques on sense selection for better robustness. The experiments on benchmark data show that the proposed approach achieves the state-ofthe-art performance on synonym selection as well as on contextual word similarities in terms of MaxSimC.", "creator": "LaTeX with hyperref package"}}}