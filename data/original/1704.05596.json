{"id": "1704.05596", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Stochastic Gradient Twin Support Vector Machine for Large Scale Problems", "abstract": "For classification problems, twin support vector machine (TSVM) with nonparallel hyperplanes has been shown to be more powerful than support vector machine (SVM). However, it is time consuming and insufficient memory to deal with large scale problems due to calculating the inverse of matrices. In this paper, we propose an efficient stochastic gradient twin support vector machine (SGTSVM) based on stochastic gradient descent algorithm (SGD). As far as now, it is the first time that SGD is applied to TSVM though there have been some variants where SGD was applied to SVM (SGSVM). Compared with SGSVM, our SGTSVM is more stable, and its convergence is also proved. In addition, its simple nonlinear version is also presented. Experimental results on several benchmark and large scale datasets have shown that the performance of our SGTSVM is comparable to the current classifiers with a very fast learning speed.", "histories": [["v1", "Wed, 19 Apr 2017 03:08:38 GMT  (73kb)", "http://arxiv.org/abs/1704.05596v1", "26 pages, 46 figures except 3 oversized figures"]], "COMMENTS": "26 pages, 46 figures except 3 oversized figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["zhen wang", "yuan-hai shao", "lan bai", "li-ming liu", "nai-yang deng"], "accepted": false, "id": "1704.05596"}, "pdf": {"name": "1704.05596.pdf", "metadata": {"source": "CRF", "title": "Stochastic Gradient Twin Support Vector Machine for Large Scale Problems", "authors": ["Zhen Wang", "Yuan-Hai Shao", "Lan Bai", "Li-Ming Liu", "Nai-Yang Deng"], "emails": ["wangzhen@imu.edu.cn.", "shaoyuanhai21@163.com.", "bailanhaomei@163.com.", "llm5609@163.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n05 59\n6v 1\n[ cs\n.L G\n] 1\n9 A\npr 2\n01 7\npowerful than support vector machine (SVM). However, it is time consuming and insufficient memory to deal with large scale problems due to calculating the inverse of matrices. In this paper, we propose an efficient stochastic gradient twin support vector machine (SGTSVM) based on stochastic gradient descent algorithm (SGD). As far as now, it is the first time that SGD is applied to TSVM though there have been some variants where SGD was applied to SVM (SGSVM). Compared with SGSVM, our SGTSVM is more stable, and its convergence is also proved. In addition, its simple nonlinear version is also presented. Experimental results on several benchmark and large scale datasets have shown that the performance of our SGTSVM is comparable to the current classifiers with a very fast learning speed.\nIndex Terms\u2014Classification, support vector machine, twin support vector machine, stochastic gradient descent.\n\u2726\n1 INTRODUCTION\nSupport vector machine (SVM), being powerful tool for classification [1], [2], [3], has already outperformed most other classifiers in a wide variety of applications [4], [5], [6]. Different from SVM with a pair of parallel hyperplanes, twin support vector machine (TSVM) [7] with a pair of nonparallel hyperplanes has been proposed and developed, e.g., twin bounded support vector machine (TBSVM) [8], twin parametric margin support vector machine (TPMSVM) [9], and weighted Lagrangian twin support vector machine (WLTSVM) [10]. These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15]. In the training stage, SVM solves a quadratic programming problem (QPP), whereas TSVM solve two smaller QPPs by traditional solver such as interior method [2], [16], [7]. However, neither SVM nor TSVM based on these solvers can deal with the large scale problems, especially millions of samples.\nIn order to deal with the large scale problems, many improvements were proposed, e.g., for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21]. The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of\n\u2022 Zhen Wang is with School of Mathematical Sciences, Inner Mongolia University, Hohhot, 010021, P.R.China, e-mail: wangzhen@imu.edu.cn. \u2022 Yuan-Hai Shao is with the Zhijiang College, Zhejiang University of Technology, Hangzhou, 310024, P.R.China, e-mail: shaoyuanhai21@163.com. \u2022 Lan Bai is with School of Mathematical Sciences, Inner Mongolia University, Hohhot, 010021, P.R.China e-mail: bailanhaomei@163.com. \u2022 Li-Ming Liu is with the School of Statistics, Capital University of Economics and Business, Beijing, 100070, P.R.China, e-mail: llm5609@163.com \u2022 Nai-Yang Deng is with the College of Science, China Agricultural University, Beijing, 100083, P.R.China, e-mail: dengnaiyang@cau.edu.cn.\nsubproblems by stochastic sampling with a suitable size. It has been proved that SGSVM is almost sure convergent, and thus is able to find an approximation of the desired solution with high probability [26], [23], [24]. The experiments confirm the effectiveness of these algorithms with an amazing learning speed.\nHowever, for large scale problems, the sampling in SGD may bring some difficulties to SVM due to only a small subset of the dataset is selected for training. In fact, if the subset is not suitable, SGSVM would be weak. It is well known that in SVM the support vectors (SVs), a small subset of the dataset, decides the final classifier. If the stochastic sampling does not include the SVs sufficiently, the classifier would lose some generalizations. Figure 1 is a toy example for SGSVM. There are two classes in this figure, where the positive and the negative class respectively include 6 and 4 samples, where the circle is one of the potential SVs. The solid blue line is the separating line obtained by SGSVM with three different sampling: (i) strengthening the circle sample; (ii) infrequently using the circle sample; (iii) ignoring the circle sample. Figure 1 shows that the circle sample plays an important role, and infrequently using or ignoring this sample would lead to misclassify.\nCompared with SVM, it is significant that TSVM is more stable for sampling and does not strongly depend on some special samples such as the SVs [7], [8], which indicates SGD is more suitable for TSVM. Therefore, in this paper, we propose a stochastic gradient twin support vector machine (SGTSVM). Different from SGSVM, our method selects two samples from different classes randomly in each iteration to construct a pair of nonparallel hyperplanes. Due to TSVM fits all of the training samples, our method is stable for the stochastic sampling and thus gains well generalizations. Moreover, the characteristics inherited from TSVM result in that our SGTSVM suits many cases, e.g., \u201ccross plane\u201d dataset [27] and preferential classification [7]. As the toy example, Figure 2 shows the corresponding results by SGTSVM. Comparing Figure 1 and Figure 2, it is clear that\nSGTSVM performs better than SGSVM. The main contributions of this paper includes: (i) a SGD-based TSVM (SGTSVM) is first proposed, and it is very easy to be extended to other TSVM-type classifiers; (ii) we prove that the proposed SGTSVM is convergent, instead of almost sure convergence in SGSVM; (iii) for the uniformly sampling, it is proved that the original objective of the solution to SGTSVM is bounded by the optimum of TBSVM, which indicates the solution to SGTSVM is an approximation of the optimal solution to TBSVM, while SGSVM only has an opportunity to obtain an approximation of the optimal solution to SVM (see Corollaries 1 and 2 in [24]);\n(iv) the nonlinear case of SGTSVM is obtained directly based on its original problem, whereas the nonlinear case of SGSVM is derived from SVM\u2019s dual problem;\n(v) each iteration of SGTSVM includes no more than 8n + 4 multiplications without additional storage, so it is the fastest one than other proposed TSVM-type classifiers.\nThe rest of this paper is organized as follow. Section 2 briefly reviews SVM, SGSVM, and TBSVM. Our linear and nonlinear SGTSVMs together with the theoretical analysis are elaborated in Section 3. Experiments are arranged in Section 4. Finally, we give the conclusions.\n2 RELATED WORKS\nConsider a binary classification problem in the ndimensional real space Rn. The set of training samples is represented by X \u2208 Rn\u00d7m, where x \u2208 Rn is the sample\nwith the label y \u2208 {+1,\u22121}. We further organize the m1 samples of Class +1 into a matrix X1 \u2208 R\nn\u00d7m1 and the m2 samples of Class \u22121 into a matrix X2 \u2208 R\nn\u00d7m2 . Below, we give a brief outlines some related works.\n2.1 CSVM\nC-support vector machine (CSVM) [28], one formulation of the standard SVM, searches for a separating hyperplane\nw\u22a4x+ b = 0, (1)\nwhere w \u2208 Rn and b \u2208 R. By introducing the regularization term, the primal problem of CSVM can be expressed as a quadratic programming problem (QPP) as follow\nmin w,b\n1 2 ||w|| 2 + c m e\u22a4\u03be\ns.t. D(X\u22a4w + b) \u2265 e\u2212 \u03be, \u03be \u2265 0, (2)\nwhere || \u00b7 || denotes the L2 norm, c > 0 is a parameter with some quantitative meanings [28], e is a vector of ones with an appropriate dimension, \u03be \u2208 Rm is the slack vector, and D = diag(y1, . . . , ym). Note that the minimization of the regularization term 12\u2016w\u2016\n2 is equivalent to maximize the margin between two parallel supporting hyperplanes w\u22a4x+ b = \u00b11. And the structural risk minimization principle is implemented in this problem [1].\n2.2 SGSVM\nSGSVM (or PEGASOS as an alias) [23], [24] considers a strongly convex problem by modifying (2) as follow\nmin w\n1 2 ||w|| 2 + c m e\u22a4\u03be\ns.t. DX\u22a4w \u2265 e\u2212 \u03be, \u03be \u2265 0, (3)\nand recasts the above problem to\nmin w\n1 2 ||w|| 2 + c m e\u22a4(e \u2212DX\u22a4w)+, (4)\nwhere (\u00b7)+ replaces negative components of a vector by zeros.\nIn the tth iteration (t \u2265 1), SGSVM constructs a temporary function, which is defined by a random sample xt \u2208 X as\ngt(w) = 1 2 ||w|| 2 + c(1\u2212 ytw \u22a4xt)+. (5)\nThen, starting with an initial w1, SGSVM iteratively updates wt+1 = wt \u2212 \u03b7t\u2207t for t \u2265 1, where \u03b7t = 1/t is the step size and \u2207t is the sub-gradient of gt(w) at wt,\n\u2207t = wt \u2212 cytxtsign(1\u2212 ytw \u22a4 t xt)+. (6)\nAfter a predetermined T iterations, the last wT+1 is outputted as w. And a new sample x can be predicted by\ny = sign(w\u22a4x). (7)\nIt has been proved that the average solution w\u0304 =\n1 T\nT\u2211 t=1 wt is bounded by the optimal solution w \u2217 to (4) with o(1), and thus SGSVM has with a probability of at least 1/2 to find a good approximation of w\u2217 [24]. The authors of [24] also pointed out that wT is often used instead of w\u0304 in practice. The sample xt which is selected randomly can be replaced with a small subset belonging to the whole dataset, and the subset only including a sample is often used in practice [23], [24], [25]. In order to extend the generalization ability of SGSVM, the bias term b in CSVM can be appended to SGSVM by replacing g(wt) of (5) with\ng(wt, b) = 1 2 ||wt|| 2 + C(1 \u2212 yt(w \u22a4 t xt + b))+. (8)\nHowever, this modification would lead to the function not to be strongly convex and thus yield a slow convergence rate [24].\n2.3 TBSVM\nTBSVM [8], a representative of TSVM, seeks a pair of nonparallel hyperplanes in Rn which can be expressed as\nw\u22a41 x+ b1 = 0 and w \u22a4 2 x+ b2 = 0, (9)\nsuch that each hyperplane is close to samples of one class and has a certain distance from the other class. To find the pair of nonparallel hyperplanes, it is required to get the solutions to the primal problems\nmin w1,b1\n1 2 (||w1|| 2 + b21) + c1 2m1 \u2016X\u22a41 w1 + b1\u2016 2 + c2 m2 e\u22a4\u03be1,\ns.t. X\u22a42 w1 + b1 \u2212 \u03be1 \u2264 \u2212e, \u03be1 \u2265 0, (10)\nand\nmin w2,b2\n1 2 (||w2|| 2 + b22) + c3 2m2 \u2016X\u22a42 w2 + b2\u2016 2 + c4 m1 e\u22a4\u03be2,\ns.t. X\u22a41 w2 + b2 + \u03be2 \u2265 e, \u03be2 \u2265 0, (11)\nwhere c1, c2, c3, and c4 are positive parameters, \u03be1 \u2208 R m2 and \u03be2 \u2208 R m1 are slack vectors. Their geometric meaning is clear. For example, for (10), its objective function makes the samples of the first class proximal to the hyperplane w\u22a41 x + b1 = 0 together with the regularization term, while the constraints make each sample of the second class has a distance more than 1/||w1|| away from the hyperplane w\u22a41 x+ b1 = \u22121.\nOnce the solutions (w1, b1) and (w2, b2) to the problems (10) and (11) are respectively obtained, a new point x \u2208 Rn is assigned to which class depends on the distance to the two hyperplanes in (9), i.e.,\ny = argmin i\n|w\u22a4i x+bi| \u2016wi\u2016 , (12)\nwhere | \u00b7 | is the absolute value.\n3 SGTSVM\nIn this section, we elaborate our SGTSVMwith its nonlinear formation, and give its convergence analysis together with the boundedness.\n3.1 Linear Formation\nFollowing the notations in Section 2, we recast the QPPs (10) and (11) to unconstrained problems\nmin w1,b1\n1 2 (||w1|| 2 + b21) + c1 2m1 ||X\u22a41 w1 + b1|| 2\n+ c2 m2 e\u22a4(e +X\u22a42 w1 + b1)+, (13)\nand\nmin w2,b2\n1 2 (||w2|| 2 + b22) + c3 2m2 ||X\u22a42 w2 + b2|| 2\n+ c4 m1 e\u22a4(e\u2212X\u22a41 w2 \u2212 b2)+, (14)\nrespectively. In order to solve the above two problems, we construct a series of strictly convex functions f1,t(w1, b1) and f2,t(w2, b2) with t \u2265 1 as\nf1,t = 1 2 (||w1|| 2 + b21) + c1 2 ||w \u22a4 1 xt + b1|| 2\n+ c2(1 + w \u22a4 1 x\u0302t + b1)+,\n(15)\nand\nf2,t = 1 2 (||w2|| 2 + b22) + c3 2 ||w \u22a4 2 x\u0302t + b2|| 2\n+ c4(1\u2212 w \u22a4 2 xt \u2212 b2)+,\n(16)\nwhere xt and x\u0302t are selected randomly from X1 and X2, respectively.\nThe sub-gradients of the above functions at (w1,t, b1,t) and (w2,t, b2,t) can be obtained as\n\u2207w1,tf1,t = w1,t + c1(w \u22a4 1,txt + b1,t)xt +c2x\u0302tsign(1 + w \u22a4 1,tx\u0302t + b1,t)+, \u2207b1,tf1,t = b1,t + c1(w \u22a4 1,txt + b1,t) +c2sign(1 + w \u22a4 1,tx\u0302t + b1,t)+,\n(17)\nand \u2207w2,tf2,t = w2,t + c3(w \u22a4 2,tx\u0302t + b2,t)x\u0302t\n\u2212c4xtsign(1\u2212 w \u22a4 2,txt \u2212 b2,t)+, \u2207b2,tf2,t = b2,t + c3(w \u22a4 2,tx\u0302t + b2,t) \u2212c4sign(1 \u2212 w \u22a4 2,txt \u2212 b1,t)+,\n(18)\nrespectively. Our SGTSVM starts from the initial (w1,1, b1,1) and (w2,t, b2,t). Then, for t \u2265 1, the updates are given by\nw1,t+1 = w1,t \u2212 \u03b7t\u2207w1,tf1,t, b1,t+1 = b1,t \u2212 \u03b7t\u2207b1,tf1,t, w2,t+1 = w2,t \u2212 \u03b7t\u2207w2,tf2,t, b2,t+1 = b2,t \u2212 \u03b7t\u2207b2,tf2,t,\n(19)\nwhere \u03b7t is the step size and typically is set to 1/t. If the terminated condition is satisfied, (w1, b1) = (w1,t, b1,t) and (w2, b2) = (w2,t, b2,t). Then, a new sample x \u2208 R\nn can be predicted the same as TBSVM.\nThe above procedures can be summarized as follows\nAlgorithm 1 SGTSVM Framework.\nInput:\nGiven the training datasetX1 \u2208 R n\u00d7m1 as positive class, X2 \u2208 R n\u00d7m2 as negative class, select parameters c1, c2,\nc3, c4, and a small tolerance tol, typically tol = 1e\u2212 4. Output:\nw1, b1, w2, b2. 1: set w1,1, b1,1, w2,1, and b2,1 be zeros;\nFor t = 1, 2, . . . 2: Choose a pair of samples xt and x\u0302t from X1 and X2 at\nrandom, respectively; 3: Compute the tth gradients by (17) and (18); 4: Update w1,t+1, b1,t+1, w2,t+1, and b2,t+1 by (19); 5: If ||w1,t+1 \u2212 w1,t|| + |b1,t+1 \u2212 b1,t| < tol, stop updating\nw1,t+1 and b1,t+1. Let w1 = w1,t+1, b1 = b1,t+1; 6: If ||w2,t+1 \u2212 w2,t|| + |b2,t+1 \u2212 b2,t| < tol, stop updating\nw2,t+1 and b2,t+1. Let w2 = w2,t+1, b2 = b2,t+1;\n3.2 Nonlinear Formation\nNow, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30]. SupposeK(\u00b7, \u00b7) is the predefined kernel function, then the nonparallel hyperplanes can be expressed as\nK(x,X)w1 + b1 = 0 and K(x,X)w2 + b2 = 0. (20)\nThe counterparts of (13) and (14) can be formulated as\nmin w1,b1\n1 2 (||w1|| 2 + b21) + c1 2m1 ||K(X1, X)w1 + b1|| 2\n+ c2 m2\ne\u22a4(e +K(X2, X)w1 + b1)+, (21)\nand\nmin w2,b2\n1 2 (||w2|| 2 + b22) + c3 2m2 ||K(X2, X)w2 + b2|| 2\n+ c4 m1\ne\u22a4(e\u2212K(X1, X)w2 \u2212 b2)+. (22)\nThen, we construct a series of functions with t \u2265 1 as\nh1,t = 1 2 (||w1|| 2 + b21) + c1 2 ||K(xt, X)w1 + b1|| 2\n+ c2(1 +K(x\u0302t, X)w1 + b1)+, (23)\nand\nh2,t = 1 2 (||w2|| 2 + b22) + c3 2 ||K(x\u0302t, X)w2 + b2|| 2\n+ c4(1\u2212K(xt, X)w2 \u2212 b2)+. (24)\nSimilar to (17), (18), and (19), the sub-gradients and updates can be obtained. The details are omitted.\nFor large scaled problems, it is time consuming to calculate the kernel K(\u00b7, X). However, the reduced kernel strategy, which has been successfully applied for SVM and TSVM [31], [32], [12], can also be applied for our SGTSVM. The reduced kernel strategy replaces K(\u00b7, X) with K(\u00b7, X\u0303), where X\u0303 is a random sampled subset of X . In practice, X\u0303 just needs 1% \u2212 10% samples from X to get a well performance, reducing the learning time without loss of generalization [32].\n3.3 Analysis\nIn this subsection, we discuss two issues: (i) the convergence of the solution in SGTSVM; (ii) the relation between the solution in SGTSVM and the optimal one in TBSVM. For convenience, we just consider the first QPP of linear TBSVM together with its SGD formation. The conclusions on another QPP and the nonlinear formations can be obtained easily the same as the first one.\nLet u = (w\u22a4, b)\u22a4, Z1 = (X \u22a4 1 , e) \u22a4, Z2 = (X \u22a4 2 , e) \u22a4, z = (x\u22a4, 1)\u22a4, and the notations with the subscripts in SGTSVM also comply with this definition. Then, the first QPP (13) is reformulated as\nmin u f(u) = 12 ||u|| 2 + c12m1 ||Z1u|| 2 + c2 m2 e\u22a4(e + Z2u)+.\n(25) Next, we reformulate the tth (t \u2265 1) function in SGTSVM as\nft(u) = 1 2 ||u|| 2 + c12 ||u \u22a4zt|| 2 + c2(1 + u \u22a4z\u0302t)+, (26)\nwhere zt and z\u0302t are the samples selected randomly from Z1 and Z2 for the tth iteration, respectively. The sub-gradient of ft(u) at ut is denoted as\n\u2207t = ut + c1(u \u22a4zt)zt + c2z\u0302tsign(1 + u \u22a4z\u0302t)+. (27)\nGiven u1 and the step size \u03b7t = 1/t, ut+1 with t \u2265 1 is updated by\nut+1 = ut \u2212 \u03b7t\u2207t, (28)\ni.e.,\nut+1 = (1 \u2212 1 t )ut \u2212 c1 t ztz \u22a4 t ut \u2212 c2 t z\u0302tsign(1 + u \u22a4 t z\u0302t)+.\n(29)\nLemma 3.1. For all t \u2265 1, ||\u2207t|| and ||ut|| have the upper bounds.\nProof. The formation (29) can be rewritten as\nut+1 = Atut + 1 t vt, (30)\nwhereAt = 1 t ((t\u22121)I\u2212c1ztz \u22a4 t ), I is the identity matrix, and vt = \u2212c2z\u0302tsign(1 + u \u22a4 t z\u0302t)+. Note that for sufficient t, there is a positive integer N such that for t > N , At is positive definite, and the largest eigenvalue \u03bbt of At is smaller than or equal to t\u22121\nt . Based on (30), we have\nut+1 = t\u220f\ni=N+1\nAt+N+1\u2212iuN+1 + t\u2211\ni=N+1\n1 i (\nt\u220f\nj=i+1\nAt+i+1\u2212j)vi.\n(31) For i \u2265 N + 1, ||At+N+1\u2212iuN+1|| \u2264 \u03bbi||uN+1|| \u2264 i\u22121 i ||uN+1|| [33]. Therefore,\n|| t\u220f\ni=N+1\nAt+N+1\u2212iuN+1|| \u2264 N t ||uN+1||, (32)\nand\n|| 1 i (\nt\u220f\nj=i+1\nAt+i+1\u2212j)vi|| \u2264 1 t max i\u2264t ||vi||. (33)\nThus, we have\n||ut+1|| \u2264 N t ||uN+1||+ t\u2212N t maxi\u2264t ||vi||\n\u2264 ||uN+1||+ c2 maxz\u2208Z2 ||z||. (34)\nLet M be the largest norm of the samples in the dataset and G1 = max{max{||u1||, . . . , ||uN ||}, ||uN+1|| + c2M}. This leads to that G1 is an upper bound of ||ut||, and G2 = G1 + c1G1M 2 + c2M is an upper bound of ||\u2207t||, for t \u2265 1.\nTheorem 3.1. The iterative formation (29) of our SGTSVM is convergent.\nProof. On the one hand, from (32) in the proof of Lemma 3.1, we have\nlim t\u2192\u221e\n|| t\u220f\ni=N+1\nAt+N+1\u2212iuN+1|| = 0, (35)\nwhich indicates\nlim t\u2192\u221e\nt\u220f\ni=N+1\nAt+N+1\u2212iuN+1 = 0. (36)\nOn the other hand, from (33), we have\nt\u2211\ni=N+1\n|| 1 i (\nt\u220f\nj=i+1\nAt+i+1\u2212j)vi|| \u2264 M, (37)\nwhich indicates that the following limit exists\nlim t\u2192\u221e\nt\u2211\ni=N+1\n|| 1 i (\nt\u220f\nj=i+1\nAt+i+1\u2212j)vi|| < \u221e. (38)\nNote that an infinite series of vectors is convergent if its norm series is convergent [34]. Therefore, the following limit exists\nlim t\u2192\u221e\nt\u2211\ni=N+1\n1 i (\nt\u220f\nj=i+1\nAt+i+1\u2212j)vi < \u221e. (39)\nCombine (36) with (39), we conclude that the series wt+1 is convergent for t \u2192 \u221e.\nBased on the above theorem, it is reasonable to take the terminate condition to be ||ut+1 \u2212 ut|| < tol. Moreover, if we reform (31) by u1, then\nut+1 = t\u220f\ni=1\nAt+1\u2212iu1 + t\u2211\ni=1\n1 i (\nt\u220f\nj=i+1\nAt+i+1\u2212j)vi. (40)\nIn order to keep ut+1 to be convergent fast, it is suggested to set u1 = 0.\nIn the following, we analyse the relation between the solution ut in SGTSVM and the optimal solution u\n\u2217 = (w\u2217\u22a4, b\u2217)\u22a4 in TBSVM.\nLemma 3.2. Let f1, . . . , fT be a sequence of convex functions, and u1, . . . , uT+1 \u2208 R\nn be a sequence of vectors. For t \u2265 1, ut+1 = ut \u2212 \u03b7t\u2207t, where \u2207t belongs to the subgradient set of ft at ut and \u03b7t = 1/t. Suppose ||ut|| and ||\u2207t|| have the upper boundsG1 andG2, respectively. Then, for all \u03b8 \u2208 Rn, we have\n(i) 1 T\nT\u2211 t=1 ft(ut) \u2264 1 T T\u2211 t=1 ft(\u03b8)+G2(G1+||\u03b8||)+ 1 2T G 2 2(1+\nlnT );\n(ii) for sufficiently large T , given any \u03b5 > 0, then\n1 T\nT\u2211 t=1 ft(ut) \u2264 1 T T\u2211 t=1 ft(\u03b8) + \u03b5.\nProof. Since ft is convex and \u2207t is the sub-gradient of ft at ut, we have that\nft(ut)\u2212 ft(\u03b8) \u2264 (ut \u2212 \u03b8) \u22a4\u2207t. (41)\nNote that\n(ut \u2212 \u03b8) \u22a4\u2207t = 1 2\u03b7t (||ut \u2212 \u03b8|| 2 \u2212 ||ut+1 \u2212 \u03b8|| 2) + \u03b7t2 ||\u2207t|| 2.\n(42) Combine (41) and (42), we have\nT\u2211 t=1 (ft(ut)\u2212 ft(\u03b8))\n\u2264 12\nT\u2211\nt=1\n1 \u03b7t (||ut \u2212 \u03b8|| 2 \u2212 ||ut+1 \u2212 \u03b8|| 2) + 12\nT\u2211 t=1 (\u03b7t||\u2207t|| 2)\n= 12 ( \u2211T t=1 ||ut \u2212 \u03b8|| 2 \u2212 T ||uT+1 \u2212 \u03b8|| 2) + 12 \u2211T t=1(\u03b7t||\u2207t|| 2) \u2264 (G1 + ||\u03b8||) T\u2211\nt=1 ||uT+1 \u2212 ut||+\n1 2G 2 2(1 + lnT )\n= (G1 + ||\u03b8||) T\u2211\nt=1 ||\nT\u2211\ni=t\n1 i \u2207i||+ 1 2G 2 2(1 + lnT )\n\u2264 TG2(G1 + ||\u03b8||) + 1 2G 2 2(1 + lnT )\n(43) Multiplying (43) by 1/T leads to the conclusion (i). On the other hand, suppose lim\nT\u2192\u221e uT = u\u0303, we have\nlim T\u2192\u221e ||uT || = ||u\u0303||. Then, lim T\u2192\u221e\n1 T\nT\u2211 t=1 ||ut\u2212\u03b8|| = lim T\u2192\u221e ||uT \u2212\n\u03b8|| = ||u\u0303 \u2212 \u03b8||. Note that lim T\u2192\u221e\nG2 2 (1+lnT ) T = 0. Given any\n\u03b5 > 0, for sufficiently large T ,\n1 T\nT\u2211 t=1 (ft(ut)\u2212 ft(\u03b8))\n\u2264 12 ( 1 T\nT\u2211 t=1 ||ut \u2212 \u03b8|| 2 \u2212 ||uT+1 \u2212 \u03b8|| 2) + 12T G 2 2(1 + lnT )\n\u2264 12\u03b5+ 1 2\u03b5 = \u03b5.\n(44)\nWe are now ready to bound the average instantaneous objective (26).\nTheorem 3.2. For ft (t = 1, . . . , T ) defined as (26) in SGTSVM, ut (t = 1, . . . , T ) is constructed by (29), and u \u2217 is the optimal solution to (25). Then, (i) there are two constants G1 and G2 (actually, they are the upper bounds of ||wt|| and ||\u2207t||, respectively) such that\n1 T\nT\u2211 t=1 ft(ut) \u2264 1 T T\u2211 t=1 ft(u \u2217) + G2(G1 + ||u \u2217||) + 12T G 2 2(1 + lnT ); (ii) for sufficiently large T , given any \u03b5 > 0, then\n1 T\nT\u2211 t=1 ft(ut) \u2264 1 T T\u2211 t=1 ft(u \u2217) + \u03b5.\nProof. Obviously, ft (t = 1, . . . , T ) is convex. Let G1 and G2 respectively be the upper bounds of ||ut|| and ||\u2207t||, the conclusions come from Lemmas 3.1 and 3.2.\nIn the following, let us discuss the relation between the solutions to SGTSVM and TBSVM with the uniform sampling.\nCorollary 3.1. Assume the conditions stated in Theorem 3.1 andm1 = m2, wherem1 andm2 are the sample number of X1 and X2, respectively. Suppose T = km1, where k > 0 is an integer, and each sample is selected k times at random. Then\n(i) f(uT ) \u2264 f(u \u2217)+G2(G1+||u \u2217||+G2)+ 1 2T G 2 1(1+lnT );\n(ii) for sufficiently large T , given any \u03b5 > 0, then f(uT ) \u2264 f(u \u2217) +G22 + \u03b5.\nProof. First, we prove that for all i, j = 1, 2, . . . , T ,\n|ft(ui)\u2212 ft(uj)| \u2264 G2||ui \u2212 uj ||, t = 1, 2, . . . , T. (45)\nFrom the formation of ft(u), we have\n|ft(ui)\u2212 ft(uj)| \u2264 1 2 |||ui|| 2 \u2212 ||uj || 2|\n+ c12 |(u \u22a4 i zt) 2 \u2212 (u\u22a4j zt) 2| +c2|(1 + u \u22a4 i z\u0302t)+ \u2212 (1 + u \u22a4 j z\u0302t)+|.\n(46) Since G1 is the upper bound of ||ut|| (t \u2265 1) and M is the largest norm of the samples in the dataset, the first part, the second part, and the third part on the right hand of (46) are respectively\n1 2 |||ui|| 2 \u2212 ||uj|| 2| \u2264 G1||ui \u2212 uj||, (47)\nc1 2 |(u \u22a4 i zt) 2 \u2212 (u\u22a4j zt) 2|\n= c12 |(ui + uj) \u22a4zt(ui \u2212 uj) \u22a4zt| \u2264 c1G1M 2||ui \u2212 uj ||,\n(48)\nand c2|(1 + u \u22a4 i z\u0302t)+ \u2212 (1 + u \u22a4 j z\u0302t)+|\n= c2|(ui \u2212 uj) \u22a4z\u0302t|\n\u2264 c2M ||ui \u2212 uj||.\n(49)\nTherefore, there is a constant G2 = G1 + c1G1M 2 + c2M satisfying (45). From ut+1 = ut \u2212 1 t \u2207t, it is easy to obtain\nut+1 = u1 \u2212 t\u2211\ni=1\n1 i \u2207t, t = 1, 2, . . . , T. (50)\nThus, for 1 \u2264 i < j \u2264 T ,\n||ui \u2212 uj|| = || j\u22121\u2211\nt=i\n1 t \u2207t|| \u2264\nj\u22121\u2211\nt=i\n1 t G2. (51)\nSince T = km1 = km2, for all u \u2208 R n, 1\nT\nT\u2211\nt=1\nft(u) =\nf(u). Note that f(u) is the objective of TBSVM. Based on (45) and (51), we have\nf(uT )\u2212 1 T\nT\u2211 t=1 ft(ut)\n= 1 T\nT\u2211 t=1 (ft(uT )\u2212 ft(ut))\n\u2264 1 T\nT\u2211 t=1 G2||uT \u2212 ut||\n\u2264 G2 2 (T\u22121) T \u2264 G22.\n(52)\nUsing the Theorem 3.1, we have the conclusion immediately.\nIf m1 6= m2, we can modify the sampling rule to obtain the same result as one in Corollary 3.1.\nCorollary 3.2. Assume the conditions stated in Corollary 3.1, but m1 6= m2. Suppose T = kd(m1,m2), where k > 0 is an integer and d is the least common multiple of m1 and m2. The sample in X1 is selected kd/m1 times at random, and the one in X2 is kd/m2 times at random. Then\n(i) f(uT ) \u2264 f(u \u2217)+G2(G1+||u \u2217||+G2)+ 1 2T G 2 1(1+lnT );\n(ii) for sufficiently large T , given any \u03b5 > 0, then f(uT ) \u2264 f(u \u2217) +G22 + \u03b5.\nNote that for all u \u2208 Rn, 1 T\nT\u2211 t=1 ft(u) = f(u). The proof\nof the above corollary is the same as Corollary 3.1. The above corollaries provide the approximations of u\u2217 by uT . If the sampling rule is not as stated in these corollaries, these upper bounds no longer holds. However, Kakade and Tewari [35] have shown a way to obtain a similar bounds with high probability.\n4 EXPERIMENTS\nIn the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets. CSVM was implemented by Libsvm [19] based on SMO algorithm on small sample size datasets (i.e., m \u2264 10, 000), while Liblinear [20] was implemented for CSVM based on trust region algorithm on large scale datasets (i.e., m > 10, 000). TBSVM was solved by SOR algorithm [8]. All of the methods were implemented on a PC with an Intel Core Duo processor (3.4 GHz) with 4 GB RAM, where LSSVM, SGSVM, WLTSVM, and SGTSVM were implemented by Matlab [37]. For practical convenience, the corresponding SGTSVM Matlab codes uploaded in http://www.optimal-group.org/Resource/SGTSVM.html.\n4.1 Artificial datasets\nWe first test our SGTSVM compared with TBSVM on two artificial datasets [9], [27] in R2 (see Figures ?? and ??). The samples of two classes consist of uniform points are denoted as \u201c+\u201d and \u201c\u00d7\u201d, respectively. Figures ?? and ?? show the learning results, in which the parameters were all fixed by 1, and the iteration in SGTSVM was 1000. It is obvious that both SGTSVM and TBSVM obtain well classification results, and SGTSVM performs a little better than TBSVM on the second dataset. Therefore, in Figures ?? and ??, SGTSVM is effective as TBSVM.\n4.2 Benchmark datasets\nIn order to analyze the convergent rate of our SGTSVM, it was tested on several small sample size datasets [38] (see Table 1) and its parameters c1, c2, c3, and c4 were selected from {2i|i = \u22128, . . . , 7}. For nonlinear case, Gaussian kernel K(x1, x2) = exp{\u2212\u00b5||x1 \u2212 x2||\n2} was used, and its parameter \u00b5 was selected from {2i|i = \u221210, . . . , 5}. For the optimal parameters, we depicted the iteration, accuracy (by ten-fold cross validation [39]), and learning time along with the parameter tol in Figures 3 and 4 for linear and nonlinear cases, respectively. From these figures, it is observed that: (i) SGTSVM terminates very fast for tol \u2265 1e \u2212 4 but slow for tol < 1e \u2212 4; (ii) the converge rates of two problems in SGTSVM is generally different, but almost the same for tol \u2265 1e \u2212 4; (iii) the accuracies are higher for smaller tol, but almost do not increase when tol < 1e \u2212 5; (iv) the learning time is very fast for large tol, but zooms when tol < 1e \u2212 5. Based on the above observations, tol in SGTSVM is suggested between 1e \u2212 5 and 1e \u2212 4. Anyone could set a smaller tol to obtain a higher accuracy together with more learning time.\nThen, we compared SGTSVM with CSVM, LSSVM, SGSVM, TBSVM, andWLTSVM on these datasets. All of the regularization parameters c in CSVM, LSSVM and SGSVM, c1, c2, c3, and c4 in TBSVM, WLTSVM, and SGTSVM were selected from {2i|i = \u22128, . . . , 7}. The Gaussian kernel parameter \u00b5 was selected from {2i|i = \u221210, . . . , 5}. Table 2 shows the ten-fold cross validation accuracy and learning time by these classifiers for linear case, where the bold accuracy is the one that is much worse than others. It can be seen from the table that our SGTSVM has a similar accuracies as CSVM and LSSVM on these datasets, and their differences are no more than two percent. TBSVM obtains similar results as CSVM, LSSVM, and SGTSVM, though it cannot work on two datasets because of out of memory, where these datasets have more than five thousands samples. However, SGSVM and WLTSVM work worse than other classifiers, for SGSVM has a much lower accuracies than the highest classifier on four datasets and WLTSVM on three datasets (these results are bolded). It also can be seen from the table that LSSVM, SGSVM, WLTSVM, and SGTSVM learn a little faster than CSVM and much faster than TBSVM on the datasets that over one thousand samples. Table 3 shows the accuracy and learning time of these classifiers for nonlinear case. The conclusions about accuracy are similar as the linear case. In addition, the learning time of CSVM, LSSVM, TBSVM, and WLTSVM increases much more than the linear case except SGSVM and SGTSVM, indicating the SGD-based methods are stable on saving time.\n4.3 Large scale datasets\nTo test the feasibility of these methods on large scale datasets, we ran them on three large scale datasets: \u201cCODRNA\u201d, \u201cSKIN\u201d, and \u201cSUSY\u201d [20]. Thereinto, CODRNA includes 59, 535 training samples and 271, 618 testing samples with 8 features, SKIN includes 245, 057 samples with 3 features, and SUSY includes 5, 000, 000 samples with 18 features. We split SKIN and SUSY into two sets, where one set including 20% samples is used for training, and the other including 80% samples is used for testing. Due to\nthe nonlinear cases spend too much time to learn on these datasets, we just considered the linear methods. Moreover, since LSSVM, TBSVM, and WLTSVM are out of memory on all of these datasets, the comparisons only include CSVM, SGSVM, and our SGTSVM, where CSVM is implemented by Liblinear [20]. The iteration of SGSVM is set to 10, 000. To speed up the learning rate of SGTSVM, we set tol = 1e \u2212 3 and add another paratactic terminate condition that the iteration is no more than 10, 000. Table 4 shows the ten-fold cross validation and testing accuracies, learning time, and required memory in training on these datasets. It is obvious that SGTSVM owns the highest accuracies on testing, and is as fast as SGSVM. In addition, our SGTSVM is much faster than CSVM with less memory. In detail, CSVM need store the entire training set in RAM, while SGSVM and SGTSVM only store a subset related to the iteration. Due to the required memory of CSVM increases with the size of dataset, it tends to out of memory with the increasing data size, while the same thing does not appear in SGSVM and SGTSVM, which indicates SGTSVM has a better generalization than CSVM.\n5 CONCLUSION\nThe stochastic gradient twin support vector machine (SGTSVM) based on stochastic gradient decent algorithm (SGD) has been proposed. As our knowledge, it is the first time that SGD is introduced for TSVM-type classifiers. In addition, according to our experiments, our method is the fastest one among the TSVM-type classifiers on large scale datasets. By hiring the nonparallel hyperplanes, SGTSVM is more stable on sampling than SGSVM. In theory, SGTSVM is convergent, and is an approximation of TSVM with uniform sampling. Experimental results on several public available datasets have indicated that our SGTSVM has comparable accuracy compared with other TSVM-type classifiers, but with the fastest learning speed. For practical convenience, the corresponding SGTSVM Matlab codes can be downloaded from http://www.optimal-group.org/Resource/SGTSVM.html. For the future work, it is possible to design and study other SGD-based nonparallel hyperplanes SVM model.\nACKNOWLEDGMENTS\nThis work is supported by the National Natural Science Foundation of China (Nos. 11501310, 11201426, and 11371365), the Natural Science Foundation of Inner Mongolia Autonomous Region of China (No. 2015BS0606), and the Zhejiang Provincial Natural Science Foundation of China (No. LY15F030013).\nREFERENCES\n[1] C. Cortes and V. Vapnik, \u201cSupport vector networks,\u201d Machine Learning, vol. 20, pp. 273\u2013297, 1995. [2] O. Mangasarian, Nonlinear Programming. SIAM, 1994. [3] C. Zhang, Y. Tian, and N. Deng, \u201cThe new interpretation of\nsupport vector machines on statistical learning theory,\u201d Science China, vol. 53, no. 1, pp. 151\u2013164, 2010. [4] W. Noble, \u201cSupport vector machine applications in computational biology,\u201d in Kernel Methods in Computational Biology, Cambridge, 2004.\n4\n5\n5\n5\n5\n5\n4\n5\n[5] T. Lal, M. Schro\u0308der, T. Hinterberger, J. Weston, M. Bogdan, N. Birbaumer, and B. Scho\u0308lkopf, \u201cSupport vector channel selection in BCI,\u201dData Mining and Knowledge Discovery, vol. 51, no. 6, pp. 1003\u2013 1010, 2004. [6] H. Ince and T. Trafalis, \u201cSupport vector machine for regression and applications to financial forecasting,\u201d in International Joint Conference on Neural Networks, Italy, 2002, pp. 6348\u20136354. [7] Jayadeva, R. Khemchandani, and S. Chandra, \u201cTwin support vector machines for pattern classification,\u201d IEEE Trans.PatternAnal. Machine Intell, vol. 29, no. 5, pp. 905\u2013910, 2007. [8] Y. Shao, C. Zhang, X. Wang, and N. Deng, \u201cImprovements on twin support vector machines,\u201d IEEE Transactions on Neural Networks, vol. 22, no. 6, pp. 962 \u2013 968, 2011. [9] X. Peng, \u201cTPMSVM: A novel twin parametric-margin support vector machine for pattern recognition,\u201d Pattern Recognition, vol. 44, no. 10-11, pp. 2678\u20132692, 2011. [10] Y. Shao, W. Chen, J. Zhang, Z. Wang, and N. Deng, \u201cAn efficient weighted lagrangian twin support vector machine for imbalanced data classification,\u201d Pattern Recognition, vol. 47, no. 9, pp. 3158\u2013 3167, 2014. [11] Y. Shao and N. Deng, \u201cA coordinate descent margin basedtwin support vector machine for classification,\u201d Neural Networks, vol. 25, pp. 114\u2013121, 2012. [12] Z. Wang, Y. Shao, and T. Wu, \u201cA ga-based model selection for smooth twin parametric-margin support vector machine,\u201d Pattern Recognition, vol. 46, no. 8, pp. 2267\u20132277, 2013. [13] D. Li, Y. Tian, and H. Xu, \u201cDeep twin support vector machine,\u201d in Data MiningWorkshop (ICDMW), 2014 IEEE International Conference on. IEEE, 2014, pp. 65\u201373. [14] Z. Wang, Y. Shao, L. Bai, and N. Deng, \u201cTwin support vector machine for clustering,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 10, pp. 2583\u20132588, 2015. [15] W. Chen, Y. Shao, C. Li, and N. Deng, \u201cMltsvm: A novel twin support vector machine to multi-label learning,\u201d Pattern Recognition, vol. 52, pp. 61\u201374, 2015. [16] M. Bazarra, H. Sherali, and C. Shetty, Nonlinear Programming\u0142Theory and Algorithms, second ed. Wiley, 2004. [17] J. Platt, \u201cFast training of support vector machines using sequential minimal optimization,\u201d in Advances in kernel methods-support vector learning, Cambridge, MA: MIT Press, 1999, pp. 185\u2013208. [18] T. Joachims, \u201cMaking large-scale SVM learning practical,\u201d in Advances in Kernel Methods-Support Vector Learning, Cambridge, 1998, pp. 169\u2013184. [19] C. Chang and C. Lin, LIBSVM: A library for support vector machines, http://www.csie.ntu.edu.tw/\u223ccjlin, 2001. [20] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin, \u201cLIBLINEAR: a library for large linear classification,\u201d Journal of Machine Learning Research, vol. 9, pp. 1871\u20131874, 2008. [21] Y. Tian and Y. Ping, \u201cLarge-scale linear nonparallel support vector machine solver,\u201d Neural Networks, vol. 50, pp. 166\u2013174, 2014. [22] J. Kivinen, A. Smola, and R. Williamson, \u201cOnline learning with kernels,\u201d Signal Processing, IEEE Transactions on, vol. 52, no. 8, pp. 2165\u20132176, 2004.\n[23] T. Zhang, \u201cSolving large scale linear prediction problems using stochastic gradient descent algorithms,\u201d in Proceedings of the twenty-first international conference onMachine learning. ACM, 2004, p. 116. [24] S. Shai, Y. Singer, N. Srebro, and A. Cotter, \u201cPegasos: Primal estimated sub-gradient solver for svm,\u201dMathematical programming, vol. 127, no. 1, pp. 3\u201330, 2011. [25] W. Xu, \u201cTowards optimal one pass large scale learning with averaged stochastic gradient descent,\u201d arXiv preprint arXiv:1107.2490, 2011. [26] A. Bennar and J. Monnez, \u201cAlmost sure convergence of a stochastic approximation process in a convex set,\u201d International Journal of Applied Mathematics, vol. 20, no. 5, pp. 713\u2013722, 2007. [27] O. Mangasarian and E. Wild, \u201cMultisurface proximal support vector classification via generalize eigenvalues,\u201d IEEE Trans.PatternAnal. Machine Intell, vol. 28, no. 1, pp. 69\u201374, 2006. [28] J. Bi and V. Vapnik, Learning with rigorous support vector machines. Springer, 2003. [29] B. Scho\u0308lkopf and A. Smola, Learning with kernels. Cambridge: MA:MIT Press, 2002. [30] R. Khemchandani, Jayadeva, and S. Chandra, \u201cOptimal kernel selection in twin support vector machines,\u201d Optimization Letters, vol. 3, pp. 77\u201388, 2009. [31] Y. Lee and O. Mangasarian, \u201cRSVM: Reduced support vector machines,\u201d in First SIAM International Conference on Data Mining, Chicago, IL, USA, 2001, pp. 5\u20137. [32] Z. Wang, Y. Shao, and T. Wu, \u201cProximal parametric-margin support vector classifier and its applications,\u201d Neural Computing and Applications, vol. 24, no. 3-4, pp. 755\u2013764, 2014. [33] G. Golub and L. Van, Matrix Computations. The John Hopkins University Press, 1996. [34] W. Rudin, Principles of mathematical analysis. McGraw-Hill New York, 1964, vol. 3. [35] S. Kakade and A. Tewari, \u201cOn the generalization ability of online strongly convex programming algorithms,\u201d in Advances in Neural Information Processing Systems, 2009, pp. 801\u2013808. [36] J. Suykens and J. Vandewalle, \u201cLeast squares support vector machine classifiers,\u201d Neural Process Letter, vol. 9, no. 3, pp. 293\u2013 300, 1999. [37] Matlab., User\u2019s Guide, The MathWorks, Inc, http://www.mathworks.com, 1994-2010. [38] C. Blake and C. Merz, UCI Repository for Machine Learning Databases, http://www.ics.uci.edu/\u223cmlearn/MLRepository.html, 1998. [39] R. Duda, P. Hart, and D. Stork, Pattern Classification, 2nd Edition. John Wiley and Sons, 2001."}], "references": [{"title": "Support vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine Learning, vol. 20, pp. 273\u2013297, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "The new interpretation of support vector machines on statistical learning theory", "author": ["C. Zhang", "Y. Tian", "N. Deng"], "venue": "Science China, vol. 53, no. 1, pp. 151\u2013164, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Support vector machine for regression and applications to financial forecasting", "author": ["H. Ince", "T. Trafalis"], "venue": "International Joint Conference on Neural Networks, Italy, 2002, pp. 6348\u20136354.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Twin support vector machines for pattern classification", "author": ["Jayadeva", "R. Khemchandani", "S. Chandra"], "venue": "IEEE Trans.PatternAnal. Machine Intell, vol. 29, no. 5, pp. 905\u2013910, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Improvements on twin support vector machines", "author": ["Y. Shao", "C. Zhang", "X. Wang", "N. Deng"], "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 6, pp. 962 \u2013 968, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "TPMSVM: A novel twin parametric-margin support vector machine for pattern recognition", "author": ["X. Peng"], "venue": "Pattern Recognition, vol. 44, no. 10-11, pp. 2678\u20132692, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient weighted lagrangian twin support vector machine for imbalanced data classification", "author": ["Y. Shao", "W. Chen", "J. Zhang", "Z. Wang", "N. Deng"], "venue": "Pattern Recognition, vol. 47, no. 9, pp. 3158\u2013 3167, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "A coordinate descent margin basedtwin support vector machine for classification", "author": ["Y. Shao", "N. Deng"], "venue": "Neural Networks, vol. 25, pp. 114\u2013121, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "A ga-based model selection for smooth twin parametric-margin support vector machine", "author": ["Z. Wang", "Y. Shao", "T. Wu"], "venue": "Pattern Recognition, vol. 46, no. 8, pp. 2267\u20132277, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep twin support vector machine", "author": ["D. Li", "Y. Tian", "H. Xu"], "venue": "Data MiningWorkshop (ICDMW), 2014 IEEE International Conference on. IEEE, 2014, pp. 65\u201373.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Twin support vector machine for clustering", "author": ["Z. Wang", "Y. Shao", "L. Bai", "N. Deng"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 10, pp. 2583\u20132588, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Mltsvm: A novel twin support vector machine to multi-label learning", "author": ["W. Chen", "Y. Shao", "C. Li", "N. Deng"], "venue": "Pattern Recognition, vol. 52, pp. 61\u201374, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear Programming\u0142Theory and Algorithms, second ed", "author": ["M. Bazarra", "H. Sherali", "C. Shetty"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J. Platt"], "venue": "Advances in kernel methods-support vector learning, Cambridge, MA: MIT Press, 1999, pp. 185\u2013208.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods-Support Vector Learning, Cambridge, 1998, pp. 169\u2013184.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "LIBSVM: A library for support vector machines, http://www.csie.ntu.edu.tw/\u223ccjlin", "author": ["C. Chang", "C. Lin"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "LIBLINEAR: a library for large linear classification", "author": ["R. Fan", "K. Chang", "C. Hsieh", "X. Wang", "C. Lin"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1871\u20131874, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1871}, {"title": "Large-scale linear nonparallel support vector machine solver", "author": ["Y. Tian", "Y. Ping"], "venue": "Neural Networks, vol. 50, pp. 166\u2013174, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A. Smola", "R. Williamson"], "venue": "Signal Processing, IEEE Transactions on, vol. 52, no. 8, pp. 2165\u20132176, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "Proceedings of the twenty-first international conference onMachine learning. ACM, 2004, p. 116.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm,\u201dMathematical", "author": ["S. Shai", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "programming, vol. 127,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["W. Xu"], "venue": "arXiv preprint arXiv:1107.2490, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Almost sure convergence of a stochastic approximation process in a convex set", "author": ["A. Bennar", "J. Monnez"], "venue": "International Journal of Applied Mathematics, vol. 20, no. 5, pp. 713\u2013722, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Multisurface proximal support vector classification via generalize eigenvalues", "author": ["O. Mangasarian", "E. Wild"], "venue": "IEEE Trans.PatternAnal. Machine Intell, vol. 28, no. 1, pp. 69\u201374, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning with rigorous support vector machines", "author": ["J. Bi", "V. Vapnik"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Learning with kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Optimal kernel selection in twin support vector machines", "author": ["R. Khemchandani", "Jayadeva", "S. Chandra"], "venue": "Optimization Letters, vol. 3, pp. 77\u201388, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "RSVM: Reduced support vector machines", "author": ["Y. Lee", "O. Mangasarian"], "venue": "First SIAM International Conference on Data Mining, Chicago, IL, USA, 2001, pp. 5\u20137.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Proximal parametric-margin support vector classifier and its applications", "author": ["Z. Wang", "Y. Shao", "T. Wu"], "venue": "Neural Computing and Applications, vol. 24, no. 3-4, pp. 755\u2013764, 2014.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix Computations", "author": ["G. Golub", "L. Van"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1996}, {"title": "Principles of mathematical analysis", "author": ["W. Rudin"], "venue": "McGraw-Hill New York,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1964}, {"title": "On the generalization ability of online strongly convex programming algorithms", "author": ["S. Kakade", "A. Tewari"], "venue": "Advances in Neural Information Processing Systems, 2009, pp. 801\u2013808.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Least squares support vector machine classifiers", "author": ["J. Suykens", "J. Vandewalle"], "venue": "Neural Process Letter, vol. 9, no. 3, pp. 293\u2013 300, 1999.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1999}, {"title": "Pattern Classification, 2nd Edition", "author": ["R. Duda", "P. Hart", "D. Stork"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION Support vector machine (SVM), being powerful tool for classification [1], [2], [3], has already outperformed most other classifiers in a wide variety of applications [4], [5], [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "1 INTRODUCTION Support vector machine (SVM), being powerful tool for classification [1], [2], [3], has already outperformed most other classifiers in a wide variety of applications [4], [5], [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "1 INTRODUCTION Support vector machine (SVM), being powerful tool for classification [1], [2], [3], has already outperformed most other classifiers in a wide variety of applications [4], [5], [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 3, "context": "Different from SVM with a pair of parallel hyperplanes, twin support vector machine (TSVM) [7] with a pair of nonparallel hyperplanes has been proposed and developed, e.", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": ", twin bounded support vector machine (TBSVM) [8], twin parametric margin support vector machine (TPMSVM) [9], and weighted Lagrangian twin support vector machine (WLTSVM) [10].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": ", twin bounded support vector machine (TBSVM) [8], twin parametric margin support vector machine (TPMSVM) [9], and weighted Lagrangian twin support vector machine (WLTSVM) [10].", "startOffset": 106, "endOffset": 109}, {"referenceID": 6, "context": ", twin bounded support vector machine (TBSVM) [8], twin parametric margin support vector machine (TPMSVM) [9], and weighted Lagrangian twin support vector machine (WLTSVM) [10].", "startOffset": 172, "endOffset": 176}, {"referenceID": 7, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 10, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "These classifiers have been widely applied in many practical problems [11], [12], [13], [14], [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "In the training stage, SVM solves a quadratic programming problem (QPP), whereas TSVM solve two smaller QPPs by traditional solver such as interior method [2], [16], [7].", "startOffset": 160, "endOffset": 164}, {"referenceID": 3, "context": "In the training stage, SVM solves a quadratic programming problem (QPP), whereas TSVM solve two smaller QPPs by traditional solver such as interior method [2], [16], [7].", "startOffset": 166, "endOffset": 169}, {"referenceID": 13, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 16, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 115, "endOffset": 119}, {"referenceID": 4, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 234, "endOffset": 237}, {"referenceID": 8, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 239, "endOffset": 243}, {"referenceID": 17, "context": ", for SVM, sequential minimal optimization, coordinate decent method, and trust region Newton in [17], [18], [19], [20], and for TSVM, successive overrelaxation technique, Newton-Armijo algorithm, and dual coordinate decent method in [8], [12], [21].", "startOffset": 245, "endOffset": 249}, {"referenceID": 18, "context": "The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of", "startOffset": 67, "endOffset": 71}, {"referenceID": 19, "context": "The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "The stochastic gradient descent algorithm for SVM (SGSVM, PEGASOS) [22], [23], [24], [25] attracts a great attention, because it partitions the large scale problem into a series of", "startOffset": 85, "endOffset": 89}, {"referenceID": 22, "context": "It has been proved that SGSVM is almost sure convergent, and thus is able to find an approximation of the desired solution with high probability [26], [23], [24].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "It has been proved that SGSVM is almost sure convergent, and thus is able to find an approximation of the desired solution with high probability [26], [23], [24].", "startOffset": 151, "endOffset": 155}, {"referenceID": 20, "context": "It has been proved that SGSVM is almost sure convergent, and thus is able to find an approximation of the desired solution with high probability [26], [23], [24].", "startOffset": 157, "endOffset": 161}, {"referenceID": 3, "context": "Compared with SVM, it is significant that TSVM is more stable for sampling and does not strongly depend on some special samples such as the SVs [7], [8], which indicates SGD is more suitable for TSVM.", "startOffset": 144, "endOffset": 147}, {"referenceID": 4, "context": "Compared with SVM, it is significant that TSVM is more stable for sampling and does not strongly depend on some special samples such as the SVs [7], [8], which indicates SGD is more suitable for TSVM.", "startOffset": 149, "endOffset": 152}, {"referenceID": 23, "context": ", \u201ccross plane\u201d dataset [27] and preferential classification [7].", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": ", \u201ccross plane\u201d dataset [27] and preferential classification [7].", "startOffset": 61, "endOffset": 64}, {"referenceID": 20, "context": "The main contributions of this paper includes: (i) a SGD-based TSVM (SGTSVM) is first proposed, and it is very easy to be extended to other TSVM-type classifiers; (ii) we prove that the proposed SGTSVM is convergent, instead of almost sure convergence in SGSVM; (iii) for the uniformly sampling, it is proved that the original objective of the solution to SGTSVM is bounded by the optimum of TBSVM, which indicates the solution to SGTSVM is an approximation of the optimal solution to TBSVM, while SGSVM only has an opportunity to obtain an approximation of the optimal solution to SVM (see Corollaries 1 and 2 in [24]); (iv) the nonlinear case of SGTSVM is obtained directly based on its original problem, whereas the nonlinear case of SGSVM is derived from SVM\u2019s dual problem; (v) each iteration of SGTSVM includes no more than 8n + 4 multiplications without additional storage, so it is the fastest one than other proposed TSVM-type classifiers.", "startOffset": 614, "endOffset": 618}, {"referenceID": 24, "context": "C-support vector machine (CSVM) [28], one formulation of the standard SVM, searches for a separating hyperplane", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "where || \u00b7 || denotes the L2 norm, c > 0 is a parameter with some quantitative meanings [28], e is a vector of ones with an appropriate dimension, \u03be \u2208 R is the slack vector, and D = diag(y1, .", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "And the structural risk minimization principle is implemented in this problem [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 19, "context": "2 SGSVM SGSVM (or PEGASOS as an alias) [23], [24] considers a strongly convex problem by modifying (2) as follow", "startOffset": 39, "endOffset": 43}, {"referenceID": 20, "context": "2 SGSVM SGSVM (or PEGASOS as an alias) [23], [24] considers a strongly convex problem by modifying (2) as follow", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "t=1 wt is bounded by the optimal solution w \u2217 to (4) with o(1), and thus SGSVM has with a probability of at least 1/2 to find a good approximation of w [24].", "startOffset": 152, "endOffset": 156}, {"referenceID": 20, "context": "The authors of [24] also pointed out that wT is often used instead of w\u0304 in practice.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "The sample xt which is selected randomly can be replaced with a small subset belonging to the whole dataset, and the subset only including a sample is often used in practice [23], [24], [25].", "startOffset": 174, "endOffset": 178}, {"referenceID": 20, "context": "The sample xt which is selected randomly can be replaced with a small subset belonging to the whole dataset, and the subset only including a sample is often used in practice [23], [24], [25].", "startOffset": 180, "endOffset": 184}, {"referenceID": 21, "context": "The sample xt which is selected randomly can be replaced with a small subset belonging to the whole dataset, and the subset only including a sample is often used in practice [23], [24], [25].", "startOffset": 186, "endOffset": 190}, {"referenceID": 20, "context": "(8) However, this modification would lead to the function not to be strongly convex and thus yield a slow convergence rate [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 4, "context": "3 TBSVM TBSVM [8], a representative of TSVM, seeks a pair of nonparallel hyperplanes in R which can be expressed as w 1 x+ b1 = 0 and w \u22a4 2 x+ b2 = 0, (9)", "startOffset": 14, "endOffset": 17}, {"referenceID": 23, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 86, "endOffset": 90}, {"referenceID": 3, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 92, "endOffset": 95}, {"referenceID": 4, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 97, "endOffset": 100}, {"referenceID": 25, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 102, "endOffset": 106}, {"referenceID": 26, "context": "2 Nonlinear Formation Now, we extend our SGTSVM to nonlinear case by the kernel trick [27], [7], [8], [29], [30].", "startOffset": 108, "endOffset": 112}, {"referenceID": 27, "context": "However, the reduced kernel strategy, which has been successfully applied for SVM and TSVM [31], [32], [12], can also be applied for our SGTSVM.", "startOffset": 91, "endOffset": 95}, {"referenceID": 28, "context": "However, the reduced kernel strategy, which has been successfully applied for SVM and TSVM [31], [32], [12], can also be applied for our SGTSVM.", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "However, the reduced kernel strategy, which has been successfully applied for SVM and TSVM [31], [32], [12], can also be applied for our SGTSVM.", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "In practice, X\u0303 just needs 1% \u2212 10% samples from X to get a well performance, reducing the learning time without loss of generalization [32].", "startOffset": 136, "endOffset": 140}, {"referenceID": 29, "context": "(31) For i \u2265 N + 1, ||At+N+1\u2212iuN+1|| \u2264 \u03bbi||uN+1|| \u2264 i\u22121 i ||uN+1|| [33].", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "Note that an infinite series of vectors is convergent if its norm series is convergent [34].", "startOffset": 87, "endOffset": 91}, {"referenceID": 31, "context": "However, Kakade and Tewari [35] have shown a way to obtain a similar bounds with high probability.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 67, "endOffset": 70}, {"referenceID": 32, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 90, "endOffset": 94}, {"referenceID": 4, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "4 EXPERIMENTS In the experiments, we compared our SGTSVM with CSVM [1], LSSVM [36], SGSVM [24], TBSVM [8], and WLTSVM [10] on several artifical and benchmark datasets.", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "CSVM was implemented by Libsvm [19] based on SMO algorithm on small sample size datasets (i.", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": ", m \u2264 10, 000), while Liblinear [20] was implemented for CSVM based on trust region algorithm on large scale datasets (i.", "startOffset": 32, "endOffset": 36}, {"referenceID": 4, "context": "TBSVM was solved by SOR algorithm [8].", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "1 Artificial datasets We first test our SGTSVM compared with TBSVM on two artificial datasets [9], [27] in R (see Figures ?? and ??).", "startOffset": 94, "endOffset": 97}, {"referenceID": 23, "context": "1 Artificial datasets We first test our SGTSVM compared with TBSVM on two artificial datasets [9], [27] in R (see Figures ?? and ??).", "startOffset": 99, "endOffset": 103}, {"referenceID": 33, "context": "For the optimal parameters, we depicted the iteration, accuracy (by ten-fold cross validation [39]), and learning time along with the parameter tol in Figures 3 and 4 for linear and nonlinear cases, respectively.", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "3 Large scale datasets To test the feasibility of these methods on large scale datasets, we ran them on three large scale datasets: \u201cCODRNA\u201d, \u201cSKIN\u201d, and \u201cSUSY\u201d [20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "Moreover, since LSSVM, TBSVM, and WLTSVM are out of memory on all of these datasets, the comparisons only include CSVM, SGSVM, and our SGTSVM, where CSVM is implemented by Liblinear [20].", "startOffset": 182, "endOffset": 186}], "year": 2017, "abstractText": "For classification problems, twin support vector machine (TSVM) with nonparallel hyperplanes has been shown to be more powerful than support vector machine (SVM). However, it is time consuming and insufficient memory to deal with large scale problems due to calculating the inverse of matrices. In this paper, we propose an efficient stochastic gradient twin support vector machine (SGTSVM) based on stochastic gradient descent algorithm (SGD). As far as now, it is the first time that SGD is applied to TSVM though there have been some variants where SGD was applied to SVM (SGSVM). Compared with SGSVM, our SGTSVM is more stable, and its convergence is also proved. In addition, its simple nonlinear version is also presented. Experimental results on several benchmark and large scale datasets have shown that the performance of our SGTSVM is comparable to the current classifiers with a very fast learning speed.", "creator": "LaTeX with hyperref package"}}}