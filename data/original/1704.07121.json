{"id": "1704.07121", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets", "abstract": "Visual question answering (QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or the both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular visual QA datasets as well as to create a new visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via", "histories": [["v1", "Mon, 24 Apr 2017 10:05:19 GMT  (774kb,D)", "http://arxiv.org/abs/1704.07121v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.LG", "authors": ["wei-lun chao", "hexiang hu", "fei sha"], "accepted": false, "id": "1704.07121"}, "pdf": {"name": "1704.07121.pdf", "metadata": {"source": "CRF", "title": "Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets", "authors": ["Wei-Lun Chao", "Hexiang Hu", "Fei Sha"], "emails": ["weilunc@usc.edu,", "hexiang.frank.hu@gmail.com,", "feisha@usc.edu"], "sections": [{"heading": "1 Introduction", "text": "Recently, multimodal information processing tasks such as image captioning [27] and visual question answering (visual QA) [3] have gained a lot of attention. A number of significant advances in learning algorithms have been made, along with the development of nearly two dozens of datasets in this very active research domain. Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others. The overarching objective is that a learning machine needs to go beyond understanding different modalities of information separately (such as image recognition alone) and to learn how to correlate them in order to perform well on those tasks.\nTo evaluate the progress on those complex and more AI-like tasks is however a challenging topic. For tasks involving language generation, developing an automatic evaluation metric is itself an open problem [2,\n\u2217 Equal contributions\nar X\niv :1\n70 4.\n07 12\n1v 1\n[ cs\n.C L\n] 2\n4 A\n15, 20, 14]. Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.\nIn this paper, we study how to design high-quality multiple choices for the visual QA task. In this task, the machine (or the human annotator) is presented with an image, a question and a list of candidate answers. The goal is to select the correct answer through a consistent understanding of the image, the question and each of the candidate answers. As in any multiple-choice based tests (such as GRE), designing what should be presented as negative answers \u2014 we refer them as decoys \u2014 is as important as deciding the questions to ask. We all have had the experience of exploiting the elimination strategy: This question is easy \u2014 none of the three answers could be right so the remaining one must be correct!\nWhile a clever strategy for taking exams, such \u201cshortcuts\u201d prevent us from studying faithfully how different learning algorithms comprehend the meanings in images and languages (e.g., the quality of the embeddings of both images and languages in a semantic space). It has been noted that machines can achieve very high accuracies of selecting the correct answer without the visual input (i.e., the image), the question, or both [12, 3]. Clearly, the learning algorithms have overfit on incidental statistics in the datasets. For instance, if the decoy answers have rarely been used as the correct answers (to any questions), then the machine can rule out a decoy answer with a binary classifier that determines whether the answers are in the set of the correct answers \u2014 note that this classifier does not need to examine the image and it just needs to memorizes the list of the correct answers in the training dataset. See Fig. 1 for an example, and Sec. 3 for more and detailed analysis.\nWe focus on minimizing the impacts of exploiting such shortcuts. We suggest a set of principles for creating decoy answers. In light of the amount of human efforts in curating existing datasets for the visual QA task, we propose two procedures that revise those datasets such that the decoy answers are better designed. In contrast to some earlier works, the procedures are fully automatic and do not incur additional human an-\nnotator efforts. We apply the procedures to revise both Visual7W [30] and VQA [3]. Additionally, we create a multiple-choice based dataset from the recently released Visual Genome dataset [16], resulting in the largest multiple-choice dataset for the visual QA task, with more than one million image-question-candidate answers triplets.\nWe conduct extensive empirical and human studies to demonstrate the effectiveness of our procedures in creating high-quality datasets for the visual QA task. In particular, we show that machines need to use all three information (image, questions and answers) to perform well \u2014 any missing information induces a large drop in performance. Furthermore, we show that humans dominate machines in the task. However, given the revised datasets are likely reflecting the true gap between the human and the machine understanding of multimodal information, we expect that advances in learning algorithms likely focus more on the task itself instead of overfitting to the idiosyncrasies in the datasets.\nThe rest of the paper is organized as follows. In Sect. 2, we describe related work. In Sect. 3, we analyze and discuss the design deficiencies in existing datasets. In Sect. 4, we describe our automatic procedures for remedying those deficiencies. In Sect. 5 we conduct experiments and analysis. We conclude the paper in Sect. 6."}, {"heading": "2 Related Work", "text": "[14, 25] provide recent overviews of the status quo of the visual QA task. There are about two dozens of datasets for the task. Most of them use real-world images, while some are based on synthetic ones. Usually, for each image, multiple questions and their corresponding answers are generated. This can be achieved either by human annotators, or with an automatic procedure that uses captions or question templates and detailed annotations such as objects. We concentrate on 3 datasets: VQA [3], Visual7W [30], and Visual Genome [16]. All of them use images from MSCOCO [18].\nBesides the pairs of questions and correct answers, VQA, Visual7W, and visual Madlibs [28] provide decoy answers for each pair so that the task can be evaluated in multiple-choice selection accuracy. What decoy answers to use is the focus of our work.\nIn VQA, the decoys consist of human-generated plausible answers as well as high-frequency and random answers from the datasets. In Visual7W, the decoys are all human-generated plausible ones. Note that, humans generate those decoys by only looking at the questions and the correct answers but not the images. Thus, the decoys might be unrelated to the corresponding images. A learning algorithm can potentially examine the image alone and be able to identify the correct answer.\nIn visual Madlibs, the questions are generated with a limited set of question templates (\u201cfill-in-theblank\u201d) and the detailed annotations (eg, objects) of the images. Thus, similarly, a learning model can examine the image alone and deduce the correct answer.\nWe propose automatic procedures to revise VQA and Visual7W (and to create one based on Visual Genome) such that the decoy generation is carefully orchestrated to prevent learning algorithms from exploiting the shortcuts in the datasets by overfitting on incident statistics. In particular, our design goal is that a learning machine needs to understand all the 3 components of an image-question-answers triplet in order to make the right choice \u2014 ignoring either one or two components will result in drastic degradation in performance.\nOur work is inspired by the experiments in [12] where they observe that machines without looking at images or questions can still perform well on the visual QA task. Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting. Our work extends theirs by providing more detailed analysis as well as automatic procedures to remedy those design deficiencies.\nBesides the visual QA task, [7] and VisDial [6] also propose automatic ways to generate decoys for the tasks of selecting the best visual caption and dialog, respectively."}, {"heading": "3 Analysis of Decoy Answers\u2019 Effects", "text": "In this section, we examine in detail the dataset Visual7W [30], a popular choice for the visual QA task. We demonstrate how the deficiencies in designing decoy questions impact the performance of learning algorithms.\nIn multiple-choice visual QA datasets, a training or test example is a triplet that consists of an image I, a question Q, and a candidate answer set A. The set A contains a target T (the correct answer) and K decoys (incorrect answers) denoted by D. An IQA triplet is thus {I,Q,A = {T,D1, \u00b7 \u00b7 \u00b7 ,DK}}. We use C to denote either the target or a decoy."}, {"heading": "3.1 Visual QA models", "text": "We investigate how well a learning algorithm can perform when supplied with different modalities of information. We concentrate on the one hidden-layer MLP model proposed in [12], which had achieved state-of-the-art results on the dataset Visual7W. The model computes a scoring function f(c, i)\nf(c, i) = \u03c3(U max(0,W g(c, i)) + b) (1)\nover a candidate answer c and the multimodal information i, where g is the joint feature of (c, i) and \u03c3(x) = 1/(1 + exp(\u2212x)). The information i can be null, the image (I) alone, the question (Q) alone, or the combination of both (I+Q).\nGiven an IQA triplet, we use the penultimate layer of ResNet-200 [9] as visual features to represent I and the average WORD2VEC embeddings [22] as text features to represent Q and C. To form the joint feature g(c, i), we just concatenate the features together. The candidate c \u2208 A that has the highest f(c, i) score in prediction is selected as the model output.\nWe use the standard training, validation and test splits of Visual7W, where each contains 69,817, 28,020, and 42,031 examples respectively. Each question has 4 candidate answers. The parameters of f(c, i) are learned by minimizing the binary logistic loss of predicting whether or not a candidate c is the target of an IQA triplet. Details are in Sect. 5 and the Supplementary Material."}, {"heading": "3.2 Analysis results", "text": "Machines find shortcuts Table 1 summarizes the performance of the learning models, together with the human studies we performed on a subset of 1,000 triplets (c.f. Sect. 5 for details). There are a few interesting observations.\nFirst, in the row of \u201cA\u201d where only the candidate answers (and whether they are right or wrong) are used to train a learning model, the model performs significantly better than random guessing and humans (52.9% vs. 25%) \u2014 humans will deem each of the answers equally likely without looking at both the image and the question! Note that in this case, the information i in eq. (1) contains nothing. Thus, the model learns the specific statistics of the candidate answers in the dataset and exploits those.\nAdding the information about the image (i.e., the row of \u201cI+A\u201d), the machine improves significantly and gets close to the performance when all information is used (62.4% vs. 65.7%). There is a weaker correlation between the question and the answers as \u201cQ+A\u201d improves over \u201cA\u201d only modestly. This is expected. In the Visual7W dataset, the decoys are generated by human annotators as plausible answers to the questions without being shown the images \u2014 thus, many decoy answers do not have visual groundings. For instance, a question of \u201cwhat animal is running?\u201d elicits equally likely answers such as \u201cdog\u201d, \u201ctiger\u201d, \u201clion\u201d, or \u201ccat\u201d, while an image of a dog running in the park will immediately rule out all 3 but the \u201cdog\u201d, see Fig. 1 for similar examples. Thus, the performance of \u201cI+A\u201d implies that many IQA triplets can be solved by object, attribute or concept detection on the image, without understanding the questions. This is indeed the case also for humans \u2014 humans can achieve 75.3% by considering \u201cI+A\u201d and not \u201cQ\u201d. Note that the difference between machine and human on \u201cI+A\u201d are likely due to the difference between the two in understanding visual information.\nNote that human improves significantly from \u201cI+A\u201d to \u201cI+Q+A\u201d with \u201cQ\u201d added, while the machine does so only marginally. The difference can be attributed to the difference in understanding the question and correlating with the answers between the two. Since each image corresponds to multiple questions or have multiple objects, solely relying on the image itself will not work well in principle. Such difference clearly indicates that in the visual QA model, the language component is weak as the model cannot fully exploit the information in \u201cQ\u201d, making a smaller relative improvement 3.3% (from 62.4% to 65.7%) where humans improved relatively 17.4%.\nShortcuts are due to design deficiencies We probe deeper on how the decoy answers have impacted the performance of learning models.\nAs explained above, the decoy answers are drawn from all plausible answers to a question, irrespective whether they are visually grounded or not. We have also discovered that the targets (i.e., correct answers) are infrequently used as decoys.\nSpecifically, among the 69,817 training samples, there are 19,503 unique correct answers and each one of them is used about 3.6 times as correct answers to a question. However, among all the 69, 817 \u00d7 3 \u2248 210K decoys, each correct answer appears 7.2 times on average, far below a chance level of 10.7 times (210K \u00f7 19, 503 \u2248 10.7). This disparity exists in the test samples too. Consequently, the following rule, computing each answer\u2019s likelihood of being correct,\nP (correct|C) =\n{ 0.5, if C is never seen in training,\n# times C as target # times C as target+(# times C as decoys)/K , otherwise,\n(2)\nshould perform well. Essentially, it measures how unbiased C is used as the target and the decoys. Indeed, it attains an accuracy of 48.73% on the test data, far better than the random ingguess and is close to the learning model using the answers information only (the \u201cA\u201d row in Table 1).\nGood rules for designing decoys Based on our analysis, we summarize the following guidance rules to design decoys: (1) Question only Unresolvable (QoU). The decoys need to be equally plausible to the\nquestion. Otherwise, machines can rely on the correlation between the question and candidate answers to tell the target from decoys, even without the images. Note that this is a principle that is being followed by most datasets. (2) Neutrality. The decoys answers should be equally likely used as the correct answers. (3) Image only Unresolvable (IoU). The decoys need to be plausible to the image. That is, they should appear in the image, or there exist questions so that the decoys can be treated as targets to the image. Otherwise, visual QA can be resolved by objects, attributes, or concepts detection in images, even without the questions.\nIdeally, each decoy in an IQA triplet should meet the three principles. Neutrality is comparably easier to achieve by reusing terms in the whole set of targets as decoys. On the contrary, a decoy may hardly meet QoU and IoU simultaneously1. However, as long as all decoys of an IQA triplet meet Neutrality and some meet QoU and others meet IoU, the triplet as a whole still achieves the three principles \u2014 a machine ignoring either the images or the questions will likely perform poorly."}, {"heading": "4 Create Better Visual QA Datasets", "text": "In this section, we describe our approaches of remedying design deficiencies in the existing datasets for the visual QA task. We introduce two automatic procedures to create new decoy answers that can prevent learning models from exploiting incident statistics in the datasets."}, {"heading": "4.1 Methods", "text": "Main Ideas Our procedures operate on a dataset that already contains image-question-target (IQT) triplets, i.e., we do not assume it has decoys already. For instance, we have used our procedures to create a multiplechoice dataset from the Visual Genome dataset which has no decoy. We assume that each image in the dataset is coupled with \u201cmultiple\u201d QT pairs, which is the case in nearly all the existing datasets. Given an IQT triplet (I, Q, T), we create two sets of decoy answers,\n\u2022 QoU-decoys. We search among all other triplets that have similar questions to Q. The targets of those triplets are then collected as the decoys for T. As the targets to similar questions are likely plausible for the question Q, QoU-decoys likely follow the rules of Neutrality and Question only Unresolvable (QoU). We compute the average WORD2VEC [22] to represent a question, and use the cos similarity to measure the similarity between questions.\n\u2022 IoU-decoys. We collect the targets from other triplets of the same image to be the decoys for T. The resulting decoys thus definitely follow the rules of Neutrality and Image only Unresolvable (IoU).\nWe then combine the triplet (I, Q, T) with QoU-decoys and IoU-decoys to form an IQA triplet as a training or test sample.\nResolving ambiguous decoys One potential drawback of automatically selected decoys is that they may be semantically similar, ambiguous, or rephrased terms to the target [30]. We utilize two filtering steps to alleviate it. First, we perform string matching between a decoy and the target, deleting those decoys that contain or are covered by the target (e.g., \u201cdaytime\u201d vs \u201cduring the daytime\u201d and \u201cponytail\u201d vs \u201cpony tail\u201d).\nSecondly, we utilize the WordNet hierarchy and the Wu-Palmer (WUP) score [26] to eliminate semantically similar decoys. The WUP score measures how similar two word senses are (in the range of [0, 1]),\n1E.g., in Fig 1, for the question \u201cWhat vehicle is pictured?\u201d, the only answer that meets both principles is \u201ctrain\u201d, which is the correct answer instead of being a decoy.\nbased on the depth of their two word senses in the taxonomy and that of their least common subsumer. We compute the similarity of two strings according to the WUP scores in a similar manner to [21], in which the WUP score is used for the evaluation of visual QA performance. We eliminate decoys that have higher WUP-based similarity to the target. We use NLTK toolkit [4] to compute the similarity. See the Supplementary Material for more details.\nOther details For QoU-decoys, we sort and keep for each triplet the top N (eg, 10,000) similar triplets from the entire dataset according to the question similarity. Then for each triplet, we compute the WUPbased similarity of each potential decoy to the target successively, and accept those with similarity below 0.9 until we have K decoys. We also perform such a check among selected decoys to ensure they are not very similar to each other. For IoU-decoys, the potential decoys are sorted randomly. The WUP-based similarity with a threshold of 0.9 is then applied to remove ambiguous decoys."}, {"heading": "4.2 Comparison to other datasets", "text": "Several authors have noticed the design deficiencies in the existing databases and have proposed \u201cfixes\u201d [3, 28, 30, 6]. No dataset has used a procedure to generate IoU-decoys. We empirically show that how the IoU-decoys significantly remedy the design deficiencies for the decoys in the datasets.\nSeveral previous efforts have generated decoys that are similar in spirit to our QoU-decoys. [28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively. The later two are for different tasks from visual QA, and only [7] considers removing semantically ambiguous decoys like ours. [3, 30] ask humans to create decoys, given the questions and targets. As shown previously, such decoys may fail the rule of Neutrality."}, {"heading": "5 Empirical Studies", "text": ""}, {"heading": "5.1 Dataset", "text": "We examine our automatic procedures of creating decoys on the following three datasets. Table 2 summarizes their characteristics.\nVQA Real [3] The dataset uses images from MSCOCO [18] under the same training/validation/testing splits to construct IQA triplets. Totally 614,163 IQA triplets are generated for 204,721 images. Each question has 18 candidate answers: in general 3 decoys are human-generated, 4 are randomly sampled, and 10 are randomly sampled frequent-occurring targets. As the test set does not indicate the targets, our studies focus on the training and validation sets.\nVisual7W Telling (Visual7W) [30] The dataset uses 47,300 images from MSCOCO [18] and contains 139,868 IQA triplets. Each has 3 decoys generated by humans.\nVisual Genome (VG) [16] The dataset uses 101,174 images from MSCOCO [18] and contains 1,445,322 IQT triplets. No decoys are provided. Human annotators are asked to write diverse pairs of questions and answers freely about an image or with respect to some regions of it. On average an image is coupled with 14 question-answer pairs. We divide the dataset into non-overlapping 50%/20%/30% for training/validation\n/testing. Additionally, we partition such that each portion is a \u201csuperset\u201d of the corresponding one in Visual7W, respectively.\nCreating decoys We create 3 QoU-decoys and 3 IoU-decoys for every IQT triplet in each dataset, following the steps in Sect. 4.1. In the cases we cannot find 3 decoys, we include random ones from the original set of decoys for VQA and Visual7W; for VG, we randomly include those from the top 10 frequently-occurring targets."}, {"heading": "5.2 Setup", "text": "Visual QA models We utilize the MLP models mentioned in Sect. 3 for all the experiments. We denote MLP-A, MLP-QA, MLP-IA, MLP-IQA as the models using A (Answers only), Q+A (Question plus Answers), I+A (Image plus Answers), and I+Q+A (Image, Question and Answers) for multimodal information, respectively. The hidden-layer has 8,192 neurons. We use a 200-layer ResNet [9] to compute visual features which are 2,048-dimensional. The ResNet is pre-trained on ImageNet [24]. The WORD2VEC feature [22] for questions and answers are 300-dimensional, pre-trained on Google News. The parameters of the MLP models are learned by minimizing the binary logistic loss of predicting whether or not a candidate answer is the target of the corresponding IQA triplet. We use stochastic gradient descent with mini-batch size of 100, momentum of 0.9, and the stepped learning rate policy in optimization. We tune the number of iterations and the step size using the validation set. Details are in the Supplementary Material.\nEvaluation Metric For Visual7W and VG, we compute the accuracy of picking the target from multiple choices. For VQA, we follow its protocol by comparing the picked answer to 10 human-generated targets. The accuracy is computed based on the number of exactly matched targets (divided by 3 and clipped at 1).\nDecoy sets to compare For each dataset, we can thus derive several variants: (1) Orig: the original sets of decoys from the datasets, (2) QoU: Orig replaced with ones selected by our QoU-decoys generating procedure, (3) IoU: Orig replaced with ones selected by our IoU-decoys generating procedure (4) QoU +IoU: Orig replaced with ones combining QoU and IoU (5) All: combining Orig, QoU, and IoU.\nUser studies Automatic decoy generation may lead to ambiguous decoys as mentioned in Sect. 4 and [30]. We thus conduct a user study via Amazon Mechanic Turk (AMT) to test humans\u2019 performance on the datasets after they are remedied by our automatic procedures. We select for each dataset 1,000 IQA triplets. Each triplet is answered by three workers and in total 169 workers get involved. We report the average human performance and compare it to the learning models\u2019. See the Supplementary Material for details."}, {"heading": "5.3 Results", "text": "The performances of learning models and humans on the 3 datasets are reported in Table 3, 4, and 5.\nEffectiveness of new decoys A better set of decoys will force learning models to integrate all 3 pieces of information \u2014 images, questions and answers \u2014 to make the correct selection from multiple-choices. In particular, they should prevent learning algorithms from exploiting shortcuts such that partial information is sufficient for performing well on the visual QA task.\nTable 3 clearly indicates that those goals have been achieved. With the Orig decoys, the relatively small gain from MLP-IA to MLP-IQA suggests that the question information can be ignored to attain good performance. However, with the IoU-decoys which require questions to help to resolve (as image itself is inadequate to resolve), the gain is substantial (from 27.3% to 84.1%). Likewise, with the QoU-decoys (question itself is not adequate to resolve), including images information improves from MLP-QA (40.7%) substantially to MLP-IQA\u2019s 57.6%. Note that with the Orig decoys, this gain is smaller (58.2% vs 65.7%).\nIt is expected that MLP-IA matches better QoU-decoys but not IoU-decoys, and MLP-QA is other way around. Thus it is natural to combine these two decoys. What is particularly appealing is that MLP-IQA improves noticeably over models learned with partial information, on the combined IoU +QoU-decoys (and \u201cAll\u201d decoys). Furthermore, using answer information only (MLP-A) attains about chance-level accuracy.\nOn the VQA dataset (Table 4), the same observations hold, though to a lesser degree. On any of the IoU or QoU columns, we observe substantial gains when the complementary information is added to the model (such as MLP-IA to MLP-IQA). All these improvements are much more visible than those observed on the original decoy sets.\nCombining both Table 3 and 4, we notice that the improvements from MLP-QA to MLP-IQA tend to be lower when facing IoU-decoys. This is also expected as it is difficult to have decoys that are simultaneously both IoU and QoU \u2014 such answers tend to be the target answers. Nonetheless, we deem this as a future direction to explore.\nDifferences across datasets Contrasting Visual7W to VQA (on the column IoU +QoU), we notice that Visual7W tends to have bigger improvements in general. This is due to the fact that VQA has many questions with \u201cYes\u201d or \u201cNo\u201d as the targets \u2014 the only valid decoy to the target Yes is No, and vice versa. As such decoys are already captured by Orig of VQA (Yes and No are both top frequency targets), adding other decoy answers will not make any noticeable improvement. In Supplementary Material, however, we show that once we remove such questions/answers pairs, the degree of improvements increases substantially.\nFor completeness, we include the results on the Visual Genome dataset in Table 5. This dataset has no \u201cOrig\u201d decoys, and we have created a multiple-choice based dataset qaVG from it for the task \u2014 it has over\n1 million triplets, the largest dataset on this task to our knowledge. With qaVG, we also investigate whether it is possible to use it to improve the performances on the other two datasets \u2014 note that the images in both VQA and Visual7W are derived from MSCOCO. So there is no mismatch in distribution between images (and their features).\nWe use the MLP-IQA trained on qaVG with both IoU and QoU decoys. This model initializes the models for the Visual7W and VQA datasets. We report the accuracies before and after fine-tuning, together with the best results learned solely from those two datasets respectively. As shown in Table 6, fine-tuning improves the performance on those datasets. In particular, the result on the original Visual7W (the row with \u201cOrig\u201d) attains the state-of-the-art \u2014 previously the best performance on this dataset was reported as 68.5% by [12] where a model pre-trained on VQA is fine-tuned on Visual7W."}, {"heading": "5.4 Qualitative Results", "text": "We present in Fig. 2 examples of image-question-target triplets from V7W, VQA, and VG, together with our IoU-decoys (A, B, C) and QoU-decoys (D, E, F). G is the target. The predictions by the corresponding MLP-IQA are also included. Ignoring information from images or questions makes it extremely challenging to answer the triplet correctly, even for humans.\nOur automatic procedures do fail at some triplets, resulting in ambiguous decoys to the targets. See Fig. 3 for examples. We categorized those failure cases into two situations.\n\u2022 Our filtering steps in Sect. 4 fail, as observed in the top example. The WUP-based similarity relies on the WordNet hierarchy. For some semantically similar words like \u201clady\u201d and \u201cwoman\u201d, the similarity is only 0.632, much lower than that of 0.857 between \u201ccat\u201d and \u201cdog\u201d. This issue can be alleviated by considering alternative semantic measures by WORD2VEC or by those used in [6, 7] for searching similar questions."}, {"heading": "6 Conclusion", "text": "We perform detailed analysis on existing datasets for multiple-choice visual QA. We found that the design of decoys can inadvertently provide \u201cshortcuts\u201d for machines to exploit to perform well on the task. We describe several principles of constructing good decoys and propose automatic procedures to remedy existing datasets. We also created a new dataset by applying our procedures to the Visual Genome, resulting in the largest multiple-choice dataset for the task with over 1 million image-question-candidate answers triplets. We conduct extensive empirical studies to demonstrate the effectiveness of our methods in creating better visual QA datasets. The remedied datasets and the Visual Genome based dataset are released and available at http://www.teds.usc.edu/website_vqa/."}, {"heading": "A Details on the MLP models", "text": "The one hidden-layer MLP model used in our experiments has 8,192 hidden units, exactly following [12]. It contains a batch normalization layer before ReLU, and a dropout layer after ReLU. We set the dropout rate to be 0.5.\nThe input to the model is the concatenated features of images, questions, and answers. We change all characters to lowercases and all integer numbers within [0, 10] to words before computing WORD2VEC. We perform `2 normalization to features of each information before concatenation.\nWe train the model using stochastic gradient descent with mini-batch size of 100, momentum of 0.9, and the stepped learning rate policy: the learning rate is divided by 10 after every M mini-batches. We set the initial learning rate to be 0.01 (we further consider 0.001 for the case of fine-tuning in Sect. 6.2 of the main text). For each model, we train with at most 600,000 iterations. We treat M and the number of iterations as hyper-parameters of training. We tune the hyper-parameters on the validation set.\nWithin each mini-batch, we sample 100 IQA triplets. For each triplet, we randomly choose to use QoUdecoys or IoU-decoys when training on IoU +QoU, or QoU-decoys or IoU-decoys or Orig when training on All. We then take the target and 3 decoys for each triplet to train the binary classifier (i.e., minimize the logistic loss). Specifically on VQA, which has 17 Orig decoys for a triplet, we randomly choose 3 decoys out of them. That is, 100 triplets in the mini-batch corresponds to 400 examples with binary labels. This procedure is to prevent unbalanced training, where machines simply learn to predict the dominant label, as suggested in [12].\nWe note that in all the experiments in the main text, we use the same type of decoy sets for training and testing."}, {"heading": "B WUP-based similarity for filtering out ambiguous decoys", "text": "We use the Wu-Palmer (WUP) score [26], which characterizes the word sense similarity, to filter out ambiguous decoys to the target (correct answer). The WUP score is computed based on the WordNet hierarchy. Essentially, it measures the similarity of two nodes (i.e., synsets) in the hierarchy. As a word might correspond to multiple nodes, we measure the word similarity as follows:\nWUP (w1, w2) = max (n1,n2)\u2208N1\u00d7N2 WUP (n1, n2), (3)\nwhere N1 and N2 are the sets of nodes that words w1 and w2 correspond to, respectively. That is, the word similarity is based on the most similar pair of nodes from both words. We consider only the NOUN and ADJ nodes for tractable computation.\nSince a candidate answer may contain more than one word (i.e., a word sequence), we compute the similarity between two word sequences WS1 and WS2 as follows\nWUP (WS1,WS2) = max{ \u220f\nw1\u2208WS1\nmax w2\u2208WS2\nWUP (w1, w2), \u220f\nw2\u2208WS2\nmax w1\u2208WS1\nWUP (w1, w2)}. (4)\nThis formulations is highly similar to the one proposed in [21] for evaluating open-ended visual QA. The main difference is that we use \u201cmax\u201d rather than \u201cmin\u201d to compute the final score. Note that our performance of using the WUP score is to filter out ambiguous decoys to the target. For example, we consider \u201ca cute cat\u201d to be ambiguous to \u201ccat\u201d. Using eq. (4) gives a similarity 1, which can not be achieved by taking \u201cmin\u201d."}, {"heading": "C Details on user studies", "text": "As mentioned in Sect. 5.2 of the main text, we provide details on user studies. Fig. 4 shows our user interface. We perform the studies using Amazon Mechanic Turk (AMT) on Visual7W [30], VQA [3] and Visual Genome (VG) [16]. We mainly evaluate on our IoU-decoys and QoU-decoys (combined together).\nFor each dataset, we randomly sample 1,000 image-question-target triplets together with the corresponding IoU-decoys and QoU-decoys to evaluate human performance. For each of these triplets, three workers are assigned to select the most correct candidate answer according to the image and the question. We compute the average accuracy of these workers and report them in Table 3, 4 and 5 of the main text.\nWe also conduct human evaluation using the Orig decoys of Visual7W so as to investigate the difference between human-generated and automatically generated decoys. We also study how humans will perform given only partial information (i.e., images + candidate answers or questions + candidate answers), again using the Orig decoys of Visual7W. The corresponding interfaces are shown in Fig. 5 and 6. For these\nstudies, we use the same set of 1,000 triplets used to evaluate our created decoys for fair comparison. We make sure that no worker works on the same triplet across the four studies on Visual7W. Results are reported in Table 1 of the main text.\nIn summary, 169 workers are involved in our studies. On our IoU-decoys and QoU-decoys, humans achieve 84.1%, 89.0%, and 82.5% on Visual7W, VQA, and VG, respectively. Compared to the human performance on the Orig decoys that involve human effort in creation (i.e., 88.4% on Visual7W, and 88.5% on VQA as reported in [3]), these results suggest that the ways we create the decoys and the filtering steps mentioned in Sect. 4.2 lead to high-quality datasets with limited ambiguity."}, {"heading": "D Detailed results on VQA w/o QA pairs that have Yes/No as the targets", "text": "As mentioned in Sect. 5.3 of the main text, the validation set of VQA contains 45,478 QA pairs (out of totally 12,1512 pairs) that have Yes or No as the correct answers. The only reasonable decoy to Yes is No, and vice versa \u2014 any other decoy could be easily recognized in principle. Since both of them are among top 10 frequent answers, they are already included in the Orig decoys \u2014 our IoU-decoys and QoU-decoys can hardly make noticeable improvement. We thus remove all those pairs (denoted as Yes/No QA pairs) to investigate the improvement on the remaining pairs, for which having multiple choices makes sense. We denote the subset of VQA as VQA\u2212 (we remove Yes/No pairs in both training and validation set).\nWe conduct the same experiments as in Sect. 5.3 on VQA\u2212. Table 7 summarizes the machines\u2019 as well as humans\u2019 results. Compared to Table 4 of the main text, most of the results drop, which is expected as those removed Yes/No pairs are considered simpler and easier ones \u2014 their effective random chance is 50%. The exception is for the MLP-IA models, which performs roughly the same or even better on VQA\u2212, suggesting that Yes/No pairs are somehow difficult to MLP-IA. This, however, makes sense since without the questions (eg, those start with \u201cIs there a ...\u201d or \u201cDoes the person ...\u201d), a machine cannot directly tell if the correct answer falls into Yes or No, or others.\nWe see that on VQA\u2212, the improvement by our IoU-decoys and QoU-decoys becomes significant. The\ngain brought by images on QoU (from 39.3% to 56.6%) is much larger than that on Orig (from 45.8% to 55.6%). Similarly, the gain brought by questions on IoU (from 44.8% to 81.8%) is much larger than that on Orig (from 43.0% to 55.6%). After combining IoU-decoys and QoU-decoys as in IoU +QoU and All, the improvement by either including images to MLP-QA or including questions to MLP-IA is noticeable higher than that on Orig. Moreover, even with only 6 decoys, the performance by MLP-A on IoU +QoU is already lower than that on Orig, which has 17 decoys, demonstrating the effectiveness of our decoys in preventing machines from overfitting to the incidental statistics. Those observations together demonstrate how our proposed ways for creating decoys improve the quality of multiple-choice visual QA datasets."}], "references": [{"title": "Analyzing the behavior of visual question answering models", "author": ["Aishwarya Agrawal", "Dhruv Batra", "Devi Parikh"], "venue": "Clear and sunny. C. Basement windows. D. On both sides of road. E. To left of truck. F. On edge of the sidewalk. G. In front of the building.  A. Certificate. B. Garland. C. Three. D. The man. E. Person in chair. F. The lady. G. The woman", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Spice: Semantic propositional image caption evaluation", "author": ["Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould"], "venue": "In ECCV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Natural language processing with Python: analyzing text with the natural language toolkit", "author": ["Steven Bird", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Microsoft coco captions: Data collection and evaluation", "author": ["Xinlei Chen", "Hao Fang", "Tsung-Yi Lin", "Ramakrishna Vedantam", "Saurabh Gupta", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "server. arXiv preprint arXiv:1504.00325,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Understanding image and text simultaneously: a dual vision-language machine comprehension", "author": ["Nan Ding", "Sebastian Goodman", "Fei Sha", "Radu Soricut"], "venue": "task. arXiv preprint arXiv:1612.07833,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "author": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "venue": "arXiv preprint arXiv:1612.00837,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Focused evaluation for image description with binary forcedchoice tasks", "author": ["Micah Hodosh", "Julia Hockenmaier"], "venue": "In ACL Workshop,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["Micah Hodosh", "Peter Young", "Julia Hockenmaier"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Revisiting visual question answering baselines", "author": ["Allan Jabri", "Armand Joulin", "Laurens van der Maaten"], "venue": "In ECCV,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "author": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C Lawrence Zitnick", "Ross Girshick"], "venue": "arXiv preprint arXiv:1612.06890,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Visual question answering: Datasets, algorithms, and future challenges", "author": ["Kushal Kafle", "Christopher Kanan"], "venue": "arXiv preprint arXiv:1610.01465,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Re-evaluating automatic metrics for image captioning", "author": ["Mert Kilickaya", "Aykut Erdem", "Nazli Ikizler-Cinbis", "Erkut Erdem"], "venue": "arXiv preprint arXiv:1612.07600,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick"], "venue": "In ECCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Leveraging visual question answering for image-caption ranking", "author": ["Xiao Lin", "Devi Parikh"], "venue": "In ECCV,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In NIPS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Visual question answering: A survey of methods and datasets", "author": ["Qi Wu", "Damien Teney", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel"], "venue": "arXiv preprint arXiv:1607.05910,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Verbs semantics and lexical selection", "author": ["Zhibiao Wu", "Martha Palmer"], "venue": "In ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Visual madlibs: Fill in the blank description generation and question answering", "author": ["Licheng Yu", "Eunbyung Park", "Alexander C Berg", "Tamara L Berg"], "venue": "In ICCV,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["Peng Zhang", "Yash Goyal", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}], "referenceMentions": [{"referenceID": 25, "context": "Recently, multimodal information processing tasks such as image captioning [27] and visual question answering (visual QA) [3] have gained a lot of attention.", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "Recently, multimodal information processing tasks such as image captioning [27] and visual question answering (visual QA) [3] have gained a lot of attention.", "startOffset": 122, "endOffset": 125}, {"referenceID": 16, "context": "Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others.", "startOffset": 50, "endOffset": 57}, {"referenceID": 4, "context": "Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others.", "startOffset": 50, "endOffset": 57}, {"referenceID": 14, "context": "Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others.", "startOffset": 73, "endOffset": 77}, {"referenceID": 2, "context": "Among those datasets, popular ones include MSCOCO [18, 5], Visual Genome [16], VQA [3], and several others.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 80, "endOffset": 91}, {"referenceID": 10, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 80, "endOffset": 91}, {"referenceID": 9, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 122, "endOffset": 137}, {"referenceID": 8, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 122, "endOffset": 137}, {"referenceID": 5, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 122, "endOffset": 137}, {"referenceID": 17, "context": "Thus, many efforts have concentrated on tasks such as multiple-choice visual QA [3, 30, 12] or selecting the best caption [11, 10, 7, 19], where the selection accuracy is a natural evaluation metric.", "startOffset": 122, "endOffset": 137}, {"referenceID": 10, "context": ", the image), the question, or both [12, 3].", "startOffset": 36, "endOffset": 43}, {"referenceID": 2, "context": ", the image), the question, or both [12, 3].", "startOffset": 36, "endOffset": 43}, {"referenceID": 2, "context": "We apply the procedures to revise both Visual7W [30] and VQA [3].", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "Additionally, we create a multiple-choice based dataset from the recently released Visual Genome dataset [16], resulting in the largest multiple-choice dataset for the visual QA task, with more than one million image-question-candidate answers triplets.", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "[14, 25] provide recent overviews of the status quo of the visual QA task.", "startOffset": 0, "endOffset": 8}, {"referenceID": 23, "context": "[14, 25] provide recent overviews of the status quo of the visual QA task.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "We concentrate on 3 datasets: VQA [3], Visual7W [30], and Visual Genome [16].", "startOffset": 34, "endOffset": 37}, {"referenceID": 14, "context": "We concentrate on 3 datasets: VQA [3], Visual7W [30], and Visual Genome [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "All of them use images from MSCOCO [18].", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "Besides the pairs of questions and correct answers, VQA, Visual7W, and visual Madlibs [28] provide decoy answers for each pair so that the task can be evaluated in multiple-choice selection accuracy.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "Our work is inspired by the experiments in [12] where they observe that machines without looking at images or questions can still perform well on the visual QA task.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting.", "startOffset": 41, "endOffset": 55}, {"referenceID": 27, "context": "Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting.", "startOffset": 41, "endOffset": 55}, {"referenceID": 11, "context": "Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting.", "startOffset": 41, "endOffset": 55}, {"referenceID": 0, "context": "Others have also reported similar issues [8, 29, 13, 1], though not in the multiple-choice setting.", "startOffset": 41, "endOffset": 55}, {"referenceID": 5, "context": "Besides the visual QA task, [7] and VisDial [6] also propose automatic ways to generate decoys for the tasks of selecting the best visual caption and dialog, respectively.", "startOffset": 28, "endOffset": 31}, {"referenceID": 10, "context": "We concentrate on the one hidden-layer MLP model proposed in [12], which had achieved state-of-the-art results on the dataset Visual7W.", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "Given an IQA triplet, we use the penultimate layer of ResNet-200 [9] as visual features to represent I and the average WORD2VEC embeddings [22] as text features to represent Q and C.", "startOffset": 65, "endOffset": 68}, {"referenceID": 20, "context": "Given an IQA triplet, we use the penultimate layer of ResNet-200 [9] as visual features to represent I and the average WORD2VEC embeddings [22] as text features to represent Q and C.", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "We compute the average WORD2VEC [22] to represent a question, and use the cos similarity to measure the similarity between questions.", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "Secondly, we utilize the WordNet hierarchy and the Wu-Palmer (WUP) score [26] to eliminate semantically similar decoys.", "startOffset": 73, "endOffset": 77}, {"referenceID": 0, "context": "The WUP score measures how similar two word senses are (in the range of [0, 1]),", "startOffset": 72, "endOffset": 78}, {"referenceID": 19, "context": "We compute the similarity of two strings according to the WUP scores in a similar manner to [21], in which the WUP score is used for the evaluation of visual QA performance.", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "We use NLTK toolkit [4] to compute the similarity.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "Several authors have noticed the design deficiencies in the existing databases and have proposed \u201cfixes\u201d [3, 28, 30, 6].", "startOffset": 105, "endOffset": 119}, {"referenceID": 26, "context": "Several authors have noticed the design deficiencies in the existing databases and have proposed \u201cfixes\u201d [3, 28, 30, 6].", "startOffset": 105, "endOffset": 119}, {"referenceID": 26, "context": "[28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively.", "startOffset": 0, "endOffset": 10}, {"referenceID": 5, "context": "[28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively.", "startOffset": 0, "endOffset": 10}, {"referenceID": 21, "context": "[28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively.", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "[28, 6, 7] automatically find decoys from similar questions or captions based on question templates and annotated objects, tri-grams and GLOVE embeddings [23], and paragraph vectors [17] and linguistic surface similarity, respectively.", "startOffset": 182, "endOffset": 186}, {"referenceID": 5, "context": "The later two are for different tasks from visual QA, and only [7] considers removing semantically ambiguous decoys like ours.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "[3, 30] ask humans to create decoys, given the questions and targets.", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "VQA Real [3] The dataset uses images from MSCOCO [18] under the same training/validation/testing splits to construct IQA triplets.", "startOffset": 9, "endOffset": 12}, {"referenceID": 16, "context": "VQA Real [3] The dataset uses images from MSCOCO [18] under the same training/validation/testing splits to construct IQA triplets.", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "Visual7W Telling (Visual7W) [30] The dataset uses 47,300 images from MSCOCO [18] and contains 139,868 IQA triplets.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Visual Genome (VG) [16] The dataset uses 101,174 images from MSCOCO [18] and contains 1,445,322 IQT triplets.", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "Visual Genome (VG) [16] The dataset uses 101,174 images from MSCOCO [18] and contains 1,445,322 IQT triplets.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "We use a 200-layer ResNet [9] to compute visual features which are 2,048-dimensional.", "startOffset": 26, "endOffset": 29}, {"referenceID": 22, "context": "The ResNet is pre-trained on ImageNet [24].", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "The WORD2VEC feature [22] for questions and answers are 300-dimensional, pre-trained on Google News.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "*:taken from [3]", "startOffset": 13, "endOffset": 16}, {"referenceID": 10, "context": "5% by [12] where a model pre-trained on VQA is fine-tuned on Visual7W.", "startOffset": 6, "endOffset": 10}, {"referenceID": 5, "context": "This issue can be alleviated by considering alternative semantic measures by WORD2VEC or by those used in [6, 7] for searching similar questions.", "startOffset": 106, "endOffset": 112}], "year": 2017, "abstractText": "Visual question answering (QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (i.e. the correct one) and the decoys (i.e. the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or the both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular visual QA datasets as well as to create a new visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via http://www.teds.usc.edu/website_vqa/.", "creator": "LaTeX with hyperref package"}}}