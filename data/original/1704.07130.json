{"id": "1704.07130", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings", "abstract": "We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.", "histories": [["v1", "Mon, 24 Apr 2017 10:38:24 GMT  (1204kb,D)", "http://arxiv.org/abs/1704.07130v1", "ACL 2017"]], "COMMENTS": "ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["he he", "anusha balakrishnan", "mihail eric", "percy liang"], "accepted": true, "id": "1704.07130"}, "pdf": {"name": "1704.07130.pdf", "metadata": {"source": "CRF", "title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings", "authors": ["He He", "Anusha Balakrishnan"], "emails": ["hehe@cs.stanford.edu", "anusha28@cs.stanford.edu", "meric@cs.stanford.edu", "pliang@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.g., slots such as food type and price range for a restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform). However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance. While these models capture open-ended aspects of dialogue, the lack of structured dialogue state prevents them from being directly applied to settings that require interfacing with structured knowledge.\nIn order to bridge the gap between the two types\nFriends of agent A:\nName School Major Company\nJessica Columbia Computer Science Google Josh Columbia Linguistics Google ... ... ... ...\nA: Hi! Most of my friends work for Google B: do you have anyone who went to columbia? A: Hello? A: I have Jessica a friend of mine A: and Josh, both went to columbia B: or anyone working at apple? B: SELECT (Jessica, Columbia, Computer Science, Google) A: SELECT (Jessica, Columbia, Computer Science, Google)\nFigure 1: An example dialogue from the MutualFriends task in which two agents, A and B, each given a private list of a friends, try to identify their mutual friend. Our objective is to build an agent that can perform the task with a human. Crosstalk (Section 2.3) is italicized.\nof systems, we focus on a symmetric collaborative dialogue setting, which is task-oriented but encourages open-ended dialogue acts. In our setting, two agents, each with a private list of items with attributes, must communicate to identify the unique shared item. Consider the dialogue in Figure 1, in which two people are trying to find their mutual friend. By asking \u201cdo you have anyone who went to columbia?\u201d, B is suggesting that she has some Columbia friends, and that they probably work at Google. Such conversational implicature is lost when interpreting the utterance as simply an information request. In addition, it is hard to define a structured state that captures the diverse semantics in many utterances (e.g., defining \u201cmost of\u201d, \u201cmight be\u201d; see details in Table 1).\nTo model both structured and open-ended context, we propose the Dynamic Knowledge Graph Network (DynoNet), in which the dialogue state is modeled as a knowledge graph with an embedding\nar X\niv :1\n70 4.\n07 13\n0v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nfor each node (Section 3). Our model is similar to EntNet (Henaff et al., 2017) in that node/entity embeddings are updated recurrently given new utterances. The difference is that we structure entities as a knowledge graph; as the dialogue proceeds, new nodes are added and new context is propagated on the graph. An attention-based mechanism (Bahdanau et al., 2015) over the node embeddings drives generation of new utterances. Our model\u2019s use of knowledge graphs captures the grounding capability of classic task-oriented systems and the graph embedding provides the representational flexibility of neural models.\nThe naturalness of communication in the symmetric collaborative setting enables large-scale data collection: We were able to crowdsource around 11K human-human dialogues on Amazon Mechanical Turk (AMT) in less than 15 hours.1 We show that the new dataset calls for more flexible representations beyond fully-structured states (Section 2.2).\nIn addition to conducting the third-party human evaluation adopted by most work (Liu et al., 2016; Li et al., 2016b,c), we also conduct partner evaluation (Wen et al., 2017) where AMT workers rate their conversational partners (other workers or our models) based on fluency, correctness, cooperation, and human-likeness. We compare DynoNet with baseline neural models and a strong rulebased system. The results show that DynoNet can perform the task with humans efficiently and naturally; it also captures some strategic aspects of human-human dialogues.\nThe contributions of this work are: (i) a new symmetric collaborative dialogue setting and a large dialogue corpus that pushes the boundaries of existing dialogue systems; (ii) DynoNet, which integrates semantically rich utterances with structured knowledge to represent open-ended dialogue states; (iii) multiple automatic metrics based on bot-bot chat and a comparison of third-party and partner evaluation."}, {"heading": "2 Symmetric Collaborative Dialogue", "text": "We begin by introducing a collaborative task between two agents and describe the human-human dialogue collection process. We show that our data exhibits diverse, interesting language phenomena.\n1The dataset is available publicly at https:// stanfordnlp.github.io/cocoa/."}, {"heading": "2.1 Task Definition", "text": "In the symmetric collaborative dialogue setting, there are two agents, A and B, each with a private knowledge base\u2014KBA and KBB, respectively. Each knowledge base includes a list of items, where each item has a value for each attribute. For example, in the MutualFriends setting, Figure 1, items are friends and attributes are name, school, etc. There is a shared item that A and B both have; their goal is to converse with each other to determine the shared item and select it. Formally, an agent is a mapping from its private KB and the dialogue thus far (sequence of utterances) to the next utterance to generate or a selection. A dialogue is considered successful when both agents correctly select the shared item. This setting has parallels in human-computer collaboration where each agent has complementary expertise."}, {"heading": "2.2 Data collection", "text": "We created a schema with 7 attributes and approximately 3K entities (attribute values). To elicit linguistic and strategic variants, we generate a random scenario for each task by varying the number of items (5 to 12), the number attributes (3 or 4), and the distribution of values for each attribute (skewed to uniform). See Appendix A and B for details of schema and scenario generation.\nWe crowdsourced dialogues on AMT by randomly pairing up workers to perform the task within 5 minutes.2 Our chat interface is shown in Figure 2. To discourage random guessing, we prevent workers from selecting more than once every 10 seconds. Our task was very popular and we col-\n2If the workers exceed the time limit, the dialogue is marked as unsuccessful (but still logged).\nlected 11K dialogues over a period of 13.5 hours.3 Of these, over 9K dialogues are successful. Unsuccessful dialogues are usually the result of either worker leaving the chat prematurely."}, {"heading": "2.3 Dataset statistics", "text": "We show the basic statistics of our dataset in Table 3. An utterance is defined as a message sent by one of the agents. The average utterance length is short due to the informality of the chat, however, an agent usually sends multiple utterances in one turn. Some example dialogues are shown in Table 6 and Appendix I.\nWe categorize utterances into coarse types\u2014 inform, ask, answer, greeting, apology\u2014by pattern matching (Appendix E). There are 7.4% multitype utterances, and 30.9% utterances contain more than one entity. In Table 1, we show example utterances with rich semantics that cannot be sufficiently represented by traditional slot-values.\n3Tasks are put up in batches; the total time excludes intervals between batches.\n4Entity names are replaced by their entity types.\nSome of the standard ones are also non-trivial due to coreference and logical compositionality.\nOur dataset also exhibits some interesting communication phenomena. Coreference occurs frequently when people check multiple attributes of one item. Sometimes mentions are dropped, as an utterance simply continues from the partner\u2019s utterance. People occasionally use external knowledge to group items with out-of-schema attributes (e.g., gender based on names, location based on schools). We summarize these phenomena in Table 2. In addition, we find 30% utterances involve cross-talk where the conversation does not progress linearly (e.g., italic utterances in Figure 1), a common characteristic of online chat (Ivanovic, 2005).\nOne strategic aspect of this task is choosing the order of attributes to mention. We find that people tend to start from attributes with fewer unique values, e.g., \u201call my friends like morning\u201d given the KBB in Table 6, as intuitively it would help exclude items quickly given fewer values to check.5 We provide a more detailed analysis of strategy in Section 4.2 and Appendix F."}, {"heading": "3 Dynamic Knowledge Graph Network", "text": "The diverse semantics in our data motivates us to combine unstructured representation of the dialogue history with structured knowledge. Our\n5Our goal is to model human behavior thus we do not discuss the optimal strategy here.\nmodel consists of three components shown in Figure 3: (i) a dynamic knowledge graph, which represents the agent\u2019s private KB and shared dialogue history as a graph (Section 3.1), (ii) a graph embedding over the nodes (Section 3.2), and (iii) an utterance generator (Section 3.3).\nThe knowledge graph represents entities and relations in the agent\u2019s private KB, e.g., item-1\u2019s company is google. As the conversation unfolds, utterances are embedded and incorporated into node embeddings of mentioned entities. For instance, in Figure 3, \u201canyone went to columbia\u201d updates the embedding of columbia. Next, each node recursively passes its embedding to neighboring nodes so that related entities (e.g., those in the same row or column) also receive information from the most recent utterance. In our example, jessica and josh both receive new context when columbia is mentioned. Finally, the utterance generator, an LSTM, produces the next utterance by attending to the node embeddings."}, {"heading": "3.1 Knowledge Graph", "text": "Given a dialogue of T utterances, we construct graphs (Gt)Tt=1 over the KB and dialogue history for agent A.6 There are three types of nodes: item nodes, attribute nodes, and entity nodes. Edges between nodes represent their relations. For example, (item-1, hasSchool, columbia) means that the first item has attribute school whose value\n6 It is important to differentiate perspectives of the two agents as they have different KBs. Thereafter we assume the perspective of agent A, i.e., accessing KBA for A only, and refer to B as the partner.\nis columbia. An example graph is shown in Figure 3. The graph Gt is updated based on utterance t by taking Gt\u22121 and adding a new node for any entity mentioned in utterance t but not in KBA.7"}, {"heading": "3.2 Graph Embedding", "text": "Given a knowledge graph, we are interested in computing a vector representation for each node v that captures both its unstructured context from the dialogue history and its structured context in the KB. A node embedding Vt(v) for each node v \u2208 Gt is built from three parts: structural properties of an entity defined by the KB, embeddings of utterances in the dialogue history, and message passing between neighboring nodes.\nNode Features. Simple structural properties of the KB often govern what is talked about; e.g., a high-frequency entity is usually interesting to mention (consider \u201cAll my friends like dancing.\u201d). We represent this type of information as a feature vector Ft(v), which includes the degree and type (item, attribute, or entity type) of node v, and whether it has been mentioned in the current turn. Each feature is encoded as a one-hot vector and they are concatenated to form Ft(v).\nMention Vectors. A mention vectorMt(v) contains unstructured context from utterances relevant to node v up to turn t. To compute it, we first define the utterance representation u\u0303t and the set of relevant entities Et. Let ut be the embedding of utterance t (Section 3.3). To differentiate between\n7 We use a rule-based lexicon to link text spans to entities. See details in Appendix D.\nthe agent\u2019s and the partner\u2019s utterances, we represent it as u\u0303t = [ ut \u00b7 1{ut\u2208Uself}, ut \u00b7 1{ut\u2208Upartner} ] , where Uself and Upartner denote sets of utterances generated by the agent and the partner, and [\u00b7, \u00b7] denotes concatenation. Let Et be the set of entity nodes mentioned in utterance t if utterance t mentions some entities, or utterance t \u2212 1 otherwise.8 The mention vector Mt(v) of node v incorporates the current utterance if v is mentioned and inherits Mt\u22121(v) if not:\nMt(v) = \u03bbtMt\u22121(v) + (1\u2212 \u03bbt)u\u0303t; (1)\n\u03bbt =\n{ \u03c3 ( W inc [Mt\u22121(v), u\u0303t] ) if v \u2208 Et,\n1 otherwise.\nHere, \u03c3 is the sigmoid function and W inc is a parameter matrix.\nRecursive Node Embeddings. We propagate information between nodes according to the structure of the knowledge graph. In Figure 3, given \u201canyone went to columbia?\u201d, the agent should focus on her friends who went to Columbia University. Therefore, we want this utterance to be sent to item nodes connected to columbia, and one step further to other attributes of these items because they might be mentioned next as relevant information, e.g., jessica and josh.\nWe compute the node embeddings recursively, analogous to belief propagation:\nV kt (v) = max v\u2032\u2208Nt(v) tanh (2)( Wmp [ V k\u22121t (v \u2032), R(ev\u2192v\u2032) ]) ,\nwhere V kt (v) is the depth-k node embedding at turn t and Nt(v) denotes the set of nodes adjacent to v. The message from a neighboring node v\u2032 depends on its embedding at depth-(k\u2212 1), the edge label ev\u2192v\u2032 (embedded by a relation embedding function R), and a parameter matrix Wmp. Messages from all neighbors are aggregated by max, the element-wise max operation.9 Example message passing paths are shown in Figure 3.\nThe final node embedding is the concatenation of embeddings at each depth:\nVt(v) = [ V 0t (v), . . . , V K t (v) ] , (3)\nwhereK is a hyperparameter (we experiment with K \u2208 {0, 1, 2}) and V 0t (v) = [Ft(v),Mt(v)].\n8 Relying on utterance t \u2212 1 is useful when utterance t answers a question, e.g., \u201cdo you have any google friends?\u201d \u201cNo.\u201d\n9Using sum or mean slightly hurts performance."}, {"heading": "3.3 Utterance Embedding and Generation", "text": "We embed and generate utterances using Long Short Term Memory (LSTM) networks that take the graph embeddings into account.\nEmbedding. On turn t, upon receiving an utterance consisting of nt tokens, xt = (xt,1, . . . , xt,nt), the LSTM maps it to a vector as follows:\nht,j = LSTMenc(ht,j\u22121, At(xt,j)), (4)\nwhere ht,0 = ht\u22121,nt\u22121 , and At is an entity abstraction function, explained below. The final hidden state ht,nt is used as the utterance embedding ut, which updates the mention vectors as described in Section 3.2.\nIn our dialogue task, the identity of an entity is unimportant. For example, replacing google with alphabet in Figure 1 should make little difference to the conversation. The role of an entity is determined instead by its relation to other entities and relevant utterances. Therefore, we define the abstraction At(y) for a word y as follows: if y is linked to an entity v, then we represent an entity by its type (school, company etc.) embedding concatenated with its current node embedding: At(y) = [Etype(y), Vt(v)]. Note that Vt(v) is determined only by its structural features and its context. If y is a non-entity, thenAt(y) is the word embedding of y concatenated with a zero vector of the same dimensionality as Vt(v). This way, the representation of an entity only depends on its structural properties given by the KB and the dialogue context, which enables the model to generalize to unseen entities at test time.\nGeneration. Now, assuming we have embedded utterance xt\u22121 into ht\u22121,nt\u22121 as described above, we use another LSTM to generate utterance xt. Formally, we carry over the last utterance embedding ht,0 = ht\u22121,nt\u22121 and define:\nht,j = LSTMdec(ht,j\u22121, [At(xt,j), ct,j ]), (5)\nwhere ct,j is a weighted sum of node embeddings in the current turn: ct,j = \u2211 v\u2208Gt \u03b1t,j,vVt(v), where \u03b1t,j,v are the attention weights over the nodes. Intuitively, high weight should be given to relevant entity nodes as shown in Figure 3. We compute the weights through standard attention mechanism (Bahdanau et al., 2015):\n\u03b1t,j = softmax(st,j),\nst,j,v = w attn \u00b7 tanh ( W attn [ht,j\u22121, Vt(v)] ) ,\nwhere vector wattn and W attn are parameters. Finally, we define a distribution over both words in the vocabulary and nodes in Gt using the copying mechanism of Jia and Liang (2016):\np(xt,j+1 = y |Gt, xt,\u2264j) \u221d exp ( W vocabht,j + b ) , p(xt,j+1 = r(v) |Gt, xt,\u2264j) \u221d exp (st,j,v) ,\nwhere y is a word in the vocabulary, W vocab and b are parameters, and r(v) is the realization of the entity represented by node v, e.g., google is realized to \u201cGoogle\u201d during copying.10"}, {"heading": "4 Experiments", "text": "We compare our model with a rule-based system and a baseline neural model. Both automatic and human evaluations are conducted to test the models in terms of fluency, correctness, cooperation, and human-likeness. The results show that DynoNet is able to converse with humans in a coherent and strategic way."}, {"heading": "4.1 Setup", "text": "We randomly split the data into train, dev, and test sets (8:1:1). We use a one-layer LSTM with 100 hidden units and 100-dimensional word vectors for both the encoder and the decoder (Section 3.3). Each successful dialogue is turned into two examples, each from the perspective of one of the two agents. We maximize the log-likelihood of all utterances in the dialogues. The parameters are optimized by AdaGrad (Duchi et al., 2010) with an initial learning rate of 0.5. We trained for at least 10 epochs; after that, training stops if there is no improvement on the dev set for 5 epochs. By default, we perform K = 2 iterations of message passing to compute node embeddings (Section 3.2). For decoding, we sequentially sample from the output distribution with a softmax temperature of 0.5.11 Hyperparameters are tuned on the dev set. We compare DynoNet with its static cousion (StanoNet) and a rule-based system (Rule). StanoNet uses G0 throughout the dialogue, thus the dialogue history is completely contained in the LSTM states instead of being injected into the knowledge graph. Rule maintains weights for each entity and each item in the KB to decide\n10 We realize an entity by sampling from the empirical distribution of its surface forms found in the training data.\n11 Since selection is a common \u2018utterance\u2019 in our dataset and neural generation models are susceptible to overgenerating common sentences, we halve its probability during sampling.\nwhat to talk about and which item to select. It has a pattern-matching semantic parser, a rulebased policy, and a templated generator. See Appendix G for details."}, {"heading": "4.2 Evaluation", "text": "We test our systems in two interactive settings: bot-bot chat and bot-human chat. We perform both automatic evaluation and human evaluation.\nAutomatic Evaluation. First, we compute the cross-entropy (`) of a model on test data. As shown in Table 4, DynoNet has the lowest test loss. Next, we have a model chat with itself on the scenarios from the test set.12 We evaluate the chats with respect to language variation, effectiveness, and strategy.\nFor language variation, we report the average utterance length Lu and the unigram entropy H in Table 4. Compared to Rule, the neural models tend to generate shorter utterances (Li et al., 2016b; Serban et al., 2017b). However, they are more diverse; for example, questions are asked in multiple ways such as \u201cDo you have ...\u201d, \u201cAny friends like ...\u201d, \u201cWhat about ...\u201d.\nAt the discourse level, we expect the distribution of a bot\u2019s utterance types to match the distribution of human\u2019s. We show percentages of each utterance type in Table 4. For Rule, the decision about which action to take is written in the rules, while StanoNet and DynoNet learned to behave in a more human-like way, frequently informing and asking questions.\nTo measure effectiveness, we compute the overall success rate (C) and the success rate per turn (CT ) and per selection (CS). As shown in Table 4, humans are the best at this game, followed by Rule which is comparable to DynoNet.\nNext, we investigate the strategies leading to these results. An agent needs to decide which entity/attribute to check first to quickly reduce the search space. We hypothesize that humans tend to first focus on a majority entity and an attribute with fewer unique values (Section 2.3). For example, in the scenario in Table 6, time and location are likely to be mentioned first. We show the average frequency of first-mentioned entities (#Ent1) and the average number of unique values for first-mentioned attributes (|Attr1|) in Ta-\n12 We limit the number of turns in bot-bot chat to be the maximum number of turns humans took in the test set (46 turns).\nble 4.13 Both DynoNet and StanoNet successfully match human\u2019s starting strategy by favoring entities of higher frequency and attributes of smaller domain size.\nTo examine the overall strategy, we show the average number of attributes (#Attr) and entities (#Ent) mentioned during the conversation in Table 4. Humans and DynoNet strategically focus on a few attributes and entities, whereas Rule needs almost twice entities to achieve similar success rates. This suggests that the effectiveness of Rule mainly comes from large amounts of unselective information, which is consistent with comments from their human partners.\nPartner Evaluation. We generated 200 new scenarios and put up the bots on AMT using the same chat interface that was used for data collection. The bots follow simple turn-taking rules explained in Appendix H. Each AMT worker is randomly paired with Rule, StanoNet, DynoNet, or another human (but the worker doesn\u2019t know which), and we make sure that all four types of agents are tested in each scenario at least once. At the end of each dialogue, humans are asked to rate their partner in terms of fluency, correctness, cooperation, and human-likeness from 1 (very bad) to 5 (very good), along with optional comments.\nWe show the average ratings (with significance tests) in Table 5 and the histograms in Appendix J. In terms of fluency, the models have similar performance since the utterances are usually short. Judgment on correctness is a mere guess since the evaluator cannot see the partner\u2019s KB; we will analyze correctness more meaningfully in the thirdparty evaluation below.\n13 Both numbers are normalized to [0, 1] with respect to all entities/attributes in the corresponding KB.\nNoticeably, DynoNet is more cooperative than the other models. As shown in the example dialogues in Table 6, DynoNet cooperates smoothly with the human partner, e.g., replying with relevant information about morning/indoor friends when the partner mentioned that all her friends prefer morning and most like indoor. StanoNet starts well but doesn\u2019t follow up on the morning friend, presumably because the morning node is not updated dynamically when mentioned by the partner. Rule follows the partner poorly. In the comments, the biggest complaint about Rule was that it was not \u2018listening\u2019 or \u2018understanding\u2019. Overall, DynoNet achieves better partner satisfaction, especially in cooperation.\nThird-party Evaluation. We also created a third-party evaluation task, where an independent AMT worker is shown a conversation and the KB of one of the agents; she is asked to rate the same aspects of the agent as in the partner evaluation and provide justifications. Each agent in a dialogue is rated by at least 5 people.\nThe average ratings and histograms are shown in Table 5 and Appendix J. For correctness, we see that Rule has the best performance since it always tells the truth, whereas humans can make mistakes due to carelessness and the neural models can generate false information. For example, in Table 6, DynoNet \u2018lied\u2019 when saying that it has a morning friend who likes outdoor.\nSurprisingly, there is a discrepancy between the two evaluation modes in terms of cooperation and human-likeness. Manual analysis of the comments indicates that third-party evaluators focus less on the dialogue strategy and more on linguistic features, probably because they were not fully engaged in the dialogue. For example, justification\nfor cooperation often mentions frequent questions and timely answers, less attention is paid to what is asked about though.\nFor human-likeness, partner evaluation is largely correlated with coherence (e.g., not repeating or ignoring past information) and task success, whereas third-party evaluators often rely on informality (e.g., usage of colloquia like \u201chiya\u201d, capitalization, and abbreviation) or intuition. Interestingly, third-party evaluators noted most phenomena listed in Table 2 as indicators of humanbeings, e.g., correcting oneself, making chit-chat other than simply finishing the task. See example comments in Appendix K."}, {"heading": "4.3 Ablation Studies", "text": "Our model has two novel designs: entity abstraction and message passing for node embeddings. Table 7 shows what happens if we ablate these. When the number of message passing iterations, K, is reduced from 2 to 0, the loss consistently increases. Removing entity abstraction\u2014meaning adding entity embeddings to node embeddings and the LSTM input embeddings\u2014also degrades performance. This shows that DynoNet benefits from contextually-defined, structural node embeddings rather than ones based on a classic lookup table."}, {"heading": "5 Discussion and Related Work", "text": "There has been a recent surge of interest in end-to-end task-oriented dialogue systems, though progress has been limited by the size of available datasets (Serban et al., 2015a). Most work focuses on information-querying tasks, using Wizard-ofOz data collection (Williams et al., 2016; Asri et al., 2016) or simulators (Bordes and Weston, 2017; Li et al., 2016d), In contrast, collaborative\ndialogues are easy to collect as natural human conversations, and are also challenging enough given the large number of scenarios and diverse conversation phenomena. There are some interesting strategic dialogue datasets\u2014settlers of Catan (Afantenos et al., 2012) (2K turns) and the cards corpus (Potts, 2012) (1.3K dialogues), as well as work on dialogue strategies (Keizer et al., 2017; Vogel et al., 2013), though no full dialogue system has been built for these datasets.\nMost task-oriented dialogue systems follow the POMDP-based approach (Williams and Young, 2007; Young et al., 2013). Despite their success (Wen et al., 2017; Dhingra et al., 2017; Su et al., 2016), the requirement for handcrafted slots limits their scalability to new domains and burdens data collection with extra state labeling. To go past this limit, Bordes and Weston (2017) proposed a Memory-Networks-based approach without domain-specific features. However, the memory is unstructured and interfacing with KBs relies on API calls, whereas our model embeds both the dialogue history and the KB structurally. Williams et al. (2017) use an LSTM to automatically infer the dialogue state, but as they focus on dialogue control rather than the full problem, the response is modeled as a templated action, which restricts the generation of richer utterances. Our network architecture is most similar to EntNet (Henaff et al., 2017), where memories are also updated by input sentences recurrently. The main difference is that our model allows information to be propagated between structured entities, which is shown to be crucial in our setting (Section 4.3).\nOur work is also related to language generation conditioned on knowledge bases (Mei et al., 2016; Kiddon et al., 2016). One challenge here is to\navoid generating false or contradicting statements, which is currently a weakness of neural models. Our model is mostly accurate when generating facts and answering existence questions about a single entity, but will need a more advanced attention mechanism for generating utterances involving multiple entities, e.g., attending to items or attributes first, then selecting entities; generating high-level concepts before composing them to natural tokens (Serban et al., 2017a).\nIn conclusion, we believe the symmetric collaborative dialogue setting and our dataset pro-\nvide unique opportunities at the interface of traditional task-oriented dialogue and open-domain chat. We also offered DynoNet as a promising means for open-ended dialogue state representation. Our dataset facilitates the study of pragmatics and human strategies in dialogue\u2014a good stepping stone towards learning more complex dialogues such as negotiation.\nAcknowledgments. This work is supported by DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF15-1-0462. Mike Kayser worked on an early version of the project while he was at Stanford. We also thank members of the Stanford NLP group for insightful discussions.\nReproducibility. All code, data, and experiments for this paper are available on the CodaLab platform: https: //worksheets.codalab.org/worksheets/ 0xc757f29f5c794e5eb7bfa8ca9c945573."}, {"heading": "A Knowledge Base Schema", "text": "The attribute setA for the MutualFriends task contains name, school, major, company, hobby, timeof-day preference, and location preference. Each attribute a has a set of possible values (entities) Ea. For name, school, major, company, and hobby, we collected a large set of values from various online sources.14 We used three possible values (morning, afternoon, and evening) for the time-of-day preference, and two possible values (indoors and outdoors) for the location preference."}, {"heading": "B Scenario Generation", "text": "We generate scenarios randomly to vary task complexity and elicit linguistic and strategic variants. A scenario S is characterized by the number of items (NS), the attribute set (AS) whose size is MS , and the values for each attribute a \u2208 AS in the two KBs.\nA scenario is generated as follows.\n1. Sample NS and MS uniformly from {5, . . . , 12} and {3, 4} respectively.\n2. Generate AS by sampling MS attributes without replacement from A.\n3. For each attribute a \u2208 AS , sample the concentration parameter \u03b1a uniformly from the set {0.3, 1, 3}.\n4. Generate two KBs by samplingNS values for each attribute a from a Dirichlet-multinomial distribution over the value set Ea with the concentration parameter \u03b1a.\nWe repeat the last step until the two KBs have one unique common item."}, {"heading": "C Chat Interface", "text": "In order to collect real-time dialogue between humans, we set up a web server and redirect AMT workers to our website. Visitors are randomly paired up as they arrive. For each pair, we choose a random scenario, and randomly assign a KB to\n14Names: https://www.ssa.gov/oact/ babynames/decades/century.html Schools: http://doors.stanford.edu/\u02dcsr/ universities.html Majors: http://www.a2zcolleges.com/majors Companies: https://en.wikipedia.org/wiki/ List_of_companies_of_the_United_States Hobbies: https://en.wikipedia.org/wiki/ List_of_hobbies\neach dialogue participant. We instruct people to play intelligently, to refrain from brute-force tactics (e.g., mentioning every attribute value), and to use grammatical sentences. To discourage random guessing, we prevent users from selecting a friend (item) more than once every 10 seconds. Each worker was paid $0.35 for a successful dialogue within a 5-minute time limit. We log each utterance in the dialogue along with timing information."}, {"heading": "D Entity Linking and Realization", "text": "We use a rule-based lexicon to link text spans to entities. For every entity in the schema, we compute different variations of its canonical name, including acronyms, strings with a certain edit distance, prefixes, and morphological variants. Given a text span, a set of candidate entities is returned by string matching. A heuristic ranker then scores each candidate (e.g., considering whether the span is a substring of a candidate, the edit distance between the span and a candidate etc.). The highestscoring candidate is returned.\nA linked entity is considered as a single token and its surface form is ignored in all models. At generation time, we realize an entity by sampling from the empirical distribution of its surface forms in the training set."}, {"heading": "E Utterance Categorization", "text": "We categorize utterances into inform, ask, answer, greeting, apology heuristically by pattern matching.\n\u2022 An ask utterance asks for information regarding the partner\u2019s KB. We detect these utterances by checking for the presence of a \u2018?\u2019 and/or a question word like \u201cdo\u201d, \u201cdoes\u201d, \u201cwhat\u201d, etc.\n\u2022 An inform utterance provides information about the agent\u2019s KB. We define it as an utterances that mentions entities in the KB and is not an ask utterance.\n\u2022 An answer utterance simply provides a positive/negative response to a question, containing words like \u201cyes\u201d, \u201cno\u201d, \u201cnope\u201d, etc.\n\u2022 A greeting utterance contains words like \u201chi\u201d or \u201chello\u201d; it often occurs at the beginning of a dialogue.\n\u2022 An apology utterance contains the word \u201csorry\u201d, which is typically associated with corrections and wrong selections.\nSee Table 2 and Table 1 for examples of these utterance types."}, {"heading": "F Strategy", "text": "During scenario generation, we varied the number of attributes, the number of items in each KB, and the distribution of values for each attribute. We find that as the number of items and/or attributes grows, the dialogue length and the completion time also increase, indicating that the task becomes harder. We also anticipated that varying the value of \u03b1 would impact the overall strategy (for example, the order in which attributes are mentioned) since \u03b1 controls the skewness of the distribution of values for an attribute.\nOn examining the data, we find that humans tend to first mention attributes with a more skewed (i.e., less uniform) distribution of values. Specifically, we rank the \u03b1 values of all attributes in a scenario (see step 3 in Section B), and bin them into 3 distribution groups\u2014least uniform, medium, and most uniform, according to the ranking where higher \u03b1 values corresponds to more uniform distributions.15 In Figure 4, we plot the histogram of the distribution group of the first-mentioned attribute in a dialogues, which shows that skewed attributes are mentioned much more frequently.\n15 For scenarios with 3 attributes, each group contains one attributes. For scenarios with 4 attributes, we put the two attributes with rankings in the middle to medium."}, {"heading": "G Rule-based System", "text": "The rule-based bot takes the following actions: greeting, informing or asking about a set of entities, answering a question, and selecting an item. The set of entities to inform/ask is sampled randomly given the entity weights. Initially, each entity is weighted by its count in the KB. We then increment or decrement weights of entities mentioned by the partner and its related entities (in the same row or column), depending on whether the mention is positive or negative. A negative mention contains words like \u201cno\u201d, \u201cnone\u201d, \u201cn\u2019t\u201d etc. Similarly, each item has an initial weight of 1, which is updated depending on the partner\u2019s mention of its attributes.\nIf there exists an item with weight larger than 1, the bot selects the highest-weighted item with probability 0.3. If a question is received, the bot informs facts of the entities being asked, e.g., \u201canyone went to columbia?\u201d, \u201cI have 2 friends who went to columbia\u201d. Otherwise, the bot samples an entity set and randomly chooses between informing and asking about the entities.\nAll utterances are generated by sentence templates, and parsing of the partner\u2019s utterance is done by entity linking and pattern matching (Section E)."}, {"heading": "H Turn-taking Rules", "text": "Turn-taking is universal in human conversations and the bot needs to decide when to \u2018talk\u2019 (send an utterance). To prevent the bot from generating utterances continuously and forming a monologue, we allow it to send at most one utterance if the utterance contains any entity, and two utterances otherwise. When sending more than one utterance in a turn, the bot must wait for 1 to 2 seconds in between. In addition, after an utterance is generated by the model (almost instantly), the bot must hold on for some time to simulate message typing before sending. We used a typing speed of 7 chars / sec and added an additional random delay between 0 to 1.5s after \u2018typing\u2019. The rules are applied to all models."}, {"heading": "I Additional Human-Bot Dialogue", "text": "We show another set of human-bot/human chats in Table 8. In this scenario, the distribution of values are more uniform compared to Table 6. Nevertheless, we see that StanoNet and DynoNet\nstill learned to start from relatively high-frequency entities. They also appear more cooperative and mentions relevant entities in the dialogue context compared to Rule."}, {"heading": "J Histograms of Ratings from Human Evaluations", "text": "The histograms of ratings from partner and thirdparty evaluations is shown in Figure 5 and Figure 6 respectively. As these figures show, there are some obvious discrepancies between the ratings made by agents who chatted with the bot and those made by an \u2018objective\u2019 third party. These ratings provide some interesting insights into how dialogue participants in this task setting perceive their partners, and what constitutes a \u2018human-like\u2019 or a \u2018fluent\u2019 partner."}, {"heading": "K Example Comments from Partner and Third-party Evaluations", "text": "In Table 9, we show several pairs of ratings and comments on human-likeness for the same dialogue from both the partner evaluation and the third-party evaluation. As a conversation participant, the dialogue partner often judges from the cooperation and strategy perspective, whereas the third-party evaluator relies more on linguistic features (e.g., length, spelling, formality).\n1 2 3 4 5 0\n10\n20\n30\n40\n50\n60\nP e rc\ne n ta\ng e\nFluency\nHuman\nRule\nStanoNet\nDynoNet\n1 2 3 4 5\nCorrectness\nCooperation\nHuman-likeness\n1 2 3 4 5 0\n10\n20\n30\n40\n50\n60\n70\nP e rc\ne n ta\ng e\nFluency\nHuman\nRule\nStanoNet\nDynoNet\n1 2 3 4 5\nCorrectness\nCooperation\nHuman-likeness"}], "references": [{"title": "Developing a corpus of strategic conversation in the settlers of catan", "author": ["S. Afantenos", "N. Asher", "F. Benamara", "A. Cadilhac", "C. D\u00e9gremont", "P. Denis", "M. Guhe", "S. Keizer", "A. Lascarides", "O. Lemon", "P. Muller", "S. Paul", "V. Rieser", "L. Vieu."], "venue": "SeineDial 2012 -", "citeRegEx": "Afantenos et al\\.,? 2012", "shortCiteRegEx": "Afantenos et al\\.", "year": 2012}, {"title": "Frames: A corpus for adding memory to goaloriented dialogue systems", "author": ["L.E. Asri", "H. Schulz", "S. Sharma", "J. Zumer", "J. Harris", "E. Fine", "R. Mehrotra", "K. Suleman."], "venue": "Maluuba Technical Report .", "citeRegEx": "Asri et al\\.,? 2016", "shortCiteRegEx": "Asri et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["A. Bordes", "J. Weston."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bordes and Weston.,? 2017", "shortCiteRegEx": "Bordes and Weston.", "year": 2017}, {"title": "End-to-end reinforcement learning of dialogue agents for information access", "author": ["B. Dhingra", "L. Li", "X. Li", "J. Gao", "Y. Chen", "F. Ahmed", "L. Deng."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Dhingra et al\\.,? 2017", "shortCiteRegEx": "Dhingra et al\\.", "year": 2017}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer."], "venue": "Conference on Learning Theory (COLT).", "citeRegEx": "Duchi et al\\.,? 2010", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Tracking the world state with recurrent entity networks", "author": ["M. Henaff", "J. Weston", "A. Szlam", "A. Bordes", "Y. LeCun."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Henaff et al\\.,? 2017", "shortCiteRegEx": "Henaff et al\\.", "year": 2017}, {"title": "Dialogue act tagging for instant messaging chat sessions", "author": ["E. Ivanovic."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Ivanovic.,? 2005", "shortCiteRegEx": "Ivanovic.", "year": 2005}, {"title": "Data recombination for neural semantic parsing", "author": ["R. Jia", "P. Liang."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Evaluating persuasion strategies and deep reinforcement learning methods for negotiation dialogue agents", "author": ["S. Keizer", "M. Guhe", "H. Cuayahuitl", "I. Efstathiou", "K. Engelbrecht", "M. Dobre", "A. Lascarides", "O. Lemon."], "venue": "European Association for", "citeRegEx": "Keizer et al\\.,? 2017", "shortCiteRegEx": "Keizer et al\\.", "year": 2017}, {"title": "Globally coherent text generation with neural checklist models", "author": ["C. Kiddon", "L.S. Zettlemoyer", "Y. Choi."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Kiddon et al\\.,? 2016", "shortCiteRegEx": "Kiddon et al\\.", "year": 2016}, {"title": "A persona-based neural conversation model", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "B. Dolan."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Li et al\\.,? 2016a", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "W.B. Dolan."], "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).", "citeRegEx": "Li et al\\.,? 2016b", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "D. Jurafsky", "M. Galley", "J. Gao."], "venue": "Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Li et al\\.,? 2016c", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A user simulator for taskcompletion dialogues", "author": ["X. Li", "Z.C. Lipton", "B. Dhingra", "L. Li", "J. Gao", "Y. Chen."], "venue": "arXiv .", "citeRegEx": "Li et al\\.,? 2016d", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["C. Liu", "R. Lowe", "I.V. Serban", "M. Noseworthy", "L. Charlin", "J. Pineau."], "venue": "Empirical Methods in Natural Language", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Training End-to-End dialogue systems with the ubuntu dialogue corpus", "author": ["R.T. Lowe", "N. Pow", "I. Serban", "L. Charlin", "C. Liu", "J. Pineau."], "venue": "Dialogue and Discourse 8.", "citeRegEx": "Lowe et al\\.,? 2017", "shortCiteRegEx": "Lowe et al\\.", "year": 2017}, {"title": "What to talk about and how? selective generation using LSTMs with coarse-to-fine alignment", "author": ["H. Mei", "M. Bansal", "M.R. Walter."], "venue": "Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Coherent dialogue with attention-based language models", "author": ["H. Mei", "M. Bansal", "M.R. Walter."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI).", "citeRegEx": "Mei et al\\.,? 2017", "shortCiteRegEx": "Mei et al\\.", "year": 2017}, {"title": "Goal-driven answers in the Cards dialogue corpus", "author": ["C. Potts."], "venue": "Proceedings of the 30th West Coast Conference on Formal Linguistics.", "citeRegEx": "Potts.,? 2012", "shortCiteRegEx": "Potts.", "year": 2012}, {"title": "Multiresolution recurrent neural networks: An application to dialogue response generation", "author": ["I. Serban", "T. Klinger", "G. Tesauro", "K. Talamadupula", "B. Zhou", "Y. Bengio", "A.C. Courville."], "venue": "Association for the Advancement of Artificial Intelligence", "citeRegEx": "Serban et al\\.,? 2017a", "shortCiteRegEx": "Serban et al\\.", "year": 2017}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["I. Serban", "A. Sordoni", "R. Lowe", "L. Charlin", "J. Pineau", "A.C. Courville", "Y. Bengio."], "venue": "Association for the Advancement of Artificial Intelligence (AAAI).", "citeRegEx": "Serban et al\\.,? 2017b", "shortCiteRegEx": "Serban et al\\.", "year": 2017}, {"title": "A survey of available corpora for building data-driven dialogue systems", "author": ["I.V. Serban", "R. Lowe", "L. Charlin", "J. Pineau."], "venue": "arXiv preprint arXiv:1512.05742 .", "citeRegEx": "Serban et al\\.,? 2015a", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["I.V. Serban", "A. Sordoni", "Y. Bengio", "A. Courville", "J. Pineau."], "venue": "arXiv preprint arXiv:1507.04808 .", "citeRegEx": "Serban et al\\.,? 2015b", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["L. Shang", "Z. Lu", "H. Li."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["A. Sordoni", "M. Galley", "M. Auli", "C. Brockett", "Y. Ji", "M. Mitchell", "J. Nie", "J. Gao", "B. Dolan."], "venue": "North American Association for Computational Linguis-", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Continuously learning neural dialogue management", "author": ["P. Su", "M. Gasic", "N. Mrksic", "L.M. Rojas-Barahona", "S. Ultes", "D. Vandyke", "T. Wen", "S.J. Young."], "venue": "arXiv preprint arXiv:1606.02689 .", "citeRegEx": "Su et al\\.,? 2016", "shortCiteRegEx": "Su et al\\.", "year": 2016}, {"title": "Emergence of gricean maxims from multi-agent decision theory", "author": ["A. Vogel", "M. Bodoia", "C. Potts", "D. Jurafsky."], "venue": "North American Association for Computational Linguistics (NAACL). pages 1072\u2013 1081.", "citeRegEx": "Vogel et al\\.,? 2013", "shortCiteRegEx": "Vogel et al\\.", "year": 2013}, {"title": "A network-based end-to-end trainable task-oriented dialogue system", "author": ["T. Wen", "M. Gasic", "N. Mrksic", "L.M. Rojas-Barahona", "P. Su", "S. Ultes", "D. Vandyke", "S. Young."], "venue": "European Association for Computational Linguistics (EACL).", "citeRegEx": "Wen et al\\.,? 2017", "shortCiteRegEx": "Wen et al\\.", "year": 2017}, {"title": "Hybrid code networks: Practical and efficient end-toend dialog control with supervised and reinforcement learning", "author": ["J.D. Williams", "K. Asadi", "G. Zweig."], "venue": "Association for Computational Linguistics (ACL).", "citeRegEx": "Williams et al\\.,? 2017", "shortCiteRegEx": "Williams et al\\.", "year": 2017}, {"title": "The dialog state tracking challenge series: A review", "author": ["J.D. Williams", "A. Raux", "M. Henderson."], "venue": "Dialogue and Discourse 7.", "citeRegEx": "Williams et al\\.,? 2016", "shortCiteRegEx": "Williams et al\\.", "year": 2016}, {"title": "Partially observable Markov decision processes for spoken dialog systems", "author": ["J.D. Williams", "S. Young."], "venue": "Computer Speech & Language 21(2):393\u2013 422.", "citeRegEx": "Williams and Young.,? 2007", "shortCiteRegEx": "Williams and Young.", "year": 2007}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["S. Young", "M. Gasic", "B. Thomson", "J.D. Williams."], "venue": "Proceedings of the IEEE 101(5):1160\u20131179.", "citeRegEx": "Young et al\\.,? 2013", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 32, "context": "Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.", "startOffset": 39, "endOffset": 99}, {"referenceID": 28, "context": "Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.", "startOffset": 39, "endOffset": 99}, {"referenceID": 4, "context": "Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.", "startOffset": 39, "endOffset": 99}, {"referenceID": 24, "context": "Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance.", "startOffset": 31, "endOffset": 150}, {"referenceID": 23, "context": "Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance.", "startOffset": 31, "endOffset": 150}, {"referenceID": 25, "context": "Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance.", "startOffset": 31, "endOffset": 150}, {"referenceID": 11, "context": "Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance.", "startOffset": 31, "endOffset": 150}, {"referenceID": 16, "context": "Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance.", "startOffset": 31, "endOffset": 150}, {"referenceID": 18, "context": "Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance.", "startOffset": 31, "endOffset": 150}, {"referenceID": 6, "context": "Our model is similar to EntNet (Henaff et al., 2017) in that node/entity embeddings are updated recurrently given new utterances.", "startOffset": 31, "endOffset": 52}, {"referenceID": 2, "context": "An attention-based mechanism (Bahdanau et al., 2015) over the node embeddings drives generation of new utterances.", "startOffset": 29, "endOffset": 52}, {"referenceID": 28, "context": ", 2016b,c), we also conduct partner evaluation (Wen et al., 2017) where AMT workers rate their conversational partners (other workers or our models) based on fluency, correctness, cooperation, and human-likeness.", "startOffset": 47, "endOffset": 65}, {"referenceID": 7, "context": ", italic utterances in Figure 1), a common characteristic of online chat (Ivanovic, 2005).", "startOffset": 73, "endOffset": 89}, {"referenceID": 2, "context": "We compute the weights through standard attention mechanism (Bahdanau et al., 2015):", "startOffset": 60, "endOffset": 83}, {"referenceID": 8, "context": "Finally, we define a distribution over both words in the vocabulary and nodes in Gt using the copying mechanism of Jia and Liang (2016):", "startOffset": 115, "endOffset": 136}, {"referenceID": 5, "context": "The parameters are optimized by AdaGrad (Duchi et al., 2010) with an initial learning rate of 0.", "startOffset": 40, "endOffset": 60}, {"referenceID": 12, "context": "Compared to Rule, the neural models tend to generate shorter utterances (Li et al., 2016b; Serban et al., 2017b).", "startOffset": 72, "endOffset": 112}, {"referenceID": 21, "context": "Compared to Rule, the neural models tend to generate shorter utterances (Li et al., 2016b; Serban et al., 2017b).", "startOffset": 72, "endOffset": 112}, {"referenceID": 22, "context": "There has been a recent surge of interest in end-to-end task-oriented dialogue systems, though progress has been limited by the size of available datasets (Serban et al., 2015a).", "startOffset": 155, "endOffset": 177}, {"referenceID": 30, "context": "Most work focuses on information-querying tasks, using Wizard-ofOz data collection (Williams et al., 2016; Asri et al., 2016) or simulators (Bordes and Weston, 2017; Li et al.", "startOffset": 83, "endOffset": 125}, {"referenceID": 1, "context": "Most work focuses on information-querying tasks, using Wizard-ofOz data collection (Williams et al., 2016; Asri et al., 2016) or simulators (Bordes and Weston, 2017; Li et al.", "startOffset": 83, "endOffset": 125}, {"referenceID": 3, "context": ", 2016) or simulators (Bordes and Weston, 2017; Li et al., 2016d), In contrast, collaborative dialogues are easy to collect as natural human conversations, and are also challenging enough given the large number of scenarios and diverse conversation phenomena.", "startOffset": 22, "endOffset": 65}, {"referenceID": 14, "context": ", 2016) or simulators (Bordes and Weston, 2017; Li et al., 2016d), In contrast, collaborative dialogues are easy to collect as natural human conversations, and are also challenging enough given the large number of scenarios and diverse conversation phenomena.", "startOffset": 22, "endOffset": 65}, {"referenceID": 0, "context": "There are some interesting strategic dialogue datasets\u2014settlers of Catan (Afantenos et al., 2012) (2K turns) and the cards corpus (Potts, 2012) (1.", "startOffset": 73, "endOffset": 97}, {"referenceID": 19, "context": ", 2012) (2K turns) and the cards corpus (Potts, 2012) (1.", "startOffset": 40, "endOffset": 53}, {"referenceID": 9, "context": "3K dialogues), as well as work on dialogue strategies (Keizer et al., 2017; Vogel et al., 2013), though no full dialogue system has been built for these datasets.", "startOffset": 54, "endOffset": 95}, {"referenceID": 27, "context": "3K dialogues), as well as work on dialogue strategies (Keizer et al., 2017; Vogel et al., 2013), though no full dialogue system has been built for these datasets.", "startOffset": 54, "endOffset": 95}, {"referenceID": 31, "context": "Most task-oriented dialogue systems follow the POMDP-based approach (Williams and Young, 2007; Young et al., 2013).", "startOffset": 68, "endOffset": 114}, {"referenceID": 32, "context": "Most task-oriented dialogue systems follow the POMDP-based approach (Williams and Young, 2007; Young et al., 2013).", "startOffset": 68, "endOffset": 114}, {"referenceID": 28, "context": "Despite their success (Wen et al., 2017; Dhingra et al., 2017; Su et al., 2016), the requirement for handcrafted slots limits their scalability to new domains and burdens data collection with extra state labeling.", "startOffset": 22, "endOffset": 79}, {"referenceID": 4, "context": "Despite their success (Wen et al., 2017; Dhingra et al., 2017; Su et al., 2016), the requirement for handcrafted slots limits their scalability to new domains and burdens data collection with extra state labeling.", "startOffset": 22, "endOffset": 79}, {"referenceID": 26, "context": "Despite their success (Wen et al., 2017; Dhingra et al., 2017; Su et al., 2016), the requirement for handcrafted slots limits their scalability to new domains and burdens data collection with extra state labeling.", "startOffset": 22, "endOffset": 79}, {"referenceID": 6, "context": "Our network architecture is most similar to EntNet (Henaff et al., 2017), where memories are also updated by input sentences recurrently.", "startOffset": 51, "endOffset": 72}, {"referenceID": 3, "context": "To go past this limit, Bordes and Weston (2017) proposed a Memory-Networks-based approach without domain-specific features.", "startOffset": 23, "endOffset": 48}, {"referenceID": 3, "context": "To go past this limit, Bordes and Weston (2017) proposed a Memory-Networks-based approach without domain-specific features. However, the memory is unstructured and interfacing with KBs relies on API calls, whereas our model embeds both the dialogue history and the KB structurally. Williams et al. (2017) use an LSTM to automatically infer the dialogue state, but as they focus on dialogue control rather than the full problem, the response is modeled as a templated action, which restricts the generation of richer utterances.", "startOffset": 23, "endOffset": 305}, {"referenceID": 17, "context": "Our work is also related to language generation conditioned on knowledge bases (Mei et al., 2016; Kiddon et al., 2016).", "startOffset": 79, "endOffset": 118}, {"referenceID": 10, "context": "Our work is also related to language generation conditioned on knowledge bases (Mei et al., 2016; Kiddon et al., 2016).", "startOffset": 79, "endOffset": 118}, {"referenceID": 20, "context": ", attending to items or attributes first, then selecting entities; generating high-level concepts before composing them to natural tokens (Serban et al., 2017a).", "startOffset": 138, "endOffset": 160}], "year": 2017, "abstractText": "We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.", "creator": "LaTeX with hyperref package"}}}