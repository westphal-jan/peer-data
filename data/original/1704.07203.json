{"id": "1704.07203", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "What is the Essence of a Claim? Cross-Domain Claim Identification", "abstract": "Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art feature-rich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent perception of claims in different datasets is indeed harmful to cross-domain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.", "histories": [["v1", "Mon, 24 Apr 2017 13:13:30 GMT  (40kb)", "http://arxiv.org/abs/1704.07203v1", "Under review"], ["v2", "Wed, 5 Jul 2017 07:25:36 GMT  (40kb)", "http://arxiv.org/abs/1704.07203v2", "To be published at EMNLP 2017"], ["v3", "Wed, 13 Sep 2017 10:22:33 GMT  (44kb,D)", "http://arxiv.org/abs/1704.07203v3", "Published at EMNLP 2017:this http URL"]], "COMMENTS": "Under review", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["johannes daxenberger", "steffen eger", "ivan habernal", "christian stab", "iryna gurevych"], "accepted": true, "id": "1704.07203"}, "pdf": {"name": "1704.07203.pdf", "metadata": {"source": "CRF", "title": "What is the Essence of a Claim? Cross-Domain Claim Identification", "authors": ["Johannes Daxenberger", "Steffen Eger", "Ivan Habernal", "Christian Stab", "Iryna Gurevych"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n07 20\n3v 1\n[ cs\n.C L\n] 2\n4 A\npr 2\n01 7\nresearch area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art featurerich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent perception of claims in different datasets is indeed harmful to crossdomain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps."}, {"heading": "1 Introduction", "text": "The key component of an argument is the claim. This simple observation has not changed much since the early works on argumentation by Aristotle more than two thousand years ago, although argumentation scholars provide us with a plethora of often clashing theories and models (van Eemeren et al., 2014). Despite the lack of a precise definition in the contemporary argumentation theory, Toulmin\u2019s influential work on argumentation in the 1950\u2019s introduced a claim as an \u2018assertion that deserves our attention\u2019 (Toulmin, 2003, p. 11); recent works describe a claim as \u2018a statement that is in dispute and that we are trying to support with reasons\u2019 (Govier, 2010).\nArgument mining, a computational counterpart of manual argumentation analysis, is a recent\ngrowing sub-field of NLP (Peldszus and Stede, 2013a). \u2018Mining\u2019 arguments usually involves several steps like separating argumentative from nonargumentative text units, parsing argument structures, and recognizing argument components such as claims\u2014the main focus of this article. Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012).\nAlthough claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017). Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (Mochales-Palau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2016). The problem of generalizing systems or features and their robustness across heterogeneous datasets thus remains fairly unexplored.\nThis situation motivated us to perform a detailed analysis of the concept of claims (as a key component of an argument) in existing argument mining datasets from different domains.1 We first review and qualitatively analyze six existing publicly available datasets for argument mining (\u00a73), showing that the conceptualizations of claims in these datasets differ largely. In a next step, we analyze the influence of these differences for crossdomain claim identification. We propose several computational models for claim identification, including systems using linguistically motivated\n1We take the machine learning perspective in which different domains mean data drawn from different distributions (Murphy, 2012, p. 297).\nfeatures (\u00a74.1) and recent deep neural networks (\u00a74.2), and rigorously evaluate them on and across all datasets (\u00a75). Finally, in order to better understand the factors influencing the performance in a cross-domain scenario, we perform an extensive quantitative analysis on the results (\u00a76).\nOur analysis reveals that despite obvious differences in conceptualizations of claims across datasets, there are some shared properties on the lexical level which can be useful for claim identification in heterogeneous or unknown domains. Furthermore, we found that the choice of the source (training) domain is crucial when the target domain is unknown and that it is preferable to train on a suitable single domain rather than mixed domains. We release our experimental framework to help other researchers build upon our findings."}, {"heading": "2 Related Work", "text": "Existing approaches to argument mining can be roughly categorized into (a) multi-document approaches which recognize claims and evidence across several documents and (b) discourse level approaches addressing the argumentative structure within a single document. Multi-document approaches have been proposed e.g. by Levy et al. (2014) and Rinott et al. (2015) for mining claims and corresponding evidence for a predefined topic over multiple Wikipedia articles. Nevertheless, to date most approaches and datasets deal with single-document argumentative discourse. This paper takes the discourse level perspective, as we aim to assess multiple datasets from different authors and compare their notion of claims.\nMochales-Palau and Moens (2009) experimented at the discourse level using feature-rich SVM and a hand-crafted context-free grammar in order to recognize claims and premises in legal decisions. Their best results for claims achieved 74.1% F1 using domain-dependent key phrases, token counts, location features, information about verbs, and the tense of the sentence. Peldszus and Stede (2015) presented an approach based on a minimum spanning tree algorithm and model the global structure of arguments considering argumentative relations, the stance and the function of argument components. Their approach yields 86.9% F1 for recognizing claims in English \u2018microtexts\u2019. Habernal and Gurevych (2017) cast argument component identification as BIO sequence labeling and jointly model separation of argumen-\ntative from non-argumentative text units and identification of argument component boundaries together with their types. They achieved 25.1% macro F1 with a combination of topic, sentiment, semantic, discourse and embedding features using structural SVM. Stab and Gurevych (2014) identified claims and other argument components in student essays. They experimented with several classifiers and achieved the best performance of 53.8% F1 score using SVM with structural, lexical, syntactic, indicator and contextual features. Although the above-mentioned approaches achieved promising results in particular domains, their ability to generalize over heterogeneous text types and domains remains unanswered.\nRosenthal and McKeown (2012) set out to explore this direction by conducting cross-domain experiments for detecting claims in blog articles from LiveJournal and discussions taken from Wikipedia. However, they focused on relatively similar datasets that both stem from the social media domain and in addition annotated the datasets themselves, leading to an identical conceptualization of the notion of claim. Although AlKhatib et al. (2016) also deal with cross-domain experiments, they address a different task; namely identification of argumentative sentences. Further, their goals are different: they want to improve argumentation mining via distant supervision rather than detecting differences in the notions of a claim.\nDomain adaptation techniques (Daume III, 2007) try to address the frequently observed drop in classifier performances entailed by a dissimilarity of training and test data distributions. Since techniques such as learning generalized crossdomain representations in an unsupervised manner (Blitzer et al., 2006; Pan et al., 2010; Glorot et al., 2011; Yang and Eisenstein, 2015) have been criticized for targeting specific source and target domains, it has alternatively been proposed to learn universal representations from general domains in order to render a learner robust across all possible domain shifts (Mu\u0308ller and Schu\u0308tze, 2015; Schnabel and Schu\u0308tze, 2013). Our approach is in a similar vein. On the one hand, rather than trying to improve classifier performances for a specific source-target domain pair, we want to detect differences between these pairs. On the other hand, we are looking for universal feature sets or classifiers that perform generally well for claim identi-\nfication across varying source and target domains."}, {"heading": "3 Claim Identification in Computational Argumentation", "text": "We briefly describe six English datasets used in our empirical studies; they all capture \u2018claims\u2019 on the discourse level. Table 1 summarizes statistics relevant to claim identification."}, {"heading": "3.1 Datasets", "text": "The AraucariaDB corpus (Reed et al., 2008) includes various genres (VG) such as newspaper editorials, parliamentary records, or judicial summaries. The annotation scheme structures arguments as trees and distinguishes between claims and premises at the clause level. Although the reliability of the annotations is unknown, the corpus has been extensively used in argument mining (Moens et al., 2007; Feng and Hirst, 2011; Rooney et al., 2012).\nThe corpus from Habernal and Gurevych (2017) includes user-generated web discourse (WD) such as blog posts, or user comments annotated (\u03b1U 0.48) with claims and premises as well as backings, rebuttals and refutations inspired by Toulmin\u2019s model of argument (Toulmin, 2003).\nThe persuasive essay (PE) corpus (Stab and Gurevych, 2016) includes 402 student essays. The scheme comprises major claims, claims and premises at the clause level (\u03b1U 0.77). The corpus has been extensively used in the argument mining community (Persing and Ng, 2015; Lippi and Torroni, 2015; Nguyen and Litman, 2016).\nBiran and Rambow (2011a) annotated claims and premises in online comments (OC) from blog threads of LiveJournal (\u03ba 0.69). In a subsequent work, Biran and Rambow (2011b) applied their annotation scheme to documents from Wikipedia talk pages (WTP) and annotated 118 threads. For our experiments, we consider each user comment in both corpora as a document, which yields 2,805 documents in the OC corpus and 1,985 documents in the WTP corpus.\nPeldszus and Stede (2016) created a corpus of German microtexts (MT) of controlled linguistic and rhetoric complexity. Each document includes a single argument and does not exceed five argument components. The scheme models the argument structure and distinguishes between premises and claims, among other properties (such as proponent/opponent or normal/example). In the first\nannotation study, 26 untrained annotators annotated 23 microtexts in a classroom experiment (\u03ba 0.38) (Peldszus and Stede, 2013b). In a subsequent work, the corpus was largely extended by expert annotators (\u03ba 0.83). Recently, they translated the corpus to English, resulting in the first parallel corpus in computational argumentation; our experiments rely on the English version."}, {"heading": "3.2 Qualitative Analysis of Claims", "text": "In order to investigate how claim annotations are tackled in the chosen corpora, one co-author manually analyzed 50 randomly sampled claims from each corpus.2 The observed characteristics are backed by argumentation theory (Schiappa and Nordin, 2013) and include among other things the claim type, signaling words and discourse markers.\nBiran and Rambow (2011b) do not back-up their claim annotations by any common argumentation theory but rather state that claims are utterances which convey subjective information and anticipate the question \u2018why are you telling me that?\u2019 and need to be supported by justifications. Using this rather loose definition, a claim might be any subjective statement that is justified by the author. Detailed examination of the LiveJournal corpus (OC) revealed that sentences with claims are extremely noisy. Their content ranges from a single word, to emotional expressions of personal regret, to general Web-chat nonsense or posts without any clear argumentative purpose. The Wikipedia Talk Page corpus (WTP) contains claims typical to Wikipedia quality discussions and policy claims (Schiappa and Nordin, 2013) are present as well. However, a small number of nonsensical claims remains.\nAnalysis of the MT dataset revealed that about a half of claim sentences contain the modal verb \u2018should\u2019, clearly indicating policy claims. Such statements also very explicitly express the stance on the controversial topic of interest.\nIn a similar vein, claims in persuasive students\u2019 essays (PE) heavily rely on phrases signaling beliefs or argumentative discourse connectors whose usage is recommended in textbooks on essay writing. Most claims are value/policy claims written in the present tense.\nThe mixture of genres in the Araucaria cor-\n2The supplementary material to this submission contains textual examples illustrating the analysis presented here.\npus (VG) is reflected in the variety of claims. While some are simple statements starting with a discourse marker, there are many legal-specific claims requiring expert knowledge, reported and direct speech claims, and several nonsensical claims which undercuts the consistency of this dataset.\nThe web-discourse (WD) claims take a clear stance to the relevant controversy, yet sometimes anaphoric, however the usage of discourse markers is seldom. Habernal and Gurevych (2017) investigated hedging in claims and found out that it varies with respect to the topic being discussed (10% up to 35% of claims are hedged). Sarcasm or rhetorical question are also common.\nThese observations make clear that annotating claims\u2014the central part of all arguments, as claimed by the majority of argumentation scholars\u2014can be approached very differently when it comes to actual empirical, data-driven operationalization. While some common traits are shared, such as that claims usually need some support to make up a \u2018full\u2019 argument (e.g., premises, evidence, or justifications), the exact definition of a claim can be arbitrary\u2014depending on the domain, register, or task."}, {"heading": "4 Methodology", "text": "Given the results from the qualitative analysis, we want to investigate whether the different conceptualizations of claims can be assessed empirically and if so, how they could be dealt with in practice. Put simply, the task we are trying to solve in the following is: given a sentence, classify whether or not it contains a claim. We opted to model the claim identification task on sentence level, as this is the only way to make all datasets compatible to each other. Different datasets model claim boundaries differently, e.g. MT includes discourse markers with the same sentence, whereas they are excluded in PE.\nAll six datasets described in the previous sec-\ntion have been preprocessed by first segmenting documents into sentences using Stanford CoreNLP (Manning et al., 2014) and then annotating every sentence as claim, if one or more tokens within the sentence were labeled as claim (or major claim in PE). Analogously, each sentence is annotated as non-claim, if none of its tokens were labeled as claim (or major claim). Although our basic units of interest are sentences, we keep the content of the entire document to be able to retrieve information about the context of (non-)claims.3\nWe are not interested in optimizing the properties of a certain learner for this task, but rather want to compare the influence of different types of lexical, syntactical, and other kinds of information across datasets.4 Thus, we used a limited set of learners for our task: a) a standard L2-regularized logistic regression approach with manually defined feature sets5, and b) several deep learning approaches, using state-of-the-art neural network architectures. Regularized logistic regression is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016).\nThe in-domain experiments were carried out in a 10-fold cross-validation fashion with fixed splits into training and test data. As for the crossdomain experiments, we train on the entire data of the source domain and test on the entire data of the target domain. In the domain adaptation terminology, this corresponds to an unsupervised setting.\nTo address class-imbalance in our datasets (see Table 1), we downsample the negative class (nonclaim) both in-domain and cross-domain, so that positive and negative class occur approximately in\n3This is true only for the feature-based learners. The neural networks do not have access to information beyond individual sentences.\n4For the same reason, we do not optimize any hyperparameters for individual learners, unless explicitly stated.\n5Using the liblinear library (Fan et al., 2008).\nan 1:1 ratio in the training data. Since this means that we discard a lot of useful information (many negative instances), we repeat this procedure 20 times, in each case randomly discarding instances of the negative class such that the required ratio is obtained. At test time, we use the majority prediction of this ensemble of 20 trained models. With the exception of very few cases, this led to consistent performance improvements across all experiments. The systems are described in more detail in the following subsections. Additionally, we report the results of two baselines. The majority baseline labels all sentences as non-claims (predominant class in all datasets), the random baseline labels sentences as claims with 0.5 probability."}, {"heading": "4.1 Linguistically Motivated Features", "text": "For the logistic regression-based experiments (LR) we employed the following feature groups. Structure Features capture the position, the length and the punctuation of a sentence. Lexical Features are lowercased unigrams. Syntax Features account for grammatical information at the sentence level. We include information about the part-of-speech and parse tree for each sentence. Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014). We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article. The full set of features and their parameters are described in the supplementary material to this article. We experiment with the full feature set, individual feature groups, and feature ablation (all features except for one group)."}, {"heading": "4.2 Deep Learning Approaches", "text": "As alternatives to our feature-based systems, we consider three deep learning approaches. The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown to perform excellently on many diverse classification tasks such as sentiment analysis and question classification and is still a strong competitor among neural techniques focusing on sentence classification (Komninos and Manandhar, 2016; Zhang et al., 2016b,c). We con-\nsider two variants of Kim\u2019s CNN, one in which words\u2019 vectors are initialized with pre-trained GoogleNews word embeddings (CNN-w2vec) and one in which the vectors are randomly initialized and updated during training (CNN-rand). Our second model is an LSTM (long short-term memory) neural net for sentence classification (LSTM) and our third model is a bidirectional LSTM (BiLSTM).\nFor all neural network classifiers, we use default hyperparameters concerning hidden dimensionalities (for the two LSTM models), number of filters (for the convolutional neural net), and others. We train each of the three neural networks for 15 iterations and, for regularization, choose in each case the learned model that performs best on a heldout development set of roughly 10% of the training data as the model to apply to unseen test data. This corresponds to an early stopping regularization scheme."}, {"heading": "5 Results", "text": "In the following, we summarize the results of the various learners described above. Obtaining all results required heavy computation, e.g. the crossvalidation experiments for feature-based systems took 56 days of computing. We intentionally do not list the results of previous work on those datasets. The scores are not comparable since we strictly work on sentence level (rather than e.g. clause level) and applied downsampling to the training data. All reported significance tests were conduced using two-tailed Wilcoxon Signed-Rank Test (Japkowicz and Shah, 2014)."}, {"heading": "5.1 In-Domain Experiments", "text": "The performance of the learners is quite divergent across datasets, with Macro-F1 scores 6 ranging from 60% (WTP) to 80% (MT), average 67% (see Table 2). On all datasets, our best systems clearly outperform both baselines. In isolation, lexical, embedding, and syntax features are most helpful, whereas structural features did not help in most cases. Discourse features only contribute significantly on MT. When looking at the performance of the feature-based approaches, the most striking finding is the importance of lexical (in our setup, unigram) information.\nThe average performances of LR\u2212syntax and CNN-rand are virtually identical, both for Macro-\n6Described as FscoreM in (Sokolova and Lapalme, 2009).\nF1 and Claim-F1, with a slight advantage for the feature-based approach, but their difference is not statistically significant (p \u2264 0.05). Altogether, these two systems exhibit significantly better average performances than all other models surveyed here, both those relying on and those not relying on hand-crafted features (p \u2264 0.05). The absence or the different nature of inter-annotator agreement measures for all datasets prevent us from searching for correlations between agreement and performance. But we observed that the systems yield better results on PE (\u03b1U 0.77) and MT (\u03ba 0.83)."}, {"heading": "5.2 Cross-Domain Experiments", "text": "For all six datasets, training on different sources resulted in a performance drop. Table 3 lists the results of the best feature-based (LR All features) and deep learning (CNN-rand) systems, as well as single feature groups (averages over all domains). We note the biggest performance drops on the datasets which performed best in the indomain setting (MT and PE). For the lowest scoring datasets, OC andWTP, the differences are only marginal when trained on a suitable dataset (VG and OC, respectively). The best feature-based approach outperforms the best deep learning approach in most scenarios. In particular, as opposed to the in-domain experiments, the difference of the\nClaim-F1 measure between the feature-based approaches and the deep learning approaches is striking. In the feature-based approaches, on average, a combination of all features yields the best results for both Macro-F1 and Claim-F1. When comparing single features, lexical ones do the best job.\nLooking at the best overall system (LR with all features), the average test results when training on different source datasets are between 54% Macro-F1 resp. 23% Claim-F1 (both MT) and 58% (VG) resp. 34% (OC). Depending on the goal that should be achieved, training on VG (highest average Macro-F1) or OC (highest average Claim-F1) seems to be the best choice when the domain of test data is unknown. MT clearly gives the best results as target domain, followed by PE and VG.\nWe also performed experiments with mixed sources. We did this in a leave-one-domain-out fashion, in particular we trained on all but one datasets and tested on the remaining one. In this scenario, the neural network-based learners seem to benefit from the increased amount of training data and thus gave the best results on average; however, on average this approach does not yield benefits over training on good (single) source training datasets.7\n7Leave-one-domain-out results, as well as the full crossdomain results, are reported in the supplementary material."}, {"heading": "6 Cross-Domain Claim Identification", "text": "To better understand which factors influence cross-domain performance of the systems we tested, we considered the following variables as potential determinants of outcome: similarity between source and target domain, the source domain itself, training data size, and the ratio between claims and non-claims.\nWe calculated the Spearman correlation of the top-500 lemmas between the datasets in each direction, see results in Table 4. The most similar\ndomains are OC (source s) and WTP (target t), coming from the same authors. OC (s) and WD (t) as well OC (s) and VG (t) are also highly correlated. For a statistical test of potential correlations between cross-domain performances and the introduced variables, we regress the cross-domain results (Table 3) on Table 4 (T4 in the following equation), on the number of claims #C (directly related to training data size in our experiments, effect of downsampling), and on the ratio of claims to non-claims R.8 More precisely, given source/training data and target data pairs (s, t) in Table 3, we estimate the linear regression model\nyst = \u03b1 \u00b7T4st +\u03b2 \u00b7 log(#Cs)+ \u03b3 \u00b7Rt + \u03b5st, (1)\nwhere yst denotes the F1 score when training on s and testing on t. In the regression, we also include binary dummy variables 1\u03c3 = 1s,\u03c3 for each domain \u03c3 whose value is 1 if s = \u03c3 (and 0 otherwise). These help us identify \u201cgood\u201d source domains.\n8Overall, we had 15 different systems, see upper 15 cells in Table 2. Therefore, we had 15 different regression models.\nThe coefficient \u03b1 for Table 4 is not statistically significantly different from zero in any case. Ultimately, this means that it is difficult to predict cross-domain performance from lexical similarity of the datasets. This is in contrast to e.g., POS tagging, where lexical similarity has been reported to predict cross-domain performance very well (Van Asch and Daelemans, 2010). The coefficient for training data size \u03b2 is statistically significantly different from zero in three out of 15 cases. In particular, it is significantly positive in two (CNN-rand, CNN-w2vec) out of four cases for the neural networks. This indicates that the neural networks would have particularly benefited from more training data, which is confirmed by the improved performance of the neural networks in the mixed sources experiments (cf. \u00a75.2). The ratio of claims to non-claims in t is among the best predictors for the variables considered here (coefficient \u03b3 is significant in three out of 15 cases, but consistently positive). This is probably due to our decision to balance training data (downsampling non-claims) to keep the assessment of claim identification realistic for real-world applications, where the class ratio of t is unknown. Our systems are thus inherently biased towards a higher claim ratio.\nFinally, the dummy variables for OC and VG are three times significantly positive, but consistently positive overall. Their average coefficient is 2.31 and 1.90, respectively, while the average coefficients for all other source datasets is negative, and not significant in most cases. Thus, even when controlling for all other factors such as training data size and the different claim ratios of target domains, OC and VG are the best source domains for cross-domain claim classification in our experiments. OC and VG are particularly good training sources for the detection of claims (as opposed to non-claims)\u2014the minority class in all datasets\u2014 as indicated by the average Claim-F1 scores in Table 3.\nOne finding that was confirmed both in-domain as well as cross-domain was the importance of lexical features as compared to other feature groups. As mere lexical similarity between domains does not explain performance (cf. coefficient \u03b1 above), this finding indicated that the learners relied on a few, but important lexical clues. To go more into depth, we carried out error analysis on the CNNrand cross-domain results. We used OC, VG and\nPE as source domains, and MT and WTP as target domains. By examining examples in which a model trained on OC and VG made correct predictions as opposed to a model trained on PE, we quickly noticed that lexical indicators indeed played a crucial role. In particular, the occurrence of the word \u201cshould\u201d (and to a lower degree: \u201cwould\u201d, \u201carticle\u201d, \u201cone\u201d) are helpful for the detection of claims across various datasets. In MT, a simple baseline labeling every sentence containing \u201cshould\u201d as claim achieves 76.1 Macro-F1 (just slightly below the best in-domain system on this dataset). In the other datasets, this phenomenon is far less dominant, but still observable. We conclude that a few rather simple rules (learned by models trained on OC and VG, but not by potentially more complex models trained on PE) make a big difference in the cross-domain setting."}, {"heading": "7 Conclusion", "text": "In a rigorous empirical assessment of different machine learning systems, we compared how six datasets model claims as the fundamental component of an argument. The varying performance of the tested in-domain systems reflects different notions of claims also observed in a qualitative study of claims across the domains. Our results reveal that the best in-domain system is not necessarily the best system in environments where the test domain is unknown. Particularly, we found that two rather noisy datasets (OC and VG) were the best candidates as source domains in the cross-domain setup. The reason for this seem to be a few important lexical indicators (like the word \u201cshould\u201d) which are learned easier in those domains. In summary, as for the six datasets we investigated here, our analysis shows that the essence of a claim is not much more than a few lexical clues.\nFrom this, we conclude that future work should address the problem of fuzzy conceptualization of claim. A more consistent notion of claim as the central component of an argument, which also holds across domains, would potentially not just benefit cross-domain claim identification, but also higher-level applications relying on argumentation mining (Wachsmuth et al., 2017). To further overcome the problem of domain dependence, multitask learning (S\u00f8gaard and Goldberg, 2016) is a framework that could be explored in the argumentation mining context (Eger et al., 2017)."}, {"heading": "Supplementary Material", "text": ""}, {"heading": "Experimental Setup: Detailed Description of Hand-Crafted Features", "text": "This part of the supplementary material describes the hand-crafted features we used in more detail.\nStructure Features: Structure features capture the position, the length and the punctuation of a sentence. First, we define two binary features which indicate if the current sentence is the first or last sentence in the paragraph in which it is contained. These feature are motivated by the findings of Stab and Gurevych (2016) who found that structural properties of argument components are effective for distinguishing the argumentative function of argument components. In addition, Peldszus and Stede (2016) found that 43% of claims appear in the first sentence. Second, we add the number of tokens of the sentence to our feature set which proved to be indicative for identifying argumentatively relevant sentences (Biran and Rambow, 2011a; Moens et al., 2007). Finally, we adopted the punctuation features from Mochales-Palau and Moens (2009).\nLexical Features: As lexical features, we employ lowercased unigrams. We assume that these features are helpful for detecting claims since they capture discourse connectors like \u201ctherefore\u201d, \u201cthus\u201d, or \u201chence\u201d which frequently signal the presence of claims. We consider the most frequent 4,000 unigrams as binary features.\nSyntactic Features: To account for grammatical information at the sentence level, we include information about the part-of-speech and parse tree for each sentence. Following Stab and Gurevych (2016), we add binary POS n-grams (the 2000 most frequent, 2 \u2264 n \u2264 4) and constituent parse tree production rules (4000 most frequent, minimum occurrence 5) as originally suggested by Lin et al. (2009). Additionally, to account for the frequency of POS tags, we include a feature which counts the occurrence of each part-of-speech per sentence.\nDiscourse Features: Cabrio et al. (2013) suggested that the relation between parts of discourse (e.g. connectives such as \u201cbecause\u201d) can be helpful to determine argumentative content. As this finding is affirmed by Stab and Gurevych (2016),\nwe include discourse features with the help of the Penn Discourse Treebank (PDTB) styled end-toend discourse parser as presented by Lin et al. (2014). We include discourse relations extracted from the parser output as a triple of i) the type of relation, ii) whether the relation is implicit or explicit, and iii) whether the current sentence is part of the first or the second discourse argument (or both).\nEmbedding Features: We represent each sentence as a summation of its word embeddings (Guo et al., 2014). These simple yet powerful latent semantic representation features have been found predictive in related work (Habernal and Gurevych, 2015, 2017). In particular, we use pretrained 300 dimensional GoogleNews word embeddings.9\nWe further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article.\nQualitative analysis of claims: Examples\nThis part of the supplementary material contains exemplary claims from each of the datasets we experimented with, to illustrate the obversations described in Section 3.2 of the main paper."}, {"heading": "Biran and Rambow (2011b)", "text": "#OC1\n\u201cBastard.\u201d\n\u201cBeautiful.\u201d\n#OC2\n\u201c::hugs:: i am so sorry hon ..\u201d\n\u201cI think that everything will be ok ...\u201d\n#OC3\n\u201cW-wow... that\u2019s a wicked awesome picture... looks like something from Pirates of the Caribbean...gone Victorian ...lolz.\u201d\n\u201cCece and I have been taking voice/singing lessons for a while now: She because she is in a chorus, and me just for fun.\u201d\n#OC4\n\u201cwhat i did with it was make this recipe for a sort of casserole/stratta (i made this up, here is the recipe) [...] butter, 4 eggs, salt, pepper, sauted onions and cabbage..add as much as you\n9https://code.google.com/archive/p/word2vec/\nwant bake for 1 hour at 350 it was seriously delicious!\u201d). #WTP1\n(\u201cThat is why this article has NPOV issues.\u201d \u201cThis is the first time such a demand has been\nmade, and it is utterly unreasonable.\u201d,\n\u201cOk, I added two pictures, each of which has a\npurpose.\u201d #WTP2\n\u201cI think the gallery should be got rid of alto-\ngether.\u201d\n\u201cI really think we should take out that para-\ngraph.\u201d) #WTP3\n\u201cA dot.\u201d"}, {"heading": "Peldszus and Stede (2015)", "text": "#MT1\n\u201cTuition fees should not generally be charged\nby universities.\u201d\n\u201cSchool uniforms should not be worn in our\nschools.\u201d.\n\u201cThe death penalty should be abandoned every-\nwhere.\u201d)."}, {"heading": "Stab and Gurevych (2016)", "text": "#PE1\n\u201cIn my opinion, although using machines have many benefits, we cannot ignore its negative effects.\u201d,\n\u201cDue to the reasons given above, to encourage children taking a part-time job is a good choice for them to get prepared for adult life.\u201d #PE2\n\u201cThus, it is not need for young people to possess\nthis ability.\u201d,\n\u201cHowever, I completely disagree with the idea that businesses should be allowed to do anything they want to make a profit.\u201d,\n\u201cIn conclusion, after analysing the effects of\n[...]\u201d\nReed et al. (2008)\n#VG1\n\u201cTherefore, the God of the Bible does not exist.\u201d \u201cTherefore, 10% of the students in my logic\nclass are left-handed.\u201d #VG2\n\u201cIn considering the intention of Parliament when passing the 1985 Act, or perhaps more properly the intention of the draftsman in settling its terms, there are [...]\u201d),\n\u201cAlexander Downer has derided the line of suspicion as \u201csilly\u201d, but surely some in his entourage - as he did the rounds of Indonesian military and police chiefs in Jakarta yesterday to discuss the Bali bombing - might have wondered how clean were some of the hands they were shaking.\u201d #VG3\n\u201cEight-month-old Kyle Mutch\u2019s tragic death was not an accident and he suffered injuries consistent with a punch or a kick, a court heard yesterday.\u201d\n\u201cThe first lady pointed out that, as well as resources, there was a need for support for the structures and legal frameworks needed to unpick some of the complicated problems.\u201d #VG4\n\u201cRE: Does the Priest Scandal Reveal the\nBeast?\u201d\n\u201cDouble standards me thinks.\u201d"}, {"heading": "Habernal and Gurevych (2015)", "text": "#WD1\n\u201cI regard single sex education as bad.\u201d\n#WD2\n\u201cMy view on the subject is no.\u201d\n#WD3\n\u201cIn 2013, is single-sex education really the way\nto go?\u201d)."}, {"heading": "Cross-Domain Experiments: Results", "text": "Table 5 contains more results from the crossdomain experiments, including full numbers for different feature groups (Section 5.2 of the main paper). Precisely, we list results for the best indomain systems, according to average F1 scores. Furthermore, we list the results when training on mixed sources, in a leave-one-domain-out fashion (training on all but one datasets and tested on the remaining one)."}], "references": [{"title": "CrossDomainMining of Argumentative Text through Distant Supervision", "author": ["Khalid Al-Khatib", "Henning Wachsmuth", "Matthias Hagen", "Jonas K\u00f6hler", "Benno Stein."], "venue": "15th Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Al.Khatib et al\\.,? 2016", "shortCiteRegEx": "Al.Khatib et al\\.", "year": 2016}, {"title": "Cats rule and dogs drool!: Classifying stance in online debate", "author": ["Pranav Anand", "Marilyn Walker", "Rob Abbott", "Jean E. Fox Tree", "Robeson Bowmani", "Michael Minor."], "venue": "Proceedings of the 2nd Workshop on Computational Approaches to Subjec-", "citeRegEx": "Anand et al\\.,? 2011", "shortCiteRegEx": "Anand et al\\.", "year": 2011}, {"title": "Identifying justifications in written dialogs", "author": ["Or Biran", "Owen Rambow."], "venue": "Fifth IEEE International Conference on Semantic Computing (ICSC). Palo Alto, CA, USA, pages 162\u2013168.", "citeRegEx": "Biran and Rambow.,? 2011a", "shortCiteRegEx": "Biran and Rambow.", "year": 2011}, {"title": "Identifying justifications in written dialogs by classifying text as argumentative", "author": ["Or Biran", "Owen Rambow."], "venue": "International Journal of Semantic Computing 05(04):363\u2013381. https://doi.org/10.1142/S1793351X11001328.", "citeRegEx": "Biran and Rambow.,? 2011b", "shortCiteRegEx": "Biran and Rambow.", "year": 2011}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Sydney, Australia, pages 120\u2013128.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "From Discourse Analysis to Argumentation Schemes and Back: Relations and Differences", "author": ["Elena Cabrio", "Sara Tonelli", "Serena Villata"], "venue": null, "citeRegEx": "Cabrio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cabrio et al\\.", "year": 2013}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daume III."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics, Prague, Czech Republic, pages 256\u2013263.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Neural End-to-End Learning for Computational Argumentation Mining", "author": ["Steffen Eger", "Johannes Daxenberger", "Iryna Gurevych."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. In press. Preprint:", "citeRegEx": "Eger et al\\.,? 2017", "shortCiteRegEx": "Eger et al\\.", "year": 2017}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin."], "venue": "J. Mach. Learn. Res. 9:1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Classifying arguments by scheme", "author": ["Vanessa Wei Feng", "Graeme Hirst."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Portland, OR, USA, pages 987\u2013996.", "citeRegEx": "Feng and Hirst.,? 2011", "shortCiteRegEx": "Feng and Hirst.", "year": 2011}, {"title": "Emergent: a novel data-set for stance classification", "author": ["William Ferreira", "Andreas Vlachos."], "venue": "In", "citeRegEx": "Ferreira and Vlachos.,? 2016", "shortCiteRegEx": "Ferreira and Vlachos.", "year": 2016}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "Proceedings of the 28th International Conference on Machine Learning. Bellevue, WA, USA, pages 513\u2013", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A Practical Study of Argument", "author": ["Trudy Govier."], "venue": "Wadsworth, Cengage Learning, 7th edition.", "citeRegEx": "Govier.,? 2010", "shortCiteRegEx": "Govier.", "year": 2010}, {"title": "Revisiting Embedding Features for Simple Semi-supervised Learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 110\u2013", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Exploiting debate portals for semi-supervised argumentation mining in user-generated web discourse", "author": ["Ivan Habernal", "Iryna Gurevych."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon,", "citeRegEx": "Habernal and Gurevych.,? 2015", "shortCiteRegEx": "Habernal and Gurevych.", "year": 2015}, {"title": "Argumentation Mining in User-Generated Web Discourse", "author": ["Ivan Habernal", "Iryna Gurevych."], "venue": "Computational Linguistics 43(1):125\u2013179.", "citeRegEx": "Habernal and Gurevych.,? 2017", "shortCiteRegEx": "Habernal and Gurevych.", "year": 2017}, {"title": "Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language", "author": ["Luheng He", "Mike Lewis", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Evaluating Learning Algorithms: A Classification Perspective", "author": ["Nathalie Japkowicz", "Mohak Shah."], "venue": "Cambridge University Press, Cambridge, UK.", "citeRegEx": "Japkowicz and Shah.,? 2014", "shortCiteRegEx": "Japkowicz and Shah.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar, pages 1746\u20131751.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Dependency based embeddings for sentence classification tasks", "author": ["Alexandros Komninos", "Suresh Manandhar."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-", "citeRegEx": "Komninos and Manandhar.,? 2016", "shortCiteRegEx": "Komninos and Manandhar.", "year": 2016}, {"title": "Context dependent claim detection", "author": ["Ran Levy", "Yonatan Bilu", "Daniel Hershcovich", "Ehud Aharoni", "Noam Slonim."], "venue": "Proceedings of the 25th International Conference on Computational Linguistics. Dublin, Ireland, pages 1489\u20131500.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Recognizing implicit discourse relations in the penn discourse treebank", "author": ["Ziheng Lin", "Min-Yen Kan", "Hwee Tou Ng."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1.", "citeRegEx": "Lin et al\\.,? 2009", "shortCiteRegEx": "Lin et al\\.", "year": 2009}, {"title": "A PDTB-Styled End-to-End Discourse Parser", "author": ["Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan."], "venue": "Natural Language Engineering 20(2):151\u2013184.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Contextindependent claim detection for argument mining", "author": ["Marco Lippi", "Paolo Torroni."], "venue": "Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence. Buenos Aires, Argentina, pages 185\u2013191.", "citeRegEx": "Lippi and Torroni.,? 2015", "shortCiteRegEx": "Lippi and Torroni.", "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Using summarization to discover argument facets in online idealogical dialog", "author": ["Amita Misra", "Pranav Anand", "Jean Fox Tree", "Marilyn Walker."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Misra et al\\.,? 2015", "shortCiteRegEx": "Misra et al\\.", "year": 2015}, {"title": "Argumentation mining: The detection, classification and structure of arguments in text", "author": ["Raquel Mochales-Palau", "Marie-Francine Moens."], "venue": "Proceedings of the 12th International Conference on Artificial Intelligence and Law. Barcelona, Spain,", "citeRegEx": "Mochales.Palau and Moens.,? 2009", "shortCiteRegEx": "Mochales.Palau and Moens.", "year": 2009}, {"title": "Automatic detection of arguments in legal texts", "author": ["Marie-Francine Moens", "Erik Boiy", "Raquel Mochales Palau", "Chris Reed."], "venue": "Proceedings of the 11th International Conference on Artificial Intelligence and Law. Stanford, CA, USA, pages 225\u2013230.", "citeRegEx": "Moens et al\\.,? 2007", "shortCiteRegEx": "Moens et al\\.", "year": 2007}, {"title": "Robust morphological tagging with word representations", "author": ["Thomas M\u00fcller", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Den-", "citeRegEx": "M\u00fcller and Sch\u00fctze.,? 2015", "shortCiteRegEx": "M\u00fcller and Sch\u00fctze.", "year": 2015}, {"title": "Machine Learning: A Probabilistic Perspective", "author": ["Kevin P. Murphy."], "venue": "MIT Press, Cambridge, MA, USA.", "citeRegEx": "Murphy.,? 2012", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "Improving argument mining in student essays by learning and exploiting argument indicators versus essay topics", "author": ["Huy Nguyen", "Diane Litman."], "venue": "Proceedings of the Twenty-Ninth International Florida Artificial Intelligence Research Soci-", "citeRegEx": "Nguyen and Litman.,? 2016", "shortCiteRegEx": "Nguyen and Litman.", "year": 2016}, {"title": "Cross\u2013 domain sentiment classification via spectral feature alignment", "author": ["Sinno Jialin Pan", "Xiaochuan Ni", "Jian-Tao Sun", "Qiang Yang", "Zheng Chen."], "venue": "Proceedings of the 19th International Conference on World Wide Web. ACM,", "citeRegEx": "Pan et al\\.,? 2010", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Identifying comparative claim sentences in full-text scientific articles", "author": ["Dae Hoon Park", "Catherine Blake."], "venue": "Proceedings of the Workshop on Detecting Structure in Scholarly Discourse. Jeju, Republic of Korea, pages 1\u20139.", "citeRegEx": "Park and Blake.,? 2012", "shortCiteRegEx": "Park and Blake.", "year": 2012}, {"title": "From Argument Diagrams to Argumentation Mining in Texts: A Survey", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "International Journal of Cognitive Informatics and Natural Intelligence 7(1):1\u201331.", "citeRegEx": "Peldszus and Stede.,? 2013a", "shortCiteRegEx": "Peldszus and Stede.", "year": 2013}, {"title": "Ranking the annotators: An agreement study on argumentation structure", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. Sofia, Bulgaria, pages 196\u2013204.", "citeRegEx": "Peldszus and Stede.,? 2013b", "shortCiteRegEx": "Peldszus and Stede.", "year": 2013}, {"title": "Joint prediction in mst-style discourse parsing for argumentation mining", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-", "citeRegEx": "Peldszus and Stede.,? 2015", "shortCiteRegEx": "Peldszus and Stede.", "year": 2015}, {"title": "An annotated corpus of argumentative microtexts", "author": ["Andreas Peldszus", "Manfred Stede."], "venue": "Argumentation and Reasoned Action: Proceedings of the 1st European Conference on Argumentation. Lisabon, pages 801\u2013815.", "citeRegEx": "Peldszus and Stede.,? 2016", "shortCiteRegEx": "Peldszus and Stede.", "year": 2016}, {"title": "Modeling argument strength in student essays", "author": ["Isaac Persing", "Vincent Ng."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-", "citeRegEx": "Persing and Ng.,? 2015", "shortCiteRegEx": "Persing and Ng.", "year": 2015}, {"title": "Linguistically debatable or just plain wrong? In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "author": ["Barbara Plank", "Dirk Hovy", "Anders S\u00f8gaard."], "venue": "Baltimore, MA, USA, pages 507\u2013511.", "citeRegEx": "Plank et al\\.,? 2014", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Language resources for studying argument", "author": ["Chris Reed", "Raquel Mochales-Palau", "Glenn Rowe", "Marie-Francine Moens."], "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation. Marrakech, Morocco, pages 2613\u2013", "citeRegEx": "Reed et al\\.,? 2008", "shortCiteRegEx": "Reed et al\\.", "year": 2008}, {"title": "Show me your evidence - an automatic method for context dependent evidence", "author": ["Ruty Rinott", "Lena Dankin", "Carlos Alzate Perez", "Mitesh M. Khapra", "Ehud Aharoni", "Noam Slonim"], "venue": null, "citeRegEx": "Rinott et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rinott et al\\.", "year": 2015}, {"title": "Applying kernel methods to argumentation mining", "author": ["Niall Rooney", "Hui Wang", "Fiona Browne."], "venue": "Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference. Marco Island, FL, USA, pages 272\u2013", "citeRegEx": "Rooney et al\\.,? 2012", "shortCiteRegEx": "Rooney et al\\.", "year": 2012}, {"title": "Detecting opinionated claims in online discussions", "author": ["Sara Rosenthal", "Kathleen McKeown."], "venue": "Proceedings of the 2012 IEEE Sixth International Conference on Semantic Computing. Washington, DC, USA, pages 30\u201337.", "citeRegEx": "Rosenthal and McKeown.,? 2012", "shortCiteRegEx": "Rosenthal and McKeown.", "year": 2012}, {"title": "I couldn\u2019t agree more: The role of conversational structure in agreement and disagreement detection in online discussions", "author": ["Sara Rosenthal", "Kathy McKeown."], "venue": "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and", "citeRegEx": "Rosenthal and McKeown.,? 2015", "shortCiteRegEx": "Rosenthal and McKeown.", "year": 2015}, {"title": "Argumentation: Keeping Faith with Reason", "author": ["Edward Schiappa", "John P. Nordin."], "venue": "Pearson UK, 1st edition.", "citeRegEx": "Schiappa and Nordin.,? 2013", "shortCiteRegEx": "Schiappa and Nordin.", "year": 2013}, {"title": "Towards robust cross-domain domain adaptation for part-of-speech tagging", "author": ["Tobias Schnabel", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Sixth International Joint Conference on Natural Language Processing. Asian Federation of Natural Lan-", "citeRegEx": "Schnabel and Sch\u00fctze.,? 2013", "shortCiteRegEx": "Schnabel and Sch\u00fctze.", "year": 2013}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Berlin, Germany, pages 231\u2013235.", "citeRegEx": "S\u00f8gaard and Goldberg.,? 2016", "shortCiteRegEx": "S\u00f8gaard and Goldberg.", "year": 2016}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["Marina Sokolova", "Guy Lapalme."], "venue": "Information Processing & Management 45(4):427\u2013437. https://doi.org/10.1016/j.ipm.2009.03.002.", "citeRegEx": "Sokolova and Lapalme.,? 2009", "shortCiteRegEx": "Sokolova and Lapalme.", "year": 2009}, {"title": "Identifying argumentative discourse structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Doha, Qatar, pages 46\u201356.", "citeRegEx": "Stab and Gurevych.,? 2014", "shortCiteRegEx": "Stab and Gurevych.", "year": 2014}, {"title": "Parsing argumentation structures in persuasive essays", "author": ["Christian Stab", "Iryna Gurevych."], "venue": "arXiv preprint arXiv:1604.07370 .", "citeRegEx": "Stab and Gurevych.,? 2016", "shortCiteRegEx": "Stab and Gurevych.", "year": 2016}, {"title": "Legal claim identification: Information extraction with hierarchically labeled data", "author": ["Mihai Surdeanu", "Ramesh Nallapati", "Christopher D. Manning."], "venue": "Workshop on the Semantic Processing of Legal Texts at LREC. Valletta, Malta, pages 22\u201329.", "citeRegEx": "Surdeanu et al\\.,? 2010", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2010}, {"title": "The Uses of Argument, Updated Edition", "author": ["Stephen E. Toulmin."], "venue": "Cambridge University Press, New York.", "citeRegEx": "Toulmin.,? 2003", "shortCiteRegEx": "Toulmin.", "year": 2003}, {"title": "Using domain similarity for performance estimation", "author": ["Vincent Van Asch", "Walter Daelemans."], "venue": "Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing. Uppsala, Sweden, pages 31\u201336.", "citeRegEx": "Asch and Daelemans.,? 2010", "shortCiteRegEx": "Asch and Daelemans.", "year": 2010}, {"title": "Handbook of Argumentation Theory", "author": ["Frans H. van Eemeren", "Bart Garssen", "Erik C.W. Krabbe", "A. Francisca Snoeck Henkemans", "Bart Verheij", "Jean H.M. Wagemans."], "venue": "Springer, Berlin/Heidelberg.", "citeRegEx": "Eemeren et al\\.,? 2014", "shortCiteRegEx": "Eemeren et al\\.", "year": 2014}, {"title": "Fact checking: Task definition and dataset construction", "author": ["Andreas Vlachos", "Sebastian Riedel."], "venue": "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science. Baltimore, MD, USA, pages 18\u201322.", "citeRegEx": "Vlachos and Riedel.,? 2014", "shortCiteRegEx": "Vlachos and Riedel.", "year": 2014}, {"title": "pagerank\u201d for argument relevance", "author": ["Henning Wachsmuth", "Benno Stein", "Yamen Ajjour."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume", "citeRegEx": "Wachsmuth et al\\.,? 2017", "shortCiteRegEx": "Wachsmuth et al\\.", "year": 2017}, {"title": "Unsupervised multi-domain adaptation with feature embeddings", "author": ["Yi Yang", "Jacob Eisenstein."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Yang and Eisenstein.,? 2015", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2015}, {"title": "ArgRewrite: A Web-based Revision Assistant for Argumentative Writings", "author": ["Fan Zhang", "Rebecca Hwa", "Diane Litman", "Homa B. Hashemi."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the As-", "citeRegEx": "Zhang et al\\.,? 2016a", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Dependency sensitive convolutional neural networks for modeling sentences and documents", "author": ["Rui Zhang", "Honglak Lee", "Dragomir R. Radev."], "venue": "Proceedings of the Conference of the North American Chapter of the As-", "citeRegEx": "Zhang et al\\.,? 2016b", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "MGNC-CNN: A simple approach to exploiting multiple word embeddings for sentence classification", "author": ["Ye Zhang", "Stephen Roller", "Byron C. Wallace."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for", "citeRegEx": "Zhang et al\\.,? 2016c", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "11); recent works describe a claim as \u2018a statement that is in dispute and that we are trying to support with reasons\u2019 (Govier, 2010).", "startOffset": 118, "endOffset": 132}, {"referenceID": 33, "context": "Argument mining, a computational counterpart of manual argumentation analysis, is a recent growing sub-field of NLP (Peldszus and Stede, 2013a).", "startOffset": 116, "endOffset": 143}, {"referenceID": 54, "context": "Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al.", "startOffset": 96, "endOffset": 122}, {"referenceID": 50, "context": "Claim identification itself is an important prerequisite for applications such as fake checking (Vlachos and Riedel, 2014), politics and legal affairs (Surdeanu et al., 2010), and science (Park and Blake, 2012).", "startOffset": 151, "endOffset": 174}, {"referenceID": 32, "context": ", 2010), and science (Park and Blake, 2012).", "startOffset": 21, "endOffset": 43}, {"referenceID": 48, "context": "Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017).", "startOffset": 130, "endOffset": 155}, {"referenceID": 15, "context": "Although claims can be identified with a promising level of accuracy in typical argumentative discourse such as persuasive essays (Stab and Gurevych, 2014), less homogeneous resources, for instance online discourse, pose challenges to current systems (Habernal and Gurevych, 2017).", "startOffset": 251, "endOffset": 280}, {"referenceID": 26, "context": "Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (Mochales-Palau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al.", "startOffset": 117, "endOffset": 149}, {"referenceID": 35, "context": "Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (Mochales-Palau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al.", "startOffset": 162, "endOffset": 188}, {"referenceID": 20, "context": "Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (Mochales-Palau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2016).", "startOffset": 209, "endOffset": 249}, {"referenceID": 40, "context": "Furthermore, existing argument mining approaches are often limited to a single, specific domain like legal documents (Mochales-Palau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Levy et al., 2014; Rinott et al., 2015) or student essays (Stab and Gurevych, 2016).", "startOffset": 209, "endOffset": 249}, {"referenceID": 49, "context": ", 2015) or student essays (Stab and Gurevych, 2016).", "startOffset": 26, "endOffset": 51}, {"referenceID": 20, "context": "by Levy et al. (2014) and Rinott et al.", "startOffset": 3, "endOffset": 22}, {"referenceID": 20, "context": "by Levy et al. (2014) and Rinott et al. (2015) for mining claims and corresponding evidence for a predefined topic over multiple Wikipedia articles.", "startOffset": 3, "endOffset": 47}, {"referenceID": 14, "context": "Habernal and Gurevych (2017) cast argument component identification as BIO sequence labeling and jointly model separation of argumentative from non-argumentative text units and identification of argument component boundaries together with their types.", "startOffset": 0, "endOffset": 29}, {"referenceID": 14, "context": "Habernal and Gurevych (2017) cast argument component identification as BIO sequence labeling and jointly model separation of argumentative from non-argumentative text units and identification of argument component boundaries together with their types. They achieved 25.1% macro F1 with a combination of topic, sentiment, semantic, discourse and embedding features using structural SVM. Stab and Gurevych (2014) identified claims and other argument components in student essays.", "startOffset": 0, "endOffset": 411}, {"referenceID": 4, "context": "Since techniques such as learning generalized crossdomain representations in an unsupervised manner (Blitzer et al., 2006; Pan et al., 2010; Glorot et al., 2011; Yang and Eisenstein, 2015) have been criticized for targeting specific source and target domains, it has alternatively been proposed to learn universal representations from general domains in order to render a learner robust across all possible domain shifts (M\u00fcller and Sch\u00fctze, 2015; Schnabel and Sch\u00fctze, 2013).", "startOffset": 100, "endOffset": 188}, {"referenceID": 31, "context": "Since techniques such as learning generalized crossdomain representations in an unsupervised manner (Blitzer et al., 2006; Pan et al., 2010; Glorot et al., 2011; Yang and Eisenstein, 2015) have been criticized for targeting specific source and target domains, it has alternatively been proposed to learn universal representations from general domains in order to render a learner robust across all possible domain shifts (M\u00fcller and Sch\u00fctze, 2015; Schnabel and Sch\u00fctze, 2013).", "startOffset": 100, "endOffset": 188}, {"referenceID": 11, "context": "Since techniques such as learning generalized crossdomain representations in an unsupervised manner (Blitzer et al., 2006; Pan et al., 2010; Glorot et al., 2011; Yang and Eisenstein, 2015) have been criticized for targeting specific source and target domains, it has alternatively been proposed to learn universal representations from general domains in order to render a learner robust across all possible domain shifts (M\u00fcller and Sch\u00fctze, 2015; Schnabel and Sch\u00fctze, 2013).", "startOffset": 100, "endOffset": 188}, {"referenceID": 56, "context": "Since techniques such as learning generalized crossdomain representations in an unsupervised manner (Blitzer et al., 2006; Pan et al., 2010; Glorot et al., 2011; Yang and Eisenstein, 2015) have been criticized for targeting specific source and target domains, it has alternatively been proposed to learn universal representations from general domains in order to render a learner robust across all possible domain shifts (M\u00fcller and Sch\u00fctze, 2015; Schnabel and Sch\u00fctze, 2013).", "startOffset": 100, "endOffset": 188}, {"referenceID": 28, "context": ", 2011; Yang and Eisenstein, 2015) have been criticized for targeting specific source and target domains, it has alternatively been proposed to learn universal representations from general domains in order to render a learner robust across all possible domain shifts (M\u00fcller and Sch\u00fctze, 2015; Schnabel and Sch\u00fctze, 2013).", "startOffset": 267, "endOffset": 321}, {"referenceID": 45, "context": ", 2011; Yang and Eisenstein, 2015) have been criticized for targeting specific source and target domains, it has alternatively been proposed to learn universal representations from general domains in order to render a learner robust across all possible domain shifts (M\u00fcller and Sch\u00fctze, 2015; Schnabel and Sch\u00fctze, 2013).", "startOffset": 267, "endOffset": 321}, {"referenceID": 39, "context": "The AraucariaDB corpus (Reed et al., 2008) includes various genres (VG) such as newspaper editorials, parliamentary records, or judicial summaries.", "startOffset": 23, "endOffset": 42}, {"referenceID": 27, "context": "Although the reliability of the annotations is unknown, the corpus has been extensively used in argument mining (Moens et al., 2007; Feng and Hirst, 2011; Rooney et al., 2012).", "startOffset": 112, "endOffset": 175}, {"referenceID": 9, "context": "Although the reliability of the annotations is unknown, the corpus has been extensively used in argument mining (Moens et al., 2007; Feng and Hirst, 2011; Rooney et al., 2012).", "startOffset": 112, "endOffset": 175}, {"referenceID": 41, "context": "Although the reliability of the annotations is unknown, the corpus has been extensively used in argument mining (Moens et al., 2007; Feng and Hirst, 2011; Rooney et al., 2012).", "startOffset": 112, "endOffset": 175}, {"referenceID": 51, "context": "48) with claims and premises as well as backings, rebuttals and refutations inspired by Toulmin\u2019s model of argument (Toulmin, 2003).", "startOffset": 116, "endOffset": 131}, {"referenceID": 14, "context": "The corpus from Habernal and Gurevych (2017) includes user-generated web discourse (WD) such as blog posts, or user comments annotated (\u03b1U 0.", "startOffset": 16, "endOffset": 45}, {"referenceID": 49, "context": "The persuasive essay (PE) corpus (Stab and Gurevych, 2016) includes 402 student essays.", "startOffset": 33, "endOffset": 58}, {"referenceID": 37, "context": "The corpus has been extensively used in the argument mining community (Persing and Ng, 2015; Lippi and Torroni, 2015; Nguyen and Litman, 2016).", "startOffset": 70, "endOffset": 142}, {"referenceID": 23, "context": "The corpus has been extensively used in the argument mining community (Persing and Ng, 2015; Lippi and Torroni, 2015; Nguyen and Litman, 2016).", "startOffset": 70, "endOffset": 142}, {"referenceID": 30, "context": "The corpus has been extensively used in the argument mining community (Persing and Ng, 2015; Lippi and Torroni, 2015; Nguyen and Litman, 2016).", "startOffset": 70, "endOffset": 142}, {"referenceID": 34, "context": "38) (Peldszus and Stede, 2013b).", "startOffset": 4, "endOffset": 31}, {"referenceID": 44, "context": "The observed characteristics are backed by argumentation theory (Schiappa and Nordin, 2013) and include among other things the claim type, signaling words and discourse markers.", "startOffset": 64, "endOffset": 91}, {"referenceID": 44, "context": "The Wikipedia Talk Page corpus (WTP) contains claims typical to Wikipedia quality discussions and policy claims (Schiappa and Nordin, 2013) are present as well.", "startOffset": 112, "endOffset": 139}, {"referenceID": 39, "context": "VG (Reed et al., 2008) various genres 507 60,383 2,842 563 (19.", "startOffset": 3, "endOffset": 22}, {"referenceID": 14, "context": "81%) WD (Habernal and Gurevych, 2015) web discourse 340 84,817 3,899 211 (5.", "startOffset": 8, "endOffset": 37}, {"referenceID": 49, "context": "41%) PE (Stab and Gurevych, 2016) persuasive essays 402 147,271 7,116 2,108 (29.", "startOffset": 8, "endOffset": 33}, {"referenceID": 2, "context": "62%) OC (Biran and Rambow, 2011a) online comments 2,805 125,677 8,946 703 (7.", "startOffset": 8, "endOffset": 33}, {"referenceID": 3, "context": "86%) WTP (Biran and Rambow, 2011b) wiki talk pages 1,985 189,140 9,140 1,138 (12.", "startOffset": 9, "endOffset": 34}, {"referenceID": 35, "context": "45%) MT (Peldszus and Stede, 2015) micro texts 112 8,865 449 112 (24.", "startOffset": 8, "endOffset": 34}, {"referenceID": 14, "context": "Habernal and Gurevych (2017) investigated hedging in claims and found out that it varies with respect to the topic being discussed (10% up to 35% of claims are hedged).", "startOffset": 0, "endOffset": 29}, {"referenceID": 24, "context": "All six datasets described in the previous section have been preprocessed by first segmenting documents into sentences using Stanford CoreNLP (Manning et al., 2014) and then annotating every sentence as claim, if one or more tokens within the sentence were labeled as claim (or major claim in PE).", "startOffset": 142, "endOffset": 164}, {"referenceID": 38, "context": "Regularized logistic regression is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016).", "startOffset": 119, "endOffset": 205}, {"referenceID": 16, "context": "Regularized logistic regression is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016).", "startOffset": 119, "endOffset": 205}, {"referenceID": 57, "context": "Regularized logistic regression is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016).", "startOffset": 119, "endOffset": 205}, {"referenceID": 10, "context": "Regularized logistic regression is a simple yet robust and established technique for many text classification problems (Plank et al., 2014; He et al., 2015; Zhang et al., 2016a; Ferreira and Vlachos, 2016).", "startOffset": 119, "endOffset": 205}, {"referenceID": 8, "context": "Using the liblinear library (Fan et al., 2008).", "startOffset": 28, "endOffset": 46}, {"referenceID": 13, "context": "Embedding Features represent each sentence as a summation of its word embeddings (Guo et al., 2014).", "startOffset": 81, "endOffset": 99}, {"referenceID": 14, "context": "We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al.", "startOffset": 48, "endOffset": 97}, {"referenceID": 1, "context": "We further experimented with sentiment features (Habernal and Gurevych, 2015; Anand et al., 2011) and dictionary features (Misra et al.", "startOffset": 48, "endOffset": 97}, {"referenceID": 25, "context": ", 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article.", "startOffset": 32, "endOffset": 81}, {"referenceID": 43, "context": ", 2011) and dictionary features (Misra et al., 2015; Rosenthal and McKeown, 2015) but these delivered very poor results and are not reported in this article.", "startOffset": 32, "endOffset": 81}, {"referenceID": 17, "context": "Discourse Features encode information extracted with help of the Penn Discourse Treebank (PDTB) styled end-to-end discourse parser as presented by Lin et al. (2014). Embedding Features represent each sentence as a summation of its word embeddings (Guo et al.", "startOffset": 147, "endOffset": 165}, {"referenceID": 18, "context": "The first is the Convolutional Neural Net of Kim (Kim, 2014) which has shown to perform excellently on many diverse classification tasks such as sentiment analysis and question classification and is still a strong competitor among neural techniques focusing on sentence classification (Komninos and Manandhar, 2016; Zhang et al.", "startOffset": 49, "endOffset": 60}, {"referenceID": 17, "context": "All reported significance tests were conduced using two-tailed Wilcoxon Signed-Rank Test (Japkowicz and Shah, 2014).", "startOffset": 89, "endOffset": 115}, {"referenceID": 47, "context": "Described as FscoreM in (Sokolova and Lapalme, 2009).", "startOffset": 24, "endOffset": 52}, {"referenceID": 55, "context": "A more consistent notion of claim as the central component of an argument, which also holds across domains, would potentially not just benefit cross-domain claim identification, but also higher-level applications relying on argumentation mining (Wachsmuth et al., 2017).", "startOffset": 245, "endOffset": 269}, {"referenceID": 46, "context": "To further overcome the problem of domain dependence, multitask learning (S\u00f8gaard and Goldberg, 2016) is a framework that could be explored in the argumentation mining context (Eger et al.", "startOffset": 73, "endOffset": 101}, {"referenceID": 7, "context": "To further overcome the problem of domain dependence, multitask learning (S\u00f8gaard and Goldberg, 2016) is a framework that could be explored in the argumentation mining context (Eger et al., 2017).", "startOffset": 176, "endOffset": 195}], "year": 2017, "abstractText": "Argument mining has become a popular research area in NLP. It typically includes the identification of argumentative components, e.g. claims, as the central component of an argument. We perform a qualitative analysis across six different datasets and show that these appear to conceptualize claims quite differently. To learn about the consequences of such different conceptualizations of claim for practical applications, we carried out extensive experiments using state-of-the-art featurerich and deep learning systems, to identify claims in a cross-domain fashion. While the divergent perception of claims in different datasets is indeed harmful to crossdomain classification, we show that there are shared properties on the lexical level as well as system configurations that can help to overcome these gaps.", "creator": "LaTeX with hyperref package"}}}