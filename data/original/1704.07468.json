{"id": "1704.07468", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2017", "title": "GaKCo: a Fast GApped k-mer string Kernel using COunting", "abstract": "String Kernel (SK) techniques, especially those using gapped $k$-mers as features (gk), have obtained great success in classifying sequences like DNA, protein, and text. However, the state-of-the-art gk-SK runs extremely slow when we increase the dictionary size ($\\Sigma$) or allow more mismatches ($M$). This is because current gk-SK uses a trie-based algorithm to calculate co-occurrence of mismatched substrings resulting in a time cost proportional to $O(\\Sigma^{M})$. We propose a \\textbf{fast} algorithm for calculating \\underline{Ga}pped $k$-mer \\underline{K}ernel using \\underline{Co}unting (GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of substrings using cumulative counting. This algorithm is fast, scalable to larger $\\Sigma$ and $M$, and naturally parallelizable. We provide a rigorous asymptotic analysis that compares GaKCo with the state-of-the-art gk-SK. Theoretically, the time cost of GaKCo is independent of the $\\Sigma^{M}$ term that slows down the trie-based approach. Experimentally, we observe that GaKCo achieves the same accuracy as the state-of-the-art and outperforms its speed by factors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein (12 datasets), and character-based English text (2 datasets), respectively", "histories": [["v1", "Mon, 24 Apr 2017 21:43:21 GMT  (890kb,D)", "http://arxiv.org/abs/1704.07468v1", null], ["v2", "Sun, 30 Apr 2017 20:12:01 GMT  (1797kb,D)", "http://arxiv.org/abs/1704.07468v2", null], ["v3", "Mon, 18 Sep 2017 17:25:17 GMT  (1786kb,D)", "http://arxiv.org/abs/1704.07468v3", "@ECML 2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CC cs.CL cs.DS", "authors": ["ritambhara singh", "arshdeep sekhon", "kamran kowsari", "jack lanchantin", "beilun wang", "yanjun qi"], "accepted": false, "id": "1704.07468"}, "pdf": {"name": "1704.07468.pdf", "metadata": {"source": "CRF", "title": "GaKCo: a Fast Gapped k-mer string Kernel using Counting", "authors": ["Ritambhara Singh", "Arshdeep Sekhon", "Kamran Kowsari", "Jack Lanchantin", "Beilun Wang", "Yanjun Qi"], "emails": [], "sections": [{"heading": null, "text": "Keywords: Fast Learning, String Kernels, Sequence Classification, Gapped k-mer String Kernel, Counting Statistics"}, {"heading": "1 Introduction", "text": "Sequence classification is one of the most important machine learning tasks used widely in fields like biology and natural language processing. Besides accuracy, one primary requirement for modern sequence classification methods is speed. For example, with the advancement of sequencing technologies, a massive amount of protein and DNA sequence data is produced daily [21]. There is an urgent need to analyze these sequences quickly for assisting time-sensitive experiments. Similarly, on-line information retrieval systems need to classify text sequences, for instance when quickly assessing customer reviews or categorizing documents to di\u21b5erent topics.\nIn this paper, we focus on the String Kernels (SK) in the Support Vector Machine (SVM) framework for supervised sequence classification. SK-SVM methods have been successfully used for classifying sequences like DNA [17, 14, 2, 26], protein [12] or character based natural language text [27]. They have provided state-of-the-art classification accuracy and can guarantee nice asymptotic behavior due to SVM\u2019s convex formulation and theoretical property [30]. Through comparing length-k local substrings (k-mers) and incorporating mismatches and gaps, this category of models calculates the similarity (i.e., so-called kernel function) among sequence samples. Then, using such similarity measures, SVM is trained to classify sequences. Recently, Ghandhi et al. [10] developed the state-of-the-art SK-SVM tool called gk-SVM. gk-SVM uses a gapped k-mer formulation [11] that reduces the feature space considerably compared to other k-mer based SK approaches.\nExisting k-mer based SK methods can become very slow or even unfeasible when we increase (1) the number of allowed mismatches (M) or (2) the size of the dictionary (\u2303) (detailed asymptotic analysis in Section 2). Allowing mismatches during substring comparisons is important since most sequences in biology are prone to mutations, i.e., insertions, deletions or substitution of sequence characters. Also, the size of the dictionary varies from one sequence classification domain to another. While DNA sequence is composed of only four characters (\u2303 = 4), most other domains have bigger dictionary sizes like for proteins, \u2303 = 20 and for character-based English text, \u2303 = 36. The state-of-the-art tool, gk-SVM, may work well for cases with small values of \u2303 and M (like for DNA sequences with \u2303 = 4 and M < 4), however, its kernel calculation is slow for cases like DNA with larger M , protein (dictionary size = 20), or character-based English text sequences (dictionary size = 36). Its trie-based implementation, in the worst case, scales exponentially with the dictionary size and the number of mismatches (O(\u2303M )). For example,\n1 GaKCo is shared as an open source tool at https://github.com/QData/GaKCo-SVM\nar X\niv :1\n70 4.\n07 46\n8v 1\n[ cs\n.L G\n] 2\n4 A\npr 2\n01 7\ngk-SVM takes more than 5 hours to calculate the kernel matrix for one protein sequence classification task with only 3312 sequences. This speed limitation hinders the practical applications of SK-SVM.\nThis paper proposes a fast algorithmic strategy, GaKCo: Gapped k-mer Kernel using Counting to speed up the gapped k-mer kernel calculation. GaKCo uses the associative array-based data structure to calculate kernel similarity through cumulative k-mer counting [14]. GaKCo groups the counting of cooccurrence of substrings at each fixed number of mismatches ({0, . . . , M}) into an independent procedure. Such grouping significantly reduces the number of updates on the kernel matrix (an operation that dominates the time cost). This algorithm is naturally parallelizable; therefore we present a multithread variation as our ultimate tool that improves the calculation speed even further.\nWe provide a rigorous theoretical analysis showing that GaKCo has a better asymptotic kernel computation time cost than gk-SVM. Our empirical experiments, on three di\u21b5erent real-world sequence classification domains, validate our theoretical analysis. For example, for the protein classification task mentioned above where gk-SVM took more than 5 hours, GaKCo takes only 4 minutes. Compared to GaKCo, gk-SVM slows down considerably especially when M 4 and for tasks with \u2303 4. Experimentally, GaKCo provides a speedup by factors of 2, 100 and 4 for sequence classification on DNA (5 datasets), protein (12 datasets) and text (2 datasets), respectively, while achieving same accuracy as gk-SVM. Fig. 1(a) compares the kernel calculation times of GaKCo (X-axis) with gk-SVM (Y-axis). We plot the kernel calculation times for the best performing (g, k) parameters (see Supplementary Information) for 19 di\u21b5erent datasets. We see that GaKCo is faster than gk-SVM for 16 out of 19 datasets that we have tested. Similarly, we plot the empirical performance (AUC scores or F1-score) of GaKCo (X-axis) versus gk-SVM (Y-axis) for the best performing (g, k) parameters (see Supplementary) for the 19 di\u21b5erent datasets in Fig. 1(b). It shows that the empirical performance of GaKCo is almost the same as gk-SVM with respect to the AUC scores. In summary, the main contributions of this work are as follows:\n\u2013 Fast: GaKCo is a novel combination of two e cient concepts: (1) reduced gapped k-mer feature space and (2) associative array based counting method, making it faster than the state-of-the-art gapped k-mer string kernel, while achieving same accuracy.\n\u2013 GaKCo can scale up to larger values of m and \u2303.\n\u2013 Parallelizable: GaKCo algorithm lends itself to a naturally parallelizable implementation.\n\u2013 We also present a detailed theoretical analysis of the asymptotic time complexity of GaKCo versus state-of-the-art gk-SVM. This analysis, to our knowledge, has not been reported before.\nRelated Work: Recently, Deep Neural Networks (NNs) have provided state-of-the-art performances for various sequence classification tasks like analyzing DNA [1, 16], proteins [24, 33], and natural language [32, 28] sequences. Despite their superior performance in accuracy and speed (e.g. through GPUs and mini-\nbatches) such NN systems usually require a significant number of training samples. This requirement can be unfeasible for many datasets, especially in the medical research domains. Here, the number of training sequences per experiment can be as low as tens or hundreds due to cost and time constraints. We compare GaKCo\u2019s empirical performance with a state-of-the-art deep convolutional neural network (CNN) model [16]. On datasets with few training samples, GaKCo achieves an average accuracy improvement of 20% over the CNN model (see Fig. 7) making it an appealing tool when the training samples are scarce. Besides, GaKCo includes only two hyperparameters (g and k) for tuning 2. This feature is desirable when comparing with NN systems for which figuring out the optimal network model and hyperparameters can be a daunting task.\nThe rest of the paper is organized as follows: Section 2 introduces the details of GaKCo and theoretically proves that asymptotically GaKCo runs faster than gk-SVM for large dictionary or allowing for more mismatches. Section 2.4 discusses the related work. Then Section 3 provides the experimental results we obtain on three major benchmark applications: TFBS binding prediction (DNA), Remote Protein Homology prediction (Proteins) and Text Classification (categorization and sentiment analysis). Empirically, GaKCo shows consistent improvements over gk-SVM in computation speed across di\u21b5erent types of datasets. When allowing a higher number of mismatches, the disparity in speed between GaKCo and the baseline becomes more apparent. Table 1 summarizes the important notations we use."}, {"heading": "2 Method", "text": "2.1 Background: Gapped k-mer String Kernels\nThe key idea of string kernels is to apply a function (\u00b7), which maps strings of arbitrary length into a vectorial feature space of fixed length. In this space, we apply a standard classifier such as Support Vector Machine (SVM) [30]. Kernel-version of SVMs calculates the decision function for an input sample x as:\nf(x) =\nNX\ni=1\n\u21b5iyiK(xi, x) + b (1)\nwhere N is the total number of training samples. String kernels ([17, 14, 10]), implicitly compute K(x, x0) as an inner product in the feature space (x) as:\nK(x, x0) = h (x), (x0)i, (2) 2 There is also one C parameter for tuning SVM training (while using linear kernel)\nwhere x = (s1, . . . , s|x|). x, x0 2 S. |x| denotes the length of the string x. S represents the set of all strings composed from a dictionary \u2303. : S ! Rp defines the mapping from a sequence x 2 S to a p-dimensional feature vector.\nThe feature representation (\u00b7) plays a vital role in string analysis since it is hard to describe strings as feature vectors. One classical method to represent a string is as an unordered set of k-mers, or combinations of k adjacent characters. A feature vector indexed by all k-mers records the number of occurrences of each k-mer in the current string. The string kernel using this representation is called spectrum kernel [18] (see Fig. 2), where the spectrum representation counts the occurrences of each k-mer in a string. Kernel scores between strings are computed by taking an inner product between corresponding \u201ck-mer-indexed\u201d feature vectors:\nK(x, x0) = X\n2 k cx( ) \u00b7 cx0( ) (3)\nwhere represents a k-mer, k is the set of all possible k-mers, and cx( ) is the number of occurrences (with normalization) of k-mer in string x. Many variations of spectrum kernels exist in the literature (summarized in Section 2.4) that mostly extend it by including mismatched k-mers when calculating the number of occurrences.\nSpectrum kernel and its mismatch variations generate extremely sparse feature vectors for even moderately sized values of k, since the size of k is \u2303\nk. To solve this issue, Ghandhi et al. [11] introduced a new set of feature representations, called gapped k-mers. It is characterized by two parameters: (1) g, the size of a substring with gaps (we call this gapped instance as g-mer hereafter) and (2) k, the size of non-gapped substring in a g-mer (we call it k-mer). The number of gaps is (g k). The inner product to compute the gapped k-mer kernel function includes sum over all possible k-mer feature counts obtained from the g-mers:\nK(x, x0) = X\n2\u21e5g cx( ) \u00b7 cx0( ) (4)\nwhere represents a k-mer, \u21e5g is the set of all possible gapped k-mers that can appear in all the g-mers (each with (g k) gaps) in a given dataset (denoted as D hereafter) of sequence samples.\nThe advantage of this formulation is that it reduces the number of possible k-mers drastically. In a \u201cnaive\u201d design of gapped k-mer string kernel when selecting k positions (k-mers) from a g-mer, there can be \u2303 possible choices for each of the\ng k position. Therefore, the total number of possible gapped k-mers\nequals F = g k \u2303k. This feature space grows rapidly with \u2303 or k. In contrast, Eq. (4) (implemented as gk-SVM [10]) includes only those k-mers whose gapped formulation has appeared as g-mers in a given dataset D. \u21e5g includes all unique g-mers of the dataset D, whose size |\u21e5g| is normally much smaller than F because the new feature space is restricted to only observable gapped k-mers in D. Ghandhi et al. [10] use this intuition to reformulate Eq.(4) into:\nK(x, x0) = l1X\ni=0\nl2X\nj=0\nhgk(g x i , g x0 j ) (5)\nFor two sequences x and x0 of lengths l1 and l2 respectively. gxi and g x0 j are the i\nth and jthg-mers of sequences x and x0 (i.e., gxi is a continuous substring of x starting from the i-th position and ending\nat the (i + g 1)th position of x). hg,k represents the inner product (or similarity) between gxi and gx 0 j using the co-occurrence of gapped k-mers as features. hgk(g x i , g x0 j ) is non-zero only when g x i and g x0 j have common k-mers.\nDefinition 1. g-pairm(x, x 0) denotes a pair of g-mers (gx1 , g x0 2 ) whose hamming distance is exactly m. gx1 is from sequence x and g x0 2 is from sequence x 0.\nEach g-pairm(.) has g m\nk\ncommon k-mers, therefore its hgk can be directly calculated as hgk(g-pairm) =\ng m k . Ghandhi et al. [10] formulate this observation formally into the coe cient hm:\nhm =\n( g m\nk\n, if g m k\n0, otherwise. (6)\nhm describes the co-occurrence count of common k-mers for each possible g-pairm(.) in D. hm > 0 only for cases of m  (g k) or (g m) k. This is because there will be no common k-mers when the number of mismatches (m) between two g-mers is more than (g k). Now we can reformulate Eq. 5 by grouping g-pairsm(x, x\n0) with respect to di\u21b5erent values of m. This is because g-pairsm(.) with same m contribute the same number of co-occurrence counts: hm. Thus, Eq. 5 can be adapted into the following compact form:\nK(x, x0) = g kX\nm=0\nNm(x, x 0)hm (7)\nNm(x, x 0) represents the number of g-pairm(x, x0) between sequence x and x0. Nm(x, x0) is named as mismatch profile by [10]. Now, to compute kernel function K(x, x0) for gapped k-mer SK, we only need to calculate Nm(x, x\n0) for m 2 {0, . . . g k}, since hm can be precomputed. The state-of-the-art tool gk-SVM [10] calculates Nm(x, x\n0) using a trie based data structure that is similar to [17] (with some modifications, details in Section 2.3).\n2.2 Proposed Method: Gapped k-mer Kernel with Counting (GaKCo)\nIn this paper, we propose GaKCo, a fast and novel algorithm for calculating gapped k-mer string kernel. GaKCo provides superior time performance over the state-of-the-art gk-SVM and is di\u21b5erent from it in three aspects:\n\u2013 Data Structure. gk-SVM uses a trie based data structure (plus a separate nodelist at each leafnode) for calculating Nm (see Figure 3(c)). In contrast, GaKCo uses simple associative arrays. \u2013 Algorithm. GaKCo performs g-mer based cumulative counting of co-occurrence to calculate Nm. \u2013 Parallelization. GaKCo groups computations for each value of m into an independent function, making it naturally parallelizable. We, therefore, provide a parallel version that uses multi-thread implementation.\nIntuition : When calculating Nm between all pairs of sequences in D for each value of m (m 2 {0, . . . , M = g k}), we can use counting to process all g-pairsm(.) (details below) from D together. Then we can calculate Nm from such count statistics of g-pairsm(.). This method is entirely di\u21b5erent from gk-SVM that uses a trie to organize g-mers such that each leafnode\u2019s (a unique g-mer\u2019s) nodelist memorizes its mismatched g-mer neighbors in D for up to g k mismatches. Section 2.3 provides theoretical analysis that GaKCo formulation is asymptotically more scalable to M and \u2303 than gk-SVM.\nAlgorithm: GaKCo calculates Nm(x, x 0) as follows (for pseudo code, see Algorithm 1):\n1. GaKCo first extracts all possible g-mers from all the sequences in D and puts them in a simple array. Given that there are N number of sequences with average length l 3, the total number of g-mers is N \u21e5 (l g + 1) \u21e0 Nl (see Fig. 3 (a)).\n3 This is a simplification of real world datasets in which sequence length varies across samples\n2. Nm=0(x, x 0) represents the number of g-pairm=0(x, x0) (pairs of g-mers whose hamming distance\nis 0) between x and x0. To compute Nm=0(xi, xj) 8i, 8j = 1, ..., N , GaKCo sorts all the g-mers lexicographically (see Fig. 3(a) [Step 1]) and counts the occurrences (if > 1) of each unique g-mer. Then we use these counts and the associated indexes of sequences to update all the kernel entries for sequences that include the matching g-mers (Fig. 3(a) [Step 2]). This computation is straight-forward and the sort and count step takes O(gNl) time cost while the kernel update costs O(zN2) (at the worst case). Here, z is the number of g-mers that occur > 1 times. 3. For cases when m = 1, . . . (g k), we use a statistics measure Cm(x, x0), called cumulative mismatch profile between two sequences x and x0. This measure describes the number of matching (g m)-mers between x and x0. Each (g m)-mer is generated from a g-mer by removing a total number of m positions. We can calculate the exact mismatch profile Nm from the cumulative mismatch profile Cm for m > 0 (explanation in the next step).\nCm=1 can be calculated from the associative-array (containing all g-mers in D and their counting statistic) that we obtain from calculating Nm=0. When m = 1, we perform the following operation on the list of all g-mers: we first pick 1 position and remove the symbol from the same position for all g-mers to get a new list of (g 1)-mers (Fig. 3 (a) [Step 3]). We then sort and count this new list to get the number of matching (g 1)-mers (Fig. 3 (b) [Step 4]). For the sequences that have the matching (g 1)-mers, we add the counts into their corresponding entries in matrix Cm. This sequence of operations is repeated for a total of\ng 1 positions, i.e every position that can be\nremoved from g-mers to get (g 1)-mers. The cumulative mismatch profile Cm=1 is equal to the sum of all counts from all\ng 1 runs (Fig. 3 [Step 5]). We use the same procedure for calculating Cm for\nm = 2, ..., M = g k. 4. We now calculate Nm from Cm and Nj for j = 0, ..., m 1. First, we explain the relationship between\nCm and Nm. Given two g-mers g1 and g2, we remove symbols from the same set of m positions of both g-mers to get two (g m)-mers: g01 and g02. If the hamming distance between g01 and g02: d(g01, g02) = 0, then we can conclude that the hamming distance between the original two g-mers g1 and g2: d(g1, g2)  m (See formal proof in Supplementary). For instance, Cm=1(x,x\u2019) records the statistic of matching (g 1)-mers among x and x0. It not only includes the matching statistics of all g-mer pairs whose hamming distance is m = 1, but it also over-counts the matching statistics of all g-mer pairs whose hamming distance is m = 0. This is because the matching g-mers that were counted for m = 0 will also contribute to the matching statistics when considering (g 1)-mers and that too for\ng 1\ntimes! Similarly, this over-counting occurs for other values of m as well. Essentially the cumulative mismatch profile Cm can be formulated as: 8m 2 {0, . . . , g k}\nCm = Nm +\nm 1X\nj=0\n\u2713 g j m j \u25c6 Nj (8)\nWe demonstrate this over-counting using Fig. 3(b) on a subset of g-mers (ACA,AAA) from Fig. 3(a). Using Eq.8, the exact mismatch profile Nm can be computed as follows:\nNm = Cm m 1X\nj=0\n\u2713 g j m j \u25c6 Nj (9)\nHere, we subtract Nj (for j = 0, . . . , i 1) from Cm to compensate for the over-counting described above.\nParallelization: For each value of m from {0, . . . M = g k}, calculating Cm is independent from other values of m. Therefore, GaKCo\u2019s algorithm can be easily revised into a parallel version. Essentially, we just need to revise Step 9 in Algorithm 1 (pseudo code) - \u201cFor each value of m\u201d- into, \u201cFor each value of m per machine/per core/per thread\u201d. In our current implementation, we create a thread for each value of m from {0, . . . M = g k} and calculate Cm in parallel. In the end, we compute the final kernel matrix K using all the resulting Cm matrices. Fig. 5 and 7(b) show the improvement of kernel calculation speed when comparing the multi-thread version with the single-thread implementation of GaKCo."}, {"heading": "2.3 Theoretical Comparison of Time Complexity", "text": "In this section we conduct asymptotic analysis to compare the time complexities of GaKCo with the state-of-the-art toolbox gk-SVM.\nTime Complexity of GaKCo: The time cost of GaKCo splits into two groups: (1) Pre-processing: those operations that indirectly update the matching statistics among sequences; (2) Kernel updates: those operations that directly update the matching statistics among sequences.\nPre-processing: For each possible m (m 2 {0, . . . M = g k}), GaKCo needs to choose m positions for symbol removing (Fig. 3 (a) [Step 3]), and then sort and count the possible (g m)-mers from D (Fig. 3 (a) [Step 4]). Therefore the time cost of pre-processing is O(\u2303M=g km=0 g m (g m)Nl) \u21e0 O(\u2303Mm=0 g i gNl). To simplifying notations, we use cgk to represent PM=(g k)\nm=0\ng m hereafter.\nKernel Updates: These operations update the entries of Cm or Nm matrices when GaKCo finishes each round of counting the number of matching (g m)-mers. Assuming z denotes the number of unique (g m)-mers that occur > 1 times, the time cost of kernel update operations is (at the worst case) equivalent to O(\u2303Mm=0 g m zN2) \u21e0 O(cgkzN2). Therefore, the overall time complexity of GaKCo is O(Cgk[gNl + zN 2]).\ngk-SVM Algorithm Now we introduce the algorithm of gk-SVM briefly. Given that there are N sequences in a dataset D, gk-SVM first constructs a trie tree recording all the unique g-mers in D. Each leafnode in the trie stores a unique g-mer (more precisely by its path to the rootnode) of D. We use u to denote the total number of the unique g-mers in this trie. Next, gk-SVM traverses the tree using the order of depth-first. For each leafnode (like ACA in (Fig. 3 (c)), it maintains a nodelist that includes all those g-mers in D whose hamming distance to the leafnode g-mer  M . When accessing a leafnode, all mismatch profile matrices Nm(x, x\n0) for m 2 {0, . . . , M = (g k)} are updated for all possible pairs of sequences x and x0. Here x consists of the g-mer of the current leafnode (like S/ACA in (Fig. 3 (c)). x0 belongs to the nodelist \u2019s sequence list. x0 includes a g-mer whose hamming distance from the leafnode is m (like T/ACA(m = 0) or T/AAA(m = 1) in (Fig. 3 (c)).\nTime Complexity of gk-SVM: We also split operations of gk-SVM into those indirectly (preprocessing) or directly (kernel-update) updating Nm.\nPre-processing: Assuming u unique g-mers exist in D, then the number of leafnodes in the trie is u. The time taken to construct the trie equals O(ug).\nKernel Update: For each leafnode of the trie (total u nodes), for each g-mer in its nodelist (assuming average size of nodelist is \u2318), gk-SVM uses the matching count among g-mers to update involved sequences\u2019 entries in Nm (if hamming distance between two g-mers is m). Therefore the time cost is O(\u2318uN2) (at the worst case). Essentially \u2318 represents on average the number of unique g-mers (in the trie) that are at a hamming distance up to M from the current leafnode. That is\n\u2318 = min(u,\nM=(g k)X\nm=0\n\u2713 g\nm\n\u25c6 (\u2303 1)m) \u21e0 min(u, cgk(\u2303 1)M ) (10)\nFig. 4 shows that \u2318 grows exponentially to M until reaching its maximum u. The total complexity of time cost from gk-SVM is thus O(ug + u\u2318N2). Asymptotically, at the worst case when \u2318 = u, the time complexity of gk-SVM is O(ug + u2N2).\nComparing Time Complexity of GaKCo with gk-SVM: Table 2 compares the asymptotic time cost of GaKCo with gk-SVM. Asymptotically the time complexity of gvm-SVM is O(ug + \u2318uN2). For GaKCo the overall time complexity is O(cgk[gNl + zN\n2]). In gk-SVM the term O(\u2318uN2) dominates the overall time. For GaKCo the term O(cgkzN\n2) dominates the time cost. For simplicity, we assume that z = u even though z  u. Upon comparing O(\u2318\u21e5uN2) of gk-SVM with O(cgk\u21e5uN2) of GaKCo, clearly the di\u21b5erence lies between the terms \u2318 in gk-SVM and cgk in GaKCo. In details,\n\u2013 \u2318 \u21e0 cgk(\u2303 1)M (gk-SVM) versus cgk (GaKCo): For a given g-mer g, the number of possible g-mers that are at a distance M from it is cgk(\u2303 1)M . That is, g M positions can be substituted with\n(\u2303 1)M possible characters. Thus in gk-SVM, the estimated size \u2318 grows exponentially with number of allowed mismatches M . We show the trend of function f = cgk(\u2303 1)M in Fig. 4 (a) for three di\u21b5erent domains - TF-DNA (\u2303 = 4), SCOP-protein (\u2303 = 20) and text (\u2303 = 36) by varying the values of M for g = 10. We threshold these curves at u = 6 \u21e5 104, which is the average observed value of u across multiple datasets. In Fig. 4 (a), when dictionary size \u2303 is small (=4), the size of the nodelist \u2318 is mostly smaller than u. But when \u2303 is larger than 4, \u2318 gets larger than u for M 4. In contrast, GaKCo\u2019s term cgk is independent of the dictionary size \u2303. \u2013 \u2318 = u (gk-SVM) versus cgk (GaKCo): For large dictionary size (e.g. \u2303 = 20), size of the nodelist \u2318 mostly equals to u in gk-SVM. Even for cases with small dictionary size (e.g. \u2303 = 4) \u2318 is close to u for M 4. While gk-SVM might be fast for small \u2303 and M < 4, its kernel calculation time will slow down considerably for M 4. For example, for one of the SCOP datasets, when g = 10, count of unique g-mers u = 6\u21e5 104 at M = 4 (close to u shown in Fig. 4 (a)). Therefore, at a modest value of M = 4 \u2318 = 6\u21e5 106 for gk-SVM while cgk = 210 for GaKCo. The former is approximately 300 times higher than GaKCo.\n1 2 3 4 5 6 7 8 9\n10 11 12\n1 2 3 4 5 6 7\nlo g\n(\n)\nM=(g-k) DNA( =4) Protein( =20) Text ( =36)\nu ~600,000"}, {"heading": "2.4 Connecting to Previous Studies", "text": "String Kernels Aside from the spectrum kernel [17] and gapped k-mer kernel [10], a few other notable string kernels include (but are not limited to): (1) (k, m)-Mismatch Kernel. This kernel calculates the dot product of contiguous k-mer counts with m mismatches allowed. The cumulative matching statistic idea was first proposed by [14] for this kernel. 4 (2) Substring Kernel. It measures the similarity between sequences based on common co-occurrence of exact matching subpatterns (e.g., substrings) [31]. (3) Profile Kernel. This method uses the notion of similarity based on a probabilistic model (e.g. profile) [13]. (4) Cluster Kernel. The \u201csequence neighborhood\u201d kernel or \u201ccluster\u201d kernel [7] is a semi-supervised extension of the string kernel. It replaces every sequence with a set of \u201csimilar\u201d (neighboring) sequences and obtains a new representation. Then, it averages over the representations of these contiguous sequences found in the unlabeled data using a sequence similarity measure.\nk-mer counting methods k-mer (or in our case, g-mer) counting is the method by which we determine the number of matching or unique k-mers (or g-mers) in any text or pattern. Tools handling large text datasets need to filter out these unique k-mers (or g-mers) to reduce the processing or counting time. GaKCo uses a \u2018sort and count\u2019 method for calculating the number of matching g-mers to compute the mismatch profile (Section 2.2). This is a widely used method that lists all the g-mers, sorts them lexicographically and counts all the consecutive matching entries while skipping the unique g-mers. It has been used previously in tools used for genome assembly [22], discovery of motifs (or most common fixed length patterns) [25], and string kernel calculation [14].\nBioSequence Classification with Deep Learning In recent years, deep learning models have become popular in the bioinformatics community, owing to their ability to extract meaningful representations from large labeled datasets (e.g., with sample size \u21e030,000 sequences). For example, Qi et al. [24] used a deep multi-layer perceptron (MLP) architecture with multitask learning to perform sequence-based\n4 Because [14] uses all possible k-mers built from the dictionary with m mismatches as the feature space, the authors [14] need to precompute a complex weight matrix to incorporate all possible k-mers with m mismatches.\nprotein structure prediction. Zhou et al. [33] created a generative stochastic network to predict secondary structure on the same data as used by Qi et al. [24]. Recently, Lin et al. [19] outperformed all the state-of-the-art works for protein property prediction task by using a deep convolutional neural network architecture. Later, Alipanahi et al. [1] applied a convolutional neural network model for predicting sequence specificity of DNA and RNA-binding proteins as well as generating motifs, or consensus patterns, from the features that were learned by their model. Lanchantin et al. [16] proposed a deep convolutional/highway MLP framework for the same task and demonstrated improved performance. In the field of natural language processing, multiple works like [29] have used deep learning models for document [32] or sentiment [28] classification."}, {"heading": "3 Experiments", "text": ""}, {"heading": "3.1 Benchmark Tasks of Sequence Classification", "text": "DNA and Protein Sequence Classification Studying DNA and Protein sequences gives us deeper insight into the biological processes that can, in turn, help us understand cell development and diseases. Two major tasks essential in the field are Transcription Factor Binding Site (TFBS) Prediction and Remote Protein Homology Prediction.\nTranscription factors (TFs) are regulatory proteins that bind to functional sites of DNA to control the regulation of genes. Each di\u21b5erent TF binds to specific locations (or sites) on a genomic sequence to regulate cell machinery. Owing to the development of chromatin immunoprecipitation and massively parallel DNA sequencing (ChIP-seq) technologies [23], maps of genome-wide binding sites are currently available for multiple TFs across di\u21b5erent organisms. Because ChIP-seq experiments are slow and expensive, computational methods to identify TFBSs accurately are essential for understanding cell regulation.\nRemote Protein Homology Prediction, i.e. classification of protein sequences according to their biological function or structure, plays a significant role in drug development. Protein sequences that are a part of the same protein superfamily are evolutionally related and functionally and structurally relevant to each other [3]. Protein sequences with feature patterns showing high homology are classified into the same superfamily. Once assigned a family, the properties of the protein can be easily narrowed down by analyzing only the superfamily to which it belongs.\nResearchers have formulated both these tasks as classification tasks, where knowing a DNA or protein sequence, we would like to classify it as a binding site or non-binding site for TF prediction and belonging or not belonging to a protein family for homology prediction respectively.\nText Classification Text classification incorporates multiple tasks like assigning subject categories or topics to documents, spam detection, language detection, sentiment analysis, etc. Generally, given a document and a fixed number of classes, the classification model has to predict the class that is most relevant to that document. Several recent studies have discovered that character-based representation provides straightforward and powerful models for relation extraction [27], sentiment classification [32], and transition based parsing [4]. Lodhi et. al. [20] first used string kernels with character level features for text categorization. However, their kernel computation used dynamic programming which was computationally intensive. Over recent years, more e cient string kernel methods have been devised [17, 13\u201315, 10]. Therefore, we use simple character-based text input for document and sentiment classification task.\nWe perform 19 di\u21b5erent classification tasks to evaluate the performance of GaKCo. These tasks belong to the discussed three categories: (1) TF binding site prediction (DNA dataset), (2) Remote Protein Homology prediction (protein dataset), and (3) Character-based English text classification (text dataset)."}, {"heading": "3.2 Experimental Setup", "text": "Datasets: ENCODE ChIP-Seq DNA Sequences: Maps of genome-wide binding sites are currently available for multiple TFs for human genome via the ENCODE [8] database. These ChIP-seq \u201cmaps\u201d mark the positions of the TF binding sites. We select 100 basepair sequences overlapping the binding sites as positive sequences and randomly select non-binding sequences from the human genome as negative sequences. We perform this selection for five di\u21b5erent transcription factors (CTCF, EP300, JUND, RAD21, and SIN3A) from the K562 (leukemia) cell type, resulting in five di\u21b5erent prediction tasks. We select 2000 sequences for training that consist of 1000 positive and negative samples each. For testing, we use another set of 2000 sequences with 1000 positive and negative samples each. We use the dictionary size of 5 (\u2303 = 5). There are four nucleotide symbols - A, T, C, G - in the DNA. Additionally, sometimes sequences have \u2018N\u2019 for nucleotides that are not read by the sequencing machines. Therefore, the dictionary is {A,T,C,G,N}. SCOP Protein Sequences: The SCOP domain database consists of protein domains, no two of which have 90% or more residual identity [12]. It is hierarchically divided into folds, superfamilies, and finally families. We use 12 sets of samples (listed in Table 3.1) and select positive test sequences (for each sample) from 1 protein family of a particular superfamily. We obtain the positive training sequences from remaining families in that superfamily. We select negative training and test sequences from non-overlapping folds outside the positive sequence fold. We use the dictionary size of 20 (\u2303 = 20) as there are 20 amino acid symbols that make up a protein sequence. Therefore the dictionary is {A,C,D,E,F,G,H,I,K,L,M,N,P,Q,R,S,T,V,W,Y}. WebKB and Sentiment Classification Datasets: The documents in the WebKB are web pages collected by the World Wide Knowledge Base project of the CMU text learning group and were downloaded from The 4 Universities Data Set Homepage. These pages were collected from computer science departments of various universities in 1997. We downloaded the processed datasets (removed stop/short words, stemming, etc.) from [5]. This task is a multi-class classification task with four classes: project, course, faculty, and student. For the sentiment analysis experiments, we used the Stanford sentiment treebank dataset [28]. This dataset provides a score for each sentence between 0 and 1 with [0, 0.4] being negative sentiment and [0.6, 1.0] being positive. We combined the validation set in the original treebank dataset with the training set. We use the dictionary size of 36 (\u2303 = 36) since we use character-based input. The dictionary includes all the alphabets [A-Z] and numbers [0-9]. Details of the datasets are in Table 3.\nBaselines: We compare the kernel calculation times and empirical performance of GaKCo with gk-SVM [10]. We also run the CNN implementation from [16] for all the datasets.\nClassification: After calculation, we input the N \u21e5 N kernel matrix into an SVM classifier as an empirical feature map using a linear kernel in LIBLINEAR [9]. Here N is the number of sequences in each dataset. The SVM maximizes the margin between the positive and negative instances of the samples in the kernel defined feature space. For the multi-class classification of WebKB data, we use the multi-class version of LIBSVM [6].\nModel parameters: We vary the hyperparameters g 2 {7, 8, 9, 10} and k 2 {1, 2, . . . , g 1} of both GaKCo and gkm-SVM. M = (g k) for all these cases. We also tune the hyperparameter C 2 {0.01, 0.1, 1, 10, 100, 1000} for the SVM. We present the results for the best g, k, and C values based\non the empirical performance metric. We ran the CNN model with default parameters for 50 epochs (number of training and testing times), and we present the results for the epoch with the best empirical performance metric.\nEvaluation Metrics: Running time: We compare the kernel calculation times of GaKCo and gk-SVM in seconds. In some figures, we have represented time in log-scale (log(seconds)) to accommodate large values. All runtime experiments have been performed on AMD Opteron(TM) Processor 6376 @ 2.30GHz with 250GB memory. Empirical performance: We use the Area Under Curve (AUC) score (from the Receiver Operating Characteristic (ROC) curve) as our empirical evaluation metric for 18 binary classification tasks. We report the results of WebKB multi-class classification using micro-averaged F1 score."}, {"heading": "3.3 Kernel Calculation Time Performance", "text": "Our experimental results confirm our theoretical analysis in Section 2 that GaKCo has a lower kernel calculation time than gk-SVM. Fig. 1(a) shows GaKCo is faster than gk-SVM for 16/19 tasks. The other three tasks for which GaKCo records similar kernel calculation times are DNA sequence prediction tasks. This is expected as DNA has a smaller dictionary size (\u2303 = 5) and thus, for a small number of allowed mismatches (M) gk-SVM gives comparable speed performance. We elaborate on this further in the following discussion.\nGaKCo scales better than gk-SVM for large dictionary size (\u2303) and large number of mismatches (M): Fig. 5 shows the kernel calculation times of GaKCo and gk-SVM for the best-performing g and varying k = {1, 2, . . . (g 1)} for three binary classification datasets: (a) EP300 (DNA), (b) 1.34 (protein), and (c)Sentiment (text) respectively. We select these three datasets as they achieve the best AUC scores out of all 19 tasks (see Supplementary). We fix g and vary k to show time performance for di\u21b5erent number of allowed mismatches i.e. M = (g k) = {1, 2, . . . (g 1)}. For GaKCo, the results are plotted for both single-thread and the multi-thread implementations. We refer to the multi-thread implementation as GaKCo because that is our final code version. Our results show that GaKCo (single-thread) scales better than gk-SVM for a large dictionary size (\u2303) and a large number of mismatches (M). The final version of GaKCo (multi-thread) further improves the performance. Details for each dataset are as follows:\n\u2013 DNA dataset (\u2303 = 5): In Fig. 5 (a), we plot the kernel calculation times for best g = 10 and varying k with M = {1, 2, . . . 9} for EP300 dataset. As expected, since the dictionary size of DNA dataset\n(\u2303) is small, gk-SVM performs fast kernel calculations for M = (g k) < 4. However, for large M 4, its kernel calculation time increases considerably compared to GaKCo. This result connects to Fig. 4 in Section 2, where our analysis showed that the nodelist size becomes closer to u as M increases, thus increasing the time cost.\n\u2013 Protein dataset (\u2303 = 20): Fig. 5 (b), shows the kernel calculation times for best g = 10 and varying k with M = (g k) = {1, 2, . . . 9} for 1.34 dataset. Since the dictionary size of protein dataset (\u2303) is larger than DNA, gk-SVM\u2019s kernel calculation time is worse than GaKCo even for smaller values of M < 4. This also connects to Fig. 4 where the size of nodelist \u21e0 u even for small M for protein dataset, resulting in higher time cost. For best-performing parameters g = 10, k = 1(M = 9), gk-SVM takes 5 hours to calculate the kernel, while GaKCo calculates it in 4 minutes.\n\u2013 Text dataset (\u2303 = 36): Fig. 5 (c), shows the kernel calculation times for best g = 8 and varying k with M = {1, 2, . . . 7} for Sentiment dataset. The results follow the same trend as presented above. For large M 4, kernel calculation time of gk-SVM is slower as compared to GaKCo. One would expect that with large dictionary size (\u2303) the performance di\u21b5erence will be same as that for protein dataset. However, unlike protein sequences, where the substitution of all 20 characters in a g-mer is equally likely, text dataset has an underlying structure. Concretely, the chance of substitution of some characters in a g-mer will be higher than others. For example, in a given g-mer \u201cmy nam\u201d, the last position is more likely to be occupied by \u2018e\u2019 than \u2018z\u2019. Therefore, even though the dictionary size is large, the growth of the nodelist is restricted by the underlying structure of the text. Therefore, while GaKCo\u2019s time performance is consistent across all three datasets, gk-SVM\u2019s time performance varies due to the characteristic properties (like dictionary size (\u2303)) of the datasets.\nAccording to our asymptotic analysis in Section 2, GaKCo should always be faster than gk-SVM. However, in Fig. 5 we notice that for certain cases (e.g. for DNA when M < 4 in Fig. 5) GaKCo\u2019s speed is lower than gk-SVM. This is because, in our analysis, we theoretically estimate the size of gk-SVM\u2019s nodelist. We see that in practice, the actual nodelist size is smaller than our estimate for certain cases where gk-SVM is faster than GaKCo. However, with a larger value of M( 4) or dictionary size (\u2303 > 5), the nodelist size in practice matches our theoretical estimation, therefore, GaKCo always has lower kernel calculation times than gk-SVM for these cases.\nGaKCo is independent of dictionary size (\u2303): GaKCo\u2019s time complexity analysis (Section 2) shows that it is independent of the \u2303M term, which controls the size of gk-SVM\u2019s nodelist. In Fig. 6 (a), we plot the average kernel calculation times for the best performing (g, k) parameters for DNA (\u2303 = 5),\nprotein (\u2303 = 20), and text (\u2303 = 36) datasets respectively. The results validate our analysis. We find that gk-SVM takes similar time as GaKCo to calculate the kernel for DNA dataset due to the small dictionary size. However, when the dictionary size increases for protein and text datasets, it slows down considerably. GaKCo, on the other hand, is consistently faster for all three datasets, despite the increase in dictionary size.\nGaKCo algorithm benefits from parallelization: As discussed earlier, the calculation of Cm (such that m = {0, 1 . . . M = (g k)}) is an independent procedure in GaKCo\u2019s algorithm. This property makes GaKCo naturally parallelizable. We implement the final parallelized version of GaKCo by distributing calculation of Cm across m threads where, m = {0, 1 . . . M = (g k)}. We have already witnessed that the multi-thread version of GaKCo improves the speed of its single thread version in Fig. 5. Next, in Fig. 6(b) we plot the average kernel calculation times across DNA (5), protein (12) and text (2) datasets for both multi-thread and single thread implementations. Through this figure, we demonstrate that the improvement in speed by parallelization is consistent across all datasets.\nGaKCo scales better than gk-SVM for increasing number of sequences (N): We now compare the kernel calculation times of GaKCo versus gk-SVM for increasing number of sequences (N). In Fig. 6(c), we plot the kernel calculation times of GaKCo and gk-SVM for best performing parameters (g, k) for three binary classification datasets: EP300 (DNA), 1.34 (protein), and Sentiment (text). We select these three datasets as they provide the best AUC scores out of all 19 tasks (see Supplementary). To show the e\u21b5ect of increasing N = {100, 250, 500, 750} on kernel calculation times, we fix the length of the sequences for all three datasets to l = 100. As expected, the time grows for both the algorithms with the increase in the number of sequences. However, this growth in time is more drastic for gk-SVM than for GaKCo across all three datasets. Therefore, GaKCo is ideal for adaptive training since its kernel calculation time increases more gradually than gk-SVM as new sequences are added.\nGaKCo is both time and memory e cient: Fig. 7 (a), shows points for the kernel calculation time (Xaxis) versus the memory usage (Y-axis) for both GaKCo and gk-SVM for all 19 classification tasks. We observe that most of these points representing GaKCo lie in the lower-left quadrant indicating that it is both time and memory e cient. For 17/19 tasks, its memory usage is lesser or comparable to gk-SVM with faster kernel calculation time. Therefore, GaKCo\u2019s time improvement over the baseline is achieved with almost no added memory cost. 5"}, {"heading": "3.4 Empirical Performance", "text": "Fig. 1 (b) demonstrated that GaKCo achieves same empirical performance as gk-SVM (AUC scores or F1-score). This is because GaKCo\u2019s gapped k-mer formulation is same as gk-SVM but with improved (faster) implementation. In this section, we compare GaKCo\u2019s empirical performance with state-of-theart CNN model [16]. Fig. 7 (b) shows the di\u21b5erences in AUC Scores (or micro-averaged F1-score for Web-KB) of GaKCo and CNN [16]. For 16/19 tasks, GaKCo outperforms the CNN model with an average of \u21e0 20% accuracy. This result can be explained by the fact that CNNs trained with a small number of samples (1000-10,000 sequences) often exhibit unstable behavior in performance.\nFor three datasets - SIN3A (DNA), 1.1 (protein), and Web-KB (text), we observe that the empirical performance of GaKCo and CNN is similar. Therefore, we further explore these datasets in Fig. 7(c). Here, we plot the AUC scores or micro-averaged F1 scores (Web-KB) for varying number of training sample (N = {100, 250, 500 and 750} sequences). We randomly select these samples from the training set and use the original test set of the respective datasets. The results are averaged over three runs of the experiment. Our aim is to find the threshold (number of training samples) for which CNN gives a lower performance to GaKCo for these three datasets. Fig. 7(c) presents the averaged AUC scores or micro-averaged F1 score (Web-KB). We see that the threshold for which CNN gives a lower performance to GaKCo is 750 sequences in the training set. We also observe that the variance in performance is high for NN (represented by error bars) across the three runs."}, {"heading": "4 Conclusion", "text": "In this paper, we presented GaKCo, a fast and naturally parallelizable algorithm for gapped k-mer based string kernel calculation. For sequence classification, it is an e cient and complementary approach to DNN system when training samples are scarce (see Fig. 7(b)). The advantages of this work are:\n\u2013 Fast: GaKCo is a novel combination of two e cient concepts: (1) reduced gapped k-mer feature space and (2) associative array based counting method, making it faster than the state-of-the-art gapped k-mer string kernel, while achieving same accuracy. (observed in Fig. 1).\n\u2013 GaKCo can scale up to larger values of m and \u2303. (Fig. 5 and Fig. 6(a))\n\u2013 Parallelizable: GaKCo algorithm naturally leads to a parallelizable implementation (Fig. 5 and Fig. 6 (b))\n\u2013 We have provided a detailed theoretical analysis comparing the asymptotic time complexity of GaKCo with gk-SVM. This analysis, to the best of the authors\u2019 knowledge, has not been reported before (Section 2.3)."}], "references": [{"title": "Predicting the sequence specificities of DNA- and RNA- binding proteins by deep learning", "author": ["Babak Alipanahi", "Andrew Delong", "Matthew T Weirauch", "Brendan J Frey"], "venue": "Nature Biotechnology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Sequence and chromatin determinants of cell-type\u2013specific transcription factor binding", "author": ["Aaron Arvey", "Phaedra Agius", "William Sta\u21b5ord Noble", "Christina Leslie"], "venue": "Genome research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Bioinformatics: the machine learning approach", "author": ["Pierre Baldi", "S\u00f8ren Brunak"], "venue": "MIT press,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A Smith"], "venue": "arXiv preprint arXiv:1508.00657,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Improving Methods for Single-label Text, Categorization", "author": ["Ana Cardoso-Cachopo"], "venue": "PdD Thesis, Instituto Superior Tecnico, Universidade Tecnica de Lisboa,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Cluster kernels for semi-supervised learning", "author": ["Olivier Chapelle", "Jason Weston", "Bernhard Sch\u00f6lkopf"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "et al", "author": ["ENCODE Project Consortiu"], "venue": "An integrated encyclopedia of dna elements in the human genome. Nature, 489(7414):57\u201374,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of machine learning research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Enhanced regulatory sequence prediction using gapped k-mer features", "author": ["Mahmoud Ghandi", "Dongwon Lee", "Morteza Mohammad-Noori", "Michael A Beer"], "venue": "PLoS Comput Biol,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Robust k-mer frequency estimation using gapped k-mers", "author": ["Mahmoud Ghandi", "Morteza Mohammad-Noori", "Michael A Beer"], "venue": "Journal of mathematical biology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "A discriminative framework for detecting remote protein homologies", "author": ["Tommi Jaakkola", "Mark Diekhans", "David Haussler"], "venue": "Journal of computational biology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Profilebased string kernels for remote homology detection and motif extraction", "author": ["Rui Kuang", "Eugene Ie", "Ke Wang", "Kai Wang", "Mahira Siddiqi", "Yoav Freund", "Christina Leslie"], "venue": "Journal of bioinformatics and computational biology,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Scalable algorithms for string kernels with inexact matching", "author": ["Pavel P. Kuksa", "Pai-Hsi Huang", "Vladimir Pavlovic"], "venue": "In NIPS\u201908,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "E cient use of unlabeled data for protein sequence classification: a comparative study", "author": ["Pavel P. Kuksa", "Pai-Hsi Huang", "Vladimir Pavlovic"], "venue": "BMC Bioinformatics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Deep motif dashboard: Visualizing and understanding genomic sequences using deep neural networks", "author": ["Jack Lanchantin", "Ritambhara Singh", "Beilun Wang", "Yanjun Qi"], "venue": "arXiv preprint arXiv:1608.03644,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Fast string kernels using inexact matching for protein sequences", "author": ["Christina Leslie", "Rui Kuang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "The spectrum kernel: A string kernel for svm protein classification", "author": ["Christina S. Leslie", "Eleazar Eskin", "William Sta\u21b5ord Noble"], "venue": "In Pacific Symposium on Biocomputing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "MUST-CNN: A multilayer shift-and-stitch deep convolutional architecture for sequence-based protein structure prediction", "author": ["Zeming Lin", "Jack Lanchantin", "Yanjun Qi"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Text classification using string kernels", "author": ["Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Biology: The big challenges of big data", "author": ["Vivien Marx"], "venue": "Nature, 498(7453):255\u2013260,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Aggressive assembly of pyrosequencing reads with mates", "author": ["Jason R Miller", "Arthur L Delcher", "Sergey Koren", "Eli Venter", "Brian P Walenz", "Anushka Brownley", "Justin Johnson", "Kelvin Li", "Clark Mobarry", "Granger Sutton"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Chip\u2013seq: advantages and challenges of a maturing technology", "author": ["Peter J Park"], "venue": "Nature Reviews Genetics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "A unified multitask architecture for predicting local protein properties", "author": ["Yanjun Qi", "Merja Oja", "Jason Weston", "William Sta\u21b5ord Noble"], "venue": "PloS One,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Exact algorithms for planted motif problems", "author": ["Sanguthevar Rajasekaran", "Sudha Balla", "C-H Huang"], "venue": "Journal of Computational Biology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Seqgl identifies context-dependent binding signals in genome-wide regulatory element maps", "author": ["Manu Setty", "Christina S Leslie"], "venue": "PLoS Comput Biol,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Character based string kernels for bio-entity relation detection", "author": ["Ritambhara Singh", "Yanjun Qi"], "venue": "ACL 2016,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "In EMNLP,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Statistical Learning Theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Alexander Johannes Smola, et al", "author": ["SVN Vishwanathan"], "venue": "Fast kernels for string and tree matching. Kernel methods in computational biology, pages 113\u2013130,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Deep supervised and convolutional generative stochastic network for protein secondary structure prediction", "author": ["Jian Zhou", "Olga G Troyanskaya"], "venue": "arXiv preprint arXiv:1403.1347,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "For example, with the advancement of sequencing technologies, a massive amount of protein and DNA sequence data is produced daily [21].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "SK-SVM methods have been successfully used for classifying sequences like DNA [17, 14, 2, 26], protein [12] or character based natural language text [27].", "startOffset": 78, "endOffset": 93}, {"referenceID": 13, "context": "SK-SVM methods have been successfully used for classifying sequences like DNA [17, 14, 2, 26], protein [12] or character based natural language text [27].", "startOffset": 78, "endOffset": 93}, {"referenceID": 1, "context": "SK-SVM methods have been successfully used for classifying sequences like DNA [17, 14, 2, 26], protein [12] or character based natural language text [27].", "startOffset": 78, "endOffset": 93}, {"referenceID": 25, "context": "SK-SVM methods have been successfully used for classifying sequences like DNA [17, 14, 2, 26], protein [12] or character based natural language text [27].", "startOffset": 78, "endOffset": 93}, {"referenceID": 11, "context": "SK-SVM methods have been successfully used for classifying sequences like DNA [17, 14, 2, 26], protein [12] or character based natural language text [27].", "startOffset": 103, "endOffset": 107}, {"referenceID": 26, "context": "SK-SVM methods have been successfully used for classifying sequences like DNA [17, 14, 2, 26], protein [12] or character based natural language text [27].", "startOffset": 149, "endOffset": 153}, {"referenceID": 29, "context": "They have provided state-of-the-art classification accuracy and can guarantee nice asymptotic behavior due to SVM\u2019s convex formulation and theoretical property [30].", "startOffset": 160, "endOffset": 164}, {"referenceID": 9, "context": "[10] developed the state-of-the-art SK-SVM tool called gk-SVM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "gk-SVM uses a gapped k-mer formulation [11] that reduces the feature space considerably compared to other k-mer based SK approaches.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "GaKCo uses the associative array-based data structure to calculate kernel similarity through cumulative k-mer counting [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "Related Work: Recently, Deep Neural Networks (NNs) have provided state-of-the-art performances for various sequence classification tasks like analyzing DNA [1, 16], proteins [24, 33], and natural language [32, 28] sequences.", "startOffset": 156, "endOffset": 163}, {"referenceID": 15, "context": "Related Work: Recently, Deep Neural Networks (NNs) have provided state-of-the-art performances for various sequence classification tasks like analyzing DNA [1, 16], proteins [24, 33], and natural language [32, 28] sequences.", "startOffset": 156, "endOffset": 163}, {"referenceID": 23, "context": "Related Work: Recently, Deep Neural Networks (NNs) have provided state-of-the-art performances for various sequence classification tasks like analyzing DNA [1, 16], proteins [24, 33], and natural language [32, 28] sequences.", "startOffset": 174, "endOffset": 182}, {"referenceID": 32, "context": "Related Work: Recently, Deep Neural Networks (NNs) have provided state-of-the-art performances for various sequence classification tasks like analyzing DNA [1, 16], proteins [24, 33], and natural language [32, 28] sequences.", "startOffset": 174, "endOffset": 182}, {"referenceID": 31, "context": "Related Work: Recently, Deep Neural Networks (NNs) have provided state-of-the-art performances for various sequence classification tasks like analyzing DNA [1, 16], proteins [24, 33], and natural language [32, 28] sequences.", "startOffset": 205, "endOffset": 213}, {"referenceID": 27, "context": "Related Work: Recently, Deep Neural Networks (NNs) have provided state-of-the-art performances for various sequence classification tasks like analyzing DNA [1, 16], proteins [24, 33], and natural language [32, 28] sequences.", "startOffset": 205, "endOffset": 213}, {"referenceID": 15, "context": "We compare GaKCo\u2019s empirical performance with a state-of-the-art deep convolutional neural network (CNN) model [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 29, "context": "In this space, we apply a standard classifier such as Support Vector Machine (SVM) [30].", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "String kernels ([17, 14, 10]), implicitly compute K(x, x0) as an inner product in the feature space (x) as: K(x, x0) = h (x), (x0)i, (2) 2 There is also one C parameter for tuning SVM training (while using linear kernel)", "startOffset": 16, "endOffset": 28}, {"referenceID": 13, "context": "String kernels ([17, 14, 10]), implicitly compute K(x, x0) as an inner product in the feature space (x) as: K(x, x0) = h (x), (x0)i, (2) 2 There is also one C parameter for tuning SVM training (while using linear kernel)", "startOffset": 16, "endOffset": 28}, {"referenceID": 9, "context": "String kernels ([17, 14, 10]), implicitly compute K(x, x0) as an inner product in the feature space (x) as: K(x, x0) = h (x), (x0)i, (2) 2 There is also one C parameter for tuning SVM training (while using linear kernel)", "startOffset": 16, "endOffset": 28}, {"referenceID": 17, "context": "The string kernel using this representation is called spectrum kernel [18] (see Fig.", "startOffset": 70, "endOffset": 74}, {"referenceID": 10, "context": "[11] introduced a new set of feature representations, called gapped k-mers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "(4) (implemented as gk-SVM [10]) includes only those k-mers whose gapped formulation has appeared as g-mers in a given dataset D.", "startOffset": 27, "endOffset": 31}, {"referenceID": 9, "context": "[10] use this intuition to reformulate Eq.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] formulate this observation formally into the coe cient hm:", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Nm(x, x0) is named as mismatch profile by [10].", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "The state-of-the-art tool gk-SVM [10] calculates Nm(x, x 0) using a trie based data structure that is similar to [17] (with some modifications, details in Section 2.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "The state-of-the-art tool gk-SVM [10] calculates Nm(x, x 0) using a trie based data structure that is similar to [17] (with some modifications, details in Section 2.", "startOffset": 113, "endOffset": 117}, {"referenceID": 9, "context": "Implementations GaKCo gk-SVM [10] Pre-processing cgkgNl ug Kernel updates cgkzN 2 \u2318uN Table 2.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "\u2318: estimated size of nodelist used in gk-SVM [10].", "startOffset": 45, "endOffset": 49}, {"referenceID": 16, "context": "String Kernels Aside from the spectrum kernel [17] and gapped k-mer kernel [10], a few other notable string kernels include (but are not limited to): (1) (k, m)-Mismatch Kernel.", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "String Kernels Aside from the spectrum kernel [17] and gapped k-mer kernel [10], a few other notable string kernels include (but are not limited to): (1) (k, m)-Mismatch Kernel.", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "The cumulative matching statistic idea was first proposed by [14] for this kernel.", "startOffset": 61, "endOffset": 65}, {"referenceID": 30, "context": ", substrings) [31].", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": "profile) [13].", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "The \u201csequence neighborhood\u201d kernel or \u201ccluster\u201d kernel [7] is a semi-supervised extension of the string kernel.", "startOffset": 55, "endOffset": 58}, {"referenceID": 21, "context": "It has been used previously in tools used for genome assembly [22], discovery of motifs (or most common fixed length patterns) [25], and string kernel calculation [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 24, "context": "It has been used previously in tools used for genome assembly [22], discovery of motifs (or most common fixed length patterns) [25], and string kernel calculation [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "It has been used previously in tools used for genome assembly [22], discovery of motifs (or most common fixed length patterns) [25], and string kernel calculation [14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "[24] used a deep multi-layer perceptron (MLP) architecture with multitask learning to perform sequence-based", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "4 Because [14] uses all possible k-mers built from the dictionary with m mismatches as the feature space, the authors [14] need to precompute a complex weight matrix to incorporate all possible k-mers with m mismatches.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "4 Because [14] uses all possible k-mers built from the dictionary with m mismatches as the feature space, the authors [14] need to precompute a complex weight matrix to incorporate all possible k-mers with m mismatches.", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "Text Classification Stanford Treebank Sentiment 3883 3579 877 878 9217 36 260 Dataset from [5] WebKB 335, 620, 744, 1083 166, 306, 371, 538 4163 36 14218", "startOffset": 91, "endOffset": 94}, {"referenceID": 32, "context": "[33] created a generative stochastic network to predict secondary structure on the same data as used by Qi et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] outperformed all the state-of-the-art works for protein property prediction task by using a deep convolutional neural network architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] applied a convolutional neural network model for predicting sequence specificity of DNA and RNA-binding proteins as well as generating motifs, or consensus patterns, from the features that were learned by their model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16] proposed a deep convolutional/highway MLP framework for the same task and demonstrated improved performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "In the field of natural language processing, multiple works like [29] have used deep learning models for document [32] or sentiment [28] classification.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "In the field of natural language processing, multiple works like [29] have used deep learning models for document [32] or sentiment [28] classification.", "startOffset": 114, "endOffset": 118}, {"referenceID": 27, "context": "In the field of natural language processing, multiple works like [29] have used deep learning models for document [32] or sentiment [28] classification.", "startOffset": 132, "endOffset": 136}, {"referenceID": 22, "context": "Owing to the development of chromatin immunoprecipitation and massively parallel DNA sequencing (ChIP-seq) technologies [23], maps of genome-wide binding sites are currently available for multiple TFs across di\u21b5erent organisms.", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "Protein sequences that are a part of the same protein superfamily are evolutionally related and functionally and structurally relevant to each other [3].", "startOffset": 149, "endOffset": 152}, {"referenceID": 26, "context": "Several recent studies have discovered that character-based representation provides straightforward and powerful models for relation extraction [27], sentiment classification [32], and transition based parsing [4].", "startOffset": 144, "endOffset": 148}, {"referenceID": 31, "context": "Several recent studies have discovered that character-based representation provides straightforward and powerful models for relation extraction [27], sentiment classification [32], and transition based parsing [4].", "startOffset": 175, "endOffset": 179}, {"referenceID": 3, "context": "Several recent studies have discovered that character-based representation provides straightforward and powerful models for relation extraction [27], sentiment classification [32], and transition based parsing [4].", "startOffset": 210, "endOffset": 213}, {"referenceID": 19, "context": "[20] first used string kernels with character level features for text categorization.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Over recent years, more e cient string kernel methods have been devised [17, 13\u201315, 10].", "startOffset": 72, "endOffset": 87}, {"referenceID": 12, "context": "Over recent years, more e cient string kernel methods have been devised [17, 13\u201315, 10].", "startOffset": 72, "endOffset": 87}, {"referenceID": 13, "context": "Over recent years, more e cient string kernel methods have been devised [17, 13\u201315, 10].", "startOffset": 72, "endOffset": 87}, {"referenceID": 14, "context": "Over recent years, more e cient string kernel methods have been devised [17, 13\u201315, 10].", "startOffset": 72, "endOffset": 87}, {"referenceID": 9, "context": "Over recent years, more e cient string kernel methods have been devised [17, 13\u201315, 10].", "startOffset": 72, "endOffset": 87}, {"referenceID": 7, "context": "Datasets: ENCODE ChIP-Seq DNA Sequences: Maps of genome-wide binding sites are currently available for multiple TFs for human genome via the ENCODE [8] database.", "startOffset": 148, "endOffset": 151}, {"referenceID": 11, "context": "SCOP Protein Sequences: The SCOP domain database consists of protein domains, no two of which have 90% or more residual identity [12].", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": ") from [5].", "startOffset": 7, "endOffset": 10}, {"referenceID": 27, "context": "For the sentiment analysis experiments, we used the Stanford sentiment treebank dataset [28].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 1, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 2, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 3, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 4, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 5, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 6, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 7, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 8, "context": "The dictionary includes all the alphabets [A-Z] and numbers [0-9].", "startOffset": 60, "endOffset": 65}, {"referenceID": 9, "context": "Baselines: We compare the kernel calculation times and empirical performance of GaKCo with gk-SVM [10].", "startOffset": 98, "endOffset": 102}, {"referenceID": 15, "context": "We also run the CNN implementation from [16] for all the datasets.", "startOffset": 40, "endOffset": 44}, {"referenceID": 8, "context": "Classification: After calculation, we input the N \u21e5 N kernel matrix into an SVM classifier as an empirical feature map using a linear kernel in LIBLINEAR [9].", "startOffset": 154, "endOffset": 157}, {"referenceID": 5, "context": "For the multi-class classification of WebKB data, we use the multi-class version of LIBSVM [6].", "startOffset": 91, "endOffset": 94}, {"referenceID": 15, "context": "(b) Di\u21b5erences in AUC Scores (or micro-averaged F1-score for Web-KB) between GaKCo and state-of-the-art CNN model [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "In this section, we compare GaKCo\u2019s empirical performance with state-of-theart CNN model [16].", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "7 (b) shows the di\u21b5erences in AUC Scores (or micro-averaged F1-score for Web-KB) of GaKCo and CNN [16].", "startOffset": 98, "endOffset": 102}], "year": 2017, "abstractText": "String Kernel (SK) techniques, especially those using gapped k-mers as features (gk), have obtained great success in classifying sequences like DNA, protein, and text. However, the state-of-the-art gk-SK runs extremely slow when we increase the dictionary size (\u2303) or allow more number of mismatches (M). This is because current gk-SK uses a trie-based algorithm to calculate co-occurrence of mismatched substrings resulting in a time cost proportional to O(\u2303 ). We propose a fast algorithm for calculating Gapped k-mer Kernel using Counting (GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of substrings using cumulative counting. This algorithm is fast, scalable to larger \u2303 and M , and naturally parallelizable. We provide a rigorous asymptotic analysis that compares GaKCo with the state-of-the-art gk-SK. Theoretically, the time cost of GaKCo is independent of the \u2303 term that slows down the trie-based approach. Experimentally, we observe that GaKCo achieves the same accuracy as the state-of-the-art and outperforms its speed by factors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein (12 datasets), and character-based English text (2 datasets), respectively .", "creator": "LaTeX with hyperref package"}}}