{"id": "1704.07535", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Apr-2017", "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "abstract": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.", "histories": [["v1", "Tue, 25 Apr 2017 04:37:35 GMT  (952kb,D)", "http://arxiv.org/abs/1704.07535v1", "ACL 2017. MR and MS contributed equally"]], "COMMENTS": "ACL 2017. MR and MS contributed equally", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG stat.ML", "authors": ["maxim rabinovich", "mitchell stern", "dan klein"], "accepted": true, "id": "1704.07535"}, "pdf": {"name": "1704.07535.pdf", "metadata": {"source": "CRF", "title": "Abstract Syntax Networks for Code Generation and Semantic Parsing", "authors": ["Maxim Rabinovich", "Mitchell Stern", "Dan Klein"], "emails": ["rabinovich@cs.berkeley.edu", "mitchell@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": null, "text": "Abstract Syntax Networks for Code Generation and Semantic Parsing\nMaxim Rabinovich\u2217 Mitchell Stern\u2217 Dan Klein Computer Science Division\nUniversity of California, Berkeley {rabinovich,mitchell,klein}@cs.berkeley.edu\nAbstract\nTasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark HEARTHSTONE dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-ofthe-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the ATIS, JOBS, and GEO semantic parsing datasets with no task-specific engineering."}, {"heading": "1 Introduction", "text": "Tasks like semantic parsing and code generation are challenging in part because they are structured (the output must be well-formed) but not synchronous (the output structure diverges from the input structure).\nSequence-to-sequence models have proven effective for both tasks (Dong and Lapata, 2016; Ling et al., 2016), using encoder-decoder frameworks to exploit the sequential structure on both the input and output side. Yet these approaches do not account for much richer structural constraints on outputs\u2014including well-formedness, well-typedness, and executability. The wellformedness case is of particular interest, since it can readily be enforced by representing outputs as abstract syntax trees (ASTs) (Aho et al., 2006), an approach that can be seen as a much lighter weight\n\u2217Equal contribution.\nversion of CCG-based semantic parsing (Zettlemoyer and Collins, 2005).\nIn this work, we introduce abstract syntax networks (ASNs), an extension of the standard encoder-decoder framework utilizing a modular decoder whose submodels are composed to natively generate ASTs in a top-down manner. The decoding process for any given input follows a dy-\nar X\niv :1\n70 4.\n07 53\n5v 1\n[ cs\n.C L\n] 2\n5 A\npr 2\n01 7\nnamically chosen mutual recursion between the modules, where the structure of the tree being produced mirrors the call graph of the recursion. We implement this process using a decoder model built of many submodels, each associated with a specific construct in the AST grammar and invoked when that construct is needed in the output tree. As is common with neural approaches to structured prediction (Chen and Manning, 2014; Vinyals et al., 2015), our decoder proceeds greedily and accesses not only a fixed encoding but also an attention-based representation of the input (Bahdanau et al., 2014).\nOur model significantly outperforms previous architectures for code generation and obtains competitive or state-of-the-art results on a suite of semantic parsing benchmarks. On the HEARTHSTONE dataset for code generation, we achieve a token BLEU score of 79.2 and an exact match accuracy of 22.7%, greatly improving over the previous best results of 67.1 BLEU and 6.1% exact match (Ling et al., 2016).\nThe flexibility of ASNs makes them readily applicable to other tasks with minimal adaptation. We illustrate this point with a suite of semantic parsing experiments. On the JOBS dataset, we improve on previous state-of-the-art, achieving 92.9% exact match accuracy as compared to the previous record of 90.7%. Likewise, we perform competitively on the ATIS and GEO datasets, matching or exceeding the exact match reported by Dong and Lapata (2016), though not quite reaching the records held by the best previous semantic parsing approaches (Wang et al., 2014)."}, {"heading": "1.1 Related work", "text": "Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015). In the latter case, work has focused on making the task look like sequence-tosequence prediction, either by flattening the output tree (Vinyals et al., 2015) or by representing it as a sequence of construction decisions (Cross and Huang, 2016; Dyer et al., 2016). Our work differs from both in its use of a recursive top-down generation procedure.\nDong and Lapata (2016) introduced a sequenceto-sequence approach to semantic parsing, includ-\ning a limited form of top-down recursion, but without the modularity or tight coupling between output grammar and model characteristic of our approach.\nNeural (and probabilistic) modeling of code, including for prediction problems, has a longer history. Allamanis et al. (2015) and Maddison and Tarlow (2014) proposed modeling code with a neural language model, generating concrete syntax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval. More recently, Shin et al. (2017) attacked the same problem using a grammar-based variational autoencoder with top-down generation similar to ours instead. Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs (Balog et al., 2016; Liang et al., 2010; Menon et al., 2013).\nThe prediction framework most similar in spirit to ours is the doubly-recurrent decoder network introduced by Alvarez-Melis and Jaakkola (2017), which propagates information down the tree using a vertical LSTM and between siblings using a horizontal LSTM. Our model differs from theirs in using a separate module for each grammar construct and learning separate vertical updates for siblings when the AST labels require all siblings to be jointly present; we do, however, use a horizontal LSTM for nodes with variable numbers of children. The differences between our models reflect not only design decisions, but also differences in data\u2014since ASTs have labeled nodes and labeled edges, they come with additional structure that our model exploits.\nApart from ours, the best results on the codegeneration task associated with the HEARTHSTONE dataset are based on a sequence-tosequence approach to the problem (Ling et al., 2016). Abstract syntax networks greatly improve on those results.\nPreviously, Andreas et al. (2016) introduced neural module networks (NMNs) for visual question answering, with modules corresponding to linguistic substructures within the input query. The primary purpose of the modules in NMNs is to compute deep features of images in the style of convolutional neural networks (CNN). These features are then fed into a final decision layer. In contrast to the modules we describe here, NMN modules do not make decisions about what to generate or which modules to call next, nor do they\nmaintain recurrent state."}, {"heading": "2 Data Representation", "text": ""}, {"heading": "2.1 Abstract Syntax Trees", "text": "Our model makes use of the Abstract Syntax Description Language (ASDL) framework (Wang et al., 1997), which represents code fragments as trees with typed nodes. Primitive types correspond to atomic values, like integers or identifiers. Accordingly, primitive nodes are annotated with a primitive type and a value of that type\u2014for instance, in Figure 3a, the identifier node storing \"create minion\" represents a function of the same name.\nComposite types correspond to language constructs, like expressions or statements. Each type has a collection of constructors, each of which specifies the particular language construct a node of that type represents. Figure 4 shows constructors for the statement (stmt) and expression (expr) types. The associated language constructs include function and class definitions, return statements, binary operations, and function calls.\nComposite types enter syntax trees via composite nodes, annotated with a composite type and a choice of constructor specifying how the node expands. The root node in Figure 3a, for example, is\n1The full grammar can be found online on the documentation page for the Python ast module: https://docs.python.org/3/library/ast. html#abstract-grammar\na composite node of type stmt that represents a class definition and therefore uses the ClassDef constructor. In Figure 3b, on the other hand, the root uses the Call constructor because it represents a function call.\nChildren are specified by named and typed fields of the constructor, which have cardinalities of singular, optional, or sequential. By default, fields have singular cardinality, meaning they correspond to exactly one child. For instance, the ClassDef constructor has a singular name field of type identifier. Fields of optional cardinality are associ-\nated with zero or one children, while fields of sequential cardinality are associated with zero or more children\u2014these are designated using ? and * suffixes in the grammar, respectively. Fields of sequential cardinality are often used to represent statement blocks, as in the body field of the ClassDef and FunctionDef constructors.\nThe grammars needed for semantic parsing can easily be given ASDL specifications as well, using primitive types to represent variables, predicates, and atoms and composite types for standard logical building blocks like lambdas and counting (among others). Figure 2 shows what the resulting \u03bb-calculus trees look like. The ASDL grammars for both \u03bb-calculus and Prolog-style logical forms are quite compact, as Figures 9 and 10 in the appendix show."}, {"heading": "2.2 Input Representation", "text": "We represent inputs as collections of named components, each of which consists of a sequence of tokens. In the case of semantic parsing, inputs have a single component containing the query sentence. In the case of HEARTHSTONE, the card\u2019s name and description are represented as sequences of characters and tokens, respectively, while categorical attributes are represented as single-token sequences. For HEARTHSTONE, we restrict our input and output vocabularies to values that occur more than once in the training set."}, {"heading": "3 Model Architecture", "text": "Our model uses an encoder-decoder architecture with hierarchical attention. The key idea behind our approach is to structure the decoder as a collection of mutually recursive modules. The modules correspond to elements of the AST grammar and are composed together in a manner that mirrors the structure of the tree being generated. A vertical LSTM state is passed from module to module to propagate information during the decoding process.\nThe encoder uses bidirectional LSTMs to embed each component and a feedforward network to combine them. Component- and token-level attention is applied over the input at each step of the decoding process.\nWe train our model using negative log likelihood as the loss function. The likelihood encompasses terms for all generation decisions made by\nthe decoder."}, {"heading": "3.1 Encoder", "text": "Each component c of the input is encoded using a component-specific bidirectional LSTM. This results in forward and backward token encodings ( \u2212\u2192 hc, \u2190\u2212 hc) that are later used by the attention mechanism. To obtain an encoding of the input as a whole for decoder initialization, we concatenate the final forward and backward encodings of each component into a single vector and apply a linear projection."}, {"heading": "3.2 Decoder Modules", "text": "The decoder decomposes into several classes of modules, one per construct in the grammar, which we discuss in turn. Throughout, we let v denote the current vertical LSTM state, and use f to represent a generic feedforward neural network. LSTM updates with hidden state h and input x are notated as LSTM(h,x).\nComposite type modules Each composite type T has a corresponding module whose role is to select among the constructors C for that type. As Figure 5a exhibits, a composite type module receives a vertical LSTM state v as input and applies a feedforward network fT and a softmax output layer to choose a constructor:\np (C | T,v) = [ softmax (fT (v)) ] C .\nControl is then passed to the module associated with constructor C.\nConstructor modules Each constructor C has a corresponding module whose role is to compute an intermediate vertical LSTM state vu,F for each of its fields F whenever C is chosen at a composite node u.\nFor each field F of the constructor, an embedding eF is concatenated with an attention-based context vector c and fed through a feedforward neural network fC to obtain a context-dependent field embedding:\ne\u0303F = fC (eF, c) .\nAn intermediate vertical state for the field F at composite node u is then computed as\nvu,F = LSTM v (vu, e\u0303F) .\nFigure 5b illustrates the process, starting with a single vertical LSTM state and ending with one updated state per field.\nConstructor field modules Each field F of a constructor has a corresponding module whose role is to determine the number of children associated with that field and to propagate an updated vertical LSTM state to them. In the case of fields with singular cardinality, the decision and update are both vacuous, as exactly one child is always generated. Hence these modules forward the field vertical LSTM state vu,F unchanged to the child w corresponding to F:\nvw = vu,F. (1)\nFields with optional cardinality can have either zero or one children; this choice is made using a feedforward network applied to the vertical LSTM state:\np(zF = 1 | vu,F) = sigmoid (fgenF (vu,F)) . (2)\nIf a child is to be generated, then as in (1), the state is propagated forward without modification.\nIn the case of sequential fields, a horizontal LSTM is employed for both child decisions and state updates. We refer to Figure 5c for an illustration of the recurrent process. After being initialized with a transformation of the vertical state, sF,0 = WFvu,F, the horizontal LSTM iteratively\ndecides whether to generate another child by applying a modified form of (2):\np (zF,i = 1 | sF,i\u22121, vu,F) = sigmoid (fgenF (sF,i\u22121, vu,F)) .\nIf zF,i = 0, generation stops and the process terminates, as represented by the solid black circle in Figure 5c. Otherwise, the process continues as represented by the white circle in Figure 5c. In that case, the horizontal state su,i\u22121 is combined with the vertical state vu,F and an attention-based context vector cF,i using a feedforward network fupdateF to obtain a joint context-dependent encoding of the field F and the position i:\ne\u0303F,i = f update F (vu,F, su,i\u22121, cF,i).\nThe result is used to perform a vertical LSTM update for the corresponding child wi:\nvwi = LSTM v(vu,F, e\u0303F,i).\nFinally, the horizontal LSTM state is updated using the same field-position encoding, and the process continues:\nsu,i = LSTM h(su,i\u22121, e\u0303F,i).\nPrimitive type modules Each primitive type T has a corresponding module whose role is to select among the values y within the domain of that type. Figure 5d presents an example of the simplest form of this selection process, where the value y is obtained from a closed list via a softmax layer applied to an incoming vertical LSTM state:\np (y | T,v) = [ softmax (fT (v)) ] y .\nSome string-valued types are open class, however. To deal with these, we allow generation both from a closed list of previously seen values, as in Figure 5d, and synthesis of new values. Synthesis is delegated to a character-level LSTM language model (Bengio et al., 2003), and part of the role of the primitive module for open class types is to choose whether to synthesize a new value or not. During training, we allow the model to use the character LSTM only for unknown strings but include the log probability of that binary decision in the loss in order to ensure the model learns when to generate from the character LSTM."}, {"heading": "3.3 Decoding Process", "text": "The decoding process proceeds through mutual recursion between the constituting modules, where the syntactic structure of the output tree mirrors the call graph of the generation procedure. At each step, the active decoder module either makes a generation decision, propagates state down the tree, or both.\nTo construct a composite node of a given type, the decoder calls the appropriate composite type module to obtain a constructor and its associated module. That module is then invoked to obtain updated vertical LSTM states for each of the constructor\u2019s fields, and the corresponding constructor field modules are invoked to advance the process to those children.\nThis process continues downward, stopping at each primitive node, where a value is generated but no further recursion is carried out."}, {"heading": "3.4 Attention", "text": "Following standard practice for sequence-tosequence models, we compute a raw bilinear attention score qrawt for each token t in the input using the decoder\u2019s current state x and the token\u2019s encoding et:\nqrawt = e > t Wx.\nThe current state x can be either the vertical LSTM state in isolation or a concatentation of the vertical LSTM state and either a horizontal LSTM state or a character LSTM state (for string generation). Each submodule that computes attention does so using a separate matrix W.\nA separate attention score qcompc is computed for each component of the input, independent of its content:\nqcompc = w > c x.\nThe final token-level attention scores are the sums of the raw token-level scores and the corresponding component-level scores:\nqt = q raw t + q comp c(t) ,\nwhere c(t) denotes the component in which token t occurs. The attention weight vector a is then computed using a softmax:\na = softmax (q) .\nGiven the weights, the attention-based context is given by:\nc = \u2211 t atet.\nCertain decision points that require attention have been highlighted in the description above; however, in our final implementation we made attention available to the decoder at all decision points.\nSupervised Attention In the datasets we consider, partial or total copying of input tokens into primitive nodes is quite common. Rather than providing an explicit copying mechanism (Ling et al., 2016), we instead generate alignments where possible to define a set of tokens on which the attention at a given primitive node should be concentrated.2 If no matches are found, the corresponding set of tokens is taken to be the whole input.\nThe attention supervision enters the loss through a term that encourages the final attention weights to be concentrated on the specified subset. Formally, if the matched subset of componenttoken pairs is S, the loss term associated with the supervision would be\nlog \u2211 t exp (at)\u2212 log \u2211 t\u2208S exp (at), (3)\n2Alignments are generated using an exact string match heuristic that also included some limited normalization, primarily splitting of special characters, undoing camel case, and lemmatization for the semantic parsing datasets.\nwhere at is the attention weight associated with token t, and the sum in the first term ranges over all tokens in the input. The loss in (3) can be interpreted as the negative log probability of attending to some token in S."}, {"heading": "4 Experimental evaluation", "text": ""}, {"heading": "4.1 Semantic parsing", "text": "Data We use three semantic parsing datasets: JOBS, GEO, and ATIS. All three consist of natural language queries paired with a logical representation of their denotations. JOBS consists of 640 such pairs, with Prolog-style logical representations, while GEO and ATIS consist of 880 and 5,410 such pairs, respectively, with \u03bb-calculus logical forms. We use the same training-test split as Zettlemoyer and Collins (2005) for JOBS and GEO, and the standard training-development-test split for ATIS. We use the preprocessed versions of these datasets made available by Dong and Lapata (2016), where text in the input has been lowercased and stemmed using NLTK (Bird et al., 2009), and matching entities appearing in the same input-output pair have been replaced by numbered abstract identifiers of the same type.\nEvaluation We compute accuracies using tree exact match for evaluation. Following the publicly released code of Dong and Lapata (2016), we canonicalize the order of the children within conjunction and disjunction nodes to avoid spurious errors, but otherwise perform no transformations before comparison."}, {"heading": "4.2 Code generation", "text": "Data We use the HEARTHSTONE dataset introduced by Ling et al. (2016), which consists of 665 cards paired with their implementations in the open-source Hearthbreaker engine.3 Our trainingdevelopment-test split is identical to that of Ling et al. (2016), with split sizes of 533, 66, and 66, respectively.\nCards contain two kinds of components: textual components that contain the card\u2019s name and a description of its function, and categorical ones that contain numerical attributes (attack, health, cost, and durability) or enumerated attributes (rarity, type, race, and class). The name of the card is represented as a sequence of characters, while\n3Available online at https://github.com/ danielyule/hearthbreaker.\nits description consists of a sequence of tokens split on whitespace and punctuation. All categorical components are represented as single-token sequences.\nEvaluation For direct comparison to the results of Ling et al. (2016), we evaluate our predicted code based on exact match and token-level BLEU relative to the reference implementations from the library. We additionally compute node-based precision, recall, and F1 scores for our predicted trees compared to the reference code ASTs. Formally, these scores are obtained by defining the intersection of the predicted and gold trees as their largest common tree prefix."}, {"heading": "4.3 Settings", "text": "For each experiment, all feedforward and LSTM hidden dimensions are set to the same value. We select the dimension from {30, 40, 50, 60, 70} for the smaller JOBS and GEO datasets, or from {50, 75, 100, 125, 150} for the larger ATIS and HEARTHSTONE datasets. The dimensionality used for the inputs to the encoder is set to 100 in all cases. We apply dropout to the non-recurrent connections of the vertical and horizontal LSTMs, selecting the noise ratio from {0.2, 0.3, 0.4, 0.5}. All parameters are randomly initialized using Glorot initialization (Glorot and Bengio, 2010).\nWe perform 200 passes over the data for the JOBS and GEO experiments, or 400 passes for the ATIS and HEARTHSTONE experiments. Early stopping based on exact match is used for the semantic parsing experiments, where performance is evaluated on the training set for JOBS and GEO or on the development set for ATIS. Parameters for the HEARTHSTONE experiments are selected based on development BLEU scores. In order to promote generalization, ties are broken in all cases with a preference toward higher dropout ratios and lower dimensionalities, in that order.\nOur system is implemented in Python using the DyNet neural network library (Neubig et al., 2017). We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 20 for the semantic parsing experiments, or a batch size of 10 for the HEARTHSTONE experiments."}, {"heading": "4.4 Results", "text": "Our results on the semantic parsing datasets are presented in Table 1. Our basic system achieves\na new state-of-the-art accuracy of 91.4% on the JOBS dataset, and this number improves to 92.9% when supervised attention is added. On the ATIS and GEO datasets, we respectively exceed and match the results of Dong and Lapata (2016). However, these fall short of the previous best results of 91.3% and 90.4%, respectively, obtained by Wang et al. (2014). This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).\nOn the HEARTHSTONE dataset, we improve significantly over the initial results of Ling et al. (2016) across all evaluation metrics, as shown in Table 2. On the more stringent exact match metric, we improve from 6.1% to 18.2%, and on tokenlevel BLEU, we improve from 67.1 to 77.6. When supervised attention is added, we obtain an additional increase of several points on each scale, achieving peak results of 22.7% accuracy and 79.2 BLEU."}, {"heading": "4.5 Error Analysis and Discussion", "text": "As the examples in Figures 6-8 show, classes in the HEARTHSTONE dataset share a great deal of common structure. As a result, in the simplest cases, such as in Figure 6, generating the code is simply a matter of matching the overall structure and plugging in the correct values in the initializer and a few other places. In such cases, our system generally predicts the correct code, with the\nexception of instances in which strings are incorrectly transduced. Introducing a dedicated copying mechanism like the one used by Ling et al. (2016) or more specialized machinery for string transduction may alleviate this latter problem.\nThe next simplest category of card-code pairs consists of those in which the card\u2019s logic is mostly implemented via nested function calls. Figure 7 illustrates a typical case, in which the card\u2019s effect is triggered by a game event (a spell being cast) and both the trigger and the effect are described by arguments to an Effect constructor. Our system usually also performs well on instances like these, apart from idiosyncratic errors that can take the form of under- or overgeneration or simply substitution of incorrect predicates.\nCards whose code includes complex logic expressed in an imperative style, as in Figure 8, pose the greatest challenge for our system. Factors like variable naming, nontrivial control flow, and interleaving of code predictable from the description with code required due to the conventions of the library combine to make the code for these cards difficult to generate. In some instances (as in the figure), our system is nonetheless able to synthesize a close approximation. However, in the most complex cases, the predictions deviate significantly from the correct implementation.\nIn addition to the specific errors our system makes, some larger issues remain unresolved. Existing evaluation metrics only approximate the actual metric of interest: functional equivalence. Modifications of BLEU, tree F1, and exact\nmatch that canonicalize the code\u2014for example, by anonymizing all variables\u2014may prove more meaningful. Direct evaluation of functional equivalence is of course impossible in general (Sipser, 2006), and practically challenging even for the HEARTHSTONE dataset because it requires integrating with the game engine.\nExisting work also does not attempt to enforce semantic coherence in the output. Long-distance semantic dependencies, between occurrences of a single variable for example, in particular are not modeled. Nor is well-typedness or executability. Overcoming these evaluation and modeling issues remains an important open problem."}, {"heading": "5 Conclusion", "text": "ASNs provide a modular encoder-decoder architecture that can readily accommodate a variety of tasks with structured output spaces. They are particularly applicable in the presence of recursive decompositions, where they can provide a simple decoding process that closely parallels the inherent structure of the outputs. Our results demonstrate their promise for tree prediction tasks, and we believe their application to more general output structures is an interesting avenue for future work."}, {"heading": "Acknowledgments", "text": "MR is supported by an NSF Graduate Research Fellowship and a Fannie and John Hertz Foundation Google Fellowship. MS is supported by an NSF Graduate Research Fellowship."}], "references": [{"title": "Compilers: Principles, Techniques, and Tools (2Nd Edition)", "author": ["Alfred V. Aho", "Monica S. Lam", "Ravi Sethi", "Jeffrey D. Ullman."], "venue": "Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.", "citeRegEx": "Aho et al\\.,? 2006", "shortCiteRegEx": "Aho et al\\.", "year": 2006}, {"title": "Bimodal modelling of source code and natural language", "author": ["Miltiadis Allamanis", "Daniel Tarlow", "Andrew D. Gordon", "Yi Wei."], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015.", "citeRegEx": "Allamanis et al\\.,? 2015", "shortCiteRegEx": "Allamanis et al\\.", "year": 2015}, {"title": "Tree-structured decoding with doubly-recurrent neural networks", "author": ["David Alvarez-Melis", "Tommi S. Jaakkola."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2017.", "citeRegEx": "Alvarez.Melis and Jaakkola.,? 2017", "shortCiteRegEx": "Alvarez.Melis and Jaakkola.", "year": 2017}, {"title": "Neural module networks", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Oral.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Deepcoder: Learning to write programs", "author": ["Matej Balog", "Alexander L. Gaunt", "Marc Brockschmidt", "Sebastian Nowozin", "Daniel Tarlow."], "venue": "CoRR abs/1611.01989.", "citeRegEx": "Balog et al\\.,? 2016", "shortCiteRegEx": "Balog et al\\.", "year": 2016}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res. 3:1137\u20131155. http://dl.acm.org/citation.cfm?id=944919.944966.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Natural Language Processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media, Inc., 1st edition.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29,", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL", "author": ["Doha"], "venue": null, "citeRegEx": "2014 and Doha,? \\Q2014\\E", "shortCiteRegEx": "2014 and Doha", "year": 2014}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Language to logical form with neural attention", "author": ["Li Dong", "Mirella Lapata."], "venue": "CoRR abs/1601.01280.", "citeRegEx": "Dong and Lapata.,? 2016", "shortCiteRegEx": "Dong and Lapata.", "year": 2016}, {"title": "Recurrent neural network", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Scaling semantic parsers with on-the-fly ontology matching", "author": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke S. Zettlemoyer."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013,", "citeRegEx": "Kwiatkowski et al\\.,? 2013", "shortCiteRegEx": "Kwiatkowski et al\\.", "year": 2013}, {"title": "Learning programs: A hierarchical bayesian approach", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein."], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel. pages 639\u2013646.", "citeRegEx": "Liang et al\\.,? 2010", "shortCiteRegEx": "Liang et al\\.", "year": 2010}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein."], "venue": "Comput. Linguist. 39(2):389\u2013446. https://doi.org/10.1162/COLI a 00127.", "citeRegEx": "Liang et al\\.,? 2013", "shortCiteRegEx": "Liang et al\\.", "year": 2013}, {"title": "Latent predictor networks for code generation", "author": ["Wang Ling", "Phil Blunsom", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Fumin Wang", "Andrew Senior."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Compu-", "citeRegEx": "Ling et al\\.,? 2016", "shortCiteRegEx": "Ling et al\\.", "year": 2016}, {"title": "Structured generative models of natural source code", "author": ["Chris J. Maddison", "Daniel Tarlow."], "venue": "Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 2126 June 2014. pages 649\u2013657.", "citeRegEx": "Maddison and Tarlow.,? 2014", "shortCiteRegEx": "Maddison and Tarlow.", "year": 2014}, {"title": "A machine learning framework for programming by example", "author": ["Aditya Krishna Menon", "Omer Tamuz", "Sumit Gulwani", "Butler W. Lampson", "Adam Kalai."], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, At-", "citeRegEx": "Menon et al\\.,? 2013", "shortCiteRegEx": "Menon et al\\.", "year": 2013}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv preprint arXiv:1701.03980 .", "citeRegEx": "Oda et al\\.,? 2017", "shortCiteRegEx": "Oda et al\\.", "year": 2017}, {"title": "Towards a theory of natural language interfaces to databases", "author": ["Ana-Maria Popescu", "Oren Etzioni", "Henry Kautz."], "venue": "Proceedings of the 8th international conference on Intelligent user interfaces. ACM, pages 149\u2013157.", "citeRegEx": "Popescu et al\\.,? 2003", "shortCiteRegEx": "Popescu et al\\.", "year": 2003}, {"title": "Tree-structured variational autoencoder", "author": ["Richard Shin", "Alexander A. Alemi", "Geoffrey Irving", "Oriol Vinyals."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR) 2017.", "citeRegEx": "Shin et al\\.,? 2017", "shortCiteRegEx": "Shin et al\\.", "year": 2017}, {"title": "Introduction to the Theory of Computation", "author": ["Michael Sipser."], "venue": "Course Technology, second edition.", "citeRegEx": "Sipser.,? 2006", "shortCiteRegEx": "Sipser.", "year": 2006}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Sys-", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Morpho-syntactic lexical generalization for ccg semantic parsing", "author": ["Adrienne Wang", "Tom Kwiatkowski", "Luke S Zettlemoyer."], "venue": "EMNLP. pages 1284\u20131295.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "The zephyr abstract syntax description language", "author": ["Daniel C. Wang", "Andrew W. Appel", "Jeff L. Korn", "Christopher S. Serra."], "venue": "Proceedings of the Conference on Domain-Specific Languages on Conference on Domain-Specific Languages (DSL),", "citeRegEx": "Wang et al\\.,? 1997", "shortCiteRegEx": "Wang et al\\.", "year": 1997}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "UAI \u201905, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence, Ed-", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}, {"title": "Online learning of relaxed ccg grammars for parsing to logical form", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Zettlemoyer and Collins.,? 2007", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2007}, {"title": "Type-driven incremental semantic parsing with polymorphism", "author": ["Kai Zhao", "Liang Huang."], "venue": "NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Zhao and Huang.,? 2015", "shortCiteRegEx": "Zhao and Huang.", "year": 2015}], "referenceMentions": [{"referenceID": 11, "context": "Sequence-to-sequence models have proven effective for both tasks (Dong and Lapata, 2016; Ling et al., 2016), using encoder-decoder frameworks to exploit the sequential structure on both the input and output side.", "startOffset": 65, "endOffset": 107}, {"referenceID": 18, "context": "Sequence-to-sequence models have proven effective for both tasks (Dong and Lapata, 2016; Ling et al., 2016), using encoder-decoder frameworks to exploit the sequential structure on both the input and output side.", "startOffset": 65, "endOffset": 107}, {"referenceID": 0, "context": "The wellformedness case is of particular interest, since it can readily be enforced by representing outputs as abstract syntax trees (ASTs) (Aho et al., 2006), an approach that can be seen as a much lighter weight", "startOffset": 140, "endOffset": 158}, {"referenceID": 11, "context": "The ci0 and ci1 tokens are entity abstractions introduced in preprocessing (Dong and Lapata, 2016).", "startOffset": 75, "endOffset": 98}, {"referenceID": 28, "context": "version of CCG-based semantic parsing (Zettlemoyer and Collins, 2005).", "startOffset": 38, "endOffset": 69}, {"referenceID": 8, "context": "As is common with neural approaches to structured prediction (Chen and Manning, 2014; Vinyals et al., 2015), our decoder proceeds greedily and accesses not only a fixed encoding but also an attention-based representation of the input (Bahdanau et al.", "startOffset": 61, "endOffset": 107}, {"referenceID": 25, "context": "As is common with neural approaches to structured prediction (Chen and Manning, 2014; Vinyals et al., 2015), our decoder proceeds greedily and accesses not only a fixed encoding but also an attention-based representation of the input (Bahdanau et al.", "startOffset": 61, "endOffset": 107}, {"referenceID": 4, "context": ", 2015), our decoder proceeds greedily and accesses not only a fixed encoding but also an attention-based representation of the input (Bahdanau et al., 2014).", "startOffset": 134, "endOffset": 157}, {"referenceID": 18, "context": "1% exact match (Ling et al., 2016).", "startOffset": 15, "endOffset": 34}, {"referenceID": 26, "context": "Likewise, we perform competitively on the ATIS and GEO datasets, matching or exceeding the exact match reported by Dong and Lapata (2016), though not quite reaching the records held by the best previous semantic parsing approaches (Wang et al., 2014).", "startOffset": 231, "endOffset": 250}, {"referenceID": 11, "context": "Likewise, we perform competitively on the ATIS and GEO datasets, matching or exceeding the exact match reported by Dong and Lapata (2016), though not quite reaching the records held by the best previous semantic parsing approaches (Wang et al.", "startOffset": 115, "endOffset": 138}, {"referenceID": 10, "context": "Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015).", "startOffset": 204, "endOffset": 268}, {"referenceID": 12, "context": "Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015).", "startOffset": 204, "endOffset": 268}, {"referenceID": 25, "context": "Encoder-decoder architectures, with and without attention, have been applied successfully both to sequence prediction tasks like machine translation and to tree prediction tasks like constituency parsing (Cross and Huang, 2016; Dyer et al., 2016; Vinyals et al., 2015).", "startOffset": 204, "endOffset": 268}, {"referenceID": 25, "context": "In the latter case, work has focused on making the task look like sequence-tosequence prediction, either by flattening the output tree (Vinyals et al., 2015) or by representing it as a sequence of construction decisions (Cross and Huang, 2016; Dyer et al.", "startOffset": 135, "endOffset": 157}, {"referenceID": 10, "context": ", 2015) or by representing it as a sequence of construction decisions (Cross and Huang, 2016; Dyer et al., 2016).", "startOffset": 70, "endOffset": 112}, {"referenceID": 12, "context": ", 2015) or by representing it as a sequence of construction decisions (Cross and Huang, 2016; Dyer et al., 2016).", "startOffset": 70, "endOffset": 112}, {"referenceID": 5, "context": "Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs (Balog et al., 2016; Liang et al., 2010; Menon et al., 2013).", "startOffset": 107, "endOffset": 167}, {"referenceID": 16, "context": "Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs (Balog et al., 2016; Liang et al., 2010; Menon et al., 2013).", "startOffset": 107, "endOffset": 167}, {"referenceID": 20, "context": "Meanwhile, a separate line of work has focused on the problem of program induction from input-output pairs (Balog et al., 2016; Liang et al., 2010; Menon et al., 2013).", "startOffset": 107, "endOffset": 167}, {"referenceID": 1, "context": "Allamanis et al. (2015) and Maddison and Tarlow (2014) proposed modeling code with a neural language model, generating concrete syntax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "Allamanis et al. (2015) and Maddison and Tarlow (2014) proposed modeling code with a neural language model, generating concrete syntax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval.", "startOffset": 0, "endOffset": 55}, {"referenceID": 1, "context": "Allamanis et al. (2015) and Maddison and Tarlow (2014) proposed modeling code with a neural language model, generating concrete syntax trees in left-first depth-first order, focusing on metrics like perplexity and applications like code snippet retrieval. More recently, Shin et al. (2017) attacked the same problem using a grammar-based variational autoencoder with top-down generation similar to ours instead.", "startOffset": 0, "endOffset": 290}, {"referenceID": 2, "context": "The prediction framework most similar in spirit to ours is the doubly-recurrent decoder network introduced by Alvarez-Melis and Jaakkola (2017), which propagates information down the tree using a vertical LSTM and between siblings using a horizontal LSTM.", "startOffset": 110, "endOffset": 144}, {"referenceID": 18, "context": "Apart from ours, the best results on the codegeneration task associated with the HEARTHSTONE dataset are based on a sequence-tosequence approach to the problem (Ling et al., 2016).", "startOffset": 160, "endOffset": 179}, {"referenceID": 3, "context": "Previously, Andreas et al. (2016) introduced neural module networks (NMNs) for visual question answering, with modules corresponding to linguistic substructures within the input query.", "startOffset": 12, "endOffset": 34}, {"referenceID": 27, "context": "Our model makes use of the Abstract Syntax Description Language (ASDL) framework (Wang et al., 1997), which represents code fragments as trees with typed nodes.", "startOffset": 81, "endOffset": 100}, {"referenceID": 6, "context": "Synthesis is delegated to a character-level LSTM language model (Bengio et al., 2003), and part of the role of the primitive module for open class types is to choose whether to synthesize a new value or not.", "startOffset": 64, "endOffset": 85}, {"referenceID": 18, "context": "Rather than providing an explicit copying mechanism (Ling et al., 2016), we instead generate alignments where possible to define a set of tokens on which the attention at a given primitive node should be concentrated.", "startOffset": 52, "endOffset": 71}, {"referenceID": 7, "context": "We use the preprocessed versions of these datasets made available by Dong and Lapata (2016), where text in the input has been lowercased and stemmed using NLTK (Bird et al., 2009), and matching entities appearing in the same input-output pair have been replaced by numbered abstract identifiers of the same type.", "startOffset": 160, "endOffset": 179}, {"referenceID": 26, "context": "We use the same training-test split as Zettlemoyer and Collins (2005) for JOBS and GEO, and the standard training-development-test split for ATIS.", "startOffset": 39, "endOffset": 70}, {"referenceID": 10, "context": "We use the preprocessed versions of these datasets made available by Dong and Lapata (2016), where text in the input has been lowercased and stemmed using NLTK (Bird et al.", "startOffset": 69, "endOffset": 92}, {"referenceID": 11, "context": "Following the publicly released code of Dong and Lapata (2016), we canonicalize the order of the children within conjunction and disjunction nodes to avoid spurious errors, but otherwise perform no transformations before comparison.", "startOffset": 40, "endOffset": 63}, {"referenceID": 18, "context": "Data We use the HEARTHSTONE dataset introduced by Ling et al. (2016), which consists of 665 cards paired with their implementations in the open-source Hearthbreaker engine.", "startOffset": 50, "endOffset": 69}, {"referenceID": 18, "context": "Data We use the HEARTHSTONE dataset introduced by Ling et al. (2016), which consists of 665 cards paired with their implementations in the open-source Hearthbreaker engine.3 Our trainingdevelopment-test split is identical to that of Ling et al. (2016), with split sizes of 533, 66, and 66, respectively.", "startOffset": 50, "endOffset": 252}, {"referenceID": 18, "context": "Evaluation For direct comparison to the results of Ling et al. (2016), we evaluate our predicted code based on exact match and token-level BLEU relative to the reference implementations from the library.", "startOffset": 51, "endOffset": 70}, {"referenceID": 13, "context": "All parameters are randomly initialized using Glorot initialization (Glorot and Bengio, 2010).", "startOffset": 68, "endOffset": 93}, {"referenceID": 14, "context": "We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 20 for the semantic parsing experiments, or a batch size of 10 for the HEARTHSTONE experiments.", "startOffset": 26, "endOffset": 47}, {"referenceID": 18, "context": "LPN refers to the system of Ling et al. (2016). Our nearest neighbor baseline NEAREST follows that of Ling et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 18, "context": "LPN refers to the system of Ling et al. (2016). Our nearest neighbor baseline NEAREST follows that of Ling et al. (2016), though it performs somewhat better; its nonzero exact match number stems from spurious repetition in the data.", "startOffset": 28, "endOffset": 121}, {"referenceID": 29, "context": "This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).", "startOffset": 141, "endOffset": 239}, {"referenceID": 15, "context": "This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).", "startOffset": 141, "endOffset": 239}, {"referenceID": 26, "context": "This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).", "startOffset": 141, "endOffset": 239}, {"referenceID": 30, "context": "This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2013; Wang et al., 2014; Zhao and Huang, 2015).", "startOffset": 141, "endOffset": 239}, {"referenceID": 11, "context": "On the ATIS and GEO datasets, we respectively exceed and match the results of Dong and Lapata (2016). However, these fall short of the previous best results of 91.", "startOffset": 78, "endOffset": 101}, {"referenceID": 11, "context": "On the ATIS and GEO datasets, we respectively exceed and match the results of Dong and Lapata (2016). However, these fall short of the previous best results of 91.3% and 90.4%, respectively, obtained by Wang et al. (2014). This difference may be partially attributable to the use of typing information or rich lexicons in most previous semantic parsing approaches (Zettlemoyer and Collins, 2007; Kwiatkowski et al.", "startOffset": 78, "endOffset": 222}, {"referenceID": 18, "context": "On the HEARTHSTONE dataset, we improve significantly over the initial results of Ling et al. (2016) across all evaluation metrics, as shown in Table 2.", "startOffset": 81, "endOffset": 100}, {"referenceID": 18, "context": "Introducing a dedicated copying mechanism like the one used by Ling et al. (2016) or more specialized machinery for string transduction may alleviate this latter problem.", "startOffset": 63, "endOffset": 82}, {"referenceID": 24, "context": "Direct evaluation of functional equivalence is of course impossible in general (Sipser, 2006), and practically challenging even for the HEARTHSTONE dataset because it requires integrating with the game engine.", "startOffset": 79, "endOffset": 93}], "year": 2017, "abstractText": "Syntax Networks for Code Generation and Semantic Parsing Maxim Rabinovich\u2217 Mitchell Stern\u2217 Dan Klein Computer Science Division University of California, Berkeley {rabinovich,mitchell,klein}@cs.berkeley.edu", "creator": "LaTeX with hyperref package"}}}