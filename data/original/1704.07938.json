{"id": "1704.07938", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "An ensemble-based online learning algorithm for streaming data", "abstract": "In this study, we introduce an ensemble-based approach for online machine learning. The ensemble of base classifiers in our approach is obtained by learning Naive Bayes classifiers on different training sets which are generated by projecting the original training set to lower dimensional space. We propose a mechanism to learn sequences of data using data chunks paradigm. The experiments conducted on a number of UCI datasets and one synthetic dataset demonstrate that the proposed approach performs significantly better than some well-known online learning algorithms.", "histories": [["v1", "Wed, 26 Apr 2017 00:33:36 GMT  (193kb)", "http://arxiv.org/abs/1704.07938v1", "19 pages, 3 figures"]], "COMMENTS": "19 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tien thanh nguyen", "thi thu thuy nguyen", "xuan cuong pham", "alan wee-chung liew", "james c bezdek"], "accepted": false, "id": "1704.07938"}, "pdf": {"name": "1704.07938.pdf", "metadata": {"source": "CRF", "title": "An ensemble-based online learning algorithm for streaming data", "authors": ["Tien Thanh Nguyen", "Thi Thu Thuy Nguyen", "Xuan Cuong Pham", "Alan Wee-Chung Liew", "James C. Bezdek"], "emails": [], "sections": [{"heading": null, "text": "In this study, we introduce an ensemble-based approach for online machine learning. The ensemble of base classifiers in our approach is obtained by learning Na\u00efve Bayes classifiers on different training sets which are generated by projecting the original training set to lower dimensional space. We propose a mechanism to learn sequences of data using data chunks paradigm. The experiments conducted on a number of UCI datasets and one synthetic dataset demonstrate that the proposed approach performs significantly better than some well-known online learning algorithms.\nKeywords: Online learning, ensemble method, multi classifiers system, random projection, Na\u00efve Bayes classifier"}, {"heading": "1. Introduction", "text": "With rapid advances in storage and sensor technologies, large volumes of data in the form of data streams are being collected in many applications such as network traffic and stock market analysis. Streaming data has created problems for traditional offline machine learning systems. First, learning the entire volume of data at once to form the discriminative model is often not possible. Moreover, offline algorithms require re-training when new\ndata are available, so they are not applicable for the situation where data is arriving continuously and the prediction model must be obtained before all data are available. Therefore, the online learning framework that deals with data streams has become increasing popular [Nguyen et al., 2016a].\nIn this paper, we focus on supervised online learning, where the training data arrives sequentially. In online learning, learning only occurs, i.e. the prediction model is updated, when a label for the observation is made available. Otherwise, the algorithm is performing classification using the current prediction model. The general paradigm of online learning is as follows. A new observation is aquired and classified by the current prediction model. The model is updated when the label of the new observation is revealed and the update condition is satisfied. The model can be updated after the arrival of each data point (1-by-1), or deferred until a group of data points has arrived (mini-batch by mini-batch).\nAmong the online learning algorithms introduced in the literature, one of the most popular approach is additive in nature in which given a misclassified observation , , the classification model is updated by shifting along the direction of + \u2192 where is the weight vector, \u2208 \u22121,1 is the class label of and is the weight of misclassified observation. Well-known additive models include the Perceptron [Rosenblatt, 1958], Online Gradient Descent (OGD) [Zinkevich, 2003], Passive Aggressive learning (PA) [Crammer et al., 2006], Soft Confident Weighted (SCW) [Wang et al., 2012], and Adaptive Regularization of Weights (AROW) [Crammer et al., 2009; Crammer et al., 2013]. Ensemble methods have also been proposed for online learning. Online Bagging and Online Boosting [Oza and Russell, 2005] are two well-known online ensemble algorithms. Other algorithms such as the Bayesian-based method [Nguyen et al., 2016a] and the Ellipsoid method [Yang et al., 2009] have also been introduced recently.\nExisting approaches have their shortcomings. First, the number of model updates is usually high. For example, in Online Bagging, base classifiers are always updated after the arrival of each new data point. Some algorithms only support 1-by-1 learning but not mini-batch learning. Finally, approaches like the Bayesian-based method [Nguyen et al., 2016a] estimate the distribution of each class and have problems dealing with very high dimensional datasets. Therefore, an algorithm that supports both 1-by-1 and mini-batch learning, only perfoms a small number of updates, and works well for high dimensional datasets, is highly desirable.\nIn this paper, we propose a novel ensemble-based online learning algorithm for very high dimensional data. To deal with the high dimensional data, our algorithm uses the theory of random projections [Johnson and Lindenstrauss, 1984] to project new observations to low dimension subspaces, thereby obtaining different data schemes for the ensemble of homogenuous base classifiers. Our base classifiers are generated by Na\u00efve Bayes learning, and the final class prediction is obtained by a fix combining rule. In the training process, our algorithm only performs updates when the arrived observations are misclassified, and the update is done in 1-by-1 or minibatch mode.\nThe combination between random projections and Na\u00efve Bayes classifier for online learning proposes a novel homogeneous ensemble method to solve the online learning problem. First, when the dimension of data is high, Na\u00efve Bayes classifiers take a long time to train since the likelihood distribution is computed for each feature. By using random projection we first project the input data to low dimensional space and then learned the Na\u00efve Bayes classifiers on the projected data, resulting in the fast learning of Na\u00efve Bayes classifiers. In an online setting we are considering, the observations come one at a time. So at instance , we only have one observation for our ensemble of classifiers. Random projection provides a principled way for us to create a set of \u2018observations\u2019 from one single incoming observation with good diversity for our ensemble of base classifiers. Since random projection is unstable, from one observation, we could create many diverse training data to train the ensemble of homogenous set of classifiers. The ensemble of Na\u00efve Bayes classifiers is expected to obtain better result than a single classifier due to the characteristic of ensemble system [Dietterich, 2000].\nThe paper is organized as follows. In section 2, we briefly discuss random projection and then develop the\nensemble system for online learning based on random projections and Na\u00efve Bayes classifier. Experimental studies are presented in section 3 in which we conduct experiments on thirty two datasets and compare the results of the proposed framework to a number of benchmark algorithms. Our conclusions appears in the last section."}, {"heading": "2. Proposed method", "text": ""}, {"heading": "2.1. Random projection", "text": "In 1984, Johnson and Lindenstrauss (JL) published a paper about extending Lipschitz continuous maps from metric spaces to Euclidean spaces and introduced the JL Lemma [Johnson and Lindenstrauss, 1984]. The lemma\nspecifies a linear transformation from a -dimensional space \u211d (called up-space) to a -dimensional space \u211d (called down-space). Specifically, given a finite set of -dimensional data vector = , , \u2026 , \u2282 \u211d , there exists a linear transformation T: \u211d \u2192 \u211d : = T! \" = # , # , \u2026 , # \u2282 \u211d , #$ = T $ that in probability preserves distance between observations under certain conditions. The linear transformation T can be represented by a matrix % so that #$ = T $ = % $. When each element of the matrix is generated according to a specified random distribution, T is known as random projection.\nRandom projection has two desirable properties:\n\u2022 Random projections are useful in dimensionality reduction since the dimension of down-space is usually much lower than that of up-space i.e. < . In fact, in some situations, random projection is preferred to Principle Component Analysis (PCA). First, the directions of random projection are independent of the data while those of PCA are data-dependent. This is useful in situations where data cannot be accessed all at once, such as in data streaming. Moreover, generating the principle components is computationally expensive compare to generating the random matrix in random projection [Bingham and Mannila, 2001].\n\u2022 Fern and Brodley [2003] indicated that random projections are unstable in the sense that the datasets generated from an original data source based on random matrices can be quite different. This property is significant since other sampling methods like bootstrapping only generate slightly different dataset schemes. An ensemble system constructed based on a set of random projections can therefore have a lot of diversity.\nIn this paper, ' random matrices \u211b = )% * + , = 1, \u2026 , ' are generated to construct the ensemble system. We follow the construction of random matrix in [Avogadri and Valentini, 2009] in which the projections are simply obtained by using a \u00d7 random matrix % * = 1 . \u2044 01 2 * 3 where 1 2 are random variables such that E 51 2 * 6 = 0 and var 51 2 * 6 = 1. Several forms of % * are summarized as:\n\u2022 Plus-minus-one or Bernoulli random projection: 1 2 * is randomly chosen in \u22121, 1 such that P 51 2 * = 16 = P 51 2 * = \u221216 = 1 2\u2044\n\u2022 Achlioptas random projection: 1 2 * is randomly chosen in )\u2212\u221a3, 0, \u221a3+ such that P 51 2 * = \u221a36 = P 51 2 * = \u2212\u221a36 = 1 6\u2044 and P 51 2 * = 06 = 2 3\u2044\n\u2022 Gaussian random projection: 1 2 * is distributed according to a Gaussian distribution @ 0, 1"}, {"heading": "2.2. Class label prediction", "text": "Having \u211b on hand, the class label of each observation is predicted by using an ensemble of Na\u00efve Bayes classifiers. The Na\u00efve Bayes classifier is a well-known learning algorithm based on Bayes theorem having assumptions about conditional independence between features of observation. Despite the oversimplified assumptions, Naive Bayes classifiers are fast and efficient to train, which is important for streaming data. In detail, given a -dimension vector = AB , B \u2026 , B C , the posterior probability that belongs to class label D is given by: P D| = PA D|B , B , \u2026 , B C~PA D , B , B , \u2026 , B C~P D \u220f P B | D H (1) The likelihood P B | D is computed based on the assumption about the distribution of each feature B given\nD, such as B | D~@AID , JD C in which the parameters ID and JD are computed from the training observations. When applying this to online classification, at the KL step, the observations in mini-batch MK will be projected to the down-space by using each random projection in \u211b. We denoted K* as the projection of MK associated with the ,KL projection matrix % * K * = \u221a MK% * , = 1, \u2026 , ' (2) Assumption 1 (Na\u00efve Bayes): The features in #K * are assumed to satisfy the conditional independence assumption. It has been shown that some violation of the independence assumption of the attributes does not matter [Domingos, Pazzani, 1996]\nFor each observation K in MK, denoted its projected vector associated with % * as #K * = )NK2* + O = 1, \u2026 , . Based on Assumption 1, the posterior probability that K belong to class label D is given by: P D| K = P 5 D|#K * 6 ~P D \u220f PANK2* | DC 2H (3)\nIn this paper, we assume that the distribution of likelihood is Gaussian i.e. NK2* | D~@ 5ID2 * , JD2 * 6. Since the likelihood distribution of each projected attribute is unknown, we used the Gaussian distribution to approximate it. Based on the Central Limit Theorem, Gaussian can be used to approximate a wide range of other distributions such as Poisson, Binominal, and Gamma when we have large enough data [Balakrishnan, Nevzorov, 2003]. Therefore, we have: PANK2* | DC = \u221a PQRS T exp X\u2212 YZ[S T \\]RS T QRS T ^\n_ (4) Taking logs, we obtain: log 0P 5 D|#K * 63 ~ log P D \u2212 \u2211 Xlog 0\u221a2\u03c0JD2 * 3 + YZ[ST \\]RS T QRS T ^ _ 2H (5)\nThe hypotheses from ' base classifiers are combined to obtain the final hypothesis. Several popular fixed combining methods, namely Sum, Product, Majority Vote, Max, Min, and Median can be used as the combiner [Nguyen et al., 2014; Nguyen et al., 2016b; Kittler et al., 1998]. Vote and Sum are the most popular rules and have been successfully applied to many combining classifier situations. In this work, we use the Sum rule to combine the outputs of ' classifiers: K \u2208 e if f = arg maxDH ,\u2026,h \u2211 log)PA D|#*C+i*H (6)"}, {"heading": "2.3. Parameters update", "text": "After obtaining the predicted class labels of observations in MK, we update the parameters of Na\u00efve Bayes classifiers. Theorem 1 provides the equations to update the mean and variance of the likelihood distribution function. Theorem 1: Assume that at the \u2212 1 KL step we have received \u2212 1 mini-batches M |j = 1, \u2026 , \u2212 1 in which mini-batch M has k 1-dimensional observations i.e. M = )N , , N , , \u2026 , N , l +. Denote I and J as the mean and variance of model at the KL step, the update equations are given by: I = \u2211 l[lmn )A\u2211 k K\\ H CI \u2212 1 + \u2211 NK, [ H +"}, {"heading": "J = \u2211 l[lmn oA\u2211 k K\\ H C 5J \u2212 1 + AI \u2212 1 \u2212 I C 6 + \u2211 5NK, \u2212 I 6 [ H p", "text": "Corollary 1: When a single observation NK arrives in the sequence, the update equations for I and J at KL step are given by: I = 1 \u2212 1 I \u2212 1 + NK"}, {"heading": "J = K 0 \u2212 1 5J \u2212 1 + AI \u2212 1 \u2212 I C 6 + ANK \u2212 I C 3", "text": "Corollary 2: If the model is updated with every arrived observation, when \u2192 q, I \u2192 I\u0305 and J \u2192 J in which I\u0305 and J are sample mean and variance computed on the whole dataset. The proof of Theorem1 is given in the Appendix while the proofs of Corollary 1 and 2 are straightforward.\nIn this study, we use the misclassified observations in MK to update the classification model. They are first segmented into s mini-batches MKD t = 1, \u2026 , s in which MKD contains all the misclassified observations that belong to class label D i.e. \u2208 MKD if = D. The mini-batch MKD will be used to update the mean and variance of the likelihood distribution associated with the OKL feature in the down-spaces among all ' projections. ID2 * = \u2211 lR[lmn 0A\u2211 k DK\\ H CID2 * \u2212 1 + \u2211 NK, 2 * [R H 3 (7) JD2 * = \u2211 lR[lmn uA\u2211 k DK\\ H C YJD2 * \u2212 1 + vID2 * \u2212 1 \u2212 ID2 * w ^ + \u2211 vNK, 2 * \u2212 ID2 * w [R H x(8) where kKD = |MKD|, t = 1, \u2026 , s, , = 1, \u2026 , ' Remark: The online update equations (7) and (8) need only keep track of the cumulative item count in MKD up to t-1, but not the actual item values. So a mini-batch can be discarded once it is used for update.\nIn case of 1-by-1 training, the model is updated when the predicted label is different from the ground truth\nclass label as follows: ID2 * = K 0 \u2212 1 ID2 * \u2212 1 + NK,2 * 3 (9)\nJD2 * = K u \u2212 1 YJD2 * \u2212 1 + vID2 * \u2212 1 \u2212 ID2 * w ^ + vNK,2 * \u2212 ID2 * w x (10) We have the following algorithm for mini-batch online training in general.\nAlgorithm: Ensemble online training based on random projection and Na\u00efve Bayes"}, {"heading": "1. Parameter initialization", "text": "Input: ID2 * 0 and JD2 * 0 ,ensemble size K, down-space dimension q For , = 1 \u2026 ' For t = 1 \u2026 s For O = 1 \u2026 Set ID2 * = ID2 * 0 ,JD2 * = JD2 * 0 End\nEnd\nEnd"}, {"heading": "2. Random matrix generation", "text": "For , = 1 \u2026 ' Generate %* = )1 2*+, 1 2*~@ 0,1 End 3. Class label prediction Input: MK = )A K, K C+ For , = 1, \u2026 , ' K * = \u221a MK% * Compute log 0P 5 D|#K * 63 using (5)\nEnd Predict label y K using Sum rule(6)"}, {"heading": "4. Parameter update", "text": "Partition misclassified observations from MK into MKD such that \u2208 MKD if = D For , = 1 \u2026 ' For t = 1 \u2026 s For O = 1 \u2026 Update ID2 * using (7) Update JD2 * using (8) End\nEnd\nEnd"}, {"heading": "3. Empirical Studies", "text": ""}, {"heading": "3.1. Setup", "text": "To evaluate the performance of the proposed method, we carry out experiments on thirty two UCI labeled datasets [UCI] and one labeled synthetic dataset named GM. The GM dataset consists of 1000 observations generated from a Gaussian Mixture with 3 components in equal proportions. The means of the components are\n1 2\u2044 , \u2026 , 1 2\u2044 zzz, 0, \u2026 ,0 zzz, and \u22121 2\u2044 , \u2026 , \u22121 2\u2044 zzz respectively while the corresponding standard deviations are diag 1, \u2026 ,1 zzz, diag 2, \u2026 ,2 zzz, and diag 3, \u2026 ,3 zzz The information about the datasets is shown in Table 1.\nWe perform extensive comparative studies with a number of state-of-the-art algorithms as benchmarks: PA [Crammer et al., 2006], SCW [Wang et al., 2012], OGD [Zinkevich, 2003], AROW [Crammer et al., 2009; Crammer et al., 2013] (we use the implementation in LIBOL library [Hoi et al., 2014] for these algorithms,\ndefault value for parameters are used if available), and Online Bagging [Oza and Russell, 2005] (we use the implementation in MOA library [Bifet et al., 2010]). AROW, OGD, and SCW are algorithms published in top machine learning venues like NIPS and ICML. Online Bagging is a high performance ensemble online learning method. For the proposed method, Gaussian random projection is used to generate the random matrix. The number of learners in Online Bagging and K in the proposed method are set to 200 as in [Nguyen et al., 2016b], and the dimension of all down-spaces is set to = 2 log . The parameters for Na\u00efve Bayes classifiers are simply initialized as ID2 * 0 = 0 and JD2 * 0 = 1 for , = 1 \u2026 ', t = 1 \u2026 s.\nIn this study, the proposed method uses 1-by-1 learning, denoted by RPNB(1b1), since all benchmark algorithms use 1-by-1 learning. The proposed method is compared to the benchmark algorithms with respect to the error rate and F1 score (which is the harmonic mean of Precision and Recall) [Sokolova and Lapalme, 2009]. We draw 10 random permutations from each data to obtain the sequences of arriving data, run the test, and compute the average of the 10 classification error rates, F1 Scores, and number of updates. Here we followed the performance measurements from LIBOL library [Hoi et al., 2014] where the authors used criteria such as mistake rate (classification error rate), and the number of updates (to measure the model stability) to evaluate the performance. In this paper, we conducted Wilcoxon signed rank test [Demsar, 2006] (level of significance is 0.05) to compare a pairs of algorithms, i.e. a benchmark algorithm and the proposed algorithm. Here we tested the specific null hypothesis that \u201ctwo methods perform equally\u201d. Based on the value of statistic in Wilcoxon procedure, we could obtain the P-Value of the test. The performance scores of two methods are treated as significantly different if the P-Value of the test is smaller than a given significant level \u03b1. When the test indicates that the performance of two algorithms is different, we then use the classification error rate to decide which algorithm wins on a particular dataset and count the number of wins and losses on the set of datasets."}, {"heading": "3.2. Results and Discussions", "text": "The mean and variance of error rates and F1 Score of the benchmark algorithms and the proposed method are shown in Tables 2 and 3. The statistical test results in Figure 1 show that RPNB(1b1) significantly outperformed all benchmark algorithms with respect to error rate . Comparing to AROW, we rejected 27 null hypotheses, in which RPNB(1b1) is better than AROW on 24 datasets and worse on 3. Comparing with Online Bagging, we\nrejected 27 null hypothesis, in which RPNB(1b1) is better on 22 datasets and worse on 5. RPNB(1b1) is also significantly better than PA (32 wins, 0 loss), OGD (32 wins and 1 loss), and SCW (25 wins and 3 losses). For F1 score, the statistical test results in Figure 2 show that RPNB(1b1) significantly outperform all benchmark algorithms.\nFile name # of features # of observations # of classes Breast Cancer 9 683 2 Breast Tissue 9 106 6 Chess-krvk 6 28056 18 Conn Bench Vowel 10 528 11 Contraceptive 9 1473 3 Ecoli 7 336 8 GM 1000 1000 3 Hayes Roth 4 160 3 Ionosphere 34 351 2 Iris 4 150 3 Isolet 617 7797 26 Led7digit 7 500 10 Letter 16 20000 26 Madelon 500 2000 2 Marketing 13 6876 9 Monk-2 6 432 2 Multiple Features 649 2000 10 Musk1 166 476 2 Musk2 166 6598 2 Nursery 8 12960 5 Optdigits 64 5620 10 Optical 64 3823 10 Penbased 16 10992 10 Satimage 36 6435 6 Skin_NonSkin 3 245057 2 Soybean 35 307 19 Tae 5 151 3 Tic_Tac_Toe 9 958 2 Twonorm 20 7400 2 Vertebral 6 310 3 Waveform_w_Noise 40 5000 3 Waveform_wo_Noise 21 5000 3 Zoo 16 101 7\nTABLE.1. INFORMATION OF DATASETS IN EVALUATION\nFigure 3 shows the number of times the model is updated on two datasets. Clearly, the proposed method requires significantly less number of updates than the benchmark algorithms. For example, on GM datasets, the average number of updates of RPNB(1b1) is 29.6, nearly 9 times less than that of OGD algorithm (247.4) which has the second smallest number of updates among all benchmark algorithms.\nThe comparative study has shown that our algorithm comfortably outperforms the state-of-the-art benchmark algorithms on the datasets used here. We believe that the success of RPNB(1b1) is partially due to the diverse data schemes generated by random projection used in our ensemble. In addition, we used Na\u00efve Bayes, a simple but efficacious learning algorithm [Webb et al., 2005] to generate the base classifiers where the parameter updates are simple and fast to compute. For fast data streams, our algorithm can be run in minibatch mode which would further reduce the number of parameter updates.\nOur use of random projection in this work serves two purposes: dimensionality reduction if the feature dimension is high, and more importantly, generation of diverse data schemes for the ensemble. A significant departure from the original JL Lemma is that the down-space dimension q is a function of number of observations in JL, whereas in this work the down-space dimension is computed as a function of feature dimension p. In other words, our dimensionality reduction step is inspired by the JL lemma, but there is no probabilistic guarantee about\ndistance preservation in our approach. Our down-space dimension is the same as p when p < 5. In this case, it is only the diverse data schemes we are interested in when applying random projection."}, {"heading": "4. Conclusions", "text": "In this paper, we have introduced an ensemble-based online learning algorithm using random projection and Na\u00efve Bayes classifiers. In our approach, the parameters of the Na\u00efve Bayes classifiers are simply initialized at the beginning and then updated if arrived observations are misclassified. We proposed the update equations for Na\u00efve Bayes classifiers\u2019 parameters via mini-batch by mini-batch learning and 1-by-1 learning. Extensive experimental results for the 1 by 1 case demonstrated the benefit of our approach compared with several well-known benchmark algorithms with respect to classification error rate, F1 score, and the number of updates."}, {"heading": "A. Appendix: Proof of Theorem 1", "text": "I = 1\u2211 k K H }~ ~ N2, l H K 2H = 1\u2211 k K H }~ ~ N2, l H K\\ 2H + ~ NK, [ H = 1\u2211 k K H } ~ k K\\ H I \u2212 1 + ~ NK, [ H\nJ = 1\u2211 k K H }~ ~ 5N2, \u2212 I 6 l H K 2H = 1\u2211 k K H }~ ~ 5N2, \u2212 I 6 l H K\\ 2H + ~ 5NK, \u2212 I 6 [ H\n= 1\u2211 k K H }~ ~ 5N2, \u2212 I \u2212 1 + I \u2212 1 \u2212 I 6 l H K\\ 2H + ~ 5NK, \u2212 I 6 [ H\n= 1\u2211 k K H }~ ~ 5N2, \u2212 I \u2212 1 6 l H K\\ 2H + ~ ~AI \u2212 1 \u2212 I C l H K\\ 2H\n+ ~ ~ 2 5N2, \u2212 I \u2212 1 6 AI \u2212 1 \u2212 I C l H K\\ 2H + ~ 5NK, \u2212 I 6 [ H\n= 1\u2211 k K H } ~ k K\\ H 5JK\\ + AI \u2212 1 \u2212 I C 6 + 2 IK\\ \u2212 IK ~ ~ 5N2, \u2212 I \u2212 1 6 l H K\\ 2H\n+ ~ 5NK, \u2212 I 6 [\nH Since\n2AI \u2212 1 \u2212 I C ~ ~ 5N2, \u2212 I \u2212 1 6 l H K\\ 2H = 2AI \u2212 1 \u2212 I C }~ ~ N2, l H K\\ 2H \u2212 ~ k K\\ H I \u2212 1\n= 2AI \u2212 1 \u2212 I C X ~ k K\\ H I \u2212 1 \u2212 ~ k K\\ H I \u2212 1 _ = 0\nWe have\nJ = 1\u2211 k K H } ~ k K\\ H 5J \u2212 1 + AI \u2212 1 \u2212 I C 6 + ~ 5NK, \u2212 I 6 [ H\nRPNB(1b1) AROW OGD PA SCW Online Bagging Mean Var Mean Var Mean Var Mean Var Mean Var Mean Var\nBreast Cancer 0.0490 5.25E-06 0.1476 \u2191 4.02E-05 0.2004 \u2191 1.64E-04 0.2045 \u2191 5.83E-05 0.1690 \u2191 8.33E-05 0.0479 1.95E-05 Breast Tissue 0.5123 4.28E-04 0.5745 \u2191 3.00E-03 0.7915 \u2191 1.25E-03 0.7811 \u2191 1.22E-03 0.6321 \u2191 6.73E-03 0.4513 \u2193 8.56E-04 Chess-krvk 0.7038 1.16E-05 0.8110 \u2191 8.16E-05 0.8298 \u2191 1.03E-05 0.8524 \u2191 3.43E-06 0.8271 \u2191 1.87E-04 0.6974 \u2193 4.03E-06 Conn Bench Vowel 0.3216 9.81E-05 0.6203 \u2191 3.73E-04 0.7187 \u2191 3.17E-04 0.7648 \u2191 2.03E-04 0.5983 \u2191 3.10E-04 0.4065 \u2191 2.60E-04 Contraceptive 0.5133 1.28E-04 0.5119 5.02E-05 0.6000 \u2191 1.42E-04 0.6356 \u2191 1.40E-04 0.5136 3.38E-05 0.5315 \u2191 6.83E-05 Ecoli 0.2774 1.25E-04 0.3449 \u2191 2.52E-03 0.4551 \u2191 2.74E-04 0.5018 \u2191 2.55E-04 0.3655 \u2191 9.72E-04 0.2755 2.20E-04 GM 0.0296 1.44E-06 0.3379 \u2191 1.78E-04 0.2460 \u2191 2.35E-04 0.2316 \u2191 1.38E-05 0.2290 \u2191 2.60E-05 0.6700 \u2191 2.34E-05 Hayes Roth 0.3675 5.22E-04 0.6750 \u2191 1.45E-03 0.6425 \u2191 6.00E-04 0.6431 \u2191 1.34E-03 0.6538 \u2191 1.13E-03 0.3900 8.83E-04 Ionosphere 0.1134 5.97E-05 0.1735 \u2191 5.75E-05 0.1972 \u2191 1.54E-04 0.2254 \u2191 9.98E-05 0.1889 \u2191 1.20E-04 0.1810 \u2191 4.44E-04 Iris 0.0727 6.62E-05 0.1240 \u2191 8.11E-04 0.3493 \u2191 5.80E-04 0.3953 \u2191 1.30E-03 0.1233 \u2191 3.22E-04 0.1037 \u2191 1.87E-04 Isolet 0.1129 6.05E-06 0.1446 \u2191 1.73E-05 0.1706 \u2191 1.52E-05 0.1755 \u2191 6.05E-06 0.0785 \u2193 2.11E-06 0.1989 \u2191 2.12E-05 Led7digit 0.3188 4.74E-05 0.3278 1.45E-04 0.4128 \u2191 1.63E-04 0.5302 \u2191 4.64E-04 0.3356 \u2191 6.22E-05 0.3418 \u2191 1.23E-04 Letter 0.2830 7.19E-06 0.4687 \u2191 4.91E-04 0.4372 \u2191 9.97E-06 0.5309 \u2191 1.64E-06 0.4859 \u2191 7.97E-04 0.3652 \u2191 9.34E-06 Madelon 0.4334 1.44E-05 0.4795 \u2191 8.15E-05 0.4778 \u2191 1.52E-04 0.5020 \u2191 1.53E-04 0.4715 \u2191 1.47E-04 0.5007 \u2191 1.85E-06 Marketing 0.6962 1.11E-05 0.7148 \u2191 1.25E-04 0.7566 \u2191 1.35E-05 0.7840 \u2191 3.08E-05 0.7324 \u2191 1.14E-04 0.6961 9.37E-06 Monk-2 0.0845 1.24E-04 0.2537 \u2191 1.10E-04 0.3157 \u2191 5.59E-05 0.3718 \u2191 9.45E-05 0.2632 \u2191 1.07E-04 0.1086 \u2191 2.94E-04 Multiple Features 0.1980 6.00E-05 0.1481 \u2193 6.97E-04 0.4280 \u2191 1.06E-04 0.5615 \u2191 1.36E-04 0.0919 \u2193 2.39E-04 0.9137 \u2191 1.85E-05 Musk1 0.2193 2.09E-04 0.3172 \u2191 2.95E-04 0.3105 \u2191 3.09E-04 0.3349 \u2191 5.70E-04 0.2431 \u2191 1.43E-04 0.5198 \u2191 4.50E-04 Musk2 0.0646 4.78E-06 0.0704 \u2191 2.90E-06 0.1097 \u2191 5.18E-06 0.1196 \u2191 5.05E-06 0.0930 \u2191 3.14E-06 0.1539 \u2191 1.73E-05 Nursery 0.1237 2.48E-05 0.2464 \u2191 8.88E-06 0.2950 \u2191 3.65E-06 0.3829 \u2191 1.70E-05 0.2647 \u2191 8.37E-05 0.0934 \u2193 2.17E-06 Optdigits 0.0480 2.17E-06 0.1248 \u2191 2.04E-04 0.0981 \u2191 3.57E-06 0.0940 \u2191 5.99E-06 0.0632 \u2191 5.25E-06 0.1140 \u2191 5.58E-06 Optical 0.0530 2.77E-06 0.1358 \u2191 1.68E-04 0.1073 \u2191 6.84E-06 0.1044 \u2191 6.62E-06 0.0648 \u2191 6.93E-06 0.1183 \u2191 5.41E-06 Penbased 0.0944 3.64E-05 0.1851 \u2191 1.28E-03 0.1452 \u2191 8.69E-06 0.1773 \u2191 6.50E-06 0.1405 \u2191 4.17E-04 0.1244 \u2191 1.98E-05 Satimage 0.1575 6.99E-06 0.3379 \u2191 1.76E-04 0.3991 \u2191 6.76E-05 0.4652 \u2191 1.34E-05 0.2713 \u2191 3.86E-04 0.2069 \u2191 2.66E-06 Skin_NonSkin 0.0029 3.32E-07 0.0917 \u2191 2.00E-07 0.1636 \u2191 1.71E-07 0.1553 \u2191 4.52E-07 0.0679 \u2191 3.14E-08 0.0152 \u2191 1.51E-06 Soybean 0.2596 2.77E-05 0.2489 3.61E-04 0.4909 \u2191 2.14E-04 0.5603 \u2191 4.20E-04 0.2873 \u2191 4.43E-04 0.3288 \u2191 2.17E-04 Tae 0.6166 3.99E-04 0.6232 7.76E-04 0.6517 \u2191 1.19E-03 0.6536 \u2191 7.55E-04 0.6146 8.75E-04 0.5189 \u2193 3.66E-04\nBreast Cancer 0.9460 6.42E-06 0.8373 \u2191 5.71E-05 0.7903 \u2191 1.64E-04 0.7564 \u2191 8.15E-05 0.8202 \u2191 8.37E-05 0.9482 2.15E-05 Breast Tissue 0.4603 4.18E-04 0.3753 \u2191 3.45E-03 0.1390 \u2191 1.00E-03 0.2102 \u2191 1.14E-03 0.3564 \u2191 6.75E-03 0.2673 \u2191 5.26E-04 Chess-krvk 0.2217 1.08E-05 0.1427 \u2191 7.35E-05 0.1393 \u2191 2.00E-05 0.1145 \u2191 4.71E-06 0.1050 \u2191 6.25E-05 0.0614 \u2191 2.09E-06 Conn Bench Vowel 0.6798 9.55E-05 0.3701 3.47E-04 0.2798 \u2191 3.20E-04 0.2340 \u2191 1.93E-04 0.3919 \u2191 3.86E-04 0.2292 \u2191 2.99E-04 Contraceptive 0.4786 1.01E-04 0.4522 \u2191 7.59E-05 0.3792 \u2191 1.65E-04 0.3443 \u2191 1.68E-04 0.4591 \u2191 3.78E-05 0.3801 \u2191 4.59E-05 Ecoli 0.4800 1.68E-04 0.4156 \u2191 2.14E-03 0.2962 \u2191 1.04E-03 0.2471 \u2191 4.43E-04 0.4045 \u2191 1.34E-03 0.2267 \u2191 1.47E-04 GM 0.9700 1.57E-06 0.6409 \u2191 1.91E-04 0.7133 \u2191 6.25E-04 0.7299 \u2191 3.43E-05 0.7330 \u2191 5.35E-05 0.2024 \u2191 7.21E-04 Hayes Roth 0.6621 5.40E-04 0.3114 \u2191 1.57E-03 0.3387 \u2191 7.81E-04 0.3203 \u2191 1.59E-03 0.3248 \u2191 1.18E-03 0.4488 \u2191 1.03E-03 Ionosphere 0.8725 9.11E-05 0.7890 \u2191 1.19E-04 0.7631 \u2191 2.36E-04 0.7423 \u2191 1.40E-04 0.7815 \u2191 1.55E-04 0.8092 \u2191 4.14E-04 Iris 0.9274 6.66E-05 0.8754 \u2191 8.31E-04 0.6513 \u2191 6.02E-04 0.6007 \u2191 1.43E-03 0.8763 \u2191 3.27E-04 0.8679 \u2191 3.63E-04 Isolet 0.8870 5.93E-06 0.8553 \u2191 1.73E-05 0.8293 \u2191 1.54E-05 0.8238 \u2191 6.15E-06 0.9214 \u2193 2.02E-06 0.2043 \u2191 6.64E-05 Led7digit 0.6816 4.95E-05 0.6670 \u2191 1.61E-04 0.5738 \u2191 2.25E-04 0.4482 \u2191 9.32E-04 0.6622 \u2191 5.94E-05 0.3060 \u2191 3.48E-04 Letter 0.7124 8.77E-06 0.5193 \u2191 5.15E-04 0.5614 \u2191 1.00E-05 0.4644 \u2191 1.94E-06 0.5057 \u2191 8.33E-04 0.1358 \u2191 6.25E-06 Madelon 0.5666 1.44E-05 0.5205 \u2191 8.13E-05 0.5222 \u2191 1.52E-04 0.4980 \u2191 1.53E-04 0.5285 \u2191 1.47E-04 0.3357 \u2191 3.95E-06 Marketing 0.2454 1.32E-05 0.2277 \u2191 6.19E-05 0.2011 \u2191 1.60E-05 0.1827 \u2191 2.65E-05 0.2230 \u2191 5.27E-05 0.0883 \u2191 2.39E-06 Monk-2 0.9152 1.25E-04 0.7461 \u2191 1.07E-04 0.6839 \u2191 5.63E-05 0.6254 \u2191 1.05E-04 0.7366 \u2191 1.09E-04 0.8912 \u2191 2.96E-04 Multiple Features 0.8028 5.66E-05 0.8523 \u2193 6.86E-04 0.5726 \u2191 1.08E-04 0.4362 \u2191 1.36E-04 0.9082 \u2193 2.35E-04 0.0195 \u2191 7.53E-06 Musk1 0.7780 2.12E-04 0.6817 \u2191 2.89E-04 0.6834 \u2191 3.16E-04 0.6602 \u2191 6.03E-04 0.7537 \u2191 1.43E-04 0.4367 \u2191 3.01E-03 Musk2 0.8831 1.21E-05 0.8613 \u2191 8.25E-06 0.7891 \u2191 1.77E-05 0.7690 \u2191 1.61E-05 0.8264 \u2191 1.23E-05 0.4802 \u2191 1.12E-03 Nursery 0.6747 3.07E-04 0.4534 \u2191 3.80E-06 0.4276 \u2191 1.86E-06 0.3928 \u2191 1.25E-05 0.4597 \u2191 1.24E-04 0.5354 \u2191 4.09E-05 Optdigits 0.9522 2.16E-06 0.8750 \u2191 2.07E-04 0.9020 \u2191 3.59E-06 0.9058 \u2191 6.03E-06 0.9368 \u2191 5.27E-06 0.6524 \u2191 8.39E-05 Optical 0.9473 2.81E-06 0.8642 \u2191 1.69E-04 0.8929 \u2191 6.61E-06 0.8954 \u2191 6.45E-06 0.9352 \u2191 6.94E-06 0.5982 \u2191 1.11E-04 Penbased 0.9010 4.62E-05 0.8136 \u2191 1.29E-03 0.8541 \u2191 8.57E-06 0.8209 \u2191 6.68E-06 0.8583 \u2191 4.47E-04 0.5595 \u2191 9.61E-05 Satimage 0.8150 8.38E-06 0.6052 \u2191 2.66E-04 0.5773 \u2191 6.57E-05 0.5113 \u2191 1.52E-05 0.6891 \u2191 4.49E-04 0.5152 \u2191 8.02E-06 Skin_NonSkin 0.9956 7.84E-07 0.8678 \u2191 5.70E-07 0.7621 \u2191 3.39E-07 0.7506 \u2191 1.56E-06 0.9057 \u2191 5.41E-08 0.9773 \u2191 3.40E-06 Soybean 0.7080 1.38E-04 0.7137 5.40E-04 0.3993 \u2191 5.84E-04 0.3330 \u2191 8.77E-04 0.6629 \u2191 4.73E-04 0.1454 \u2191 4.56E-04 Tae 0.3748 3.64E-04 0.3744 8.87E-04 0.3485 \u2191 1.21E-03 0.3463 \u2191 7.76E-04 0.3845 9.59E-04 0.3855 4.69E-04 Tic_Tac_Toe 0.6758 2.36E-04 0.5428 \u2191 2.08E-04 0.5306 \u2191 4.17E-04 0.5148 \u2191 3.03E-04 0.5115 \u2191 5.65E-04 0.4950 \u2191 1.28E-03 Twonorm 0.9629 1.52E-06 0.9754 \u2193 5.70E-07 0.9741 \u2193 7.52E-07 0.9635 1.03E-06 0.9719 \u2193 6.53E-07 0.9747 \u2193 8.45E-07 Vertebral 0.7352 2.48E-04 0.6958 \u2191 4.77E-04 0.6118 \u2191 6.37E-04 0.5640 \u2191 4.47E-04 0.6936 \u2191 3.61E-04 0.6808 \u2191 5.03E-04 Waveform_w_Noise 0.8323 1.42E-05 0.8314 2.13E-05 0.8281 \u2191 1.07E-05 0.7999 \u2191 1.97E-05 0.8312 8.97E-06 0.7154 \u2191 1.29E-05 Waveform_wo_Noise 0.8327 1.77E-05 0.8394 \u2193 2.88E-05 0.8299 3.14E-05 0.7938 \u2191 1.83E-05 0.8393 \u2193 1.49E-05 0.7175 \u2191 1.02E-05 Zoo 0.6485 6.93E-04 0.7117 \u2193 1.04E-03 0.5031 \u2191 1.56E-03 0.4969 \u2191 1.02E-03 0.6559 3.86E-04 0.2710 \u2191 7.13E-04\nTABLE 3: MEAN AND VARIANCE OF F1 SCORES OF BENCHMARK ALGORITHMS AND PROPOSED METHOD \u2193: The benchmark algorithm is better than RPNB(1b1), \u2191: The benchmark algorithm is worse than RPNB(1b1)\nTic_Tac_Toe 0.3086 2.46E-04 0.3399 \u2191 3.99E-05 0.4023 \u2191 2.99E-04 0.4410 \u2191 2.52E-04 0.3574 \u2191 1.33E-04 0.3243 1.20E-04 Twonorm 0.0371 1.52E-06 0.0246 \u2193 5.70E-07 0.0259 \u2193 7.53E-07 0.0365 1.03E-06 0.0281 \u2193 6.53E-07 0.0253 \u2193 8.45E-07 Vertebral 0.2171 1.29E-04 0.2403 \u2191 4.79E-04 0.3103 \u2191 3.51E-04 0.3632 \u2191 3.11E-04 0.2432 \u2191 2.36E-04 0.2092 2.05E-04 Waveform_w_Noise 0.1647 1.26E-05 0.1685 2.13E-05 0.1720 \u2191 1.09E-05 0.2003 \u2191 1.98E-05 0.1688 8.94E-06 0.2024 \u2191 7.14E-06 Waveform_wo_Noise 0.1629 1.42E-05 0.1601 2.86E-05 0.1699 \u2191 3.13E-05 0.2061 \u2191 1.83E-05 0.1604 1.45E-05 0.1924 \u2191 4.66E-06 Zoo 0.2020 1.61E-04 0.1564 \u2193 9.41E-05 0.2861 \u2191 2.24E-04 0.2931 \u2191 3.96E-04 0.1931 8.33E-05 0.2649 \u2191 6.67E-04"}], "references": [{"title": "A novel online Bayes classifier", "author": ["T.T.T. Nguyen", "T.T. Nguyen", "X.C. Pham", "A.W.-C. Liew", "Y. Hu", "T. Liang", "C.-T. Li"], "venue": "Proceedings of the International Conference on Digital Image Computing: Techniques and Applications (DICTA), Gold Coast, Australia", "citeRegEx": "Nguyen et al.. 2016a", "shortCiteRegEx": null, "year": 2016}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65(6): 386\u2013408", "citeRegEx": "Rosenblatt. 1958", "shortCiteRegEx": null, "year": 1958}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "Proceedings of the International Conference on Machine Learning (ICML), pages 928\u2013936", "citeRegEx": "Zinkevich. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Online passive aggressive algorithms", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz", "Y. Singer"], "venue": "Journal of Machine Learning Research, 7: 551\u2013585", "citeRegEx": "Crammer et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "Exact soft confidence-weighted learning", "author": ["J. Wang", "P. Zhao", "S.C.H Hoi"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML), Edinburgh, Scotland, UK", "citeRegEx": "Wang et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive regularization of weight vectors", "author": ["K. Crammer", "A. Kulesza", "M. Dredze"], "venue": "Proceedings of the 22th Advances in Neural Information Processing Systems (NIPS), pages 414\u2013422", "citeRegEx": "Crammer et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive regularization of weight vectors", "author": ["K. Crammer", "A. Kulesza", "M. Dredze"], "venue": "Machine Learning, 91(2): 155\u2013187", "citeRegEx": "Crammer et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Online bagging and boosting", "author": ["N. Oza", "S. Russell"], "venue": "Proceedings of the International Conference on Systems, Man and Cybernetics, pages 2340-2345", "citeRegEx": "Oza and Russell. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Online learning by ellipsoid method", "author": ["Yang et al", "2009] L. Yang", "R. Jin", "J Ye"], "venue": "Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Extensions of Lipshitz mapping into Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Proceeding of the Conference in modern analysis and probability. 26, pages 189-206, American Mathematical Society", "citeRegEx": "Johnson and Lindenstrauss. 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "Proceeding of the 7th International Conferene on Knowledge Discovery and Data Mining (ACM SIGKDD), pages 245-250", "citeRegEx": "Bingham and Mannila. 2001", "shortCiteRegEx": null, "year": 2001}, {"title": "Random Projection for High Dimensional Data Clustering: A Cluster Ensemble Approach", "author": ["X.Z. Fern", "C.E. Brodley"], "venue": "Proceedings of the 20th International Conference on Machine Learning (ICML), pages 186-193", "citeRegEx": "Fern and Brodley. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Fuzzy ensemble clustering based on random projections for DNA microarray data analysis", "author": ["R. Avogadri", "G. Valentini"], "venue": "Artificial Intelligence in Medicine, 45: 173-183", "citeRegEx": "Avogadri and Valentini. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond independence: Conditions for the optimality of the simple Bayesian classifier", "author": ["P. Domingos", "M. Pazzani"], "venue": "Proceedings of the Thirteenth International Conference on Machine Learning, pp. 105\u2013112", "citeRegEx": "Domingo. Pazzani. 1996", "shortCiteRegEx": null, "year": 1996}, {"title": "A novel genetic algorithm approach for simultaneous feature and classifier selection in multi classifier system", "author": ["T.T. Nguyen", "A.W.-C. Liew", "M.T. Tran", "X.C. Pham", "M.P. Nguyen"], "venue": "Proceeding of the IEEE Congress on Evolutionary Computation (CEC), pages 1698-1705", "citeRegEx": "Nguyen et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble methods in machine learning", "author": ["T. Dietterich"], "venue": "the first International Workshop on Multiple Classifier Systems, Springer, pp. 1-15", "citeRegEx": "Dietterich. 2000", "shortCiteRegEx": null, "year": 2000}, {"title": "A Novel Combining Classifier Method based on Variational Inference", "author": ["T.T. Nguyen", "T.T.T. Nguyen", "X.C. Pham", "A.W-C. Liew"], "venue": "Pattern Recognition, 49: 198-212", "citeRegEx": "Nguyen et al.. 2016b", "shortCiteRegEx": null, "year": 2016}, {"title": "Nevzorov, A Primer on Statistical Distributions", "author": ["Balakrishnan", "Nevzorov", "V.B. 2003] N. Balakrishnan"], "venue": null, "citeRegEx": "Balakrishnan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2003}, {"title": "On Combining Classifiers", "author": ["J. Kittler", "M. Hatef", "R.P.W. Duin", "J. Matas"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(3): 226-239", "citeRegEx": "Kittler et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "LIBOL: A Library for Online Learning Algorithms", "author": ["S.C.H. Hoi", "J. Wang", "P. Zhao"], "venue": "Journal of Machine Learning Research. 15: 495-499", "citeRegEx": "Hoi et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "MOA: Massive Online Analysis", "author": ["A. Bifet", "G. Holmes", "B. Pfahringer", "P. Kranen", "H. Kremer", "T. Jansen", "T. Seidl"], "venue": "a Framework for Stream Classification and Clustering. in Journal of Machine Learning Research Workshop and Conference Proceedings, Vol. 11: Workshop on Applications of Pattern Analysis", "citeRegEx": "Bifet et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing and Management.45: 427-437", "citeRegEx": "Sokolova and Lapalme. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Not So Na\u00efve Bayes: Aggregating One-Dependence Estimators", "author": ["G.I. Webb", "J.R. Boughton", "Z. Wang"], "venue": "Machine Learning. 58: 5-24", "citeRegEx": "Webb et al.. 2005", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Therefore, the online learning framework that deals with data streams has become increasing popular [Nguyen et al., 2016a].", "startOffset": 100, "endOffset": 122}, {"referenceID": 1, "context": "Well-known additive models include the Perceptron [Rosenblatt, 1958], Online Gradient Descent (OGD) [Zinkevich, 2003], Passive Aggressive learning (PA) [Crammer et al.", "startOffset": 50, "endOffset": 68}, {"referenceID": 2, "context": "Well-known additive models include the Perceptron [Rosenblatt, 1958], Online Gradient Descent (OGD) [Zinkevich, 2003], Passive Aggressive learning (PA) [Crammer et al.", "startOffset": 100, "endOffset": 117}, {"referenceID": 3, "context": "Well-known additive models include the Perceptron [Rosenblatt, 1958], Online Gradient Descent (OGD) [Zinkevich, 2003], Passive Aggressive learning (PA) [Crammer et al., 2006], Soft Confident Weighted (SCW) [Wang et al.", "startOffset": 152, "endOffset": 174}, {"referenceID": 4, "context": ", 2006], Soft Confident Weighted (SCW) [Wang et al., 2012], and Adaptive Regularization of Weights (AROW) [Crammer et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 5, "context": ", 2012], and Adaptive Regularization of Weights (AROW) [Crammer et al., 2009; Crammer et al., 2013].", "startOffset": 55, "endOffset": 99}, {"referenceID": 6, "context": ", 2012], and Adaptive Regularization of Weights (AROW) [Crammer et al., 2009; Crammer et al., 2013].", "startOffset": 55, "endOffset": 99}, {"referenceID": 7, "context": "Online Bagging and Online Boosting [Oza and Russell, 2005] are two well-known online ensemble algorithms.", "startOffset": 35, "endOffset": 58}, {"referenceID": 0, "context": "Other algorithms such as the Bayesian-based method [Nguyen et al., 2016a] and the Ellipsoid method [Yang et al.", "startOffset": 51, "endOffset": 73}, {"referenceID": 0, "context": "Finally, approaches like the Bayesian-based method [Nguyen et al., 2016a] estimate the distribution of each class and have problems dealing with very high dimensional datasets.", "startOffset": 51, "endOffset": 73}, {"referenceID": 9, "context": "To deal with the high dimensional data, our algorithm uses the theory of random projections [Johnson and Lindenstrauss, 1984] to project new observations to low dimension subspaces, thereby obtaining different data schemes for the ensemble of homogenuous base classifiers.", "startOffset": 92, "endOffset": 125}, {"referenceID": 15, "context": "The ensemble of Na\u00efve Bayes classifiers is expected to obtain better result than a single classifier due to the characteristic of ensemble system [Dietterich, 2000].", "startOffset": 146, "endOffset": 164}, {"referenceID": 9, "context": "In 1984, Johnson and Lindenstrauss (JL) published a paper about extending Lipschitz continuous maps from metric spaces to Euclidean spaces and introduced the JL Lemma [Johnson and Lindenstrauss, 1984].", "startOffset": 167, "endOffset": 200}, {"referenceID": 10, "context": "Moreover, generating the principle components is computationally expensive compare to generating the random matrix in random projection [Bingham and Mannila, 2001].", "startOffset": 136, "endOffset": 163}, {"referenceID": 12, "context": "follow the construction of random matrix in [Avogadri and Valentini, 2009] in which the projections are simply obtained by using a \u00d7 random matrix % * = 1 .", "startOffset": 44, "endOffset": 74}, {"referenceID": 14, "context": "Several popular fixed combining methods, namely Sum, Product, Majority Vote, Max, Min, and Median can be used as the combiner [Nguyen et al., 2014; Nguyen et al., 2016b; Kittler et al., 1998].", "startOffset": 126, "endOffset": 191}, {"referenceID": 16, "context": "Several popular fixed combining methods, namely Sum, Product, Majority Vote, Max, Min, and Median can be used as the combiner [Nguyen et al., 2014; Nguyen et al., 2016b; Kittler et al., 1998].", "startOffset": 126, "endOffset": 191}, {"referenceID": 18, "context": "Several popular fixed combining methods, namely Sum, Product, Majority Vote, Max, Min, and Median can be used as the combiner [Nguyen et al., 2014; Nguyen et al., 2016b; Kittler et al., 1998].", "startOffset": 126, "endOffset": 191}, {"referenceID": 3, "context": "We perform extensive comparative studies with a number of state-of-the-art algorithms as benchmarks: PA [Crammer et al., 2006], SCW [Wang et al.", "startOffset": 104, "endOffset": 126}, {"referenceID": 4, "context": ", 2006], SCW [Wang et al., 2012], OGD [Zinkevich, 2003], AROW [Crammer et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": ", 2012], OGD [Zinkevich, 2003], AROW [Crammer et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 5, "context": ", 2012], OGD [Zinkevich, 2003], AROW [Crammer et al., 2009; Crammer et al., 2013] (we use the implementation in LIBOL library [Hoi et al.", "startOffset": 37, "endOffset": 81}, {"referenceID": 6, "context": ", 2012], OGD [Zinkevich, 2003], AROW [Crammer et al., 2009; Crammer et al., 2013] (we use the implementation in LIBOL library [Hoi et al.", "startOffset": 37, "endOffset": 81}, {"referenceID": 19, "context": ", 2013] (we use the implementation in LIBOL library [Hoi et al., 2014] for these algorithms,", "startOffset": 52, "endOffset": 70}, {"referenceID": 7, "context": "10 default value for parameters are used if available), and Online Bagging [Oza and Russell, 2005] (we use the implementation in MOA library [Bifet et al.", "startOffset": 75, "endOffset": 98}, {"referenceID": 20, "context": "10 default value for parameters are used if available), and Online Bagging [Oza and Russell, 2005] (we use the implementation in MOA library [Bifet et al., 2010]).", "startOffset": 141, "endOffset": 161}, {"referenceID": 16, "context": "The number of learners in Online Bagging and K in the proposed method are set to 200 as in [Nguyen et al., 2016b],", "startOffset": 91, "endOffset": 113}, {"referenceID": 21, "context": "The proposed method is compared to the benchmark algorithms with respect to the error rate and F1 score (which is the harmonic mean of Precision and Recall) [Sokolova and Lapalme, 2009].", "startOffset": 157, "endOffset": 185}, {"referenceID": 19, "context": "Here we followed the performance measurements from LIBOL library [Hoi et al., 2014] where the authors used criteria such as mistake rate (classification error rate), and the number of updates (to measure the model stability) to evaluate the performance.", "startOffset": 65, "endOffset": 83}, {"referenceID": 22, "context": "In addition, we used Na\u00efve Bayes, a simple but efficacious learning algorithm [Webb et al., 2005] to generate the base classifiers where the parameter updates are simple and fast to compute.", "startOffset": 78, "endOffset": 97}], "year": 2017, "abstractText": "In this study, we introduce an ensemble-based approach for online machine learning. The ensemble of base classifiers in our approach is obtained by learning Na\u00efve Bayes classifiers on different training sets which are generated by projecting the original training set to lower dimensional space. We propose a mechanism to learn sequences of data using data chunks paradigm. The experiments conducted on a number of UCI datasets and one synthetic dataset demonstrate that the proposed approach performs significantly better than some well-known online learning algorithms.", "creator": "PDFCreator 2.5.1.5"}}}