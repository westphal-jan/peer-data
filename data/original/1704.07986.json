{"id": "1704.07986", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2017", "title": "Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization", "abstract": "We present in this paper our approach for modeling inter-topic preferences of Twitter users: for example, those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion surveys, electoral predictions, electoral campaigns, and online debates. In order to extract users' preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., \"A is completely wrong\"). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling inter-topic preferences as matrix factorization: representing users' preferences as a user-topic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our proposed approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.", "histories": [["v1", "Wed, 26 Apr 2017 07:04:46 GMT  (1596kb,D)", "http://arxiv.org/abs/1704.07986v1", "To appear in ACL2017"]], "COMMENTS": "To appear in ACL2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["akira sasaki", "kazuaki hanawa", "naoaki okazaki", "kentaro inui"], "accepted": true, "id": "1704.07986"}, "pdf": {"name": "1704.07986.pdf", "metadata": {"source": "CRF", "title": "Other Topics You May Also Agree or Disagree: Modeling Inter-Topic Preferences using Tweets and Matrix Factorization", "authors": ["Akira Sasaki", "Kazuaki Hanawa", "Naoaki Okazaki"], "emails": ["inui}@ecei.tohoku.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "Social media have changed the way people shape public opinion. The latest survey by the Pew Research Center reported that a majority of US adults (62%) obtain news via social media, and of those, 18% do so often (Gottfried and Shearer, 2016). Given that news and opinions are shared\nand amplified by friend networks of individuals (Jamieson and Cappella, 2008), individuals are thereby isolated from information that does not fit well with their opinions (Pariser, 2011). Ironically, cutting-edge social media technologies promote ideological groups even with its potential to deliver diverse information.\nA large number of studies already analyzed discussions, interactions, influences, and communities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016). Even though these studies provide intuitive visualizations and interpretations along the liberalconservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984).\nA potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated across topics. More recently, all participants of SemEval 2016 Task 6A (with five topics) could not outperform the baseline supervised method using n-gram features (Mohammad et al., 2016).\nIn addition, stance detection encounters dif-\nar X\niv :1\n70 4.\n07 98\n6v 1\n[ cs\n.C L\n] 2\n6 A\npr 2\n01 7\nficulties with different user types. Cohen and Ruths (2013) observed that existing methods on stance detection fail on \u201cordinary\u201d users because such methods primarily obtain training and test data from politically vocal users (e.g., politicians); for example, they found that a stance detector trained on a dataset with politicians achieved 91% accuracy on other politicians but only achieved 54% accuracy on \u201cordinary\u201d users. Establishing a bridge across different topics and users remains a major challenge not only in stance detection, but also in social media analytics.\nAn important component in establishing this bridge is commonsense knowledge about topics. For example, consider a topic a revision of Article 96 of the Japanese Constitution. We infer that the statement \u201cwe should maintain armed forces\u201d tends to favor this topic even without any lexical overlap between the topic and the statement. This inference is reasonable because: the writer of the statement favors armed forces; those who favor armed forces also favor a revision of Article 91; and those who favor a revision of Article 9 also favor a revision of Article 962. In general, this kind of commonsense knowledge can be expressed in\n1Article 9 prohibits armed forces in Japan. 2Article 96 specifies high requirements for making\namendments to Constitution of Japan (including Article 9).\nthe format: those who agree/disagree with topic A also agree/disagree with topic B. We call this kind of knowledge inter-topic preference throughout this paper.\nWe conjecture that previous work on stance detection indirectly learns inter-topic preferences within the same target through the use of n-gram features on a supervision data. In contrast, in the present paper, we directly acquire inter-topic preferences from an unlabeled corpus of tweets. This acquired knowledge regarding inter-topic preferences is useful not only for stance detection, but also for various real-world applications including public opinion survey, electoral campaigns, electoral predictions, and online debates.\nFigure 1 provides an overview of this work. In our system, we extract linguistic patterns in which people agree and disagree about specific topics (e.g., \u201cA is completely wrong\u201d); to accomplish this, as described in Section 2.1, we make use of hashtags within a large collection of tweets. The patterns are then used to extract instances of users\u2019 preferences regarding various topics, as detailed in Section 2.2. Inspired by previous work on item recommendation, in Section 3, we formalize the task of modeling inter-topic preferences as a matrix factorization: representing a sparse user-topic matrix (i.e., the extracted instances) with the prod-\nuct of low-rank user and topic matrices. These low-rank matrices provide latent vector representations of both users and topics. This approach is also useful for completing preferences of \u201cordinary\u201d (i.e., less vocal) users, which fills the gap between different types of users.\nThe contributions of this paper are threefold.\n1. To the best of our knowledge, this is the first study that models inter-topic preferences for unlimited targets on real-world data.\n2. Our experimental results show that this approach can accurately predict missing topic preferences of users accurately (80\u201394%).\n3. Our experimental results also demonstrate that the latent vector representations of topics successfully encode inter-topic preferences, e.g., those who agree with nuclear power plants also agree with nuclear fuel cycles.\nThis study uses a Japanese Twitter corpus because of its availability from the authors, but the core idea is applicable to any language."}, {"heading": "2 Mining Topic Preferences of Users", "text": "In this section, we describe how we collect statements in which users agree or disagree with various topics on Twitter, which then serves as source data for modeling inter-topic preferences. More formally, we are interested in acquiring a collection of tuples (u, t, v), where: u \u2208 U is a user; U is the set of all users on Twitter; t \u2208 T is a topic; T is the set of all topics; and v \u2208 {+1,\u22121} is +1 when the user u agrees with the topic t and \u22121 otherwise (i.e., disagreement).\nThroughout this work, we use a corpus consisting of 35,328,745,115 Japanese tweets (7,340,730 users) crawled from February 6, 2013 to September 30, 2016. We removed retweets from the corpus."}, {"heading": "2.1 Mining Linguistic Patterns of Agreement and Disagreement", "text": "We use linguistic patterns to extract tuples (u, t, v) from the aforementioned corpus. More specifically, when a tweet message matches to one of linguistic patterns of agreement (e.g., \u201ct is necessary\u201d), we regard that the author u of the tweet agrees with topic t. Conversely, a statement of disagreement is identified by linguistic patterns for disagreement (e.g., \u201ct is unacceptable\u201d).\nIn order to design linguistic patterns, we focus on hashtags appearing in the corpus that have been popular clues for locating subjective statements such as sentiments (Davidov et al., 2010), emotions (Qadir and Riloff, 2014), and ironies (Van Hee et al., 2016). Hashtags are also useful for finding strong supporters and critics, as well as their target topics; for example, #immigrantsWelcome indicates that the author favors immigrants; and #StopAbortion is against abortion.\nBased on this intuition, we design regular expressions for both pro hashtags \u201c#(.+)sansei\u201d3 and con hashtags \u201c#(.+)hantai\u201d4, where (.+) matches a target topic. These regular expressions can find users who have strong preferences to topics. Using this approach, we extracted 31,068 occurrences of pro/con hashtags used by 18,582 users for 4,899 topics. We regard the set of topics found using this procedure as set of target topics T in this study.\nEach time we encounter a tweet containing a pro/con hashtag, we searched for corresponding textual statements as follows. Suppose that a tweet includes a hashtag (e.g., #TPPsansei) for a topic t (e.g., TPP). Assuming that the author of the given tweet does not change their attitude toward a topic over time, we search for other tweets posted by the same author that also have the topic keyword t. This process retrieves tweets like \u201cI support TPP.\u201d Then, we replace the topic keyword into a variable A to extract patterns, e.g., \u201cI support A.\u201d Here, the definition of the pattern unit is language specific. For Japanese tweets, we simply recognize a pattern that starts with a variable (i.e., topic) and ends at the end of the sentence5.\nBecause this procedure also extracts useless patterns such as \u201cto A\u201d and \u201cthis is A\u201d, we manually choose useful patterns in a systematic way: sort patterns in descending order of the number of users who use the pattern; and check the sorted list of patterns manually; and remove useless patterns.\n3Unlike English hashtags, we systematically attach a noun sansei, which stands for pro (agreement) in Japanese, to a topic, for example, #TPPsansei. This paper uses the alphabetical expression sansei only for explanation; the actual pattern uses Chinese characters corresponding to sansei.\n4A Japanese noun hantai stands for con (disagreement), for example, #TPPhantai. This paper uses the alphabetical expression hantai only for explanation; the actual pattern uses Chinese characters corresponding to hantai.\n5In English, this treatment roughly corresponds to extracting a verb phrase with the variable A.\nUsing this approach, we obtained 100 pro patterns (e.g., \u201cwelcome A\u201d and \u201cA is necessary\u201d) and 100 con patterns (\u201cdo not letA\u201d and \u201cI don\u2019t wantA\u201d)."}, {"heading": "2.2 Extracting Instances of Topic Preferences", "text": "By using the pro and con patterns acquired using the approach described in Section 2.1, we extract instances of (u, t, v) as follows. When a sentence in a tweet whose author is user u matches one of the pro patterns (e.g., \u201ct is necessary\u201d) and the topic t is included in the set of target topics T , we recognize this as an instance of (u, t,+1). Similarly, when a sentence matches one of the con patterns (e.g., \u201cI don\u2019t want t\u201d) and the topic t is included in the set of target topics T , we recognize this as an instance of (u, t,\u22121). Using this approach, we collected 25,805,909 tuples corresponding to 3,302,613 users and 4,899 topics. Because these collected tuples included comparatively infrequent users and topics, we removed users and topics that appeared less than five times. In addition, there were also meaningless frequent topics such as \u201cof\u201d and \u201cit\u201d. Therefore, we sorted topics in descending order of their co-occurrence frequencies with each of the pro patterns and con patterns, and then removed meaningless topics in the top 100 topics. This resulted in 9,961,509 tuples regarding 273,417 users and 2,323 topics."}, {"heading": "3 Matrix Factorization", "text": "Using the methods described in Section 2, from the corpus, we collected a number of instances of users\u2019 preferences regarding various topics. However, Twitter users do not necessarily express preferences for all topics. In addition, it is by nature impossible to predict whether a new (i.e., nonexistent in the data) user agrees or disagrees with given topics. Therefore, in this section, we apply matrix factorization (Koren et al., 2009) in order to predict missing values, inspired by research regarding item recommendation (Bell and Koren, 2007; Dror et al., 2011). In essence, matrix factorization maps both users and topics onto a latent feature space that abstracts topic preferences of users.\nHere, letR be a sparse matrix of |U |\u00d7|T |. Only when a user u expresses a preference for topic t do we compute an element of the sparse matrix ru,t,\nru,t = #(u, t,+1)\u2212#(u, t,\u22121) #(u, t,+1) + #(u, t,\u22121)\n(1)\nHere, #(u, t,+1) and #(u, t,\u22121) represent the numbers of occurrences of instances (u, t,+1)\nand (u, t,\u22121), respectively. Thus, an element ru,t approaches +1 as the user u favors the topic t, and \u22121 otherwise. If the user u does not make any statement regarding the topic t (i.e., neither (u, t,+1) nor (u, t,\u22121) exists in the data), we do not fill the corresponding element, leaving it as a missing value.\nMatrix factorization decomposes the sparse matrix R into low-dimensional matrices P \u2208 Rk\u00d7|U | andQ \u2208 Rk\u00d7|T |, where k is a parameter that specifies the number of dimensions of the latent space. We minimize the following objective function to find the matrices P and Q,\nmin P,Q \u2211 (u,t)\u2208R ( (ru,t \u2212 pu\u1d40qt)2\n+\u03bbP \u2016pu\u20162 + \u03bbQ \u2016qt\u20162 ) . (2)\nHere, (u, t) \u2208 R is repeated for elements filled in the sparse matrix R, pu \u2208 Rk and qv \u2208 Rk are u column vectors of P and v column vectors of Q, respectively, and \u03bbP \u2265 0 and \u03bbQ \u2265 0 represent coefficients of regularization terms. We call pu and qt the user vector and topic vector, respectively.\nUsing these user and topic vectors, we can predict an element r\u0302u,t that may be missing in the original matrix R,\nr\u0302u,t ' pu\u1d40qt. (3)\nWe use libmf6 (Chin et al., 2015) to solve the optimization problem in Equation 2. We set regularization coefficients \u03bbP = 0.1 and \u03bbQ = 0.1 and use default values for the other parameters of libmf."}, {"heading": "4 Evaluation", "text": ""}, {"heading": "4.1 Determining the Dimension Parameter k", "text": "How good is the low-rank approximation found by matrix factorization? And can we find the \u201csweet spot\u201d for the number of dimensions k of the latent space? We investigate the reconstruction error of matrix factorization using different values of k to answer these questions. We use Root Mean Squared Error (RMSE) to measure error,\nRMSE =\n\u221a\u2211 (u,t)\u2208R (pu \u1d40qt \u2212 ru,t)2\nN . (4)\n6https://github.com/cjlin1/libmf\nHere, N is the number of elements in the sparse matrix R (i.e., the number of known values).\nFigure 2 shows RMSE values over iterations of libmf with the dimension parameter k \u2208 {1, 2, 5, 10, 30, 50, 100, 300, 500}. We observed that the reconstruction error decreased as the iterative method of libmf progressed. The larger the number of dimensions k was, the smaller the reconstruction error became; the lowest reconstruction error was 0.3256 with k = 500. We also observed the error with k = 1, which corresponds to mapping users and topics onto one dimension similarly to the political spectrum of liberal and conservative. Judging from the relatively high RMSE values with k = 1, we conclude that it may be difficult to represent everything in the data using a one-dimensional axis. Based on this result, we concluded that matrix factorization with k = 100 is sufficient for reconstructing the original matrix R and therefore used this parameter value for the rest of our experiments."}, {"heading": "4.2 Predicting Missing Topic Preferences", "text": "How accurately can the user and topic vectors predict missing topic preferences? To answer this question, we evaluate the accuracy in predicting hidden preferences in the matrix R as follows. First, we randomly selected 5% of existing elements in R and let Y represent the collection of the selected elements (test set). We then perform matrix factorization on the sparse matrix without the selected elements of Y , that is, only with the remaining 95% elements of R (training set). We define the accuracy of the prediction as\n1 |Y | \u2211 u,t\u2208Y 1 (sign(r\u0302u,t) = sign(ru,t)) (5)\nHere, ru,t denotes the actual (i.e., self-declared) preference values, r\u0302u,t represents the preference value predicted by Equation 3, sign(.) represents the sign of the argument, and 1(.) yields 1 only when the condition described in the argument holds and 0 otherwise. In other words, Equation 5 computes the proportion of correct predictions to all predictions, assuming zero to be the decision boundary between pro and con.\nFigure 3 plots prediction accuracy values calculated from different sets of users. Here the xaxis represents a threshold \u03b8, which filters out users whose declarations of topic preferences are no greater than \u03b8 topics. In other words, Figure 3 shows prediction accuracy when we know user preferences for at least \u03b8 topics. For comparison, we also include the majority baseline that predicts pro and con based on the majority of preferences regarding each topic in the training set.\nOur proposed method was able to predict missing preferences with an 82.1% accuracy for users stating preferences for at least five topics. This accuracy increased as our method received more information regarding the users, reaching a 94.0% accuracy when \u03b8 = 100. This result again indicates that our proposed method reasonably utilizes known preferences to complete missing preferences.\nIn contrast, the performance of the majority baseline decreased as it received more information regarding the users. Because this result was rather counter-intuitive, we examined the cause of this phenomenon. Consequently, this result turned out to be reasonable because preferences of vocal users deviated from those of the average users. Figure 4 illustrates this finding, showing the mean of variances of preference values ru,t across self-\ndeclared topics. In the figure, the x-axis represents a threshold \u03b8, which filters out users whose statements of topic preferences are no greater than \u03b8 topics. We observe that the mean variance increased as we focused on vocal users. Overall, these results demonstrate the usefulness of user and topic vectors in predicting missing preferences.\nTable 1 shows examples in which missing preferences of two users were predicted from known statements of agreements and disagreements7. In the table, predicted topics are accompanied by the corresponding r\u0302u,t value in parentheses. As an example, our proposed method predicted that the user A, who is positive toward regime change but negative toward Okinawa US military base, may also be positive toward vote of non-confidence to Cabinet but negative toward construction of a new base."}, {"heading": "4.3 Inter-topic Preferences", "text": "Do the topic vectors obtained by matrix factorization capture inter-topic preferences, such as \u201cPeople who agree with A also agree with B\u201d?\nBecause no dataset exists for this evaluation, we created a dataset of pairwise inter-topic preferences by using a crowdsourcing service8. Sampling topic pairs randomly, we collected 150 topic pairs whose cosine similarities of topic vectors\n7We anonymized user names in these examples. In addition, we removed topics that are too discriminatory or aggressive to other countries and races. Even though the experimental results of this paper do not necessarily reflect our idea, we do not think it is a good idea to distribute politically incorrect ideas through this paper.\n8We used Yahoo! Crowdsourcing, a Japanese online service for crowdsourcing. http://crowdsourcing.yahoo.co.jp/\nwere below \u22120.6, 150 pairs whose cosine similarities were between \u22120.6 and 0.6, and 150 pairs whose cosine similarities were above 0.6. In this way, we obtained 450 topic pairs for evaluation.\nGiven a pair of topics A and B, a crowd worker was asked to choose a label from the following three options: (a) those who agree/disagree with topic A may also agree/disagree with topic B; (b) those who agree/disagree with topic A may conversely disagree/agree with topic B; (c) otherwise (no association between A and B). Creating twenty pairs of topics as gold data, we removed labeling results from workers whose accuracy is less than 90%.\nConsequently, we obtained 6\u201310 human judgements for every topic pair. Regarding (a) as +1 point, (b) as \u22121 point, and (c) as 0 point, we computed the mean of the points (i.e., average human judgements) for each topic pair. Spearman\u2019s rank correlation coefficient (\u03c1) between cosine similarity values of topic vectors and human judgements was 0.2210. We could observe a moderate correlation even though inter-topic preferences collected in this manner were highly subjective.\nIn addition to the quantitative evaluation, as summarized in Table 2, we also checked similar topics for three controversial topics, Liberal Democratic Party (LDP), constitutional amendment and right of foreigners to vote (Table 2). Topics similar to LDP included synonymous ones (e.g., Abe\u2019s LDP and Abe administration) and other topics promoted by the LDP (e.g., resuming nuclear power plant operations, bus rapid transit (BRT) and hate speech countermeasure law). Considering that people who support the LDP may also tend to favor its policies, we found these results reasonable. As for the other example, constitutional amendment had a feature vector that was similar to that of amendment of Article 9, enforcement of specific secret protection law and security related law. From these results, we concluded that topic vectors were able to capture inter-topic preferences."}, {"heading": "5 Related Work", "text": "In this section, we summarize the related work that spreads across various research fields.\nSocial Science and Political Science A number of of studies analyze social phenomena regarding political activities, political thoughts, and public opinions on social media. These studies\nmodel the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016), political parties (Tumasjan et al., 2010; Boutet et al., 2013; Makazhanov and Rafiei, 2013), and elections (O\u2019Connor et al., 2010; Conover et al., 2011).\nEmploying a single axis (e.g., liberal to conservative) or a few axes (e.g., political parties and candidates of elections), these studies provide intuitive visualizations and interpretations along the respective axes. In contrast, this study is the first attempt to recognize and organize various axes of topics on social media with no prior assumptions regarding the axes. Therefore, we think our study provides a new tool for computational social science and political science that enables researchers to analyze and interpret phenomena on social media.\nNext, we describe previous research focused on acquiring lexical knowledge of politics. Sim et al. (2013) measured ideological positions of candidates in US presidential elections from their\nspeeches. The study first constructs \u201ccue lexicons\u201d from political writings labeled with ideologies by domain experts, using sparse additive generative models (Eisenstein et al., 2011). These constructed cue lexicons were associated with such ideologies as left, center, and right. Representing each speech of a candidate with cue lexicons, they inferred the proportions of ideologies of the candidate. The study requires a predefined set of labels and text data associated with the labels.\nBamman and Smith (2015) presented an unsupervised method for assessing the political stance of a proposition, such as \u201cglobal warming is a hoax,\u201d along the political spectrum of liberal to conservative. In their work, a proposition was represented by a tuple in the form \u3008subject, predicate\u3009, for example, \u3008global warming, hoax\u3009. They presented a generative model for users, subjects, and predicates to find a one-dimensional latent space that corresponded to the political spectrum.\nSimilar to our present work, their work (Bamman and Smith, 2015) did not require labeled data\nto map users and topics (i.e., subjects) onto a latent feature space. In their paper, they reported that the generative model outperformed Principal Component Analysis (PCA), which is a method for matrix factorization. Empirical results here probably reflected the underlying assumptions that PCA treats missing elements as zero and not as missing data. In contrast, in the present work, we properly distinguish missing values from zero, excluding missing elements of the original matrix from the objective function of Equation 2. Further, this work demonstrated the usefulness of the latent space, that is, topic and user vectors, in predicting missing topic preferences of users and inter-topic preferences.\nFine-grained Opinion Analysis The method presented in Section 2 is an instance of finegrained opinion analysis (Wiebe et al., 2005; Choi et al., 2006; Johansson and Moschitti, 2010; Yang and Cardie, 2013; Deng and Wiebe, 2015), which extracts a tuple of a subjective opinion, a holder of the opinion, and a target of the opinion from text. Although these previous studies have the potential to improve the quality of the user-topic matrix R, unfortunately, no corpus or resource is available for the Japanese language. We do not currently have a large collection of English tweets, but combining fine-grained opinion analysis with matrix factorization is an immediate future work.\nCausality Relation Some of inter-topic preferences in this work can be explained by causality relation, for example, \u201cTPP promotes free trade.\u201d A number of previous studies acquire instances of causal relation (Girju, 2003; Do et al., 2011) and promote/suppress relation (Hashimoto et al., 2012; Fluck et al., 2015) from text. The causality knowledge is useful for predicting (hypotheses of) future events (Radinsky et al., 2012; Radinsky and Davidovich, 2012; Hashimoto et al., 2015).\nInter-topic preferences, however, also include pairs of topics in which causality relation hardly holds. As an example, it is unreasonable to infer that nuclear plant and railroading of bills have a causal relation, but those who dislike nuclear plant also oppose railroading of bills because presumably they think the governing political parties rush the bill for resuming a nuclear plant. In this study, we model these inter-topic preferences based on preferences of the public. That said, we have as a promising future direction of our work plans to in-\ncorporate approaches to acquire causality knowledge."}, {"heading": "6 Conclusion", "text": "In this paper, we presented a novel approach for modeling inter-topic preferences of users on Twitter. Designing linguistic patterns for identifying support and opposition statements, we extracted users\u2019 preferences regarding various topics from a large collection of tweets. We formalized the task of modeling inter-topic preferences as a matrix factorization that maps both users and topics onto a latent feature space that abstracts users\u2019 preferences. Through our experimental results, we demonstrated that our approach was able to accurately predict missing topic preferences of users (80\u201394%) and that our latent vector representations of topics properly encoded inter-topic preferences.\nFor our immediate future work, we plan to embed the topic and user vectors to create a crosstopic stance detector. It is possible to generalize our work to model heterogeneous signals, such as interests and behaviors of people, for example, \u201cthose who are interested in A also support B,\u201d and \u201cthose who favor A also vote for B\u201d. Therefore, we believe that our work will bring about new applications in the field of NLP and other disciplines."}, {"heading": "Acknowledgements", "text": "This work was supported by JSPS KAKENHI Grant Number 15H05318 and JST CREST Grant Number J130002054, Japan."}], "references": [{"title": "The political blogosphere and the 2004 U.S. election: Divided they blog", "author": ["Lada A. Adamic", "Natalie Glance"], "venue": "In Proceedings of the 3rd International Workshop on Link Discovery (LinkKDD", "citeRegEx": "Adamic and Glance.,? \\Q2005\\E", "shortCiteRegEx": "Adamic and Glance.", "year": 2005}, {"title": "Cats rule and dogs drool!: Classifying stance in online debate", "author": ["Pranav Anand", "Marilyn Walker", "Rob Abbott", "Jean E. Fox Tree", "Robeson Bowmani", "Michael Minor."], "venue": "Proceedings of the 2nd Workshop on Computational Approaches to Subjec-", "citeRegEx": "Anand et al\\.,? 2011", "shortCiteRegEx": "Anand et al\\.", "year": 2011}, {"title": "Exposure to ideologically diverse news and opinion", "author": ["Eytan Bakshy", "Solomon Messing", "Lada A. Adamic"], "venue": null, "citeRegEx": "Bakshy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bakshy et al\\.", "year": 2015}, {"title": "Open extraction of fine-grained political statements", "author": ["David Bamman", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP 2015). pages 76\u201385.", "citeRegEx": "Bamman and Smith.,? 2015", "shortCiteRegEx": "Bamman and Smith.", "year": 2015}, {"title": "Lessons from the Netflix prize challenge", "author": ["Robert M. Bell", "Yehuda Koren."], "venue": "ACM SIGKDD Explorations Newsletter 9(2):75\u201379. https://doi.org/10.1145/1345448.1345465.", "citeRegEx": "Bell and Koren.,? 2007", "shortCiteRegEx": "Bell and Koren.", "year": 2007}, {"title": "What\u2019s in Twitter, I know what parties are popular and who you are supporting now", "author": ["Antoine Boutet", "Hyoungshick Kim", "Eiko Yoneki"], "venue": "Social Network Analysis and Mining (SNAM", "citeRegEx": "Boutet et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boutet et al\\.", "year": 2013}, {"title": "A fast parallel stochastic gradient method for matrix factorization in shared memory systems", "author": ["Wei-Sheng Chin", "Yong Zhuang", "Yu-Chin Juan", "Chih-Jen Lin."], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST) 6(1):2.", "citeRegEx": "Chin et al\\.,? 2015", "shortCiteRegEx": "Chin et al\\.", "year": 2015}, {"title": "Joint extraction of entities and relations for opinion recognition", "author": ["Yejin Choi", "Eric Breck", "Claire Cardie."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006). pages 431\u2013439.", "citeRegEx": "Choi et al\\.,? 2006", "shortCiteRegEx": "Choi et al\\.", "year": 2006}, {"title": "Classifying political orientation on Twitter: It\u2019s not easy! In Proc", "author": ["Raviv Cohen", "Derek Ruths."], "venue": "of the Seventh International AAAI Conference on Weblogs and Social Media (ICWSM 2013). pages 91\u201399.", "citeRegEx": "Cohen and Ruths.,? 2013", "shortCiteRegEx": "Cohen and Ruths.", "year": 2013}, {"title": "Predicting the political alignment of twitter users", "author": ["Michael D Conover", "Bruno Gon\u00e7alves", "Jacob Ratkiewicz", "Alessandro Flammini", "Filippo Menczer."], "venue": "Privacy, 2011 IEEE Third International Conference on Security, Risk and", "citeRegEx": "Conover et al\\.,? 2011", "shortCiteRegEx": "Conover et al\\.", "year": 2011}, {"title": "Enhanced sentiment learning using twitter hashtags and smileys", "author": ["Dmitry Davidov", "Oren Tsur", "Ari Rappoport."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010). pages 241\u2013249.", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "MPQA 3.0: An entity/event-level sentiment corpus", "author": ["Lingjia Deng", "Janyce Wiebe"], "venue": "In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Deng and Wiebe.,? \\Q2015\\E", "shortCiteRegEx": "Deng and Wiebe.", "year": 2015}, {"title": "Minimally supervised event causality identification", "author": ["Quang Do", "Yee Seng Chan", "Dan Roth."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011). pages 294\u2013303.", "citeRegEx": "Do et al\\.,? 2011", "shortCiteRegEx": "Do et al\\.", "year": 2011}, {"title": "The Yahoo! Music dataset and KDD-Cup\u201911", "author": ["Gideon Dror", "Noam Koenigstein", "Yehuda Koren", "Markus Weimer."], "venue": "Proceedings of the 2011 International Conference on KDD Cup 2011 (KDDCUP 2011). pages 3\u201318.", "citeRegEx": "Dror et al\\.,? 2011", "shortCiteRegEx": "Dror et al\\.", "year": 2011}, {"title": "Sparse additive generative models of text", "author": ["Jacob Eisenstein", "Amr Ahmed", "Eric P Xing."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML 2011).", "citeRegEx": "Eisenstein et al\\.,? 2011", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2011}, {"title": "Track 4 overview: Extraction of causal network information in biological expression language (BEL)", "author": ["Juliane Fluck", "Sumit Madan", "Tilia Renate Ellendorff", "Theo Mevissen", "Simon Clematide", "Adrian van der Lek", "Fabio Rinaldi."], "venue": "Proceedings of", "citeRegEx": "Fluck et al\\.,? 2015", "shortCiteRegEx": "Fluck et al\\.", "year": 2015}, {"title": "Automatic detection of causal relations for question answering", "author": ["Roxana Girju."], "venue": "Proceedings of the ACL 2003 Workshop on Multilingual Summarization and Question Answering - Volume 12. pages 76\u201383. https://doi.org/10.3115/1119312.1119322.", "citeRegEx": "Girju.,? 2003", "shortCiteRegEx": "Girju.", "year": 2003}, {"title": "News use across social media platforms 2016", "author": ["Jeffrey Gottfried", "Elisa Shearer."], "venue": "Technical report, Pew Research Center.", "citeRegEx": "Gottfried and Shearer.,? 2016", "shortCiteRegEx": "Gottfried and Shearer.", "year": 2016}, {"title": "Excitatory or inhibitory: A new semantic orientation extracts contradiction and causality from the web", "author": ["Chikara Hashimoto", "Kentaro Torisawa", "Stijn De Saeger", "Jong-Hoon Oh", "Jun\u2019ichi Kazama"], "venue": null, "citeRegEx": "Hashimoto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2012}, {"title": "Generating event causality hypotheses through semantic relations", "author": ["Chikara Hashimoto", "Kentaro Torisawa", "Julien Kloetzer", "Jong-Hoon Oh."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2015). pages 2396\u2013", "citeRegEx": "Hashimoto et al\\.,? 2015", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2015}, {"title": "Echo Chamber: Rush Limbaugh and the Conservative Media Establishment", "author": ["Kathleen Hall Jamieson", "Joseph N. Cappella."], "venue": "Oxford University Press.", "citeRegEx": "Jamieson and Cappella.,? 2008", "shortCiteRegEx": "Jamieson and Cappella.", "year": 2008}, {"title": "Syntactic and semantic structure for opinion expression detection", "author": ["Richard Johansson", "Alessandro Moschitti."], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL 2010). pages 67\u201376.", "citeRegEx": "Johansson and Moschitti.,? 2010", "shortCiteRegEx": "Johansson and Moschitti.", "year": 2010}, {"title": "All I know about politics is what I read in Twitter\u201d: Weakly supervised models for extracting politicians\u2019 stances from twitter", "author": ["Kristen Johnson", "Dan Goldwasser."], "venue": "Proceedings of the 26th International Conference on Computa-", "citeRegEx": "Johnson and Goldwasser.,? 2016", "shortCiteRegEx": "Johnson and Goldwasser.", "year": 2016}, {"title": "Liberalism and Conservatism: The Nature and Structure of Social Attitudes", "author": ["Fred N. Kerlinger."], "venue": "Lawrence Erlbaum Associates.", "citeRegEx": "Kerlinger.,? 1984", "shortCiteRegEx": "Kerlinger.", "year": 1984}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky."], "venue": "Computer 42(8):30\u201337. https://doi.org/10.1109/MC.2009.263.", "citeRegEx": "Koren et al\\.,? 2009", "shortCiteRegEx": "Koren et al\\.", "year": 2009}, {"title": "Beyond Liberal and Conservative: Reassessing the Political Spectrum", "author": ["William S. Maddox", "Stuart A. Lilie."], "venue": "Cato Inst.", "citeRegEx": "Maddox and Lilie.,? 1984", "shortCiteRegEx": "Maddox and Lilie.", "year": 1984}, {"title": "Predicting political preference of twitter users", "author": ["Aibek Makazhanov", "Davood Rafiei."], "venue": "Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013). pages 298\u2013305.", "citeRegEx": "Makazhanov and Rafiei.,? 2013", "shortCiteRegEx": "Makazhanov and Rafiei.", "year": 2013}, {"title": "Semeval-2016 task 6: Detecting stance in tweets", "author": ["Saif Mohammad", "Svetlana Kiritchenko", "Parinaz Sobhani", "Xiaodan Zhu", "Colin Cherry."], "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). pages 31\u201341.", "citeRegEx": "Mohammad et al\\.,? 2016", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "Support or oppose?: classifying positions in online debates from reply activities and opinion expressions", "author": ["Akiko Murakami", "Rudy Raymond."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010). pages", "citeRegEx": "Murakami and Raymond.,? 2010", "shortCiteRegEx": "Murakami and Raymond.", "year": 2010}, {"title": "From tweets to polls: Linking text sentiment to public opinion time series", "author": ["Brendan O\u2019Connor", "Ramnath Balasubramanyan", "Bryan R. Routledge", "Noah A. Smith"], "venue": "In Proceedings of the Fourth International AAAI Conference on Weblogs", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "The Filter Bubble: How the New Personalized Web Is Changing What We Read and How We Think", "author": ["Eli Pariser."], "venue": "Penguin Books.", "citeRegEx": "Pariser.,? 2011", "shortCiteRegEx": "Pariser.", "year": 2011}, {"title": "Learning emotion indicators from tweets: Hashtags, hashtag patterns, and phrases", "author": ["Ashequl Qadir", "Ellen Riloff."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014). pages 1203\u2013", "citeRegEx": "Qadir and Riloff.,? 2014", "shortCiteRegEx": "Qadir and Riloff.", "year": 2014}, {"title": "Learning to predict from textual data", "author": ["Kira Radinsky", "Sagie Davidovich."], "venue": "Journal of Artificial Intelligence Research (JAIR) 45(1):641\u2013684.", "citeRegEx": "Radinsky and Davidovich.,? 2012", "shortCiteRegEx": "Radinsky and Davidovich.", "year": 2012}, {"title": "Learning causality for news events prediction", "author": ["Kira Radinsky", "Sagie Davidovich", "Shaul Markovitch."], "venue": "Proceedings of the 21st International Conference on World Wide Web (WWW 2012). pages 909\u2013918.", "citeRegEx": "Radinsky et al\\.,? 2012", "shortCiteRegEx": "Radinsky et al\\.", "year": 2012}, {"title": "Measuring ideological proportions in political speeches", "author": ["Yanchuan Sim", "Brice D.L. Acree", "Justin H. Gross", "Noah A. Smith."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013). pages 91\u2013", "citeRegEx": "Sim et al\\.,? 2013", "shortCiteRegEx": "Sim et al\\.", "year": 2013}, {"title": "Recognizing stances in online debates", "author": ["Swapna Somasundaran", "Janyce Wiebe."], "venue": "Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language", "citeRegEx": "Somasundaran and Wiebe.,? 2009", "shortCiteRegEx": "Somasundaran and Wiebe.", "year": 2009}, {"title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts", "author": ["Matt Thomas", "Bo Pang", "Lillian Lee."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006).", "citeRegEx": "Thomas et al\\.,? 2006", "shortCiteRegEx": "Thomas et al\\.", "year": 2006}, {"title": "Predicting elections with twitter: What 140 characters reveal about political sentiment", "author": ["Andranik Tumasjan", "Timm Oliver Sprenger", "Philipp G Sandner", "Isabell M Welpe."], "venue": "Fourth International AAAI Conference on Weblogs and Social Media", "citeRegEx": "Tumasjan et al\\.,? 2010", "shortCiteRegEx": "Tumasjan et al\\.", "year": 2010}, {"title": "Monday mornings are my fave :) #not exploring the automatic recognition of irony in english tweets", "author": ["Cynthia Van Hee", "Els Lefever", "Veronique Hoste."], "venue": "Proceedings of the 26th International Conference on Computational", "citeRegEx": "Hee et al\\.,? 2016", "shortCiteRegEx": "Hee et al\\.", "year": 2016}, {"title": "That is your evidence?: Classifying stance in online political debate", "author": ["Marilyn A. Walker", "Pranav Anand", "Rob Abbott", "Jean E. Fox Tree", "Craig Martell", "Joseph King."], "venue": "Decision Support Systems 53(4):719\u2013729.", "citeRegEx": "Walker et al\\.,? 2012", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Annotating expressions of opinions and emotions in language", "author": ["Janyce Wiebe", "Theresa Wilson", "Claire Cardie."], "venue": "Language Resources and Evaluation 39(2):165\u2013210. https://doi.org/10.1007/s10579-005-7880-9.", "citeRegEx": "Wiebe et al\\.,? 2005", "shortCiteRegEx": "Wiebe et al\\.", "year": 2005}, {"title": "Quantifying political leaning from tweets, retweets, and retweeters", "author": ["Felix Ming Fai Wong", "Chee Wei Tan", "Soumya Sen", "Mung Chiang."], "venue": "IEEE Transactions on Knowledge and Data Engineering 28(8):2158\u20132172.", "citeRegEx": "Wong et al\\.,? 2016", "shortCiteRegEx": "Wong et al\\.", "year": 2016}, {"title": "Joint inference for fine-grained opinion extraction", "author": ["Bishan Yang", "Claire Cardie."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013). pages 1640\u20131649. http://aclweb.org/anthology/P13-", "citeRegEx": "Yang and Cardie.,? 2013", "shortCiteRegEx": "Yang and Cardie.", "year": 2013}, {"title": "Classifying the political leaning of news articles and users from user votes", "author": ["Daniel Xiaodan Zhou", "Paul Resnick", "Qiaozhu Mei."], "venue": "Fifth International AAAI Conference on Weblogs and Social Media (ICWSM 2011). pages 417\u2013424.", "citeRegEx": "Zhou et al\\.,? 2011", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 17, "context": "The latest survey by the Pew Research Center reported that a majority of US adults (62%) obtain news via social media, and of those, 18% do so often (Gottfried and Shearer, 2016).", "startOffset": 149, "endOffset": 178}, {"referenceID": 20, "context": "Given that news and opinions are shared and amplified by friend networks of individuals (Jamieson and Cappella, 2008), individuals are thereby isolated from information that does not fit well with their opinions (Pariser, 2011).", "startOffset": 88, "endOffset": 117}, {"referenceID": 30, "context": "Given that news and opinions are shared and amplified by friend networks of individuals (Jamieson and Cappella, 2008), individuals are thereby isolated from information that does not fit well with their opinions (Pariser, 2011).", "startOffset": 212, "endOffset": 227}, {"referenceID": 0, "context": "nities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016).", "startOffset": 81, "endOffset": 188}, {"referenceID": 43, "context": "nities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016).", "startOffset": 81, "endOffset": 188}, {"referenceID": 8, "context": "nities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016).", "startOffset": 81, "endOffset": 188}, {"referenceID": 2, "context": "nities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016).", "startOffset": 81, "endOffset": 188}, {"referenceID": 41, "context": "nities on social media along the political spectrum from liberal to conservative (Adamic and Glance, 2005; Zhou et al., 2011; Cohen and Ruths, 2013; Bakshy et al., 2015; Wong et al., 2016).", "startOffset": 81, "endOffset": 188}, {"referenceID": 23, "context": "conservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984).", "startOffset": 132, "endOffset": 173}, {"referenceID": 25, "context": "conservative axis, political analysts argue that the axis is flawed and insufficient for representing public opinion and ideologies (Kerlinger, 1984; Maddox and Lilie, 1984).", "startOffset": 132, "endOffset": 173}, {"referenceID": 36, "context": "A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.", "startOffset": 111, "endOffset": 284}, {"referenceID": 35, "context": "A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.", "startOffset": 111, "endOffset": 284}, {"referenceID": 28, "context": "A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.", "startOffset": 111, "endOffset": 284}, {"referenceID": 1, "context": "A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.", "startOffset": 111, "endOffset": 284}, {"referenceID": 39, "context": "A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.", "startOffset": 111, "endOffset": 284}, {"referenceID": 27, "context": "A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.", "startOffset": 111, "endOffset": 284}, {"referenceID": 22, "context": "A potential solution for analyzing multiple axes of the political spectrum on social media is stance detection (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.", "startOffset": 111, "endOffset": 284}, {"referenceID": 27, "context": "More recently, all participants of SemEval 2016 Task 6A (with five topics) could not outperform the baseline supervised method using n-gram features (Mohammad et al., 2016).", "startOffset": 149, "endOffset": 172}, {"referenceID": 1, "context": ", 2006; Somasundaran and Wiebe, 2009; Murakami and Raymond, 2010; Anand et al., 2011; Walker et al., 2012; Mohammad et al., 2016; Johnson and Goldwasser, 2016), whose task is to determine whether the author of a text is for, neutral, or against a topic (e.g., free trade, immigration, abortion). However, stance detection across different topics is extremely difficult. Anand et al. (2011) reported that a sophisticated method with topic-dependent features substantially improved the performance of stance detection within a topic, but such an approach could not outperform a baseline method with simple n-gram features when evaluated across topics.", "startOffset": 66, "endOffset": 390}, {"referenceID": 8, "context": "Cohen and Ruths (2013) observed that existing methods on stance detection fail on \u201cordinary\u201d users because such methods primarily obtain training and test", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "In order to design linguistic patterns, we focus on hashtags appearing in the corpus that have been popular clues for locating subjective statements such as sentiments (Davidov et al., 2010), emotions (Qadir and Riloff, 2014), and ironies (Van Hee et al.", "startOffset": 168, "endOffset": 190}, {"referenceID": 31, "context": ", 2010), emotions (Qadir and Riloff, 2014), and ironies (Van Hee et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 24, "context": "Therefore, in this section, we apply matrix factorization (Koren et al., 2009) in order to predict missing values, inspired by research regarding item recommendation (Bell and Koren, 2007; Dror et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 4, "context": ", 2009) in order to predict missing values, inspired by research regarding item recommendation (Bell and Koren, 2007; Dror et al., 2011).", "startOffset": 95, "endOffset": 136}, {"referenceID": 13, "context": ", 2009) in order to predict missing values, inspired by research regarding item recommendation (Bell and Koren, 2007; Dror et al., 2011).", "startOffset": 95, "endOffset": 136}, {"referenceID": 6, "context": "We use libmf6 (Chin et al., 2015) to solve the", "startOffset": 14, "endOffset": 33}, {"referenceID": 37, "context": ", 2016), political parties (Tumasjan et al., 2010; Boutet et al., 2013; Makazhanov and Rafiei, 2013), and elections (O\u2019Connor et al.", "startOffset": 27, "endOffset": 100}, {"referenceID": 5, "context": ", 2016), political parties (Tumasjan et al., 2010; Boutet et al., 2013; Makazhanov and Rafiei, 2013), and elections (O\u2019Connor et al.", "startOffset": 27, "endOffset": 100}, {"referenceID": 26, "context": ", 2016), political parties (Tumasjan et al., 2010; Boutet et al., 2013; Makazhanov and Rafiei, 2013), and elections (O\u2019Connor et al.", "startOffset": 27, "endOffset": 100}, {"referenceID": 29, "context": ", 2013; Makazhanov and Rafiei, 2013), and elections (O\u2019Connor et al., 2010; Conover et al., 2011).", "startOffset": 52, "endOffset": 97}, {"referenceID": 9, "context": ", 2013; Makazhanov and Rafiei, 2013), and elections (O\u2019Connor et al., 2010; Conover et al., 2011).", "startOffset": 52, "endOffset": 97}, {"referenceID": 34, "context": "Sim et al. (2013) measured ideological positions of candidates in US presidential elections from their speeches.", "startOffset": 0, "endOffset": 18}, {"referenceID": 14, "context": "tive models (Eisenstein et al., 2011).", "startOffset": 12, "endOffset": 37}, {"referenceID": 3, "context": "Similar to our present work, their work (Bamman and Smith, 2015) did not require labeled data", "startOffset": 40, "endOffset": 64}, {"referenceID": 16, "context": "\u201d A number of previous studies acquire instances of causal relation (Girju, 2003; Do et al., 2011) and promote/suppress relation (Hashimoto et al.", "startOffset": 68, "endOffset": 98}, {"referenceID": 12, "context": "\u201d A number of previous studies acquire instances of causal relation (Girju, 2003; Do et al., 2011) and promote/suppress relation (Hashimoto et al.", "startOffset": 68, "endOffset": 98}, {"referenceID": 18, "context": ", 2011) and promote/suppress relation (Hashimoto et al., 2012; Fluck et al., 2015) from text.", "startOffset": 38, "endOffset": 82}, {"referenceID": 15, "context": ", 2011) and promote/suppress relation (Hashimoto et al., 2012; Fluck et al., 2015) from text.", "startOffset": 38, "endOffset": 82}, {"referenceID": 33, "context": "The causality knowledge is useful for predicting (hypotheses of) future events (Radinsky et al., 2012; Radinsky and Davidovich, 2012; Hashimoto et al., 2015).", "startOffset": 79, "endOffset": 157}, {"referenceID": 32, "context": "The causality knowledge is useful for predicting (hypotheses of) future events (Radinsky et al., 2012; Radinsky and Davidovich, 2012; Hashimoto et al., 2015).", "startOffset": 79, "endOffset": 157}, {"referenceID": 19, "context": "The causality knowledge is useful for predicting (hypotheses of) future events (Radinsky et al., 2012; Radinsky and Davidovich, 2012; Hashimoto et al., 2015).", "startOffset": 79, "endOffset": 157}], "year": 2017, "abstractText": "We present in this paper our approach for modeling inter-topic preferences of Twitter users: for example, those who agree with the Trans-Pacific Partnership (TPP) also agree with free trade. This kind of knowledge is useful not only for stance detection across multiple topics but also for various real-world applications including public opinion surveys, electoral predictions, electoral campaigns, and online debates. In order to extract users\u2019 preferences on Twitter, we design linguistic patterns in which people agree and disagree about specific topics (e.g., \u201cA is completely wrong\u201d). By applying these linguistic patterns to a collection of tweets, we extract statements agreeing and disagreeing with various topics. Inspired by previous work on item recommendation, we formalize the task of modeling intertopic preferences as matrix factorization: representing users\u2019 preferences as a usertopic matrix and mapping both users and topics onto a latent feature space that abstracts the preferences. Our experimental results demonstrate both that our proposed approach is useful in predicting missing preferences of users and that the latent vector representations of topics successfully encode inter-topic preferences.", "creator": "LaTeX with hyperref package"}}}