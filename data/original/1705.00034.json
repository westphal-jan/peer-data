{"id": "1705.00034", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Deep Multi-view Models for Glitch Classification", "abstract": "Non-cosmic, non-Gaussian disturbances known as \"glitches\", show up in gravitational-wave data of the Advanced Laser Interferometer Gravitational-wave Observatory, or aLIGO. In this paper, we propose a deep multi-view convolutional neural network to classify glitches automatically. The primary purpose of classifying glitches is to understand their characteristics and origin, which facilitates their removal from the data or from the detector entirely. We visualize glitches as spectrograms and leverage the state-of-the-art image classification techniques in our model. The suggested classifier is a multi-view deep neural network that exploits four different views for classification. The experimental results demonstrate that the proposed model improves the overall accuracy of the classification compared to traditional single view algorithms.", "histories": [["v1", "Fri, 28 Apr 2017 18:45:57 GMT  (1707kb,D)", "http://arxiv.org/abs/1705.00034v1", "Accepted to the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'17)"]], "COMMENTS": "Accepted to the 42nd IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP'17)", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["sara bahaadini", "neda rohani", "scott coughlin", "michael zevin", "vicky kalogera", "aggelos k katsaggelos"], "accepted": false, "id": "1705.00034"}, "pdf": {"name": "1705.00034.pdf", "metadata": {"source": "CRF", "title": "DEEP MULTI-VIEW MODELS FOR GLITCH CLASSIFICATION", "authors": ["Sara Bahaadini", "Neda Rohani", "Scott Coughlin", "Michael Zevin", "Vicky Kalogera", "Aggelos K Katsaggelos"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014 Multi-view learning, deep learning, image classification, neural network\n1. INTRODUCTION\nIn many machine learning problems, samples are collected from more than one source. Also, various feature extraction methods can be used to provide more than one set of feature vectors per sample. Such extra sources or feature vectors are referred to as \u201cviews\". Using multiple views can improve performance as they may provide complementary or redundant information [1].\nFusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6]. Integrating multiple sources of data is a challenging task, and various approaches have been proposed in the literature [4][7]. More recently, deep learning techniques have shown promising performance for multimodal fusion [8, 9, 10]. Moreover, deep learning methods have shown superb performance for many classification problems including image classification. In this paper, we propose deep multi-view models for a particular classification problem from the aLIGO project [11].\nAdvanced LIGO (Advanced Laser Interferometer Gravitational -wave Observatory, or aLIGO) has recently made the first direct observations of gravitational waves [12] [13],\nwhich are ripples in the fabric of spacetime caused by accelerating masses. Since aLIGO is sensitive to minuscule changes in distance, its experimental data is affected by a variety of non-cosmic disturbances. When such disturbances, called \u201cglitches\u201d, show up in the gravitational-wave data, they generally worsen the quality of the detection of candidate cosmic signals. The elimination and prevention of these glitches will improve the quality of the detection system and increase the chance of detecting gravitational waves. Therefore, it is necessary to develop methods for identifying and characterizing glitches, which will help to determine their origin and eliminate their cause. Since glitches can be visualized as time-frequency-energy images (spectrograms), image classification techniques can be used to identify and characterize them.\nIn this paper, we present the development of a multiview deep neural network framework for glitch classification. Compared to standard methods that use just one set of images, we propose four-input models. We exploit four different time durations that are available for each glitch, namely each glitch plotted over time windows of 0.5, 1, 2, and 4 seconds. An example of such a glitch (from the \u201cHelix\u201d class) with four durations is shown in Fig. 1. We suggest two multi-view deep neural network models (a parallel view and a merged\nar X\niv :1\n70 5.\n00 03\n4v 1\n[ cs\n.L G\n] 2\n8 A\npr 2\n01 7\nview) and we compare their performances to deep single view models. Experimental analysis shows that single view models trained with shorter glitches have a better performance for the classes that have shorter duration, while single view models trained with longer-duration glitches work better for long-duration classes. Our experimental results show that the developed multi-view framework improves the glitch classification accuracy by capturing the required information from glitches of various morphological characteristics.\nThe rest of this paper is organized as follows. In the next section, we present our model. Experiments and results are discussed in Section 3. We conclude this paper in Section 4.\n2. THE PROPOSED MODEL\nThe main motivation for this study is to exploit multiple views for glitch classification instead of depending on just a single view. We investigate this by combining views\u2019 information at two points as we go through the deep network layers. We thus propose one model in which fusion take place at an early step (referred to as \u201cmerged view\u201d) and one in which information is integrated at the middle level (referred to as \u201cparallel view\u201d). In the following sections we explain these two architectures in detail."}, {"heading": "2.1. Parallel view model", "text": "The idea behind the parallel view model is to project each view into a feature space which is based more on the statistical properties of the samples than their view-specific properties. This projection makes the views interact with each other and be presented in a common feature space more efficiently. We illustrate the construction of the parallel view model using the four durations as views in Fig. 2. At the very first layer, each view passes through a convolutional layer, followed by maxpooling and ReLU activation. Then, we introduce a shared layer (merger layer) to map all views into a common feature space. Another set of convolutional, max-pooling, and activation layers is used after the merger layer to model the obtained common features from the previous layers. In the end, a fully connected layer and a softmax layer are employed (see Fig. 2)."}, {"heading": "2.2. Merged view model", "text": "In this model, we introduce the network layers on top of the merged views. We merge the views by forming a 2m \u00d7 2k\nmatrix by placing next to each other four m\u00d7 k images. After merging the views, a set of convolutional layer followed by max-pooling and activation layer is used, and then a fully connected layer and a softmax layer are exploited (see Fig. 3). This approach clearly models jointly the distribution of the different views in their original feature space. This seems a reasonable approach since the correlations among the views in our problem are not highly non-linear compared to other tasks where the views are very different, such as image and text [8], or audio and video [9]. Therefore it is possible for the model to learn such correlations even in the original space of data."}, {"heading": "2.3. Training", "text": "The models presented above optimize a loss function defined on the training data. For training the model, we can use either the mean squared error or the average cross-entropy error as the loss function. Due to the advantages of the average cross-entropy error over the mean squared error, e.g., the derivation of a \u201cbetter\u201d gradient for back propagation, for multi-class classification problems [14], in our model we use a cross-entropy based loss function defined as follows:\nE = \u2212 N\u2211\nn=1 C\u2211 i=1 yni log o n i (1)\nwhere oni is the model\u2019s output for class i when the n th training sample is given to the network, yni is one if the n th sample is from class i, otherwise it is zero, and N and C are the total numbers of the training samples and classes, respectively. There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function. We use the Adadelta [15] optimizer. It monotonically decreases the learning rate and shows good performance in our experiments.\n3. EXPERIMENTS AND RESULTS"}, {"heading": "3.1. Dataset", "text": "The dataset consists of glitch samples from 20 classes. The glitches are represented as a spectrogram, a time-frequency representation where the x-axis represents the duration of the glitch and the y-axis shows the frequency content of the glitch. The colors indicate the \u201cloudness\u201d of the glitch in the aLIGO detector. The classes arise from different environmental and instrumental mechanisms, and are based on the shape and intensity of the glitch in time-frequency space. The primary sample duration is 0.5 second. However, in this study we use three other durations, i.e., 1.0, 2.0 and 4 seconds, per glitch to train our multi-view models. An example of the dataset with four durations is shown in the Fig. 1. In total, there are 7730 glitches in our dataset. We use 75%, 12.5%,\nand, 12.5% of samples as training, validation, and test sets, respectively1."}, {"heading": "3.2. Experiment", "text": "Baseline A straightforward approach is to use just one glitch duration, as is done in a traditional single view approach. We use this as a baseline to compare the performance of our multi-view deep models. For single view models, we use CNNs with the structure shown in Table 1 (left column). The architecture of CNNs is optimized for the best classification accuracy. We use two convolutional layers with 128 kernels of size 5 \u00d7 5, max-pooling of 2 \u00d7 2, and the ReLU activation function. Batch size is set to 30, and the number of iterations is 130. In Table 2, the classification accuracies of single view CNNs are presented.\nDeep multi-view models In the parallel view model, first, we use four separate convolutional layers (see Fig. 2). Each has 128 kernels with size 5\u00d75, and 2\u00d7 2 max-pooling and ReLu activation. Then, we merge the output of these four convolutional layers into the merger layer and use another convolutional layer with the same structure, and finally a fully connected layer with 256 nodes and a softmax layer with 20 nodes (equal to the number of classes). All these details are shown in the middle column of Table 1. The parameters of this architecture were obtained based on extensive experimentation and guidance from literature.\nIn the merged view model, we use the structure shown in Fig. 3. Two convolutional layers with 128 kernel of size 5\u00d75, max-pooling of 2\u00d72, and ReLU activation function are used. One fully connected layer with 256 nodes and softmax with 20 outputs are added to the model. All details are shown in the right column of Table 1.\nFor both structures, the number of iterations is set to 130 and the batch size is 30. We use Keras [19] with Theano [20] back-end for all implementations. In Table 3, the best performances of parallel and merged view models are compared\n1This dataset will be publicly available soon.\nwith the best single view model performance. See Table 1 for full models specifications."}, {"heading": "3.3. Analysis", "text": "As the results in Table 3 show, the performance of multi-view deep models is better than single view models. An example of a misclassified sample by all single view models that was classified correctly by the multi-view models is shown in Fig. 4. Such examples show that in many cases, single view models do not have the needed sight and horizon for recognizing glitches correctly. Glitch classes are divided into short and long duration based on the glitch duration. In Table 4,\nwe show the category of each class plus the accuracy of two of the single view models; the 0.5-second model (Classifier 1) and 4-second model (Classifier 4). As can be seen in this table Classifier 1 performs at least as good as Classifier 4 for short duration glitches, while the opposite is true for long duration glitches, as expected for some classes (e.g., \u201cAir Compressor\u201d and \u201cTomte\u201d) the performance is perfect with both classifiers. Clearly the multi-view models which use all durations can capture the needed information to classify all types of glitches (according to their duration) more accurately.\n4. CONCLUSIONS\nIn this paper, we proposed multi-view deep neural network models for the glitch classification problem in aLIGO data. Two multi-view models, merged view and parallel view, are presented. The parallel view model projects samples into a common feature space where the views can interact efficiently. In the merged view model, the deep model is introduced on the concatenated durations. The experimental results show that multi-view models provide higher classification accuracy compared to the single view models, since they can accommodate efficiently the various classes independently of the glitch durations.\n5. ACKNOWLEDGMENT\nThis work was supported in part by an NSF INSPIRE grant (award number IIS-1547880). The authors would like to thank Joshua Smith from California State - Fullerton University for being the chief point of contact between this study and the LIGO detector characterization working group and for being a resource on LIGO glitches and current methods of data analysis.\n6. REFERENCES\n[1] C. Xu, D. Tao, and C. Xu, \u201cA survey on multi-view learning,\u201d arXiv preprint arXiv:1304.5634, 2013.\n[2] X. Huang, J. Kortelainen, G. Zhao, X. Li, A. Moilanen, T. Sepp\u00e4nen, and M. Pietik\u00e4inen, \u201cMulti-modal emotion analysis from facial expressions and electroencephalogram,\u201d Computer Vision and Image Understanding, vol. 147, pp. 114\u2013124, 2016.\n[3] L. Zheng, V. Noroozi, and Ph. S. Yu, \u201cJoint deep modeling of users and items using reviews for recommendation,\u201d in ACM International Conference on Web Search and Data Mining (WSDM), 2017.\n[4] A. K. Katsaggelos, S. Bahaadini, and R. Molina, \u201cAudiovisual fusion: Challenges and new approaches,\u201d Proceedings of the IEEE, vol. 103, no. 9, pp. 1635\u20131653, Sept 2015.\n[5] S. Bahaadini, A. Asaei, D. Imseng, and H. Bourlard, \u201cPosterior-based sparse representation for automatic speech recognition,\u201d Proceeding of Interspeech, 2014.\n[6] P. S. Aleksic and A. K. Katsaggelos, \u201cAn audio-visual person identification and verification system using faps as visual features,\u201d in ACM Workshop on Multimodal User Authentication. Citeseer, 2003, pp. 80\u201384.\n[7] N. Rohani, P. Ruiz, E. Besler, R. Molina, and A. K. Katsaggelos, \u201cVariational gaussian process for sensor fusion,\u201d in Signal Processing Conference (EUSIPCO), 2015 23rd European, Aug 2015, pp. 170\u2013174.\n[8] N. Srivastava and R. R. Salakhutdinov, \u201cMultimodal learning with deep boltzmann machines,\u201d in Advances in neural information processing systems, 2012, pp. 2222\u20132230.\n[9] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng, \u201cMultimodal deep learning,\u201d in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.\n[10] H. Suk, S. Lee, and D. Shen, \u201cHierarchical feature representation and multimodal fusion with deep learning for ad/mci diagnosis,\u201d NeuroImage, vol. 101, pp. 569\u2013 582, 2014.\n[11] M. Zevin, S. Coughlin, S. Bahaadini, E. Besler, N. Rohani, S. Allen, M. Cabero, K. Crowston, A. Katsaggelos, Sh. Larson, et al., \u201cGravity spy: Integrating advanced ligo detector characterization, machine learning, and citizen science,\u201d arXiv preprint arXiv:1611.04596, 2016.\n[12] B. P. Abbott, R. Abbott, T. D. Abbott, M. R. Abernathy, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso, R. X. Adhikari, and et al., \u201cObservation of Gravitational Waves from a Binary Black Hole Merger,\u201d Physical Review Letters, vol. 116, no. 6, pp. 061102, Feb. 2016.\n[13] B. P. Abbott, R. Abbott, T. D. Abbott, M. R. Abernathy, F. Acernese, K. Ackley, C. Adams, T. Adams, P. Addesso, R. X. Adhikari, and et al., \u201cGW151226: Observation of Gravitational Waves from a 22-Solar-Mass Binary Black Hole Coalescence,\u201d Physical Review Letters, vol. 116, no. 24, pp. 241103, June 2016.\n[14] P. Golik, P. Doetsch, and H. Ney, \u201cCross-entropy vs. squared error training: a theoretical and experimental comparison.,\u201d in INTERSPEECH, 2013, pp. 1756\u2013 1760.\n[15] M. D. Zeiler, \u201cAdadelta: an adaptive learning rate method,\u201d arXiv preprint arXiv:1212.5701, 2012.\n[16] Diederik P. Kingma and Jimmy Ba, \u201cAdam: A method for stochastic optimization,\u201d CoRR, vol. abs/1412.6980, 2014.\n[17] V. Noroozi, A. Hashemi, and M.R Meybodi, \u201cAlpinist cellularde: a cellular based optimization algorithm for dynamic environments,\u201d in Proceedings of the 14th annual conference companion on Genetic and evolutionary computation. ACM, 2012, pp. 1519\u20131520.\n[18] A. Sharifi, V. Noroozi, M. Bashiri, A. Hashemi, and M.R. Meybodi, \u201cTwo phased cellular pso: A new collaborative cellular algorithm for optimization in dynamic environments,\u201d in 2012 IEEE Congress on Evolutionary Computation. IEEE, 2012, pp. 1\u20138.\n[19] F. Chollet, \u201cKeras,\u201d https://github.com/fchollet/keras, 2015.\n[20] J. Bergstra, O. Breuleux, Fr. Bastien, P. Lamblin, . Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio, \u201cTheano: a CPU and GPU math expression compiler,\u201d in Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010, Oral Presentation."}], "references": [{"title": "A survey on multi-view learning", "author": ["C. Xu", "D. Tao", "C. Xu"], "venue": "arXiv preprint arXiv:1304.5634, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-modal emotion analysis from facial expressions and electroencephalogram", "author": ["X. Huang", "J. Kortelainen", "G. Zhao", "X. Li", "A. Moilanen", "T. Sepp\u00e4nen", "M. Pietik\u00e4inen"], "venue": "Computer Vision and Image Understanding, vol. 147, pp. 114\u2013124, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Joint deep modeling of users and items using reviews for recommendation", "author": ["L. Zheng", "V. Noroozi", "Ph. S. Yu"], "venue": "ACM International Conference on Web Search and Data Mining (WSDM), 2017.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2017}, {"title": "Audiovisual fusion: Challenges and new approaches", "author": ["A.K. Katsaggelos", "S. Bahaadini", "R. Molina"], "venue": "Proceedings of the IEEE, vol. 103, no. 9, pp. 1635\u20131653, Sept 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Posterior-based sparse representation for automatic speech recognition", "author": ["S. Bahaadini", "A. Asaei", "D. Imseng", "H. Bourlard"], "venue": "Proceeding of Interspeech, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "An audio-visual person identification and verification system using faps as visual features", "author": ["P.S. Aleksic", "A.K. Katsaggelos"], "venue": "ACM Workshop on Multimodal User Authentication. Citeseer, 2003, pp. 80\u201384.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Variational gaussian process for sensor fusion", "author": ["N. Rohani", "P. Ruiz", "E. Besler", "R. Molina", "A.K. Katsaggelos"], "venue": "Signal Processing Conference (EUSIPCO), 2015 23rd European, Aug 2015, pp. 170\u2013174.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R.R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2012, pp. 2222\u20132230.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 689\u2013696.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical feature representation and multimodal fusion with deep learning for ad/mci diagnosis", "author": ["H. Suk", "S. Lee", "D. Shen"], "venue": "NeuroImage, vol. 101, pp. 569\u2013 582, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Gravity spy: Integrating advanced ligo detector characterization, machine learning, and citizen science", "author": ["M. Zevin", "S. Coughlin", "S. Bahaadini", "E. Besler", "N. Rohani", "S. Allen", "M. Cabero", "K. Crowston", "A. Katsaggelos", "Sh. Larson"], "venue": "arXiv preprint arXiv:1611.04596, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Observation of Gravitational Waves from a Binary Black Hole Merger", "author": ["B.P. Abbott", "R. Abbott", "T.D. Abbott", "M.R. Abernathy", "F. Acernese", "K. Ackley", "C. Adams", "T. Adams", "P. Addesso", "R.X. Adhikari"], "venue": "Physical Review Letters, vol. 116, no. 6, pp. 061102, Feb. 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "GW151226: Observation of Gravitational Waves from a 22-Solar-Mass Binary Black Hole Coalescence", "author": ["B.P. Abbott", "R. Abbott", "T.D. Abbott", "M.R. Abernathy", "F. Acernese", "K. Ackley", "C. Adams", "T. Adams", "P. Addesso", "R.X. Adhikari"], "venue": "Physical Review Letters, vol. 116, no. 24, pp. 241103, June 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-entropy vs. squared error training: a theoretical and experimental comparison", "author": ["P. Golik", "P. Doetsch", "H. Ney"], "venue": "INTERSPEECH, 2013, pp. 1756\u2013 1760.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Alpinist cellularde: a cellular based optimization algorithm for dynamic environments", "author": ["V. Noroozi", "A. Hashemi", "M.R Meybodi"], "venue": "Proceedings of the 14th annual conference companion on Genetic and evolutionary computation. ACM, 2012, pp. 1519\u20131520.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Two phased cellular pso: A new collaborative cellular algorithm for optimization in dynamic environments", "author": ["A. Sharifi", "V. Noroozi", "M. Bashiri", "A. Hashemi", "M.R. Meybodi"], "venue": "2012 IEEE Congress on Evolutionary Computation. IEEE, 2012, pp. 1\u20138.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "Fr. Bastien", "P. Lamblin", "Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010, Oral Presentation.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Using multiple views can improve performance as they may provide complementary or redundant information [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 133, "endOffset": 136}, {"referenceID": 3, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 157, "endOffset": 163}, {"referenceID": 4, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 157, "endOffset": 163}, {"referenceID": 5, "context": "Fusion of multiple sources of information has been used in many applications such as emotion recognition [2], recommendation systems [3], speech recognition [4, 5], and biometric verification [6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "Integrating multiple sources of data is a challenging task, and various approaches have been proposed in the literature [4][7].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "Integrating multiple sources of data is a challenging task, and various approaches have been proposed in the literature [4][7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "More recently, deep learning techniques have shown promising performance for multimodal fusion [8, 9, 10].", "startOffset": 95, "endOffset": 105}, {"referenceID": 8, "context": "More recently, deep learning techniques have shown promising performance for multimodal fusion [8, 9, 10].", "startOffset": 95, "endOffset": 105}, {"referenceID": 9, "context": "More recently, deep learning techniques have shown promising performance for multimodal fusion [8, 9, 10].", "startOffset": 95, "endOffset": 105}, {"referenceID": 10, "context": "In this paper, we propose deep multi-view models for a particular classification problem from the aLIGO project [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "Advanced LIGO (Advanced Laser Interferometer Gravitational -wave Observatory, or aLIGO) has recently made the first direct observations of gravitational waves [12] [13], Fig.", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "Advanced LIGO (Advanced Laser Interferometer Gravitational -wave Observatory, or aLIGO) has recently made the first direct observations of gravitational waves [12] [13], Fig.", "startOffset": 164, "endOffset": 168}, {"referenceID": 7, "context": "This seems a reasonable approach since the correlations among the views in our problem are not highly non-linear compared to other tasks where the views are very different, such as image and text [8], or audio and video [9].", "startOffset": 196, "endOffset": 199}, {"referenceID": 8, "context": "This seems a reasonable approach since the correlations among the views in our problem are not highly non-linear compared to other tasks where the views are very different, such as image and text [8], or audio and video [9].", "startOffset": 220, "endOffset": 223}, {"referenceID": 13, "context": ", the derivation of a \u201cbetter\u201d gradient for back propagation, for multi-class classification problems [14], in our model we use a cross-entropy based loss function defined as follows:", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function.", "startOffset": 41, "endOffset": 57}, {"referenceID": 15, "context": "There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function.", "startOffset": 41, "endOffset": 57}, {"referenceID": 16, "context": "There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function.", "startOffset": 41, "endOffset": 57}, {"referenceID": 17, "context": "There exits many optimization techniques [15, 16, 17, 18] that we can use to optimize the objective function.", "startOffset": 41, "endOffset": 57}, {"referenceID": 14, "context": "We use the Adadelta [15] optimizer.", "startOffset": 20, "endOffset": 24}, {"referenceID": 18, "context": "We use Keras [19] with Theano [20] back-end for all implementations.", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "We use Keras [19] with Theano [20] back-end for all implementations.", "startOffset": 30, "endOffset": 34}], "year": 2017, "abstractText": "Non-cosmic, non-Gaussian disturbances known as \u201cglitches\u201d, show up in gravitational-wave data of the Advanced Laser Interferometer Gravitational-wave Observatory, or aLIGO. In this paper, we propose a deep multi-view convolutional neural network to classify glitches automatically. The primary purpose of classifying glitches is to understand their characteristics and origin, which facilitates their removal from the data or from the detector entirely. We visualize glitches as spectrograms and leverage the state-of-the-art image classification techniques in our model. The suggested classifier is a multi-view deep neural network that exploits four different views for classification. The experimental results demonstrate that the proposed model improves the overall accuracy of the classification compared to traditional single view algorithms.", "creator": "LaTeX with hyperref package"}}}