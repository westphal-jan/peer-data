{"id": "1705.00321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2017", "title": "Generative Neural Machine for Tree Structures", "abstract": "Tree structures are commonly used in the tasks of semantic analysis and understanding over the data of different modalities, such as natural language, 2D or 3D graphics and images, or Web pages. Previous studies model the tree structures in a bottom-up manner, where the leaf nodes (given in advance) are merged into internal nodes until they reach the root node. However, these models are not applicable when the leaf nodes are not explicitly specified ahead of prediction. Here, we introduce a neural machine for top-down generation of tree structures that aims to infer such tree structures without the specified leaf nodes. In this model, the history memories from ancestors are fed to a node to generate its (ordered) children in a recursive manner. This model can be utilized as a tree-structured decoder in the framework of \"X to tree\" learning, where X stands for any structure (e.g. chain, tree etc.) that can be represented as a latent vector. By transforming the dialogue generation problem into a sequence-to-tree task, we demonstrate the proposed X2Tree framework achieves a 11.15% increase of response acceptance ratio over the baseline methods.", "histories": [["v1", "Sun, 30 Apr 2017 15:09:10 GMT  (2873kb,D)", "https://arxiv.org/abs/1705.00321v1", null], ["v2", "Sat, 6 May 2017 14:38:10 GMT  (3033kb,D)", "http://arxiv.org/abs/1705.00321v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["ganbin zhou", "ping luo", "rongyu cao", "yijun xiao", "fen lin", "bo chen", "qing he"], "accepted": false, "id": "1705.00321"}, "pdf": {"name": "1705.00321.pdf", "metadata": {"source": "CRF", "title": "Generative Neural Machine for Tree Structures", "authors": ["Ganbin Zhou", "Ping Luo", "Rongyu Cao", "Yijun Xiao", "Fen Lin", "Bo Chen", "Qing He"], "emails": ["caory}@ics.ict.ac.cn,", "heqing}@ict.ac.cn"], "sections": [{"heading": null, "text": "Tree structures are commonly used in the tasks of semantic analysis and understanding over the data of different modalities, such as natural language, 2d or 3d graphics and images, or Web pages. Previous studies model the tree structures in a bottomup manner, where the leaf nodes (given in advance) are merged into internal nodes until they reach the root node. However, these models are not applicable when the leaf nodes are not explicitly specified ahead of prediction. Here, we introduce a neural machine for top-down generation of tree structures that aims to infer such tree structures without the specified leaf nodes. In this model, the history memories from ancestors are fed to a node to generate its (ordered) children in a recursive manner. This model can be utilized as a tree-structured decoder in the framework of \u201cX to tree\u201d learning, where X stands for any structure (e.g. chain, tree etc.) that can be represented as a latent vector. By transforming the dialogue generation problem into a sequence-to-tree task, we demonstrate the proposed X2Tree framework achieves a 11.15% increase of response acceptance ratio over the baseline methods."}, {"heading": "1 Introduction", "text": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23]. In these tasks, while the input and output data are from different modalities ( e.g. spoken and written languages, audio signals, musical notes), they usually are represented as a structure consisting of multiple random variables. Then, neu-\nral models are adopted to model the internal dependency among variables inside the structures, and also the external dependency between the input and output structures. Since sequence is a basic structure for data representation, most of these studies focus on the Seq2Seq models [19, 13], which achieve the state-of-the-art performance for many tasks.\nIn this paper we focus on modeling of tree structure which is a general case to its simpler counterpart sequence. In computer science, tree is a widely used abstract data type implementing a hierarchical structure. For example, in semantic analysis of various digital data, sentences are usually parsed into trees [16], 2d and 3d photograph can be represented as a Quadtree [17] or a Octree [10] respectively, and Web pages are naturally coded as HTML tree. Hence, the generative model for tree structures might support many intelligent applications with tree-structured output, such as neural-based parser and text-to-image task.\nIn contrast to the great efforts put into modeling sequences, only a few studies focused on the neuralbased modeling of tree structures. Some works generate trees in a top-down fashion. For example, Zhang et al.[25] proposed treelstm and ldtreelstm via Tree LSTM activation functions in top-down fashion. Here, we find two important points, which can differentiate our work with theirs clearly. Firstly, Zhang et al. mainly handle the dependency tree (a special instance of the sequence-preserved tree, defined in section 3.2), while we aim at handling different types of trees, including: a) Tree with Non-fixed child number vs. tree with fixed child number; b) Ordered tree vs. sequence-preserved tree. The key of our work is the proposed tree canonicalization method, which can transform these various trees into ordered tree with fixed child number. Thus, we handle various\nar X\niv :1\n70 5.\n00 32\n1v 2\n[ cs\n.A I]\n6 M\nay 2\n01 7\ntypes of trees in a unified framework. Secondly, due to the canonicalization method, our model does not need to decide the number of children while tree generating, because we only need to generate trees with fixed number of children. However, Zhang et al. develop a binary classifier to decide whether the model should continue to generate children or not.\nSome models also generate trees in a bottom-up fashion. Socher et al. [15] proposed a max-margin structure prediction architecture based on recursive neural networks, and demonstrated that it successfully parses sentences and understands scene images. Tai et al. [20] and Zhu et al. [26] extended the chainstructured LSTM to tree-structured LSTM, which is shown to be more effective in representing a tree structure as a latent vector. All these models generate trees in a bottom-up fashion, where children nodes are recursively merged into parent nodes until the root is generated.\nHowever, bottom-up generative models require all the leaf nodes in the predicted tree given in advance. For example, to generate the constituency parse tree for a sentence (shown in Fig. 1(a)), tokens appeared in the given sentence are used as leaf nodes in this tree. Similarly, to parse natural scene images [15], an image is first divided into segments, each of which corresponds to one leaf node in output tree. With these given leaves bottom-up process recursively generates the internal nodes until the root is built.\nHere, we argue that the bottom-up generative models may not work well when the leaf nodes are\nnot specified ahead of prediction. Consider the task in Fig. 1(b), which is an intermediate task for automatic conversation generation. Instead of generating the response to a given input post directly, we aim to generate the dependency parse tree of the corresponding response. Then, a postprocessing step converts the dependency tree into a sequence as the final response1. Compared to the Seq2Seq solution to conversation generation, we argue that this treestructured modeling method is more effective due to a shorter average decoding length and the extra structure information provided from the parse tree. In this task, it is clearly seen that: since all the tokens in the response are not explicitly given by the input post, it may not be appropriate to generate the dependency from bottom to top.\nAnother motivation application of this study is multi-label hierarchical classification, where the classes to be predicted are organized into a class hierarchy, and each input instance may correspond to multiple labels in this class hierarchy. Obviously, this task can also be formulated as X-to-Tree learning problem (X stands for the data structure of input). Again, since the nodes in the output label tree are not specified by the input before prediction, the bottom-up generation of tree structures may not be applicable to this task.\nTo address this issue, we design a top-down generative process handling different types of trees, including: a) Tree with Non-fixed child number vs. tree with fixed child number; b) Ordered tree vs. sequence-preserved tree(defined in section 3.2), as an example shown in Fig. 1(b). Based on the latent vector learned from the input post, the proposed model first generates the root node, denoted by the token \u201csays\u201d. Then, based on this root the model generates its two immediate children, namely the two tokens \u201cmama\u201d and \u201cstupid\u201d respectively. This process continues on each new nodes until we cannot generate any valid child nodes in the tree. We argue that this top-down generative process is consistent with how human constructs sentences. Although people speak a sentence in sequential order, they may keep some keywords, such as verbs and nouns, in mind before filling in more descriptive adjectives and adverbs for a full sentence. These keywords may correspond to the nodes close to the root of the dependency tree, while the descriptive words may correspond to the ones near leaf nodes. Thus, top-down generation may be a more natural solution to the tasks, where nodes\n1The motivation of this solution is detailed in Section 3.\nin the output tree are not explicitly given by the input.\nIn this paper, this top-down model is developed as a tree-structured decoder in the framework of \u201cX to tree\u201d (X2Tree) learning, where X represents any structure (e.g. chain, tree) that can be encoded as a latent vector. To this end, we need to address the following challenges:\n1) We need to carefully model the different dependencies between a tree node and its children. Children at different positions may have different meanings, and the generation of a child node depends on not only its parent and ancestors but also its siblings. Thus, we need to fully consider the memory inherited from both its ancestors and siblings (detailed in Section 2.1).\n2) In model inference, since the number of probable tree structures is too large to enumerate, it is required to develop a fast algorithm searching for the most probable trees. Since the beam search utilized by previous studies only handles chain structures, a more general search algorithm for tree structures needs to be developed (detailed in Section 2.2).\n3) A tree node could obtain any number of children. It is non-trivial to automatically determine the number of children. Furthermore, GPU-based parallel computing is difficult when the children number is different for each node. We therefore need a tree canonicalization process, which outputs a equivalent standard tree, where each internal node has a fixed number of child nodes (detailed in Sections 2.3 and 3.3).\nWith all these challenges addressed, our main contributions are twofold: 1) We propose a top-down generative neural machine for tree structures, and apply it to X2Tree learning. Specifically, we introduce a tree canonicalization method to standardize the generative process and a greedy search method for tree structure inference. 2) We empirically demonstrate that the proposed method successfully predict the dependency trees of conversational responses to an input post. Specifically, for the task of automatic conversation the proposed X2Tree framework achieves a 11.15% increase of acceptance ratio over the compared Seq2Seq baselines. Additionally, we also apply X2Tree to hierarchical classification, results and analysis is shown in supplementary slides."}, {"heading": "2 X2Tree Neural Network", "text": "In this section, we introduce the X2Tree learning framework. The training dataset is given as:\nD = {(x, Tx)|Tx is the corresponding tree of x}\nOur task is to learn the mapping from x to a tree structure Tx. Specifically, it adopts the encoderdecoder framework. We assume x has already been encoded as a latent vector ( see e.g. [19, 26] ) , and mostly focus on the tree-structured decoder for the generation of Tx.\nAs aforementioned, the developed decoder adopts the top-down generative process as shown in Fig. 2. The atom step is generating the children for a given node. This atom step is performed on each node until it cannot generate any valid nodes. Thus, the key to the decoder is modeling the parent-children dependency, shown in the red triangles in Fig. 2. Note also that the model parameters for parent-children dependency are shared for all the atom steps in tree generation.\nWe first assume the tree isK-ary full tree where every internal node has exactly K children, and model this type of tree in Section 2.1. Then, we introduce an algorithm for tree inference in Section 2.2. Finally, we propose a canonicalization method that transforms any tree into a K-ary full tree and discuss the of K for different applications in Section 2.3 and 3.3.\nIt is worth mentioning here we consider the tree where each node corresponds to a discrete random variables. The modeling of continuous random variables and its applications will be briefly discussed in Section 6.\n2.1 Generative Model for K-ary Full Tree\nHere, we propose a generative model for K-ary full tree. For simplicity, x also represents the latent vector encoded from the input. Within the probabilistic learning framework, our main task is to express the conditional probability p(T |x) for a pair (x, T ) \u2208 D. In this study, we consider the discrete random variables for tree nodes while the continuous random variables will be considered in our future work.\nWe can first reformulate p(T |x) as:\np(T |x) = p(tr|x) \u00b7 p(T\u00acr | tr,x) (1)\nwhere tr and T \u00acr denotes the root and the set of non-root nodes respectively. The first term p(tr|x) in Equ. (1) is modeled as follows:\np(tr|x) = exp gr(tr,x)\u2211 v\u2208V exp gr(v,x)\n(2)\nwhere gr is a nonlinear and potentially multi-layered function, and V is the vocabulary containing all possible values for the discrete random variables.\nTo model p(T\u00acr | tr,x), we make the following conditional independence assumption:\nAssumption 1. The children of different nodes are conditionally independent given their ancestors.\nWith assumption 1, p(T\u00acr|tr,x) is decomposed as:\np(T\u00acr|tr,x) = \u220f t\u2208T p ( C(t) | x, t, A(t) ) (3)\nwhere C(t) = (c1, c2, \u00b7 \u00b7 \u00b7 , cK) denotes the set of t\u2019s children, and A(t) denotes all t\u2019s ancestors.\nWe then move to model the conditional probability p ( C(t) | x, t, A(t) ) . Concretely, since the child nodes to a parent usually correlate with each other, it is inappropriate to assume conditional independence among them. Thus, the probability p ( C(t)|x, t, A(t)\n) is then decomposed into the following ordered conditional probabilities:\np ( C(t)|x, t, A(t) ) = \u220f ck\u2208C(t) p ( ck|x, t, A(t), c<k ) (4) Furthermore, we argue that children at different positions obtain different underlying meanings. Hence, K different types of hidden states are designed for the K children of node t:\nhk = fk(t,h,x) , k = 1, 2, . . . ,K (5)\nwhere {fk}Kk=1 are activation functions which can be LSTM or other RNN cells. h denotes the hidden state fed to node t, containing the memory from t\u2019s ancestors , and hr = 0 for the root node. With hk, we define p ( ck | x, t, A(t), c<k ) as follows: p ( ck | x, t, A(t), c<k ) = exp gk ( ck,x, t,hk, c\u0303k\u22121 )\u2211 v\u2208V exp gk ( v,x, t,hk, c\u0303k\u22121\n) c\u0303k\u22121 = [c1; c2; \u00b7 \u00b7 \u00b7 ; ck\u22121]\n(6) where c\u0303k\u22121 is the concatenation of {ci}k\u22121i=1 .\nModeling of parent-children dependency is summarized in Fig.3. With all these modelings, we train the X2Tree model by maximizing the data likelihood, namely\u220f\n(x,T )\u2208D\np(T |x)\n= \u220f\n(x,T )\u2208D p(tr|x) \u220f t\u2208T \u220f ck\u2208C(t) p ( ck | x, t, A(t), c<k ) (7)\nIt is worth mentioning that in order to explicitly notify the end of tree generation we need to add the special token \u201ceob\u201d (short for \u201cEnd Of Branch\u201d) to the leaf nodes as their children. Hence, all the leaf nodes of the tree in the training dataset are eob nodes."}, {"heading": "2.2 Tree Generation", "text": "With the trained model we can infer the most probable trees for a given input x. In this section we develop a greedy search algorithm for this inference task.\nPrevious studies focus on the inference task of sequence structures where beam search is often adopted. At each step, it keeps G (called global beam size) best candidates with the maximal probabilities so far. Then, only those candidates are expanded\nnext. For each candidate on the beam it grows a new node at the current end of the sequence. This process repeats recursively until all candidates end with eob nodes.\nSince sequence is a special case of trees, the greedy search for tree generation has more challenges to address. First, an arbitrary tree has multiple leaves which could potentially generate new children. Second, when growing new children for a leaf node we need to generate all children as a whole since they correlate with each other (as mentioned in Section 2.1). Multiple groups of such K children need to be generated as the best candidates.\nWe use the example in Fig. 4 to describe this tree generation method. The original tree has two leaves, nodes \u201ci\u201d and \u201ca\u201d. For each of these leaves, we can generate new children. Specifically, for node \u201ci\u201d it generates L groups of K children, as shown in Fig. 4(b) (L = 3 and K = 2 in this example). Since these new children are ordered, this local step of children generation is actually a task of sequence generation, thus the conventional beam search can be used. Here, L (called local beam size) is to specify the number of candidate sequences generated for each leaf. After the child generation for all the leaves, we compare all these candidate trees and only retain top-G (G = 2 in this example) trees for the next round of generation. This process recursively continues until all the leaves in the tree are eob nodes. Note that the proposed method is a generalized beam search. Beam search for sequence generation is a special case with K = 1, since sequence is equivalent to 1-ary tree. The method is detailed in Algorithm 1. A visualization of this search process is provided in the slides in the supplementary files."}, {"heading": "2.3 Tree Canonicalization", "text": "As aforementioned, the proposed X2Tree model requires that the tree is K-ary full tree. This section discusses how to transform any tree to K-ary full, and the selection of K.\nBasically, the transformed K-ary full tree should be equivalent to the original one. In other words, there must exist an algorithm to support the bidirectional transformation between a tree and its Kary full counterpart. Considering the number of K is linear to the number of model parameters, to reduce model complexity, we usually hope K to be as small as possible. For a given tree, a simple method to transform it into a full tree is to fill all the empty positions with eob nodes. With this method, ev-\nAlgorithm 1 GeneralizedBeamSearch(x,G,L,K)\nInput: latent vector, x global beam size, G local beam size, L child number of each node, K Output: A set of trees, R 1: S \u2190 {G roots with highest p(tr|x)}; R \u2190 \u03c6; 2: while |R| < G do 3: for each T \u2208 S do 4: for each leaf t \u2208 T do 5: if t = eob then 6: continue; 7: end if 8: Using chain beam search, find L groups of C(t)\nvia maximizing p ( C(t)|x, t, A(t) ) ;\n9: for each C(t) do 10: Connect C(t) to T as a new tree T \u2032; 11: Add T \u2032 to S; 12: end for 13: end for 14: Delete T from S; 15: end for 16: S \u2190 {G trees with highest p(T |x) in S}; 17: for each T \u2208 S do 18: if T \u2019s leaves are all eobs then 19: Add T to R; 20: end if 21: end for 22: end while 23: R \u2190 {G trees with highest p(T |x) in R}; 24: return R;\nery tree node obtains K children where K is the maximal number of immediate children over all tree nodes. However, when K is large and the tree nodes are sparse, the redundant eob nodes significantly increase the learning complexity. Hence, ideally, before the eob filling step we want to transform the tree into a binary or ternary tree.\nHere, we mainly consider two scenarios. For an ordered tree, where ordering is specified for the children of each node, we transform it to a left-child right-sibling (LCRS) binary tree [5]. This transformation is reversible with a one-to-one mapping between the ordered tree and its LCRS counterpart. Furthermore, in some applications the resultant tree is required to support postprocessing. For example, in the application we conduct in this study the output dependency tree will be further converted into a sequence. Thus, the output tree should be sequencepreserved. In Section 3 we will discuss that we need to transform trees into ternary ones in order to make them sequence-preserved."}, {"heading": "3 Experimental Validation: Ap-", "text": "plication to Dialog Generation"}, {"heading": "3.1 Dialog Generation as a Seq2Tree Task", "text": "To show the effectiveness of the X2Tree model, we apply it to dialog generation. This task is conventionally modeled as a Seq2Seq learning problem, which maps an input post x to its response y where x and y are both sequences of words.\nRecently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12]. These neural models in essence use a chainstructured decoder to sequentially generate tokens given a context vector encoded from an input sequence. It basically considers the dependency between any word and all its preceding ones. LSTM or GRU units are often adopted to memorize all the preceding word information. However, it is difficult to perfectly preserve the information across long distances, which makes some sub-sentence of generated responses irrelevant or ungrammatical. For example, for the post \u201cCould I shop online?\u201d, the Seq2Seq model might respond \u201cDon\u2019t do that, I promise to work harder\u201d. Here, the later half of sentence \u201cI promise to work harder\u201d is irrelevant to the input post.\nTo address this issue of long-distance gap, we transform the learning into a Seq2Tree problem. Specifically, we perform a preprocessing step before training which converts each response into its corresponding dependency tree. For example, let (x,y) be a pair of input post and response. With a dependency parser we transform y into its dependency tree Ty, and obtain (x, Ty) for Seq2Tree learning. In Ty, the average depth of nodes is O( \u221a |y|) [6], which is much smaller than the sentence length |y|. Since the generation of a tree node only depends on\nits ancestor nodes (referring to Section 2.1), the tree transformation may alleviate the long-distance gap.\nThen, by applying dependency parsing to all the response sentences in the original training data we have the new training data\nD\u2032 = {(x, Ty)|Ty is the dependency tree of y}\nwhere y is the response to x. Next, the proposed X2Tree model can be applied to this Seq2Tree learning problem2."}, {"heading": "3.2 From Generated Tree to Response Sentence", "text": "Note that the X2Tree model outputs a tree structure. For the dialog generation task, we need to flatten the predicted tree into a sequence. Therefore we need to store position information in the dependency tree. For this purpose we first give the following definition of sequence-preserved tree (SP tree for short).\nDefinition 1. An SP tree is an ordered tree where each node t is tagged an integer I(t) \u2208 {0, 1, \u00b7 \u00b7 \u00b7 ,K}, and K is the number of children to node t.\nThe in-order traversal of an SP tree corresponds to a node sequence. Node t\u2019s children are divided into two parts. The left part contains the first I(t) nodes (child nodes are ordered from left to right), while the right part contains the remaining nodes. In the in-order traversal we first visit the nodes in the left part, then the current node, finally the right part. Fig. 5 shows three SP trees with their corresponding sequences.\n2We can also apply the dependency parsing to the input post x, and perform a Tree2Tree learning task. It is also easily developed by switching the chain-structured encoder with a tree-structured encoder [26].\nObviously, the dependency tree of a sentence is an SP tree, where the number attached on each node can be obtained by checking the position relationship of the node and its children in the original sentence, as shown in Figure 1(b). For example, the node \u201csays\u201d obtains a number \u201c1\u201d which means one child of this node are on its left part in the original sequence."}, {"heading": "3.3 Canonicalization of SP Trees", "text": "As discussed earlier, a tree canonicalization step is needed to transform the original dependency tree into a K-ary full tree. To preserve sequence order, we transform the dependency into a ternary tree. We now present the algorithm and discuss why ternary tree is the \u201cbest\u201d choice. Alg. 2 details this canonicalization process, and an illustration is shown in Fig.6.\nIn a ternary tree, each node has three children, namely left, middle and right nodes. For node t with attached number I(t), Alg. 2 first determines its left and middle child in the ternary tree. Specifically, its left child is set to c1, the first child in the original tree; and its middle child is set to cI(t)+1. Any other child cj ( j 6= 1 and j 6= I(t) + 1 ) is set as the right child of cj\u22121 recursively. With this ternary tree a simple in-order traversal in the order of left child, parent, middle child and right child can restore it into a sequence.\nNext, we prove that the resulting ternary tree is equivalent to the original SP tree in the sense that they can be transformed into each other.\nAlgorithm 2 Canonicalize(t)\nInput: A node of SP tree, t Output: A ternary tree node corresponding to t, t\u2032\n1: Let c1, c2, \u00b7 \u00b7 \u00b7 , cn denote t\u2019s children; 2: Create an new node t\u2032 = t; 3: for j \u2190 1 to n do 4: currentNode \u2190 Canonicalize(cj); 5: if j = 1 and j 6 I(t) then 6: t\u2032.leftChild\u2190currentNode; 7: else if j = I(t) + 1 then 8: t\u2032.middleChild\u2190currentNode; 9: else\n10: lastNode.rightChild\u2190currentNode; 11: end if 12: lastNode \u2190 currentNode; 13: end for 14: return t\u2032;\nTheorem 1. Given any SP tree T , it can be transformed into a ternary tree T \u2032, and T \u2032 can be transformed back into the original tree T .\nProof. Using the Alg.2, we can transform T into a ternary tree T \u2032.\nWe now show how to transform T \u2032 back into T . For each node t \u2208 T \u2032, if t is not a right child, let r1 denote the right child of t, r2 denote the right child of r1, rn denote the right child of rn\u22121 until rn obtains no right child.\nIn the original tree T , t and {rj}nj=1 must be siblings. For simplicity, let t\u2032 denote their parent.\n1) If t is a left child in T \u2032, (t , r1 , \u00b7 \u00b7 \u00b7 , rn) are first, second, . . . , (n + 1)-th child of t\u2032 in the original SP tree T . 2) If t is a middle child in T \u2032, (t , r1 , \u00b7 \u00b7 \u00b7 , rn) are( I(t\u2032) + 1 ) -th, ( I(t\u2032) + 2 ) -th, . . . , ( I(t\u2032) + n + 1 ) -th child of t\u2032 in the original SP tree T . In this way, for each node in T \u2032, we can find its original position in T , and then re-converts T \u2032 to T .\nAdditionally, we prove that ternary tree is the \u201cbest\u201d choice for model complexity. Theoretically, a dependency tree is equivalent to a K-ary tree when K > 3. Since the number of K is linear to parameter size in the X2Tree model, we prefer simpler models with smaller values of K. Theorem 2 formally shows that SP trees are not equivalent to binary trees. Therefore, the ternary tree is the \u201cbest\u201d choice.\nTheorem 2. Given any SP tree T , no algorithm exists which transforms T into an LCRS tree T \u2032 and re-converts T \u2032 to T .\nProof. Let Sn, On and Ln respectively denote the set of sequence-preserved trees, ordered trees and LCRS trees with n nodes. Since the ordered trees and LCRS trees obtain one-to-one correspondence [5], it can be inferred that the element number |On| = |Ln|.\nFor a node t in ordered tree, if t and its children obtain specified ordering, namely I(t) is defined, it converts to a SP tree. Furthermore, for different I(t), the SP trees are different. Thus, |Sn| > |On| = |Ln|.\nMoreover, suppose that an algorithm exists that transforms T into an LCRS tree T \u2032, and re-converts T \u2032 to T . This infers that |Sn| 6 |Ln|. It is contradictory to |Sn| > |Ln|."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Dataset Details", "text": "We apply the X2Tree model to dialog generation task and demonstrate its effectiveness. To construct the dialog corpus, 14 million post-response pairs were obtained from Tencent Weibo3. 815, 852 were left after removing spams and advertisements, among which 775, 852 are for training, 40, 000 for model validation. Note that in the dataset, each post and response is a sequence. To enable X2Tree modeling, the LTP (Language Technology Platform4) dependency parser is utilized to transform the responses into dependency trees, which are fed to X2Tree model for training after canonicalization. Furthermore, at predicting time, given an input post, a tree is generated before getting flatten into a sentence as response."}, {"heading": "4.2 Benchmark Methods", "text": "We implemented the following four popular neuralbased conversation models for comparison:\n1) Seq2Seq[19]: A RNN model that utilizes the last hidden state of the encoder as the initial hidden state of the decoder. 2) EncDec[3]: A RNN model that feeds the last hidden state of the encoder to every cell and softmax unit of the decoder. 3) ATT[1]: A RNN model based on EncDec with attention signal. 4) NRM[13]: Neural Responding Machine with both global and local schemes.\nAll these models map sequences to sequences directly, and only differ in how to summarize the encoder hidden states into a latent vector. Thus, the\n3http://t.qq.com/?lang=en_US 4http://www.ltp-cloud.com/\nproposed tree decoder can be applied to any of these models, and potentially improve the response quality from a different perspective. In this study, a tree decoder grafted on EncDec [3] is implemented for evaluation (denoted as X2Tree). Here, we stress that this tree-decoder can be easily applied to the model [12], which summarizes multiple rounds of dialogues into a latent vector. In the future, tree decoder for multi-round dialog will be evaluated."}, {"heading": "4.3 Implementation Details", "text": "As segmentation granularity and vocabulary size have an impact on model performances, for fair comparison, all sentences in the experiments are segmented by LTP. A vocabulary of 28,000 most frequent Chinese words in the corpus is used for training, which contains 97% words. Out-of-vocabulary words are replaced with \u201cunk\u201d.\nOur implementations are based on the Theano library [2]. We applied one-layer GRU [3] with 1,024- dimensional hidden states to {fk}Kk=1 and all baseline models. As suggested in [13], the word embeddings for the encoders and decoders are learned separately, whose dimensions are set to 128 for all models. All the parameters were initialized using a uniform distribution between -0.01 and 0.01. In training, we divided the corpus into mini-batches, each of which contains 128 pairs of posts and responses, and used ADADELTA [24] for optimization. The training stops if the perplexity on the validation set increases for 4 consecutive epochs. Models with best perplexities are selected for further evaluation.\nWhen generating responses, for X2Tree we use generalized beam search with global beam sizeG = 6, local beam size L = 6. For other X2Seq baseline models, conventional beam search with beam size 200 is used."}, {"heading": "4.4 Human Judgment", "text": "Due to the high diversity nature of dialogs, it is practically impossible to construct a data set which adequately covers all responses for each given post. Thus, match-based metrics, for example the BLEU [11], are not appropriate. While perplexity is widely used in SMT, lower values of this measure do not lead to better responses [9].\nHence, we only apply human judgment in our experiments. In detail, 3 labelers were invited to evaluate the quality of responses to 300 randomly sampled posts. For each post, each model generated 5\nresponses (for a total of 25). For fair comparison, we create a single file in which each post is followed by its 25 responses which are shuffled to avoid labelers knowing which model each response is generated by.\nFor each response the labelers determine the quality to be one of the following three levels:\n\u2022 Level 1: The response is ungrammatical. \u2022 Level 2: The response is basically grammatical\nbut irrelevant to the input post. \u2022 Level 3: The response is grammatical and rel-\nevant to the input post. The response on this level is acceptable for dialog system.\nFrom labeling results, average percentages of responses in different levels are calculated. Additionally, labeling agreement is evaluated by Fleiss\u2019 kappa [7] which is a measure of inter-rater consistency."}, {"heading": "4.5 Experimental Results and Analysis", "text": "The experimental results are summarized in Table 1. For Seq2Seq, NRM and X2Tree, the agreement value is in a range from 0.6 to 0.8 which is interpreted as \u201csubstantial agreement\u201d. Meanwhile, EncDec and ATT obtain a relatively higher kappa value between 0.8 to 1.0 which is \u201calmost perfect agreement\u201d. Hence, the labeling standard is considered clear which leads to high agreement among labelers.\nFor the Level-3 (acceptable ratio), X2Tree visibly outperforms other models. The best baseline method NRM achieves 54.38% Level-3 ratio, while X2Tree reaches 65.53% with an increase percentage of 11.15%. This improvement is mainly due to less irrelevant (Level-2) responses being generated (34.02% v.s. 44.98%), indicating X2Tree outputs more acceptable responses.\nWe further notice from Table 1 that the percentage of ungrammatical (Level-1) responses from X2Tree is smaller than other models in the experiments. It shows that responses generated by the tree-structured decoder are more grammatical than those from the chain-structured decoders.\nIn summary, the experiments demonstrate that X2Tree is able to generate more grammatical and relevant responses, and also show X2Tree obtains the ability to generate correct trees."}, {"heading": "5 Related Work", "text": "Given an input x and an output y, conventional Encoder-Decoder neural framework aims to learn the probability p(y|x). x and y could be simple structure (e.g. sequence) or complicated structure (e.g. image). In this framework, encoder transforms input x into a context vector and decoder transforms the context vector into the output y.\nSo far, various types of encoders have been developed, while the number of decoder types is relatively fewer: 1) Concept-to-text models encode nonlinguistic input (e.g. structured data) and output text [22, 8]. 2) Image captioning models generate text descriptions of a given image [8, 23]. 3) Sequence-to-sequence models are popular for chatbot and translation, which encode a source sequence to a context vector and decode the vector to a target sequence [3, 19, 13]. 4) Recursive neural networks which encode tree structures in bottom-up fashion have been developed for sentence classification [16, 20, 26].\nAll these applications operate in an X to sequence (X2Seq) fashion where X denotes aforementioned data structures such as concept, image, sequence or tree. In this paper, X2Tree network is developed. It is a generalization of X2Seq network given sequence is a special case of tree. Thus, X2Tree model covers the applications of X2Seq models. Note that some models generate tree in the bottom-up fashion, for example constituency parser [15]. However, X2Tree can generate trees where nodes are not given in advance, while the bottom-up models can not.\nFurthermore, the chain-structured Seq2Seq models obtain wide applications, for example statistical machine translation (SMT) and dialog system. Sutskever et al. [19] used multi-layered LSTM as the encoder and the decoder for machine translation. Later, Cho et al. [3] proposed the encoder-decoder framework, where the context vector is fed to every unit in the decoder. Recent research of encoderdecoder framework for dialog system follows the line of SMT. For short conversation, Shang et al. [13] proposed the Neural Responding Machine which further extended the attention mechanism with both global and local schemes. Most recently, some re-\nsearchers focused on multi-round conversation. Serban et al. [12] built an end-to-end dialogue system using hierarchical neural network. Our proposed model can also be applied to these multi-round dialog systems and potentially improve the performances.\nRecently, some studies proposed to use treestructured neural network instead of the conventional chain-structured neural network to improve the quality of semantic representation. Socher et al. [14] proposed the Recursive Neural Tensor Network. Each phrase is represented by word vectors and its parse tree. Vectors of higher level nodes are computed using their child phrase vectors. Tai et al. [20] and Zhu et al. [26] extended the chain-structured LSTM to tree structures. All these models use tree structures to summarize a sentence into a context vector, while we propose to decode from a context vector to generate sentences in a root-to-leaf direction. Addtionally, Zhang et al.[25] proposed treelstm and ldtreelstm via Tree LSTM activation functions. These two algorithms aim at generate the dependency tree (a special instance of the sequence-preserved tree), while X2Tree algorithm can generate different types of trees."}, {"heading": "6 Conclusion and Future Work", "text": "In this study, we proposed a tree-structured neural network aiming to predict tree structures and apply it to conversational generation for evaluation. The modules of parent-children dependency, tree canonicalization, generalized beam search are developed to guarantee the success of this generative process.\nFinally, we argue that the proposed X2Tree model has wide applications. For example, it can be applied for the text-to-image task, since a 2d picture can be represented as a Quadtree [17]. The only difference is that it is better to model the tree nodes for images as continuous random variables. We will explore more on this in our future work."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Theano: New Features and Speed Improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua  Bengio"], "venue": "In NIPS Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Learning Phrase Representations using RNN Encoderdecoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In EMNLP,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Attention-Based Models for Speech Recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Introduction to algorithms", "author": ["Thomas H Cormen"], "venue": "MIT press,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "The Average Height of Binary Trees and Other Simple Trees", "author": ["Philippe Flajolet", "Andrew Odlyzko"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "Measuring Nominal Scale Agreement Among Many Raters", "author": ["Joseph L Fleiss"], "venue": "Psychological Bulletin,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1971}, {"title": "Unsupervised concept-to-text generation with hypergraphs", "author": ["Ioannis Konstas", "Mirella Lapata"], "venue": "In NAACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V. Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Geometric Modeling Using Octree Encoding", "author": ["Donald Meagher"], "venue": "Computer Graphics & Image Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1982}, {"title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wj Zhu"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Neural Responding Machine for Short-Text Conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In ACL,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "In ACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank", "author": ["Richard Socher", "Alex Perelygin", "Jean Y.Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": "In EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Image Processing, Analysis, and Machine Vision", "author": ["Milan Sonka", "Vaclav Hlavac", "Roger Boyle"], "venue": "Cengage Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A Hierarchical Recurrent Encoder-Decoder For Generative Context- Aware Query Suggestion", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Hossein Vahabi", "Christina Lioma", "Jakob G. Simonsen", "Jian-Yun Nie"], "venue": "In CIKM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning"], "venue": "In ACL,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Stochastic Language Generation in Dialogue using Recurrent Neural Networks with Convolutional Sentence Reranking", "author": ["Tsung Hsien Wen", "Milica Gasic", "Dongho Kim", "Nikola Mrksic", "Pei Hao Su", "David Vandyke", "Steve Young"], "venue": "DIAL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning", "author": ["Matthew D. Zeiler"], "venue": "Rate Method. arXiv,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Top-down Tree Long Short-Term Memory Networks", "author": ["Xingxing Zhang", "Liang Lu", "Mirella Lapata"], "venue": "In NAACL,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Long Short-Term Memory Over Tree Structures", "author": ["Xiaodan Zhu", "Parinaz Sobhani", "Hongyu Guo"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 138, "endOffset": 145}, {"referenceID": 18, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 138, "endOffset": 145}, {"referenceID": 12, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 170, "endOffset": 174}, {"referenceID": 3, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 195, "endOffset": 198}, {"referenceID": 7, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 217, "endOffset": 224}, {"referenceID": 21, "context": "Recent years have witnessed a great success of neural methods for structured prediction in wide applications, such as machine translation [3, 19], automatic conversation [13], speech recognition [4], image captioning [8, 23].", "startOffset": 217, "endOffset": 224}, {"referenceID": 18, "context": "Since sequence is a basic structure for data representation, most of these studies focus on the Seq2Seq models [19, 13], which achieve the state-of-the-art performance for many tasks.", "startOffset": 111, "endOffset": 119}, {"referenceID": 12, "context": "Since sequence is a basic structure for data representation, most of these studies focus on the Seq2Seq models [19, 13], which achieve the state-of-the-art performance for many tasks.", "startOffset": 111, "endOffset": 119}, {"referenceID": 15, "context": "For example, in semantic analysis of various digital data, sentences are usually parsed into trees [16], 2d and 3d photograph can be represented as a Quadtree [17] or a Octree [10] respectively, and Web pages are naturally coded as HTML tree.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "For example, in semantic analysis of various digital data, sentences are usually parsed into trees [16], 2d and 3d photograph can be represented as a Quadtree [17] or a Octree [10] respectively, and Web pages are naturally coded as HTML tree.", "startOffset": 159, "endOffset": 163}, {"referenceID": 9, "context": "For example, in semantic analysis of various digital data, sentences are usually parsed into trees [16], 2d and 3d photograph can be represented as a Quadtree [17] or a Octree [10] respectively, and Web pages are naturally coded as HTML tree.", "startOffset": 176, "endOffset": 180}, {"referenceID": 23, "context": "[25] proposed treelstm and ldtreelstm via Tree LSTM activation functions in top-down fashion.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a max-margin structure prediction architecture based on recursive neural networks, and demonstrated that it successfully parses sentences and understands scene images.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] and Zhu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] extended the chainstructured LSTM to tree-structured LSTM, which is shown to be more effective in representing a tree structure as a latent vector.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Similarly, to parse natural scene images [15], an image is first divided into segments, each of which corresponds to one leaf node in output tree.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "[19, 26] ) , and mostly focus on the tree-structured decoder for the generation of Tx.", "startOffset": 0, "endOffset": 8}, {"referenceID": 24, "context": "[19, 26] ) , and mostly focus on the tree-structured decoder for the generation of Tx.", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "For an ordered tree, where ordering is specified for the children of each node, we transform it to a left-child right-sibling (LCRS) binary tree [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Recently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12].", "startOffset": 86, "endOffset": 105}, {"referenceID": 12, "context": "Recently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12].", "startOffset": 86, "endOffset": 105}, {"referenceID": 17, "context": "Recently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12].", "startOffset": 86, "endOffset": 105}, {"referenceID": 11, "context": "Recently, neural models achieved the state-of-theart performances in Seq2Seq learning [3, 13, 21, 18, 12].", "startOffset": 86, "endOffset": 105}, {"referenceID": 5, "context": "In Ty, the average depth of nodes is O( \u221a |y|) [6], which is much smaller than the sentence length |y|.", "startOffset": 47, "endOffset": 50}, {"referenceID": 24, "context": "It is also easily developed by switching the chain-structured encoder with a tree-structured encoder [26].", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "Since the ordered trees and LCRS trees obtain one-to-one correspondence [5], it can be inferred that the element number |On| = |Ln|.", "startOffset": 72, "endOffset": 75}, {"referenceID": 18, "context": "We implemented the following four popular neuralbased conversation models for comparison: 1) Seq2Seq[19]: A RNN model that utilizes the last hidden state of the encoder as the initial hidden state of the decoder.", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "2) EncDec[3]: A RNN model that feeds the last hidden state of the encoder to every cell and softmax unit of the decoder.", "startOffset": 9, "endOffset": 12}, {"referenceID": 0, "context": "3) ATT[1]: A RNN model based on EncDec with attention signal.", "startOffset": 6, "endOffset": 9}, {"referenceID": 12, "context": "4) NRM[13]: Neural Responding Machine with both global and local schemes.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "In this study, a tree decoder grafted on EncDec [3] is implemented for evaluation (denoted as X2Tree).", "startOffset": 48, "endOffset": 51}, {"referenceID": 11, "context": "Here, we stress that this tree-decoder can be easily applied to the model [12], which summarizes multiple rounds of dialogues into a latent vector.", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "Our implementations are based on the Theano library [2].", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "We applied one-layer GRU [3] with 1,024dimensional hidden states to {fk}k=1 and all baseline models.", "startOffset": 25, "endOffset": 28}, {"referenceID": 12, "context": "As suggested in [13], the word embeddings for the encoders and decoders are learned separately, whose dimensions are set to 128 for all models.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "In training, we divided the corpus into mini-batches, each of which contains 128 pairs of posts and responses, and used ADADELTA [24] for optimization.", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "Thus, match-based metrics, for example the BLEU [11], are not appropriate.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "While perplexity is widely used in SMT, lower values of this measure do not lead to better responses [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "Additionally, labeling agreement is evaluated by Fleiss\u2019 kappa [7] which is a measure of inter-rater consistency.", "startOffset": 63, "endOffset": 66}, {"referenceID": 20, "context": "structured data) and output text [22, 8].", "startOffset": 33, "endOffset": 40}, {"referenceID": 7, "context": "structured data) and output text [22, 8].", "startOffset": 33, "endOffset": 40}, {"referenceID": 7, "context": "2) Image captioning models generate text descriptions of a given image [8, 23].", "startOffset": 71, "endOffset": 78}, {"referenceID": 21, "context": "2) Image captioning models generate text descriptions of a given image [8, 23].", "startOffset": 71, "endOffset": 78}, {"referenceID": 2, "context": "3) Sequence-to-sequence models are popular for chatbot and translation, which encode a source sequence to a context vector and decode the vector to a target sequence [3, 19, 13].", "startOffset": 166, "endOffset": 177}, {"referenceID": 18, "context": "3) Sequence-to-sequence models are popular for chatbot and translation, which encode a source sequence to a context vector and decode the vector to a target sequence [3, 19, 13].", "startOffset": 166, "endOffset": 177}, {"referenceID": 12, "context": "3) Sequence-to-sequence models are popular for chatbot and translation, which encode a source sequence to a context vector and decode the vector to a target sequence [3, 19, 13].", "startOffset": 166, "endOffset": 177}, {"referenceID": 15, "context": "4) Recursive neural networks which encode tree structures in bottom-up fashion have been developed for sentence classification [16, 20, 26].", "startOffset": 127, "endOffset": 139}, {"referenceID": 19, "context": "4) Recursive neural networks which encode tree structures in bottom-up fashion have been developed for sentence classification [16, 20, 26].", "startOffset": 127, "endOffset": 139}, {"referenceID": 24, "context": "4) Recursive neural networks which encode tree structures in bottom-up fashion have been developed for sentence classification [16, 20, 26].", "startOffset": 127, "endOffset": 139}, {"referenceID": 14, "context": "Note that some models generate tree in the bottom-up fashion, for example constituency parser [15].", "startOffset": 94, "endOffset": 98}, {"referenceID": 18, "context": "[19] used multi-layered LSTM as the encoder and the decoder for machine translation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] proposed the encoder-decoder framework, where the context vector is fed to every unit in the decoder.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "[13] proposed the Neural Responding Machine which fur-", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] built an end-to-end dialogue system using hierarchical neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed the Recursive Neural Tensor Network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] and Zhu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] extended the chain-structured LSTM to tree structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] proposed treelstm and ldtreelstm via Tree LSTM activation functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "For example, it can be applied for the text-to-image task, since a 2d picture can be represented as a Quadtree [17].", "startOffset": 111, "endOffset": 115}], "year": 2017, "abstractText": "Tree structures are commonly used in the tasks of semantic analysis and understanding over the data of different modalities, such as natural language, 2d or 3d graphics and images, or Web pages. Previous studies model the tree structures in a bottomup manner, where the leaf nodes (given in advance) are merged into internal nodes until they reach the root node. However, these models are not applicable when the leaf nodes are not explicitly specified ahead of prediction. Here, we introduce a neural machine for top-down generation of tree structures that aims to infer such tree structures without the specified leaf nodes. In this model, the history memories from ancestors are fed to a node to generate its (ordered) children in a recursive manner. This model can be utilized as a tree-structured decoder in the framework of \u201cX to tree\u201d learning, where X stands for any structure (e.g. chain, tree etc.) that can be represented as a latent vector. By transforming the dialogue generation problem into a sequence-to-tree task, we demonstrate the proposed X2Tree framework achieves a 11.15% increase of response acceptance ratio over the baseline methods.", "creator": "LaTeX with hyperref package"}}}