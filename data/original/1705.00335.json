{"id": "1705.00335", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2017", "title": "Quantifying Mental Health from Social Media with Neural User Embeddings", "abstract": "Mental illnesses adversely affect a significant proportion of the population worldwide. However, the methods traditionally used for estimating and characterizing the prevalence of mental health conditions are time-consuming and expensive. Consequently, best-available estimates concerning the prevalence of mental health conditions are often years out of date. Automated approaches to supplement these survey methods with broad, aggregated information derived from social media content provides a potential means for near real-time estimates at scale. These may, in turn, provide grist for supporting, evaluating and iteratively improving upon public health programs and interventions.", "histories": [["v1", "Sun, 30 Apr 2017 16:12:28 GMT  (6879kb,D)", "http://arxiv.org/abs/1705.00335v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.SI", "authors": ["silvio amir", "glen coppersmith", "paula carvalho", "m\\'ario j silva", "byron c wallace"], "accepted": false, "id": "1705.00335"}, "pdf": {"name": "1705.00335.pdf", "metadata": {"source": "CRF", "title": "Quantifying Mental Health from Social Media with Neural User Embeddings Quantifying Mental Health from Social Media with Neural User Embeddings", "authors": ["Silvio Amir", "Glen Coppersmith", "M\u00e1rio J. Silva", "Byron C. Wallace"], "emails": ["samir@inesc-id.pt", "glen@qntfy.com", "pcc@inesc-id.pt", "mjs@inesc-id.pt", "b.wallace@northeastern.edu"], "sections": [{"heading": null, "text": "We propose a novel model for automated mental health status quantification that incorporates user embeddings. This builds upon recent work exploring representation learning methods that induce embeddings by leveraging social media post histories. Such embeddings capture latent characteristics of individuals (e.g., political leanings) and encode a soft notion of homophily. In this paper, we investigate whether user embeddings learned from twitter post histories encode information that correlates with mental health statuses. To this end, we estimated user embeddings for a set of users known to be affected by depression and post-traumatic stress disorder (PTSD), and for a set of demographically matched \u2018control\u2019 users. We then evaluated these embeddings with respect to: (i) their ability to capture homophilic relations with respect to mental health status; and (ii) the performance of downstream mental health prediction models based on these features. Our experimental results demonstrate that the user embeddings capture similarities between users with respect to mental conditions, and are predictive of mental health."}, {"heading": "1. Introduction", "text": "Mental illness is a critically important concern, significantly and adversely affecting a wide swath of the population directly and indirectly. An estimate by the Centers for Disease Control from 2008 (CDC, 2010), suggests that 9% of US adults may meet the criteria for\nar X\niv :1\n70 5.\n00 33\n5v 1\n[ cs\ndepression at any given time. While not as prevalent as depression, post traumatic stress disorder (PTSD) issues still cost hundreds of billions of dollars worldwide, according to a conservative estimate from the NIH1. The collective effect of mental health conditions, as measured by Daily Adjusted Life Years (DALYs), exceeds that of malaria, war, or violence2 (?). At the same time, mental health problems are often difficult to identify and thus treat. For example, perhaps half of depressive cases go undetected, in part due to the heterogeneous and complex expression of this condition (Paykel et al., 1997). Another exacerbating factor is that diagnosis generally requires individuals to actively seek out treatment. Yet, the manifestation of this condition and prevailing social stigmas may disincline afflicted individuals to seek treatment.\nThe internet may provide a comfortable medium for people to express their feelings anonymously and connect with health-care professionals (McCaughey et al., 2014) and others affected by similar conditions (De Choudhury et al., 2016). Furthermore, individuals openly discuss mental health challenges on public social network platforms such as Twitter (Coppersmith et al., 2014a, 2015a). Prior work has demonstrated the potential of using social media to investigate mental health issues (Paul and Dredze, 2011), including depression (Schwartz et al., 2014), PTSD (Coppersmith et al., 2014b) and suicidal ideation (Coppersmith et al., 2016; De Choudhury et al., 2016) in individuals. However, models and techniques to identify and quantify mental health related signals from social media are relatively novel. Interest in these applications has motivated the creation of a shared task for the Computational Linguistics and Clinical Psychology workshop (CLPsych)3, which aimed to advance the state-of-the-art in technologies capable of discriminating users affected by mental illness from controls, given their post history (Coppersmith et al., 2015b). A variety of methods have been proposed for this task, but none have achieved consistently superior performance, which implies that improvements may yet be realized by improved models.\nNeural representation learning methods have been shown capable of automatically discover good representations (e.g., predictive features) from data, freeing practitioners from the burden of manually designing and encoding task-specific features (Bengio et al., 2015a; Goldberg, 2016). In natural language applications, this has resulted in neural distributed representations becoming the de-facto standard representational approach. Word embeddings in particular aim to implicitly encode latent word semantics (in the distributional sense), and can be learned in an unsupervised fashion by means of predictive models that exploit word co-occurrence statistics and other regularities in unlabeled corpora (Bengio et al., 2003). These models have been recently extended to infer representations for larger textual units (Le and Mikolov, 2014), and even user representations (Li et al., 2015; Amir et al., 2016). It has been shown that these user embeddings also capture latent user aspects and can be used in downstream applications, such as sarcasm detection (Amir et al., 2016) and content recommendation (Yu et al., 2016).\nIn this paper we investigate whether user representations induced via neural models can inform clinical models operating over social media. In particular we consider whether user embeddings (learned directly from historical data) capture aspects of mental health status. To this end we leverage the dataset created for the CLPsych shared task to address two\n1. https://www.nimh.nih.gov/health/statistics/cost/index.shtml 2. For a visualization of DALYs see https://vizhub.healthdata.org/gbd-compare/ 3. http://clpsych.org\nresearch questions: (1) To what extent do user embeddings capture information relevant for mental health analysis applications, over social media? (2) Can user embeddings be leveraged to discriminate between users suffering from mental illness and demographically matched controls? We answer the first question by comparing different approaches to induce user representations from a collection of posts. In particular, we investigated whether the induced embeddings capture homophilic relations between users with respect to mental health. To answer the second question, we developed and evaluated predictive models, leveraging user embeddings, to discriminate users affected by depression, PTSD, and ageand gender-match controls.\nThe main contributions of this paper are as follows: (i) we show that unsupervised user embeddings induced from posting histories capture user similarities, and are predictive of mental health conditions; (ii) we develop a novel neural model that incorporates and refines these embeddings to improve the categorization of users with respect to mental health status; furthermore, we show that the resultant fine-tuned user embeddings better align with mental health conditions.\nThe remainder of the paper is organized as follows. The next section introduces the aforementioned CLPsych shared task and the corresponding dataset. Section 3 reviews the literature on user modelling for social media analysis and neural embedding learning. In Section 4, we formally describe the user embedding model used in our experiments, and discusses the connections with prior representation learning methods. Section 5 addresses the first research question by evaluating the properties captured by the user embeddings. Section 6 reports on the classification experiments we conducted to answer the second research question. Finally, we present our conclusions in Section 7."}, {"heading": "2. Depression and PTSD on Twitter", "text": "In 2015, the CLPsych workshop held a shared task to foster progress in NLP technologies with potential for applications related to mental health analysis, over social media streams (Mitchell et al., 2015; Coppersmith et al., 2015b). To that end, a dataset was compiled comprising users that have publicly stated on Twitter that they were diagnosed with depression (327 users) or PTSD (246 users), and an equal number of randomly selected demographically-matched users as controls4. For each user in this dataset, associated metadata and posting history was also collected \u2014 up to the 3000 most recent tweets, per limitations of the Twitter API. For more details on the construction and validation of the data, see (Coppersmith et al., 2015b, 2014a,b).\nThe participants were then asked to develop models to discriminate between users affected by mental illness from controls, given their posts and metadata. Specifically this entailed three binary sub-tasks: (i) depression vs control, (ii) PTSD vs control and (iii) depression vs PTSD. The proposed systems were based on a wide range of approaches including: rule-based systems leveraging lexical decision lists (Pedersen, 2015), linear classifiers exploiting features based on word clusters and topic models (Preotiuc-Pietro et al., 2015), supervised topic models (Resnik et al., 2015) and systems exploiting character-level language models (Coppersmith et al., 2015b). However, we note that none of the proposed\n4. This data was collected according to the ethical protocol of ?, and follows the recommendations spelled out in Mikal et al. (2016).\nsystems performed consistently better than the others across all the sub-tasks and evaluation metrics, highlighting the difficulty of this problem. Moreover, none of these systems used explicit representations of users, which is the innovation we propose here. We did not participate in this shared task, and thus could not obtain the official test data. Therefore, our results are not directly comparable to those of the participating teams. Nevertheless, we compared our proposed approach with the majority of the previously proposed methods."}, {"heading": "3. Related Work", "text": "Most of the research in social media analysis has been concerned with deriving better models that operate on representations of the texts comprising individual users posts, both via manually crafted features and, more recently, representation learning approaches (Severyn and Moschitti, 2015; Astudillo et al., 2015). Nevertheless, for a variety of problems it is crucial to also capture characteristics of the users involved in the communications. These include, information extraction (Yang et al., 2016), opinion mining (Tang et al., 2015), sarcasm detection (Bamman and Smith, 2015) and content recommendation (Yu et al., 2016). The most straightforward approach to induce user representations is by scrapping \u201cprofile\u201d information from social websites, to manually extract features based on social ties, demographic attributes, or posting habits (Bamman and Smith, 2015; Rajadesingan et al., 2015). However, these approaches require significant effort for data collection, and for task- and domain-specific feature engineering. Furthermore, the available user profile information depends on specific social websites and may not always be available. It may also be inaccurate or simply outdated.\nNeural Embedding Learning\nIn recent years, models in NLP have moved from discrete word representations, based on scalars representing indices into a pre-defined vocabulary, towards distributed continuous vector word representations that encode latent semantics \u2014 these are usually referred as word embeddings (Goldberg, 2016). The general framework to learn unsupervised word embeddings involves associating words with parameter vectors, which are then optimized to be good predictors of other words that occur in the same contexts (Bengio et al., 2003). Skip-Gram (Mikolov et al., 2013), one of the most popular word embedding models, operationalizes this approach by sliding a window of a pre-specified size across the corpus. At each step, the center word is used to predict the probability of one of the surrounding words, sampled proportionally to the distance to the center word. Le and Mikolov (2014) later expanded this approach with two Paragraph2Vec models that also learn representations for paragraphs (or, more broadly, any sequence of words): (i) PV-dm, tries to predict the center word of the sliding window, given the surrounding words and the paragraph (i.e., their respective embeddings); and (ii) PV-dbow, tries to predict the words of a sliding window within a paragraph, conditioned only on the respective paragraph embedding.\nRecently proposed methods to learn user representations use essentially the same approach \u2014 associate users with parameter vectors, and optimize these to accurately predict observable attributes or the words used by said user in previous posts (Li et al., 2015; Amir et al., 2016; Yu et al., 2016). As discussed above, leveraging user profile information to collect attributes is not always reliable. Interestingly, however, the embeddings induced\nby Amir et al. (2016), using only the previous posts from a user, were shown to capture latent user aspects (e.g. political leanings) and a soft notion of \u2018homophily\u2019 \u2014 similar users were generally represented with similar vectors. Furthermore, these user representations were successfully used to improve a downstream model for sarcasm detection in tweets. Similarly, Yu et al. (2016) used user embeddings to improve a microblog recommendation system. Our work aims to ascertain if user embeddings learnt only from previous posts, can capture useful signals for clinical applications."}, {"heading": "4. Learning User Embeddings", "text": "To learn user embeddings, we adopted an approach similar to that recently proposed by Amir et al. (2016). The idea is to capture relations between users and the content (i.e., the words) they generate, by optimizing the probability of sentences conditioned on their authors. Formally, let U be a set of users, Cj be a collection of posts authored by user uj \u2208 U , and S = {w1, . . . , wN} be a post composed of words wi from a vocabulary V. The goal is to estimate the parameters of a user vector uj , that maximize the conditional probability:\nP (Cj |uj) \u221d \u2211 S\u2208Cj \u2211 wi\u2208S logP (wi|uj) (1)\nHowever, directly estimating these quantities (e.g., with a log-linear model) would require calculating a normalizing constant over a potentially large number of words, a computationally expensive operation. Because we are only interested in the user vectors uj and not the actual probabilities as such, we can approximate the term P (wi|uj) by minimizing the following Hinge-loss objective:\nL(wi, uj) = \u2211 w\u0303k\u2208V max(0, 1\u2212wi \u00b7 uj + w\u0303k \u00b7 uj) (2)\nwhere word w\u0303k (and associated embedding, w\u0303k) is a negative sample, i.e. a word not occurring in the sentence under consideration, which was written by user uj . By learning to discriminate between observed positive examples and pseudo-negative examples, the model shifts probability mass to more plausible observations (Smith and Eisner, 2005). Note that we represent both words and users via d-dimensional embeddings \u2014 word embeddings, wi \u2208 Rd which are assumed to have been pre-trained trough some neural language model; and user embeddings uj \u2208 Rd to be learned. We will refer to this approach as User2Vec.5\nWe note that barring some minor operational differences, this model is equivalent to the PV-dbow variant of Paragraph2vec \u2014 if users are viewed as paragraphs. The key differences are that: (i) User2Vec predicts all the words in a post, whereas PVdbow slides a window along the paragraph and only predicts one word per step; and (ii) User2Vec assumes that the word embeddings are pre-trained, whereas PV-dbow aims to jointly learn the word and paragraph vectors.\n5. This formulation is a simplification of Amir et al. (2016) model. Specifically, we omitted a term in Eq.1, encoding the marginal probability of S; and we allow the negative samples to be drawn from all the words in V. These simplifications dramatically reduce training time without significant loss of quality on the resulting embeddings."}, {"heading": "5. User Embedding Analysis", "text": "In this section we address our first research question by investigating whether user embeddings learned directly from social media data encode information relevant for public health applications. Previous work has shown that these representations capture latent user aspects and a soft notion of \u2018homophily\u2019. If some of these aspects correlate with mental health, then the embeddings could be used to identify risk groups; for example, we might identify and characterize users that \u2018look\u2019 like patients affected by depression. The ability to do so would potentially enable scalable, real-time estimates concerning the prevalence of mental health issues in particular populations.\nWe estimated User2Vec embeddings from the shared task dataset described in Section 2, as follows.6 First, we pre-processed tweets by: lower-casing; reducing character repetitions to at most three repetitions; and replacing usernames and URLs with a canonical form. Users with fewer than 100 tweets were discarded. Next, we pre-trained a set of SkipGram word vectors from the task data and another large unlabeled Twitter corpus, using the Gensim7 python package (R\u030ceh\u030aur\u030cek and Sojka, 2010). Finally, for each user uj \u2208 U , we sampled a held-out set Hj \u2282 Cj with 10% of the posting history. The rest of the data was used to estimate an embedding uj by minimizing Eq. 1 via stochastic gradient descent, using P (Hj |uj) as early stopping criteria.\nThe same dataset was used to derive embeddings with PV-dm and PV-dbow models (also via Gensim). To ensure a fair comparison, we used the same hyper-parameters for all the models, which were set as follows: window size w = 5, negative sample size s = 20 and vector size d = 400.\n6. Code will be made available after publication 7. http://radimrehurek.com/gensim/"}, {"heading": "5.1 Measuring Homophily", "text": "To investigate if the induced user vectors capture homophilic relations with respect to mental health status, we first projected the d-dimensional vectors into a 2-dimensional space using t-Stochastic Neighborhood Embedding (TSNE) (Van der Maaten and Hinton, 2008). In Figure 1, we plot the resulting points colored according to the respective class. One can see that, at least to some extent, embeddings do seem to capture some notion of homophily, i.e. users are often surrounded by others of the same cohort.\nTo better quantify this effect and allow comparison of different user embedding models, we proceeded as follows. For each user in the corpus, we calculated the similarities to all other users (i.e., the cosine similarity between their respective embeddings), inducing a ranking of users in terms of similarity to the \u2018query\u2019 user. Intuitively, we would hope\nto see that users in the same mental health categories are comparatively similar to one another, i.e., users suffering from depression are most similar to other users also afflicted with depression.\nFigure 2 shows the results obtained with PV-dm vectors. The induced similarity ranking is shown in Figure 2a, where the first row correspond to the query users, and each column shows their top k = 100 most similar users, colored according to their class. Figure 2b shows the respective Receiver Operating Characteristic (ROC) curves under the induced ranking. In general, we found that all the user embedding models are able to capture user similarities. The embeddings induced with the User2Vec, pv-dbow and pv-dm all perform significantly better than chance, with AUC scores of 0.57, 0.57 and 0.59, respectively. Detailed plots can be found in the Appendix. The fact that user vectors are more likely to be close to those of others in the same cohort demonstrates that this approach does indeed capture signals relevant to mental health. This aligns with prior work showing that one\u2019s choice of words can be indicative of psychological states and mental health (Pennebaker et al., 2001)."}, {"heading": "6. Predicting Mental Health from Twitter Data", "text": "To address our second research question, we evaluated the user representations with respect to their predictive performance in downstream mental health analysis applications. Above we showed that representations induced from user posting histories encode relevant signals about mental health. But generic features estimated from unsupervised tasks are suboptimal for downstream tasks (Astudillo et al., 2015). Neural networks, when trained endto-end, can refine generic embeddings to specific tasks by modifying their parameters during supervised training (Collobert et al., 2011). However, this strategy requires updating a large number of parameters, which is difficult when only small training datasets are available. The CLPysch dataset comprises 1094 labeled instances (after discarding users with fewer than 100 tweets). Given the modest size of the dataset, we adopted the Astudillo et al. (2015) Non-Linear Subspace Embedding approach, (NLSE), which is able to adapt generic representations to specific tasks with scarce labeled data."}, {"heading": "6.1 Proposed Model", "text": "The NLSE model adapts generic embeddings to specific applications by learning linear projections into lower-dimensional subspaces, while the keeping the original embeddings fixed. Hence, the resulting embedding subspaces capture domain- and task-specific aspects, while preserving the rich information encoded by the original embeddings. More formally, given an user embedding matrix U \u2208 Rd\u00d7|U|, in which column U[j] represents user uj \u2208 U , we induce new representations by factorizing the input as S \u00b7 U where S \u2208 Rs\u00d7d, with s d, is a (learned) linear projection matrix. Crucially, this imposes dimensionality reduction on the feature space, simultaneously eliminating noise and reducing the number of free parameters, making the model easier to train with small datasets.\nThis model is similar to a feed-forward neural network with a word embedding layer and a single hidden layer. The main differences are: (1) the factorization of the embedding layer into two components (the original embedding matrix and a linear projection matrix)\nand, (2) the dimensionality reduction induced by the subspace projection, with typical reductions greater than an order of magnitude. Similar to feed-forward networks, we can use the backpropagation algorithm (Rumelhart et al., 1988) to jointly learn task-specific embeddings and the parameters for the classification layer. Using this approach, our proposed mental health prediction model can be formalized as:\nP (Y|uj) \u221d \u03b2 \u00b7 g(uj) g(uj) = \u03c3 ( S \u00b7U[j] ) (3) where, \u03c3(\u00b7) denotes an element-wise sigmoid non-linearity, and the matrix \u03b2 \u2208 R|Y|\u00d7s maps the embedding subspace to the classification space. Notice that at inference time this model reduces to a linear classifier with embedding subspace features."}, {"heading": "6.2 Experimental Setup", "text": "We evaluated embeddings induced with the User2Vec (u2v), and Paragraph2vec\u2019s PVdm and PV-dbow models (Section 5), as features in Logistic Regression (LR) and NLSE classifiers. These were compared against baselines using textual features based on:\n1. bow: bag-of-words vectors with binary weights, x \u2208 {0, 1}|V|;\n2. boe: bag-of-embeddings. We leveraged Skip-Gram embeddings to build vectors, x = \u2211 w E[w], where E[w] \u2208 Rd is the embedding of word w;\n3. lda: bag-of-topics. We induced t = 100 topics using Latent Dirichlet Allocation (Blei et al., 2003), to build vectors x \u2208 {0, 1}t indicating the topics present in user\u2019s posts;\n4. bwc: bag-of-word-clusters. We induced k = 1000 Brown et al. (1992) word clusters, to build vectors x \u2208 {0, 1}k mapping words in a user\u2019s posts to their respective clusters;\nWe also evaluated baselines that combine user vectors with textual features (u2v+bow and u2v+boe).\nExperiments were conducted with a 10-fold cross-validation protocol; at each iteration, the training partition was split into 80% for model training and the remainder for validation purposes (i.e. hyper-parameter selection and early-stopping). For consistency, we used the same splits for all the models. We performed a grid-search to choose the best `2 regularization coefficient, over the range c = {0.001, 0.01, 0.5, 1, 10, 100}, for the LR models; and the optimal subspace size s = {10, 15, 20, 25} and learning rate \u03b1 = {0.01, 0.1, 0.5, 1}, for the NLSE model."}, {"heading": "6.3 Results", "text": "The classifiers were mainly evaluated with respect to the macro average F1. We also report results in terms of binary F1, where we only average the scores for the depression and ptsd classes, to better ascertain the ability of the models to discriminate between mentally afflicted patients, which are less prevalent than the controls, but are the cases that we mostly care about.\nThe main classification results are shown in Figure 3. The first thing to note is that the BOW is a very strong baseline, essentially outperforming all the other linear classifiers\nbased on textual features and user embeddings. One reason is that users affected with mental illnesses, often talk about their conditions and the bow model can easily pick-up on such clues. Regarding the user embeddings, we found that, despite being equivalent, the PV-dbow performed much worse than the User2Vec, showing that better embeddings can be obtained by trying to predict all the words in users posts, and leveraging pre-trained word vectors. On the other hand, the PV-dm model has a performance comparable to that of the User2Vec.\nAs discussed above, generic embeddings are sub-optimal for downstream tasks, and our results are in line with this observation. By inducing task-specific representations via subspace projection, we were able to outperform all the other baselines by a fair margin. Note also that the NLSE approach is particularly better at discriminating the minority classes, i.e. patients suffering from depression and ptsd, as evidenced by the greater improvements in binary F1, when compared to the other baselines. To better understand the effects of the embedding adaptation, we repeated the same analysis described in Section 5 over the adapted embeddings (Figures 1 and 2). We can see that the new representations are much better at discriminating the controls from the other users, suggesting that induced embedding subspace captures more fine-grained signal related to mental statuses."}, {"heading": "7. Conclusions", "text": "In this paper, we investigated if embeddings induced from user posting histories capture relevant signals for clinical applications. In particular, we compared different user embedding methods, with respect to their ability to capture homophilic relations between users, and their performance as features in downstream mental health prediction models. The evaluation conducted over a dataset comprising of users diagnosed with depression and PTSD, and demographically matched controls, showed that these representations can indeed capture mental health related signals. This is in agreement with prior results from the field of psychology, establishing connections between word usage and mental status (Pennebaker et al., 2001). Interestingly, embeddings induced without knowledge of user labels capture similarities with respect to mental condition. Furthermore, we have shown that\nthese embeddings can be tailored \u2014 with a small amount of task-specific labeled data \u2014 to capture more granular information, thus improving the quality of downstream models and applications. Ultimately, this work is a step toward more accurate inference concerning the mental health status of social media users, in turn enabling more accurate epidemiological real-time population-wide monitoring of mental health. Such accurate monitoring \u2013 which is currently impossible \u2013 may provide empirical support for increased resource allocation to programs dedicated to preventing and alleviating mental health issues.\nMoving forward, user embeddings may provide a pivotal piece to allow clinical psychologists to take full advantage of digital phenotyping data. In particular, learned user embeddings may provide a representation at a sweet spot between instantaneous (proximal) state and lifelong (distal) state, which is critical to understanding psychological phenomena and risk of crisis."}, {"heading": "A. Measuring Homophily (continued)", "text": "B. Visualizing Embedding Subspace Features\nThe NLSE induces task-specific representations by learning a low dimensional embedding. We exploit the fact that the sigmoid non-linearity (Eq. 3) projects all the feature values into the range [0; 1], to map these values into color intensities in a heatmap. We then used the same sample used to produce the plots in Section 5 (100 users per class), and plotted the representations learnt by model. The resulting plot is shown in Figure 6. We can\nobserve that specific groups of features are mostly activated in specific classes. From this plot, we can see that the features induced with PV-DM model are sparser than the other models, which might explain why these features can better capture user similarities. On the other hand, the features induced with PV-DM and User2Vec seem \u2018noisier\u2019 but also seem to capture differences between classes. In Figure 7, we show a similar plot but where we average the feature vectors of all the users in each class. Interestingly, it seems to be case that the \u2018prototyipcal\u2019 class vectors learned with different embeddings are very similar."}], "references": [{"title": "Modelling context with user embeddings for sarcasm detection in social media", "author": ["Silvio Amir", "Byron C Wallace", "Hao Lyu", "Paula Carvalho M\u00e1rio J Silva"], "venue": "arXiv preprint arXiv:1607.00976,", "citeRegEx": "Amir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amir et al\\.", "year": 2016}, {"title": "Learning word representations from scarce and noisy data with embedding subspaces", "author": ["Ram\u00f3n Astudillo", "Silvio Amir", "Wang Ling", "Mario Silva", "Isabel Trancoso"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "Astudillo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Astudillo et al\\.", "year": 2015}, {"title": "Contextualized sarcasm detection on twitter", "author": ["David Bamman", "Noah A Smith"], "venue": "In Proceedings of the 9th International Conference on Web and Social Media,", "citeRegEx": "Bamman and Smith.,? \\Q2015\\E", "shortCiteRegEx": "Bamman and Smith.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Janvin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Deep learning. Book in preparation for MIT Press, 2015a. URL http://www.iro.umontreal.ca/~bengioy/dlbook", "author": ["Yoshua Bengio", "Ian J. Goodfellow", "Aaron Courville"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Class-based n-gram models of natural language", "author": ["Peter F. Brown", "Peter V. Desouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": "Computational Linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Ethical issues in using twitter for public health surveillance and research: developing a taxonomy of ethical concepts from the research literature", "author": ["Mike Conway"], "venue": "Journal of medical Internet research,", "citeRegEx": "Conway.,? \\Q2014\\E", "shortCiteRegEx": "Conway.", "year": 2014}, {"title": "Quantifying mental health signals in Twitter", "author": ["Glen Coppersmith", "Mark Dredze", "Craig Harman"], "venue": "In Proceedings of the ACL Workshop on Computational Linguistics and Clinical Psychology,", "citeRegEx": "Coppersmith et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2014}, {"title": "Measuring post traumatic stress disorder in Twitter", "author": ["Glen Coppersmith", "Craig Harman", "Mark Dredze"], "venue": "In Proceedings of the 8th International AAAI Conference on Weblogs and Social Media (ICWSM),", "citeRegEx": "Coppersmith et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2014}, {"title": "From ADHD to SAD: Analyzing the language of mental health on Twitter through self-reported diagnoses", "author": ["Glen Coppersmith", "Mark Dredze", "Craig Harman", "Kristy Hollingshead"], "venue": "In Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,", "citeRegEx": "Coppersmith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2015}, {"title": "CLPsych 2015 shared task: Depression and PTSD on Twitter", "author": ["Glen Coppersmith", "Mark Dredze", "Craig Harman", "Kristy Hollingshead", "Margaret Mitchell"], "venue": "In Proceedings of the Shared Task for the NAACL Workshop on Computational Linguistics and Clinical Psychology,", "citeRegEx": "Coppersmith et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2015}, {"title": "Exploratory data analysis of social media prior to a suicide attempt", "author": ["Glen Coppersmith", "Kim Ngo", "Ryan Leary", "Tony Wood"], "venue": "In Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,", "citeRegEx": "Coppersmith et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Coppersmith et al\\.", "year": 2016}, {"title": "Discovering shifts to suicidal ideation from mental health content in social media", "author": ["Munmun De Choudhury", "Emre Kiciman", "Mark Dredze", "Glen Coppersmith", "Mrinal Kumar"], "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Choudhury et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Choudhury et al\\.", "year": 2016}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Goldberg.", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "Le and Mikolov.,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Learning multi-faceted representations of individuals from heterogeneous evidence using neural networks", "author": ["Jiwei Li", "Alan Ritter", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1510.05198,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Raichura. Best practices in social media: Utilizing a value matrix to assess social media\u2019s impact on health care", "author": ["Deirdre McCaughey", "Catherine Baumgardner", "Andrew Gaudes", "Dominique LaRochelle", "Kayla Jiaxin Wu", "Tejal"], "venue": "Social Science Computer Review,", "citeRegEx": "McCaughey et al\\.,? \\Q2014\\E", "shortCiteRegEx": "McCaughey et al\\.", "year": 2014}, {"title": "Ethical issues in using twitter for population-level depression monitoring: a qualitative study", "author": ["Jude Mikal", "Samantha Hurst", "Mike Conway"], "venue": "BMC medical ethics,", "citeRegEx": "Mikal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mikal et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality", "author": ["Margaret Mitchell", "Glen Coppersmith", "Kristy Hollingshead", "editors"], "venue": "North American Association for Computational Linguistics,", "citeRegEx": "Mitchell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2015}, {"title": "You are what you tweet: Analyzing twitter for public health", "author": ["Michael J Paul", "Mark Dredze"], "venue": "Icwsm, 20:265\u2013272,", "citeRegEx": "Paul and Dredze.,? \\Q2011\\E", "shortCiteRegEx": "Paul and Dredze.", "year": 2011}, {"title": "The defeat depression campaign: psychiatry in the public arena", "author": ["ES Paykel", "A Tylee", "A Wright", "RG Priest"], "venue": "The American journal of psychiatry,", "citeRegEx": "Paykel et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Paykel et al\\.", "year": 1997}, {"title": "Screening twitter users for depression and ptsd with lexical decision lists", "author": ["Ted Pedersen"], "venue": "In Proceedings of the Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,", "citeRegEx": "Pedersen.,? \\Q2015\\E", "shortCiteRegEx": "Pedersen.", "year": 2015}, {"title": "Linguistic inquiry and word count", "author": ["James W Pennebaker", "Martha E Francis", "Roger J Booth"], "venue": "Liwc", "citeRegEx": "Pennebaker et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Pennebaker et al\\.", "year": 2001}, {"title": "Mental illness detection at the world well-being project for the clpsych 2015 shared task", "author": ["Daniel Preotiuc-Pietro", "Maarten Sap", "H Andrew Schwartz", "LH Ungar"], "venue": "NAACL HLT 2015,", "citeRegEx": "Preotiuc.Pietro et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Preotiuc.Pietro et al\\.", "year": 2015}, {"title": "Sarcasm detection on twitter: A behavioral modeling approach", "author": ["Ashwin Rajadesingan", "Reza Zafarani", "Huan Liu"], "venue": "In Proceedings of the Eighth ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Rajadesingan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajadesingan et al\\.", "year": 2015}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim \u0158eh\u030au\u0159ek", "Petr Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "\u0158eh\u030au\u0159ek and Sojka.,? \\Q2010\\E", "shortCiteRegEx": "\u0158eh\u030au\u0159ek and Sojka.", "year": 2010}, {"title": "Beyond lda: exploring supervised topic modeling for depression-related language in twitter", "author": ["Philip Resnik", "William Armstrong", "Leonardo Claudino", "Thang Nguyen", "Viet-An Nguyen", "Jordan Boyd-Graber"], "venue": "In Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality,", "citeRegEx": "Resnik et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Resnik et al\\.", "year": 2015}, {"title": "Learning representations by back-propagating errors", "author": ["David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams"], "venue": "Cognitive modeling,", "citeRegEx": "Rumelhart et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1988}, {"title": "Towards assessing changes in degree of depression through Facebook", "author": ["H. Andrew Schwartz", "Johannes Eichstaedt", "Margaret L. Kern", "Gregory Park", "Maarten Sap", "David Stillwell", "Michal Kosinski", "Lyle Ungar"], "venue": "In Proceedings of the ACL Workshop on Computational Linguistics and Clinical Psychology,", "citeRegEx": "Schwartz et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2014}, {"title": "Twitter sentiment analysis with deep convolutional neural networks", "author": ["Aliaksei Severyn", "Alessandro Moschitti"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Severyn and Moschitti.,? \\Q2015\\E", "shortCiteRegEx": "Severyn and Moschitti.", "year": 2015}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Noah A Smith", "Jason Eisner"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Smith and Eisner.,? \\Q2005\\E", "shortCiteRegEx": "Smith and Eisner.", "year": 2005}, {"title": "Visualizing data using t-sne", "author": ["Laurens Van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton.,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "Toward socially-infused information extraction: Embedding authors, mentions, and entities", "author": ["Yi Yang", "Ming-Wei Chang", "Jacob Eisenstein"], "venue": "arXiv preprint arXiv:1609.08084,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "User embedding for scholarly microblog recommendation", "author": ["Yang Yu", "Xiaojun Wan", "Xinjie Zhou"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 23, "context": "For example, perhaps half of depressive cases go undetected, in part due to the heterogeneous and complex expression of this condition (Paykel et al., 1997).", "startOffset": 135, "endOffset": 156}, {"referenceID": 18, "context": "The internet may provide a comfortable medium for people to express their feelings anonymously and connect with health-care professionals (McCaughey et al., 2014) and others affected by similar conditions (De Choudhury et al.", "startOffset": 138, "endOffset": 162}, {"referenceID": 22, "context": "Prior work has demonstrated the potential of using social media to investigate mental health issues (Paul and Dredze, 2011), including depression (Schwartz et al.", "startOffset": 100, "endOffset": 123}, {"referenceID": 31, "context": "Prior work has demonstrated the potential of using social media to investigate mental health issues (Paul and Dredze, 2011), including depression (Schwartz et al., 2014), PTSD (Coppersmith et al.", "startOffset": 146, "endOffset": 169}, {"referenceID": 13, "context": ", 2014b) and suicidal ideation (Coppersmith et al., 2016; De Choudhury et al., 2016) in individuals.", "startOffset": 31, "endOffset": 84}, {"referenceID": 15, "context": ", predictive features) from data, freeing practitioners from the burden of manually designing and encoding task-specific features (Bengio et al., 2015a; Goldberg, 2016).", "startOffset": 130, "endOffset": 168}, {"referenceID": 3, "context": "Word embeddings in particular aim to implicitly encode latent word semantics (in the distributional sense), and can be learned in an unsupervised fashion by means of predictive models that exploit word co-occurrence statistics and other regularities in unlabeled corpora (Bengio et al., 2003).", "startOffset": 271, "endOffset": 292}, {"referenceID": 16, "context": "These models have been recently extended to infer representations for larger textual units (Le and Mikolov, 2014), and even user representations (Li et al.", "startOffset": 91, "endOffset": 113}, {"referenceID": 17, "context": "These models have been recently extended to infer representations for larger textual units (Le and Mikolov, 2014), and even user representations (Li et al., 2015; Amir et al., 2016).", "startOffset": 145, "endOffset": 181}, {"referenceID": 0, "context": "These models have been recently extended to infer representations for larger textual units (Le and Mikolov, 2014), and even user representations (Li et al., 2015; Amir et al., 2016).", "startOffset": 145, "endOffset": 181}, {"referenceID": 0, "context": "It has been shown that these user embeddings also capture latent user aspects and can be used in downstream applications, such as sarcasm detection (Amir et al., 2016) and content recommendation (Yu et al.", "startOffset": 148, "endOffset": 167}, {"referenceID": 36, "context": ", 2016) and content recommendation (Yu et al., 2016).", "startOffset": 35, "endOffset": 52}, {"referenceID": 21, "context": "In 2015, the CLPsych workshop held a shared task to foster progress in NLP technologies with potential for applications related to mental health analysis, over social media streams (Mitchell et al., 2015; Coppersmith et al., 2015b).", "startOffset": 181, "endOffset": 231}, {"referenceID": 24, "context": "The proposed systems were based on a wide range of approaches including: rule-based systems leveraging lexical decision lists (Pedersen, 2015), linear classifiers exploiting features based on word clusters and topic models (Preotiuc-Pietro et al.", "startOffset": 126, "endOffset": 142}, {"referenceID": 26, "context": "The proposed systems were based on a wide range of approaches including: rule-based systems leveraging lexical decision lists (Pedersen, 2015), linear classifiers exploiting features based on word clusters and topic models (Preotiuc-Pietro et al., 2015), supervised topic models (Resnik et al.", "startOffset": 223, "endOffset": 253}, {"referenceID": 29, "context": ", 2015), supervised topic models (Resnik et al., 2015) and systems exploiting character-level language models (Coppersmith et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 19, "context": "This data was collected according to the ethical protocol of ?, and follows the recommendations spelled out in Mikal et al. (2016).", "startOffset": 111, "endOffset": 131}, {"referenceID": 32, "context": "Most of the research in social media analysis has been concerned with deriving better models that operate on representations of the texts comprising individual users posts, both via manually crafted features and, more recently, representation learning approaches (Severyn and Moschitti, 2015; Astudillo et al., 2015).", "startOffset": 263, "endOffset": 316}, {"referenceID": 1, "context": "Most of the research in social media analysis has been concerned with deriving better models that operate on representations of the texts comprising individual users posts, both via manually crafted features and, more recently, representation learning approaches (Severyn and Moschitti, 2015; Astudillo et al., 2015).", "startOffset": 263, "endOffset": 316}, {"referenceID": 35, "context": "These include, information extraction (Yang et al., 2016), opinion mining (Tang et al.", "startOffset": 38, "endOffset": 57}, {"referenceID": 2, "context": ", 2015), sarcasm detection (Bamman and Smith, 2015) and content recommendation (Yu et al.", "startOffset": 27, "endOffset": 51}, {"referenceID": 36, "context": ", 2015), sarcasm detection (Bamman and Smith, 2015) and content recommendation (Yu et al., 2016).", "startOffset": 79, "endOffset": 96}, {"referenceID": 2, "context": "The most straightforward approach to induce user representations is by scrapping \u201cprofile\u201d information from social websites, to manually extract features based on social ties, demographic attributes, or posting habits (Bamman and Smith, 2015; Rajadesingan et al., 2015).", "startOffset": 218, "endOffset": 269}, {"referenceID": 27, "context": "The most straightforward approach to induce user representations is by scrapping \u201cprofile\u201d information from social websites, to manually extract features based on social ties, demographic attributes, or posting habits (Bamman and Smith, 2015; Rajadesingan et al., 2015).", "startOffset": 218, "endOffset": 269}, {"referenceID": 15, "context": "Neural Embedding Learning In recent years, models in NLP have moved from discrete word representations, based on scalars representing indices into a pre-defined vocabulary, towards distributed continuous vector word representations that encode latent semantics \u2014 these are usually referred as word embeddings (Goldberg, 2016).", "startOffset": 309, "endOffset": 325}, {"referenceID": 3, "context": "The general framework to learn unsupervised word embeddings involves associating words with parameter vectors, which are then optimized to be good predictors of other words that occur in the same contexts (Bengio et al., 2003).", "startOffset": 205, "endOffset": 226}, {"referenceID": 20, "context": "Skip-Gram (Mikolov et al., 2013), one of the most popular word embedding models, operationalizes this approach by sliding a window of a pre-specified size across the corpus.", "startOffset": 10, "endOffset": 32}, {"referenceID": 17, "context": "Recently proposed methods to learn user representations use essentially the same approach \u2014 associate users with parameter vectors, and optimize these to accurately predict observable attributes or the words used by said user in previous posts (Li et al., 2015; Amir et al., 2016; Yu et al., 2016).", "startOffset": 244, "endOffset": 297}, {"referenceID": 0, "context": "Recently proposed methods to learn user representations use essentially the same approach \u2014 associate users with parameter vectors, and optimize these to accurately predict observable attributes or the words used by said user in previous posts (Li et al., 2015; Amir et al., 2016; Yu et al., 2016).", "startOffset": 244, "endOffset": 297}, {"referenceID": 36, "context": "Recently proposed methods to learn user representations use essentially the same approach \u2014 associate users with parameter vectors, and optimize these to accurately predict observable attributes or the words used by said user in previous posts (Li et al., 2015; Amir et al., 2016; Yu et al., 2016).", "startOffset": 244, "endOffset": 297}, {"referenceID": 2, "context": "The general framework to learn unsupervised word embeddings involves associating words with parameter vectors, which are then optimized to be good predictors of other words that occur in the same contexts (Bengio et al., 2003). Skip-Gram (Mikolov et al., 2013), one of the most popular word embedding models, operationalizes this approach by sliding a window of a pre-specified size across the corpus. At each step, the center word is used to predict the probability of one of the surrounding words, sampled proportionally to the distance to the center word. Le and Mikolov (2014) later expanded this approach with two Paragraph2Vec models that also learn representations for paragraphs (or, more broadly, any sequence of words): (i) PV-dm, tries to predict the center word of the sliding window, given the surrounding words and the paragraph (i.", "startOffset": 206, "endOffset": 581}, {"referenceID": 0, "context": "by Amir et al. (2016), using only the previous posts from a user, were shown to capture latent user aspects (e.", "startOffset": 3, "endOffset": 22}, {"referenceID": 0, "context": "by Amir et al. (2016), using only the previous posts from a user, were shown to capture latent user aspects (e.g. political leanings) and a soft notion of \u2018homophily\u2019 \u2014 similar users were generally represented with similar vectors. Furthermore, these user representations were successfully used to improve a downstream model for sarcasm detection in tweets. Similarly, Yu et al. (2016) used user embeddings to improve a microblog recommendation system.", "startOffset": 3, "endOffset": 386}, {"referenceID": 0, "context": "To learn user embeddings, we adopted an approach similar to that recently proposed by Amir et al. (2016). The idea is to capture relations between users and the content (i.", "startOffset": 86, "endOffset": 105}, {"referenceID": 33, "context": "By learning to discriminate between observed positive examples and pseudo-negative examples, the model shifts probability mass to more plausible observations (Smith and Eisner, 2005).", "startOffset": 158, "endOffset": 182}, {"referenceID": 0, "context": "This formulation is a simplification of Amir et al. (2016) model.", "startOffset": 40, "endOffset": 59}, {"referenceID": 28, "context": "Next, we pre-trained a set of SkipGram word vectors from the task data and another large unlabeled Twitter corpus, using the Gensim7 python package (\u0158eh\u030au\u0159ek and Sojka, 2010).", "startOffset": 148, "endOffset": 174}, {"referenceID": 25, "context": "This aligns with prior work showing that one\u2019s choice of words can be indicative of psychological states and mental health (Pennebaker et al., 2001).", "startOffset": 123, "endOffset": 148}, {"referenceID": 1, "context": "But generic features estimated from unsupervised tasks are suboptimal for downstream tasks (Astudillo et al., 2015).", "startOffset": 91, "endOffset": 115}, {"referenceID": 7, "context": "Neural networks, when trained endto-end, can refine generic embeddings to specific tasks by modifying their parameters during supervised training (Collobert et al., 2011).", "startOffset": 146, "endOffset": 170}, {"referenceID": 1, "context": "But generic features estimated from unsupervised tasks are suboptimal for downstream tasks (Astudillo et al., 2015). Neural networks, when trained endto-end, can refine generic embeddings to specific tasks by modifying their parameters during supervised training (Collobert et al., 2011). However, this strategy requires updating a large number of parameters, which is difficult when only small training datasets are available. The CLPysch dataset comprises 1094 labeled instances (after discarding users with fewer than 100 tweets). Given the modest size of the dataset, we adopted the Astudillo et al. (2015) Non-Linear Subspace Embedding approach, (NLSE), which is able to adapt generic representations to specific tasks with scarce labeled data.", "startOffset": 92, "endOffset": 611}, {"referenceID": 30, "context": "Similar to feed-forward networks, we can use the backpropagation algorithm (Rumelhart et al., 1988) to jointly learn task-specific embeddings and the parameters for the classification layer.", "startOffset": 75, "endOffset": 99}, {"referenceID": 5, "context": "We induced t = 100 topics using Latent Dirichlet Allocation (Blei et al., 2003), to build vectors x \u2208 {0, 1}t indicating the topics present in user\u2019s posts;", "startOffset": 60, "endOffset": 79}, {"referenceID": 6, "context": "We induced k = 1000 Brown et al. (1992) word clusters, to build vectors x \u2208 {0, 1}k mapping words in a user\u2019s posts to their respective clusters;", "startOffset": 20, "endOffset": 40}, {"referenceID": 25, "context": "This is in agreement with prior results from the field of psychology, establishing connections between word usage and mental status (Pennebaker et al., 2001).", "startOffset": 132, "endOffset": 157}], "year": 2017, "abstractText": "Mental illnesses adversely affect a significant proportion of the population worldwide. However, the methods traditionally used for estimating and characterizing the prevalence of mental health conditions are time-consuming and expensive. Consequently, best-available estimates concerning the prevalence of mental health conditions are often years out of date. Automated approaches to supplement these survey methods with broad, aggregated information derived from social media content provides a potential means for near realtime estimates at scale. These may, in turn, provide grist for supporting, evaluating and iteratively improving upon public health programs and interventions. We propose a novel model for automated mental health status quantification that incorporates user embeddings. This builds upon recent work exploring representation learning methods that induce embeddings by leveraging social media post histories. Such embeddings capture latent characteristics of individuals (e.g., political leanings) and encode a soft notion of homophily. In this paper, we investigate whether user embeddings learned from twitter post histories encode information that correlates with mental health statuses. To this end, we estimated user embeddings for a set of users known to be affected by depression and post-traumatic stress disorder (PTSD), and for a set of demographically matched \u2018control\u2019 users. We then evaluated these embeddings with respect to: (i) their ability to capture homophilic relations with respect to mental health status; and (ii) the performance of downstream mental health prediction models based on these features. Our experimental results demonstrate that the user embeddings capture similarities between users with respect to mental conditions, and are predictive of mental health.", "creator": "LaTeX with hyperref package"}}}