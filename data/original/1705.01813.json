{"id": "1705.01813", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2017", "title": "Fast k-means based on KNN Graph", "abstract": "In the era of big data, k-means clustering has been widely adopted as a basic processing tool in various contexts. However, its computational cost could be prohibitively high as the data size and the cluster number are large. It is well known that the processing bottleneck of k-means lies in the operation of seeking closest centroid in each iteration. In this paper, a novel solution towards the scalability issue of k-means is presented. In the proposal, k-means is supported by an approximate k-nearest neighbors graph. In the k-means iteration, each data sample is only compared to clusters that its nearest neighbors reside. Since the number of nearest neighbors we consider is much less than k, the processing cost in this step becomes minor and irrelevant to k. The processing bottleneck is therefore overcome. The most interesting thing is that k-nearest neighbor graph is constructed by iteratively calling the fast $k$-means itself. Comparing with existing fast k-means variants, the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality. As it is tested on 10 million 512-dimensional data, it takes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the same scale of clustering, it would take 3 years for traditional k-means.", "histories": [["v1", "Thu, 4 May 2017 12:27:28 GMT  (165kb,D)", "http://arxiv.org/abs/1705.01813v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["cheng-hao deng", "wan-lei zhao"], "accepted": false, "id": "1705.01813"}, "pdf": {"name": "1705.01813.pdf", "metadata": {"source": "CRF", "title": "Fast k-means based on KNN Graph", "authors": ["Cheng-Hao Deng", "Wan-Lei Zhao"], "emails": [], "sections": [{"heading": null, "text": "Index Terms\u2014fast clustering, k-means, k-nearest neighbor graph\nF"}, {"heading": "1 INTRODUCTION", "text": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4]. Since the general k-means algorithm [5], [6] was proposed in 1982, continuous efforts have been made to search for better solution for this issue. Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc. Among these algorithms, k-means [6] remains popular for its simplicity, efficiency and moderate but stable performance under different contexts. It is known as one of top ten most popular algorithms in data mining [13].\nIn traditional k-means, given a set of n data samples in real d-dimensional space Rd, and an integer k, clustering is modeled as a distortion minimization process. The clustering process partitions n samples into k sets such that to minimize the mean squared distance from each sample to its nearest cluster centroid. It could be formularized as\nmin \u2211\nq(xi)=r\n\u2016 Cr \u2212 xi \u20162, (1)\nwhere xi \u2208 Rd and Cr is the centroid of cluster r. In Eqn. 1, function q(xi) returns the closest centroid (among k centroids) for sample xi. In general, there are two major steps in the k-means iteration. In the first step, each sample is assigned to its closest centroid. In its second step, each centroid Cr is updated by taking the average over assigned samples. The above two steps are repeated until there is no distortion change (Eqn. 1) in two consecutive iterations.\n\u2022 Fujian Key Laboratory of Sensing and Computing for Smart City, and the School of Information Science and Engineering, Xiamen University, Xiamen, 361005, P. R. China. Wan-Lei Zhao is the corresponding author. E-mail: wlzhao@xmu.edu.cn.\nAlthough k-means remains popular, it actually suffers from two major issues. Firstly, it is well-known that k-means only converges to local optima. Recent researches have been working on improving its clustering quality [14], [15], [16]. Thanks to the introduction of incremental optimization strategy in [16], k-means is able to converge to considerably lower distortion.\nThe second issue is mainly about its scalability. Although, the complexity of k-means is linear to the size of input data, the clustering cost could be prohibitively high given both the size of data and the expected number of clusters k are very large. Moreover, according to [17], [18], in its worst case, the running time for k-means could be exponential against the size of input samples. Due to the steady growth of data volume in various forms (web-pages, images, videos, audios and business transactions) on a daily basis, the scalability issue of this traditional algorithm becomes more and more imminent. In each k-means iteration, the most intensive operation is of assigning samples to their closest centroid. As a result, the scalability issue principally is due to the heavy cost of computing nearest centroid for each sample, which is O(n\u00b7d\u00b7k).\nIn recent years, continuous efforts have been devoted to looking for effective solutions that are still workable in webscale data. Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28]. However, most of the k-means variants achieve high speed efficiency while sacrificing the clustering quality. Algorithm presented in [29] demonstrates faster speed and maintains relatively high quality. Unfortunately, a lot of extra memory are required. Specifically, its memory complexity is quadratic to k, which turns out to be unsuitable in the case that k is very large.\nIn this paper, an efficient k-means variant is proposed, in which the k-means clustering process is supported by an approximate k-nearest neighbor graph (KNN graph). The approximate KNN graph is built in its pre-processing step, in which the fast k-means itself and a KNN graph construction process are jointly undertaken. We interestingly\nar X\niv :1\n70 5.\n01 81\n3v 1\n[ cs\n.L G\n] 4\nM ay\n2 01\n7\n2 0 0.1 0.2 0.3 0.4 0.5\n0 50 100 150\nP ro\nba bi\nlit y\nRank\n(a) k-means\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0 50 100 150\nP ro\nba bi\nlit y\nRank\n(b) 2M tree\nFig. 1. The stastitics on co-occurrence rate of one sample and its k - th nearest neighbor that in the same cluster. Experiments have been conducted on SIFT100K [30] with traditional k-means [5] and two-means tree [31]. In the experiment, the size of each cluster is fixed to 50. Note that the probability of two randomly selected samples that fall into the same cluster is only 50/100000 = 0.0005 0.1.\ndiscover that, these two processes could be actually beneficial to each other. This idea is inspired by the following observation\n\u2022 With high probability that one sample and its nearest neighbors reside in the same cluster\nFig. 1 shows the co-occurrence rate of one sample and its \u03ba-th nearest neighbor in one cluster. k-means and its variants two-means tree [31] are tested on SIFT100K [30]. The same trend is observed in both cases. If the samples are closer, the probability that they appear in the same cluster is higher. This probability is much higher than the probability of a random collision.\nWith this observation, we learn that one sample and its nearest neighbors should be arranged into the same cluster. On one hand, this indicates if one sample and its neighbors temporarily are not in the same cluster, it is reasonable to compare one sample only to the clusters where its neighbors reside. Among these clusters, there is probably a true one that all the neighboring samples should live together. On the other hand, from the viewpoint of k-nearest neighbor graph (KNN graph) construction, in order to build KNN list for one sample, it is sufficient to compare one sample to samples reside in the same cluster since its neighbors are most likely reside in the same cluster.\nBased on the above analysis, the scalability issue of kmeans clustering is addressed in two steps in the paper. Firstly, the fast k-means is called to build an approximate KNN graph for itself. Secondly, the fast k-means clustering is undertaken with the support of constructed KNN graph. During the fast k-means iteration, one sample will only compare to the clusters that its top-\u03ba nearest neighbors reside. Usually, the number of nearest neighbors we consider is considerably smaller than the clustering number k. The number of clusters we actually visit is even fewer. As a consequence, significant speed-up is expected. Moreover, as revealed later, the clustering quality drops very little with such kind of speed-up scheme.\nAlthough our primary goal is to speed-up the k-means clustering with the support of KNN graph, we interestingly discover that satisfactory performance is achieved when\nthe constructed KNN graph is applied on the approximate nearest neighbor search (ANNS) tasks. Moreover, comparing with the KNN graph construction algorithms that are specifically designed for approximate nearest neighbor search [32], [33], [34], our algorithm requires much lower computational cost.\nThe remainder of this paper is organized as follows. The reviews about representative works on improving the performance of traditional k-means are presented in Section 2. In Section 3, two important k-means variants are reviewed, which facilitates the discussion of our algorithm in Section 4. Extensive experiment studies over proposed clustering method are presented in Section 5. Section 6 concludes the paper."}, {"heading": "2 RELATED WORKS", "text": ""}, {"heading": "2.1 k -means Variants", "text": "Due to its versatility and simplicity, k-means has been widely adopted in different contexts. In the era of big-data, k-means has been used as a basic tool to process large-scale data of various forms. Unfortunately, as discussed in Section 1, the computational cost could be prohibitively high as the scale of data increases to extraordinarily large, i.e. billion level. Recently, several k-means variants are proposed to either enhance its clustering quality or scalability.\nIn terms of the clustering quality, one of the important work comes from S. Vassilvitskii et al. [14], [21]. The motivation is based on the observation that k-means converges to a better local optima if the initial clustering centroids are carefully selected. According to [14], k-means iteration also converges faster due to the careful selection on the initial cluster centroids. However, in order to adapt the initial centroids to the data distribution, k rounds of scanning over the data are necessary. Although the number of scanning rounds has been reduced to a few in [21], the extra computational cost is still inevitable.\nRecently, a new variant called boost k-means is proposed [16]. The \u201cegg-chicken\u201d loop in k-means has been simplified as a stochastic optimization process, which is also known as incremental k-means [1]. As indicated by extensive experiments, it is able to converge to a considerably better local optima while involving no extra cost. Due to its superior performance, this incremental optimization scheme is adopted in our design. In order to facilitate our discussion, a more detailed review about boost k-means is given in Section 3.\nIn each k-means iteration, the processing bottleneck is the operation of assigning each sample to its closest centroid. The iteration becomes unbearably slow when both the size and the dimension of the data are very large. Noticed that this is a nearest neighbor search problem, Kanungo et al. [35] proposed to index dataset in a KD Tree [36] to speed-up the sample-to-centroid nearest neighbor search. Unfortunately, this is only feasible when the dimension of data is in few tens. Similar scheme has been adopted by Dan et al. [37]. However, due to the curse of dimensionality, this method becomes ineffective when the dimension of data grows to a few hundreds. A recent work [38] takes similar way to speed-up the nearest neighbor search by indexing dataset with inverted file structure. During the iteration,\n3 each centroid is queried against all the indexed data. Attributing to the efficiency of inverted file structure, one to two orders of magnitude speed-up is observed. However, inverted file indexing structure is only effective for sparse vectors.\nAlternatively, the scalability issue of k-means is addressed by subsampling over the dataset during k-means iteration. Namely, methods in [20], [39] only pick a small portion of the whole dataset to update the clustering centroids each time. For the sake of speed efficiency, the number of iterations is empirically set to small value. It is therefore possible that the clustering terminates without a single pass over the whole dataset, which leads to higher speed but also higher clustering distortion. Even though, when coping with high dimensional data in big size, the speed-up achieved by these methods are still limited.\nApart from above methods, the speed-up could be achieved by reducing the comparisons between samples and centroids. In [27], only the \u201cactive points\u201d, which are the samples located on the cluster boundaries, are considered to be swapped between clusters. Comparing with other kmeans variants, it makes a good trade-off between efficiency and clustering quality whereas considerable quality degradation is still inevitable.\nAnother easy way to reduce the number of comparisons between samples and centroids is to conduct the clustering in a top-down hierarchical manner [1], [40], [41]. Specifically, the clustering solution is obtained via a sequence of repeated bisections. The clustering complexity of k-means is reduced from O(t\u00b7k\u00b7n\u00b7d) to O(t\u00b7log(k)\u00b7n\u00b7d) [16]. This is particularly significant when n, d and k are all very large. However, poor clustering performance is achieved in the usual case as it breaks the Lloyd\u2019s condition [16]."}, {"heading": "2.2 K-Nearest Neighbor Graph Construction", "text": "KNN graph is primarily built to support nearest neighbor search [32], [42]. It is also the key data structure in the manifold learning and machine learning, etc [32]. Basically, it tries to find the top-\u03ba nearest neighbors for each data point. When it is built in brute-force way, its time complexity is O(d\u00b7n2), where both d and n could be very large. As a result, it is computationally expensive to build an exact KNN graph. For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution. In [42], an approximate KNN graph is built efficiently by divide-and-conquer strategy. In this algorithm, the original dataset is partitioned into thousands of small subsets by KD trees. KNN list is built by exhaustive comparison within each subset. However, the recall of KNN graph turns out to be very low. Recent works [33], [43] could be viewed as improvements over this work. In 2011, a very successful KNN graph construction algorithm called NN Descent/KGraph [32] has been proposed. This algorithm is proposed based on the observation that \u201ca neighbor of a neighbor is also likely to be a neighbor\u201d. According to [32], its empirical time complexity is onlyO(n1.14). Unfortunately, according to our observation, its recall drops dramatically as the scale of data increases to very large, i.e. 10M. Algorithm presented in [33] faces similar problem.\nIn this paper, a novel KNN graph construction algorithm is proposed and used to support the fast k-means clustering.\nTo the best of our knowledge, this is the first piece of work that KNN graph is used to speed-up k-means clustering. In addition, comparing with other KNN graph construction algorithms, our algorithm is computationally efficient and leads to lowest clustering distortion. Furthermore, when it is applied in ANNS problem, it shows satisfactory performance across different datasets."}, {"heading": "3 PRELIMINARIES", "text": "In order to facilitate our discussions in later sections, two important k-means variants are reviewed, namely, boost k-means (BKM) [16] and two means (2M) tree [31]. As shown later, our speed-up scheme is built upon boost kmeans instead of traditional k-means as the former always produces clusters of higher quality. While two means tree is used to produce initial clusters for its high efficiency."}, {"heading": "3.1 Boost k -means", "text": "As an extension of incremental k-means [1], boost k-means allows the optimization iteration to be feasible for the whole l2 space. Different from other k-means variants, boost kmeans iteration is driven by an explicit objective function. Given clusters Sr=1\u00b7\u00b7\u00b7k, the composite vector of a cluster is defined as Dr = \u2211 xi\u2208Sr xi. The objective function of boost k-means is written as\nI = k\u2211 r=1 D\u2032rDr nr , (2)\nwhich is directly derived from Eqn. 1. With this objective function, the traditional k-means clustering is revised to a stochastic optimization procedure. Each time, one sample is randomly selected and searches for a better re-allocation that leads to highest increase of I . Namely, the variation of function value that is incurred by the possible movement (moving xi from Su to Sv) is given by\n\u2206I(xi) = (Dv + xi)\n\u2032(Dv + xi)\nnv + 1 + (Du \u2212 xi)\u2032(Du \u2212 xi) nu \u2212 1\n\u2212 D \u2032 vDv nv \u2212 D \u2032 uDu nu .\n(3) The optimization process seeks for the movement that \u2206I(xi) is the highest and positive. In particular, the movement of xi from Su to Sv is made as soon as we find the movement is appropriate. According to [16], it is able to converge to a much better local optima in comparison to k-means and its variants. The cost of checking the best movement in boost k-means is equivalent to seeking for closest centroid in traditional k-means. As a result, boost k-means is on the same complexity level as traditional kmeans.\nDue to its superior performance, boost k-means is fully adopted in our design. Particularly, the speed-up we made in this paper is on boost k-means instead of traditional kmeans.\n4"}, {"heading": "3.2 Two Means Tree", "text": "Two means (2M) tree [31] is a variant of hierarchical bisecting k-means. It has been adopted in KNN graph construction for its high speed efficiency [31]. Alg. 1 shows the general procedure of two means tree. Similar as bisecting k-means, the samples are partitioned recursively into two clusters each time until k clusters are produced. Different from bisecting k-means, one more step is taken in the end of each bisecting. The resulting two clusters are adjusted to equal size. Its complexity is the same as bisecting kmeans, namely O(d\u00b7n\u00b7log(n)), which is even faster than one round k-means iteration. In this paper, two means tree is adopted only to generate initial k-means partition. In order to enhance its performance, the aforementioned boost kmeans is integrated in the bisecting operation (Step 8 in Alg. 1). Algorithm 1. TwoMeans(Xn\u00d7d, k)\n1: Input: matrix Xn\u00d7d 2: Output: matrix cLabeln\u00d71 3: t = 1. 4: cLabel[1\u00b7 \u00b7 \u00b7n]\u2190 1; 5: Map cLabel[1\u00b7 \u00b7 \u00b7n] to partition S ; 6: while t < k do 7: Pop Si with largest size out of S 8: Bisect Si into Su and Sv 9: Adjust Su and Sv to equal size\n10: S \u2190 S \u222a Su \u222a Sv 11: t = t+ 1; 12: end while 13: Map S to cLabel[1\u00b7 \u00b7 \u00b7n];\nend\nIn order to facilitate the operations in later steps, the mapping at Line 5 converts cluster labels of samples into cluster set S . At the end of two means tree clustering, cluster set S is mapped back as cluster label representation at Line 13."}, {"heading": "4 KNN GRAPH BASED k -MEANS", "text": "In this section, our solution to the scalability issue of kmeans is presented. Firstly, a general procedure that how boost k-means is undertaken with the support of KNN graph is given. To support fast clustering, the process of KNN graph construction should be sufficiently fast otherwise it becomes another processing bottleneck. To overcome this problem, a novel light-weight KNN graph construction procedure is also introduced."}, {"heading": "4.1 Motivation", "text": "As illustrated in Fig. 2, there is a strong correlation between the closeness of data samples and their membership living in one cluster. This correlation could be interpreted from either the side of k-means clustering or the side of KNN graph construction.\n\u2022 From the clustering side, if the KNN list of each sample is known, clustering is a process of arranging close neighbors into one cluster. As a result, given one sample, the clustering only needs to check with the clusters its \u03ba-nearest neighbors live in. Such\nthat it seeks the approriate cluster to move to. It is therefore no need to check with all k \u2212 1 clusters. As a consequence, the processing bottleneck is overcomed. \u2022 From the KNN graph construction side, if the data samples are already partitioned into small clusters, KNN graph construction is undertaken within each cluster by an exhaustive pair-wise comparison. As a consequence, the KNN graph construction is pulled out in a very efficient manner.\nBased on the first piece of interpretation, we work out the fast k-means algorithm. Similarly, based on the second piece of interpretation, KNN graph construction algorithm is conceived."}, {"heading": "4.2 Fast k -means Driven by KNN Graph", "text": "Given a KNN graph is ready, boost k-means procedure presented in [16] is revised as Alg. 2. At the beginning of the clustering, 2M tree (Alg. 1) is called to produce k clusters. The initial clusters will be incrementally optimized in the later steps. In each step of the optimization iteration, one sample is randomly selected. Thereafter, all the clusters in which its \u03ba neighbors reside are collected. The selected sample is therefore checked with these clusters to seek for the best move. The iteration terminates until convergence condition is reached. Algorithm 2. GK-means(Xn\u00d7d, k, Gn\u00d7\u03ba)\n1: Input: matrix Xn\u00d7d, k, KNN graph Gn\u00d7\u03ba 2: Output: S1, \u00b7 \u00b7 \u00b7, Sr, \u00b7 \u00b7 \u00b7Sk 3: cLabel = TwoMeans(Xn\u00d7d, k); 4: Q\u2190\u2205; 5: while not convergence do 6: for each xi \u2208 X do 7: for j = 1; j \u2264 \u03ba; do 8: b = G[i][j]; 9: Q\u2190 Q \u222a cLabel[b];\n10: j = j + 1; 11: end for 12: Seek v in Q that maximizes \u2206I(xi); 13: if \u2206I(xi) > 0 then 14: Move xi from current cluster to Sv ; 15: end if 16: Q\u2190\u2205; 17: end for 18: end while\nend\nComparing with the procedure presented in boost kmeans [16], there are basically two major modifications. Firstly, the initial clusters are initialized by two means tree, whose complexity is only O(n\u00b7log(k)\u00b7d) [16]. It is considerably faster than traditional k-means initialization. Secondly, as shown from Line 6-12, only clusters that keep the first \u03ba neighbors of xi are visited, the number of which is much smaller than k. Furthermore, it is possible that several neighbors of xi may live in the same cluster. As a consequence, the number of clusters that one sample visits is even smaller than \u03ba.\nAlg. 2 is built upon boost k-means. Alternatively, similar speed-up is also feasible for traditional k-means. To achieve\n5 that, Line 12-15 in Alg. 2 is modified to seeking for the closest centroid from the collected clusters. In Section 5, the performance of this alternative configuration will be presented. As will be revealed, similar speed-up is achieved whereas it shows inferior clustering quality in comparison to the one built upon boost k-means."}, {"heading": "4.3 KNN Graph Construction with Fast k -means", "text": "As discussed in Section 1, samples live in the same cluster are likely to be neighbors. In the KNN graph construction, this clue could be fully exploited. Namely, the search for \u03ba nearest neighbors for one sample is undertaken within the cluster it resides. Based on this principle, the fast KNN graph construction is conceived. Firstly, fast k-means clustering (Line 6, Alg. 2) is called to produce fixed number of clusters. Thereafter, exhaustive comparisons are conducted within each cluster. The new closer point pairs are used to update the KNN graph (Line 7-13, Alg. 3).\nIn order to control the complexity of KNN graph construction on a low level, the cluster size is fixed to a small constant \u03be, i.e. 50. Given the cluster size is fixed to a constant, it is easy to see the cluster number is k0 = bn\u03be c. According to Alg. 2, a KNN graph is required as an input parameter. In our design, a random KNN graph is supplied at the beginning. Since the KNN graph is randomly initialized, one would not expect good cluster partitions returned by Alg. 2 at the beginning. However, as the iteration continues, the quality of KNN graph Gt is enhanced incrementally. Accordingly, the structure of cluster partitions returned by Alg. 2 becomes better. As a result, the structures of KNN graph and the cluster structures evolve alternatively. Fig. 3 illustrates this intertwined evolving process. The iteration parameter \u03c4 controls the final quality of KNN graph. Larger \u03c4 leads to preciser KNN graph while taking higher time cost. Algorithm 3. KNN Graph Construction\n1: Input: Xn\u00d7d: reference set, \u03ba: scale of k-NN Graph 2: Output: KNN Graph Gn\u00d7\u03ba 3: t\u2190 0; 4: Initialize Gtn\u00d7\u03ba with random lists; 5: k0 = bn\u03be c; 6: for t < \u03c4 do 7: S = GK-means(Xn\u00d7d, k0, Gt) 8: for each Sm \u2208 S do 9: for each < i, j >(i<j)\u2208 Sm\u00d7Sm do\n10: if < i, j > is NOT visited then 11: Update Gt[i] and Gt[j] with d(xi, xj); 12: end if 13: end for 14: end for 15: t\u2190 t + 1 16: end for 17: G\u2190 Gt;\nend\nFig. 2 shows the curves of average recall (top-1) of KNN graph and clustering distortion as the functions of \u03c4 . As shown in the figure, at the beginning of this procedure, both the quality of clustering and the quality of KNN graph are very poor. The clustering results are nearly random.\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0 5 10 15 20 25 30 50000\n52000\n54000\n56000\nR ec\nal l\nA ve\nra ge\nD is\nto rt\nio n\n\u03c4\nRecall Distortion\nFig. 2. The correlation between the KNN graph recall and clustering distortion shown as the function of \u03c4 . The experiment is conducted on SIFT100K [30].\nGK-means update KNN Graph\nGt\nS t+1 G\nEnd\nFig. 3. The illustration of the intertwined evolving process in the KNN graph construction.\nCorrespondingly, the average recall of KNN graph is close to 0. However, after only 5 iterations, the clustering distortion drops considerably. In the meantime, the average recall increases to above 0.6.\nIn Alg. 2, there is no specification that which KNN graph construction algorithm should be adopted. As a result, KNN graph supplied by any construction algorithms will achieve similar speed-up. However, as revealed later, the KNN graph algorithm presented in Alg. 3 produces the best clustering quality. Moreover, comparing with other KNN graph construction algorithms [32], [34], [33], Alg. 3 takes much less memory. The extra memory it takes is to keep the KNN graph. Furthermore, it is at least two times faster than NN Descent [32] and small world graph construction [34].\nDue to its low computational cost, Alg. 3 can be also adopted to construct KNN graph for approximate nearest neighbor search. According to our observation, although the quality of KNN graph (measured by recall) is usually lower than that of NN Descent [32], it is able to achieve similar or even better performance than the methods presented in [34], [44]. For instance, it takes less than 3ms to fulfill a query on 100 million SIFTs [30] with its recall above 0.9. The full discussion about ANNS with our KNN graph is beyond the focus of this paper.\nAs a summary, the proposed fast k-means consists of two major steps. In the first step, the fast k-means is called to build an approximate KNN graph for itself. In the second step, the fast k-means is performed again to produce k clusters with the support of the approximate KNN graph. Since the KNN graph is built based on the intermediate\n6\nclustering results in the first step, the information that how the samples are organized as clusters is kept with the KNN graph. The clustering in the second step is therefore guided by such kind of \u201cprior knowledge\u201d. Since this algorithm is based on KNN graph, it is called as graph based k-means (GK-means) from now on."}, {"heading": "4.4 Discussion on Parameters", "text": "In Alg. 2 and Alg. 3, besides the cluster number k, there are additionally three parameters are involved. Parameter \u03c4 in Alg. 2 controls the quality of KNN graph. According to our observation, it is sufficient to set \u03c4 = 10 for clustering task. While if Alg. 3 is called to produce KNN graph for ANNS task, \u03c4 = 10 could be set up-to, i.e. 32. Parameter \u03be controls the size of cluster that is used for KNN graph construction. Larger \u03be leads to better KNN graph quality whereas it also induces more number of pair-wise comparisons. For this reason, a trade-off has to be made. According to our observation, the recommended range of \u03be is [40, 100]. Parameter \u03ba controls the number of neighbors that one sample should consider during the fast k-means clustering. This in turn determines the number of clusters that one sample visits. If only few neighbors are considered, the chance that we miss the true cluster will be high. On the other hand, if too many neighbors are considered during the comparison, a lot of comparisons are required. The speedup over traditional k-means becomes less significant. Again a trade-off has to be made. According to our empirical study, the clustering quality becomes very stable as \u03ba is larger than 40. In our implementation, \u03c4 , \u03be and \u03ba are fixed to 10, 50 and 50 respectively."}, {"heading": "4.5 Complexity Analysis", "text": "In this section, the complexity of Alg. 2 and Alg. 3 is analyzed. As shown above, GK-means (Alg. 2) is comprised by two major parts, namely two means initialization and fast k-means clustering. For the first part, the complexity of 2M tree initialization is O(d\u00b7n\u00b7log(k)) [16]. For the second part, since one sample only visits at most \u03ba clusters in the iteration, the cost of clustering is only d\u00b7n\u00b7\u03ba in each iteration. As a result, the overall complexity isO(d\u00b7n\u00b7log(k)+t\u00b7d\u00b7n\u00b7\u03ba), where t is the number of iterations. From above analysis, it is clear to see that the cluster number k has very minor impact on the clustering complexity.\nThe KNN graph construcion (Alg. 3) consists of two major steps, namely fast k-means clustering and KNN graph refinement. In the clustering step, according to above analysis, its complexity is O(d\u00b7n\u00b7log(n\u03be ) + d\u00b7n\u00b7\u03ba). Noticed that, t is fixed to 1 in the KNN graph construction. In KNN graph refinement step, one sample is compared to around \u03be samples. Therefore, its complexity is O(d\u00b7n\u00b7\u03be). As a result, the complexity of KNN graph construction is O(d\u00b7n\u00b7log(n\u03be )+ \u00b7d\u00b7n\u00b7\u03ba+d\u00b7n\u00b7\u03be), where both \u03be and \u03ba are small constants. Overall, the complexity of the whole procedure is on O(d\u00b7n\u00b7log(n)) level."}, {"heading": "5 EXPERIMENTS ON CLUSTERING TASK", "text": "In this section, the performance of GK-means is studied in comparison to k-means and its representative variants\nsuch as boost k-means (BKM) [16], closure k-means [27] and Mini-Batch [20]. AKM [22] and HKM [45] are not considered as inferior performance to closure k-means is reported in [27] . Before the comparisons with other k-means variants, the performance of GK-means is studied under different configurations. Namely, we try to see how well GK-means is performed when it is built upon traditional kmeans instead of boost k-means. Additionally, GK-means is also tested when the approximate KNN graph is supplied by NN Descent [32]. To do that, we want to search for the best configuration that we can currently set for GK-means.\nWe mainly study the clustering quality and scalability on four large-scale datasets, which are summarized in Tab. 1. The type of data covers from image local features, image global features to vectorized text word features. The dimension of data varies from 100 to 960 dimensions. The scale of all the datasets are above 1 million level. All the methods considered in the paper are implemented in C++ and compiled with GCC 5.4. The simulations are conducted by single thread on a PC with 2.4GHz Xeon CPU and 32G memory setup."}, {"heading": "5.1 Evaluation Protocol", "text": "Similar as [16], [26], the average distortion (or mean squared error [30]) is adopted to evaluate the clustering quality. Basically, it is the average distance between samples and their cluster centroid, which is given in Eqn. 4. As seen from the equation, it is nothing more than taking the average over k-means objective function Eqn. 1. The lower the distortion value, the better quality of the clustering result. This measure is the same as within-cluster sum of squared distortions (WCSSD) in [27].\nE = \u2211 q(xi)=r \u2016 Cr \u2212 xi \u20162\nn . (4)\nIn order to study the relation between the quality of KNN graph and the quality of clustering result, the average recall of KNN graph is also considered. In our evaluation, only the recall of top-1 nearest neighbor is measured. For SIFT1M dataset, the ground-truth of KNN graph is produced by brute-force search, which takes more than 20 hours. While for VLAD10M dataset, it is too costly to produce the ground-truth for the whole set, the recall is therefore estimated by only considering nearest neighbors of 100 randomly selected samples."}, {"heading": "5.2 Configuration Test", "text": "In this section, different configurations on Alg. 2 are tested. As we discussed in Section 4, Alg. 2 could be supported by other KNN graph construction algorithm. In addition, we also pointed out that similar speed-up scheme in Alg. 2 is feasible for traditional k-means. In this section, three\n7 40000 40200 40400 40600 40800 41000\n0.6 0.7 0.8 0.9 1\nA ve\nra ge\nD is\nto rt\nio n\nRecall\nKGraph+GK-means GK-means GK-means-\nFig. 4. Configuration test on Alg. 2. Alg. 2 is tested with the support of KGraph [32]. In addition, Alg. 2 is modified to doing clustering with traditional k-means.\ndifferent configurations are tested. In the first run, Alg. 2 is supplied with KNN graph from NN Descent [32], which is denoted as \u201cKGraph+GK-means\u201d run. In the second run, Alg. 2 is modified to being built upon traditional k-means, which is denoted as \u201cGK-means\u2212\u201d run. In the standard setup run \u201cGK-means\u201d, the clustering is built upon boost kmeans. For both \u201cGK-means\u2212\u201d and \u201cGK-means\u201d, the KNN graph is supplied by Alg. 3. The experments are conducted on SIFT1M dataset. For all the runs, the cluster number is fixed to 10,000.\nFig. 4 shows the distortion trend of these three configurations when KNN graphs of different qualities (reflected by their recall) are supplied. Basically, for all the configurations, higher KNN graph quality leads to steadily lower clustering distortion. When the KNN graphs are on the same recall level, GK-means built upon boost k-means shows much lower clustering distortion. Furthermore, GK-means converges to slightly lower distortion when KNN graph is supplied by Alg. 3. Comparing with KNN graph supplied by NN Descent, KNN graph from Alg. 3 carries the information that how samples should be roughly organized as clusters since KNN list is built in Alg. 3 based on clustering results. In the following, due to its superior performance, the run \u201cGK-means\u201d is selected as the standard configuration of Alg. 2 for further comparison."}, {"heading": "5.3 Clustering Quality", "text": "In this section, the clustering quality of GK-means is studied in comparison to k-means, boost k-means (BKM), closure kmeans and Mini-Batch. The quality of fast k-means clustering is measured by studying the trend of clustering distortion against the number of iterations. Experiments are conducted on datasets SIFT1M, Glove1M and GIST1M. The cluster number is fixed to 10,000 in all the experiments. Fig. 5(a), (c) and (e) show the trend of clustering distortion as the function of iteration for datasets SIFT1M, Glove and GIST1M respectively. While Fig. 5(b), (d) and (f) show the trend of clustering distortion as the function of clustering time for the algorithms that make a relatively good trade-off between efficiency and quality. The performance of k-means, boost k-means and Mini-Batch are not presented due to their\n40000\n41000\n42000\n43000\n44000\n45000\n20 40 60 80 100 120 140 160\nA ve\nra ge\nD is\nto rt\nio n\nIteration\nSIFT1M\nMini-Batch closure k-means k-means BKM KGraph+GK-means GK-means\n(a) distortion vs. iteration\n40000\n40500\n41000\n41500\n42000\n0 500 1000 1500 2000 2500 3000 3500 4000\nA ve\nra ge\nD is\nto rt\nio n\ntime (s)\nSIFT1M\nclosure k-means KGraph+GK-means GK-means\n(b) distortion vs. time\n20\n20.5\n21\n21.5\n22\n22.5\n23\n23.5\n24\n0 20 40 60 80\nA ve\nra ge\nD is\nto rt\nio n\nIteration\nGlove1M\nMini-Batch closure k-means k-means BKM KGraph+GK-means GK-means\n(c) distortion vs. iteration\n20\n20.5\n21\n21.5\n22\n22.5\n23\n23.5\n24\n0 500 1000 1500 2000 2500 3000 3500 4000\nA ve\nra ge\nD is\nto rt\nio n\ntime (s)\nGlove1M\nclosure k-means KGraph+GK-means GK-means\n(d) distortion vs. time\n0.95\n1\n1.05\n1.1\n1.15\n1.2\n0 20 40 60 80\nA ve\nra ge\nD is\nto rt\nio n\nIteration\nGIST1M\nclosure k-means k-means BKM KGraph+GK-means GK-means\n(e) distortion vs. iteration\n0.96\n0.98\n1\n1.02\n1.04\n1.06\n1.08\n1.1\n0 2000 4000 6000 8000 10000 A\nve ra\nge D\nis to\nrt io\nn\nTime (s)\nGIST1M\nclosure k-means KGraph+GK-means GK-means\n(f) distortion vs. time\nFig. 5. Average distortion as a function of iteration times (shown in (a), (c) and (e)) and as a function of running time (shown in (b), (d) and (f)).\nefficiency or distortion (notably Mini-Batch) are not on the same level as GK-means and closure k-means.\nAs shown from Fig. 5(a), (c) and (e), for all the methods except Mini-Batch, the clustering distortion changes very little after 30 iterations. Boost k-means always demonstrates the best performance in terms of clustering quality. In most of the cases, GK-means shows only slightly lower clustering quality than boost k-means. On SIFT1M and GIST1M, it even outperforms traditional k-means. KGraph+GK-means achieves similar performance as GKmeans across all datasets. However, it is around 2 times slower since it is more costly to construct KNN graph by NN Descent. GK-means shows highest efficiency under all tests. Overall, GK-means offers a much better trade-off between efficiency and clustering quality among all the existing kmeans variants.\n8"}, {"heading": "5.4 Scalability Test on Image Clustering", "text": "In this section, the scalability of GK-means is tested on VLAD10M. In the test, the number of iterations for all kmeans variants is fixed to 30.\nIn the first experiment, clustering methods are tested in the way that the scale of input images varies from 10K to 10M. For data in different scales, they are clustered into fixed number of clusters, i.e., 1,024. The time costs for all the methods are presented in Fig. 6(a). Accordingly, the average distortion of all the methods are presented in Fig. 7(a).\nAs shown from Fig. 6(a), GK-means is constantly faster than closure k-means and at least 10 times faster than kmeans and boost k-means. In the mean time, as shown in Fig. 7, clustering quality of GK-means is close to boost kmeans across different scales of input data. In contrast, although Mini-Batch demonstrates fastest speed in this test, its clustering quality turns out to be very poor under different settings as is shown in Fig. 7(a).\nIn addition, the scalability of clustering methods is tested in the way that the number of clusters varies from 1,024 to 8,192, while the scale of input data is fixed to 1 million. Fig. 6(b) shows the time cost of all 5 methods. Accordingly, the average distortion from these methods are given in Fig. 7(b). As shown in the figure, for k-means, boost k-means and Mini-Batch clustering methods, the time cost increases linearly as the number of clusters increases. Mini-Batch is no longer efficient as k increases. In contrast, the time cost of closure k-means and GK-means remains nearly constant across different cluster numbers. In terms of clustering\nquality, as seen from Fig. 7(b), GK-means demonstrates similar quality as boost k-means and it is considerably better than closure k-means, Mini-Batch and k-means. A clear trend is observed from Fig. 7(b), methods based on boost k-means shows increasingly higher performance than the rest as k grows. Overall, clustering driven by the proposed optimization process shows higher speed and better quality. The highest speed is achieved by GK-means, for which only 18 minutes are required to cluster 1 million 512-dimensional data into 8,192 clusters.\nAnother more challenging scalability test is also conducted, in which VLAD10M is partitioned into 1 million clusters. Two workable algorithms in such case, namely closure k-means and GK-means are tested. For GK-means, besides the standard configuration, GK-means that KNN graph is supplied by NN Descent is also tested, which is denoted as \u201cKGraph+GK-means\u201d. Their performance is shown in Tab. 2. For GK-means and KGraph+GK-means, the recall level of the approximate KNN graph is also reported. As shown in the table, compared to closure kmeans, the runs from GK-means show significantly lower clustering distortion. In particular, GK-means with standard configuration shows the lowest clustering distortion. Similar as the experiments in Section 5.2, GK-means shows better performance when the KNN graph is supplied by Alg. 3. KNN graph provided by Alg. 3 keeps the information of intermediate clustering structures. Such kind of information will be transferred to the clustering process. It is therefore able to produce better quality even though the recall of its KNN graph is lower than that of NN Descent. GKmeans also achieves the highest speed efficiency in such a challenging test. According to our estimation, it would take more than 3 years to fulfill the same task for traditional kmeans."}, {"heading": "6 CONCLUSION", "text": "In this paper, we have presented our solution to the scalability issue of k-means. We show that fast k-means clustering is achievable with the support of an approximate KNN graph. Specifically, in the k-means iteration, one sample only needs to compare with clusters that its nearest neighbors live in. The clustering complexity is therefore irrelevant to clustering number. As shown in the paper, hundreds to thousands times speed-up is achieved in particular in the case that both n and k are very large. In addition, since the fast k-means is built upon boost k-means, it also shows very high clustering quality. Overall, the proposed GKmeans shows considerably better trade-off between clustering quality and efficiency over existing solutions. Moreover, the beauty of this algorithm also lies in the design of fast KNN graph construction process. The KNN graph is built\n9 by calling GK-means itself in an intertwined evolving process. In the process, the KNN graph and k-means clustering are incrementally optimized. This intertwined self-evolving process could be generalized as an unsupervised learning framework, which will be our future research work."}, {"heading": "ACKNOWLEDGEMENT", "text": "This work is supported by National Natural Science Foundation of China under grants 61572408."}], "references": [{"title": "Empirical and theoretical comparisons of selected criterion functions for document clustering", "author": ["Y. Zhao", "G. Karypis"], "venue": "Machine Learning, vol. 55, pp. 311\u2013331, Jun. 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Aggregating local descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "J. S\u00e1nchez", "P. P\u00e9rez", "C. Schmid"], "venue": "Trans. PAMI, vol. 34, pp. 1704\u20131716, Sep. 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Web scale photo hash clustering on a single machine", "author": ["Y. Gong", "M. Pawlowski", "F. Yang", "L. Brandy", "L. Bourdev", "R. Fergus"], "venue": "CVPR, pp. 19\u201327, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Video google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "ICCV, Oct. 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory, vol. 28, pp. 129\u2013137, Mar. 1982.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["MacQueen", "James"], "venue": "Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, vol. 1, pp. 281\u2013297, 1967.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1967}, {"title": "Mean shift, mode seeking, and clustering", "author": ["Y. Cheng"], "venue": "Trans. PAMI, vol. 17, pp. 790\u2013799, Aug. 1995.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1995}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H. peter Kriegel", "J. Sander", "X. Xu"], "venue": "IEEE Transactions on Knowledge and Data Engineering, pp. 226\u2013231, 1996.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1996}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computin, vol. 17, pp. 395\u2013416, Aug. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Clustering millions of faces by identity", "author": ["C. Otto", "D. Wang", "A. Jain"], "venue": "Trans. PAMI, pp. 1\u201314, Mar. 2017.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Birch: an efficient data clustering method for very large databases", "author": ["T. Zhang", "R. Ramakrishnan", "M. Livny"], "venue": "Proceedings of the 1996 ACM SIGMOD International Conference on Management of Data, vol. 25, pp. 103\u2013114, Jun. 1996.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Clustering by fast search and find of density peaks", "author": ["A. Rodriguez", "A. Laio"], "venue": "Science, vol. 344, no. 6191, pp. 1492\u20131496, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "P.S. Yu", "Z.-H. Zhou", "M. Steinbach", "D.J. Hand", "D. Steinberg"], "venue": "Knowledge and Information System, vol. 14, pp. 1\u201337, Dec. 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the Eighteenth Annual ACM- SIAM Symposium on Discrete Algorithms, pp. 1027\u20131035, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "KDD Workshop on Text Mining, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Boost k-means", "author": ["W.-L. Zhao", "C.-H. Deng", "C.-W. Ngo"], "venue": "arXiv preprint arXiv:1610.02483, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "NIPS, pp. 10\u201318, Dec. 2009.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "k-means requires exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "Discrete and Computational Geometry, vol. 45, pp. 596\u2013 616, Mar. 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Trans. PAMI, vol. 24, no. 7, pp. 881\u2013 892, 2002.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "Proceedings of the 19th international conference on World wide web, pp. 1177\u20131178, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable k-means++", "author": ["B. Bahmani", "B. Moseley", "A. Vattani", "R. Kumar", "S. Vassilvitskii"], "venue": "Proceedings of the VLDB Endowment, vol. 5, no. 7, pp. 622\u2013633, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Object retrieval with large vocabularies and fast spatial matching", "author": ["J. Philbin", "O. Chum", "M. Isard", "J. Sivic", "A. Zisserman"], "venue": "CVPR, pp. 1\u20138, Jun. 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Scalable k-means by ranked retrieval", "author": ["A. Broder", "L. Garcia-Pueyo", "V. Josifovski", "S. Vassilvitskii", "S. Venkatesan"], "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pp. 233\u2013242, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantize and conquer: A dimensionality-recursive solution to clustering, vector quantization, and image retrieval", "author": ["Y. Avrithis"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pp. 3024\u20133031, Dec. 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximate gaussian mixtures for large scale vocabularies", "author": ["Y. Avrithis", "Y. Kalantidis"], "venue": "ECCV, pp. 15\u201328, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Web-scale image clustering revisited", "author": ["Y. Avrithis", "Y. Kalantidis", "E. Anagnostopoulos", "I.Z. Emiris"], "venue": "ICCV, pp. 1502\u20131510, Dec. 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast approximate k-means via cluster closures", "author": ["J. Wang", "J. Wang", "Q. Ke", "G. Zeng", "S. Li"], "venue": "CVPR, pp. 3037\u20133044, Jun. 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Fast and accurate k-means for large datasets", "author": ["A. Meyerson", "A. Wong"], "venue": "NIPS, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Using the triangle inequality to accelerate", "author": ["C. Elkan"], "venue": "ICML, 2003.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "Trans. PAMI, vol. 33, pp. 117\u2013128, Jan. 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Which spatial partition trees are adaptive to intrinsic dimension", "author": ["N. Verma", "S. Kpotufe", "S. Dasgupta"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 565\u2013574, Jun. 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient k-nearest neighbor graph construction for generic similarity measures", "author": ["W. Dong", "C. Moses", "K. Li"], "venue": "WWW, pp. 577\u2013 586, Mar. 2011.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2011}, {"title": "EFANNA: An extremely fast approximate nearest neighbor search algorithm based on knn graph", "author": ["C. Fu", "D. Cai"], "venue": "arXiv preprint arXiv:1609.07228, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs", "author": ["Y.A. Malkov", "D. Yashunin"], "venue": "arXiv preprint arXiv:1603.09320, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "An efficient k-means clustering algorithm: analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Trans. PAMI, vol. 24, pp. 881\u2013892, Jun. 2002.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Commun. ACM, vol. 18, pp. 509\u2013517, Sep. 1975.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1975}, {"title": "Accelerating exact k-means algorithms with geometric reasoning", "author": ["D. Pelleg", "A. Moore"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 277\u2013281, Aug. 1999.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1999}, {"title": "Scalable k-means by ranked retrieval", "author": ["A. Broder", "L. Garcia-Pueyo", "V. Josifovski", "S. Vassilvitskii", "S. Venkatesan"], "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pp. 233\u2013242, Feb. 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and exact out-of-core kmeans clustering", "author": ["A. Goswami", "R. Jin", "G. Agrawal"], "venue": "Fourth IEEE International Conference on Data Mining, pp. 83\u201390, Nov. 2004.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical clustering algorithms for document datasets", "author": ["Y. Zhao", "G. Karypis"], "venue": "Data Mining and Knowledge Discovery, vol. 10, pp. 141\u2013168, Mar. 2005.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast approximate knn graph construction for high dimensional data via recursive lanczos bisection", "author": ["J. Chen", "H. ren Fang", "Yousef"], "venue": "Journal of Machine Learning Research, vol. 10, pp. 1989\u20132012, Dec. 2009.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1989}, {"title": "Scalable k-nn graph construction for visual descriptors", "author": ["J. Wang", "J. Wang", "G. Zeng", "Z. Tu", "R. Gan", "S. Li"], "venue": "CVPR, pp. 1106\u2013 1113, Jun. 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Approximate nearest neighbor search on high dimensional data\u2014experiments, analyses, and improvement", "author": ["W. Li", "Y. Zhang", "Y. Sun", "W. Wang", "W. Zhang", "X. Lin"], "venue": "arXiv preprint arXiv:1610.02455, 2016.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["M. Muja", "D.G. Lowe"], "venue": "Trans. PAMI, vol. 36, pp. 2227\u20132240, Nov. 2014.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2014}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "IJCV, vol. 60, pp. 91\u2013110, Nov. 2004.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2004}, {"title": "YFCC100M: The new data in multimedia research", "author": ["B. Thomee", "D.A. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L.-J. Li"], "venue": "Communications of the ACM, vol. 59, pp. 64\u2013 73, Feb. 2016.  10", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pp. 1532\u20131543, 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Rapid biologically-inspired scene classification using features shared with visual attention", "author": ["C. Siagian", "L. Itti"], "venue": "Trans. PAMI, vol. 29, pp. 300\u2013312, Feb 2007.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4].", "startOffset": 112, "endOffset": 115}, {"referenceID": 2, "context": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4].", "startOffset": 143, "endOffset": 146}, {"referenceID": 3, "context": "Clustering problems arise from a wide variety of applications such as knowledge discovery [1], data compression [2], large-scale image linking [3] and visual vocabulary construction [4].", "startOffset": 182, "endOffset": 185}, {"referenceID": 4, "context": "Since the general k-means algorithm [5], [6] was proposed in 1982, continuous efforts have been made to search for better solution for this issue.", "startOffset": 36, "endOffset": 39}, {"referenceID": 5, "context": "Since the general k-means algorithm [5], [6] was proposed in 1982, continuous efforts have been made to search for better solution for this issue.", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 135, "endOffset": 139}, {"referenceID": 10, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "Various algorithms have been proposed in the last two decades, such as mean shift [7], DB-SCAN [8], spectral clustering [9], RankOrder [10] BIRCH [11] and Clusterdp [12], etc.", "startOffset": 165, "endOffset": 169}, {"referenceID": 5, "context": "Among these algorithms, k-means [6] remains popular for its simplicity, efficiency and moderate but stable performance under different contexts.", "startOffset": 32, "endOffset": 35}, {"referenceID": 12, "context": "It is known as one of top ten most popular algorithms in data mining [13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "Recent researches have been working on improving its clustering quality [14], [15], [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "Recent researches have been working on improving its clustering quality [14], [15], [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "Recent researches have been working on improving its clustering quality [14], [15], [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "Thanks to the introduction of incremental optimization strategy in [16], k-means is able to converge to considerably lower distortion.", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "Moreover, according to [17], [18], in its worst case, the running time for k-means could be exponential against the size of input samples.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "Moreover, according to [17], [18], in its worst case, the running time for k-means could be exponential against the size of input samples.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 31, "endOffset": 35}, {"referenceID": 20, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 61, "endOffset": 65}, {"referenceID": 25, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 67, "endOffset": 71}, {"referenceID": 26, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "Representative works are [19], [20], [21], [22], [23], [24], [25], [26], [27], [28].", "startOffset": 79, "endOffset": 83}, {"referenceID": 28, "context": "Algorithm presented in [29] demonstrates faster speed and maintains relatively high quality.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "Experiments have been conducted on SIFT100K [30] with traditional k-means [5] and two-means tree [31].", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "Experiments have been conducted on SIFT100K [30] with traditional k-means [5] and two-means tree [31].", "startOffset": 74, "endOffset": 77}, {"referenceID": 30, "context": "Experiments have been conducted on SIFT100K [30] with traditional k-means [5] and two-means tree [31].", "startOffset": 97, "endOffset": 101}, {"referenceID": 30, "context": "k-means and its variants two-means tree [31] are tested on SIFT100K [30].", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "k-means and its variants two-means tree [31] are tested on SIFT100K [30].", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "Moreover, comparing with the KNN graph construction algorithms that are specifically designed for approximate nearest neighbor search [32], [33], [34], our algorithm requires much lower computational cost.", "startOffset": 134, "endOffset": 138}, {"referenceID": 32, "context": "Moreover, comparing with the KNN graph construction algorithms that are specifically designed for approximate nearest neighbor search [32], [33], [34], our algorithm requires much lower computational cost.", "startOffset": 140, "endOffset": 144}, {"referenceID": 33, "context": "Moreover, comparing with the KNN graph construction algorithms that are specifically designed for approximate nearest neighbor search [32], [33], [34], our algorithm requires much lower computational cost.", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "[14], [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[14], [21].", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "According to [14], k-means iteration also converges faster due to the careful selection on the initial cluster centroids.", "startOffset": 13, "endOffset": 17}, {"referenceID": 20, "context": "Although the number of scanning rounds has been reduced to a few in [21], the extra computational cost is still inevitable.", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Recently, a new variant called boost k-means is proposed [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "The \u201cegg-chicken\u201d loop in k-means has been simplified as a stochastic optimization process, which is also known as incremental k-means [1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 34, "context": "[35] proposed to index dataset in a KD Tree [36] to speed-up the sample-to-centroid nearest neighbor search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[35] proposed to index dataset in a KD Tree [36] to speed-up the sample-to-centroid nearest neighbor search.", "startOffset": 44, "endOffset": 48}, {"referenceID": 36, "context": "[37].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "A recent work [38] takes similar way to speed-up the nearest neighbor search by indexing dataset with inverted file structure.", "startOffset": 14, "endOffset": 18}, {"referenceID": 19, "context": "Namely, methods in [20], [39] only pick a small portion of the whole dataset to update the clustering centroids each time.", "startOffset": 19, "endOffset": 23}, {"referenceID": 38, "context": "Namely, methods in [20], [39] only pick a small portion of the whole dataset to update the clustering centroids each time.", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "In [27], only the \u201cactive points\u201d, which are the samples located on the cluster boundaries, are considered to be swapped between clusters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Another easy way to reduce the number of comparisons between samples and centroids is to conduct the clustering in a top-down hierarchical manner [1], [40], [41].", "startOffset": 146, "endOffset": 149}, {"referenceID": 39, "context": "Another easy way to reduce the number of comparisons between samples and centroids is to conduct the clustering in a top-down hierarchical manner [1], [40], [41].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "The clustering complexity of k-means is reduced from O(t\u00b7k\u00b7n\u00b7d) to O(t\u00b7log(k)\u00b7n\u00b7d) [16].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "However, poor clustering performance is achieved in the usual case as it breaks the Lloyd\u2019s condition [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": "KNN graph is primarily built to support nearest neighbor search [32], [42].", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "KNN graph is primarily built to support nearest neighbor search [32], [42].", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "It is also the key data structure in the manifold learning and machine learning, etc [32].", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution.", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution.", "startOffset": 36, "endOffset": 40}, {"referenceID": 40, "context": "For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution.", "startOffset": 42, "endOffset": 46}, {"referenceID": 41, "context": "For this reason, recent works [32], [33], [42], [43] aim to search for an approximate but efficient solution.", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "In [42], an approximate KNN graph is built efficiently by divide-and-conquer strategy.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "Recent works [33], [43] could be viewed as improvements over this work.", "startOffset": 13, "endOffset": 17}, {"referenceID": 41, "context": "Recent works [33], [43] could be viewed as improvements over this work.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "In 2011, a very successful KNN graph construction algorithm called NN Descent/KGraph [32] has been proposed.", "startOffset": 85, "endOffset": 89}, {"referenceID": 31, "context": "According to [32], its empirical time complexity is onlyO(n).", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": "Algorithm presented in [33] faces similar problem.", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "In order to facilitate our discussions in later sections, two important k-means variants are reviewed, namely, boost k-means (BKM) [16] and two means (2M) tree [31].", "startOffset": 131, "endOffset": 135}, {"referenceID": 30, "context": "In order to facilitate our discussions in later sections, two important k-means variants are reviewed, namely, boost k-means (BKM) [16] and two means (2M) tree [31].", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "As an extension of incremental k-means [1], boost k-means allows the optimization iteration to be feasible for the whole l2 space.", "startOffset": 39, "endOffset": 42}, {"referenceID": 15, "context": "According to [16], it is able to converge to a much better local optima in comparison to k-means and its variants.", "startOffset": 13, "endOffset": 17}, {"referenceID": 30, "context": "Two means (2M) tree [31] is a variant of hierarchical bisecting k-means.", "startOffset": 20, "endOffset": 24}, {"referenceID": 30, "context": "It has been adopted in KNN graph construction for its high speed efficiency [31].", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "Given a KNN graph is ready, boost k-means procedure presented in [16] is revised as Alg.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "Comparing with the procedure presented in boost kmeans [16], there are basically two major modifications.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Firstly, the initial clusters are initialized by two means tree, whose complexity is only O(n\u00b7log(k)\u00b7d) [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 29, "context": "The experiment is conducted on SIFT100K [30].", "startOffset": 40, "endOffset": 44}, {"referenceID": 31, "context": "Moreover, comparing with other KNN graph construction algorithms [32], [34], [33], Alg.", "startOffset": 65, "endOffset": 69}, {"referenceID": 33, "context": "Moreover, comparing with other KNN graph construction algorithms [32], [34], [33], Alg.", "startOffset": 71, "endOffset": 75}, {"referenceID": 32, "context": "Moreover, comparing with other KNN graph construction algorithms [32], [34], [33], Alg.", "startOffset": 77, "endOffset": 81}, {"referenceID": 31, "context": "Furthermore, it is at least two times faster than NN Descent [32] and small world graph construction [34].", "startOffset": 61, "endOffset": 65}, {"referenceID": 33, "context": "Furthermore, it is at least two times faster than NN Descent [32] and small world graph construction [34].", "startOffset": 101, "endOffset": 105}, {"referenceID": 31, "context": "According to our observation, although the quality of KNN graph (measured by recall) is usually lower than that of NN Descent [32], it is able to achieve similar or even better performance than the methods presented in [34], [44].", "startOffset": 126, "endOffset": 130}, {"referenceID": 33, "context": "According to our observation, although the quality of KNN graph (measured by recall) is usually lower than that of NN Descent [32], it is able to achieve similar or even better performance than the methods presented in [34], [44].", "startOffset": 219, "endOffset": 223}, {"referenceID": 42, "context": "According to our observation, although the quality of KNN graph (measured by recall) is usually lower than that of NN Descent [32], it is able to achieve similar or even better performance than the methods presented in [34], [44].", "startOffset": 225, "endOffset": 229}, {"referenceID": 29, "context": "For instance, it takes less than 3ms to fulfill a query on 100 million SIFTs [30] with its recall above 0.", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "For the first part, the complexity of 2M tree initialization is O(d\u00b7n\u00b7log(k)) [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 29, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 55, "endOffset": 58}, {"referenceID": 45, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 69, "endOffset": 73}, {"referenceID": 46, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 82, "endOffset": 86}, {"referenceID": 46, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 127, "endOffset": 131}, {"referenceID": 47, "context": "SIFT1M [30] 1M 128 SIFT [46] VLAD10M [16] 10M 512 VLAD [2] from YFCC [47] Glove1M [48] 1M 100 Vectorized text word [48] GIST1M [30] 1M 960 GIST [49]", "startOffset": 144, "endOffset": 148}, {"referenceID": 15, "context": "such as boost k-means (BKM) [16], closure k-means [27] and Mini-Batch [20].", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "such as boost k-means (BKM) [16], closure k-means [27] and Mini-Batch [20].", "startOffset": 50, "endOffset": 54}, {"referenceID": 19, "context": "such as boost k-means (BKM) [16], closure k-means [27] and Mini-Batch [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 21, "context": "AKM [22] and HKM [45] are not considered as inferior performance to closure k-means is reported in [27] .", "startOffset": 4, "endOffset": 8}, {"referenceID": 43, "context": "AKM [22] and HKM [45] are not considered as inferior performance to closure k-means is reported in [27] .", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "AKM [22] and HKM [45] are not considered as inferior performance to closure k-means is reported in [27] .", "startOffset": 99, "endOffset": 103}, {"referenceID": 31, "context": "Additionally, GK-means is also tested when the approximate KNN graph is supplied by NN Descent [32].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Similar as [16], [26], the average distortion (or mean squared error [30]) is adopted to evaluate the clustering quality.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "Similar as [16], [26], the average distortion (or mean squared error [30]) is adopted to evaluate the clustering quality.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "Similar as [16], [26], the average distortion (or mean squared error [30]) is adopted to evaluate the clustering quality.", "startOffset": 69, "endOffset": 73}, {"referenceID": 26, "context": "This measure is the same as within-cluster sum of squared distortions (WCSSD) in [27].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": "2 is tested with the support of KGraph [32].", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "2 is supplied with KNN graph from NN Descent [32], which is denoted as \u201cKGraph+GK-means\u201d run.", "startOffset": 45, "endOffset": 49}], "year": 2017, "abstractText": "In the era of big data, k-means clustering has been widely adopted as a basic processing tool in various contexts. However, its computational cost could be prohibitively high as the data size and the cluster number are large. It is well known that the processing bottleneck of k-means lies in the operation of seeking closest centroid in each iteration. In this paper, a novel solution towards the scalability issue of k-means is presented. In the proposal, k-means is supported by an approximate k-nearest neighbors graph. In the k-means iteration, each data sample is only compared to clusters that its nearest neighbors reside. Since the number of nearest neighbors we consider is much less than k, the processing cost in this step becomes minor and irrelevant to k. The processing bottleneck is therefore overcome. The most interesting thing is that k-nearest neighbor graph is constructed by iteratively calling the fast k-means itself. Comparing with existing fast k-means variants, the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality. As it is tested on 10 million 512-dimensional data, it takes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the same scale of clustering, it would take 3 years for traditional k-means.", "creator": "LaTeX with hyperref package"}}}