{"id": "1705.02269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2017", "title": "Sequential Attention: A Context-Aware Alignment Function for Machine Reading", "abstract": "In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (Who did What and CNN datasets) and show that it dramatically improves a strong baseline like the Stanford Reader. The resulting model is competitive with the state of the art.", "histories": [["v1", "Fri, 5 May 2017 15:37:11 GMT  (816kb,D)", "http://arxiv.org/abs/1705.02269v1", "4 pages"], ["v2", "Mon, 26 Jun 2017 22:25:55 GMT  (428kb,D)", "http://arxiv.org/abs/1705.02269v2", "To appear in ACL 2017 2nd Workshop on Representation Learning for NLP. Contains additional experiments in section 4 and a revised Figure 1"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian brarda", "philip yeres", "samuel r bowman"], "accepted": false, "id": "1705.02269"}, "pdf": {"name": "1705.02269.pdf", "metadata": {"source": "CRF", "title": "Sequential Attention", "authors": ["Sebastian Brarda", "Philip Yeres", "Samuel R. Bowman"], "emails": ["sb5518@nyu.edu", "pcy214@nyu.edu", "bowman@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Soft attention (Bahdanau et al., 2014), a differentiable method for selecting the inputs for a component of a model from a set of possibilities, has been crucial to the success of artificial neural network models for natural language understanding tasks like reading comprehension that take short passages as inputs. However, standard approaches to attention in NLP select words with only very indirect consideration of their context, limiting their effectiveness. This paper presents a method to address this by adding explicit context sensitivity into the soft attention scoring function.\nWe demonstrate the effectiveness of this approach on the task of cloze-style reading comprehension. A problem in the cloze style consists of a passage p, a question q and an answer a drawn from among the entities mentioned in the passage. In particular, we use the CNN dataset (Hermann et al., 2015), which introduced the task into widespread use in evaluating neural networks for language understanding, and the newer and more carefully quality-controlled Who did What dataset (Onishi et al., 2016).\nIn standard approaches to soft attention over passages, a scoring function is first applied to every word in the source text to evaluate how closely that word matches a locally-fixed query vector (here, a function of the question). The resulting scores are then normalized and used as the weights in a weighted sum which produces an output or context vector summarizing the most salient words of the input, which is then used in a downstream model (here, to select an answer). In this work we propose a novel scoring function for soft attention that we call sequential attention, shown in Figure 1. In a Sequential Attention model, a modified version of the bilinear attention scoring function is used to produce a score vector for each word in the source text. A newly added bidirectional RNN then consumes those score vectors and uses them to produce a context-aware scalar score for each word. We evaluate this scoring function within the context of the Stanford Reader (Chen et al., 2016), and show that it yields dramatic improvements in performance. On both datasets, it is outperformed only by the Gated Attention Reader (Dhingra et al., 2016), which in some cases has access to features not explicitly seen by our model. ar X iv :1 70 5. 02 26 9v 1\n[ cs\n.C L\n] 5\nM ay\n2 01\n7"}, {"heading": "2 Related Work", "text": "In addition to Chen et al. (2016)\u2019s Stanford Reader model, there have been several other modeling approaches developed to address these reading comprehension tasks.\nSeo et al. (2016) introduced the Bi-Directional Attention Flow which consists of a multi-stage hierarchical process to represent context at different levels of granularity; it use the concatenation of passage word representation, question word representation, and the element-wise product of these vectors in their attention flow layer. This is a more complex variant of the classic bilinear term that multiplies this concatenated vector with a vector of weights, producing attention scalars. Dhingra et al. (2016) recently proposed a Gated-Attention Reader which integrates a multihop structure with a novel attention mechanism, essentially building query specific representations of the tokens in the document to improve prediction. This model conducts a classic dot-product soft attention to weight the query representations which are then multiplied element-wise with the context representations, and fed into the next layer of RNN. After several hidden layers that repeat the same process, the dot product between the context representation and the query is used to compute a classic soft-attention.\nOutside the task of reading comprehension there has been other work on soft attention over text, largely focusing on the problem of attending over single sentences. Luong et al. (2015) study several issues in the design of soft attention models in the context of translation, and propose the use of a bilinear scoring function. In work largely concurrent to our own, Kim et al. (2017) explore the use of conditional random fields (CRFs) to impose a variety of constraints on attention distributions achieving strong results on several sentence level tasks."}, {"heading": "3 Modeling", "text": "Given the tuple (passage, question, answer), our goal is to predict Pr(a|d, q) where a refers to answer, d to passage, and q to question. We define the words of each passage and question as d = d1, .., dm and q = q1, ..., ql, respectively, where exactly one qi contains the token @placeholder, representing a blank that can be correctly filled in by the answer. With calibrated probabilities Pr(a|d, q), we take the argmaxa Pr(a|d, q)\nwhere possible a are restricted to the subset of anonymized entity symbols present in d. In this section, we present two models for this distribution: the Stanford Reader and our extended Sequential Attention model."}, {"heading": "3.1 Stanford Reader", "text": "Encoding Each word of the vocabulary, including anonymized entities, is mapped to a ddimensional vector via embedding matrix E \u2208 Rd\u00d7|V |. For simplicity, we denote the vectors of the passage and question as d = d1, .., dm and q = q1, ..., ql, respectively. The Stanford Reader (Chen et al., 2016) uses bidirectional GRUs (Cho et al., 2014) to encode the passage and questions. For the passage, the hidden state is defined: hi = concat( \u2212\u2192 hi, \u2190\u2212 hi). Where contextual embeddings di of each word in the passage are encoded in both directions. For the question, the last hidden representation of each direction is concatenated: j = concat( \u2212\u2192 jl , \u2190\u2212 j1 ).\nAttention and answer selection In this case, a bilinear attention layer (Luong et al., 2015) is used, where matrix Ws is a learned parameter:\n\u03b1i = softmaxi(jWshi);o = \u2211 \u03b1ihi (1)\nThe prediction is\na = argmax a\u2208p\u2229entities\nW Ta o (2)\nmeaning the word with highest probabilities within the anonymized entities. Finally a softmax layer is added on top of W Ta o with a negative loglikelihood objective for training."}, {"heading": "3.2 Sequential Attention", "text": "Our model uses the idea of attention input-feeding (Luong et al., 2015) in a different way. Luong et al. (2015) feed their original attention vectors1,\ns\u0303i = tanh(Wc[ci;hi]) (3)\ninto the next RNN step by concatenating them with the previous hidden state. The goal is to make the model fully aware of the previous alignment choices.\nIn the Sequential Attention model instead of getting a single value \u03b1i for each word in the passage by using a bilinear term, we define the vectors \u03b3i\n1Where hi is the hidden representation and ci is the context representation\nwith a pseudo-bilinear term2. Instead of doing the dot product as in the bilinear term, we conduct an element wise multiplication to end up with a vector instead of a scalar:\n\u03b3i = j \u25e6Wshi (4)\nWe then feed the \u03b3i vectors into a new bidirectional GRU layer to get the hidden attention \u03b7i vector representation.\n\u2190\u2212\u03b7i = GRU(\u2190\u2212\u2212\u03b7i+1,\u03b3i);\u2212\u2192\u03b7i = GRU(\u2212\u2212\u2192\u03b7i\u22121,\u03b3i) (5) We concatenated the directional \u03b7 vectors to be consistent with the structure of previous layers.\n\u03b7i = concat(\u2212\u2192\u03b7i ,\u2190\u2212\u03b7i ) (6)\nFinally, we compute the \u03b1 weights as below (where sum[] is the sum of the elements of a vector) and proceed as before.\n\u03b1i = softmaxi(sum[\u03b7i]) (7)"}, {"heading": "4 Experiments and Results", "text": "We evaluate our model on two tasks, CNN and Who did What (WDW). For CNN, we used the anonymized version of the dataset released by Hermann et al. (2015), containing 380,298 training, 3,924 dev, and 3,198 test examples. For WDW we used Onishi et al. (2016)\u2019s data generation script to reproduce their WDW data, yielding 127,786 training, 10,000 dev, and 10,000 test examples.3 We used the strict version of WDW which is a more difficult task and is used for their leaderboard.4\nTraining We implemented all our models in Theano (Theano Development Team, 2016) and Lasagne (Dieleman et al., 2015) and used the Stanford Reader (Chen et al., 2016) open source implementation as a reference. We largely used the same hyperparameters as Chen et al. (2016) in\n2Note that doing softmax over the sum of the terms of the \u03b3i vectors would lead to the same \u03b1i of the Stanford Reader.\n3In the WDW data we found 340 examples in the strict training set, 545 examples in the relaxed training set, 20 examples in the test set, and 30 examples in the validation set that were not answerable because the anonymized answer entity did not exist in the passage. We removed these examples, reducing the size of the WDW test set by 0.2%, to 9,980. We believe this difference is not significant and did not bias the comparison between models.\n4https://tticnlp.github.io/who_did_ what/leaderBoard.html\nthe Stanford Reader: |V | = 50K, embedding size d = 100, 100 dimensional GloVe (Pennington et al., 2014) word embeddings5 for initialization, hidden size h = 128. The size of the hidden layer of the bidirectional RNN used to encode the attention vectors is double the size of the one that encodes the words, since it receives vectors that result from the concatenation of GRUs that go in both directions, \u03b7 = 256. Attention and output parameters were initialized from a U \u223c (\u22120.01, 0.01) while GRU weights were intialized from a N \u223c (0, 0.1). Learning was carried out with SGD with a learning rate of 0.1, batch size of 32, gradient clipping of norm 10 and dropout of 0.2 in all the vertical layers6 (including the Sequential Attention layer). Also, all the anonymized entities were relabeled according to the order of occurrence, as in the Stanford Reader. We trained all models for 30 epochs."}, {"heading": "4.1 Results", "text": "Who did What In our experiments, Stanford Reader got a 65.6% accuracy on the strict WDW dataset compared to the 64% that Onishi et al. (2016) they reported. Sequential Attention Reader got 67.21%, which achieves the second position in the leaderboard, only surpassed by the 71.2% from the Gated Attention Reader with qe-comm (Li et al., 2016) features and fixed GloVe embeddings. However, GA reader without features and fixed embeddings performs significantly lower at 67%. We did not use these kind of features in our experiments, so it is likely that our model performs even better with those in future work. We experimented with fixed embeddings but the results were\n5The GloVe word vectors used were pretrained with 6 billion tokens with an uncased vocab of 400K words, and were obtained from Wikipedia 2014 and Gigaword 5.\n6We also tried increasing the hidden size to 200, using 200d GloVe word representations and increasing the dropout rate to 0.3. Finally we increased the number of hidden encoding layers to two. None of these changes resulted in significant performance improvements in accordance with Chen et al. (2016).\nconsiderably worst. Another experiment we conducted was to add 100K training samples from CNN to the WDW data. This increase in the training data size boosted accuracy by 1.4% with the SR and 1.8% with the Sequential Attention model reaching a 69% accuracy. This improvement strongly suggests that the gap in performance/difficulty between the CNN and the WDW datasets is partially related to the difference in the training sizes which results in overfitting.\nCNN For a final sanity check and a fair comparison against a well known benchmark, we ran our model with the Sequential Attention mechanism on exactly the same CNN data used by Chen et al. (2016).\nOur Sequential Attention model took an average of 2X more time per epoch to train vs. the Stanford Reader. However, our model converged in only 17 epochs vs. 30 for the SR. The results of training the SR on CNN where slightly lower than the 73.6% reported by Chen et al. (2016). Our model achieved an accuracy of 77.1% which, to the best of our knowledge, has only been surpassed by Dhingra et al. (2016) with their GatedAttention Reader model."}, {"heading": "4.2 Discussion", "text": "The difference between our Sequential Attention and the classical Bilinear Attention is that we are conserving the distributed representation of the Bilinear similarity at each component and propagating that contextual information to attention over other words. In other words, when the Bilinear Attention layer is computing: \u03b1i = softmaxi(jWshi), it only cares about the absolute value of the resulting \u03b1i (the amount of attention that it gives to that word). Whereas if we keep the vector \u03b3i we can also know which were the dimensions of the distributed representation of the atten-\ntion that weighted in that decision. Furthermore, if we use that information to feed a new GRU, it helps the model to learn how to assign attention to surrounding words.\nCompared to Sequential Attention (SA), Bidirectional attention flow uses a considerably more complex architecture with a query representations for each word in the question. Unlike the Gated Attention Reader, SA does not require intermediate soft attention and it makes use of the advantages of the pseudo-bilinear term with only one additional RNN layer. Furthermore, in SA no dot product is required to compute attention, only the sum of the elements of the \u03b7 vector. SA\u2019s simpler architecture performs close to the state-of-the-art.\nFigure 2 shows some sample model behavior. In this example and elsewhere, Sequential Attention results in less sparse attention vectors, and this helps the model assign attention not only to potential target strings (anonymized entities) but also to relevant contextual words that are related to those entities. This ultimately leads to richer semantic representations o = \u2211 \u03b1ihi of the passage."}, {"heading": "5 Conclusion and Discussion", "text": "In this this paper we created a novel and simple model with a Sequential Attention mechanism that performs near the state-of-the-art on the CNN and WDW datasets by improving the bilinear attention mechanism with an additional bi-directional RNN layer. This additional layer allows the flow of attentional information of context to compute the attentional weight for each token.\nFor future work we would like to run our model on other datasets such as SQuAD and Marco. Also, we think that some elements of the Sequential Attention model could be mixed with ideas applied in recent research from Dhingra et al. (2016) and Seo et al. (2016). We also believe that the Sequential Attention mechanism may benefit other tasks as well, such as neural machine translation."}, {"heading": "Acknowledgements", "text": "This paper was the result of a term project for the NYU Course DS-GA 3001, Natural Language Understanding with Distributed Representations. Bowman acknowledges support from a Google Faculty Research Award."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D. Manning."], "venue": "CoRR abs/1606.02858. http://arxiv.org/abs/1606.02858.", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "CoRR abs/1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov."], "venue": "CoRR abs/1606.01549. http://arxiv.org/abs/1606.01549.", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Lasagne: First release. https://doi.org/10.5281/zenodo.27878", "author": ["man", "Gbor Takcs", "Peter de Rivaz", "Jon Crall", "Gregory Sanders", "Kashif Rasul", "Cong Liu", "Geoffrey French", "Jonas Degrave"], "venue": null, "citeRegEx": "man et al\\.,? \\Q2015\\E", "shortCiteRegEx": "man et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "CoRR abs/1506.03340. http://arxiv.org/abs/1506.03340.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Structured attention networks", "author": ["Yoon Kim", "Carl Denton", "Luong Hoang", "Alexander M. Rush."], "venue": "CoRR abs/1702.00887. http://arxiv.org/abs/1702.00887.", "citeRegEx": "Kim et al\\.,? 2017", "shortCiteRegEx": "Kim et al\\.", "year": 2017}, {"title": "Dataset and neural recurrent sequence labeling model for open-domain factoid question answering", "author": ["Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu."], "venue": "CoRR abs/1607.06275. http://arxiv.org/abs/1607.06275.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "CoRR abs/1508.04025. http://arxiv.org/abs/1508.04025.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Who did what: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David A. McAllester."], "venue": "CoRR abs/1608.05457. http://arxiv.org/abs/1608.05457.", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Min Joon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "CoRR abs/1611.01603. http://arxiv.org/abs/1611.01603.", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "CoRR abs/1605.02688. http://arxiv.org/abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Soft attention (Bahdanau et al., 2014), a differentiable method for selecting the inputs for a component of a model from a set of possibilities, has been crucial to the success of artificial neural network models for natural language understanding tasks like reading comprehension that take short passages as inputs.", "startOffset": 15, "endOffset": 38}, {"referenceID": 5, "context": "In particular, we use the CNN dataset (Hermann et al., 2015), which introduced the task into widespread use in evaluating neural networks for language understanding, and the newer and more carefully quality-controlled Who did What dataset (Onishi et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 9, "context": ", 2015), which introduced the task into widespread use in evaluating neural networks for language understanding, and the newer and more carefully quality-controlled Who did What dataset (Onishi et al., 2016).", "startOffset": 186, "endOffset": 207}, {"referenceID": 1, "context": "We evaluate this scoring function within the context of the Stanford Reader (Chen et al., 2016), and show that it yields dramatic improvements in performance.", "startOffset": 76, "endOffset": 95}, {"referenceID": 3, "context": "On both datasets, it is outperformed only by the Gated Attention Reader (Dhingra et al., 2016), which in some cases has access to features not explicitly seen by our model.", "startOffset": 72, "endOffset": 94}, {"referenceID": 1, "context": "In addition to Chen et al. (2016)\u2019s Stanford Reader model, there have been several other modeling approaches developed to address these reading comprehension tasks.", "startOffset": 15, "endOffset": 34}, {"referenceID": 3, "context": "Dhingra et al. (2016) recently proposed a Gated-Attention Reader which integrates a multihop structure with a novel attention mechanism, essentially building query specific representations of the tokens in the document to improve predic-", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": "Luong et al. (2015) study several issues in the design of soft attention models in the context of translation, and propose the use of a bilinear scoring function.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "In work largely concurrent to our own, Kim et al. (2017) explore the use of conditional random fields (CRFs) to impose a variety of constraints on attention distributions achieving strong results on several sentence level tasks.", "startOffset": 39, "endOffset": 57}, {"referenceID": 1, "context": "The Stanford Reader (Chen et al., 2016) uses bidirectional GRUs (Cho et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 2, "context": ", 2016) uses bidirectional GRUs (Cho et al., 2014) to encode the passage and questions.", "startOffset": 32, "endOffset": 50}, {"referenceID": 8, "context": "Attention and answer selection In this case, a bilinear attention layer (Luong et al., 2015) is used, where matrix Ws is a learned parameter:", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "Our model uses the idea of attention input-feeding (Luong et al., 2015) in a different way.", "startOffset": 51, "endOffset": 71}, {"referenceID": 8, "context": "Our model uses the idea of attention input-feeding (Luong et al., 2015) in a different way. Luong et al. (2015) feed their original attention vectors1,", "startOffset": 52, "endOffset": 112}, {"referenceID": 5, "context": "anonymized version of the dataset released by Hermann et al. (2015), containing 380,298 training, 3,924 dev, and 3,198 test examples.", "startOffset": 46, "endOffset": 68}, {"referenceID": 5, "context": "anonymized version of the dataset released by Hermann et al. (2015), containing 380,298 training, 3,924 dev, and 3,198 test examples. For WDW we used Onishi et al. (2016)\u2019s data generation script to reproduce their WDW data, yielding 127,786 training, 10,000 dev, and 10,000 test examples.", "startOffset": 46, "endOffset": 171}, {"referenceID": 1, "context": ", 2015) and used the Stanford Reader (Chen et al., 2016) open source implementation as a reference.", "startOffset": 37, "endOffset": 56}, {"referenceID": 1, "context": ", 2015) and used the Stanford Reader (Chen et al., 2016) open source implementation as a reference. We largely used the same hyperparameters as Chen et al. (2016) in", "startOffset": 38, "endOffset": 163}, {"referenceID": 10, "context": "the Stanford Reader: |V | = 50K, embedding size d = 100, 100 dimensional GloVe (Pennington et al., 2014) word embeddings5 for initialization, hidden size h = 128.", "startOffset": 79, "endOffset": 104}, {"referenceID": 7, "context": "2% from the Gated Attention Reader with qe-comm (Li et al., 2016) features and fixed GloVe embeddings.", "startOffset": 48, "endOffset": 65}, {"referenceID": 8, "context": "6% accuracy on the strict WDW dataset compared to the 64% that Onishi et al. (2016) they reported.", "startOffset": 63, "endOffset": 84}, {"referenceID": 1, "context": "None of these changes resulted in significant performance improvements in accordance with Chen et al. (2016).", "startOffset": 90, "endOffset": 109}, {"referenceID": 1, "context": "on exactly the same CNN data used by Chen et al. (2016).", "startOffset": 37, "endOffset": 56}, {"referenceID": 1, "context": "6% reported by Chen et al. (2016). Our model achieved an accuracy of 77.", "startOffset": 15, "endOffset": 34}, {"referenceID": 1, "context": "6% reported by Chen et al. (2016). Our model achieved an accuracy of 77.1% which, to the best of our knowledge, has only been surpassed by Dhingra et al. (2016) with their GatedAttention Reader model.", "startOffset": 15, "endOffset": 161}, {"referenceID": 3, "context": "Also, we think that some elements of the Sequential Attention model could be mixed with ideas applied in recent research from Dhingra et al. (2016) and Seo et al.", "startOffset": 126, "endOffset": 148}, {"referenceID": 3, "context": "Also, we think that some elements of the Sequential Attention model could be mixed with ideas applied in recent research from Dhingra et al. (2016) and Seo et al. (2016). We also believe that the Sequential Attention mechanism may benefit other tasks as well, such as neural machine translation.", "startOffset": 126, "endOffset": 170}], "year": 2017, "abstractText": "In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (Who did What and CNN) and show that it dramatically improves a strong baseline like the Stanford Reader. The resulting model is competitive with the state of the art.", "creator": "LaTeX with hyperref package"}}}