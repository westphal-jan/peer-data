{"id": "1705.03597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "Solving Multi-Objective MDP with Lexicographic Preference: An application to stochastic planning with multiple quantile objective", "abstract": "In most common settings of Markov Decision Process (MDP), an agent evaluate a policy based on expectation of (discounted) sum of rewards. However in many applications this criterion might not be suitable from two perspective: first, in risk aversion situation expectation of accumulated rewards is not robust enough, this is the case when distribution of accumulated reward is heavily skewed; another issue is that many applications naturally take several objective into consideration when evaluating a policy, for instance in autonomous driving an agent needs to balance speed and safety when choosing appropriate decision. In this paper, we consider evaluating a policy based on a sequence of quantiles it induces on a set of target states, our idea is to reformulate the original problem into a multi-objective MDP problem with lexicographic preference naturally defined. For computation of finding an optimal policy, we proposed an algorithm \\textbf{FLMDP} that could solve general multi-objective MDP with lexicographic reward preference.", "histories": [["v1", "Wed, 10 May 2017 03:13:30 GMT  (10kb)", "http://arxiv.org/abs/1705.03597v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yan li", "zhaohan sun"], "accepted": false, "id": "1705.03597"}, "pdf": {"name": "1705.03597.pdf", "metadata": {"source": "CRF", "title": "Solving Multi-Objective MDP with Lexicographic Preference: An application to stochastic planning with multiple quantile objective", "authors": ["Yan Li", "Zhaohan Sun"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n03 59\n7v 1\nIn most common settings of Markov Decision Process (MDP), an agent evaluate a policy based on expectation of (discounted) sum of rewards. However in many applications this criterion might not be suitable from two perspective: first, in risk aversion situation expectation of accumulated rewards is not robust enough, this is the case when distribution of accumulated reward is heavily skewed; another issue is that many applications naturally take several objective into consideration when evaluating a policy, for instance in autonomous driving an agent needs to balance speed and safety when choosing appropriate decision. In this paper, we consider evaluating a policy based on a sequence of quantiles it induces on a set of target states, our idea is to reformulate the original problem into a multi-objective MDP problem with lexicographic preference naturally defined. For computation of finding an optimal policy, we proposed an algorithm FLMDP that could solve general multi-objective MDP with lexicographic reward preference."}, {"heading": "1 Introduction", "text": "The most classical MDP problem consider maximizing a scalar reward\u2019s expectation [3], however in many situation a single scalar objective is not enough to represent an agent\u2019s objective. For example, in self-autonomous driving one need to balance speed and safety [2]. A common approach is to use a weight vector and scalarization function to project the multi-objective function to single objective problem. However in practice it is hard to evaluate and analyze the projected problem since there might be many viable Pareto optimal solutions to the original problem [1]. On the other hand,\nin some cases, an agent might have explicit preference over the objectives, that is, an agent might expect to optimize the higher priority objective over the lower priority ones when finding optimal policy. For example, in autonomous driving an agent would consider safety the highest priority, placing speed in the second place.\nSeveral previous studies have considered such multi-objective problem with lexicographical order. Using a technique called Ordinal dynamic programming, Mitten [5] assumed a specific preference ordering over outcomes for a finite horizon MDP; Sobel [7] extended this model to infinite horizon MDPs. Ordinal dynamic programming has been explored under reinforcement learning. Wray et.al [2] also consider a more general setting when lexicographical order depends on initial state and slack for higher objective value is allowed for improvement over lower priority objective. In their paper they proposed an algorithm called LVI that tries to approximate optimal policy in infinite horizon setting, although work empirically well, the algorithm lacks theoretical guarantee, in fact, the performance could be arbitrarily worse if the MDP is adversarially designed.\nEven in the setting that an agent indeed has only one reward, the expectation of accumulated reward is not always suitable. This is the case when the agent is risk aversion, for instance in financial market an institutional fund would like to design an auto-trading system that maximize certain lower quantile. The essential idea of such strategy is to improve the worst case situation as much as possible. Based on this motivation, Hugo and Weng [8] proposed quantile based reinforcement learning algorithm which seeks to optimize certain lower/upper quantile on the random outcome. In their paper they define a set of end states in finite horizon setting, let P\u03c0(\u00b7) be the probability distribution induced by policy \u03c0 on the end states, they seek to find the optimal policy in the sense that the \u03c4 -lower quantile of P\u03c0(\u00b7) is maximized. Note that their objective could be improved by following observation:\n1. Among all the policy that achieve the optimal \u03c4 -lower quantile, a refined class of policy could be chosen in the sense that following such policy, the probability of ending at a state the is less preferable than the optimal quantile state is minimized.\n2. Suppose \u03c41 < \u03c42, then after finding policy class that maximize \u03c41-quantile, one can further find policy that maximize \u03c42-quantile in this policy class. For situation when multiple \u03c4i-quantile are to be optimized, we can find optimal policy by repeating the same procedure iteratively.\nIn general, if \u03c41 < \u03c42 < . . . < \u03c4L are in consideration, we have a multi-quantileobjective MDP, in this paper, we showed a proper way to transfer this problem into a pure multi-objective MDP with lexicographic preference. To tackle computation of an optimal policy, we will introduce an algorithm called FLMDP that not only solve our multi-quantile-objective MDP, but also generalize multi-objective MDP with finite states, action, and horizon. Generalization to infinite states or actions to find \u01eb-optimal policy could be done fairly easy with small modification in our algorithm."}, {"heading": "2 Problem Definition", "text": "We consider finite horizon problem here, a multi-objective Markov Decision Process is described by a tuple (S,A, P,R) where:\n\u2022 S is finite state space.\n\u2022 A is finite action space.\n\u2022 G is finite end state space.\n\u2022 T is finite horizon.\n\u2022 P is transition function given by: P (s, a, s\u2032) = P(s\u2032|s, a), i.e., the probability of transiting from s to s\u2032 after performing action a.\n\u2022 R = [R1, R2, . . . , Rk] is reward vector, with each component Ri(s, a, s \u2032) defining\nreward of starting from state s, performing action a and transit to state s\u2032.\nWithout loss of generality we may assume G = {g1, . . . , gn}. On G we may define our preference as g1 6 g2 6 . . . 6 gn where gi 6 gj denotes gj is preferred over gi. To enforce end state nature of set G, we further define transition probability and reward function have following properties:\nP (g, a, g) = 1, \u2200g \u2208 G, \u2200a \u2208 A\nR(g, a, g) = 0, \u2200g \u2208 G, \u2200a \u2208 A\nThat is, whenever the current state is in set G, we remains at state g until process ends at horizon T, in the meantime receiving no rewards at all. To enforce the process ends at one of the end state, we define a special end state g0 = t = T and declare g0 6 gi, \u2200i > 1. Let \u03c0 be any policy, we define the probability distribution P\n\u03c0(\u00b7) induced by \u03c0 induced on set G as P\u03c0(gi) = P\n\u03c0(sT = gi). Then we can further define cumulative distribution function.\nF \u03c0(g) = \u2211\ngi6g\nP \u03c0(gi)\nThe associated \u03c4 -lower quantile is given by:\nq\u03c0 \u03c4 = min{gi : F \u03c0(gi) > \u03c4}"}, {"heading": "Finding Optimal Policy", "text": "Given \u03c41 < . . . < \u03c4L \u2208 [0, 1], following our motivation in Introduction section, our procedure to find optimal policy is a series of optimization procedure, we will show later this could be reshaped into multi-objective MDP with lexicographic preference.\nAlgorithmic Scheme 1\n1. Denote \u03a00 = {all possible policy}.\n2. After finding \u03a0i\u22121, construct \u03a0i:\nq\u22c6 \u03c4i = max \u03c0\u2208\u03a0i\u22121 {q\u03c0 \u03c4i }. \u03a0\u0302i = {\u03c0 \u2208 \u03a0i\u22121 : q \u03c0\n\u03c4i = q\u22c6 \u03c4i }\nThat is, \u03a0\u0302i is the set of policy that maximize the \u03c4i quantile in \u03a0i. Let pi be the \u201dbiggest\u201d state that is \u201dsmaller\u201d than q\u22c6\n\u03c4i . Here \u201dbiggest\u201d and \u201dsmaller\u201d should\nbe interpreted in terms of preference. Then to minimize the probability of ending at a state that is less preferable than q\u22c6\n\u03c4i , we should have \u03a0i as follows:\n\u03a0i = argmin \u03c0\u2208\u03a0\u0302i\nF \u03c0(pi) (1)\n3. Proceed as step 2 until we have found \u03a0L. Then any policy \u03c0 that is in \u03a0L will be our optimal policy.\nNote however it is unclear how to translate such algorithmic scheme into a tractable algorithm, the problem is that we do not know how to properly \u201dchoose\u201d an policy from a policy class. We\u2019ll tackle this issue in the next section."}, {"heading": "3 Multi-Quantile-Objective MDP", "text": "In this section we first present a lemma that generalize the Lemma 1 of Hugo and Weng [8], this lemma fully characterize the q\u22c6\n\u03c4i\nLemma 1. For i = 1, . . . , L, let q\u22c6 \u03c4i and \u03a0i be defined as before, then q \u22c6 \u03c4i satisfies the following condition:\nq\u22c6 \u03c4i = min{g : F \u22c6i (g) > \u03c4i}\nF \u22c6i (g) = min \u03c0\u2208\u03a0i\u22121 F \u03c0(g), \u2200g \u2208 G\nProof. We proof by induction: For i=1: observe that\nF \u22c61 (g) 6 F \u03c0(g), \u2200\u03c0, \u2200g\nThis follows directly from the definition of F \u22c61 (g). Hence the \u03c41-quantile of F \u22c6 1 (g)(denoted as gi1) is greater or equal than q \u03c0 \u03c41 for all \u03c0. Now by the definition of gi1 , we have F \u22c6 1 (gi1) > \u03c41 and F \u22c6 1 (gi1\u22121) < \u03c41. Then by definition of F \u22c61 (), we have \u2203\u03c01, s.t.:\nF \u03c01(gi1\u22121) = F \u22c6 1 (gi1\u22121) < \u03c41\nF \u03c01(gi1) > F \u22c6 1 (gi1) > \u03c41\nThis means that gi1 is \u03c41-quantile of both F \u22c6 1 () and F \u03c01(). Hence we have gi1 > q \u03c0 \u03c41 , \u2200\u03c0, and gi1 = q \u03c01 \u03c41 . Thus q\u22c6 \u03c41 = gi1 by definition of q \u22c6 \u03c41 . Assume the claim holds for i < k: For i = k: observe that\nF \u22c6k (g) 6 F \u03c0(g), \u2200\u03c0 \u2208 \u03a0k\u22121, \u2200g\nHence the \u03c4k-quantile of F \u22c6 k (g)(denoted as gik) is greater or equal than q \u03c0 \u03c4k for all \u03c0 \u2208 \u03a0k\u22121.\nNow by the definition of gik , we have F \u22c6 k (gik) > \u03c4k and F \u22c6 k (gik\u22121) < \u03c4k. Then by definition of F \u22c6k (), we have \u2203\u03c0k \u2208 \u03a0k\u22121, s.t.:\nF \u03c0k(gik\u22121) = F \u22c6 k (gik\u22121) < \u03c4k\nF \u03c0k(gik) > F \u22c6 k (gik) > \u03c4k\nThis means that gik is \u03c4k-quantile of both F \u22c6 k () and F \u03c0k(). Hence we have gik > q \u03c0 \u03c4k , \u2200\u03c0, and gik = q \u03c0k \u03c4k . Thus q\u22c6 \u03c4k = gik by definition of q \u22c6 \u03c4k . By induction, proof complete.\nFollowing the proof of Lemma 1, we could construct \u03a0i as follows:\nAlgorithmic Scheme 2\n1. Let \u03a00={all possible policy}.\n2. Suppose \u03a0i\u22121 has been constructed, then we construct \u03a0i as following:\nq\u22c6 \u03c4i = max \u03c0\u2208\u03a0i\u22121 {q\u03c0 \u03c4i }.\nLet pi the same as before, i.e. pi be the \u201dbiggest state\u201d that is \u201dsmaller\u201d than q\u22c6 \u03c4i . Then we construct \u03a0i as follows:\n\u03a0i = argmin \u03c0\u2208\u03a0i\u22121\nF \u03c0(pi) (2)\nNote that in equation (2) we construct \u03a0i here directly from \u03a0i\u22121 instead of from \u03a0\u0302i in equation (1), the reason here is that by proof of Lemma 1, the policy \u03c0 that minimize F \u03c0(pi) also has q\n\u22c6 \u03c4i as its \u03c4i-quantile.\nSolving the Algorithmic Scheme mentioned before is hard in general, but giving our work before we are now ready to formulate the previous Algorithmic Scheme into a MDP with Lexicographical objective preference. We may now restrict ourself in the setting that q\u22c6\n\u03c41 , q\u22c6 \u03c42 \u00b7 \u00b7 \u00b7 q\u22c6 \u03c4L are known beforehand, and consider the more general case\nlater.\nTo do this, we define reward functions {Ri} L i=1 as follows:\nRi(st, at, st+1) =\n{\n1 if st 6\u2208 G and st+1 = gi, gi > q \u22c6 \u03c4i 0 otherwise (3)\nThen it is easy to verify that E\u03c0[ \u2211T t=0 Ri(st, at, st+1)] = 1\u2212F \u03c0(pi). Hence minimizing F \u03c0(pi) is equivalent to maximizing expected reward of the MDP. Define V \u03c0i = E \u03c0[ \u2211T\nt=1 Ri(st, at, st+1)] the expected total reward corresponding to reward function Ri, then equation (2) becomes as:\n\u03a0i = argmax \u03c0\u2208\u03a0i\u22121\nV \u03c0i (4)\nWe will show in the next subsection, if {q\u22c6 \u03c4i }Li=1 are known, the procedure described in Algorithmic Scheme 2 exactly corresponds to the procedure of solving a multi-objective MDP with lexicographic preference."}, {"heading": "Multi-Objective MDP with Lexicographic Preference", "text": "Definition 1. Recall that a point u\u0304 is lexicographical larger than 0 if ui = 0 for i =1,2 \u00b7 \u00b7 \u00b7 j and uj > 0 for some 1 6 j 6 n, we write u = (u1, u2 \u00b7 \u00b7 \u00b7 un) >l 0. We then define our lexicographical order index as j, which is the first index in the vector that strictly larger than zero. Thus say u\u0304 is lexicographical larger than v\u0304 if u\u0304\u2212 v\u0304 >l 0\nA multi-objective MDP differs from standard MDP that it has reward vector R(s, a) = [R1(s, a), . . . , RL(s, a)] and associated value vector V(s) = [V1(s), . . . , VL(s)], and a preference is defined on the value function associated with different rewards, say V1(s) > V2(s) > . . . > VL(s). Classic multi-objective MDP seeks to find a policy that has Pareto optimal value vector. With lexicographic preference defined on value vectors, we say a policy \u03c0\u22c6 is lexicographic optimal if there is no policy \u03c0 so that V \u03c0(s) >l V \u03c0\u22c6(s).\nIn pure algorithmic scheme, an multi-objective MDP is solved by iteratively finding the optimal policy class for lower priority value function in the optimal policy class for higher priority ones. That is, denote \u03a00={any policy}, \u03a0i+1 is found by:\n\u03a0i+1 = argmax \u03c0\u2208\u03a0i\nV \u03c0i+1(s)\nIn our multi-quantile-objective MDP, (S,A, P,R) is defined as the same as in section 2, the reward functions Ri is defined as in equation (3). With value vector V\n\u03c0 = (V \u03c01 , . . . , V \u03c0 L ), we define lexicographical preference on V\n\u03c0 as defined in definition 1. Then with equation (4) replacing equation (2) in Algorithmic 2, it is easy to see that Algorithmic Scheme 2 now become a procedure of solving multi-objective lexicographic MDP with parameters (S,A, P,R,V)."}, {"heading": "4 Solving Multi-Quantile-Objective MDP", "text": "Solving Multi-Quantile-Objective MDP lies in general situation of solving multi-objective MDP with lexicographical preference. A natural one is to shape the original problem to a sequence of constrained case MDP and solve this sequence of constrained MDP iteratively. In the next subsection we proposed an algorithm that can solve general multi-objective MDP with lexicographic preference directly, thus solving multiquantile-objective MDP here is just a special case."}, {"heading": "Constrained MDP formulation", "text": "The following procedure reshape a multi-objective MDP with lexicographic preference to a sequence of constrained MDP problem.\n1. At step 1, \u03a00={all possible policy}. Optimize objective V \u03c0 1 , V \u22c6 1 = max\u03c0\u2208\u03a00 V \u03c0 1 . 2. At step i, Optimize objective V \u03c0i with constraints:\nminimize \u03c0\nV \u03c0i\nsubject to V \u03c0j > V \u22c6 j , j = 1, . . . , i\u2212 1.\n3. Proceed as in 2 until step L is finished.\nIt is easy to see that at step i the constraints in the optimization procedure naturally restrict the algorithm to search policy in the class that is identical to \u03a0i\u22121, thus correctness of this reshape is guaranteed. Altman [6] has shown that an optimal randomized policy could be found in such constrained MDP, Chen and Feinberg [4] also showed how to find optimal deterministic policy. Note this type of algorithm indeed does unnecessary work by restarting from searching whole policy space in every step. In this next subsection, we design a dynamic programming flavor algorithm that finds an optimal deterministic policy for general lexicographic order MDP."}, {"heading": "Lexicographic Markov Decision Process", "text": "In this subsection we introduce an algorithm FLMDP that solves general lexicographic MDP in finite horizon, in particular it can be used to solve our previous formulated multi-quantile-objective MDP.\nLet V \u03c0i,t: L\u00d7S\u00d7T \u2192 R be the expected reward obtained by using policy \u03c0 in decision epochs t, t+1, \u00b7 \u00b7 \u00b7 T, here, for simplicity, we let reward of end state equals zero, thus V \u03c0i,t can be represented as\nV \u03c0i,t(s) = E \u03c0 st=s[\nT \u2211\nn=t\nRi(sn, an)]\nNote that although in our problem Ri() relates to out next state, we can solve this problem by simply define Ri(st, at) = E[Ri(st, at, st+1)] with expectation taken w.r.t st+1.\nWe first define state value function:\nQ\u03c0i,t(s, a) = Ri(s, a) + \u2211\ns\u2032\u2208S\nPr(s\u2032|s, a)V \u03c0i,t+1(s \u2032)\nThen following the definition in constrained MDP, \u2200t = T, T\u22121 \u00b7 \u00b7 \u00b7 1, and \u2200i = 1, 2 \u00b7 \u00b7 \u00b7L, we define restricted bellman equation operator Bti as\nBtiV \u03c0 i,t(s) = max\na\u2208Ati\u22121\n{Ri(s, a) + \u2211\ns\u2032\u2208S\nPr(s\u2032|s, a)V \u03c0i,t+1(s \u2032)}\nwhere Ati+1(s) = {a \u2208 A t i(s)| max\na\u2032\u2208At i (s)\nQ\u03c0i,t(s, a \u2032) = Q\u03c0i,t(s, a)}\nand At0(s) = A(s)\nAlgorithm 1 Finite-horizon Lexicographic MDP - FLMDP\nInput Ri(s, a), i = 1, 2 \u00b7 \u00b7 \u00b7L Set V \u03c0i,T (s) = 0, \u2200i = 1, 2 \u00b7 \u00b7 \u00b7L, \u2200s \u2208 S for t = T \u2212 1, T \u2212 2 \u00b7 \u00b7 \u00b71 do for i = 1, 2 \u00b7 \u00b7 \u00b7L do V \u03c0i,t(s) = B t iV \u03c0 i,t(s)\nend for \u03c0\u22c6t \u2208 A t L\nend for Output \u03c0\u22c61 , \u03c0 \u22c6 2, \u00b7 \u00b7 \u00b7 , \u03c0 \u22c6 T\nTheorem 1. In our algorithm 1, \u2200t = T \u22121, T \u22122, \u00b7 \u00b7 \u00b7 1, {\u03c0\u22c6t }t6T\u22121 are optimal policy for our Lexicographic MDP problem.\nProof. Before beginning our proof, we need some notations. Recall:\nV \u03c0i,t(s) = E \u03c0 st=s[\nT \u2211\nt\nRi(st, at)]\nV \u22c6i,t(s) = E \u03c0\u22c6 st=s[\nT \u2211\nt\nRi(st, at)]\nV \u03c0 t (s) = [V \u03c0 1,t(s), . . . , V \u03c0 L,t(s)] V \u22c6 t (s) = [V \u22c6 1,t(s), . . . , V \u22c6 L,t(s)]\nwhere {\u03c0\u22c6t } denotes the policy output by Algorithm 1. Then V \u03c0 j,t(s) defines the value function associated with reward Ri, starting a tail problem with initial state s at time t following given policy \u03c0. Note that V\u03c0t (s) is exactly the value vector for full horizon MDP with initial state s. By our specification of reward function Ri, we naturally have V\n\u03c0 T (s) = 0 and V \u22c6 T (s) = 0.\nLet 6l, <l, >l,>l denotes lexicographical order relationship on value vector V \u03c0 t (s). We use backward induction to show that for \u2200\u03c0, and for \u2200 t = 1, . . . , T \u22121, for \u2200 s, we have V\n\u03c0 t (s) 6l V \u22c6 t (s).\nFor t = T \u2212 1, V\u03c0T\u22121(s) 6l V \u22c6 T\u22121(s) is trivial by procedure of our algorithm. A simple induction on i suffice to give a formal proof, we omit the details here.\nSuppose the claim holds for t+ 1, . . . , T \u2212 1, now we proceed to prove the claim holds for t: assume V\u03c0t+1(s) <l V \u22c6 t+1(s)\nV \u03c0i,t+1(s) = V \u22c6 i,t+1(s), i = 1, . . . , it+1 \u2212 1\nV \u03c0it+1,t+1(s) <l V \u22c6 it+1,t+1(s)\nWe next show that V\u03c0t (s) <l V \u22c6 t (s) also holds:\n1. if V \u03c01,t(s) < V \u22c6 1,t(s), then we are done. 2. if V \u03c01,t(s) = V \u22c6 1,t(s), construction of \u03c0\n\u22c6 and value iteration for finite horizon MDP gives us:\nV \u03c01,t(s) = R1(s, \u03c0(s)) + \u2211\nj\nP (s, \u03c0(s), j)V \u03c01,t+1(j)\nV \u22c61,t(s) = max a\nR1(s, a) + \u2211\nj\nP (s, a, j)V \u22c61,t+1(j)\nBy induction hypothesis we have V \u03c01,t+1(j) = V \u22c6 1,t+1(j), then we must have V \u03c0 1,t(s) 6 V \u22c61,t(s). Now since we have equality achieved, by our definition of A1(s) in our algorithm, we must have \u03c0(s) \u2208 A1(s).\n3. We now use induction to show that for if i < it+1 \u2212 1, and\nV \u03c0j,t(s) = V \u22c6 j,t(s), j = 1, . . . , i\nthen we must have \u03c0(s) \u2208 Ai(s) and V \u03c0 i+1,t(s) \u2264 V \u22c6 i+1,t(s). The base case i=1 have been proved in step 2. Suppose the claim holds for i\u2212 1, then for i: By induction hypothesis we have \u03c0(s) \u2208 Ai\u22121(s). Construction of \u03c0\n\u22c6 and value iteration for finite horizon MDP gives us:\nV \u03c0i,t(s) = Ri(s, \u03c0(s)) + \u2211\nj\nP (s, \u03c0(s), j)V \u03c0i,t+1(j) (5)\nV \u22c6i,t(s) = max a\u2208Ai\u22121(s)\nRi(s, a) + \u2211\nj\nP (s, a, j)V \u22c6i,t+1(j) (6)\nBy induction hypothesis we have V \u03c0i,t+1(j) = V \u22c6 i,t+1(j), then we must have V \u03c0 i,t(s) 6 V \u22c6i,t(s). Now since we have equality achieved, by our definition of Ai(s) in our algorithm, we must have \u03c0(s) \u2208 Ai(s). Then replacing i equation (5) with i + 1 we have:\nV \u03c0i+1,t(s) = Ri+1(s, \u03c0(s)) + \u2211\nj\nP (s, \u03c0(s), j)V \u03c0i+1,t+1(j)\nV \u22c6i+1,t(s) = max a\u2208Ai(s)\nRi+1(s, a) + \u2211\nj\nP (s, a, j)V \u03c0i+1,t+1(j)\nBy induction hypothesis we have V \u03c0i+1,t+1(j) = V \u22c6 i+1,t+1(j), noticing that now \u03c0(s) \u2208 Ai(s), then we have:\nV \u03c0i+1,t(s) 6 V \u22c6 i+1,t(s)\nFinally, when i = it+1 \u2212 1, if\nV \u03c0j,t(s) = V \u22c6 j,t(s), j = 1, . . . , i\nThen following the argument as before, and utilizing that now V \u03c0i+1,t+1(j) < V \u22c6i+1,t+1(j), we must have:\nV \u03c0i+1,t(s) < V \u22c6 i+1,t(s)\nwhich gives us V\u03c0t (s) <l V \u22c6 t (s)\nNotice that our previous argument could also be used to prove V\u03c0t+1(s) = V \u22c6 t+1(s) \u21d2 V \u03c0 t (s) 6l V \u22c6 t (s). Then combining all the ingredients we have, the following statement holds: V\n\u03c0 t+1(s) 6l V \u22c6 t+1(s) \u21d2 V \u03c0 t (s) 6l V \u22c6 t (s) (7)\nTo conclude our proof, notice we have V\u03c0T\u22121(s) 6l V \u22c6 T\u22121(s), apply equation (7) iteratively, we have V\u03c01 (s) 6l V \u22c6 1(s), the optimality of our output policy follows immediately.\nNow we return to the general case where optimal quantiles {q\u22c6 \u03c4i }Li=1 is not known before hand. Out idea is to proceed iteratively, at kth iteration, we used bisection to guess the location of the unknown q\u22c6\n\u03c4k , we then solve a lexicographic MDP with k reward\n[R1, . . . , Rk] and preference aligns with our preference for total L reward. Specifically, at k-th iteration, we maintain u and l such that F \u22c6k (gl\u22121) < \u03c4k and F \u22c6 k (gu\u22121) > \u03c4k. We successively reduce u\u2212 l by half until u\u2212 l = 1. Then q\u22c6 \u03c4k = gu\u22121. To proceed the k-th iteration, we need to define our reward function as follows:\nR q\u03c4i i (st, at, st+1) =\n{\n1 if st 6\u2208 G and st+1 = gi, gi > q\u03c4i 0 otherwise\nThe reward vector at k-th iteration is then given by:\nR = [R q\u22c6 \u03c41 1 , . . . , R q\u22c6 \u03c4k\u22121 k\u22121 , R q \u03c4k k ]\nwhere q \u03c4k is our guess for q\u22c6 \u03c4k .\nAlgorithm 2 Multi-Quantile-Objective(MQO) MDP\nSet V \u03c0i,T (s) = 0, \u2200i = 1, 2 \u00b7 \u00b7 \u00b7L, \u2200s \u2208 S for i = 1, 2 \u00b7 \u00b7 \u00b7L do Guess a proper q\n\u03c4i , which should be larger than q\u22c6 \u03c4k , \u2200k = i\u2212 1 \u00b7 \u00b7 \u00b71\nSet l be the largest index of {gk} s.t. gk < q\u03c4i , Set u \u2190 n repeat\nSolve Lexicographic MDP with q\u22c6 \u03c41 , q\u22c6 \u03c42 \u00b7 \u00b7 \u00b7 q\u22c6 \u03c4j , j 6 i\u2212 1 Output V \u03c0i,t(s) if V \u03c0i,t(s) 6 1\u2212 \u03c4i then l \u2190 i\nelse\nu \u2190 i end if i \u2190 \u2308 l+u 2 \u2309\nuntil u\u2212 l = 1 q\u22c6 \u03c4i \u2190 gu\u22121\nend for"}, {"heading": "5 Conclusion", "text": "In this paper we consider a multi-quantile-objective MDP problem that combines previous work in quantile objective MDP and multi-objective MDP. Our contribution is\ntwo folds, first we formulate the problem into multi-objective MDP problem, the second is that our algorithm to solve this problem could also solve general multi-objective MDP problem with finite horizon, state space and action space. Extension to infinite state space or action space could be also done with slight modification.\nWe note our possible future work here: Pineda et.al [1] has showed that constrained MDP could be reshaped into a sequence of multi-objective MDP with lexicographic preference and additional slack variables, thus if one could solve lexicographic MDP with slack variable efficiently, then solution of constrained MDP follows. For finite horizon, we believe similar dynamic programming flavor algorithm could be invented for solving lexicographic MDP with slack variables, we leave it here as an open problem and our future work."}], "references": [{"title": "Revisiting Multi-Objective MDPs with relaxed Lexicographic Preferences", "author": ["Luis Pineda", "Kyle H. Wray", "Shlomo Zilberstein"], "venue": "AAAI Fall Symposium on Sequential Decision Making for Intelligent Agents,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multi-Objective MDPs with Conditional Lexicographic Reward Preferences", "author": ["Kyle H. Wray", "Shlomo Zilberstein", "Abdel-Illah Mouaddib"], "venue": "In Proceedings of the Twenty-Ninth Conference on Artificial Intelligence (AAAI),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Preference Order Dynamic Programming", "author": ["L.G. Mitten"], "venue": "Management Science. Volume 21,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1974}, {"title": "Constrained Markov Decision Process", "author": ["Eitan Altman"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Ordinal Dynamic Programming", "author": ["Matthew J. Sobel"], "venue": "Management Science", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1975}, {"title": "Quantile Reinforcement Learning", "author": ["Hugo Gilbert", "Paul Weng"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["Sutton", "Richard S", "Andrew G. Barto"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Markov Decision Processes with Ordinal Rewards: Reference Point- Based Preferences", "author": ["Weng", "Paul"], "venue": "In ICAPS", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "For example, in self-autonomous driving one need to balance speed and safety [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "However in practice it is hard to evaluate and analyze the projected problem since there might be many viable Pareto optimal solutions to the original problem [1].", "startOffset": 159, "endOffset": 162}, {"referenceID": 2, "context": "Using a technique called Ordinal dynamic programming, Mitten [5] assumed a specific preference ordering over outcomes for a finite horizon MDP; Sobel [7] extended this model to infinite horizon MDPs.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Using a technique called Ordinal dynamic programming, Mitten [5] assumed a specific preference ordering over outcomes for a finite horizon MDP; Sobel [7] extended this model to infinite horizon MDPs.", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "al [2] also consider a more general setting when lexicographical order depends on initial state and slack for higher objective value is allowed for improvement over lower priority objective.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Based on this motivation, Hugo and Weng [8] proposed quantile based reinforcement learning algorithm which seeks to optimize certain lower/upper quantile on the random outcome.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "< \u03c4L \u2208 [0, 1], following our motivation in Introduction section, our procedure to find optimal policy is a series of optimization procedure, we will show later this could be reshaped into multi-objective MDP with lexicographic preference.", "startOffset": 7, "endOffset": 13}, {"referenceID": 5, "context": "In this section we first present a lemma that generalize the Lemma 1 of Hugo and Weng [8], this lemma fully characterize the q \u03c4i Lemma 1.", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "Altman [6] has shown that an optimal randomized policy could be found in such constrained MDP, Chen and Feinberg [4] also showed how to find optimal deterministic policy.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "al [1] has showed that constrained MDP could be reshaped into a sequence of multi-objective MDP with lexicographic preference and additional slack variables, thus if one could solve lexicographic MDP with slack variable efficiently, then solution of constrained MDP follows.", "startOffset": 3, "endOffset": 6}], "year": 2017, "abstractText": "In most common settings of Markov Decision Process (MDP), an agent evaluate a policy based on expectation of (discounted) sum of rewards. However in many applications this criterion might not be suitable from two perspective: first, in risk aversion situation expectation of accumulated rewards is not robust enough, this is the case when distribution of accumulated reward is heavily skewed; another issue is that many applications naturally take several objective into consideration when evaluating a policy, for instance in autonomous driving an agent needs to balance speed and safety when choosing appropriate decision. In this paper, we consider evaluating a policy based on a sequence of quantiles it induces on a set of target states, our idea is to reformulate the original problem into a multi-objective MDP problem with lexicographic preference naturally defined. For computation of finding an optimal policy, we proposed an algorithm FLMDP that could solve general multi-objective MDP with lexicographic reward preference.", "creator": "LaTeX with hyperref package"}}}