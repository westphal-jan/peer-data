{"id": "1705.05040", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes", "abstract": "Debate and deliberation play essential roles in politics and government, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model's combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74% accuracy, significantly outperforming linguistic features alone (66%). Our model finds that winning sides employ stronger arguments, and allows us to identify the linguistic features associated with strong or weak arguments.", "histories": [["v1", "Mon, 15 May 2017 00:21:03 GMT  (435kb,D)", "http://arxiv.org/abs/1705.05040v1", "Accepted by TACL, 14 pages"]], "COMMENTS": "Accepted by TACL, 14 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "nick beauchamp", "sarah shugars", "kechen qin"], "accepted": true, "id": "1705.05040"}, "pdf": {"name": "1705.05040.pdf", "metadata": {"source": "CRF", "title": "Winning on the Merits: The Joint Effects of Content and Style on Debate Outcomes", "authors": ["Lu Wang", "Nick Beauchamp", "Sarah Shugars", "Kechen Qin"], "emails": ["luwang@ccs.neu.edu,", "n.beauchamp@northeastern.edu", "qin.ke@husky.neu.edu", "shugars.s@husky.neu.edu"], "sections": [{"heading": "1 Introduction", "text": "What determines the outcome of a debate? In an ideal setting, a debate is a mechanism for determining which side has the better arguments and for an audience to reevaluate their views in light of what they have learned. This ideal vision of debate and deliberation has taken an increasingly central role in modern theories of democracy (Habermas, 1984; Cohen, 1989; Rawls, 1997; Mansbridge, 2003). However, empirical evidence has also led to an increasing awareness of the dangers of style and rhetoric in biasing participants towards the most skillful, charismatic, or numerous speakers (NoelleNeumann, 1974; Sunstein, 1999).\nIn light of these concerns, most efforts to predict the persuasive effects of debate have focused on the linguistic features of debate speech (Katzav and Reed, 2008; Mochales and Moens, 2011; Feng and Hirst, 2011) or on simple measures of topic control (Dryzek and List, 2003; Mansbridge, 2015; Zhang et al., 2016). In the ideal setting, however, we would wish for the winning side to win based on the strength and merits of their arguments, not based on their skillful deployment of linguistic style. Our model therefore predicts debate outcomes by modeling not just the persuasive effects of directly observable linguistic features, but also by incorporating the inherent, latent strengths of topics and issues specific to each side of a debate.\nTo illustrate this idea, Figure 1 shows a brief ex-\nar X\niv :1\n70 5.\n05 04\n0v 1\n[ cs\n.C L\n] 1\n5 M\nay 2\nchange from a debate about the death penalty. Although the arguments from both sides are on the same subtopic (the execution of innocents), they make their points with a variety of stylistic maneuvers, including rhetorical questions, factual numbers, and logical phrasing. Underlying these features is a shared content, the idea of the execution of innocents. Consistent with the work of Baumgartner et al. (2008), this subtopic appears to inherently support one side \u2013 the side opposed to the death penalty \u2013 more strongly than the other, independent of stylistic presentation. We hypothesize that within the overall umbrella of a debate, some topics will tend to be inherently more persuasive for one side than the other, such as the execution of innocents for those opposed to the death penalty, or the gory details of a murder for those in favor of it. Strategically, debaters seek to introduce topics that are strong for them and weaker for their opponent, while also working to craft the most persuasive stylistic delivery they can. Because these stylistic features themselves vary with the inherent strength of topics, we are able to predict the latent strength of topics even in new debates on entirely different issues, allowing us to predict debate winners with greater accuracy than before.\nIn this paper, then, we examine the latent persuasive strength of debate topics, how they interact with linguistic styles, and how both predict debate outcomes. Although the task here is fundamentally predictive, it is motivated by the following substantive questions: How do debates persuade listeners? By merit of their content and not just their linguistic structure, can we capture the sense in which debates are an exchange of arguments that are strong or weak? How do these more superficial linguistic or stylistic features interact with the latent persuasive effects of topical content? Answering these questions is crucial for modern theories of democratic representation, although we seek only to understand how these features predict audience reactions in the context of a formal debates where substance perhaps has the best chance of overcoming pure style. We discuss in detail the relevance of our work to existing research on framing, agenda setting, debate, persuasion, and argument mining in \u00a7 6.\nWe develop here a joint model that simultaneously 1) infers the latent persuasive strength inherent\nin debate topics and how it differs between opposing sides, and 2) captures the interactive dynamics between topics of different strength and the linguistic structures with which those topics are presented. Experimental results on a dataset of 118 Oxford-style debates show that our topic-aware debate prediction model achieves an accuracy of 73.7% in predicting the winning side. This result is significantly better than a classifier trained with only linguistic features (66.1%), or using audience feedback (applause and laughter; 58.5%), and significantly outperforms previous predictive work using the same data (Zhang et al., 2016) (73% vs. 65%). This shows that the inherent persuasive effect of argument content plays a crucial role in affecting the outcome of debates.\nMoreover, we find that winning sides are more likely to have used inherently strong topics (as inferred by the model) than losing sides (59.5% vs. 54.5%), a result echoed by human ratings of topics without knowing debate outcomes (44.4% vs. 30.1%). Winning sides also tend to shift discussion to topics that are strong for themselves and weak for their opponents. Finally, our model is able to identify linguistic features that are specifically associated with strong or weak topics. For instance, when speakers are using inherently stronger topics, they tend to use more first person plurals and negative emotion, whereas when using weaker arguments, they tend to use more second person pronouns and positive language. These associations are what allow us to predict topic strength, and hence debate outcomes, out of sample even for debates on entirely new issues."}, {"heading": "2 Data Description", "text": "This study uses transcripts from Intelligence Squared U.S. (IQ2) debates.1 Each debate brings together panels of renowned experts to argue for or against a given issue before a live audience. The debates cover a range of current issues in politics and policy, and are intended to \u201crestore civility, reasoned analysis, and constructive public discourse.\u201d Following the Oxford style of debate, each side is given a 7-minute opening statement. The moderator then begins the discussion phase, allowing questions from the audience and panelists, followed by\n1http://intelligencesquaredus.org\n2-minute closing statements. The live audience members record their pre- and post-debate opinions as PRO, CON, or UNDECIDED relative to the resolution under debate. The results are shared only after the debate has concluded. According to IQ2, the winning side is the one that gains the most votes after the debate. 118 debate transcripts were collected, with PRO winning 60.2 84 debates had two debaters on each side, and the rest had three per side. Each debate contains about 255 speaking turns and 17,513 words on average.3\nThese debates are considerably more structured and moderated, and have more educated speakers and audience members, than one generally finds in public debates. As such, prediction results of our model may vary on other types of debates. Meanwhile, since we do not focus on formal logic and reasoning structure, but rather on the intrinsic persuasiveness of different topics, it may be that the results here are more general to all types of argument. Answering this question depends on subsequent work comparing debates of varying degrees of formality."}, {"heading": "3 The Debate Prediction Model", "text": "We consider debate as a process of argument exchange. Arguments have content with inherent (or exogenously determined) persuasive effects as well as a variety of linguistic features shaping that content. We present here a debate outcome prediction model that combines directly observed linguistic features with latent persuasive effects specific to different topical content."}, {"heading": "3.1 Problem Statement", "text": "Assume that the corpus D contains N debates, where D = {di}Ni=1. Each debate di comprises a sequence of arguments, denoted as xi = {xi,j}nij=1, where ni is the number of arguments. For the present purposes, an argument is a continuous unit of text on the same topic, and may contain multiple sentences within a given turn (see Figure 1). We use xpi and x c i to denote arguments for PRO and CON.\n2Zhang et al. (2016) also use IQ2 to study talking points and their predictive power on debate outcome. Our dataset includes theirs plus 11 additional debates (excluding one with result as tie).\n3The dataset can be downloaded from http://www. ccs.neu.edu/home/luwang/.\nThe outcome for debate di is yi \u2208 {1,\u22121}, where 1 indicates PRO wins and -1 indicates CON wins.\nWe assume that each debate di has a topic system, where debaters issue arguments from K topics relevant to the motion (K varies for different debates). Each topic has an intrinsic persuasion strength which may vary between sides (e.g. a discussion of innocent convicts may intrinsically help the antideath-penalty side more than the pro). Thus we have a topic strength system hi = {hpi ,h c i }, where the strengths for K topics are hpi = {h p i,k} K k=1 for PRO, and hci = {hci,k}Kk=1 for CON. Topic strength h\u2217\u2217,\u2217 is a categorical variable in {STRONG,WEAK}.4 Neither the topics nor their strength are known a priori, and thus must be inferred.\nFor debate di, we define \u03a6(x p i ,hi) and \u03a6(x c i ,hi) to be feature vectors for arguments from PRO and CON. We first model and characterize features for each argument and then aggregate them by side to predict the relative success of each side. Therefore, the feature vectors for a side can be formulated as the summation of feature vectors of its arguments, i.e. \u03a6(xpi ,hi) = \u2211 xi,j\u2208xpi\n\u03c6(xi,j ,hi), and \u03a6(xci ,hi) =\u2211 xi,j\u2208xci \u03c6(xi,j ,hi), where \u03c6(xi,j ,hi) is the feature vector of argument xi,j .5\nEach argument feature in \u03c6(xi,j ,hi) combines a stylistic feature directly observed from the text with a latent strength dependent on the topic of the argument. For instance, consider an argument xi,j of a topic with an inferred strength of STRONG and which contains 3 usages of the word \u201cyou\u201d. Then xi,j has two coupled topic-aware features of the form \u03c6M(feature,strength)(xi,j ,hi): \u03c6M(\u201c#you\u201d,\u201cstrong\u201d)(xi,j ,hi) takes a value of 3, and \u03c6M(\u201c#you\u201d,\u201cweak\u201d)(xi,j ,hi) is 0. xi,j also has a feature without strength, i.e. \u03c6M(\u201c#you\u201d)(xi,j ,hi) = 3. Function M(\u00b7) maps each feature to a unique index.\nFor predicting the outcome of debate xi, we compute the difference of feature vectors from PRO and CON in two ways: \u03a6\u0303p(xi,hi) = \u03a6(x p i ,hi) \u2212\n4Binary topic strength is better-suited for our proposed discriminative learning framework. In exploratory work we found that continuous-value strength under the same framework tended to be pushed towards extreme values during learning.\n5This assumes the strength of arguments is additive, though it is possible that a single extremely strong or weak argument could decide a debate, or that debates are won via \u201crounds\u201d rather than in aggregate.\n\u03a6(xci ,hi) and \u03a6\u0303 c(xi,hi) = \u03a6(x c i ,hi)\u2212 \u03a6(x p i ,hi). Two decision scores are computed as fp(xi) = maxhi [w \u00b7 \u03a6\u0303p(xi,hi)] and f c(xi) = maxhi [w \u00b7 \u03a6\u0303c(xi,hi)]. The output is 1 if fp(xi) > f c(xi) (PRO wins); otherwise, the prediction is \u22121 (CON wins).\nWeights w are learned during training, while topic strengths hi are latent variables, and we use Integer Linear Programming to search for hi."}, {"heading": "3.2 Learning with Latent Variables", "text": "To learn the weight vector w, we use the large margin training objective:\nmin w\n1 2 \u2016w\u20162 + C \u00b7 \u2211 i l(\u2212yi \u00b7max hi [w \u00b7 \u03a6\u0303(xi,hi)]) (1) We consider samples based on difference feature vectors \u03a6\u0303p(xi,hi) during training, which is represented as \u03a6\u0303(xi,hi) in Eq. 1 and the rest of this section. l(\u00b7) is squared-hinge loss function. C controls the trade-off between the two items.\nThis objective function is non-convex due to the maximum operation (Yu and Joachims, 2009). We utilize Alg. 1, which is an iterative optimization algorithm, to search for the solution for w and h. We first initialize latent topic strength variables as h0 (see next paragraph) and learn the weight vector as w\u2217. Adopted from Chang et al. (2010), our iterative algorithm consists of two major steps. For each iteration, the algorithm first decides the latent variables for positive examples. In the second step, the solver iteratively searches for latent variable assignments for negative samples and updates the weight vector w with a cutting plane algorithm until convergence. Global variable Hi is maintained for each negative sample to store all the topic strength assignments that give the highest scores during training.6 This strategy facilitates efficient training while a local optimum is guaranteed. Topic strength initialization. We investigate three approaches for initializing topic strength variables. The first is based on the usage frequency per topic. If one side uses more arguments of a given topic, then\n6A similar latent variable model is presented in Goldwasser and Daume\u0301 III (2014) to predict the objection behavior in courtroom dialogues. In their work, a binary latent variable is designed to indicate the latent relevance of each utterance to an objection, and only relevant utterances contribute to the final objection decision. In our case our latent variables model argument strength, and all arguments matter for the debate outcome.\nInput : {xi, yi}i: training samples of arguments xi and outcome yi, \u03a6\u0303(\u00b7, \u00b7): feature vectors, C: trade-off coefficient, \u03c4 : iteration number threshold Output: feature weights w\u2217 foreach hi do Initialize hi as h0i (see \u00a7 3.2) end w\u2217 \u2190 arg minw 12\u2016w\u2016 2 + C \u00b7 \u2211 i l(\u2212yi \u00b7 [w \u00b7 \u03a6\u0303(xi,h 0 i )]) // Hi: storing h\u2217i for negative samples foreach negative sample xi (yi = \u22121) do\nHi \u2190 \u2205 end t\u2190 0 while w\u2217 not converge and t < \u03c4 do\n// Assign strength for positive samples foreach positive sample xi (yi = 1) do\nh\u2217i \u2190 arg maxhi [w \u00b7 \u03a6\u0303(xi,hi)] (*) end // Iteration over negative samples t\u2032 \u2190 0 while w\u2217 not converge and t\u2032 < \u03c4 do\nforeach negative sample xi, yi = \u22121 do h\u2217i \u2190 arg maxhi [w\n\u2217 \u00b7 \u03a6\u0303(xi,hi)] (*) Hi \u2190 Hi \u222a {h\u2217i }\nend w\u2217 \u2190 arg minw 12\u2016w\u2016 2 + C \u00b7 \u2211 i,yi=1 l(\u2212yi \u00b7\n[w \u00b7 \u03a6\u0303(xi,h\u2217i )]) + C \u00b7 \u2211\ni,yi=\u22121 l(\u2212yi \u00b7 maxh\u2208Hi [w \u00b7 \u03a6\u0303(xi,hi)]) t\u2032 \u2190 t\u2032 + 1\nend t\u2190 t+ 1\nend Algorithm 1: Iterative algorithm for learning weights w and latent topic strength variables h. Iteration threshold \u03c4 is set as 1000. Steps with (*) are solved as in \u00a7 3.4.\nits strength is likely to be strong for them and weak for their opponent. Another option is to initialize all topics as strong for both sides, then w0 learns the association between strong topics and features that lead to winning. The third option is to initialize all topics as strong for winners and weak for losers."}, {"heading": "3.3 Features", "text": "We group our directly observed linguistic features, roughly ordered by increasing complexity, into categories that characterize various aspects of arguments. For each linguistic feature, we compute two versions: one for the full debate and one for the discussion phase. Basic features. We consider the frequencies of words, numbers, named entities per type, and each personal pronoun. For instance, usage of personal pronouns may imply communicative goals of the speaker (Brown and Gilman, 1960; Wilson, 1990). We also count the frequency of each POS tag output by Stanford parser (Klein and Manning, 2003). Sentiment and emotional language usage is prevalent\nin discussions on controversial topics (Wang and Cardie, 2014b). We thus count words of positive and negative sentiment based on MPQA lexicon (Wilson et al., 2005), and words per emotion type according to a lexicon from Mohammad and Turney (2013). Moreover, based on the intuition that agreement carries indications on topical alignment (Bender et al., 2011; Wang and Cardie, 2014a), occurrence of agreement phrases (\u201cI/we agree\u201d, \u201cyou\u2019re right\u201d) is calculated. Finally, audience feedback, including applause and laughter, is also considered. Style features. Existing work suggests that formality can reveal speakers\u2019 opinions or intentions (Irvine, 1979). Here we utilize a formality lexicon collected by Brooke et al. (2010), which counts the frequencies of formal or informal words in each argument. According to Durik et al. (2008), hedges are indicators of weak arguments, so we compile a list of hedge word from Hyland (2005), and hedging of verbs and non-verbs are counted separately. Lastly, we measure word attributes for their concreteness (perceptible vs. conceptual), valence (or pleasantness), arousal (or intensity of emotion), and dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al. (2013), following Tan et al. (2016), who observe correlations between word attributes and their persuasive effect in online arguments. The average score for each of these features is then computed for each argument. Semantic features. We encode semantic information via semantic frames (Fillmore, 1976), which represent the context of word meanings. CanoBasave and He (2016) show that arguments of different types tend to employ different semantic frames, e.g., frames of \u201creason\u201d and \u201cevaluative comparison\u201d are frequently used in making claims. We count the frequency of each frame, as labeled by SEMAFOR (Das et al., 2014). Discourse features. The usage of discourse connectors has been shown to be effective for detecting argumentative structure in essays (Stab and Gurevych, 2014). We collect discourse connectives from the Penn Discourse Treebank (Prasad et al., 2007), and count the frequency of phrases for each discourse class. Four classes on level one (temporal, comparison, contingency, and expansion) and sixteen classes on level two are considered. Finally, pleading behavior is encoded as counting phrases of \u201curge\u201d, \u201cplease\u201d, \u201cask you\u201d, and \u201cencourage you\u201d, which may be used by debaters to appeal to the audience. Sentence-level features. We first consider the frequency of questioning since rhetorical questions are commonly used for debates and argumentation. To model the sentiment distribution of arguments, sentence-level sentiment is labeled by the Stanford sentiment classifier (Socher et al., 2013) as positive (sentiment score of 4 or 5), negative (score of 1 or 2), and neutral (score of 3). We then count single sentence sentiment as well as transitions between adjacent sentences (e.g. positive \u2192 negative) for each type. Since readability level may affect how the audience perceives arguments, we compute readability levels based on Flesch reading ease scores, FleschKincaid grade levels, and the Coleman-Liau index for each sentence. We use the maximum, minimum, and average of scores as the final features. The raw number of sentences is also calculated. Argument-level features. Speakers generally do not just repeat their best argument ad infinitum, which suggests that arguments may lose power with repetition. For each argument, we add an indicator feature (i.e. each argument takes value of 1) and an additional version with a decay factor of exp(\u2212\u03b1 \u00b7 tk), where tk is the number of preceding arguments by a given side which used topic k, and \u03b1 is fixed at 1.0. Interruption is also measured, when the last argument (of more than 50 words) in a turn is cut off by at most two sentences from opponent or moderator. Word repetition is often used for emphasis in arguments (Cano-Basave and He, 2016), so we measure bigram repetition more than twice in sequential clauses or sentences. Interaction features. In addition to independent language usage, debate strategies are also shaped by interactions with other debaters. For instance, previous work (Zhang et al., 2016) finds that debate winners frequently pursue talking points brought up by their opponents\u2019. Here we construct different types of features to measure how debaters address opponents\u2019 arguments and shift to their favorable subjects. First, for a given argument, we detect if there is an argument of the same topic from the previous turn by the opponent. If yes, we further measure the number of words of the current argument, the number of common words between the two argu-\nments (after lemmatization is applied), the concatenation of the sentiment labels, and the concatenation of the emotion labels of the two arguments as features; these interactions thus capture interactive strategies regarding quantity speech and sentiment. We also consider if the current argument is of a different topic from the previous argument in the same turn to encode topic shifting behavior.\nFeature functions \u03c6M(feature,strength)(xi,j ,hi) in \u00a7 3.1 only consider the strengths of single arguments. To capture interactions between sides that relate to their relative argument strengths, we add features \u03c6M(feature,strengthself,strengthoppo)(xi,j ,hi), so that strengths of pairwise arguments on the same topic from both sides are included. For instance, for topic \u201cexecution of innocents\u201d, side PRO with STRONG strength uses an argument of 100 words to address the challenge raised by CON with WEAK strength. We add four grouped features associated with the number of words addressing an opponent: \u03c6M(\u201c#words to oppo\u201d,\u201cstrong,weak\u201d)(xi,j ,hi) is 100, while \u03c6M(\u201c#words to oppo\u201d,\u201cweak,weak\u201d)(xi,j ,hi), \u03c6M(\u201c#words to oppo\u201d,\u201cstrong,strong\u201d)(xi,j ,hi), and \u03c6M(\u201c#words to oppo\u201d,\u201cweak,strong\u201d)(xi,j ,hi) are all 0."}, {"heading": "3.4 Topic Strength Inference", "text": "Topic strength inference is used both for training (Alg. 1) and for prediction. Our goal is to find an assignment h\u2217i that maximizes the scorer w\n\u2217\u00b7\u03a6\u0303(xi,hi) for a given w\u2217. We formulate this problem as an Integer Linear Programming (ILP) instance.7 Since topic strength assignment only affects feature functions that consider strengths, here we discuss how to transform those functions into the ILP formulation.\nFor each topic k of a debate di, we create binary variables rpk,strong and r p k,weak for pro, where rpk,strong = 1 indicates the topic is STRONG for pro and rpk,weak = 1 denotes the topic is WEAK. Similarly, rck,strong and r c k,weak are created for con.\nGiven weights associated with different strengths wM(feature,strong) and wM(feature,weak), the contribution of any feature to the objective (i.e. scoring difference between pro and con) transforms from wM(feature,strong) \u00b7 [ \u2211 xi,j\u2208x p i \u03c6M(feature,strong)(xi,j ,hi)\n\u2212 \u2211\nxi,j\u2208xci \u03c6M(feature,strong)(xi,j ,hi)]\n7We use LP Solve: http://lpsolve.sourceforge. net/5.5/.\n+ wM(feature,weak) \u00b7 [ \u2211\nxi,j\u2208x p i \u03c6M(feature,weak)(xi,j ,hi) \u2212 \u2211\nxi,j\u2208xci \u03c6M(feature,weak)(xi,j ,hi)]\nto the following form: wM(feature,strong) \u00b7 [ \u2211 xi,j\u2208x p i \u03c6M(feature)(xi,j ,hi) \u00b7 r p k,strong\n\u2212 \u2211\nxi,j\u2208xci \u03c6M(feature)(xi,j ,hi) \u00b7 rck,strong ] + wM(feature,weak) \u00b7 [ \u2211\nxi,j\u2208x p i \u03c6M(feature)(xi,j ,hi) \u00b7 r p k,weak \u2212 \u2211\nxi,j\u2208xci \u03c6M(feature)(xi,j ,hi) \u00b7 rck,weak]\nThe above equation can be reorganized into a linear combination of variables r\u2217\u2217,\u2217. We further include constraints as discussed below, and solve the maximization problem as an ILP instance.\nFor features that consider strength for pairwise arguments, i.e. \u03c6M(feature,strengthself,strengthoppo), we have binary variables rp,ck,strong,strong (strength is strong for both sides), rp,ck,strong,weak (strong for pro, weak for con), rp,ck,weak,strong (weak for pro, strong for con), and rp,ck,weak,weak (weak for both). Constraints. We consider three types of topic strength constraints for our ILP formulation: \u2022 C1 \u2013 Single Topic Consistency: each topic can either be strong or weak for a given side, but not both. pro: rpk,strong + r p k,weak = 1; con: rck,strong + rck,weak = 1 \u2022 C2 \u2013 Pairwise Topic Consistency: for pairwise arguments from pro and con on the same topic, their joint assignment is true only when each of the individual assignments is true. C2 applies only for features of pairwise arguments. rp,ck,strong,strong = r p k,strong \u2227 rck,strong; rp,ck,strong,weak = r p k,strong \u2227 rck,weak; rp,ck,weak,strong = r p k,weak \u2227 rck,strong; rp,ck,weak,weak = r p k,weak \u2227 rck,weak \u2022 C3 \u2013 Exclusive Strength: a topic cannot be strong for both sides. This constraint is optional and will be tested in experiments. rpk,strong + r c k,strong \u2264 1"}, {"heading": "3.5 Argument Identification", "text": "In order to identify the topics associated with a debate and the contiguous chunks of same-topic text that we take to be arguments, for each separate debate we utilize a Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) which jointly models the topics and topic transitions between sentences. For details on HTMM, we refer the readers to Gruber et al. (2007).\nThe HTMM assigns topics on the sentence level, assuming each sentence is generated by a topic draw\nfollowed by word draws from that topic, with a transition probability determining whether each subsequent sentence has the same topic as the preceding, or is a fresh draw from the topic distribution. Unlike the standard HTMM process, however, we presume that while both sides of a debate share the same topics, they may have different topic distributions reflecting the different strengths of those topics for either side. We thus extend the HTMM by allowing different topics distributions for the pro and con speech transcripts, but enforce shared word distributions for those topics. To implement this, we first run HTMM on the entire debate, and then rerun it on the pro and con sides while fixing the topic-word distributions. Consecutive sentences by the same side with the same topic are treated as a single argument."}, {"heading": "4 Experimental Results", "text": ""}, {"heading": "4.1 Experimental Setup", "text": "We test via leave-one-out for all experiments. For logistic regression classifiers, L2 regularization with a trade-off parameter of 1.0 is used. For Support Vector Machines (SVM) classifiers and our models, we fix the trade-off parameter between training error and margin as 0.01. Real-valued features are normalized to [0, 1] via linear transformation.\nOur modified HTMM is run on each debate for between 10 and 20 topics. Topic coherence, measured via Ro\u0308der et al. (2015), is used to select the topic number that yields highest score. On average, there are 13.7 unique topics and about 322.0 arguments per debate."}, {"heading": "4.2 Baselines and Comparisons", "text": "We consider two baselines trained with logistic regression and SVMs classifiers: (1) NGRAMS, including unigrams and bigrams, are used as features, and (2) AUDIENCE FEEDBACK (applause and laughter) are used as features, following Zhang et al. (2016). We also experiment with SVMs trained with different sets of features, presented in \u00a7 3.3."}, {"heading": "4.3 Results", "text": "The debate outcome prediction results are shown in Table 1. For our models, we only display results with latent strength initialization based on frequency per topic, which achieves the best performance. Re-\nsults for different initialization methods are exhibited and discussed later in this section. As can be seen, our model that leverages learned latent topic strengths and their interactions with linguistic features significantly outperform the non-trivial baselines8 (bootstrap resampling test, p < 0.05). Our latent variable models also obtain better accuracies than SVMs trained on the same linguistic feature sets. Without the audience feedback features, our model yields an accuracy of 72.0%, while SVM produces 65.3%. This is because our model can predict topic strength out of sample by learning the interaction between observed linguistic features and unobserved latent strengths. During test time, it infers the latent strengths of entirely new topics based on observable linguistic features, and thereby predict debate outcomes more accurately than using the directly observable features alone. Using the data in Zhang et al. (2016) (a subset of ours), our best model obtains an accuracy of 73% compared to 65% based on leave-one-out setup.\nAs mentioned above, we experimented with a variety of latent topic strength initializations: argument frequency per topic (Freq); all topics strong for both sides (AllStrong); strong just for winners\n8For baselines with logistic regression classifiers, the accuracy is 63.6 with ngram features, and 58.5 with audience feedback features.\n(AllStrongwin); and Random initialization. From Table 2, we can see that there is no significant difference among different initialization methods. Furthermore, the strength constraints make little difference, though their effects slightly vary with different initializations. Most importantly, C3 (the constraint that topics cannot be strong for both sides) does not systematically help, suggesting that in many cases topics may indeed be strong for both sides, as discussed below."}, {"heading": "5 Discussion", "text": "In this section, we first analyze argument strengths for winning and losing sides, followed by a comparison of these results with human evaluations (\u00a7 5.2). We then examine the interactive topic shifting behavior of debaters (\u00a7 5.3) and analyze the linguistic features predictive of debate outcome, particularly the ones that interact with topic strength (\u00a7 5.4). The results are reported by training our model on the full dataset. Initialization of topic strength is based on usage frequency unless otherwise specified."}, {"heading": "5.1 Topic and Argument Usage Analysis", "text": "We start with a basic question: do winning sides more frequently use strong arguments? For each side, we calculate the proportion of strong and weak topics as well as the total number of strong and weak arguments on each side. Figure 2 shows that under all three topic strength initializations, our model infers a greater number of strong topics for winners than for losers. This result is echoed by human judgment of topic strength, as described in \u00a7 5.2. Similarly, winners also use significantly more individually strong arguments.\nAs can be seen in Table 2, the C3 constraint, that a topic be strong for at most one side, only increased accuracy for one initialization case. This indicates that, in general, the model was improved by allowing some topics to be strong for both sides. Interestingly, while the majority (53%) of topics are\nSTRONG for one side and WEAK for the other, about a third (31%) of topics are inferred as STRONG for both sides. While it is clear what it means for a topic to be strong for one side and not the other (as in our death penalty example), or weak for both sides (as in a digression off of the general debate topic), the importance of both-strong for prediction is a somewhat surprising result. Figure 3 illustrates an example as judged by our model. What this shows is that even on a given topic within a debate (Syrian refugees: resettlement), there are different subtopics that may be selectively deployed (resettlement success; resettlement cost) that make the general topic strong for both sides in different ways. For subsequent work, a hierarchical model with nested strength relationships (McCombs, 2005; Nguyen et al., 2015) can be designed to better characterize the topics.\nLastly, we display the usage of strong arguments during the course of debates in Figure 4. Each de-\nbate is divided into opening statements, two interacting phases (equal number of turns), and closing statements. Similar usage of strong arguments are observed as debates progress, though a slight, statistically non-significant drop is noted in the closing statement. One possible interpretation is that debaters have fully delivered their strong arguments during opening and interactions, while only weaker arguments remain when closing the debates."}, {"heading": "5.2 Human Validation of Topic Strength", "text": "Here we evaluate whether our inferred topic strength matches human judgment. We randomly selected 20 debates with a total of 268 topics. For each debate, we first displayed its motion and a brief description constructed by IQ2. Then for each topic, the top 30 topic words from the HTMM model were listed, followed by arguments from PRO and CON. Note that debate results were not shown to the annotators.\nWe hired three human annotators who are native speakers of English. Each of them was asked to first evaluate topic coherence by reading the word list and\nrate it on a 1-3 scale (1 as incoherent, 3 as very coherent). If the judgment was coherent (i.e. a 2 or 3), they then read the arguments and judged whether (a) both sides are strong on the topic, (b) both sides are weak, (c) pro is strong and con is weak, or (d) con is strong and pro is weak.\n54.9% of the topics were labeled as coherent by at least two annotators, although since topics are estimated separately for each debate, even the less coherent topics generally had readily interpretable meanings in the context of a given debate. Among coherent topics, inter-annotator agreement for topic strength annotation had a Krippendorff\u2019s \u03b1 of 0.32. Judging argument strength is clearly a more difficult and subjective task.\nNevertheless, without knowing debate outcomes, all three human judges identified more strong topics for winning sides than losing sides. Specially, among the coherent topics, a (macro-)average of 44.4% of topics were labeled as strong for winners, compared to 30.1% for losers. This echoes the results from our models as illustrated in Figure 2.\nFurthermore, we calculate the correlation between topic strength inferred by our models and the ones labeled by each human judge using Cohen\u2019s \u03ba. The results are illustrated in Figure 5, which shows our three different initializations, with and without learning. The highest human \u03ba is also displayed. Our trained models clearly match human judgments better than untrained ones."}, {"heading": "5.3 Topic Shifting Behavior Analysis", "text": "Within competitive debates, strategy can be quite interactive: one often seeks not just to make the best arguments, but to better the opponent from round to round. Agenda setting, or shifting the topic of debate, is thus a crucial strategy. An important question is therefore: do debaters strategically change topics to ones that benefit themselves and weaken their opponent? According to the HTMM results, debaters make 1.5 topical shifts per turn on average. Both winning and losing teams are more likely to change subjects to their strong topics: winners in particular are much more likely to change the topic to something strong for them (61.4% of shifts), although debate losers also attempt this strategy (53.6% of shifts).\nA more sophisticated strategy is if the debaters also attempt to put their opponents at a disadvantage with topic shifts. We consider the topic strengths of a current argument for both the speaker (\u201cself \u201d) and their \u201copponent\u201d, as well as the strength of the following argument. The top 3 types of shifts are listed in Table 3. As can be seen, winners are more likely to be in a strong (for them) and weak (for the opponent) situation and to stay there, while losers are more likely to be in the reverse. Both sides generally stay in the same strength configuration from argument 1 to argument 2, but winners are also likely (row 3) to employ the strategy of shifting from a topic that is strong for both sides, to one that is strong for them and weak for the opponent."}, {"heading": "5.4 Feature Analysis", "text": "Lastly, we investigate the linguistic features associated with topics of different strengths that affect the audience. Table 4 displays some of the 50 highest weighted features that interact with strong and weak\ntopics. Personal pronoun usage has been found to be related to communicative goals in many previous studies (Brown and Gilman, 1960; Wilson, 1990). We find that strong topics are associated with more first person plurals, potentially an indicator of group responsibility (Wilson, 1990). On the other hand, our model finds that weak topics are associated with second person pronouns, which may be arguments either attacking other discussants or addressing the audience (Simons and Jones, 2011). For sentiment, previous work (Tan et al., 2016) has found that persuasive arguments are more negative in online discussions. Our model associates negative sentiment and anger words with strong topics, and neutral and joyful languages with weak topics.\nIn terms of style and discourse, debaters tend to use more formal and more concrete words for arguments with strong topics. By contrast, arguments with weak topics show more frequent usage of words with intense emotion (higher arousal scores), and contrast discourse connectives. Figure 6 shows how some of these features differ between winners and losers, illustrating the effects on outcome via strong or weak arguments in particular.\nInteraction features also play an important role for\naffecting audiences\u2019 opinions. In particular, debaters spend more time (i.e. use more words) addressing their opponents\u2019 arguments if it is a strong topic for their opponents. But even for weak topics, it appears helpful to address opponents\u2019 arguments."}, {"heading": "6 Related Work", "text": "Previous work on debate and persuasion has studied the dynamic of audience response to debates and the rhetorical frames the speakers use (Boydstun et al., 2014). However, this work is limited by the scarcity of data and does not focus on the interactions between content and language usage. Topic control, operationalized as the tendency of one side to adopt or avoid the words preferentially used by the other, is investigated in Zhang et al. (2016) to predict debate outcome using the Intelligence Squared data. Our work complements theirs in examining topic interactions, but brings an additional focus on the latent persuasive strength of topics, as well as strength interactions. Tan et al. (2016) examine various structural and linguistic features associated with persuasion on Reddit; they find that some topics correlates more with malleable opinions. Here we develop a more general model of latent topic strength and the linguistic features associated with strength.\nAdditional work has focused on the influence of agenda setting \u2014 controlling which topics are discussed (Nguyen et al., 2014), and framing (Card et al., 2015; Tsur et al., 2015) \u2014 emphasizing certain aspects or interpretations of an issue. Greene and Resnik (2009) study the syntactic aspects of framing, where syntactic choices are found to correlate with the sentiment perceived by readers. Based on the topic shifting model of Nguyen et al. (2014), Prabhakaran et al. (2014) find that changing topics in presidential primary debates positively correlates with the candidates\u2019 power, which is measured based on their relative standing in recent public polls. This supports our finding that both sides seek to shift topics, but that winners are more likely to shift and shift to topics which are strong for them but weak for their opponents.\nOur work is in line with argumentation mining. Existing work in this area focuses on argument extraction (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011) and argument scheme classification (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014). Though stance prediction has also been studied (Thomas et al., 2006; Hasan and Ng, 2014), we are not aware of any work that extracts arguments according to topics and position. Argument strength prediction is also studied largely in the domain of student essays (Higgins et al., 2004; Stab and Gurevych, 2014; Persing and Ng, 2015). Notably, none of these distinguishes an argument\u2019s strength from its linguistic surface features. This is a gap we aim to fill."}, {"heading": "7 Conclusion", "text": "We present a debate prediction model that learns latent persuasive strengths of topics, linguistic style of arguments, and the interactions between the two. Experiments on debate outcome prediction indicate that our model outperforms comparisons using audience responses or linguistic features alone. Our model also shows that winners use stronger arguments and strategically shift topics to stronger ground. We also find that strong and weak arguments differ in their language usage in ways relevant to various behavioral theories of persuasion."}, {"heading": "Acknowledgments", "text": "This work was supported in part by National Science Foundation Grant IIS-1566382 and a GPU gift from Nvidia. We thank the TACL reviewers for valuable suggestions on various aspects of this work."}], "references": [{"title": "The decline of the death penalty and the discovery of innocence", "author": ["Suzanna L. De Boef", "Amber E. Boydstun"], "venue": null, "citeRegEx": "Baumgartner et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Baumgartner et al\\.", "year": 2008}, {"title": "Annotating social acts: Authority claims and alignment moves in wikipedia talk pages", "author": ["Jonathan T. Morgan", "Meghan Oxley", "Mark Zachry", "Brian Hutchinson", "Alex Marin", "Bin Zhang", "Mari Ostendorf"], "venue": null, "citeRegEx": "Bender et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bender et al\\.", "year": 2011}, {"title": "Identifying justifications in written dialogs by classifying text as argumentative", "author": ["Biran", "Rambow2011] Or Biran", "Owen Rambow"], "venue": "International Journal of Semantic Computing,", "citeRegEx": "Biran et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Biran et al\\.", "year": 2011}, {"title": "Real-time reactions to a 2012 presidential debate a method for understanding which messages matter", "author": ["Rebecca A. Glazier", "Matthew T. Pietryka", "Philip Resnik"], "venue": "Public Opinion Quarterly,", "citeRegEx": "Boydstun et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boydstun et al\\.", "year": 2014}, {"title": "Automatic acquisition of lexical formality", "author": ["Brooke et al.2010] Julian Brooke", "Tong Wang", "Graeme Hirst"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,", "citeRegEx": "Brooke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Brooke et al\\.", "year": 2010}, {"title": "The pronouns of power and solidarity, pages 253\u2013276", "author": ["Brown", "Gilman1960] Roger Brown", "Albert Gilman"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1960\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1960}, {"title": "Concreteness ratings for 40 thousand generally known english word lemmas", "author": ["Amy Beth Warriner", "Victor Kuperman"], "venue": "Behavior research methods,", "citeRegEx": "Brysbaert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brysbaert et al\\.", "year": 2014}, {"title": "A study of the impact of persuasive argumentation in political debates", "author": ["Cano-Basave", "Yulan He"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Cano.Basave et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cano.Basave et al\\.", "year": 2016}, {"title": "Discriminative learning over constrained latent representations", "author": ["Chang et al.2010] Ming-Wei Chang", "Dan Goldwasser", "Dan Roth", "Vivek Srikumar"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the As-", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "Deliberation and Democratic Legitimacy. The Good Polity: Normative Analysis of the State", "author": ["Joshua Cohen"], "venue": null, "citeRegEx": "Cohen.,? \\Q1989\\E", "shortCiteRegEx": "Cohen.", "year": 1989}, {"title": "Social choice theory and deliberative democracy: A reconciliation", "author": ["Dryzek", "List2003] John S. Dryzek", "Christian List"], "venue": "British Journal of Political Science,", "citeRegEx": "Dryzek et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dryzek et al\\.", "year": 2003}, {"title": "The effects of hedges in persuasive arguments: A nuanced analysis of language", "author": ["M. Anne Britt", "Rebecca Reynolds", "Jennifer Storey"], "venue": "Journal of Language and Social Psychology", "citeRegEx": "Durik et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Durik et al\\.", "year": 2008}, {"title": "Classifying arguments by scheme", "author": ["Feng", "Hirst2011] Vanessa Wei Feng", "Graeme Hirst"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Feng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2011}, {"title": "I object!\u201d modeling latent pragmatic effects in courtroom dialogues", "author": ["Goldwasser", "Daum\u00e9 III2014] Dan Goldwasser", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Goldwasser et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goldwasser et al\\.", "year": 2014}, {"title": "More than words: Syntactic packaging and implicit sentiment", "author": ["Greene", "Resnik2009] Stephan Greene", "Philip Resnik"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Greene et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Greene et al\\.", "year": 2009}, {"title": "Hidden topic markov models", "author": ["Gruber et al.2007] Amit Gruber", "Yair Weiss", "Michal Rosen-Zvi"], "venue": "In International conference on artificial intelligence and statistics,", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "The theory of communicative action", "author": ["J\u00fcrgen Habermas"], "venue": null, "citeRegEx": "Habermas.,? \\Q1984\\E", "shortCiteRegEx": "Habermas.", "year": 1984}, {"title": "Why are you taking this stance? identifying and classifying reasons in ideological debates", "author": ["Hasan", "Ng2014] Kazi Saidul Hasan", "Vincent Ng"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Hasan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hasan et al\\.", "year": 2014}, {"title": "Evaluating multiple aspects of coherence in student essays", "author": ["Jill Burstein", "Daniel Marcu", "Claudia Gentile"], "venue": "In Human Language Technology Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Higgins et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2004}, {"title": "Metadiscourse: Exploring interaction in writing", "author": ["Ken Hyland"], "venue": null, "citeRegEx": "Hyland.,? \\Q2005\\E", "shortCiteRegEx": "Hyland.", "year": 2005}, {"title": "Formality and informality in communicative events", "author": ["Judith T. Irvine"], "venue": "American Anthropologist,", "citeRegEx": "Irvine.,? \\Q1979\\E", "shortCiteRegEx": "Irvine.", "year": 1979}, {"title": "Modelling argument recognition and reconstruction", "author": ["Katzav", "Reed2008] Joel Katzav", "Chris Reed"], "venue": "Journal of Pragmatics,", "citeRegEx": "Katzav et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Katzav et al\\.", "year": 2008}, {"title": "Accurate unlexicalized parsing", "author": ["Klein", "Manning2003] Dan Klein", "Christopher D. Manning"], "venue": "In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Klein et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "A look at agenda-setting: Past, present and future", "author": ["Maxwell McCombs"], "venue": "Journalism studies,", "citeRegEx": "McCombs.,? \\Q2005\\E", "shortCiteRegEx": "McCombs.", "year": 2005}, {"title": "Automatic detection of arguments in legal texts", "author": ["Erik Boiy", "Raquel Mochales Palau", "Chris Reed"], "venue": "In Proceedings of the 11th international conference on Artificial intelligence and law,", "citeRegEx": "Moens et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Moens et al\\.", "year": 2007}, {"title": "Crowdsourcing a wordemotion association lexicon", "author": ["Mohammad", "Turney2013] Saif M. Mohammad", "Peter D. Turney"], "venue": "Computational Intelligence,", "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Modeling topic control to detect influence in conversations using nonparametric topic models", "author": ["Jordan BoydGraber", "Philip Resnik", "Deborah A. Cai", "Jennifer E. Midberry", "Yuanxin Wang"], "venue": "Machine Learning,", "citeRegEx": "Nguyen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2014}, {"title": "Tea party in the house: A hierarchical ideal point topic model and its application to republican legislators in the 112th congress", "author": ["Jordan BoydGraber", "Philip Resnik", "Kristina Miler"], "venue": null, "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Argumentation mining: the detection, classification and structure of arguments in text", "author": ["Palau", "Moens2009] Raquel Mochales Palau", "Marie-Francine Moens"], "venue": "In Proceedings of the 12th international conference on artificial intelligence and", "citeRegEx": "Palau et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Palau et al\\.", "year": 2009}, {"title": "Modeling argument strength in student essays", "author": ["Persing", "Ng2015] Isaac Persing", "Vincent Ng"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "citeRegEx": "Persing et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Persing et al\\.", "year": 2015}, {"title": "Staying on topic: An indicator of power in political debates", "author": ["Ashima Arora", "Owen Rambow"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Prabhakaran et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prabhakaran et al\\.", "year": 2014}, {"title": "The penn discourse treebank 2.0 annotation manual", "author": ["Prasad et al.2007] Rashmi Prasad", "Eleni Miltsakaki", "Nikhil Dinesh", "Alan Lee", "Aravind Joshi", "Livio Robaldo", "Bonnie L. Webber"], "venue": null, "citeRegEx": "Prasad et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2007}, {"title": "The idea of public reason revisited", "author": ["John Rawls"], "venue": "The University of Chicago Law Review,", "citeRegEx": "Rawls.,? \\Q1997\\E", "shortCiteRegEx": "Rawls.", "year": 1997}, {"title": "Exploring the space of topic coherence measures", "author": ["R\u00f6der et al.2015] Michael R\u00f6der", "Andreas Both", "Alexander Hinneburg"], "venue": null, "citeRegEx": "R\u00f6der et al\\.,? \\Q2015\\E", "shortCiteRegEx": "R\u00f6der et al\\.", "year": 2015}, {"title": "Applying kernel methods to argumentation mining", "author": ["Rooney et al.2012] Niall Rooney", "Hui Wang", "Fiona Browne"], "venue": "In Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society", "citeRegEx": "Rooney et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rooney et al\\.", "year": 2012}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y. Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Y. Ng", "Christopher Potts"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Identifying argumentative discourse structures in persuasive essays", "author": ["Stab", "Gurevych2014] Christian Stab", "Iryna Gurevych"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Stab et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Stab et al\\.", "year": 2014}, {"title": "Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions", "author": ["Tan et al.2016] Chenhao Tan", "Vlad Niculae", "Cristian Danescu-Niculescu-Mizil", "Lillian Lee"], "venue": "In Proceedings of the 25th International Conference", "citeRegEx": "Tan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2016}, {"title": "Get out the vote: Determining support or opposition from congressional floor-debate transcripts", "author": ["Thomas et al.2006] Matt Thomas", "Bo Pang", "Lillian Lee"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Thomas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Thomas et al\\.", "year": 2006}, {"title": "A frame of mind: Using statistical models for detection of framing and agenda setting campaigns", "author": ["Tsur et al.2015] Oren Tsur", "Dan Calacci", "David Lazer"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Tsur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsur et al\\.", "year": 2015}, {"title": "Improving agreement and disagreement identification in online discussions with a socially-tuned sentiment lexicon", "author": ["Wang", "Cardie2014a] Lu Wang", "Claire Cardie"], "venue": "In Proceedings of the 5th Workshop on Computational Approaches to Subjectivity,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "A piece of my mind: A sentiment analysis approach for online dispute detection", "author": ["Wang", "Cardie2014b] Lu Wang", "Claire Cardie"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Norms of valence, arousal, and dominance for 13,915 english lemmas", "author": ["Victor Kuperman", "Marc Brysbaert"], "venue": "Behavior research methods,", "citeRegEx": "Warriner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Warriner et al\\.", "year": 2013}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["Janyce Wiebe", "Paul Hoffmann"], "venue": "In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Pro-", "citeRegEx": "Wilson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2005}, {"title": "Politically speaking: The pragmatic analysis of political language", "author": ["John Wilson"], "venue": null, "citeRegEx": "Wilson.,? \\Q1990\\E", "shortCiteRegEx": "Wilson.", "year": 1990}, {"title": "Learning structural svms with latent variables", "author": ["Yu", "Joachims2009] Chun-Nam John Yu", "Thorsten Joachims"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Yu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "Conversational flow in oxford-style debates", "author": ["Zhang et al.2016] Justine Zhang", "Ravi Kumar", "Sujith Ravi", "Cristian Danescu-Niculescu-Mizil"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "bate and deliberation has taken an increasingly central role in modern theories of democracy (Habermas, 1984; Cohen, 1989; Rawls, 1997; Mansbridge, 2003).", "startOffset": 93, "endOffset": 153}, {"referenceID": 9, "context": "bate and deliberation has taken an increasingly central role in modern theories of democracy (Habermas, 1984; Cohen, 1989; Rawls, 1997; Mansbridge, 2003).", "startOffset": 93, "endOffset": 153}, {"referenceID": 32, "context": "bate and deliberation has taken an increasingly central role in modern theories of democracy (Habermas, 1984; Cohen, 1989; Rawls, 1997; Mansbridge, 2003).", "startOffset": 93, "endOffset": 153}, {"referenceID": 46, "context": "and Hirst, 2011) or on simple measures of topic control (Dryzek and List, 2003; Mansbridge, 2015; Zhang et al., 2016).", "startOffset": 56, "endOffset": 117}, {"referenceID": 0, "context": "Consistent with the work of Baumgartner et al. (2008), this subtopic appears to inher-", "startOffset": 28, "endOffset": 54}, {"referenceID": 46, "context": "5%), and significantly outperforms previous predictive work using the same data (Zhang et al., 2016) (73% vs.", "startOffset": 80, "endOffset": 100}, {"referenceID": 8, "context": "Adopted from Chang et al. (2010), our itera-", "startOffset": 13, "endOffset": 33}, {"referenceID": 44, "context": "For instance, usage of personal pronouns may imply communicative goals of the speaker (Brown and Gilman, 1960; Wilson, 1990).", "startOffset": 86, "endOffset": 124}, {"referenceID": 43, "context": "We thus count words of positive and negative sentiment based on MPQA lexicon (Wilson et al., 2005), and words per emotion type ac-", "startOffset": 77, "endOffset": 98}, {"referenceID": 1, "context": "Moreover, based on the intuition that agreement carries indications on topical alignment (Bender et al., 2011; Wang and Cardie, 2014a), occurrence of agreement phrases (\u201cI/we agree\u201d, \u201cyou\u2019re", "startOffset": 89, "endOffset": 134}, {"referenceID": 20, "context": "tions (Irvine, 1979).", "startOffset": 6, "endOffset": 20}, {"referenceID": 4, "context": "Here we utilize a formality lexicon collected by Brooke et al. (2010), which counts the frequencies of formal or informal words in each argument.", "startOffset": 49, "endOffset": 70}, {"referenceID": 4, "context": "Here we utilize a formality lexicon collected by Brooke et al. (2010), which counts the frequencies of formal or informal words in each argument. According to Durik et al. (2008), hedges are indicators of weak arguments, so we compile a", "startOffset": 49, "endOffset": 179}, {"referenceID": 19, "context": "list of hedge word from Hyland (2005), and hedging of verbs and non-verbs are counted separately.", "startOffset": 24, "endOffset": 38}, {"referenceID": 6, "context": "dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al.", "startOffset": 68, "endOffset": 92}, {"referenceID": 6, "context": "dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al. (2013), following Tan et al.", "startOffset": 68, "endOffset": 119}, {"referenceID": 6, "context": "dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al. (2013), following Tan et al. (2016), who observe correlations between word attributes and their persuasive effect in online arguments.", "startOffset": 68, "endOffset": 148}, {"referenceID": 31, "context": "We collect discourse connectives from the Penn Discourse Treebank (Prasad et al., 2007), and count the frequency of phrases for each discourse class.", "startOffset": 66, "endOffset": 87}, {"referenceID": 35, "context": "sentiment classifier (Socher et al., 2013) as positive (sentiment score of 4 or 5), negative (score of 1 or 2), and neutral (score of 3).", "startOffset": 21, "endOffset": 42}, {"referenceID": 46, "context": "For instance, previous work (Zhang et al., 2016) finds that debate winners frequently pursue talking points brought up by their opponents\u2019.", "startOffset": 28, "endOffset": 48}, {"referenceID": 15, "context": "bate and the contiguous chunks of same-topic text that we take to be arguments, for each separate debate we utilize a Hidden Topic Markov Model (HTMM) (Gruber et al., 2007) which jointly models the topics and topic transitions between sentences.", "startOffset": 151, "endOffset": 172}, {"referenceID": 15, "context": "For details on HTMM, we refer the readers to Gruber et al. (2007).", "startOffset": 45, "endOffset": 66}, {"referenceID": 33, "context": "Topic coherence, measured via R\u00f6der et al. (2015), is used to select the topic number that yields highest score.", "startOffset": 30, "endOffset": 50}, {"referenceID": 46, "context": "We consider two baselines trained with logistic regression and SVMs classifiers: (1) NGRAMS, including unigrams and bigrams, are used as features, and (2) AUDIENCE FEEDBACK (applause and laughter) are used as features, following Zhang et al. (2016).", "startOffset": 229, "endOffset": 249}, {"referenceID": 46, "context": "Using the data in Zhang et al. (2016) (a subset of ours), our best model", "startOffset": 18, "endOffset": 38}, {"referenceID": 23, "context": "ships (McCombs, 2005; Nguyen et al., 2015) can be designed to better characterize the topics.", "startOffset": 6, "endOffset": 42}, {"referenceID": 27, "context": "ships (McCombs, 2005; Nguyen et al., 2015) can be designed to better characterize the topics.", "startOffset": 6, "endOffset": 42}, {"referenceID": 44, "context": "be related to communicative goals in many previous studies (Brown and Gilman, 1960; Wilson, 1990).", "startOffset": 59, "endOffset": 97}, {"referenceID": 44, "context": "We find that strong topics are associated with more first person plurals, potentially an indicator of group responsibility (Wilson, 1990).", "startOffset": 123, "endOffset": 137}, {"referenceID": 37, "context": "For sentiment, previous work (Tan et al., 2016) has found that per-", "startOffset": 29, "endOffset": 47}, {"referenceID": 3, "context": "the dynamic of audience response to debates and the rhetorical frames the speakers use (Boydstun et al., 2014).", "startOffset": 87, "endOffset": 110}, {"referenceID": 46, "context": "operationalized as the tendency of one side to adopt or avoid the words preferentially used by the other, is investigated in Zhang et al. (2016) to predict debate outcome using the Intelligence Squared data.", "startOffset": 125, "endOffset": 145}, {"referenceID": 37, "context": "Tan et al. (2016) examine various structural and linguistic features associated with persuasion on Reddit; they find that some topics correlates", "startOffset": 0, "endOffset": 18}, {"referenceID": 26, "context": "Additional work has focused on the influence of agenda setting \u2014 controlling which topics are discussed (Nguyen et al., 2014), and framing (Card et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 39, "context": ", 2014), and framing (Card et al., 2015; Tsur et al., 2015) \u2014 emphasizing certain", "startOffset": 21, "endOffset": 59}, {"referenceID": 26, "context": "Based on the topic shifting model of Nguyen et al. (2014),", "startOffset": 37, "endOffset": 58}, {"referenceID": 34, "context": "2009; Mochales and Moens, 2011) and argument scheme classification (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014).", "startOffset": 67, "endOffset": 159}, {"referenceID": 18, "context": "Argument strength prediction is also studied largely in the domain of student essays (Higgins et al., 2004; Stab and Gurevych, 2014; Persing and Ng, 2015).", "startOffset": 85, "endOffset": 154}], "year": 2017, "abstractText": "Debate and deliberation play essential roles in politics and government, but most models presume that debates are won mainly via superior style or agenda control. Ideally, however, debates would be won on the merits, as a function of which side has the stronger arguments. We propose a predictive model of debate that estimates the effects of linguistic features and the latent persuasive strengths of different topics, as well as the interactions between the two. Using a dataset of 118 Oxford-style debates, our model\u2019s combination of content (as latent topics) and style (as linguistic features) allows us to predict audience-adjudicated winners with 74% accuracy, significantly outperforming linguistic features alone (66%). Our model finds that winning sides employ stronger arguments, and allows us to identify the linguistic features associated with strong or weak arguments.", "creator": "LaTeX with hyperref package"}}}