{"id": "1705.05427", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2017", "title": "Repeated Inverse Reinforcement Learning", "abstract": "How detailed should we make the goals we prescribe to AI agents acting on our behalf in complex environments? Detailed and low-level specification of goals can be tedious and expensive to create, and abstract and high-level goals could lead to negative surprises as the agent may find behaviors that we would not want it to do, i.e., lead to unsafe AI. One approach to addressing this dilemma is for the agent to infer human goals by observing human behavior. This is the Inverse Reinforcement Learning (IRL) problem. However, IRL is generally ill-posed for there are typically many reward functions for which the observed behavior is optimal. While the use of heuristics to select from among the set of feasible reward functions has led to successful applications of IRL to learning from demonstration, such heuristics do not address AI safety. In this paper we introduce a novel repeated IRL problem that captures an aspect of AI safety as follows. The agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human. Each time the human is surprised the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.", "histories": [["v1", "Mon, 15 May 2017 20:06:35 GMT  (59kb)", "https://arxiv.org/abs/1705.05427v1", "The first two authors contributed equally to this work"], ["v2", "Thu, 18 May 2017 19:32:27 GMT  (59kb)", "http://arxiv.org/abs/1705.05427v2", "The first two authors contributed equally to this work"]], "COMMENTS": "The first two authors contributed equally to this work", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["kareem amin", "nan jiang", "satinder singh"], "accepted": true, "id": "1705.05427"}, "pdf": {"name": "1705.05427.pdf", "metadata": {"source": "META", "title": "Repeated Inverse Reinforcement Learning", "authors": ["Kareem Amin", "Nan Jiang", "Satinder Singh"], "emails": ["<nanjiang@umich.edu>."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n05 42\n7v 2\n[ cs\n.A I]\n1 8\nM ay\n2 01\n7\nscribe to AI agents acting on our behalf in complex environments? Detailed & low-level specification of goals can be tedious and expensive to create, and abstract & high-level goals could lead to negative surprises as the agent may find behaviors that we would not want it to do, i.e., lead to unsafe AI. One approach to addressing this dilemma is for the agent to infer human goals by observing human behavior. This is the Inverse Reinforcement Learning (IRL) problem. However, IRL is generally ill-posed for there are typically many reward functions for which the observed behavior is optimal. While the use of heuristics to select from among the set of feasible reward functions has led to successful applications of IRL to learning from demonstration, such heuristics do not address AI safety. In this paper we introduce a novel repeated IRL problem that captures an aspect of AI safety as follows. The agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human. Each time the human is surprised the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results."}, {"heading": "1. Introduction", "text": "One challenge in building AI agents that learn from experience is how to set their goals or rewards. In the Reinforcement Learning (RL) setting, one interesting answer to this question is inverse RL (or IRL) in which the agent infers the\n*Equal contribution 1Google Research, New York. This work was done when KA was a postdoctoral researcher at University of Michigan, Ann Arbor. 2University of Michigan, Ann Arbor. Correspondence to: Nan Jiang <nanjiang@umich.edu>.\n+This paper extends the following arXiv paper by the authors: https://arxiv.org/abs/1601.06569.\nrewards of a human by observing the human\u2019s policy in a task (Ng & Russell, 2000). Unfortunately, the IRL problem is ill-posed for there are typically many reward functions for which the observed behavior is optimal in a single task (Abbeel & Ng, 2004). While the use of heuristics to select from among the set of feasible reward functions has led to successful applications of IRL to the problem of learning from demonstration (e.g., Abbeel et al., 2007), not identifying the reward function poses fundamental challenges to the question of how well and how safely the agent will perform when using the learned reward function in other tasks. This is particularly relevant because IRL is a possible approach to the concern about aligning the agent\u2019s values/goals with those of humans for AI safety as society deploys more capable learning agents that impact more people in more ways (Russell et al., 2015; Amodei et al., 2016).\nAdding AI safety concerns to IRL could take many forms: which human\u2019s reward function matters?, which task should we watch the human perform?, how does the agent generalize what it learns from one task to other tasks?, etc. Here we focus solely on extending IRL to the generalization across tasks aspect of AI safety. We formalize multiple variations of a new repeated IRL problem in which the agent and (the same) human are placed in multiple tasks. We separate the reward function into two components, one which is invariant across tasks and can be viewed as intrinsic to the human, and a second that is task specific. As a motivating example, consider a human doing tasks throughout a work day, e.g., getting coffee, driving to work, interacting with co-workers, and so on. Each of these tasks has a task-specific goal but the human brings to each task intrinsic goals that correspond to maintaining health, financial well-being, not violating moral and legal principles, etc. In our repeated IRL setting, the agent presents a policy for each new task that it thinks the human would do. If the agent\u2019s policy \u201csurprises\u201d the human by being sub-optimal, the human presents the agent with the optimal policy. The objective of the agent is to minimize the number of surprises to the human, i.e., to generalize the human\u2019s behavior to new tasks.\nQuite apart from the connection to AI safety, the repeated IRL problem we introduce and our results are of independent interest in resolving the question of unidentifiability\nof rewards from observations in standard IRL. Our contributions include: (1) an efficient identification algorithm when the agent can choose the tasks in which it observes human behavior; (2) an upper bound on the number of total surprises when no assumptions are made on the tasks, along with a corresponding lower bound; (3) an extension to the setting where the human provides sample trajectories instead of complete behavior; and (4) identification guarantees when the agent can only choose the task rewards but is given a fixed task environment."}, {"heading": "2. Markov Decision Processes (MDPs)", "text": "We are interested in environments that can be represented as MDPs. An MDP is specified by its state space S, action space A, initial state distribution \u00b5 \u2208 \u2206(S), transition (or dynamics) function P : S \u00d7 A \u2192 \u2206(S), reward function Y : S \u2192 R, and discount factor \u03b3 \u2208 [0, 1). A policy \u03c0 : S \u2192 A describes an agent\u2019s behavior by specifying the action to take in each state. The (normalized) value function or long-term utility of \u03c0 is defined as V \u03c0(s) = (1 \u2212 \u03b3)E[ \u2211\u221e t=1 \u03b3\nt\u22121Y (st)|s0 = s;\u03c0].1 Similarly, the Q-value function is Q\u03c0(s, a) = (1 \u2212 \u03b3)E[ \u2211\u221e t=1 \u03b3\nt\u22121Y (st)|s0 = s, a0 = a;\u03c0]. Where necessary we will use the notation V \u03c0P,Y to avoid ambiguity about the environment and the reward function used in computing V \u03c0. Let \u03c0\u22c6 : S \u2192 A be an optimal policy, which maximizes V \u03c0 and Q\u03c0 in all states (and actions) simultaneously.\nGiven an initial distribution over states, \u00b5, a scalar value that measures the goodness of \u03c0 is defined as Es\u223c\u00b5[V \u03c0(s)]. We introduce some further notation to define Es\u223c\u00b5[V \u03c0(s)] in vector-matrix form. Let \u03b7\u03c0\u00b5,P \u2208 R|S| be the normalized state occupancy under initial distribution \u00b5, dynamics P , and policy \u03c0, whose s-th entry is (1 \u2212 \u03b3)E[\u2211\u221et=1 \u03b3t\u22121I(st = s)|s0 \u223c \u00b5;\u03c0]. This vector can be computed in closed-form as \u03b7\u03c0\u00b5,P = (1 \u2212 \u03b3) ( \u00b5\u22a4P \u03c0 ( I|S| \u2212 \u03b3P \u03c0 )\u22121)\u22a4 , where P \u03c0 is an |S| \u00d7 |S| matrix whose (s, s\u2032)-th element is P (s\u2032|s, \u03c0(s)), and I|S| is the |S| \u00d7 |S| identity matrix. For convenience we will also treat the reward function Y as a vector in R|S|, and we have\nEs\u223c\u00b5[V \u03c0(s)] = Y \u22a4\u03b7\u03c0\u00b5,P . (1)"}, {"heading": "3. Problem setup", "text": "Here we define the repeated IRL problem. The human\u2019s reward function \u03b8\u22c6 captures his/her safety concerns and intrinsic/general preferences. This \u03b8\u22c6 is unknown to the agent and is the object of interest herein, i.e., if \u03b8\u22c6 were known to the agent, the concerns addressed in this paper would be\n1Here we differ (w.l.o.g.) from common IRL literature in assuming that reward occurs after transition.\nsolved. We assume that the human cannot directly communicate \u03b8\u22c6 to the agent but can evaluate the agent\u2019s behavior in a task as well as demonstrate optimal behavior.\nFormally, a task is defined by a pair (E,R), where E = (S,A, \u00b5, P, \u03b3) is the task environment (i.e., an MDP without a reward function), and R is the task-specific reward function (task reward). We assume that all tasks share the same S,A, \u03b3, with |A| \u2265 2, but may differ in the initial distribution \u00b5, dynamics P , and task reward R; all of the task-specifying quantities are known to the agent. In any task, the human\u2019s optimal behavior is always with respect to the reward function Y := \u03b8\u22c6 + R. We emphasize again that \u03b8\u22c6 is intrinsic to the human and remains the same across all tasks. Our use of task specific reward functions R allows for greater generality than the usual IRL setting, but we note that our results apply equally to the case where the task reward is always zero.\nWhile \u03b8\u22c6 is private to the human, the agent has some prior knowledge on \u03b8\u22c6, represented as a set of possible parameters \u03980 \u2282 R|S| that contains \u03b8\u22c6. Throughout, we assume that the human\u2019s reward has bounded and normalized magnitude, that is, \u2016\u03b8\u22c6\u2016\u221e \u2264 1. A demonstration in (E,R)means revealing \u03c0\u22c6 to the agent, which optimizes for Y := \u03b8\u22c6 + R under environment E. A common assumption in the IRL literature is that the full mapping is revealed, which can be unrealistic if some states are unreachable from the initial distribution. We address the issue by requiring only the state occupancy vector \u03b7\u03c0 \u2217\n\u00b5,P .\nIn Section 7 we show that this also allows an easy extension to the setting where the human only demonstrates trajectories instead of providing a policy.\nUnder the above framework for repeated IRL, we consider two settings that differ in how the sequence of tasks are chosen. In both settings, we will want to minimize the number of demonstrations needed.\n1. (Section 5) Agent chooses the tasks, observes the hu-\nman\u2019s behavior in each of them, and infers the reward function. In this setting where the agent is powerful enough to choose tasks arbitrarily, we will show that the agent will be able to identify the human\u2019s reward function which of course implies the ability to generalize to new tasks.\n2. (Section 6) Nature chooses the tasks, and the agent\nproposes a policy in each task. The human demonstrates a policy only if the agent\u2019s policy is a mistake (a negative surprise), i.e., significantly suboptimal. In this setting we will derive upper and lower bounds on the number of mistakes our agent will make."}, {"heading": "4. The challenge of identifying rewards", "text": "Note that it is impossible to identify \u03b8\u22c6 from watching human behavior in a single task. This is because any \u03b8\u22c6 is fundamentally indistinguishable from an infinite set of reward functions that yield exactly the policy observed in the task. We introduce the idea of behavioral equivalence below to tease apart two separate issues wrapped up in the challenge of identifying rewards.\nDefinition 1. Two reward functions \u03b8, \u03b8\u2032 \u2208 R|S| are behaviorally equivalent in MDP tasks, if for any (E,R), the set of optimal policies for (R+ \u03b8) and (R + \u03b8\u2032) are the same.\nWe argue that the task of identifying the reward function should amount only to identifying the (behaviorally) equivalence class to which \u03b8\u22c6 belongs. In particular, identifying the equivalence class is sufficient to get perfect generalization to new tasks. Any remaining unidentifiability is merely representational and of no real consequence. Next we present a constraint that captures the reward functions that belong to the same equivalence class.\nProposition 1. Two reward functions \u03b8 and \u03b8\u2032 are behaviorally equivalent in MDP tasks if and only if \u03b8 \u2212 \u03b8\u2032 = c \u00b7 1|S| for some c \u2208 R, where 1|S| is an all-1 vector of length |S|.\nProof. To show that \u03b8 \u2212 \u03b8\u2032 = c \u00b7 1|S| implies behavioral equivalence, we note that for any policy \u03c0 the occupancy vector \u03b7\u03c0\u00b5,P always satisfies 1 \u22a4 |S|\u03b7 \u03c0 \u00b5,P = 1, so \u2200\u03c0, |\u03b8T \u03b7\u03c0\u00b5,P \u2212 \u03b8\u2032T \u03b7\u03c0\u00b5,P | = c, and therefore the set of optimal policies is the same.\nTo show the other direction, we prove that if \u03b8 \u2212 \u03b8\u2032 /\u2208 span({1|S|}), then there exists (E,R) such that the sets of optimal policies differ. In particular, we choose R = \u2212\u03b8\u2032, so that all policies are optimal under R + \u03b8\u2032 = 0. Since \u03b8 \u2212 \u03b8\u2032 /\u2208 span({1|S|}), there exists states i and j such that \u03b8(i) + R(i) 6= \u03b8(j) + R(j). Suppose i is the one with smaller sum of rewards, then we can make j an absorbing state, and have two deterministic actions in i that transition to i and j respectively. Under R + \u03b8, the self-loop in state i is suboptimal, and this completes the proof.\nFor any class of \u03b8\u2019s that are equivalent to each other, we can choose a canonical element to represent this class. For example, we can fix an arbitrary reference state sref \u2208 S, and fix the reward of this state to 0 for \u03b8\u22c6 and all candidate \u03b8\u2019s. In the rest of the paper, we will always assume such canonicalization in the MDP setting, hence \u03b8\u22c6 \u2208 \u03980 \u2286 {\u03b8 \u2208 [\u22121, 1]|S| : \u03b8(sref) = 0}."}, {"heading": "5. Agent chooses the tasks", "text": "In this section, the protocol is that the agent chooses a sequence of tasks {(Et, Rt)}. For each task (Et, Rt), the human reveals \u03c0\u22c6t , which is optimal for environment Et and reward function \u03b8\u22c6+Rt. Our goal is to design an algorithm which chooses {(Et, Rt)} and identifies \u03b8\u22c6 to a desired accuracy (\u01eb) using as few tasks as possible."}, {"heading": "5.1. Omnipotent identification algorithm", "text": "Theorem 1 shows that a simple algorithm can identify \u03b8\u22c6 after only O(log(1/\u01eb)) tasks, if any tasks may be chosen. Roughly speaking, the algorithm amounts to a binary search on each component of \u03b8\u22c6 by manipulating the task reward Rt. 2 See the proof for the algorithm specification. Theorem 1. If \u03b8\u22c6 \u2208 \u03980 \u2286 {\u03b8 \u2208 [\u22121, 1]|S| : \u03b8(sref) = 0}, there exists an algorithm that outputs \u03b8 \u2208 R|S| that satisfies \u2016\u03b8 \u2212 \u03b8\u22c6\u2016\u221e \u2264 \u01eb after O(log(1/\u01eb)) demonstrations.\nProof. The algorithm chooses the following fixed environment in all tasks: for each s \u2208 S \\{sref}, let one action be a self-loop, and the other action transitions to sref. In sref, all actions cause self-loops. The initial distribution over states is uniformly at random over S \\ {sref}. Each task only differs in the task reward Rt (where Rt(sref) \u2261 0 always). After observing the state occupancy of the optimal policy, for each s we check if the occupancy is equal to 0. If so, it means that the demonstrated optimal policy chooses to go to sref from s in the first time step, and \u03b8\u22c6(s) + Rt(s) \u2264 \u03b8\u22c6(sref) + Rt(sref) = 0; if not, we have \u03b8\u22c6(s) +Rt(s) \u2265 0. Consequently, after each task we learn the relationship between \u03b8\u22c6(s) and \u2212Rt(s) on each s \u2208 S \\{sref}, so conducting a binary search by manipulating Rt(s) will identify \u03b8\u22c6 to \u01eb-accuracy after O(log(1/\u01eb)) tasks.\nAs noted before, once the agent has identified \u03b8\u22c6 within an appropriate tolerance, it can compute a sufficiently-nearoptimal policy for all tasks, thus completing the generalization objective through the far stronger identification objective in this setting."}, {"heading": "6. Nature chooses the tasks", "text": "While Theorem 1 yields a strong identification guarantee, it also relies on a strong assumption, that {(Et, Rt)} may be chosen by the agent in an arbitrary manner. In this section, we let nature, who is allowed to be adversarial for the\n2While we present a proof that manipulates Rt, an only slightly more complex proof applies to the setting where all the Rt are exactly zero and the manipulation is limited to the environment; see full details in the previous version of the paper on arXiv (Amin & Singh, 2016).\npurpose of the analysis, choose {(Et, Rt)}. Generally speaking, we cannot obtain identification guarantees in such an adversarial setup. As an example, if Rt \u2261 0 and Et remains the same over time, we are essentially back to the classical IRL setting and suffer from the degeneracy issue. However, generalization to future tasks, which is our ultimate goal, is easy in this special case: after the initial demonstration, the agent can mimic it to behave optimally in all subsequent tasks without requiring further demonstrations.\nMore generally, if nature repeats similar tasks, then the agent obtains little new information, but presumably it knows how to behave in most cases; if nature chooses a task unfamiliar to the agent, then the agent is likely to err, but it may learn about \u03b8\u22c6 from the mistake.\nTo formalize this intuition, we consider the following protocol: the nature chooses a sequence of tasks {(Et, Rt)} in an arbitrary manner. For every task (Et, Rt), the agent proposes a policy \u03c0t. The human examines the policy\u2019s value under \u00b5t, and if the loss\nlt = Es\u223c\u00b5 [ V \u03c0\u22c6t Et, \u03b8\u22c6+Rt (s) ] \u2212 Es\u223c\u00b5 [ V \u03c0tEt, \u03b8\u22c6+Rt(s) ] (2)\nis less than some \u01eb then the human is satisfied and no demonstration is needed; otherwise a mistake is counted and \u03b7 \u03c0\u22c6t \u00b5t,Pt is revealed to the agent (note that \u03b7 \u03c0\u22c6t \u00b5t,Pt can be computed by the agent if needed from \u03c0\u2217t and its knowledge of the task, so the reader can consider the case of the human presenting the policy w.l.o.g.). The main goal of this section is to design an algorithm that has a provable guarantee on the total number of mistakes.\nBefore describing and analyzing our algorithm, we first notice that the Equation 2 can be rewritten as\nlt = (\u03b8\u22c6 +R) \u22a4(\u03b7 \u03c0\u22c6t \u00b5t,Pt \u2212 \u03b7\u03c0t\u00b5t,Pt), (3)\nusing Equation 1. So effectively, the given environment Et in each round defines a set of state occupancy vectors {\u03b7\u03c0\u00b5t,Pt : \u03c0 \u2208 (S \u2192 A)}, and we want the agent to choose the vector that has the largest dot product with \u03b8\u22c6+R. The exponential size of the set will not be a concern because our main result (Theorem 2) has no dependence on the number of vectors, and only depends on the dimension of those vectors. The result is enabled by studying the linear bandit version of the problem, which subsumes the MDP setting for our purpose and is also a model of independent interest."}, {"heading": "6.1. The linear bandit setting", "text": "In the linear bandit setting, D is a finite action space with size |D| = K . Each task is denoted as a pair (X,R). X = [x(1) \u00b7 \u00b7 \u00b7 x(K)] is a d\u00d7K feature matrix, where x(i) is the feature vector for the i-th action, and \u2016x(i)\u20161 \u2264 1.\nWhen we reduce MDPs to linear bandits, each element of D corresponds to an MDP policy, and the feature vector is the state occupancy of that policy.\nAs before,R, \u03b8\u22c6 \u2208 Rd are the task reward and the human\u2019s unknown reward, respectively. The initial uncertainty set for \u03b8\u22c6 is \u03980 \u2286 [\u22121, 1]d. The value of the i-th action is calculated as (\u03b8\u22c6+R)\n\u22a4x(i), and a\u22c6 is the action that maximizes this value. Every round the agent proposes an action a \u2208 D, whose loss is defined as\nlt = (\u03b8\u22c6 +R) \u22a4(xa \u22c6 \u2212 xa).\nWe now show how to embed the previous MDP setting in linear bandits.\nExample 1. Given an MDP problem with variables S,A, \u03b3, \u03b8\u22c6, sref,\u03980, {(Et, Rt)}, we can convert it into a linear bandit problem as follows. All variables with prime belong to the linear bandit problem, and we use v\\i to denote the vector v with the i-th coordinate removed.\n\u2022 D = {\u03c0 : S \u2192 A}, d = |S| \u2212 1. \u2022 \u03b8\u2032\u22c6 = \u03b8 \\sref \u22c6 ,\u0398 \u2032 0 = {\u03b8\\sref : \u03b8 \u2208 \u03980}. \u2022 x\u03c0t = (\u03b7\u03c0\u00b5t,Pt)\\sref . R\u2032t = R \\sref t \u2212Rt(sref) \u00b7 1d.\nThen for any sequence of policies chosen in the MDP problem, the corresponding sequence of actions in the linear bandit problem suffer exactly the same sequence of losses.\nNote that there is a more straightforward conversion by letting d = |S|, \u03b8\u2032\u22c6 = \u03b8\u22c6,\u0398\u20320 = \u03980, x\u03c0t = \u03b7\u03c0\u00b5t,Pt , R\u2032t = Rt, which also preserves losses. We perform a more succinct conversion in Example 1 by canonicalizing both \u03b8\u22c6 (already assumed) andRt (explicitly done here) and dropping the coordinate for sref in all relevant vectors.\nMDPs with linear rewards In IRL literature, a generalization of the MDP setting is often considered, that reward is linear in state features \u03c6(s) \u2208 Rd (Ng & Russell, 2000; Abbeel & Ng, 2004). In this new setting, \u03b8\u22c6 and R are reward parameters, and the actual reward is the dot product between the reward parameter and \u03c6(s). This new setting can also be reduced to linear bandits similarly to Example 1, except that the state occupancy is replaced by the discounted sum of expected feature values. Our main result, Theorem 2, will still apply automatically, but now the guarantee will only depend on the dimension of the feature space and has no dependence on |S|. We include the conversion below but do not further discuss this setting in the rest of the paper.\nExample 2. Consider an MDP problem with state features, defined by S,A, \u03b3, d \u2208 Z+, \u03b8\u22c6 \u2208 Rd,\u03980 \u2286 [\u22121, 1]d, {(Et, \u03c6t \u2208 Rd, Rt \u2208 Rd)}, where task reward and background reward in state s are \u03b8\u22a4\u22c6 \u03c6t(s) andR\n\u22a4\u03c6t(s) respectively, and \u03b8\u22c6 \u2208 \u03980. Suppose \u2016\u03c6t(s)\u2016\u221e \u2264 1 always\nAlgorithm 1 Ellipsoid Algorithm for Repeated Inverse Reinforcement Learning\n1: Input: \u03980. 2: \u03981 := MVEE(\u03980). 3: for t = 1, 2, . . . do 4: Nature reveals (Xt, Rt). 5: Learner plays at = argmaxa\u2208D c\u22a4t x a t , where ct is the center of \u0398t. 6: if lt > \u01eb then 7: Human reveals a\u22c6t . 8: \u0398t+1 =\nMVEE({\u03b8 \u2208 \u0398t : (\u03b8 \u2212 ct)\u22a4(xa \u22c6 t\nt \u2212 xatt ) \u2265 0}). 9: else\n10: \u0398t+1 = \u0398t. 11: end if 12: end for\nholds, then we can convert it into a linear bandit problem as follows:\n\u2022 D = {\u03c0 : S \u2192 A}; d, \u03b8\u22c6, and Rt remain the same. \u2022 x\u03c0t = (1\u2212 \u03b3) \u2211\u221e h=1 \u03b3\nh\u22121E[\u03c6(sh) |\u00b5t, Pt, \u03c0]/d. Note that the division of d in x\u03c0t is for normalization purpose, so that \u2016x\u03c0t \u20161 \u2264 \u2016\u03c6\u20161/d \u2264 \u2016\u03c6\u2016\u221e \u2264 1."}, {"heading": "6.2. Ellipsoid Algorithm for Repeated Inverse Reinforcement Learning", "text": "We propose Algorithm 1, and provide the mistake bound in the following theorem. Note that the pseudo-code also contains the formal protocol of the process.\nTheorem 2. For \u03980 = [\u22121, 1]d, the number of mistakes made by Algorithm 1 is guaranteed to be O(d2 log(d/\u01eb)).\nTo prove Theorem 2, we quote a result from linear programming literature in Lemma 1, which is found in standard lecture notes (e.g., O\u2019Donnell 2011, Theorem 8.8; see also Gro\u0308tschel et al. 2012, Lemma 3.1.34).\nLemma 1 (Volume reduction in ellipsoid algorithm). Given any non-degenerate ellipsoid B in Rd centered at c \u2208 Rd, and any non-zero vector v \u2208 Rd, let B+ be the minimum-volume enclosing ellipsoid (MVEE) of\n{u \u2208 B : (u\u2212 c)\u22a4v \u2265 0}.\nWe have vol(B+)\nvol(B) \u2264 e\u2212 1 2(d+1) .\nProof of Theorem 2. Whenever a mistake is made and the optimal action a\u22c6t is revealed, we can induce the constraint (Rt+ \u03b8\u22c6) \u22a4(x a\u22c6t t \u2212xatt ) > \u01eb.Meanwhile, since at is greedy w.r.t. ct, we have (Rt + ct) \u22a4(x a\u22c6t t \u2212 xatt ) \u2264 0, where ct is the center of \u0398t as in Line 5. Taking the difference of the\ntwo inequalities, we obtain\n(\u03b8\u22c6 \u2212 ct)\u22a4(xa \u22c6 t t \u2212 xatt ) > \u01eb. (4)\nTherefore, the update rule on Line 8 preserves \u03b8\u22c6 in \u0398t+1. Since the update makes a central cut through the ellipsoid, Lemma 1 applies and the volume shrinks by a multiplicative constant e\u2212 1 2(d+1) every time a mistake is made.\nTo prove the theorem, it remains to upper bound the initial volume and lower bound the terminal volume of \u0398t. We first show that an update never eliminatesB\u221e(\u03b8\u22c6, \u01eb/2), the \u2113\u221e ball centered at \u03b8\u22c6 with radius \u01eb/2. This is because, any eliminated \u03b8 satisfies (\u03b8+ct) \u22a4(x a\u22c6t t \u2212xatt ) < 0. Combining this with Equation 4, we have\n\u01eb < (\u03b8\u22c6 \u2212 \u03b8)\u22a4(xa \u22c6 t\nt \u2212 xatt ) \u2264 \u2016\u03b8\u22c6 \u2212 \u03b8\u2016\u221e\u2016xa \u22c6 t t \u2212 xatt \u20161 \u2264 2\u2016\u03b8\u22c6 \u2212 \u03b8\u2016\u221e.\nThe last step follows from \u2016x\u20161 \u2264 1. So we conclude that any eliminated \u03b8 should be \u01eb/2 far away from \u03b8\u22c6 in \u2113\u221e distance. Therefore, we can lower bound the volume of \u0398t for any t by that of \u03980 \u22c2 B\u221e(\u03b8\u22c6, \u01eb/2), which contains an infinite-norm ball with radius \u01eb/4 in the worst case (when \u03b8\u22c6 is one of\u03980\u2019s vertices). To simplify calculation, we will further relax this \u2113\u221e ball to its inscribed \u21132 ball.\nFinally we put everything together: let MT be the number of mistakes made from round 1 to T , and Cd be the volume of the unit sphere in Rd, we have\nMT 2(d+ 1) \u2264 log(vol(\u03981))\u2212 log(vol(\u0398T+1))\n\u2264 log(Cd( \u221a d)d)\u2212 log(Cd(\u01eb/4)d) = d log\n4 \u221a d\n\u01eb .\nSoMT \u2264 2d(d+ 1) log 4 \u221a d \u01eb = O(d 2 log d\u01eb )."}, {"heading": "6.3. Lower bound", "text": "In Section 5, we get an O(log(1/\u01eb)) upper bound on the number of demonstrations, which has no dependence on |S| (which corresponds to d + 1 in linear bandits). Comparing Theorem 2 to 1, one may wonder whether the polynomial dependence on d is an artifact of the inefficiency of Algorithm 1. We clarify this issue by proving a lower bound, showing that \u2126(d log(1/\u01eb)) mistakes are inevitable in the worst case when nature chooses the tasks. We provide a proof sketch below, and the complete proof is deferred to Appendix D.\nTheorem 3. For any randomized algorithm3 in the linear bandit setting, there always exists \u03b8\u22c6 \u2208 [\u22121, 1]d and 3While our Algorithm 1 is deterministic, randomization is often crucial for online learning in general (Shalev-Shwartz, 2011).\n{(Xt, Rt)} which are fixed before the execution of the algorithm,4 such that the expected number of mistakes made by the algorithm under \u03b8\u22c6 and {(Xt, Rt)} is\u2126(d log(1/\u01eb)).\nProof Sketch. We randomize \u03b8\u22c6 by sampling each element i.i.d. from Unif([\u22121, 1]). We will prove that there exists a strategy of choosing (Xt, Rt) such that any algorithm\u2019s expected number of mistakes is \u2126(d log(1/\u01eb), which proves the theorem as max is no less than average.\nIn our construction,Xt = [0d, ejt ], where jt is some index to be specified. Hence, every round the agent is essentially asked to decided whether \u03b8(jt) \u2265 \u2212Rt(jt). The adversary\u2019s strategy goes in phases, and Rt remains the same during each phase. Every phase has d rounds where jt is enumerated over {1, . . . , d}. The adversary will use Rt to shift the posterior on \u03b8(jt) + Rt(jt) so that it is (approximately) centered around the origin; in this way, the agent has about 1/2 probability to make an error (regardless of the algorithm), and the posterior interval will be halved. Overall, the agent makes d/2 mistakes in each phase, and there will be about log(1/\u01eb) phases in total, which gives the lower bound.\nApplying the lower bound to MDPs The above lower bound is stated for linear bandits. In principle, we need to prove lower bound for MDPs separately, because linear bandits are more general than MDPs for our purpose, and the hard instances in linear bandits may not have correspondingMDP instances. In Lemma 2 below, we show that a certain type of linear bandit instances can always be emulated by MDPs with the same number of actions, and the hard instances constructed in Theorem 3 indeed satisfy the conditions for such a type; in particular, we require the feature vectors to be non-negative and have \u21131 norm bounded by 1. As a corollary, an \u2126(|S| log(1/\u01eb)) lower bound for the MDP setting (even with a small action space |A| = 2) follows directly from Theorem 3.\nLemma 2 (Linear bandit to MDP conversion). Let (X,R) be a linear bandit task, and K be the number of actions. If every xa is non-negative and \u2016xa\u20161 \u2264 1, then there exists an MDP task (E,R\u2032) with d+ 1 states andK actions, such that under some choice of sref, converting (E,R\n\u2032) as in Example 1 recovers the original problem.\nThe proof of this lemma is deferred to Appendix A."}, {"heading": "6.4. On identification when Nature Chooses Tasks", "text": "While Theorem 2 successfully controls the number of total mistakes, it completely avoids the identification problem\n4This means that the lower bound can be realized by an oblivious adversary, who cannot adapt the tasks to the realization of the random variables drawn by the algorithm.\nand does not guarantee to recover \u03b8\u22c6. In this section we explore further conditions under which we can obtain identification guarantees when Nature chooses the tasks.\nThe first condition, stated in Proposition 2, implies that if we have made all the possible mistakes, then we have indeed identified the \u03b8\u22c6, where the identification accuracy is determined by the tolerance parameter \u01eb that defines what is counted as a mistake.\nProposition 2. Consider the linear bandit setting. If there exists T0 such that for any round t \u2265 T0, no more mistakes can be ever made by the algorithm for any choice of (Et, Rt) and any tie-braking mechanism, then we have \u03b8\u22c6 \u2208 B\u221e(cT0 , \u01eb).\nProof. Assume towards contradiction that \u2016cT0 \u2212 \u03b8\u22c6\u2016\u221e > \u01eb. We will choose (Rt, x (1) t , x (2) t ) to make the algorithm err. In particular, let Rt = \u2212cT0 , so that the algorithm acts greedily with respect to 0d. Since 0 \u22a4 d x a t \u2261 0, any action would be a valid choice for the algorithm.\nOn the other hand, \u2016cT0 \u2212 \u03b8\u22c6\u2016\u221e > \u01eb implies that there exists a coordinate j such that |e\u22a4j (\u03b8\u22c6 \u2212 cT0)| > \u01eb, where ej is a basis vector. Let x (1) t = 0d and x (2) t = ej . So the value of action 1 is always 0 under any reward function (including \u03b8\u22c6 + Rt), and the value of action 2 is (\u03b8\u22c6 + Rt) \u22a4x(2)t = (\u03b8\u22c6 \u2212 cT0)\u22a4ej , whose absolute value is greater than \u01eb. At least one of the 2 actions is more than \u01eb suboptimal, and the algorithm may take any of them, so the algorithm can err again.\nWhile Proposition 2 shows that identification is guaranteed if the agent exhausts the mistakes, the agent has no ability to actively fulfill this condition when Nature chooses tasks. For a stronger identification guarantee, we may need to grant the agent some freedom in choosing the tasks.\nIdentification with fixed environment Here we consider a setting that fits in between Section 5 (completely active) and Section 6.1 (completely passive), where the environment E (hence the induced feature vectors {x(1), x(2), . . . , x(K)}) is given and fixed, and the agent can arbitrarily choose the task reward Rt. The goal is to obtain an identification guarantee in this new intermediate setting.\nUnfortunately, a degenerate case can be easily constructed that prevents the revelation of any information about \u03b8\u22c6. In particular, if x(1) = x(2) = . . . = x(K), i.e., the environment is completely uncontrolled, then all actions are equally optimal and nothing can be learned.\nMore generally, if for some non-zero vector v we have v\u22a4x(1) = v\u22a4x(2) = . . . = v\u22a4x(K), then we may never recover \u03b8\u22c6 along the direction of v. In fact, Proposition 1\ncan be viewed as an instance of this result where v = 1|S| (recall that the entries of the state occupancy vector always sum up to 1), and that is why we have to remove such redundancy in Example 1 in order to discuss identification in MDPs. Therefore, to guarantee identification in a fixed environment, the feature vectors must be substantially different in all directions, and we capture this intuition by defining a diversity score spread(X) (Definition 2) and showing that the identification accuracy depends inversely on the score (Theorem 4).\nDefinition 2. Given the feature matrix X =[ x(1) x(2) \u00b7 \u00b7 \u00b7 x(K) ] whose size is d \u00d7 K , define spread(X) as the d-th largest singular value of X\u0303 := X(IK \u2212 1K1K1\u22a4K). Theorem 4. For a fixed feature matrix X , if spread(X) > 0, then there exists a sequence R1, R2, . . . , RT with T = O(d2 log(d/\u01eb)) and a sequence of tie-break choices of the algorithm, such that after round T we have \u2016cT \u2212 \u03b8\u22c6\u2016\u221e \u2264 \u01eb \u221a (K \u2212 1)/2\nspread(X) .\nProof. It suffices to show that in any round t, if \u2016ct \u2212 \u03b8\u22c6\u2016\u221e > \u01eb \u221a (K\u22121)/2\nspread(X) , then lt > \u01eb. The bound on T follows\ndirectly from Theorem 2. Similar to the proof of Proposition 2, our choice of the task reward is Rt = \u2212ct, so that any a \u2208 A would be a valid choice of at, and we will choose the worst action. Note that \u2200a, a\u2032 \u2208 D,\nlt = (\u03b8\u22c6 +Rt) \u22a4(xa \u22c6 t \u2212 xat) \u2265 (\u03b8\u22c6 \u2212 ct)\u22a4(xa \u2212 xa \u2032 ).\nSo it suffices to show that there exists a, a\u2032 \u2208 D, such that (\u03b8\u22c6 \u2212 ct)\u22a4(xa \u2212 xa \u2032 ) > \u01eb. Let yt = \u03b8\u22c6 \u2212 ct, and the\nprecondition implies that \u2016yt\u20162 \u2265 \u2016yt\u2016\u221e > \u01eb \u221a (K\u22121)/2 spread(X) .\nDefine a matrix of size K \u00d7 (K(K \u2212 1))\nD =   1 1 \u00b7 \u00b7 \u00b7 0 \u22121 0 \u00b7 \u00b7 \u00b7 0 0 \u22121 \u00b7 \u00b7 \u00b7 0 . . .\n0 0 \u00b7 \u00b7 \u00b7 \u22121 0 0 \u00b7 \u00b7 \u00b7 1\n  . (5)\nEvery column of this matrix contains exactly one 1 and one \u22121, and the columns enumerate all possible positions of them. With the help of this matrix, we can rewrite the desired result (\u2203 a, a\u2032 \u2208 A, s.t. (\u03b8\u22c6 \u2212 ct)\u22a4(xa \u2212 xa \u2032 ) > \u01eb) as \u2016y\u22a4t XD\u2016\u221e \u2265 \u01eb. We relax the LHS as \u2016y\u22a4t XD\u2016\u221e \u2265 \u2016y\u22a4t XD\u20162/ \u221a K(K \u2212 1), and will provide a lower bound on \u2016y\u22a4t XD\u20162. Note that\ny\u22a4t XD = y \u22a4 t (X\u0303 + (X \u2212 X\u0303))D = y\u22a4t X\u0303D,\nbecause every row of (X \u2212 X\u0303) is some multiple of 1\u22a4K (recall Definition 2), and every column of D is orthogonal to 1K . Let (\u0302\u00b7) be the vector normalized to unit length,\n\u2016y\u22a4t X\u0303D\u20162 = \u2016yt\u20162\u2016y\u0302\u22a4t X\u0303D\u20162\n= \u2016yt\u20162\u2016y\u0302\u22a4t X\u0303\u20162D\u20162 = \u2016yt\u20162\u2016y\u0302\u22a4t X\u0303\u20162\u2016 \u0302 y\u0302\u22a4t X\u0303 D\u20162.\nWe lower bound each of the 3 terms. For the first term, we have the precondition \u2016yt\u20162 > \u01eb \u221a (K\u22121)/2 spread(X) . The second term is X\u0303 left multiplied by a unit vector, so its \u21132 norm can be lower bounded by the smallest non-zero singular value of X\u0303 (recall that X\u0303 is full-rank), which is spread(X).\nTo lower bound the last term, note that DD\u22a4 = 2KIK \u2212 21K1 \u22a4 K , and rows of X\u0303 are orthogonal to 1 \u22a4 K and so is y\u22a4t X\u0303 , so\n\u2016\u0302y\u0302\u22a4t X\u0303 D\u201622 \u2265 inf\u2016z\u20162=1, z\u22a51K z\u22a4DD\u22a4z\n= inf \u2016z\u20162=1, z\u22a51K\nz\u22a4(2KIK \u2212 21K1\u22a4K)z\n= inf \u2016z\u20162=1, z\u22a51K\n2Kz\u22a4z = 2K.\nPutting all the pieces together, we have\n\u2016y\u22a4t X\u0303D\u2016\u221e \u2265 \u2016yt\u20162\u2016y\u0302\u22a4t X\u0303\u20162\u2016 \u0302 y\u0302\u22a4t X\u0303 D\u20162/\n\u221a d\n> \u01eb \u221a (K \u2212 1)/2\nspread(X) \u00b7 spread(X) \u00b7\n\u221a 2K\u221a\nK(K \u2212 1) = \u01eb.\nThe \u221a K dependence in Theorem 4may be of concern asK can be exponentially large. However, Theorem 4 also holds if we replaceX by anymatrix that consists ofX\u2019s columns, so we may choose a small yet most diverse set of columns as to optimize the bound. We also show in Appendix B that Theorem 4 is tight in the worst case."}, {"heading": "7. Working with trajectories", "text": "In previous sections, we have assumed that the human evaluates the agent\u2019s performance based on the state occupancy of the agent\u2019s policy, and demonstrates the optimal policy in terms of state occupancy as well. In practice, we would like to instead assume that for each task, the agent rolls out a trajectory, and the human shows an optimal trajectory if he/she finds the agent\u2019s trajectory unsatisfying. We are still concerned about upper bounding the number of total mistakes, and aim to provide a parallel version of Theorem 2.\nUnlike in traditional IRL, in our setting the agent is also acting, which gives rise to many subtleties. First, the total reward on the agent\u2019s single trajectory is a random variable, and may deviate from the expected value of its policy. Therefore, it is generally impossible to decide if the agent\u2019s\npolicy is near-optimal, and instead we assume that the human can check if each action that the agent takes in the trajectory is near-optimal: when the agent takes a at state s, an error is counted if and only if Q\u22c6(s, a) < V \u22c6(s)\u2212 \u01eb. While this resolves the issue on the agent\u2019s side, how should the human provide his/her optimal trajectory?. The most straightforward protocol is that the human rolls out a trajectory from the specified \u00b5t. We argue that this is not a reasonable protocol for two reasons: (1) in expectation, the reward collected by the human may be less than that by the agent, which is due to us conditioning on the event that an error is spotted; (2) the human may not encounter the problematic state in his/her own trajectory, hence the information provided in the trajectory may be irrelevant.\nTo resolve this issue, we consider a different protocol where the human rolls out a trajectory using optimal policy from the very state where the agent errs.\nNow we discuss how we can prove a parallel of Theorem 2 under this new protocol. First, let\u2019s assume that the demonstration were still given in state occupancy induced by the optimal policy from the problematic state. In this case, we can treat the problematic state as the initial state, thanks to our assumption-free setup about (Et, Rt) (hence \u00b5t). To reduce to our previous solution in Section 6, it remains to show that the notion of error in this section (a suboptimal action) implies the notion of error in Section 6 (a suboptimal policy): let s be the problematic state and \u03c0 be the agent\u2019s policy, we have\nV \u03c0(s) = Q\u03c0(s, \u03c0(s)) \u2264 Q\u22c6(s, \u03c0(s)) < V \u22c6(s)\u2212 \u01eb.\nSo whenever a suboptimal action is spotted in state s, it indeed implies that the agent\u2019s policy is suboptimal for s as the initial state. Hence, we can run Algorithm 1 and Theorem 2 immediately applies.\nTo tackle the remaining issue that the demonstration is in terms of a single trajectory, we will not update \u0398t after each mistake as in Algorithm 1, but only make an update after every mini-batch of mistakes, and aggregate them to form accurate update rules. See Algorithm 2. The choice of batch size n depends on the accuracy we need, and will be determined by the following concentration inequality.\nLemma 3 (Azuma\u2019s inequality for martingales). Suppose {S0, S1, . . . , Sn} is a martingale and |Si \u2212 Si\u22121| \u2264 b almost surely. Then with probability at least 1 \u2212 \u03b4 we have |Sn \u2212 S0| \u2264 b \u221a 2n log(2/\u03b4).\nTheorem 5. \u2200\u03b4 \u2208 (0, 1), with probability at least 1\u2212\u03b4, the number of mistakes made by Algorithm 2 with parameters\n\u03980 = {\u03b8 \u2208 [\u22121, 1]d : \u03b8(sref) = 0}, H = \u2308 log(12/\u01eb)\n1\u2212\u03b3\n\u2309 , and\nn =\n\u2308 log( 4d(d+1) log 6 \u221a d \u01eb\n\u03b4 )\n32\u01eb2\n\u2309 where d = |S| \u2212 1, is at most\nAlgorithm 2 Trajectory version of Algorithm 1 for MDPs\n1: Input: \u03980, H, n. // variables with \u2032 are converted as in Example 1. 2: \u03981 := MVEE(\u0398 \u2032 0), i \u2190 0, Z\u0304 \u2190 0, Z\u0304\u22c6 \u2190 0. 3: for t = 1, 2, . . . do 4: Nature reveals (Et, Rt). 5: Agent rolls-out a trajectory using \u03c0t greedily\nw.r.t. ct + R \u2032 t, where ct is the center of \u0398t.\n6: if agent takes a in swithQ\u22c6(s, a) < V \u22c6(s)\u2212\u01eb then 7: Human produces an H-step trajectory from s,\nwhose empirical state occupancy vector (excluding the sref coordinate) is denoted as z\u0302 \u22c6,H i .\n8: i \u2190 i+ 1, Z\u0304\u22c6 \u2190 Z\u0304\u22c6 + z\u0302\u22c6,Hi . 9: Let zi be the state occupancy of \u03c0t from initial\nstate s, and Z\u0304 \u2190 Z\u0304 + zi. 10: if i = n then 11: \u0398t+1 := MVEE({\u03b8 \u2208 \u0398t : (\u03b8\u2212ct)\u22a4(Z\u0304\u22c6\u2212Z\u0304) \u2265 0}). 12: i \u2190 0, Z\u0304 \u2190 0, Z\u0304\u22c6 \u2190 0. 13: else 14: \u0398t+1 = \u0398t. 15: end if 16: else 17: \u0398t+1 = \u0398t. 18: end if 19: end for\nO\u0303(d 2\n\u01eb2 log( d \u03b4\u01eb )). 5\nThe proof of Theorem 5 is deferred to Appendix E."}, {"heading": "8. Related work & Conclusions", "text": "Most existing work in IRL focused on inferring the reward function using data acquired from a fixed environment (Ng & Russell, 2000; Abbeel & Ng, 2004; Coates et al., 2008; Ziebart et al., 2008; Ramachandran & Amir, 2007; Syed & Schapire, 2007; Regan & Boutilier, 2010). There is prior work on using data collected from multiple \u2014 but exogenously fixed\u2014 environments to predict agent behavior (Ratliff et al., 2006). There are also applications where methods for single-environment MDPs have been adapted to multiple environments (Ziebart et al., 2008). Nevertheless, all these works consider the objective of mimicking an optimal behavior in the presented environment(s), and do not aim at generalization to new tasks.\nIn the economics literature, the problem of inferring an agent\u2019s utility from behavior has long been studied under the heading of utility or preference elicitation (Chajewska et al., 2000; Von Neumann & Morgenstern, 2007; Regan & Boutilier,\n5A log log(1/\u01eb) term is suppressed in O\u0303(\u00b7).\n2009; 2011; Rothkopf & Dimitrakakis, 2011). When these models analyze Markovian environments, they assume a fixed environment where the learner can ask certain types of queries, such as bound queries eliciting whether the reward in a state (and action) is above a threshold. While our result in Section 5 uses similar techniques to elicit the reward function, we do so purely by observing the human\u2019s behavior without external source of information (e.g., query responses).\nThe issue of reward misspecification is often mentioned in AI safety articles (e.g., Bostrom, 2003; Russell et al., 2015; Amodei et al., 2016). These articles mostly discuss the ethical concerns and possible research directions, while our paper develops mathematical formulations and algorithmic solutions. Recently, Hadfield-Menell et al. (2016) proposed cooperative inverse reinforcement learning, where the human and the agent act in the same environment, allowing the human to actively resolve the agent\u2019s uncertainty on the reward function. However, they only consider a single environment (or task), and the unidentifiability issue of IRL still exists. Combining their interesting framework with our resolution to unidentifiability (by multiple tasks) can be an interesting future direction."}, {"heading": "A. Proof of Lemma 2", "text": "The construction is as follows. Choose sref as the initial state, and make all other states absorbing. Let R\u2032(sref) = 0 and R\u2032 restricted on S \\ {sref} coincide with R. The remaining work is to design the transition distribution of each action in sref so that the induced state occupancy matches exactly one column ofX .\nFixing any action a, and let x be the feature that we want to associate a with. The next-state distribution of (sref, a) is as follows: with probability p = 1\u2212\u2016x\u201611\u2212\u03b3\u2016x\u20161 the next-state is sref itself, and the probability of transitioning to the j-th state in S \\ {sref} is 1\u2212\u03b31\u2212\u03b3\u2016x\u20161x(j). Given \u2016x\u20161 \u2264 1 and x \u2265 0, it is easy to verify that this is a valid distribution. Nowwe calculate the occupancy of policy \u03c0(sref) = a. The normalized occupancy on sref is\n(1 \u2212 \u03b3)(p+ \u03b3p2 + \u03b32p3 + \u00b7 \u00b7 \u00b7 ) = p(1\u2212 \u03b3) 1\u2212 \u03b3p = 1\u2212 \u2016x\u20161.\nThe remaining occupancy, with a total \u21131 mass of \u2016x\u20161, is split among S \\ {sref} proportional to x. Therefore, when we convert the MDP problem as in Example 1, the corresponding feature vector is exactly x, so we recover the original linear bandit problem."}, {"heading": "B. Theorem 4 is tight in the worst case", "text": "We show that the theorem is tight up to a constant factor in the worst case. Let X = [ U \u2212U ] where Ud\u00d7d is any orthonormal matrix, so K = 2d. This is a valid choice of X because its column vector x satisfies \u2016x\u20161 \u2264 \u2016x\u20162 = 1. All d singular values ofX are \u221a 2, and X\u0303 = X ,\nso spread(X) = \u221a 2, and the bound is \u01eb \u221a 2d\u2212 1/2 = O(\u01eb \u221a d). Since U is arbitrary, we choose its first row to be 1\u22a4d / \u221a d.\nThen we choose an ellipsoid center c and \u03b8\u22c6 that are \u01eb \u221a d/2 different from each other in \u2113\u221e distance, and show that a mistake is impossible. In particular, let c be equal to \u03b8\u22c6 except on the first coordinate where they differ by \u01eb \u221a d/2. Let a be the action taken by the algorithm and a\u22c6 be an optimal action, and R be any task reward, we have\nloss = (\u03b8\u22c6 +R)(x a\u22c6 \u2212 xa)\n\u2264 (\u03b8\u22c6 +R)(xa \u22c6 \u2212 xa)\u2212 (c+R)(xa\u22c6 \u2212 xa) = (\u03b8\u22c6 \u2212 c)(xa \u22c6 \u2212 xa) = |\u03b8\u22c6(1)\u2212 c(1)||xa \u22c6\n(1)\u2212 xa(1)| \u2264 \u01eb \u221a d/2 \u00b7 (2/ \u221a d) = \u01eb.\nIn addition, note that the same construction also works if we rescale X with any multiplicative constant C \u2208 (0, 1),\nhence the bound is tight in the worst case not only for spread(X) = \u221a 2, but for a range spread(X) \u2208 (0, \u221a 2].\nC. Bounding the \u2113 \u221e distance between \u03b8\u22c6 and\nthe ellipsoid center\nTo prove Theorem 5, we need an upper bound on \u2016\u03b8\u22c6 \u2212 c\u2016\u221e for quantifying the error due toH-step truncation and sampling effects, where c is the ellipsoid center. As far as we know there is no standard result on this issue. However, a simple workaround, described below, allows us to assume \u2016\u03b8\u22c6 \u2212 c\u2016\u221e \u2264 2 without loss of generality. Whenever \u2016c\u2016\u221e > 1, there exists coordinate j such that |cj | > 1. We can make a central cut e\u22a4j (\u03b8 \u2212 c) < 0 (or > 0 depending on the sign of cj), and replace the original ellipsoid with the MVEE of the remaining shape. This operation never excludes any point in \u03980, hence it allows the proofs of Theorem 2 and 5 to work. We keep making such cuts and update the ellipsoid accordingly, until the new center satisfies \u2016c\u2016\u221e \u2264 1. Since central cuts reduce volume substantially (Lemma 1) and there is a lower bound on the volume, the process must stop after finite number of operations. After the process stops, we have \u2016\u03b8\u22c6 \u2212 c\u2016\u221e \u2264 \u2016\u03b8\u22c6\u2016\u221e + \u2016c\u2016\u221e \u2264 2."}, {"heading": "D. Proof of Theorem 3", "text": "As a standard trick, we randomize \u03b8\u22c6 by sampling each element i.i.d. from Unif([\u22121, 1]). We will prove that there exists a strategy of choosing (Xt, Rt) such that any algorithm\u2019s expected number of mistakes is \u2126(d log(1/\u01eb), where the expectation is with respect to the randomness of \u03b8\u22c6 and the internal randomness of the algorithm. This immediately implies a worst-case result as max is no less than average (regarding the sampling of \u03b8\u22c6).\nIn our construction,Xt = [0d, ejt ], where jt is some index to be specified. Hence, every round the agent is essentially asked to decided whether \u03b8(jt) \u2265 \u2212Rt(jt). The adversary\u2019s strategy goes in phases, and Rt remains the same during each phase. Every phase has d rounds where jt is enumerated over {1, . . . , d}. To fully specify the nature\u2019s strategy, it remains to specify Rt for each phase.\nIn the 1st phase, Rt \u2261 0. For each coordinate j, the information revealed to the agent is one of the following: \u03b8\u22c6(j) > \u01eb, \u03b8\u22c6(j) \u2265 \u2212\u01eb, \u03b8\u22c6(j) < \u2212\u01eb, \u03b8\u22c6(j) \u2264 \u01eb. For clarity we first make an simplification, that the revealed information is either \u03b8\u22c6(j) > 0 or \u03b8\u22c6(j) \u2264 0; we will deal with the subtleties related to \u01eb at the end of the proof.\nIn the 2nd phase, we fix Rt as\nRt(j) = { \u22121/2 if \u03b8\u22c6(j) \u2265 0, 1/2 if \u03b8\u22c6(j) < 0.\nSince \u03b8\u22c6 is randomized i.i.d. for each coordinate, the posterior of \u03b8\u22c6 + Rt conditioned on the revealed information is Unif[\u22121/2, 1/2], for any algorithm and any interaction history. Therefore the 2nd phase is almost identical to the 1st phase except that the intervals have shrunk by a factor of 2. Similarly in the 3rd phase we use Rt to offset the posterior of \u03b8\u22c6 +Rt to Unif([\u22121/4, 1/4]), and so on. In phase m, the half-length of the interval is 2\u2212m+1, and the probability that a mistake occurs is at least 1/2 \u2212 \u01eb/2\u2212m+2 for any algorithm. The whole process continues as long as this probability is greater than 0. By linearity of expectation, we can lower bound the total mistakes by the sum of expected mistakes in each phase, which gives\n\u2211\n2\u2212m+1\u2265\u01eb d(1/2\u2212 \u01eb/2\u2212m+2)\n\u2265 \u2211\n2\u2212m+1\u22652\u01eb d \u00b7 1/4 \u2265 \u230alog2(1/\u01eb)\u230bd/4.\nThe above analysis made a simplification that the posterior of \u03b8\u22c6 + Rt in phase m is [\u22122\u2212m+1, 2\u2212m+1]. We now remove the simplification. Note, however, that the actual posterior cannot be too different from this simplified version, and their end points can differ by at most \u01eb. So the error probability is at least 1/2\u2212 2\u01eb/(2\u2212m+2 \u2212 2\u01eb). The rest of the analysis is similar: we count the number of mistakes until the error probability drops below 1/4, and in each of these phases we get at least d/4 mistakes in expectation. The number of such phases is given by\n1/2\u2212 2\u01eb/(2\u2212m+2 \u2212 2\u01eb) \u2265 1/4,\nwhich is satisfied if 2\u2212m+2 \u2265 6\u01eb, so m \u2265 \u230alog2 23\u01eb\u230b. This completes the proof."}, {"heading": "E. Proof of Theorem 5", "text": "Since the update rule is still in the format of a central cut through the ellipsoid, Lemma 1 applies. It remains to show that the update rule preserves \u03b8\u22c6 and a certain volume around it, and then we can follow the same argument as for Theorem 2.\nFixing a mini-batch, let t0 be the round on which the last update occurs, and \u0398 = \u0398t0 , c = ct0 . Note that \u0398t = \u0398 during the collection of the current mini-batch and does not change, and ct = c similarly.\nFor each i = 1, 2, . . . , n, define z\u22c6,Hi as the expected value of z\u0302\u22c6,Hi , where expectation is with respect to the randomness of the trajectory produced by the human, and let z\u22c6i be the infinite-step expected state occupancy. Note that z\u0302\u22c6,Hi , z \u22c6,H i , z \u22c6 i \u2208 R|S|\u22121 because the occupancy on sref is not included.\nAs before, we have \u03b8\u22a4\u22c6 (z \u22c6 i \u2212 zi) > \u01eb and c\u22a4(z\u22c6i \u2212 zi) \u2264 0, so (\u03b8\u22c6 \u2212 c)\u22a4(z\u22c6i \u2212 zi) > \u01eb. Taking average over i, we get (\u03b8\u22c6 \u2212 c)\u22a4( 1n \u2211n i=1 z \u22c6 i \u2212 1n \u2211n i=1 zi) > \u01eb. What we will show next is that (\u03b8\u22c6 \u2212 c)\u22a4( Z\u0304 \u22c6\nn \u2212 Z\u0304n ) > \u01eb/3 for Z\u0304\u22c6 and Z\u0304 on Line 11, which implies that the update rule is valid and has enough slackness for lower bounding the volume of \u0398t as before. Note that\n(\u03b8\u22c6 \u2212 c)\u22a4( Z\u0304 \u22c6 n \u2212 Z\u0304n ) = (\u03b8\u22c6 \u2212 c)\u22a4( 1n \u2211n i=1 z \u22c6 i \u2212 1n \u2211n i=1 zi)\n\u2212 (\u03b8\u22c6 \u2212 c)\u22a4( 1n \u2211n i=1 z \u22c6 i \u2212 1n \u2211n i=1 z \u22c6,H i ) \u2212 (\u03b8\u22c6 \u2212 c)\u22a4( 1n \u2211n i=1 z \u22c6,H i \u2212 1n \u2211n i=1 z\u0302 \u22c6,H i ).\nHere we decompose the expression of interest into 3 terms. The 1st term is lower bounded by \u01eb as shown above, and we will upper bound each of the remaining 2 terms by \u01eb/3. For the 2nd term, since \u2016z\u22c6,Hi \u2212 z\u22c6i \u20161 \u2264 \u03b3H , the \u21131 norm of the average follows the same inequality due to convexity, and we can bound the term using Ho\u0308lder\u2019s inequality given \u2016\u03b8\u22c6 \u2212 c\u2016\u221e \u2264 2 (see details of this result in Appendix C). To verify that the choice of H in the theorem statement is appropriate, we can upper bound the 2nd term as\n2\u03b3H = 2((1\u2212 (1 \u2212 \u03b3)) 11\u2212\u03b3 )log(6/\u01eb) \u2264 2e\u2212 log(6/\u01eb) = \u01eb3 .\nFor the 3rd term, fixing \u03b8\u22c6 and c, the partial sum\u2211i j=1(\u03b8\u22c6 \u2212 c)\u22a4(z \u22c6,H i \u2212 z\u0302 \u22c6,H i ) is a martingale. Since \u2016z\u22c6,Hi \u20161 \u2264 1, \u2016z\u0302 \u22c6,H i \u20161 \u2264 1, and \u2016\u03b8\u22c6 \u2212 c\u2016\u221e \u2264 2, we can initiate Lemma 3 by letting b = 4, and setting n to sufficiently large to guarantee that the 3rd term is upper bounded by \u01eb/3 with high probability.\nGiven (\u03b8\u22c6 \u2212 c)\u22a4( Z\u0304 \u22c6 n \u2212 Z\u0304n ) > \u01eb/3, we can follow exactly the same analysis as for Theorem 2 to show that B\u221e(\u03b8\u22c6, \u01eb/6) is never eliminated, and the number of updates can be bounded by 2d(d + 1) log 12 \u221a d\n\u01eb . The number\nof total mistakes is the number of updates multiplied by n, the size of the mini-batches. Via Lemma 3, we can verify that the choice of n in the theorem statement satisfies |\u2211ij=1(\u03b8\u22c6 \u2212 c)\u22a4(z \u22c6,H i \u2212 z\u0302 \u22c6,H i )| \u2264 n\u01eb/3 with probability\nat least 1\u2212\u03b4/ ( 2d(d+ 1) log 12 \u221a d\n\u01eb\n) . Union bounding over\nall updates and the total failure probability can be bounded by \u03b4."}], "references": [{"title": "Apprenticeship Learning via Inverse Reinforcement Learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In Proceedings of the 21st International Conference on Machine learning,", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["Abbeel", "Pieter", "Coates", "Adam", "Quigley", "Morgan", "Ng", "Andrew Y"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Abbeel et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2007}, {"title": "Towards resolving unidentifiability in inverse reinforcement learning", "author": ["Amin", "Kareem", "Singh", "Satinder"], "venue": "arXiv preprint arXiv:1601.06569,", "citeRegEx": "Amin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2016}, {"title": "Concrete problems in ai safety", "author": ["Amodei", "Dario", "Olah", "Chris", "Steinhardt", "Jacob", "Christiano", "Paul", "Schulman", "John", "Man\u00e9", "Dan"], "venue": "arXiv preprint arXiv:1606.06565,", "citeRegEx": "Amodei et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Amodei et al\\.", "year": 2016}, {"title": "Ethical issues in advanced artificial intelligence. Science Fiction and Philosophy: From Time Travel to Superintelligence", "author": ["Bostrom", "Nick"], "venue": null, "citeRegEx": "Bostrom and Nick.,? \\Q2003\\E", "shortCiteRegEx": "Bostrom and Nick.", "year": 2003}, {"title": "Making rational decisions using adaptive utility elicitation", "author": ["Chajewska", "Urszula", "Koller", "Daphne", "Parr", "Ronald"], "venue": "In AAAI/IAAI, pp", "citeRegEx": "Chajewska et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chajewska et al\\.", "year": 2000}, {"title": "Geometric algorithms and combinatorial optimization, volume 2", "author": ["Gr\u00f6tschel", "Martin", "Lov\u00e1sz", "L\u00e1szl\u00f3", "Schrijver", "Alexander"], "venue": "Springer Science & Business Media,", "citeRegEx": "Gr\u00f6tschel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gr\u00f6tschel et al\\.", "year": 2012}, {"title": "Cooperative inverse reinforcement learning", "author": ["Hadfield-Menell", "Dylan", "Russell", "Stuart J", "Abbeel", "Pieter", "Dragan", "Anca"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hadfield.Menell et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hadfield.Menell et al\\.", "year": 2016}, {"title": "Algorithms for inverse reinforcement learning", "author": ["Ng", "AndrewY", "Russell", "Stuart J"], "venue": "In Proceedings of the 17th International Conference onMachine Learning,", "citeRegEx": "Ng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2000}, {"title": "Bayesian inverse reinforcement learning", "author": ["Ramachandran", "Deepak", "Amir", "Eyal"], "venue": "Urbana, 51:61801,", "citeRegEx": "Ramachandran et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ramachandran et al\\.", "year": 2007}, {"title": "Maximum margin planning", "author": ["Ratliff", "Nathan D", "Bagnell", "J Andrew", "Zinkevich", "Martin A"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Regret-based reward elicitation for markov decision processes", "author": ["Regan", "Kevin", "Boutilier", "Craig"], "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Regan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Regan et al\\.", "year": 2009}, {"title": "Robust policy computation in reward-uncertain mdps using nondominated policies", "author": ["Regan", "Kevin", "Boutilier", "Craig"], "venue": "In AAAI,", "citeRegEx": "Regan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Regan et al\\.", "year": 2010}, {"title": "Eliciting additive reward functions for markov decision processes", "author": ["Regan", "Kevin", "Boutilier", "Craig"], "venue": "In IJCAI Proceedings-International Joint Conference on Artificial Intelligence,", "citeRegEx": "Regan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Regan et al\\.", "year": 2011}, {"title": "Preference elicitation and inverse reinforcement learning", "author": ["Rothkopf", "Constantin A", "Dimitrakakis", "Christos"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Rothkopf et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rothkopf et al\\.", "year": 2011}, {"title": "Research priorities for robust and beneficial artificial intelligence", "author": ["Russell", "Stuart", "Dewey", "Daniel", "Tegmark", "Max"], "venue": "AI Magazine,", "citeRegEx": "Russell et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russell et al\\.", "year": 2015}, {"title": "Online learning and online convex optimization", "author": ["Shalev-Shwartz", "Shai"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Shalev.Shwartz and Shai.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz and Shai.", "year": 2011}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["Syed", "Umar", "Schapire", "Robert E"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Syed et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2007}, {"title": "Theory of games and economic behavior (60th Anniversary Commemorative Edition)", "author": ["Von Neumann", "John", "Morgenstern", "Oskar"], "venue": "Princeton university press,", "citeRegEx": "Neumann et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Neumann et al\\.", "year": 2007}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 15, "context": "This is particularly relevant because IRL is a possible approach to the concern about aligning the agent\u2019s values/goals with those of humans for AI safety as society deploys more capable learning agents that impact more people in more ways (Russell et al., 2015; Amodei et al., 2016).", "startOffset": 240, "endOffset": 283}, {"referenceID": 3, "context": "This is particularly relevant because IRL is a possible approach to the concern about aligning the agent\u2019s values/goals with those of humans for AI safety as society deploys more capable learning agents that impact more people in more ways (Russell et al., 2015; Amodei et al., 2016).", "startOffset": 240, "endOffset": 283}, {"referenceID": 19, "context": "Most existing work in IRL focused on inferring the reward function using data acquired from a fixed environment (Ng & Russell, 2000; Abbeel & Ng, 2004; Coates et al., 2008; Ziebart et al., 2008; Ramachandran & Amir, 2007; Syed & Schapire, 2007; Regan & Boutilier, 2010).", "startOffset": 112, "endOffset": 269}, {"referenceID": 10, "context": "There is prior work on using data collected from multiple \u2014 but exogenously fixed\u2014 environments to predict agent behavior (Ratliff et al., 2006).", "startOffset": 122, "endOffset": 144}, {"referenceID": 19, "context": "There are also applications where methods for single-environment MDPs have been adapted to multiple environments (Ziebart et al., 2008).", "startOffset": 113, "endOffset": 135}, {"referenceID": 15, "context": "The issue of reward misspecification is often mentioned in AI safety articles (e.g., Bostrom, 2003; Russell et al., 2015; Amodei et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 3, "context": "The issue of reward misspecification is often mentioned in AI safety articles (e.g., Bostrom, 2003; Russell et al., 2015; Amodei et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 3, "context": ", 2015; Amodei et al., 2016). These articles mostly discuss the ethical concerns and possible research directions, while our paper develops mathematical formulations and algorithmic solutions. Recently, Hadfield-Menell et al. (2016) proposed cooperative inverse reinforcement learning, where the human and the agent act in the same environment, allowing the human to actively resolve the agent\u2019s uncertainty on the reward function.", "startOffset": 8, "endOffset": 233}], "year": 2017, "abstractText": "How detailed should we make the goals we prescribe to AI agents acting on our behalf in complex environments? Detailed & low-level specification of goals can be tedious and expensive to create, and abstract & high-level goals could lead to negative surprises as the agent may find behaviors that we would not want it to do, i.e., lead to unsafe AI. One approach to addressing this dilemma is for the agent to infer human goals by observing human behavior. This is the Inverse Reinforcement Learning (IRL) problem. However, IRL is generally ill-posed for there are typically many reward functions for which the observed behavior is optimal. While the use of heuristics to select from among the set of feasible reward functions has led to successful applications of IRL to learning from demonstration, such heuristics do not address AI safety. In this paper we introduce a novel repeated IRL problem that captures an aspect of AI safety as follows. The agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human. Each time the human is surprised the agent is provided a demonstration of the desired behavior by the human. We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.", "creator": "LaTeX with hyperref package"}}}