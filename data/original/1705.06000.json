{"id": "1705.06000", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "One Shot Joint Colocalization and Cosegmentation", "abstract": "This paper presents a novel framework in which image cosegmentation and colocalization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. In contrast to multi-task learning paradigm that learns similar tasks using a shared representation, the proposed framework leverages two representations at different levels and simultaneously discriminates between foreground and background at the bounding box and superpixel level using discriminative clustering. We show empirically that constraining the two problems at different scales enables the transfer of semantic localization cues to improve cosegmentation output whereas local appearance based segmentation cues help colocalization. The unified framework outperforms strong baseline approaches, of learning the two problems separately, by a large margin on four benchmark datasets. Furthermore, it obtains competitive results compared to the state of the art for cosegmentation on two benchmark datasets and second best result for colocalization on Pascal VOC 2007.", "histories": [["v1", "Wed, 17 May 2017 04:18:19 GMT  (1271kb,D)", "http://arxiv.org/abs/1705.06000v1", "8 pages, Under Review"]], "COMMENTS": "8 pages, Under Review", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["abhishek sharma"], "accepted": false, "id": "1705.06000"}, "pdf": {"name": "1705.06000.pdf", "metadata": {"source": "CRF", "title": "One shot Joint Colocalization & Cosegmentation", "authors": ["Abhishek Sharma"], "emails": ["asharma@mpi-inf.mpg.de"], "sections": [{"heading": null, "text": "Index Terms\u2014Discriminative clustering, weak supervision, cosegmentation, colocalization, multi-task learning\nF"}, {"heading": "1 INTRODUCTION", "text": "Localizing and segmenting objects in an image is a fundamental problem in computer vision since it facilitates many high level vision tasks such as object recognition, action recognition [39], natural language description of images [40] to name a few. Thus, any advancements in image segmentation and localization algorithm are automatically transferred to the performance of high level tasks [40].\nWith the recent success of deep networks, supervised top down segmentation methods obtain impressive performance [46] by learning on pixel level labelled datasets. The same is true for object detection [19]. However, the amount of annotations required to achieve pixel or bounding box labelled datasets is tremendous [25]. Taking into account the cost of obtaining such annotations, recent work has explored the problem of weakly-supervised object discovery [10], [30], [36], [37]. The degree of supervision used in these problems varies from weak ( positive and negative image-level labels for a target class [38]), very weak ( image level labels e.g. colocalization [9], [21] and cosegmentation [2], [29], and null [30]. In this paper, we focus on colocalization and cosegmentation and use very weak supervision to imply that labels are given only at the image level.\nCosegmentation is the problem of segmenting common foreground regions out of a set of images whereas colocalization aims to localize the common object. Prior work in the supervised setting has used off-the-shelf object detectors to guide the segmentation process [4] and also used segmentation as an initial phase for detection. However, existing work for cosegmentation and colocalization either completely ignores these complimentary cues or use them in a two stage decision process, either as pre-processing step [28] or for post processing [31]. For example, Quan et al. [28] refines the coarse localization heat map obtained by a VGG network [32] to improve cosegmentation. However, it is difficult to recover from errors introduced in the initial stage and the post processing steps are prone to unwanted heuristics.\nThis paper advocates an alternative to the prevalent trends of either ignoring these complimentary cues or placing a clear separation between segmentation and localization. In the weakly supervised scenario, the goal of knowledge transfer between the two tasks becomes even more challenging. The key idea here is to avoid making hard decisions and instead, couple these two problems by linear constraints. We empirically show that constraining the two problems jointly improves the performance of both tasks significantly. Our work, although similar in spirit to the prior work that embeds pixels and parts in a graph [26], [27], builds on the discriminative framework of [1], [17] which utilizes a more powerful top down maximum margin machinery in an unsupervised fashion.\nContrary to the conventional approach of multi task learning. [41], [42] where two (or more) similar tasks are jointly learned using a shared representation, we instead leverage two representations at different scales and enable the transfer of information implicit in these representations during a one shot optimization scheme. More precisely, the proposed formulation exploits the semantic, localization cues of bounding boxes to guide cosegmentation and leverages low level segmentation appearance cues cues at superpixel level to improve colocalization.\nOur contributions are as follows: 1) We propose a novel framework that simultaneously learns to localize and segment common objects in images. The unified framework obtains competitive results compared to the state of the art for cosegmentation on three benchmark datasets and second best result for colocalization on Pascal VOC 2007. 2) We show a novel mechanism to constrain the two problems via linear constraints in an unsupervised way that lifts the output performance of cosegmentation and colocalization by more than 10 points. 3)We provide an extensive evaluation of our approach that shows contribution of novel terms in the objective function and the effect of different cues on three benchmark datasets.\nar X\niv :1\n70 5.\n06 00\n0v 1\n[ cs\n.C V\n] 1\n7 M\nay 2\n01 7\n2"}, {"heading": "2 RELATED WORK", "text": "Supervised Setting. Numerous works have used off-the-shelf object detectors to guide segmentation process. Ladicky et al. [4] used object detections as higher order potentials in a CRF-based segmentation system by encouraging all pixels in the foreground of a detected object to share the same category label as that of the detection. Xu et al. [23] also used bounding box as a weak supervision for semantic segmentation.Vicente et al. [11] introduced the idea of using bounding box for cosegmentation in a supervised setting. Alternatively, segmentation cues have been used before to help detection [6], [31]. Parkhi et al. [6] uses color models from predefined rectangles on cat and dog faces to do GrabCut [44] and improve the predicted bounding box. Hariharan et al. [22] used CNN to simultaneously detect and segment by classifying image regions. All these approaches require ground truth annotation either in the form of bounding boxes or segmented objects during training phase which is challenging to obtain on large scale.\nWeakly Supervised Setting. Rother et al. [7] first introduced the idea of cosegmentation in a relatively simple setting where the same object lies in front of different backgrounds in a pair of images. Since then, many work [2], [12], [14], [28], [33], [34] have been proposed to improve cosegmentation performance which can be broadly classified into discriminative and similarity based approaches. Similarity based approaches [7], [10], [11], [12] exploit the information of having common foreground across images and seek to segment it out by learning the foreground distribution or matching it across images [10], [29]. For example, Faktor & Irani [29] propose to discover the co-occurring regions first, and then perform cosegmentation by mapping between the co-occurring regions across the different images. In contrast, discriminative techniques [2], [3] mainly rely on separating a set of images into most separable clusters while taking care of local spatial consistency. For example,Joulin et al. [2] leverages discriminative clustering [1] to segment out the most discriminative parts in a set of images. However, most of these approaches are tailored only for cosegmentation task and do not use localization cues.\nColocalization is a similar problem [9] where the aim is to localize the common object, given a set of images. It was proposed under different names before. For example, object codetection [5] is similar, but is given additional bounding boxes and correspondence annotations. Deselaers et al. [21] generated candidate bounding boxes and tried to select the correct box within each image using a conditional random field. Cho et al. [30], in contrast, localizes the common object by matching common object parts. However, all these approaches are designed for colocalization alone.\nOur work is mainly inspired by the discriminative framework, proposed first for cosegmentation in Joulin et al. [2] and later extended for colocalization by Tang et al. [9] & Joulin et al. [45]. We first briefly explain the two main components of the discriminative framework of [2].\nDiscriminative clustering. Xu et al. [17] first proposed the idea of using supervised classifier such as SVM to perform unsupervised clustering. It formulates the clustering problem into the following optimization problem :\nmin y\u2208{0,1}n,\u03b1\u2208Rd\nl(y,X\u03b1+ b1) + \u03b2||\u03b1||2, (1)\nwhere X is an n\u00d7d feature matrix (also known as design matrix), l : Rd \u2192 R is some loss function, and \u03b1 a weight vector in Rd and scalar b are the parameters of a linear classifier. When l is the square loss, [1] shows that the problem is equivalent to\nmin y\u2208{0,1}n\nyTDy, (2)\nwhere\nD = \u03a0[Id \u2212X (X T \u03a0X + \u03b2Id)\u22121X T ]\u03a0, (3)\nNote that Id is an identity matrix of dimension d, \u03a0 = Id\u2212 1n11 T is the usual centering projection matrix and D is positive semidefinite. We refer to [1] for more details.\nLocal Spatial Similarity To enforce spatial consistency, a similarity term is combined with the discriminative term yTDy. The\n3 similarity term yTLy is based on the idea of normalised cut [8] that encourages nearby superpixels with similar appearance to have the same label. Thus, a similarity matrix Wi is defined to represent local interactions between superpixels of same image. For any pair of (a, b) of superpixels in image i and for positions pa and color vectors ca, :\nWiab = exp(\u2212\u03bbp||pa \u2212 pb||22 \u2212 \u03bbc||ca \u2212 cb||2)\nThe \u03bbp is set empirically to .001 & \u03bbc to .05. Normalised laplacian matrix is given by:\nL = IN \u2212Q\u22121/2WQ\u22121/2 (4)\nwhere IN is an identity matrix of dimension d, Q is the corresponding diagonal degree matrix, with Qii = \u2211n j=1 wij .\nRest of the paper is organized as follows: Section 3 describes our novel joint framework. Section 4 gives the implementation details while section 5 evaluates it for the task of cosegmentation on three benchmark datasets. We then move on to colocalization experiments. Lastly, we conclude with discussions of empirical and qualitative results."}, {"heading": "3 JOINT COLOCALIZATION & COSEGMENTATION", "text": "Notation. We use italic Roman or Greek letters (e.g., x or \u03b3) for scalars, bold italic fonts (e.g., y = (y1, . . . , yn)T ) for vectors, and calligraphic ones (e.g., C) for matrices. We assume we have m bounding boxes per image."}, {"heading": "3.1 Formulation for one Image", "text": "For the sake of simplicity and clarity, let us first consider a single image, and a set of m bounding boxes per image, with a binary vector z in {0, 1}m such that zi = 1 when bounding box i in {1, . . . ,m} is in the foreground and zi = 0 otherwise. We oversegment the image into n superpixels and define the global superpixel binary vector y in {0, 1}n such that yj = 1 when superpixel number j in {1, . . . , n} is in the foreground and yj = 0 otherwise. We also compute a normalized saliency map M (with values in [0, 1]), and define : s = \u2212log(M).\nAn Image as a collection of bounding boxes. We define our optimization problem (in particular, linear constrains) over bounding box and superpixel level. This requires an additional indexing of superpixels on bounding box level and thus, we maintain the following encoding of superpixels: for each bounding box, we maintain the set Si of its superpixels and define the corresponding indicator vector xi in {0, 1}|Si| such that xij = 1 when superpixel j of bounding box i is in the foreground, and xij = 0 otherwise. Note that x (indexing at bounding box level) and y (indexing at image level) are related by a linear constraint. We define an indicator projection matrix P that encodes the occurrence of a superpixel in all bounding box by 1 and 0 as follows: for every box i, we define a matrix Pi of dimensions |Si| \u00d7 n such that Pij is 1 if superpixel j is present in bounding box i and 0 otherwise.\nOptimization Problem. We propose to combine the objective function defined for cosegmentation and colocalization and thus, define:\nE(y, z) = yT (Ds +\u03b1Ls)y+zTDbz+\u03bdyTss +\u00b5zTsb, (5)\nThe quadratic term zTDbz penalizes the selection of bounding boxes whose features are not easily linearly separable from the other boxes. Similarly, minimizing yTDsy encourages the most discriminative superpixels to be in the foreground. Minimizing the similarity term yTLsy encourages nearby similar superpixels to have same label whereas the linear terms yTss and zTsb encourage selection of salient superpixels and bounding box respectively. Given the feature matrix for superpixels and bounding box, the matrix Ds and Db are computed by Equation 3 whereas Ls is computed by Eq.4. We define the features and value of scalars later in the implementation detail.\nWe now impose appropriate constraints and define the optimization problem as follows:\nmin y,z E(y, z) under the constraints:\n\u03b3|Si|zi \u2264 \u2211 j\u2208Si\nxij \u2264 (1\u2212 \u03b3)|Si|zi for i = 1, . . . ,m, (6)\u2211 i:j\u2208Si xij \u2264 \u2211 i:j\u2208Si zi, for j = 1, . . . , n, (7)\nPi y = xi, for i = 1, . . . ,m. (8) m\u2211 i=1 zi = 1 (9)\nThe constraint (6) guarantees that when a bounding box is in the background, so are all its superpixels, and when it is in the foreground, a proportion of at least \u03b3 and at most (1-\u03b3 of its superpixels are in the foreground as well, with 0 \u2264 \u03b3 \u2264 1. We set \u03b3 to .1. The constraint (7) guarantees that a superpixel is in the foreground for only one box, the foreground box that contains it (only one of the variables zi in the summation can be equal to 1). For each bounding box i, the constraint (8) relates the two indexing of superpixels, x and y, by a projection matrix Pi defined earlier. The constraint (9) guarantees that there is exactly one foreground box per image. We illustrate the above optimization problem by a toy example of 1 image and 2 bounding boxes in appendix at the end.\nIn equations (5)-(9), we obtain an integer quadratic program. Thus, we relax the boolean constraints, allowing y and z to take any value between 0 and 1. The optimization problem becomes convex since all the matrix defined in equation(5)are positive semi-definite [2] and the constraints are linear. Given the solution to the quadratic program, we obtain the bounding box by choosing zi with highest value . For superpixels, since the value of x (and thus y) are upper bounded by z, we first normalize y and then, round the values to 0 (background) and 1 (foreground).\nWhy Joint Optimization. We briefly visit the intuition behind joint optimization. Note that the superpixel variables x and y are bounded by bounding box variable z in Eq. 6 and 7. If the discriminative colocalization part considers some bounding box zi to be background and sets it to close to 0, this , in principle, enforces the cosegmentation part that superpixels in this bounding box are more likely to be background (= 0)as defined by the right hand side of Equation 6: \u2211 j\u2208Si xij \u2264 \u03b4|Si|zi. Similarly, the segmentation cues influence the final score of zi variable if the superpixels inside this bounding box are highly discriminative and more likely to be foreground.\n4"}, {"heading": "4 IMPLEMENTATION DETAILS", "text": "We will release source code of our implementation at the time of publication. We use superpixels obtained from publicly available implementation of [16]. This reduces the size of the matrix Ds,Ls and allows us to optimize at superpixel level. Using the publicly available implementation of [20], we generate 20 bounding boxes for each image. We use unsupervised method of [13] to compute off the shelf saliency maps in our experiments.\nFeatures. Following [2], we densely extract SIFT features at every 4 pixels and kernelize them using Chi-square distance. For each bounding box, we extract 4096 dimensional feature vector using AlexNet [24].\nHyperparameters Following [9], we set \u00b5, the balancing scalar for box saliency, to .001. To set \u03b1, we follow [2] and set it \u03b1 = .1 for foreground objects with fairly uniform colors, and = .001 corresponding to objects with sharp color variations. We empirically set scalar \u03bd = .005 by optimizing over a small set."}, {"heading": "5 EVALUATION OF JOINT FRAMEWORK", "text": "The goal of this section is two fold: First, we propose several baselines that help understand the individual contribution of various cues in the optimization problem defined in section 3.1. Second, we empirically validate and show that learning the two problems jointly significantly improve the performance over learning them individually."}, {"heading": "5.1 Cosegmentation Experiments", "text": ""}, {"heading": "5.1.1 Baseline Methods", "text": "In the section 3.1, we make the following two changes to the cosegmentation framework of [2]: First, we add the saliency cues as a linear term sp to the framework of [2]. Second, we propose to optimize the objective function of [2] with a quadratic program(QP) solver whereas in [2], it is optimized with a semi-definite programming (SDP) solver [18]. To illustrate the importance of saliency cues and better understand the different optimization techniques, we propose the following baseline methods: B1. Discriminative clustering [1] objective is usually optimized with a SDP solver as the semi-definite relaxations are strong and do not suffer from trivial solutions. To validate this, we optimize the objective function of [2] with a quadratic program (QP) solver and compare the results with the SDP solver of [18].\nB2. To quantify the impact of saliency cues, we propose to solve a linear program that obtains an image segmentation by finding the most salient pixels. Thus, we minimize a linear saliency term ss under a linear constraint that minimum number of foreground pixels should be greater than a fraction of total image pixels. This basically means choosing a fraction of the most salient pixels in an image. We set the fraction to .4 as a rough measure of total foreground pixels in MSRC [15] and Object Discovery dataset [10].\nB3. To illustrate the benefits of combining discriminative framework and saliency cues, we solve a QP that optimizes the\nnew objective function of [2] that includes the saliency cues.\nWe denote the results obtained from our joint framework by Ours. In addition to the baselines proposed above and JBP10 [2], we compare our method with four state of the art approaches RJKL13 [10],WHG13 [14], FI13 [29] and QHZN16 [28]. Unless stated otherwise, we measure the segmentation accuracy as the percentage of pixels labeled accurately i.e. average precision (AP)."}, {"heading": "5.1.2 Benchmark Datasets.", "text": "We evaluate the cosegmentation performance of our framework on three benchmark datasets: MSRC [15], Object Discovery dataset [10] and PASCAL-VOC 2010. MSRC contains a subset of 10 object classes, each containing 24 to 30 images. The Object Discovery dataset [10] was collected by downloading images from Internet for airplane, car and horse. It is significantly larger and thus, diverse in terms of viewpoints, texture, color etc. Faktor & Irani [29] collected a subset of PASCAL-VOC 2010 dataset to evaluate the cosegmentation performance. This subset is obtained by choosing images in which the total size of a co-object is at least 1% of the image size. Overall, it contains 1037 images from the 20 PASCAL classes.\nIn Table 1, 2 and 3, we show our results and comparison with other approaches on these three datasets. Note that the results mentioned for JBP10 [2] are obtained by running their open source code and verified with the authors while for others, we simply cite their numbers from their paper. WHG13 [14] shows results on MSRC in two modes: supervised and unsupervised. We compare with unsupervised performance on six classes from their paper. For fair comparison with the state of the art [28] on Object Discovery dataset and PASCAL-VOC 2010, we report performance obtained by applying grab cut [44] based post processing on our output.\nMSRC Dataset In Table 2, we observe that the B1 is consistently outperformed by the SDP solver of JBP10 [2] on both datasets by an average margin of 5 % AP. However, B3 consistently improves the performance of JBP10 [2] by an average of 5 % AP. This shows that the objective function of [2], combined with saliency\n5\ncues, can be optimized efficiently and accurately with a QP solver. Also, only saliency based segmentation, B2, gives a reasonable accuracy of 71% AP. Compared to JBP10 [2], our framework improves the average precision on MSRC dataset by almost 14 %. Our results compete well with RJKL13 [10], on 6 out of 10 classes on MSRC dataset.\nExperiments on Object Discovery Dataset In Table 1, we observe the same trend. We improve upon the result of JBP10 [2] by 14 % and consistent gains over the baselines demonstrate the robustness of the model using localization cues. We outperform RJKL13 on all three classes. We compare well with QHZN16 [28] on classes Car and Horse but worse on aeroplane class.\nExperiments on Pascal VOC 2010 As argued by Faktor & Irani [29], average precision metric is not reliable to evaluate cosegmentation algorithm on this subset as 90 % of overall image content lies in background. Therefore, in addition to average precision(AP), we also evaluate our algorithm using Intersection over union (IoU) metric, also known as Jaccard similarity, in Table 3. We compare with two state of the art approaches that have shown results on this dataset yet: FI13 [29] and QHZN16. We outperform FI13 [29] in both metrics but perform sightly worse than QHZN16 [28].\nQualitative Results In Figure 2 and 3, we provide some examples of our end result. Figure 2 illustrates that our framework considers saliency as one of the various helpful cues and is robust to not\nsalient objects or incorrect saliency maps. For example, in the very first example in Figure 2, the smaller cow is not at all salient and yet, our method rightly segments it as a foreground. In Figure 3, we show some examples from MSRC dataset. on top, we have an original image shown with the selected bounding box and underneath, we show the segmentation of the whole image."}, {"heading": "5.2 Colocalization Experiments", "text": "Evaluation Metrics. We conduct colocalization experiments on PASCAL VOC 2007 [35]. We use two evaluation metrics to compare with state of the art colocalization techniques:\n1) The standard Intersection over union (IoU) metric for object detection(intersection of predicted bounding box area and groundtruth bounding box area divided by the area of their union)\n2) Correct Localization (CorLoc) metric, an evaluation metric used in related work [9], [30], and defined as the percentage of images correctly localized according to the criterion: IoU > .5."}, {"heading": "5.2.1 Baseline Methods", "text": "We analyze individual components of our colocalization model by removing various terms in the objective function and consider the following baselines: Sal. This baseline only minimizes the saliency term for bounding boxes, without any segmentation cue, and picks the most salient one in each image. It is important as it gives an approximate idea about which object classes are more salient in the dataset.\nSal+Disc. This baseline includes the saliency and discriminative term for boxes, without any segmentation cues.\nTJLF14 Tang et al.,TJLF14 [9] tackles colocalization alone without any segmentation spatial support. It quantifies how much we gain in colocalization performance by leveraging segmentation cues.\n6"}, {"heading": "5.2.2 Colocalization evaluation on Pascal VOC 2007", "text": "Following the experimental setup defined in [9], [21], [30], we evaluate our method on the PASCAL07-6x2 subset to compare to previous methods for co-localization. This subset consists of all images from 6 classes (aeroplane, bicycle, boat, bus, horse, and motorbike) of the PASCAL VOC 2007 [35]. Each of the 12 class/viewpoint combinations contains between 21 and 50 images for a total of 463 images. Compared to the Object Discovery dataset, it is significantly more challenging due to considerable clutter, occlusion, and diverse viewpoints.\nCorLoc Metric In Table 4, we report our experiments based on CorLoc Metric and compare them with our baselines proposed before. Note that we use plane instead of Aeroplane and bike instead of Motorbike. We see that results using stripped down versions of our model are not consistent and less reliable. In particular, we observe that on salient classes, saliency term alone, Sal., is a very strong baseline. This can also be partially predicted by looking at the individual images of Bike class. However, it fails badly on classes such as Boat and Bus which are more cluttered, occluded and exhibit huge change in scale space. Sal + Disc improves upon the saliency baseline only\nin some classes such as Plane but overall fails to improve upon TJLF14 without segmentation cues. Our results improve upon both Sal. and Sal + Disc in all classes. This again validates our hypothesis of leveraging segmentation cues to lift the colocalization performance. Our results outperforms TJLF14 [9] on most classes.\nIoU Metric. In Figure 4, we show failure cases of our colocalization results based on CorLoc Metric. In the first row, we show several instances where the localization is near perfect and yet, the bounding box only achieves the CorLoc score of approximately .45. to .49 and thus, counts as a failure case according to CorLoc metric. This is mainly because it does not include the tail or a wing of aeroplane inside bounding box. We observe similar cases in class Horse too. To further support our argument quantitatively, we compare our results based on IoU metric with [30] on Pascal VOC 2007 in Table 5. We could not compare with TJLF14 on IoU metric as their source code and hyper-parameters are not publicly available. IoU metric gives a value of .45 to instances such as shown in Figure 3 whereas CorLoc gives it a score of 0.\nComparison to state of the art. Cho et al., CSP15 [30], outperforms all approaches by a huge margin on CorLoc metric where it obtains an absolute score of 64. This is partially because it leverages part based matching by Hough Transform where the predicted bounding box is selected by a heuristic standout score. In contrast, the discriminative framework of ours does not incorporate any constraints on including parts of objects in the predicted bounding box. This is also partially evident in Table 5 where the margin between our performance and CSP15 on IoU metric is almost half that of CorLoc. Moreover, CSP15, by design, is tailored for colocalization only whereas our framework tackles both colocalization and cosegmentation.\n7"}, {"heading": "6 CONCLUSION & FUTURE WORK", "text": "We proposed a novel framework that jointly learns to localize and segment objects. The proposed formulation is based on two different level of visual representations and uses linear constraints as a means to transfer information implicit in these representations in an unsupervised manner. Although we demonstrate the effectiveness of our approach with a variant of maximum margin clustering, the key idea of transferring knowledge between tasks at different granularity is general and can be incorporated in the framework of constrained CNN [43]. Future work could also extend our model by including an image-video classifier, thereby providing a single framework that simultaneously classify, localize and segment common objects or actions in images and videos respectively."}, {"heading": "7 ACKNOWLEDGEMENT", "text": "This work started as a Master\u2019s thesis project at INRIA Willow team and was partially supported by ERC Advanced grants VideoWorld and Allegro. Many thanks to Armand Joulin for numerous helpful discussions and comments on this paper. The author also thanks anonymous reviewers for their comments."}, {"heading": "8 APPENDIX", "text": "We illustrate our joint colocalization and cosegmentation framework by a simple toy example. Suppose the image contains 5 superpixels. Thus, the global image level superpixel indexing is defined by y = (y1, y2, y3, y4, y5)T . Also, assume that there are two bounding boxes per image and that bounding box 1, z1, contains superpixel 1, 3, 4 while bounding box 2, z2, contains superpixel 1, 2, 4. Thus, bounding box indexing for first proposal z1 is defined by x1 = (y1, y3, y4)T and for z2 is defined by x2 = (y1, y2, y4)\nT . Vector x is obtained by concatenating x1 and x2. Then, vector x1 and vector y are related by an indicator projection matrix P1 as follows:\n[ x1 ] =  y1y3 y4  = 1 0 0 0 00 0 1 0 0 0 0 0 1 0  \ufe38 \ufe37\ufe37 \ufe38\nP1\n\u00d7  y1 y2 y3 y4 y5  \ufe38 \ufe37\ufe37 \ufe38\ny\nNote that matrix P basically tells how many times a superpixel occurs in all bounding boxes or equivalently, how many times yi is duplicated in the vector x. We now translate the other three constraints from the paper one by one. Note that |Si| = 3 since each bounding box contains 3 superpixels, m = 2 and n = 5.\nTo keep it short, we only demonstrate the constraints for the superpixels of the first bounding box(i = 1).\n\u03b3|Si|zi \u2264 \u2211 j\u2208Si xij \u2264 (1\u2212 \u03b3)|Si|zi for i = 1\n\u21d2\u03b3 \u2217 3z1 \u2264 (x11 + x12 + x13) \u2264 (1\u2212 \u03b3) \u2217 3z1\n\u21d2\u03b3 \u2217 3z1 \u2264 (y1 + y3 + y4) \u2264 (1\u2212 \u03b3) \u2217 3z1 (By P1y = x1)\nSimilarly, the second constraint for superpixels is equivalent to:\u2211 i:j\u2208Si xij \u2264 \u2211 i:j\u2208Si zi, for j = 1, 2, 3, 4, 5\n(x11 + x21) \u2264 (z1 + z2)\u21d2 2y1 \u2264 (z1 + z2)\nx22 \u2264 z2 \u21d2 y2 \u2264 z2\nx12 \u2264 z1 \u21d2 y3 \u2264 z1\n(x13 + x23) \u2264 (z1 + z2)\u21d2 2y4 \u2264 (z1 + z2)\nFinally, for the bounding boxes, we have: m\u2211 i=1 zi = 1\u21d2 z1 + z2 = 1"}], "references": [{"title": "DIFFRAC: a discriminative and flexible framework for clustering", "author": ["F. Bach", "Z. Harchaoui"], "venue": "Proc. Neural Info. Proc. Systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Discriminative clustering for image cosegmentation", "author": ["A. Joulin", "F. Bach", "J. Ponce"], "venue": "CVPR", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-class cosegmentation", "author": ["A. Joulin", "F. Bach", "J. Ponce"], "venue": "CVPR", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "What where and how many? combining object detectors and crfs", "author": ["L. Ladicky", "P. Sturgess", "K. Alahari", "C. Russel", "P.H. Torr"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Object co-detection", "author": ["S.Y. Bao", "Y. Xiang", "S. Savarese"], "venue": "In ECCV ,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "The truth about cats and dogs", "author": ["O.M. Parkhi", "A. Vedaldi", "C. Jawahar", "A. Zisserman"], "venue": "ICCV", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Cosegmentation of image pairs by histogram matching - incorporating a global constraint into mrfs", "author": ["C. Rother", "V. Kolmogorov", "T. Minka", "A. Blake"], "venue": "CVPR", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "PAMI, 22(8):888\u2013905", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Co-localization in real-world images", "author": ["K. Tang", "A. Joulin", "L.-J. Li", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["M. Rubinstein"], "venue": "Joulin J. Kopf C. Liu Unsupervised Joint Object Discovery and Segmentation in Internet Images In CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised co-segmentation through region matching", "author": ["J. Rubio", "J. Serrat", "A. Lopez", "N. Paragios"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "J", "author": ["Q. Yan", "L. Xu"], "venue": "Shi and J. Jia Hierarchical Saliency Detection In CVPR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "and L", "author": ["F. Wang", "Q. Huang"], "venue": "Guibas Image Co-Segmentation via Consistent Functional Maps In ICCV", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "TextonBoost: Joint Appearance, Shape and Context Modeling for Mulit-Class Object Recognition and Segmentation InECCV", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Quick shift and kernel methods for mode seeking InECCV", "author": ["A. Vedaldi", "S. Soatto"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Maximum margin clustering InNIPS", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Low rank optimization for semidefinite convex problems", "author": ["M. Journee", "F. Bach", "P.-A. Absil", "R. Sepulchre"], "venue": "In Technical Report,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Measuring the objectness of image windows", "author": ["B. Alexe", "T. Deselaers", "V. Ferrari"], "venue": "PAMI ,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Weakly supervised localization and learning with generic knowledge IJCV", "author": ["T. Deselaers", "B. Alexe", "V. Ferrari"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Simultaneous Detection and Segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Learning to Segment Under Various Forms of Weak Supervision", "author": ["J. Xu", "A. Schwing", "R. Urtasun"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS ,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Learning Visual Features from Large Weakly Supervised Data", "author": ["A. Joulin", "L. Maaten", "A. Jabri", "N. Vasilache"], "venue": "In ECCV,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Concurrent Object Recognition and Segmentation by Graph Partitioning", "author": ["S.X. Yu", "R. Gross", "J. Shi"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Object Detection and Segmentation from Joint Embedding of Parts and Pixels", "author": ["M. Maire", "S.X. Yu", "P. Perona"], "venue": "In ICCV,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Object Co-Segmentation via Graph Optimized-Flexible Manifold Ranking", "author": ["R. Quan", "J. Han", "D. Zhang", "F. Nie"], "venue": "In CVPR, 2016 1,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Co-segmentation by Composition", "author": ["A. Faktor", "M. Irani"], "venue": "In ICCV,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Unsupervised Object Discovery and Localization in the Wild: Part-based Matching with Bottom-up Region Proposals", "author": ["M. Cho", "S. Kwak", "C. Schmid", "J. Ponce"], "venue": "In CVPR,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Image Co-localization by Mimicking a Good Detector\u2019s Confidence Score Distribution", "author": ["Y. Li", "L.Liu", "C. Shen", "A. van den Hengel"], "venue": "In ECCV,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Return of the Devil in the Details: Delving Deep into Convolutional Nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "In BMVC,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "Half-integrality based algorithms for cosegmentation of images", "author": ["L. Mukherjee", "V. Singh", "C.R. Dyer"], "venue": "In CVPR,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Scale invariant cosegmentation for image groups", "author": ["L. Mukherjee", "V. Singh", "J. Peng"], "venue": "In CVPR,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "and A", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn"], "venue": "Zisserman The PASCAL Visual Object Classes Challenge 2007 ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Weakly Supervised Object Localization with Latent Category Learning", "author": ["C. Wang", "W. Ren", "K. Huang", "T. Tan"], "venue": "In ECCV,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Scene recognition and weakly supervised object localization with deformable part-based models", "author": ["M. Pandey", "S. Lazebnik"], "venue": "In ICCV,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}, {"title": "Looking Beyond the Image: Unsupervised Learning for Object Saliency and Detection", "author": ["P. Siva", "C. Russell", "T. Xiang", "L. Agapito"], "venue": "In CVPR,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Recognizing human actions from still images with latent poses", "author": ["W. Yang", "Y. Wang", "G. Mori"], "venue": "In CVPR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping", "author": ["A. Karpathy", "A. Joulin", "L. Fei-Fei"], "venue": "In NIPS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2014}, {"title": "Algorithms and Applications for Multitask Learning", "author": ["R. Caruana"], "venue": "In ICML,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1996}, {"title": "Scalable Multitask Representation Learning for Scene Classification", "author": ["M. Lapin", "B. Schiele", "M. Hein"], "venue": "In CVPR,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Constrained Convolutional Neural Networks for Weakly Supervised Segmentation", "author": ["D. Pathak", "P. Kr\u00e4henb\u00fchl", "T. Darrell"], "venue": "In ICCV,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2015}, {"title": "GrabCut: Interactive Foreground Extraction using Iterated Graph Cuts", "author": ["C. Rother", "V. Kolmogorov", "T. Minka", "A. Blake"], "venue": "In SIGGRAPH,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "Efficient Image and Video Colocalization with Frank-Wolfe Algorithm", "author": ["A. Joulin", "K. Tang", "L. Fei-Fei"], "venue": "In ECCV,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Fully Convolutional Networks for Semantic Segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In CVPR,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2015}], "referenceMentions": [{"referenceID": 37, "context": "Localizing and segmenting objects in an image is a fundamental problem in computer vision since it facilitates many high level vision tasks such as object recognition, action recognition [39], natural language description of images [40] to name a few.", "startOffset": 187, "endOffset": 191}, {"referenceID": 38, "context": "Localizing and segmenting objects in an image is a fundamental problem in computer vision since it facilitates many high level vision tasks such as object recognition, action recognition [39], natural language description of images [40] to name a few.", "startOffset": 232, "endOffset": 236}, {"referenceID": 38, "context": "Thus, any advancements in image segmentation and localization algorithm are automatically transferred to the performance of high level tasks [40].", "startOffset": 141, "endOffset": 145}, {"referenceID": 44, "context": "With the recent success of deep networks, supervised top down segmentation methods obtain impressive performance [46] by learning on pixel level labelled datasets.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "The same is true for object detection [19].", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "However, the amount of annotations required to achieve pixel or bounding box labelled datasets is tremendous [25].", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Taking into account the cost of obtaining such annotations, recent work has explored the problem of weakly-supervised object discovery [10], [30], [36], [37].", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "Taking into account the cost of obtaining such annotations, recent work has explored the problem of weakly-supervised object discovery [10], [30], [36], [37].", "startOffset": 141, "endOffset": 145}, {"referenceID": 34, "context": "Taking into account the cost of obtaining such annotations, recent work has explored the problem of weakly-supervised object discovery [10], [30], [36], [37].", "startOffset": 147, "endOffset": 151}, {"referenceID": 35, "context": "Taking into account the cost of obtaining such annotations, recent work has explored the problem of weakly-supervised object discovery [10], [30], [36], [37].", "startOffset": 153, "endOffset": 157}, {"referenceID": 36, "context": "The degree of supervision used in these problems varies from weak ( positive and negative image-level labels for a target class [38]), very weak ( image level labels e.", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "colocalization [9], [21] and cosegmentation [2], [29], and null [30].", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "colocalization [9], [21] and cosegmentation [2], [29], and null [30].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "colocalization [9], [21] and cosegmentation [2], [29], and null [30].", "startOffset": 44, "endOffset": 47}, {"referenceID": 27, "context": "colocalization [9], [21] and cosegmentation [2], [29], and null [30].", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "colocalization [9], [21] and cosegmentation [2], [29], and null [30].", "startOffset": 64, "endOffset": 68}, {"referenceID": 3, "context": "Prior work in the supervised setting has used off-the-shelf object detectors to guide the segmentation process [4] and also used segmentation as an initial phase for detection.", "startOffset": 111, "endOffset": 114}, {"referenceID": 26, "context": "However, existing work for cosegmentation and colocalization either completely ignores these complimentary cues or use them in a two stage decision process, either as pre-processing step [28] or for post processing [31].", "startOffset": 187, "endOffset": 191}, {"referenceID": 29, "context": "However, existing work for cosegmentation and colocalization either completely ignores these complimentary cues or use them in a two stage decision process, either as pre-processing step [28] or for post processing [31].", "startOffset": 215, "endOffset": 219}, {"referenceID": 26, "context": "[28] refines the coarse localization heat map obtained by a VGG network [32] to improve cosegmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[28] refines the coarse localization heat map obtained by a VGG network [32] to improve cosegmentation.", "startOffset": 72, "endOffset": 76}, {"referenceID": 24, "context": "Our work, although similar in spirit to the prior work that embeds pixels and parts in a graph [26], [27], builds on the discriminative framework of [1], [17] which utilizes a more powerful top down maximum margin machinery in an unsupervised fashion.", "startOffset": 95, "endOffset": 99}, {"referenceID": 25, "context": "Our work, although similar in spirit to the prior work that embeds pixels and parts in a graph [26], [27], builds on the discriminative framework of [1], [17] which utilizes a more powerful top down maximum margin machinery in an unsupervised fashion.", "startOffset": 101, "endOffset": 105}, {"referenceID": 0, "context": "Our work, although similar in spirit to the prior work that embeds pixels and parts in a graph [26], [27], builds on the discriminative framework of [1], [17] which utilizes a more powerful top down maximum margin machinery in an unsupervised fashion.", "startOffset": 149, "endOffset": 152}, {"referenceID": 15, "context": "Our work, although similar in spirit to the prior work that embeds pixels and parts in a graph [26], [27], builds on the discriminative framework of [1], [17] which utilizes a more powerful top down maximum margin machinery in an unsupervised fashion.", "startOffset": 154, "endOffset": 158}, {"referenceID": 39, "context": "[41], [42] where two (or more) similar tasks are jointly learned using a shared representation, we instead leverage two representations at different scales and enable the transfer of information implicit in these representations during a one shot optimization scheme.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41], [42] where two (or more) similar tasks are jointly learned using a shared representation, we instead leverage two representations at different scales and enable the transfer of information implicit in these representations during a one shot optimization scheme.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "[4] used object detections as higher order potentials in a CRF-based segmentation system by encouraging all pixels in the foreground of a detected object to share the same category label as that of the detection.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[23] also used bounding box as a weak supervision for semantic segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Alternatively, segmentation cues have been used before to help detection [6], [31].", "startOffset": 73, "endOffset": 76}, {"referenceID": 29, "context": "Alternatively, segmentation cues have been used before to help detection [6], [31].", "startOffset": 78, "endOffset": 82}, {"referenceID": 5, "context": "[6] uses color models from predefined rectangles on cat and dog faces to do GrabCut [44] and improve the predicted bounding box.", "startOffset": 0, "endOffset": 3}, {"referenceID": 42, "context": "[6] uses color models from predefined rectangles on cat and dog faces to do GrabCut [44] and improve the predicted bounding box.", "startOffset": 84, "endOffset": 88}, {"referenceID": 20, "context": "[22] used CNN to simultaneously detect and segment by classifying image regions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] first introduced the idea of cosegmentation in a relatively simple setting where the same object lies in front of different backgrounds in a pair of images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Since then, many work [2], [12], [14], [28], [33], [34] have been proposed to improve cosegmentation performance which can be broadly classified into discriminative and similarity based approaches.", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "Since then, many work [2], [12], [14], [28], [33], [34] have been proposed to improve cosegmentation performance which can be broadly classified into discriminative and similarity based approaches.", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "Since then, many work [2], [12], [14], [28], [33], [34] have been proposed to improve cosegmentation performance which can be broadly classified into discriminative and similarity based approaches.", "startOffset": 33, "endOffset": 37}, {"referenceID": 26, "context": "Since then, many work [2], [12], [14], [28], [33], [34] have been proposed to improve cosegmentation performance which can be broadly classified into discriminative and similarity based approaches.", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "Since then, many work [2], [12], [14], [28], [33], [34] have been proposed to improve cosegmentation performance which can be broadly classified into discriminative and similarity based approaches.", "startOffset": 45, "endOffset": 49}, {"referenceID": 32, "context": "Since then, many work [2], [12], [14], [28], [33], [34] have been proposed to improve cosegmentation performance which can be broadly classified into discriminative and similarity based approaches.", "startOffset": 51, "endOffset": 55}, {"referenceID": 6, "context": "Similarity based approaches [7], [10], [11], [12] exploit the information of having common foreground across images and seek to segment it out by learning the foreground distribution or matching it across images [10], [29].", "startOffset": 28, "endOffset": 31}, {"referenceID": 9, "context": "Similarity based approaches [7], [10], [11], [12] exploit the information of having common foreground across images and seek to segment it out by learning the foreground distribution or matching it across images [10], [29].", "startOffset": 33, "endOffset": 37}, {"referenceID": 10, "context": "Similarity based approaches [7], [10], [11], [12] exploit the information of having common foreground across images and seek to segment it out by learning the foreground distribution or matching it across images [10], [29].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "Similarity based approaches [7], [10], [11], [12] exploit the information of having common foreground across images and seek to segment it out by learning the foreground distribution or matching it across images [10], [29].", "startOffset": 212, "endOffset": 216}, {"referenceID": 27, "context": "Similarity based approaches [7], [10], [11], [12] exploit the information of having common foreground across images and seek to segment it out by learning the foreground distribution or matching it across images [10], [29].", "startOffset": 218, "endOffset": 222}, {"referenceID": 27, "context": "For example, Faktor & Irani [29] propose to discover the co-occurring regions first, and then perform cosegmentation by mapping between the co-occurring regions across the different images.", "startOffset": 28, "endOffset": 32}, {"referenceID": 1, "context": "In contrast, discriminative techniques [2], [3] mainly rely on separating a set of images into most separable clusters while taking care of local spatial consistency.", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "In contrast, discriminative techniques [2], [3] mainly rely on separating a set of images into most separable clusters while taking care of local spatial consistency.", "startOffset": 44, "endOffset": 47}, {"referenceID": 1, "context": "[2] leverages discriminative clustering [1] to segment out the most discriminative parts in a set of images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[2] leverages discriminative clustering [1] to segment out the most discriminative parts in a set of images.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "Colocalization is a similar problem [9] where the aim is to localize the common object, given a set of images.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "For example, object codetection [5] is similar, but is given additional bounding boxes and correspondence annotations.", "startOffset": 32, "endOffset": 35}, {"referenceID": 19, "context": "[21] generated candidate bounding boxes and tried to select the correct box within each image using a conditional random field.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30], in contrast, localizes the common object by matching common object parts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] and later extended for colocalization by Tang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] & Joulin et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "[45].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "We first briefly explain the two main components of the discriminative framework of [2].", "startOffset": 84, "endOffset": 87}, {"referenceID": 15, "context": "[17] first proposed the idea of using supervised classifier such as SVM to perform unsupervised clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "When l is the square loss, [1] shows that the problem is equivalent to", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "We refer to [1] for more details.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "similarity term yLy is based on the idea of normalised cut [8] that encourages nearby superpixels with similar appearance to have the same label.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "We also compute a normalized saliency map M (with values in [0, 1]), and define : s = \u2212log(M).", "startOffset": 60, "endOffset": 66}, {"referenceID": 1, "context": "The optimization problem becomes convex since all the matrix defined in equation(5)are positive semi-definite [2] and the constraints are linear.", "startOffset": 110, "endOffset": 113}, {"referenceID": 14, "context": "We use superpixels obtained from publicly available implementation of [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "Using the publicly available implementation of [20], we generate 20 bounding boxes for each image.", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "We use unsupervised method of [13] to compute off the shelf saliency maps in our experiments.", "startOffset": 30, "endOffset": 34}, {"referenceID": 1, "context": "Following [2], we densely extract SIFT features at every 4 pixels and kernelize them using Chi-square distance.", "startOffset": 10, "endOffset": 13}, {"referenceID": 22, "context": "For each bounding box, we extract 4096 dimensional feature vector using AlexNet [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "Hyperparameters Following [9], we set \u03bc, the balancing scalar for box saliency, to .", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "To set \u03b1, we follow [2] and set it \u03b1 = .", "startOffset": 20, "endOffset": 23}, {"referenceID": 1, "context": "1, we make the following two changes to the cosegmentation framework of [2]: First, we add the saliency cues as a linear term sp to the framework of [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "1, we make the following two changes to the cosegmentation framework of [2]: First, we add the saliency cues as a linear term sp to the framework of [2].", "startOffset": 149, "endOffset": 152}, {"referenceID": 1, "context": "Second, we propose to optimize the objective function of [2] with a quadratic program(QP) solver whereas in [2], it is optimized with a semi-definite programming (SDP) solver [18].", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "Second, we propose to optimize the objective function of [2] with a quadratic program(QP) solver whereas in [2], it is optimized with a semi-definite programming (SDP) solver [18].", "startOffset": 108, "endOffset": 111}, {"referenceID": 16, "context": "Second, we propose to optimize the objective function of [2] with a quadratic program(QP) solver whereas in [2], it is optimized with a semi-definite programming (SDP) solver [18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "Discriminative clustering [1] objective is usually optimized with a SDP solver as the semi-definite relaxations are strong and do not suffer from trivial solutions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "To validate this, we optimize the objective function of [2] with a quadratic program (QP) solver and compare the results with the SDP solver of [18].", "startOffset": 56, "endOffset": 59}, {"referenceID": 16, "context": "To validate this, we optimize the objective function of [2] with a quadratic program (QP) solver and compare the results with the SDP solver of [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 13, "context": "4 as a rough measure of total foreground pixels in MSRC [15] and Object Discovery dataset [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 9, "context": "4 as a rough measure of total foreground pixels in MSRC [15] and Object Discovery dataset [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 1, "context": "new objective function of [2] that includes the saliency cues.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "In addition to the baselines proposed above and JBP10 [2], we compare our method with four state of the art approaches RJKL13 [10],WHG13 [14], FI13 [29] and QHZN16 [28].", "startOffset": 54, "endOffset": 57}, {"referenceID": 9, "context": "In addition to the baselines proposed above and JBP10 [2], we compare our method with four state of the art approaches RJKL13 [10],WHG13 [14], FI13 [29] and QHZN16 [28].", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "In addition to the baselines proposed above and JBP10 [2], we compare our method with four state of the art approaches RJKL13 [10],WHG13 [14], FI13 [29] and QHZN16 [28].", "startOffset": 137, "endOffset": 141}, {"referenceID": 27, "context": "In addition to the baselines proposed above and JBP10 [2], we compare our method with four state of the art approaches RJKL13 [10],WHG13 [14], FI13 [29] and QHZN16 [28].", "startOffset": 148, "endOffset": 152}, {"referenceID": 26, "context": "In addition to the baselines proposed above and JBP10 [2], we compare our method with four state of the art approaches RJKL13 [10],WHG13 [14], FI13 [29] and QHZN16 [28].", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "We evaluate the cosegmentation performance of our framework on three benchmark datasets: MSRC [15], Object Discovery dataset [10] and PASCAL-VOC 2010.", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "We evaluate the cosegmentation performance of our framework on three benchmark datasets: MSRC [15], Object Discovery dataset [10] and PASCAL-VOC 2010.", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "The Object Discovery dataset [10] was collected by downloading images from Internet for airplane, car and horse.", "startOffset": 29, "endOffset": 33}, {"referenceID": 27, "context": "Faktor & Irani [29] collected a subset of PASCAL-VOC 2010 dataset to evaluate the cosegmentation performance.", "startOffset": 15, "endOffset": 19}, {"referenceID": 1, "context": "Note that the results mentioned for JBP10 [2] are obtained by running their open source code and verified with the authors while for others, we simply cite their numbers from their paper.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "WHG13 [14] shows results on MSRC in two modes: supervised and unsupervised.", "startOffset": 6, "endOffset": 10}, {"referenceID": 26, "context": "For fair comparison with the state of the art [28] on Object Discovery dataset and PASCAL-VOC 2010, we report performance obtained by applying grab cut [44] based post processing on our output.", "startOffset": 46, "endOffset": 50}, {"referenceID": 42, "context": "For fair comparison with the state of the art [28] on Object Discovery dataset and PASCAL-VOC 2010, we report performance obtained by applying grab cut [44] based post processing on our output.", "startOffset": 152, "endOffset": 156}, {"referenceID": 1, "context": "MSRC Dataset In Table 2, we observe that the B1 is consistently outperformed by the SDP solver of JBP10 [2] on both datasets by an average margin of 5 % AP.", "startOffset": 104, "endOffset": 107}, {"referenceID": 1, "context": "However, B3 consistently improves the performance of JBP10 [2] by an average of 5 % AP.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "This shows that the objective function of [2], combined with saliency", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "Compared to JBP10 [2], our framework improves the average precision on MSRC dataset by almost 14 %.", "startOffset": 18, "endOffset": 21}, {"referenceID": 9, "context": "Our results compete well with RJKL13 [10], on 6 out of 10 classes on MSRC dataset.", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "We improve upon the result of JBP10 [2] by 14 % and consistent gains over the baselines demonstrate the robustness of the model using localization cues.", "startOffset": 36, "endOffset": 39}, {"referenceID": 26, "context": "We compare well with QHZN16 [28] on classes Car and Horse but worse on aeroplane class.", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "Experiments on Pascal VOC 2010 As argued by Faktor & Irani [29], average precision metric is not reliable to evaluate cosegmentation algorithm on this subset as 90 % of overall image content lies in background.", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "We compare with two state of the art approaches that have shown results on this dataset yet: FI13 [29] and QHZN16.", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "We outperform FI13 [29] in both metrics but perform sightly worse than QHZN16 [28].", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "We outperform FI13 [29] in both metrics but perform sightly worse than QHZN16 [28].", "startOffset": 78, "endOffset": 82}, {"referenceID": 33, "context": "We conduct colocalization experiments on PASCAL VOC 2007 [35].", "startOffset": 57, "endOffset": 61}, {"referenceID": 8, "context": "2) Correct Localization (CorLoc) metric, an evaluation metric used in related work [9], [30], and defined as the percentage of images correctly localized according to the criterion: IoU > .", "startOffset": 83, "endOffset": 86}, {"referenceID": 28, "context": "2) Correct Localization (CorLoc) metric, an evaluation metric used in related work [9], [30], and defined as the percentage of images correctly localized according to the criterion: IoU > .", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": ",TJLF14 [9] tackles colocalization alone without any segmentation spatial support.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "Following the experimental setup defined in [9], [21], [30], we evaluate our method on the PASCAL07-6x2 subset to compare to previous methods for co-localization.", "startOffset": 44, "endOffset": 47}, {"referenceID": 19, "context": "Following the experimental setup defined in [9], [21], [30], we evaluate our method on the PASCAL07-6x2 subset to compare to previous methods for co-localization.", "startOffset": 49, "endOffset": 53}, {"referenceID": 28, "context": "Following the experimental setup defined in [9], [21], [30], we evaluate our method on the PASCAL07-6x2 subset to compare to previous methods for co-localization.", "startOffset": 55, "endOffset": 59}, {"referenceID": 33, "context": "This subset consists of all images from 6 classes (aeroplane, bicycle, boat, bus, horse, and motorbike) of the PASCAL VOC 2007 [35].", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "Our results outperforms TJLF14 [9] on most classes.", "startOffset": 31, "endOffset": 34}, {"referenceID": 28, "context": "To further support our argument quantitatively, we compare our results based on IoU metric with [30] on Pascal VOC 2007 in Table 5.", "startOffset": 96, "endOffset": 100}, {"referenceID": 28, "context": ", CSP15 [30], outperforms all approaches by a huge margin on CorLoc metric where it obtains an absolute score of 64.", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "Although we demonstrate the effectiveness of our approach with a variant of maximum margin clustering, the key idea of transferring knowledge between tasks at different granularity is general and can be incorporated in the framework of constrained CNN [43].", "startOffset": 252, "endOffset": 256}], "year": 2017, "abstractText": "This paper presents a novel framework in which image cosegmentation and colocalization are cast into a single optimization problem that integrates information from low level appearance cues with that of high level localization cues in a very weakly supervised manner. In contrast to multi-task learning paradigm that learns similar tasks using a shared representation, the proposed framework leverages two representations at different levels and simultaneously discriminates between foreground and background at the bounding box and superpixel level using discriminative clustering. We show empirically that constraining the two problems at different scales enables the transfer of semantic localization cues to improve cosegmentation output whereas local appearance based segmentation cues help colocalization. The unified framework outperforms strong baseline approaches, of learning the two problems separately, by a large margin on four benchmark datasets. Furthermore, it obtains competitive results compared to the state of the art for cosegmentation on two benchmark datasets and second best result for colocalization on Pascal VOC 2007.", "creator": "LaTeX with hyperref package"}}}