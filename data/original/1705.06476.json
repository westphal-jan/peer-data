{"id": "1705.06476", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-May-2017", "title": "ParlAI: A Dialog Research Software Platform", "abstract": "We introduce ParlAI (pronounced \"par-lay\"), an open-source software platform for dialog research implemented in Python, available at", "histories": [["v1", "Thu, 18 May 2017 08:54:47 GMT  (1008kb,D)", "http://arxiv.org/abs/1705.06476v1", null], ["v2", "Fri, 9 Jun 2017 18:35:03 GMT  (1008kb,D)", "http://arxiv.org/abs/1705.06476v2", null], ["v3", "Thu, 10 Aug 2017 04:17:48 GMT  (1020kb,D)", "http://arxiv.org/abs/1705.06476v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander h miller", "will feng", "adam fisch", "jiasen lu", "dhruv batra", "antoine bordes", "devi parikh", "jason weston"], "accepted": true, "id": "1705.06476"}, "pdf": {"name": "1705.06476.pdf", "metadata": {"source": "CRF", "title": "ParlAI: A Dialog Research Software Platform", "authors": ["Alexander H. Miller", "Will Feng", "Adam Fisch", "Jiasen Lu", "Dhruv Batra", "Antoine Bordes"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The purpose of language is to accomplish communication goals, which typically involve a dialog between two or more communicators (Crystal, 2004). Hence, trying to solve dialog is a fundamental goal for researchers in the NLP community. From a machine learning perspective, building a learning agent capable of dialog is also fundamental for various reasons, chiefly that the solution involves achieving most of the subgoals of the field, and in many cases those subtasks are directly impactful to the task.\nOn the one hand dialog can be seen as a single task (learning how to talk) and on the other hand as thousands of related tasks that require different skills, all using the same input and output format. The task of booking a restaurant, chatting about sports or the news, or answering factual or\nperceptually-grounded questions all fall under dialog. Hence, methods that perform task transfer appear crucial for the end-goal. Memory, logical and commonsense reasoning, planning, learning from interaction, learning compositionality and other AI subgoals also have clear roles in dialog.\nHowever, to pursue these research goals, we require software tools that unify the different dialog sub-tasks and the agents that can learn from them. Working on individual datasets can lead to siloed research, where the overfitting to specific\nar X\niv :1\n70 5.\n06 47\n6v 1\n[ cs\n.C L\n] 1\n8 M\nay 2\n01 7\nqualities of a dataset do not generalize to solving other tasks. For example, methods that do not generalize beyond WebQuestions (Berant et al., 2013) because they specialize on knowledge bases only, SQuAD (Rajpurkar et al., 2016) because they predict start and end context indices (see Sec. 7), or bAbI (Weston et al., 2015) because they use supporting facts or make use of its simulated nature.\nIn this paper we present a software platform, ParlAI (pronounced \u201cpar-lay\u201d), that provides researchers a unified framework for training and testing dialog models, especially multitask training or evaluation over many tasks at once, as well as seamless integration with Amazon Mechanical Turk. Over 20 tasks are supported in the first release, including many popular datasets, see Fig. 1. Included are examples of training neural models with PyTorch and Lua Torch1. Using Theano2 or Tensorflow3 instead is also straightforward.\nThe overarching goal of ParlAI is to build a community-based platform for easy access to both tasks and learning algorithms that perform well on them, in order to push the field forward. This paper describes our goals in detail, and gives a technical overview of the platform."}, {"heading": "2 Goals", "text": "The goals of ParlAI are as follows: A unified framework for development of dialog models. ParlAI aims to unify dialog dataset input formats fed to machine learning agents to a single format, and to standardize evaluation frameworks and metrics as much as possible. Researchers can submit their new tasks and their agent training code to the repository to share with others in order to aid reproducibility, and to better enable follow-on research. General dialog involving many different skills. ParlAI contains a seamless combination of real and simulated language datasets, and encourages multitask model development & evaluation by making multitask models as easy to build as single task ones. This should reduce overfitting of model design to specific datasets and encourage models that perform task transfer, an important prerequisite for a general dialog agent. Real dialog with people. ParlAI allows collecting, training and evaluating on live dialog with\n1 http://pytorch.org/ and http://torch.ch/ 2 http://deeplearning.net/software/theano/ 3 https://www.tensorflow.org/\nhumans via Amazon Mechanical Turk by making it easy to connect Turkers with a dialog agent. This also enables comparison of Turk experiments across different research groups, which has been historically difficult. Towards a common general dialog model. Our aim is to motivate the building of new tasks and agents that move the field towards a working dialog model. Hence, each new task that goes into the repository should build towards that common goal, rather than being seen solely as a piece of independent research."}, {"heading": "3 General Properties of ParlAI", "text": "ParlAI consists of a number of tasks and agents that can be used to solve them. All the tasks in ParlAI have a single format (API) which makes applying any agent to any task, or multiple tasks at once, simple. The tasks include both fixed supervised/imitation learning datasets (i.e. conversation logs) and interactive (online or reinforcement learning) tasks, as well as both real language and simulated tasks, which can all be seamlessly trained on. ParlAI also supports other media, e.g. images as well as text for visual question answering (Antol et al., 2015) or visually grounded dialog (Das et al., 2017). ParlAI automatically downloads tasks and datasets the first time they are used. One or more Mechanical Turkers can be embedded inside an environment (task) to collect data, train or evaluate learning agents.\nExamples are included in the first release of training with PyTorch and Lua Torch. ParlAI uses ZeroMQ to talk to languages other than Python (such as Lua Torch). Both batch training and hogwild training of models are supported and built into the code. An example main for training an agent is given in Fig. 6."}, {"heading": "4 Worlds, Agents and Teachers", "text": "The main concepts (classes) in ParlAI are worlds, agents and teachers:\n\u2022 world \u2013 the environment. This can vary from very simple, e.g. just two agents talking to each other, to much more complex, e.g. multiple agents in an interactive environment. \u2022 agent \u2013 an agent that can act (especially, speak) in the world. An agent is either a learner (i.e. a machine learned system), a hard-coded bot such as one designed to interact with learners, or a human (e.g. a Turker).\n\u2022 teacher \u2013 a type of agent that talks to the learner in order to teach it, e.g. implements one of the tasks in Fig. 1.\nAfter defining a world and the agents in it, a main loop can be run for training, testing or displaying, which calls the function world.parley() to run one time step of the world. Example code to display data is given in Fig. 6, and the output of running it is in Fig. 5."}, {"heading": "5 Actions and Observations", "text": "All agents (including teachers) speak to each other in a single common format \u2013 the observation/action object (a python dict), see Fig. 3. It is used to pass text, labels and rewards between agents. The same object type is used when talking (acting) or listening (observing), but a different view (i.e. with different values in the fields). Hence, the object is returned from agent.act() and passed in to agent.observe(), see Fig. 6.\nThe fields of the message are as follows: \u2022 text: a speech act. \u2022 id: the speaker\u2019s identity. \u2022 reward: a real-valued reward assigned to the\nreceiver of the message. \u2022 episode done: indicating the end of a dialog. For supervised datasets, there are some additional fields that can be used: \u2022 label: a set of answers the speaker is expect-\ning to receive in reply, e.g. for QA datasets the right answers to a question.\n\u2022 label candidates: a set of possible ways of responding supplied by a teacher, e.g. for multiple choice datasets or ranking tasks. \u2022 text candidates: ranked candidate predictions from a learner. Used to evaluate ranking metrics, rather than just evaluating the single response in the text field. \u2022 metrics: A teacher can communicate to a learning agent metrics on its performance.\nFinally other media can also be supported with additional fields:\n\u2022 image: an image, e.g. for Visual Question Answering or Visual Dialog datasets.\nAs the dict is extensible, we can add more fields over time, e.g. for audio and other sensory data, as well as actions other than speech acts.\nEach of these fields are technically optional, depending on the dataset, though the text field will most likely be used in nearly all exchanges. A typical exchange from a ParlAI training set is shown in Fig. 4."}, {"heading": "6 Code Structure", "text": "The ParlAI codebase has five main directories: \u2022 core: the primary code for the platform. \u2022 agents: contains agents which can interact\nwith the worlds/tasks (e.g. learning models). \u2022 examples: contains examples of different\nmains (display data, training and evaluation).\n\u2022 tasks: contains code for the different tasks available from within ParlAI. \u2022 mturk: contains code for setting up Mechanical Turk, as well as sample MTurk tasks."}, {"heading": "6.1 Core", "text": "The core library contains the following files: \u2022 agents.py: defines the Agent base class for\nall agents, which implements the observe() and act() methods, the Teacher class which also reports metrics, and MultiTaskTeacher for multitask training. \u2022 dialog teacher.py: the base teacher class for doing dialog with fixed chat logs. \u2022 dict.py: code for building language dictionaries. \u2022 metrics.py: computes exact match, F1 and ranking metrics for evaluation. \u2022 params.py: uses argparse to interpret command line arguments for ParlAI \u2022 worlds.py: defines the base World class, DialogPartnerWorld for two speakers, MultiAgentDialogWorld for more than two, and two containers that can wrap a chosen environment: BatchWorld for batch training, and HogwildWorld for training across multiple threads."}, {"heading": "6.2 Agents", "text": "The agents directory contains machine learning agents. Currently available within this directory:\n\u2022 drqa: an attentive LSTM model DrQA (Chen et al., 2017) implemented in PyTorch that has competitive results on SQuAD (Rajpurkar et al., 2016) amongst other datasets. \u2022 memnn: code for an end-to-end memory network (Sukhbaatar et al., 2015) in Lua Torch.\n\u2022 remote agent: basic class for any agent connecting over ZeroMQ. \u2022 ir baseline: simple information retrieval (IR) baseline that scores candidate responses with TFIDF-weighted matching. \u2022 repeat label: basic class for merely repeating all data sent to it (e.g. for debugging)."}, {"heading": "6.3 Examples", "text": "This directory contains examples of different mains:.\n\u2022 display data: display data from a particular task provided on the command-line. \u2022 display model: shows the predictions of a provided model. \u2022 eval model: compute evaluation metrics for a given model on a given task. \u2022 memnn luatorch cpu: training an end-toend memory network (Sukhbaatar et al., 2015). \u2022 drqa: training the attentive LSTM DrQA model of (Chen et al., 2017).\nFor example, one can display 10 random examples from the bAbI tasks (Weston et al., 2015): python display data.py -t babi -n 10\nDisplay multitasking bAbI and SQuAD (Rajpurkar et al., 2016) at the same time:\npython display data.py -t babi,squad\nEvaluate an IR baseline model on the Movies Subreddit:\npython eval model.py -m ir baseline -t\n\u2018#moviedd-reddit\u2019 -dt valid\nTrain an attentive LSTM model on the SQuAD dataset with a batch size of 32 examples:\npython drqa/train.py -t squad -b 32"}, {"heading": "6.4 Tasks", "text": "Over 20 tasks are supported in the first release, including popular datasets such as SQuAD (Rajpurkar et al., 2016), bAbI tasks (Weston et al., 2015), QACNN and QADailyMail (Hermann et al., 2015), CBT (Hill et al., 2015), bAbI Dialog tasks (Bordes and Weston, 2016), Ubuntu (Lowe et al., 2015) and VQA (Antol et al., 2015). All the datasets in the first release are shown in Fig. 14.\nThe tasks are separated into five categories: \u2022 Question answering (QA): one of the sim-\nplest forms of dialog, with only 1 turn per speaker. Any intelligent dialog agent should be capable of answering questions, and there are many kinds of questions (and hence datasets) that one can build, providing a set of very important tests. Question answering is particularly useful in that the evaluation is simpler than other forms of dialog if the dataset is labeled with QA pairs and the questions are mostly unambiguous. \u2022 Sentence Completion (Cloze Tests): in these tests the agent has to fill in a missing word in the next utterance in a dialog. Again, this is very specialized dialog task, but it has the advantage that these datasets are cheap to make and evaluation is simple, which is why the community has built several such datasets. \u2022 Goal-Oriented Dialog: a much more realistic class of dialog is where there is a goal to be achieved by the end of the dialog. For example, a customer and a travel agent discussing a flight, one speaker recommending another a movie to watch, two speakers agreeing when and where to eat together, and so on. \u2022 Chit-Chat: dialog tasks where there may not be an explicit goal, but more of a discussion \u2014 for example two speakers discussing sports, movies or a mutual interest. \u2022 Visual Dialog: dialog is often grounded in physical objects in the world, so we also include visual dialog tasks, with images as well as text. In the future we could also add other sensory information, such as audio.\nChoosing a task in ParlAI is as easy as specifying it on the command line, as shown in the dataset display utility, Fig. 5. If the dataset has not been used before, ParlAI will automatically download it. As all datasets are treated in the same way in\n4All dataset descriptions and references are at http:// parl.ai in the README.md and task list.py.\nParlAI (with a single dialog API, see Sec. 5), a dialog agent can switch training and testing between any of them. Importantly, one can specify many tasks at once (multitasking) by simply providing a comma-separated list, e.g. the command line arguments -t babi,squad, to use those two datasets, or even all the QA datasets at once (-t #qa) or indeed every task in ParlAI at once (-t #all). The aim is to make it easy to build and evaluate very rich dialog models.\nEach task is contained in a folder with the following standardized files:\n\u2022 build.py: file for setting up data for the task, including downloading the data the first time it is requested. \u2022 agents.py: contains teacher class(es), agents that live in the world of the task. \u2022 worlds.py: optionally added for tasks that need to define new/complex environments.\nTo add a new task, one must implement build.py to download any required data, and agents.py, usually with a DefaultTeacher (extending Teacher or one of its children). If the data consists of fixed logs/dialog scripts such as in many supervised datasets (SQuAD, Ubuntu, etc.) there is very little code to write. For more complex setups where an environment and interactive dialogs have to be defined, an act() function must be implemented for the teacher."}, {"heading": "6.5 Mechanical Turk", "text": "An important part of ParlAI is seamless integration with Mechanical Turk for data collection, training or evaluation. Human Turkers are also viewed as agents in ParlAI and hence humanhuman, human-bot, or multiple humans and bots in group chat can all converse within the standard framework, switching out the roles as desired with no code changes to the agents. This is because Turkers also receive and send via the same interface: using the fields of the observation/action dict. We provide two examples in the first release:\n(i) qa collector: an agent that talks to Turkers to collect question-answer pairs given a context paragraph to build a QA dataset, see Fig. 2. (ii) model evaluator: an agent which collects ratings from Turkers on the performance of a bot on a given task.\nRunning a new MTurk task involves running a main file (like run mturk.py) and defining several task specific parameters for the world and agent(s)\nyou wish humans to talk to. For data collection tasks the agent should pose the problem and ask the Turker for e.g. the answers to questions, see Fig. 2. Other parameters include the task description, the role of the Turker in the task, keywords to describe the task, the number of hits and the rewards for the Turkers. One can run in a sandbox mode before launching the real task where Turkers are paid.\nFor online training or evaluation, the Turker can talk to your machine learning agent, e.g. LSTM, memory network or other implemented technique. New tasks can be checked into the repository so researchers can share data collection and data evaluation procedures and reproduce experiments."}, {"heading": "7 Demonstrative Experiment", "text": "To demonstrate ParlAI in action, we give results in Table 1 of DrQA, an attentive LSTM architecture with single task and multitask training on the SQuAD and bAbI tasks, a combination not shown before with any method, to our knowledge.\nThis experiment simultaneously shows the power of ParlAI \u2014 how easy it is to set up this experiment \u2014 and the limitations of current methods. Almost all methods working well on SQuAD have been designed to predict a phrase from the given context (they are given labeled start and end indices in training). Hence, those models cannot be applied to all dialog datasets, e.g. some of the bAbI tasks include yes/no questions, where yes and no do not appear in the context. This highlights that researchers should not focus models on a single dataset. ParlAI does not provide start and end label indices as its API is dialog only, see Fig. 3. This is a deliberate choice that discourages such dataset overfitting/ specialization. However, this also results in a slight drop in performance because less information is given5 (66.4 EM vs. 69.5 EM, see (Chen et al., 2017), which is still in the range of many existing well performing methods, see https://stanford-qa.com).\nOverall, while DrQA can solve some of the bAbI tasks and performs well on SQuAD, it does not match the best performing methods on bAbI (Seo et al., 2016; Henaff et al., 2016), and multitasking does not help. Hence, ParlAI lays out the\n5As we now do not know the location of the true answer, we randomly pick the start and end indices of any context phrase matching the given training set answer, in some cases this is unique.\nchallenge to the community to find learning algorithms that are generally applicable and that benefit from training over many dialog datasets."}, {"heading": "8 Related Software", "text": "There are many existing independent dialog datasets, and training code for individual models that work on some of them. Many are framed in slightly different ways (different formats, with different types of supervision), and ParlAI attempts to unify this fragmented landscape.\nThere are some existing software platforms that are related in their scope, but not in their specialization. OpenAI\u2019s Gym and Universe6 are toolkits for developing and comparing reinforcement learning (RL) algorithms. Gym is for games like Pong or Go, and Universe is for online games and websites. Neither focuses on dialog or covers the case of supervised datasets as we do.\nCommAI7 is a framework that uses textual communication for the goal of developing artificial general intelligence through incremental tasks that test increasingly more complex skills, as described in (Mikolov et al., 2015). CommAI is in a RL setting, and contains only synthetic datasets, rather than real natural language datasets as we do here. In that regard it has a different goal to ParlAI, which focuses on the more immediate task of real dialog, rather than directly on evaluation of machine intelligence.\n6 https://gym.openai.com/ and https://universe.openai.com/ 7 https://github.com/facebookresearch/CommAI-env"}, {"heading": "9 Conclusion and Outlook", "text": "ParlAI is a framework allowing the research community to share existing and new tasks for dialog, agents that learn on them, and to collect and evaluate conversations between agents and humans via Mechanical Turk. We hope this tool enables systematic development and evaluation of dialog agents, helps push the state of the art in dialog further, and benefits the field as a whole."}, {"heading": "Acknowledgments", "text": "We thank Mike Lewis, Denis Yarats, Douwe Kiela, Michael Auli, Y-Lan Boureau, Arthur Szlam, Marc\u2019Aurelio Ranzato, Yuandong Tian, Maximilian Nickel, Martin Raison, Myle Ott, Marco Baroni, Leon Bottou and other members of the FAIR team for discussions helpful to building ParlAI."}], "references": [{"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE International Conference on Computer Vision. pages 2425\u20132433.", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Semantic parsing on freebase from questionanswer pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "EMNLP. volume 2, page 6.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Antoine Bordes", "Jason Weston."], "venue": "arXiv preprint arXiv:1605.07683 .", "citeRegEx": "Bordes and Weston.,? 2016", "shortCiteRegEx": "Bordes and Weston.", "year": 2016}, {"title": "Reading wikipedia to answer open-domain questions", "author": ["Danqi Chen", "Adam Fisch", "Jason Weston", "Antoine Bordes."], "venue": "arXiv:1704.00051 .", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "The Cambridge encyclopedia of the English language", "author": ["David Crystal."], "venue": "Ernst Klett Sprachen.", "citeRegEx": "Crystal.,? 2004", "shortCiteRegEx": "Crystal.", "year": 2004}, {"title": "Learning cooperative visual dialog agents with deep reinforcement learning", "author": ["Abhishek Das", "Satwik Kottur", "Jos\u00e9 MF Moura", "Stefan Lee", "Dhruv Batra."], "venue": "arXiv preprint arXiv:1703.06585 .", "citeRegEx": "Das et al\\.,? 2017", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "Tracking the world state with recurrent entity networks", "author": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun."], "venue": "arXiv preprint arXiv:1612.03969 .", "citeRegEx": "Henaff et al\\.,? 2016", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1693\u20131701.", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301 .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1506.08909 .", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "A roadmap towards machine intelligence", "author": ["Tomas Mikolov", "Armand Joulin", "Marco Baroni."], "venue": "arXiv preprint arXiv:1511.08130 .", "citeRegEx": "Mikolov et al\\.,? 2015", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Query-reduction networks for question answering", "author": ["Minjoon Seo", "Sewon Min", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1606.04582 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "End-to-end memory networks. In Advances in neural information processing systems", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merri\u00ebnboer", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv:1502.05698 .", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": "The purpose of language is to accomplish communication goals, which typically involve a dialog between two or more communicators (Crystal, 2004).", "startOffset": 129, "endOffset": 144}, {"referenceID": 1, "context": "For example, methods that do not generalize beyond WebQuestions (Berant et al., 2013) because they specialize on knowledge bases only, SQuAD (Rajpurkar et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 11, "context": ", 2013) because they specialize on knowledge bases only, SQuAD (Rajpurkar et al., 2016) because they predict start and end context indices (see Sec.", "startOffset": 63, "endOffset": 87}, {"referenceID": 14, "context": "7), or bAbI (Weston et al., 2015) because they use supporting facts or make use of its simulated nature.", "startOffset": 12, "endOffset": 33}, {"referenceID": 0, "context": "images as well as text for visual question answering (Antol et al., 2015) or visually grounded dialog (Das et al.", "startOffset": 53, "endOffset": 73}, {"referenceID": 5, "context": ", 2015) or visually grounded dialog (Das et al., 2017).", "startOffset": 36, "endOffset": 54}, {"referenceID": 3, "context": "Currently available within this directory: \u2022 drqa: an attentive LSTM model DrQA (Chen et al., 2017) implemented in PyTorch that has competitive results on SQuAD (Rajpurkar et al.", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": ", 2017) implemented in PyTorch that has competitive results on SQuAD (Rajpurkar et al., 2016) amongst other datasets.", "startOffset": 69, "endOffset": 93}, {"referenceID": 13, "context": "\u2022 memnn: code for an end-to-end memory network (Sukhbaatar et al., 2015) in Lua Torch.", "startOffset": 47, "endOffset": 72}, {"referenceID": 13, "context": "\u2022 memnn luatorch cpu: training an end-toend memory network (Sukhbaatar et al., 2015).", "startOffset": 59, "endOffset": 84}, {"referenceID": 3, "context": "\u2022 drqa: training the attentive LSTM DrQA model of (Chen et al., 2017).", "startOffset": 50, "endOffset": 69}, {"referenceID": 14, "context": "For example, one can display 10 random examples from the bAbI tasks (Weston et al., 2015):", "startOffset": 68, "endOffset": 89}, {"referenceID": 11, "context": "Display multitasking bAbI and SQuAD (Rajpurkar et al., 2016) at the same time:", "startOffset": 36, "endOffset": 60}, {"referenceID": 11, "context": "Over 20 tasks are supported in the first release, including popular datasets such as SQuAD (Rajpurkar et al., 2016), bAbI tasks (Weston et al.", "startOffset": 91, "endOffset": 115}, {"referenceID": 14, "context": ", 2016), bAbI tasks (Weston et al., 2015), QACNN and QADailyMail (Hermann et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 7, "context": ", 2015), QACNN and QADailyMail (Hermann et al., 2015), CBT (Hill et al.", "startOffset": 31, "endOffset": 53}, {"referenceID": 8, "context": ", 2015), CBT (Hill et al., 2015), bAbI Dialog tasks (Bordes and Weston, 2016), Ubuntu (Lowe et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 2, "context": ", 2015), bAbI Dialog tasks (Bordes and Weston, 2016), Ubuntu (Lowe et al.", "startOffset": 27, "endOffset": 52}, {"referenceID": 9, "context": ", 2015), bAbI Dialog tasks (Bordes and Weston, 2016), Ubuntu (Lowe et al., 2015) and VQA (Antol et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 0, "context": ", 2015) and VQA (Antol et al., 2015).", "startOffset": 16, "endOffset": 36}, {"referenceID": 3, "context": "5 EM, see (Chen et al., 2017), which is still in the range of many existing well performing methods, see https://stanford-qa.", "startOffset": 10, "endOffset": 29}, {"referenceID": 12, "context": "Overall, while DrQA can solve some of the bAbI tasks and performs well on SQuAD, it does not match the best performing methods on bAbI (Seo et al., 2016; Henaff et al., 2016), and multitasking does not help.", "startOffset": 135, "endOffset": 174}, {"referenceID": 6, "context": "Overall, while DrQA can solve some of the bAbI tasks and performs well on SQuAD, it does not match the best performing methods on bAbI (Seo et al., 2016; Henaff et al., 2016), and multitasking does not help.", "startOffset": 135, "endOffset": 174}, {"referenceID": 10, "context": "CommAI7 is a framework that uses textual communication for the goal of developing artificial general intelligence through incremental tasks that test increasingly more complex skills, as described in (Mikolov et al., 2015).", "startOffset": 200, "endOffset": 222}], "year": 2017, "abstractText": "We introduce ParlAI (pronounced \u201cparlay\u201d), an open-source software platform for dialog research implemented in Python, available at http://parl.ai. Its goal is to provide a unified framework for training and testing of dialog models, including multitask training, and integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning. Over 20 tasks are supported in the first release, including popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail, CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Included are examples of training neural models with PyTorch and Lua Torch, including both batch and hogwild training of memory networks and attentive LSTMs.", "creator": "LaTeX with hyperref package"}}}