{"id": "1705.06936", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Atari games and Intel processors", "abstract": "The asynchronous nature of the state-of-the-art reinforcement learning algorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes them exceptionally suitable for CPU computations. However, given the fact that deep reinforcement learning often deals with interpreting visual information, a large part of the train and inference time is spent performing convolutions. In this work we present our results on learning strategies in Atari games using a Convolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0 machine learning framework. We also analyze effects of asynchronous computations on the convergence of reinforcement learning algorithms.", "histories": [["v1", "Fri, 19 May 2017 11:19:45 GMT  (1399kb,D)", "http://arxiv.org/abs/1705.06936v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.LG", "authors": ["robert adamski", "tomasz grel", "maciej klimek", "henryk michalewski"], "accepted": false, "id": "1705.06936"}, "pdf": {"name": "1705.06936.pdf", "metadata": {"source": "CRF", "title": "Atari games and Intel processors", "authors": ["Robert Adamski", "Tomasz Grel", "Maciej Klimek", "Henryk Michalewski"], "emails": ["Robert.Adamski@intel.com,", "T.Grel@deepsense.io,", "M.Klimek@deepsense.io,", "H.Michalewski@mimuw.edu.pl"], "sections": [{"heading": null, "text": "Keywords: reinforcement learning, deep learning, Atari games, asynchronous computations"}, {"heading": "1 Introduction", "text": "In this work we approach the problem of learning strategies in Atari games from the hardware architecture perspective. We use a variation of the statistical model developed in [13,14]. Using the provided code1 our experiments are easy to re-create and we encourage the reader to draw his own conclusions about how CPUs perform in the context of Atari games. Following [7,13,14] we treat Atari games as a key benchmark problem for modern reinforcement learning.\nWe use a statistical model consisting of approximately one million floating point numbers which are iteratively updated using a gradient descent algorithm described in [12].\nAt first glance a training of such model appears as a relatively straightforward task: a screen from the simulator is fed into the statistical model which decides which button must be pressed; over an episode of a game we estimate how the agent performs and calculate the loss accordingly and update the model so that the loss is reduced.\nIn practice filling details of the above scenario is quite challenging. In this work we accept a number of technical solutions presented in [13]. Our work also\n1 https://github.com/deepsense-io/BA3C-CPU\nar X\niv :1\n70 5.\n06 93\n6v 1\n[ cs\n.D C\n] 1\n9 M\nay 2\n2 follows closely research done in [5], where a batch version of [13] is analyzed. We describe our algorithmic decisions in considerable detail in Section 2.2.\nWe obtained state-of-the-art results in all tested games (see Section 6) and in the process of obtaining them we detected certain interesting issues described in Sections 2.3, 6.2 related to batch sizes, learning rates and the asynchronous learning algorithm we use in this paper. The issues are illustrated by Figures 5 and 6. Apparently our algorithm relies on timely emptying of queues. If queues are growing, then updates are delayed and learning performance degenerates up to the point where the trained agent goes back to an essentially a random behavior. This in turn implies certain \u201cpreferred\u201d sizes of batches as illustrated by Figure 8. Those batch sizes in turn imply \u201cpreferred\u201d learning rates also visible in Figure 8.\nOur contribution can be considered as a snapshot of the CPU performance in the domain of reinforcement learning illustrating engineering opportunities and obstacles one can encounter relying solely on a CPU hardware. We also contributed an integration of Google\u2019s machine learning framework TensorFlow 0.11rc0 with Intel\u2019s Math Kernel Library (MKL). Details of the integration are described in Section 5 and benchmarks comparing behavior of the out-of-thebox TensorFlow 0.11rc0 with our modified version are included in Section 5.3. Section 3 contains a description of our hardware. Let us underline that we relied on a completely standard Intel servers. Section 4 contains a brief characteristic of the MKL library and its currently available deep learning functionalities.\nThe learning times on our hardware described in Section 3 are very competitive (see Figure 7) and in a future work we are planning to bring it down to minutes using sufficiently large CPU clusters."}, {"heading": "1.1 Related tools", "text": "This work would be impossible without a number of custom machine learning and reinforcement learning engineering tools. Our work is based on\n\u2013 OpenAI Gym [7], an open source machine learning platform allowing a very easy access to a rich library of games, including Atari games, \u2013 Google\u2019s TensorFlow 0.11rc0, an open source machine learning framework [4] allowing for streamlined integration of various neural networks primitives (layers) implemented elsewhere, \u2013 Tensorpack, an open source library [23] implementing a very efficient reinforcement learning algorithm, \u2013 Intel\u2019s Math Kernel Library 2017 (MKL) [19], a freely available library which implemented neural networks primitives (layers) and overall speeds up matrix and in particular deep learning computations on Intel\u2019s processors."}, {"heading": "1.2 Related work", "text": "Relation to [13]. Decisions in which we follow [13]. One of the key decisions is to run many independent agents in separate environments in an asynchronous\n3 way. In the training process in every environment we play an episode of 2000 steps (the number may be smaller if the agent dies). An input to the statistical model consists of 4 subsequent screens in the RGB format. An output of the statistical model is one of 18 possible moves of the controller. Over each episode the agent generates certain reward. The reward allows us to estimate how good were decisions made for every screen appearing during the episode. At every step an impact of the reward on decisions made earlier in the episode is discounted by a factor \u03b3 (0 < \u03b3 \u2264 1).\nHaving computed rewards for a given episode we can update weights of the model according to rewards \u2014 this is done through gradient updates which are applied directly to the statistical model weights. The updates are scaled by a learning rate \u03bb. Authors of [13] reported good CPU performance and this encouraged the experiment described in this paper. Decisions left to readers of [13]. The key missing detail are all technical decisions related to communication between processes.\nRelation to [5] and [18]. Since the publication of [14] a significant number of new results was obtained in the domain of Atari games, however to the best of our knowledge only the works [5] and [18] were focused on the hardware performance. In [5] authors modify the approach from [13] so it fits better into the GPU multicore infrastructure. In this work we show that a similar modification can be also quite helpful for the CPU performance. This work can be considered a CPU variant of [5]. In [18] a significant speedup of the A3C algorithm was obtained using large CPU clusters. However, it is unclear if the method scales beyond the game of Pong. Also the announcement [18] does not contain neither technical details or implementation.\nRelation to [22]. The fork of TensorFlow announced in [22] will offer a much deeper integration of TensorFlow and Intel\u2019s Math Kernel Library (MKL). In particular it should resolve the dimensionality issue mentioned in Section 5.4. However, at the moment of writing of this paper we had to do the integration of these tools on our own, because the fork mentioned in [22] was not ready for our experiments.\nOther references. The work [14] approaches the problem of learning a strategy in Atari games through approximation of the Q-function, that is implicitly it learns a synthesized values of every move of a player in a given situation on the screen. We did not consider this method, because of overall weaker results and much longer training times comparing to the asynchronous methods in [13].\nThe DeepBench [8], the FALCON Library [3] and the study [1] compare a performance of CPU and GPU on neural network primitives (single convolutional and dense layers) as well as on a supervised classification problem. Our article can be considered a reinforcement learning variant of these works.\nA recently published work [20] shows a very promising CPU-only results for agent training tasks. The learning algorithm proposed in [20] is a novel approach with yet untested stability properties. Our work focuses on a more established\n4 family of algorithms with better understood theoretical properties and applicability tested on a broader class of domains.\nFor a broad introduction to reinforcement learning we refer the reader to [21]. For a historical background on Atari games we refer to [14]."}, {"heading": "2 The Batch Asynchronous Advantage Actor Critic Algorithm (BA3C)", "text": "The Advantage Actor Critic algorithm (A2C) is a reinforcement learning algorithm combining positive aspects of both policy-based and value function based approaches to reinforcement learning. The results reported recently by Mnih et al. in [13] provide strong arguments for using its asynchronous version (A3C). After testing several implementations of this algorithm we found that a high quality open source implementation of this algorithm is provided in the TensorPack (TP) framework [23]. However, the differences between this variant, which resembles an algorithm introduced in [5], and the one described originally in [13] are significant enough to justify a new name. Therefore we will refer to this implementation as the Batch Asynchronous Advantage Actor Critic (BA3C) algorithm.2"}, {"heading": "2.1 Asynchronous reinforcement learning algorithms", "text": "Asynchronous reinforcement learning procedures are designed to use multiple concurrent environments to speed up the training process. This leaves an issue how the model or models are stored and synchronized between environments. We discuss some possible options in 2.2, including description of our own decisions.\nApart from obvious speedups resulting from utilizing concurrency, this approach has also some statistical consequences. Usually in one environment the subsequent states are highly correlated. This can have some adverse effects on the training process. However, when using multiple environments simultaneously, the states in each environment are likely to be significantly different, thus decorrelating the training points and enabling the algorithm to converge even faster."}, {"heading": "2.2 BA3C \u2013 details of the implementation", "text": "The batch variant of the A3C algorithm was designed to better utilize massively parallel hardware by batching data points. Multiple environments are still used, but there\u2019s only one instance of the model. This forces the extensive use of threading and message queues to decouple the part of the algorithm that generates the data from the one responsible for updates of the model. In a simple case of only one environment the BA3C algorithm consists of the steps described in algorithm 1. 2 In [5] is proposed a different name GA3C derived from \u201chybrid CPU/GPU implementation of the A3C algorithm\u201d. This seems a bit inconvenient, because it suggests a particular link between the batch algorithm and the GPU hardware; in this work we obtain good results for a similar algorithm running only on CPU.\n5 Algorithm 1 Basic synchronous Reinforcement Learning scheme 1: Randomly initialize the model. 2: Initialize the environment. 3: repeat 4: Play n episodes by using the current model to choose optimal actions. 5: Memorize obtained states and rewards. 6: Use the generated data points to train and update the model. 7: until results are satisfactory.\nWhen using multiple environments one can follow a similar approach - each environment could simply use the global model to predict the optimal action given its current state. Let us notice that the model always performs prediction on just a single data point from a single environment (i.e.: a single state vector of the environment). Obviously, this is far from optimal in terms of processing speed. Also accessing the shared model from different environments will quickly become a bottleneck. The two most popular approaches for solving this problem are:\n\u2013 Maintaining several local copies of the model (one for each environment) and synchronizing them with a global model. This approach is used and extensively described in [13,16,17] and we refer to it as A3C.\n\u2013 Using a single model and batching the predictions from multiple environments together (the \u201cbatch\u201d variant, BA3C). This is much more suitable for use on massively parallel hardware [5].\nThe batch variant requires using the following queues for storing data:\nTraining queue \u2013 stores the data points generated by the environments; the data points are used in training. See Figure 1.\n6 Prediction requests queue \u2013 stores the prediction requests made by the environments; the predictions are made according to the current weights stored in the model. See Figure 2.\nPrediction results queue \u2013 stores the results of the predictions made by the model; the predictions are later used by the environments for choosing actions. See Figure 3.\n7 Hyperparameters In Table 1 we list the most important hyperparameters of the algorithm is presented.\nConvNet architecture We made rather minor changes to the original TensorPack ConvNet. The main focus of the changes was to better utilize the MKL convolution primitives to enhance the performance. The architecture is presented in the diagram below.\n8"}, {"heading": "2.3 Effects of asynchronism on convergence", "text": "Training and prediction part of the above described algorithm work in separate threads and there\u2019s a possibility that one of those parts will work faster than the other (in terms of data points processed per unit time). This is rarely an issue when the training thread is faster \u2013 in this case it\u2019ll simply find out that the training queue is empty and wait for a batch of training data to be generated. This is inefficient since the hardware is not fully utilized when the train thread is waiting for data, but it should not impact the correctness of the algorithm.\nA much more interesting case arises when data points are generated faster than can be consumed by the training thread. If we\u2019re using default first-in-firstout training queue and this queue is not empty, then there\u2019s some delay between the batch of data being generated by the prediction thread and it being used for training. It turns out that if this delay is large enough it will have detrimental effect on the convergence of the algorithm.\nWhen there\u2019s a significant delay between the generation of a batch and training on it, the training will be performed using a data point generated by an older model. That is because when the batch of data was \u201cwaiting\u201d in the training queue, other batches were used for training and the model was updated. The number of such updates is equal to the size of the queue at the time when this batch was generated. Therefore the updates are performed using out-of-date training data which may have little to do with the current policy maintained by the current model.\nOf course when this delay is small and the learning rate is moderate the current policy is almost equal to the \u201cold\u201d one used for generating the training batch and the training process will converge. In other cases one should have means of constraining the delay to force correct behavior.\nThe solution is to restrict the size of the training queue. This way, when the training thread is generating too many training batches it will at some point reach the full capacity of the queue and will be forced to wait until some batch is popped. Usually the the size of the training queue is set to ensure that the training can take place smoothly. What we found out, however, is that setting the queue capacity to extremely small values (i.e., less than five), has little if any impact on the overall training speed.\nImpact of delay on convergence \u2013 experiments This section describes a series of experiments we\u2019ve carried out in order to establish how big a delay in the pipeline has to be to negatively impact the convergence. The setup involved inserting a fixed size first-in-first-out buffer between the prediction and training parts of the algorithm. This buffer\u2019s task was to ensure a predefined delay in the algorithm was present. With this modification we were able to conduct a series of experiments for different sizes of this buffer (delays). The results are shown below.\nBased on our results presented in the figures 5 and 6 we can conclude that even small delays have significant impact on the results and delays of more than\n10\n10 batches (1280 data points) effectively prevented the BA3C from converging. Therefore when designing an asynchronous RL algorithm it might be a good idea to try to streamline the pipeline as much as possible by making the queues as small as possible. This should not have significant effects on processing speed and can significantly improve obtained results."}, {"heading": "3 Specification of involved hardware", "text": ""}, {"heading": "3.1 Intel Xeon\u00ae (Broadwell)", "text": "We used Intel Xeon\u00ae E5 2600 v4 processors to perform benchmarks tests of convolutions. Xeon Broadwell is based on processor microarchitecture known as a \u201ctick\u201d [15] \u2013 a die shrink of an existing architecture, rather than a new architecture. In that sense, Broadwell is basically a Haswell made on Intel\u2019s 14nm second generation tri-gate transistor process with few improvements to the micro-architecture. Important changes are: up to 22 cores per CPU; support for DDR4 memory up to 2400 MHz; faster floating point instruction performance; improved performance on large data sets. Results reported here are obtained on a system with two Intel Xeon\u00ae Processor E5 2689 (3.10 GHz, 10 core) with 128 GB of DDR4 2400MHz RAM, Intel Compute Module S2600TP and Intel Server Chassis H2312XXLR2. The system was running Ubuntu 16.04 LTS operating system. The code was compiled with GCC 5.4.0 and linked against the Intel MKL 2017 library (build date 20160802)."}, {"heading": "3.2 Intel Xeon\u00ae (Haswell)", "text": "Intel Xeon\u00ae E5 2600 v3 Processor, was used as base for series of experiments to test hyperparameters of our algorithm. Haswell brings, along with new microarchitecture, important features like AVX2. We used the Prometheus cluster with a peak performance of 2.4 PFlops located at the Academic Computer Center Cyfronet AGH as our testbed platform. Prometheus consists of more than 2,200 servers, accompanied by 279 TB RAM in total, and by two storage file systems of 10 PB total capacity and 180 GB/s access speed. Experiments were performed in single-node mode, each node consisting of two Intel Xeon\u00ae E5-2680v3 processors with 24 cores at 2.5GHz with 128GB of RAM, with peak performance of 1.07 TFlops.\nXeon Haswell CPU allows effective computations of CNN algorithms, and convolutions in particular, by taking advantage of SIMD (single instruction, multiple data) instructions via vectorization and of multiple compute cores via threading. Vectorization is extremely important as these processors operate on vectors of data up to 256 bits long (8 single-precision numbers) and can perform up to two multiply and add (Fused Multiply Add, or FMA) operations per cycle. Processors support Intel Advanced Vector Extensions 2.0 (AVX2) vectorinstruction sets which provide: (1) 256-bit floating-point arithmetic primitives, (2) Enhancements for flexible SIMD data movements. These architecture-specific\n11\nadvantages have been implemented in the Math Kernel Library (MKL) and used in deep learning framework Caffe [9], [2] resulting in improved convolutions performance."}, {"heading": "4 The MKL library", "text": "The Intel Math Kernel Library (Intel MKL) 2017 introduces a set of Deep Neural Networks (DNN) [19] primitives for DNN applications optimized for the Intel architecture. The primitives implement forward and backward passes for the following operations: (1) Convolution: direct batched convolution, (2) Inner product, (3) Pooling: maximum, minimum, and average, (4) Normalization: local response normalization across channels and batch normalization, (5) Activation: rectified linear neuron activation (ReLU), (6) Data manipulation: multi-dimensional transposition (conversion), split, concatenation, sum, and scale. Intel MKL DNN primitives implement a plain C application programming interface (API) that can be used in the existing C/C++ DNN frameworks, as well as in custom DNN applications."}, {"heading": "5 Changes in TensorFlow 0.11rc0", "text": ""}, {"heading": "5.1 Motivation", "text": "Preliminary benchmarks showed that the vast majority of computation time during training is spent performing convolutions. On CPU the single most expensive operation was the backward pass with respect to the convolution\u2019s kernels, especially in the first layers working on the largest inputs. Therefore significant increases in performance had to be achieved by optimizing the convolution operation.\nWe considered the following approaches to this problem:\nTuning the current implementation of convolutions \u2013 TensorFlow (TF) uses the Eigen [10] library as a backend for performing matrix operations on CPU. Therefore this approach would require performing changes in the code of this library. The matrix multiplication procedures used inside Eigen have multiple hyperparameters that determine the way in which the work is divided between the threads. Also, some rather strong assumptions about the configuration of the machine (e.g., its cache size) are made. This certainly leaves space for improvements, especially when optimizing for a very specific use-case and hardware. Providing alternative implementation of convolutions \u2013 The MKL library provides deep neural network operations optimized for the Intel architectures. Some tests of convolutions on a comparable hardware had already been performed by Baidu [8] and showed promising results. This also had the added benefit of leaving the original implementation unchanged thus making it possible for the user to decide which implementation (the default or the optimized one) to use.\n12\nWe decided to employ the second approach that involved using the MKL convolution. A similar decision was taken also in the development of the Intelfocused fork of TensorFlow [22]."}, {"heading": "5.2 Implementation", "text": "TensorFlow provides a well documented mechanism for adding user-defined operations in C++, which makes it possible to load additional operations as shared objects. However, maintaining a build for a separate binary would make it harder to use some internal TF\u2019s utilities and sharing code with the original convolution operation. Therefore we decided to fork the entire framework and provide the additional operations.\nAnother TF\u2019s feature called \u2019labels\u2019 made it very simple to provide several different implementations of the same operation in C++ and choose between them from the python layer by specifying a \u2019label map\u2019. This proved especially helpful while testing and benchmarking our implementation since we could quickly compare it to the original implementation.\nThe implementation consisted of linking against the MKL library and providing the three additional operations: (1) MKL convolution forward pass, (2) MKL convolution backpropagation w.r.t. the input feature map, (3) MKL convolution backpropagation w.r.t. the kernels.\nThe code of these operations formed a glue layer between the TF\u2019s and MKL\u2019s programming interfaces. The computations were performed inside highly optimized MKL primitives."}, {"heading": "5.3 Benchmark results", "text": ""}, {"heading": "128,84,84,16 16,32,5,5 10.03 23.61 90.11 99.74", "text": ""}, {"heading": "128,40,40,32 32,32,5,5 4.58 8.76 43.83 33.61", "text": ""}, {"heading": "128,18,18,32 32,64,5,5 1.61 2.71 17.20 10.22", "text": "Multiple benchmarks were conducted in order to assess the performance of our implementation. They are focused on a specific 4-layer ConvNet architecture used for processing the Atari input images. The results are shown below.\n13\nTables 2, 3 and 4 show the benchmark results for the TensorFlow modified to use MKL and standard TensorFlow. Measurements consist of the times of performing convolutions with specific parameters (input and filter sizes) for Xeon\u00ae and Xeon Phi\u00ae CPUs. The same convolution parameters were used in the convolutional network used in the atari games experiments.\nThe results show that the MKL convolutions can be substantially faster than the ones implemented in TensorFlow. For some operations a speed-up of more than 10 times was achieved. The results agree with the ones reported in [8]. It is also worth noticing that most of the time is spent in the first layer which is responsible for processing the largest images."}, {"heading": "128,40,40,32 32,32,5,5 11.17 16.99 468.82 112.77", "text": ""}, {"heading": "128,84,84,16 16,32,5,5 8.97 29.63 1,236.98 368.18", "text": ""}, {"heading": "128,40,40,32 32,32,5,5 6.33 19.55 343.73 114.72", "text": ""}, {"heading": "5.4 Possible improvements", "text": "The data layout can have a tremendous impact on performance of low-level array operations. In turn, efficiency of these operations is critical for performance of higher-level machine learning algorithms.\n14\nTensorFlow and MKL have radically different philosophies of storing visual data. TensorFlow uses mostly its default \u201cNHWC\u201d format, in which pixels with the same spatial location but different channel indices are placed close to each other in memory. Some operations also provide the \u201cNCHW\u201d format widely used by other deep learning frameworks such as Caffe [11]. On the other hand MKL does not have a predefined default format, rather it is designed to easily connect MKL layers to one another. In particular, the same operation can require different data layouts depending on the sizes of its input (e.g. the number of input channels). This is supposed to ensure that the number of intermediate \u201cconversions\u201d or \u201ctranspositions\u201d in the pipeline is minimal, while at the same time letting each operation use its preferred data layout.\nIt is important to note that our implementation provided an alternative \u201cMKL\u201d implementation only for the convolution. We did not provide similar alternatives for max pooling, ReLU etc. This forced us to repeatedly convert the data between the TF\u2019s NHWC format and the formats required by the MKL convolution. Obviously this is not an optimal approach, however, implementing it optimally would most probably require significant changes in the very heart of the framework \u2013 its compiler. This task was beyond the scope of the project, but it\u2019s certainly feasible and with enough effort our implementation\u2019s performance could be even further improved. The times necessary to perform data conversions are provided in the Table 5."}, {"heading": "6 Results", "text": ""}, {"heading": "6.1 Game scores and overall training time", "text": "By using the custom convolution primitives from the MKL library it was possible to increase the training speed by a factor of 3.8 (from 151.04 examples/s to 517.12 examples/s). This made it possible to train well performing agents in under 24 hours. As a result, novel concepts and improvements to the algorithm can now be tested more quickly, possibly leading to further advances in the field of reinforcement learning. The increase in speed was achieved without hurting\n15\nthe results obtained by the agents trained. Example training curves for 3 different games are presented in the Figure 7."}, {"heading": "6.2 Batch size and learning rate tuning", "text": "Using the previously described pipeline optimized for better CPU performance we conducted a series of experiments designed to determine the optimal batch size and learning rate hyperparameters. The experiments were performed using the random search method [6]. For each hyperparameter its value was drawn from a loguniform distribution defined on a range [10\u22124, 10\u22122] for learning rate and [21, 210] for batch size. Overall, over 200 experiments were conducted in this manner for 5 different games. The results are presented in the figures 8,9 below. It appears that for the 5 games tested one could choose a combination of learning rate and batch size that would work reasonably well for all of them. However, the optimal settings for specific games seem to diverge.\nAs one could expect when using large batch sizes, better results were obtained with greater learning rate\u2019s. This is most probably caused by the stabilizing effects of bigger batch sizes on the mean gradient vector used for training. For smaller batch sizes using the same learning rate would cause instabilities, impeding the training process.\nOverall, batch size of around 32 a learning rate of the order of 10\u22124 seems to have been a general good choice for the games tested. The detailed listing of the best results obtained for each game is presented in the Table 6.\n16\nTable 6. Mean scores and hyperparameters obtained for the best models for each game\ngame learning rate batch size score mean score max\nBreakout-v0 0.00087 22 390.28 654\nPong-v0 0.00017 19 16.64 21\nRiverraid-v0 0.00024 87 10,018.40 11570\nSeaquest-v0 0.00160 162 1,823.41 1840\nSpaceInvaders-v0 0.00032 14 764.70 2000\n101 102 103\nbatch size\n10-3\nle a rn\nin g r\na te\nRiverraid Seaquest Pong Breakout SpaceInvaders\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFig. 8. Overall results of the random search for all the games tested. The brighter the color the better the result for a given game. Color value 1 means the best score for the game, color value 0 means the worst result for the given game.\n17"}, {"heading": "7 Conclusions and further work", "text": "Preliminary results contained in this work can be considered as a next step in reducing the gap between CPU and GPU performance in deep learning applications. As shown in this paper, in the area of reinforcement learning and in the context of asynchronous algorithms, CPU-only algorithms already achieve a very competitive performance.\nAs the most interesting future research direction we perceive extending results of [18] and tuning of performance of asynchronous reinforcement learning algorithms on large computer clusters with the idea of bringing the training time down from hours to minutes.\nConstructing a compelling experiment for the Xeon Phi\u00ae platform also seems to be an interesting challenge. Our current approach would require a significant modification because of much slower single core performance of Xeon Phi\u00ae. However, preliminary results on the Pong game are quite promising with a stateof-the-art results obtained in 12 hours on a single Xeon Phi\u00ae server.\n18"}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems (2015), http://tensorflow.org/, software available from tensorflow.org", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "GA3C: gpubased A3C for deep reinforcement learning", "author": ["M. Babaeizadeh", "I. Frosio", "S. Tyree", "J. Clemons", "J. Kautz"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "J. Mach. Learn. Res", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Myth busted: General purpose CPUs can\u2019t tackle deep neural network training (Jun 2016)", "author": ["P. Dubey"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Eigen v3", "author": ["G. Guennebaud", "B Jacob"], "venue": "http://eigen.tuxfamily.org", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R.B. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR abs/1412.6980", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M.A. Riedmiller", "A. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Intel xeon processor e5-2600 v4 product family technical overview (Jan 2017)", "author": ["D. Mulnix"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A.D. Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "HOGWILD!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": "ArXiv e-prints", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Learning Episode 4: Supercomputer vs Pong II (Oct 2016)", "author": ["Mark O\u2019Connor"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Introducing DNN primitives in Intel Math Kernel Library (Mar 2017)", "author": ["V. Pirogov"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Evolution strategies as a scalable alternative to reinforcement learning (Mar 2017)", "author": ["T. Salimans", "J. Ho", "X. Chen", "I. Sutskever"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Reinforcement learning - an introduction. Adaptive computation and machine learning, MIT", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Optimizing Tensorflow on Intel architecture for AI applications (Mar 2017)", "author": ["E. Ould-ahmed vall"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Tensorpack", "author": ["Y. Wu"], "venue": "https://github.com/ppwwyyxx/tensorpack", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 8, "context": "We use a variation of the statistical model developed in [13,14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 9, "context": "We use a variation of the statistical model developed in [13,14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 8, "context": "Following [7,13,14] we treat Atari games as a key benchmark problem for modern reinforcement learning.", "startOffset": 10, "endOffset": 19}, {"referenceID": 9, "context": "Following [7,13,14] we treat Atari games as a key benchmark problem for modern reinforcement learning.", "startOffset": 10, "endOffset": 19}, {"referenceID": 7, "context": "We use a statistical model consisting of approximately one million floating point numbers which are iteratively updated using a gradient descent algorithm described in [12].", "startOffset": 168, "endOffset": 172}, {"referenceID": 8, "context": "In this work we accept a number of technical solutions presented in [13].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "follows closely research done in [5], where a batch version of [13] is analyzed.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "follows closely research done in [5], where a batch version of [13] is analyzed.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "11rc0, an open source machine learning framework [4] allowing for streamlined integration of various neural networks primitives (layers) implemented elsewhere, \u2013 Tensorpack, an open source library [23] implementing a very efficient reinforcement learning algorithm, \u2013 Intel\u2019s Math Kernel Library 2017 (MKL) [19], a freely available library which implemented neural networks primitives (layers) and overall speeds up matrix and in particular deep learning computations on Intel\u2019s processors.", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "11rc0, an open source machine learning framework [4] allowing for streamlined integration of various neural networks primitives (layers) implemented elsewhere, \u2013 Tensorpack, an open source library [23] implementing a very efficient reinforcement learning algorithm, \u2013 Intel\u2019s Math Kernel Library 2017 (MKL) [19], a freely available library which implemented neural networks primitives (layers) and overall speeds up matrix and in particular deep learning computations on Intel\u2019s processors.", "startOffset": 197, "endOffset": 201}, {"referenceID": 14, "context": "11rc0, an open source machine learning framework [4] allowing for streamlined integration of various neural networks primitives (layers) implemented elsewhere, \u2013 Tensorpack, an open source library [23] implementing a very efficient reinforcement learning algorithm, \u2013 Intel\u2019s Math Kernel Library 2017 (MKL) [19], a freely available library which implemented neural networks primitives (layers) and overall speeds up matrix and in particular deep learning computations on Intel\u2019s processors.", "startOffset": 307, "endOffset": 311}, {"referenceID": 8, "context": "Relation to [13].", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "Decisions in which we follow [13].", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "Authors of [13] reported good CPU performance and this encouraged the experiment described in this paper.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "Decisions left to readers of [13].", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "Relation to [5] and [18].", "startOffset": 12, "endOffset": 15}, {"referenceID": 13, "context": "Relation to [5] and [18].", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "Since the publication of [14] a significant number of new results was obtained in the domain of Atari games, however to the best of our knowledge only the works [5] and [18] were focused on the hardware performance.", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "Since the publication of [14] a significant number of new results was obtained in the domain of Atari games, however to the best of our knowledge only the works [5] and [18] were focused on the hardware performance.", "startOffset": 161, "endOffset": 164}, {"referenceID": 13, "context": "Since the publication of [14] a significant number of new results was obtained in the domain of Atari games, however to the best of our knowledge only the works [5] and [18] were focused on the hardware performance.", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "In [5] authors modify the approach from [13] so it fits better into the GPU multicore infrastructure.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [5] authors modify the approach from [13] so it fits better into the GPU multicore infrastructure.", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "This work can be considered a CPU variant of [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 13, "context": "In [18] a significant speedup of the A3C algorithm was obtained using large CPU clusters.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Also the announcement [18] does not contain neither technical details or implementation.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "Relation to [22].", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "The fork of TensorFlow announced in [22] will offer a much deeper integration of TensorFlow and Intel\u2019s Math Kernel Library (MKL).", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "However, at the moment of writing of this paper we had to do the integration of these tools on our own, because the fork mentioned in [22] was not ready for our experiments.", "startOffset": 134, "endOffset": 138}, {"referenceID": 9, "context": "The work [14] approaches the problem of learning a strategy in Atari games through approximation of the Q-function, that is implicitly it learns a synthesized values of every move of a player in a given situation on the screen.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "We did not consider this method, because of overall weaker results and much longer training times comparing to the asynchronous methods in [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "The DeepBench [8], the FALCON Library [3] and the study [1] compare a performance of CPU and GPU on neural network primitives (single convolutional and dense layers) as well as on a supervised classification problem.", "startOffset": 14, "endOffset": 17}, {"referenceID": 15, "context": "A recently published work [20] shows a very promising CPU-only results for agent training tasks.", "startOffset": 26, "endOffset": 30}, {"referenceID": 15, "context": "The learning algorithm proposed in [20] is a novel approach with yet untested stability properties.", "startOffset": 35, "endOffset": 39}, {"referenceID": 16, "context": "For a broad introduction to reinforcement learning we refer the reader to [21].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "For a historical background on Atari games we refer to [14].", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "in [13] provide strong arguments for using its asynchronous version (A3C).", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "After testing several implementations of this algorithm we found that a high quality open source implementation of this algorithm is provided in the TensorPack (TP) framework [23].", "startOffset": 175, "endOffset": 179}, {"referenceID": 1, "context": "However, the differences between this variant, which resembles an algorithm introduced in [5], and the one described originally in [13] are significant enough to justify a new name.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "However, the differences between this variant, which resembles an algorithm introduced in [5], and the one described originally in [13] are significant enough to justify a new name.", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "2 In [5] is proposed a different name GA3C derived from \u201chybrid CPU/GPU implementation of the A3C algorithm\u201d.", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "This approach is used and extensively described in [13,16,17] and we refer to it as A3C.", "startOffset": 51, "endOffset": 61}, {"referenceID": 11, "context": "This approach is used and extensively described in [13,16,17] and we refer to it as A3C.", "startOffset": 51, "endOffset": 61}, {"referenceID": 12, "context": "This approach is used and extensively described in [13,16,17] and we refer to it as A3C.", "startOffset": 51, "endOffset": 61}, {"referenceID": 1, "context": "This is much more suitable for use on massively parallel hardware [5].", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "0 5 10 15 20 25 30 35 40 45 training step [10] 0 20 40 60 80 100 120 140", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "Xeon Broadwell is based on processor microarchitecture known as a \u201ctick\u201d [15] \u2013 a die shrink of an existing architecture, rather than a new architecture.", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "advantages have been implemented in the Math Kernel Library (MKL) and used in deep learning framework Caffe [9], [2] resulting in improved convolutions performance.", "startOffset": 108, "endOffset": 111}, {"referenceID": 14, "context": "The Intel Math Kernel Library (Intel MKL) 2017 introduces a set of Deep Neural Networks (DNN) [19] primitives for DNN applications optimized for the Intel architecture.", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "We considered the following approaches to this problem: Tuning the current implementation of convolutions \u2013 TensorFlow (TF) uses the Eigen [10] library as a backend for performing matrix operations on CPU.", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "Some tests of convolutions on a comparable hardware had already been performed by Baidu [8] and showed promising results.", "startOffset": 88, "endOffset": 91}, {"referenceID": 17, "context": "A similar decision was taken also in the development of the Intelfocused fork of TensorFlow [22].", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "The results agree with the ones reported in [8].", "startOffset": 44, "endOffset": 47}, {"referenceID": 6, "context": "Some operations also provide the \u201cNCHW\u201d format widely used by other deep learning frameworks such as Caffe [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "The experiments were performed using the random search method [6].", "startOffset": 62, "endOffset": 65}, {"referenceID": 13, "context": "As the most interesting future research direction we perceive extending results of [18] and tuning of performance of asynchronous reinforcement learning algorithms on large computer clusters with the idea of bringing the training time down from hours to minutes.", "startOffset": 83, "endOffset": 87}], "year": 2017, "abstractText": "The asynchronous nature of the state-of-the-art reinforcement learning algorithms such as the Asynchronous Advantage ActorCritic algorithm, makes them exceptionally suitable for CPU computations. However, given the fact that deep reinforcement learning often deals with interpreting visual information, a large part of the train and inference time is spent performing convolutions. In this work we present our results on learning strategies in Atari games using a Convolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0 machine learning framework. We also analyze effects of asynchronous computations on the convergence of reinforcement learning algorithms.", "creator": "LaTeX with hyperref package"}}}