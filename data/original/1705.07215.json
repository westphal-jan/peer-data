{"id": "1705.07215", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "How to Train Your DRAGAN", "abstract": "Generative Adversarial Networks have emerged as an effective technique for estimating data distributions. The basic setup consists of two deep networks playing against each other in a zero-sum game setting. However, it is not understood if the networks reach an equilibrium eventually and what dynamics makes this possible. The current GAN training procedure, which involves simultaneous gradient descent, lacks a clear game-theoretic justification in the literature. In this paper, we introduce regret minimization as a technique to reach equilibrium in games and use this to motivate the use of simultaneous GD in GANs. In addition, we present a hypothesis that mode collapse, which is a common occurrence in GAN training, happens due to the existence of spurious local equilibria in non-convex games. Motivated by these insights, we develop an algorithm called DRAGAN that is fast, simple to implement and achieves competitive performance in a stable fashion across different architectures, datasets (MNIST, CIFAR-10, and CelebA), and divergence measures with almost no hyperparameter tuning.", "histories": [["v1", "Fri, 19 May 2017 22:41:56 GMT  (2994kb,D)", "http://arxiv.org/abs/1705.07215v1", null], ["v2", "Wed, 24 May 2017 15:13:01 GMT  (2463kb,D)", "http://arxiv.org/abs/1705.07215v2", "Fixed multiple typos and added small clarifications. No new results"], ["v3", "Thu, 25 May 2017 00:51:40 GMT  (2463kb,D)", "http://arxiv.org/abs/1705.07215v3", "Updated Abstract. No new results"], ["v4", "Fri, 27 Oct 2017 21:47:51 GMT  (4869kb,D)", "http://arxiv.org/abs/1705.07215v4", "Some new results, fixes"]], "reviews": [], "SUBJECTS": "cs.AI cs.CV cs.GT cs.LG cs.NE", "authors": ["naveen kodali", "jacob abernethy", "james hays", "zsolt kira"], "accepted": false, "id": "1705.07215"}, "pdf": {"name": "1705.07215.pdf", "metadata": {"source": "CRF", "title": "How to Train Your DRAGAN", "authors": ["Naveen Kodali", "Jacob Abernethy"], "emails": ["nkodali3@gatech.edu", "jabernet@umich.edu", "hays@gatech.edu", "zkira@gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "Generative modeling involves using a training set to learn probability distribution Pmodel that closely resembles the data generating distribution Preal. Generative Adversarial Networks (GANs) [9] are a class of implicit generative models that have achieved significant success in generating realistic samples for high-dimensional data. GANs find applications in a variety of domains, including settings demanding multi-modal outputs, use within model-based reinforcement learning algorithms, and incorporation of unlabeled data in semi-supervised settings [13]. At its core, GAN training is framed as a zero-sum game between two players - a generator G and discriminator D. The goal is to reach equilibrium of this game where the Jenson-Shannon divergence between Pmodel and Preal is minimized. The common practice in the community is to use simultaneous gradient descent to try and converge to this point. However, this procedure lacks a clear game theoretical justification from the literature and in fact, the original paper argues that ideally players should be trained to optimality at each step.\nIn this paper, we introduce regret-minimization (RM) [7, 6] as a technique to reach Nash Equilibrium in games. Leveraging the analysis of no-regret algorithms (specifically, online gradient descent [4])\n\u2217Submitted to NIPS\u201917. Code - https://github.com/kodalinaveen3/DRAGAN\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 5.\n07 21\n5v 1\n[ cs\n.A I]\nin convex games, we motivate the use of simultaneous gradient descent in GAN training. Further, we hypothesize that the difficulty in training GANs, especially due to mode collapse, results from the existence of spurious local Nash Equilibria in non-convex games [8]. To address this problem, we propose a new algorithm (DRAGAN) that performs smoothing of the discriminator function by constraining its gradients around the real samples. Finally, we present a series of experiments to support our claims and demonstrate stable, competitive performance of DRAGAN in a wide variety of settings. We show results on the MNIST, CIFAR, and CelebA datasets."}, {"heading": "2 Related Work", "text": "There have been a lot of works aimed at finding a stable way to train GANs. Radford et al. [11] proposed a stable family of architectures called deep convolutional generative adversarial networks (DCGANs). We show that such constraints on architectures can be relaxed while still being able to achieve stability in the training process. In an alternate direction, a number of works have focused on developing specific objective functions that improve stability and performance of GANs. Salimans et al. [16] introduced a variety of techniques to improve the quality of samples. Che et.al [12] proposed a family of regularizers to address the missing modes problem in GANs. Zhao et.al [17] introduced energy based GAN framework which is more stable to train. Metz et al. [14] developed unrolled GANs taking inspiration from game theory literature. However, it suffers from slow performance due to the requirement of multiple unrolling steps in each iteration.\nRecently, we have seen a series of works based on imposing a Lipschitz constraint on the discriminator function. Guo-Jun Qi [20] introduced LS-GAN with the idea of maintaining a margin between losses assigned to real and fake samples. Specifically, they enforce the following condition (the discriminator is used as the loss function in this setting) -\nD\u03b8(G\u03c6(z))\u2212D\u03b8(x) \u2265 \u2206(x, G\u03c6(z)) where the \u2206(.) is some distance metric. Further, they impose a Lipschitz constraint on both G and D using weight decay. This leads to D having non-vanishing gradients everywhere between all real and fake sample pairs in the limit. Arjovsky et al. [18] proposed Wasserstein GAN which uses Earth-Mover distance as the objective to address problems with the vanilla objective. Their procedure requires the discriminator to be a Lipschitz function and they use weight clipping to achieve that. Gulrajani et al. [19] proposed an extension to address various shortcomings of the original WGAN and they impose the following condition on D -\n||\u2207x\u0302D\u03b8(x\u0302)||2 \u2248 1\nwhere x\u0302 = (\u01eb)x + (1 \u2212 \u01eb)G\u03c6(z) is some point between randomly chosen real and fake sample pairs. This leads to D having norm 1 gradients everywhere between all real and fake sample pairs in the limit. Notice that this behavior is very similar to that of LSGAN\u2019s discriminator function. We argue that such constraints are too restrictive and can encourage poor generator functions. This is demonstrated in section 4 where improved WGAN fails to accurately capture a simple data distribution. Additionally, the requirement of multiple updates to D at each step makes it slow. Our proposed method address a fundamental game theoretic issue in the GAN training process and hence it can be used on top of a variety of objective functions. We show that it is possible to get excellent results using just the vanilla objective function and a single update to D."}, {"heading": "3 Formulation and Proposed Algorithm", "text": ""}, {"heading": "3.1 Game Formulation", "text": "The GAN framework consists of two players - the generator whose aim is to create realistic samples and the discriminator whose aim is to classify an instance as having come from training data or the generator. The generator model G is parameterized by \u03c6, takes a latent variable z as input, and outputs sample G\u03c6(z). The discriminator model D is parameterized by \u03b8, takes a sample x as input and outputs D\u03b8(x) which represents the probability that it is real. The models G, D are usually represented by deep networks and their cost functions are defined as -\nJ (D)(\u03c6,\u03b8) = \u22121 2 Ex\u223cpreal logD\u03b8(x)\u2212 1 2 Ez log(1\u2212D\u03b8(G\u03c6(z)))\nJ (G)(\u03c6,\u03b8) = 1\n2 Ez log(1\u2212D\u03b8(G\u03c6(z)))\nThe complete game can be specified as -\nmin \u03c6 max \u03b8 {g(\u03c6,\u03b8) = Ex\u223cpreal logD\u03b8(x) + Ez log(1\u2212D\u03b8(G\u03c6(z)))}\nAt equilibrium, JS divergence between the data and model distributions is minimized meaning G has converged to Preal and D outputs 1/2 for all x."}, {"heading": "3.2 Proposed Algorithm", "text": "Given this background, we first describe our algorithm and then provide our theoretical motivation and justification for it in Section 4. Algorithm 1 shows pseudo-code for our proposed approach. We deviate from the vanilla algorithm in how we update the discriminator. Our proposed method involves constraining points in local regions around real examples to have norm 1 gradients. To achieve this, we generate three different mini-batches in each iteration: Real data examples from the training set xi, samples from the normal distribution zi, and real data examples perturbed with small noise xi + \u03b4i (the magnitude of noise decides the size of local regions). Ideally, we would like to be able to impose the constraint everywhere in these local regions but to keep it tractable, we only apply it at randomly chosen points (as shown in step 6). The objective function of D is appended with a regularization term that penalizes violations of the above constraint.\nAlgorithm 1 DRAGAN 1: Initial weights (\u03c60,\u03b80) (for the generator and descriminator, respectively) 2: for number of training iterations do 3: Get a mini-batch of real examples {x1, x2, ..., xm} from training data. 4: Get a mini-batch of samples {z1, z2, ..., zm} from N (0, 1) 5: Get {x1 + \u03b41, x2 + \u03b42, ..., xm + \u03b4m} where each \u03b4i is small noise vector. 6: Define x\u0302i = \u03b1 \u00b7 xi + (1\u2212 \u03b1) \u00b7 (xi + \u03b4i) for random \u03b1 \u2208 U [0, 1] 7: Update the discriminator by descending its gradient (using Adam):\n8: \u2207\u03b8 1m \u2211m i=1 [ \u2212 logD\u03b8(xi)\u2212 log(1\u2212D\u03b8(G\u03c6(zi))) + \u03bb \u00b7 (||\u2207x\u0302iD\u03b8(x\u0302i)||2 \u2212 1)2 ]\n9: Update the generator by descending its gradient (using Adam):\n10: \u2207\u03c6 1m \u2211m i=1 log [ 1\u2212D\u03b8(G\u03c6(zi)) ]\n11: end for\nWe now provide some implementation details for our algorithm:\nHyperparameters We use Adam [10] with default parameters everywhere. The hyperparameter \u03bb is set to 10 in all of our experiments.\nPerturbed Minibatch In step 6 of our algorithm, we get a perturbed minibatch of sizem by adding small noise vectors {\u03b41, ..., \u03b4m} to the real minibatch. As described earlier, we do this to obtain points in local regions of {x1, ..., xm} and constrain D to have norm 1 gradients there. Gulrajani et al. [19] use a similar penalty term albeit for different reasons. Moreover, we apply this constraint in local regions and show that this performs better experimentally in the next few sections.\nIn our code, we perform the operation shown below to perturb minibatches -\nperturbed minibatch = minibatch+ C \u00b7 minibatch.std() \u00b7 U [0, 1]\nWe set C = 0.5 in all of the experiments.\nBatch Normalization (BN) We did not use batch normalization in most of our experiments. This is because BN introduces correlations between samples in a minibatch and this affects our ability to impose local constraints. We replaced it with layer normalization in some experiments and show one experiment with BN."}, {"heading": "4 Theoretical Foundations", "text": "In the present section we describe how the training procedure for GANs in general can be viewed through the lens of equilibrium computation in zero-sum convex/concave games. The simultaneous gradient descent procedure corresponds to regret minimization techniques which are well-studied in the online learning and game theory communities. We observe, of course, that the lack of convexity leads to multiple saddle points (Nash equilibria), but we argue that the existence of such non-optimal solutions can be mitigated by the appropriate smoothing of the payoff function via regularization. This idea was used by Nash himself in his early work [2]. We argue in favor of this hypothesis empirically in Section 5."}, {"heading": "4.1 Equilibrium Computation and Generalization via Regret Minimization", "text": "Von Neumann established the minimax theorem for zero-sum games in [1] or equivalently the existence of mixed Nash equilibria. Nash and von Neumann originally envisioned two players selecting probabililty distributions over a finite strategy set, and receiving randomized rewards according to the game payoff function and these distributions. The minimax theorem has been significantly generalized since this original formulation, and the work of Sion [3] provides a broad class of scenarios in which inf sup{} = sup inf{}. For example, when we are given convex and compact subsets or \u0398 \u2282 Rn and \u03a6 \u2282 Rm and a function g : \u0398\u00d7 \u03a6\u2192 R that is convex in its first argument and concave in its second, then Sion\u2019s theorem implies that\nmin \u03b8\u2208\u0398 max \u03c6\u2208\u03a6 g(\u03b8, \u03c6) = max \u03c6\u2208\u03a6 min \u03b8\u2208\u0398 g(\u03b8, \u03c6).\nFor such min/max optimization scenarios, we will use the term Nash equilibrium pair to refer to any \u03b8\u2217, \u03c6\u2217 such that \u03b8\u2217 \u2208 argmin\u03b8\u2208\u0398max\u03c6\u2208\u03a6 g(\u03b8, \u03c6) and \u03c6\u2217 \u2208 argmax\u03c6\u2208\u03a6min\u03b8\u2208\u0398 g(\u03b8, \u03c6). Many problems, including the development of GANs, can be posed as a min/max formulation and hence a natural question is how we can find such equilibrium pairs. The most straightforward procedure by which players might search for an equilibrium is best-response dynamics (BRD). In each round, best-responding players play their optimal strategy given their opponent\u2019s current strategy. Despite its simplicity, BRD does not necessarily converge even in simple convex settings and can lead to oscillations/instability. At least in the setting where the payoff g(\u00b7, \u00b7) is convex/concave, we have a technique that is both efficient and provably works: no-regret learning algorithms (See, e.g., Chapter 4 of [7]). If both players treat the selection of their strategy parameters \u03b8t, \u03c6t as a sequential game (t = 1, 2, . . .), and they update these parameters via no-regret dynamics, then it is easy to show that the time-average parameter vectors will converge to a Nash equilibrium pair.\nLet us recall the definition of a no-regret algorithm. Given a sequence of convex loss functions L1, L2, . . . : \u0398\u2192 R, an algorithm that selects a sequence of \u03b8t\u2019s, each of which may only depend on previously observed L1, . . . , Lt\u22121, is said to have no regret if R(T ) T = o(1), where we define\nR(T ) := \u2211T t=1 Lt(\u03b8t)\u2212min\u03b8\u2208\u0398 \u2211T t=1 Lt(\u03b8)\nWe apply no-regret learning to the problem of equilibrium finding in a game g(\u00b7, \u00b7) as follows. The \u03b8 player imagines the function g(\u00b7, \u03c6t) as his loss function on round t, and similarly the \u03c6 player imagines \u2212g(\u03b8t, \u00b7) as her loss function at t. After T rounds of play, each player computes the average iterates \u03b8\u0304T := 1T \u2211T t=1 \u03b8t and \u03c6\u0304T := 1 T \u2211T t=1 \u03c6t. If V\n\u2217 is the equilibrium value of the game, and the \u03b8 and \u03c6 players suffer regret R1(T ) and R2(T ) respectively, then it is easy to show that\nV \u2217 \u2212 R2(T )T \u2264 max\u03c6\u2208\u03a6 g(\u03b8\u0304T , \u03c6)\u2212 R2(T ) T \u2264 min\u03b8\u2208\u0398 g(\u03b8, \u03c6\u0304T ) + R1(T ) T \u2264 V \u2217 + R1(T ) T .\nIn other words, \u03b8\u0304T and \u03c6\u0304T are \u201calmost optimal\u201d solutions to the game, where the \u201calmost\u201d approximation factor is given by the average regret terms R1(T )+R2(T )T . Under the no-regret condition, the former will vanish, and hence we can guarantee convergence in the limit.\nLet us consider a specific family of no-regret algorithms, and in particular we look at the well-studied familiy [6] known as Follow The Regularized Leader (FTRL). Specifically, FTRL selects \u03b8t on round t by solving for argmin\u03b8\u2208\u0398{ \u2211t\u22121 s=1 Ls(\u03b8) + 1 \u03b7\u2126(\u03b8)}, where \u2126(\u00b7) is some convex \u201cregularization function\u201d and \u03b7 is a learning rate. Roughly speaking, if you select the regularization as \u2126(\u00b7) = 12\u2016 \u00b7\u20162,\nthen FTRL becomes the well-known Online Gradient Descent (OGD) [4]. Ignoring the case of constraint violations, OGD can be written in a simple iterative form: \u03b8t = \u03b8t\u22121 \u2212 \u03b7\u2207Lt(\u03b8t\u22121). Of course the min/max objective function in GANs involves a stochastic payoff function, with two randomized inputs given on each round, x and z which are sampled from the data distribution and a standard multivariate normal, respectively. Let us write gx,z(\u03b8, \u03c6) := logD\u03b8(x) + log(1 \u2212 D\u03b8(G\u03c6(z))). Taking expectations with respect to x and z we define the full (non-stochastic) game as g(\u03b8, \u03c6) = EzEx\u223cpreal [gx,z(\u03b8, \u03c6)]. But the above online training procedure is still valid with stochastic inputs. That is, the equilibrium computation would proceed similarly, where on each round we sample xt and zt, and follow the updates\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b7\u2207\u03b8gxt,zt(\u03b8t, \u03c6t) and \u03c6t+1 \u2190 \u03c6t \u2212 \u03b7\u2207\u03c6gxt,zt(\u03b8t, \u03c6t).\nA huge benefit of this stochastic perspective is that we immediately get a generalization bound on the mean parameters \u03b8\u0304T after T rounds of optimization. The celebrated \u201conline-to-batch conversion\u201d [5], now a standard result in online learning theory, implies that Ex,z[gx,z(\u03b8\u0304T , \u03c6)], for any \u03c6, is no more than the optimal value Ex,z[gx,z(\u03b8\u2217, \u03c6)] plus an \u201cestimation error\u201d bounded by E [ R1(T )+R2(T )\nT\n] ,\nwhere the expectation is taken with respect to the sequence of samples observed along the way, and any randomness in the algorithm. (A limitation of this result, however, is that it requires a fresh sample xt to be used on every round.)"}, {"heading": "4.2 Local Nash equilibria and Smoothing in games", "text": "The discussion above, and the very general results one can prove using no-regret techniques, still relies on the crucial assumption that the payoff function g(\u00b7, \u00b7) is convex-concave. Indeed convexity is used in a number of places in the proofs: vanishing regret relies on a sequence of convex loss functions, and the online-to-batch conversion rests on Jensen\u2019s inequality to show 1 T \u2211T t=1 g(\u03b8t, \u03c6) \u2265 g ( 1 T \u2211T t=1 \u03b8t, \u03c6 ) . However, due to the non-convex nature of GAN settings, we are not guaranteed that a sole Nash equilibrium exists \u2013 there may be many non-optimal saddle points, which are essentially local minima of the game. On the other hand, if the no-regret dynamics exhibits convergence of \u03b8\u0304T and \u03c6\u0304T to some fixed points \u03b8\u0302 and \u03c6\u0302, then at the very least we know this pair will be a saddle point, albeit potentially non-optimal. In the GAN literature, it has been widely observed that mode collapse is quite commonly observed in practice and has been attributed to the non-convex nature of the model.\nOne of the striking features of the experimental results presented herein is that mode collapse is significantly rarer under the modified game we have considered, and this feature of the optimization may be a significant contributor to our robust performance gains. The novel aspect of our algorithm is the additional penalization of the gradients in feature space, which is (roughly) \u03bbExE(\u03b4\u223cN(0,\u01ebI)[\u2016\u2207xD\u03b8(x+ \u03b4)\u2016\u2212 1)2]. This is, in effect, biasing the discriminator function to have norm-(\u2248 1) gradients in a region around the real-data manifold. We recall an important fact, that any smooth function with norm-1 gradients on a compact set is necessarily a linear function, which suggests that our procedure encourages \u201cclose to linearity\u201d at least along the data manifold. The space of linear functions, of course, forms a convex set and would therefore involve only a single global optimum. We believe the following conjecture accounts for the dramatic reduction in mode collapse: the local norm-1 gradient regularization penalty provides sufficient smoothing of the game payoff to significantly reduce the space of non-optimal saddle points. We present some experimental validation on MNIST that the penalty increases as we enter mode collapse events in supplementary Sections 2.1 and 2.2."}, {"heading": "4.3 Regularization Scheme", "text": "Several recent works have proposed regularized models of GANs to improve stability . Our proposed approach roughly falls under this class of models and specifically, improved WGAN is the closest related approach to ours. Despite being developed from completely different motivations (Wasserstein distance and game theory), both impose a similar form of constraint on the gradients of D(x). However, there are differences with important implications for generative modeling performance.\nThe most important distinction is that we only impose these constraints in local regions around real samples, while improved WGAN imposes them everywhere. We claim that the resulting class\nof functions that improved WGAN can model is highly restricted compared to our method. This can be demonstrated with a simple experiment. In Figure 1, we track the performance of vanilla GAN, improved Wasserstein GAN and our algorithm on the swissroll dataset2 over the training period. Observe that the vanilla algorithm quickly captures the data density, our algorithm captures it approximately but improved WGAN fails to accurately capture this distribution (we show another experiment with 8-Gaussians dataset in the supplementary Section 2.3). The experimental results in section 5.1 further support our claim.\nThe proposed regularization, which restricts the discriminator function only along the real-data manifold, allows for broader flexibility of D\u03b8(\u00b7) in the vastly larger region outside of true density. Indeed the natural image space is mostly empty, i.e. there are low probability regions between any two modes. Modes that are far away from each other (for instance images of cars vs animals) can be equally realistic and modes close to each other (for instance images of cats with four legs vs cats with three legs) can have very different probabilities of being real. Such variations in data density cannot be modeled accurately when norm 1 gradients are imposed everywhere.\nFigure 1: Comparing the performance of different algorithms on swissroll dataset - Vanilla GAN (top), Improved WGAN (middle), and our algorithm (bottom). The contours represent regions where the discriminator D\u03b8(x) assigns low weight (purple) to high weight (yellow) of predicted likelihood of \u201creal\u201d. We also plot real samples in orange, and generated samples in green."}, {"heading": "5 Experimental Results", "text": "We present a series of experiments using the MNIST, CIFAR-10, and CelebA datasets demonstrating competitive inception score [16] results and sample quality compared to the baseline algorithms. Importantly, we also demonstrate significant improvement in stability for our algorithm across a large randomized set of network architectures and across a range of divergence measures. For the set of randomized architectures, we perform qualitative and quantitative assessment ranking of our algorithm compared to the vanilla GAN algorithm to assess improvements in stability, i.e. performance characteristics related to mode collapse or failure to learn."}, {"heading": "5.1 Inception Scores on CIFAR-10 using DCGAN architecture", "text": "The DCGAN is a family of architectures designed to perform well with the vanilla training procedure [11]. They are ubiquitous in the GAN literature owing to the instability of the vanilla algorithm in general settings. We use this architecture to model the CIFAR-10 dataset and compare to the vanilla GAN, WGAN, and improved WGAN. We did not add normalization layers in the discriminator for our algorithm and improved WGAN. As can be seen from Figure 2, our algorithm performs competitively with the vanilla procedure in terms of speed, stability and best inception score. Our final sample quality is superior in some cases. In contrast, improved WGAN achieves a significantly\n2We use experimental setups from https://github.com/igul222/improved_wgan_training\n(a) Inception Score Plot\n(c) Vanilla GAN\nInception Score (CIFAR-10)\nReal Images 11.24 WGAN 3.25 Improved WGAN 5.99 Vanilla GAN 6.99 Our algorithm 6.90\n(b) Inception scores (d) Our algorithm\nFigure 2: Performance using DCGAN on CIFAR-10 across several algorithms. Right: Samples from Vanilla GAN and our algorithm.\nlower score as shown in the table on Figure 2. Note that the vanilla procedure is hard to beat in this setting since DCGANs are built with architectural constraints that make them inherently stable. We present samples for all algorithms and some latent space walks, including for the CelebA dataset, in supplementary Section 1."}, {"heading": "5.2 BogoNet score to measure stability across architectures", "text": "One of the key advantages of our algorithm is that it can achieve better stability in a wider range of architectures (other than DCGANs). Similar to [18, 19] we have removed stabilizing components of DCGAN and demonstrated improved reliability compared to the vanilla GAN algorithm (see Section 2.4 in supplementary). However, to measure this to a larger extent, we introduce a metric termed the BogoNet score to compare stability of different training procedures in the GAN setting. The basic idea is to choose random architectures for players G and D independently and evaluate the performance of different algorithms in the resulting games. A good algorithm should give stable performance without failing to learn or more importantly mode collapse despite the imbalances.\nIn our experiment, each player is assigned a network from a diverse pool of architectures from three families (MLP, ResNet, DCGAN). See Section 3 in supplementary for details. To demonstrate that our algorithm is more stable compared to the vanilla procedure, we created 150 such instances of hard games. For each instance, we trained using both the Vanilla and our algorithm for the CIFAR-10 dataset and plotted the inception score over time. We performed both qualitative and quantitative analysis of the resulting graphs to measure stability in the performance of both algorithms.\nFor qualitative analysis, we used a bounty model to assign scores to each algorithm. We used 50 of the total 150 instances generated for this analysis. Each instance is worth 5 points and we split this bounty between the two algorithms depending on their performance. If both perform well or perform poorly, they get 2.5 points each. However, if one algorithm achieves stable performance compared to the other (in terms of lack of learning or mode collapses), we assign it higher portions of the bounty. Results were judged by two of the authors in a blind manner: The curves were shown side-by-side with the choice of algorithm for each side being randomized and unlabeled. The Vanilla\nGAN received an average score of 92.5 while our algorithm achieved an average score of 157.5, demonstrating much better stability properties across a range of architectures.\nFor quantitative analysis, we used the remaining 100 instances. For each algorithm we calculated statistics of final inception scores and area under the curve (AUC) over all 100 instances. However, these statistics do not capture the notion of stability well. To alleviate this, we perform linear regression on the series of inception scores observed in each instance to capture the overall trend. Ideally, we want consistent increase in the inception score over time and this would indicate stable performance. This means the resulting linear fit should have a positive slope with high magnitude. We calculate statistics of this slope over all 100 instances. The results are shown below in Table 1.\nNotice that while the mean and y-intercept in the linear fit of both the algorithms is almost the same, the mean slope of inception score linear fits is 2.5 times higher for our algorithm. Clearly, this is the result of mitigating mode collapse and other stability issues. In some architectures, the vanilla algorithm does well but it goes into mode collapse in a lot of the cases. The average slop of linear fit captures this behavior well. Another way to interpret this is that given a random architecture, we are likely to get 2.5 times higher inception score, which is a non-trivial result. To summarize, both the qualitative and the quantitative analysis demonstrate that we achieve more stable performance compared to the vanilla procedure and solve stability issues to some extent."}, {"heading": "5.3 Robustness across Divergence Measures", "text": "Nowozin et al. [15] show that any f -divergence can be used for training GANs with the help of an appropriate generatorfunction. We show experiments using Forward KL-Divergence, Reverse KL, Pearson \u03c72, Squared Hellinger, and Total Variation divergences. We use the case \"4-layer 512-dim ReLU MLP generator\" from previous subsection to demonstrate this in a challenging setting. Our algorithm is stable in all cases except for Total Variation while the vanilla algorithm failed in all cases (see Figure 3 for two examples and supplementary 2.5 for all five). Hence, practitioners can now use a larger set of objective functions that are appropriate for their application (unlike the LSGAN or WGAN family of algorithms).\n(a) Reverse KL (b) Pearson \u03c72\nFigure 3: Inception score curves for two divergence measures, demonstrating superior stability for our algorithm."}, {"heading": "6 Conclusions", "text": "In this paper, we draw upon the game theory literature to justify the current GAN training procedure and propose an improved algorithm. We connect the idea of regret minimization to GANs and provided a novel way to reason about dynamics in this game. Given this background, we analyze mode collapse from a game-theoretic perspective and hypothesize that spurious local equilibria are responsible for this issue. Based on this, we propose a novel regularization scheme as part of our algorithm DRAGAN. Unlike previous extensions, our algorithm is simple to implement, fast, and improves stability in a wide variety of settings."}, {"heading": "1 Samples and Latent Space Walks", "text": "In this section, we provide additional samples across algorithms and datasets. Further, Radford et al. [1] suggest that walking on manifold learned by the generator can expose signs of memorization. We use DCGAN to model MNIST and CelebA datasets using our algorithm and the results shown below demonstrate that our generator learns smooth transitions between different images.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA."}, {"heading": "2 Additional Experiments", "text": "The inception score was introduced as a rough guide to evaluate generative models. To compute this score, we need a classifier model and the original paper suggests use of inception model [2] for this purpose. However, this model is too powerful for smaller datasets like MNIST and this makes it hard to analyze differences in the modeling capabilities of various algorithms. Therefore, we use a custom classifier built for the MNIST dataset to calculate inception scores in the next two subsections (we use a dagger to make this distinction)."}, {"heading": "2.1 One layer networks with MNIST dataset", "text": "We design a simple experiment where G and D are both fully connected networks with just one hidden layer. Vanilla GAN performs poorly even in this simple setting and we observe severe mode collapse events. In contrast, our algorithm is stable throughout and obtains decent quality samples despite the constrained setup."}, {"heading": "2.2 Smoothing penalty tracking", "text": "The stability in our algorithm comes from regularizing D with the penalty (||\u2207x\u0302D\u03b8(x\u0302)||2 \u2212 1)2 as formulated in step 9 of Algorithm 1. We now take a closer look at mode collapse events that occur in the vanilla GAN and track the penalty in parallel (see Figure 7). Notice how the penalty increases as we enter mode collapse and drops as we recover. This indicates that the nature of the discriminator\u2019s gradients has a strong correlation with stability in GANs."}, {"heading": "2.3 8-Gaussians Experiment", "text": "We track the performance of improved WGAN and our algorithm on the 8-Gaussians dataset over time. As seen in Figure 8, both of them approximately converge to the real density but notice that in case of improved WGAN, the discriminator function is more constrained throughout the training period. ."}, {"heading": "2.4 Robustness across DCGAN Architecture Variations", "text": "DCGANs have to be designed following specific guidelines to make them stable [1]. We restate the suggested rules here.\n1. Use all convolutional networks which learn their own spatial downsampling (discriminator) or upsampling (generator)\n2. Remove fully connected hidden layers for deeper architectures\n3. Use batch normalization in both the generator and the discriminator\n4. Use ReLU activation in the generator for all layers except the output layer, which uses tanh\n5. Use LeakyReLU activation in the discriminator for all layers\nWe show that such constraints can relaxed for our algorithm and hence, practitioners are free to choose from a more diverse set of architectures. Below, we present a series of experiments in which we remove different stabilizing components from DCGAN architecture and analyze the performance of our algorithm. In each case, our algorithm is stable while the vanilla procedure fails. A similar approach is used to establish the robustness of training procedures in [3, 4]. Note that we add layer normalization to the discriminator (in our architecture experiments) in place of batch normalization. We chose the following four architectures which are difficult to train (in each case, we start with base DCGAN architecture and apply the changes) -\n\u2022 No BN and a constant number of filters in the generator\n\u2022 4-layer 512-dim ReLU MLP generator\n\u2022 tanh nonlinearities everywhere\n\u2022 tanh nonlinearity in the generator and 4-layer 512-dim LeakyReLU MLP discriminator"}, {"heading": "2.5 Robustness across Divergence Measures (Complete)", "text": "Due to space limitations, we only showed two examples for varying the divergence measures. Below we show results for all five measures."}, {"heading": "3 BogoNet Details", "text": "First, we choose from three families of architectures with probabilities - DCGAN (0.6), ResNet (0.2), MLP (0.2). Next, we further parameterize each family to create additional variation. For instance, the DCGAN family can result in networks with or without batch normalization, have LeakyReLU or Tanh non-linearities. The number and width of filters, latent space dimensionality are some other variations. Similarly, the number of layers and hidden units in each layer for MLP are chosen randomly. For\nResNets, we choose their depth randomly. This creates a set of hard games which test the stability of a given training algorithm."}], "references": [{"title": "Zur Theorie der Gesellschaftsspiele", "author": ["J. von Neumann"], "venue": "Mathematische Annalen", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1928}, {"title": "Two-person cooperative games", "author": ["John Nash"], "venue": "Journal of the Econometric Society", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1953}, {"title": "On general minimax theorems", "author": ["Maurice Sion"], "venue": "Pacific J. Math 8.1", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1958}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": "Cambridge university press,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Algorithmic game theory", "author": ["Noam Nisan"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Characterization and computation of local nash equilibria in continuous games", "author": ["Lillian J Ratliff", "Samuel A Burden", "S Shankar Sastry"], "venue": "Communication, Control, and Computing (Allerton),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Generative Adversarial Nets", "author": ["Ian Goodfellow"], "venue": "Advances in Neural Information Processing Systems 27. Ed. by Z. Ghahramani et al. Curran Associates, Inc.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "(Nov", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Mode Regularized Generative Adversarial Networks", "author": ["Tong Che"], "venue": "arXiv preprint arXiv:1612.02136", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "NIPS 2016 Tutorial: Generative Adversarial Networks", "author": ["Ian Goodfellow"], "venue": "arXiv preprint arXiv:1701.00160", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Unrolled Generative Adversarial Networks", "author": ["Luke Metz"], "venue": "CoRR abs/1611.02163", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "f-GAN: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Improved Techniques for Training GANs", "author": ["Tim Salimans"], "venue": "CoRR abs/1606.03498", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Energy-based generative adversarial network", "author": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "venue": "arXiv preprint arXiv:1609.03126", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Wasserstein gan", "author": ["Martin Arjovsky", "Soumith Chintala", "L\u00e9on Bottou"], "venue": "arXiv preprint arXiv:1701.07875", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Improved Training of Wasserstein GANs", "author": ["Ishaan Gulrajani"], "venue": "arXiv preprint arXiv:1704.00028", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}], "referenceMentions": [{"referenceID": 8, "context": "Generative Adversarial Networks (GANs) [9] are a class of implicit generative models that have achieved significant success in generating realistic samples for high-dimensional data.", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "GANs find applications in a variety of domains, including settings demanding multi-modal outputs, use within model-based reinforcement learning algorithms, and incorporation of unlabeled data in semi-supervised settings [13].", "startOffset": 220, "endOffset": 224}, {"referenceID": 6, "context": "In this paper, we introduce regret-minimization (RM) [7, 6] as a technique to reach Nash Equilibrium in games.", "startOffset": 53, "endOffset": 59}, {"referenceID": 5, "context": "In this paper, we introduce regret-minimization (RM) [7, 6] as a technique to reach Nash Equilibrium in games.", "startOffset": 53, "endOffset": 59}, {"referenceID": 3, "context": "Leveraging the analysis of no-regret algorithms (specifically, online gradient descent [4]) \u2217Submitted to NIPS\u201917.", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "Further, we hypothesize that the difficulty in training GANs, especially due to mode collapse, results from the existence of spurious local Nash Equilibria in non-convex games [8].", "startOffset": 176, "endOffset": 179}, {"referenceID": 10, "context": "[11] proposed a stable family of architectures called deep convolutional generative adversarial networks (DCGANs).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] introduced a variety of techniques to improve the quality of samples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "al [12] proposed a family of regularizers to address the missing modes problem in GANs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "al [17] introduced energy based GAN framework which is more stable to train.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "[14] developed unrolled GANs taking inspiration from game theory literature.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] proposed Wasserstein GAN which uses Earth-Mover distance as the objective to address problems with the vanilla objective.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] proposed an extension to address various shortcomings of the original WGAN and they impose the following condition on D ||\u2207x\u0302D\u03b8(x\u0302)||2 \u2248 1 where x\u0302 = (\u01eb)x + (1 \u2212 \u01eb)G\u03c6(z) is some point between randomly chosen real and fake sample pairs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "6: Define x\u0302 = \u03b1 \u00b7 x + (1\u2212 \u03b1) \u00b7 (x + \u03b4) for random \u03b1 \u2208 U [0, 1] 7: Update the discriminator by descending its gradient (using Adam):", "startOffset": 57, "endOffset": 63}, {"referenceID": 9, "context": "We now provide some implementation details for our algorithm: Hyperparameters We use Adam [10] with default parameters everywhere.", "startOffset": 90, "endOffset": 94}, {"referenceID": 18, "context": "[19] use a similar penalty term albeit for different reasons.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "std() \u00b7 U [0, 1]", "startOffset": 10, "endOffset": 16}, {"referenceID": 1, "context": "This idea was used by Nash himself in his early work [2].", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "Von Neumann established the minimax theorem for zero-sum games in [1] or equivalently the existence of mixed Nash equilibria.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "The minimax theorem has been significantly generalized since this original formulation, and the work of Sion [3] provides a broad class of scenarios in which inf sup{} = sup inf{}.", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": ", Chapter 4 of [7]).", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "Let us consider a specific family of no-regret algorithms, and in particular we look at the well-studied familiy [6] known as Follow The Regularized Leader (FTRL).", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "then FTRL becomes the well-known Online Gradient Descent (OGD) [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "The celebrated \u201conline-to-batch conversion\u201d [5], now a standard result in online learning theory, implies that Ex,z[gx,z(\u03b8\u0304T , \u03c6)], for any \u03c6, is no more than the optimal value Ex,z[gx,z(\u03b8\u2217, \u03c6)] plus an \u201cestimation error\u201d bounded by E [ R1(T )+R2(T ) T ] , where the expectation is taken with respect to the sequence of samples observed along the way, and any randomness in the algorithm.", "startOffset": 44, "endOffset": 47}, {"referenceID": 15, "context": "We present a series of experiments using the MNIST, CIFAR-10, and CelebA datasets demonstrating competitive inception score [16] results and sample quality compared to the baseline algorithms.", "startOffset": 124, "endOffset": 128}, {"referenceID": 10, "context": "The DCGAN is a family of architectures designed to perform well with the vanilla training procedure [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 17, "context": "Similar to [18, 19] we have removed stabilizing components of DCGAN and demonstrated improved reliability compared to the vanilla GAN algorithm (see Section 2.", "startOffset": 11, "endOffset": 19}, {"referenceID": 18, "context": "Similar to [18, 19] we have removed stabilizing components of DCGAN and demonstrated improved reliability compared to the vanilla GAN algorithm (see Section 2.", "startOffset": 11, "endOffset": 19}, {"referenceID": 14, "context": "[15] show that any f -divergence can be used for training GANs with the help of an appropriate generatorfunction.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Generative Adversarial Networks have emerged as an effective technique for estimating data distributions. The basic setup consists of two deep networks playing against each other in a zero-sum game setting. However, it is not understood if the networks reach an equilibrium eventually and what dynamics makes this possible. The current GAN training procedure, which involves simultaneous gradient descent, lacks a clear game-theoretic justification in the literature. In this paper, we introduce regret minimization as a technique to reach equilibrium in games and use this to motivate the use of simultaneous GD in GANs. In addition, we present a hypothesis that mode collapse, which is a common occurrence in GAN training, happens due to the existence of spurious local equilibria in non-convex games. Motivated by these insights, we develop an algorithm called DRAGAN that is fast, simple to implement and achieves competitive performance in a stable fashion across different architectures, datasets (MNIST, CIFAR-10, and CelebA), and divergence measures with almost no hyperparameter tuning.", "creator": "LaTeX with hyperref package"}}}