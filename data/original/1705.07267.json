{"id": "1705.07267", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Search Engine Guided Non-Parametric Neural Machine Translation", "abstract": "In this paper, we extend an attention-based neural machine translation (NMT) model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage--retrieval stage--, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage--translation stage--, a novel translation model, called translation memory enhanced NMT (TM-NMT), seamlessly uses both the source sentence and a set of retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs (En-Fr, En-De, and En-Es) shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved.", "histories": [["v1", "Sat, 20 May 2017 06:53:09 GMT  (2338kb,D)", "http://arxiv.org/abs/1705.07267v1", "8 pages, 4 figures, 2 tables"]], "COMMENTS": "8 pages, 4 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["jiatao gu", "yong wang", "kyunghyun cho", "victor o k li"], "accepted": false, "id": "1705.07267"}, "pdf": {"name": "1705.07267.pdf", "metadata": {"source": "CRF", "title": "Search Engine Guided Non-Parametric Neural Machine Translation", "authors": ["Jiatao Gu", "Yong Wang", "Kyunghyun Cho", "Victor O.K. Li"], "emails": ["vli}@eee.hku.hk", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013). The success of neural machine translation, which has already been adopted by major industry players in machine translation (Wu et al., 2016; Crego et al., 2016), is often attributed to the advances in building and training recurrent networks as well as the availability of large-scale parallel corpora for machine translation.\nNeural machine translation is most characteristically distinguished from the existing approaches to machine translation, such as phrase-based statistical machine translation (Koehn et al., 2003), in that it projects a sequence of discrete source symbols into a continuous space and decodes back the corresponding translation. This allows one to easily incorporate other auxiliary information into the neural machine translation system as long as such auxiliary information could be encoded into a continuous space using a neural network. This property has been recently noticed and used for building more advanced translation systems such as multilingual neural machine translation (Firat et al., 2016a; Luong et al., 2015; Dong et al., 2015), multi-source translation (Zoph and Knight, 2016; Firat et al., 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al., 2016) and linguistic knowledge guided translation (Nadejde et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017).\nIn this paper, we first notice that this ability in incorporating arbitrary meta-data by neural machine translation allows us to naturally extend it to a non-parametric model in which a neural machine translation system explicitly takes into account a full training set consisting of source-target sentence pairs (in this paper we refer them as a general translation memory). We can build a neural machine translation system that considers not only a given source sentence, which is to be translated but also a set of training sentence pairs in the process of translation. To do so, we propose a novel extension of\nar X\niv :1\n70 5.\n07 26\n7v 1\n[ cs\n.C L\n] 2\n0 M\nay 2\nattention-based neural machine translation that seamlessly fuses two information streams, each of which corresponds to the current source sentence and a set of training sentence pairs, respectively.\nA major technical challenge, other than designing such a neural machine translation system, is the scale of a training parallel corpus which often consists of hundreds of thousands to millions of sentence pairs. We address this issue by incorporating an off-the-shelf black-box search engine into the proposed neural machine translation system. The proposed approach first queries a search engine, which indexes a whole training set, with a given source sentence, and the proposed neural translation system translates the source sentence while incorporating all the retrieved training sentence pairs. In this way, the proposed translation system automatically adapts to the search engine and its ability to retrieve relevant sentence pairs from a training corpus.\nWe evaluate the proposed search engine guided non-parametric neural machine translation on three language pairs (En-Fr, En-De, and En-Es, in both directions) from JRC-Acquis Corpus (Steinberger et al., 2006) which consists of documents from a legal domain. This corpus was selected to demonstrate the efficacy of the proposed approach when a training corpus and a set of test sentences are both from a similar domain. Our experiments reveal that the proposed approach exploits the availability of the retrieved training sentence pairs very well, achieving significant improvement over the strong baseline of attention-based neural machine translation (Bahdanau et al., 2014)."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Neural Machine Translation", "text": "In this paper, we start from a recently proposed, and widely used, attention-based neural machine translation model (Bahdanau et al., 2014). The attention-based neural translation model is a conditional recurrent language model of a conditional distribution p(Y |X) over all possible translations Y = {y1, . . . , yT } given a source sentence X = {x1, . . . , xTx}. This conditional recurrent language model is an autoregressive model that estimates the conditional probability as p(Y |X) = \u220fT t=1 p(yt|y<t, X). Each term on the right hand side is approximated by a recurrent network by\np(yt|y<t, X) \u221d exp (g (yt, zt; \u03b8g)) , (1)\nwhere zt = f(zt\u22121, yt\u22121, ct(X, zt\u22121, yt\u22121); \u03b8f ). g and f correspond to a read-out function that maps the hidden state zt into a distribution over a target vocabulary, and a recurrent activation function that summarizes all the previously decoded target symbols y1, . . . , yt\u22121 with respect to the time-dependent context vector ct(X; \u03b8e), respectively. Both of these functions are parametrized, and their parameters are learned jointly to maximize the log-likelihood of a training parallel corpus.\nct(X, zt\u22121, yt\u22121) is composed of a bidirectional recurrent network encoder and an attention mechanism. The source sequence X is first encoded into a set of annotation vector {h1, . . . , hTx}, each of which is a concatenation of the hidden states of the forward and reverse recurrent networks. The attention mechanism, which is implemented as a feedforward network with a single hidden layer, then computes an attention score \u03b1t,\u03c4 for each hidden state h\u03c4 given the previously decoded target symbol yt\u22121 and the previous decoder hidden state zt\u22121:\n\u03b1t,\u03c4 = exp {\u03c6att(h\u03c4 , yt\u22121, zt\u22121)}\u2211Tx\n\u03c4 \u2032=1 exp {\u03c6att(h\u03c4 \u2032 , yt\u22121, zt\u22121)} .\nThese attention scores are used to compute the time-dependent context vector ct as\nct = Tx\u2211 \u03c4=1 \u03b1t,\u03c4h\u03c4 . (2)\nThe attention-based neural machine translation system is end-to-end trained to maximize the likelihood of a correct translation given a corresponding source sentence. During testing, a given source sentence is translated by searching for the most likely translation from a trained model. The entire process of training and testing can be considered as compressing the whole training corpus into a neural machine translation system, as the training corpus is discarded once training is over."}, {"heading": "2.2 Translation Memory", "text": "Translation memory is a computer-aided translation tool widely used by professional human translators. It is a database of pairs of source phrase and its translation. This database is constructed incrementally as a human translator translates sentences. When a new source sentence is present, a set of (overlapping) phrases from the original sentence are queried against the translation memory, and the corresponding entries are displayed to the human translator to speed up the process of translation. Due to the problem of sparsity (Sec. 5.2 of Cho, 2015), exact matches rarely occur, and approximate string matching is often used.\nIn this paper, we consider a more general notion of translation memory in which not only translation phrase pairs but any kind of translation pairs are stored. In this more general definition, a training parallel corpus is also considered a translation memory. This saves us from building a phrase table (Koehn et al., 2003), which is yet another active research topic, but requires us to be efficient and flexible in retrieving relevant translation pairs given a source sentence, as the issue of data sparsity amplifies. This motivates us to come up with an efficient query algorithm tied together with a downstream translation model that can overcome the problem of data sparsity."}, {"heading": "3 Search Engine Guided Non-Parametric Neural Machine Translation", "text": "We propose a non-parametric neural machine translation model guided by an off-the-shelf, efficient search engine. Unlike the conventional neural machine translation system, the proposed model does not discard a training corpus but maintain and actively exploit it in the test time. This effectively makes the proposed neural translation model a fully non-parametric model.\nThe proposed nonparametric neural translation model consists of two stages. The first stage is a retrieval stage, in which the proposed model queries a training corpus, or equivalently a translation memory, to retrieve a set of source-translation pairs given a current source sentence. To maximize the computational efficiency, first we utilize an off-the-shelf, highly-optimized search engine to quickly retrieve a large set of similar source sentences, and their translations, after which the top-K pairs are selected using approximate string matching based on edit distance.\nIn the second stage, a given source sentence is translated by an attention-based neural machine translation model, which we refer to as a translation memory enhanced neural machine translation (TM-NMT), and incorporates the retrieved translation pairs from the first stage. In order to maximize the use of the retrieved pairs, we build a novel extension of the attention-based model that performs attention not only over the source symbols but also over the retrieved symbols (and their respective translations). We further allow the model an option to copy over a target symbol directly from the retrieved translation pairs. The overall architecture with a simple translation example of the proposed TM-NMT is shown in Fig. 1 for reference."}, {"heading": "3.1 Retrieval Stage", "text": "We refer to the first stage as a retrieval stage. In this stage, we go over the entire training set M = { (X1, Y 1), . . . , (XN , Y N ) } to find pairs whose source side is similar to a current source X . That is, we define a similarity function s(X,X \u2032), and find (Xn, Y n) where s(X,Xn) is large.\nAlgorithm 1 Greedy selection procedure to maximize the coverage of the source symbols. Require: input X , translation memoryM\n1: Obtain the subset M\u0303 \u2286 M using an off-the-shelf search engine; 2: Re-rank retrieved pairs (X \u2032, Y \u2032) \u2208 M\u0303 using the similarity score function s in descending order; 3: Initialize the dictionary of selected pairs R = \u2205; 4: Initialize the coverage score c = 0; 5: for k = 1...|M\u0303 | do 6: ctmp = \u2211 x\u2208X \u03b4 [x \u2208 R.keys \u222a {X \u2032 k}] /|X| 7: if ctmp > c then 8: c = ctmp; R\u2190 {X \u2032k : Y \u2032k} 9: return R\nSimilarity score function s In this paper, we constrain ourselves to a setting in which only a neural translation model is trainable. That is, we do not assume the availability of other trainable sentence similarity functions. This allows us to focus entirely on the effectiveness of the proposed algorithm while being agnostic to the choice of similarity metric. Under this constraint, we follow an earlier work by Li et al. (2016) and use a fuzzy matching score which is defined as\nsfuzzy(X,X \u2032) = 1\u2212 Dedit(X,X \u2032) max (|X|, |X \u2032|) , (3)\nwhere Dedit is an edit distance.\nOff-the-shelf Search Engine The computational complexity of the similarity search grows linearly with the size of the translation memory which in our case contains all the pairs from a training corpus. Despite the simplicity and computational efficiency of the similarity score in Eq. (3), this is clearly not practical, as the size of the training corpus is often in the order of hundreds of thousands or even tens of millions. We overcome this issue of scalability by incorporating an off-the-shelf search engine, more specifically Apache Lucene.1 We then use Lucene to retrieve an initial set of translation pairs based on the source side, and use the similarity score above to re-rank them.\nFinal selection process Let M\u0303 \u2208 M be an initial set of translation pairs returned by Lucene. We rank the translation pairs within this set by s(X,X \u2032). We design and test two methods for selecting the final set from this initial set based on the similarity scores. The first method is a top-K retrieval, where we simply return the K most similar translation pairs from M\u0303. The second method returns an adaptive number of translation pairs based on the coverage of the symbols x in the current source sentence X within the retrieved translation pairs. We select greedily starting from the most similar translation pair, as described in Alg. 1."}, {"heading": "3.2 Translation Stage", "text": "In the second stage, we build a novel extension of the attention-based neural machine translation, TM-NMT, that seamlessly fuses both a current source sentence and a set M\u0302 of retrieved translation pairs. In a high level, the proposed TM-NMT first stores each target symbol of each retrieved translation pair into a key-value memory (Gulcehre et al., 2016b; Miller et al., 2016). At each time step of the decoder, TM-NMT first performs attention over the current source sentence to compute the time-dependent context vector based on which the key-value memory is queried. TM-NMT fuses information from both context vector of the current source sentence and the retrieved value from the key-value memory to generate a next target symbol.\nKey-Value Memory For each retrieved translation pair (X \u2032, Y \u2032) \u2208 M\u0302, we run a full attentionbased neural machine translation model,2 specified by a parameter set \u03b8, and obtain, for each target symbol y\u2032t \u2208 Y \u2032, a decoder\u2019s hidden state z\u2032t and an associated time-dependent context vector c\u2032t (see Eq. (2) which summarizes a subset of the source sentence X \u2032 that best describes y\u2032t). We consider c\u2032t as a key and (z \u2032 t, y \u2032 t) as a value, and store all of them from all the retrieved translation pairs in a key-value memory. Note that this approach is agnostic to how many translation pairs were retrieved during the first stage.\nMatching and Retrieval At each time step of the TM-NMT decoder, we first compute the context vector ct given the previous decoder hidden state zt, the previously decoded symbol yt\u22121 and all the annotation vector h\u03c4 \u2019s, as in Eq. (2). This context vector is used as a key for querying the key-value memory. Instead of hard matching, we propose soft matching based on a bilinear function, where we compute the matching score of each key-value slot by\nqt,\u03c4 = exp{E(ct, c\u2032\u03c4 )}\u2211 \u03c4 \u2032 exp{E(ct, c\u2032\u03c4 \u2032)} . (4)\n1 https://lucene.apache.org/core/ 2 We use a single copy of attention-based model for both key extraction and translation.\nwhere E(ct, c\u2032\u03c4 ) = c T t Mc \u2032 \u03c4 and M is a trainable parameter matrix.\nThese scores are used to retrieve a value from the key-value memory. In the case of the decoder\u2019s hidden states, we retrieve a weighted sum: z\u0303t = \u2211 \u03c4 qt,\u03c4z \u2032 \u03c4 ; In the case of target symbols, we consider each computed score as a probability of the corresponding target symbol. That is, pcopy(y\u2032\u03c4 ) = qt,\u03c4 , similarly to the pointer network (Vinyals et al., 2015).\nAlgorithm 2 Learning for TM-NMT Require: Search engine FSS , MT model \u03b8, TM model \u03b8\u2032,\nM,\u03bb, \u03b7, parallel training set D, translation memoryM. 1: Initialize \u03c6 = {\u03b8, \u03b8\u2032,M, \u03bb, \u03b7}; 2: Set the number of returned answers as K; 3: while stopping criterion is not met do 4: Draw a translation pair: (X,Y ) \u223c D; 5: Obtain memory pairs {X \u2032k, Y \u2032k}Kk=1 = FSS(X,M) 6: Reference Memory C = \u2205. 7: for k = 1...K do # generate dynamic keys 8: Let Y \u2032k = {y\u20321, ..., y\u2032T \u2032}, X \u2032k = {x\u20321, ..., x\u2032T \u2032s} 9: for \u03c4 = 1...T \u2032 do\n10: Generate key c\u2032\u03c4 = fatt(y\u2032<\u03c4 , X \u2032k) 11: Initialize coverage \u03b2\u03c4 = 0. 12: C \u2190 (c\u2032\u03c4 , y\u2032\u03c4 , \u03b2\u03c4 ) 13: Let Y = {y1, ..., yT }, X = {x1, ..., xTs} 14: for t = 1...T do # translate each word 15: Generate query ct = fatt(y<t, X) 16: for \u03c4 = 1...T \u2032 do Read c\u2032\u03c4 , y\u2032\u03c4 , \u03b2\u03c4 \u2208 C 17: Compute the score qt,\u03c4 using Eq. 7; 18: Gompute the gate \u03b6t with fgate; 19: Update \u03b2\u03c4 \u2190 \u03b2\u03c4 + qt,\u03c4 \u00b7 \u03b6t; 20: Compute the probability p(yt|\u00b7) 21: \u2013option1: shallow-fusion, Eq. 6 22: \u2013option2: deep-fusion, Eq. 5 23: Update \u03c6\u2190 \u03c6+ \u03b3 \u2202\n\u2202\u03c6 \u2211T t=1 log p(yt|\u00b7)\nIncorporation We consider two separate approaches to incorporating the retrieved values from the key-value memory, motivated by Gulcehre et al. (2015). The first approach, called deep fusion, weightedaverage the retrieved hidden state z\u0303t and the decoder\u2019s hidden state zt:\nzfusion =\u03b6t \u00b7 z\u0303t + (1\u2212 \u03b6t) \u00b7 zt (5)\nwhen computing the output distribution p(yt|y<t, X,M) (see Eq. (1)). The second approach is called shallow fusion and computes the output distribution as a mixture:\np(yt|y<t, X,M) = \u03b6tpcopy(yt) +(1\u2212 \u03b6t)p(yt|y<t, X). (6)\nThis is equivalent to copying over a retrieved target symbol y\u2032\u03c4 with the probability of \u03b6tpcopy(y \u2032 \u03c4 ) as the next target symbol (Gulcehre et al., 2016a; Gu et al., 2016).\nIn both of the approaches, there is a gating variable \u03b6t. As each target symbol may require a different source of information, we let this variable be determined automatically by the proposed TM-NMT. That is, we introduce another feedforward network that computes \u03b6t = fgate(ct, zt, z\u0303t). This gate closes when the retrieved pairs are not useful for predicting the next target symbol yt, and opens otherwise.\nCoverage In the preliminary experiments, we notice that the access pattern of the key-value memory was highly skewed toward only a small number of slots. Motivated by the coverage penalty from (Tu et al., 2016), we propose to augment the bilinear matching function (in Eq. (4)) with a coverage vector \u03b2t,\u03c4 such that\nE(ct, c \u2032 \u03c4 ) = c T t Mc \u2032 \u03c4 \u2212 \u03bb\u03b2t\u22121,\u03c4 , (7)\nwhere the coverage vector is defined as \u03b2t,\u03c4 = \u2211t t\u2032=1 qt\u2032,\u03c4 \u00b7 \u03b6t\u2032 . \u03bb is a trainable parameter."}, {"heading": "3.3 Learning and Inference", "text": "The proposed model, including both the first and second stages, can be trained end-to-end to maximize the log-likelihood given a parallel corpus. For practical training, we preprocess a training parallel corpus by augmenting each sentence pair with a set of translation pairs retrieved by a search engine, while ensuring that the exact copy is not included in the retrieved set. See Alg. 2 for a detailed description. During testing, we search through the whole training set to retrieve relevant translation pairs. Similarly to a standard neural translation model, we use beam search to decode the best translation given a source sentence."}, {"heading": "4 Related Work", "text": "The proposed TM-NMT has been largely motivated by recently proposed multilingual attention-based neural machine translation models (see, e.g., Firat et al., 2016a; Zoph and Knight, 2016). Similar to these multilingual models, our model takes into account more information than a current source\nsentence. This allows the model to better cope with any uncertainty or ambiguity arising from a single source sentence. More recently, this kind of larger context translation has been applied to cross-sentential modeling, where the translation of a current sentence is done with respect to previous sentences (Jean et al., 2017; Wang et al., 2017).\nDevlin et al. (2015) proposed an automatic image caption generation model based on nearest neighbours. In their approach, a given image is queried against a set of training pairs of images and their corresponding captions. They then proposed to use a median caption among those nearest neighboring captions, as a generated caption of the given image. This approach shares some similarity with the first stage of the proposed TM-NMT. However, unlike their approach, we learn to generate a sentence rather than simply choose one among retrieved ones.\nBordes et al. (2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al., 2008). The output module of the memory network used simple n-gram matching to create a small set of candidate facts from the Freebase. Each of these candidates was scored by the memory network to create a representation used by the response module. This is similar to our approach in that it exploits an intermediate black-box search module (n-gram matching) for generating a small candidate set.\nThe proposed method makes the attention-based neural machine translation model non-parametric by incorporating a key-value memory that stores a training set. A similar approach was very recently proposed for deep reinforcement learning by Pritzel et al. (2017), where they store pairs of observed state and the corresponding (estimated) value in a key-value memory to build a non-parametric deep Q network. We consider it as a confirmation of the general applicability of the proposed approach to a wider array of problems in machine learning. In the context of neural machine translation, Kaiser et al. (2017) also proposed to use an external key-value memory to remember training examples in the test time. Due to the lack of efficient search mechanism, they do not update the memory jointly with the translation model, unlike the proposed approach in this paper.\nOne important property of the proposed TM-NMT is that it relies on an external, black-box search engine to retrieve relevant translation pairs. Such a search engine is used both during training and testing, and an obvious next step is to allow the proposed TM-NMT to more intelligently query the search engine, for instance, by reformulating a given source sentence. Recently, Nogueira and Cho (2017) proposed task-oriented query reformulation in which a neural network is trained to use a black-box search engine to maximize the recall of relevant documents, which can be integrated into the proposed TM-NMT. We leave this as future work.\n5 Experimental Settings\nData We use the JRC-Acquis corpus (Steinberger et al., 2006) for evaluating the proposed TM-NMT model.3 The JRC-Acquis corpus consists of the total body of European Union (EU) law applicable to the member states. The text in this corpus is well structured, and most of the text in this corpus are related, making it an ideal test bed to evaluate the proposed TM-NMT which relies on the availability of appropriate translation pairs from a training set. This corpus\nwas recently used by Li et al. (2016) in investigating the combination of translation memory and phrase-based statistical machine translation.\nWe select three language pairs, namely, En-Fr, En-Es, and En-De, for evaluation. For each language pair, we uniformly select 3000 sentence pairs at random for both the development and test sets. The rest is used as a training set, after removing any sentence which contains special characters only. We use sentences of lengths up to 80 and 100 from the training and dev/test sets respectively. We do not lowercase the text, and use byte-pair encoding (BPE, Sennrich et al., 2015) to extract a vocabulary of 20,000 subword symbols. See Table 1 for detailed statistics. Retrieval Stage We use Apache Lucene to index a whole training set and retrieve 100 pairs per source sentence for the initial retrieval. These 100 pairs are scored against the current source sentence\n3 http://optima.jrc.it/Acquis/JRC-Acquis.3.0/corpus/\nusing the fuzzy matching score from Eq. (3) to select top-K relevant translation pairs. We vary K among 1 and 2 during training and among 1, 2, 4, 8, 16 during testing to investigate the trade-off between retrieval and translation quality. During testing, we also evaluate the effect of adaptively deciding the number of retrieved pairs using the proposed greedy selection algorithm (Alg. 1). Translation Stage We use a standard attention-based neural machine translation model (Bahdanau et al., 2014) with 1,024 gated recurrent units (GRU, Cho et al., 2014) on each of the encoder and decoder. We train both the vanilla model as well as the proposed TM-NMT based on this configuration from scratch using Adam optimizer (Kingma and Ba, 2014) with the initial learning rate set to 0.001. We use a minibatch of up to 32 sentence pairs. We early-stop based on the development set performance. For evaluation, we use beam search with width set to 5.\nIn the case of the proposed TM-NMT, we parametrize the metric matrix M in the similarity score function from Eq. (7) to be diagonal and initialized to an identity matrix. \u03bb in Eq. (7) is initialized to 0. The gating network fgate is a feedforward network with a single hidden layer, just like the attention mechanism fatt. We use either deep fusion or shallow fusion in our experiments.\n6 Result and Analysis\nIn Table 2, we present the BLEU scores obtained on all the three language pairs (both directions each) using three approaches; TM \u2013 a carbon copy of the target side of a retrieved translation pair with the highest matching score, NMT - a baseline translation model, and our proposed TM-NMT model. It is evident from the table that the proposed TMNMT significantly outperforms the\nbaseline model in all the cases, and that this improvement is not merely due to their copying over the most similar translation from a training set.\n-5\n0\n5\n10\n15\nIm pr ov\nem en t (B LE U)\nFuzzy Matching Score\nFigure 2: The improvement over the baseline by TM-NMT on Fr\u2192En w.r.t. the fuzzy matching scores of one retrieved translation pair.\nFuzzy matching score vs. Translation quality For Fr\u2192En, we broke down the development set into a set of bins according to the matching score of a retrieved translation pair, and computed the BLEU score for each bin. As shown in Fig. 2, we note that the improvement grows as the relevance of the retrieved translation pair increases. This verifies that TM-NMT effectively exploits retrieved translation pairs, but also suggests a future improvement for the case in which no\n64 64.5 65\n65.5 66\n1 2 4 Adaptive 8 16\nBL EU\n# of retrieved pairs\nachieved when the proposed greedy selection algorithm in Alg. 1 was used, in which case 4.814 translation pairs were retrieved on average.\nDeep vs. Shallow Fusion On both directions of En-Fr, we implemented and tested both deep and shallow fusion (Eqs. (5)\u2013(6)) for incorporating the information from the retrieved translation pairs.\nWith deep fusion only, the BLEU scores on the development set improved over the baseline by 1.30 and 1.20 respectively, while the improvements were 5.21 and 4.95, respectively. This suggests that the proposed model effectively exploits the availability of target symbols in the retrieved translation pairs. All other experiments were thus done using shallow fusion only.\nExamples We list two good examples and one in which the proposed method makes a mistake, in Fig. 4. From these examples, we see that the proposed TM-NMT selects a term or phrase used in a retrieved pair whenever there are ambiguities or multiple correct translations. For instance, in the first example, TM-NMT translated \u201cpr\u00e9cis\u201d into \u201cexact\u201d which was used in the retrieved pair, while the baseline model chose \u201cprecise\u201d. A similar behavior is found with \u201cexamen\u201d in the second example. This behavior helps the proposed TM-NMT generate a translation of which style and choice of vocabulary match better with translations from a training corpus, which improves the overall consistency of the translation."}, {"heading": "7 Conclusion", "text": "We proposed a practical, non-parametric extension of attention-based neural machine translation by utilizing an off-the-shelf, black-box search engine for quickly selecting a small subset of training translation pairs. The proposed model, called TM-NMT, then learns to incorporate both the sourceand target-side information from these retrieved pairs to improve the translation quality. We empirically showed the effectiveness of the proposed approach on the JRC-Acquis corpus using six language pair-directions.\nAlthough we presented the proposed approach in the context of machine translation, it is generally applicable to a wide array of problems. By embedding an input of any modality into a fixed vector space and using fast approximate search (Douze et al., 2017), this approach can, for instance, be used for multimedia description generation (Cho et al., 2015). Also, by replacing the decoder with an answer selection module, the proposed approach can be used for open-domain question answering, where the seamless fusion of multiple sources of information retrieved by a search engine is at the core. We leave these as future work."}, {"heading": "Acknowledgments", "text": "KC thanks support by Tencent, eBay, Facebook, Google and NVIDIA. This work was partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI)."}], "references": [{"title": "Towards string-to-tree neural machine translation", "author": ["Roee Aharoni", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1704.04743 .", "citeRegEx": "Aharoni and Goldberg.,? 2017", "shortCiteRegEx": "Aharoni and Goldberg.", "year": 2017}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data. AcM, pages 1247\u20131250.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1506.02075 .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Does multimodality help human and machine for translation and image captioning", "author": ["Ozan Caglayan", "Walid Aransa", "Yaxing Wang", "Marc Masana", "Mercedes Garc\u00eda-Mart\u00ednez", "Fethi Bougares", "Lo\u00efc Barrault", "Joost van de Weijer"], "venue": null, "citeRegEx": "Caglayan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caglayan et al\\.", "year": 2016}, {"title": "Natural language understanding with distributed representation", "author": ["Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1511.07916 .", "citeRegEx": "Cho.,? 2015", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Describing multimedia content using attention-based encoder-decoder networks", "author": ["Kyunghyun Cho", "Aaron Courville", "Yoshua Bengio."], "venue": "Multimedia, IEEE Transactions on 17(11):1875\u20131886.", "citeRegEx": "Cho et al\\.,? 2015", "shortCiteRegEx": "Cho et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078 .", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Systran\u2019s pure neural machine translation systems", "author": ["Josep Crego", "Jungi Kim", "Guillaume Klein", "Anabel Rebollo", "Kathy Yang", "Jean Senellart", "Egor Akhanov", "Patrice Brunelle", "Aurelien Coquard", "Yongchao Deng"], "venue": null, "citeRegEx": "Crego et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Crego et al\\.", "year": 2016}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["Jacob Devlin", "Saurabh Gupta", "Ross Girshick", "Margaret Mitchell", "C Lawrence Zitnick."], "venue": "arXiv preprint arXiv:1505.04467 .", "citeRegEx": "Devlin et al\\.,? 2015", "shortCiteRegEx": "Devlin et al\\.", "year": 2015}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "ACL.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "FAISS: A library for efficient similarity search", "author": ["Matthijs Douze", "Jeff Johnson", "Herv\u00e9 Jegou."], "venue": "https://code.facebook.com/posts/1373769912645926/ faiss-a-library-for-efficient-similarity-search/. Accessed: 2017-05-14.", "citeRegEx": "Douze et al\\.,? 2017", "shortCiteRegEx": "Douze et al\\.", "year": 2017}, {"title": "Learning to parse and translate improves neural machine translation", "author": ["Akiko Eriguchi", "Yoshimasa Tsuruoka", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1702.03525 .", "citeRegEx": "Eriguchi et al\\.,? 2017", "shortCiteRegEx": "Eriguchi et al\\.", "year": 2017}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "NAACL.", "citeRegEx": "Firat et al\\.,? 2016a", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Zero-resource translation with multi-lingual neural machine translation", "author": ["Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T Yarman Vural", "Kyunghyun Cho."], "venue": "EMNLP.", "citeRegEx": "Firat et al\\.,? 2016b", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Ensemble learning for multi-source neural machine translation", "author": ["Ekaterina Garmash", "Christof Monz."], "venue": "COLING.", "citeRegEx": "Garmash and Monz.,? 2016", "shortCiteRegEx": "Garmash and Monz.", "year": 2016}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li."], "venue": "arXiv preprint arXiv:1603.06393 .", "citeRegEx": "Gu et al\\.,? 2016", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Pointing the unknown words", "author": ["Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1603.08148 .", "citeRegEx": "Gulcehre et al\\.,? 2016a", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "Dynamic neural turing machine with continuous and discrete addressing schemes", "author": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1607.00036 .", "citeRegEx": "Gulcehre et al\\.,? 2016b", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["Caglar Gulcehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Loic Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1503.03535 .", "citeRegEx": "Gulcehre et al\\.,? 2015", "shortCiteRegEx": "Gulcehre et al\\.", "year": 2015}, {"title": "Does neural machine translation benefit from larger context", "author": ["Sebastien Jean", "Stanislas Lauly", "Orhan Firat", "Kyunghyun Cho"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2017}, {"title": "Learning to remember rare events", "author": ["\u0141ukasz Kaiser", "Ofir Nachum", "Aurko Roy", "Samy Bengio."], "venue": "arXiv preprint arXiv:1703.03129 .", "citeRegEx": "Kaiser et al\\.,? 2017", "shortCiteRegEx": "Kaiser et al\\.", "year": 2017}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP. pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1. Association for Computational Linguistics, pages 48\u201354.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Phrase-level combination of smt and tm using constrained word lattice", "author": ["Liangyou Li", "Andy Way", "Qun Liu."], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. page 275.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114 .", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander Miller", "Adam Fisch", "Jesse Dodge", "Amir-Hossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "arXiv preprint arXiv:1606.03126 .", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Syntax-aware neural machine translation using ccg", "author": ["Maria Nadejde", "Siva Reddy", "Rico Sennrich", "Tomasz Dwojak", "Marcin Junczys-Dowmunt", "Philipp Koehn", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1702.01147 .", "citeRegEx": "Nadejde et al\\.,? 2017", "shortCiteRegEx": "Nadejde et al\\.", "year": 2017}, {"title": "Task-oriented query reformulation with reinforcement learning", "author": ["Rodrigo Nogueira", "Kyunghyun Cho."], "venue": "arXiv preprint arXiv:1704.04572 .", "citeRegEx": "Nogueira and Cho.,? 2017", "shortCiteRegEx": "Nogueira and Cho.", "year": 2017}, {"title": "Neural episodic control", "author": ["Alexander Pritzel", "Benigno Uria", "Sriram Srinivasan", "Adri\u00e0 Puigdom\u00e8nech", "Oriol Vinyals", "Demis Hassabis", "Daan Wierstra", "Charles Blundell."], "venue": "arXiv preprint arXiv:1703.01988 .", "citeRegEx": "Pritzel et al\\.,? 2017", "shortCiteRegEx": "Pritzel et al\\.", "year": 2017}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "arXiv preprint arXiv:1508.07909 .", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages", "author": ["Ralf Steinberger", "Bruno Pouliquen", "Anna Widiger", "Camelia Ignat", "Tomaz Erjavec", "Dan Tufis", "D\u00e1niel Varga."], "venue": "arXiv preprint cs/0609058 .", "citeRegEx": "Steinberger et al\\.,? 2006", "shortCiteRegEx": "Steinberger et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "NIPS .", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li."], "venue": "arXiv preprint arXiv:1601.04811 .", "citeRegEx": "Tu et al\\.,? 2016", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Pointer networks", "author": ["Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly."], "venue": "Advances in Neural Information Processing Systems. pages 2692\u20132700.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Exploiting cross-sentence context for neural machine translation", "author": ["Longyue Wang", "Zhaopeng Tu", "Andy Way", "Qun Liu."], "venue": "arXiv preprint arXiv:1704.04347 .", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "arXiv preprint arXiv:1601.00710 .", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).", "startOffset": 259, "endOffset": 356}, {"referenceID": 7, "context": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).", "startOffset": 259, "endOffset": 356}, {"referenceID": 33, "context": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).", "startOffset": 259, "endOffset": 356}, {"referenceID": 22, "context": "Neural machine translation is a recently proposed paradigm in machine translation, where a single neural network, often consisting of encoder and decoder recurrent networks, is trained end-to-end to map from a source sentence to its corresponding translation (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013).", "startOffset": 259, "endOffset": 356}, {"referenceID": 37, "context": "The success of neural machine translation, which has already been adopted by major industry players in machine translation (Wu et al., 2016; Crego et al., 2016), is often attributed to the advances in building and training recurrent networks as well as the availability of large-scale parallel corpora for machine translation.", "startOffset": 123, "endOffset": 160}, {"referenceID": 8, "context": "The success of neural machine translation, which has already been adopted by major industry players in machine translation (Wu et al., 2016; Crego et al., 2016), is often attributed to the advances in building and training recurrent networks as well as the availability of large-scale parallel corpora for machine translation.", "startOffset": 123, "endOffset": 160}, {"referenceID": 24, "context": "Neural machine translation is most characteristically distinguished from the existing approaches to machine translation, such as phrase-based statistical machine translation (Koehn et al., 2003), in that it projects a sequence of discrete source symbols into a continuous space and decodes back the corresponding translation.", "startOffset": 174, "endOffset": 194}, {"referenceID": 13, "context": "This property has been recently noticed and used for building more advanced translation systems such as multilingual neural machine translation (Firat et al., 2016a; Luong et al., 2015; Dong et al., 2015), multi-source translation (Zoph and Knight, 2016; Firat et al.", "startOffset": 144, "endOffset": 204}, {"referenceID": 26, "context": "This property has been recently noticed and used for building more advanced translation systems such as multilingual neural machine translation (Firat et al., 2016a; Luong et al., 2015; Dong et al., 2015), multi-source translation (Zoph and Knight, 2016; Firat et al.", "startOffset": 144, "endOffset": 204}, {"referenceID": 10, "context": "This property has been recently noticed and used for building more advanced translation systems such as multilingual neural machine translation (Firat et al., 2016a; Luong et al., 2015; Dong et al., 2015), multi-source translation (Zoph and Knight, 2016; Firat et al.", "startOffset": 144, "endOffset": 204}, {"referenceID": 38, "context": ", 2015), multi-source translation (Zoph and Knight, 2016; Firat et al., 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al.", "startOffset": 34, "endOffset": 102}, {"referenceID": 14, "context": ", 2015), multi-source translation (Zoph and Knight, 2016; Firat et al., 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al.", "startOffset": 34, "endOffset": 102}, {"referenceID": 15, "context": ", 2015), multi-source translation (Zoph and Knight, 2016; Firat et al., 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al.", "startOffset": 34, "endOffset": 102}, {"referenceID": 4, "context": ", 2016b; Garmash and Monz, 2016), multimodal translation (Caglayan et al., 2016) and linguistic knowledge guided translation (Nadejde et al.", "startOffset": 57, "endOffset": 80}, {"referenceID": 28, "context": ", 2016) and linguistic knowledge guided translation (Nadejde et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017).", "startOffset": 52, "endOffset": 125}, {"referenceID": 0, "context": ", 2016) and linguistic knowledge guided translation (Nadejde et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017).", "startOffset": 52, "endOffset": 125}, {"referenceID": 12, "context": ", 2016) and linguistic knowledge guided translation (Nadejde et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017).", "startOffset": 52, "endOffset": 125}, {"referenceID": 32, "context": "We evaluate the proposed search engine guided non-parametric neural machine translation on three language pairs (En-Fr, En-De, and En-Es, in both directions) from JRC-Acquis Corpus (Steinberger et al., 2006) which consists of documents from a legal domain.", "startOffset": 181, "endOffset": 207}, {"referenceID": 1, "context": "Our experiments reveal that the proposed approach exploits the availability of the retrieved training sentence pairs very well, achieving significant improvement over the strong baseline of attention-based neural machine translation (Bahdanau et al., 2014).", "startOffset": 233, "endOffset": 256}, {"referenceID": 1, "context": "In this paper, we start from a recently proposed, and widely used, attention-based neural machine translation model (Bahdanau et al., 2014).", "startOffset": 116, "endOffset": 139}, {"referenceID": 24, "context": "This saves us from building a phrase table (Koehn et al., 2003), which is yet another active research topic, but requires us to be efficient and flexible in retrieving relevant translation pairs given a source sentence, as the issue of data sparsity amplifies.", "startOffset": 43, "endOffset": 63}, {"referenceID": 5, "context": "This allows us to focus entirely on the effectiveness of the proposed algorithm while being agnostic to the choice of similarity metric. Under this constraint, we follow an earlier work by Li et al. (2016) and use a fuzzy matching score which is defined as sfuzzy(X,X \u2032) = 1\u2212 Dedit(X,X \u2032) max (|X|, |X \u2032|) , (3) where Dedit is an edit distance.", "startOffset": 108, "endOffset": 206}, {"referenceID": 18, "context": "In a high level, the proposed TM-NMT first stores each target symbol of each retrieved translation pair into a key-value memory (Gulcehre et al., 2016b; Miller et al., 2016).", "startOffset": 128, "endOffset": 173}, {"referenceID": 27, "context": "In a high level, the proposed TM-NMT first stores each target symbol of each retrieved translation pair into a key-value memory (Gulcehre et al., 2016b; Miller et al., 2016).", "startOffset": 128, "endOffset": 173}, {"referenceID": 35, "context": "That is, pcopy(y\u2032 \u03c4 ) = qt,\u03c4 , similarly to the pointer network (Vinyals et al., 2015).", "startOffset": 64, "endOffset": 86}, {"referenceID": 17, "context": "5 23: Update \u03c6\u2190 \u03c6+ \u03b3 \u2202 \u2202\u03c6 \u2211T t=1 log p(yt|\u00b7) Incorporation We consider two separate approaches to incorporating the retrieved values from the key-value memory, motivated by Gulcehre et al. (2015). The first approach, called deep fusion, weightedaverage the retrieved hidden state z\u0303t and the decoder\u2019s hidden state zt: zfusion =\u03b6t \u00b7 z\u0303t + (1\u2212 \u03b6t) \u00b7 zt (5)", "startOffset": 173, "endOffset": 196}, {"referenceID": 17, "context": "This is equivalent to copying over a retrieved target symbol y\u2032 \u03c4 with the probability of \u03b6tpcopy(y \u2032 \u03c4 ) as the next target symbol (Gulcehre et al., 2016a; Gu et al., 2016).", "startOffset": 132, "endOffset": 173}, {"referenceID": 16, "context": "This is equivalent to copying over a retrieved target symbol y\u2032 \u03c4 with the probability of \u03b6tpcopy(y \u2032 \u03c4 ) as the next target symbol (Gulcehre et al., 2016a; Gu et al., 2016).", "startOffset": 132, "endOffset": 173}, {"referenceID": 34, "context": "Motivated by the coverage penalty from (Tu et al., 2016), we propose to augment the bilinear matching function (in Eq.", "startOffset": 39, "endOffset": 56}, {"referenceID": 38, "context": "The proposed TM-NMT has been largely motivated by recently proposed multilingual attention-based neural machine translation models (see, e.g., Firat et al., 2016a; Zoph and Knight, 2016).", "startOffset": 131, "endOffset": 186}, {"referenceID": 20, "context": "More recently, this kind of larger context translation has been applied to cross-sentential modeling, where the translation of a current sentence is done with respect to previous sentences (Jean et al., 2017; Wang et al., 2017).", "startOffset": 189, "endOffset": 227}, {"referenceID": 36, "context": "More recently, this kind of larger context translation has been applied to cross-sentential modeling, where the translation of a current sentence is done with respect to previous sentences (Jean et al., 2017; Wang et al., 2017).", "startOffset": 189, "endOffset": 227}, {"referenceID": 2, "context": "(2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al., 2008).", "startOffset": 100, "endOffset": 124}, {"referenceID": 6, "context": "Devlin et al. (2015) proposed an automatic image caption generation model based on nearest neighbours.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "Bordes et al. (2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 2, "context": "(2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al., 2008). The output module of the memory network used simple n-gram matching to create a small set of candidate facts from the Freebase. Each of these candidates was scored by the memory network to create a representation used by the response module. This is similar to our approach in that it exploits an intermediate black-box search module (n-gram matching) for generating a small candidate set. The proposed method makes the attention-based neural machine translation model non-parametric by incorporating a key-value memory that stores a training set. A similar approach was very recently proposed for deep reinforcement learning by Pritzel et al. (2017), where they store pairs of observed state and the corresponding (estimated) value in a key-value memory to build a non-parametric deep Q network.", "startOffset": 101, "endOffset": 777}, {"referenceID": 2, "context": "(2015) proposed a memory network for large-scale simple question-answering using an entire Freebase (Bollacker et al., 2008). The output module of the memory network used simple n-gram matching to create a small set of candidate facts from the Freebase. Each of these candidates was scored by the memory network to create a representation used by the response module. This is similar to our approach in that it exploits an intermediate black-box search module (n-gram matching) for generating a small candidate set. The proposed method makes the attention-based neural machine translation model non-parametric by incorporating a key-value memory that stores a training set. A similar approach was very recently proposed for deep reinforcement learning by Pritzel et al. (2017), where they store pairs of observed state and the corresponding (estimated) value in a key-value memory to build a non-parametric deep Q network. We consider it as a confirmation of the general applicability of the proposed approach to a wider array of problems in machine learning. In the context of neural machine translation, Kaiser et al. (2017) also proposed to use an external key-value memory to remember training examples in the test time.", "startOffset": 101, "endOffset": 1127}, {"referenceID": 5, "context": "Recently, Nogueira and Cho (2017) proposed task-oriented query reformulation in which a neural network is trained to use a black-box search engine to maximize the recall of relevant documents, which can be integrated into the proposed TM-NMT.", "startOffset": 23, "endOffset": 34}, {"referenceID": 32, "context": "Data We use the JRC-Acquis corpus (Steinberger et al., 2006) for evaluating the proposed TM-NMT model.", "startOffset": 34, "endOffset": 60}, {"referenceID": 25, "context": "This corpus was recently used by Li et al. (2016) in investigating the combination of translation memory and phrase-based statistical machine translation.", "startOffset": 33, "endOffset": 50}, {"referenceID": 1, "context": "Translation Stage We use a standard attention-based neural machine translation model (Bahdanau et al., 2014) with 1,024 gated recurrent units (GRU, Cho et al.", "startOffset": 85, "endOffset": 108}, {"referenceID": 23, "context": "We train both the vanilla model as well as the proposed TM-NMT based on this configuration from scratch using Adam optimizer (Kingma and Ba, 2014) with the initial learning rate set to 0.", "startOffset": 125, "endOffset": 146}, {"referenceID": 11, "context": "By embedding an input of any modality into a fixed vector space and using fast approximate search (Douze et al., 2017), this approach can, for instance, be used for multimedia description generation (Cho et al.", "startOffset": 98, "endOffset": 118}, {"referenceID": 6, "context": ", 2017), this approach can, for instance, be used for multimedia description generation (Cho et al., 2015).", "startOffset": 88, "endOffset": 106}], "year": 2017, "abstractText": "In this paper, we extend an attention-based neural machine translation (NMT) model by allowing it to access an entire training set of parallel sentence pairs even after training. The proposed approach consists of two stages. In the first stage\u2013retrieval stage\u2013, an off-the-shelf, black-box search engine is used to retrieve a small subset of sentence pairs from a training set given a source sentence. These pairs are further filtered based on a fuzzy matching score based on edit distance. In the second stage\u2013translation stage\u2013, a novel translation model, called translation memory enhanced NMT (TM-NMT), seamlessly uses both the source sentence and a set of retrieved sentence pairs to perform the translation. Empirical evaluation on three language pairs (En-Fr, En-De, and En-Es) shows that the proposed approach significantly outperforms the baseline approach and the improvement is more significant when more relevant sentence pairs were retrieved.", "creator": "LaTeX with hyperref package"}}}