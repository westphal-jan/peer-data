{"id": "1705.07445", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2017", "title": "Learning to Mix n-Step Returns: Generalizing lambda-Returns for Deep Reinforcement Learning", "abstract": "Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using next state's value function. $\\lambda$-returns generalize beyond 1-step returns and strike a balance between Monte Carlo and TD learning methods. While lambda-returns have been extensively studied in RL, they haven't been explored a lot in Deep RL. This paper's first contribution is an exhaustive benchmarking of lambda-returns. Although mathematically tractable, the use of exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our second major contribution is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. This allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. In contrast, lambda-returns restrict RL agents to use an exponentially decaying weighting scheme. Autodidactic returns can be used for improving any RL algorithm which uses TD learning. We empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.", "histories": [["v1", "Sun, 21 May 2017 12:47:37 GMT  (5800kb,D)", "http://arxiv.org/abs/1705.07445v1", "10 pages + 11 page appendix"]], "COMMENTS": "10 pages + 11 page appendix", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sahil sharma", "srivatsan ramesh", "girish raguvir j", "balaraman ravindran"], "accepted": false, "id": "1705.07445"}, "pdf": {"name": "1705.07445.pdf", "metadata": {"source": "CRF", "title": "Learning to Mix n-Step Returns: Generalizing \u03bb-Returns for Deep Reinforcement Learning", "authors": ["Sahil Sharma", "Srivatsan Ramesh", "Balaraman Ravindran"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Reinforcement Learning (RL) (Sutton & Barto, 1998) is often used to solve goal-directed sequential decision making tasks wherein conventional Machine Learning methods such as supervised learning are not suitable. An RL agent is not explicitly told the optimal actions but must instead discover them based on evaluative feedback given in terms of rewards sampled from the environment. Goal-directed sequential decision making tasks are modeled as Markov Decision Processs (MDP) (Puterman, 2014). Traditionally, tabular methods were extensively used for solving MDPs wherein value function or policy estimates were maintained for every state. Such methods become infeasible when the underlying state space of the problem is exponentially large or continuous. Traditional RL methods have also used linear function approximators in conjunction with hand-crafted state spaces for learning policies & value functions. This need for hand-crafted task-specific features has limited the applicability of RL, traditionally. It was necessary to learn hierarchical and abstract state representations which can capture the essential features required for optimal control in a generalizable fashion.\nar X\niv :1\n70 5.\n07 44\n5v 1\n[ cs\n.L G\n] 2\n1 M\nRecent advances in representation learning in the form of deep neural networks provide us with an effective way to achieve such generalization (Bengio et al., 2009; LeCun et al., 2015). Deep neural networks can learn hierarchically compositional representations that enable RL algorithms to generalize over large state spaces. The use of deep neural networks in conjunction with RL objectives has shown remarkable results such as learning to solve the Atari 2600 tasks from raw pixels (Bellemare et al., 2013; Mnih et al., 2015, 2016; Sharma et al., 2017; Jaderberg et al., 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al., 2016). Building accurate and powerful (in terms of generalization capabilities) state and action value function (Sutton & Barto, 1998) estimators is important for successful RL solutions. This is because many practical RL solutions (Q-Learning (Watkins & Dayan, 1992), SARSA (Rummery & Niranjan, 1994) and Actor-Critic Methods (Konda & Tsitsiklis, 2000)) use TD Learning (Sutton, 1988). The ability to build better estimates of the value functions directly results in better policy estimates as well as faster learning. \u03bb-returns (LR) (Sutton & Barto, 1998) are very effective in this regard. They are effective for faster propagation of delayed rewards and also result in more reliable learning. LR provide a trade-off between Monte Carlo and TD learning methods. They model the TD target using a mixture of n-step returns, wherein the weighs are exponentially decayed. With the advent of deep RL, the use of multi-step returns has gained a lot of popularity (Mnih et al., 2016). However, \u03bb-returns have not been explored extensively in the deep RL setting. Having said that, it is to be noted that the use of exponentially decaying weighting for various n-step returns seems to be an ad-hoc design choice made by LR. In this paper, we start off by extensively benchmarking \u03bb-returns (our experiments only use truncated \u03bb-returns due to the nature of the DRL algorithm (A3C) that we work with) and also propose a generalization of \u03bb-returns called the Confidence-based Autodidactic Returns (CAR), In CAR, the DRL agent learns in an end-to-end way, the weights to assign to the various n-step return based targets. Also note that in CAR, the weights assigned to various n-step returns change based on the current state that the DRL agent is in. In this sense, CAR weights are dynamic and using them represents a significant level of sophistication as compared to the usage of \u03bb-returns. In the Deep Reinforcement Learning (DRL) setting not much has been done in the context of using mixture of multi-step returns to build better value function estimators. An exception to that statement is the investigation done in Harb & Precup (2017). It shows promising results on two Atari 2600 tasks using the backward view of eligibility traces (\u03bb-returns constitute the forward view of eligibility traces) - Pong and Tennis. The contributions of this work are four-fold:\n1. We propose an extension to the classical \u03bb-returns algorithm in the DRL setting. 2. We propose a novel generalization of \u03bb-returns, called Confidence-based Autodidactic\nReturns (CAR). 3. We empirically demonstrate that using sophisticated mixtures of multi-step return methods\nlike \u03bb-returns and Confidence-based autodidactic returns leads to considerable improvement in the performance of a DRL agent. While both \u03bb-returns and CAR can be used with any algorithm which uses TD-learning, all our experiments have been with the A3C.\n4. We provide an analysis of how the weights learned by CAR for the n-step returns vary during training and also within an episode."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Preliminaries", "text": "An MDP (Puterman, 2014) is defined as the tuple \u3008S,A, r,P, \u03b3\u3009, where S is the set of states in the MDP, A is the set of actions, r : S \u00d7A 7\u2192 R is the reward function, P : S \u00d7A\u00d7 S 7\u2192 [0, 1] is the transition probability function such that \u2211 s\u2032 p(s, a, s\n\u2032) = 1, p(s, a, s\u2032) \u2265 0, and \u03b3 \u2208 [0, 1) is the discount factor. We consider a standard RL setting wherein the sequential decision-making task is modeled as an MDP and the agent interacts with an environment E over a number of discrete time steps. At a time step t, the agent receives a state st and selects an action at from the set of available actions A. Given a state, the agent could decide to pick its action stochastically. Its policy \u03c0 is in general a mapping defined by: \u03c0 : S \u00d7A 7\u2192 [0, 1] such that \u2211 a\u2208A \u03c0(s, a) = 1, \u03c0(s, a) \u2265 0 \u2200s \u2208 S, \u2200a \u2208 A. At any point in the MDP, the goal of the agent is to maximize the return, defined as: Gt = \u2211\u221e k=0 \u03b3 krt+k which is the cumulative discounted future reward. The state value function of a\npolicy \u03c0, V \u03c0(s) is defined as the expected return obtained by starting in state s and picking actions according to \u03c0."}, {"heading": "2.2 Actor Critic Algorithms", "text": "Actor Critic algorithms (Konda & Tsitsiklis, 2000) are a class of approaches that directly parameterize the policy (using an actor) \u03c0\u03b8a(a|s) and the value function (using a critic) V\u03b8c(s). They update the policy parameters using Policy Gradient Theorem (Sutton et al., 1999; Silver et al., 2014) based objective functions. The value function estimates are used to reduce the variance in policy gradient estimates."}, {"heading": "2.3 Asynchronous Advantage Actor Critic", "text": "Asynchronous Advantage Actor Critic(A3C) (Mnih et al. (2016)) introduced the first class of actor-critic algorithms which worked on high-dimensional complex visual input space. The key insight in this work is that by executing multiple actor learners on different threads in a CPU, the RL agent can explore different parts of the state space simultaneously. This ensures that the updates made to the parameters of the agent are uncorrelated.\nThe actor can improve its policy by following an unbiased low-variance sample estimate of the gradient of its objective function with respect to its parameters, given by:\n\u2207\u03b8a log \u03c0\u03b8a(at|st)(Gt \u2212 V (st))\nIn practice, Gt is often replaced with a biased lower variance estimate based on multi-step returns. In the A3C algorithm n-step returns are used as an estimate for the target Gt, where n \u2264 m and m is a hyper-parameter (which controls the level of rolling out of the policies). A3C estimates Gt as:\nGt \u2248 V\u0302 (st) = n\u2211 i=1 \u03b3i\u22121rt+i + \u03b3 nV (st+n)\nand hence the objective function for the actor becomes:\nL(\u03b8a) = log \u03c0\u03b8a(at|st)\u03b4t\nwhere \u03b4t = t+j\u22121\u2211 i=t \u03b3i\u2212tri + \u03b3 jV (st+1)\u2212 V (st) is the j-step returns based TD error.\nThe critic in A3C models the value function V (s) and improves its parameters based on sample estimates of the gradient of its loss function, given as: \u2207\u03b8c(V\u0302 (st)\u2212 V\u03b8c(st))2."}, {"heading": "2.4 Weighted Returns", "text": "Weighted average of n-step return estimates for different n\u2019s can be used for arriving at TD-targets as long as the sum of weights assigned to the various n-step returns is 1 (Sutton & Barto (1998)). In\nother words, given a weight vector w = ( w(1), w(2), \u00b7 \u00b7 \u00b7 , w(m) ) , such that i=m\u2211 i=1 w(i)=1, and n-step returns for n \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,m}: G(1)t , G (2) t , \u00b7 \u00b7 \u00b7 , G (m) t , we define a weighted return as\nGwt = m\u2211 n=1 w(n)G (n) t (1)\nNote that the n-step return G(n)t is defined as:\nG (n) t = n\u2211 i=1 \u03b3i\u22121rt+i + \u03b3 nV (st+n) (2)"}, {"heading": "2.5 \u03bb-Returns", "text": "A special case of Gwt is G \u03bb t (known as \u03bb-returns) which is defined as:\nG\u03bbt = (1\u2212 \u03bb) m\u22121\u2211 n=1 \u03bbn\u22121G (n) t + \u03bb m\u22121G (m) t (3)\nWhat we have defined here are a form of truncated \u03bb-returns for TD-learning. These are the only kind that we experiment with, in our paper. We use truncated \u03bb-returns because the A3C algorithm is designed in a way which makes it suitable for extension under truncated \u03bb-returns. We leave the problem of generalizing our work to the full \u03bb-returns as well as eligibility-traces (\u03bb-returns are the forward view of eligibility traces) to future work."}, {"heading": "3 \u03bb-Returns and Beyond: Autodidactic Returns", "text": ""}, {"heading": "3.1 Autodidactic Returns", "text": "Autodidactic returns are a weighted average of the n-step returns wherein the weight vector is also learned alongside the value functions which are being approximated. It is this generalization which makes the returns autodidactic. Since the autodidactic returns we propose are constructed using weight vectors that are dynamic (the weights change with the state the agent encounters in the MDP), we denote the weight vector as w(st). The autodidactic returns can be used for learning better approximations for the value functions using the TD(0) learning rule based update equation:\nVt(st)\u2190 Vt(st) + \u03b1 ( G w(st) t \u2212 Vt(st) ) (4)\nIn contrast with autodidactic returns, \u03bb-returns assign weights to the various n-steps returns which are constants given a particular \u03bb. We reiterate that the weights assigned by \u03bb-returns don\u2019t change during the learning process. Therefore, the autodidactic returns are a generalization and assign weights to returns which are dynamic by construction. The autodidactic weights are learned by the agent, using the reward signal it receives while interacting with the environment."}, {"heading": "3.2 Confidence-based Autodidactic Returns", "text": "All the n-step returns for state st are estimates for V (st) bootstrapped using the value function of corresponding nth future state (V (st+n)). All the value functions are estimates themselves. Hence, a natural way for the RL agent to weigh an n-step return ( G\n(n) t\n) would be to compute this weight\nusing the confidence, c(st+n), that the agent has in the value function estimate, V (st+n), using which the n-step return was estimated. The agent weighs the n-returns based on how confident it is about that n-step return being a good estimate for V (st). We denote this confidence as c(st+n). Since the n-step return in turn depends on V (st+n), c(st+n) also reflects the confidence that the agent has about the value function estimate V (st+n). Next, the weight vector w(st) is computed as:\nw(st) = ( w(st) (1), w(st) (2), \u00b7 \u00b7 \u00b7 , w(st)(m) ) where w(st)(i) is given by:\nw(st) (i) = ec(st+i)\u2211j=m j=1 e c(st+j) (5)"}, {"heading": "3.3 Using \u03bb-returns in A3C", "text": "While \u03bb-returns have been well studied in literature (Peng & Williams, 1996; Sutton & Barto, 1998; Seijen & Sutton, 2014), their usage in DRL has been limited. We propose a straightforward way to incorporate (truncated) \u03bb-returns into the A3C framework. We call this combination as LRA3C.\nThe critic in A3C using TD(0) algorithm for arriving at good estimates for the value function. However, note that the TD-target can in general be based any n-step returns (or a mixture\nthereof). The A3C algorithm in specific is well suited for using weighted returns such as \u03bb-returns since the algorithm already uses n-step returns for bootstrapping. Using eqs. (1) to (3) makes it very easy to incorporate weighted returns into the A3C framework. The respective sample estimates for the gradients of the actor and the critic become:\n\u2207\u03b8a log \u03c0\u03b8a(at|st)(G\u03bbt \u2212 V (st))\n\u2207\u03b8c(G\u03bbt \u2212 V\u03b8c(st))2"}, {"heading": "3.4 Using Autodidactic Returns in A3C", "text": "We propose to use autodidactic returns in place of normal n-step returns in the A3C framework. We call this combination as CARA3C. In a generic DRL setup, a forward pass is done through the network to obtain the value function of the current state. The parameters of the network are progressively updated based on the gradient of the loss function and the value function estimation (in general) becomes better. For predicting the confidence values, a distinct neural network is created which shares all but the last layer with the value function estimation network. So, every forward pass of the network on state st now outputs the value function V (st) and the confidence the network has in its value function prediction, c(st). Figure 1 visually demonstrates how the confidence values are calculated using an A3C network. Next, using eqs. (1) to (5) the weighted average of n-step returns is calculated and used as a target for improving V (st). Algorithm 1 contains detailed pseudo-code for training CARA3C agents is in Appendix F. The policy improvement is carried out by following sample estimates of the loss function\u2019s gradient, given by: \u2207\u03b8a log \u03c0\u03b8a(at|st)\u03b4t, where \u03b4t is now defined in terms of the TD error term obtained by using autodidactic returns as the TD-target. Overall, the sample estimates for the gradient of the actor and the critic loss functions are:\n\u2207\u03b8a log \u03c0\u03b8a(at|st)(G w(st) t \u2212 V (st))\n\u2207\u03b8c(G w(st) t \u2212 V\u03b8c(st))2"}, {"heading": "3.5 Avoiding pitfalls in TD learning of Critic", "text": "The LSTM-A3C neural networks for representing the policy and the value function share all but the last output layer. In specific, the LSTM Hochreiter & Schmidhuber (1997) controller which aggregates the observations temporally is shared by the policy and the value networks. As stated in the previous sub-section, we extend the A3C network to predict the confidence values by creating a new output layer which takes as input the LSTM output vector (LSTM outputs are the pre-final layer). Figure 1 contains a demonstration of how w(s1) is computed. Since all the three outputs (policy, value function, confidence on value function) share all but the last layer, Gw(st)t depends on the parameters of the network which are used for value function prediction. Hence, the autodidactic returns also influence the gradients of the LSTM controller parameters. It has observed that when the TD target, Gwt is allowed to move towards the value function prediction V (st, it can make the learning unstable. This happens because the L2 loss between the TD-target and the value function prediction can now be minimized by moving the TD-target towards the erogenous value function predictions V (st) instead of the other way round. To avoid this instability we ensure that gradients do not flow back from the confidence values computation\u2019s last layer to the LSTM layer\u2019s outputs. In effect, the gradient of the critic loss with respect to the parameters decided to the autodidactic return computation can no longer influence the gradients of the LSTM parameters (or any of the previous convolutional layers) at all. To summarize, during back-propagation of gradients in the A3C network, the parameters specific to the autodidactic return computation do not contribute at all to the gradient which flows back into the LSTM layer. This ensures that the parameters of the confidence network are learned while treating the LSTM outputs as fixed feature vectors. This entire scheme of not allowing gradients to flow back from the confidence value computation to the LSTM outputs has been demonstrated in Figure 1. The forward arrows depict the parts of the network which are involved in forward propagation whereas the backward arrows depict the path taken by the back-propagation of gradients."}, {"heading": "4 Experimental Setup and Results", "text": "We performed general game-play experiments with CARA3C and LRA3C on 22 tasks in the Atari domain. All the networks were trained for 100 million time steps. The hyper-parameters for each of the methods were tuned on a subset of four tasks: Seaquest, Space Invaders, Gopher and Breakout. The same hyper-parameters were used for the rest of the tasks. The baseline scores were taken from\nSharma et al. (2017). All our experiments were repeated thrice with different random seeds to ensure that our results were robust to random initialization. All results reported are the average of results obtained by using these three random seeds. Since the A3C scores were taken from Sharma et al. (2017), we followed their training and testing regime as well. Appendix A contains experimental details about the training and testing regimes. Appendix G documents the procedure we used for picking important hyper-parameters for our methods."}, {"heading": "4.1 General gameplay performance", "text": "Figure 2 shows the percentage improvement of our sophisticated mixture of n-step return methods (CARA3C and LRA3C) over A3C. If the scores obtained by one of our methods and A3C in a task\nare p and q respectively, then the percentage improvement is calculated as:\n(\np\u2212q q \u00d7 100\n)\n. As we can see, CARA3C achieves a staggering 67\u00d7 performance in the task Kangaroo. Table 1 (in Appendix B) contains a comparison of the raw scores obtained by our methods to A3C baseline scores. Evolution of the average performance of our methods with training progress has been shown in Figure 3. An expanded version of the graph for all the tasks can be found in Appendix C."}, {"heading": "4.2 Analysis of the Evolution of Weights During Training", "text": "Figure 4 plots the weights assigned to various n-step (for n \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 20} returns for states s1, s21, \u00b7 \u00b7 \u00b7 in a trajectory. The weights given to each of the n-step returns are learned and hence change to minimize the critic loss function. Figure 4 demonstrates that the weights assigned to each of the n-step returns start off with being random (all equal to with 0.05) and slowly evolve to different values depending on the corresponding confidence value predicted by the network. An expanded version of the graphs for all the tasks can be found in Appendix D. We observe that the 1-step returns and the 20-step returns are assigned most confidence. This seems to indicate a bias-variance trade-off between the various n-step returns with both extremes being favorable, perhaps in different parts of the state space. In contrast, for \u03bb-returns, since \u03bb < 1, 1-step returns always get the highest weight. We observe a similar trend only in a few tasks, for Confidence-based Autodidactic Returns.However, in some of the tasks, such as Gopher one can observe that the network is much more confident about the 20-step returns (on an average) than the 1-step returns."}, {"heading": "4.3 Analysis of the Evolution of Weights During an Episode", "text": "Figure 5 demonstrates the evolution of the weights assigned to various n-step returns during the duration of an episode. It can be seen that for many tasks, the weights evolve in a dynamic fashion as the episode goes on. This seems to validate our motivation for using dynamic Autodidactic Returns. An expanded version of the graphs for all the tasks can be found in Appendix E.\nFigure 4: Evolution of weights given to each of the n-step returns(where n \u2264 20) with training time"}, {"heading": "4.4 Analysis of the Learned Value Function", "text": "In this paper, we propose two methods for learning value functions in a more sophisticated manner than using n-step returns. Hence, it is important to analyze the value functions learned by our methods and understand whether our methods are indeed able to learn better value functions than baseline methods or not. For this sub-section we trained A3C agents to serve as baselines. To verify our claims about better learning of value functions, we conducted the following experiment. We took trained CARA3C LRA3C and A3C agents and computed the L2 loss between the value function V (st) predicted by a methods and the actual discounted sum of returns ( \u2211T\u2212t k=0 \u03b3\nkrt+k). We averaged this quantity over 10 episodes and plotted it as a function of time steps within an episode. Figure 6 demonstrates that our novel method CARA3C learns a much better estimate of the Value function V (st) than LRA3C and A3C. The only exception to this is the game of Kangaroo. The reason that A3C and LRA3C critics manage to estimate the value function well in Kangaroo is because the policy is no better than random and in fact their agents often scores score of 0 (which is easy to estimate)."}, {"heading": "5 Conclusion and Future Work", "text": "We propose a straightforward way to incorporate \u03bb-returns into the A3C algorithm and carry out a large-scale benchmarking of the resulting algorithm LRA3C. We go on to propose a natural generalization of \u03bb-returns called Confidence-based Autodidactic returns (CAR). In CAR, the agent learns to assign weights dynamically to the various n-step returns from which it can bootstrap. Our experiments demonstrate the efficacy of sophisticated mixture of multi-steps returns with at least one of CARA3C or LRA3C out-performing A3C in 18 out of 22 tasks. In 9 of the tasks CARA3C performs the best whereas in 9 of them LRA3C is the best. CAR gives the agent the freedom to learn and decide how much it wants to weigh each of its n-step returns. The concept of Autodidactic Returns is about the generic idea of giving the DRL agent the ability to model confidence in its own predictions. We demonstrate that this can lead to better TD-targets, in turn leading to improved performances. We have proposed only one way of modeling the autodidactic weights wherein we use the confidence values that are predicted alongside the value function estimates. There are multiple other ways in which these n-step return weights can be modeled. We believe these ways of modeling weighted returns can lead to even better generalization in terms how the agent perceives it\u2019s TD-target. Modeling and bootstrapping off TD-targets is fundamental to RL. We believe that our proposed idea of CAR can be combined with any DRL algorithm (Mnih et al., 2015; Jaderberg et al., 2017; Sharma et al., 2017)wherein the TD-target is modeled in terms of n-step returns."}, {"heading": "Appendix A: Experimental Details", "text": "Since the baseline scores used in this work are from Sharma et al. (2017), we use the same training and evaluation regime as well."}, {"heading": "On hyper-parameters", "text": "We used the LSTM-variant of A3C [Mnih et al. (2016)] algorithm for the CARA3C and LRA3C experiments. The async-rmsprop algorithm [Mnih et al. (2016)] was used for updating parameters with the same hyper-parameters as in Mnih et al. (2016). The initial learning rate used was 10\u22123 and it was linearly annealed to 0 over 100 million time steps, which was the length of the training period. The n used in n-step returns was 20. Entropy regularization was used to encourage exploration, similar to Mnih et al. (2016). The \u03b2 for entropy regularization was found to be 0.01 after hyper-parameter tuning, both for CARA3C and LRA3C, separately. The \u03b2 was tuned in the set {0.01, 0.02}. The optimal initial learning rate was found to be 10\u22123 for both CARA3C and LRA3C separately. The learning rate was tuned over the set {7 \u00d7 10\u22124, 10\u22123, 3 \u00d7 10\u22123}. The discounting factor for rewards was retained at 0.99 since it seems to work well for a large number of methods (Mnih et al., 2016; Sharma et al., 2017; Jaderberg et al., 2017). The most important hyper-parameter in the LRA3C method is the \u03bb for the \u03bb-returns. This was tuned extensively over the set {0.05, 0.15, 0.5, 0.85, 0.9, 0.95, 0.99}. The best four performing models have been reported in Figure 14b. The best performing models had \u03bb = 0.9.\nAll the models were trained for 100 million time steps. This is in keeping with the training regime in Sharma et al. (2017) to ensure fair comparisons to the baseline scores. Evaluation was done after every 1 million steps of training and followed the strategy described in Sharma et al. (2017) to ensure fair comparison with the baseline scores. This evaluation was done after each 1 million time steps of training for 100 episodes , with each episode\u2019s length capped at 20000 steps, to arrive at an average score. The evolution of this average game-play performance with training progress has been demonstrated for a few tasks in Figure 3. An expanded version of the figure for all the tasks can be found in Appendix C. Table 1 in Appendix B contains the raw scores obtained by CARA3C, LRA3C and A3C agents on 22 Atari 2600 tasks. The evaluation was done using the latest agent obtained after training for 100 million steps, to be consistent with the evaluation regime presented in Sharma et al. (2017) and Mnih et al. (2016).\nArchitecture details\nWe used a low level architecture similar to Mnih et al. (2016); Sharma et al. (2017) which in turn uses the same low level architecture as Mnih et al. (2015). Figure 1 contains a visual depiction of the network used for CARA3C. The common parts of the CARA3C and LRA3C networks are described below: The first three layers of both the methods are convolutional layers with same filter sizes, strides, padding and number of filters as Mnih et al. (2015, 2016); Sharma et al. (2017). These convolutional layers are followed by two fully connected (FC) layers and an LSTM layer. A policy and a value function are derived from the LSTM outputs using two different output heads. The number of neurons in each of the FC layers and the LSTM layers is 256. These design choices have been taken from Sharma et al. (2017) to ensure fair comparisons to the baseline A3C model and apply to both the CARA3C and LRA3C methods.\nSimilar to Mnih et al. (2016) the Actor and Critic share all but the final layer. In the case of CARA3C, Each of the three functions: policy, value function and the confidence value are realized with a different final output layer, with the confidence and value function outputs having no non-linearity and one output-neuron and with the policy and having a softmax-non linearity of size equal to size of the action space of the task. This non-linearity is used to model the multinomial distribution."}, {"heading": "Appendix B: Table of Raw Scores", "text": "All the evaluations were done using the agent obtained after training for 100 million steps, to be consistent with the evaluation paradigm presented in Sharma et al. (2017) and Mnih et al. (2016). Both CARA3C and LRA3C scores are obtained by averaging across 3 random seeds. The scores for A3C column were taken from Table 4 of Sharma et al. (2017)."}, {"heading": "Appendix C: Training Graphs", "text": "The evaluation strategy described in Appendix A was executed to generate training curves for all the 22 Atari tasks. This appendix contains all those training curves. These curves demonstrate how the performance of the CARA3C and LRA3C agents evolves with time."}, {"heading": "Appendix D: Evolution of confidence-based weights with training time", "text": "This appendix reports the results of an expanded version of the experiments done in Section 4.3. If we assume that states are numbered from 1, then for the set of states\nSt\u22611 (mod 20) = {st : t \u2261 1 (mod 20)} = {s1, s21, \u00b7 \u00b7 \u00b7 }\nthe confidence values were used for obtaining the weights wi as demonstrated in Section 3.2. These weights were plotted as a function of training progress. Figure 8 demonstrates that in general, the confidence-based weights assigned to n-step returns based on larger n become larger with training progress.\nA similar plot for all states in the set St\u226110 (mod 20) is in Figure 9 . Note that these states can only bootstrap from 10 possible next states, because of the way that A3C is implemented. Hence, these states use a 10-sized vector for confidence-based weights and subsequently the plots of these weights versus training time have ten lines. This plot helps us conclude that the trend of the confidence-based weights assigned to n-step returns based on larger n becoming larger with training progress is not limited to the set of states St\u22611 (mod 20) but generalizes to states in the set St\u226110 (mod 20) as well.\nNote that plots similar to Figure 9 and Figure 8 can be obtained for any Set St\u2261i (mod 20) with i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 19}. However, for the sake of keeping the length of this manuscript tangible we report such plots only for other extreme case, namely for the set of states St\u226119 (mod 20). Such states can bootstrap from only two possible next states and hence the plots of their corresponding weights versus training time have two lines.\nAppendix E: Evolution of weights assigned to n-step returns within an Episode\nThis appendix is dedicated to understanding how the weights assigned to n-step returns evolve during the duration of a single episode. Similar to the previous section St\u22611 (mod 20) is defined as:\nSt\u22611 (mod 20) = {st : t \u2261 1 (mod 20)} = {s1, s21, \u00b7 \u00b7 \u00b7 }\nNext, states in the set St\u22611 (mod 20) are sorted and the weights assigned to various n-step returns are plotted versus the index in that sorted set. This demonstrates how the weights evolve with the passage of time within an episode. Figure 11 demonstrates that in general the weights assigned to various n-step returns stabilize with the progress of time within an episode.\nThis seems to indicate that the vast majority of improvement brought about by our algorithm CARA3C is through the dynamic bootstrapping done at the beginning of the episode and that it becomes less important towards the end of the episode. This is to be expected as the Value estimates become more accurate towards the end of the episode. Figure 12 contains a plot similar to Figure 11, albeit for the set of states St\u226110 (mod 20)\nFigure 13 contains a plot similar to Figure 11, albeit for the set of states St\u226119 (mod 20)\nFigure 13: Evolution of weights given to each of the n-step returns(where n \u2264 2) with training time"}, {"heading": "Appendix F: Algorithm for training CARA3C", "text": "The algorithm corresponding to CARA3C, our main novel contribution has been presented in Algorithm 8. A similar algorithm can be constructed for LRA3C easily.\nAlgorithm 1 CARA3C 1: // Assume global shared parameter vectors \u03b8. 2: // Assume global step counter (shared) T = 0 3: 4: K \u2190 Maximum value of n in n-step returns 5: Tmax \u2190 Total number of training steps for CARA3C 6: \u03c0 \u2190 Policy of the agent 7: Initialize local thread\u2019s step counter t\u2190 1 8: Let \u03b8\u2032 be the local thread\u2019s parameters 9: 10: repeat 11: tinit = t 12: d\u03b8 \u2190 0 13: dw \u2190 0 14: Synchronize local thread parameters \u03b8\u2032 = \u03b8 15: confidences\u2190[ ] 16: Obtain state st 17: states\u2190 [ ] 18: repeat 19: states.append(st) 20: Sample at \u223c \u03c0(at|st; \u03b8\u2032) 21: Execute action at to obtain reward rt and obtain next state st+1 22: confidences.append(c(st+1; \u03b8\u2032)) 23: t\u2190 t+ 1 24: T \u2190 T + 1 25: until st is terminal or t == tinit +K 26: if st is terminal then 27: R\u2190 0 28: else 29: R\u2190 V (st; \u03b8\u2032) 30: C \u2190 get_weights_matrix(confidences) 31: Gij \u2190 the i-step return used for bootstraping estimate for states[j] // K \u00d7K matrix 32: for i \u2208 {t\u2212 1, \u00b7 \u00b7 \u00b7 , tinit} do 33: j \u2190 i\u2212 tinit 34: R\n\u2032 \u2190 R 35: for k \u2208 {K \u2212 1, \u00b7 \u00b7 \u00b7 , j} do"}, {"heading": "36: R", "text": "\u2032 \u2190 ri + \u03b3R \u2032 37: Gkj \u2190 Ckj .R \u2032\n// Assuming 0-based indexing 38: R\u2190 V (si; \u03b8\u2032) 39: 40: for i \u2208 {t\u2212 1, . . . tinit} do 41: j \u2190 i\u2212 tinit 42: TD-target\u2190 \u2211K\u22121 j=0 Gij\n43: Gradients for \u03b8 based on \u03c0: d\u03b8\u2032 \u2190 d\u03b8\u2032 +\u2207\u03b8 ( log(\u03c0(ai|si; \u03b8\u2032) )( TD-target\u2212 V (si) ) 44: Gradients for \u03b8 based on V : d\u03b8\u2032 \u2190 d\u03b8\u2032 +\u2207\u03b8 ( TD-target\u2212 V (si; \u03b8\u2032)\n)2 45: Perform asynchronous update of \u03b8 using d\u03b8\u2032 46: until T > Tmax\nAlgorithm 2 Creates a 2D weights matrix with the confidence numbers 1: function GET_WEIGHTS_MATRIX(C) 2: w \u2190 softmax(C) 3: W \u2190 1\u2297 wT // W is a K \u00d7K weight matrix with Wi = wT . \u2297 is outer product. 4: for wi,j \u2208W do 5: if i > j then 6: wi,j \u2190 0 7: for wi \u2208W do 8: wi =\nwi sum(wi)\nreturn W\nChoice of Important hyper-parameters for our methods\nPerhaps the most important hyper-parameter in CARA3C is the network which calculates the confidence values. Since gradients do not flow back from confidence computation to the LSTM controller, this becomes an important design choice. We experimented extensively with different types of confidence computation networks including shallow ones, deep ones, wide ones and narrow ones. We found a \"zero hidden layer\" network on top of the LSTM controller (much like one which computes the value function) works the best (Figure 14a). Similarly, the most important hyper-parameter in \u03bb-returns is the \u03bb from eq. (3). While we experimented with a large number and range of values for \u03bb the best performing ones have been reported in Figure 14b."}], "references": [{"title": "Investigating recurrence and eligibility traces in deep q-networks", "author": ["Harb", "Jean", "Precup", "Doina"], "venue": "arXiv preprint arXiv:1704.05495,", "citeRegEx": "Harb et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Harb et al\\.", "year": 2017}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "To appear in 5th International Conference on Learning Representations,", "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "Actor-critic algorithms. In Advances in neural information processing", "author": ["Konda", "Vijay R", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Konda et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2000}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Incremental multi-step q-learning", "author": ["Peng", "Jing", "Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Peng et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Peng et al\\.", "year": 1996}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["Puterman", "Martin L"], "venue": null, "citeRegEx": "Puterman and L.,? \\Q2014\\E", "shortCiteRegEx": "Puterman and L.", "year": 2014}, {"title": "On-line Q-learning using connectionist systems", "author": ["Rummery", "Gavin A", "Niranjan", "Mahesan"], "venue": "University of Cambridge, Department of Engineering,", "citeRegEx": "Rummery et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Rummery et al\\.", "year": 1994}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "True online td (lambda)", "author": ["Seijen", "Harm", "Sutton", "Rich"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Seijen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2014}, {"title": "Learning to repeat: Fine grained action repetition for deep reinforcement learning", "author": ["Sharma", "Sahil", "Lakshminarayanan", "Aravind S", "Ravindran", "Balaraman"], "venue": "To appear in 5th International Conference on Learning Representations,", "citeRegEx": "Sharma et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2017}, {"title": "Deterministic policy gradient algorithms", "author": ["Silver", "David", "Lever", "Guy", "Heess", "Nicolas", "Degris", "Thomas", "Wierstra", "Daan", "Riedmiller", "Martin A"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Silver et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2014}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Silver", "David", "Huang", "Aja", "Maddison", "Chris J", "Guez", "Arthur", "Sifre", "Laurent", "Van Den Driessche", "George", "Schrittwieser", "Julian", "Antonoglou", "Ioannis", "Panneershelvam", "Veda", "Lanctot", "Marc"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Sutton", "Richard S", "McAllester", "David A", "Singh", "Satinder P", "Mansour", "Yishay"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Technical note: Q-learning", "author": ["Watkins", "Christopher J.C. H", "Dayan", "Peter"], "venue": "Mach. Learn.,", "citeRegEx": "Watkins et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Watkins et al\\.", "year": 1992}, {"title": "Experimental Details Since the baseline scores used in this work are from Sharma et al. (2017), we use the same training and evaluation regime as well. On hyper-parameters We used the LSTM-variant of A3C [Mnih et al. (2016)] algorithm for the CARA3C and LRA3C", "author": ["A Appendix"], "venue": null, "citeRegEx": "Appendix,? \\Q2016\\E", "shortCiteRegEx": "Appendix", "year": 2016}, {"title": "The async-rmsprop algorithm [Mnih et al", "author": ["Mnih"], "venue": "Mnih et al", "citeRegEx": "Mnih,? \\Q2016\\E", "shortCiteRegEx": "Mnih", "year": 2016}, {"title": "\u03bb for the \u03bb-returns", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2017\\E", "shortCiteRegEx": "Sharma", "year": 2017}, {"title": "after every 1 million steps of training and followed the strategy", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2000\\E", "shortCiteRegEx": "Sharma", "year": 2000}, {"title": "Table 1 in Appendix B contains the raw scores obtained by CARA3C, LRA3C and A3C agents on 22 Atari 2600 tasks. The evaluation was done using the latest agent obtained after training for 100 million steps, to be consistent with the evaluation regime", "author": ["C. Appendix"], "venue": "Mnih et al", "citeRegEx": "Appendix,? \\Q2016\\E", "shortCiteRegEx": "Appendix", "year": 2016}, {"title": "Architecture details We used a low level architecture", "author": ["Mnih"], "venue": "Mnih et al", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "The first three layers of both the methods are convolutional layers with same filter sizes, strides, padding and number of filters", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2017\\E", "shortCiteRegEx": "Sharma", "year": 2017}, {"title": "2017) to ensure fair comparisons to the baseline A3C model and apply to both the CARA3C and LRA3C methods. Similar to Mnih et al. (2016) the Actor and Critic share all but the final layer. In the case of CARA3C, Each of the three functions: policy, value function and the confidence value are realized with a different final output layer, with the confidence and value function outputs having no non-linearity", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2016\\E", "shortCiteRegEx": "Sharma", "year": 2016}], "referenceMentions": [{"referenceID": 12, "context": "The use of deep neural networks in conjunction with RL objectives has shown remarkable results such as learning to solve the Atari 2600 tasks from raw pixels (Bellemare et al., 2013; Mnih et al., 2015, 2016; Sharma et al., 2017; Jaderberg et al., 2017), learning to solve complex simulated physics tasks (Todorov et al.", "startOffset": 158, "endOffset": 252}, {"referenceID": 2, "context": "The use of deep neural networks in conjunction with RL objectives has shown remarkable results such as learning to solve the Atari 2600 tasks from raw pixels (Bellemare et al., 2013; Mnih et al., 2015, 2016; Sharma et al., 2017; Jaderberg et al., 2017), learning to solve complex simulated physics tasks (Todorov et al.", "startOffset": 158, "endOffset": 252}, {"referenceID": 18, "context": ", 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al.", "startOffset": 59, "endOffset": 128}, {"referenceID": 10, "context": ", 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al.", "startOffset": 59, "endOffset": 128}, {"referenceID": 4, "context": ", 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al.", "startOffset": 59, "endOffset": 128}, {"referenceID": 14, "context": ", 2015) and showing super-human performance on the ancient board game of Go (Silver et al., 2016).", "startOffset": 76, "endOffset": 97}, {"referenceID": 6, "context": "With the advent of deep RL, the use of multi-step returns has gained a lot of popularity (Mnih et al., 2016).", "startOffset": 89, "endOffset": 108}, {"referenceID": 2, "context": ", 2017; Jaderberg et al., 2017), learning to solve complex simulated physics tasks (Todorov et al., 2012; Schulman et al., 2015; Lillicrap et al., 2015) and showing super-human performance on the ancient board game of Go (Silver et al., 2016). Building accurate and powerful (in terms of generalization capabilities) state and action value function (Sutton & Barto, 1998) estimators is important for successful RL solutions. This is because many practical RL solutions (Q-Learning (Watkins & Dayan, 1992), SARSA (Rummery & Niranjan, 1994) and Actor-Critic Methods (Konda & Tsitsiklis, 2000)) use TD Learning (Sutton, 1988). The ability to build better estimates of the value functions directly results in better policy estimates as well as faster learning. \u03bb-returns (LR) (Sutton & Barto, 1998) are very effective in this regard. They are effective for faster propagation of delayed rewards and also result in more reliable learning. LR provide a trade-off between Monte Carlo and TD learning methods. They model the TD target using a mixture of n-step returns, wherein the weighs are exponentially decayed. With the advent of deep RL, the use of multi-step returns has gained a lot of popularity (Mnih et al., 2016). However, \u03bb-returns have not been explored extensively in the deep RL setting. Having said that, it is to be noted that the use of exponentially decaying weighting for various n-step returns seems to be an ad-hoc design choice made by LR. In this paper, we start off by extensively benchmarking \u03bb-returns (our experiments only use truncated \u03bb-returns due to the nature of the DRL algorithm (A3C) that we work with) and also propose a generalization of \u03bb-returns called the Confidence-based Autodidactic Returns (CAR), In CAR, the DRL agent learns in an end-to-end way, the weights to assign to the various n-step return based targets. Also note that in CAR, the weights assigned to various n-step returns change based on the current state that the DRL agent is in. In this sense, CAR weights are dynamic and using them represents a significant level of sophistication as compared to the usage of \u03bb-returns. In the Deep Reinforcement Learning (DRL) setting not much has been done in the context of using mixture of multi-step returns to build better value function estimators. An exception to that statement is the investigation done in Harb & Precup (2017). It shows promising results on two Atari 2600 tasks using the backward view of eligibility traces (\u03bb-returns constitute the forward view of eligibility traces) - Pong and Tennis.", "startOffset": 8, "endOffset": 2374}, {"referenceID": 17, "context": "They update the policy parameters using Policy Gradient Theorem (Sutton et al., 1999; Silver et al., 2014) based objective functions.", "startOffset": 64, "endOffset": 106}, {"referenceID": 13, "context": "They update the policy parameters using Policy Gradient Theorem (Sutton et al., 1999; Silver et al., 2014) based objective functions.", "startOffset": 64, "endOffset": 106}, {"referenceID": 5, "context": "Asynchronous Advantage Actor Critic(A3C) (Mnih et al. (2016)) introduced the first class of actor-critic algorithms which worked on high-dimensional complex visual input space.", "startOffset": 42, "endOffset": 61}, {"referenceID": 5, "context": "We believe that our proposed idea of CAR can be combined with any DRL algorithm (Mnih et al., 2015; Jaderberg et al., 2017; Sharma et al., 2017)wherein the TD-target is modeled in terms of n-step returns.", "startOffset": 80, "endOffset": 144}, {"referenceID": 2, "context": "We believe that our proposed idea of CAR can be combined with any DRL algorithm (Mnih et al., 2015; Jaderberg et al., 2017; Sharma et al., 2017)wherein the TD-target is modeled in terms of n-step returns.", "startOffset": 80, "endOffset": 144}, {"referenceID": 12, "context": "We believe that our proposed idea of CAR can be combined with any DRL algorithm (Mnih et al., 2015; Jaderberg et al., 2017; Sharma et al., 2017)wherein the TD-target is modeled in terms of n-step returns.", "startOffset": 80, "endOffset": 144}], "year": 2017, "abstractText": "Reinforcement Learning (RL) can model complex behavior policies for goaldirected sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using next state\u2019s value function. \u03bb-returns generalize beyond 1-step returns and strike a balance between Monte Carlo and TD learning methods. While \u03bb-returns have been extensively studied in RL, they haven\u2019t been explored a lot in Deep RL. This paper\u2019s first contribution is an exhaustive benchmarking of \u03bb-returns. Although mathematically tractable, the use of exponentially decaying weighting of n-step returns based targets in \u03bb-returns is a rather ad-hoc design choice. Our second major contribution is that we propose a generalization of \u03bb-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. This allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. In contrast, \u03bb-returns restrict RL agents to use an exponentially decaying weighting scheme. Autodidactic returns can be used for improving any RL algorithm which uses TD learning. We empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and \u03bb-returns) considerably outperforms the use of n-step returns. We perform our experiments on the Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.", "creator": "LaTeX with hyperref package"}}}