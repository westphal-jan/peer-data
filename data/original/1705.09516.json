{"id": "1705.09516", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2017", "title": "Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models", "abstract": "Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex hand-crafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network (RNN) to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task.", "histories": [["v1", "Fri, 26 May 2017 10:36:12 GMT  (589kb,D)", "http://arxiv.org/abs/1705.09516v1", "The work has been accepted in BioNLP at ACL-2017"]], "COMMENTS": "The work has been accepted in BioNLP at ACL-2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["patchigolla v s s rahul", "sunil kumar sahu", "ashish anand"], "accepted": false, "id": "1705.09516"}, "pdf": {"name": "1705.09516.pdf", "metadata": {"source": "CRF", "title": "Biomedical Event Trigger Identification Using Bidirectional Recurrent Neural Network Based Models", "authors": ["Patchigolla V S S Rahul", "Sunil Kumar Sahu", "Ashish Anand"], "emails": ["rahul.rahul.pvss@gmail.com", "anand.ashish}@iitg.ernet.in"], "sections": [{"heading": null, "text": "Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex handcrafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network (RNN) to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-ofart F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task."}, {"heading": "1 Introduction", "text": "Biomedical events play an important role in improving biomedical research in many ways. Some of its applications include pathway curation (Ohta et al., 2013) and development of domain specific semantic search engine (Ananiadou et al., 2015). So as to gain attraction among researchers many challenges such as BioNLP\u201909 (Kim et al., 2009), BioNLP\u201911 (Kim et al., 2011), BioNLP\u201913 (Ne\u0301dellec et al., 2013) have been organized and many novel methods have also been proposed addressing these tasks.\nAn event can be defined as a combination of a trigger word and arbitrary number of arguments. Figure 1 shows two events with trigger words as \u201cInhibition\u201d and \u201cAngiogenesis\u201d of trigger types \u201cNegative Regulation\u201d and \u201cBlood Vessel Development\u201d respectively. Pipelined based approaches for biomedical event extraction include event trigger identification followed by event argument identification. Analysis in multiple studies (Wang et al., 2016b; Zhou et al., 2014) reveal that more than 60% of event extraction errors are caused due to incorrect trigger identification.\nExisting event trigger identification models can be broadly categorized in two ways: rule based approaches and machine learning based approaches. Rule based approaches use various strategies including pattern matching and regular expression to define rules (Vlachos et al., 2009). However, defining these rules are very difficult, time consuming and requires domain knowledge. The overall performance of the task depends on the quality of rules defined. These approaches often fail to generalize for new datasets when compared with machine learning based approaches. Machine learning based approaches treat the trigger identification problem as a word level classification problem, where many features from the data are extracted using various NLP toolkits (Pyysalo et al., 2012; Zhou et al., 2014) or learned automatically (Wang et al., 2016a,b).\nIn this paper, we propose an approach using RNN to learn higher level features without the requirement of complex feature engineering. We thoroughly evaluate our proposed approach on\nar X\niv :1\n70 5.\n09 51\n6v 1\n[ cs\n.C L\n] 2\n6 M\nay 2\n01 7\nMLEE corpus. We also have performed categorywise analysis and investigate the importance of different features in trigger identification task."}, {"heading": "2 Related Work", "text": "Many approaches have been proposed to address the problem of event trigger identification. Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification. Zhou et al. (2014) proposed a novel framework for trigger identification where embedding features of the word combined with hand-crafted features are fed to SVM for final classification using multiple kernel learning. Wei et al. (2015) proposed a pipeline method on BioNLP\u201913 corpus based on Conditional Random Field (CRF) and Support vector machine (SVM) where the CRF is used to tag valid triggers and SVM is finally used to identify the trigger type. The above methods rely on various NLP toolkits to extract the hand-crafted features which leads to error propagation thus affecting the classifier\u2019s performance. These methods often need to tailor different features for different tasks, thus not making them generalizable. Most of the hand-crafted features are also traditionally sparse one-hot features vector which fail to capture the semantic information.\nWang et al. (2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al., 2011) to perform final classification. Wang et al. (2016a) proposed another model based on convolutional neural network (CNN) where word and entity mention features of words within a window around the word are fed to a CNN to perform final classification. Although both of the methods have achieved good performance they fail to capture features outside the window."}, {"heading": "3 Model Architecture", "text": "We present our model based on bidirectional RNN as shown in Figure 2 for the trigger identification task. The proposed model detects trigger word as well as their type. Our model uses embedding features of words in the input layer and learns higher level representations in the subsequent layers and\nmakes use of both the input layer and higher level features to perform the final classification. We now briefly explain about each component of our model."}, {"heading": "3.1 Input Feature Layer", "text": "For every word in the sentence we extract two features, exact word w \u2208 W and entity type e \u2208 E. Here W refers the dictionary of words and E refers to dictionary of entities. Apart from all the entities, E also contains a None entity type which indicates absence of an entity. In some cases the entity might span through multiple words, in that case we assign every word spanned by that entity the same entity type."}, {"heading": "3.2 Embedding or Lookup Layer", "text": "In this layer every input feature is mapped to a dense feature vector. Let us say that Ew and Ee be the embedding matrices of W and E respectively. The features obtained from these embedding matrices are concatenated and treated as the final word-level feature (l) of the model.\nThe Ew \u2208 Rnw\u00d7dw embedding matrix is initialized with pre-trained word embeddings and Ee \u2208 Rne\u00d7de embedding matrix is initialized with random values. Here nw, ne refer to length of the word dictionary and entity type dictionary respectively and dw, de refer to dimension of word and entity type embedding respectively."}, {"heading": "3.3 Bidirectional RNN Layer", "text": "RNN is a powerful model for learning features from sequential data. We use both LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) variants of RNN in our ex-\nperiments as they handle the vanishing and exploding gradient problem (Pascanu et al., 2012) in a better way. We use bidirectional version of RNN (Graves, 2013) where for every word forward RNN captures features from the past and the backward RNN captures features from future, inherently each word has information about whole sentence."}, {"heading": "3.4 Feed Forward Neural Network", "text": "The hidden state of the bidirectional RNN layer acts as sentence-level feature (g), the word and entity type embeddings (l) act as a word-level features, are both concatenated (1) and passed through a series of hidden layers (2), (3) with dropout (Srivastava et al., 2014) and an output layer. In the output layer, the number of neurons are equal to the number of trigger labels. Finally we use Softmax function (4) to obtain probability score for each class.\nf = gk \u2295 lk (1) h0 = tanh(W0f + b0) (2)\nhi = tanh(Wihi\u22121 + bi) (3) p(y|x) = Softmax(Wohi + bo) (4)\nHere k refers to the kth word of the sentence, i refers to the ith hidden layer in the network and \u2295 refers to concatenation operation. Wi,Wo and bi,bo are parameters of the hidden and output layers of the network respectively."}, {"heading": "3.5 Training and Hyperparameters", "text": "We use cross entropy loss function and the model is trained using stochastic gradient descent. The implementation1 of the model is done in python language using Theano (Bergstra et al., 2010) library. We use pre-trained word embeddings obtained by Moen et al. (2013) using word2vec tool (Mikolov et al., 2013).\nWe use training and development set for hyperparameter selection. We use word embeddings of 200 dimension, entity type embeddings of 50 dimension, RNN hidden state dimension of 250 and 2 hidden layers with dimension 150 and 100. In both the hidden layers we use dropout of 0.2.\n1Implementation is available at https: //github.com/rahulpatchigolla/ EventTriggerDetection"}, {"heading": "4 Experiments and discussion", "text": ""}, {"heading": "4.1 Dataset Description", "text": "We use MLEE (Pyysalo et al., 2012) corpus for performing our trigger identification experiments. Unlike other corpora on event extraction it covers events across various levels from molecular to organism level. The events in this corpus are broadly divided into 4 categories namely \u201cAnatomical\u201d, \u201cMolecular\u201d, \u201cGeneral\u201d, \u201cPlanned\u201d which are further divided into 19 sub-categories as shown in Table 1. Here our task is to identify correct subcategory of an event. The entity types associated with the dataset are summarized in Table 2."}, {"heading": "4.2 Experimental Design", "text": "The data is provided in three parts as training, development and test sets. Hyperparameters are tuned using development set and then final model is trained on the combined set of training and development sets using the selected hyperparameters. The final results reported here are the best results over 5 runs.\nWe have used micro averaged F1-score as the evaluation metric and evaluated the performance of the model by ignoring the trigger classes with counts \u2264 10 in test set while training and considered them directly as false-negative while testing."}, {"heading": "4.3 Performance comparison with Baseline Models", "text": "We compare our results with baseline models shown in Table 3. Pyysalo et al. (2012) defined a SVM based classifier with hand-crafted features. Zhou et al. (2014) also defined a SVM based classifier with word embeddings and hand-crafted features. Wang et al. (2016a) defined window based CNN classifier. Apart from the proposed models we also compare our results with two more baseline methods FFNN and CNN\u03c8 which are our implementations. Here FFNN is a window based feed forward neural network where embedding features of words within the window are used to predict the trigger label (Collobert et al., 2011). We chose window size as 3 (one word from left\nand one word from right) after tuning it in validation set. CNN\u03c8 is our implementation of window based CNN classifier proposed by Wang et al. (2016a) due to unavailability of their code in public domain. Our proposed model have shown slight improvement in F1-score when compared with baseline models. The proposed model\u2019s ability to capture the context of the whole sentence is likely to be one of the reasons of improvement in performance.\nWe perform one-side t-test over 5 runs of F1Scores to verify our proposed model\u2019s performance when compared with FFNN and CNN\u03a8. The p value of the proposed model (GRU) when compared with FFNN and CNN\u03c8 are 8.57\u00d710\u221207 and 1.178 \u00d7 10\u221210 respectively. This indicates statistically superior performance of the proposed model."}, {"heading": "4.4 Category Wise Performance Analysis", "text": "The category wise performance of the proposed model is shown in Table 4. It can be observed that\nmodel\u2019s performance in anatomical and molecular categories are better than general and planned categories. We can also infer from the confusion matrix shown in Figure 3 that positive regulation, negative regulation and regulation among general category and planned category triggers are causing many false positives and false negatives thus degrading the model\u2019s performance."}, {"heading": "4.5 Further Analysis", "text": "In this section we investigate the importance of various features and model variants as shown in Table 5. Here Ew and Ee refer to using word and entity type embedding as a feature in the model, l and g refer to using word-level and sentence-level feature respectively for the final prediction. For example, Ew + Ee and g means using both word and entity type embedding as the input feature for the model and g means only using the global feature (hidden state of RNN) for final prediction.\nExamples in Table 6 illustrate importance of features used in best performing models. In phrase 1 the word \u201cknockdown\u201d, is a part of an entity namely \u201cround about knockdown endothelial\ncells\u201d of type \u201cCell\u201d and in phrase 2 it is trigger word of type \u201cPlanned Process\u201d, methods 1 and 2 failed to differentiate both of them because of no knowledge about the entity type. In phrase 3 \u201cimpaired\u201d is a trigger word of type \u201cNegative Regulation\u201d methods 1 and 3 failed to correctly identify but when reinforced with word-level feature the model succeeded in identification. So, we can say that Ee feature and l+ g model variant help in improving the model\u2019s performance."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper we have proposed a novel approach for trigger identification by learning higher level features using RNN. Our experiments have shown to achieve state-of-art results on MLEE corpus. In future we would like to perform complete event extraction using deep learning techniques."}], "references": [{"title": "Event-based text mining for biology and functional genomics", "author": ["Sophia Ananiadou", "Paul Thompson", "Raheel Nawaz", "John McNaught", "Douglas B Kell."], "venue": "Briefings in functional genomics 14(3):213\u2013230.", "citeRegEx": "Ananiadou et al\\.,? 2015", "shortCiteRegEx": "Ananiadou et al\\.", "year": 2015}, {"title": "Theano: A cpu and gpu math compiler in python", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio."], "venue": "Proc. 9th Python in Science", "citeRegEx": "Bergstra et al\\.,? 2010", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "J. Mach. Learn. Res. 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "CoRR abs/1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. pages 1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Overview of bionlp\u201909 shared task on event extraction", "author": ["Jin-Dong Kim", "Tomoko Ohta", "Sampo Pyysalo", "Yoshinobu Kano", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the Workshop on Current Trends", "citeRegEx": "Kim et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "Overview of bionlp shared task", "author": ["Jin-Dong Kim", "Sampo Pyysalo", "Tomoko Ohta", "Robert Bossy", "Ngan Nguyen", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the BioNLP Shared Task 2011 Workshop. Association for Computational Linguistics,", "citeRegEx": "Kim et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2011}, {"title": "Dependencybased word embeddings", "author": ["Omer Levy", "Yoav Goldberg."], "venue": "ACL 2014. pages 302\u2013 308.", "citeRegEx": "Levy and Goldberg.,? 2014", "shortCiteRegEx": "Levy and Goldberg.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributional semantics resources for biomedical text processing", "author": ["Hans Moen", "Sampo Pyysalo", "Filip Ginter", "Tapio Salakoski", "Sophia Ananiadou"], "venue": null, "citeRegEx": "Moen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Moen et al\\.", "year": 2013}, {"title": "Overview of bionlp shared task 2013", "author": ["Claire N\u00e9dellec", "Robert Bossy", "Jin-Dong Kim", "JungJae Kim", "Tomoko Ohta", "Sampo Pyysalo", "Pierre Zweigenbaum."], "venue": "Proceedings of the BioNLP Shared Task 2013 Workshop. Association for Computational", "citeRegEx": "N\u00e9dellec et al\\.,? 2013", "shortCiteRegEx": "N\u00e9dellec et al\\.", "year": 2013}, {"title": "Overview of the pathway curation (pc) task of bionlp shared task", "author": ["Tomoko Ohta", "Sampo Pyysalo", "Rafal Rak", "Andrew Rowley", "Hong-Woo Chun", "Sung-Jae Jung", "Changhoo Jeong", "Sung-pil Choi", "Sophia Ananiadou"], "venue": null, "citeRegEx": "Ohta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ohta et al\\.", "year": 2013}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "CoRR, abs/1211.5063 .", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Event extraction across multiple levels of biological organization", "author": ["Sampo Pyysalo", "Tomoko Ohta", "Makoto Miwa", "HanCheol Cho", "Jun\u2019ichi Tsujii", "Sophia Ananiadou"], "venue": null, "citeRegEx": "Pyysalo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pyysalo et al\\.", "year": 2012}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Biomedical event extraction without training data", "author": ["Andreas Vlachos", "Paula Buttery", "Diarmuid O S\u00e9aghdha", "Ted Briscoe."], "venue": "Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing: Shared", "citeRegEx": "Vlachos et al\\.,? 2009", "shortCiteRegEx": "Vlachos et al\\.", "year": 2009}, {"title": "Biomedical event trigger detection based on convolutional neural network", "author": ["Jian Wang", "Honglei Li", "Yuan An", "Hongfei Lin", "Zhihao Yang."], "venue": "International Journal of Data Mining and Bioinformatics 15(3):195\u2013213.", "citeRegEx": "Wang et al\\.,? 2016a", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Biomedical event trigger detection by dependencybased word embedding", "author": ["Jian Wang", "Jianhai Zhang", "Yuan An", "Hongfei Lin", "Zhihao Yang", "Yijia Zhang", "Yuanyuan Sun."], "venue": "BMC Medical Genomics 9(2):45.", "citeRegEx": "Wang et al\\.,? 2016b", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "A hybrid method to extract triggers in biomedical events", "author": ["Xiaomei Wei", "Qin Zhu", "Chen Lyu", "Kai Ren", "Bo Chen."], "venue": "Journal of Digital Information Management 13(4):299.", "citeRegEx": "Wei et al\\.,? 2015", "shortCiteRegEx": "Wei et al\\.", "year": 2015}, {"title": "Event trigger identification for biomedical events extraction using domain knowledge", "author": ["Deyu Zhou", "Dayou Zhong", "Yulan He."], "venue": "Bioinformatics 30(11):1587\u20131594.", "citeRegEx": "Zhou et al\\.,? 2014", "shortCiteRegEx": "Zhou et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 12, "context": "Some of its applications include pathway curation (Ohta et al., 2013) and development of domain specific semantic search engine (Ananiadou et al.", "startOffset": 50, "endOffset": 69}, {"referenceID": 0, "context": ", 2013) and development of domain specific semantic search engine (Ananiadou et al., 2015).", "startOffset": 66, "endOffset": 90}, {"referenceID": 6, "context": "So as to gain attraction among researchers many challenges such as BioNLP\u201909 (Kim et al., 2009), BioNLP\u201911 (Kim et al.", "startOffset": 77, "endOffset": 95}, {"referenceID": 7, "context": ", 2009), BioNLP\u201911 (Kim et al., 2011), BioNLP\u201913 (N\u00e9dellec et al.", "startOffset": 19, "endOffset": 37}, {"referenceID": 11, "context": ", 2011), BioNLP\u201913 (N\u00e9dellec et al., 2013) have been organized and many novel methods have also been proposed addressing these tasks.", "startOffset": 19, "endOffset": 42}, {"referenceID": 18, "context": "Analysis in multiple studies (Wang et al., 2016b; Zhou et al., 2014) reveal that more than 60% of event extraction errors are caused due to incorrect trigger identification.", "startOffset": 29, "endOffset": 68}, {"referenceID": 20, "context": "Analysis in multiple studies (Wang et al., 2016b; Zhou et al., 2014) reveal that more than 60% of event extraction errors are caused due to incorrect trigger identification.", "startOffset": 29, "endOffset": 68}, {"referenceID": 16, "context": "Rule based approaches use various strategies including pattern matching and regular expression to define rules (Vlachos et al., 2009).", "startOffset": 111, "endOffset": 133}, {"referenceID": 14, "context": "Machine learning based approaches treat the trigger identification problem as a word level classification problem, where many features from the data are extracted using various NLP toolkits (Pyysalo et al., 2012; Zhou et al., 2014) or learned automatically (Wang et al.", "startOffset": 190, "endOffset": 231}, {"referenceID": 20, "context": "Machine learning based approaches treat the trigger identification problem as a word level classification problem, where many features from the data are extracted using various NLP toolkits (Pyysalo et al., 2012; Zhou et al., 2014) or learned automatically (Wang et al.", "startOffset": 190, "endOffset": 231}, {"referenceID": 8, "context": "(2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al.", "startOffset": 79, "endOffset": 104}, {"referenceID": 3, "context": "(2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al., 2011) to perform final classification.", "startOffset": 187, "endOffset": 211}, {"referenceID": 12, "context": "Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification. Zhou et al. (2014) proposed a novel framework for trigger identification where embedding features of the word combined with hand-crafted features are fed to SVM for final classification using multiple kernel learning.", "startOffset": 0, "endOffset": 209}, {"referenceID": 12, "context": "Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification. Zhou et al. (2014) proposed a novel framework for trigger identification where embedding features of the word combined with hand-crafted features are fed to SVM for final classification using multiple kernel learning. Wei et al. (2015) proposed a pipeline method on BioNLP\u201913 corpus based on Conditional Random Field (CRF) and Support vector machine (SVM) where the CRF is used to tag valid triggers and SVM is finally used to identify the trigger type.", "startOffset": 0, "endOffset": 426}, {"referenceID": 12, "context": "Pyysalo et al. (2012) proposed a model where various hand-crafted features are extracted from the processed data and fed into a Support Vector Machine (SVM) to perform final classification. Zhou et al. (2014) proposed a novel framework for trigger identification where embedding features of the word combined with hand-crafted features are fed to SVM for final classification using multiple kernel learning. Wei et al. (2015) proposed a pipeline method on BioNLP\u201913 corpus based on Conditional Random Field (CRF) and Support vector machine (SVM) where the CRF is used to tag valid triggers and SVM is finally used to identify the trigger type. The above methods rely on various NLP toolkits to extract the hand-crafted features which leads to error propagation thus affecting the classifier\u2019s performance. These methods often need to tailor different features for different tasks, thus not making them generalizable. Most of the hand-crafted features are also traditionally sparse one-hot features vector which fail to capture the semantic information. Wang et al. (2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al.", "startOffset": 0, "endOffset": 1073}, {"referenceID": 3, "context": "(2016b) proposed a neural network model where dependency based word embeddings (Levy and Goldberg, 2014) within a window around the word are fed into a feed forward neural network (FFNN) (Collobert et al., 2011) to perform final classification. Wang et al. (2016a) proposed another model based on convolutional neural network (CNN) where word and entity mention features of words within a window around the word are fed to a CNN to perform final classification.", "startOffset": 188, "endOffset": 265}, {"referenceID": 5, "context": "We use both LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al.", "startOffset": 17, "endOffset": 51}, {"referenceID": 2, "context": "We use both LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) variants of RNN in our ex-", "startOffset": 60, "endOffset": 80}, {"referenceID": 13, "context": "periments as they handle the vanishing and exploding gradient problem (Pascanu et al., 2012) in a better way.", "startOffset": 70, "endOffset": 92}, {"referenceID": 4, "context": "We use bidirectional version of RNN (Graves, 2013) where for every word forward RNN captures features from the past and the backward RNN captures features from future, inherently each word has information about whole sentence.", "startOffset": 36, "endOffset": 50}, {"referenceID": 15, "context": "The hidden state of the bidirectional RNN layer acts as sentence-level feature (g), the word and entity type embeddings (l) act as a word-level features, are both concatenated (1) and passed through a series of hidden layers (2), (3) with dropout (Srivastava et al., 2014) and an output layer.", "startOffset": 247, "endOffset": 272}, {"referenceID": 1, "context": "The implementation1 of the model is done in python language using Theano (Bergstra et al., 2010) library.", "startOffset": 73, "endOffset": 96}, {"referenceID": 9, "context": "(2013) using word2vec tool (Mikolov et al., 2013).", "startOffset": 27, "endOffset": 49}, {"referenceID": 1, "context": "The implementation1 of the model is done in python language using Theano (Bergstra et al., 2010) library. We use pre-trained word embeddings obtained by Moen et al. (2013) using word2vec tool (Mikolov et al.", "startOffset": 74, "endOffset": 172}, {"referenceID": 14, "context": "We use MLEE (Pyysalo et al., 2012) corpus for performing our trigger identification experiments.", "startOffset": 12, "endOffset": 34}, {"referenceID": 3, "context": "Here FFNN is a window based feed forward neural network where embedding features of words within the window are used to predict the trigger label (Collobert et al., 2011).", "startOffset": 146, "endOffset": 170}, {"referenceID": 13, "context": "Pyysalo et al. (2012) defined a SVM based classifier with hand-crafted features.", "startOffset": 0, "endOffset": 22}, {"referenceID": 13, "context": "Pyysalo et al. (2012) defined a SVM based classifier with hand-crafted features. Zhou et al. (2014) also defined a SVM based classifier with word embeddings and hand-crafted features.", "startOffset": 0, "endOffset": 100}, {"referenceID": 13, "context": "Pyysalo et al. (2012) defined a SVM based classifier with hand-crafted features. Zhou et al. (2014) also defined a SVM based classifier with word embeddings and hand-crafted features. Wang et al. (2016a) defined window based CNN classifier.", "startOffset": 0, "endOffset": 204}, {"referenceID": 3, "context": "Here FFNN is a window based feed forward neural network where embedding features of words within the window are used to predict the trigger label (Collobert et al., 2011). We chose window size as 3 (one word from left and one word from right) after tuning it in validation set. CNN\u03c8 is our implementation of window based CNN classifier proposed by Wang et al. (2016a) due to unavailability of their code in public domain.", "startOffset": 147, "endOffset": 368}, {"referenceID": 14, "context": "SVM (Pyysalo et al., 2012) 81.", "startOffset": 4, "endOffset": 26}, {"referenceID": 20, "context": "SVM+We (Zhou et al., 2014) 80.", "startOffset": 7, "endOffset": 26}, {"referenceID": 17, "context": "82 CNN (Wang et al., 2016a) 80.", "startOffset": 7, "endOffset": 27}], "year": 2017, "abstractText": "Biomedical events describe complex interactions between various biomedical entities. Event trigger is a word or a phrase which typically signifies the occurrence of an event. Event trigger identification is an important first step in all event extraction methods. However many of the current approaches either rely on complex handcrafted features or consider features only within a window. In this paper we propose a method that takes the advantage of recurrent neural network (RNN) to extract higher level features present across the sentence. Thus hidden state representation of RNN along with word and entity type embedding as features avoid relying on the complex hand-crafted features generated using various NLP toolkits. Our experiments have shown to achieve state-ofart F1-score on Multi Level Event Extraction (MLEE) corpus. We have also performed category-wise analysis of the result and discussed the importance of various features in trigger identification task.", "creator": "LaTeX with hyperref package"}}}