{"id": "1705.10229", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Latent Intention Dialogue Models", "abstract": "Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.", "histories": [["v1", "Mon, 29 May 2017 15:01:44 GMT  (397kb,D)", "http://arxiv.org/abs/1705.10229v1", "Accepted at ICML 2017"]], "COMMENTS": "Accepted at ICML 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE stat.ML", "authors": ["tsung-hsien wen", "yishu miao", "phil blunsom", "steve j young"], "accepted": true, "id": "1705.10229"}, "pdf": {"name": "1705.10229.pdf", "metadata": {"source": "CRF", "title": "Latent Intention Dialogue Models", "authors": ["Tsung-Hsien Wen", "Yishu Miao", "Phil Blunsom", "Steve Young"], "emails": ["<thw28@cam.ac.uk>,", "<yishu.miao@cs.ox.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "Recurrent neural networks (RNNs) have shown impressive results in modeling generation tasks that have a sequential structured output form, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy & Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al., 2015; Kiddon et al., 2016). These discriminative models are trained to learn only a conditional output distribution over strings and despite the sophisticated architectures and condition-\n*Equal contribution 1Department of Engineering, University of Cambridge, Cambridge, United Kingdom 2Department of Computer Science, University of Oxford, Oxford, United Kingdom. Correspondence to: Tsung-Hsien Wen <thw28@cam.ac.uk>, Yishu Miao <yishu.miao@cs.ox.ac.uk>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s).\ning mechanisms used to ensure salience, they are not able to model the underlying actions needed to generate natural dialogues. As a consequence, these sequence-to-sequence models are limited in their ability to exhibit the intrinsic variability and stochasticity of natural dialogue. For example both goal-oriented dialogue systems (Wen et al., 2017; Bordes & Weston, 2017) and sequence-to-sequence learning chatbots (Vinyals & Le, 2015; Shang et al., 2015; Serban et al., 2015) struggle to generate diverse yet causal responses (Li et al., 2016a; Serban et al., 2016). In addition, there is often insufficient training data for goal-oriented dialogues which results in over-fitting which prevents deterministic models from learning effective and scalable interactions. In this paper, we propose a latent variable model \u2013 Latent Intention Dialogue Model (LIDM) \u2013 for learning the complex distribution of communicative intentions in goaloriented dialogues. Here, the latent variable representing dialogue intention can be considered as the autonomous decision-making center of a dialogue agent for composing appropriate machine responses.\nRecent advances in neural variational inference (Kingma & Welling, 2014; Mnih & Gregor, 2014) have sparked a series of latent variable models applied to NLP (Bowman et al., 2015; Serban et al., 2016; Miao et al., 2016; Cao & Clark, 2017). For models with continuous latent variables, the reparameterisation trick (Kingma & Welling, 2014) is commonly used to build an unbiased and low-variance gradient estimator for updating the models. However, since a continuous latent space is hard to interpret, the major benefits of these models are the stochasticity and the regularisation brought by the latent variable. In contrast, models with discrete latent variables are able to not only produce interpretable latent distributions but also provide a principled framework for semi-supervised learning (Kingma et al., 2014). This is critical for NLP tasks, especially where additional supervision and external knowledge can be utilized for bootstrapping (Faruqui et al., 2015; Miao & Blunsom, 2016; Koc\u030cisky\u0301 et al., 2016). However, variational inference with discrete latent variables is relatively difficult due to the problem of high variance during sampling. Hence we introduce baselines, as in the REINFORCE (Williams, 1992) algorithm, to mitigate the high variance problem, and carry out efficient neural variational inference (Mnih & Gregor, 2014) for the latent variable model. ar X iv :1 70 5. 10 22 9v 1\n[ cs\n.C L\n] 2\n9 M\nay 2\n01 7\nIn the LIDM, the latent intention is inferred from user input utterances. Based on the dialogue context, the agent draws a sample as the intention which then guides the natural language response generation. Firstly, in the framework of neural variational inference (Mnih & Gregor, 2014), we construct an inference network to approximate the posterior distribution over the latent intention. Then, by sampling the intentions for each response, we are able to directly learn a basic intention distribution on a human-human dialogue corpus by optimising the variational lower bound. To further reduce the variance, we utilize a labeled subset of the corpus in which the labels of intentions are automatically generated by clustering. Then, the latent intention distribution can be learned in a semi-supervised fashion, where the learning signals are either from the direct supervision (labeled set) or the variational lower bound (unlabeled set).\nFrom the perspective of reinforcement learning, the latent intention distribution can be interpreted as the intrinsic policy that reflects human decision-making under a particular conversational scenario. Based on the initial policy (latent intention distribution) learnt from the semi-supervised variational inference framework, the model can refine its strategy easily against alternative objectives using policy gradient-based reinforcement learning. This is somewhat analagous to the training process used in AlphaGo (Silver et al., 2016) for the game of Go. Based on LIDM, we show that different learning paradigms can be brought together under the same framework to bootstrap the development of a dialogue agent (Li et al., 2016c; Weston, 2016).\nIn summary, the contribution of this paper is two-fold: firstly, we show that the neural variational inference framework is able to discover discrete, interpretable intentions from data to form the decision-making basis of a dialogue agent; secondly, the agent is capable of revising its conversational strategy based on an external reward within the same framework. This is important because it provides a stepping stone towards building an autonomous dialogue agent that can continuously improve itself through interaction with users. The experimental results demonstrate the effectiveness of our latent intention model which achieves state-of-the-art performance on both automatic corpus-based evaluation and human evaluation."}, {"heading": "2. Latent Intention Dialogue Model for Goal-oriented Dialogue", "text": "Goal-oriented dialogue1 (Young et al., 2013) aims at building models that can help users to complete certain tasks via natural language interaction. Given a user input utterance ut at turn t and a knowledge base (KB), the model needs to\n1Like most of the goal-oriented dialogue research, we focus on information seek type dialogues.\nparse the input into actionable commands Q and access the KB to search for useful information in order to answer the query. Based on the search result, the model needs to summarise its findings and reply with an appropriate response mt in natural language."}, {"heading": "2.1. Model", "text": "The LIDM is based on the end-to-end system architecture described in (Wen et al., 2017). It comprises three components: (1) Representation Construction; (2) Policy Network; and (3) Generator, as shown in Figure 1. To capture the user\u2019s intent and match it against the system\u2019s knowledge, a dialogue state vector st = ut \u2295 bt \u2295 xt is derived from the user input ut and the knowledge base KB: ut is the distributed utterance representation, which is formed by encoding the user utterance2 ut with a bidirectional LSTM (Hochreiter & Schmidhuber, 1997) and concatenating the final stage hidden states together,\nut = biLSTM\u0398(ut). (1)\nThe belief vector bt, which is a concatenation of a set of probability distributions over domain specific slot-value pairs, is extracted by a set of pre-trained RNN-CNN belief trackers (Wen et al., 2017; Mrks\u030cic\u0301 et al., 2017), in which ut and mt\u22121 are processed by two different CNNs as shown in Figure 1,\nbt = RNN-CNN(ut,mt\u22121,bt\u22121) (2)\nwhere mt\u22121 is the preceding machine response and bt\u22121 is the preceding belief vector. They are included to model the current turn of the discourse and the long-term dialogue context, respectively. Based on the belief vector, a query Q is formed by taking the union of the maximum values of each slot. Q is then used to search the internal KB and return a vector xt representing the degree of matching in the KB. This is produced by counting all the matching venues and re-structuring it into a six-bin one-hot vector. Among the three vectors that comprise the dialogue state st, ut is completely trainable from data, bt is pre-trained using a separate objective function, and xt is produced by a discrete database accessing operation. For more details about the belief trackers and database operation refer to Wen et al (2016; 2017).\nConditioning on the state st, the policy network parameterises the latent intention zt by a single layer MLP,\n\u03c0\u0398(zt|st) = softmax(W\u1d402 \u00b7 tanh(W \u1d40 1st +b1) +b2) (3)\nwhere W1, b1, W2, b2 are model parameters. Since \u03c0\u0398(zt|st) is a discrete conditional probability distribution\n2All sentences are pre-processed by delexicalisation (Henderson et al., 2014) where slot-value specific words are replaced with their corresponding generic tokens based on an ontology.\nbased on dialogue state, we can also interpret the policy network here as a latent dialogue management component in the traditional POMDP-based framework (Young et al., 2013; Gas\u030cic\u0301 et al., 2013). A latent intention z(n)t (or an action in the reinforcement learning literature) can then be sampled from the conditional distribution,\nz (n) t \u223c \u03c0\u0398(zt|st). (4)\nThis sampled intention (or action) z(n)t and the state vector st can then be combined into a control vector dt, which is used to govern the generation of the system response based on a conditional LSTM language model,\ndt = W \u1d40 4zt \u2295 [sigmoid(W \u1d40 3zt + b3) \u00b7W \u1d40 5st] (5)\np\u0398(mt|st, zt) = \u220f j p(wtj+1|wtj ,htj\u22121,dt) (6)\nwhere b3 and W3\u223c5 are parameters, zt is the 1-hot representation of z(n)t , w t j is the last output token (i.e. a word, a delexicalised2 slot name or a delexicalised2 slot value), and htj\u22121 is the decoder\u2019s last hidden state. Note in Equation 5 the degree of information flow from the state vector is controlled by a sigmoid gate whose input signal is the sampled intention z(n)t . This prevents the decoder from overfitting to the deterministic state information and forces it to take the sampled stochastic intention into account. The LIDM can then be formally written down in its parame-\nterised form with parameter set \u0398, p\u0398(mt|st) = \u2211 zt p\u0398(mt|zt, st)\u03c0\u0398(zt|st). (7)"}, {"heading": "2.2. Inference", "text": "To carry out inference for the LIDM, we introduce an inference network q\u03a6(zt|st,mt) to approximate the posterior distribution p(zt|st,mt) so that we can optimise the variational lower bound of the joint probability in a neural variational inference framework (Miao et al., 2016). We can then derive the variational lower bound as,\nL = Eq\u03a6(zt)[log p\u0398(mt|zt, st)]\u2212 \u03bbDKL(q\u03a6(zt)||\u03c0\u0398(zt|st)) \u2264 log \u2211 zt p\u0398(mt|zt, st)\u03c0\u0398(zt|st)\n= log p\u0398(mt|st) (8)\nwhere q\u03a6(zt) is a shorthand for q\u03a6(zt|st,mt). Note that we use a modified version of the lower bound here by incorporating a trade-off factor \u03bb (Higgins et al., 2017). The inference network q\u03a6(zt|st,mt) is then constructed by\nq\u03a6(zt|st,mt) = Multi(ot) = softmax(W6ot) (9)\not = MLP\u03a6(bt,xt,ut,mt) (10)\nut = biLSTM\u03a6(ut),mt = biLSTM\u03a6(mt) (11)\nwhere ot is the joint representation, and both ut and mt are modeled by a bidirectional LSTM network. Although both\nq\u03a6(zt|st,mt) and \u03c0\u0398(zt|st) are modelled as parameterised multinomial distributions, the approximation q\u03a6(zt|st,mt) only functions during inference by producing samples to compute the stochastic gradients, while \u03c0\u0398(zt|st) is the generative distribution that generates the required samples for composing the machine response.\nBased on the samples z(n)t \u223c q\u03a6(zt|st,mt), we use different strategies to alternately optimise the parameters \u0398 and \u03a6 against the variational lower bound (Equation 8). To do this, we further divide \u0398 into two sets \u0398 = {\u03981,\u03982}. Parameters \u03981 on the decoder side are directly updated by back-propagating the gradients,\n\u2202L \u2202\u03981 = Eq\u03a6(zt|st,mt)[ \u2202 log p\u03981(mt|zt, st) \u2202\u03981 ]\n\u2248 1 N \u2211 n \u2202 log p\u03981(mt|z (n) t , st) \u2202\u03981 . (12)\nParameters \u03982 in the generative network, however, are updated by minimising the KL divergence,\n\u2202L \u2202\u03982 = \u2212\u2202\u03bbDKL(q\u03a6(zt|st,mt)||\u03c0\u03982(zt|st)) \u2202\u03982\n= \u2212\u03bb \u2211 zt q\u03a6(zt|st,mt) \u2202 log \u03c0\u03982(zt|st) \u2202\u03982 (13)\nwhere the entropy derivative \u2202H[q\u03a6(zt|st,mt)]/\u2202\u03982 = 0 and therefore can be ignored. Finally, for the parameters \u03a6 in the inference network, we firstly define the learning signal r(mt, z (n) t , st),\nr(mt,z (n) t , st) = log p\u03981(mt|z (n) t , st)\u2212\n\u03bb(log q\u03a6(z (n) t |st,mt)\u2212 log \u03c0\u03982(z (n) t |st)). (14)\nThen the parameters \u03a6 are updated by,\n\u2202L \u2202\u03a6 = Eq\u03a6(at|st,mt)[r(mt, at, st) \u2202 log q\u03a6(at|st,mt) \u2202\u03a6 ]\n\u2248 1 N \u2211 n r(mt, z (n) t , st) \u2202 log q\u03a6(z (n) t |st,mt) \u2202\u03a6 . (15)\nHowever, this gradient estimator has a large variance because the learning signal r(mt, z (n) t , st) relies on samples from the proposal distribution q\u03a6(zt|st,mt). To reduce the variance during inference, we follow the REINFORCE algorithm (Mnih et al., 2014; Mnih & Gregor, 2014) and introduce two baselines b and b(st), the centered learning signal and input dependent baseline respectively to help reduce the variance. b is a learnable constant and b(st) = MLP(st). During training, the two baselines are updated by minimising the distance,\nLb = [ r(mt, z (n) t , st)\u2212 b\u2212 b(st) ]2 (16)\nand the gradient w.r.t. \u03a6 can be rewritten as\n\u2202L \u2202\u03a6 \u2248 1 N \u2211 n [r(mt, z (n) t , st)\u2212b\u2212b(st)] \u2202 log q\u03a6(z (n) t |st,mt) \u2202\u03a6 .\n(17)"}, {"heading": "2.3. Semi-Supervision", "text": "Despite the steps described above for reducing the variance, there remain two major difficulties in learning latent intentions in a completely unsupervised manner: (1) the high variance of the inference network prevents it from generating sensible intention samples in the early stages of training, and (2) the overly strong discriminative power of the LSTM language model is prone to the disconnection phenomenon between the LSTM decoder and the rest of the components whereby the decoder learns to ignore the samples and focuses solely on optimising the language model. To ensure more stable training and prevent disconnection, a semi-supervised learning technique is introduced.\nInferring the latent intentions underlying utterances is similar to an unsupervised clustering task. Standard clustering algorithms can therefore be used to pre-process the corpus and generate automatic labels z\u0302t for part of the training examples (mt, st, z\u0302t) \u2208 L. Then when the model is trained on the unlabeled examples (mt, st) \u2208 U, we optimise it against the modified variational lower bound given in Equation 8\nL1 = \u2211\n(mt,st)\u2208U\nEq\u03c6(zt|st,mt) [log p\u03b8(mt|zt, st)]\n\u2212 \u03bbDKL(q\u03c6(zt|st,mt)||\u03c0\u03b8(zt|st)) (18)\nHowever, when the model is updated based on examples from the labeled set (mt, st, z\u0302t) \u2208 L, we treat the labeled intention z\u0302t as an observed variable and train the model by maximising the joint log-likelihood,\nL2 = \u2211\n(mt,z\u0302t,st)\u2208L\nlog [p\u0398(mt|z\u0302t, st)\u03c0\u0398(z\u0302t|st)q\u03a6(z\u0302t|st,mt)]\n(19) The final joint objective function can then be written as L\u2032 = \u03b1L1 + L2, where \u03b1 controls the trade-off between the supervised and unsupervised examples."}, {"heading": "2.4. Reinforcement Learning", "text": "One of the main purposes of learning interpretable, discrete latent intention inside a dialogue system is to be able to control and refine the model\u2019s behaviour with operational experience. The learnt generative network \u03c0\u0398(zt|st) encodes the policy discovered from the underlying data distribution but this is not necessarily optimal for any specific task. Since \u03c0\u0398(zt|st) is a parameterised policy network itself, any policy gradient-based reinforcement learning algorithm (Williams, 1992; Konda & Tsitsiklis, 2003) can be\nused to fine-tune the initial policy against other objective functions that we are more interested in.\nBased on the initial policy \u03c0\u0398(zt|st), we revisit the training dialogues and update parameters based on the following strategy: when encountering unlabeled examples U at turn t the system samples an action from the learnt policy z\n(n) t \u223c \u03c0\u0398(zt|st) and receives a reward r (n) t . Conditioning on these, we can directly fine-tune a subset of the model parameters \u0398\u2032 by the policy gradient method,\n\u2202J \u2202\u0398\u2032 \u2248 1 N \u2211 n r (n) t \u2202 log \u03c0\u0398(z (n) t |st) \u2202\u0398\u2032 (20)\nwhere \u0398\u2032 = {W1,b1,W2,b2} is the MLP that parameterises the policy network (Equation 3). However, when a labeled example \u2208 L is encountered we force the model to take the labeled action z(n)t = z\u0302t and update the parameters by Equation 20 as well. Unlike Li et al (2016c) where the whole model is refined end-to-end using RL, updating only \u0398\u2032 effectively allows us to refine only the decision-making of the system and avoid the problem of over-fitting."}, {"heading": "3. Experiments", "text": ""}, {"heading": "3.1. Dataset & Setup for Goal-oriented Dialogue", "text": "We explored the properties of the LIDM model3 using the CamRest676 corpus4 collected by Wen et al (2017), in which the task of the system is to assist users to find a restaurant in the Cambridge, UK area. The corpus was collected based on a modified Wizard of Oz (Kelley, 1984) online data collection. Workers were recruited on Amazon Mechanical Turk and asked to complete a task by carrying out a conversation, alternating roles between a user and a wizard. There are three informable slots (food, pricerange, area) that users can use to constrain the search and six requestable slots (address, phone, postcode plus the three informable slots) that the user can ask a value for once a restaurant has been offered. There are 676 dialogues in the dataset (including both finished and unfinished dialogues) and approximately 2750 conversational turns in total. The database contains 99 unique restaurants.\nTo make a direct comparison with prior work we follow the same experimental setup as in Wen et al (2016; 2017). The corpus was partitioned into training, validation, and test sets in the ratio 3:1:1. The LSTM hidden layer sizes were set to 50, and the vocabulary size is around 500 after pre-processing, to remove rare words and words that can be delexicalised2. All the system components were trained jointly by fixing the pre-trained belief trackers and the discrete database operator with the model\u2019s latent intention\n3Will be available at https://github.com/shawnwun/NNDIAL 4https://www.repository.cam.ac.uk/handle/1810/260970\nsize I set to 50, 70, and 100, respectively. The trade-off constants \u03bb and \u03b1 were both set to 0.1. To produce selflabeled response clusters for semi-supervised learning of the intentions, we firstly removed function words from all the responses and clustered them according to their content words. We then assigned the responses in the i-th frequent cluster to the i-th latent dimension as its supervised set. This results in about 35% (I = 50) to 43% (I = 100) labeled responses across the whole dataset. An example of the resulting seed set is shown in Table 1. During inference we carried out stochastic estimation by taking one sample for estimating the stochastic gradients. The model is trained by Adam (Kingma & Ba, 2014) and tuned (early stopping, hyper-parameters) on the held-out validation set. We alternately optimised the generative model and the inference network by fixing the parameters of one while updating the parameters of the other.\nDuring reinforcement fine-tuning, we generated a sentence mt from the model to replace the ground truth m\u0302t at each turn and define an immediate reward as whether mt can improve the dialogue success (Su et al., 2015) relative to m\u0302t, plus the sentence BLEU score (Auli & Gao, 2014),\nrt = \u03b7 \u00b7 sBLEU(mt, m\u0302t) +  1 mt improves \u22121 mt degrades 0 otherwise\n(21)\nwhere the constant \u03b7 was set to 0.5. We fine-tuned the model parameters using RL for only 3 epochs. During testing, we greedily selected the most probable intention and applied beam search with the beamwidth set to 10 when decoding the response. The decoding criterion was the average log-probability of tokens in the response. We then evaluated our model on task success rate (Su et al., 2015) and BLEU score (Papineni et al., 2002) as in Wen et al (2016; 2017) in which the model is used to predict each system response in the held-out test set."}, {"heading": "3.2. Experiments on Goal-oriented Dialogue", "text": "Table 2 presents the results of the corpus-based evaluation. The Ground Truth block shows the two metrics when we compute them on the human-authored responses. This sets a gold standard for the task. In the Published Models block, the results for the three baseline models were borrowed from Wen et al (2016), they are: (1) the vanilla neural dialogue model (NDM), (2) NDM plus an attention mechanism on the belief trackers, and (3) the attentive NDM with self-supervised sub-task neurons. The results of the LIDM model with and without RL fine-tuning are shown in the LIDM Models and the LIDM Models + RL blocks, respectively. As can be seen, the initial policy learned by fitting the latent intention to the underlying data distribution yielded reasonably good results on BLEU but did not perform well on task success when compared to their deterministic counterparts (block 2 v.s. 3). This may be due to the fact that the variational lower bound of the dataset was optimised rather than task success during variational inference. However, once RL was applied to optimise the success rate as part of the reward function (Equation 21) during the fine-tuning phase, the resulting LIDM+RL models outperformed the three baselines in terms of task success without significantly sacrificing BLEU (block 2 v.s. 4)5.\nIn order to assess the human perceived performance, we evaluated the three models (1) NDM, (2) LIDM, and (3) LIDM+RL by recruiting paid subjects on Amazon Mechanical Turk. Each judge was asked to follow a task and carried out a conversation with the machine. At the end of each conversation the judges were asked to rate and compare the model\u2019s performance. We assessed the subjective\n5Note that both NDM+Att+SS and LIDM use self-supervised information\nsuccess rate, the perceived comprehension ability and the naturalness of responses on a scale of 1 to 5. For each model, we collected 200 dialogues and averaged the scores. During human evaluation, we sampled from the top-5 intentions of the LIDM models and decoded a response based on the sample. The result is shown in Table 3. One interesting fact to note is that although the LIDM did not perform well on the corpus-based task success metric, the human judges rated its subjective success almost indistinguishably from the others. This discrepancy between the two experiments arises mainly from a flaw in the corpus-based success metric in that it favors greedy policies because the user side behaviours are fixed rather than interactional6. Despite the fact that LIDMs are considered only marginally better than NDM on subjective success, the LIDMs do outperform NDM on both comprehension and naturalness scores. This is because the proposed LIDM models can better capture multiple modes in the communicative intention and thereby respond more naturally by sampling from the latent intention variable.\nThree example conversations are shown between a human judge and a machine, one from LIDM in Table 4 and two from LIDM+RL in Table 5, respectively. The results are displayed one exchange per block. Each induced latent intention is shown by a tuple (index, probability) followed by a decoded response, and the sample dialogues were produced by following the responses highlighted in bold. As can be seen, the LIDM shown in Table 4 clearly has multiple modes in the distribution over the learned intention latent variable, and what it represents can be easily interpreted by the response generated. However, some intentions (such as intent 0) can result in very different responses under different dialogue states even though they were supervised by a small response set as shown in Table 1. This is mainly because of the variance introduced during variational inference. Finally, when comparing Table 4 and Table 5, we can observe the difference between the two dialogue strategies: the LIDM, by inferring its policy from the supervised dataset, reflects the diverse set of modes in the underlying distribution; whereas the LIDM+RL, which\n6The system tries to provide as much information as possible in the early turns, in case the fixed user side behaviours a few turns later do not fit the scenario the system originally planned.\nrefined its strategy using RL, exhibits a much greedier behavior in achieving task success (e.g. in Table 5 in block 2 & 4 the LIDM+RL agent provides the address and phone number even before the user asks). This is also supported by the human evaluation in Table 3 where LIDM+RL has much shorter dialogues on average compared to the other two models."}, {"heading": "4. Discussion", "text": "Learning an end-to-end dialogue system is appealing but challenging because of the credit assignment problem. Discrete latent variable dialogue models such as LIDM are attractive because the latent variable can serve as an interface for decomposing the learning of language and the internal dialogue decision-making. This decomposition can effectively help us resolve the credit assignment problem where different learning signals can be applied to different sub-modules to update the parameters. In variational inference for discrete latent variables, the latent distribution is basically updated by the reward from the variational lower bound. While in reinforcement learning, the latent distribution (i.e. policy network) is updated by the rewards from dialogue success and sentence BLEU score. Hence, the latent variable bridges the different learning paradigms such as Bayesian learning and reinforcement learning and brings them together under the same framework. This framework provides a more robust neural network-based approach than previous approaches because it does not depend solely on sequence-to-sequence learning but instead\nexplicitly models the hidden dialogue intentions underlying the user\u2019s utterances and allows the agent to directly learn a dialogue policy through interaction."}, {"heading": "5. Related work", "text": "Modeling chat-based dialogues (Serban et al., 2015; Shang et al., 2015) as a sequence-to-sequence learning (Sutskever et al., 2014) problem is a common theme in the deep learning community. Vinyals and Le (2015) has demonstrated a seq2seq-based model trained on a huge amount of conversation corpora which learns interesting replies conditioned on different user queries. However, due to an inability to model dialogue context, these models generally suffer from the generic response problem (Li et al., 2016a; Serban et al., 2016). Several approaches have been proposed to mitigate this issue, such as modeling the persona (Li et al., 2016b), reinforcement learning (Li et al., 2016c), and introducing continuous latent variables (Serban et al., 2016; Cao & Clark, 2017). While in our case, we not only make use of the latent variable to inject stochasticity for generating natural and diverse machine responses but also model the hidden dialogue intentions explicitly. This combines the merits of reinforcement learning and generative models.\nAt the other end of the spectrum, goal-oriented dialogue systems typically adopt the POMDP framework (Young et al., 2013) and break down the development of the dialogue systems into a pipeline of modules: natural language understanding (Henderson, 2015), dialogue manage-\nment (Gas\u030cic\u0301 et al., 2013), and natural language generation (Wen et al., 2015). These system modules communicate through a dialogue act formalism (Traum, 1999), which in effect constitute a fixed set of handcrafted intentions. This limits the ability of such systems to scale to more complex tasks. In contrast, the LIDM directly infers all underlying dialogue intentions from data and can handle intention distributions with long tails by measuring similarities against the existing ones during variational inference. Modeling of end-to-end goal-oriented dialogue systems has also been studied recently (Wen et al., 2016; 2017; Bordes & Weston, 2017), however, these models are typically deterministic and rely on decoder supervision signals to finetune a large set of model parameters.\nMuch research has focused on combining different learning paradigms and signals to bootstrap performance. For example, semi-supervised learning (Kingma et al., 2014) has been applied in the sample-based neural variational inference framework as a way to reduce sample variance. In practice, this relies on a discrete latent variable (Miao & Blunsom, 2016; Koc\u030cisky\u0301 et al., 2016) as the vehicle for the supervision labels. As in reinforcement learning, which has been a very common learning paradigm in dialogue systems (Gas\u030cic\u0301 et al., 2013; Su et al., 2016; Li et al., 2017), the policy is also parameterised by a discrete set of actions. As a consequence, the LIDM, which parameterises the intention space via a discrete latent variable, can automatically enjoy the benefit of bootstrapping from signals coming from different learning paradigms. In addition, selfsupervised learning (Snow et al., 2004) (or distant, weak\nsupervision) as a simple way to generate automatic labels by heuristics is popular in many NLP tasks and has been applied to memory networks (Hill et al., 2016) and neural dialogue systems (Wen et al., 2016) recently. Since there is no additional effort required in labeling, it can also be viewed as a method for bootstrapping."}, {"heading": "6. Conclusion", "text": "In this paper, we have proposed a framework for learning dialogue intentions via discrete latent variable models and introduced the Latent Intention Dialogue Model (LIDM) for goal-oriented dialogue modeling. We have shown that the LIDM can discover an effective initial policy from the underlying data distribution and is capable of revising its strategy based on an external reward using reinforcement learning. We believe this is a promising step forward for building autonomous dialogue agents since the learnt discrete latent variable interface enables the agent to perform learning using several differing paradigms. The experiments showed that the proposed LIDM is able to communicate with human subjects and outperforms previous published results."}, {"heading": "Acknowledgements", "text": "Tsung-Hsien Wen is supported by Toshiba Research Europe Ltd, Cambridge Research Laboratory. The authors would like to thank the members of the Cambridge Dialogue Systems Group for their valuable comments."}], "references": [{"title": "Decoder integration and expected bleu training for recurrent neural network language models", "author": ["Auli", "Michael", "Gao", "Jianfeng"], "venue": "In ACL. Association for Computational Linguistics,", "citeRegEx": "Auli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Auli et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Learning end-to-end goal-oriented dialog", "author": ["Bordes", "Antoine", "Weston", "Jason"], "venue": "In ICLR,", "citeRegEx": "Bordes et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2017}, {"title": "Generating sentences from a continuous space", "author": ["Bowman", "Samuel R", "Vilnis", "Luke", "Vinyals", "Oriol", "Dai", "Andrew M", "J\u00f3zefowicz", "Rafal", "Bengio", "Samy"], "venue": "arXiv preprint:,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Latent variable dialogue models and their diversity", "author": ["Cao", "Kris", "Clark", "Stephen"], "venue": "In EACL,", "citeRegEx": "Cao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2017}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Faruqui", "Manaal", "Dodge", "Jesse", "Jauhar", "Sujay Kumar", "Dyer", "Chris", "Hovy", "Eduard", "Smith", "Noah A"], "venue": "In NAACL-HLT,", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human interaction", "author": ["Ga\u0161i\u0107", "Milica", "Breslin", "Catherine", "Henderson", "Matthew", "Kim", "Dongho", "Szummer", "Martin", "Thomson", "Blaise", "Tsiakoulis", "Pirros", "Young", "Steve"], "venue": "In ICASSP,", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "Machine learning for dialog state tracking: A review", "author": ["Henderson", "Matthew"], "venue": "In Machine Learning in Spoken Language Processing Workshop,", "citeRegEx": "Henderson and Matthew.,? \\Q2015\\E", "shortCiteRegEx": "Henderson and Matthew.", "year": 2015}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Henderson", "Matthew", "Thomson", "Blaise", "Young", "Steve"], "venue": "In SigDial,", "citeRegEx": "Henderson et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "beta-vae: Learning basic visual concepts with a constrained variational framework", "author": ["Higgins", "Irina", "Matthey", "Loic", "Pal", "Arka", "Burgess", "Christopher", "Glorot", "Xavier", "Botvinick", "Matthew", "Mohamed", "Shakir", "Lerchner", "Alexander"], "venue": "In ICLR,", "citeRegEx": "Higgins et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2017}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Hill", "Felix", "Bordes", "Antoine", "Chopra", "Sumit", "Weston", "Jason"], "venue": "In ICLR,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long shortterm memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Andrej", "Fei-Fei", "Li"], "venue": "In CVPR,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "An iterative design methodology for userfriendly natural language office information applications", "author": ["Kelley", "John F"], "venue": "ACM Transaction on Information Systems,", "citeRegEx": "Kelley and F.,? \\Q1984\\E", "shortCiteRegEx": "Kelley and F.", "year": 1984}, {"title": "Globally coherent text generation with neural checklist models", "author": ["Kiddon", "Chlo\u00e9", "Zettlemoyer", "Luke", "Choi", "Yejin"], "venue": "In EMNLP,", "citeRegEx": "Kiddon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiddon et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "arXiv preprint: 1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICML,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Kingma", "Diederik P", "Mohamed", "Shakir", "Rezende", "Danilo J", "Welling", "Max"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "On actorcritic algorithms", "author": ["Konda", "Vijay R", "Tsitsiklis", "John N"], "venue": "SIAM J. Control Optim.,", "citeRegEx": "Konda et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2003}, {"title": "Semantic parsing with semi-supervised sequential autoencoders", "author": ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", "Melis", "G\u00e1bor", "Grefenstette", "Edward", "Dyer", "Chris", "Ling", "Wang", "Blunsom", "Phil", "Hermann", "Karl Moritz"], "venue": "In EMNLP,", "citeRegEx": "Ko\u010disk\u00fd et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ko\u010disk\u00fd et al\\.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Li", "Jiwei", "Galley", "Michel", "Brockett", "Chris", "Gao", "Jianfeng", "Dolan", "Bill"], "venue": "In NAACL-HLT,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A personabased neural conversation model", "author": ["Li", "Jiwei", "Galley", "Michel", "Brockett", "Chris", "Spithourakis", "Georgios", "Gao", "Jianfeng", "Dolan", "Bill"], "venue": "In ACL,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Ritter", "Alan", "Jurafsky", "Dan", "Galley", "Michel", "Gao", "Jianfeng"], "venue": "In EMNLP,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Dialogue learning with human-in-the-loop", "author": ["Li", "Jiwei", "Miller", "Alexander H", "Chopra", "Sumit", "Ranzato", "Marc\u2019Aurelio", "Weston", "Jason"], "venue": "In ICLR,", "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Language as a latent variable: Discrete generative models for sentence compression", "author": ["Miao", "Yishu", "Blunsom", "Phil"], "venue": "In EMNLP,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Neural variational inference for text processing", "author": ["Miao", "Yishu", "Yu", "Lei", "Blunsom", "Phil"], "venue": "In ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex", "kavukcuoglu", "koray"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Neural belief tracker: Data-driven dialogue state tracking", "author": ["Mrk\u0161i\u0107", "Nikola", "\u00d3 S\u00e9aghdha", "Diarmuid", "Wen", "TsungHsien", "Thomson", "Blaise", "Young", "Steve"], "venue": "In ACL,", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2017}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A hierarchical latent variable encoderdecoder model for generating dialogues", "author": ["Serban", "Iulian V", "Sordoni", "Alessandro", "Lowe", "Ryan", "Charlin", "Laurent", "Pineau", "Joelle", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint:", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Serban", "Iulian Vlad", "Sordoni", "Alessandro", "Bengio", "Yoshua", "Courville", "Aaron C", "Pineau", "Joelle"], "venue": "In AAAI,", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang", "Lifeng", "Lu", "Zhengdong", "Li", "Hang"], "venue": "In ACL,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Snow", "Rion", "Jurafsky", "Daniel", "Ng", "Andrew Y"], "venue": "In NIPS,", "citeRegEx": "Snow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems", "author": ["Su", "Pei-Hao", "Vandyke", "David", "Gasic", "Milica", "Kim", "Dongho", "Mrksic", "Nikola", "Wen", "Tsung-Hsien", "Young", "Steve"], "venue": "In Interspeech,", "citeRegEx": "Su et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Su et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Foundations of Rational Agency, chapter Speech Acts for Dialogue Agents", "author": ["Traum", "David R"], "venue": null, "citeRegEx": "Traum and R.,? \\Q1999\\E", "shortCiteRegEx": "Traum and R.", "year": 1999}, {"title": "A neural conversational model", "author": ["Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In ICML Deep Learning Workshop,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Wen", "Tsung-Hsien", "Gasic", "Milica", "Mrk\u0161i\u0107", "Nikola", "Su", "Pei-Hao", "Vandyke", "David", "Young", "Steve"], "venue": "In EMNLP,", "citeRegEx": "Wen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based endto-end trainable task-oriented dialogue system", "author": ["Wen", "Tsung-Hsien", "Vandyke", "David", "Mrk\u0161i\u0107", "Nikola", "Ga\u0161i\u0107", "Milica", "M. Rojas-Barahona", "Lina", "Su", "Pei-Hao", "Ultes", "Stefan", "Young", "Steve"], "venue": null, "citeRegEx": "Wen et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2017}, {"title": "Dialog-based language learning", "author": ["Weston", "Jason E"], "venue": "In NIPS,", "citeRegEx": "Weston and E.,? \\Q2016\\E", "shortCiteRegEx": "Weston and E.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine Learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron", "Salakhutdinov", "Ruslan", "Zemel", "Richard", "Bengio", "Yoshua"], "venue": "In ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "author": ["Young", "Steve", "Ga\u0161i\u0107", "Milica", "Thomson", "Blaise", "Williams", "Jason D"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Young et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Young et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 35, "context": "Recurrent neural networks (RNNs) have shown impressive results in modeling generation tasks that have a sequential structured output form, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy & Fei-Fei, 2015; Xu et al.", "startOffset": 167, "endOffset": 214}, {"referenceID": 1, "context": "Recurrent neural networks (RNNs) have shown impressive results in modeling generation tasks that have a sequential structured output form, such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy & Fei-Fei, 2015; Xu et al.", "startOffset": 167, "endOffset": 214}, {"referenceID": 42, "context": ", 2015), caption generation (Karpathy & Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 38, "context": ", 2015), and natural language generation (Wen et al., 2015; Kiddon et al., 2016).", "startOffset": 41, "endOffset": 80}, {"referenceID": 14, "context": ", 2015), and natural language generation (Wen et al., 2015; Kiddon et al., 2016).", "startOffset": 41, "endOffset": 80}, {"referenceID": 39, "context": "For example both goal-oriented dialogue systems (Wen et al., 2017; Bordes & Weston, 2017) and sequence-to-sequence learning chatbots (Vinyals & Le, 2015; Shang et al.", "startOffset": 48, "endOffset": 89}, {"referenceID": 32, "context": ", 2017; Bordes & Weston, 2017) and sequence-to-sequence learning chatbots (Vinyals & Le, 2015; Shang et al., 2015; Serban et al., 2015) struggle to generate diverse yet causal responses (Li et al.", "startOffset": 74, "endOffset": 135}, {"referenceID": 31, "context": ", 2017; Bordes & Weston, 2017) and sequence-to-sequence learning chatbots (Vinyals & Le, 2015; Shang et al., 2015; Serban et al., 2015) struggle to generate diverse yet causal responses (Li et al.", "startOffset": 74, "endOffset": 135}, {"referenceID": 30, "context": ", 2015) struggle to generate diverse yet causal responses (Li et al., 2016a; Serban et al., 2016).", "startOffset": 58, "endOffset": 97}, {"referenceID": 3, "context": "Recent advances in neural variational inference (Kingma & Welling, 2014; Mnih & Gregor, 2014) have sparked a series of latent variable models applied to NLP (Bowman et al., 2015; Serban et al., 2016; Miao et al., 2016; Cao & Clark, 2017).", "startOffset": 157, "endOffset": 237}, {"referenceID": 30, "context": "Recent advances in neural variational inference (Kingma & Welling, 2014; Mnih & Gregor, 2014) have sparked a series of latent variable models applied to NLP (Bowman et al., 2015; Serban et al., 2016; Miao et al., 2016; Cao & Clark, 2017).", "startOffset": 157, "endOffset": 237}, {"referenceID": 24, "context": "Recent advances in neural variational inference (Kingma & Welling, 2014; Mnih & Gregor, 2014) have sparked a series of latent variable models applied to NLP (Bowman et al., 2015; Serban et al., 2016; Miao et al., 2016; Cao & Clark, 2017).", "startOffset": 157, "endOffset": 237}, {"referenceID": 15, "context": "In contrast, models with discrete latent variables are able to not only produce interpretable latent distributions but also provide a principled framework for semi-supervised learning (Kingma et al., 2014).", "startOffset": 184, "endOffset": 205}, {"referenceID": 5, "context": "This is critical for NLP tasks, especially where additional supervision and external knowledge can be utilized for bootstrapping (Faruqui et al., 2015; Miao & Blunsom, 2016; Ko\u010disk\u00fd et al., 2016).", "startOffset": 129, "endOffset": 195}, {"referenceID": 19, "context": "This is critical for NLP tasks, especially where additional supervision and external knowledge can be utilized for bootstrapping (Faruqui et al., 2015; Miao & Blunsom, 2016; Ko\u010disk\u00fd et al., 2016).", "startOffset": 129, "endOffset": 195}, {"referenceID": 43, "context": "Goal-oriented dialogue1 (Young et al., 2013) aims at building models that can help users to complete certain tasks via natural language interaction.", "startOffset": 24, "endOffset": 44}, {"referenceID": 39, "context": "The LIDM is based on the end-to-end system architecture described in (Wen et al., 2017).", "startOffset": 69, "endOffset": 87}, {"referenceID": 39, "context": "The belief vector bt, which is a concatenation of a set of probability distributions over domain specific slot-value pairs, is extracted by a set of pre-trained RNN-CNN belief trackers (Wen et al., 2017; Mrk\u0161i\u0107 et al., 2017), in which ut and mt\u22121 are processed by two different CNNs as shown in Figure 1,", "startOffset": 185, "endOffset": 224}, {"referenceID": 28, "context": "The belief vector bt, which is a concatenation of a set of probability distributions over domain specific slot-value pairs, is extracted by a set of pre-trained RNN-CNN belief trackers (Wen et al., 2017; Mrk\u0161i\u0107 et al., 2017), in which ut and mt\u22121 are processed by two different CNNs as shown in Figure 1,", "startOffset": 185, "endOffset": 224}, {"referenceID": 8, "context": "All sentences are pre-processed by delexicalisation (Henderson et al., 2014) where slot-value specific words are replaced with their corresponding generic tokens based on an ontology.", "startOffset": 52, "endOffset": 76}, {"referenceID": 43, "context": "based on dialogue state, we can also interpret the policy network here as a latent dialogue management component in the traditional POMDP-based framework (Young et al., 2013; Ga\u0161i\u0107 et al., 2013).", "startOffset": 154, "endOffset": 194}, {"referenceID": 6, "context": "based on dialogue state, we can also interpret the policy network here as a latent dialogue management component in the traditional POMDP-based framework (Young et al., 2013; Ga\u0161i\u0107 et al., 2013).", "startOffset": 154, "endOffset": 194}, {"referenceID": 24, "context": "To carry out inference for the LIDM, we introduce an inference network q\u03a6(zt|st,mt) to approximate the posterior distribution p(zt|st,mt) so that we can optimise the variational lower bound of the joint probability in a neural variational inference framework (Miao et al., 2016).", "startOffset": 259, "endOffset": 278}, {"referenceID": 9, "context": "Note that we use a modified version of the lower bound here by incorporating a trade-off factor \u03bb (Higgins et al., 2017).", "startOffset": 98, "endOffset": 120}, {"referenceID": 26, "context": "To reduce the variance during inference, we follow the REINFORCE algorithm (Mnih et al., 2014; Mnih & Gregor, 2014) and introduce two baselines b and b(st), the centered learning signal and input dependent baseline respectively to help reduce the variance.", "startOffset": 75, "endOffset": 115}, {"referenceID": 34, "context": "During reinforcement fine-tuning, we generated a sentence mt from the model to replace the ground truth m\u0302t at each turn and define an immediate reward as whether mt can improve the dialogue success (Su et al., 2015) relative to m\u0302t, plus the sentence BLEU score (Auli & Gao, 2014),", "startOffset": 199, "endOffset": 216}, {"referenceID": 34, "context": "We then evaluated our model on task success rate (Su et al., 2015) and BLEU score (Papineni et al.", "startOffset": 49, "endOffset": 66}, {"referenceID": 29, "context": ", 2015) and BLEU score (Papineni et al., 2002) as in Wen et al (2016; 2017) in which the model is used to predict each system response in the held-out test set.", "startOffset": 23, "endOffset": 46}, {"referenceID": 31, "context": "Modeling chat-based dialogues (Serban et al., 2015; Shang et al., 2015) as a sequence-to-sequence learning (Sutskever et al.", "startOffset": 30, "endOffset": 71}, {"referenceID": 32, "context": "Modeling chat-based dialogues (Serban et al., 2015; Shang et al., 2015) as a sequence-to-sequence learning (Sutskever et al.", "startOffset": 30, "endOffset": 71}, {"referenceID": 35, "context": ", 2015) as a sequence-to-sequence learning (Sutskever et al., 2014) problem is a common theme in the deep learning community.", "startOffset": 43, "endOffset": 67}, {"referenceID": 30, "context": "However, due to an inability to model dialogue context, these models generally suffer from the generic response problem (Li et al., 2016a; Serban et al., 2016).", "startOffset": 120, "endOffset": 159}, {"referenceID": 30, "context": ", 2016c), and introducing continuous latent variables (Serban et al., 2016; Cao & Clark, 2017).", "startOffset": 54, "endOffset": 94}, {"referenceID": 26, "context": "Modeling chat-based dialogues (Serban et al., 2015; Shang et al., 2015) as a sequence-to-sequence learning (Sutskever et al., 2014) problem is a common theme in the deep learning community. Vinyals and Le (2015) has demonstrated a seq2seq-based model trained on a huge amount of conversation corpora which learns interesting replies conditioned on different user queries.", "startOffset": 31, "endOffset": 212}, {"referenceID": 43, "context": "At the other end of the spectrum, goal-oriented dialogue systems typically adopt the POMDP framework (Young et al., 2013) and break down the development of the dialogue systems into a pipeline of modules: natural language understanding (Henderson, 2015), dialogue manage-", "startOffset": 101, "endOffset": 121}, {"referenceID": 6, "context": "ment (Ga\u0161i\u0107 et al., 2013), and natural language generation (Wen et al.", "startOffset": 5, "endOffset": 25}, {"referenceID": 38, "context": ", 2013), and natural language generation (Wen et al., 2015).", "startOffset": 41, "endOffset": 59}, {"referenceID": 15, "context": "For example, semi-supervised learning (Kingma et al., 2014) has been applied in the sample-based neural variational inference framework as a way to reduce sample variance.", "startOffset": 38, "endOffset": 59}, {"referenceID": 19, "context": "In practice, this relies on a discrete latent variable (Miao & Blunsom, 2016; Ko\u010disk\u00fd et al., 2016) as the vehicle for the supervision labels.", "startOffset": 55, "endOffset": 99}, {"referenceID": 6, "context": "As in reinforcement learning, which has been a very common learning paradigm in dialogue systems (Ga\u0161i\u0107 et al., 2013; Su et al., 2016; Li et al., 2017), the policy is also parameterised by a discrete set of actions.", "startOffset": 97, "endOffset": 151}, {"referenceID": 23, "context": "As in reinforcement learning, which has been a very common learning paradigm in dialogue systems (Ga\u0161i\u0107 et al., 2013; Su et al., 2016; Li et al., 2017), the policy is also parameterised by a discrete set of actions.", "startOffset": 97, "endOffset": 151}, {"referenceID": 33, "context": "In addition, selfsupervised learning (Snow et al., 2004) (or distant, weak supervision) as a simple way to generate automatic labels by heuristics is popular in many NLP tasks and has been applied to memory networks (Hill et al.", "startOffset": 37, "endOffset": 56}, {"referenceID": 10, "context": ", 2004) (or distant, weak supervision) as a simple way to generate automatic labels by heuristics is popular in many NLP tasks and has been applied to memory networks (Hill et al., 2016) and neural dialogue systems (Wen et al.", "startOffset": 167, "endOffset": 186}], "year": 2017, "abstractText": "Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. Traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture natural conversational variability. In this paper, we propose a Latent Intention Dialogue Model (LIDM) that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. In a goal-oriented dialogue scenario, these latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experimental evaluation of LIDM shows that the model out-performs published benchmarks for both corpus-based and human evaluation, demonstrating the effectiveness of discrete latent variable models for learning goal-oriented dialogues.", "creator": "LaTeX with hyperref package"}}}