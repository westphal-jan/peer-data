{"id": "1705.10461", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "The Numerics of GANs", "abstract": "In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.", "histories": [["v1", "Tue, 30 May 2017 05:54:59 GMT  (6041kb,D)", "http://arxiv.org/abs/1705.10461v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lars mescheder", "sebastian nowozin", "reas geiger"], "accepted": true, "id": "1705.10461"}, "pdf": {"name": "1705.10461.pdf", "metadata": {"source": "CRF", "title": "The Numerics of GANs", "authors": ["Lars Mescheder"], "emails": ["lmescheder@tuebingen.mpg.de", "sebastian.nowozin@microsoft.com", "andreas.geiger@tuebingen.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "Generative Adversarial Networks (GANs) [8] have been very successful in learning probability distributions. Since their first appearance, GANs have been successfully applied to a variety of tasks, including image-to-image translation [10], image super-resolution [11], image in-painting [23] domain adaptation [22], probabilistic inference [12, 7, 6] and many more.\nWhile very powerful, GANs are known to be notoriously hard to train. The standard strategy for stabilizing training is to carefully design the model, either by adapting the architecture [17] or by selecting an easy-to-optimize objective function [20, 2, 9].\nIn this work, we examine the general problem of finding local Nash-equilibria of smooth games. We revisit the de-facto standard algorithm for finding such equilibrium points, simultaneous gradient ascent. We theoretically show that the main factors preventing the algorithm from converging are the presence of eigenvalues of the Jacobian of the associated gradient vector field with zero real-part and eigenvalues with a large imaginary part. The presence of the latter is also one of the reasons that makes saddle-point problems more difficult than local optimization problems. Utilizing these insights, we design a new algorithm that overcomes some of these problems. Experimentally, we show that our algorithm leads to stable training on many GAN architectures, including some that are known to be hard to train.\nOur technique is orthogonal to strategies that try to make the GAN-game well-defined, e.g. by adding instance noise [21] or by using the Wasserstein-divergence [2, 9]: while these strategies try to ensure the existence of Nash-equilibria, our paper deals with their computation and the numerical difficulties that can arise in practice.\nar X\niv :1\n70 5.\n10 46\n1v 1\n[ cs\n.L G\n] 3\nIn summary, our contributions are as follows:\n\u2022 We identify the main reasons why simultaneous gradient ascent often fails to find local Nash-equilibria. \u2022 By utilizing these insights, we design a new, more robust algorithm for finding Nashequilibria of smooth two-player games. \u2022 We empirically demonstrate that our method enables stable training of GANs on a variety of architectures and divergence measures.\nThe proofs for the theorems in this paper can be found the supplementary material."}, {"heading": "2 Background", "text": "In this section we first revisit the concept of Generative Adversarial Networks (GANs) from a divergence minimization point of view. We then introduce the concept of a smooth (non-convex) two-player game and define the terminology used in the rest of the paper. Finally, we describe simultaneous gradient ascent, the de-facto standard algorithm for finding Nash-equilibria of such games, and derive some of its properties."}, {"heading": "2.1 Divergence Measures and GANs", "text": "Generative Adversarial Networks are best understood in the context of divergence minimization: assume we are given a divergence function D, i.e. a function that takes a pair of probability distributions as input, outputs an element from [0,\u221e] and satisfies D(p, p) = 0 for all probability distributions p. Moreover, assume we are given some target distribution p0 from which we can draw i.i.d. samples and a parametric family of distribution q\u03b8 that also allows us to draw i.i.d. samples. In practice q\u03b8 is usually implemented as a neural network that acts on a hidden code z sampled from some known distribution and outputs an element from the target space. Our goal is to find \u03b8\u0304 that minimizes the divergence D(p0, q\u03b8), i.e. we want to solve the optimization problem\nmin \u03b8 D(p0, q\u03b8). (1)\nMost divergences that are used in practice can be represented in the following form [8, 14, 2]:\nD(p, q) = max f\u2208F\nEx\u223cq [g1(f(x))]\u2212 Ex\u223cp [g2(f(x))] (2)\nfor some function class F \u2286 X \u2192 R and convex functions g1, g2 : R\u2192 R. Together with (1), this leads to mini-max problems of the form\nmin \u03b8 max f\u2208F Ex\u223cq\u03b8 [g1(f(x))]\u2212 Ex\u223cp0 [g2(f(x))] . (3)\nThese divergences include the Jensen-Shannon divergence [8], all f-divergences [14], the Wasserstein divergence [2] and even the indicator divergence, which is 0 if p = q and\u221e otherwise. In practice, the function class F in (3) is approximated with a parametric family of functions, e.g. parameterized by a neural network. Of course, when minimizing the divergence w.r.t. this approximated family, we no longer minimize the correct divergence. However, it can be verified that taking any class of functions in (3) leads to a divergence function for appropriate choices of g1 and g2. Therefore, some authors call these divergence functions neural network divergences [3]."}, {"heading": "2.2 Smooth Two-Player Games", "text": "A differentiable two-player game is defined by two utility functions f(\u03c6, \u03b8) and g(\u03c6, \u03b8) defined over a common space (\u03c6, \u03b8) \u2208 \u21261\u00d7\u21262. \u21261 corresponds to the possible actions of player 1, \u21262 corresponds to the possible actions of player 2. The goal of player 1 is to maximize f , whereas player 2 tries to maximize g. In the context of GANs, \u21261 is the set of possible parameter values for the generator, whereas \u21262 is the set of possible parameter values for the discriminator. We call a game a zero-sum game if f = \u2212g. Note that the derivation of the GAN-game in Section 2.1 leads to a zero-sum game, whereas in practice people usually employ a variant of this formulation that is not a zero-sum game for better convergence [8].\nAlgorithm 1 Simultaneous Gradient Ascent (SimGA) 1: while not converged do 2: v\u03c6 \u2190 \u2207\u03c6f(\u03b8, \u03c6) 3: v\u03b8 \u2190 \u2207\u03b8g(\u03b8, \u03c6) 4: \u03c6\u2190 \u03c6+ hv\u03c6 5: \u03b8 \u2190 \u03b8 + hv\u03b8 6: end while\nOur goal is to find a Nash-equilibrium of the game, i.e. a point x\u0304 = (\u03c6\u0304, \u03b8\u0304) given by the two conditions\n\u03c6\u0304 \u2208 argmax \u03c6 f(\u03c6, \u03b8\u0304) and \u03b8\u0304 \u2208 argmax \u03b8 g(\u03c6\u0304, \u03b8). (4)\nWe call a point (\u03c6\u0304, \u03b8\u0304) a local Nash-equilibrium, if (4) holds in a local neighborhood of (\u03c6\u0304, \u03b8\u0304).\nEvery differentiable two-player game defines a vector field\nv(\u03c6, \u03b8) = ( \u2207\u03c6f(\u03c6, \u03b8) \u2207\u03b8g(\u03c6, \u03b8) ) . (5)\nWe call v the associated gradient vector field to the game defined by f and g.\nFor the special case of zero-sum two-player games, we have g = \u2212f and thus\nv\u2032(\u03c6, \u03b8) = ( \u22072\u03c6f(\u03c6, \u03b8) \u2207\u03c6,\u03b8f(\u03c6, \u03b8) \u2212\u2207\u03c6,\u03b8f(\u03c6, \u03b8) \u2212\u22072\u03b8f(\u03c6, \u03b8) ) . (6)\nAs a direct consequence, we have the following:\nLemma 1. For zero-sum games, v\u2032(x) is negative (semi-)definite if and only if\u22072\u03c6f(\u03c6, \u03b8) is negative (semi-)definite and\u22072\u03b8f(\u03c6, \u03b8) is positive (semi-)definite.\nCorollary 2. For zero-sum games, v\u2032(x\u0304) is negative semi-definite for any local Nash-equilibrium x\u0304. Conversely, if x\u0304 is a stationary point of v(x) and v\u2032(x\u0304) is negative definite, then x\u0304 is a local Nash-equilibrium.\nNote that Corollary 2 is not true for general two-player games."}, {"heading": "2.3 Simultaneous Gradient Ascent", "text": "The de-facto standard algorithm for finding Nash-equilibria of general smooth two-player games is Simultaneous Gradient Ascent (SimGA), which was described in several works, for example in [18] and, more recently also in the context of GANs, in [14]. The idea is simple and is illustrated in Algorithm 1. We iteratively update the parameters of the two players by simultaneously applying gradient ascent to the utility functions of the two players. This can also be understood as applying the Euler-method to the ordinary differential equation\nd dt x(t) = v(x(t)), (7)\nwhere v(x) is the associated gradient vector field of the two-player game.\nIt can be shown that simultaneous gradient ascent converges locally to a Nash-equilibrium for a zero-sum-game, if the Hessian of both players is negative definite [14, 18] and the learning rate is small enough. Unfortunately, in the context of GANs the former condition is rarely met. We revisit the properties of simultaneous gradient ascent in Section 3 and also show a more subtle property, namely that even if the conditions for the convergence of simultaneous gradient ascent are met, it might require extremely small step sizes for convergence if the Jacobian of the associated gradient vector field has eigenvalues with large imaginary part.\n<(z)\n=(z)\n(a) Illustration how the eigenvalues are projected into unit ball.\n<(z)\n=(z)\n(b) Example where h has to be chosen extremely small.\n<(z)\n=(z)\n(c) Illustration how our method alleviates the problem.\nFigure 1: Images showing how the eigenvalues of A are projected into the unit circle and what causes problems: when discretizing the gradient flow with step size h, the eigenvalues of the Jacobian at a fixed point are projected into the unit ball along rays from 1. However, this is only possible if the eigenvalues lie in the left half plane and requires extremely small step sizes h if the eigenvalues are close to the imaginary axis. The proposed method moves the eigenvalues to the left in order to make the problem better posed, thus allowing the algorithm to converge for reasonable step sizes."}, {"heading": "3 Convergence Theory", "text": "In this section, we analyze the convergence properties of the most common method for training GANs, simultaneous gradient ascent1. We show that two major failure causes for this algorithm are eigenvalues of the Jacobian of the associated gradient vector field with zero real-part as well as eigenvalues with large imaginary part.\nFor our theoretical analysis, we start with the following classical theorem about the convergence of fixed-point iterations:\nProposition 3. Let F : \u2126 \u2192 \u2126 be a continuously differential function on an open subset \u2126 of Rn and let x\u0304 \u2208 \u2126 be so that\n1. F (x\u0304) = x\u0304, and\n2. the absolute values of the eigenvalues of the Jacobian F \u2032(x\u0304) are all smaller than 1.\nThen there is an open neighborhood U of x\u0304 so that for all x0 \u2208 U , the iterates F (k)(x0) converge to x\u0304. The rate of convergence is at least linear. More precisely, the error \u2016F (k)(x0) \u2212 x\u0304\u2016 is in O(|\u03bbmax|k) for k \u2192\u221e where \u03bbmax is the eigenvalue of F \u2032(x\u0304) with the largest absolute value.\nProof. See [4], Proposition 4.4.1.\nIn numerics, we often consider functions of the form\nF (x) = x+ hG(x) (8)\nfor some h > 0. Finding fixed points of F is then equivalent to finding solutions to the nonlinear equation G(x) = 0 for x. For F as in (8), the Jacobian is given by\nF \u2032(x) = I + hG\u2032(x). (9)\nNote that in general neither F \u2032(x) nor G\u2032(x) are symmetric and can therefore have complex eigenvalues.\nThe following Lemma gives an easy condition, when a fixed point of F as in (8) satisfies the conditions of Proposition 3.\n1A similar analysis of alternating gradient ascent, a popular alternative to simultaneous gradient ascent, can be found in the supplementary material.\nLemma 4. Assume that A \u2208 Rn\u00d7n only has eigenvalues with negative real-part and let h > 0. Then the eigenvalues of the matrix I + hA lie in the unit ball if and only if\nh < 1 |<(\u03bb)| 2 1 + ( =(\u03bb) <(\u03bb) )2 (10) for all eigenvalues \u03bb of A.\nCorollary 5. If v\u2032(x\u0304) only has eigenvalues with negative real-part at a stationary point x\u0304, then Algorithm 1 is locally convergent to x\u0304 for h > 0 small enough.\nEquation 10 shows that there are two major factors that determine the maximum possible step size h: (i) the maximum value of <(\u03bb) and (ii) the maximum value q of |=(\u03bb)/<(\u03bb)|. Note that as q goes to infinity, we have to choose h according to O(q\u22122) which can quickly become extremely small. This is visualized in Figure 1: if G\u2032(x\u0304) has an eigenvalue with small absolute real part but big imaginary part, h needs to be chosen extremely small to still achieve convergence. Moreover, even if we make h small enough, most eigenvalues of F \u2032(x\u0304) will be very close to 1, which leads by Proposition 3 to very slow convergence of the algorithm. This is in particular a problem of simultaneous gradient ascent for two-player games (in contrast to gradient ascent for local optimization), where the Jacobian G\u2032(x\u0304) is not symmetric and can therefore have non-real eigenvalues."}, {"heading": "4 Consensus Optimization", "text": "In this section, we derive the proposed method and analyze its convergence properties."}, {"heading": "4.1 Derivation", "text": "Finding stationary points of the vector field v(x) is equivalent to solving the equation v(x) = 0. In the context of two-player games this means solving the two equations\n\u2207\u03c6f(\u03c6, \u03b8) = 0 and \u2207\u03b8g(\u03c6, \u03b8) = 0. (11)\nA simple strategy for finding such stationary points is to minimize L(x) = 12\u2016v(x)\u2016 2 for x. Unfortunately, this can result in unstable stationary points of v or other local minima of 12\u2016v(x)\u2016 2 and in practice, we found it did not work well.\nWe therefore consider a modified vector field w(x) that is as close as possible to the original vector field v(x), but at the same time still minimizes L(x) (at least locally). A sensible candidate for such a vector field is w(x) = v(x)\u2212 \u03b3\u2207L(x) (12) for some \u03b3 > 0. A simple calculation shows that the gradient\u2207L(x) is given by\n\u2207L(x) = v\u2032(x)Tv(x). (13)\nThis vector field is the gradient vector field associated to the modified two-player game given by the two modified utility functions\nf\u0303(\u03c6, \u03b8) = f(\u03c6, \u03b8)\u2212 \u03b3L(\u03c6, \u03b8) and g\u0303(\u03c6, \u03b8) = g(\u03c6, \u03b8)\u2212 \u03b3L(\u03c6, \u03b8). (14) The regularizer L(\u03c6, \u03b8) encourages agreement between the two players. Therefore we call the resulting algorithm Consensus Optimization (Algorithm 2)."}, {"heading": "4.2 Convergence", "text": "For analyzing convergence, we consider a more general algorithm than in Section 4.1 which is given by iteratively applying a function F of the form\nF (x) = x+ hA(x)v(x). (15)\nfor some step size h > 0 and an invertible matrix A(x) to x. Consensus optimization is a special case of this algorithm for A(x) = I \u2212 \u03b3 v\u2032(x)T. We assume that 1\u03b3 is not an eigenvalue of v\n\u2032(x)T for any x, so that A(x) is indeed invertible.\nAlgorithm 2 Consensus optimization 1: while not converged do 2: v\u03c6 \u2190 \u2207\u03c6(f(\u03b8, \u03c6)\u2212 \u03b3L(\u03b8, \u03c6)) 3: v\u03b8 \u2190 \u2207\u03b8(g(\u03b8, \u03c6)\u2212 \u03b3L(\u03b8, \u03c6)) 4: \u03c6\u2190 \u03c6+ hv\u03c6 5: \u03b8 \u2190 \u03b8 + hv\u03b8 6: end while\nLemma 6. Assume h > 0 and A(x) invertible for all x. Then x\u0304 is a fixed point of (15) if and only if it is a stationary point of v. Moreover, if x\u0304 is a stationary point of v, we have\nF \u2032(x\u0304) = I + hA(x\u0304)v\u2032(x\u0304). (16)\nLemma 7. Let A(x) = I \u2212 \u03b3v\u2032(x)T and assume that v\u2032(x\u0304) is negative semi-definite and invertible2. Then A(x\u0304)v\u2032(x\u0304) is negative definite.\nAs a consequence of Lemma 6 and Lemma 7, we can show local convergence of our algorithm to a local Nash equilibrium: Corollary 8. Let v(x) be the associated gradient vector field of a two-player zero-sum game and A(x) = I \u2212 \u03b3v\u2032(x)T. If x\u0304 is a local Nash-equilibrium, then there is an open neighborhood U of x\u0304 so that for all x0 \u2208 U , the iterates F (k)(x0) converge to x\u0304 for h > 0 small enough.\nOur method solves the problem of eigenvalues of the Jacobian with (approximately) zero real-part. As the next Lemma shows, it also alleviates the problem of eigenvalues with a big imaginary-to-realpart-quotient:\nLemma 9. Assume that A \u2208 Rn\u00d7n is negative semi-definite. Let q(\u03b3) be the maximum of |=(\u03bb)||<(\u03bb)| (possibly infinite) with respect to \u03bb where \u03bb denotes the eigenvalues of A \u2212 \u03b3ATA and <(\u03bb) and =(\u03bb) denote their real and imaginary part respectively. Moreover, assume that A is invertible with |Av| \u2265 \u03c1|v| for \u03c1 > 0 and let\nc = min v\u2208S(Cn) |v\u0304T(A+AT)v| |v\u0304T(A\u2212AT)v|\n(17)\nwhere S(Cn) denotes the unit sphere in Cn. Then\nq(\u03b3) \u2264 1 c+ 2\u03c12\u03b3 . (18)\nLemma 9 shows that the imaginary-to-real-part-quotient can be made arbitrarily small for an appropriate choice of \u03b3. According to Proposition 3, this leads to better convergence properties near a local Nash-equilibrium."}, {"heading": "5 Experiments", "text": "Mixture of Gaussians In our first experiment we evaluate our method on a simple 2D-example where our goal is to learn a mixture of 8 Gaussians with modes uniformly distributed around the unit circle. While simplistic, algorithms training GANs often fail to converge even on such simple examples without extensive fine-tuning of the architecture and hyper parameters [13].\nFor both the generator and critic we use fully connected neural networks with 4 hidden layers and 256 hidden units for each layer. For all layers, we use RELU-nonlinearities. We use a 64-dimensional Gaussian prior for the latent code z and set up the game between the generator and critic using the utility functions as in [8]. To test our method, we run both SimGA and our method with RMSProp and a learning rate of 10\u22124 for 10000 steps. For our method, we use a regularization parameter of \u03b3 = 10.\nThe results produced by SimGA and our method for 0, 2500, 5000 and 10000 iterations are depicted in Figure 2. We see that while SimGA jumps around the modes of the distribution and fails to converge, our method converges smoothly to the target distribution (shown in red).\n2Note that v\u2032(x\u0304) is usually not symmetric and therefore it is possible that v\u2032(x\u0304) is negative semi-definite and invertible but not negative-definite.\nCIFAR-10 and CelebA In our second experiment, we apply our method to the cifar-10 and celebAdatasets, using a DC-GAN-like architecture [17] without batch normalization in the generator or the discriminator. For celebA, we additionally use a constant number of filters in each layer and add additional RESNET-layers. These architectures are known to be hard to optimize using simultaneous (or alternating) gradient ascent [17, 2].\nFigure 3a and 3b depict samples from the model trained with our method. We see that our method successfully trains the models and we also observe that unlike when using alternating gradient ascent, the generator and discriminator losses remain almost constant during training. This is illustrated in Figure 4. For a quantitative evaluation, we also measured the inception-score [19] over time (Figure 4c), showing that our method compares favorably to a DC-GAN trained with alternating gradient ascent.\nAdditional experimental results can be found in the supplementary material."}, {"heading": "6 Discussion", "text": "While we could prove local convergence of our method in Section 4, we believe that even more insights can be gained by examining global convergence properties. In particular, our analysis from Section 4 cannot explain why the generator and discriminator losses remain almost constant during training.\nOur theoretical results assume the existence of a Nash-equilibrium. When we are trying to minimize an f-divergence and the dimensionality of the generator distribution is misspecified, this might not be the case [1]. Nonetheless, we found that our method works well in practice and we leave a closer theoretical investigation of this fact to future research.\nIn practice, our method can potentially make formerly instable stationary points of the gradient vector field stable if the regularization parameter is chosen to be high. This may lead to poor solutions. We also found that our method becomes less stable for deeper architectures, which we attribute to the fact that the gradients can have very different scales in such architectures, so that the simple L2-penalty from Section 4 needs to be rescaled accordingly. Our initial experiments suggest that this problem can be at least partially alleviated by choosing adaptive weights in such cases.\nOur method can be regarded as an approximation to the implicit Euler method for integrating the gradient vector field. It can be shown that the implicit Euler method has appealing stability properties [5] that can be translated into convergence theorems for local Nash-equilibria. However, the implicit Euler method requires the solution of a nonlinear equation in each iteration. Nonetheless, we believe that further progress can be made by finding better approximations to the implicit Euler method."}, {"heading": "7 Related Work", "text": "Saddle point problems do not only arise in the context of training GANs. For example, the popular actor-critic models [16] in reinforcement learning are also special cases of saddle-point problems.\nFinding a stable algorithm for training GANs is a long standing problem and multiple solutions have been proposed. Unrolled GANs [13] unroll the optimization with respect to the critic, thereby giving the generator more informative gradients. Though unrolling the optimization was shown to stabilize training, it can be cumbersome to implement and in addition it also results in a big model. As was recently shown, the stability of GAN-training can be improved by using objectives derived from the Wasserstein-1-distance (induced by the Kantorovich-Rubinstein-norm) instead of f-divergences [2, 9]. While Wasserstein-GANs often provide a good solution for the stable training of GANs, they require keeping the critic optimal, which can be time-consuming and can in practice only be achieved approximately, thus violating the conditions for theoretical guarantees. Moreover, some methods like Adversarial Variational Bayes [12] explicitly prescribe the divergence measure to be used, thus making it impossible to apply Wasserstein-GANs. Other approaches that try to stabilize training, try to design an easy-to-optimize architecture [19, 17] or make use of additional labels [19, 15].\nIn contrast to all the approaches described above, our work focuses on stabilizing training on a wide range of architecture and divergence functions."}, {"heading": "8 Conclusion", "text": "In this work, starting from GAN objective functions we analyzed the general difficulties of finding local Nash-equilibria in smooth two-player games. We pinpointed the major numerical difficulties that arise in the current state-of-the-art algorithms and, using our insights, we presented a new algorithm for training generative adversarial networks. Our novel algorithm has favorable properties in theory and practice: from the theoretical viewpoint, we showed that it is locally convergent to a Nashequilibrium even if the eigenvalues of the Jacobian are problematic. This is particularly interesting\nfor games that arise in the context of GANs where such problems are common. From the practical viewpoint, our algorithm can be used in combination with any GAN-architecture whose objective can be formulated as a two-player game to stabilize the training. We demonstrated experimentally that our algorithm stabilizes the training and successfully combats training issues like mode collapse. We believe our work is a first step towards an understanding of the numerics of GAN training and more general deep learning objective functions."}, {"heading": "Acknowledgements", "text": "This work was supported by Microsoft Research through its PhD Scholarship Programme."}, {"heading": "The Numerics of GANs: Supplementary Material", "text": "Lars Mescheder Autonomous Vision Group\nMPI T\u00fcbingen 72076 T\u00fcbingen\nlmescheder@tuebingen.mpg.de\nSebastian Nowozin Microsoft Research\nCambridge sebastian.nowozin@microsoft.com"}, {"heading": "Andreas Geiger", "text": "Autonomous Vision Group\nMPI T\u00fcbingen 72076 T\u00fcbingen\nandreas.geiger@tuebingen.mpg.de"}, {"heading": "Abstract", "text": "This document contains proofs that were omitted in the main text of the paper \u201cThe Numerics of GANs\u201d as well as additional theoretical results. We also include additional experimental results and demonstrate that our method leads to stable training of GANs on a variety of architectures and divergence measures."}, {"heading": "Proofs", "text": "This section contains proofs that were omitted in the main text."}, {"heading": "Smooth two player games", "text": "Lemma 1. For zero-sum games, v\u2032(x) is negative (semi-)definite if and only if\u22072\u03c6f(\u03c6, \u03b8) is negative (semi-)definite and\u22072\u03b8f(\u03c6, \u03b8) is positive (semi-)definite.\nProof. We have for any w = (w1, w2) 6= 0\nwTv\u2032(x)w = wT1\u22072\u03c6f(\u03c6, \u03b8)w1 \u2212 wT2\u22072\u03b8f(\u03c6, \u03b8)w2. (19)\nHence, we have wTv\u2032(x)w < 0 for all vectors w 6= 0 if and only if wT1\u22072\u03c6f(\u03c6, \u03b8)w1 < 0 and wT2\u22072\u03b8f(\u03c6, \u03b8)w2 > 0 for all vectors w1, w2 6= 0.\nThis shows that v\u2032(x) is negative definite if and only if\u22072\u03c6f(\u03c6, \u03b8) is negative definite and \u22072\u03b8f(\u03c6, \u03b8) is positive definite.\nA similar proof shows that v\u2032(x) is negative semi-definite if and only if \u22072\u03c6f(\u03c6, \u03b8) is negative semi-definite and\u22072\u03b8f(\u03c6, \u03b8) is positive semi-definite.\nCorollary 2. For zero-sum games, v\u2032(x\u0304) is negative semi-definite for any local Nash-equilibrium x\u0304. Conversely, if x\u0304 is a stationary point of v(x) and v\u2032(x\u0304) is negative definite, then x\u0304 is a local Nash-equilibrium.\nProof. If x\u0304 is a local Nash-equilibrium,\u22072\u03c6f(\u03c6\u0304, \u03b8\u0304) is negative semi-definite and\u22072\u03b8f(\u03c6\u0304, \u03b8\u0304) is positive semi-definite, so v\u2032(x\u0304) is negative definite by Lemma 1.\nConversely, if v\u2032(x\u0304) is negative definite, \u22072\u03c6f(\u03c6\u0304, \u03b8\u0304) is negative definite and \u22072\u03b8f(\u03c6\u0304, \u03b8\u0304) positive definite by Lemma 1. This implies that x\u0304 is a local Nash-equilibrium of the two-player game defined by f ."}, {"heading": "Convergence theory", "text": "Proposition 3. Let F : \u2126 \u2192 \u2126 be a continuously differential function on an open subset \u2126 of Rn and let x\u0304 \u2208 \u2126 be so that\n1. F (x\u0304) = x\u0304, and\n2. the absolute values of the eigenvalues of the Jacobian F \u2032(x\u0304) are all smaller than 1.\nThen there is an open neighborhood U of x\u0304 so that for all x0 \u2208 U , the iterates F (k)(x0) converge to x\u0304. The rate of convergence is at least linear. More precisely, the error \u2016F (k)(x0) \u2212 x\u0304\u2016 is in O(|\u03bbmax|k) for k \u2192\u221e where \u03bbmax is the eigenvalue of F \u2032(x\u0304) with the largest absolute value.\nProof. See [4], Proposition 4.4.1.\nLemma 4. Assume that A \u2208 Rn\u00d7n only has eigenvalues with negative real-part and let h > 0. Then the eigenvalues of the matrix I + hA lie in the unit ball if and only if\nh < 1 |<(\u03bb)| 2 1 + ( =(\u03bb) <(\u03bb) )2 (10) for all eigenvalues \u03bb of A.\nProof. For \u03bb = \u2212a+ b i with a > 0 we have\n|1 + h\u03bb|2 = (1\u2212 h a)2 + h2 b2 = 1 + h2 b2 + h2 a2 \u2212 2h a. (20)\nFor h > 0, this is smaller than 1 if and only if\nh < 2 a\nb2 + a2 =\n2a\u22121\n(b/a) 2 + 1 . (21)\nCorollary 5. If v\u2032(x\u0304) only has eigenvalues with negative real-part at a stationary point x\u0304, then Algorithm 1 is locally convergent to x\u0304 for h > 0 small enough.\nProof. This is a direct consequence of Proposition 3 and Lemma 4.\nLemma 6. Assume h > 0 and A(x) invertible for all x. Then x\u0304 is a fixed point of (15) if and only if it is a stationary point of v. Moreover, if x\u0304 is a stationary point of v, we have\nF \u2032(x\u0304) = I + hA(x\u0304)v\u2032(x\u0304). (16)\nProof. If v(x\u0304) = 0, then F (x\u0304) = x\u0304, so x\u0304 is a fixed point of F . Conversely, if x\u0304 satisfies F (x\u0304) = x\u0304, we have A(x\u0304) v(x\u0304) = 0. Because we assume A(x\u0304) to be invertible, this shows v(x\u0304) = 0.\nNow, the ith partial derivative of F (x) is given by\n\u2202xiF (x) = bi + h \u2202xiA(x) v(x) + hA(x)\u2202xiv(x) (22)\nwhere bi denotes the ith unit basis vector. For a fixed point x\u0304 we have by the first part of the proof v(x\u0304) = 0 and therefore \u2202xiF (x\u0304) = bi + hA(x\u0304)\u2202xiv(x\u0304). (23) This shows\nF (x\u0304) = I + hA(x\u0304) v\u2032(x\u0304). (24)\nLemma 7. Let A(x) = I \u2212 \u03b3v\u2032(x)T and assume that v\u2032(x\u0304) is negative semi-definite and invertible3. Then A(x\u0304)v\u2032(x\u0304) is negative definite.\nProof. We have for all w 6= 0:\nwTA(x\u0304)v\u2032(x\u0304)w = wTv\u2032(x\u0304)w \u2212 \u03b3\u2016v\u2032(x\u0304)w\u20162 \u2264 \u2212\u03b3\u2016v\u2032(x\u0304)w\u20162 < 0 (25)\nas v\u2032(x\u0304)w 6= 0 for w 6= 0.\nCorollary 8. Let v(x) be the associated gradient vector field of a two-player zero-sum game and A(x) = I \u2212 \u03b3v\u2032(x)T. If x\u0304 is a local Nash-equilibrium, then there is an open neighborhood U of x\u0304 so that for all x0 \u2208 U , the iterates F (k)(x0) converge to x\u0304 for h > 0 small enough.\nProof. This is a direct consequence of Proposition 3, Lemma 6 and Lemma 7.\nLemma 9. Assume that A \u2208 Rn\u00d7n is negative semi-definite. Let q(\u03b3) be the maximum of |=(\u03bb)||<(\u03bb)| (possibly infinite) with respect to \u03bb where \u03bb denotes the eigenvalues of A \u2212 \u03b3ATA and <(\u03bb) and =(\u03bb) denote their real and imaginary part respectively. Moreover, assume that A is invertible with |Av| \u2265 \u03c1|v| for \u03c1 > 0 and let\nc = min v\u2208S(Cn) |v\u0304T(A+AT)v| |v\u0304T(A\u2212AT)v|\n(17)\nwhere S(Cn) denotes the unit sphere in Cn. Then\nq(\u03b3) \u2264 1 c+ 2\u03c12\u03b3 . (18)\nProof. Let v \u2208 Cn \\ {0} be any eigenvector of B := A \u2212 \u03b3ATA and \u03bb \u2208 C the corresponding eigenvalue. We can assume w.l.o.g. that \u2016v\u2016 = 1. Then\n\u03bb = \u03bbv\u0304Tv = v\u0304TBv. (26)\nThis implies that\n<(\u03bb) = \u03bb+ \u03bb\u0304 2 = 1 2 v\u0304T (B +BT) v (27)\nand, similarly,\n=(\u03bb) = \u03bb\u2212 \u03bb\u0304 2 i = 1 2 i v\u0304T (B \u2212BT) v. (28)\nConsequently, we have \u2223\u2223\u2223\u2223=(\u03bb)<(\u03bb) \u2223\u2223\u2223\u2223 = |vT (B \u2212BT) v||v\u0304T (B +BT) v| (29)\nand thus\nq(\u03b3) \u2264 max v\u2208S(Cn) |vT (B \u2212BT) v| |v\u0304T (B +BT) v| . (30)\nHowever, we have\nB \u2212BT = A\u2212AT and B +BT = A+AT \u2212 2\u03b3ATA. (31)\nThis implies, because A is negative semi-definite, that\n|vT (B \u2212BT) v| |v\u0304T (B +BT) v| = |vT (A\u2212AT) v| |v\u0304T (A+AT) v|+ 2 \u03b3\u2016Av\u20162 . (32)\nBecause \u2016Av\u20162 \u2265 \u03c1\u2016v\u20162 = \u03c1 this implies the assertion. 3Note that v\u2032(x\u0304) is usually not symmetric and therefore it is possible that v\u2032(x\u0304) is negative semi-definite and invertible but not negative-definite."}, {"heading": "Additional Theoretical Results", "text": "This section contains some additional theoretical results. On the one hand, we demonstrate how the convergence of gradient ascent and common modifications like momentum and gradient rescaling can be analyzed naturally using Proposition 3. On the other hand, we analyze alternating gradient ascent for two-player games and show that for small step sizes h > 0 it is locally convergent towards a Nash-equilibrium if all the eigenvalues of the Jacobian of the associated gradient vector field have negative real-part."}, {"heading": "Gradient Ascent", "text": "Let v(x) = \u2207f(x). (33)\nThen v\u2032(x) = Hf (x) is the Hessian-matrix of f at x. Let\nF (x) = x+ h v(x). (34)\nA direct consequence of Proposition 3 is Proposition 10. Any fixed point of (34) is a stationary point of the gradient vector field v(x). Moreover, if Hf (x) is negative definite, the fixed point iteration defined by F is locally convergent towards x\u0304 for small h > 0.\nProof. This follows directly from Proposition 3."}, {"heading": "Momentum", "text": "Let v(x) be a vector field. Using momentum, the operator F can be written as\nF (x,m) = 1 + hG(x,m) (35)\nwith G(x,m) = (m, v(x)\u2212 \u03b3 m). The Jacobian of G is given by\nG\u2032(x,m) =\n( 0 I\nv\u2032(x) \u2212\u03b3I\n) . (36)\nLemma 11. \u03bb is an eigenvalue of G\u2032(x,m) if and only if \u03bb(\u03bb+ \u03b3) is an eigenvalue of v\u2032(x).\nProof. Let (w1, w2) be an eigenvector of G\u2032(x,m) as in (36) with associated eigenvalue \u03bb. Then\n\u03bbw1 = w2 (37) \u03bbw2 = v \u2032(x\u0304)w1 \u2212 \u03b3w2, (38)\nshowing that \u03bb(\u03bb+\u03b3)w1 = v\u2032(x\u0304)w1. Asw1 = 0 impliesw2 = 0, this shows thatw1 is an eigenvector of v\u2032(x\u0304) with associated eigenvalue \u03bb(\u03bb+ \u03b3).\nConversely, let \u03bb(\u03bb+ \u03b3) be an eigenvalue of v\u2032(x) to the eigenvector w. We have( 0 I\nv\u2032(x) \u2212\u03b3I )( w \u03bbw ) = ( \u03bbw \u03bb(\u03bb+ \u03b3)w \u2212 \u03b3\u03bbw ) = \u03bb ( w \u03bbw ) (39)\nshowing that \u03bb is an eigenvalue of G\u2032(x,m) to the eigenvector (w, \u03bbw).\nCorollary 12. Any fixed point (x\u0304, m\u0304) of (35) satisfies m\u0304 = 0 and v(x\u0304) = 0. Moreover, assume that \u03b3 > 0 and that v\u2032(x\u0304) only has real negative eigenvalues. Then the fixed point iteration defined by (35) is locally convergent towards (x\u0304, m\u0304) for small h > 0.\nProof. It is easy to see that any fixed point of (35) must satisfy m\u0304 = 0 and v(x\u0304) = 0. If all eigenvalues \u00b5 of v\u2032(x\u0304) are real and non-positive, then all solutions to \u03bb(\u03bb+ \u03b3) = \u00b5 have negative real part, showing local convergence by Proposition 3.\nNote that the proof of Corollary 12 breaks down if v\u2032(x\u0304) has complex eigenvalues, as it is often the case for the associated gradient vector field of two-player games.\nAlgorithm 3 Alternating Gradient Ascent 1: while not converged do 2: \u03c6\u2190 \u03c6+ h\u2207\u03c6f(\u03b8, \u03c6) 3: \u03b8 \u2190 \u03b8 + h\u2207\u03b8g(\u03b8, \u03c6) 4: end while"}, {"heading": "Gradient Rescaling", "text": "In this section we investigate the effect of gradient rescaling as used in ADAM and RMSProp on local convergence. In particular, let\nF (x, \u03b2) =\n( x+ h\u221a\n\u03b2+ v(x)\n(1\u2212 \u03b1)\u03b2 + \u03b1\u2016v(x)\u20162\n) (40)\nfor some \u03b1 > 0. The Jacobian of (40) is then given by\nF \u2032(x, \u03b2) =\n( I + h\u221a\n\u03b2+ v\u2032(x) \u2212 h 2(\u03b2+ )3/2 v(x)\n\u03b1 v\u2032(x)Tv(x) 1\u2212 \u03b1\n) . (41)\nProposition 13. Any fixed point (x\u0304, \u03b2\u0304) of (40) satisfies \u03b2\u0304 = 0 and v(x\u0304) = 0. Moreover, assume that \u03b1 \u2208 (0, 1) and that all eigenvalues of I + h\u221a v\u2032(x\u0304) lie in the unit ball. Then the fixed point iteration\ndefined by (40) is locally convergent towards (x\u0304, \u03b2\u0304).\nProof. It is easy to see that any fixed point (x\u0304, \u03b2\u0304) of (40) must satisfy \u03b2\u0304 = 0 and v(x\u0304) = 0. We therefore have\nF \u2032(x\u0304, \u03b2\u0304) =\n( I + h\u221a v\u2032(x) 0\n0 1\u2212 \u03b1\n) . (42)\nThe eigenvalues of (42) are just the eigenvalues of I + h\u221a v\u2032(x) and 1\u2212 \u03b1 which all lie in the unit\nball by assumption."}, {"heading": "Alternating Gradient Ascent", "text": "Alternating Gradient Ascent (AltGA) applies gradient ascent-updates for the two players in an alternating fashion, see Algorithm 3. For a theoretical analysis, we more generally regard fixed-point methods that iteratively apply a function of the form\nF (x) = F2(F1(x)) (43)\nto x. By the chain rule, the Jacobian of F at x is given by\nF \u2032(x) = F \u20322(F1(x)))F \u2032 1(x). (44)\nAssume now that F1(x) = x+ hG1(x) and F2(x) = x+ hG2(x). Then, if x\u0304 is a fixed point of both F1 and F2, we have\nF \u2032(x\u0304) = (I + hG\u20322(x\u0304))(I + hG \u2032 1(x\u0304)) = I + hG \u2032 1(x\u0304) + hG \u2032 2(x\u0304) + h 2G\u20322(x\u0304)G \u2032 1(x\u0304). (45)\nLetA(x) := G\u20322(x)+G \u2032 1(x). Notice that for alternating gradient ascent,A(x) is equal to the Jacobian v\u2032(x) of the gradient vector field v(x). The eigenvalues of A\u0303h(x) := A(x) + hG\u20322(x)G \u2032 1(x) depend continuously on h. Hence, for h small enough, all eigenvalues of A\u0303h(x) will be arbitrarily close to the eigenvalues of A(x).\nAs a consequence, we the following analogue of Corollary 5: Proposition 14. If the Jacobian v\u2032(x\u0304) of the gradient vector field v(x) at a fixed point x\u0304 only has eigenvalues with negative real part, Algorithm 3 is locally convergent to x\u0304 for h > 0 small enough.\nProof. This follows directly from the derivation above and Proposition 3."}, {"heading": "Additional Experimental Results", "text": ""}], "references": [{"title": "Towards principled methods for training generative adversarial networks", "author": ["Martin Arjovsky", "L\u00e9on Bottou"], "venue": "In NIPS 2016 Workshop on Adversarial Training. In review for ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Generalization and equilibrium in generative adversarial nets (GANs)", "author": ["Sanjeev Arora", "Rong Ge", "Yingyu Liang", "Tengyu Ma", "Yi Zhang"], "venue": "arXiv preprint arXiv:1703.00573,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Constrained optimization and Lagrange multiplier methods", "author": ["Dimitri P Bertsekas"], "venue": "Academic press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Numerical methods for ordinary differential equations", "author": ["John Charles Butcher"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Adversarial feature learning", "author": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1605.09782,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Adversarially learned inference", "author": ["Vincent Dumoulin", "Ishmael Belghazi", "Ben Poole", "Alex Lamb", "Martin Arjovsky", "Olivier Mastropietro", "Aaron Courville"], "venue": "arXiv preprint arXiv:1606.00704,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Improved training of Wasserstein GANs", "author": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron Courville"], "venue": "arXiv preprint arXiv:1704.00028,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A Efros"], "venue": "arXiv preprint arXiv:1611.07004,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Photorealistic single image super-resolution using a generative adversarial network", "author": ["Christian Ledig", "Lucas Theis", "Ferenc Husz\u00e1r", "Jose Caballero", "Andrew Cunningham", "Alejandro Acosta", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang"], "venue": "arXiv preprint arXiv:1609.04802,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Adversarial variational Bayes: Unifying variational autoencoders and generative adversarial networks", "author": ["Lars Mescheder", "Sebastian Nowozin", "Andreas Geiger"], "venue": "arXiv preprint arXiv:1701.04722,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "Unrolled generative adversarial networks", "author": ["Luke Metz", "Ben Poole", "David Pfau", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1611.02163,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "f-GAN: Training generative neural samplers using variational divergence minimization", "author": ["Sebastian Nowozin", "Botond Cseke", "Ryota Tomioka"], "venue": "arXiv preprint arXiv:1606.00709,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Conditional image synthesis with auxiliary classifier GANs", "author": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "venue": "arXiv preprint arXiv:1610.09585,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Connecting generative adversarial networks and actor-critic methods", "author": ["David Pfau", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1610.01945,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1511.06434,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Characterization and computation of local nash equilibria in continuous games", "author": ["Lillian J Ratliff", "Samuel A Burden", "S Shankar Sastry"], "venue": "In Communication, Control, and Computing (Allerton),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Improved techniques for training GANs", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Improved techniques for training", "author": ["Tim Salimans", "Ian J. Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "gans. CoRR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Amortised map inference for image super-resolution", "author": ["Casper Kaae S\u00f8nderby", "Jose Caballero", "Lucas Theis", "Wenzhe Shi", "Ferenc Husz\u00e1r"], "venue": "arXiv preprint arXiv:1610.04490,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Adversarial discriminative domain adaptation", "author": ["Eric Tzeng", "Judy Hoffman", "Kate Saenko", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1702.05464,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}], "referenceMentions": [{"referenceID": 6, "context": "Generative Adversarial Networks (GANs) [8] have been very successful in learning probability distributions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 8, "context": "Since their first appearance, GANs have been successfully applied to a variety of tasks, including image-to-image translation [10], image super-resolution [11], image in-painting [23] domain adaptation [22], probabilistic inference [12, 7, 6] and many more.", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "Since their first appearance, GANs have been successfully applied to a variety of tasks, including image-to-image translation [10], image super-resolution [11], image in-painting [23] domain adaptation [22], probabilistic inference [12, 7, 6] and many more.", "startOffset": 155, "endOffset": 159}, {"referenceID": 20, "context": "Since their first appearance, GANs have been successfully applied to a variety of tasks, including image-to-image translation [10], image super-resolution [11], image in-painting [23] domain adaptation [22], probabilistic inference [12, 7, 6] and many more.", "startOffset": 202, "endOffset": 206}, {"referenceID": 10, "context": "Since their first appearance, GANs have been successfully applied to a variety of tasks, including image-to-image translation [10], image super-resolution [11], image in-painting [23] domain adaptation [22], probabilistic inference [12, 7, 6] and many more.", "startOffset": 232, "endOffset": 242}, {"referenceID": 5, "context": "Since their first appearance, GANs have been successfully applied to a variety of tasks, including image-to-image translation [10], image super-resolution [11], image in-painting [23] domain adaptation [22], probabilistic inference [12, 7, 6] and many more.", "startOffset": 232, "endOffset": 242}, {"referenceID": 4, "context": "Since their first appearance, GANs have been successfully applied to a variety of tasks, including image-to-image translation [10], image super-resolution [11], image in-painting [23] domain adaptation [22], probabilistic inference [12, 7, 6] and many more.", "startOffset": 232, "endOffset": 242}, {"referenceID": 15, "context": "The standard strategy for stabilizing training is to carefully design the model, either by adapting the architecture [17] or by selecting an easy-to-optimize objective function [20, 2, 9].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "The standard strategy for stabilizing training is to carefully design the model, either by adapting the architecture [17] or by selecting an easy-to-optimize objective function [20, 2, 9].", "startOffset": 177, "endOffset": 187}, {"referenceID": 7, "context": "The standard strategy for stabilizing training is to carefully design the model, either by adapting the architecture [17] or by selecting an easy-to-optimize objective function [20, 2, 9].", "startOffset": 177, "endOffset": 187}, {"referenceID": 19, "context": "by adding instance noise [21] or by using the Wasserstein-divergence [2, 9]: while these strategies try to ensure the existence of Nash-equilibria, our paper deals with their computation and the numerical difficulties that can arise in practice.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "by adding instance noise [21] or by using the Wasserstein-divergence [2, 9]: while these strategies try to ensure the existence of Nash-equilibria, our paper deals with their computation and the numerical difficulties that can arise in practice.", "startOffset": 69, "endOffset": 75}, {"referenceID": 6, "context": "Most divergences that are used in practice can be represented in the following form [8, 14, 2]:", "startOffset": 84, "endOffset": 94}, {"referenceID": 12, "context": "Most divergences that are used in practice can be represented in the following form [8, 14, 2]:", "startOffset": 84, "endOffset": 94}, {"referenceID": 6, "context": "These divergences include the Jensen-Shannon divergence [8], all f-divergences [14], the Wasserstein divergence [2] and even the indicator divergence, which is 0 if p = q and\u221e otherwise.", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "These divergences include the Jensen-Shannon divergence [8], all f-divergences [14], the Wasserstein divergence [2] and even the indicator divergence, which is 0 if p = q and\u221e otherwise.", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "Therefore, some authors call these divergence functions neural network divergences [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 6, "context": "1 leads to a zero-sum game, whereas in practice people usually employ a variant of this formulation that is not a zero-sum game for better convergence [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 16, "context": "The de-facto standard algorithm for finding Nash-equilibria of general smooth two-player games is Simultaneous Gradient Ascent (SimGA), which was described in several works, for example in [18] and, more recently also in the context of GANs, in [14].", "startOffset": 189, "endOffset": 193}, {"referenceID": 12, "context": "The de-facto standard algorithm for finding Nash-equilibria of general smooth two-player games is Simultaneous Gradient Ascent (SimGA), which was described in several works, for example in [18] and, more recently also in the context of GANs, in [14].", "startOffset": 245, "endOffset": 249}, {"referenceID": 12, "context": "It can be shown that simultaneous gradient ascent converges locally to a Nash-equilibrium for a zero-sum-game, if the Hessian of both players is negative definite [14, 18] and the learning rate is small enough.", "startOffset": 163, "endOffset": 171}, {"referenceID": 16, "context": "It can be shown that simultaneous gradient ascent converges locally to a Nash-equilibrium for a zero-sum-game, if the Hessian of both players is negative definite [14, 18] and the learning rate is small enough.", "startOffset": 163, "endOffset": 171}, {"referenceID": 2, "context": "See [4], Proposition 4.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "While simplistic, algorithms training GANs often fail to converge even on such simple examples without extensive fine-tuning of the architecture and hyper parameters [13].", "startOffset": 166, "endOffset": 170}, {"referenceID": 6, "context": "We use a 64-dimensional Gaussian prior for the latent code z and set up the game between the generator and critic using the utility functions as in [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 15, "context": "(a) cifar-10 (b) celebA Figure 3: Samples generated from a model where both the generator and discriminator are given as in [17], but without batch-normalization.", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "CIFAR-10 and CelebA In our second experiment, we apply our method to the cifar-10 and celebAdatasets, using a DC-GAN-like architecture [17] without batch normalization in the generator or the discriminator.", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "These architectures are known to be hard to optimize using simultaneous (or alternating) gradient ascent [17, 2].", "startOffset": 105, "endOffset": 112}, {"referenceID": 17, "context": "For a quantitative evaluation, we also measured the inception-score [19] over time (Figure 4c), showing that our method compares favorably to a DC-GAN trained with alternating gradient ascent.", "startOffset": 68, "endOffset": 72}, {"referenceID": 0, "context": "When we are trying to minimize an f-divergence and the dimensionality of the generator distribution is misspecified, this might not be the case [1].", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "It can be shown that the implicit Euler method has appealing stability properties [5] that can be translated into convergence theorems for local Nash-equilibria.", "startOffset": 82, "endOffset": 85}, {"referenceID": 14, "context": "For example, the popular actor-critic models [16] in reinforcement learning are also special cases of saddle-point problems.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "Unrolled GANs [13] unroll the optimization with respect to the critic, thereby giving the generator more informative gradients.", "startOffset": 14, "endOffset": 18}, {"referenceID": 7, "context": "As was recently shown, the stability of GAN-training can be improved by using objectives derived from the Wasserstein-1-distance (induced by the Kantorovich-Rubinstein-norm) instead of f-divergences [2, 9].", "startOffset": 199, "endOffset": 205}, {"referenceID": 10, "context": "Moreover, some methods like Adversarial Variational Bayes [12] explicitly prescribe the divergence measure to be used, thus making it impossible to apply Wasserstein-GANs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Other approaches that try to stabilize training, try to design an easy-to-optimize architecture [19, 17] or make use of additional labels [19, 15].", "startOffset": 96, "endOffset": 104}, {"referenceID": 15, "context": "Other approaches that try to stabilize training, try to design an easy-to-optimize architecture [19, 17] or make use of additional labels [19, 15].", "startOffset": 96, "endOffset": 104}, {"referenceID": 17, "context": "Other approaches that try to stabilize training, try to design an easy-to-optimize architecture [19, 17] or make use of additional labels [19, 15].", "startOffset": 138, "endOffset": 146}, {"referenceID": 13, "context": "Other approaches that try to stabilize training, try to design an easy-to-optimize architecture [19, 17] or make use of additional labels [19, 15].", "startOffset": 138, "endOffset": 146}], "year": 2017, "abstractText": "In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.", "creator": "LaTeX with hyperref package"}}}