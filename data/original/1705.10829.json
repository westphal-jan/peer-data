{"id": "1705.10829", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM", "abstract": "Traditional approaches to differential privacy assume a fixed privacy requirement $\\epsilon$ for a computation, and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings, it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general \"noise reduction\" framework that can apply to a variety of private empirical risk minimization (ERM) algorithms, using them to \"search\" the space of privacy levels to find the empirically strongest one that meets the accuracy constraint, incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data, which we term ex-post privacy, and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool, modifying it to allow for queries chosen depending on the database. Finally, we apply our approach to two common objectives, regularized linear and logistic regression, and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger, empirical baseline based on binary search.", "histories": [["v1", "Tue, 30 May 2017 19:20:28 GMT  (115kb,D)", "http://arxiv.org/abs/1705.10829v1", "24 pages single-column"]], "COMMENTS": "24 pages single-column", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["katrina ligett", "seth neel", "aaron roth", "bo waggoner", "z steven wu"], "accepted": true, "id": "1705.10829"}, "pdf": {"name": "1705.10829.pdf", "metadata": {"source": "CRF", "title": "Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM", "authors": ["Katrina Ligett", "Seth Neel", "Aaron Roth", "Bo Waggoner", "Steven Wu"], "emails": [], "sections": [{"heading": "1 Introduction and Related Work", "text": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10]. As the large theoretical literature is put into practice, we start to see disconnects between assumptions implicit in the theory and the practical necessities of applications. In this paper we focus our attention on one such assumption in the domain of private empirical risk minimization (ERM): that the data analyst first chooses a privacy requirement, and then attempts to obtain the best accuracy guarantee (or empirical performance) that she can, given the chosen privacy constraint. Existing theory is tailored to this view: the data analyst can pick her privacy parameter \u03b5 via some exogenous process, and either plug it into a \u201cutility theorem\u201d to upper bound her accuracy loss, or simply\nar X\niv :1\n70 5.\n10 82\n9v 1\n[ cs\n.L G\n] 3\n0 M\ndeploy her algorithm and (privately) evaluate its performance. There is a rich and substantial literature on private convex ERM that takes this approach, weaving tight connections between standard mechanisms in differential privacy and standard tools for empirical risk minimization. These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].\nWhile these existing algorithms take a privacy-first perspective, in practice, product requirements may impose hard accuracy constraints, and privacy (while desirable) may not be the overriding concern. In such situations, things are reversed: the data analyst first fixes an accuracy requirement, and then would like to find the smallest privacy parameter consistent with the accuracy constraint. Here, we find a gap between theory and practice. The only theoretically sound method available is to take a \u201cutility theorem\u201d for an existing private ERM algorithm and solve for the smallest value of \u03b5 (the differential privacy parameter)\u2014and other parameter values that need to be set\u2014consistent with her accuracy requirement, and then run the private ERM algorithm with the resulting \u03b5. But because utility theorems tend to be worst-case bounds, this approach will generally be extremely conservative, leading to a much larger value of \u03b5 (and hence a much larger leakage of information) than is necessary for the problem at hand. Alternately, the analyst could attempt an empirical search for the smallest value of \u03b5 consistent with her accuracy goals. However, because this search is itself a data-dependent computation, it incurs the overhead of additional privacy loss. Furthermore, it is not a priori clear how to undertake such a search with nontrivial privacy guarantees for two reasons: first, the worst case could involve a very long search which reveals a large amount of information, and second, the selected privacy parameter is now itself a data-dependent quantity, and so it is not sensible to claim a \u201cstandard\u201d guarantee of differential privacy for any finite value of \u03b5 ex-ante.\nIn this paper, we describe, analyze, and empirically evaluate a principled variant of this second approach, which attempts to empirically find the smallest value of \u03b5 consistent with an accuracy requirement. We give a meta-method that can be applied to several interesting classes of private learning algorithms and introduces very little privacy overhead as a result of the privacy-parameter search. Conceptually, our meta-method initially computes a very private hypothesis, and then gradually subtracts noise (making the computation less and less private) until a sufficient level of accuracy is achieved. One key technique that saves significant factors in privacy loss over naive search is the use of correlated noise generated by the method of [14], which formalizes the conceptual idea of \u201csubtracting\u201d noise without incurring additional privacy overhead. In order to select the most private of these queries that meets the accuracy requirement, we introduce a natural modification of the now-classic AboveThreshold algorithm [7], which iteratively checks a sequence of queries on a dataset and privately releases the index of the first to approximately exceed some fixed threshold. Its privacy cost increases only logarithmically with the number of queries. We provide an analysis of AboveThreshold that holds even if the queries themselves are the result of differentially private computations, showing that if AboveThreshold terminates after t queries, one only pays the privacy costs of AboveThreshold plus the privacy cost of revealing those first t private queries. When combined with the above-mentioned correlated noise technique of [14], this gives an algorithm whose privacy loss is equal to that of the final hypothesis output \u2013 the previous ones coming \u201cfor free\u201d \u2013 plus the privacy loss of AboveThreshold. Because the privacy guarantees achieved by this approach are not fixed a priori, but rather are a function of the data,\nwe introduce and apply a new, corresponding privacy notion, which we term ex-post privacy, and which is closely related to the recently introduced notion of \u201cprivacy odometers\u201d [16].\nIn Section 4, we empirically evaluate our noise reduction meta-method, which applies to any ERM technique which can be described as a post-processing of the Laplace mechanism. This includes both direct applications of the Laplace mechanism, like output perturbation [4]; and more sophisticated methods like covariance perturbation [18], which perturbs the covariance matrix of the data and then performs an optimization using the noisy data. Our experiments concentrate on `2 regularized least-squares regression and `2 regularized logistic regression, and we apply our noise reduction meta-method to both output perturbation and covariance perturbation. Our empirical results show that the active, ex-post privacy approach massively outperforms inverting the theory curve, and also improves on a baseline \u201c\u03b5-doubling\u201d approach."}, {"heading": "2 Privacy Background and Tools", "text": ""}, {"heading": "2.1 Differential Privacy and Ex-Post Privacy", "text": "Let X denote the data domain. We call two datasets D,D \u2032 \u2208 X \u2217 neighbors (written as D \u223c D \u2032) if D can be derived from D \u2032 by replacing a single data point with some other element of X .\nDefinition 2.1 (Differential Privacy [6]). Fix \u03b5 \u2265 0. A randomized algorithm A : X \u2217 \u2192 O is \u03b5differentially private if for every pair of neighboring data sets D \u223c D \u2032 \u2208 X \u2217, and for every event S \u2286 O:\nPr[A(D) \u2208 S] \u2264 exp(\u03b5)Pr[A(D \u2032) \u2208 S].\nWe call exp(\u03b5) the privacy risk factor.\nIt is possible to design computations that do not satisfy the differential privacy definition, but whose outputs are private to an extent that can be quantified after the computation halts. For example, consider an experiment that repeatedly runs an \u03b5\u2032-differentially private algorithm, until a stopping condition defined by the output of the algorithm itself is met. This experiment does not satisfy \u03b5-differential privacy for any fixed value of \u03b5, since there is no fixed maximum number of rounds for which the experiment will run (for a fixed number of rounds, a simple composition theorem, Theorem 2.5, shows that the \u03b5-guarantees in a sequence of computations \u201cadd up.\u201d) However, if ex-post we see that the experiment has stopped after k rounds, the data can in some sense be assured an \u201cex-post privacy loss\u201d of only k\u03b5\u2032. Rogers et al. [16] initiated the study of privacy odometers, which formalize this idea. Their goal was to develop a theory of privacy composition when the data analyst can choose the privacy parameters of subsequent computations as a function of the outcomes of previous computations.\nWe apply a related idea here, for a different purpose. Our goal is to design one-shot algorithms that always achieve a target accuracy but that may have variable privacy levels depending on their input.\nDefinition 2.2. Given a randomized algorithm A : X \u2217\u2192O, define the ex-post privacy loss1 of A on outcome o to be\nLoss(o) = max D,D \u2032 :D\u223cD \u2032 log Pr[A(D) = o] Pr[A(D \u2032) = o] .\n1If A\u2019s output is from a continuous distribution rather than discrete, we abuse notation and write Pr[A(D) = o] to mean the probability density at output o.\nWe refer to exp(Loss(o)) as the ex-post privacy risk factor.\nDefinition 2.3 (Ex-Post Differential Privacy). Let E : O\u2192 (R\u22650\u222a{\u221e}) be a function on the outcome space of algorithm A : X \u2217\u2192O. Given an outcome o = A(D), we say that A satisfies E(o)-ex-post differential privacy if for all o \u2208 O, Loss(o) \u2264 E(o).\nNote that if E(o) \u2264 \u03b5 for all o, A is \u03b5-differentially private. Ex-post differential privacy has the same semantics as differential privacy, once the output of the mechanism is known: it bounds the log-likelihood ratio of the dataset being D vs. D \u2032, which controls how an adversary with an arbitrary prior on the two cases can update her posterior."}, {"heading": "2.2 Differential Privacy Tools", "text": "Differentially private computations enjoy two nice properties:\nTheorem 2.4 (Post Processing [6]). Let A : X \u2217\u2192O be any \u03b5-differentially private algorithm, and let f : O\u2192O\u2032 be any function. Then the algorithm f \u25e6A : X \u2217\u2192O\u2032 is also \u03b5-differentially private.\nPost-processing implies that, for example, every decision process based on the output of a differentially private algorithm is also differentially private.\nTheorem 2.5 (Composition [6]). Let A1 : X \u2217 \u2192 O, A2 : X \u2217 \u2192 O\u2032 be algorithms that are \u03b51- and \u03b52differentially private, respectively. Then the algorithm A : X \u2217\u2192O\u00d7O\u2032 defined as A(x) = (A1(x),A2(x)) is (\u03b51 + \u03b52)-differentially private.\nThe composition theorem holds even if the composition is adaptive\u2014-see [8] for details. The Laplace mechanism. The most basic subroutine we will use is the Laplace mechanism. The Laplace Distribution centered at 0 with scale b is the distribution with probability density function Lap(z|b) = 12be \u2212 |z|b . We say X \u223c Lap(b) when X has Laplace distribution with scale b. Let f : X \u2217 \u2192 Rd be an arbitrary d-dimensional function. The `1 sensitivity of f is defined to be \u22061(f ) = maxD\u223cD \u2032 \u2016f (D)\u2212f (D \u2032)\u20161. The Laplace mechanism with parameter \u03b5 simply adds noise drawn independently from Lap ( \u22061(f ) \u03b5 ) to each coordinate of f (x).\nTheorem 2.6 ([6]). The Laplace mechanism is \u03b5-differentially private.\nGradual private release. Koufogiannis et al. [14] study how to gradually release private data using the Laplace mechanism with an increasing sequence of \u03b5 values, with a privacy cost scaling only with the privacy of the marginal distribution on the least private release, rather than the sum of the privacy costs of independent releases. For intuition, the algorithm can be pictured as a continuous random walk starting at some private data v with the property that the marginal distribution at each point in time is Laplace centered at v, with variance increasing over time. Releasing the value of the random walk at a fixed point in time gives a certain output distribution, for example, v\u0302, with a certain privacy guarantee \u03b5. To produce v\u0302\u2032 whose ex-ante distribution has higher variance (is more private), one can simply \u201cfast forward\u201d the random walk from a starting point of v\u0302 to reach v\u0302\u2032; to produce a less private v\u0302\u2032, one can \u201crewind.\u201d The total privacy cost is max{\u03b5,\u03b5\u2032} because, given the \u201cleast private\u201d point (say v\u0302), all \u201cmore private\u201d points can be derived as post-processings given by taking a random walk of a certain length starting at v\u0302. Note that were the Laplace random variables used for each release independent, the composition theorem would require summing the \u03b5 values of all releases.\nIn our private algorithms, we will use their noise reduction mechanism as a building block to generate a list of private hypotheses \u03b81, . . . ,\u03b8T with gradually increasing \u03b5 values. Importantly, releasing any prefix (\u03b81, . . . ,\u03b8t) only incurs the privacy loss in \u03b8t. More formally:\nAlgorithm 1 Noise Reduction [14]: NR(v,\u2206, {\u03b5t}) Input: private vector v, sensitivity parameter \u2206, list \u03b51 < \u03b52 < \u00b7 \u00b7 \u00b7 < \u03b5T Set v\u0302T := v + Lap(\u2206/\u03b5T ) . drawn i.i.d. for each coordinate for t = T \u2212 1,T \u2212 2, . . . ,1 do\nWith probability ( \u03b5t \u03b5t+1 )2 : set v\u0302t := v\u0302t+1 Else: set v\u0302t := v\u0302t+1 + Lap(\u2206/\u03b5t) . drawn i.i.d. for each coordinate\nReturn v\u03021, . . . , v\u0302T\nTheorem 2.7 ([14]). Let f have `1 sensitivity \u2206 and let v\u03021, . . . , v\u0302T be the output of Algorithm 1 on v = f (D), \u2206, and the increasing list \u03b51, . . . , \u03b5T . Then for any t, the algorithm which outputs the prefix (v\u03021, . . . , v\u0302t) is \u03b5t-differentially private."}, {"heading": "2.3 AboveThreshold with Private Queries", "text": "Our high-level approach to our eventual ERM problem will be as follows: Generate a sequence of hypotheses \u03b81, . . . ,\u03b8T , each with increasing accuracy and decreasing privacy; then test their accuracy levels sequentially, outputting the first one whose accuracy is \u201cgood enough.\u201d The classical AboveThreshold algorithm [7] takes in a dataset and a sequence of queries and privately outputs the index of the first query to exceed a given threshold (with some error due to noise). We would like to use AboveThreshold to perform these accuracy checks, but there is an important obstacle: for us, the \u201cqueries\u201d themselves depend on the private data.2 A standard composition analysis would involve first privately publishing all the queries, then running AboveThreshold on these queries (which are now public). Intuitively, though, it would be much better to generate and publish the queries one at a time, until AboveThreshold halts, at which point one would not publish any more queries. The problem with analyzing this approach is that, a-priori, we do not know when AboveThreshold will terminate; to address this, we analyze the ex-post privacy guarantee of the algorithm.3\nLet us say that an algorithm M(D) = (f1, . . . , fT ) is (\u03b51, . . . , \u03b5T )-prefix-private if for each t, the function that runs M(D) and outputs just the prefix (f1, . . . , ft) is \u03b5t-differentially private.\nLemma 2.8. Let M : X \u2217\u2192 (X \u2217\u2192O)T be a (\u03b51, . . . , \u03b5T )-prefix private algorithm that returns T queries, and let each query output by M have `1 sensitivity at most \u2206. Then Algorithm 2 run on D, \u03b5A, W , \u2206, and M is E-ex-post differentially private for E((t, \u00b7)) = \u03b5A + \u03b5t for any t \u2208 [T ].\nThe proof, which is a variant on the proof of privacy for AboveThreshold [7], appears in the appendix, along with an accuracy theorem for IAT.\n2In fact, there are many applications beyond our own in which the sequence of queries input to AboveThreshold might be the result of some private prior computation on the data, and where we would like to release both the stopping index of AboveThreshold and the \u201cquery object.\u201d (In our case, the query objects will be parameterized by learned hypotheses \u03b81, . . . ,\u03b8T .)\n3This result does not follow from a straightforward application of privacy odometers from [16], because the privacy analysis of algorithms like the noise reduction technique is not compositional.\nAlgorithm 2 InteractiveAboveThreshold: IAT(D,\u03b5,W ,\u2206,M)\nInput: Dataset D, privacy loss \u03b5, threshold W , `1 sensitivity \u2206, algorithm M Let W\u0302 =W + Lap ( 2\u2206 \u03b5 ) for each query t = 1, . . . ,T do\nQuery ft\u2190M(D)t if ft(D) + Lap ( 4\u2206 \u03b5 ) \u2265 W\u0302 : then Output (t, ft); Halt.\nOutput (T , \u22a5).\nRemark 2.9. Throughout we study \u03b5-differential privacy, instead of the weaker (\u03b5,\u03b4) (approximate) differential privacy. Part of the reason is that an analogue of Lemma 2.8 does not seem to hold for (\u03b5,\u03b4)-differentially private queries without further assumptions, as the necessity to union-bound over the \u03b4 \u201cfailure probability\u201d that the privacy loss is bounded for each query can erase the ex-post gains. We leave obtaining similar results for approximate differential privacy as an open problem."}, {"heading": "3 Noise-Reduction with Private ERM", "text": "In this section, we provide a general private ERM framework that allows us to approach the best privacy guarantee achievable on the data given a target excess risk goal. Throughout the section, we consider an input dataset D that consists of n row vectors X1,X2, . . . ,Xn \u2208 Rp and a column y \u2208 Rn. We will assume that each \u2016Xi\u20161 \u2264 1 and |yi | \u2264 1. Let di = (Xi , yi) \u2208 Rp+1 be the i-th data record. Let ` be a loss function such that for any hypothesis \u03b8 and any data point (Xi , yi) the loss is `(\u03b8, (Xi , yi)). Given an input dataset D and a regularization parameter \u03bb, the goal is to minimize the following regularized empirical loss function over some feasible set C:\nL(\u03b8,D) = 1 n n\u2211 i=1 `(\u03b8, (Xi , yi)) + \u03bb 2 \u2016\u03b8\u201622.\nLet \u03b8\u2217 = argmin\u03b8\u2208C `(\u03b8,D). Given a target accuracy parameter \u03b1, we wish to privately compute a \u03b8p that satisfies L(\u03b8p,D) \u2264 L(\u03b8\u2217,D) +\u03b1, while achieving the best ex-post privacy guarantee. For simplicity, we will sometimes write L(\u03b8) for L(\u03b8,D).\nOne simple baseline approach is a \u201cdoubling method\u201d: Start with a small \u03b5 value, run an \u03b5-differentially private algorithm to compute a hypothesis \u03b8 and use the Laplace mechanism to estimate the excess risk of \u03b8; if the excess risk is lower than the target, output \u03b8; otherwise double the value of \u03b5 and repeat the same process. (See the appendix for details.) As a result, we pay for privacy loss for every hypothesis we compute and every excess risk we estimate.\nIn comparison, our meta-method provides a more cost-effective way to select the privacy level. The algorithm takes a more refined set of privacy levels \u03b51 < . . . < \u03b5T as input and generates a sequence of hypotheses \u03b81, . . . ,\u03b8T such that the generation of each \u03b8t is \u03b5t-private. Then it releases the hypotheses \u03b8t in order, halting as soon as a released hypothesis meets the accuracy goal. Importantly, there are two key components that reduce the privacy loss in our method:\n1. We use Algorithm 1, the \u201cnoise reduction\u201d method of [14], for generating the sequence of hypotheses: we first compute a very private and noisy \u03b81, and then obtain the subsequent hypotheses by gradually \u201cde-noising\u201d \u03b81. As a result, any prefix (\u03b81, . . . ,\u03b8k) incurs a privacy loss of only \u03b5k (as opposed to (\u03b51 + . . .+ \u03b5k) if the hypotheses were independent).\n2. When evaluating the excess risk of each hypothesis, we use Algorithm 2, InteractiveAboveThreshold, to determine if its excess risk exceeds the target threshold. This incurs substantially less privacy loss than independently evaluating the excess risk of each hypothesis using the Laplace mechanism (and hence allows us to search a finer grid of values).\nFor the rest of this section, we will instantiate our method concretely for two ERM problems: ridge regression and logistic regression. In particular, our noise-reduction method is based on two private ERM algorithms: the recently introduced covariance perturbation technique of [18], and output perturbation [4]."}, {"heading": "3.1 Covariance Perturbation for Ridge Regression", "text": "In ridge regression, we consider the squared loss function: `((Xi , yi),\u03b8) = 1 2 (yi \u2212 \u3008\u03b8,Xi\u3009) 2, and hence empirical loss over the data set is defined as\nL(\u03b8,D) = 1\n2n \u2016y \u2212X\u03b8\u201622 + \u03bb\u2016\u03b8\u201622 2 ,\nwhere X denotes the (n\u00d7p) matrix with row vectors X1, . . . ,Xn and y = (y1, . . . , yn). Since the optimal solution for the unconstrained problem has `2 norm no more than \u221a 1/\u03bb (see the appendix for a\nproof), we will focus on optimizing \u03b8 over the constrained set C = {a \u2208 Rp | \u2016a\u20162 \u2264 \u221a\n1/\u03bb}, which will be useful for bounding the `1 sensitivity of the empirical loss.\nBefore we formally introduce the covariance perturbation algorithm due to [18], observe that the optimal solution \u03b8\u2217 can be computed as\n\u03b8\u2217 = argmin \u03b8\u2208C L(\u03b8,D) = argmin \u03b8\u2208C (\u03b8\u1d40(X\u1d40X)\u03b8 \u2212 2\u3008X\u1d40y,\u03b8\u3009) 2n + \u03bb\u2016\u03b8\u201622 2 .\nIn other words, \u03b8\u2217 only depends on the private data through X\u1d40y and X\u1d40X. To compute a private hypothesis, the covariance perturbation method simply adds Laplace noise to each entry of X\u1d40y and X\u1d40X (the covariance matrix), and solves the optimization based on the noisy matrix and vector. The formal description of the algorithm and its guarantee are in Theorem 3.1. Our analysis differs from the one in [18] in that their paper considers the \u201clocal privacy\u201d setting, and also adds Gaussian noise whereas we use Laplace. The proof is deferred to the appendix.\nTheorem 3.1. Fix any \u03b5 > 0. For any input data set D, consider the mechanismM that computes\n\u03b8p = argmin \u03b8\u2208C 1 2n\n(\u03b8\u1d40(X\u1d40X +B)\u03b8 \u2212 2\u3008X\u1d40y + b,\u03b8\u3009) + \u03bb\u2016\u03b8\u201622\n2 ,\nwhere B \u2208Rp\u00d7p and b \u2208Rp\u00d71 are random Laplace matrices such that each entry of B and b is drawn from Lap (4/\u03b5). ThenM satisfies \u03b5-differential privacy and the output \u03b8p satisfies\nE B,b\n[ L(\u03b8p)\u2212L(\u03b8\u2217) ] \u2264 4 \u221a 2(2 \u221a p/\u03bb+ p/\u03bb) n\u03b5 .\nIn our algorithm CovNR, we will apply the noise reduction method, Algorithm 1, to produce a sequence of noisy versions of the private data (X\u1d40X,X\u1d40y): (Z1, z1), . . . , (ZT , zT ), one for each privacy level. Then for each (Zt , zt), we will compute the private hypothesis by solving the noisy version of the optimization problem in Equation (1). The full description of our algorithm CovNR is in Algorithm 3, and satisfies the following guarantee:\nAlgorithm 3 Covariance Perturbation with Noise-Reduction: CovNR(D, {\u03b51, . . . , \u03b5T },\u03b1,\u03b3) Input: private data set D = (X,y), accuracy parameter \u03b1, privacy levels \u03b51 < \u03b52 < . . . < \u03b5T , and failure probability \u03b3 Instantiate InteractiveAboveThreshold: A = IAT(D,\u03b50,\u2212\u03b1/2,\u2206, \u00b7) with \u03b50 = 16\u2206(log(2T /\u03b3))/\u03b1 and \u2206 = ( \u221a 1/\u03bb+ 1)2/(n)\nLet C = {a \u2208Rp | \u2016a\u20162 \u2264 \u221a\n1/\u03bb} and \u03b8\u2217 = argmin\u03b8\u2208C L(\u03b8) Compute noisy data:\n{Zt} = NR((X\u1d40X),2, {\u03b51/2, . . . , \u03b5T /2}), {zt} = NR((X\u1d40Y ),2, {\u03b51/2, . . . , \u03b5T /2})\nfor t = 1, . . . ,T : do\n\u03b8t = argmin \u03b8\u2208C 1 2n\n( \u03b8\u1d40Zt\u03b8 \u2212 2\u3008zt ,\u03b8\u3009 ) + \u03bb\u2016\u03b8\u201622\n2 (1)\nLet f t(D) = L(\u03b8\u2217,D)\u2212L(\u03b8t ,D); Query A with query f t to check accuracy if A returns (t, f t) then Output (t,\u03b8t) . Accurate hypothesis found.\nOutput: (\u22a5,\u03b8\u2217)\nTheorem 3.2. The instantiation of CovNR(D, {\u03b51, . . . , \u03b5T },\u03b1,\u03b3) outputs a hypothesis \u03b8p that with probability 1\u2212\u03b3 satisfies L(\u03b8p)\u2212L(\u03b8\u2217) \u2264 \u03b1. Moreover, it is E-ex-post differentially private, where the privacy loss function E : (([T ]\u222a{\u22a5})\u00d7Rp)\u2192 (R\u22650\u222a{\u221e}) is defined as E((k, \u00b7)) = \u03b50+\u03b5k for any k ,\u22a5, E((\u22a5, \u00b7)) =\u221e, and \u03b50 = 16( \u221a 1/\u03bb+1)2 log(2T /\u03b3) n\u03b1 is the privacy loss incurred by IAT."}, {"heading": "3.2 Output Perturbation for Logistic Regression", "text": "Next, we show how to combine the output perturbation method with noise reduction for the ridge regression problem.4 In this setting, the input data consists of n labeled examples (X1, y1), . . . , (Xn, yn), such that for each i, Xi \u2208Rp, \u2016Xi\u20161 \u2264 1, and yi \u2208 {\u22121,1}. The goal is to train a linear classifier given by a weight vector \u03b8 for the examples from the two classes. We consider the logistic loss function: `(\u03b8, (Xi , yi)) = log(1 + exp(\u2212yi\u03b8\u1d40Xi)), and the empirical loss is\nL(\u03b8,D) = 1 n n\u2211 i=1 log(1 + exp(\u2212yi\u03b8\u1d40Xi)) + \u03bb\u2016\u03b8\u201622 2 .\nThe output perturbation method is straightforward: we simply add Laplace noise to perturb each coordinate of the optimal solution \u03b8\u2217. The following is the formal guarantee of output perturbation. Our analysis deviates slightly from the one in [4] since we are adding Laplace noise (see the appendix).\nTheorem 3.3. Fix any \u03b5 > 0. Let r = 2 \u221a p\nn\u03bb\u03b5 . For any input dataset D, consider the mechanism that first computes \u03b8\u2217 = argmin\u03b8\u2208Rp L(\u03b8), then outputs \u03b8p = \u03b8 \u2217 + b, where b is a random vector with\n4We study the ridge regression problem for concreteness. Our method works for any ERM problem with strongly convex loss functions.\nits entries drawn i.i.d. from Lap (r). Then M satisfies \u03b5-differential privacy, and \u03b8p has excess risk Eb [ L(\u03b8p)\u2212L(\u03b8\u2217) ] \u2264 2 \u221a 2p n\u03bb\u03b5 + 4p2 n2\u03bb\u03b52 .\nGiven the output perturbation method, we can simply apply the noise reduction method NR to the optimal hypothesis \u03b8\u2217 to generate a sequence of noisy hypotheses. We will again use InteractiveAboveThreshold to check the excess risk of the hypotheses. The full algorithm OutputNR follows the same structure in Algorithm 3, and we defer the formal description to the appendix.\nTheorem 3.4. The instantiation of OutputNR(D,\u03b50, {\u03b51, . . . , \u03b5T },\u03b1,\u03b3) is E-ex-post differentially private and outputs a hypothesis \u03b8p that with probability 1\u2212\u03b3 satisfies L(\u03b8p)\u2212L(\u03b8\u2217) \u2264 \u03b1, where the privacy loss function E : (([T ]\u222a {\u22a5})\u00d7Rp)\u2192 (R\u22650 \u222a {\u221e}) is defined as E((k, \u00b7)) = \u03b50 + \u03b5k for any k ,\u22a5, E((\u22a5, \u00b7)) =\u221e, and \u03b50 \u2264 32log(2T /\u03b3) \u221a 2log2/\u03bb n\u03b1 is the privacy loss incurred by IAT.\nProof sketch of Theorems 3.2 and 3.4. The accuracy guarantees for both algorithms follow from an accuracy guarantee of the IAT algorithm (a variant on the standard AboveThreshold bound) and the fact that we output \u03b8\u2217 if IAT identifies no accurate hypothesis. For the privacy guarantee, first note that any prefix of the noisy hypotheses \u03b81, . . . ,\u03b8t satisfies \u03b5t-differential privacy because of our instantiation of the Laplace mechanism (see the appendix for the `1 sensitivity analysis) and noise-reduction method NR. Then the ex-post privacy guarantee directly follows Lemma 2.8."}, {"heading": "4 Experiments", "text": "To evaluate the methods described above, we conducted empirical evaluations in two settings. We used ridge regression to predict (log) popularity of posts on Twitter in the dataset of [1], with p = 77 features and subsampled to n =100,000 data points. Logistic regression was applied to classifying network events as innocent or malicious in the KDD-99 Cup dataset [12], with 38 features and subsampled to 100,000 points. Details of parameters and methods appear in the appendix.\nIn each case, we tested the algorithm\u2019s average ex-post privacy loss for a range of input accuracy goals \u03b1, fixing a modest failure probability \u03b3 = 0.1 (and we observed that excess risks were concentrated well below \u03b1/2, suggesting a pessimistic analysis). The results show our meta-method gives a large improvement over the \u201ctheory\u201d approach of simply inverting utility theorems for private ERM algorithms. (In fact, the utility theorem for the popular private stochastic gradient descent algorithm does not even give meaningful guarantees for the ranges of parameters tested; one would need an order of magnitude more data points, and even then the privacy losses are enormous, perhaps due to loose constants in the analysis.)\nTo gauge the more modest improvement over DoublingMethod, note that the variation in the privacy risk factor e\u03b5 can still be very large; for instance, in the ridge regression setting of \u03b1 = 0.05, Noise Reduction has e\u03b5 \u2248 10.0 while DoublingMethod has e\u03b5 \u2248 495; at \u03b1 = 0.075, the privacy risk factors are 4.65 and 56.6 respectively.\nInterestingly, for our meta-method, the contribution to privacy loss from \u201ctesting\u201d hypotheses (the InteractiveAboveThreshold technique) was significantly larger than that from \u201cgenerating\u201d them (NoiseReduction). One place where the InteractiveAboveThreshold analysis is loose is in using a theoretical bound on the maximum norm of any hypothesis to compute the sensitivity of queries. The actual norms of hypotheses tested was significantly lower which, if taken as guidance\nto the practitioner in advance, would drastically improve the privacy guarantee of both adaptive methods."}, {"heading": "Acknowledgements", "text": "This work was supported in part by NSF grants CNS-1253345, CNS-1513694, CNS-1254169 and CNS-1518941, US-Israel Binational Science Foundation grant 2012348, Israeli Science Foundation (ISF) grant #1044/16, a subcontract on the DARPA Brandeis Project, the Warren Center for Data and Network Sciences, and the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister\u2019s Office."}, {"heading": "A Missing Details and Proofs", "text": ""}, {"heading": "A.1 AboveThreshold", "text": "Proof of Lemma 2.8. Let D,D \u2032 be neighboring databases. We will instead analyze the algorithm that outputs the entire prefix f1, . . . , ft when stopping at time t. Because IAT is a post-processing of this algorithm, and privacy can only be improved under post-processing, this suffices to prove the theorem. We wish to show for all outcomes o = (t, f1, . . . , ft):\nPr[IAT(D) = (t, f1, f2, . . . , ft)] \u2264 e\u03b5A+\u03b5t Pr [ IAT(D \u2032) = (t, f1, f2, . . . , ft) ] .\nWe have directly from the privacy guarantee of InteractiveAboveThreshold that for every fixed sequence of queries f1, . . . , ft:\nPr[IAT(D) = t | f1, . . . , ft] \u2264 e\u03b5A Pr [ IAT(D \u2032) = t | f1, . . . , ft ] (2)\nbecause the guarantee of InteractiveAboveThreshold is quantified over all data-independent sequences of queries f1, . . . , fT , and by definition of the algorithm, the probability of stopping at time t is independent of the identity of any query f \u2032t for t\n\u2032 > t. Now we can write:\nPr[IAT(D) = t, f1, . . . ft] = Pr[IAT(D) = t | f1, . . . ft]Pr[M(D) = f1, . . . ft] .\nBy assumption, M is prefix-private, in particular, for fixed t and any f1, . . . , ft: Pr[M(D) = f1, . . . ft] \u2264 e\u03b5t Pr [ M(D \u2032) = f1, . . . ft ] Thus,\nPr[IAT(D) = t, f1, . . . ft] Pr[IAT(D \u2032) = t, f1, . . . ft] = Pr[IAT(D) = t | f1, . . . ft] Pr[IAT(D \u2032) = t|f1, . . . , ft] Pr[M(D) = f1, . . . ft] Pr[M(D \u2032) = f1, . . . ft]\n\u2264 e\u03b5A \u00b7 e\u03b5t = e\u03b5A+\u03b5t ,\nas desired.\nWe also include the following utility theorem. We say that an instantiation of InteractiveAboveThreshold is (\u03b1,\u03b2) accurate with respect to a threshold W and stream of queries f1, . . . fT if except with probability at most \u03b3 , the algorithm outputs a query ft only if ft(D) \u2265W \u2212\u03b1.\nTheorem A.1. For any sequence of 1-sensitive queries f1, . . . , fT such InteractiveAboveThreshold is (\u03b1,\u03b2)-accurate for\n\u03b1 = 8\u2206(log(T ) + log(2/\u03b3))\n\u03b5 ."}, {"heading": "A.2 Doubling Method", "text": "We now formally describe the DoublingMethod discussed in Section 1 and Section 3, and give a formal ex-post privacy analysis. Let \u03b8\u2217 = argmin\u03b8\u2208Rp L(\u03b8). DoublingMethod accepts a list of privacy levels \u03b51 < \u03b52 < . . . < \u03b5T , where \u03b5i = 2\u03b5i\u22121. We show in Claim B.1 that 2 is the optimal factor to scale \u03b5 by. It also takes in a failure probability \u03b3 , and a black-box private ERM mechanism M that has the following guarantee: Fixing a dataset D, M takes as input D and a privacy level \u03b5i , and generates an \u03b5i-differentially private hypothesis \u03b8i , such that the query f i(D) = L(D,\u03b8\u2217)\u2212L(D,\u03b8i) has `1 sensitivity at most \u2206.\nAlgorithm 4 Doubling Method: DoublingMethod(D, {\u03b51, . . . , \u03b5T },M,\u03b1,\u03b3) Input: private dataset D, accuracy \u03b1, failure probability \u03b3 , mechanism M\nfor each t = 1, . . . ,T do Generate \u03b8t\u2190M(D)t Let f t(D) = L(D,\u03b8\u2217)\u2212L(D,\u03b8t)\nGenerate wt \u223c Lap (\n\u03b1 2log( T\u03b3 ) ) if f t(D) +wt \u2265 \u2212\u03b1/2: then Output (t, f t); Halt.\nOutput T + 1,\u03b8\u2217.\nTheorem A.2. For k \u2264 T , define the privacy loss function E(k,\u03b8k) = 2k\u2206 log(T /\u03b3) \u03b1 + (2 k \u2212 1)\u03b51, andE(T + 1,\u03b8\u2217) =\u221e. Then DoublingMethod is E-ex-post differentially private, and is 1\u2212\u03b3 accurate.\nProof. Since if the algorithm reaches step T + 1 it outputs the true minimizer which has error 0 < \u03b1, it could only fail to output a hypothesis with error less than \u03b1 if it stops at i \u2264 T . DoublingMethod only stops early if the noisy query is greater than \u2212\u03b1/2; or f i(D)+wi \u2265 \u2212\u03b1/2. But f i(D) \u2264 \u2212\u03b1, which forces wi \u2265 \u03b1/2. By properties of the Laplace distribution, Pr[wi \u2265 \u03b1/2] = 12exp( \u2212\u03b1 2 2log( T2\u03b3 ) \u03b1 ) = \u03b3/T . Hence by union bound over T the total failure probability is at most \u03b3 . By the assumption, generating the kth private hypothesis incurs privacy loss \u03b51 \u2217 2k\u22121. By the Laplace mechanism, evaluating the error of the sensitivity \u2206 query f i is 2\u2206 log(T /\u03b3)\u03b1 -differentially private. Theorem 3.6 in [16] then says that the ex-post privacy loss of outputting k \u2264 T is \u2211k i=1[\u03b51 \u2217 2k\u22121 + 2\u2206 log(T /\u03b3)\u03b1 ] = 2k\u2206 log(T /\u03b3) \u03b1 + (2 k \u2212 1)\u03b51, as desired.\nRemark A.3. In practice, the private empirical risk minimization mechanism M may not always output a hypothesis that leads to queries with uniformly bounded `1 sensitivity. In this case, a projection that scales down, the hypothesis norm can be applied prior to evaluating the private query error. For a discussion of scaling the norm down refer to the experiments section of the appendix."}, {"heading": "A.3 Ridge Regression", "text": "In this subsection, we let `(\u03b8, (Xi , yi)) = 1 2 (yi \u2212 \u3008\u03b8,Xi\u3009) 2, and the empirical loss over the data set is defined as\nL(D,\u03b8) = 1\n2n \u2016y \u2212X\u03b8\u201622 + \u03bb\u2016\u03b8\u201622 2 ,\nwhere X denotes the (n\u00d7 p) matrix with row vectors X1, . . . ,Xn and y = (y1, . . . , yn). We assume that for each i, \u2016Xi\u20161 \u2264 1 and |yi | \u2264 1. For simplicity, we will sometimes write L(\u03b8) for L(D,\u03b8).\nFirst, we show that the unconstrained optimal solution in ridge regression has bounded norm.\nLemma A.4. Let \u03b8\u2217 = argmin\u03b8\u2208Rd L(\u03b8). Then ||\u03b8\u2217||2 \u2264 1\u221a \u03bb .\nProof. For any \u03b8 \u2208Rp,L(\u03b8\u2217) \u2264 L(\u03b8). In particular for \u03b8 = 0,\nL(\u03b8\u2217) \u2264 L(0) = n\u2211 i=1 1 2n `((Xi , yi),0) \u2264 1 2 .\nNote that for any \u03b8,`((Xi , yi),\u03b8) \u2265 0, so this means L(\u03b8\u2217) \u2265 \u03bb2 ||\u03b8 \u2217||22, which forces \u03bb 2 ||\u03b8 \u2217||22 \u2264 1 2 , and so ||\u03b8\u2217||2 \u2264 1\u221a\u03bb as desired.\nThe following claim provides a bound on the sensitivity for the excess risk, which are the queries we send to InteractiveAboveThreshold.\nClaim A.5. Let C be a bounded convex set in Rp with \u2016C\u20162 \u2264M. Let D and D \u2032 be a pair of adjacent datasets, and let \u03b8\u2217 = argmin\u03b8\u2208C L(\u03b8,D) and \u03b8 \u2022 = argmin\u03b8\u2208C L(\u03b8,D \u2032). Then for any \u03b8 \u2208 C,\n|(L(\u03b8,D)\u2212L(\u03b8\u2217,D))\u2212 (L(\u03b8,D \u2032)\u2212L(\u03b8\u2022,D \u2032))| \u2264 (M + 1) 2\nn .\nThe following lemma provides a bound on the `1 sensitivity for the matrix X\u1d40X and vector X\u1d40y.\nLemma A.6. Fix any i \u2208 [n]. Let X and Z be two n\u00d7p matrices such that for all rows j , i, Xj = Zj . Let y,y\u2032 \u2208Rn such that yj = y\u2032j for all j , i. Then\n\u2016X\u1d40X \u2212Z\u1d40Z\u20161 \u2264 2 and \u2016X\u1d40y \u2212Z\u1d40y\u2032\u20161 \u2264 2,\nas long as \u2016Xi\u2016,\u2016Zi\u2016, |yi |, |y\u2032i | \u2264 1.\nProof. We can write\n\u2016X\u1d40X \u2212Z\u1d40Z\u20161 = \u2016 \u2211 j ( X \u1d40 j Xj \u2212Z \u1d40 j Zj ) \u20161\n= \u2016X\u1d40i Xi \u2212Z \u1d40 i Z\u20161 \u2264 \u2016X\u1d40i Xi\u20161 + \u2016Z \u1d40 i Zi\u20161 = \u2016Xi\u201621 + \u2016Zi\u2016 2 1 \u2264 2.\nSimilarly,\n\u2016X\u1d40y \u2212Z\u1d40y\u2032\u20161 = \u2016 \u2211 j ( yjXj \u2212 y\u2032jZj ) \u20161\n= \u2016yiXi \u2212 y\u2032iZi\u20161 = \u2016yiXi\u20161 + \u2016y\u2032iZi\u20161 = \u2016Xi\u20161 + \u2016Zi\u20161 \u2264 2.\nThis completes the proof.\nBefore we proceed to give a formal proof for Theorem 3.1, we will also give the following basic fact about Laplace random vectors.\nClaim A.7. Let \u03bd = (\u03bd1, . . . ,\u03bdk) \u2208Rk such that each \u03bdi is an independent random variable drawn from the Laplace distribution Lap (r). Then E [\u2016\u03bd\u20162] \u2264 \u221a 2kr.\nProof. By Jensen\u2019s inequality,\nE [\u2016\u03bd\u20162] = E  \u221a\u2211\ni\n\u03bd2i  \u2264 \u221a E \u2211 i \u03bd2i . Note that by linearity of expectation and the variance of the Laplace distribution\nE \u2211 i \u03bd2i  = \u2211 i E [ \u03bd2i ] = \u2211 i 2r2 = 2kr2.\nTherefore, we have E [\u2016\u03bd\u20162] \u2264 \u221a 2kr.\nProof of Theorem 3.1. In the algorithm, we compute Z = X\u1d40X +B and z = X\u1d40y +b, where the entries of B and b are drawn i.i.d. from Lap(4/\u03b5). Note that the output \u03b8p is simply a post-processing of the noisy matrix Z and vector z. Furthermore, by Lemma A.6, the joint vector (Z,z) is has sensitivity bounded by 4 with respect to `1 norm. Therefore, the mechanism satisfies \u03b5-differential privacy by the privacy guarantee of the Laplace mechanism.\nLet M = \u221a 1/\u03bb and Lp(\u03b8) = 1 2n (\u22122\u3008z,\u03b8\u3009) + 1 2n (\u03b8 \u1d40Z\u03b8) + \u03bb\u2016\u03b8\u2016 2 2\n2 . Observe that \u03b8p = argmin\u03b8\u2208C Lp(\u03b8). Our goal is to bound L(\u03b8p)\u2212L(\u03b8\u2217), which can be written as follows\nL(\u03b8p)\u2212L(\u03b8\u2217) = L(\u03b8p)\u2212Lp(\u03b8p) +Lp(\u03b8p)\u2212Lp(\u03b8\u2217) +Lp(\u03b8\u2217)\u2212L(\u03b8\u2217) \u2264 L(\u03b8p)\u2212Lp(\u03b8p) +Lp(\u03b8\u2217)\u2212L(\u03b8\u2217)\n= 1\n2n\n( 2\u3008b,\u03b8p\u3009 \u2212\u03b8 \u1d40 pB\u03b8p ) \u2212 1\n2n (2\u3008b,\u03b8\u2217\u3009 \u2212 (\u03b8\u2217)\u1d40B\u03b8\u2217)\nMoreover, \u3008b,\u03b8p\u3009 \u2264 \u2016b\u20162\u2016\u03b8p\u20162 \u2264M\u2016b\u20162 and \u2212\u03b8\u1d40pB\u03b8p = \u2212 \u2211\n(s,t)\u2208[p]2 Bst(\u03b8p)s(\u03b8p)t\n\u2264 \u2211 (s,t) B2st  1/2 \u2211 s,t (\u03b8p) 2 s (\u03b8p) 2 t 1/2\n= \u2016B\u2016F  \u2211 s (\u03b8p) 2 s 2  1/2 \u2264 \u2016B\u2016FM2\nBy Claim A.7, we also have E [\u2016B\u2016F] \u2264 4 \u221a 2p/\u03b5 and E [\u2016b\u20162] \u2264 4 \u221a 2p/\u03b5. Finally,\nE [ L(\u03b8p)\u2212L(\u03b8\u2217) ] \u2264 E [ 1 2n ( 2\u3008b,\u03b8p\u3009 \u2212\u03b8 \u1d40 pB\u03b8p ) \u2212 1 2n (2\u3008b,\u03b8\u2217\u3009 \u2212 (\u03b8\u2217)\u1d40B\u03b8\u2217) ] = E [ 2\u3008b,\u03b8p\u3009 \u2212\u03b8 \u1d40 pB\u03b8p\n2n ] \u2264 E [2M\u2016b\u20162] +E [ M2\u2016B\u2016F ] 2n \u2264 4 \u221a 2(2 \u221a pM + pM2)\nn\u03b5\nwhich recovers our stated bound.\nNext, we will also provide a theoretical result for applying output perturbation (with Laplace noise) to the ridge regression problem. This will provides us the \u201ctheory curve\u201d for output perturbation in ridge regression plot of Figure 1a.\nFirst, the following sensitivity bound on the optimal solution for L follows directly from the strong convexity of L.\nLemma A.8. Let C be a bounded convex set in Rp with \u2016C\u20162 \u2264M. Let D and D \u2032 be a pair of neighboring datasets, and let \u03b8\u2217 = argmin\u03b8\u2208C L(\u03b8,D) and \u03b8 \u2022 = argmin\u03b8\u2208C L(\u03b8,D \u2032). Then \u2016\u03b8\u2217 \u2212\u03b8\u2022\u20161 \u2264 (M + 1) \u221a p n\u03bb . Theorem A.9. Let \u03b5 > 0 and C be a bounded convex set with \u2016C\u20162 \u2264 \u221a 1/\u03bb. Let r = ( \u221a 1/\u03bb+1) \u221a p/(n\u03bb)/\u03b5. Consider the following mechanismM that for any input dataset D first computes the optimal solution \u03b8\u2217 = argmin\u03b8\u2208C L(\u03b8), and then outputs \u03b8p = \u03b8\n\u2217 + b, where b is a random vector with its entries drawn i.i.d. from Lap (r). ThenM satisfies \u03b5-differential privacy, and \u03b8p satisfies\nE b\n[ L(\u03b8p)\u2212L(\u03b8\u2217) ] \u2264= (1 n +\u03bb ) (\u221a1/\u03bb+ 1)2p2 n\u03bb\u03b52 .\nProof. The privacy guarantee follows directly from the use of Laplace mechanism and the `1 sensitivity bound in Lemma A.8.\nFor each data point di = (Xi , yi), we have\n(yi \u2212 \u3008\u03b8p,Xi\u3009)2 \u2212 (yi \u2212 \u3008\u03b8\u2217,Xi\u3009)2 = (\u3008\u03b8p,Xi\u3009)2 \u2212 (\u3008\u03b8\u2217,Xi\u3009)2 \u2212 2\u3008b,Xi\u3009 = b\u1d40(X\u1d40i Xi)b+ (\u03b8 \u2217)\u1d40(X\u1d40i Xi)b+ b \u1d40(X\u1d40i Xi)\u03b8 \u2217 \u2212 2\u3008b,Xi\u3009\nSince each entry in b has mean 0, we can simplify the expectation as E [ (yi \u2212 \u3008\u03b8p,Xi\u3009)2 \u2212 (yi \u2212 \u3008\u03b8\u2217,Xi\u3009)2 ] = E [ b\u1d40(X\u1d40i Xi)b ] = E [ (\u3008b,Xi\u3009)2\n] \u2264 E [ \u2016b\u201622\u2016Xi\u2016 2 2\n] = E [ \u2016b\u201622 ] E [ \u2016Xi\u201622\n] \u2264 E [ \u2016b\u201622 ] \u2264 2pr2\nIn the following, let M = \u221a 1/\u03bb. We can then bound\n\u2016\u03b8p\u201622 \u2212 \u2016\u03b8 \u2217\u201622 = \u2211 s\u2208[p] [ (\u03b8s + bs) 2 \u2212\u03b82s ]\n= \u2211 s\u2208[p] [ 2\u03b8sbs + b 2 s ] ,\nAgain, since each bs is drawn from Lap(r), we get\nE [ \u2016\u03b8p\u201622 \u2212 \u2016\u03b8 \u2217\u201622 ] = E \u2211 s b2s  =\n\u2211 s E [ b2s ] = 2pr2.\nTo put all the pieces together and plugging in the value of r, we get\nE b\n[ L(\u03b8p)\u2212L(\u03b8\u2217) ] \u2264 ( 1 2n + \u03bb 2 ) 2pr2\n= (1 n +\u03bb ) (M + 1)2p2 n\u03bb\u03b52\nwhich recovers our stated bound."}, {"heading": "A.4 Logistic Regression", "text": "In this subsection, the input data D consists of n labelled examples (X1, y1), . . . , (Xn, yn), such that for each i, xi \u2208Rp, \u2016xi\u20161 \u2264 1, and yi \u2208 {\u22121,1}.\nWe consider the logistic loss function: `(\u03b8, (Xi , yi)) = log(1 + exp(\u2212yi\u03b8\u1d40Xi)), and our empirical loss is defined as\nL(\u03b8,D) = 1 n n\u2211 i=1 log(1 + exp(\u2212yi\u03b8\u1d40Xi)) + \u03bb\u2016\u03b8\u201622 2 .\nIn output perturbation, the noise needs to scale with the `1-sensitivity of the optimal solution, which is given by the following lemma.\nLemma A.10. Let D and D \u2032 be a pair of neighboring datasets. Let \u03b8 = argminw\u2208Rp L(w,D) and \u03b8\u2032 = argminw\u2032\u2208Rp L(w \u2032 ,D \u2032). Then \u2016\u03b8 \u2212\u03b8\u2032\u20161 \u2264 2 \u221a p n\u03bb .\nProof of Lemma A.10. By Corollary 8 of [4], we can bound\n\u2016\u03b8 \u2212\u03b8\u2032\u20162 \u2264 2 n\u03bb\nBy the fact that \u2016a\u20161 \u2264 \u221a p\u2016a\u20162 for any a \u2208Rp, we recover the stated result.\nWe will show that the optimal solution for the unconstrained problem has `2 norm no more than \u221a 2log2/\u03bb.\nClaim A.11. The (unconstrained) optimal solution \u03b8\u2217 has norm \u2016\u03b8\u2217\u20162 \u2264 \u221a 2log2 \u03bb .\nProof. Note that the weight vector \u03b8 = ~0 has loss log2. Therefore, L(\u03b8\u2217) \u2264 log2. Since the logistic loss is positive, we know that the regularization term\n\u03bb 2 \u2016\u03b8\u2217\u201622 \u2264 log2.\nIt follows that \u2016\u03b8\u2217\u20162 \u2264 \u221a 2log2 \u03bb .\nWe will focus on generating hypotheses \u03b8 within the set C = {a \u2208Rp | \u2016a\u20162 \u2264 \u221a\n2log2/\u03bb}. Then we can bound the `1 sensitivity of the excess risk using the following result.\nClaim A.12. Let D and D \u2032 be a pair of neighboring datasets. Then for any \u03b8 \u2208Rp such that \u2016\u03b8\u20162 \u2264M,\n|L(\u03b8,D)\u2212L(\u03b8,D \u2032)| \u2264 2 n\nlog (\n1 + exp(M) 1 + exp(\u2212M)\n)\nThe following fact is useful for our utility analysis for the output perturbation method.\nClaim A.13. Fix any data point (x,y) such that \u2016x\u20161 \u2264 1 and y \u2208 {\u22121,1}. The logistic loss function `(\u03b8, (x,y)) is a 1-Lipschitz function in \u03b8.\nProof of Theorem 3.3. The privacy guarantee follows directly from the use of Laplace mechanism and the `1-sensitivity bound in Lemma A.10. Since the logistic loss function is 1-Lipschitz. For any (x,y) in our domain,\n|`(\u03b8\u2217, (x,y))\u2212 `(\u03b8p, (x,y))| \u2264 \u2016\u03b8\u2217 \u2212\u03b8p\u20162 = \u2016b\u20162.\nFurthermore, \u2016\u03b8p\u201622 \u2212 \u2016\u03b8 \u2217\u201622 = \u2016\u03b8 \u2217 + b\u201622 \u2212 \u2016\u03b8 \u2217\u201622 = 2\u3008b,\u03b8 \u2217\u3009+ \u2016b\u201622\nBy Claim A.7 and the property of the Laplace distribution, we know that\nE [\u2016b\u20162] \u2264 \u221a 2pr and E [ \u2016b\u201622 ] = 2pr2.\nIt follows that\nE b\n[ L(\u03b8p)\u2212L(\u03b8\u2217) ] \u2264 E\nb [\u2016b\u20162] + \u03bb 2 E\n[ \u2016b\u201622 ] \u2264 \u221a 2pr + p\u03bbr2 = 2 \u221a\n2pr n\u03bb\u03b5 + 4p2 n2\u03bb\u03b52 ,\nwhich recovers the stated bound.\nWe include the full details of OutputNR in Algorithm 5.\nAlgorithm 5 Output Perturbation with Noise-Reduction: OutputNR(D, {\u03b51, . . . , \u03b5T },\u03b1,\u03b3) Input: private data set D = (X,y), accuracy parameter \u03b1, privacy levels \u03b51 < \u03b52 < . . . < \u03b5T , and failure probability \u03b3 Let M = \u221a 2log2/\u03bb\nInstantiate Interactive AboveThreshold: A = (D,\u03b50,\u03b1/2,2log(1 + exp(M))/(1 + exp(\u2212M))/(n), \u00b7)with \u03b50 = 16\u2206(log(2T /\u03b3))/\u03b1 and \u2206 = 2log(1 + exp(M))/(1 + exp(\u2212M))/(n) Let C = {a \u2208Rp | \u2016a\u20162 \u2264 \u221a 1/\u03bb} and \u03b8\u2217 = argmin\u03b8\u2208Rp L(\u03b8) Generate hypotheses: {\u03b8t} = NR(\u03b8\u2217, 2 \u221a p\nn\u03bb , {\u03b51, . . . , \u03b5T }) for t = 1, . . . ,T : do\nif \u2016\u03b8t\u20162 \u2264M then Set \u03b8t =M(\u03b8t/\u2016\u03b8t\u20162) . Rescale the norm for bounded sensitivity Let f t(D) = L(D,\u03b8\u2217)\u2212L(D,\u03b8t) Query A with f t if yes then Output (t,\u03b8t)\nOutput: (\u22a5,\u03b8\u2217)"}, {"heading": "B Experiments", "text": ""}, {"heading": "B.1 Parameters and data", "text": "For simplicity and to avoid over-fitting, we fixed the following parameters for both experiments:\n\u2022 n =100,000 (number of data points)\n\u2022 \u03bb = 0.005 (regularization parameter)\n\u2022 \u03b3 = 0.10 (requested failure probability)\n\u2022 \u03b51 = 4E, where E is the inversion of the theory guarantee for the underlying algorithm. For example in the logistic regression setting where the algorithm is Output Perturbation, E is the value such that setting \u03b5 = E guarantees expected excess risk of at most \u03b1.\n\u2022 \u03b5T = 1.0/n.\n\u2022 \u03b1 = 0.005,0.010,0.015, . . . ,0.200 (requested excess error bound).\nFor NoiseReduction, we choose T = 1000 (maximum number of iterations) and set \u03b5t = \u03b51rt for the appropriate r, i.e. r = ( \u03b5T \u03b51 )1/T .\nFor the Doubling method, T is equal to the number of doubling steps until \u03b5t exceeds \u03b5T , i.e. T = dlog2(\u03b51/\u03b5T )e.\nFeatures, labels, and transformations. The Twitter dataset has p = 77 features (dimension of each x), relating to measurements of activity relating to a posting; the label y is a measurement of the \u201cbuzz\u201d or success of the posting. Because general experience suggests that such numbers likely follow a heavy-tailed distribution, we transformed the labels by y 7\u2192 log(1 + y) and set the taks of predicting the transformed label.\nThe KDD-99 Cup dataset has p = 38 features relating to attributes of a network connection such as duration of connection, number of bytes sent in each direction, binary attributes, etc. The goal is to classify connections as innocent or malicious, with malicious connections broken down into further subcategories. We transformed three attributes containing likely heavy-tailed data (the first three mentioned above) by xi 7\u2192 log(1 + xi), dropped three columns containing textual categorical data, and transformed the labels into 1 for any kind of malicious connection and 0 for an innocent one. (The feature length p = 38 is after dropping the text columns.)\nFor both datasets, we transformed the data by renormalizing to maximum L1-norm 1. That is, we computed M = maxi \u2016xi\u20161, and transformed each xi 7\u2192 xi/M. In the case of the Twitter dataset, we did the same (separately) for the y labels. This is not a private operation (unlike the previous ones) on the data, as it depends precisely on the maximum norm. We do not consider the problem of privately ensuring bounded-norm data, as it is orthogonal to the questions we study.\nThe code for the experiments is implemented in python3 using the numpy and scikit-learn libraries."}, {"heading": "B.2 Additional results", "text": "Figure 2 plots the empirical accuracies of the output hypotheses, to ensure that the algorithms are achieving their theoretical guarantees. In fact, they do significantly better, which is reasonable considering the private testing methodology: set a threshold significantly below the goal \u03b1, add independent noise to each query, and accept only if the query plus noise is smaller than the threshold. Combined with the requirement to use tail bounds, the accuracies tend to be significantly smaller than \u03b1 and with significantly higher probability than 1\u2212\u03b3 . (Recall: this is not necessarily a good thing, as it probably costs a significant amount of extra privacy.)\nFigure 3 shows the breakdown in privacy losses between the \u201cprivacy test\u201d and the \u201chypothesis generator\u201d. In the case of NoiseReduction, these are AboveThreshold\u2019s \u03b5A and the \u03b5t of the private method, Covariance Perturbation or Output Perturbation. In the case of Doubling, these are the accrued \u03b5 due to tests at each step and due to Covariance Perturbation or Output Perturbation for outputting the hypotheses.\nThis shows the majority of the privacy loss is due to testing for privacy levels. One reason why might be that the cost of privacy tests depends heavily on certain constants, such as the norm of the hypothesis being tested. This norm is upper-bounded by a theoretical maximum which is used, but a smaller maximum would allow for significantly higher computed privacy levels for the same algorithm. In other words, the analysis might be loose compared to an analysis that knows the norms of the hypotheses, although this is a private quantity. Figure 4 supports the conclusion that generally, the theoretical maximum was very pessimistic in our cases. Note that a tenfold reduction in norm gives a tenfold reduction in privacy level for logistic regression, where sensitivity is linear in maximum norm; and a hundred-fold reduction for ridge regression."}, {"heading": "B.3 Supporting theory", "text": "Claim B.1. For the \u201cdoubling method\u201d, the factor 2 increase in \u03b5 at each time step gives the optimal worst case ex post privacy loss guarantee.\nProof. In a given setting, suppose \u03b5\u2217 is the \u201cfinal\u201d level of privacy at which the algorithm would halt. With a factor 1/r increase for r < 1, the final loss may be as large as \u03b5\u2217/r. The total loss is the sum of that loss and all previous losses, i.e. if t steps were taken:\n(\u03b5\u2217/r) + r \u00b7 (\u03b5\u2217/r) + \u00b7 \u00b7 \u00b7+ rt\u22121 \u00b7 (\u03b5\u2217/r) = (\u03b5\u2217/r) t\u22121\u2211 j=0 rj\n\u2192 (\u03b5\u2217/r) \u221e\u2211 j=0 rj = \u03b5\u2217\nr(1\u2212 r) \u2265 4\u03b5\u2217.\nThe final inequality implies that setting r = 0.5 and (1/r) = 2 is optimal. The asymptotic \u2192 is justified by noting that the starting \u03b51 may be chosen arbitrarily small, so there exist parameters that exceed the value of that summation for any finite t; and the summation limits to 11\u2212r as t\u2192\u221e."}], "references": [{"title": "Private empirical risk minimization, revisited", "author": ["Raef Bassily", "Adam D. Smith", "Abhradeep Thakurta"], "venue": "CoRR, abs/1405.7085,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Privacy-preserving logistic regression", "author": ["Kamalika Chaudhuri", "Claire Monteleoni"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Local privacy and statistical minimax rates", "author": ["John C. Duchi", "Michael I. Jordan", "Martin J. Wainwright"], "venue": "In 51st Annual Allerton Conference on Communication, Control, and Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Calibrating noise to sensitivity in private data analysis", "author": ["Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith"], "venue": "In Theory of Cryptography Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "The algorithmic foundations of differential privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends\u00ae in Theoretical Computer Science,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Boosting and differential privacy", "author": ["Cynthia Dwork", "Guy N Rothblum", "Salil Vadhan"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Building a rappor with the unknown: Privacypreserving learning of associations and data dictionaries", "author": ["Giulia Fanti", "Vasyl Pihur", "\u00dalfar Erlingsson"], "venue": "Proceedings on Privacy Enhancing Technologies (PoPETS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Apple\u2019s \u2019differential privacy\u2019 is about collecting your data\u2014but not your data", "author": ["Andy Greenberg"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Differentially private online learning", "author": ["Prateek Jain", "Pravesh Kothari", "Abhradeep Thakurta"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Private convex optimization for empirical risk minimization with applications to high-dimensional regression", "author": ["Daniel Kifer", "Adam D. Smith", "Abhradeep Thakurta"], "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Gradual release of sensitive data under differential privacy", "author": ["Fragkiskos Koufogiannis", "Shuo Han", "George J. Pappas"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Mechanism design via differential privacy", "author": ["Frank McSherry", "Kunal Talwar"], "venue": "In Foundations of Computer Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Privacy odometers and filters: Pay-as-you-go composition", "author": ["Ryan M Rogers", "Aaron Roth", "Jonathan Ullman", "Salil Vadhan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Learning in a large function space: Privacy-preserving mechanisms for SVM learning", "author": ["Benjamin I.P. Rubinstein", "Peter L. Bartlett", "Ling Huang", "Nina Taft"], "venue": "CoRR, abs/0911.5708,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Is interaction necessary for distributed private learning", "author": ["Adam Smith", "Jalaj Upadhyay", "Abhradeep Thakurta"], "venue": "IEEE Symposium on Security and Privacy,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2017}, {"title": "Stochastic gradient descent with differentially private updates", "author": ["Shuang Song", "Kamalika Chaudhuri", "Anand D. Sarwate"], "venue": "In IEEE Global Conference on Signal and Information Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}], "referenceMentions": [{"referenceID": 4, "context": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10].", "startOffset": 21, "endOffset": 27}, {"referenceID": 5, "context": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10].", "startOffset": 21, "endOffset": 27}, {"referenceID": 7, "context": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10].", "startOffset": 171, "endOffset": 174}, {"referenceID": 8, "context": "Differential Privacy [6, 7] enjoys over a decade of study as a theoretical construct, and a much more recent set of large-scale practical deployments, including by Google [9] and Apple [10].", "startOffset": 185, "endOffset": 189}, {"referenceID": 1, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 72, "endOffset": 86}, {"referenceID": 2, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 72, "endOffset": 86}, {"referenceID": 10, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 72, "endOffset": 86}, {"referenceID": 14, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 72, "endOffset": 86}, {"referenceID": 15, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 144, "endOffset": 151}, {"referenceID": 12, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 144, "endOffset": 151}, {"referenceID": 0, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 185, "endOffset": 203}, {"referenceID": 3, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 185, "endOffset": 203}, {"referenceID": 9, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 185, "endOffset": 203}, {"referenceID": 16, "context": "These methods for private ERM include output and objective perturbation [3, 4, 13, 17], covariance perturbation [18], the exponential mechanism [2, 15], and stochastic gradient descent [2, 5, 11, 19, 20].", "startOffset": 185, "endOffset": 203}, {"referenceID": 11, "context": "One key technique that saves significant factors in privacy loss over naive search is the use of correlated noise generated by the method of [14], which formalizes the conceptual idea of \u201csubtracting\u201d noise without incurring additional privacy overhead.", "startOffset": 141, "endOffset": 145}, {"referenceID": 5, "context": "In order to select the most private of these queries that meets the accuracy requirement, we introduce a natural modification of the now-classic AboveThreshold algorithm [7], which iteratively checks a sequence of queries on a dataset and privately releases the index of the first to approximately exceed some fixed threshold.", "startOffset": 170, "endOffset": 173}, {"referenceID": 11, "context": "When combined with the above-mentioned correlated noise technique of [14], this gives an algorithm whose privacy loss is equal to that of the final hypothesis output \u2013 the previous ones coming \u201cfor free\u201d \u2013 plus the privacy loss of AboveThreshold.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "we introduce and apply a new, corresponding privacy notion, which we term ex-post privacy, and which is closely related to the recently introduced notion of \u201cprivacy odometers\u201d [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 2, "context": "This includes both direct applications of the Laplace mechanism, like output perturbation [4]; and more sophisticated methods like covariance perturbation [18], which perturbs the covariance matrix of the data and then performs an optimization using the noisy data.", "startOffset": 90, "endOffset": 93}, {"referenceID": 15, "context": "This includes both direct applications of the Laplace mechanism, like output perturbation [4]; and more sophisticated methods like covariance perturbation [18], which perturbs the covariance matrix of the data and then performs an optimization using the noisy data.", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "1 (Differential Privacy [6]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 13, "context": "[16] initiated the study of privacy odometers, which formalize this idea.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "4 (Post Processing [6]).", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "5 (Composition [6]).", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "The composition theorem holds even if the composition is adaptive\u2014-see [8] for details.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "6 ([6]).", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "[14] study how to gradually release private data using the Laplace mechanism with an increasing sequence of \u03b5 values, with a privacy cost scaling only with the privacy of the marginal distribution on the least private release, rather than the sum of the privacy costs of independent releases.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Algorithm 1 Noise Reduction [14]: NR(v,\u2206, {\u03b5t}) Input: private vector v, sensitivity parameter \u2206, list \u03b51 < \u03b52 < \u00b7 \u00b7 \u00b7 < \u03b5T Set v\u0302T := v + Lap(\u2206/\u03b5T ) .", "startOffset": 28, "endOffset": 32}, {"referenceID": 11, "context": "7 ([14]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "\u201d The classical AboveThreshold algorithm [7] takes in a dataset and a sequence of queries and privately outputs the index of the first query to exceed a given threshold (with some error due to noise).", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "The proof, which is a variant on the proof of privacy for AboveThreshold [7], appears in the appendix, along with an accuracy theorem for IAT.", "startOffset": 73, "endOffset": 76}, {"referenceID": 13, "context": ") 3This result does not follow from a straightforward application of privacy odometers from [16], because the privacy analysis of algorithms like the noise reduction technique is not compositional.", "startOffset": 92, "endOffset": 96}, {"referenceID": 11, "context": "We use Algorithm 1, the \u201cnoise reduction\u201d method of [14], for generating the sequence of hypotheses: we first compute a very private and noisy \u03b81, and then obtain the subsequent hypotheses by gradually \u201cde-noising\u201d \u03b81.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "In particular, our noise-reduction method is based on two private ERM algorithms: the recently introduced covariance perturbation technique of [18], and output perturbation [4].", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "In particular, our noise-reduction method is based on two private ERM algorithms: the recently introduced covariance perturbation technique of [18], and output perturbation [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "Before we formally introduce the covariance perturbation algorithm due to [18], observe that the optimal solution \u03b8\u2217 can be computed as \u03b8\u2217 = argmin \u03b8\u2208C L(\u03b8,D) = argmin \u03b8\u2208C (\u03b8T(XTX)\u03b8 \u2212 2\u3008XTy,\u03b8\u3009) 2n + \u03bb\u2016\u03b8\u20162 2 .", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "Our analysis differs from the one in [18] in that their paper considers the \u201clocal privacy\u201d setting, and also adds Gaussian noise whereas we use Laplace.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": "Our analysis deviates slightly from the one in [4] since we are adding Laplace noise (see the appendix).", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "[2] Raef Bassily, Adam D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] Kamalika Chaudhuri and Claire Monteleoni.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Kamalika Chaudhuri, Claire Monteleoni, and Anand D.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] John C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Cynthia Dwork and Aaron Roth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Cynthia Dwork, Guy N Rothblum, and Salil Vadhan.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Giulia Fanti, Vasyl Pihur, and \u00dalfar Erlingsson.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Andy Greenberg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] Daniel Kifer, Adam D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Fragkiskos Koufogiannis, Shuo Han, and George J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Frank McSherry and Kunal Talwar.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Ryan M Rogers, Aaron Roth, Jonathan Ullman, and Salil Vadhan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Benjamin I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Adam Smith, Jalaj Upadhyay, and Abhradeep Thakurta.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Shuang Song, Kamalika Chaudhuri, and Anand D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "6 in [16] then says that the ex-post privacy loss of outputting k \u2264 T is \u2211k i=1[\u03b51 \u2217 2k\u22121 + 2\u2206 log(T /\u03b3) \u03b1 ] = 2k\u2206 log(T /\u03b3) \u03b1 + (2 k \u2212 1)\u03b51, as desired.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "By Corollary 8 of [4], we can bound", "startOffset": 18, "endOffset": 21}], "year": 2017, "abstractText": "Traditional approaches to differential privacy assume a fixed privacy requirement \u03b5 for a computation, and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings, it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general \u201cnoise reduction\u201d framework that can apply to a variety of private empirical risk minimization (ERM) algorithms, using them to \u201csearch\u201d the space of privacy levels to find the empirically strongest one that meets the accuracy constraint, incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data, which we term ex-post privacy, and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool, modifying it to allow for queries chosen depending on the database. Finally, we apply our approach to two common objectives, regularized linear and logistic regression, and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger, empirical baseline based on binary search.", "creator": "LaTeX with hyperref package"}}}