{"id": "1706.02248", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Comparative Analysis of Open Source Frameworks for Machine Learning with Use Case in Single-Threaded and Multi-Threaded Modes", "abstract": "The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation.", "histories": [["v1", "Wed, 7 Jun 2017 16:41:21 GMT  (410kb)", "http://arxiv.org/abs/1706.02248v1", "4 pages, 6 figures, 4 tables; XIIth International Scientific and Technical Conference on Computer Sciences and Information Technologies (CSIT 2017), Lviv, Ukraine"]], "COMMENTS": "4 pages, 6 figures, 4 tables; XIIth International Scientific and Technical Conference on Computer Sciences and Information Technologies (CSIT 2017), Lviv, Ukraine", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.DC", "authors": ["yuriy kochura", "sergii stirenko", "anis rojbi", "oleg alienin", "michail novotarskiy", "yuri gordienko"], "accepted": false, "id": "1706.02248"}, "pdf": {"name": "1706.02248.pdf", "metadata": {"source": "CRF", "title": "Comparative Analysis of Open Source Frameworks for Machine Learning with Use Case in Single- Threaded and Multi-Threaded Modes", "authors": ["Yuriy Kochura", "Sergii Stirenko", "Anis Rojbi", "Oleg Alienin", "Michail Novotarskiy", "Yuri Gordienko"], "emails": ["iuriy.kochura@gmail.com"], "sections": [{"heading": null, "text": "popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation.\nKeywords\u2014machine learning; deep learning; TensorFlow;\nDeep Learning4j; H2O; MNIST; multicore CPU; GPU.\nI. INTRODUCTION\nNowadays, machine learning (ML) has advanced many fields like pedestrian detection, object recognition, visualsemantic embedding, language identification, acoustic modeling in speech recognition, video classification, etc. This success is related to the invention and application of more sophisticated machine learning models and the development of software platforms that enable the easy use of large amounts of computational resources for training such models [1]. The main aims of this paper are to review some available open source frameworks for machine learning, analyze their advantages and disadvantages, and test one of them in various computing environments including CPU and GPU-based platforms. The section II. State of the Art contains the short characterization of some of the most popular and versatile available open source frameworks (TensorFlow, Deep Learning4j, and H2O) for machine learning and motivation for selection of one of them for the performance tests. The section III. Performance tests includes description of the testing methodology, data set used, and results of these tests. The section IV. Conclusions dedicated to discussion of the results obtained and lessons learned.\nII. STATE OF THE ART\nDuring the last decade numerous frameworks for machine learning appeared, but their open source implementations are seems to be most promising due to several reasons: available source codes, big community of developers and end users,\nand, consequently, numerous applications, which demonstrate and validate the maturity of these frameworks. Below the short characterization of the most versatile open source frameworks (Deep Learning4j, TensorFlow, and H2O) for machine learning is presented along with their comparative analysis."}, {"heading": "A. Deep Learning4j", "text": "Deep Learning4j (DL4J) is positioned as the open-source distributed deep-learning library written for Java and Scala that can be integrated with Hadoop and Spark [2]. It is designed to be used on distributed GPUs and CPUs platforms, and provides the ability to work with arbitrary n-dimensional arrays (also called tensors), and usage of CPU and GPU resources. Unlike many other frameworks, DL4J splits the optimization algorithm from the updater algorithm. This allows to be flexible while trying to find a combination that works best for data and problem."}, {"heading": "B. TensorFlow", "text": "TensorFlow is an open source software library for numerical computation was originally developed by researchers and engineers working on the Google Brain Team within Google\u2019s Machine Intelligence research organization [3] for the purposes of conducting machine learning and deep neural networks research. This software is the successor to DistBelief, which is the distributed system for training neural networks that Google has used since 2011. TensorFlow operates at large scale and in heterogeneous environments. This system uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). Such architecture gives flexibility to the application developer: whereas in previous \u2015parameter server\u2016 designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms."}, {"heading": "C. H2O", "text": "H2O software is built on Java, Python, and R with a purpose to optimize machine learning for Big Data [4]. It is offered as an open source platform with the following distinctive features. Big Data Friendly means that one can use all of their data in real-time for better predictions with H2O\u2019s fast in-memory distributed parallel processing capabilities. For production deployment a developer need not worry about the variation in the development platform and production environment. H2O models once created can be utilized and deployed like any Standard Java Object. H2O models are compiled into POJO (Plain Old Java Files) or a MOJO (Model Object Optimized) format which can easily embed in any Java environment. The beauty of H2O is that its algorithms can be utilized by various categories of end users from business analysts and statisticians (who are not familiar with programming languages using its Flow web-based GUI) to developers who know any of the widely used programming languages (e.g Java, R, Python, Spark). Using in-memory compression techniques, H2O can handle billions of data rows in-memory, even with a fairly small cluster. H2O implements almost all common machine learning algorithms, such as generalized linear modeling (linear regression, logistic regression, etc.), Naive Bayes, principal components analysis, time series, k-means clustering, Random Forest, Gradient Boosting, and Deep Learning."}, {"heading": "D. Comparative Analysis", "text": "From the point of view of an end user, several aspects of these frameworks are of the main interest. Except for performance and maturity, the open source frameworks could be attractive and useful, if they have the wide language and operating system support (see Table I). All of these frameworks are characterized by a quite wide ranges of supported languages and operating systems. But nowadays it is not enough in the view of the fast development of parallel and distributed computing like cluster and, especially, GPGPU computing. In this connection, TensorFlow has clear notification as to the pre-requisites for NVIDIA GPGPU cards, that should have CUDA Compute Capability (CC) 3.0 or higher. As to DL4J this is not clear because the developers stated just general support of NVIDIA GPGPU cards from GeForce GTX to Titan and Tesla that have various CC from 2.0 to 3.5. For H2O types of supported NVIDIA cards and CC are not specified, but proposed in the branching sub-framework Deep Water. The additional important aspects are the low entrance barrier and fast learning curve. They usually are based on the convenient graphical user interface, workflow management, and visualization tools. Now these features become \"de facto standard\" tools for integration of end users, workflows, and resources. The examples of their implementations (like WSPGRADE/gUSE [5], KNIME [6], etc.) and applications in physics [7], chemistry [8], astronomy [9], brain-computing [10], eHealth [11] can be found elsewhere. In this context TensorFlow and H2O propose web-based graphic user interfaces TensorBoard and Flow, respectively, which are actually workflow management and visualization tools. In\ncontrast to other frameworks H2O proposes the much shorter learning curve due to Flow, the web-based and selfexplanatory user interface. In general, Flow allows end users without experience in software programming even to import remote data, create model, train it, validate it, and then save the whole workflow. In addition, the machine learning model developed in Flow can be compiled into Plain Old Java Files (POJO) format, which can be easily embedded in any Java environment. Due to these advantages, now more than 5000 organizations currently use H2O, and many well-known companies (like Cisco, eBay, PayPal, etc.) are using it for big data processing.\nThe performance of the mentioned frameworks was a topic of many investigations performed by developers of these frameworks and independent end users [12]. But performance of H2O was not investigated thoroughly except for its developers for unknown CPU and GPU platforms [13]. That is why H2O was selected for performance tests in this paper.\nThe data set used in this work, called the \u2015MNIST data,\u2016 was proposed in 1998 to identify handwritten numbers. We have tested the H2O system by recognizing the handwritten digits (Fig. 1) from the publicly available MNIST data set for machine learning methods [14]. Now it is well-known \"de facto standard\" data set for a typical \"easy-for-humans-buthard-for-machine\" problem. The used MNIST database of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples. Each digit is represented by 28x28=784 gray-scale pixel values (features). This data set\ncontains 785 columns. The final column is the correct answer, 0 to 9. The first 784 are the 28x28 grid of grayscale pixels, and each is 0 (for white) through to 255 (for black).\nThe tests were performed on different platforms including Intel Core i5-7200U with 4 cores (CPU1), Intel Core i72700K with 8 cores (CPU2), NVIDIA Tesla K40 GPU accelerator using single-threaded and multi-threaded modes of operation. The parameters of neural network were the same for the Deep Learning (CPU only) and Deep Water (CPU+GPU) algorithms. The details of these platforms and modes of operation are given below in Tables II-IV.\nThe performance tests were carried out with Rectifier activation function for two algorithms Deep Learning (CPU only) and Deep Water (CPU+GPU). The stopping criterion was based on convergence of stopping_metric (equal to misclassification). The stop event occurs, if simple moving average of length k of the stopping_metric does not improve for k:=stopping_rounds (equal to 3) scoring events. The relative tolerance for metric-based stopping criterion was equal to 0.01. The typical convergence of training (lower) and validation (upper) logloss values with epochs is shown on Fig. 2. The results of these performance tests using H2O system are presented below in Tables II-IV. It should be noted that the results of learning neural network to recognize the handwritten digits on CPUs and GPU by using multi-threaded mode of operation are inherently not reproducible due to randomization. To estimate data scattering in multi-threaded modes of operation the runs were repeated for 12 times with determination of mean and standard deviation (Table IV).\nThe time of convergence for logloss values with epochs was not very different for all regimes, if the standard deviation (~17 sec) of duration for multi-threaded operation on GPU will be taken into account as an estimation (Fig. 3).\nDespite the much higher computing power of GPU the better training speed was observed for multi-threaded regime for CPU2 with 8 cores with speedup up to 5.2 in comparison to single-threaded regime (Fig. 4). For CPU1 with 4 cores the similar speedup for multi-threaded regime was equal to 1.7 in comparison to single-threaded regime. As to GPU training speed these results can be explained by much bigger number (by ~100 times) of performed iterations.\nAs it is well-known the logloss values are very sensitive to outliers and this tendency is very pronounced in the case of GPU, where the much bigger iterations were used and higher training logloss values were found (Fig. 5).\nThe ratio of validation logloss to training logloss is equal to 1.53 for Deep Water case, which is much lower in comparison to the same ratio 2.88 for Deep Learning single-threaded case, and 3.89 and 5.44 even for Deep Learning multi-threaded case CPU1 and CPU2, respectively. This allows to make assumption that the more iterations in GPU mode give the more realistic model with the lower risk of overfitting.\nFinally, in this paper we described the basic features of some open source frameworks for machine learning, namely TensorFlow, Deep Learning4j, and H2O. For usability and performance tests H2O framework was selected. It was tested on several platforms like Intel Core i5-7200U (4 cores), Intel Core i7-2700K (8 cores), Tesla K40 GPU with the goal to evaluate their performance in the context of recognizing handwritten digits from MNIST data set. To reach this goal the same parameters of the neural network were used for Deep Learning and Deep Water algorithms. The influence of many other aspects like nature of data (for example, sparsity level and sparsity pattern), number of hidden layers and their sizes should be taken into account for the better comparative analysis, but these aspects were out of scope of the current work and will be published separately elsewhere [15,16].\nThe work carried out and the results obtained allow us to make the following conclusions as to H2O framework:\n H2O propose the unprecedentedly fast learning curve due to the available web-based GUI, easy workflow management tools, and visualization tools for representation of data;\n H2O allows the data scientists without any programming experience easily operate by several deep learning backends (mxnet, Caffe, TensorFlow) with various\nactivation functions (rectifier, tahn), parameters of neural network, stopping criteria, and convergence conditions;\n H2O propose opportunities for reproducible singlethreaded and non-reproducible multi-threading modes of operation for multicore CPUs and GPUs;\n multi-threaded operations on CPUs give the smaller logloss values than single-threaded operations, but the ratio of validation logloss to training logloss is much lower in comparison to multi-threaded operations on GPU, which gives the more realistic model with the lower risk of overfitting.\nThe work was partially supported by NVIDIA Research and Education Centers in National Technical University of Ukraine \"Igor Sikorsky Kyiv Polytechnic Institute\"."}], "references": [{"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank", "M.A. Hall", "C.J. Pal"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "TensorFlow: A system for large-scale machine learning", "author": ["M Abadi"], "venue": "12th USENIX Symposium on Operating Systems Design and Implementation (OSDI", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Deep Learning with H2O. H2O", "author": ["A. Candel", "V. Parmar", "E. LeDell", "A. Arora"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "DCI bridge: Executing WS-PGRADE workflows in distributed computing infrastructures. Science Gateways for Distributed Computing Infrastructures (pp. 51-67)", "author": ["M Kozlovszky"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Software review: the KNIME workflow environment and its applications in Genetic Programming and machine learning", "author": ["S. O\u2019Hagan", "D.B. Kell"], "venue": "Genetic Programming and Evolvable Machines,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "IMP Science Gateway: from the Portal to the Hub of Virtual Experimental Labs in e-Science and Multiscale Courses in e-Learning", "author": ["Y Gordienko"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Quantum chemical meta-workflows in MoSGrid", "author": ["S Herres-Pawlis"], "venue": "Concurrency and Computation: Practice and Experience,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "VO-compliant workflows and science gateways", "author": ["G Castelli"], "venue": "Astronomy and Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "User-driven Intelligent Interface on the Basis of Multimodal Augmented Reality and Brain-Computer Interaction for People with Functional Disabilities, arXiv:1704.05915", "author": ["S.Stirenko"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2017}, {"title": "Augmented Coaching Ecosystem for Nonobtrusive Adaptive Personalized Elderly Care on the Basis of Cloud- Fog-Dew Computing Paradigm", "author": ["Yu.Gordienko"], "venue": "Proc. IEEE 40th Int. Conv. Inform. and Communic. Technology, Electronics and Microelectronics (MIPRO)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Software Frameworks for Deep Learning at Scale", "author": ["James Fox", "Yiming Zou", "Judy Qiu"], "venue": "Internal Indiana University Technical Report", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Deep Learning Benchmark Use Case for TensorFlow, Caffe, and mxnet", "author": ["Yu.Kochura"], "venue": "Proc. 9th Int. Conf. Communication and Information Technologies", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Deep Learning for Fatigue Estimation on the Basis of Multimodal Human-Machine Interactions", "author": ["Yu.Kochura"], "venue": "Proc. XXIX IUPAP Conference in Computational Physics", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "This success is related to the invention and application of more sophisticated machine learning models and the development of software platforms that enable the easy use of large amounts of computational resources for training such models [1].", "startOffset": 239, "endOffset": 242}, {"referenceID": 1, "context": "TensorFlow is an open source software library for numerical computation was originally developed by researchers and engineers working on the Google Brain Team within Google\u2019s Machine Intelligence research organization [3] for the purposes of conducting machine learning and deep neural networks research.", "startOffset": 218, "endOffset": 221}, {"referenceID": 2, "context": "H2O software is built on Java, Python, and R with a purpose to optimize machine learning for Big Data [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "The examples of their implementations (like WSPGRADE/gUSE [5], KNIME [6], etc.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "The examples of their implementations (like WSPGRADE/gUSE [5], KNIME [6], etc.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": ") and applications in physics [7], chemistry [8], astronomy [9], brain-computing [10], eHealth [11] can be found elsewhere.", "startOffset": 30, "endOffset": 33}, {"referenceID": 6, "context": ") and applications in physics [7], chemistry [8], astronomy [9], brain-computing [10], eHealth [11] can be found elsewhere.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": ") and applications in physics [7], chemistry [8], astronomy [9], brain-computing [10], eHealth [11] can be found elsewhere.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": ") and applications in physics [7], chemistry [8], astronomy [9], brain-computing [10], eHealth [11] can be found elsewhere.", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": ") and applications in physics [7], chemistry [8], astronomy [9], brain-computing [10], eHealth [11] can be found elsewhere.", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "The performance of the mentioned frameworks was a topic of many investigations performed by developers of these frameworks and independent end users [12].", "startOffset": 149, "endOffset": 153}, {"referenceID": 11, "context": "1) from the publicly available MNIST data set for machine learning methods [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "The influence of many other aspects like nature of data (for example, sparsity level and sparsity pattern), number of hidden layers and their sizes should be taken into account for the better comparative analysis, but these aspects were out of scope of the current work and will be published separately elsewhere [15,16].", "startOffset": 313, "endOffset": 320}, {"referenceID": 13, "context": "The influence of many other aspects like nature of data (for example, sparsity level and sparsity pattern), number of hidden layers and their sizes should be taken into account for the better comparative analysis, but these aspects were out of scope of the current work and will be published separately elsewhere [15,16].", "startOffset": 313, "endOffset": 320}], "year": 2017, "abstractText": "The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation. Keywords\u2014machine learning; deep learning; TensorFlow; Deep Learning4j; H2O; MNIST; multicore CPU; GPU.", "creator": "Microsoft\u00ae Office Word 2007"}}}