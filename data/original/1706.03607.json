{"id": "1706.03607", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Clustering over Multi-Objective Samples: The one2all Sample", "abstract": "Clustering is a fundamental technique in data analysis. Consider data points $X$ that lie in a (relaxed) metric space (where the triangle inequality can be relaxed by a constant factor). Each set of points $Q$ ({\\em centers}) defines a clustering of $X$ according to the closest center with {\\em cost} $V(Q)=\\sum_{x\\in X} d_{xQ}$. This formulation generalizes classic $k$-means clustering, which uses squared distances. Two basic tasks, parametrized by $k \\geq 1$, are {\\em cost estimation}, which returns (approximate) $V(Q)$ for queries $Q$ such that $|Q|=k$ and {\\em clustering}, which returns an (approximate) minimizer of $V(Q)$ of size $|Q|=k$. With very large data sets $X$, we seek efficient constructions of small summaries that allow us to efficiently approximate clustering costs over the full data.", "histories": [["v1", "Mon, 12 Jun 2017 13:05:46 GMT  (19kb,D)", "http://arxiv.org/abs/1706.03607v1", "10 pages, 1 figure"], ["v2", "Sun, 29 Oct 2017 10:27:20 GMT  (76kb,D)", "http://arxiv.org/abs/1706.03607v2", "17 pages, 2 figure"]], "COMMENTS": "10 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["edith cohen", "shiri chechik", "haim kaplan"], "accepted": false, "id": "1706.03607"}, "pdf": {"name": "1706.03607.pdf", "metadata": {"source": "CRF", "title": "Clustering over Multi-Objective Samples: The one2all Sample", "authors": ["Edith Cohen", "Shiri Chechik", "Haim Kaplan"], "emails": [], "sections": [{"heading": null, "text": "metric space (where the triangle inequality can be relaxed by a constant factor). Each set of points Q (centers) defines a clustering of X according to the closest center with cost V (Q) = \u2211 x\u2208X dxQ. This formulation generalizes classic k-means clustering, which uses squared distances. Two basic tasks, parametrized by k \u2265 1, are cost estimation, which returns (approximate) V (Q) for queries Q such that |Q| = k and clustering, which returns an (approximate) minimizer of V (Q) of size |Q| = k. With very large data sets X, we seek efficient constructions of small summaries that allow us to efficiently approximate clustering costs over the full data.\nWe present a novel data reduction tool based on multi-objective probability-proportional-to-size (pps) sampling: Our one2all construction inputs any set of centers M and efficiently computes a sample of size O(|M |) from which we can tightly estimate the clustering cost V (Q) for any Q that has at least a fraction of the clustering cost of M . For cost queries, we apply one2all to a bicriteria approximation to obtain a sample of size O(k \u22122) for all |Q| = k. For clustering, we propose a wrapper that applies a black-box algorithm to a sample and tests clustering quality over X, adaptively increasing the sample size. Our approach exploits the structure of the data to provide quality guarantees through small samples, without the use of typically much larger worst-case-size summaries."}, {"heading": "1 Introduction", "text": "Clustering is a fundamental and prevalent tool in data analysis. We have a set X of data points that lie in a (relaxed) metric spaceM, where distances satisfy a relaxed triangle inequality: For some constant \u03c1 \u2265 1, for any three points x, y, z, dxy \u2264 \u03c1(dxz + dzy). Note that any metric space with distances replaced by their pth power satisfies this relaxation: For p \u2264 1 it remains a metric and otherwise we have \u03c1 = 2p\u22121. In particular, for squared distances, commonly used for clustering, we have \u03c1 = 2.\nEach set Q \u2282M of points (centers) defines a clustering, which is a partition of X into |Q| clusters, which we denote by Xq for q \u2208 Q, so that a point x \u2208 X is in Xq if and only if it is in the Voronoi region of q, that is q = arg miny\u2208Q dxy. We allow points x \u2208 X to have optional weights wx > 0, and define the cost of clustering X by Q to be\nV (Q | X,w) = \u2211 x\u2208X wxdxQ , (1)\nwhere dxQ = miny\u2208Q dxy is the distance from point x to the set Q. Two fundamental computational tasks are cost queries and clustering (cost minimization). The clustering cost (1) of query Q can be computed using n|Q| pairwise distance computations, where n = |X| is the number of points in X. With multiple queries, it is useful to pre-process X and return fast approximate answers. Clustering amounts to finding Q of size |Q| \u2264 k with minimum cost:\narg min Q||Q|\u2264k\nV (Q | X,w) . (2)\nar X\niv :1\n70 6.\n03 60\n7v 1\n[ cs\n.L G\n] 1\n2 Ju\nn 20\nClustering is computationally hard [2] even on Euclidean spaces and even to tightly approximate [4]. There is a local search polynomial algorithm with 9 + approximation ratio [20]. In practice, clustering is solved using heuristics, most notably Lloyd\u2019s algorithm [22], and scalable approximation algorithms such as kmeans++ [3], which obtains a log k factor approximation using k centers. A recent result [27] established that kmeans++ provides constant factors bi-criteria guarantees: When applied to obtain \u03b2k centers for some constant \u03b2 > 1, the clustering cost is within a constant factor of the optimum k-means cost.\nWhen the set of points X is very large, we seek an efficient procedure that computes a small summary structure from which we can approximate clustering costs. These structures are commonly in the form of subsets S \u2282 X with weights w\u2032 so that V (Q | S,w\u2032) approximates V (Q | X,w) for each Q of size k. The term coresets for such structures was introduced in the computational geometry literature [1, 18], building on the theory of -nets. Some notable constructions include [23, 9, 15, 16]. Initial coresets constructions had bounds with high (exponential or high polynomial) dependence on some parameters (dimension, , k) and poly logarithmic dependence on n, with the best current asymptotic bound of O(k \u22122 log k log n) claimed in [5].\nThe bulk of these randomized summary structures are aimed to provide strong \u201cForAll\u201d statistical guarantees, which bound the distribution of the maximum approximation error over all Q of size k. The constructions are worst-case and based on general (VC) dimension bounds. Moreover, they do not exploit typical structure present in the data and must use size that meets the worst-case bounds in order to obtain guarantees on quality of the results. The ForAll requirement, however, is a costly overkill for the two tasks we have at hand: For clustering cost queries, weaker per-query \u201cForEach\u201d typically suffice, which for each Q, with very high probability over the structure distribution, bound the error of the estimate of V (Q). For clustering, it suffices to guarantee that the (approximate) minimizers of V (Q | S,w\u2032) are approximate minimizers of V (Q | X,w). In particular, sets Q with high clustering costs require only very coarse approximation. Moreover, our data typically has structure that includes a natural clustering (which is what we seek) and lower dimensionality than the ambient space. While a notion of \u201cweak coresets\u201d that aim to only support optimization was considered in the coreset literature [15], the constructions are also worst-case.\nContribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].\nConsider a particular set Q of centers. The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample \u22122 points with probabilities px \u221d wxdQx proportional to their contribution to the sum [17]. The inverse-probability [19] estimate obtained from the sample S,\nV\u0302 (Q | X,w) \u2261 V (Q | S, {wx/px}) ,\nis an unbiased estimate of V (Q | X,w) with well-concentrated normalized squared error of . The challenge here for us is that we are interested in quality guarantees for all subsets Q of size k whereas the estimate V (Q\u2032 | S, {wx/px}) when S is taken for a particular Q 6= Q\u2032 will not provide quality guarantees for the set Q\u2032. To obtain these quality guarantees for all Q by a single sample, we apply the machinery of multi-objective sampling: We use multi-objective pps sampling probabilities, where each point x \u2208 X is sampled with the maximum pps probability over all Q of size k.\nApriori, however, it seems that the size of such a multi-objective sample can be very large. Surprisingly, we show that on any (relaxed) metric space, the multi-objective pps sample size is O(k \u22122). Note that the size does not depend on the dimensionality of the space or on the size of the data. Our result generalizes previous work [8] that only applied to the case where k = 1, where clustering cost reduces to inverse classic closeness centrality (sum of distances from a single point Q).\nFor our applications, we also need to efficiently perform the sampling. Clearly, a straightforward computation of these multi-objective pps probabilities is not feasible.\nOur main technical contribution, which is the basis of both the existential and algorithmic results, is an efficient general construction which we refer to as one2all: For any given set M of centers, and any \u03b1 \u2264 1,\nwe compute using |M |n distance computation, multi-objective sampling probabilities for all subsets Q with clustering cost V (Q) \u2265 \u03b1V (M). The size of the sample is O(\u03b1\u22121|M | \u22122).\nBy applying our one2all construction to an optimal clustering M of size k and \u03b1 = 1, we establish existentially that a multi-objective pps sample for all sets Q of size k has size O(k \u22122). To obtain an efficient construction, we can apply kmeans++ [3] or another efficient bi-criteria approximation algorithm to compute M of size 2k that has cost that is at most the optimum divided by some constant \u03b1 [27]. We then apply our one2all construction to M with \u03b1 to obtain a multi-objective sample for all k-subsets of size O(\u03b1\u22121|M | \u22122) = O(k \u22122).\nFor the task of approximate cost queries, we pre-process the data as above to obtain a multi-objective sample of size O(k \u22122). We then process cost queries Q by computing and returning the clustering cost of S by Q: V (Q | S, {wx/px}). Our sample provide statistical guarantees that apply to each Q of size k, or more generally, for each Q with V (Q | X,w) \u2265 \u03b1V (M | X,w) over the distribution of the sample S.\nFor approximate clustering tasks, we propose a wrapper algorithm which uses as a black-box (approximate, bicriteria) clustering algorithm A. The algorithm A is applied to the smaller sample to obtain a respective approximate minimizer of the clustering cost over the sample. The ForEach guarantee, however, in-and-off itself, does not guarantee us that the solution over the sample has the respective quality over the full data set. A larger sample size may be required and can be achieved by controlling the size parameter of the multi-objective sample. We use an optimization framework over multi-objective samples [11]. This framework relies on a critical property of ForEach samples: While not guaranteeing optimization, they do facilitate testing of the quality of the sample approximate optimizer Q returned by A: If the clustering cost of V (Q | X,w) agrees with the estimate V (Q | S,w\u2032) then we can certify that Q is an approximate optimizer (in the sense of A) over the full data X. Otherwise, V (Q | S,w\u2032) serves as an approximate lower bound for what we could do over X. Conveniently, this test can be performed with high confidence using another independent validation ForEach multi-objective pps sample. Our wrapper algorithm iteratively doubles the multi-objective pps sample size S and applies A to cluster S until the sample (approximate) minimizer satisfies the test. This approach allows us to work with the smallest sample that suffices to meet the specified quality.\nThe paper is organized as follows. The relevant components of the machinery of multi-objective pps sampling and its application to clustering are reviewed in Section 2. Section 3 presents a statement of our one2all theorem and some implications. Section 4 provides a full proof of our one2all Theorem."}, {"heading": "2 Multi-objective pps samples for clustering", "text": "We review the framework of weighted and multi-objective weighted sampling in our context of clustering costs. Consider approximating the clustering cost V (Q | X,w) from a sample S of X. For probabilities px > 0 for x \u2208 X and a sample S drawn according to these probabilities, we have the unbiased inverse probability estimator [19] of V (Q | X,w):\nV\u0302 (Q | X,w) = \u2211 x\u2208S wx dxQ px = V (Q | S, {wx/px}) . (3)\nNote that the estimate is equal to the clustering cost of S with weights wx/px by Q."}, {"heading": "2.1 Probability proportional to size (pps) sampling", "text": "[17] To obtain guarantees on the estimate quality of the clustering cost by Q, we need to use weighted sampling. The pps base probabilities for x \u2208 X are\n\u03c8x(Q | X,w) = wxdxQ\u2211 y\u2208X wydyQ . (4)\nThe pps probabilities for a sample with size parameter r > 1 are\n\u03c8(r)x (Q | X,w) = min{1, r\u03c8x(Q | X,w)} .\nWith pps sampling we obtain the following guarantees:\nTheorem 2.1 Consider a sample S where each x \u2208 X is included (independently or using VarOpt dependent sampling [7, 12]) with probability px \u2265 \u03c8(r)x (Q | X,w). Then the estimate (3) has the following statistical guarantees:\n\u2022 The coefficient of variation (CV), defined as the ratio of the standard deviation to the mean, (measure of the \u201crelative error\u201d) is at most = 1/ \u221a r.\n\u2022 The estimate is well concentrated in the Chernoff-Bernstein sense: The probability of relative error larger than c decreases exponentially with c.\nNote that the (expected) sample size is \u2211 x px. When px = \u03c8 (r) x (Q | X,w), the size is at most r."}, {"heading": "2.2 Multi-objective pps sampling", "text": "When we seek estimates with statistical guarantees for a set Q of queries (for example, all sets of k points in the metric spaceM), we use multi-objective samples [13, 11]. The multi-objective (MO) pps base sampling probabilities are defined as the maximum of the pps base probabilities over Q \u2208 Q:\n\u03c8x(Q | X,w) = max Q\u2208Q \u03c8x(Q | X,w) . (5)\nFor a size parameter r, the multi-objective pps probabilities are\n\u03c8(r)x (Q | X,w) = min{1, r\u03c8x(Q | X,w)} = max Q\u2208Q \u03c8(r)x (Q | X,w) .\nAs a corollary of Theorem 2.1, noting that\n\u2200Q \u2208 Q, \u03c8x(Q | X,w) \u2265 \u03c8x(Q | X,w) ,\nwe obtain the following.\nCorollary 2.1 Consider a sample of X where each x \u2208 X is included (independently or using VarOpt dependent sampling) with probability px \u2265 min{1, r\u03c8x(Q | X)}. Then for each Q \u2208 Q the inverse probability estimator (3) has the statistical guarantees stated in Theorem 2.1\nWith some notation abuse, we define the overhead of multi-objective sampling Q to be:\nh(Q | X,w) \u2261 h(\u03c8(Q | X,w)) = \u2211 x\u2208X \u03c8x(Q | X,w) .\nNote that with size parameter r, the multi-objective pps sample size is at most rh(Q | X,w), so h(Q | X,w) bounds the factor-increase in sample size due to the \u201cmulti-objectiveness\u201d of the sample.\nSometimes we can not compute \u03c8 exactly but can instead efficiently obtain upper bounds \u03c0 \u2265 \u03c8(Q | X,w). Accordingly, we use sampling probabilities \u03c0(r)x = min{1, r\u03c0x}. The use of upper bounds increases the sample size, and we refer to h(\u03c0) = \u2211 x \u03c0x as the overhead of \u03c0. We therefore seek upper-bounds \u03c0 with a overhead not much larger than h(Q | X,w). Note that when sampling with the upper-bounds \u03c0, we retain the statistical guarantees on estimation quality as stated in Theorem 5."}, {"heading": "2.3 Optimization over multi-objective pps samples", "text": "We now consider clustering, with an approximate or bicriteria-approximate objective. We would like to perform this optimization by running a clustering algorithm A on the sample S with weights wx/px.\nA multi-objective pps sample for Q with size parameter r = \u22122 provides ForEach guarantees. This guarantee is not sufficient to ensure that Q returned by A satisfies the quality requirements of A, even approximately, over X, but it does allow us to test that using:\nV (Q | X,w) \u2264 (1 + )V (Q | S, {wx/px}) . (6)\nTo perform the test, we can compute the exact cost V (Q | X,w). Alternatively, we can simply draw a different, independent, ForEach sample S\u2032, using the same distribution, and compute the estimate V (Q | S\u2032, {wx/px}).\nIf the test is satisfied, we know (within the ForEach guarantees and the approximation quality guarantee provided by A) that Q is an approximate solution over X. If the condition (6) does not hold, it still provides us with an approximate lower bound of the solution over X. We can increase the sample size and apply A to the larger sample. A pseudocode for this wrapper is provided as Algorithm 1.\nAlgorithm 1 Wrapper for clustering over samples Input: points X, weights w > 0, (upper bounds on) base pps probabilities \u03c0 for Q, > 0, an approximate clustering\nalgorithm A that inputs a weighted set of points and returns Q \u2208 Q that satisfies some (bi-criteria) quality guarantees.\n// Initialization foreach x \u2208 X do // for sampling\nux \u223c U [0, 1] r \u2190 \u22122 // Start with ForEach guarantee // Main Loop repeat\nS \u2190\u22a5 // Initialize sample foreach x \u2208 X such that ux \u2264 r\u03c0x do // sample points\nS \u2190 S \u222a {x} // Cluster the sample S foreach x \u2208 S do // weights for sampled points\nw\u2032x \u2190 wx/min{1, r\u03c0x} Q\u2190 A(S,w) // Apply approximate clustering algorithm A to sample r \u2190 2r // Double the sample size parameter until V (Q | X,w)) \u2264 (1 + )V (Q | S,w\u2032) // Exact or approx using a validation sample\nreturn Q"}, {"heading": "3 The one2all Theorem statement and implications", "text": "Consider a relaxed metric spaceM where distances satisfy all properties of a metric space except that the triangle inequality is relaxed using a parameter \u03c1 \u2265 1:\n\u2200x, y, z \u2208M, dxy \u2264 \u03c1(dxz + dzy) . (7)\nFor v > 0, we use the notation Q(v) = {Q | V (Q | X,w) \u2265 v}\nfor the set of all subsets Q \u2282M that cluster X with cost at least v. For a set M , we denote by\nXq = {x \u2208 X | dxq = dxM}\nthe points in X that are closest to q \u2208 M . In case of ties we apply arbitrary tie breaking to ensure that Xq q \u2208M is a partition of X. We will assume that Xq is not empty for all q \u2208M , since otherwise, we can remove the point q from M without affecting the clustering cost of X by M .\nFor the set M , we define the probabilities \u03c0(M) \u2261 \u03c0(M |X,w) as follows:\n\u2200m \u2208M, \u2200x \u2208 Xm, \u03c0(M |X,w)x = min { 1,max { 2\u03c1\ndxM V (M | X,w) , 8\u03c12\nw(Xm)\n}} . (8)\nWe are now ready to state our one2all Theorem: We show that 1\u03b1\u03c0 (M |X,w) upper bound the multi-objective pps base probabilities for Q(v) for v = \u03b1V (M | X,w). The full proof of the Theorem is provided in the next section.\nTheorem 3.1 LetM be a relaxed metric space with parameter \u03c1, X \u2282M be finite set of points with weights w, and M \u2282M be a finite set of points. Let v = V (M | X,w) Then for all \u03b1 \u2264 1,\nmin{1, 1 \u03b1 \u03c0(M |X,w)} \u2265 \u03c8(Q(\u03b1v) | X,w) .\nCorollary 3.1 The multi-objective overhead of the set of all sets of centers Q with clustering cost at least v = \u03b1V (M | X,w) is at most h(Q(v) \u2264 1\u03b1 (8\u03c1 2|M |+ 2\u03c1). Proof Note that the sum \u2211 x \u03c0 (M |X,w) x \u2264 8\u03c12|M |+ 2\u03c1\nCorollary 3.2 For k \u2265 1, let Q be the set of all k-subsets of points in a relaxed metric space M with parameter \u03c1. The multi-objective pps overhead of Q satisfies\nh(Q) \u2264 8\u03c12k + 2\u03c1 .\nProof We apply Theorem 3.1 with M being the k-means optimum and \u03b1 = 1."}, {"heading": "4 Proof of the one2all Theorem", "text": "To prove Theorem 3.1, we need to show that \u2200Q such that V (Q | X,w) \u2265 \u03b1V (M | X,w) and \u2200x \u2208 X,\ndxQ V (Q | X,w) \u2264 1 \u03b1 \u03c0(M |X,w)x . (9)\nWe will do a case analysis, as illustrated in Figure 1. We first consider Q and points x such that the distance of x to Q is not much larger than the distance of x to M . Property (9) follows using the first term of the maximum in (8) by applying the following Lemma with c = 2\u03c1:\nLemma 4.1 Let Q and x be such that dxQ \u2264 cdxM . Then\ndxQ V (Q | X,w) \u2264 c \u03b1 dxM V (M | X,w) .\nProof Since Q \u2208 Q(V (M)), V (Q | X,w) \u2265 \u03b1V (M | X,w). Therefore\ndxQ V (Q | X,w) \u2264 1 \u03b1 dxQ V (M | X,w) \u2264 c \u03b1 dxM V (M | X,w) .\nIt remains to consider the complementary case where point x is much closer to M than to Q:\ndxQ \u2265 2\u03c1dxM . (10)\nWe first introduce a useful definition: For a point q \u2208 M , we denote by \u2206q the weighted median of the distances dqy for y \u2208 Xq, weighted by wy. The median \u2206q is a value that satisfies the following two conditions:\u2211\nx\u2208Xq|dxq\u2264\u2206q\nwx \u2265 1\n2 w(Xq) (11)\n\u2211 x\u2208Xq|dxq\u2265\u2206q wx \u2265 1 2 w(Xm) . (12)\nIt follows from (12) that for all q \u2208M ,\nV (M | Xq,w) = \u2211 x\u2208Xq wxdqx \u2265 \u2211 x\u2208Xq|dxq\u2265\u2206q wxdxq \u2265 \u2206q \u2211 x\u2208Xq|dxq\u2265\u2206q wx \u2265 1 2 w(Xm)\u2206q .\nTherefore,\nV (M | X,w) = \u2211 q\u2208M V (M | Xq,w) \u2265 1 2 \u2211 q\u2208M w(Xm)\u2206q . (13)\nWe now return to our proof for x that satisfies (10). We will show that property (9) holds using the second term in the max operation in the definition (8). Specifically, let m be the closest M point to x. We will show that\ndxQ V (Q | X,w) \u2264 8\u03c1 2 \u03b1\n1\nw(Xm) . (14)\nWe divide the proof to two subcases, in the two following Lemmas, each covering the complement of the other: When dmQ \u2265 2\u03c1\u2206m and when dmQ \u2264 2\u03c1\u2206m.\nLemma 4.2 Let Q and point x be such that\n\u2203m \u2208M, dmx < 1\n2\u03c1 dxQ and dmQ \u2265 2\u03c1\u2206m .\nThen dxQ\nV (Q | X,w) \u2264 8\u03c1\n2\nw(Xm) .\nProof Let q = arg minz\u2208Q dmz be the closest Q point to m. From (relaxed) triangle inequality (7) and our assumptions:\ndxQ \u2264 dxq \u2264 \u03c1(dmq + dmx) = \u03c1(dmQ + dmx) \u2264 \u03c1dmQ + 1\n2 dxQ .\nRearranging, we get dxQ \u2264 2\u03c1dmQ . (15)\nConsider a point y such that dmy \u2264 \u2206m. Let q\u2032 = arg minz\u2208Q dyz be the closest Q point to y. From relaxed triangle inequality we have dmq\u2032 \u2264 \u03c1(dyq\u2032 + dym) and therefore\ndyQ = dyq\u2032 \u2265 1\n\u03c1 dmq\u2032 \u2212 dym \u2265\n1 \u03c1 dmQ \u2212\u2206m \u2265 1 \u03c1 dmQ \u2212 1 2\u03c1 dmQ \u2265 1 2\u03c1 dmQ .\nThus, using the definition of \u2206m (11):\nV (Q | X,w) \u2265 \u2211\ny|dyQ\u2264\u2206m\nwydyQ \u2265 1\n2\u03c1 \u2211 y|dyQ\u2264\u2206m wydmQ\n\u2265 1 2\u03c1 dmQ \u2211 y\u2208Xm|dyQ\u2264\u2206m wy \u2265 1 2\u03c1 dmQ w(Xm) 2 = 1 4\u03c1 dmQw(Xm) . (16)\nCombining (15) and (16) we obtain:\ndxQ V (Q | X,w) \u2264 2\u03c1dmQ1 4\u03c1w(Xm)dmQ = 8\u03c12 1 w(Xm) .\nLemma 4.3 Let Q and point x be such that\n\u2203m \u2208M, dxm < 1\n2\u03c1 dxQ and dmQ \u2264 2\u03c1\u2206m .\nThen dxQ\nV (Q | X,w) \u2264 8\u03c1\n2\n\u03b1\n1\nw(Xm) .\nProof Let q = arg minz\u2208Q dzm be the closest Q point to m. We have\ndxQ \u2264 dxq \u2264 \u03c1(dxm + dmq) \u2264 1\n2 dxQ + \u03c1dmQ \u2264\n1 2 dxQ + 2\u03c1 2\u2206m\nTherefore, dxQ \u2264 4\u03c12\u2206m . (17)\nUsing (13) we obtain\nV (Q | X,w) \u2265 \u03b1V (M | X,w) \u2265 \u03b11 2 \u2211 y\u2208M w(Xy)\u2206y \u2265 \u03b1 1 2 w(Xm)\u2206m . (18)\nCombining (17) and (18) we obtain\ndxQ V (Q | X,w) \u2264 4\u03c1 2\u2206m\n1 2\u03b1w(Xm)\u2206m\n\u2264 8\u03c1 2\n\u03b1\n1\nw(Xm) ."}, {"heading": "5 Conclusion", "text": "We presented the one2all construction for clustering costs: From any set of centers M with cost V (M) we obtain a summary structure of size O(|M |), in the form of a multi-objective sample, with which we can estimate the clustering cost by any set Q with cost that is at least a fraction of V (M).\nLooking forward, we point on some potential applications beyond estimation and optimization of clustering cost. First, we note that the set of distances of Q to the one2all sample S is essentially a sketch of the full (weighted) distance vector of Q to X [13]. Sketches of different sets Q allow us to estimate relations between the respective full vectors, such as distance norms, weighted Jaccard similarity, quantile aggregates, and more, which can be useful building blocks in other applications. Second, recent work casted Euclidean k-means clustering as a constrained rank-k approximation problem [14]. This connection facilitated interesting feedback between techniques designed for low-rank approximation and for clustering. We thus hope that our technique and insights might lead to further progress on other low-rank approximation problems."}], "references": [{"title": "Geometric approximation via coresets", "author": ["P.K. Agarwal", "S. Har-Peled", "K.R. Varadarajan"], "venue": "Combinatorial and computational geometry, MSRI. University Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "NP-hardness of Euclidean sum-of-squares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Mach. Learn., 75(2)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "The hardness of approximation of Euclidean k-means", "author": ["P. Awasthi", "M. Charikar", "R. Krishnaswamy", "A.K. Sinop"], "venue": "SoCG", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "New frameworks for offline and streaming coreset constructions", "author": ["V. Braverman", "D. Feldman", "H. Lang"], "venue": "CoRR, abs/1612.00889", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Selecting several samples from a single population", "author": ["K.R.W. Brewer", "L.J. Early", "S.F. Joyce"], "venue": "Australian Journal of Statistics, 14(3):231\u2013239", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1972}, {"title": "A general purpose unequal probability sampling plan", "author": ["M.T. Chao"], "venue": "Biometrika, 69(3):653\u2013656", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1982}, {"title": "Average distance queries through weighted samples in graphs and metric spaces: High scalability with tight statistical guarantees", "author": ["S. Chechik", "E. Cohen", "H. Kaplan"], "venue": "RANDOM. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "On coresets for k-median and k-means clustering in metric and Euclidean spaces and their applications", "author": ["K. Chen"], "venue": "SIAM J. Comput., 39(3)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Size-estimation framework with applications to transitive closure and reachability", "author": ["E. Cohen"], "venue": "J. Comput. System Sci., 55:441\u2013453", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Multi-objective weighted sampling", "author": ["E. Cohen"], "venue": "HotWeb. IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient stream sampling for variance-optimal estimation of subset sums", "author": ["E. Cohen", "N. Duffield", "C. Lund", "M. Thorup", "H. Kaplan"], "venue": "SIAM J. Comput., 40(5)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Coordinated weighted sampling for estimating aggregates over multiple weight assignments", "author": ["E. Cohen", "H. Kaplan", "S. Sen"], "venue": "VLDB, 2(1\u20132)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "author": ["M.B. Cohen", "S. Elder", "C. Musco", "C. Musco", "M. Persu"], "venue": "STOC. ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A unified framework for approximating and clustering data", "author": ["D. Feldman", "M. Langberg"], "venue": "STOC. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means", "author": ["D. Feldman", "M. Schmidt", "C. Sohler"], "venue": "PCA and projective clustering. In SODA. ACM-SIAM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "On the theory of sampling from finite populations", "author": ["M.H. Hansen", "W.N. Hurwitz"], "venue": "Ann. Math. Statist., 14(4)", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1943}, {"title": "On coresets for k-means and k-median clustering", "author": ["S. Har-Peled", "S. Mazumdar"], "venue": "STOC. ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "A generalization of sampling without replacement from a finite universe", "author": ["D.G. Horvitz", "D.J. Thompson"], "venue": "Journal of the American Statistical Association, 47(260):663\u2013685", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1952}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Computational Geometry, 28(2)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Retaining units after changing strata and probabilities", "author": ["L. Kish", "A. Scott"], "venue": "Journal of the American Statistical Association, 66(335):pp. 461\u2013470", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1971}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Trans. Inf. Theor.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1982}, {"title": "Optimal time bounds for approximate clustering", "author": ["R.R. Mettu", "C.G. Plaxton"], "venue": "Mach. Learn., 56(1-3)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Fixed sample size pps approximations with a permanent random number", "author": ["P.J. Saavedra"], "venue": "Proc. of the Section on Survey Research Methods, pages 697\u2013700, Alexandria, VA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Model Assisted Survey Sampling", "author": ["C-E. S\u00e4rndal", "B. Swensson", "J. Wretman"], "venue": "Springer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1992}, {"title": "Sampling Algorithms", "author": ["Y. Till\u00e9"], "venue": "Springer-Verlag, New York", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "A constant-factor bi-criteria approximation guarantee for k-means++", "author": ["D. Wei"], "venue": "NIPS", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Clustering is computationally hard [2] even on Euclidean spaces and even to tightly approximate [4].", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "Clustering is computationally hard [2] even on Euclidean spaces and even to tightly approximate [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 19, "context": "There is a local search polynomial algorithm with 9 + approximation ratio [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "In practice, clustering is solved using heuristics, most notably Lloyd\u2019s algorithm [22], and scalable approximation algorithms such as kmeans++ [3], which obtains a log k factor approximation using k centers.", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "In practice, clustering is solved using heuristics, most notably Lloyd\u2019s algorithm [22], and scalable approximation algorithms such as kmeans++ [3], which obtains a log k factor approximation using k centers.", "startOffset": 144, "endOffset": 147}, {"referenceID": 26, "context": "A recent result [27] established that kmeans++ provides constant factors bi-criteria guarantees: When applied to obtain \u03b2k centers for some constant \u03b2 > 1, the clustering cost is within a constant factor of the optimum k-means cost.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "The term coresets for such structures was introduced in the computational geometry literature [1, 18], building on the theory of -nets.", "startOffset": 94, "endOffset": 101}, {"referenceID": 17, "context": "The term coresets for such structures was introduced in the computational geometry literature [1, 18], building on the theory of -nets.", "startOffset": 94, "endOffset": 101}, {"referenceID": 22, "context": "Some notable constructions include [23, 9, 15, 16].", "startOffset": 35, "endOffset": 50}, {"referenceID": 8, "context": "Some notable constructions include [23, 9, 15, 16].", "startOffset": 35, "endOffset": 50}, {"referenceID": 14, "context": "Some notable constructions include [23, 9, 15, 16].", "startOffset": 35, "endOffset": 50}, {"referenceID": 15, "context": "Some notable constructions include [23, 9, 15, 16].", "startOffset": 35, "endOffset": 50}, {"referenceID": 4, "context": "Initial coresets constructions had bounds with high (exponential or high polynomial) dependence on some parameters (dimension, , k) and poly logarithmic dependence on n, with the best current asymptotic bound of O(k \u22122 log k log n) claimed in [5].", "startOffset": 243, "endOffset": 246}, {"referenceID": 14, "context": "While a notion of \u201cweak coresets\u201d that aim to only support optimization was considered in the coreset literature [15], the constructions are also worst-case.", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 145, "endOffset": 153}, {"referenceID": 10, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 145, "endOffset": 153}, {"referenceID": 20, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 212, "endOffset": 227}, {"referenceID": 5, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 212, "endOffset": 227}, {"referenceID": 23, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 212, "endOffset": 227}, {"referenceID": 9, "context": "Contribution Overview We propose summary structures for clustering costs based on multi-objective probability-proportional-to-size (pps) samples [13, 11], which build on the classic notion of sample coordination [21, 6, 24, 10].", "startOffset": 212, "endOffset": 227}, {"referenceID": 24, "context": "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample \u22122 points with probabilities px \u221d wxdQx proportional to their contribution to the sum [17].", "startOffset": 32, "endOffset": 40}, {"referenceID": 25, "context": "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample \u22122 points with probabilities px \u221d wxdQx proportional to their contribution to the sum [17].", "startOffset": 32, "endOffset": 40}, {"referenceID": 16, "context": "The theory of weighted sampling [25, 26] tells us that to estimate the sum V (Q | X,w) it suffices to sample \u22122 points with probabilities px \u221d wxdQx proportional to their contribution to the sum [17].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "The inverse-probability [19] estimate obtained from the sample S, V\u0302 (Q | X,w) \u2261 V (Q | S, {wx/px}) , is an unbiased estimate of V (Q | X,w) with well-concentrated normalized squared error of .", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "Our result generalizes previous work [8] that only applied to the case where k = 1, where clustering cost reduces to inverse classic closeness centrality (sum of distances from a single point Q).", "startOffset": 37, "endOffset": 40}, {"referenceID": 2, "context": "To obtain an efficient construction, we can apply kmeans++ [3] or another efficient bi-criteria approximation algorithm to compute M of size 2k that has cost that is at most the optimum divided by some constant \u03b1 [27].", "startOffset": 59, "endOffset": 62}, {"referenceID": 26, "context": "To obtain an efficient construction, we can apply kmeans++ [3] or another efficient bi-criteria approximation algorithm to compute M of size 2k that has cost that is at most the optimum divided by some constant \u03b1 [27].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "We use an optimization framework over multi-objective samples [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "For probabilities px > 0 for x \u2208 X and a sample S drawn according to these probabilities, we have the unbiased inverse probability estimator [19] of V (Q | X,w):", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "1 Probability proportional to size (pps) sampling [17] To obtain guarantees on the estimate quality of the clustering cost by Q, we need to use weighted sampling.", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "1 Consider a sample S where each x \u2208 X is included (independently or using VarOpt dependent sampling [7, 12]) with probability px \u2265 \u03c8 x (Q | X,w).", "startOffset": 101, "endOffset": 108}, {"referenceID": 11, "context": "1 Consider a sample S where each x \u2208 X is included (independently or using VarOpt dependent sampling [7, 12]) with probability px \u2265 \u03c8 x (Q | X,w).", "startOffset": 101, "endOffset": 108}, {"referenceID": 12, "context": "2 Multi-objective pps sampling When we seek estimates with statistical guarantees for a set Q of queries (for example, all sets of k points in the metric spaceM), we use multi-objective samples [13, 11].", "startOffset": 194, "endOffset": 202}, {"referenceID": 10, "context": "2 Multi-objective pps sampling When we seek estimates with statistical guarantees for a set Q of queries (for example, all sets of k points in the metric spaceM), we use multi-objective samples [13, 11].", "startOffset": 194, "endOffset": 202}, {"referenceID": 0, "context": "// Initialization foreach x \u2208 X do // for sampling ux \u223c U [0, 1] r \u2190 \u22122 // Start with ForEach guarantee // Main Loop repeat S \u2190\u22a5 // Initialize sample foreach x \u2208 X such that ux \u2264 r\u03c0x do // sample points S \u2190 S \u222a {x} // Cluster the sample S foreach x \u2208 S do // weights for sampled points w\u2032 x \u2190 wx/min{1, r\u03c0x} Q\u2190 A(S,w) // Apply approximate clustering algorithm A to sample r \u2190 2r // Double the sample size parameter until V (Q | X,w)) \u2264 (1 + )V (Q | S,w\u2032) // Exact or approx using a validation sample", "startOffset": 58, "endOffset": 64}, {"referenceID": 12, "context": "First, we note that the set of distances of Q to the one2all sample S is essentially a sketch of the full (weighted) distance vector of Q to X [13].", "startOffset": 143, "endOffset": 147}, {"referenceID": 13, "context": "Second, recent work casted Euclidean k-means clustering as a constrained rank-k approximation problem [14].", "startOffset": 102, "endOffset": 106}], "year": 2017, "abstractText": "Clustering is a fundamental technique in data analysis. Consider data points X that lie in a (relaxed) metric space (where the triangle inequality can be relaxed by a constant factor). Each set of points Q (centers) defines a clustering of X according to the closest center with cost V (Q) = \u2211 x\u2208X dxQ. This formulation generalizes classic k-means clustering, which uses squared distances. Two basic tasks, parametrized by k \u2265 1, are cost estimation, which returns (approximate) V (Q) for queries Q such that |Q| = k and clustering, which returns an (approximate) minimizer of V (Q) of size |Q| = k. With very large data sets X, we seek efficient constructions of small summaries that allow us to efficiently approximate clustering costs over the full data. We present a novel data reduction tool based on multi-objective probability-proportional-to-size (pps) sampling: Our one2all construction inputs any set of centers M and efficiently computes a sample of size O(|M |) from which we can tightly estimate the clustering cost V (Q) for any Q that has at least a fraction of the clustering cost of M . For cost queries, we apply one2all to a bicriteria approximation to obtain a sample of size O(k \u22122) for all |Q| = k. For clustering, we propose a wrapper that applies a black-box algorithm to a sample and tests clustering quality over X, adaptively increasing the sample size. Our approach exploits the structure of the data to provide quality guarantees through small samples, without the use of typically much larger worst-case-size summaries.", "creator": "LaTeX with hyperref package"}}}