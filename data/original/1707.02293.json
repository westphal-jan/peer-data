{"id": "1707.02293", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jul-2017", "title": "Bayesian Models of Data Streams with Hierarchical Power Priors", "abstract": "Making inferences from data streams is a pervasive problem in many modern data analysis applications. But it requires to address the problem of continuous model updating and adapt to changes or drifts in the underlying data generating distribution. In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models. Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models. The approach is validated on three real data sets over three latent variable models.", "histories": [["v1", "Fri, 7 Jul 2017 09:44:15 GMT  (5603kb,D)", "http://arxiv.org/abs/1707.02293v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["andr\u00e9s r masegosa", "thomas d nielsen", "helge langseth", "dar\u00edo ramos-l\u00f3pez", "antonio salmer\u00f3n", "anders l madsen"], "accepted": true, "id": "1707.02293"}, "pdf": {"name": "1707.02293.pdf", "metadata": {"source": "CRF", "title": "Bayesian Models of Data Streams with Hierarchical Power Priors", "authors": ["Andr\u00e9s Masegosa", "Thomas D. Nielsen", "Helge Langseth", "Dar\u0131\u0301o Ramos-L\u00f3pez", "Antonio Salmer\u00f3n", "Anders L. Madsen"], "emails": ["<andresmasegosa@ual.es>."], "sections": [{"heading": "1. Introduction", "text": "Flexible and computationally efficient models for streaming data are required in many machine learning applications, and in this paper we propose a new class of models for these situations. Specifically, we are interested in models suitable for domains that exhibit changes in the underlying generative process (Gama et al., 2014). We envision a situation, where one receives batches of data at discrete points in time. As each new batch arrives, we want to glean information from the new data, while also retaining relevant information from the historical observations.\nOur modelling is inspired by previous works on Bayesian recursive estimation (O\u0308zkan et al., 2013; Ka\u0301rny\u0300, 2014), power priors (Ibrahim & Chen, 2000) and exponential for-\n1Department of Mathematics, Unversity of Almer\u0131\u0301a, Almer\u0131\u0301a, Spain 2Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway 3Department of Computer Science, Aalborg University, Aalborg, Denmark 4Hugin Expert A/S, Aalborg, Denmark. Correspondence to: Andre\u0301s Masegosa <andresmasegosa@ual.es>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ngetting approaches (Honkela & Valpola, 2003). However, all of these methods were developed for slowly changing processes, where the rate of change anticipated by the model is controlled by a quantity that must be set manually. Our solution, on the other hand, can accommodate both gradual and abrupt concept drift by continuously assessing the similarity between new and historic data using a fully Bayesian paradigm.\nBuilding Bayesian models for data streams raises computational problems, as data may arrive with high velocity and is unbounded in size. We therefore develop an approximate variational inference technique based on a novel lower-bound of the data likelihood function. The appropriateness of the approach is investigated through experiments using both synthetic and real-life data, giving encouraging results. The proposed methods are released as part of an open-source toolbox for scalable probabilistic machine learning (http://www.amidsttoolbox.com) (Masegosa et al., 2017; 2016b; Caban\u0303as et al., 2016)."}, {"heading": "2. Preliminaries", "text": "In this paper we focus on conjugate exponential Bayesian network models for performing Bayesian learning on streaming data. To simplify the presentation, we shall initially focus on the model structure shown in Figure 1 (a). This model includes the observed data x = xi=1:N , global hidden variables (or parameters) \u03b2 = \u03b21:M , a set of local hidden variables z = z1:N , and a vector of fixed (hyper) parameters denoted by \u03b1. Notice how the dynamics of the process is not included in the model of Figure 1 (a); the model will be set in the context of data streams in Section 4, where we extend it to incorporate explicit dynamics over the (global) parameters to capture concept drift.\nWith the conditional distributions in the model belonging to the exponential family, we have that all distributions are of the following form\nln p(Y |pa(Y )) = lnhY + \u03b7Y (pa(Y ))\nT tY (Y )\u2212 aY (\u03b7Y (pa(Y ))), where pa(Y ) denotes the parents of Y in the directed acyclic graph of the induced Bayesian network model. The scalar functions hY and aY (\u00b7) are the base measure and\nar X\niv :1\n70 7.\n02 29\n3v 1\n[ cs\n.L G\n] 7\nJ ul\n2 01\n7\nthe log-normalizer, respectively; the vector functions \u03b7Y (\u00b7) and tY (\u00b7) are the natural parameters and the sufficient statistics vectors, respectively. The subscript Y means that the associated functional forms may be different for the different factors of the model, but we may remove the subscript when clear from the context. By also requiring that the distributions are conjugate, we have that the posterior distribution for each variable in the model has the same functional form as its prior distribution. Consequently, learning (i.e. conditioning the model on observations) only changes the values of the parameters of the model, and not the functional form of the distributions.\nVariational inference is a deterministic technique for finding tractable posterior distributions, denoted by q, which approximates the Bayesian posterior, p(\u03b2, z|x), that is often intractable to compute. More specifically, by letting Q be a set of possible approximations of this posterior, variational inference solves the following optimization problem for any model in the conjugate exponential family:\nmin q(\u03b2,z)\u2208Q\nKL(q(\u03b2, z)|p(\u03b2, z|x)), (1)\nwhere KL denotes the Kullback-Leibler divergence between two probability distributions.\nIn the mean field variational approach the approximation familyQ is assumed to fully factorize. Extending the notation of Hoffman et al. (2013), we have that\nq(\u03b2, z|\u03bb,\u03c6) = M\u220f\nk=1\nq(\u03b2k|\u03bbk) N\u220f\ni=1\nJ\u220f\nj=1\nq(zi,j |\u03c6i,j),\nwhere J is the number of local hidden variables, which is assumed fixed for all i = 1, . . . , N . The parameterizations of the variational distributions are made explicit, in that \u03bb parameterize the variational distribution of \u03b2, while \u03c6 has the same role for the variational distribution of z.\nTo solve the minimization problem in Equation (1), the\nvariational approach exploits the transformation\nlnP (x) = L(\u03bb,\u03c6|x,\u03b1u) + KL(q(\u03b2, z|\u03bb,\u03c6)|p(\u03b2, z|x)), (2) where L(\u00b7|\u00b7) is a lower bound of lnP (x) since KL is nonnegative. x and \u03b1u are introduced in L\u2019s notation to make explicit the function\u2019s dependency on x, the data sample, and \u03b1u, the natural parameters of the prior over \u03b2. As lnP (x) is constant, minimizing the KL term is equivalent to maximizing the lower bound. Variational methods maximize this lower bound by applying a coordinate ascent that iteratively updates the individual variational distributions while holding the others fixed (Winn & Bishop, 2005). The key advantage of having a conjugate exponential model is that the gradients of theL function can be always computed in closed form (Winn & Bishop, 2005)."}, {"heading": "3. Related Work", "text": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation. The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category. Alternatively, one could cast the inference problem as a stochastic optimization problem. Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group. SVI assumes the existence of a fixed data set observed in a sequential manner, and in particular that this data set has a known finite size. This is unrealistic when modeling data streams. PVB addresses this problem by using the frequentist notion of a population distribution, F, which is assumed to generate the data stream by repeatedly sampling M data points at the time. M parameterizes the size of the population, and helps control the variance of the population posterior. Unfortunately, M must be specified by the user. No clear rule exists regarding how to set it, and McInerney et al. (2015) show that its optimal value may differ from one data stream to another.\nThe problem of Bayesian modeling of non-stationary data streams (i.e., with concept drift (Gama et al., 2014)) is not addressed by SVB, as it assumes data exchangeability. An online variational inference method, which exponentially forgets the variational parameters associated with old data, was proposed by Honkela & Valpola (2003). The so-called power prior approach (Ibrahim & Chen, 2000) is also based on an exponential forgetting mechanisms, and has nice theoretical properties (Ibrahim et al., 2003). Nevertheless, both approaches rely on a hyper-parameter determining forgetting, which has to be set manually. PVB can\nalso adapt to concept drift, because the variance of the variational posterior never decreases below a given threshold indirectly controlled by M , but again, the hyper-parameter has to be set manually.\nA time series based modeling approach for concept drift using implicit transition models was pursued by O\u0308zkan et al. (2013); Ka\u0301rny\u0300 (2014). Unfortunately, the implicit transition model depends on a hyper-parameter determining the forgetting-factor, which has to be manually set. In this paper we build on this approach, adapt it to variational settings, and place a hierarchical prior on its forgetting parameter. This greatly improves the flexibility and accuracy of the resulting model when making inferences over drifting data streams."}, {"heading": "4. Hierarchical Power Priors", "text": "In this section we extend the model in Figure 1 (a) to also account for the dynamics of the data stream being modeled. We shall here assume that only the parameters \u03b2 in Figure 1 (a) are time-varying, which we will indicate with the subscript t, i.e., \u03b2t. First we briefly describe the approach on which the proposed model is based. Afterwards, we introduce the hierarchical power prior and detail a variational inference procedure for this model class."}, {"heading": "4.1. Power Priors as Implicit Transition Models", "text": "In order to extend the model in Figure 1 (a) to data streams, we may introduce a transition model p(\u03b2t|\u03b2t\u22121) to explicitly model the evolution of the parameters over time, enabling the estimation of the predictive density at time t:\np(\u03b2t|x1:t\u22121) = \u222b p(\u03b2t|\u03b2t\u22121)p(\u03b2t\u22121|x1:t\u22121)d\u03b2t\u22121.\n(3) However, this approach introduces two problems. First of all, in non-stationary domains we may not have a single transition model or the transition model may be unknown. Secondly, if we seek to position the model within the conjugate exponential family in order to be able to compute the gradients of L in closed-form, we need to ensure that the distribution family for \u03b2t is its own conjugate distribution, thereby severely limiting model expressivity (we can, e.g., not assign a Dirichlet distribution to \u03b2t).\nRather than explicitly modeling the evolution of the \u03b2t parameters as in Equation (3), we instead follow the approach of Ka\u0301rny\u0300 (2014) and O\u0308zkan et al. (2013) who define the time evolution model implicitly by constraining the maximum KL divergence over consecutive parameter distributions. Specifically, by defining\np\u03b4(\u03b2t|x1:t\u22121) = \u222b \u03b4(\u03b2t \u2212 \u03b2t\u22121)p(\u03b2t\u22121|x1:t\u22121)d\u03b2t\u22121\n(4)\none can restrict the space of possible distributions p(\u03b2t|x1:t\u22121), supported by an unknown transition model, by the constraint\nKL(p(\u03b2t|x1:t\u22121), p\u03b4(\u03b2t|x1:t\u22121)) \u2264 \u03ba. (5)\nKa\u0301rny\u0300 (2014) and O\u0308zkan et al. (2013) seek to approximate p(\u03b2t|x1:t\u22121) by the distribution p\u0302(\u03b2t|x1:t\u22121) having maximum entropy under the constraint in (5); for continuous distributions the maximum entropy can be formulated relative to an uninformative prior density pu(\u03b2t), which corresponds to the Kullbach-Leibler divergence between the two distributions. This approach ensures that we will not underestimate the uncertainty in the parameter distribution and the particular solution being sought takes the form\np\u0302(\u03b2t|x1:t\u22121, \u03c1t) \u221d p\u03b4(\u03b2t|x1:t\u22121)\u03c1tpu(\u03b2t)(1\u2212\u03c1t), (6)\nwhere 0 \u2264 \u03c1t \u2264 1 is indirectly defined by (5) which in turn depends on the user defined parameter \u03ba.\nIn our streaming data setting we follow assumed density filtering (Lauritzen, 1992) and the SVB approach (Broderick et al., 2013) and employ the approximation p(\u03b2t\u22121|x1:t\u22121) \u2248 q(\u03b2t\u22121|\u03bbt\u22121), where q(\u03b2t\u22121|\u03bbt\u22121) is the variational distribution calculated in the previous time step. Using this approximation in (3) and (4), we can express p\u03b4 in terms of \u03bbt\u22121 in which case (6) becomes\np\u0302(\u03b2t|\u03bbt\u22121, \u03c1t) \u221d p\u03b4(\u03b2t|\u03bbt\u22121)\u03c1tpu(\u03b2t)(1\u2212\u03c1t), (7)\nwhich we use as the prior density for time step t. Now, if pu(\u03b2t) belong to the same family as q(\u03b2t\u22121|\u03bbt\u22121), then p\u0302(\u03b2t|\u03bbt\u22121, \u03c1t) will stay within the same family and have natural parameters \u03c1t\u03bbt\u22121+(1\u2212\u03c1t)\u03b1u, where \u03b1u are the natural parameters of pu(\u03b2t). Thus, under this approach, the transitioned posterior remains within the same exponential family, so we can enjoy the full flexibility of the conjugate exponential family (i.e. computing gradients of the L function in closed form), an option that would not be available if one were to explicitly specify a transition model as in Equation (3).\nSo, at each time step, we simply have to solve the following variational problem, where only the prior changes with respect to the original SVB approach,\narg max \u03bbt,\u03c6t\nL(\u03bbt,\u03c6t|xt, \u03c1t\u03bbt\u22121 + (1\u2212 \u03c1t)\u03b1u).\nAs stated in the following lemma, this approach coincides with the so-called power priors approach (Ibrahim & Chen, 2000), a term that we will also adopt in the following.\nLemma 1. The Bayesian updating scheme described by Figure 1 (b) and Equation 6, but with \u03c1t fixed to a constant value, is equivalent to the recursive application of\nthe Bayesian updating scheme of power priors (Ibrahim & Chen, 2000). This scheme is expressed as follows:\np(\u03b2|x1,x0, \u03c1) \u221d p(x1|\u03b2)p(x0|\u03b2)\u03c1p(\u03b2),\nwhere x0 and x1 is the observation at time 0 (historical observation) and time 1 (current observation), respectively.\nProof sketch. Translate the recursive Bayesian updating approach of power priors into an equivalent two time slice model, where \u03b20 is given a prior distribution p and p(\u03b21|\u03b20) is a Dirac delta function. The distribution p(\u03b21|x0,x1, \u03c1) in this model is equivalent to p(\u03b2|x1,x0, \u03c1), which, in turn, is equivalent (up to proportionality) to p(x1|\u03b21)p\u0302(\u03b21|x0, \u03c1t). Note that the last p\u0302 term can alternatively be expressed as p\u0302(\u03b21|x0, \u03c1t) \u221d p\u03b4(\u03b21|x0)\u03c1p(\u03b21)1\u2212\u03c1 \u221d p\u03b4(x0|\u03b21)\u03c1p(\u03b21).\nThe perspective provided by Lemma 1 introduces a well known result of power priors, which is also applicable in the current context (see the discussion after Theorem 1 in (Ibrahim et al., 2003)): \u201cthe power prior is an optimal prior to use and in fact minimizes the convex combination of KL divergences between two extremes: one in which no historical data is used and the other in which the historical data and current data are given equal weight.\u201d As noted in (Olesen et al., 1992; O\u0308zkan et al., 2013), this schema works as a moving window with exponential forgetting of past data, where the effective number of samples or, more technically, the so-called equivalent sample size of the posterior (Heckerman et al., 1995), converges to,\nlim t\u2192\u221e ESSt = |xt| 1\u2212 \u03c1 (8)\nif the size of the data batches is constant1.\nFor the experimental results reported in Section 5 we shall refer to the method outlined above as SVB with power priors (SVB-PP)."}, {"heading": "4.2. The Hierarchical Power Prior Model", "text": "In the approach taken by O\u0308zkan et al. (2013) (and, by extension, SVB-PP), the forgetting factor \u03c1t is user-defined. In this paper, we instead pursue a (hierarchical) Bayesian approach and introduce a prior distribution over \u03c1t allowing the distribution over \u03c1t (and thereby the forgetting mechanism) to adapt to the data stream.\nAs we shall see below, in order to support a variational updating scheme we need to restrict the prior distribution over \u03c1t, effectively limiting the choice of prior distribution to\n1For instance, the ESS of a Beta distribution is equal to the sum of the components of \u03bbt and, in turn, equal to the number of data samples seen so far plus the prior\u2019s pseudo-samples.\neither an exponential distribution or a normal distribution with fixed variance, both of which should be truncated to the interval [0, 1]. Unless explicitly stated otherwise, we shall for now assume a truncated exponential distribution with natural parameter \u03b3 as prior distribution over \u03c1t:\np(\u03c1t|\u03b3) = \u03b3 exp(\u2212\u03b3\u03c1t) 1\u2212 exp(\u2212\u03b3) . (9)\nThe resulting model can be illustrated as in Figure 1 (b). We shall refer to models of this type as hierarchical power prior (HPP) models."}, {"heading": "4.3. Variational Updating", "text": "For updating the model distributions we pursue a variational approach, where we seek to maximize the evidence lower bound L in Equation (2) for time step t. However, since the model in Figure 1 (b) does not define a conjugate exponential distribution due to the introduction of p(\u03c1t) we cannot maximize L directly. Instead we will derive a (double) lower bound L\u0302 (L\u0302 \u2264 L) and use this lower bound as a proxy for the updating rules for the variational posteriors.\nFirst of all, by instantiating the lower bound LHPP (\u03bbt,\u03c6t, \u03c9t|xt,\u03bbt\u22121) in Equation (2) for the HPP model we obtain\nLHPP (\u03bbt,\u03c6t, \u03c9t|xt,\u03bbt\u22121) = Eq[ln p(xt,Zt|\u03b2t)] + Eq[ln p\u0302(\u03b2t|\u03bbt\u22121, \u03c1t)] + Eq[p(\u03c1t|\u03b3)]\u2212 Eq[ln q(Zt|\u03c6t)] \u2212 Eq[q(\u03b2t|\u03bbt)]\u2212 Eq[q(\u03c1t|\u03c9t)], (10)\nwhere \u03c9t is the variational parameter for the variational distribution for \u03c1t; as we shall see later, \u03c9t is a scalar and is therefore not shown in boldface. For ease of presentation we shall sometimes drop from LHPP (\u03bbt,\u03c6t, \u03c9t|xt,\u03bbt\u22121) the subscript as well as the explicit specification of the parameters when these is otherwise clear from the context.\nWe now define L\u0302HPP (\u03bbt,\u03c6t, \u03c9t|xt,\u03bbt\u22121) as\nL\u0302HPP (\u03bbt,\u03c6t, \u03c9t|xt,\u03bbt\u22121) = Eq[ln p(xt,Zt|\u03b2t)] + Eq[\u03c1t]Eq[ln p\u03b4(\u03b2t|\u03bbt\u22121)] + (1\u2212 Eq[\u03c1t])Eq[ln pu(\u03b2t)] + Eq[p(\u03c1t|\u03b3)]\u2212 Eq[ln q(Zt|\u03c6t)] \u2212 Eq[q(\u03b2t|\u03bbt)]\u2212 Eq[q(\u03c1t|\u03c9t)], (11)\nwhich provide a lower bound for L. Theorem 1. L\u0302HPP gives a lower bound for LHPP :\nL\u0302HPP (\u03bbt,\u03c6t, \u03c9t|xt,\u03bbt\u22121) \u2264 LHPP (\u03bbt,\u03c6t, \u03c9t|xt,\u03bbt\u22121).\nProof sketch. The inequality derives by using Equation (12) and observing that ag(\u03c1t\u03bbt\u22121 + (1 \u2212 \u03c1t)\u03b1u) \u2264 \u03c1tag(\u03bbt\u22121) + (1\u2212 \u03c1t)ag(\u03b1u) because the log-normalizer\nag is always a convex function (Wainwright et al., 2008). Full details are given in the supplementary material.\nRather than seeking to maximize L we will instead maximize L\u0302. The gap between the two bounds is determined only by the log-normalizer of p\u0302(\u03b2t|\u03bbt\u22121, \u03c1t):\nL\u0302 \u2212 L = Eq[\u03c1tag(\u03bbt\u22121) + (1\u2212 \u03c1t)ag(\u03b1u) + ag(\u03c1t\u03bbt\u22121 + (1\u2212 \u03c1t)\u03b1u)]\n(12)\nThus, maximizing L\u0302 wrt. the variational parameters \u03bbt and \u03c6 also maxmizes L. By the same observation, we also have that the (natural) gradients are consistent relative to the two bounds:\nCorollary 1.\n\u2207\u0302\u03bbtL = \u2207\u0302\u03bbtL\u0302 \u2207\u0302\u03c6tL = \u2207\u0302\u03c6tL\u0302 .\nProof. Follows immediately from Equation (12) because the difference does not depend of \u03bbt and \u03c6t.\nThus, updating the variational parameters\u03bbt and\u03c6t in HPP models can be done as for regular conjugate exponential models of the form in Figure 1.\nIn order to update \u03c9t we rely on L\u0302, which we can maximize using the natural gradient wrt. \u03c9t (Sato, 2001) and which can be calculated in closed form for a restricted distribution family for \u03c1t.\nLemma 2. Assuming that the sufficient statistics function for \u03c1t is the identity function, t(\u03c1t) = \u03c1t, then we have\n\u2207\u0302\u03c9tL\u0302 =KL(q(\u03b2t|\u03bbt), pu(\u03b2t)) \u2212 KL(q(\u03b2t|\u03bbt), p\u03b4(\u03b2t|\u03bbt\u22121)) + \u03b3 \u2212 \u03c9t (13)\nProof sketch. Based on a straightforward algebraic derivation of the gradient using standard properties of the exponential family. Full details are given in the supplementary material.\nNote that the truncated exponential distribution (see Equation (9)) satisfies the restriction expressed in Lemma 2, and also note that the variational posterior q(\u03c1t|\u03c9t) will be a truncated exponential density too.\nOn the other hand, observe that the form of the natural gradient of \u03c9t have an intuitive semantic interpretation, which also extends to the coordinate ascent variational message passing framework (Winn & Bishop, 2005) as shown by Masegosa et al. (2016a). Specifically, using the constant \u03b3 as a threshold, we see that if the uninformed prior pu(\u03b2t) provides a better fit to the variational posterior at time t than the variational parameters \u03bbt from the previous time step (KL(q(\u03b2t|\u03bbt), pu(\u03b2t)) +\n\u03b3 < KL(q(\u03b2t|\u03bbt), p\u03b4(\u03b2t|\u03bbt\u22121))), then we will get a negative value for \u03c9t when performing coordinate ascent using Equation (13). This in turn implies that Eq[\u03c1] < 0.5 because Eq[\u03c1] = 1/(1 \u2212 e\u2212\u03c9t) \u2212 1/\u03c9t (plotted in Figure 2), which means that we have a higher degree of forgetting for past data. If \u03c9t > 0 then Eq[\u03c1] > 0.5 and less past data is forgotten. Figure 2 graphically illustrates this trade-off."}, {"heading": "4.4. The Multiple Hierarchical Power Prior Model", "text": "The HPP model can immediately be extended to include multiple power priors \u03c1(i)t , one for each global parameter \u03b2i. In this model the \u03c1 (i) t \u2019s are pair-wise independent. The latter ensures that optimizing the L\u0302 can be performed as above, since the variational distribution for each \u03c1(i)t can be updated independently of the other variational distributions over \u03c1(j)t , for j 6= i. This extended model allows local model substructures to have different forgetting mechanisms, thereby extending the expressivity of the model. We shall refer to this extended model as a multiple hierarchical power prior (MHPP) model."}, {"heading": "5. Experiments", "text": ""}, {"heading": "5.1. Experimental Set-up", "text": "In this section we will evaluate the following methods:\n\u2022 Streaming variational Bayes (SVB). \u2022 Four versions of Population Variational Bayes\n(PVB)2: Population-size M equal to the average size of each data-batch, or M equal to a fixed value (M = 1000 in Section 5.2 and M = 10 000 in Section 5.3). Learning-rate \u03bd = 0.1 or \u03bd = 0.01. \u2022 Two versions of SVB-PP: \u03c1 = 0.9 or \u03c1 = 0.99. \u2022 Two versions of SVB-HPP: A single shared \u03c1 (de-\nnoted SVB-HPP) or separate \u03c1(i) parameters (SVBMHPP).\nThe underlying variational engine is the VMP algorithm (Winn & Bishop, 2005) for all models; VMP was termi-\n2We do not compare with SVI, because SVI is a special case of PVB when M is equal to the total size of the stream.\nnated after 100 iterations or if the relative increase in the lower bound fell below 0.01%. All priors were uninformative, using either flat Gaussians, flat Gamma priors or uniform Dirichlet priors. We set \u03b3 = 0.1 for the HPP priors. Variational parameters were randomly initialized using the same seed for all methods."}, {"heading": "5.2. Evaluation using an Artificial Data Set", "text": "First, we illustrate the behavior of the different approaches in a controlled experimental setting: We produced an artificial data stream by generating 100 samples (i.e., |xt| = 100) from a Binomial distribution at each time step. We artificially introduce concept drift by changing the parameter p of the Binomial distribution: p = 0.2 for the first 30 time steps, then p = 0.5 for the following 30 time steps, and finally p = 0.8 for the last 40 time steps. The data stream was modelled using a Beta-Binomial model.\nParameter Estimation: Figure 3 shows the evolution of Eq[\u03b2t] for the different methods. We recognize that SVB simply generates a running average of the data, as it is not able to adapt to the concept drift. The results from PVB depend heavily on the learning rate \u03bd, where the higher learning rate, which results in the more aggressive forgetting, works better in this example. Recall, though, that \u03bd needs to be hand-tuned to achieve an optimal performance. As expected, the choice of M does not have an impact, because the present model has no local hidden variables (cf. Section 3). SVB-PP produces results almost identical to PVB when \u03c1 matches the learning rate of PVB (i.e., \u03c1 = 1 \u2212 \u03bd). Finally, SVB-HPP provides the best results, almost mirroring the true model.\nEquivalent Sample Size (ESS): Figure 4 (left) gives the evolution of the equivalent sample size, ESSt, for the different methods 3. The ESS of PVB is always given by the constant M . For SVB, the ESS monotonically increases as more data is seen, while SVB-PP exhibits convergence to the limiting value computed in Equation (8). A different behaviour is observed for SVB-HPP: It is automatically ad-\n3For this model, ESS is simply computed by summing up the components of the \u03bbt defining the Beta posterior.\njusted. Notice that the values for this model is to be read off the alternative y-axis. We can detect the the concept drift, by identifying where the ESS rapidly declines.\nEvolution of Expected Forgetting factor: In Figure 4 (right) the series denoted \u201cE[\u03c1]\u2212100\u201d shows the evolution of Eq[\u03c1t] for the artificial data set. Notice how the model clearly identifies abrupt concept drift at time steps t = 30 and t = 60. The series denoted \u201cE[\u03c1] \u2212 1000\u201d illustrates the evolution of the parameter when we increase the batch size to 1000 samples. We recognize a more confident assessment about the absence of concept drift as more data is made available."}, {"heading": "5.3. Evaluation using Real Data Sets", "text": ""}, {"heading": "5.3.1. DATA AND MODELS", "text": "For this evaluation we consider three real data sets from different domains:\nElectricity Market (Harries, 1999): The data set describes the electricity market of two Australian states. It contains 45312 instances of 6 attributes, including a class label comparing the change of the electricity price related to a moving average of the last 24 hours. Each instance in the data set represents 30 minutes of trading; during our analysis we created batches such that xt contains all information associated with month t.\nThe data is analyzed using a Bayesian linear regression model. The binary class label is assumed to follow a Gaussian distribution in order to fit within the conjugate model class. Similarly, the marginal densities of the predictive attributes are also assumed to be Gaussian. The regression coefficients are given Gaussian prior distributions, and the variance is given a Gamma prior. Note that the overall distribution does not fall inside the conditional conjugate exponential family (Hoffman et al., 2013), hence PVB cannot be applied here, because lower-bound\u2019s gradient cannot be computed in closed-form.\nGPS (Zheng et al., 2008; 2009; 2010): This data set contains 17 621 GPS trajectories (time-stamped x and y coordinates), totalling more than 4.5 million observations. To reduce the data-size we kept only one out of every ten measurements. We grouped the data so that xt contains all data collected during hour t of the day, giving a total of 24 batches of this stream.\nHere we employ a model with one independent Gaussian mixture model per day of the week, each mixture with 5 components. This enables us to track changes in the users\u2019 profiles across hours of the day, and also to monitor how the changes are affected by the day of the week.\nFinance (reference withheld): The data contains monthly aggregated information about the financial profile of\naround 50 000 customers over 62 (non-consecutive) months. Three attributes were extracted per customer, in addition to a class-label telling whether or not the customer will default within the next 24 months.\nWe fit a na\u0131\u0308ve Bayes model to this data set, where the distribution at the leaf-nodes is 5-component mixture of Gaussians distribution. The distribution over the mixture node is shared by all the attributes, but not between the two classes of customers.\nA detailed description of all the models, including their structure and their variational families, is given at the supplementary material."}, {"heading": "5.3.2. EVALUATION AND DISCUSSION", "text": "To evaluate the different methods discussed, we look at the test marginal log-likelihood (TMLL). Specifically, each data batch is randomly split in a train data set, xt, and a test data set, x\u0303t, containing two thirds and one third of the data batch, respectively. Then, TMLLt is computed as TMLLt = 1|x\u0303t| \u222b p(x\u0303t, zt|\u03b2t)p(\u03b2t|xt)dztd\u03b2t. Figure 5 (left) shows for each method the difference between its TMLLt and that obtained by SVB (which is considered the baseline method). To improve readability, we only plot the results of the best performing method inside each group of methods. The right-hand side of Figure 5 shows the development of Eq[\u03c1t] over time for SVB-HPP and SVB-MHPP. For SVB-HPP we only have one \u03c1t-parameter, and its value is given by the solid line. SVB-MHPP utilizes one \u03c1(i) for each variational parameter.4 In this case, we plot Eq[\u03c1(i)t ] at each point in time to indicate the variability between the different estimates throughout the series. Finally, we compute each method\u2019s aggregated test marginal log-likelihood measure \u2211T t=1 TMLLt, and report these values in Table 1.\nFor the electricity data set, we can see that the two proposed methods (SVB-HPP and SVB-MHPP) perform best. All models are comparable during the first nine months, which is a period where our models detect no or very limited con-\n4The numbers of variational parameters are 14, 78 and 33 for the Electricity, GPS and Financial model, respectively.\ncept drift (cf. top right plot or Figure 5). However, after this period, both SVB-HPP and SVB-MHPP detects substantial drift, and is able to adapt better than the other methods, which appear unable to adjust to the complex concept drift structure in the latter part of the data. SVB-HPP and SVB-MHPP continue to behave at a similar level, mainly because when drift happens it typically includes a high proportion of the parameters of the model.\nFor the GPS data set, we can observe how the SVB-MHPP is superior to the rest of the methods, particularly towards the end of the series. When looking at Figure 5 (middle right panel), we can see that a significative proportion of the model parameters are drifting (i.e., Eq[\u03c1(i)t ] \u2264 0.05) at all times, while another proportion of the parameters show a quite stable behavior (\u03c1-values above 0.9). This complex pattern is not captured well by SVB-HPP, which ends up assuming no concept drift after the initial time-step.\nThe financial data set shows a different behavior. During the first months, SVB-MHPP slightly outperforms the rest of the approaches, but after month 30, SVB-PP with \u03c1 = 0.9 is superior, with SVB-MHPP second. Looking at the E[\u03c1(i)t ]-values of SVB-MHPP, we observe that there is significant concept drift in some of the parameters over the first few months. However, only a few parameters exhibit noteworthy drift after the first third of the sequence. Apparently, the simple SVB-PP approach has the upper hand when the drift is constant and fairly limited, at least when the optimal forgetting factor \u03c1 has been identified.\nWe conclude this section by highlighting that the performance of SVB-PP and PVB depend heavily on the hyperparameters of the model, cf. Table 1. As an example, consider SVB-PP for the financial data set. While it was the best overall with \u03c1 = 0.9, it is inferior to SVB-MHPP if \u03c1 = 0.99. Similarly, PVB\u2019s performance is sensitive both to \u03bd (see in particular the results for the GPS data) and M (financial data). These hyper-parameters are hard to fix, as their optimal values depend on data characteristics (see Broderick et al. (2013); McInerney et al. (2015) for similar conclusions). We therefore believe that the fully Bayesian formulation is an important strong point of our approach."}, {"heading": "6. Conclusions and Future Work", "text": "We have introduced a new class of Bayesian models for streaming data, able to capture changes in the underlying generative process. Unlike existing solutions to this problem, aimed at modeling slowly changing processes, our proposal is able to handle both abrupt and gradual concept drift following a Bayesian approach. The new model accounts for the dynamics of the data stream by assuming that only the global parameters evolve over time. We intro-\nduce the so-called hierarchical power priors, where a prior on the learning rate is given allowing it to adapt to the data stream. We have addressed the complexity of the underlying inference tasks by developing an approximate variational inference scheme that optimizes a novel lower bound of the likelihood function.\nAs future work we aim to provide a sound approach to semantically characterize concept drift by inspecting the E[\u03c1(i)t ] values provided by SVB-MHPP."}, {"heading": "Acknowledgements", "text": "This work was partly carried out as part of the AMIDST project. AMIDST has received funding from the European Union\u2019s Seventh Framework Programme for research, technological development and demonstration under grant agreement no 619209. Furthermore, this research has been partly funded by the Spanish Ministry of Economy and Competitiveness, through projects TIN2015-74368JIN, TIN2013-46638-C3-1-P, TIN2016-77902-C3-3-P and by ERDF funds."}, {"heading": "A. Proof of Theorem 1 and Lemma 2", "text": "Proof of Theorem 1. In the specification of L we have that Eq[ln p\u0302(\u03b2t|\u03bbt\u22121, \u03c1t)] (defined in Equation (7)) can be expanded as (ignoring the base measure) :\nEq[(\u03c1t\u03bbt\u22121+(1\u2212\u03c1t)\u03b1u)t(\u03b2t)\u2212ag(\u03c1t\u03bbt\u22121+(1\u2212\u03c1t)\u03b1u)]. Since ag is convex we have\nag(\u03c1t\u03bbt\u22121+(1\u2212\u03c1t)\u03b1u) \u2264 \u03c1tag(\u03bbt\u22121)+(1\u2212\u03c1t)ag(\u03b1u), which combined with Equation (10) gives\nEq[ln p(xt,Zt|\u03b2t)] + Eq[(\u03c1t\u03bbt\u22121 + (1\u2212 \u03c1t)\u03b1u)t(\u03b2t) \u2212 \u03c1tag(\u03bbt\u22121)\u2212 (1\u2212 \u03c1t)ag(\u03b1u)] + Eq[p(\u03c1t|\u03b3)] \u2212 Eq[ln q(Zt|\u03c6t)]\u2212 Eq[q(\u03b2t|\u03bbt)]\u2212 Eq[q(\u03c1t|\u03c9t)] \u2264 L.\nLastly, by exploiting the mean field factorization of q and using the exponential family form of p\u03b4(\u03b2t|\u03bbt\u22121) and pu(\u03b2t) we get the desired result.\nProof of Lemma 2. Firstly, by ignoring the terms in L\u0302 (Equation (11)) that do not involve \u03c9t we get\nL\u0302(\u03c9t) = Eq[\u03c1t](Eq[ln(p\u03b4(\u03b2t|\u03bbt\u22121))\u2212 Eq[ln pu(\u03b2t)]) + Eq[p(\u03c1t|\u03b3)]\u2212 Eq[q(\u03c1t|\u03c9t)].\nAssuming that the sufficient statistics function t(\u03c1t) for p(\u03c1t|\u03b3) and q(\u03b2t|\u03bbt) is the identity function (t(\u03c1t) = \u03c1t) we have\nL\u0302(\u03c9t) = Eq[\u03c1t](Eq[ln(p\u03b4(\u03b2t|\u03bbt\u22121))\u2212 Eq[ln pu(\u03b2t)]) + \u03b3Eq[\u03c1t]\u2212 (\u03c9tEq[\u03c1t]\u2212 ag(\u03c9t)) + cte.\nUsing Eq[t(\u03c1t)] = Eq[\u03c1t] = \u2207\u03c9tag(\u03c9t) we get\nL\u0302(\u03c9t) = \u2207\u03c9tag(\u03c9t)(Eq[ln(p\u03b4(\u03b2t|\u03bbt\u22121))\u2212 Eq[ln pu(\u03b2t)]) + \u03b3\u2207\u03c9tag(\u03c9t)\u2212 (\u03c9t\u2207\u03c9tag(\u03c9t)\u2212 ag(\u03c9t)).\n1Department of Mathematics, Unversity of Almer\u0131\u0301a, Almer\u0131\u0301a, Spain 2Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway 3Department of Computer Science, Aalborg University, Aalborg, Denmark 4Hugin Expert A/S, Aalborg, Denmark. Correspondence to: Andre\u0301s Masegosa <andresmasegosa@ual.es>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nand thereby\n\u2207\u03c9tL\u0302 = \u22072\u03c9tag(\u03c9t)(Eq[ln(p\u03b4(\u03b2t|\u03bbt\u22121))\u2212ln pu(\u03b2t)]+\u03b3\u2212\u03c9t).\nWe can now find the natural gradient by premultiplying \u2207\u03c9tL\u0302 by the inverse of the Fisher information matrix, which for the exponential family corresponds to the inverse of the Hessian of the log-normalizer:\n\u2207\u0302\u03c9tL\u0302 = (\u22072\u03c9tag(\u03c9t))\u22121\u2207\u03c9tL\u0302 = Eq[ln(p\u03b4(\u03b2t|\u03bbt\u22121))\u2212 ln pu(\u03b2t)] + \u03b3 \u2212 \u03c9t.\nLastly, by introducing q(\u03b2t|\u03bbt) \u2212 q(\u03b2t|\u03bbt) inside the expectation we get the difference in Kullbach-Leibler divergence KL(q(\u03b2t|\u03bbt), pu(\u03b2t)) \u2212 KL(q(\u03b2t|\u03bbt), p\u03b4(\u03b2t|\u03bbt\u22121))."}, {"heading": "B. Experimental Evaluation", "text": "B.1. Probabilistic Models\nWe provide a (simplified) graphical description of the probabilistic models used in the experiments. We also detail the distributional assumptions of the parameters, which are then used to define the variational approximation family.\nELECTRICITY MODEL\nx1,t x2,t x3,t\nyt\n(\u00b5i, \u03b3i) \u223c NormalGamma(1, 1, 0, 1e\u2212 10) \u03b3 \u223c Gamma(1, 1) bi \u223c N (0,+\u221e)\nxi,t \u223c N (\u00b5i, \u03b3i)\nyt \u223c N ( b0 + \u2211\ni\nbixi,t, \u03b3\n)\nar X\niv :1\n70 7.\n02 29\n3v 1\n[ cs\n.L G\n] 7\nJ ul\n2 01\n7\nzt\nDayt\nxt yt\nGPS MODEL\np \u223c Dirichlet(1, . . . , 1) pk \u223c Dirichlet(1, . . . , 1)\n(\u00b5 (x) j,k , \u03b3 (x) j,k ) \u223c NormalGamma(1, 1, 0, 1e\u2212 10)\n(\u00b5 (y) j,k , \u03b3 (y) j,k ) \u223c NormalGamma(1, 1, 0, 1e\u2212 10)\nDayt \u223cMultinomial(p) (zt|Dayt = k) \u223cMultinomial(pk)\n(xt|zt = j,Dayt = k) \u223c N (\u00b5(x)j,k , \u03b3 (x) j,k )\n(yt|zt = j,Dayt = k) \u223c N (\u00b5(y)j,k , \u03b3 (y) j,k )\nFINANCIAL MODEL\np \u223c Dirichlet(1, . . . , 1) pk \u223c Dirichlet(1, . . . , 1)\n(\u00b5i;j,k, \u03b3i;j,k) \u223c NormalGamma(1, 1, 0, 1e\u2212 10) Defaultt \u223c Binomial(p)\n(zt|Defaultt = k) \u223cMultinomial(pk) (xi,t|zt = j,Dayt = k) \u223c N (\u00b5i;j,k, \u03b3i;j,k)\nB.2. Real Life Data Sets\nIn the experimental section of the original paper, we plot the relative values for the TMLLt measure with respect to\nthe SVB method. Here, we provides the plots of the absolute values of the TMLLt series for the different methods studied in the paper."}], "references": [{"title": "Online inference for the infinite topic-cluster model: Storylines from streaming text", "author": ["Ahmed", "Amr", "Ho", "Qirong", "Teo", "Choon Hui", "Eisenstein", "Jacob", "Smola", "Alexander J", "Xing", "Eric P"], "venue": "In AISTATS, pp", "citeRegEx": "Ahmed et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2011}, {"title": "Streaming variational Bayes", "author": ["Broderick", "Tamara", "Boy", "Nicholas", "Wibisono", "Andre", "Wilson", "Ashia C", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Broderick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Broderick et al\\.", "year": 2013}, {"title": "Financial data analysis with PGMs using AMIDST", "author": ["Caba\u00f1as", "Rafael", "Mart\u0131\u0301nez", "Ana M", "Masegosa", "Andr\u00e9s R", "Ramos-L\u00f3pez", "Dar\u0131\u0301o", "Samer\u00f3n", "Antonio", "Nielsen", "Thomas D", "Langseth", "Helge", "Madsen", "Anders L"], "venue": "In Data Mining Workshops (ICDMW),", "citeRegEx": "Caba\u00f1as et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Caba\u00f1as et al\\.", "year": 2016}, {"title": "On sequential Monte Carlo sampling methods for Bayesian filtering", "author": ["Doucet", "Arnaud", "Godsill", "Simon", "Andrieu", "Christophe"], "venue": "Statistics and computing,", "citeRegEx": "Doucet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Doucet et al\\.", "year": 2000}, {"title": "A survey on concept drift adaptation", "author": ["Gama", "Jo\u00e3o", "\u017dliobait\u0117", "Indr\u0117", "Bifet", "Albert", "Pechenizkiy", "Mykola", "Bouchachia", "Abdelhamid"], "venue": "ACM Computing Surveys,", "citeRegEx": "Gama et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gama et al\\.", "year": 2014}, {"title": "Online variational Bayesian learning", "author": ["Ghahramani", "Zoubin", "H. Attias"], "venue": "In Slides from talk presented at NIPS workshop on Online Learning,", "citeRegEx": "Ghahramani et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ghahramani et al\\.", "year": 2000}, {"title": "Splice-2 comparative evaluation: Electricity pricing. NSW-CSE-TR-9905", "author": ["Harries", "Michael"], "venue": "School of Computer Siene and Engineering,", "citeRegEx": "Harries and Michael.,? \\Q1999\\E", "shortCiteRegEx": "Harries and Michael.", "year": 1999}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["Heckerman", "David", "Geiger", "Dan", "Chickering", "David M"], "venue": "Machine learning,", "citeRegEx": "Heckerman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1995}, {"title": "Stochastic variational inference", "author": ["Hoffman", "Matthew D", "Blei", "David M", "Wang", "Chong", "Paisley", "John"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hoffman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2013}, {"title": "On-line variational Bayesian learning", "author": ["Honkela", "Antti", "Valpola", "Harri"], "venue": "In 4th International Symposium on Independent Component Analysis and Blind Signal Separation,", "citeRegEx": "Honkela et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Honkela et al\\.", "year": 2003}, {"title": "Power prior distributions for regression models", "author": ["Ibrahim", "Joseph G", "Chen", "Ming-Hui"], "venue": "Statistical Science, pp", "citeRegEx": "Ibrahim et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ibrahim et al\\.", "year": 2000}, {"title": "On optimality properties of the power prior", "author": ["Ibrahim", "Joseph G", "Chen", "Ming-Hui", "Sinha", "Debajyoti"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Ibrahim et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ibrahim et al\\.", "year": 2003}, {"title": "Approximate Bayesian recursive estimation", "author": ["K\u00e1rn\u1ef3", "Miroslav"], "venue": "Information Sciences,", "citeRegEx": "K\u00e1rn\u1ef3 and Miroslav.,? \\Q2014\\E", "shortCiteRegEx": "K\u00e1rn\u1ef3 and Miroslav.", "year": 2014}, {"title": "Propagation of probabilities, means, and variances in mixed graphical association models", "author": ["Lauritzen", "Steffen L"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Lauritzen and L.,? \\Q1992\\E", "shortCiteRegEx": "Lauritzen and L.", "year": 1992}, {"title": "d-VMP: Distributed variational message passing", "author": ["A.R. Masegosa", "A.M. Mart\u0131\u0301nez", "H. Langseth", "T.D. Nielsen", "A. Salmer\u00f3n", "D. Ramos-L\u00f3pez", "A.L. Madsen"], "venue": "In PGM\u20192016. JMLR: Workshop and Conference Proceedings,", "citeRegEx": "Masegosa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Masegosa et al\\.", "year": 2016}, {"title": "Probabilistic graphical models on multi-core cpus using Java 8", "author": ["Masegosa", "Andres R", "Martinez", "Ana M", "Borchani", "Hanen"], "venue": "IEEE Computational Intelligence Magazine,", "citeRegEx": "Masegosa et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Masegosa et al\\.", "year": 2016}, {"title": "Amidst: a Java toolbox for scalable probabilistic machine learning", "author": ["Masegosa", "Andr\u00e9s R", "Mart\u0131\u0301nez", "Ana M", "Ramos-L\u00f3pez", "Dar\u0131\u0301o", "Caba\u00f1as", "Rafael", "Salmer\u00f3n", "Antonio", "Nielsen", "Thomas D", "Langseth", "Helge", "Madsen", "Anders L"], "venue": "arXiv preprint arXiv:1704.01427,", "citeRegEx": "Masegosa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Masegosa et al\\.", "year": 2017}, {"title": "The population posterior and Bayesian modeling on streams", "author": ["McInerney", "James", "Ranganath", "Rajesh", "Blei", "David"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "McInerney et al\\.,? \\Q2015\\E", "shortCiteRegEx": "McInerney et al\\.", "year": 2015}, {"title": "Marginalized adaptive particle filtering for nonlinear models with unknown time-varying noise parameters", "author": ["\u00d6zkan", "Emre", "\u0160mdl", "V\u00e1clav", "Saha", "Saikat", "Lundquist", "Christian", "Gustafsson", "Fredrik"], "venue": null, "citeRegEx": "\u00d6zkan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "\u00d6zkan et al\\.", "year": 2013}, {"title": "Online model selection based on the variational Bayes", "author": ["Sato", "Masa-Aki"], "venue": "Neural Computation,", "citeRegEx": "Sato and Masa.Aki.,? \\Q2001\\E", "shortCiteRegEx": "Sato and Masa.Aki.", "year": 2001}, {"title": "Variational message passing", "author": ["Winn", "John M", "Bishop", "Christopher M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Winn et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Winn et al\\.", "year": 2005}, {"title": "Efficient methods for topic model inference on streaming document collections", "author": ["Yao", "Limin", "Mimno", "David", "McCallum", "Andrew"], "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Yao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2009}, {"title": "Understanding mobility based on gps data", "author": ["Zheng", "Yu", "Li", "Quannan", "Chen", "Yukun", "Xie", "Xing", "Ma", "Wei-Ying"], "venue": "In Proceedings of the 10th International Conference on Ubiquitous Computing,", "citeRegEx": "Zheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2008}, {"title": "Mining interesting locations and travel sequences from gps trajectories", "author": ["Zheng", "Yu", "Zhang", "Lizhu", "Xie", "Xing", "Ma", "Wei-Ying"], "venue": "In Proceedings of the 18th International Conference on World Wide Web,", "citeRegEx": "Zheng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2009}, {"title": "Geolife: A collaborative social networking service among user, location and trajectory", "author": ["Zheng", "Yu", "Xie", "Xing", "Ma", "Wei-Ying"], "venue": "IEEE Data Eng. Bull.,", "citeRegEx": "Zheng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zheng et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "Specifically, we are interested in models suitable for domains that exhibit changes in the underlying generative process (Gama et al., 2014).", "startOffset": 121, "endOffset": 140}, {"referenceID": 18, "context": "Our modelling is inspired by previous works on Bayesian recursive estimation (\u00d6zkan et al., 2013; K\u00e1rn\u1ef3, 2014), power priors (Ibrahim & Chen, 2000) and exponential forDepartment of Mathematics, Unversity of Almer\u0131\u0301a, Almer\u0131\u0301a, Spain Department of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway Department of Computer Science, Aalborg University, Aalborg, Denmark Hugin Expert A/S, Aalborg, Denmark.", "startOffset": 77, "endOffset": 110}, {"referenceID": 16, "context": "com) (Masegosa et al., 2017; 2016b; Caba\u00f1as et al., 2016).", "startOffset": 5, "endOffset": 57}, {"referenceID": 2, "context": "com) (Masegosa et al., 2017; 2016b; Caba\u00f1as et al., 2016).", "startOffset": 5, "endOffset": 57}, {"referenceID": 8, "context": "Extending the notation of Hoffman et al. (2013), we have that", "startOffset": 26, "endOffset": 48}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009).", "startOffset": 61, "endOffset": 120}, {"referenceID": 3, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009).", "startOffset": 61, "endOffset": 120}, {"referenceID": 21, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009).", "startOffset": 61, "endOffset": 120}, {"referenceID": 1, "context": "The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category.", "startOffset": 48, "endOffset": 72}, {"referenceID": 8, "context": "Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 17, "context": ", 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group.", "startOffset": 67, "endOffset": 91}, {"referenceID": 4, "context": ", with concept drift (Gama et al., 2014)) is not addressed by SVB, as it assumes data exchangeability.", "startOffset": 21, "endOffset": 40}, {"referenceID": 11, "context": "The so-called power prior approach (Ibrahim & Chen, 2000) is also based on an exponential forgetting mechanisms, and has nice theoretical properties (Ibrahim et al., 2003).", "startOffset": 149, "endOffset": 171}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al.", "startOffset": 62, "endOffset": 221}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation.", "startOffset": 62, "endOffset": 246}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation. The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category. Alternatively, one could cast the inference problem as a stochastic optimization problem. Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group. SVI assumes the existence of a fixed data set observed in a sequential manner, and in particular that this data set has a known finite size. This is unrealistic when modeling data streams. PVB addresses this problem by using the frequentist notion of a population distribution, F, which is assumed to generate the data stream by repeatedly sampling M data points at the time. M parameterizes the size of the population, and helps control the variance of the population posterior. Unfortunately, M must be specified by the user. No clear rule exists regarding how to set it, and McInerney et al. (2015) show that its optimal value may differ from one data stream to another.", "startOffset": 62, "endOffset": 1312}, {"referenceID": 0, "context": "Bayesian inference on streaming data has been widely studied (Ahmed et al., 2011; Doucet et al., 2000; Yao et al., 2009). In the context of variational inference, there are two main approaches. Ghahramani & Attias (2000); Broderick et al. (2013) propose recursive Bayesian updating of the variational approximation. The streaming variational Bayes (SVB) algorithm (Broderick et al., 2013) is the most known approach of this category. Alternatively, one could cast the inference problem as a stochastic optimization problem. Stochastic variational inference (SVI) (Hoffman et al., 2013) and the closely related population variational Bayes (PVB) (McInerney et al., 2015) are prominent examples from this group. SVI assumes the existence of a fixed data set observed in a sequential manner, and in particular that this data set has a known finite size. This is unrealistic when modeling data streams. PVB addresses this problem by using the frequentist notion of a population distribution, F, which is assumed to generate the data stream by repeatedly sampling M data points at the time. M parameterizes the size of the population, and helps control the variance of the population posterior. Unfortunately, M must be specified by the user. No clear rule exists regarding how to set it, and McInerney et al. (2015) show that its optimal value may differ from one data stream to another. The problem of Bayesian modeling of non-stationary data streams (i.e., with concept drift (Gama et al., 2014)) is not addressed by SVB, as it assumes data exchangeability. An online variational inference method, which exponentially forgets the variational parameters associated with old data, was proposed by Honkela & Valpola (2003). The so-called power prior approach (Ibrahim & Chen, 2000) is also based on an exponential forgetting mechanisms, and has nice theoretical properties (Ibrahim et al.", "startOffset": 62, "endOffset": 1718}, {"referenceID": 18, "context": "A time series based modeling approach for concept drift using implicit transition models was pursued by \u00d6zkan et al. (2013); K\u00e1rn\u1ef3 (2014).", "startOffset": 104, "endOffset": 124}, {"referenceID": 18, "context": "A time series based modeling approach for concept drift using implicit transition models was pursued by \u00d6zkan et al. (2013); K\u00e1rn\u1ef3 (2014). Unfortunately, the implicit transition model depends on a hyper-parameter determining the forgetting-factor, which has to be manually set.", "startOffset": 104, "endOffset": 138}, {"referenceID": 18, "context": "Rather than explicitly modeling the evolution of the \u03b2t parameters as in Equation (3), we instead follow the approach of K\u00e1rn\u1ef3 (2014) and \u00d6zkan et al. (2013) who define the time evolution model implicitly by constraining the maximum KL divergence over consecutive parameter distributions.", "startOffset": 138, "endOffset": 158}, {"referenceID": 18, "context": "K\u00e1rn\u1ef3 (2014) and \u00d6zkan et al. (2013) seek to approximate p(\u03b2t|x1:t\u22121) by the distribution p\u0302(\u03b2t|x1:t\u22121) having maximum entropy under the constraint in (5); for continuous distributions the maximum entropy can be formulated relative to an uninformative prior density pu(\u03b2t), which corresponds to the Kullbach-Leibler divergence between the two distributions.", "startOffset": 17, "endOffset": 37}, {"referenceID": 1, "context": "In our streaming data setting we follow assumed density filtering (Lauritzen, 1992) and the SVB approach (Broderick et al., 2013) and employ the approximation p(\u03b2t\u22121|x1:t\u22121) \u2248 q(\u03b2t\u22121|\u03bbt\u22121), where q(\u03b2t\u22121|\u03bbt\u22121) is the variational distribution calculated in the previous time step.", "startOffset": 105, "endOffset": 129}, {"referenceID": 11, "context": "The perspective provided by Lemma 1 introduces a well known result of power priors, which is also applicable in the current context (see the discussion after Theorem 1 in (Ibrahim et al., 2003)): \u201cthe power prior is an optimal prior to use and in fact minimizes the convex combination of KL divergences between two extremes: one in which no historical data is used and the other in which the historical data and current data are given equal weight.", "startOffset": 171, "endOffset": 193}, {"referenceID": 18, "context": "\u201d As noted in (Olesen et al., 1992; \u00d6zkan et al., 2013), this schema works as a moving window with exponential forgetting of past data, where the effective number of samples or, more technically, the so-called equivalent sample size of the posterior (Heckerman et al.", "startOffset": 14, "endOffset": 55}, {"referenceID": 7, "context": ", 2013), this schema works as a moving window with exponential forgetting of past data, where the effective number of samples or, more technically, the so-called equivalent sample size of the posterior (Heckerman et al., 1995), converges to,", "startOffset": 202, "endOffset": 226}, {"referenceID": 18, "context": "In the approach taken by \u00d6zkan et al. (2013) (and, by extension, SVB-PP), the forgetting factor \u03c1t is user-defined.", "startOffset": 25, "endOffset": 45}, {"referenceID": 14, "context": "On the other hand, observe that the form of the natural gradient of \u03c9t have an intuitive semantic interpretation, which also extends to the coordinate ascent variational message passing framework (Winn & Bishop, 2005) as shown by Masegosa et al. (2016a). Specifically, using the constant \u03b3 as a threshold, we see that if the uninformed prior pu(\u03b2t) provides a better fit to the variational posterior at time t than the variational parameters \u03bbt from the previous time step (KL(q(\u03b2t|\u03bbt), pu(\u03b2t)) + \u221230 \u221220 \u221210 0 10 20 30 0.", "startOffset": 230, "endOffset": 254}, {"referenceID": 8, "context": "Note that the overall distribution does not fall inside the conditional conjugate exponential family (Hoffman et al., 2013), hence PVB cannot be applied here, because lower-bound\u2019s gradient cannot be computed in closed-form.", "startOffset": 101, "endOffset": 123}, {"referenceID": 22, "context": "GPS (Zheng et al., 2008; 2009; 2010): This data set contains 17 621 GPS trajectories (time-stamped x and y coordinates), totalling more than 4.", "startOffset": 4, "endOffset": 36}, {"referenceID": 1, "context": "These hyper-parameters are hard to fix, as their optimal values depend on data characteristics (see Broderick et al. (2013); McInerney et al.", "startOffset": 100, "endOffset": 124}, {"referenceID": 1, "context": "These hyper-parameters are hard to fix, as their optimal values depend on data characteristics (see Broderick et al. (2013); McInerney et al. (2015) for similar conclusions).", "startOffset": 100, "endOffset": 149}], "year": 2017, "abstractText": "Making inferences from data streams is a pervasive problem in many modern data analysis applications. But it requires to address the problem of continuous model updating, and adapt to changes or drifts in the underlying data generating distribution. In this paper, we approach these problems from a Bayesian perspective covering general conjugate exponential models. Our proposal makes use of non-conjugate hierarchical priors to explicitly model temporal changes of the model parameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate priors while maintaining the computational efficiency of variational methods over conjugate models. The approach is validated on three real data sets over three latent variable models.", "creator": "LaTeX with hyperref package"}}}