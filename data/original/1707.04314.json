{"id": "1707.04314", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jul-2017", "title": "Bayesian Optimization for Probabilistic Programs", "abstract": "We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.", "histories": [["v1", "Thu, 13 Jul 2017 20:49:29 GMT  (7064kb,D)", "http://arxiv.org/abs/1707.04314v1", null]], "reviews": [], "SUBJECTS": "stat.ML cs.AI cs.PL stat.CO", "authors": ["tom rainforth", "tuan anh le", "jan-willem van de meent", "michael a osborne", "frank wood"], "accepted": true, "id": "1707.04314"}, "pdf": {"name": "1707.04314.pdf", "metadata": {"source": "CRF", "title": "Bayesian Optimization for Probabilistic Programs\u2217", "authors": ["Tom Rainforth", "Tuan Anh Le", "Jan-Willem van de Meent", "Frank Wood"], "emails": ["TWGR@ROBOTS.OX.AC.UK", "TUANANH@ROBOTS.OX.AC.UK", "J.VANDEMEENT@NORTHEASTERN.EDU", "MOSB@ROBOTS.OX.AC.UK", "FWOOD@ROBOTS.OX.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data (Carpenter et al., 2015; Goodman et al., 2008; Goodman and Stuhlmu\u0308ller, 2014; Mansinghka et al., 2014; Minka et al., 2010; Wood et al., 2014). Their core philosophy is to decouple model specification and inference, the former corresponding to the user-specified program code and the latter to an inference engine capable of operating on arbitrary programs. Removing the need for users to write inference algorithms significantly reduces the burden of developing new models and makes effective statistical methods accessible to non-experts.\nAlthough significant progress has been made on the problem of general purpose inference of program variables, less attention has been given to their optimization. Optimization is an essential tool for effective machine learning, necessary when the user requires a single estimate. It also often forms a tractable alternative when full inference is infeasible (Murphy, 2012). Moreover, coincident optimization and inference is often required, corresponding to a marginal maximum a posteriori (MMAP) setting where one wishes to maximize some variables, while marginalizing out others. Examples of MMAP problems include hyperparameter optimization, expectation maximization, and policy search (van de Meent et al., 2016).\nIn this paper we develop the first system that extends probabilistic programming (PP) to this more general MMAP framework, wherein the user specifies a model in the same manner as existing systems, but then selects some subset of the sampled variables in the program to be optimized, with the rest marginalized out using existing inference algorithms. The optimization query we introduce can be implemented and utilized in any PPS that supports an inference method returning a marginal likelihood estimate. This framework increases the scope of models that can be expressed in PPS and gives additional flexibility in the outputs a user can request from the program.\n*Please cite this version: Tom Rainforth, Tuan-Anh Le, Jan-Willem van de Meent, Michael A Osborne, and Frank Wood. Bayesian optimization for probabilistic programs. In Advances in Neural Information Processing Systems, pages 280\u2013288, 2016\nar X\niv :1\n70 7.\n04 31\n4v 1\n[ st\nat .M\nL ]\n1 3\nMMAP estimation is difficult as it corresponds to the optimization of an intractable integral, such that the optimization target is expensive to evaluate and gives noisy results. Current PPS inference engines are typically unsuited to such settings. We therefore introduce BOPP1 (Bayesian optimization for probabilistic programs) which couples existing inference algorithms from PPS, like Anglican (Wood et al., 2014), with a new Gaussian process (GP) (Rasmussen and Williams, 2006) based Bayesian optimization (BO) package (Gutmann and Corander, 2016; Jones et al., 1998; Osborne et al., 2009; Shahriari et al., 2016a).\nTo demonstrate the functionality provided by BOPP, we consider an example application of engineering design. Engineering design relies extensively on simulations which typically have two things in common: the desire of the user to find a single best design and an uncertainty in the environment in which the designed component will live. Even when these simulations are deterministic, this is an approximation to a truly stochastic world. By expressing the utility of a particular design-environment combination using an approximate Bayesian computation (ABC) likelihood (Csille\u0301ry et al., 2010), one can pose this as a MMAP problem, optimizing the design while marginalizing out the environmental uncertainty.\nFigure 1 illustrates how BOPP can be applied to engineering design, taking the example of optimizing the distribution of power between radiators in a house so as to homogenize the temperature, while marginalizing out possible weather conditions and subject to a total energy budget. The probabilistic program shown in Figure 2 allows us to define a prior over the uncertain weather, while conditioning on the output of a deterministic simulator (here Energy2D (Xie, 2012)-a finite element package for heat transfer) using an ABC likelihood. BOPP now allows the required coincident inference and optimization to be carried out automatically, directly returning increasingly optimal configurations.\nBO is an attractive choice for the required optimization in MMAP as it is typically efficient in the number of target evaluations, operates on non-differentiable targets, and incorporates noise in the target function evaluations. However, applying BO to probabilistic programs presents challenges, such as the need to give robust performance on a wide range of problems with varying scaling and potentially unbounded support. Furthermore, the target program may contain unknown constraints, implicitly defined by the generative model, and variables whose type is unknown (i.e. they may be continuous or discrete).\nOn the other hand, the availability of the target source code in a PPS presents opportunities to overcome these issues and go beyond what can be done with existing BO packages. BOPP exploits the source code in a number of ways, such as optimizing the acquisition function using the original generative model to ensure the solution satisfies the implicit constaints, performing adaptive domain scaling to ensure that GP kernel hyperparameters can be set according to problem-independent hyperpriors, and defining an adaptive non-stationary mean function to support unbounded BO.\nTogether, these innovations mean that BOPP can be run in a manner that is fully black-box from the user\u2019s perspective, requiring only the identification of the target variables relative to current syntax for operating on arbitrary programs. We further show that BOPP is competitive with existing BO engines for direct optimization on common benchmarks problems that do not require marginalization."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1 Probabilistic Programming", "text": "Probabilistic programming systems allow users to define probabilistic models using a domain-specific programming language. A probabilistic program implicitly defines a distribution on random variables, whilst the system back-end implements general-purpose inference methods.\nPPS such as Infer.Net (Minka et al., 2010) and Stan (Carpenter et al., 2015) can be thought of as defining graphical models or factor graphs. Our focus will instead be on systems such as Church (Goodman et al., 2008), Venture (Mansinghka et al., 2014), WebPPL (Goodman and Stuhlmu\u0308ller, 2014), and Anglican (Wood et al., 2014), which employ a general-purpose programming language for model specification. In these systems, the set of random variables is dynamically typed, such that it is possible to write programs in which this set\n1Code available at http://www.github.com/probprog/bopp/\n(defopt house-heating [alphas target-temperatures] [powers] (let [solar-intensity (sample weather-prior)\npowers (sample (dirichlet alphas)) temperatures (simulate solar-intensity powers)]\n(observe (abc-likelihood temperatures) target-temperatures)))\nFigure 2: BOPP query for optimizing the power allocation to radiators in a house. Here weather-prior is a distribution over the solar intensity and a uniform Dirichlet prior with concentration alpha is placed over the powers. Calling simulate performs an Energy2D simulation of house temperatures. The utility of the resulting output is incorporated using abc-likelihood, which measures a discrepency from the target-temperatures. Calling doopt on this query invokes the BOPP algorithm to perform MMAP estimation, where the second input powers indicates the variable to be optimized.\ndiffers from execution to execution. This allows an unspecified number of random variables and incorporation of arbitrary black box deterministic functions, such as was exploited by the simulate function in Figure 2. The price for this expressivity is that inference methods must be formulated in such a manner that they are applicable to models where the density function is intractable and can only be evaluated during forwards simulation of the program.\nOne such general purpose system, Anglican, will be used as a reference in this paper. In Anglican, models are defined using the inference macro defquery. These models, which we refer to as queries (Goodman et al., 2008), specify a joint distribution p(Y,X) over data Y and variables X . Inference on the model is performed using the macro doquery, which produces a sequence of approximate samples from the conditional distribution p(X|Y ) and, for importance sampling based inference algorithms (e.g. sequential Monte Carlo), a marginal likelihood estimate p(Y ).\nRandom variables in an Anglican program are specified using sample statements, which can be thought of as terms in the prior. Conditioning is specified using observe statements which can be thought of as likelihood terms. Outputs of the program, taking the form of posterior samples, are indicated by the return values. There is a finite set of sample and observe statements in a program source code, but the number of times each statement is called can vary between executions. We refer the reader to http: //www.robots.ox.ac.uk/\u02dcfwood/anglican/ for more details."}, {"heading": "2.2 Bayesian Optimization", "text": "Consider an arbitrary black-box target function f : \u03d1\u2192 R that can be evaluated for an arbitrary point \u03b8 \u2208 \u03d1 to produce, potentially noisy, outputs w\u0302 \u2208 R. BO (Jones et al., 1998; Osborne et al., 2009) aims to find the global maximum\n\u03b8\u2217 = argmax \u03b8\u2208\u03d1 f (\u03b8) . (1)\nThe key idea of BO is to place a prior on f that expresses belief about the space of functions within which f might live. When the function is evaluated, the resultant information is incorporated by conditioning upon the observed data to give a posterior over functions. This allows estimation of the expected value and uncertainty in f (\u03b8) for all \u03b8 \u2208 \u03d1. From this, an acquisition function \u03b6 : \u03d1 \u2192 R is defined, which assigns an expected utility to evaluating f at particular \u03b8, based on the trade-off between exploration and exploitation in finding the maximum. When direct evaluation of f is expensive, the acquisition function constitutes a cheaper to evaluate substitute, which is optimized to ascertain the next point at which the target function should be evaluated in a sequential fashion. By interleaving optimization of the acquisition function, evaluating f at the suggested point, and updating the surrogate, BO forms a global optimization algorithm that is typically very efficient in the required number of function evaluations, whilst naturally dealing with noise in the outputs. Although alternatives such as random forests (Bergstra et al., 2011; Hutter et al., 2011) or neural networks (Snoek et al., 2015) exist, the most common prior used for f is a GP (Rasmussen and Williams, 2006). For further information on BO we refer the reader to the recent review by Shahriari et al Shahriari et al. (2016b)."}, {"heading": "2.3 Gaussian Processes", "text": "Informally one can think of a Gaussian Process (GP) (Rasmussen and Williams, 2006) as being a nonparametric distribution over functions which is fully specified by a mean function \u00b5 : \u03d1 \u2192 R and covariance function k : \u03d1\u00d7\u03d1\u2192 R, the latter of which must be a bounded (i.e. k (\u03b8, \u03b8\u2032) <\u221e, \u2200\u03b8, \u03b8\u2032 \u2208 \u03d1) and reproducing kernel. We can describe a function f as being distributed according to a GP:\nf (\u03b8) \u223c GP (\u00b5 (\u03b8) , k (\u03b8, \u03b8\u2032)) (2)\nwhich by definition means that the functional evaluations realized at any finite number of sample points is distributed according to a multivariate Gaussian. Note that the inputs to \u00b5 and k need not be numeric and as such a GP can be defined over anything for which kernel can be defined.\nAn important property of a GP is that it is conjugate with a Gaussian likelihood. Consider pairs of input-output data points {\u03b8\u0302j , w\u0302j}j=1:m, W\u0302 = {w\u0302j}j=1:m, \u0398\u0302 = {\u03b8\u0302j}j=1:m and the separable likelihood function\np(W\u0302 |\u0398\u0302, f) = m\u220f j=1 p(w\u0302j |f(\u03b8\u0302j)) = m\u220f j=1\n1 \u03c3n \u221a 2\u03c0 exp\n\u2212 ( w\u0302j \u2212 f(\u03b8\u0302j) )2 2\u03c32n  (3) where \u03c3n is an observation noise. Using a GP prior f (\u03b8) \u223c GP (\u00b5prior (\u03b8) , kprior (\u03b8, \u03b8)) leads to an analytic GP posterior\n\u00b5post (\u03b8) = \u00b5prior (\u03b8) + kprior\n( \u03b8, \u0398\u0302 ) [ kprior ( \u0398\u0302, \u0398\u0302 ) + \u03c32nI ]\u22121 ( W\u0302 \u2212 \u00b5prior ( \u0398\u0302 ))\n(4)\nkpost (\u03b8, \u03b8 \u2032) = kprior (\u03b8, \u03b8 \u2032)\u2212 kprior ( \u03b8, \u0398\u0302 ) [ kprior ( \u0398\u0302, \u0398\u0302 ) + \u03c32nI ]\u22121 kprior ( \u0398\u0302, \u03b8\u2032 ) (5)\nand Gaussian predictive distribution\nw|\u03b8, W\u0302 , \u0398\u0302 \u223c N ( \u00b5post (\u03b8) , kpost (\u03b8, \u03b8) + \u03c3 2 nI )\n(6)\nwhere we have used the shorthand kprior(\u0398\u0302, \u0398\u0302) = [ kprior(\u03b8\u03021,\u03b8\u03021) kprior(\u03b8\u03021,\u03b8\u03022) ...\nkprior(\u03b8\u03022,\u03b8\u03021) kprior(\u03b8\u03022,\u03b8\u03022) ... ... ... ...\n] and similarly for \u00b5prior, \u00b5post and\nkpost."}, {"heading": "3. Problem Formulation", "text": "Given a program defining the joint density p(Y,X, \u03b8) with fixed Y , our aim is to optimize with respect to a subset of the variables \u03b8 whilst marginalizing out latent variables X\n\u03b8\u2217 = argmax \u03b8\u2208\u03d1 p(\u03b8|Y ) = argmax \u03b8\u2208\u03d1 p(Y, \u03b8) = argmax \u03b8\u2208\u03d1\n\u222b p(Y,X, \u03b8)dX. (7)\nTo provide syntax to differentiate between \u03b8 and X , we introduce a new query macro defopt. The syntax of defopt is identical to defquery except that it has an additional input identifying the variables to be optimized. To allow for the interleaving of inference and optimization required in MMAP estimation, we further introduce doopt, which, analogous to doquery, returns a lazy sequence {\u03b8\u0302\u2217m, \u2126\u0302\u2217m, u\u0302\u2217m}m=1,... where \u2126\u0302\u2217m \u2286 X are the program outputs associated with \u03b8 = \u03b8\u0302\u2217m and each u\u0302\u2217m \u2208 R+ is an estimate of the corresponding log marginal log p(Y, \u03b8\u0302\u2217m) (see Section 4.2). The sequence is defined such that, at any time, \u03b8\u0302 \u2217 m corresponds to the point expected to be most optimal of those evaluated so far and allows both inference and optimization to be carried out online.\nAlthough no restrictions are placed on X , it is necessary to place some restrictions on how programs use the optimization variables \u03b8 = \u03c61:K specified by the optimization argument list of defopt. First, each optimization variable \u03c6k must be bound to a value directly by a sample statement with fixed measure-type distribution argument. This avoids change of variable complications arising from nonlinear deterministic mappings. Second, in order for the optimization to be well defined, the program must be written such that any possible execution trace binds each optimization variable \u03c6k exactly once. Finally, although any \u03c6k may be lexically multiply bound, it must have the same base measure in all possible execution traces, because, for instance, if the base measure of a \u03c6k were to change from Lebesgue to counting, the notion of optimality would no longer admit a conventional interpretation. Note that although the transformation implementations shown in Figure 3 do not contain runtime exception generators that disallow continued execution of programs that violate these constraints, those actually implemented in the BOPP system do."}, {"heading": "4. Bayesian Program Optimization", "text": "In addition to the syntax introduced in the previous section, there are five main components to BOPP:\n- A program transformation, q\u2192q-marg, allowing estimation of the evidence p(Y, \u03b8) at a fixed \u03b8. - A high-performance, GP based, BO implementation for actively sampling \u03b8.\n- A program transformation, q\u2192q-prior, used for automatic and adaptive domain scaling, such that a problem-independent hyperprior can be placed over the GP hyperparameters.\n- An adaptive non-stationary mean function to support unbounded optimization.\n- A program transformation, q\u2192q-acq, and annealing maximum likelihood estimation method to optimize the acquisition function subject the implicit constraints imposed by the generative model.\nTogether these allow BOPP to perform online MMAP estimation for arbitrary programs in a manner that is black-box from the user\u2019s perspective - requiring only the definition of the target program in the same way as existing PPS and identifying which variables to optimize. The BO component of BOPP is both probabilistic programming and language independent, and is provided as a stand-alone package.2 It requires as input only a target function, a sampler to establish rough input scaling, and a problem specific optimizer for the acquisition function that imposes the problem constraints.\nFigure 3 provides a high level overview of the algorithm invoked when doopt is called on a query q that defines a distribution p (Y, a, \u03b8, b). We wish to optimize \u03b8 whilst marginalizing out a and b, as indicated by the the second input to q. In summary, BOPP performs iterative optimization in 5 steps\n- Step 1 (blue arrows) generates unweighted samples from the transformed prior program q-prior (top center), constructed by removing all conditioning. This initializes the domain scaling for \u03b8.\n- Step 2 (red arrows) evaluates the marginal p(Y, \u03b8) at a small number of the generated \u03b8\u0302 by performing inference on the marginal program q-marg (middle centre), which returns samples from the distribution p (a, b|Y, \u03b8) along with an estimate of p(Y, \u03b8). The evaluated points (middle right) provide an initial domain scaling of the outputs and starting points for the BO surrogate.\n- Step 3 (black arrow) fits a mixture of GPs posterior Rasmussen and Williams (2006) to the scaled data (bottom centre) using a problem independent hyperprior. The solid blue line and shaded area show the posterior mean and \u00b12 standard deviations respectively. The new estimate of the optimum \u03b8\u0302\u2217 is the value for which the mean estimate is largest, with u\u0302\u2217 equal to the corresponding mean value.\n- Step 4 (purple arrows) constructs an acquisition function \u03b6 : \u03d1\u2192 R+ (bottom left) using the GP posterior. This is optimized, giving the next point to evaluate \u03b8\u0302next, by performing annealed importance sampling on a transformed program q-acq (middle left) in which all observe statements are removed and replaced with a single observe assigning probability \u03b6(\u03b8) to the execution.\n- Step 5 (green arrow) evaluates \u03b8\u0302next using q-marg and continues to step 3."}, {"heading": "4.1 Program Transformation to Generate the Target", "text": "Consider the defopt query q in Figure 3, the body of which defines the joint distribution p (Y, a, \u03b8, b). Calculating (7) (defining X = {a, b}) using a standard optimization scheme presents two issues: \u03b8 is a random variable within the program rather than something we control and its probability distribution is only defined conditioned on a.\nWe deal with both these issues simultaneously using a program transformation similar to the disintegration transformation in Hakaru (Zinkov and Shan, 2016). Our marginal transformation returns a new query object, q-marg as shown in Figure 3, that defines the same joint distribution on program variables and inputs, but\n2Code available at http://www.github.com/probprog/deodorant/\nBAYESIAN OPTIMIZATION FOR PROBABILISTIC PROGRAMS\n3\n-15 -10 -5 0 5 10 15\nEx pe\nct ed\nim pr\nov em\nen t\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n3\n-15 -10 -5 0 5 10 15\n3 -15 -10 -5 0 5 10 15\nlo g\np( Y,\n3 )\n-60\n-40\n-20\n0\n\u2713\u0302next\n{\u2713\u0302\u21e4, \u2326\u0302\u21e4, u\u0302\u21e4}\n\u2713\u0302\u21e4\nu\u0302\u21e4\n1 1\n2\n2\n2\n3\n34\n4 5\n4\n(defopt q [y] [\u2713] (let [a (sample (p-a)) \u2713 (sample (p-\u2713 a)) b (sample (p-b a \u2713))] (observe (lik a \u2713 b) y) [a b])) (a) Original query\n(defquery q-marg [y \u2713\u0302] (let [a (sample (p-a)) \u2713 (observe<- (p-\u2713 a) \u2713\u0302) b (sample (p-b a \u2713))] (observe (lik a \u2713 b) y) [a b])) (b) Conditional query Figure 1: Left: a simple example optimization query where we want to optimize \u2713. Right: the same query after the transformation applied by BOPP to make the query amenable to optimization. Note p-u represents a distribution object, whilst p-\u2713, p-v and lik all represent functions which return distributions objects.\n(defquery q-prior [y] (let [a (sample (p-a))\n\u2713 (sample (p-\u2713 a))] \u2713))\n(a) Prior query\n(defquery q-acq [y \u21e3] (let [a (sample (p-a))\n\u2713 (sample (p-\u2713 a))] (observe (factor) (\u21e3 \u2713)) \u2713))\n(b) Acquisition query\nFigure 2: Left: a transformation of q that samples from the prior p(\u2713). Right: a transformation of q\nused in the optimization of the acquisition function. Observing from factor assigns a probability\nexp \u21e3(\u2713) to the execution, i.e. (factor) returns a distribution of object for which the log probability density function is the identity function.\n1\n(defopt q [y] [\u2713] (let [a (sample (p-a)) \u2713 (sample (p-\u2713 a)) b (sample (p-b a \u2713))] (observe (lik a \u2713 b) y) [a b])) (a) Original query\n(defquery q-marg [y \u2713\u0302] (let [a (sample (p-a)) \u2713 (observe<- (p-\u2713 a) \u2713\u0302) b (sample (p-b a \u2713))] (observe (lik a \u2713 b) y) [a b])) (b) Conditional query Figure 1: Left: a simple example optimization query where we want to optimize \u2713. Right: the same query after the transformation applied by BOPP to make the query amenable to optimization. Note p-u represents a distribution object, whilst p-\u2713, p-v and lik all represent functions which return distributions objects.\n(defquery q-prior [y] (let [a (sample (p-a))\n\u2713 (sample (p-\u2713 a))] \u2713))\n(a) Prior query\n(defquery q-acq [y \u21e3] (let [a (sample (p-a))\n\u2713 (sample (p-\u2713 a))] (observe (factor) (\u21e3 \u2713)) \u2713))\n(b) Acquisition query\nFigure 2: Left: a transformation of q that samples from the prior p(\u2713). Right: a transformation of q\nused in the optimization of the acquisition function. Observing from factor assigns a probability\nexp \u21e3(\u2713) to the execution, i.e. (factor) returns a distribution of object for which the log probability\ndensity function is the identity function.\n(defopt q [y] [\u2713] (let [a (sample (p-a))\n\u2713 (sample (p-\u2713 a)) b (sample (p-b a \u2713))]\n(observe (lik a \u2713 b) y) [a b]))\n(a) Original query\n(defquery q-marg [y \u2713\u0302] (let [a (sample (p-a))\n\u2713 (observe<- (p-\u2713 a) \u2713\u0302) b (sample (p-b a \u2713))]\n(observe (lik a \u2713 b) y) [a b]))\n(b) Conditional query\nFigure 1: Left: a simple example optimization query where we want to optimize \u2713. Right: the same query after the transformation applied by BOPP to make the query amenable to optimization. Note p-u represents a distribution object, whilst p-\u2713, p-v and lik all represent functions which return\ndistributions objects.\n(defquery q-prior [y] (let [a (sample (p-a))\n\u2713 (sample (p-\u2713 a))] \u2713))\n(a) Prior query\n(defquery q-acq [y \u21e3] (let [a (sample (p-a))\n\u2713 (sample (p-\u2713 a))] (observe (factor) (\u21e3 \u2713)) \u2713))\n(b) Acquisition query\nFigure 2: Left: a transformation of q that samples from the prior p(\u2713). Right: a transformation of q used in the optimization of the acquisition function. Observing from factor assigns a probability exp \u21e3(\u2713) to the execution, i.e. (factor) returns a distribution of object for which the log probability density function is the identity function.\n1\n(defopt q [y] [\u2713] (let [a (sample (p-a))\n\u2713 (sample (p-\u2713 a)) b (sample (p-b a \u2713))]\n(observe (lik a \u2713 b) y) [a b]))\n(a) Original query\n(defquery q-marg [y \u2713\u0302] (let [a (sample (p a))\n\u2713 observe<- ( -\u2713 a) \u2713\u0302 b (sample p-b a \u2713))]\n(observe (lik a \u2713 b) y) [a b]))\n(b) Conditional query\nFigure 1: Left: a simple example optimization query where we want to optimize \u2713. Right: the same\nquery after the transformation applied by BOPP to make the query amenable to optimization. Note p-u represents a distribution object, whilst p-\u2713, p-v and lik all represent functions which return distributions objects.\n(a) Prior query\n(b) Acquisition query\nFigure 2: Left: a transformation of q that samples from the prior p(\u2713). Right: a transformation of q used in the optimization of the acquisition function. Observing from factor assigns a probability exp \u21e3(\u2713) to the execution, i.e. (factor) returns a distribution of object for which the log probability density function is the identity function.\n1\nnow accepts the value for \u03b8 as an input. This is done by replacing all sample statements associated with \u03b8 with equivalent observe<- statements, taking \u03b8 as the observed value, where observe<- is identical to observe except that it returns the observed value. As both sample and observe operate on the same variable type - a distribution object - this transformation can always be made, while the identical returns of sample and observe<- trivially ensures validity of the transformed program."}, {"heading": "4.2 Bayesian Optimization of the Marginal", "text": "The target function for our BO scheme is log p(Y, \u03b8), noting argmax f (\u03b8) = argmax log f (\u03b8) for any f : \u03d1\u2192 R+. The log is taken because GPs have unbounded support, while p (Y, \u03b8) is always positive, and because we expect variations over many orders of magnitude. PPS with importance sampling based inference engines, e.g. sequen al Monte Carlo (Wood et al., 2014) or th particle cascade (P ige et al., 2014), can return noisy stimates f this target giv n the transformed program q-marg.\nOur BO scheme uses a GP prior and a Gaussian likelihood. Though the rationale for the latter is predominantly computational, giving an analytic posterior, there are also theoretical results suggesting that this choice is appropriate (Be\u0301rard et al., 2014). We use as a default covariance function a combination of a Mate\u0301rn-3/2 and Mate\u0301rn-5/2 kernel. Specifically, let D = \u2016\u03b8\u20160 be the dimensionality of \u03b8 and define\nd3/2(\u03b8, \u03b8 \u2032) = \u221a\u221a\u221a\u221a D\u2211 i=1 \u03b8i \u2212 \u03b8\u2032i \u03c1i\n(8a)\nd5/2(\u03b8, \u03b8 \u2032) = \u221a\u221a\u221a\u221a D\u2211 i=1 \u03b8i \u2212 \u03b8\u2032i %i\n(8b)\nwhere i indexes a dimension of \u03b8 and \u03c1i and %i are dimension specific length scale hyperparameters. Our prior covariance function is now given by\nkprior (\u03b8, \u03b8 \u2032) =\u03c323/2\n( 1 + \u221a 3d3/2 (\u03b8, \u03b8 \u2032) ) exp ( \u2212 \u221a 3d3/2 (\u03b8, \u03b8 \u2032) ) +\n\u03c325/2\n( 1 + \u221a 5d5/2 (\u03b8, \u03b8 \u2032) + 5\n3 (d5/2 (\u03b8, \u03b8\n\u2032))2 ) exp ( \u2212 \u221a 5d5/2 (\u03b8, \u03b8 \u2032) ) (9)\nwhere \u03c33/2 and \u03c35/2 represent signal standard deviations for the two respective kernels. The full set of GP hyperparameters is defined by \u03b1 = {\u03c3n, \u03c33/2, \u03c35/2, \u03c1i=1:D, %i=1:D}. A key feature of this kernel is that it is only once differentiable and therefore makes relatively weak assumptions about the smoothness of f . The ability to include branching in a probabilistic program means that, in some cases, an even less smooth kernel than (9) might be preferable. However, there is clear a trade-off between generality of the associated reproducing kernel Hilbert space and modelling power.\nAs noted by (Snoek et al., 2012), the performance of BO using a single GP posterior is heavily influenced by the choice of these hyperparameters. We therefore exploit the automated domain scaling introduced in Section 4.3 to define a problem independent hyperprior p(\u03b1) and perform inference to give a mixture of GPs posterior. Details on this hyperprior are given in Appendix B.\nInference over \u03b1 is performed using Hamiltonian Monte Carlo (HMC) (Duane et al., 1987), giving an unweighted mixture of GPs. Each term in this mixture has an analytic distribution fully specified by its mean function \u00b5im : \u03d1\u2192 R and covariance function kim : \u03d1\u00d7 \u03d1\u2192 R, where m indexes the BO iteration and i the hyperparameter sample. HMC was chosen because of the availability of analytic derivatives of the GP log marginal likelihoods. As we found that the performance of HMC was often poor unless a good initialization point was used, BOPP runs a small number of independent chains and allocates part of the computational budget to their initialization using a L-BFGS optimizer (Broyden, 1970).\nThe inferred posterior is first used to estimate which of the previously evaluated \u03b8\u0302j is the most optimal, by taking the point with highest expected value , u\u0302\u2217m = maxj\u22081...m \u2211N i=1 \u00b5 i m(\u03b8\u0302j). This completes the definition of the output sequence returned by the doopt macro. Note that as the posterior updates globally with each new observation, the relative estimated optimality of previously evaluated points changes at each iteration. Secondly it is used to define the acquisition function \u03b6, for which we take the expected improvement (Snoek et al., 2012), defining \u03c3im (\u03b8) = \u221a kim (\u03b8, \u03b8) and \u03b3 i m (\u03b8) = \u00b5im(\u03b8)\u2212u\u0302\u2217m \u03c3im(\u03b8) ,\n\u03b6 (\u03b8) = N\u2211 i=1 ( \u00b5im (\u03b8)\u2212 u\u0302\u2217m ) \u03a6 ( \u03b3im (\u03b8) ) + \u03c3im (\u03b8)\u03c6 ( \u03b3im (\u03b8) ) (10)\nwhere \u03c6 and \u03a6 represent the pdf and cdf of a unit normal distribution respectively. We note that more powerful, but more involved, acquisition functions, e.g. (Herna\u0301ndez-Lobato et al., 2014), could be used instead."}, {"heading": "4.3 Automatic and Adaptive Domain Scaling", "text": "Domain scaling, by mapping to a common space, is crucial for BOPP to operate in the required blackbox fashion as it allows a general purpose and problem independent hyperprior to be placed on the GP hyperparameters. BOPP therefore employs an affine scaling to a [\u22121, 1] hypercube for both the inputs and outputs of the GP. To initialize scaling for the input variables, we sample directly from the generative model defined by the program. This is achieved using a second transformed program, q-prior, which removes all conditioning, i.e. observe statements, and returns \u03b8. This transformation also introduces code to terminate execution of the query once all \u03b8 are sampled, in order to avoid unnecessary computation. As observe statements return nil, this transformation trivially preserves the generative model of the program, but the\nprobability of the execution changes. Simulating from the generative model does not require inference or calling potentially expensive likelihood functions and is therefore computationally inexpensive. By running inference on q-marg given a small number of these samples as arguments, a rough initial characterization of output scaling can also be achieved. If points are observed that fall outside the hypercube under the initial scaling, the domain scaling is appropriately updated3 so that the target for the GP remains the [\u22121, 1] hypercube."}, {"heading": "4.4 Unbounded Bayesian Optimization via Non-Stationary Mean Function Adaptation", "text": "Unlike standard BO implementations, BOPP is not provided with external constraints and we therefore develop a scheme for operating on targets with potentially unbounded support. Our method exploits the knowledge that the target function is a probability density, implying that the area that must be searched in practice to find the optimum is finite, by defining a non-stationary prior mean function. This takes the form of a bump function that is constant within a region of interest, but decays rapidly outside. Specifically we define this bump function in the transformed space as\n\u00b5prior (r; re, r\u221e) = { 0 if r \u2264 re log (\nr\u2212re r\u221e\u2212re ) + r\u2212rer\u221e\u2212re otherwise\n(11)\nwhere r is the radius from the origin, re is the maximum radius of any point generated in the initial scaling or subsequent evaluations, and r\u221e is a parameter set to 1.5re by default. Consequently, the acquisition function also decays and new points are never suggested arbitrarily far away. Adaptation of the scaling will automatically update this mean function appropriately, learning a region of interest that matches that of the true problem, without complicating the optimization by over-extending this region. We note that our method shares similarity with the recent work of Shahriari et al (Shahriari et al., 2016a), but overcomes the sensitivity of their method upon a user-specified bounding box representing soft constraints, by initializing automatically and adapting as more data is observed."}, {"heading": "4.5 Optimizing the Acquisition Function", "text": "Optimizing the acquisition function for BOPP presents the issue that the query contains implicit constraints that are unknown to the surrogate function. The problem of unknown constraints has been previously covered in the literature (Gardner et al., 2014; Herna\u0301ndez-Lobato et al., 2016) by assuming that constraints take the form of a black-box function which is modeled with a second surrogate function and must be evaluated in guess-and-check strategy to establish whether a point is valid. Along with the potentially significant expense such a method incurs, this approach is inappropriate for equality constraints or when the target variables are potentially discrete. For example, the Dirichlet distribution in Figure 2 introduces an equality constraint on powers, namely that its components must sum to 1.\nWe therefore take an alternative approach based on directly using the program to optimize the acquisition function. To do so we consider a transformed program q-acq that is identical to q-prior (see Section 4.3), but adds an additional observe statement that assigns a weight \u03b6(\u03b8) to the execution. By setting \u03b6(\u03b8) to the acquisition function, the maximum likelihood corresponds to the optimum of the acquisition function subject to the implicit program constraints. We obtain a maximum likelihood estimate for q-acq using a variant of annealed importance sampling (Neal, 2001) in which lightweight Metropolis Hastings (LMH) (Wingate et al., 2011) with local random-walk moves is used as the base transition kernel."}, {"heading": "5. Experiments", "text": "We first demonstrate the ability of BOPP to carry out unbounded optimization using a 1D problem with a significant prior-posterior mismatch as shown in Figure 4. It shows BOPP adapting to the target and effectively\n3An important exception is that the output mapping to the bottom of the hypercube remains fixed such that low likelihood new points are not incorporated. This ensures stability when considering unbounded problems.\nFigure 4: Convergence of BOPP on unconstrained bimodal problem with p (\u2713) = Normal(0, 0.5) and p (Y |\u2713) = Normal(5 |\u2713| , 0.5) giving significant prior misspecification. The top plots show the regressed GP, with the solid line corresponding to the mean and the shading shows \u00b1 2 standard deviations. Below is the corresponding acquisition function which away from the region of interest.\nOptimizing the acquisition function for BOPP presents the issue that the query contains implicit\nconstraints that are unknown to the surrogate function. The problem of unknown constraints has\nbeen previously covered in the literature [8, 11] by assuming that constraints take the form of a\nblack-box function which is modelled with a second surrogate function and must be evaluated in\nguess-and-check strategy to establish whether a point is valid. Along with the potentially significance\nexpense such a method incurs, this approach is inappropriate for equality constraints or when the\ntarget variables are potentially discrete.\nWe therefore take an alternative approach based on directly using the program to optimize the acquisition function. To do so we consider use a transformed program q-acq that is identical to q-prior (see Section 4.3), but adds an additional observe statement that assigns a weight \u21e3(\u2713) to the execution. By setting \u21e3(\u2713) to the acquisition function, the maximum likelihood corresponds to the optimum of the acquisition function subject to the implicit program constraints. We obtain a maximum likelihood estimate for q-acq using a variant of annealed importance sampling [18] in which lightweight Metropolis Hastings (LMH) [29] with local random-walk moves is used as the base transition kernel.\n5 Experiments\nWe first demonstrate the ability of BOPP to carry out unbounded optimization using a 1D problem with a significant prior-posterior mismatch as shown in Figure 4. It shows BOPP adapting to the target and effectively establishing a maxima in the presence of multiple modes. After 20 evaluations the acquisitions begin to explore the left mode, after 50 both modes have been fully uncovered.\nNext we compare BOPP to the prominent BO packages SMAC [12], Spearmint [26] and TPE [3] on a number of classical benchmarks as shown in Figure 5. These results demonstrate that BOPP provides substantial advantages over these systems when used simply as an optimizer on both continuous and discrete optimization problems.\nFinally we demonstrate performance of BOPP on a MMAP problem. Comparison here is more difficult due to the dearth of existing alternatives for PPS. In particular, simply running inference does not return estimates of the density function p (Y, \u2713). We consider the possible alternative of using our conditional code transformation to design a particle marginal Metropolis Hastings (PMMH,\nFigure 4: Convergence of BO P on unconstrained bimodal problem with p (\u2713) = Normal(0, 0.5) and p (Y |\u2713) = Normal(5 |\u2713| , 0.5) giving significant prior mi specification. The top plot show the regre sed GP, with the solid line corresponding to the mean and the shading shows \u00b1 2 standard deviations. Below is the corresponding acquis tion function which away from the region of interest.\nOptimizing the acquis tion function for BO P presents the i sue tha the query contains implicit\nconstraints that are unknown to the surrogate function. The problem of unknown constraints has\nb en previously covered in the literature [8, 1] by a suming that constraints take the form of a\nblack-box function which is modelled with a second surrogate function and must be evaluated in\ngue s-and-check strategy to establish whether a point is valid. Along with the potentially significance\nexpense such a method incurs, this a proach is ina propriate for equality constraints or when the\ntarget variables are potentially discrete.\nWe therefore take an alternative a proach based on directly using the program to optimize the acquis tion function. To do so we consider use a transformed program q-acq that is identical to q-prior (s e Section 4.3), but a ds an a d tional observe statemen that assigns a weight \u21e3(\u2713) to the execution. By setting \u21e3(\u2713) to the acquis tion function, the maximum likelih od corresponds to the optimum of the acquis tion function subjec to the implicit program constraints. We obtain a maximum likelih od estimate for q-acq using a variant of a nealed importance sampling [18] in which lightweight Metropolis Hastings (LMH) [29] with local random-walk moves is used as the base trans tion kernel.\n5 Experiments\nWe first demonstrate the ab lity of BO P to carry out unbounded optimization using a 1D problem with a significant prior-posterior mismatch as shown in Figure 4. It shows BO P adapting to the target and effectively establishing a maxima in the presence of multiple modes. After 20 evaluations the acquis tions begin to explore the left mode, after 50 both modes have b en fully uncovered.\nNext we compare BO P to the prominent BO packages SMAC [12], Spearmint [26] and TPE [3] on a number of cla sical benchmarks a shown in Figure 5. These results demonstrate that BO P provides substantial advantages over these systems when used simply as an optimizer on both continuous and discrete optimization problems.\nFinally we demonstrate performance of BO P on a MMAP problem. Comparison here is more difficult due to the dearth of existing alternatives for PS. In particular, simply ru ning inference does not return estimates of the density function p (Y, \u2713). We consider the possible alternative of using our cond tional code transformation to design a particle marginal Metropolis Hastings (PMMH,\n7\nFigure 4: Conv rg nce of BOPP o u constrained bimodal problem with p (\u2713) = Normal( , 0.5) and p (Y |\u2713) = Normal(5 \u2713| , 0.5) g vin s gnificant prior misspe ification. The to plots show the gr ssed GP, with the so d line correspo ding to th mea and the shading shows \u00b1 2 st n ard deviations. Below is the correspo ding acquisition function w ich away from the regi n of int rest.\nOptim zing the acqu sition function for BOPP pr sents the issue that the query contains impl cit\nconstraints hat are unknown to the surrogate function. The problem of unknown constraints has\nbeen previously cov red in the literature [8, 11] by assuming that constraints take the form of a\nblack-box function w ich is modelled with a second surrogate functio and must b ev luated in\nguess-and- heck strategy to establis w ther a po nt is valid. Along with the potentially signifi ance\nxp nse such a method incurs, this approach is inap opriate for equality constraints or when the\ntarget variables are potentially discr te.\nWe th refore take n alternative approach based on directly using the p ogram t optimize the acqu sition function. T d so we consider use transformed p ogram q-acq that is identical to q-prior (se Section 4.3), but adds n additional observe stat ment that assigns a weight \u21e3(\u2713) to th xecution. By setting \u21e3(\u2713) to the acqu sition function, the maxi um likelihood corresponds to the opti um of the acqu sition function subject to the impl cit p ogram constraints. We obtain a maxi um likelihood estimate for q-acq using variant of annealed importance sampling [18] in w ich lightweight Metropolis Hastings (LMH) [29] with loc l random-walk move is used as the base transition k rnel.\n5 Experiments\nWe first demonstrate the ability of BOPP to carry out unboun ed optimization using a 1D problem with a s gnificant prior-posterior ismatch as shown in Figure 4. It shows BOPP dapting to the target and effectiv ly establishing maxima in the pr s nce of mu tiple modes. After 20 evaluations the acqu sitions begin to explor the left mode, after 50 both modes have been fully uncov red.\nNext we compare BOPP to the promi ent BO p ckages SMAC [12], Spearmint [26] and TPE [3] on a number of classical benchmarks as shown in Figure 5. Th se re ults demonstrate that BOPP provides ubstantial dv ntages over th se ystems when used simply as an optimizer on both continuous and discr te optimization problems.\nFinally w demonstrat performance of BOPP on a MAP problem. Comparison h re is more difficult due to th dearth of existing alternatives for PPS. In particular, simply ru ing inf r nce does no return estimates of th density function p (Y, \u2713). We consider the possible alternative of using our conditional code transformation to design a particle marginal Metr polis Hastings (P MH,\n7\nFigure 4: Convergence of BO P on u constrained bimodal problem with p (\u2713) = Normal(0, 0.5) and p (Y |\u2713) = Normal(5 \u2713| , 0.5) vin significant prior misspec fication. The t plot show the regressed GP, with the solid line corresponding to th mean and t e shading hows \u00b1 2 st ndard deviations. Below is the corresponding acqu sitio fu ction which away from the regio of int rest.\nOptim zing the acqu sitio fu cti n for BO P pr ents the issue that the query co ta ns impl cit\nconstraints that are unk own to the surrogate fu ction. The problem of unknown constraint has\nbe n previously cov red in h li e ature [8, 11] by assuming that constraints ak the f rm of a\nblack-box fu ction wh ch is mo elled with a second surrogate fu ction and must b evaluated in\nguess-and-check strategy to e tablis whether a point s valid. Along with th potentially sig ificance\nxpense such a method incur , this approach is inappropriat for equality constraints or w n the\narget variables ar potentially discr te.\nW th for take an al ernative approach based on directly using the program t optimiz the acqu sitio fu ction. To do so we consid r use a transformed program -acq that is den ical to q-p ior (s e Section 4.3), but dds an additi nal obs rv s atement that a signs a weight \u21e3(\u2713) to th execution. By setting \u21e3(\u2713) to the acqu sitio fu c ion, the axi um kelih od corresponds to the optimum of the acqu sitio fu ction subject to the impl cit program constraints. We obt in a\naximum kelihood estimate for -acq using a variant of nnealed importance sampling [18] in which lightweight Metropolis Ha tings (LMH) [29] with local random-walk move is u ed as the b se tra sitio kernel.\n5 Experiments\nWe first demons rat the ability of BO P to carry out bounded optim zatio using a 1D problem with a significant pri r-posterior ism tch a shown in Figure 4. It hows BO P adapting to the t rget and effectiv ly e tablishing xima in th pr sence of multiple modes. After 20 evaluations the acqu sitions begin to explor the left mod , after 50 both modes hav been fully uncov red.\nN xt we compare BOPP to the prominent BO packages SMAC [12], Spearmint [26] and TPE [3] on a number of lassical benchmark a shown n Figure 5. Th se results demons r e that BO P provides subst ntial advantag s over th e systems wh n u ed simply as an optimizer on b th continuous and discr te optimizati n problems.\nFinally we demonstrate performance of BO P on a MAP problem. Comparison h re is more difficult due to the dearth of existing al ernatives for PS. In particular, simply run g i f r nce d es not eturn estimates of the density fu ction p (Y, \u2713). We consider the possib e al ernative of using ur conditional code transformati n to design a particle m rginal Metropolis Hastings (P MH,\n7\np(Y, \u2713)p(Y, \u2713)p(Y, \u2713)p(Y, \u2713)\nlo g\np (Y\n,\u2713 )\nlo g\np (Y\n,\u2713 )\nlo g\np (Y\n,\u2713 )\nlo g\np (Y\n,\u2713 )\nFigure 4: Convergence on an unconstrained bimodal proble with p (\u03b8) = Normal(0, 0.5) and p (Y |\u03b8) = Normal(5\u2212 |\u03b8| , 0.5) giving significant prior misspecification. The top lots show a regressed GP, with the solid line corr sponding to t mea and the sha i sh ws \u00b1 2 sta d r viations. T e bottom plots show the c rresponding acquisition functions.\nestablishing a maxima in the presence of multiple modes. After 20 evaluations the acquisitions begin to explore the right mode, after 50 both modes have been fully uncovered."}, {"heading": "5.1 Classic Optimizatio Benchmarks", "text": "Next we compare BOPP to the prominent BO packages SMAC Hutter et al. (2011), Spearmint Snoek et al. (2012) and TPE Bergstra et al. (2011) on a number of classical benchmarks as shown in Figure 5. These results demonstrate that BOPP provides substantial advantages over these systems when used simply as an optimizer on both continuous and discrete optimization problems. In particular, it offers a large advantage over SMAC and TPE on the continuous problems (Branin and Hartmann), due to using a more powerful surrogate, and over Spearmint on the others due to not needing to make approximations to deal with discrete problems."}, {"heading": "5.2 Marginal Maximum a Posteriori Estimation Problems", "text": "We now demonstrate application of BOPP on a number of MMAP problems. Comparisons here are more difficult due to the dearth of existing alternatives for PPS. In particular, simply running inference on the original\n10\n(defopt mvn-mixture [data mu0 kappa psi] [nu alpha] (let [[n d] (shape data)\nquery does not return estimates for p (Y, \u03b8). We consider the possible alternative of using our conditional code transformation to design a particle marginal Metropolis Hastings (PMMH, Andrieu et al. (2010)) sampler which operates in a similar fashion to BOPP except that new \u03b8 are chosen using a MH step instead of actively sampling with BO. For these MH steps we consider both LMH (Wingate et al., 2011) with proposals from the prior and the random-walk MH (RMH) variant introduced in Section 4.5."}, {"heading": "5.2.1 HYPERPARAMETER OPTIMIZATION FOR GAUSSIAN MIXTURE MODEL", "text": "We start with an illustrative case study of optimizing the hyperparameters in a multivariate Gaussian mixture model. We consider a Bayesian formulation with a symmetric Dirichlet prior on the mixture weights and a Gaussian-inverse-Wishart prior on the likelihood parameters:\n\u03c0 \u223c Dir(\u03b1, . . . , \u03b1) (12) (\u00b5k,\u03a3k) \u223c NIW(\u00b50, \u03ba,\u03a8, \u03bd) for k = 1, . . . ,K (13)\nzn \u223c Disc(\u03c0) (14) yn \u223c Norm(\u00b5zn ,\u03a3zn) for n = 1, . . . , N (15)\nAnglican code for this model is shown in Figure 4. Anglican provides stateful objects, which are referred to as random processes, to represent the predictive distributions for the cluster assignments z and the observations yk assigned to each cluster\nzn+1 \u223c p(\u00b7 | z1:n, \u03b1), (16) ykm+1 \u223c p(\u00b7 |yk1:m,\u00b50, \u03ba,\u03a8, \u03bd). (17)\nIn this collapsed representation marginalization over the model parameters \u03c0, \u00b5k=1:K , and \u03a3k=1:K is performed analytically. Using the Iris dataset, a standard benchmark for mixture models that contains 150 labeled examples with 4 real-valued features, we optimize the marginal with respect to the subset of the\nparameters \u03bd and \u03b1 under uniform priors over a fixed interval. For this model, BOPP aims to maximize\np(\u03bd, \u03b1|yn=1:N ,\u00b50, \u03ba,\u03a8)\n= \u222b\u222b\u222b\u222b p(\u03bd, \u03b1, zn=1:N ,\u03c0,\u00b5k=1:K ,\u03a3k=1:K |yn=1:N , \u00b50, \u03ba,\u03a8)dzn=1:Nd\u03c0d\u00b5k=1:Kd\u03a3k=1:K . (18)\nFigure 7 shows GP regressions on the evidence after different numbers of the SMC evaluations have been performed on the model. This demonstrates how the GP surrogate used by BO builds up a model of the target, used to both estimate the expected value of log p(Y, \u03b8) for a particular \u03b8 and actively sample the \u03b8 at which to undertake inference."}, {"heading": "5.2.2 EXTENDED KALMAN FILTER FOR THE PICKOVER CHAOTIC ATTRACTOR", "text": "We next consider the case of learning the dynamics parameters of a chaotic attractor. Chaotic attractors present an interesting case for tracking problems as, although their underlying dynamics are strictly deterministic with bounded trajectories, neighbouring trajectories diverge exponentially4. Therefore regardless of the available precision, a trajectory cannot be indefinitely extrapolated to within a given accuracy and probabilistic methods such as the extended Kalman filter must be incorporated (Fujii, 2013; Ruan et al., 2003). From an empirical perspective, this forms a challenging optimization problem as the target transpires to be multi-modal, has variations at different length scales, and has local minima close to the global maximum.\nSuppose we observe a noisy signal yt \u2208 RK , t = 1, 2, . . . , T in some K dimensional observation space were each observation has a lower dimensional latent parameter xt \u2208 RD, t = 1, 2, . . . , T whose dynamics correspond to a chaotic attractor of known type, but with unknown parameters. Our aim will be to find the MMAP values for the dynamics parameters \u03b8, marginalizing out the latent states. The established parameters can then be used for forward simulation or tracking.\n4It is beyond the scope of this paper to properly introduce chaotic systems. We refer the reader to Devaney et al. (1989) for an introduction.\nTo carry out the required MMAP estimation, we apply BOPP to the extended Kalman smoother\nx1 \u223cN (\u00b51, \u03c31I) (19) xt =A (xt\u22121, \u03b8) + \u03b4t\u22121, \u03b4t\u22121 \u223c N (0, \u03c3qI) (20) yt =Cxt + \u03b5t, \u03b5t \u223c N (0, \u03c3yI) (21)\nwhere I is the identity matrix, C is a known K \u00d7D matrix, \u00b51 is the expected starting position, and \u03c31, \u03c3q and \u03c3y are all scalars which are assumed to be known. The transition function A (\u00b7, \u00b7)\nxt,1 = sin (\u03b2xt\u22121,2)\u2212 cos (\n5xt\u22121,1 2\n) xt\u22121,3 (22a)\nxt,2 =\u2212 sin (\n3xt\u22121,1 2\n) xt\u22121,3 \u2212 cos (\u03b7xt\u22121,2) (22b)\nxt,3 = sin (xt\u22121,1) (22c)\ncorresponds to a Pickover attractor (Pickover, 1995) with unknown parameters \u03b8 = {\u03b2, \u03b7} which we wish to optimize. Note that \u03b7 and \u2212\u03b7 will give the same behaviour.\nSynthetic data was generated for 500 time steps using the parameters of \u00b51 = [\u22120.2149,\u22120.0177, 0.7630]T , \u03c31 = 0, \u03c3q = 0.01, \u03c3y = 0.2, a fixed matrix C where K = 20 and each column was randomly drawn from a symmetric Dirichlet distribution with parameter 0.1, and ground truth transition parameters of \u03b2 = \u22122.3 and \u03b7 = 1.25 (note that the true global optimum for finite data need not be exactly equal to this).\nMMAP estimation was performed on this data using the same model and parameters, with the exceptions of \u03b8, \u00b51 and \u03c31. The prior on \u03b8 was set to a uniform in over a bounded region such that\np (\u03b2, \u03b7) = { 1/18, if \u2212 3 \u2264 \u03b2 \u2264 3 \u2229 0 \u2264 \u03b7 \u2264 3 0, otherwise . (23)\nThe changes \u00b51 = [0, 0, 0] and \u03c31 = 1 were further made to reflect the starting point of the latent state being unknown. For this problem, BOPP aims to maximize\np(\u03b2, \u03b7|yt=1:T ) = \u222b p(\u03b2, \u03b7, xt=1:T |yt=1:T )dxt=1:T . (24)\nInference on the transformed marginal query was carried out using SMC with 500 particles. Convergence results are given in Figure 8 showing that BOPP comfortably outperforms the PMMH variants, while Figure 9 shows the simulated attractors generated from the dynamics parameters output by various iterations of a particular run of BOPP."}, {"heading": "5.2.3 HIDDEN MARKOV MODEL WITH UNKNOWN NUMBER OF STATES", "text": "We finally consider a hidden Markov model (HMM) with an unknown number of states. This example demonstrates how BOPP can be applied to models which conceptually have an unknown number of variables, by generating all possible variables that might be needed, but then leaving some variables unused for some execution traces. This avoids problems of varying base measures so that the MMAP problem is well defined and provides a function with a fixed number of inputs as required by the BO scheme. From the BO perspective, the target function is simply constant for variations in an unused variable.\nHMMs are Markovian state space models with discrete latent variables. Each latent state xt \u2208 {1, . . . ,K}, t = 1, . . . , T is defined conditionally on xt\u22121 through a set of discrete transition probabilities, whilst each output yt \u2208 R is considered to be generated i.i.d. given xt. We consider the following HMM, in which the number of states K, is also a random variable:\nK \u223c Discrete{1, 2, 3, 4, 5} (25) Tk \u223c Dirichlet{11:K}, \u2200k = 1, . . . ,K (26)\n\u03c6k \u223c Uniform[0, 1], \u2200k = 1, . . . ,K (27) \u00b50 \u2190 min{y1:T } (28) \u00b5k \u2190 \u00b5k\u22121 + \u03c6k \u00b7 (max{y1:T } \u2212 \u00b5k\u22121), \u2200k = 1, . . . ,K (29) x1 \u2190 1 (30)\nxt|xt\u22121 \u223c Discrete{Txt\u22121} (31) yt|xt \u223c N (\u00b5(xt\u22121), 0.2). (32)\nOur experiment is based on applying BOPP to the above model to do MMAP estimation with a single synthetic dataset, generated using K = 3, \u00b51 = \u22121, \u00b52 = 0, \u00b53 = 4, T1 = [0.9, 0.1, 0], T2 = [0.2, 0.75, 0.05] and T3 = [0.1, 0.2, 0.7].\nWe use BOPP to optimize both the number of states K and the stick-breaking parameters \u03c6k, with full inference performed on the other parameters. BOPP therefore aims to maximize\np(K,\u03c6k=1:5|yt=1:T ) = \u222b\u222b p(K,\u03c6k=1:5, xt=1:T , Tk=1:K |yt=1:T )dxt=1:TdTk=1:K . (33)\nAs with the chaotic Kalman filter example, we compare to two PMMH variants using the same code transformations. The results, given in Figure 10, again show that BOPP outperforms these PMMH alternatives."}, {"heading": "6. Discussion and Future Work", "text": "We have introduced a new method for carrying out MMAP estimation of probabilistic program variables using Bayesian optimization, representing the first unified framework for optimization and inference of probabilistic programs. By using a series of code transformations, our method allows an arbitrary program to be optimized with respect to a defined subset of its variables, whilst marginalizing out the rest. To carry out the required optimization, we introduce a new GP-based BO package that exploits the availability of the target source code to provide a number of novel features, such as automatic domain scaling and constraint satisfaction.\nThe concepts we introduce lead directly to a number of extensions of interest, including but not restricted to smart initialization of inference algorithms, adaptive proposals, and nested optimization. Further work might consider maximum marginal likelihood estimation and risk minimization. Though only requiring minor algorithmic changes, these cases require distinct theoretical considerations."}, {"heading": "Appendix A. Program Transformations in Detail", "text": "In this section we give a more detailed and language specific description of our program transformations, code for which can be found at http://www.github.com/probprog/bopp.\nA.1 Anglican\nAnglican is a probabilistic programming language integrated into Clojure (a dialect of Lisp) and inherits most of the corresponding syntax. Anglican extends Clojure with the special forms sample and observe (Tolpin et al., 2015). Each random draw in an Anglican program corresponds to a sample call, which can be thought of as a term in the prior. Each observe statement applies weighting to a program trace and thus constitutes a term in the likelihood. Compilation of an Anglican program, performed by the macro query, corresponds to transforming the code into a variant of continuation-passing style (CPS) code, which results in a function that can be executed using a particular inference algorithm.\nAnglican program code is represented by a nested list of expressions, symbols, non-literals for contructing data structures (e.g. [...] for vectors), and command dependent literals (e.g. [...] as a second argument of a let statement which is used for binding pairs). In order to perform program transformations, we can recursively traverse this nested list which can be thought of as an abstract syntax tree of the program.\nOur program transformations also make use of the Anglican forms store and retrieve. These allow storing any variable in the probabilistic program\u2019s execution trace in a state which is passed around during execution and from which we can retrieve these stored values. The core use for this is to allow the outer query to return variables which are only locally scoped.\nTo allow for the early termination that will be introduced in Section A.5, it was necessary to add a mechanism for non-local returns to Anglican. Clojure supports non-local returns only through Java exception handling, via the keywords try throw, catch and finally. Unfortunately, these are not currently supported by Anglican and their behaviour is far from ideal for our purposes. In particular, for programs containing nested try statements, throwing to a particular try in the stack, as opposed to the most recently invoked, is cumbersome and error prone.\nWe have instead, therefore, added to Anglican a non-local return mechanism based on the Common Lisp control form catch/throw. This uses a catch tag to link each throw to a particular catch. For example\n(catch :tag (when (> a 0) (throw :tag a))\n0)\nis equivalent to (max a 0). More precisely, throw has syntax (throw tag value) and will cause the catch block with the corresponding tag to exit, returning value. If a throw goes uncaught, i.e. it is not contained within a catch block with a matching tag, a custom Clojure exception is thrown.\nA.2 Representations in the Main Paper\nIn the main paper we presented the code transformations as static transformations as shown in Figure 3. Although for simple programs, such as the given example, these transformations can be easily expressed as static transformations, for more complicated programs it would be difficult to actually implement these as purely static generic transformations in a higher-order language. Therefore, even though all the transformations dynamically execute as shown at runtime, in truth, the generated source code for the prior and acquisition transformations varies from what is shown and has been presented this way in the interest of exposition. Our true transformations exploit store, retrieve, catch and throw to generate programs that dynamically execute in the same way at run time as the static examples shown, but whose actual source code varies significantly.\nA.3 Prior Transformation\nThe prior transformation recursively traverses the program tree and applies two local transformations. Firstly it replaces all observe statements by nil. As observe statements return nil, this trivially preserves the generative model of the program, but the probability of the execution changes. Secondly, it inspects the binding variables of let forms in order to modify the binding expressions for the optimization variables, as specified by the second input of defopt, asserting that these are directly bound to a sample statement of the form (sample dist). The transformation then replaces this expression by one that stores the result of this sample in Anglican\u2019s store before returning it. Specifically, if the binding variable in question is phi-k, then the original binding expression (sample dist) is transformed into\n(let [value (sample dist)] ;; Store the sampled value in Anglican\u2019s store (store OPTIM-ARGS-KEY\n\u2019phi-k value)\nvalue)\nAfter all these local transformation have been made, we wrap the resulting query block in a do form and append an expression extracting the optimization variables using Anglican\u2019s retrieve. This makes the optimization variables the output of the query. Denoting the list of optimization variable symbols from defopt as optim-args and the query body after applying all the above location transformations as . . . , the prior query becomes\n(query query-args (do ... (map (fn [x] (retrieve OPTIM-ARGS-KEY x))\noptim-args)))\nNote that the difference in syntax from Figure 3 is because defquery is in truth a syntactic sugar allowing users to bind query to a variable. As previously stated, query is macro that compiles an Anglican program to its CPS transformation. An important subtlety here is that the order of the returned samples is dictated by optim-args and is thus independent of the order in which the variables were actually sampled, ensuring consistent inputs for the BO package.\nWe additionally add a check (not shown) to ensure that all the optimization variables have been added to the store, and thus sampled during the execution, before returning. This ensures that our assumption that each optimization variable is assigned for each execution trace is satisfied.\nA.4 Acquisition Transformation\nThe acquisition transformation is the same as the prior transformation except we append the acquisition function, ACQ-F, to the inputs and then observe its application to the optimization variables before returning. The acquisition query is thus\n(query [query-args ACQ-F] (do ... (let [theta (map (fn [x] (retrieve OPTIM-ARGS-KEY x))\noptim-args)] (observe (factor) (ACQ-F theta)) theta)))\nA.5 Early Termination\nTo ensure that q-prior and q-acq are cheap to evaluate and that the latter does not include unnecessary terms which complicate the optimization, we wish to avoid executing code that is not required for generating\nthe optimization variables. Ideally we would like to directly remove all such redundant code during the transformations. However, doing so in a generic way applicable to all possible programs in a higher order language represents a significant challenge. Therefore, we instead transform to programs with additional early termination statements, triggered when all the optimization variables have been sampled. Provided one is careful to define the optimization variables as early as possible in the program (in most applications, e.g. hyperparameter optimization, they naturally occur at the start of the program), this is typically sufficient to ensure that the minimum possible code is run in practise.\nTo carry out this early termination, we first wrap the query in a catch block with a uniquely generated tag. We then augment the transformation of an optimization variable\u2019s binding described in Section A.3 to check if all optimization variables are already stored, and invoke a throw statement with the corresponding tag if so. Specifically we replace relevant binding expressions (sample dist) with\n(let [value (sample dist)] ;; Store the sampled value in Anglican\u2019s store (store OPTIM-ARGS-KEY\n\u2019phi-k value)\n;; Terminate early if all optimization variables are sampled (if (= (set (keys (retrieve OPTIM-ARGS-KEY)))\n(set optim-args)) (throw BOPP-CATCH-TAG prologue-code) value))\nwhere prologue-code refers to one of the following expressions depending on whether it is used for a prior or an acquisition transformation\n;; Prior query prologue-code (map (fn [x] (retrieve OPTIM-ARGS-KEY x))\noptim-args)\n;; Acquisition query prologue-code (do (let [theta (map (fn [x] (retrieve OPTIM-ARGS-KEY x))\noptim-args)] (observe (factor) (ACQ-F theta)) theta))\nWe note that valid programs for both q-prior and q-acq should always terminate via one of these early stopping criteria and therefore never actually reach the appending statements in the query blocks shown in Sections A.3 and A.4. As such, these are, in practise, only for exposition and error catching.\nA.6 Marginal/MMAP Transformation\nThe marginal transformation inspects all let binding pairs and if a binding variable phi-k is one of the optimization variables, the binding expression (sample dist) is transformed to the following\n(do (observe dist phi-k-hat) phi-k-hat)\ncorresponding to the observe<- form used in the main paper.\nA.7 Error Handling\nDuring program transformation stage, we provide three error-handling mechanisms to enforce the restrictions on the probabilistic programs described in Section 3.\n1. We inspect let binding pairs and throw an error if an optimization variable is bound to anything other than a sample statement.\n2. We add code that throws a runtime error if any optimization variable is assigned more than once or not at all.\n3. We recursively traverse the code and throw a compilation error if sample statements of different base measures are assigned to any optimization variable. At present, we also throw an error if the base measure assigned to an optimization variable is unknown, e.g. because the distribution object is from a user defined defdist where the user does not provide the required measure type meta-information."}, {"heading": "Appendix B. Problem Independent Gaussian Process Hyperprior", "text": "Remembering that the domain scaling introduced in Section 4.3 means that both the input and outputs of the GP are taken to vary between \u00b11, we define the problem independent GP hyperprior as p(\u03b1) = p(\u03c3n)p(\u03c33/2)p(\u03c35/2) \u220fD i=1 p(\u03c1i)p(%i) where\nlog (\u03c3n) \u223c N (\u22125, 2) (34a) log ( \u03c33/2 ) \u223c N (\u22127, 0.5) (34b)\nlog ( \u03c35/2 ) \u223c N (\u22120.5, 0.15) (34c)\nlog (\u03c1i) \u223c N (\u22121.5, 0.5) \u2200i \u2208 {1, . . . , D} (34d) log (%i) \u223c N (\u22121, 0.5) \u2200i \u2208 {1, . . . , D}. (34e)\nThe rationale of this hyperprior is that the smoother Mate\u0301rn 5/2 kernel should be the dominant effect and model the higher length scale variations. The Mate\u0301rn 3/2 kernel is included in case the evidence suggests that the target is less smooth than can be modelled with the Mate\u0301rn 5/2 kernel and to provide modelling of smaller scale variations around the optimum."}, {"heading": "Appendix C. Full Details for House Heating Experiment", "text": "In this case study, illustrated in Figure 1, we optimize the parameters of a stochastic engineering simulation. We use the Energy2D system from Xie (2012) to perform finite-difference numerical simulation of the heat equation and Navier-Stokes equations in a user-defined geometry.\nIn our setup, we designed a 2-dimensional representation of a house with 4 interconnected rooms using the GUI provided by Energy2D. The left side of the house receives morning sun, modelled at a constant incident angle of 30\u25e6. We assume a randomly distributed solar intensity and simulate the heating of a cold house in the morning by 4 radiators, one in each of the rooms. The radiators are given a fixed budget of total power density Pbudget. The optimization problem is to distribute this power budget across radiators in a manner that minimizes the variance in temperatures across 8 locations in the house.\nEnergy2D is written in Java, which allows the simulation to be integrated directly into an Anglican program that defines a prior on model parameters and an ABC likelihood for evaluating the utility of the simulation outputs. Figure 2 shows the corresponding program query. In this, we define a Clojure function simulate that accepts a solar power intensity Isun and power densities for the radiators Pr, returning the thermometer temperature readings {Ti,t}. We place a symmetric Dirichlet prior on PrPbudget and a gamma prior on Isun Ibase\n, where Pbudget and Ibase are constants. This gives the generative model:\npr \u223c Dirichlet([1, 1, 1, 1]) (35) Pr \u2190 Pbudget \u00b7 pr (36) \u03c5 \u223c Gamma(5, 1) (37)\nIsun \u2190 Ibase \u00b7 \u03c5. (38)\nAfter using these to call simulate, the standard deviations of the returned temperatures is calculated for each time point,\n\u03c9t = \u221a\u221a\u221a\u221a 8\u2211 i=1 T 2i,t \u2212 ( 8\u2211 i=1 Ti,t )2 (39)\nand used in the ABC likelihood abc-likelihood to weight the execution trace using a multivariate Gaussian:\np ({Ti,t}i=1:8,t=1:\u03c4 ) = Normal ( \u03c9t=1:\u03c4 ; 0, \u03c3 2 T I )\nwhere I is the identity matrix and \u03c3T = 0.8\u25e6C is the observation standard deviation. Figure 1 demonstrates the improvement in homogeneity of temperatures as a function of total number of simulation evaluations. Visual inspection of the heat distributions also shown in Figure 1 confirms this result, which serves as an exemplar of how BOPP can be used to estimate marginally optimal simulation parameters."}, {"heading": "Acknowledgements", "text": "Tom Rainforth is supported by a BP industrial grant. Tuan Anh Le is supported by a Google studentship, project code DF6700. Frank Wood is supported under DARPA PPAML through the U.S. AFRL under Cooperative Agreement FA8750-14-2-0006, Sub Award number 61160290-111668."}], "references": [{"title": "Particle Markov chain Monte Carlo methods", "author": ["Christophe Andrieu", "Arnaud Doucet", "Roman Holenstein"], "venue": "J Royal Stat. Soc.: Series B (Stat. Methodol.),", "citeRegEx": "Andrieu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2010}, {"title": "A lognormal central limit theorem for particle approximations of normalizing constants", "author": ["Jean B\u00e9rard", "Pierre Del Moral", "Arnaud Doucet"], "venue": "Electronic Journal of Probability,", "citeRegEx": "B\u00e9rard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "B\u00e9rard et al\\.", "year": 2014}, {"title": "Algorithms for hyper-parameter optimization", "author": ["James S Bergstra", "R\u00e9mi Bardenet", "Yoshua Bengio", "Bal\u00e1zs K\u00e9gl"], "venue": "In NIPS,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "The convergence of a class of double-rank minimization algorithms 1. general considerations", "author": ["Charles George Broyden"], "venue": "IMA Journal of Applied Mathematics,", "citeRegEx": "Broyden.,? \\Q1970\\E", "shortCiteRegEx": "Broyden.", "year": 1970}, {"title": "Stan: a probabilistic programming language", "author": ["B Carpenter", "A Gelman", "M Hoffman", "D Lee", "B Goodrich", "M Betancourt", "M A Brubaker", "J Guo", "P Li", "A Riddell"], "venue": "Journal of Statistical Software,", "citeRegEx": "Carpenter et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Carpenter et al\\.", "year": 2015}, {"title": "Approximate Bayesian Computation (ABC) in practice", "author": ["Katalin Csill\u00e9ry", "Michael GB Blum", "Oscar E Gaggiotti", "Olivier Fran\u00e7ois"], "venue": "Trends in Ecology & Evolution,", "citeRegEx": "Csill\u00e9ry et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Csill\u00e9ry et al\\.", "year": 2010}, {"title": "An introduction to chaotic dynamical systems, volume 13046", "author": ["Robert L Devaney", "Luke Devaney"], "venue": null, "citeRegEx": "Devaney et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Devaney et al\\.", "year": 1989}, {"title": "Hybrid Monte Carlo", "author": ["Simon Duane", "Anthony D Kennedy", "Brian J Pendleton", "Duncan Roweth"], "venue": "Physics letters B,", "citeRegEx": "Duane et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Duane et al\\.", "year": 1987}, {"title": "Towards an empirical foundation for assessing Bayesian optimization of hyperparameters", "author": ["Katharina Eggensperger", "Matthias Feurer", "Frank Hutter", "James Bergstra", "Jasper Snoek", "Holger Hoos", "Kevin Leyton-Brown"], "venue": "In NIPS workshop on Bayesian Optimization in Theory and Practice,", "citeRegEx": "Eggensperger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Eggensperger et al\\.", "year": 2013}, {"title": "Extended Kalman filter", "author": ["Keisuke Fujii"], "venue": "Refernce Manual,", "citeRegEx": "Fujii.,? \\Q2013\\E", "shortCiteRegEx": "Fujii.", "year": 2013}, {"title": "Bayesian optimization with inequality constraints", "author": ["Jacob R Gardner", "Matt J Kusner", "Zhixiang Eddie Xu", "Kilian Q Weinberger", "John Cunningham"], "venue": "In ICML,", "citeRegEx": "Gardner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2014}, {"title": "Church: a language for generative models", "author": ["N Goodman", "V Mansinghka", "D M Roy", "K Bonawitz", "J B Tenenbaum"], "venue": "In UAI,", "citeRegEx": "Goodman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "The Design and Implementation of Probabilistic Programming Languages", "author": ["Noah D Goodman", "Andreas Stuhlm\u00fcller"], "venue": null, "citeRegEx": "Goodman and Stuhlm\u00fcller.,? \\Q2014\\E", "shortCiteRegEx": "Goodman and Stuhlm\u00fcller.", "year": 2014}, {"title": "Bayesian optimization for likelihood-free inference of simulatorbased statistical models", "author": ["Michael U Gutmann", "Jukka Corander"], "venue": null, "citeRegEx": "Gutmann and Corander.,? \\Q2016\\E", "shortCiteRegEx": "Gutmann and Corander.", "year": 2016}, {"title": "Predictive entropy search for efficient global optimization of black-box functions", "author": ["Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Matthew W Hoffman", "Zoubin Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2014}, {"title": "A general framework for constrained Bayesian optimization using information-based", "author": ["Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Michael A. Gelbart", "Ryan P. Adams", "Matthew W. Hoffman", "Zoubin Ghahramani"], "venue": "search. JMLR,", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2016}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": "In Learn. Intell. Optim.,", "citeRegEx": "Hutter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hutter et al\\.", "year": 2011}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["Donald R Jones", "Matthias Schonlau", "William J Welch"], "venue": "J Global Optim,", "citeRegEx": "Jones et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Jones et al\\.", "year": 1998}, {"title": "Venture: a higher-order probabilistic programming platform with programmable inference", "author": ["Vikash Mansinghka", "Daniel Selsam", "Yura Perov"], "venue": "arXiv preprint arXiv:1404.0099,", "citeRegEx": "Mansinghka et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mansinghka et al\\.", "year": 2014}, {"title": "Infer .NET 2.4", "author": ["T Minka", "J Winn", "J Guiver", "D Knowles"], "venue": "Microsoft Research Cambridge,", "citeRegEx": "Minka et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Minka et al\\.", "year": 2010}, {"title": "Machine learning: a probabilistic perspective", "author": ["Kevin P Murphy"], "venue": "MIT press,", "citeRegEx": "Murphy.,? \\Q2012\\E", "shortCiteRegEx": "Murphy.", "year": 2012}, {"title": "Annealed importance sampling", "author": ["Radford M Neal"], "venue": "Statistics and Computing,", "citeRegEx": "Neal.,? \\Q2001\\E", "shortCiteRegEx": "Neal.", "year": 2001}, {"title": "Gaussian processes for global optimization", "author": ["Michael A Osborne", "Roman Garnett", "Stephen J Roberts"], "venue": "In 3rd international conference on learning and intelligent optimization", "citeRegEx": "Osborne et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Osborne et al\\.", "year": 2009}, {"title": "Asynchronous anytime sequential monte carlo", "author": ["Brooks Paige", "Frank Wood", "Arnaud Doucet", "Yee Whye Teh"], "venue": "In NIPS,", "citeRegEx": "Paige et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paige et al\\.", "year": 2014}, {"title": "The pattern book: Fractals, art, and nature", "author": ["Clifford A Pickover"], "venue": "World Scientific,", "citeRegEx": "Pickover.,? \\Q1995\\E", "shortCiteRegEx": "Pickover.", "year": 1995}, {"title": "Bayesian optimization for probabilistic programs", "author": ["Tom Rainforth", "Tuan-Anh Le", "Jan-Willem van de Meent", "Michael A Osborne", "Frank Wood"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rainforth et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rainforth et al\\.", "year": 2016}, {"title": "Gaussian Processes for Machine Learning", "author": ["Carl Rasmussen", "Chris Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "A chaotic secure communication scheme with extended Kalman filter based parameter estimation", "author": ["Huawei Ruan", "Tongyan Zhai", "Edwin Engin Yaz"], "venue": "In Control Applications,", "citeRegEx": "Ruan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ruan et al\\.", "year": 2003}, {"title": "Unbounded Bayesian optimization via regularization", "author": ["Bobak Shahriari", "Alexandre Bouchard-C\u00f4t\u00e9", "Nando de Freitas"], "venue": "AISTATS,", "citeRegEx": "Shahriari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2016}, {"title": "Taking the human out of the loop: A review of Bayesian optimization", "author": ["Bobak Shahriari", "Kevin Swersky", "Ziyu Wang", "Ryan P Adams", "Nando de Freitas"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Shahriari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shahriari et al\\.", "year": 2016}, {"title": "Practical Bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": "In NIPS,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Scalable Bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Mostofa Patwary", "Mostofa Ali", "Ryan P Adams"], "venue": "In ICML,", "citeRegEx": "Snoek et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2015}, {"title": "Probabilistic programming in Anglican", "author": ["David Tolpin", "Jan-Willem van de Meent", "Frank Wood"], "venue": "Springer International Publishing,", "citeRegEx": "Tolpin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tolpin et al\\.", "year": 2015}, {"title": "Black-box policy search with probabilistic programs", "author": ["Jan-Willem van de Meent", "Brooks Paige", "David Tolpin", "Frank Wood"], "venue": "In AISTATS,", "citeRegEx": "Meent et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Meent et al\\.", "year": 2016}, {"title": "Lightweight implementations of probabilistic programming languages via transformational compilation", "author": ["David Wingate", "Andreas Stuhlmueller", "Noah D Goodman"], "venue": "In AISTATS,", "citeRegEx": "Wingate et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wingate et al\\.", "year": 2011}, {"title": "A new approach to probabilistic programming inference", "author": ["Frank Wood", "Jan Willem van de Meent", "Vikash Mansinghka"], "venue": "In AISTATS,", "citeRegEx": "Wood et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2014}, {"title": "Interactive heat transfer simulations for everyone", "author": ["Charles Xie"], "venue": "The Physics Teacher,", "citeRegEx": "Xie.,? \\Q2012\\E", "shortCiteRegEx": "Xie.", "year": 2012}], "referenceMentions": [{"referenceID": 4, "context": "Introduction Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data (Carpenter et al., 2015; Goodman et al., 2008; Goodman and Stuhlm\u00fcller, 2014; Mansinghka et al., 2014; Minka et al., 2010; Wood et al., 2014).", "startOffset": 172, "endOffset": 313}, {"referenceID": 11, "context": "Introduction Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data (Carpenter et al., 2015; Goodman et al., 2008; Goodman and Stuhlm\u00fcller, 2014; Mansinghka et al., 2014; Minka et al., 2010; Wood et al., 2014).", "startOffset": 172, "endOffset": 313}, {"referenceID": 12, "context": "Introduction Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data (Carpenter et al., 2015; Goodman et al., 2008; Goodman and Stuhlm\u00fcller, 2014; Mansinghka et al., 2014; Minka et al., 2010; Wood et al., 2014).", "startOffset": 172, "endOffset": 313}, {"referenceID": 18, "context": "Introduction Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data (Carpenter et al., 2015; Goodman et al., 2008; Goodman and Stuhlm\u00fcller, 2014; Mansinghka et al., 2014; Minka et al., 2010; Wood et al., 2014).", "startOffset": 172, "endOffset": 313}, {"referenceID": 19, "context": "Introduction Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data (Carpenter et al., 2015; Goodman et al., 2008; Goodman and Stuhlm\u00fcller, 2014; Mansinghka et al., 2014; Minka et al., 2010; Wood et al., 2014).", "startOffset": 172, "endOffset": 313}, {"referenceID": 35, "context": "Introduction Probabilistic programming systems (PPS) allow probabilistic models to be represented in the form of a generative model and statements for conditioning on data (Carpenter et al., 2015; Goodman et al., 2008; Goodman and Stuhlm\u00fcller, 2014; Mansinghka et al., 2014; Minka et al., 2010; Wood et al., 2014).", "startOffset": 172, "endOffset": 313}, {"referenceID": 20, "context": "It also often forms a tractable alternative when full inference is infeasible (Murphy, 2012).", "startOffset": 78, "endOffset": 92}, {"referenceID": 35, "context": "We therefore introduce BOPP1 (Bayesian optimization for probabilistic programs) which couples existing inference algorithms from PPS, like Anglican (Wood et al., 2014), with a new Gaussian process (GP) (Rasmussen and Williams, 2006) based Bayesian optimization (BO) package (Gutmann and Corander, 2016; Jones et al.", "startOffset": 148, "endOffset": 167}, {"referenceID": 26, "context": ", 2014), with a new Gaussian process (GP) (Rasmussen and Williams, 2006) based Bayesian optimization (BO) package (Gutmann and Corander, 2016; Jones et al.", "startOffset": 42, "endOffset": 72}, {"referenceID": 13, "context": ", 2014), with a new Gaussian process (GP) (Rasmussen and Williams, 2006) based Bayesian optimization (BO) package (Gutmann and Corander, 2016; Jones et al., 1998; Osborne et al., 2009; Shahriari et al., 2016a).", "startOffset": 114, "endOffset": 209}, {"referenceID": 17, "context": ", 2014), with a new Gaussian process (GP) (Rasmussen and Williams, 2006) based Bayesian optimization (BO) package (Gutmann and Corander, 2016; Jones et al., 1998; Osborne et al., 2009; Shahriari et al., 2016a).", "startOffset": 114, "endOffset": 209}, {"referenceID": 22, "context": ", 2014), with a new Gaussian process (GP) (Rasmussen and Williams, 2006) based Bayesian optimization (BO) package (Gutmann and Corander, 2016; Jones et al., 1998; Osborne et al., 2009; Shahriari et al., 2016a).", "startOffset": 114, "endOffset": 209}, {"referenceID": 5, "context": "By expressing the utility of a particular design-environment combination using an approximate Bayesian computation (ABC) likelihood (Csill\u00e9ry et al., 2010), one can pose this as a MMAP problem, optimizing the design while marginalizing out the environmental uncertainty.", "startOffset": 132, "endOffset": 155}, {"referenceID": 36, "context": "The probabilistic program shown in Figure 2 allows us to define a prior over the uncertain weather, while conditioning on the output of a deterministic simulator (here Energy2D (Xie, 2012)-a finite element package for heat transfer) using an ABC likelihood.", "startOffset": 177, "endOffset": 188}, {"referenceID": 19, "context": "Net (Minka et al., 2010) and Stan (Carpenter et al.", "startOffset": 4, "endOffset": 24}, {"referenceID": 4, "context": ", 2010) and Stan (Carpenter et al., 2015) can be thought of as defining graphical models or factor graphs.", "startOffset": 17, "endOffset": 41}, {"referenceID": 11, "context": "Our focus will instead be on systems such as Church (Goodman et al., 2008), Venture (Mansinghka et al.", "startOffset": 52, "endOffset": 74}, {"referenceID": 18, "context": ", 2008), Venture (Mansinghka et al., 2014), WebPPL (Goodman and Stuhlm\u00fcller, 2014), and Anglican (Wood et al.", "startOffset": 17, "endOffset": 42}, {"referenceID": 12, "context": ", 2014), WebPPL (Goodman and Stuhlm\u00fcller, 2014), and Anglican (Wood et al.", "startOffset": 16, "endOffset": 47}, {"referenceID": 35, "context": ", 2014), WebPPL (Goodman and Stuhlm\u00fcller, 2014), and Anglican (Wood et al., 2014), which employ a general-purpose programming language for model specification.", "startOffset": 62, "endOffset": 81}, {"referenceID": 36, "context": "Shown are output heat maps from Energy2D (Xie, 2012) simulations at one intensity, corresponding to setting all the radiators to the same power (top left), the best result from a set of 5 randomly chosen powers used for initializing BOPP (top right), and the best setup found after 100 iterations of BOPP (bottom left).", "startOffset": 41, "endOffset": 52}, {"referenceID": 11, "context": "These models, which we refer to as queries (Goodman et al., 2008), specify a joint distribution p(Y,X) over data Y and variables X .", "startOffset": 43, "endOffset": 65}, {"referenceID": 17, "context": "BO (Jones et al., 1998; Osborne et al., 2009) aims to find the global maximum \u03b8\u2217 = argmax \u03b8\u2208\u03b8 f (\u03b8) .", "startOffset": 3, "endOffset": 45}, {"referenceID": 22, "context": "BO (Jones et al., 1998; Osborne et al., 2009) aims to find the global maximum \u03b8\u2217 = argmax \u03b8\u2208\u03b8 f (\u03b8) .", "startOffset": 3, "endOffset": 45}, {"referenceID": 2, "context": "Although alternatives such as random forests (Bergstra et al., 2011; Hutter et al., 2011) or neural networks (Snoek et al.", "startOffset": 45, "endOffset": 89}, {"referenceID": 16, "context": "Although alternatives such as random forests (Bergstra et al., 2011; Hutter et al., 2011) or neural networks (Snoek et al.", "startOffset": 45, "endOffset": 89}, {"referenceID": 31, "context": ", 2011) or neural networks (Snoek et al., 2015) exist, the most common prior used for f is a GP (Rasmussen and Williams, 2006).", "startOffset": 27, "endOffset": 47}, {"referenceID": 26, "context": ", 2015) exist, the most common prior used for f is a GP (Rasmussen and Williams, 2006).", "startOffset": 56, "endOffset": 86}, {"referenceID": 2, "context": "Although alternatives such as random forests (Bergstra et al., 2011; Hutter et al., 2011) or neural networks (Snoek et al., 2015) exist, the most common prior used for f is a GP (Rasmussen and Williams, 2006). For further information on BO we refer the reader to the recent review by Shahriari et al Shahriari et al. (2016b).", "startOffset": 46, "endOffset": 325}, {"referenceID": 26, "context": "3 Gaussian Processes Informally one can think of a Gaussian Process (GP) (Rasmussen and Williams, 2006) as being a nonparametric distribution over functions which is fully specified by a mean function \u03bc : \u03b8 \u2192 R and covariance function k : \u03b8\u00d7\u03b8\u2192 R, the latter of which must be a bounded (i.", "startOffset": 73, "endOffset": 103}, {"referenceID": 26, "context": "- Step 3 (black arrow) fits a mixture of GPs posterior Rasmussen and Williams (2006) to the scaled data (bottom centre) using a problem independent hyperprior.", "startOffset": 55, "endOffset": 85}, {"referenceID": 35, "context": "sequen al Monte Carlo (Wood et al., 2014) or th particle cascade (P ige et al.", "startOffset": 22, "endOffset": 41}, {"referenceID": 1, "context": "Though the rationale for the latter is predominantly computational, giving an analytic posterior, there are also theoretical results suggesting that this choice is appropriate (B\u00e9rard et al., 2014).", "startOffset": 176, "endOffset": 197}, {"referenceID": 30, "context": "As noted by (Snoek et al., 2012), the performance of BO using a single GP posterior is heavily influenced by the choice of these hyperparameters.", "startOffset": 12, "endOffset": 32}, {"referenceID": 7, "context": "Inference over \u03b1 is performed using Hamiltonian Monte Carlo (HMC) (Duane et al., 1987), giving an unweighted mixture of GPs.", "startOffset": 66, "endOffset": 86}, {"referenceID": 3, "context": "As we found that the performance of HMC was often poor unless a good initialization point was used, BOPP runs a small number of independent chains and allocates part of the computational budget to their initialization using a L-BFGS optimizer (Broyden, 1970).", "startOffset": 243, "endOffset": 258}, {"referenceID": 30, "context": "Secondly it is used to define the acquisition function \u03b6, for which we take the expected improvement (Snoek et al., 2012), defining \u03c3 m (\u03b8) = \u221a ki m (\u03b8, \u03b8) and \u03b3 i m (\u03b8) = \u03bcm(\u03b8)\u2212\u00fbm \u03c3i m(\u03b8) ,", "startOffset": 101, "endOffset": 121}, {"referenceID": 14, "context": "(Hern\u00e1ndez-Lobato et al., 2014), could be used instead.", "startOffset": 0, "endOffset": 31}, {"referenceID": 10, "context": "The problem of unknown constraints has been previously covered in the literature (Gardner et al., 2014; Hern\u00e1ndez-Lobato et al., 2016) by assuming that constraints take the form of a black-box function which is modeled with a second surrogate function and must be evaluated in guess-and-check strategy to establish whether a point is valid.", "startOffset": 81, "endOffset": 134}, {"referenceID": 15, "context": "The problem of unknown constraints has been previously covered in the literature (Gardner et al., 2014; Hern\u00e1ndez-Lobato et al., 2016) by assuming that constraints take the form of a black-box function which is modeled with a second surrogate function and must be evaluated in guess-and-check strategy to establish whether a point is valid.", "startOffset": 81, "endOffset": 134}, {"referenceID": 21, "context": "We obtain a maximum likelihood estimate for q-acq using a variant of annealed importance sampling (Neal, 2001) in which lightweight Metropolis Hastings (LMH) (Wingate et al.", "startOffset": 98, "endOffset": 110}, {"referenceID": 34, "context": "We obtain a maximum likelihood estimate for q-acq using a variant of annealed importance sampling (Neal, 2001) in which lightweight Metropolis Hastings (LMH) (Wingate et al., 2011) with local random-walk moves is used as the base transition kernel.", "startOffset": 158, "endOffset": 180}, {"referenceID": 8, "context": "The dashed lines shows the final mean error of SMAC (red), Spearmint (green) and TPE (black) as quoted by Eggensperger et al. (2013). The dark blue line shows the mean error for BOPP averaged over 100 runs, whilst the median and 25/75% percentiles are shown in cyan.", "startOffset": 106, "endOffset": 133}, {"referenceID": 8, "context": "The dashed lines shows the final mean error of SMAC (red), Spearmint (green) and TPE (black) as quoted by Eggensperger et al. (2013). The dark blue line shows the mean error for BOPP averaged over 100 runs, whilst the median and 25/75% percentiles are shown in cyan. Results for Spearmint on Branin and SMAC on SVM on-grid are omitted because both BOPP and the respective algorithms averaged zero error to the provided number of significant figures in Eggensperger et al. (2013).", "startOffset": 106, "endOffset": 479}, {"referenceID": 15, "context": "1 Classic Optimizatio Benchmarks Next we compare BOPP to the prominent BO packages SMAC Hutter et al. (2011), Spearmint Snoek et al.", "startOffset": 88, "endOffset": 109}, {"referenceID": 15, "context": "1 Classic Optimizatio Benchmarks Next we compare BOPP to the prominent BO packages SMAC Hutter et al. (2011), Spearmint Snoek et al. (2012) and TPE Bergstra et al.", "startOffset": 88, "endOffset": 140}, {"referenceID": 2, "context": "(2012) and TPE Bergstra et al. (2011) on a number of classical benchmarks as shown in Figure 5.", "startOffset": 15, "endOffset": 38}, {"referenceID": 34, "context": "For these MH steps we consider both LMH (Wingate et al., 2011) with proposals from the prior and the random-walk MH (RMH) variant introduced in Section 4.", "startOffset": 40, "endOffset": 62}, {"referenceID": 0, "context": "We consider the possible alternative of using our conditional code transformation to design a particle marginal Metropolis Hastings (PMMH, Andrieu et al. (2010)) sampler which operates in a similar fashion to BOPP except that new \u03b8 are chosen using a MH step instead of actively sampling with BO.", "startOffset": 139, "endOffset": 161}, {"referenceID": 9, "context": "Therefore regardless of the available precision, a trajectory cannot be indefinitely extrapolated to within a given accuracy and probabilistic methods such as the extended Kalman filter must be incorporated (Fujii, 2013; Ruan et al., 2003).", "startOffset": 207, "endOffset": 239}, {"referenceID": 27, "context": "Therefore regardless of the available precision, a trajectory cannot be indefinitely extrapolated to within a given accuracy and probabilistic methods such as the extended Kalman filter must be incorporated (Fujii, 2013; Ruan et al., 2003).", "startOffset": 207, "endOffset": 239}, {"referenceID": 6, "context": "We refer the reader to Devaney et al. (1989) for an introduction.", "startOffset": 23, "endOffset": 45}, {"referenceID": 24, "context": "xt,3 = sin (xt\u22121,1) (22c) corresponds to a Pickover attractor (Pickover, 1995) with unknown parameters \u03b8 = {\u03b2, \u03b7} which we wish to optimize.", "startOffset": 62, "endOffset": 78}, {"referenceID": 32, "context": "Anglican extends Clojure with the special forms sample and observe (Tolpin et al., 2015).", "startOffset": 67, "endOffset": 88}, {"referenceID": 36, "context": "We use the Energy2D system from Xie (2012) to perform finite-difference numerical simulation of the heat equation and Navier-Stokes equations in a user-defined geometry.", "startOffset": 32, "endOffset": 43}], "year": 2017, "abstractText": "We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.", "creator": "LaTeX with hyperref package"}}}