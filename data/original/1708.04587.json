{"id": "1708.04587", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "Automatic Summarization of Online Debates", "abstract": "Debate summarization is one of the novel and challenging research areas in automatic text summarization which has been largely unexplored. In this paper, we develop a debate summarization pipeline to summarize key topics which are discussed or argued in the two opposing sides of online debates. We view that the generation of debate summaries can be achieved by clustering, cluster labeling, and visualization. In our work, we investigate two different clustering approaches for the generation of the summaries. In the first approach, we generate the summaries by applying purely term-based clustering and cluster labeling. The second approach makes use of X-means for clustering and Mutual Information for labeling the clusters. Both approaches are driven by ontologies. We visualize the results using bar charts. We think that our results are a smooth entry for users aiming to receive the first impression about what is discussed within a debate topic containing waste number of argumentations.", "histories": [["v1", "Tue, 15 Aug 2017 16:44:28 GMT  (320kb,D)", "http://arxiv.org/abs/1708.04587v1", "Accepted and to be published in Natural Language Processing and Information Retrieval workshop, Recent Advances in Natural Language Processing 2017 (RANLP 2017)"]], "COMMENTS": "Accepted and to be published in Natural Language Processing and Information Retrieval workshop, Recent Advances in Natural Language Processing 2017 (RANLP 2017)", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR", "authors": ["nattapong sanchan", "ahmet aker", "kalina bontcheva"], "accepted": false, "id": "1708.04587"}, "pdf": {"name": "1708.04587.pdf", "metadata": {"source": "CRF", "title": "Automatic Summarization of Online Debates", "authors": ["Nattapong Sanchan", "Ahmet Aker"], "emails": ["k.bontcheva}@sheffield.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "As the number of Internet users has been growing significantly, information is published and stored digitally in textual forms. Online debate is one example of the information which has been massively published. As more and more debate content increases, it becomes a difficult task to easily and quickly discover key arguments that are expressed in the vast amount of debate data. Automatic Text Summarization can help users to extract or summarize those key arguments more efficiently and reduce the reading time.\nOnline debate forums normally contain two sides of debates: proponent and opponent. This structure gives opportunities for users to choose a stance (side) for a debate topic, expresses their opinions to support their propositions, and opposes other propositions. In this paper, we explore online debates which are related to the existence of global warming. A side of proponents believes in the existence of global warming and the other side, the opponents, says that global warming is not true. When the proponents and the opponents express their sentiments, opinions, and pieces of evidence to support their propositions, arguments between them arise.\nIn this paper we aim to summarize online debates about global warming. In our approach we first extract salient sentences from the two opposing sides of debates (i.e. arguments published by users). Next, we cluster them by some sort of similarity. For clustering we investigate two different approaches. Our first approach is a term-based clustering approach. The second approach is based on flat clustering, namely X-means, which can automatically determine the number of clusters. Ontologies are used as the backbone for both approaches. Ontologies have been used broadly in automatic text summarization studies. However, to the best of our knowledge, this approach has not yet been applied for summarizing online debates, especially when our purpose is to capture arguments conversed in both opposing sides. Once clusters are generated, labels representing the clusters are extracted. Again we follow two approaches. The first approach is a simple one and selects as a label an ontological term that is shared by all salient sentences within the cluster. The second labeling approach extracts such a term, based on Mutual Information (MI). The resulting clusters along with their labels are visualized using bar charts. Our results show that clustering with ar X iv :1 70 8. 04 58 7v 1\n[ cs\n.C L\n] 1\n5 A\nug 2\n01 7\nX-means and label generation using MI is a better choice for the purpose of online debates summarization.\nThe rest of the paper is organized as follows. Section 2 discusses about related work in online debate summarization. The online debate data related to the existence of global warming are elaborated in Section 3. Section 4 illustrates the system structure for developing our debate summarization system. Within the same section we also present our evaluation results. We conclude in 5."}, {"heading": "2 Related Work", "text": "Debate summarization is one of the novel research areas in automatic text summarization which has been largely unexplored (Ranade et al., 2013). Examples of related work in debate summarization includes Contrastive Summarization, Comparative Summarization, and Debate Stance Recognition. Contrastive Summarization is the study of generating summary for two entities and finding the difference in sentiments among them (Lerman and McDonald, 2009). This kind of summarization requires the classification of polarity in order to contrast opinions expressed in different sentiments (Campr and Jezek, 2012; Paul et al., 2010). Kim and Zhai (2009) summarized contrastive pairs of sentences by aligning positive and negative opinions on the same aspect. In this work, contrastive sentence pairs were constructed based on two criteria: 1) choose sentences that represent a major sentiment orientation; and 2) the two sentences should have opposite opinions on the same aspect. Similarity functions were used for determining contrastive sentence pairs. Then sentence pairs were used as input for generating the final summary. The summary was aimed to help readers compare the pros and cons of mixed opinions.\nComparative Summarization aims to find the difference between two comparable entities so that sentiment classification may not be required (Campr and Jezek, 2012). Zhai et al. (2004) worked on comparative text mining problem which aimed to discover common topics in news articles and laptop reviews and to summarize commonalities and differences in a given set of comparable text collections. A probabilistic mixture model was proposed. It generates clusters of topics across all collections and in each collection of document. The model generates k collections of specific topics for each collection and k common\ntopics across all collections. Each topic is characterized by multinomial word distribution (also called a unigram language model). High probability words were used as representatives of each cluster and are also included in the summary.\nDebate Stance Recognition aims to detect stance of an opinion\u2019s holder in text. Somasundaran and Wiebe (2009) noticed that in online debate posts, people debate issues, express their favorites, oppose other stances, and argue why their thoughts are correct. To determine positive sentiment about one target, expressing negative sentiment about the other side is a key target. For instance, in a debate \u201cWhich mobile phone is better: iPhone VS Blackberry?\u201d, people supporting iPhone may give reasons to affirm why iPhone is better. In addition, they also express why Blackberry is not. On the Blackberry side, people may also find reasons to support their opinions and argue why the phone is unfavorable. Therefore, to identify stance, it is important to not only consider positive and negative sentiment, but also consider which target an opinion refers to.\nUnlike these, the study of Ranade et al. (2013) directly tackled debate summarization problem and it is the one which is closest to our work. In that work, system summaries are extracted by ranking the smallest units of debates, called Dialogue Acts (DAs). The ranking of sentences is based on features including, words in DAs that is co-occurring in debate topic, topics with opinions expressed on it, sentence position, and sentence length features. However, this work does not explicitly highlight what is the key content to be summarized and how the debate summary is presented. This is different to our work. On the other hand, in our work, we highlight the summarization of key content in debates and visualize them to be easily accessed by users."}, {"heading": "3 Online Debate Data", "text": "In our earlier work we created freely available debate dataset on climate change domain1, also referred as Salient Sentence Selection Dataset (SSSD). Each debate consists of two opposing sides, Agree and Disagree. Whereas the opinion on the Agree side believes that global warming exists, the Disagree side opposes this opinion. In this dataset, each debate comment was manually anno-\n1This dataset can be downloaded at https://goo. gl/3aicDN.\ntated by 5 judgments. The aim of the annotation was to select from each comment 20% sentences that were salient and worth for inclusion in a summary. For instance, for a comment containing 10 sentences 2 sentences were extracted by each annotator. The dataset contains 11 debate topics with 341 comments in total. Average number of comments for a topic is 31 comments, with the minimum and maximum of 5 and 103 comments respectively."}, {"heading": "4 Framework", "text": "To generate a bar chart representing a summary of an online debate topic we proposed a pipeline with two branches where each branch presents either a term-based clustering and the term-based labeling method or X-means based clustering and the MI labeling strategy. The flow of the pipeline is shown in Figure 1. The system assumes an input of n comments from the agree and disagree sides. Each comment consists of several sentences. We aim to select the most salient sentences from each comment, cluster the salient sentences according to their content similarity, generate clusters representing labels, and finally visualize the results using a bar chart summary. In the following sections we introduce each of the components and provide evaluation results."}, {"heading": "4.1 Automatic Salient Sentence Selection", "text": "For the shown pipeline in Figure 1 we used an extractive automatic summarization system reported in our earlier work.2 There are 8 main features defined in this system. Those features include sentence position (SP), sentence length (SL), title words (TT), the presence of conjunctive adverbs (CJ), cosine similarity of topic signatures3 and sentences (COS TPS), cosine similarity of climate change terms4 and sentences (COS CCTS), cosine similarity of sentence and title words (COS TTS), and the semantic similarity of sentence and title words (COS STT) using Doc2Vec. Additionally, we also investigated the Combination of features (CB) in the salient sentence selection. For a given comment the system extracts 20% sentences from it that are deemed as salient.\n2https://goo.gl/xqVeJf. 3We used an approach described by (Lin and Hovy, 2000) to obtain a list of topic signatures. We extract the topic signatures from our dataset which is related to climate change.\n4The terms are obtained by aggregating document keywords from online news media coverage on climate change."}, {"heading": "4.1.1 Evaluation", "text": "We used ROUGE-1, ROUGE-2, and ROUGESU4 evaluation metrics to evaluate the quality of the system summaries, i.e. the selection of salient sentences. As reference summaries we used the manually generated summaries from the freely available dataset, SSSD. Our results revealed that sentence position outperforms other features indicating that the most salient sentences are always. In addition, other useful key features are debate title words feature, and cosine similarity of debate title words and sentences feature. The complete set of results are shown in Table 1."}, {"heading": "4.2 Term-Based Clustering", "text": "To perform clustering we used terms extracted through ontologies. We employed the English ClimaPinion service5 from the DecarboNet project6 as the background knowledge to capture climate change topics and extract from each salient sentence topical terms. To obtain clusters we grouped sentences containing the same label within the same cluster. If a sentence contained more than one term then it was assigned to several groups allowing the sentence to be soft-clustered.7 Also note, terms with the same semantic meaning can be expressed differently. To address this, for each label, we obtained a list of its synonyms from WordNet (Miller, 1995). If the labels shared common synonyms, we considered they are the same labels. Consequentially, the sentences automatically annotated with such labels were merged to the same clusters."}, {"heading": "4.2.1 Evaluation", "text": "The evaluation of the ontology based term extraction has been evaluated somewhere else. By consisting of two environmental ontologies, GEMET (GEneral Multilingual Environmental Thesaurus) and Reegle, the ClimaPinion yields great results in recognizing environmental terms in text, with the precision, recall, and F1 measure of 85.87%, 53.05%, and 65.58% respectively (Maynard and Bontcheva, 2015).\nThe results derived from the term-based clustering approach are evaluated with the Silhouette index (Rousseeuw, 1987). Silhouette evaluates\n5http://services.gate.ac.uk/decarbonet/sentiment/ 6https://www.decarbonet.eu 7Within a cluster all sentences must share one particular term but each sentence may contain other terms that are not shared by other sentences within the same cluster."}, {"heading": "39 0.0000", "text": "the clustering performance by determining the appropriateness of documents assigned to a cluster rather than the other clusters. These documents are represented as coordinates. Silhouette calculates the pairwise difference in both inter-cluster and intra-cluster distance. We calculated an average silhouette score and reported it in Table 2. As shown in the table, the system generated 39 clusters based on the climate change terms annotated by the ClimaPinion service. It achieved the silhouette score around zero, similar to the work presented by Wang and Koopman (2017). The interpretation based on the score is the data points are\nassigned nearly to the decision boundaries of the clusters. Especially, when salient sentences contain multiple climate change topics, clear clustering boundaries are difficult to achieve. This circumstance indicates that such a simple clustering approach is less applicable for grouping semantically similar sentences together and that the task asks for more sophisticated ways for achieving a better performance. We will discuss an alternative solution in Section 4.4."}, {"heading": "4.3 Term-Based Label Extraction", "text": "After grouping salient sentences together, the groups or clusters should be given labels which clearly reflect the content in the clusters (Aker et al., 2016). Similar to the clustering approach, where we grouped salient sentences by the ontological term they share, we used the sharing term as the label to represent the cluster. This is based on the assumption that the climate change terms\nwhich are annotated in the sentences do already elaborate the central meaning of the clusters."}, {"heading": "4.3.1 Evaluation", "text": "In the labeling evaluation, we compared the system labels against the baseline labels. We generated the baseline labels by applying tf*idf. It is a common approach in most information retrieval systems (Ponte and Croft, 1998) which consists of two main components, tf and idf. In our experiment, tf indicates the frequency of terms occurs in a cluster8. idf presents the number clusters in which the term occurs. These components allow us to reduce common terms in the clusters and discover more discriminative terms having fairly low term frequency in the clusters. To determine the candidate labels, we calculated the score for each term by the multiplication of tf and idf. The term with the top score was chosen as the cluster label.\nIn the evaluation of cluster labels, we followed the manual evaluation method presented by Aker et al. (2016). We invited three participants, two PhD candidates and one researcher having background in Computer Science, to evaluate the labels. The evaluation was presented as an online form. The participants were asked to read the sentences in the given clusters and score the labels. The baseline and system labels were shown in random order. For each label, the participants were asked to answer five-point Likert scale questions, ranking from strongly disagree (1) to strongly agree (5). The questions include i) Question 1: By reading the label, I can understand it, ii) Question 2: This label is a complete phrase, and iii) Question 3: This label precisely reflects the content of the sentences in cluster. Along with the three questions, we presented 13 clusters with a maximum of 10 salient sentences (so that the participants are able to read the content prior to the labeling evaluation) and a minimum of 2 salient sentences. Figure 2 illustrates the results of the labeling evaluation.\nAs we can see from the figure, in overall, the quality of system labels outperforms the baseline labels. In Q1, the system labels compared to the baseline labels are more understandable with the average score of 4.59 and 3.33 respectively. Likewise, in Q2, the system labels are more completed phrases than the baseline with the mean difference of 1.51. Lastly, with the average preference\n8Since sentences can carry more than a term it is likely that a cluster has several climate change terms.\nscores of 4.23 in Q3, the system labels are more reflecting the quality of content in the clusters than those generated by the baseline having the score of 2.79. Additionally, the quality of the system labels is further confirmed by a statistical significance analysis with Mann-Whitney U Test. The test reveals that significance difference is found in the system labels (MdQ1\u2212Q3 = 5, nQ1\u2212Q3 = 39) and the baseline labels (MdQ1 = 4, MdQ2 = 3, MdQ3 = 2), UQ1 = 363, UQ2 = 343, UQ3 = 386, zQ1 = \u22124.25, zQ2 = \u22124.36, zQ3 = \u22123.92, p < 0.01, rQ1 = 0.48, rQ2 = 0.49, rQ3 = 0.44. We also measured the inter-annotator agreement using Krippendorffs alpha coefficient9. The agreement in Q1, Q2, and Q3 are 0.31, 0.27, and 0.35 respectively."}, {"heading": "4.4 X-means Clustering", "text": "In Section 4.2 we have shown that the idea of performing clustering based on shared terms results in poor clustering performance. The approach leads to too many clusters which are very close to each other. In this section we aim to overcome the problem of poor performance of the term-based clustering approach and use X-means (Pelleg and Moore, 2000), an extended version of K-means, to cluster the salient sentences selected by the summarization system. One of the benefits of Xmeans is that it is able to automatically detect the number of clusters. By computing the Bayesian Information Criterion (BIC) scores, X-means decides if cluster centroids should be split. We applied ontology-based vector space model approach\n9The measurement is performed using nltk metrics, http://www.nltk.org/api/nltk.metrics.html.\nto create vectors as the similarity inputs for Xmeans."}, {"heading": "4.4.1 Similarity Measurement", "text": "To enable X-means to process the clustering, a similarity needs to be defined to determine which sentences are close to each other. In the definition of our similarity measurement, the automatic selected salient sentences are transformed into vectors using the Vector Space Model (VSM). In the document indexing stage, we employed the ontologies to automatically annotate key climate change terms in the SSSD. The employment of ontology-based approach benefits the transformation of words to vectors by help capturing relevance of specific topics. We derived 64 significant climate change topics. Term frequency was counted for each term to generate vectors for each sentence. To generate a similarity matrix, cosine similarity measure was used to calculate cosine similarity scores among the vectors. After the similarity matrix was constructed, we applied a Principal Component Analysis (PCA)10 for the dimensionality reduction."}, {"heading": "4.4.2 Evaluation", "text": "Similar to the ontology term-based clustering we evaluated the results of the X-means clustering using Silhouette index. Results are reported in Table 3.\nAs shown in the table, the average silhouette score is derived from the calculation based on the similarity definition obtained by the ontologybased vector space model. We achieved a high silhouette score of 0.9878, with the total number of 19 generated clusters. A silhouette close to 1.0 indicates good cohesion and separation of the clustering results, meaning that the average distance from a coordinate in a cluster to the other coordinates within its own cluster is less than the average distance to all coordinate in the nearest cluster. In addition, when the score is close to 0, the coordinates in the clusters are nearly close or on the decision boundary between two neighbouring clusters. A negative silhouette score is obtained when coordinates might be assigned to wrong clusters. In other words, the coordinates are very close to the neighbouring cluster rather than the coordinates in their own clusters (Rousseeuw, 1987). In our experiment, we concluded that the clustering results obtained by X-means clustering algorithm\n10sklearn.decomposition.PCA: https://goo.gl/QqiWec\nhave strong clustering structure and is more appropriate for the task of summarizing debate data."}, {"heading": "19 0.9878", "text": ""}, {"heading": "4.5 Label Generation with Mutual Information", "text": "To generate labels from the X-means clusters we could have followed the same approach as described in Section 4.3, namely picking up a term that is shared by all or majority of the salient sentences within a cluster. We tried this however, to our surprise the performance was very low compared to what we achieved in Section 4.3. Nevertheless this helped us to draw two conclusions. First, the performance in Section 4.3 is high because the labels were so selected that all salient sentences within a group shared that label. Second the size of the clusters was not big so that the label had high chance to be representative of the cluster. This pictures changed once the cluster size increased and also the salient sentences covered several different climate change terms. Because of this selecting a label was not about just simply selecting the term that appears in all or majority of the salient sentences. We used Mutual Information (MI) to make this decision for us.\nMI is a prevalent feature selection approach that involves in the calculation of a utility measure A(t,c). MI quantifies how much information that term t contributing to the correct classification judgment on class c (Manning et al., 2008). The MI formula is shown in Equation 1, where U is a random variable that holds the value et. If a sentence contains term t, the value of et is 1. Otherwise, the et is 0. C is a random variable that holds the value ec. The value of ec is 1 indicating that a sentence is in class c and it is 0 if it is not.\nI(U ;C) = \u2211\net\u2208{1,0} \u2211 ec\u2208{1,0} P (U = et, C = ec) log2 P (U = et, C = ec) P (U = et)P (C = ec)\u2032 (1)\nI(U ;C) = N11 N log2 NN11 N1.N.1 + N01 N log2 NN01 N0.N.1 + N10 N log2 NN10 N1.N.0 + N00 N log2 NN00 N0.N.0\n(2)\nTo calculate the mutual information scores for candidate terms, we applied the maximum likelihood estimation of probability as shown in Equation 2 (Manning et al., 2008). From the equation, N refers to the counts of sentences in which its subscripts take the values of et and ec. For instance, N01 refers to the number of sentences that do not containing term t (et = 1) but in class c (ec = 1). N1. is derived from the addition of N10 and N11. N refers to the total number of sentences. In each cluster, we calculated the score of each candidate term. The term with the higher MI score was selected as the cluster label for that cluster."}, {"heading": "4.5.1 Labeling Evaluation", "text": "In order to evaluate the system labels generated by the results derived from X-means clustering approach, we applied the same evaluation procedure as well the baseline discussed in Section 4.3.1. The results are illustrated in Figure 2. As can be seen from the figure, the average preference scores of the system outperform the baseline. In Q1, the system labels are more understandable than the baseline, with the mean difference of 0.10. In Q2, the system labels more completed phrases than the baseline labels, with a higher mean score of 0.13. Lastly, in Q3, the system labels are still better than the baseline labels with the mean difference of 0.05. The system labels are more meaningful for presenting the central meaning of the content in the clusters. However, as there is a slight difference between the results of the system labels and baseline labels, Mann-Whitney U test reveals that no significant difference, with the z values of -0.705,-0.427, and -0.389, with the significance levels of p= 0.481, 0.670, and 0.697 respectively. The values of Krippendorff\u2019s alpha, by another three participants, for Q1, Q2, and Q3 are 0.33, 0.44, and 0.56 respectively."}, {"heading": "4.6 Visualization", "text": "Sanchan et al. (2016) have manually investigated various representation models for displaying or vi-\nsualizing summaries of online debates. Unlike traditional summaries, the debates extracts have to capture main concepts discussed in both sides and enable the reader to look at those concepts from both proponent and opponent sites. The authors proposed the Chart Summary which presents the clusters by bars. Each bar is marked with the cluster label. In this work we adopt the Chart Summary for visualization purposes. An example Chart Summary is shown in Figure 4.\nIn the generation of the bars in Chart Summary, the bars are the clusters that express related content in both opposing sides. Therefore, it is important to match clusters from the two opposing sides which express the related content. We call this approach as alignment. From the two opposing sides, we align the clusters based on the cluster labels. The clusters sharing mutual labels are aligned. For alignment, we used cosine similarity over vector spaces representing the labels. The vector also contains semantically related words enriched from WordNet. Clusters which have no pair will not be aligned and thus will not be presented in the Chart Summary. Once the pairs of aligned clusters are derived, we count the number of salient sentences in those clusters, separately in each opposing side. Those numbers represent the\nfrequencies of the bars. After all components of a Chart Summary are completely generated, they are exported to NVD3 JAVA script11 for the purpose of visualizing the Chart Summary. Figure 4 illustrates a Chart Summary for the online debates data which runs on a web browser12."}, {"heading": "5 Conclusion", "text": "In this paper, we investigated one of the novel and challenging problems in automatic text summarization of online debates and proposed a framework to tackle this problem. We aimed to generate Chart Summaries which represent the highlevel topics of online debates. The Chart Summary is composed of three main components, including the bars, labels, and frequencies of the bars. We proposed an ontological term-based driven clustering and cluster labeling pipeline to guide the debate summary generation. In our approach we used an online service to automatically annotate climate change terms in salient sentences and to group related salient sentences into the same cluster. For clustering we investigated two variants both making use of ontological terms. The first, a simply approach, groups salient sentences by shared terms. The second approach applies Xmeans clustering. The evaluation has shown that the X-means approach is a better choice for clustering. We create labels to represent each cluster. Again here we investigated two different approaches both making use of ontological terms. The first approach, again a simply one, labels each\n11http://nvd3.org 12A full version of Chart Summary can be accessed via\nhttps://goo.gl/wjBh7V.\ncluster with the term shared by all members within the cluster. The second approach picks up the best term according to Mutual Information. The manual evaluation showed that the simple approach achieves higher results than the MI one. However, as discussed the simply approach achieved high results because of the size of the clusters and led to poor results when the size of the cluster grew which is the case with the X-means clustering. Once the clusters and labels are generated with align the pro and contra parts and visualize the results using NVD3.\nIn future work we plan to enrich the Chart Summary with additional details such as enabling the users to see example debates for each pair of clusters."}], "references": [{"title": "Automatic label generation for news comment clusters", "author": ["Ahmet Aker", "Monica Lestari Paramita", "Emina Kurtic", "Adam Funk", "Emma Barker", "Mark Hepple", "Robert J. Gaizauskas."], "venue": "INLG 2016 - Proceedings of the Ninth Inter-", "citeRegEx": "Aker et al\\.,? 2016", "shortCiteRegEx": "Aker et al\\.", "year": 2016}, {"title": "Comparative summarization via latent semantic analysis", "author": ["Michal Campr", "Karel Jezek."], "venue": "Lastest Trends in Information Technology;Proceedings of the 1st WSEAS International Conference on Information Technology and Com-", "citeRegEx": "Campr and Jezek.,? 2012", "shortCiteRegEx": "Campr and Jezek.", "year": 2012}, {"title": "Generating comparative summaries of contradictory opinions in text", "author": ["Hyun Duk Kim", "ChengXiang Zhai."], "venue": "David WaiLok Cheung, Il-Yeol Song, Wesley W. Chu, Xiaohua Hu, and Jimmy J. Lin, editors,", "citeRegEx": "Kim and Zhai.,? 2009", "shortCiteRegEx": "Kim and Zhai.", "year": 2009}, {"title": "Contrastive summarization: An experiment with consumer reviews", "author": ["Kevin Lerman", "Ryan McDonald."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of", "citeRegEx": "Lerman and McDonald.,? 2009", "shortCiteRegEx": "Lerman and McDonald.", "year": 2009}, {"title": "Wordnet: a lexical database", "author": ["242\u2013250. George A Miller"], "venue": null, "citeRegEx": "Miller.,? \\Q1995\\E", "shortCiteRegEx": "Miller.", "year": 1995}, {"title": "Online debate summariza", "author": ["Radhika Mamidi"], "venue": null, "citeRegEx": "Mamidi.,? \\Q2013\\E", "shortCiteRegEx": "Mamidi.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "the difference in sentiments among them (Lerman and McDonald, 2009).", "startOffset": 40, "endOffset": 67}, {"referenceID": 2, "context": "Kim and Zhai (2009) summarized contrastive pairs of sentences by aligning positive and negative opinions on the same aspect.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Comparative Summarization aims to find the difference between two comparable entities so that sentiment classification may not be required (Campr and Jezek, 2012).", "startOffset": 139, "endOffset": 162}, {"referenceID": 1, "context": "Comparative Summarization aims to find the difference between two comparable entities so that sentiment classification may not be required (Campr and Jezek, 2012). Zhai et al. (2004) worked on comparative text mining problem which aimed to discover common topics in news articles and laptop reviews and to summarize commonalities and differences in a given set of comparable text collections.", "startOffset": 140, "endOffset": 183}, {"referenceID": 4, "context": "To address this, for each label, we obtained a list of its synonyms from WordNet (Miller, 1995).", "startOffset": 81, "endOffset": 95}, {"referenceID": 0, "context": "groups or clusters should be given labels which clearly reflect the content in the clusters (Aker et al., 2016).", "startOffset": 92, "endOffset": 111}, {"referenceID": 0, "context": "In the evaluation of cluster labels, we followed the manual evaluation method presented by Aker et al. (2016). We invited three participants, two", "startOffset": 91, "endOffset": 110}], "year": 2017, "abstractText": "Debate summarization is one of the novel and challenging research areas in automatic text summarization which has been largely unexplored. In this paper, we develop a debate summarization pipeline to summarize key topics which are discussed or argued in the two opposing sides of online debates. We view that the generation of debate summaries can be achieved by clustering, cluster labeling, and visualization. In our work, we investigate two different clustering approaches for the generation of the summaries. In the first approach, we generate the summaries by applying purely term-based clustering and cluster labeling. The second approach makes use of X-means for clustering and Mutual Information for labeling the clusters. Both approaches are driven by ontologies. We visualize the results using bar charts. We think that our results are a smooth entry for users aiming to receive the first impression about what is discussed within a debate topic containing waste number of argumentations.", "creator": "LaTeX with hyperref package"}}}