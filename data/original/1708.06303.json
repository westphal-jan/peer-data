{"id": "1708.06303", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Aug-2017", "title": "Network Model Selection for Task-Focused Attributed Network Inference", "abstract": "Networks are models representing relationships between entities. Often these relationships are explicitly given, or we must learn a representation which generalizes and predicts observed behavior in underlying individual data (e.g. attributes or labels). Whether given or inferred, choosing the best representation affects subsequent tasks and questions on the network. This work focuses on model selection to evaluate network representations from data, focusing on fundamental predictive tasks on networks. We present a modular methodology using general, interpretable network models, task neighborhood functions found across domains, and several criteria for robust model selection. We demonstrate our methodology on three online user activity datasets and show that network model selection for the appropriate network task vs. an alternate task increases performance by an order of magnitude in our experiments.", "histories": [["v1", "Mon, 21 Aug 2017 16:04:17 GMT  (2757kb,D)", "http://arxiv.org/abs/1708.06303v1", null], ["v2", "Sat, 16 Sep 2017 04:08:22 GMT  (5515kb,D)", "http://arxiv.org/abs/1708.06303v2", null]], "reviews": [], "SUBJECTS": "cs.SI cs.AI", "authors": ["ivan brugere", "chris kanich", "tanya y berger-wolf"], "accepted": false, "id": "1708.06303"}, "pdf": {"name": "1708.06303.pdf", "metadata": {"source": "CRF", "title": "Network Model Selection for Task-Focused Attributed Network Inference", "authors": ["Ivan Brugere", "Chris Kanich", "Tanya Y. Berger-Wolf"], "emails": ["ibruge2@uic.edu", "ckanich@uic.edu", "tanyabw@uic.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nNetworks are models representing relationships between entities: individuals, genes, documents and media, language, products etc. We often assume the expressed or inferred edge relationships of the network are correlated with the underlying behavior of individuals or entities, reflected in their actions or preferences over time.\nOn some problems, the correspondence between observed behavior and network structure is well-established. For example, simple link-prediction heuristics in social networks tend to perform well because they correspond to the social processes for how networks grow [1]. However, content sharing in online social networks shows that \u2018weak\u2019 ties among friends account for much of the influence on users [2]. For these different tasks, the same friendship network \u201cas-is\u201d is a relatively better model for predicting new links than predicting content sharing. Learning an alternative network representation for content sharing better predicts this behavior and is more informative of the relevant relationships for one task against another.\nWhy should we learn a network representation at all? First, a good network for a particular predictive task will perform better than methods over aggregate populations. Otherwise, edge relationships are not informative with respect to our purpose. Evaluating network models against population methods measures whether there is a network effect at all within the underlying data. In addition, network edges are interpretable and suitable for descriptive analysis and further hypothesis generation. Finally, a good network model generalizes several behaviors of entities observed on the network, and we can evaluate this robustness under a shared representation. For\nthese reasons, we should learn a network representation if we can evaluate \u2018which\u2019 network is useful and whether there is the presence of any network on the underlying data.\nMuch of the previous work focuses on method development for better predictive accuracy on a given network. However, predictive method development on networks treats the underlying network representation as independent of the novel task method, rather than coupled to the network representation of underlying data. Whether network structure is given or inferred, choosing the best representation affects subsequent tasks and questions on the network. How do we measure the effect of these network modeling choices on common and general classes of predictive tasks (e.g. link prediction, label prediction, influence maximization)?\nOur work fills this methodological gap, coupling general network inference models from data with common predictive task methods in a network model selection framework. This framework evaluates both the definition of an edge, and the \u2018neighborhood\u2019 function on the network most suitable for evaluating various tasks."}, {"heading": "II. RELATED WORK", "text": "Our work is primarily related to research in relational machine learning, and network structure inference.\nRelational learning in networks uses correlations between network structure, attribute distributions, and label distributions to build network-constrained predictive methods for several types of tasks. Our work uses two fundamental relational learning tasks, link prediction and collective classification to evaluate network models inferred from data.\nThe link prediction task [3] predicts edges from \u2018local\u2019 information in the network. This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].\nThe collective classification task [7], [8] learns relationships between local neighborhood structure of labels and/or attributes [9] to predict unknown labels in the network. This problem has been extended to joint prediction of edges [10], and higher-order joint prediction [11]. Our work does not extend these methods. Where suitable, more sophisticated predictive task methods and higher-order joint methods may be used for evaluating network models and model selection.\nar X\niv :1\n70 8.\n06 30\n3v 1\n[ cs\n.S I]\n2 1\nA ug\n2 01\n7\nNetwork structure inference [12], [13] is a broad area of research aimed at transforming data on individuals or entities into a network representation which can leverage methods such as relational machine learning. Previous work spans numerous domains including bioinformatics [14], neuroscience [15], and recommender systems [16]. Much of this work has domaindriven network model evaluation and lacks a general methodology for transforming data to useful network representations.\nModels for network inference are generally either parametric, or similarity-based. Parametric models make assumptions of the underlying data distribution and learn an edge structure which best explains the underlying data. For example, previous work has modeled the \u2018arrival time\u2019 of information in a content network with unknown edges, where rates of transmission are the learned model [17], [18].\nSeveral generative models can sample networks with correlations between attributes, labels, and network structure, and can estimate model parameters from data. These include the Attributed Graph Model (AGM) [19], Multiplicative Attribute Graph model (MAG) [20], and Exponential Random Graph Model (ERGM) [21]. However, these models are typically not estimated against a task of interest, so while it may fit our modeling assumption, it may not model the subsequent task; our proposed methodology straightforwardly accepts any of these models estimated from data.\nSimilarity-based methods tend to be ad-hoc, incorporating domain knowledge to set network model parameters. Recent work on task-focused network inference evaluates inferred network models according to their ability to perform a set of tasks [22]. These methods often have a high sensitivity to threshold/parameter choice, and added complexity of interactions between network representations and task models. Our work identifies these sensitivities, and yields robust model selection over several stability criteria."}, {"heading": "III. CONTRIBUTIONS", "text": "We present a general, modular methodology for model selection in task-focused network inference. Our work: \u2022 identifies constituent components of the common\nnetwork inference workflow, including network models, tasks, task neighborhood functions, and measures for evaluating networks inferred from data, \u2022 uses fundamental, interpretable network models relevant in many application domains, and fundamental tasks and task localities measuring different aspects of network-attribute correlations, \u2022 presents significance, stability, and null-model testing for task-focused network model selection on three online user activity datasets.\nOur work focuses on process and methodology; novel network models and task methods are complimentary. Our work demonstrates that network representation learning is a crucial step for evaluating predictive methods on networks."}, {"heading": "IV. METHODS", "text": ""}, {"heading": "A. Task-Focused Network Inference and Evaluation", "text": "Model selection for task-focused network inference learns a network model and associated parameters which perform a task or set of tasks with high precision. We learn joint relationships between node attribute vectors and some target label (e.g. node behavior) of interest.\nFigure 1 presents a high-level schematic of our methodology and the constituent components of task-oriented network inference problems. First, (a) our data is a collection of attributes and labels associated with discrete entities. Edges are optionally provided to our methodology, indicated by dashed lines. Edges may be entirely missing or one or several edge-sets. In all these cases we evaluate each edge-set as one of many possible network \u2018models.\u2019 Attributes are any data associated with nodes (and/or edges) in the network. These are often very high dimensional, for example user activity logs and post content in social networks, gene expression profiles in gene regulatory networks, or full document text and other metadata in document, video or audio content networks.\nLabels are a notational convenience denoting a specific attribute of predictive interest. Labels are typically lowcardinality fields (e.g. boolean) appropriate for supervised classification methods. Our methodology accepts (and prefers) multi-labeled networks. This simply indicates multiple fields of predictive interest. For example, we induce labels as an indicator for whether a user is a listener or a viewer of \u2018this\u2019 music or movie genre, given by historical activity data over many such genres. Multiple sets of these labels more robustly evaluate whether attribute relationships in the fixed underlying network model generalize to these many \u2018behaviors.\u2019\nSecond, in Figure 1 (b) we apply several fundamental network inference models. These are modular and can be specified according to the domain. These models are either parametric statistical methods, or functions on similarity spaces, both of which take attributes and/or labels as input and produce an inferred edge-set. Note in 1 (b) that the stacked edge-sets vary.\nThird, in Figure 1 (c) for every node of interest (black) for some predictive task, we select sets of nodes (cyan, grey nodes excluded) and their associated attributes and labels according to various task locality functions (i.e. varying \u2018neighborhood\u2019 or network query functions). These functions include globally selecting the population of all nodes, sampling nodes from within \u2018this\u2019 node\u2019s community by the output of some structural community method, an ensemble of a small number of nodes according to attribute or network ranking (e.g. degree, centrality), or local sampling by simple network adjacency or some other ordered traversal.\nOnce selected, the associated attributes and labels of the cyan nodes yield a supervised classification problem predicting label values from attribute vectors, using a predictive task model of interest. Several fundamental prediction problems fit within this supervised setting. For example, link prediction can use positive (edge) and negative (non-edge) label instances rather than learning classifiers on the input labelsets.\nWe evaluate network models (b) over the collection of its task methods and varying localities (c) to produce Figure 1 (d), some predictive output (e.g. label prediction, shown here). The model and locality producing the best performance is selected. Finally, we further evaluate the stability and significance of our selected model over the collection of possible models.\nBelow, we infer networks to identify listeners/reviewers of music genres, film genres, and beer types in three user activity datasets: Last.fm, MovieLens and BeerAdvocate, respectively.\nProblem 1: Task-Focused Network Inference Model Selection\nGiven: Node Attribute-set A, Node Label-sets L\u2217, Network model-set Mj \u2208M where Mj(A,L)\u2192E\u2032j , Task-set Ck\u2208C where Ck(E\u2032j ,A,L\u2217)\u2192P \u2032kj Find: Network edge-set E\u2032j Where: argminjL(P,P \u2032kj) on validation P\nProblem 1 gives a concise specification of our model selection framework, including inputs and outputs. Given individual node-attribute vectors ~ai \u2208A, where \u2018i\u2019 corresponds to node vi in node-set V , and a collection of node-labelsets L \u2208 L\u2217, where li \u2208L the label of node vi in a single labelset L, our general task-focused network inference framework evaluates a set of possible network models M = {M1...Mn} where each Mj :Mj(A,L)\u2192E\u2032j produces a network edge-set E\u2032j .1\nThese instantiated edge-sets are evaluated on a set of inference task methods C = {C1...Cm} where each Ck produces P \u2032k, a collection of predicted edges, attributes, or labels depending on context: Ck(E\u2032j , A, L\u2217) \u2192 P \u2032kj . We evaluate a P \u2032 under loss L(P,P \u2032), where P is the validation or evaluation data. We select Mselect = argminj L(P,P \u2032kj)\n1Notation: capital letters denote sets, lowercase letters denote instances and indices. Primes indicate predicted and inferred entities. Greek letters denote method parameters. Script characters (e.g. C()) denotes functions, sometimes with specified parameters.\nbased on performance over C and/or L\u2217. Finally, we evaluate generalized performance of this model choice.\nOur methodology can be instantiated under many different network model-sets M , task methods C, and loss functions L(). We present several fundamental, interpretable modeling choices common to many domains. All of these identified choices are modular and can be defined according to the needs of the application. For clarity, we refer to the network representation as a model, and the subsequent task as a method, because we focus on model selection for the former."}, {"heading": "B. Network models", "text": "Network models are primarily parametric under some model assumption, or non-parametric under a similarity space. We focus on the latter to avoid assumptions about joint attribute-label distributions.\nWe present two fundamental similarity-based network models applicable in most domains: the k-nearest neighbor (KNN) and thresholded (TH) networks.Given a similarity measure S(~ai, ~aj) \u2192 sij and a target edge count \u03bb, this measure is used to produces a pairwise attribute similarity space. We then select node-pairs according to: \u2022 k-nearest neighbor MKNN(A,S,\u03bb): for a fixed vi, select\nthe top b \u03bb|V |c most similar S(~ai,{A \\~ai}). In directed networks, this produces a network which is k-regular in out-degree, with k=b \u03bb|V |c. \u2022 Threshold MTH(A,S,\u03bb): for any pair (i,j), select the top \u03bb most similar S(~ai,~aj)\nFor a fixed number of edges, the varying performance of these network models measures the extent that absolute similarity, or an equal allocation of modeling per node produces better predictive performance.\n1) Node attribute similarity measures: When constructing a network model on node attribute and label data, we compare pairwise attribute vectors by some similarity and criteria to determine an edge. These measures may vary greatly according to the nature of underlying data. Issues of\nprojections, kernels, and efficient calculation of the similarity space are complimentary to our work, but not our focus.\nOur example applications focus on distributions of item counts: artist plays, movie ratings, beer ratings, where each attribute dimension is a unique item, and the value is its associated rating, play-count etc. Therefore we measure the simple \u2018intersection\u2019 of these vectors.\nGiven a pair of attribute vectors (~ai,~aj) with non-negative elements, intersection SINT () and normalized intersection SINT\u2212N () are given by:\nSINT (~ai,~aj)= \u2211\nd min(aid,ajd) SINT\u2212N (~ai,~aj)= \u2211 dmin(aid,ajd)\u2211 dmax(aid,ajd)\n(1)\nThese different similarity functions measure the extent that absolute or relative common elements produce a better network model for the task method. Absolute similarity favors more active nodes which are highly ranked with other active nodes before low activity node-pairs. For example, in our last.fm application below, this corresponds to testing whether the total count of artist plays between two users is more predictive than the fraction of common artist plays.\n2) Explicit network models: Our methodology accepts observed, explicit edge-sets (e.g. a given social network) not constructed as a function of node attributes and labels. This allows us to test given networks as fixed models on attributes and labels, for our tasks of interest.\n3) Network density: We evaluate network models at varying density, which has previously been the primary focus of networks inferred on similarity spaces [18], [22]. When evaluating network models against a given explicit network, we fix density as a factor of the explicit network density (e.g. 0.75\u00d7d(E)). Otherwise, we explore sparse network settings. We don\u2019t impose any density penalty, allowing the framework to equally select according to predictive performance."}, {"heading": "C. Tasks for Evaluating Network Models", "text": "We evaluate network models on two fundamental network tasks: collective classification and link prediction.\n1) Collective classification (CC): The collective classification problem learns relationships between network edge structure and attributes and/or labels to predict label values [8], [10]. This task is often used to infer unknown labels on the network from \u2018local\u2019 discriminative relationships in attributes, e.g. labeling political affiliations, brand or media preferences.\nGiven an edge-set E\u2032, a neighborhood function N (E\u2032, i), and node-attribute set A, the collective classification method CCC(AN (E\u2032,i), LN (E\u2032,i),~ai) \u2192 l\u2032i trains on attributes and/or labels in the neighborhood of vi to predict a label from attribute test vector ~ai.\nWe use this task to learn network-attribute-label correlations to identify positive instances of rare-class labels. As an oracle, we provide the method only these positive instances because we want to learn the true rather than null association of the label behavior. Learning positive instances over many\nsuch labelsets allows us to robustly evaluate learning on the network model under such sparse labels.\n2) Link prediction (LP): The link prediction problem [4] learns a method for the appearance of edges from one edge-set to another. Link prediction methods can incorporate attribute and/or label data, or using simple structural ranking [1].\nGiven a training edge-set E\u2032 induced from an above network model, a neighborhood function N (E\u2032, i), and a node-attribute set A, we learn edge/non-edge relationships in the neighborhood as a binary classification problem on edge/non-edge attribute vectors, ~ajk where (j,k) \u2208 N (E\u2032,i). On input test attributes ~ajk, the model produces an edge/nonedge label: CLP(AN (E\u2032,i),~ajk) \u2192 l\u2032jk. For our applications, the simple node attribute intersection is suitable to construct edge/non-edge attributes: ~ajk= min\nd=1...|~aj | (ajd,akd)."}, {"heading": "D. Task Locality", "text": "Both of the above tasks are classifiers that take attributes and labels as input, provided by the neighborhood function N (E\u2032,i). However, this neighborhood need not be defined as network adjacency. We redefine the neighborhood function to provide each task method with node/edge attributes selected at varying locality from the test node. These varying localities give interpretable feedback to which level of abstraction the network model best performs the task of interest. We propose four general localities:\n1) Local: For CC, local methods use simple network adjacency of a node vi. For LP, local methods use the egonet of a node vi. This is defined as an edge-set from nodes adjacent to vi, plus the edges of these adjacent nodes. Non-edges are given by the compliment of the egonet edge-set. We also evaluate a breadth-first-search neighborhood (BFS) on each model, collecting k (=200) nodes encountered in the search order.\n2) Community: We calculate structural community labels for each network using the Louvain method [23]. We sample nodes and edges/non-edges from the induced subgraph of the nodes in the \u2018test\u2019 node\u2019s community.\n3) Ensemble: We select k (=30) nodes according to some fixed ordering. These nodes become a collection of locallytrained task methods. For each test node, we select the KNN (=3) ensemble nodes according to the similarity measure of the current network model, and take a majority of their prediction.\nWe use decreasing node-order on (1) Degree, (2) Sum of attributes (i.e. most \u2018active\u2019 nodes), (3) Unique attribute count (i.e. most diverse nodes), and (4) Random order (one fixed sample per ensemble).\nEnsembles provide \u2018exemplar\u2019 nodes expected to have more robust local task methods than those trained on the test node\u2019s neighborhood, on account of their ordering criteria. These nodes are expected to be suitably similar to the test node, on account of the KNN selection from the collection.\n4) Global: We sample a fixed set of nodes or edges/nonedges without locality constraint. This measures whether the network model is informative at all, compared to a single global classification method. This measures the extent that a task is better represented by aggregate population methods\nthan encoding local relationships. Although models with narrower locality are trained on less training data, they can learn local heterogeneity of local attribute relationships which may confuse an aggregated model."}, {"heading": "E. Classification Methods", "text": "For all different localities, both of our tasks reduce to a supervised classification problem. For the underlying classification method, we use standard linear support vector machines (SVM), and random forests (RF). These are chosen due to speed and suitability in sparse data. Absolute predictive accuracy is not the primary goal of our work; these methods need only to produces consistent network model ranking over many model configurations."}, {"heading": "F. Network Model Configurations", "text": "We define a network model configuration as a combination of all specified modeling choices: network model, similarity measure, density, task locality, and task method. Each network model configuration can be evaluated independently, meaning we can easily scale to hundreds of configurations on small and medium-sized networks."}, {"heading": "V. DATASETS", "text": "We demonstrate our methodology on three datasets of user activity data. This data includes beer review history from BeerAdvocate, music listening history from Last.fm, and movie rating history from MovieLens."}, {"heading": "A. BeerAdvocate", "text": "BeerAdvocate is a website founded in 1996 hosting userprovided text reviews and numeric ratings of individual beers. The BeerAdvocate review dataset [26] contains 1.5M beer reviews of 264K unique beers from 33K users. We summarize each review by the average rating across 5 review categories on a 0-5 scale (\u2018appearance\u2019, \u2018aroma\u2019, \u2018palate\u2019, \u2018taste\u2019, \u2018overall\u2019), yielding node attribute vectors where non-zero elements are the user\u2019s average rating for \u2018this\u2019 beer (Details in Table I).\n1) Genre Labels: The BeerAdvocate dataset contains a field of beer \u2018style,\u2019 with 104 unique values. We consider a user a \u2018reviewer\u2019 of a particular beer style if they\u2019ve reviewed at least 5 beers of the style. We further prune styles with fewer than 100 reviewers to yield 45 styles (e.g. \u2018Russian Imperial Stout\u2019, \u2018Hefeweizen\u2019, \u2018Doppelbock\u2019). A labelset L(\u2018style\u2032) : li \u2208 {0,1} is generated by the indicator function for whether user vi is a reviewer of \u2018style.\u2019"}, {"heading": "B. Last.fm", "text": "The Last.fm social network is a platform focused on music logging,recommendation, and discussion. The platform was founded in 2002, presenting many opportunities for longitudinal study of user preferences in social networks.\nThe largest connected component of the Last.fm social network and all associated music listening logs were collected through March 2016. Users in this dataset are ordered by their discovery in a breadth-first search on the explicit \u2018friendship\u2019 network from the seed node of a user account opened in 2006.\nPrevious authors sample a dataset of the first \u2248 20K users, yielding a connected component with 678K edges, a median degree of 35, and over 1B plays collectively by the users over 2.8M unique artists. For each node, sparse non-zero attribute values correspond to that user\u2019s total plays of the unique artist. The Last.fm \u2018explicit\u2019 social network of declared \u2018friendship\u2019 edges is given as input to our framework as one of several network models. Other network models are inferred on attributes.\n1) Genre Labels: We use the last.fm API to collect crowd-sourced artist-genre tag data. We describe a user as a \u2018listener\u2019 of an artist if they\u2019ve listened to the artist for a total of at least 5 plays. A user is a \u2018listener\u2019 of a genre if they are a listener of at least 5 artists in the top-1000 most tagged artists with that genre tag. We generate the labelset for \u2018genre\u2019 as an indicator function for whether user vi is a listener of this genre. We collect artist-genre information for 60 genres, which are hand-verified for artist ranking quality."}, {"heading": "C. MovieLens", "text": "The MovieLens project is a movie recommendation engine created in 1995. The MovieLens 20M ratings dataset [25] contains movie ratings (1-5 stars) data over 138,493 users, through March 2015. Non-zero values in a node\u2019s attribute vector correspond to the user\u2019s star rating of \u2018this\u2019 unique film.\n1) Genre and Tag Labels: We generate two different types of labelsets on this data. Each movie includes a coarse \u2018genre\u2019 field, with 19 possible values (e.g. \u2018Drama\u2019, \u2018Musical\u2019, \u2018Horror\u2019). For each genre value, we generate a binary node label-set as an indicator function of whether \u2018genre\u2019 is this node\u2019s highest-rated genre on average (according to star ratings).\nThis collection of labelsets is limiting because each node has a positive label in exactly one labelset. This means that we cannot build node-level statistics on task performance over multiple positive instances.\nTo address this, we also generate labels from user-generated tags. We find the top-100 tags, and the top-100 movies with the highest frequency of the tag. A user is a \u2018viewer\u2019 of \u2018this\u2019 tag if they have rated at least 5 of the these top-100 movies. Our binary \u2018tag\u2019 labels are an indicator function for this \u2018viewer\u2019 relationship. These tags include more abstract groupings (e.g. \u2018boring\u2019,\u2018inspirational\u2019, \u2018based on a book\u2019, \u2018imdb top 250\u2019) as well as well-defined genres (e.g. \u2018zombies\u2019, \u2018superhero\u2019, \u2018classic\u2019).\nD. Validation, Training and Testing Partitions\nWe split our three datasets into contiguous time segments for validation, training, and testing, in this order. This choice is so models are always validated or tested against adjacent partitions, and testing simulates \u2018future\u2019 data. These partitions are roughly 2-7 years depending on the dataset, to produce roughly equal-frequency attribute partitions. For all of our label definitions, we evaluate them per partition, so a user could be a reviewer/listener/viewer in one partition and not another, according to activity.\nFor the Last.fm explicit social network, we do not have edge creation time between two users. For the LP task, edges are split at random 50% to training, and 25% to validation\nand testing. We sample non-edges disjoint from the union of all time segments, e.g. sampled non-edges appear in none of training, validation or test edge-sets. This ensures a non-edge in each partition is a non-edge in the joined network.\nE. Interpreting Tasks on Our Datasets\nOn each of these datasets, what do the link prediction (LP) and collective classification (CC) tasks measure for each network model? For CC, we construct \u2018listener\u2019, \u2018viewer\u2019, and \u2018reviewer\u2019 relationships, which are hidden function mappings between sets of items (unique artists, movies, beers) and tags. The network model is evaluated on how well it implements all of these functions under a unified relational model (queried by a task at some locality). We then evaluate how well this learned function performs over time.\nThe LP task measures the stability of underlying similarity measures over time. On datasets such as Last.fm and MovieLens, new artists/films and genres emerge over the time of data collection. LP measures whether learned discriminative artists/movies at some locality predict the similarity ranking (e.g. the ultimate edge/nonedge status) of future or past node attribute distributions, while the constituent contents of those distributions may change."}, {"heading": "VI. EVALUATION", "text": "We validate network models under varying configurations (similarity measure, network density, task locality and task method) on each dataset, over the dataset\u2019s defined L\u2217 set of labelsets. We measure precision on both collective classification (CC) and link prediction (LP). Over all network model configurations, we rank models on precision evaluated on the validation partition, and select the top ranked model to be used in testing. To evaluate robustness of model selection, we evaluate all network model configurations on both validation and testing partitions to closely examine their full ranking.\nLet pi denote the precision of the i-th network model configuration, ps as the precision of the selected model configuration evaluated on the testing partition, p(1) as the precision of the \u2018best\u2019 model under the current context, and more generally p(10) as the vector of the top-10 precision values."}, {"heading": "A. Model Stability: Precision", "text": "Table II reports the mean precision over all network configurations evaluated on the testing partition. A data point in this distribution is the precision value of one network configuration, organized by each row\u2019s task method. \u00b5 reports the mean precision over all such configurations (|N |> 100). This represents a baseline precision from any considered\nmodeling choice. \u00b5(10) reports the mean precision over the top-10 ranked configurations. p(1) reports the precision of the best network model configuration for \u2018this\u2019 task method. The difference between p(1) and \u00b5 represents the maximum possible gain in precision added by model selection.\nTable II reports \u2206\u00b5, the stability of mean precision over validation and testing partitions. A data point in this distribution is pi,validation\u2212pi,testing the difference in precision for the same \u2018i\u2019 model configuration in validation and testing partitions. Positive values are more common in Table II, indicating better aggregate performance in the validation partition. This matches our intuition that relationships among new items found in the testing partition may be more difficult to learn than preceding validation data, which is largely a subset of items found in the training partition.\nThe best model in validation need not be the best possible model in testing, especially with many similar models. Table II reports \u2206p(1) =ps\u2212p(1), the difference in precision between the selected model configuration, and the best possible model configuration, both evaluated on the testing partition (0 is best). We highlight values in bold with \u2206p(1)\u22640.1(p(1)\u2212\u00b5), i.e. less than 10% of the possible lift in the testing evaluation.\n\u2206p(1) is one of many possible measures of network model robustness. We look more closely at the selected model, as well as stability in deeper model rankings."}, {"heading": "B. Model Consistency: Selected Model Ranking", "text": "Table III reports the normalized rank of the selected model configuration, evaluated on the testing partition. Values can be interpreted as percentiles, where 1 indicates the same model is top-ranked in both partitions (i.e. higher is better). We highlight models with high stability in precision between validation and testing partitions: rank>0.9, i.e. the selected model is in the top 10% of model configurations on the testing partition.\nTable III shows several cases of rank inconsistency for particular problem settings (e.g. BeerAdvocate LP-RF, MovieLens LP-RF) and notable consistency for others (BeerAdvocate CC-RF, CC-SVM) ranked over many total network configurations (|N |>100). This is a key result demonstrating that appropriate network models change according to the task and the underlying dataset. For Last.fm, community localities are selected for both CC methods. The social network and community locality are selected for CC-SVM. This is very surprising from previous results show poor performance of local models on the social network but did not evaluate other localities [24]. For CC on BeerAdvocate, local models are consistently selected and have a high rank in testing, even though SVM and RF methods have very different performance in absolute precision. This might indicate that preferences are more local in the BeerAdvocate population than Last.fm. In this way, interpretability of locality, network model, and underlying similarity measures can drive further hypothesis generation and testing, especially for network comparison across domains."}, {"heading": "C. Model Stability: Rank Order", "text": "Table IV reports the Kendall\u2019s \u03c4 rank order statistic between the ranking of model configurations by precision, for validation and testing partitions where \u03c4 =1 indicates the rankings are the same. We report the associated p-value of the\n\u03c4 rank order statistic. For several tasks on several data-sets, ranking is very consistent over all model configurations.\nWhile this ranking shows remarkable consistency, it\u2019s not suitable when the result contains many bad models, which may dominate \u03c4 at low ranks. Due to this, intersection(10) reports the shared model configurations in the top-10 of validation and testing partitions. Since top-k lists may be short and have disjoint elements, we find the simple intersection rather than rank order. We highlight tasks in bold at a rank order significance level of p<1.00E\u221203, and intersection(10)\u22655.\nTable IV \u2018Total\u2019 summarize the count of bold entries across Tables II, III, and IV. This corresponds to scoring the network model on (1) precision stability, (2) selected model rank consistency, (3) full ranking stability, and (4) top-10 ranking consistency.\nMovieLens under \u2018tag\u2019 labels is a peculiar result. It performs very well at both \u00b5(10) and p(1) for both SVM and RF. However it has a high \u2206p(1) and low intersection(10). Looking closer at the results, two similar groups of local model perform well. However, in validation, this is under \u2018adjacency\u2019 locality, and the testing partition favors the wider \u2018BFS\u2019 local configurations. One challenge to address is appropriately grouping models where the ranking of specific configurations is uninformative and can introduce ranking noise, but the ranking between categories (e.g. locality) is informative.\nMovieLens improves learning by several factors by using \u2018tag\u2019 labelsets where each node may have several positive instances rather than a single positive instance. BeerAdvocate and Last.fm have very clear signals of robust selected models over our scoring criteria."}, {"heading": "D. Consistency: Task Method Locality", "text": "Our framework allows further investigation of localities suitable for particular types of tasks, measured by their\nranking. Figure 2 reports the counts of model configurations at varying localities for the top-10 model configurations in the validation and testing partition (20 total), for CC (left) and LP (right). Each principal color represents a dataset, and shades denote different task methods.\nCollective classification on BeerAdvocate strongly favors local task localities, and Last.fm favors community and global localities; both of these agree with model selections in Table III. \u2018Global\u2019 locality measures the extent that population-level models are favored to any locality using network information. Looking closer at Last.fm, the \u2206p(1) for the best-ranked \u2018Global\u2019 configuration in testing is only -0.01 for CC-SVM, and -0.05 for CC-RF. This indicates a very weak network effect on Last.fm for CC under our explored models.\nFrom CC to LP tasks (left to right), model ranking preferences change greatly per dataset. BeerAdvocate increases preference for ensemble methods (primarily \u201cSum of Attributes\u201d, see Section IV-D). The preference for global locality largely disappears on Last.fm for LP, instead the task has a very strong preference for local models (all using \u2018adjacency,\u2019 rather than BFS local models). This demonstrates that we find robust indicators for models and localities suited for different tasks, which change both by dataset and task.\nFigure 3 reports similar counts across types of network models and densities for the top-10 model configurations. The first three bar groups report a total of 20 model configurations over \u2018Social\u2019 (only Last.fm), \u2018KNN,\u2019 and \u2018TH\u2019 network models. The next two bar groups report 20 configurations over \u2018Sparse\u2019 and \u2018Dense\u2019 settings. \u2018Sparse\u2019 refers to very sparse networks on the order of 0.0025 density, while \u2018dense\u2019 is on the order of densities ordinarily observed in social networks (e.g. 0.01). In the case of the Last.fm social network, factors on observed density [0.75,1], (e.g. 0.75\u00d7 d(E)) are considered dense and [0.25,0.5] are sparse.\nFor all tested datasets, there is not a strong preference for a particular network model or density for either CC or LP. However, this does not mean that precision is \u2018random\u2019 over varying network models, or that other types of underlying data may have model preferences for our set of models. The \u03c4 rank order and intersection10 are very consistent in several of these task instances (Table IV). Instead, locality preferences seem to drive the three datasets we examine, where network models will perform similarly under the same locality than across localities.\nWe evaluate this hypothesis in Table V. We report the median of pairwise differences of precision between configuration pairs by matched and mismatched models or localities: median(pi \u2212 pj) \u2212 median(pk \u2212 pl), where (i, j) are all matched pairs grouped by the same locality or model, and (k,l) all mismatched pairs. More negative values represent higher differences in precision on mismatches than matches, for that row\u2019s criteria. Mismatching localities indeed account for more difference in precision than mismatching network models."}, {"heading": "E. Model Selection and Cross-Task Performance", "text": "Table VI (Upper) reports model performance across tasks. We do model selection on the validation partition (for each task on the left) and report task performance in testing, on the task method given by the column. The \u2206p(1) and rank are calculated as previously, where values on the diagonal are the same as in Tables II and III, respectively. On the off-diagonal, the model is evaluated in testing on the task it was not selected on. Table VI (Lower) reports model performance by doing model selection on the average of CC and LP precision.\nThis result clearly demonstrates the main take-away of our study: the \u2018best\u2019 network model depends on the subsequent task. The off-diagonal shows that in every case, models selected on the \u2018other\u2019 task perform very poorly. Consider the worst case for same-task selection\u2013LP on MovieLens\u2013scored 0 on our 4 selection criteria (Table IV), yet has a selected\nmodel performing 3x better in \u2206p(1) than it\u2019s cross-task selected model. Over our three datasets, the average factor increase in \u2206p(1) performance from model selection using same-task compared to using cross-task is \u224810x.\nAverage-Precision Model Selection performs poorly in both tasks for BeerAdvocate, and is dominated by the CC task in Last.fm and MovieLens, closely matching the CC rows. Therefore, by selecting a network model on both tasks, we never recover the suitable model for link prediction in any of the three datasets."}, {"heading": "F. Node Difficulty", "text": "Both of our prediction tasks evaluate the same node over many predictions. For collective classification, we make a prediction at a node for each positive label instance over many labelsets. For link prediction, we associate predictions on the ego-net (on the order of hundreds), with that node.\nWe can therefore build robust distributions of precision for individual nodes over a particular model configuration or union of configurations. Figure 4 reports the density map of precision of individual nodes for Last.fm, aggregated over the top-5 models in validation and testing partitions (10 models total), on varying methods (left) and tasks (right). The left plot shows that RF and SVM methods are mostly linearly correlated, with SVM performing better. The right plot shows low variance in LP-SVM, with some skew to higher-performing LP nodes in purple. CC-SVM shows higher variance, with a higher ceiling (the band at y=1) and many poorly-performing nodes below y=0.4.\nFigure 5 reports the same comparison for the BeerAdvocate dataset. Compared to Last.fm, CC-RF performs weaker in relative to CC-SVM, which has higher variance. Comparing CC and LP, the gradient has higher variance in both axes than in Last.fm, with the skew to poorer-performing nodes in LP.\nMeasuring the distribution of node hardness over varying multiple predictions on varying tasks allows further hypothesis\ngeneration and testing related to the distribution of these values over the network topology or other feature extraction to characterize the task according to the application. Our focus in this work is primarily on the evaluation methodology for network model selection, so we only give the highest-level introduction of this characterization step."}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "This work focused on a general task-focused network model selection methodology which uses fundamental network tasks\u2013collective classification and link prediction\u2013to evaluate several common network models and localities. We propose evaluating model selection for network tasks under several criteria including (1) task precision stability, (2) selected model rank consistency, (3) full rank stability, and (4) top-k rank consistency. We evaluate three user rating datasets and show robust selection of particular models for several of the task settings. We demonstrate that network model selection is highly subject to a particular task of interest, showing that model selection across tasks performs an order of magnitude better than selecting on another task."}, {"heading": "A. Limitations and Future Improvements", "text": "1) Incorporating model cost: We currently do not incorporate network model cost (e.g. sparsity), nor prediction method cost (e.g. method encoding size in bytes, runtime)\nas criteria for model selection. In future work we wish to penalize more costly models.\nFor example, we train on the order of thousands of small \u2018local\u2019 models, while an ensemble model which may have similar performance trains on tens of nodes. Future work will explore ensembles of local methods to summarize the task under minimal cost. Some task methods are also fairly robust to our choice of network density parameters; the sparser network model would be preferable.\n2) Network model Alternativeness: We would also like to discover alternative network models. Model \u2018alternativeness\u2019 refers to discovering maximally different model representations (by some criteria) which satisfy given constraints [27], [28]. In future work we would like to identify maximally orthogonal network models of similar (high) performance over our task and labelset regime, under some informative structural or task orthogonality. Section VI-F explores node-level joint density of task performance. Alternativeness in this setting may maximize differences in the joint distributions of well-performing models, and report or merge this set of models in model selection.\n3) Model Stationarity: Our results in Section VI-A show some indication of improved performance on the preceding partition (validation) than the future partition (testing). Our model selection framework tests network model temporal stationarity \u2018for free,\u2019 and can be used to measure the decay of both predictive performance and model rank ordering over increased time-horizons. Both of these signals can indicate a model change in the underlying data over time."}], "references": [{"title": "Friends and neighbors on the Web", "author": ["L.A. Adamic", "E. Adar"], "venue": "Social Networks, vol. 25, no. 3, pp. 211\u2013230, jul 2003. [Online]. Available: https://doi.org/10.1016/S0378-8733(03)00009-1", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "The Role of Social Networks in Information Diffusion", "author": ["E. Bakshy", "I. Rosenn", "C. Marlow", "L. Adamic"], "venue": "Proceedings of the 21st International Conference on World Wide Web, ser. WWW \u201912. New York, NY, USA: ACM, 2012, pp. 519\u2013528. [Online]. Available: http://doi.acm.org/10.1145/2187836.2187907", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A Survey of Link Prediction in Social Networks", "author": ["M.A. Hasan", "M.J. Zaki"], "venue": "Social Network Data Analytics SE - 9, C. C. Aggarwal, Ed. Springer US, 2011, pp. 243\u2013275. [Online]. Available: http://dx.doi.org/10.1007/978-1-4419-8462-3_9", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The link-prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "Journal of the American Society for Information Science and Technology, vol. 58, no. 7, pp. 1019\u20131031, May 2007. [Online]. Available: http://doi.wiley.com/10.1002/asi.20591", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Link prediction using supervised learning", "author": ["M. Al Hasan", "V. Chaoji", "S. Salem", "M. Zaki"], "venue": "SDM06: workshop on link analysis, counter-terrorism and security, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Joint Link Prediction and Attribute Inference Using a Social-Attribute Network", "author": ["N.Z. Gong", "A. Talwalkar", "L. Mackey", "L. Huang", "E.C.R. Shin", "E. Stefanov", "E.R. Shi", "D. Song"], "venue": "ACM Trans. Intell. Syst. Technol., vol. 5, no. 2, pp. 27:1\u2014-27:20, Apr. 2014. [Online]. Available: http://doi.acm.org/10.1145/2594455", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Collective classification of network data.", "author": ["B. London", "L. Getoor"], "venue": "Data Classification: Algorithms and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Collective Classification in Network Data", "author": ["P. Sen", "G.M. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi- Rad"], "venue": "AI Magazine, vol. 29, no. 3, pp. 93\u2013106, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Labels or Attributes?: Rethinking the Neighbors for Collective Classification in Sparsely-labeled Networks", "author": ["L.K. McDowell", "D.W. Aha"], "venue": "Proceedings of the 22Nd ACM International Conference on Information & Knowledge Management, ser. CIKM \u201913. New  York, NY, USA: ACM, 2013, pp. 847\u2013852. [Online]. Available: http://doi.acm.org/10.1145/2505515.2505628", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining Collective Classification and Link Prediction", "author": ["M. Bilgic", "G.M. Namata", "L. Getoor"], "venue": "Seventh IEEE International Conference on Data Mining Workshops (ICDMW 2007), oct 2007, pp. 381\u2013386.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Collective Graph Identification", "author": ["G.M. Namata", "B. London", "L. Getoor"], "venue": "2015. [Online]. Available: https://doi.org/10.1145/2818378", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Network Structure Inference, A Survey: Motivations, Methods, and Applications", "author": ["I. Brugere", "B. Gallagher", "T.Y. Berger-Wolf"], "venue": "ArXiv e-prints, Oct. 2016. [Online]. Available: https://arxiv.org/abs/1610.00782", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Network Topology Inference", "author": ["E. Kolaczyk", "G. Cs\u00e1rdi"], "venue": "Statistical Analysis of Network Data with R SE - 7, ser. Use R! Springer New York, 2014, vol. 65, pp. 111\u2013134. [Online]. Available: http://dx.doi.org/10.1007/978-1-4939-0983-4_7", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A General Framework for Weighted Gene Co-Expression Network Analysis", "author": ["B. Zhang", "S. Horvath"], "venue": "Statistical Applications in Genetics and Molecular Biology, vol. 4, no. 1, 2005.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Contributions and challenges for network models in cognitive neuroscience", "author": ["O. Sporns"], "venue": "Nature Neuroscience, vol. 17, no. 5, pp. 652\u2013660, May 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Inferring Networks of Substitutable and Complementary Products", "author": ["J. McAuley", "R. Pandey", "J. Leskovec"], "venue": "Proc. Proc. of ACM SIGKDD 2015, ser. KDD \u201915. ACM, 2015, pp. 785\u2013794. [Online]. Available: http://doi.acm.org/10.1145/2783258.2783381", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Inferring Networks of Diffusion and Influence", "author": ["M. Gomez-Rodriguez", "J. Leskovec", "A. Krause"], "venue": "ACM Transactions on Knowledge Discovery from Data, vol. 5, no. 4, pp. 21:1\u2014-21:37, Feb. 2012. [Online]. Available: http://doi.acm.org/10.1145/2086737.2086741", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "On the Convexity of Latent Social Network Inference", "author": ["S. Myers", "J. Leskovec"], "venue": "2010. [Online]. Available: http://papers.nips.cc/ paper/4113-on-the-convexity-of-latent-social-network-inference", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Attributed Graph Models: Modeling Network Structure with Correlated Attributes", "author": ["J.J. Pfeiffer III", "S. Moreno", "T. La Fond", "J. Neville", "B. Gallagher"], "venue": "Proceedings of the 23rd International Conference on World Wide Web, ser. WWW \u201914. ACM, 2014, pp. 831\u2013842. [Online]. Available: http://doi.acm.org/10.1145/2566486.2567993", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiplicative Attribute Graph Model of Real-World Networks", "author": ["M. Kim", "J. Leskovec"], "venue": "Internet Mathematics, vol. 8, no. 1-2, pp. 113\u2013160, mar 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "An introduction to exponential random graph (p*) models for social networks", "author": ["G. Robins", "P. Pattison", "Y. Kalish", "D. Lusher"], "venue": "Social Networks, vol. 29, no. 2, pp. 173\u2013191, may 2007. [Online]. Available: https://doi.org/10.1016/j.socnet.2006.08.002", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Inferring Relevant Social Networks from Interpersonal Communication", "author": ["M. De Choudhury", "W.A. Mason", "J.M. Hofman", "D.J. Watts"], "venue": "Proceedings of the 19th International Conference on World Wide Web, ser. WWW \u201910. ACM, 2010, pp. 301\u2013310. [Online]. Available: http://doi.acm.org/10.1145/1772690.1772722", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast unfolding of communities in large networks", "author": ["V.D. Blondel", "J.-L. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "Journal of Statistical Mechanics: Theory and Experiment, vol. 10, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Evaluating social networks using task-focused network inference", "author": ["I. Brugere", "C. Kanich", "T. Berger-Wolf"], "venue": "Proceedings of the 13th International Workshop on Mining and Learning with Graphs (MLG), 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "The MovieLens Datasets: History and Context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Trans. Interact. Intell. Syst., vol. 5, no. 4, pp. 19:1\u2014-19:19, dec 2015. [Online]. Available: http://doi.acm.org/10.1145/2827872", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning Attitudes and Attributes from Multi-aspect Reviews", "author": ["J. McAuley", "J. Leskovec", "D. Jurafsky"], "venue": "Proceedings of the 2012 IEEE 12th International Conference on Data Mining, ser. ICDM \u201912. Washington, DC, USA: IEEE Computer Society, 2012, pp. 1020\u20131025. [Online]. Available: http://dx.doi.org/10.1109/ICDM.2012.110", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "A Principled and Flexible Framework for Finding Alternative Clusterings", "author": ["Z. Qi", "I. Davidson"], "venue": "Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201909. New York, NY, USA: ACM, 2009, pp. 717\u2013726. [Online]. Available: http://doi.acm.org/10.1145/1557019.1557099", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiple non-redundant spectral clustering views", "author": ["D. Niu", "J.G. Dy", "M.I. Jordan"], "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), 2010, pp. 831\u2013838.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "For example, simple link-prediction heuristics in social networks tend to perform well because they correspond to the social processes for how networks grow [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "However, content sharing in online social networks shows that \u2018weak\u2019 ties among friends account for much of the influence on users [2].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "The link prediction task [3] predicts edges from \u2018local\u2019", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].", "startOffset": 234, "endOffset": 237}, {"referenceID": 5, "context": "This is done by simple ranking, on information such as comparisons over common neighbors [1], higher-order measures such as path similarity [4], or learning edge/non-edge classification as a supervised task on extracted edge features [5], [6].", "startOffset": 239, "endOffset": 242}, {"referenceID": 6, "context": "The collective classification task [7], [8] learns relationships between local neighborhood structure of labels and/or attributes [9] to predict unknown labels in the network.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "The collective classification task [7], [8] learns relationships between local neighborhood structure of labels and/or attributes [9] to predict unknown labels in the network.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "The collective classification task [7], [8] learns relationships between local neighborhood structure of labels and/or attributes [9] to predict unknown labels in the network.", "startOffset": 130, "endOffset": 133}, {"referenceID": 9, "context": "This problem has been extended to joint prediction of edges [10], and higher-order joint prediction [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 10, "context": "This problem has been extended to joint prediction of edges [10], and higher-order joint prediction [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "Network structure inference [12], [13] is a broad area of research aimed at transforming data on individuals or entities into a network representation which can leverage methods such as relational machine learning.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "Network structure inference [12], [13] is a broad area of research aimed at transforming data on individuals or entities into a network representation which can leverage methods such as relational machine learning.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Previous work spans numerous domains including bioinformatics [14], neuroscience [15], and recommender systems [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "Previous work spans numerous domains including bioinformatics [14], neuroscience [15], and recommender systems [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "Previous work spans numerous domains including bioinformatics [14], neuroscience [15], and recommender systems [16].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "For example, previous work has modeled the \u2018arrival time\u2019 of information in a content network with unknown edges, where rates of transmission are the learned model [17], [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 17, "context": "For example, previous work has modeled the \u2018arrival time\u2019 of information in a content network with unknown edges, where rates of transmission are the learned model [17], [18].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "These include the Attributed Graph Model (AGM) [19], Multiplicative Attribute Graph model (MAG) [20], and Exponential Random Graph Model (ERGM) [21].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "These include the Attributed Graph Model (AGM) [19], Multiplicative Attribute Graph model (MAG) [20], and Exponential Random Graph Model (ERGM) [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "These include the Attributed Graph Model (AGM) [19], Multiplicative Attribute Graph model (MAG) [20], and Exponential Random Graph Model (ERGM) [21].", "startOffset": 144, "endOffset": 148}, {"referenceID": 21, "context": "Recent work on task-focused network inference evaluates inferred network models according to their ability to perform a set of tasks [22].", "startOffset": 133, "endOffset": 137}, {"referenceID": 17, "context": "3) Network density: We evaluate network models at varying density, which has previously been the primary focus of networks inferred on similarity spaces [18], [22].", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "3) Network density: We evaluate network models at varying density, which has previously been the primary focus of networks inferred on similarity spaces [18], [22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 7, "context": "1) Collective classification (CC): The collective classification problem learns relationships between network edge structure and attributes and/or labels to predict label values [8], [10].", "startOffset": 178, "endOffset": 181}, {"referenceID": 9, "context": "1) Collective classification (CC): The collective classification problem learns relationships between network edge structure and attributes and/or labels to predict label values [8], [10].", "startOffset": 183, "endOffset": 187}, {"referenceID": 3, "context": "2) Link prediction (LP): The link prediction problem [4] learns a method for the appearance of edges from one edge-set to another.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "Link prediction methods can incorporate attribute and/or label data, or using simple structural ranking [1].", "startOffset": 104, "endOffset": 107}, {"referenceID": 22, "context": "2) Community: We calculate structural community labels for each network using the Louvain method [23].", "startOffset": 97, "endOffset": 101}, {"referenceID": 25, "context": "The BeerAdvocate review dataset [26] contains 1.", "startOffset": 32, "endOffset": 36}, {"referenceID": 24, "context": "The MovieLens 20M ratings dataset [25] contains movie ratings (1-5 stars) data over 138,493 users, through March 2015.", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "fm 20K [24] 19,990 1,243,483,909 artist plays 578/1713 artists 3/11 music genres 60 genres", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": "MovieLens [25] 138,493 20,000,263 movie ratings 81/407 ratings 7/53 movie tags 100 tags", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "BeerAdvocate [26] 33,387 1,586,259 beer ratings 3/91 ratings 0/3 beer types 45 types", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "This is very surprising from previous results show poor performance of local models on the social network but did not evaluate other localities [24].", "startOffset": 144, "endOffset": 148}, {"referenceID": 26, "context": "Model \u2018alternativeness\u2019 refers to discovering maximally different model representations (by some criteria) which satisfy given constraints [27], [28].", "startOffset": 139, "endOffset": 143}, {"referenceID": 27, "context": "Model \u2018alternativeness\u2019 refers to discovering maximally different model representations (by some criteria) which satisfy given constraints [27], [28].", "startOffset": 145, "endOffset": 149}], "year": 2017, "abstractText": "Networks are models representing relationships between entities. Often these relationships are explicitly given, or we must learn a representation which generalizes and predicts observed behavior in underlying individual data (e.g. attributes or labels). Whether given or inferred, choosing the best representation affects subsequent tasks and questions on the network. This work focuses on model selection to evaluate network representations from data, focusing on fundamental predictive tasks on networks. We present a modular methodology using general, interpretable network models, task neighborhood functions found across domains, and several criteria for robust model selection. We demonstrate our methodology on three online user activity datasets and show that network model selection for the appropriate network task vs. an alternate task increases performance by an order of magnitude in our experiments.", "creator": "LaTeX with hyperref package"}}}