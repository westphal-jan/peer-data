{"id": "1708.07104", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "Automatic Detection of Fake News", "abstract": "The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analysis on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors. In addition, we provide comparative analyses of the automatic and manual identification of fake news.", "histories": [["v1", "Wed, 23 Aug 2017 17:12:03 GMT  (28kb)", "http://arxiv.org/abs/1708.07104v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ver\\'onica p\\'erez-rosas", "bennett kleinberg", "alexandra lefevre", "rada mihalcea"], "accepted": false, "id": "1708.07104"}, "pdf": {"name": "1708.07104.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Alexandra Lefevre", "Rada Mihalcea"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n07 10\n4v 1\n[ cs\n.C L\n] 2\n3 A\nug 2\n01 7\ntion in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analysis on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors. In addition, we provide comparative analyses of the automatic and manual identification of fake news."}, {"heading": "1 Introduction", "text": "Fake news detection has recently attracted a growing interest from the general public and researchers as the circulation of missinformation online increases, particularly in media outlets such as social media feeds, news blogs, and online newspapers. For instance, a recent report by the Jumpshot Tech Blog1 found that Facebook referrals accounted for 50% of the total traffic to fake news sites and 20% total traffic to reputable websites. Since the majority of U.S. adults \u201362%\u2013 gets news on social media (Jeffrey and Elisa, 2016), being\n1https://www.jumpshot.com/data-facebooks-fake-newsproblem/\nable to identify fake content in online sources is a pressing need.\nTo date, computational approaches for fake news detection have relied on satirical news sources such as \u201cThe Onion\u201d and fact-checking websites such as \u201dpolitiFact\u201d and \u201dSnopes\u201d. However, the use of these sources poses several challenges and potential drawbacks. For instance, using satirical content as a source for fake content can bring underlying confounding factors into the analysis, such as humor and absurdity. This is particularly the case for satirical news from \u201cThe Onion\u201d, which has been used in the past to explore other text properties such as humor (Mihalcea and Strapparava, 2005) and irony (Wallace, 2015). On the other hand, fact-checking websites are usually constrained to a particular domain of interest, such as politics, and require human expertise, thus making it difficult to obtain datasets that provide some degree of generalization over several domains.\nIn this paper, we develop computational resources and models for the task of fake news detection. We present the construction of two novel datasets covering seven different domains. One of the datasets is collected using a combination of manual and crowdsourced annotation efforts, while the second is collected directly from the web. Using these datasets, we conduct several exploratory analyses to identify linguistic properties that are predominantly present in fake content, and we build fake news detectors relying on linguistic features that achieve accuracies of up to 78%. To place our results in perspective, we also compare the accuracy of our fake news detection models with an empirical human baseline accuracy."}, {"heading": "2 Related Work", "text": "To date, there are three important lines of research into the automated classification of genuine and\nfake news items. First, on a conceptual level, a distinction has been made between \u2019three types of fake news\u2019 (Rubin et al., 2015): serious fabrications (i.e. news items about false and nonexisting events or information such as celebrity gossip), hoaxes (i.e. providing false information via, for example, social media with the intention to be picked up by traditional news websites) and satire (i.e. humorous news items that mimic genuine news but contain irony and absurdity). Here, we focus on the first category, serious fabrication, in the two domains of general news (in six different categories), as well as on celebrity gossip.\nSecond, attempts to differentiate satire from real news yielded promising results (Rubin et al., 2016). The authors built a corpus of satire news (from The Onion and The Beaverton) and real news (The Toronto Star and The New York Times) in four domains (civics, science, business, soft news), resulting in a total of 240 news articles. The best classification performances were achieved with feature sets representing absurdity, punctuation, and grammar (each with an F1 score of 0.87).\nThird, recently, a stylometric (i.e. writing-style) approach has been proposed for the identification of fake and genuine news articles (Potthast et al., 2017). The investigation used the Buzzfeed dataset2 of mainstream and hyperpartisan news articles of which the veracity was manually annotated. Stylometric features were, among others, character and stop word n-grams, readability indices, as well as features such as external links and the average number of words per paragraph. As a comparison, a topic-based feature set of a non-domain specific bag-of-words approach was used. The dataset used by (Potthast et al., 2017) consisted of 1,627 news articles that were obtainable from the original Buzzfeed dataset, including 299 fake news articles. Although the stylometric approach was promising for the classification of hyperpartisan versus mainstream articles (accuracy: 0.75, compared to 0.71 for the topic-based feature set), both approaches were not able to differentiate fake from real news (accuracy: 0.55 and 0.52 for stylometric and topic-based feature sets, respectively).\nAlso related to our research is work done on the automatic identification of deceptive content, which has explored domains such as forums, consumer reviews websites, online ad-\n2http://github.com/BuzzFeedNews/2016-10-facebookfact-check\nvertising, online dating, and crowdfounding platforms (Warkentin et al., 2010; Ott et al., 2011a; Zhang and Guan, 2008; Toma and Hancock, 2010; Shafqat et al., 2016). Linguistic clues such as self references or positive and negative words have been used to profile true tellers from liars (Newman et al., 2003). Other work has focused on analyzing the number of words, sentences, self references, affect, spatial and temporal information associated with deceptive content (Qin et al., 2005). Expressivity, informality, diversity and non-immediacy have also been explored to identify deceitful behaviors (Shafqat et al., 2016)."}, {"heading": "3 Fake News Datasets", "text": "As highlighted earlier, the datasets used in previous work have either relied on satirical news (e.g., \u201cThe Onion\u201d), which also have confounds such as humor or irony; or used fact-checking websites (e.g., \u201cpolitiFact\u201d or \u201cSnopes\u201d), which are typically focused on only one domain (generally politics). We thus decided to construct two new datasets of fake news that cover several news domains and specifically model the deceptive property of fake news without major confounds. One dataset is collected via crowdsourcing, and covers six news domains; the second dataset is obtained directly from the web, and covers celebrity fake news.\nGuidelines for a Fake News Corpus. In building a fake news dataset, we adhered to the nine requirements of a fake news corpus proposed by (Rubin et al., 2016). Specifically, the authors suggested that such a corpus should (1) include both fake and real news items, (2) contain text-only news items, (3) have a verifiable ground-truth, (4) be homogeneous in length and (5) writing style, (6) contain news from a predefined time frame, (7) be delivered in the same manner and for the same purpose (e.g. humor, breaking news) for fake and real cases, (8) be made publicly available, and (9) should take language and cultural differences into account. In our work, to the extent possible, we aimed to address all of the above guidelines.3 As outlined in the following, the ground-truth remains challenging since we cannot verify with absolute certainty whether all the content of real news items is in fact true.\n3We did not explicitly account for cultural differences since the primary aim was to build a fake news dataset that met criterion 1 to 8."}, {"heading": "3.1 Building a Crowdsourced Dataset", "text": "Collecting Legitimate News. We started by collecting a dataset of legitimate news belonging to six different domains (sports, business, entertainment, politics, technology, and education). The news were obtained from a variety of mainstream news websites (predominantly in the US) such as the ABCNews, CNN, USAToday, NewYorkTimes, FoxNews, Bloomberg, and CNET among others.\nTo ensure the veracity of the news, we conducted manual fact-checking on the news content, which included verifying the news source and cross-referencing information among several sources. Using this approach, we collected 40 news in each of the six domains, for a total of 240 legitimate news.\nCollecting Fake News using Crowdsourcing. To generate fake versions of the news in the legitimate news dataset, we make use of crowdsourcing via Amazon Mechanical Turk, which has been successfully used in the past for collecting deception data on several domains, including opinion reviews (Ott et al., 2011b), and controversial topics such as abortion and death penalty (Pe\u0301rez-Rosas and Mihalcea, 2015).\nHowever, collecting deceptive data via AMT poses additional challenges on the news domain. First, the reporting language used by journalists might differ from AMT workers language (e.g., journalistic vs. informal style). Second, journalistic articles are usually lengthier than consumer reviews and opinions, thus increasing the difficulty of the task for AMT workers as they would be required to read a full news article and create a fake version from it.\nTo address the former, we asked the workers to the extent possible to emulate a journalistic style in their writing. This decision was motivated by the 5th point of the fake news corpus guidelines described in section 3, which suggests to obtain news with homogeneous writing style. To address the latter, we opted to working with smaller information units. Our approach consists of manually selecting a news excerpt that briefly describes the news article.4 Thus, from the legitimate news dataset collected earlier, we manually extracted 240 news excerpts. The final dataset consists of 33,378 words. Each news excerpt has on average 139 words and approximately 5 sentences.\n4In many cases, this corresponded to the first 2-3 paragraphs in the document.\nWe set up an AMT task that asked workers to generate a fake version of the provided news. Each hit included the legitimate news headline and its corresponding body. We instructed workers to produce both a fake headline and a fake news body within the same topic and length as the original news. Workers were also requested to avoid unrealistic content and to keep the names mentioned in the news. The fake news were produced by unique authors, as we allowed only a single submission per worker. We restricted the submission to workers located in the US as they might be more familiar with news published in the US media. In addition, we restricted participation to workers who maintained an approval rate of at least 95% to reduce potential spam contributions.\nIt took approximately five days to collect 240 fake news. Each hit was manually checked for spam and to make sure workers followed the provided guidelines. In general, we received few spam responses and most of the workers followed instructions satisfactorily; the only exceptions were a few cases where they provided only the headline or included unrealistic content.\nInterestingly, we observed that AMT workers succeeded in mimicking the reporting style from the original news, which may be partly explained by typical verbal mirroring behaviors with drive individuals to produce utterances that match the grammatical structure of sentences they have recently read (Ireland and Pennebaker, 2010). This partially addresses our initial concern of authors reporting style being a source of noise while analyzing news generated by journalists and AMT workers.\nThe final set of fake news consists of 31,990 words. Each fake news has on average 132 words and approximately 5 sentences. Table 1 shows a sample fake news, along with its legitimate version, in the technology domain.\nThroughout the rest of the paper, we refer to this\ncrowdsourced dataset as FakeNewsAMT."}, {"heading": "3.2 Building a Web Dataset", "text": "We collected a second dataset of fake news from web sources following similar guidelines as in the previous dataset. However, this time, we aimed to identify fake content that naturally occurs on the web. We opted for collecting news from public figures as they are frequently targeted by rumors, hoaxes, and fake reports. We focused mainly on celebrities (actors, singers, socialites,\nand politicians) and our sources include online magazines such as Entertainment Weekly, People Magazine, RadarOnline, among other tabloid and entertainment-oriented publications. The data were collected in pairs, with one article being legitimate and the other fake. In order to determine if a given celebrity news was legitimate or not, the claims made in the article were evaluated using gossip-checking sites such as \u201dGossipCop.com\u201d, and were cross-referenced with information from other sources.\nDuring the initial stages of the data collection, we noticed that celebrity news tend to center on sensational topics that sources believe readers want to read about, such as divorces, pregnancies, and fights. Consequently, celebrity news tends to follow certain celebrities more than others further leading to an inherent lack in topic diversity in celebrity news. To address this issue, we evaluated several sources to make sure we obtain a diversified pool of celebrities and topics. Upon beginning the data collection procedure using these\nguidelines, another characteristic surfaced: several pairs contained nearly the same information with similar lexicon and reporting style, with differences being as simple as just negating the false news. For example, the following headlines correspond to a news pair where the legitimate version only negates the fake version: \u201cAniston gets into fight with husband\u201d (fake) and \u201cAniston did NOT get into fight with husband\u201d (legitimate). To address this issue, we sought to identify related news that still followed the fake-legitimate pair property while being sufficiently diverse in lexicon and tone. In the former example, the fake news was paired with an article titled \u201cAniston and Husband enjoy dinner\u201d that was published on the date of the alleged fight.\nUsing this approach, we collected 100 fake news articles and 100 legitimate news articles in the celebrity domain. The final fake news set has an average of 399 words and 17 sentences per article, for a total of 39,940 words. The corresponding legitimate news set has an average of 709 words\nand 33 sentences per article, for a total of 70,975 words. 2 shows an example of an article pairing in the dataset.\nThroughout the rest of the paper, we refer to this\nweb dataset as Celebrity."}, {"heading": "4 Linguistic Features", "text": "To build the fake news detection models, we start by extracting several sets of linguistic features:\nNgrams. We extract unigrams and bigrams derived from the bag of words representation of each news article. To account for occasional differences in content length, these features are encoded as tfidf values.\nPunctuation. Previous work on fake news detection (Rubin et al., 2016) as well as on opinion spam (Ott et al., 2011b) suggests that the use of punctuation might be useful to differentiate deceptive from truthful texts. We construct a punctuation feature set consisting of eleven types of punctuation derived from the Linguistic Inquiry and Word Count software (LIWC, Version 1.3.1 2015) (Pennebaker et al., 2015). This includes punctuation characters such as periods, commas, dashes, question marks and exclamation marks.\nPsycholinguistic features. We use the LIWC lexicon to extract the proportions of words that fall into psycholinguistic categories. LIWC is based on large lexicons of word categories that represent psycholinguistic processes (e.g., positive emotions, perceptual processes), summary categories (e.g., words per sentence), as well as partof-speech categories (e.g., articles, verbs). Previous work on verbal deception detection showed that LIWC is a valuable tool for the deception detection in various contexts (e.g., genuine and fake hotel reviews, (Ott et al., 2011b, 2013); prisoners\u2019 lies (Bond and Lee, 2005)). In our work, we cluster the single LIWC categories into the following feature sets: summary categories (e.g., analytical thinking, emotional tone), linguistic processes (e.g., function words, pronouns), and psychological processes (e.g., affective processes, social processes).\nWe also test a combined feature set of all the\nLIWC categories (including punctuation).5\nReadability. We also extract features that indicate text understandability. These include con-\n5The feature sets linguistic processes and punctuation correspond to the \u2019grammar\u2019 and punctuation feature set, respectively, in (Rubin et al., 2016)\ntent features such as the number of characters, complex words, long words, number of syllables, word types, and number of paragraphs, among others content features. We also calculate several readability metrics, including the Flesch-Kincaid, Flesch Reading Ease, Gunning Fog, and the Automatic Readability Index (ARI).\nSyntax. Finally, we extract a set of features derived production rules based on context free grammars (CFG) trees using the Stanford Parser (Klein and Manning, 2003). The CFG derived features consist of all the lexicalized production rules (rules including child nodes) combined with their parent and grandparent node, e.g., *NN\u02c6NP\u2192commission (in this example NN \u2013a noun\u2013 is the grandparent node, NP \u2013personal pronoun\u2013 the parent node, and \u201ccommissions\u201d the child node. Features in this set are also encoded as tf-idf values."}, {"heading": "5 Computational Models for Fake News Detection", "text": "We conduct several experiments with different (combinations of) feature sets. We use a linear SVM classifier and five-fold cross-validation, with accuracy, precision, recall, and F1 measures averaged over the five iterations.\nThe machine learning classification was conducted with R (R Core Team, 2016) and the caret (Kuhn et al., 2016) and e1071 packages (Meyer et al., 2015).\nTables 3 and 4 show the results obtained for the different feature sets. As seen in the tables, most of the classifiers obtain performances well above the random baseline of 0.50. The best performing classifier for the FakeNewsAMT dataset is derived from the Readability features, followed by the combination of all linguistic feature sets. For the Celebrity dataset, the most accurate model is built using the Punctuation features, followed by the Ngrams, Complete LIWC, and Syntax features.\nLearning Curves. Next, we investigate whether larger amounts of training data can improve the identification of fake content. We plot the learning curves of the bests sets of features using incremental amounts of data as shown in Figures 1 and 2. Except for the decrease obtained with the Readability features on the Celebrity dataset, the learning trend for all the other feature sets on both datasets show steady improvement, thus suggesting that larger quantities of training data could im-\nprove the classification performance.\nCross-domain Analyses. We also explore the applicability of our methods across domains, using the two best feature sets (Readability and Complete LIWC), as well as the classifier relying on all the features (All Features). Table 5 shows the results obtained in cross-domain experiments between the FakeNewsAMT dataset and the Celebrity dataset. Perhaps not surprisingly, there is a significant loss in accuracy as compared to the within-domain results shown in Tables 3 and 4.\nThe metrics suggest that the generalization from the crowdsourced data to the celebrity news is biased towards the truth (i.e., the classifier almost exclusively predicted the \u2019true\u2019 class). Possible explanations for the drop in performance might\nbe (1) that the linguistic properties of deception in one domain are structurally different from those of deception in a second domain, and (2) that the feature sets applied for the cross-domain evaluation, in particular the readability feature set (accuracy = 0.50), were not performing well in the respective domain in the first place. To test this idea, we also applied cross-domain evaluation where we trained the classifier of domain A on the with the best feature set of domain B and tested in on domain B (here: Complete LIWC and Readability for the Celebrity and FakeNewsAMT data, respectively). The readability feature set classifier of the Celebrity data yielded an accuracy of 0.61 on the FakeNewsAMT data (compared to the original 0.78), and, vice versa, the Complete LIWC\nclassifier resulted in an accuracy of 0.61 (compared to 0.70). These findings indicate that different linguistic properties underlying different kinds of deception are more likely to explain crossdomain performance decreases than poorly performing feature sets.\nWe also assess the cross-domain classification performance for the six news domains in the FakeNewsAMT dataset. We do this by training on five of the six domains in the datset, and testing the remaining one. Table 6 shows the results obtained in these experiments. The politics, education, and technology domains appear to be rather robust against classifiers trained on other domains. The technology and politics domains, moreover, are classified both with a high accuracy of 0.91 with the Readability feature set, which may suggest that fake and legitimate news in each of these three domains might be structurally similar to the fake and legitimate content in the other five domains. By contrast, domains such as sports, business and entertainment are less generalizable and might therefore be more domain-dependent. Although further research is needed to consolidate these findings, a possible explanation could be the rather unique content and style of these domains"}, {"heading": "6 Human Performance", "text": "To identify a human baseline for the fake news detection task, we conducted a study to evaluate the human ability to spot fake news on the two developed datasets. We created an annotation interface that shows an annotator either a fake or a legitimate news article, and asks them to judge its credibility. We asked annotators to select a label of\neither \u201cFake\u201d or \u201cLegitimate\u201d according to their own perceptions. We also asked them to indicate whether or not they have read or heard about the presented news in the past; overall, the annotators read less than 5% of the news before, which we considered to be a negligible fraction.\nTwo annotators labeled the news in each dataset. In both cases, the news articles were presented in a random order to avoid annotation bias. Annotators evaluated 480 and 200 news for the FakeNewsAMT and Celebrity datasets respectively. Annotators were not offered a monetary reward and we consider their judgments to be honest as they participated voluntarily in this experiment. Table 7 shows the observed agreement and Kappa statistics for each dataset. Resulting Kappa values show moderate agreement values with slightly lower Kappa for the FakeNewAMT dataset. The results suggest that humans are better at identifying fake news in the celebrity domain than fake news in other domains.\nIn addition, we evaluate the performance of the automatic fake news classifiers against the human capability to spot fake news. Thus, we compare the accuracy of our system to that of human annotators. Table 8 summarizes the accuracies obtained by the human annotators and our system on the two fake news datasets. Results confirm that humans are better at detecting fake content in the Celebrity domain. Notably, our system outperforms humans while detecting fake news in more serious and diverse news sources."}, {"heading": "7 Further Insights", "text": "Our experiments suggest important differences in fake news content as compared to legitimate news content. Particularly, we observe that classifiers relying on the semantic information encoded in the LIWC lexicon show consistently good performance across domains. To gain further insights into the semantic classes that are associated with fake and legitimate content, we evaluate which classes show significant differences between the two groups of news. To compare both types of content, we subtract the average percentage of words in each LIWC category in the fake news from its corresponding values in the legitimate news set. Therefore, a positive result indicates an association between a LIWC class and legitimate content, and a negative result indicates an association between a LIWC class and fake content. Results for the FakeNewsAMT and Celebrity datasets are shown in Figures 3 and 4 respectively. All the differences shown in the graphs are statically significant (one tailed t-test, p < 0.5).\nFigure 3 indicates that the language used to report legitimate content in the FakeNewsAMT dataset, often includes words associated with cognitive processes such as insight and differentiation. In addition, legitimate content includes more function words such as he, she, and negations, and expresses relativity. On the other hand, language used when reporting fake content uses more social and positive words, expresses more certainty and focuses on present and future actions. Moreover, the authors of fake news use more adverbs, verbs, and punctuation characters than the authors of legitimate news. Likewise, results in Figure 4 show noticeable differences among legitimate and fake content on the celebrity domain. Specifically, fake content in tabloid and entertainment magazines seem to use more perceptual words, e.g., hear, see, feeling, and positive emotions cate-\ngories. In addition, fake content in this domain has a predominant use of the \u201cI\u201d pronoun and prepositions. In contrast, legitimate content uses words that indicate cognitive processes such as insight, cause, discrepancy, and tentative language."}, {"heading": "8 Conclusions", "text": "In this paper, we addressed the task of automatic identification of fake news. We introduced two new fake news datasets, one obtained through crowdsourcing and covering six news domains, and another one obtained from the web covering celebrities. We developed classification models that rely on a combination of lexical, syntactic, and semantic information, as well features representing text readability properties. Our best performing models achieved accuracies that are comparable to human ability to spot fake content."}], "references": [{"title": "Language of lies in prison: Linguistic classification of prisoners\u2019 truthful and deceptive natural language", "author": ["Gary D Bond", "Adrienne Y Lee."], "venue": "Applied Cognitive Psychology 19(3):313\u2013329.", "citeRegEx": "Bond and Lee.,? 2005", "shortCiteRegEx": "Bond and Lee.", "year": 2005}, {"title": "Language style matching in writing: synchrony in essays, correspondence, and poetry", "author": ["Molly E Ireland", "James W Pennebaker."], "venue": "Journal of personality and social psychology 99(3):549.", "citeRegEx": "Ireland and Pennebaker.,? 2010", "shortCiteRegEx": "Ireland and Pennebaker.", "year": 2010}, {"title": "News use across social media platforms 2016", "author": ["Gottfried Jeffrey", "Shearer Elisa."], "venue": "Pew Research Center Reports. http://www.journalism.org/2016/05/26/news-use-across-social-media-platforms-2016/.", "citeRegEx": "Jeffrey and Elisa.,? 2016", "shortCiteRegEx": "Jeffrey and Elisa.", "year": 2016}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D. Manning."], "venue": "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1. Association for Computational Linguistics, Strouds-", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "e1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien", "author": ["David Meyer", "Evgenia Dimitriadou", "Kurt Hornik", "Andreas Weingessel", "Friedrich Leisch."], "venue": "R package version 1.6-7.", "citeRegEx": "Meyer et al\\.,? 2015", "shortCiteRegEx": "Meyer et al\\.", "year": 2015}, {"title": "Making computers laugh: Investigations in automatic humor recognition", "author": ["Rada Mihalcea", "Carlo Strapparava."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Mihalcea and Strapparava.,? 2005", "shortCiteRegEx": "Mihalcea and Strapparava.", "year": 2005}, {"title": "Lying words: Predicting deception from linguistic styles", "author": ["M. Newman", "J. Pennebaker", "D. Berry", "J. Richards."], "venue": "Personality and Social Psychology Bulletin 29.", "citeRegEx": "Newman et al\\.,? 2003", "shortCiteRegEx": "Newman et al\\.", "year": 2003}, {"title": "Negative deceptive opinion spam", "author": ["Myle Ott", "Claire Cardie", "Jeffrey T Hancock."], "venue": "HLT-NAACL. pages 497\u2013501.", "citeRegEx": "Ott et al\\.,? 2013", "shortCiteRegEx": "Ott et al\\.", "year": 2013}, {"title": "Finding deceptive opinion spam by any stretch of the imagination", "author": ["Myle Ott", "Yejin Choi", "Claire Cardie", "Jeffrey Hancock."], "venue": "Proceedings of the 49th AnnualMeeting of the Association for Computational Linguistics: Human Language Technolo-", "citeRegEx": "Ott et al\\.,? 2011a", "shortCiteRegEx": "Ott et al\\.", "year": 2011}, {"title": "Finding deceptive opinion spam by any stretch of the imagination", "author": ["Myle Ott", "Yejin Choi", "Claire Cardie", "Jeffrey T. Hancock."], "venue": "Proceedings of the 49th Annual Meet-", "citeRegEx": "Ott et al\\.,? 2011b", "shortCiteRegEx": "Ott et al\\.", "year": 2011}, {"title": "The development and psychometric properties of liwc2015", "author": ["James W Pennebaker", "Ryan L Boyd", "Kayla Jordan", "Kate Blackburn."], "venue": "Technical report.", "citeRegEx": "Pennebaker et al\\.,? 2015", "shortCiteRegEx": "Pennebaker et al\\.", "year": 2015}, {"title": "Experiments in open domain deception detection", "author": ["Ver\u00f3nica P\u00e9rez-Rosas", "Rada Mihalcea."], "venue": "Proceedings of the 2015 Conference on", "citeRegEx": "P\u00e9rez.Rosas and Mihalcea.,? 2015", "shortCiteRegEx": "P\u00e9rez.Rosas and Mihalcea.", "year": 2015}, {"title": "A stylometric inquiry into hyperpartisan and fake news", "author": ["Martin Potthast", "Johannes Kiesel", "Kevin Reinartz", "Janek Bevendorff", "Benno Stein"], "venue": null, "citeRegEx": "Potthast et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Potthast et al\\.", "year": 2017}, {"title": "Modality effects in deception detection and applications in automatic deception-detection", "author": ["T. Qin", "J.K. Burgoon", "J.P. Blair", "J.F. Nunamaker."], "venue": "Proceedings of the 38th Hawaii International Conference on System Sciences.", "citeRegEx": "Qin et al\\.,? 2005", "shortCiteRegEx": "Qin et al\\.", "year": 2005}, {"title": "R: A Language and Environment for Statistical Computing", "author": ["R Core Team."], "venue": "R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Deception detection for news: Three types of fakes", "author": ["Victoria L. Rubin", "Yimin Chen", "Niall J. Conroy."], "venue": "Proceedings of the Association for Information Science and Technology 52(1):1\u20134.", "citeRegEx": "Rubin et al\\.,? 2015", "shortCiteRegEx": "Rubin et al\\.", "year": 2015}, {"title": "Fake news or truth? using satirical cues to detect potentially misleading news", "author": ["Victoria L Rubin", "Niall J Conroy", "Yimin Chen", "Sarah Cornwell."], "venue": "Proceedings of NAACL-HLT. pages 7\u201317.", "citeRegEx": "Rubin et al\\.,? 2016", "shortCiteRegEx": "Rubin et al\\.", "year": 2016}, {"title": "The language of deceivers: Linguistic features of crowdfunding scams", "author": ["Wafa Shafqat", "Seunghun Lee", "Sehrish Malik", "Hyun-chul Kim."], "venue": "Proceedings of the 25th International Conference Companion on World Wide Web. International World", "citeRegEx": "Shafqat et al\\.,? 2016", "shortCiteRegEx": "Shafqat et al\\.", "year": 2016}, {"title": "Reading between the lines: linguistic cues to deception in online dating profiles", "author": ["C. Toma", "J. Hancock."], "venue": "Proceedings of the 2010 ACM conference on Computer supported cooperative work. ACM, New York, NY, USA, CSCW \u201910, pages 5\u20138.", "citeRegEx": "Toma and Hancock.,? 2010", "shortCiteRegEx": "Toma and Hancock.", "year": 2010}, {"title": "Computational irony: A survey and new perspectives", "author": ["Byron C Wallace."], "venue": "Artificial Intelligence Review 43(4):467\u2013483.", "citeRegEx": "Wallace.,? 2015", "shortCiteRegEx": "Wallace.", "year": 2015}, {"title": "Warrants and deception in computer mediated communication", "author": ["D. Warkentin", "M. Woodworth", "J. Hancock", "N. Cormier."], "venue": "Proceedings of", "citeRegEx": "Warkentin et al\\.,? 2010", "shortCiteRegEx": "Warkentin et al\\.", "year": 2010}, {"title": "Detecting click fraud in pay-per-click streams of online advertising networks", "author": ["Linfeng Zhang", "Yong Guan."], "venue": "Distributed Computing Systems, 2008. ICDCS\u201908. The 28th International Conference on. IEEE, pages 77\u201384.", "citeRegEx": "Zhang and Guan.,? 2008", "shortCiteRegEx": "Zhang and Guan.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "adults \u201362%\u2013 gets news on social media (Jeffrey and Elisa, 2016), being", "startOffset": 39, "endOffset": 64}, {"referenceID": 5, "context": "This is particularly the case for satirical news from \u201cThe Onion\u201d, which has been used in the past to explore other text properties such as humor (Mihalcea and Strapparava, 2005) and irony (Wallace, 2015).", "startOffset": 146, "endOffset": 178}, {"referenceID": 19, "context": "This is particularly the case for satirical news from \u201cThe Onion\u201d, which has been used in the past to explore other text properties such as humor (Mihalcea and Strapparava, 2005) and irony (Wallace, 2015).", "startOffset": 189, "endOffset": 204}, {"referenceID": 15, "context": "First, on a conceptual level, a distinction has been made between \u2019three types of fake news\u2019 (Rubin et al., 2015): serious fabrications (i.", "startOffset": 93, "endOffset": 113}, {"referenceID": 16, "context": "Second, attempts to differentiate satire from real news yielded promising results (Rubin et al., 2016).", "startOffset": 82, "endOffset": 102}, {"referenceID": 12, "context": "writing-style) approach has been proposed for the identification of fake and genuine news articles (Potthast et al., 2017).", "startOffset": 99, "endOffset": 122}, {"referenceID": 12, "context": "The dataset used by (Potthast et al., 2017) consisted of 1,627 news articles that were obtainable from the original Buzzfeed dataset, including 299 fake news articles.", "startOffset": 20, "endOffset": 43}, {"referenceID": 20, "context": "com/BuzzFeedNews/2016-10-facebookfact-check vertising, online dating, and crowdfounding platforms (Warkentin et al., 2010; Ott et al., 2011a; Zhang and Guan, 2008; Toma and Hancock, 2010; Shafqat et al., 2016).", "startOffset": 98, "endOffset": 209}, {"referenceID": 8, "context": "com/BuzzFeedNews/2016-10-facebookfact-check vertising, online dating, and crowdfounding platforms (Warkentin et al., 2010; Ott et al., 2011a; Zhang and Guan, 2008; Toma and Hancock, 2010; Shafqat et al., 2016).", "startOffset": 98, "endOffset": 209}, {"referenceID": 21, "context": "com/BuzzFeedNews/2016-10-facebookfact-check vertising, online dating, and crowdfounding platforms (Warkentin et al., 2010; Ott et al., 2011a; Zhang and Guan, 2008; Toma and Hancock, 2010; Shafqat et al., 2016).", "startOffset": 98, "endOffset": 209}, {"referenceID": 18, "context": "com/BuzzFeedNews/2016-10-facebookfact-check vertising, online dating, and crowdfounding platforms (Warkentin et al., 2010; Ott et al., 2011a; Zhang and Guan, 2008; Toma and Hancock, 2010; Shafqat et al., 2016).", "startOffset": 98, "endOffset": 209}, {"referenceID": 17, "context": "com/BuzzFeedNews/2016-10-facebookfact-check vertising, online dating, and crowdfounding platforms (Warkentin et al., 2010; Ott et al., 2011a; Zhang and Guan, 2008; Toma and Hancock, 2010; Shafqat et al., 2016).", "startOffset": 98, "endOffset": 209}, {"referenceID": 6, "context": "Linguistic clues such as self references or positive and negative words have been used to profile true tellers from liars (Newman et al., 2003).", "startOffset": 122, "endOffset": 143}, {"referenceID": 13, "context": "Other work has focused on analyzing the number of words, sentences, self references, affect, spatial and temporal information associated with deceptive content (Qin et al., 2005).", "startOffset": 160, "endOffset": 178}, {"referenceID": 17, "context": "Expressivity, informality, diversity and non-immediacy have also been explored to identify deceitful behaviors (Shafqat et al., 2016).", "startOffset": 111, "endOffset": 133}, {"referenceID": 16, "context": "In building a fake news dataset, we adhered to the nine requirements of a fake news corpus proposed by (Rubin et al., 2016).", "startOffset": 103, "endOffset": 123}, {"referenceID": 9, "context": "To generate fake versions of the news in the legitimate news dataset, we make use of crowdsourcing via Amazon Mechanical Turk, which has been successfully used in the past for collecting deception data on several domains, including opinion reviews (Ott et al., 2011b), and controversial topics such as abortion and death penalty (P\u00e9rez-Rosas and Mihalcea, 2015).", "startOffset": 248, "endOffset": 267}, {"referenceID": 11, "context": ", 2011b), and controversial topics such as abortion and death penalty (P\u00e9rez-Rosas and Mihalcea, 2015).", "startOffset": 70, "endOffset": 102}, {"referenceID": 1, "context": "Interestingly, we observed that AMT workers succeeded in mimicking the reporting style from the original news, which may be partly explained by typical verbal mirroring behaviors with drive individuals to produce utterances that match the grammatical structure of sentences they have recently read (Ireland and Pennebaker, 2010).", "startOffset": 298, "endOffset": 328}, {"referenceID": 16, "context": "Previous work on fake news detection (Rubin et al., 2016) as well as on opinion spam (Ott et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 9, "context": ", 2016) as well as on opinion spam (Ott et al., 2011b) suggests that the use of punctuation might be useful to differentiate deceptive from truthful texts.", "startOffset": 35, "endOffset": 54}, {"referenceID": 10, "context": "1 2015) (Pennebaker et al., 2015).", "startOffset": 8, "endOffset": 33}, {"referenceID": 0, "context": ", 2011b, 2013); prisoners\u2019 lies (Bond and Lee, 2005)).", "startOffset": 32, "endOffset": 52}, {"referenceID": 16, "context": "The feature sets linguistic processes and punctuation correspond to the \u2019grammar\u2019 and punctuation feature set, respectively, in (Rubin et al., 2016) tent features such as the number of characters, complex words, long words, number of syllables, word types, and number of paragraphs, among others content features.", "startOffset": 128, "endOffset": 148}, {"referenceID": 3, "context": "Finally, we extract a set of features derived production rules based on context free grammars (CFG) trees using the Stanford Parser (Klein and Manning, 2003).", "startOffset": 132, "endOffset": 157}, {"referenceID": 4, "context": ", 2016) and e1071 packages (Meyer et al., 2015).", "startOffset": 27, "endOffset": 47}], "year": 2017, "abstractText": "The proliferation of misleading information in everyday access media outlets such as social media feeds, news blogs, and online newspapers have made it challenging to identify trustworthy news sources, thus increasing the need for computational tools able to provide insights into the reliability of online content. In this paper, we focus on the automatic identification of fake content in online news. Our contribution is twofold. First, we introduce two novel datasets for the task of fake news detection, covering seven different news domains. We describe the collection, annotation, and validation process in detail and present several exploratory analysis on the identification of linguistic differences in fake and legitimate news content. Second, we conduct a set of learning experiments to build accurate fake news detectors. In addition, we provide comparative analyses of the automatic and manual identification of fake news.", "creator": "LaTeX with hyperref package"}}}