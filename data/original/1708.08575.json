{"id": "1708.08575", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Identifying Subjective and Figurative Language in Online Dialogue", "abstract": "More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic resources such as news, highly social dialogue is very frequent in social media. We aim to automatically identify sarcastic and nasty utterances in unannotated online dialogue, extending a bootstrapping method previously applied to the classification of monologic subjective sentences in Riloff and Weibe 2003. We have adapted the method to fit the sarcastic and nasty dialogic domain. Our method is as follows: 1) Explore methods for identifying sarcastic and nasty cue words and phrases in dialogues; 2) Use the learned cues to train a sarcastic (nasty) Cue-Based Classifier; 3) Learn general syntactic extraction patterns from the sarcastic (nasty) utterances and define fine-tuned sarcastic patterns to create a Pattern-Based Classifier; 4) Combine both Cue-Based and fine-tuned Pattern-Based Classifiers to maximize precision at the expense of recall and test on unannotated utterances.", "histories": [["v1", "Tue, 29 Aug 2017 02:21:48 GMT  (61kb,D)", "http://arxiv.org/abs/1708.08575v1", "Pacific Northwest Natural Language Processing Workshop (NWNLP 2014)"]], "COMMENTS": "Pacific Northwest Natural Language Processing Workshop (NWNLP 2014)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stephanie m lukin", "luke eisenberg", "thomas corcoran", "marilyn a walker"], "accepted": false, "id": "1708.08575"}, "pdf": {"name": "1708.08575.pdf", "metadata": {"source": "CRF", "title": "Identifying Subjective and Figurative Language in Online Dialogue", "authors": ["Stephanie M. Lukin", "Luke Eisenberg", "Thomas Corcoran", "Marilyn A. Walker"], "emails": ["slukin@ucsc.edu", "leisenbe@ucsc.edu", "tcorcora@ucsc.edu", "mawalker@ucsc.edu"], "sections": [{"heading": null, "text": "Identifying Subjective and Figurative Language in Online Dialogue\nStephanie M. Lukin, Luke Eisenberg, Thomas Corcoran, & Marilyn A. Walker Natural Language and Dialogue Systems Computer Science Department, SOE-3\nUniversity of California, Santa Cruz slukin,leisenbe,tcorcora,mawalker@ucsc.edu\nMore and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic resources such as news, highly social dialogue is very frequent in social media, as illustrated in the snippets in Fig. 1 from the publicly available Internet Argument Corpus (IAC) (Walker et al., 2012). Utterances are frequently sarcastic, e.g., Really? Well, when I have a kid, I\u2019ll be sure to just leave it in the woods, since it can apparently care for itself (R2 in Fig. 1 as well as Q1 and R1), and are often nasty, (R2 in Fig. 1). Note also the frequent use of dialogue specific discourse cues, e.g. the use of No in R1, Really? Well in R2, and okay, well in Q3 in Fig. 1 (Fox Tree and Schrock, 1999; Bryant and Fox Tree, 2002; Fox Tree, 2010).\nQuote Q, Response R Sarc Nasty Q1: I jsut voted. sorry if some people actually have, you know, LIVES and don\u2019t sit around all day on debate forums to cater to some atheists posts that he thiks they should drop everything for. emoticon-rolleyes emoticonrolleyes emoticon-rolleyes As to the rest of your post, well, from your attitude I can tell you are not Christian in the least. Therefore I am content in knowing where people that spew garbage like this will end up in the End. R1: No, let me guess . . . er . . . McDonalds. No, Disneyland. Am I getting closer? 1 -3.6 Q2: The key issue is that once children are born they are not physically dependent on a particular individual. R2 Really? Well, when I have a kid, I\u2019ll be sure to just leave it in the woods, since it can apparently care for itself. 1 -1\nFigure 1: Sample Quote/Response Pairs from 4forums.com with Mechanical Turk annotations for Sarcasm and Nasty/Nice. Highly negative values of Nasty/Nice indicate strong nastiness and sarcasm is indicated by values near 1.\nWe aim to automatically identify sarcastic and nasty utterances in unannotated online dialogue, extending a bootstrapping method previously applied to the classification of monologic subjective sentences by Riloff & Wiebe, henceforth R&W (Riloff and Wiebe, 2003; Thelen and Riloff, 2002). We look at both sarcastic and nasty dialogic turns as a way to explore generalization of the method. R&W\u2019s method creates a High-Precision, CueBased Classifier to be a first approximation on unannotated text. They improve their classifier by learning and bootstrapping patterns (Fig. 2).\nWe found that this bootstrapping method \u2018as is\u2019 is not\nappropriate for our data because our Cue-Based Classifier yields a much lower precision than the bootstrapping requires. We have adapted the method to fit the sarcastic and nasty dialogic domain. Our method is as follows:\n1. Explore methods for identifying sarcastic and nasty cue words and phrases in dialogues;\n2. Use the learned cues to train a sarcastic (nasty) CueBased Classifier\n3. Learn general syntactic extraction patterns from the sarcastic (nasty) utterances and define fine-tuned sarcastic patterns to create a Pattern-Based Classifier;\n4. Combine both Cue-Based and fine-tuned PatternBased Classifiers to maximize precision at the expense of recall and test on unannotated utterances.\nCue Words. Sarcasm is known to be highly variable in form, and to depend, in some cases, on context for its interpretation (Sperber and Wilson, 1981; Gibbs, 2000; Bryant and Fox Tree, 2002). We elicit annotations from Mechanical Turk to identify sarcastic (nasty) cues in utterances from a development set. Turkers were presented with dialogic turns (a quote and its response) previously labeled sarcastic or nasty in the IAC by 7 different annotators, and were asked to identify sarcastic (nasty) or potentially sarcastic (nasty) phrases in the turn response. The Turkers then selected words or phrases from the response they believed could lead someone to believing the utterance was sarcastic or nasty. (Snow et al., 2008) measure the quality of Mechanical Turk annotations on common NLP tasks by comparing them to a gold standard. Pearson\u2019s correlation coefficient shows that very few Mechanical Turk annotators were required to beat the gold standard data, often less than 5. Because our sarcasm (nasty) task does not have gold standard data, we asked 100 annotators to participate in the pilot. For all unigrams, bigrams, and trigrams, interannotator agreement\nar X iv :1\n70 8.\n08 57\n5v 1\n[ cs\n.C L\n] 2\n9 A\nug 2\n01 7\nplateaued around 20 annotators and is about 90%agreement with 10 annotators, showing that the Mechanical Turk task is well formed and there is high agreement. We begin to form a sarcastic and nasty vocabulary from these cues.\nCue based classifier. We use a development set to measure \u201cgoodness\u201d of a cue that could serve as a high precision cue by using the percent sarcastic (nasty) and frequency statistics in the development set. These features rely on how frequent (FREQ) (subject to a \u03b81), and how reliable (%SARC and %NASTY) (subject to a \u03b82) a cue has to be to be useful. We select candidate cues by exhausting \u03b81 = [2, 4, 6, 8, 10] and \u03b82 = [.55, .60, .65, .70, .75, .80, .85, .90, .95, 1.00] for \u03b81 \u2264 FREQ and \u03b82 \u2264 SARC. At least two cues must be present and above the thresholds in an utterance to be classified by the Cue-Based Classifier. Less than two cues are needed to be classified as the counter-class. We select the best combination of parameters from our training set by selecting the parameters yielding the highest weighted f-measure that favors precision over recall. We then ran the Cue-Based Classifier with the best parameters on a test set. However as previously mentioned, R&W\u2019s method expects the CueBased Classifier to yield high precision, whereas our results (CUE rows in Table 1) are just barely above baseline.\nPattern Based Classifier. The next step in R&W\u2019s method is to create a Pattern-Based Classifier that takes as input the predicted labels from the Cue-Based Classifier. R&W\u2019s Pattern-Based Classifier is trained on general, syntactic templates known to exist for subjectivity. These patterns are not limited to exact surface matches as the Cue-Based Classifiers require. We reimplement these patterns, and further developed new patterns specifically fine-tuned towards sarcasm in dialogue. For example, our new pattern OH RB (oh adverb) matches utterances like \u201coh right\u201d and \u201coh sorry\u201d and the pattern NP WHphrase matches \u201csomeone who\u201d and \u201csomeone what\u201d. Patterns are extracted from another development set and we again compute FREQ and %SARC and %NASTY for each pattern subject to \u03b81 \u2264 FREQ and \u03b82 \u2264 %SARC or % NASTY. Classifications are made if at least two patterns are present and both are above the specified \u03b81 and \u03b82, again exhausting all combinations of \u03b81 and \u03b82. Also following R&W, we do not learn \u201cnot sarcastic\u201d or \u201cnice\u201d patterns. The counter-classes are predicted when the utterance contains less than two patterns. We test two Pattern-Based Classifiers: one with the original patterns proposed in R&W (BASELINE PATS) and one with the original patterns in addition to our new, fine-tuned patterns (NEW PATS). Table 1 shows the results of the pa-\nrameters with the highest weighted f-measure. The Pattern-Based Classifier performs better on Nasty than Sarcasm. We conclude that R&W\u2019s patterns alone generalize well on our Sarcasm and Nasty datasets. By adding the fine-tuned patterns in the NEW PATS Classifier, we see a drastic increase in Sarcasm precision. There seems to be little change in recall for Sarcasm. Furthermore, we see a huge increase in precision for Nasty, but a steep decline in recall with the new patterns. We believe this is because these new patterns are tailored towards sarcastic utterances, not nasty. We did not create our own fine-tuned nasty patterns because we do well with R&W\u2019s general patterns.\nCombined Classifier. To attempt to create a HighPrecision Classifier, we combine the Cue-Based Classifier and the Pattern-Based Classifier. We classify a post as sarcastic if it meets either the criteria of the Cue-Based Classifier (e.g. with \u03b81 = 2, \u03b82 = .55 for Sarcasm) or the Pattern-Based Classifier (e.g. with \u03b81 = 2, \u03b82 = .65 for Sarcasm). We use the same test set with which we test the Cue-Based Classifier and compare the results (Table 2). We furthermore distinguish between a Combined Classifier that makes a classification if both schemata are true (AND), or if only one is true (OR).\nOR does better than only the Cue-Based Classifier for all precision, recall, and f-measure. AND does better for precision by far than the Cue-Based Classifier, but with a lower recall. Despite the very low recall for AND, the f-measure of AND and OR is identical. AND is a more selective classifier, only saying \u201cyes\u201d if both schemata are true. This will naturally yield lower recall, but grant higher confidence in those classified.\nWe believe our Combined AND Classifier now has a high enough precision to be compared with R&W\u2019s first approximation High-Precision, Cue-Based Classifier. After running the Combined Classifier on unannotated data, we select 100 predicted sarcastic and 100 predicted not sarcastic utterances and ask human annotators to label them. We expect a high overlap between annotators and the Combined Classifier, which would indicate that human annotators agree with the labels we are automatically predicting. These results are currently in progress.\nDespite the fact that we could not create a first approximation High-Precision, Cue-Based classifier like R&W, we have succeeded in creating a High-Precision Combined Classifier using both cues and fine-tuned patterns (71% precision for sarcasm and 88% precision for nastiness). Future work will involve developing fine-tuned patterns for nastiness and exploring different patterns for sarcasm."}], "references": [{"title": "Irony in talk among friends", "author": ["R.W. Gibbs"], "venue": "Metaphor and Symbol,", "citeRegEx": "Gibbs.,? \\Q2000\\E", "shortCiteRegEx": "Gibbs.", "year": 2000}, {"title": "Learning extraction patterns for subjective expressions", "author": ["Riloff", "Wiebe2003] E. Riloff", "J. Wiebe"], "venue": "In Proceedings of the 2003 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Riloff et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Riloff et al\\.", "year": 2003}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["Snow et al.2008] R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Irony and the use-mention distinction", "author": ["Sperber", "Wilson1981] Dan Sperber", "Deidre Wilson"], "venue": "Radical Pragmatics,", "citeRegEx": "Sperber et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Sperber et al\\.", "year": 1981}, {"title": "A bootstrapping method for learning semantic lexicons using extraction pattern contexts", "author": ["Thelen", "Riloff2002] M. Thelen", "E. Riloff"], "venue": "In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume", "citeRegEx": "Thelen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Thelen et al\\.", "year": 2002}, {"title": "A corpus for research on deliberation and debate", "author": ["Pranav Anand", "Robert Abbott", "Jean E. Fox Tree"], "venue": "In Language Resources and Evaluation Conference,", "citeRegEx": "Walker et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 5, "context": "1 from the publicly available Internet Argument Corpus (IAC) (Walker et al., 2012).", "startOffset": 61, "endOffset": 82}, {"referenceID": 0, "context": "Sarcasm is known to be highly variable in form, and to depend, in some cases, on context for its interpretation (Sperber and Wilson, 1981; Gibbs, 2000; Bryant and Fox Tree, 2002).", "startOffset": 112, "endOffset": 178}, {"referenceID": 2, "context": "(Snow et al., 2008) measure the quality of Mechanical Turk annotations on common NLP tasks by comparing them to a gold standard.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic resources such as news, highly social dialogue is very frequent in social media, as illustrated in the snippets in Fig. 1 from the publicly available Internet Argument Corpus (IAC) (Walker et al., 2012). Utterances are frequently sarcastic, e.g., Really? Well, when I have a kid, I\u2019ll be sure to just leave it in the woods, since it can apparently care for itself (R2 in Fig. 1 as well as Q1 and R1), and are often nasty, (R2 in Fig. 1). Note also the frequent use of dialogue specific discourse cues, e.g. the use of No in R1, Really? Well in R2, and okay, well in Q3 in Fig. 1 (Fox Tree and Schrock, 1999; Bryant and Fox Tree, 2002; Fox Tree, 2010).", "creator": "LaTeX with hyperref package"}}}