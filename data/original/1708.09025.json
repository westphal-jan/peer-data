{"id": "1708.09025", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling", "abstract": "In this paper, we present hierarchical relationbased latent Dirichlet allocation (hrLDA), a data-driven hierarchical topic model for extracting terminological ontologies from a large number of heterogeneous documents. In contrast to traditional topic models, hrLDA relies on noun phrases instead of unigrams, considers syntax and document structures, and enriches topic hierarchies with topic relations. Through a series of experiments, we demonstrate the superiority of hrLDA over existing topic models, especially for building hierarchies. Furthermore, we illustrate the robustness of hrLDA in the settings of noisy data sets, which are likely to occur in many practical scenarios. Our ontology evaluation results show that ontologies extracted from hrLDA are very competitive with the ontologies created by domain experts.", "histories": [["v1", "Tue, 29 Aug 2017 21:04:11 GMT  (1614kb,D)", "http://arxiv.org/abs/1708.09025v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["xiaofeng zhu", "diego klabjan", "patrick bless"], "accepted": false, "id": "1708.09025"}, "pdf": {"name": "1708.09025.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling", "authors": ["Xiaofeng Zhu", "Diego Klabjan", "Patrick N Bless"], "emails": ["xiaofengzhu2013@u.northwestern.edu", "d-klabjan@northwestern.edu", "patrick.n.bless@intel.com"], "sections": [{"heading": null, "text": "Keywords-terminological ontology; ontology learning; hierarchical topic modeling; knowledge acquisition\nI. INTRODUCTION\nAlthough researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases. This raises challenges for learning ontologies in new domains. While a strong ontology parser is effective in small-scale corpora, an unsupervised model is beneficial for learning new entities and their relations from new data sources, and is likely to perform better on larger corpora.\nIn this paper, we focus on unsupervised terminological ontology learning and formalize a terminological ontology as a hierarchical structure of subject-verb-object triplets. We divide a terminological ontology into two components: topic hierarchies and topic relations. Topics are presented in a tree structure where each node is a topic label (noun phrase), the root node represents the most general topic, the leaf nodes represent the most specific topics, and every topic is composed of its topic label and its descendant topic labels. Topic hierarchies are preserved in topic paths, and a topic path connects a list of topics labels from the\nroot to a leaf. Topic relations are semantic relationships between any two topics or properties used to describe one topic. Figure 1 depicts an example of a terminological ontology learned from a corpus about European cities. We extract terminological ontologies by applying unsupervised hierarchical topic modeling and relation extraction to plain text.\nTopic modeling was originally used for topic extraction and document clustering. The classical topic model, latent Dirichlet allocation (LDA) [12], simplifies a document as a bag of its words and describes a topic as a distribution of words. Prior research [13], [14], [15], [16], [17], [18], [19] has shown that LDA-based approaches are adequate for (terminological) ontology learning. However, these models are deficient in that they still need human supervision to decide the number of topics, and to pick meaningful topic labels usually from a list of unigrams. Among models not using unigrams, LDA-based Global Similarity Hierarchy Learning (LDA+GSHL) [14] only extracts a subset of relations: \u201cbroader\u201d and \u201crelated\u201d relations. In addition, the topic hierarchies of KB-LDA [18] rely on hypernym-hyponym\nar X\niv :1\n70 8.\n09 02\n5v 1\n[ cs\n.C L\n] 2\n9 A\nug 2\npairs capturing only a subset of hierarchies. Considering the shortcomings of the existing methods, the main objectives of applying topic modeling to ontology learning are threefold.\n1) In topic models, a topic is usually represented with a list of unigrams. In a terminological ontology, a topic/entity needs to be represented with a more descriptive identifier (i.e., noun phrase). Currently, the number of topics is usually a fixed parameter, which restricts the number of classes an ontology could have. For instance, it is difficult to add a new species to an animal ontology. 2) Both relations among different noun phrases and relations/properties (see the relations in Figure 1) for describing single noun phrases should be captured during the topic generation process. 3) Hierarchies need to be built on topical affiliations. If topic B is a sub-topic of topic A, B has a more specific meaning than A. The depth of each topic path should be determined by a data-driven method.\nTo achieve the first objective, we extract noun phrases and then propose a sampling method to estimate the number of topics. For the second objective, we use language parsing and relation extraction to learn relations for the noun phrases. Regarding the third objective, we adapt and improve the hierarchical latent Dirichlet allocation (hLDA) model [20], [21]. hLDA is not ideal for ontology learning because it builds topics from unigrams (which are not descriptive enough to serve as entities in ontologies) and the topics may contain words from multiple domains when input data have documents from many domains (see Section II and Figure 9). Our model, hrLDA, overcomes these deficiencies. In particular, hrLDA represents topics with noun phrases, uses syntax and document structures such as paragraph indentations and item lists, assigns multiple topic paths for every document, and allows topic trees to grow vertically and horizontally.\nThe primary contributions of this work can be specified as follows. \u2022 We develop a hierarchical topic model, hrLDA, that\ndoes not require one to set the topic number at every level of a topic tree or to set the topic path lengths from the root to leaves. \u2022 We integrate relation extraction into topic modeling leading to lower perplexity. \u2022 We propose a multiple topic path drawing strategy, which is an improvement over the simple topic path drawing method proposed in hLDA. \u2022 We present automatic extraction of terminological ontologies via hrLDA.\nThe rest of this paper is organized into five parts. In Section 2, we provide a brief background of hLDA. In Section 3, we present our hrLDA model and the ontology\ngeneration method. In Section 4, we demonstrate empirical results regarding topic hierarchies and generated terminological ontologies. Finally, in Section 5, we present some concluding remarks and discuss avenues for future work and improvements."}, {"heading": "II. BACKGROUND", "text": "In this section, we introduce our main baseline model, hierarchical latent Dirichlet allocation (hLDA), and some of its extensions. We start from the components of hLDA - latent Dirichlet allocation (LDA) and the Chinese Restaurant Process (CRP)- and then explain why hLDA needs improvements in both building hierarchies and drawing topic paths.\nLDA is a three-level Bayesian model in which each document is a composite of multiple topics, and every topic is a distribution over words. Due to the lack of determinative information, LDA is unable to distinguish different instances containing the same content words, (e.g. \u201cI trimmed my polished nails\u201d and \u201cI have just hammered many rusty nails\u201d). In addition, in LDA all words are probabilistically independent and equally important. This is problematic because different words and sentence elements should have different contributions to topic generation. For instance, articles contribute little compared to nouns, and sentence subjects normally contain the main topics of a document.\nIntroduced in hLDA, CRP partitions words into several topics by mimicking a process in which customers sit down in a Chinese restaurant with an infinite number of tables and an infinite number of seats per table. Customers enter one by one, with a new customer choosing to sit at an occupied table or a new table. The probability of a new customer sitting at the table with the largest number of customers is the highest. In reality, customers do not always join the largest table but prefer to dine with their acquaintances. The theory of distance-dependent CRP was formerly proposed by David Blei [22]. We provide later in Section III-C an explicit formula for topic partition given that adjacent words and sentences tend to deal with the same topics.\nhLDA combines LDA with CRP by setting one topic path with fixed depth L for each document. The hierarchical relationships among nodes in the same path depend on an L dimensional Dirichlet distribution that actually arranges the probabilities of topics being on different topic levels. Despite the fact that the single path was changed to multiple paths in some extensions of hLDA - the nested Chinese restaurant franchise processes [23] and the nested hierarchical Dirichlet Processes [24], - this topic path drawing strategy puts words from different domains into one topic when input data are mixed with topics from multiple domains. This means that if a corpus contains documents in four different domains, hLDA is likely to include words from the four domains in every topic (see Figure 9). In light of the various inadequacies discussed above, we propose a relation-based model, hrLDA. hrLDA incorporates semantic topic modeling with relation\nextraction to integrate syntax and has the capacity to provide comprehensive hierarchies even in corpora containing mixed topics."}, {"heading": "III. HIERARCHICAL RELATION-BASED LATENT DIRICHLET ALLOCATION", "text": "The main problem we address in this section is generating terminological ontologies in an unsupervised fashion. The fundamental concept of hrLDA is as follows. When people construct a document, they start with selecting several topics. Then, they choose some noun phrases as subjects for each topic. Next, for each subject they come up with relation triplets to describe this subject or its relationships with other subjects. Finally, they connect the subject phrases and relation triplets to sentences via reasonable grammar. The main topic is normally described with the most important relation triplets. Sentences in one paragraph, especially adjacent sentences, are likely to express the same topic.\nWe begin by describing the process of reconstructing LDA. Subsequently, we explain relation extraction from heterogeneous documents. Next, we propose an improved topic partition method over CRP. Finally, we demonstrate how to build topic hierarchies that bind with extracted relation triplets."}, {"heading": "A. Relation-based Latent Dirichlet Allocation", "text": "Documents are typically composed of chunks of texts, which may be referred to as sections in Word documents, paragraphs in PDF documents, slides in presentation documents, etc. Each chunk is composed of multiple sentences that are either atomic or complex in structure, which means a document is also a collection of atomic and/or complex sentences. An atomic sentence (see module T in Figure 2) is a sentence that contains only one subject (S), one object (O) and one verb (V ) between the subject and the object. For every atomic sentence whose object is also a noun phrase, there are at least two relation triplets (e.g., \u201cThe tiger that gave the excellent speech is handsome\u201d has relation triplets: (tiger, give, speech), (speech, be given by, tiger), and (tiger, be, handsome)). By contrast, a complex sentence can be subdivided into multiple atomic sentences. Given that the syntactic verb in a relation triplet is determined by the subject and the object, a document d in a corpus D can be ultimately reduced to Nd subject phrases (we convert objects to subjects using passive voice) associated with Nd relation triplets Td. Number Nd is usually larger than the actual number of noun phrases in document d. By replacing the unigrams in LDA with relation triplets, we retain definitive information and assign salient noun phrases high weights.\nWe define Dir(\u03b1) as a Dirichlet distribution parameterized by hyperparameters \u03b1, Multi(\u03b8) as a multinomial distribution parameterized by hyperparameters \u03b8, Dir(\u03b7) as a Dirichlet distribution parameterized by \u03b7, and Multi(\u03b2)\nas a multinomial distribution parameterized by \u03b2. We assume the corpus has K topics. Assigning K topics to the Nd relation triplets of document d follows a multinomial distribution Multi(\u03b8) with prior Dir(\u03b1). Selecting the Nd relation triplets for document d given the K topics follows a multinomial distribution Multi(\u03b2) with prior Dir(\u03b7). We denote T = {Td}d\u2208D as the list of relation triplet lists extracted from all documents in the corpus, and Z as the list of topic assignments of T . We denote the relation triplet counts of documents in the corpus by N = {Nd}d\u2208D. The graphical representation of the relation-based latent Dirichlet allocation (rLDA) model is illustrated in Figure 2.\nThe plate notation can be decomposed into two types of Dirichlet-multinomial conjugated structures: document-topic distribution Dir(\u03b1) \u2192 Multi(\u03b8) \u2192 Z and topic-relation distribution Dir(\u03b7) \u2192 Multi(\u03b2) \u2192 T |Z. Hence, the joint distribution of T and Z can be represented as\nP (T,Z|\u03b1, \u03b7) = P (T |Z, \u03b7) P (Z|\u03b1)\n= K\u220f k=1 Dir(Ck + \u03b7) Dir(\u03b7) D\u220f d Dir(Bd + \u03b1) Dir(\u03b1)\nCk = (C 1 k , C 2 k , ..., C w k , ..., C W k ) Bd = (B 1 d, B 2 d, ..., B k d , ..., B K d ),\n(1)\nwhere W is the number of unique relations in all documents, Cwk is the number of occurrences of the relation triplet w generated by topic k in all documents, and Bkd is the number of relation triplets generated by topic k in document d. Dir(\u03b1) is a conjugate prior for Multi(\u03b8) and thus the posterior distribution is a new Dirichlet distribution parameterized by (Bd + \u03b1). The same rule applies to Dir(Ck + \u03b7)."}, {"heading": "B. Relation Triplet Extraction", "text": "Extracting relation triplets is the essential step of hrLDA, and it is also the key process for converting a hierarchical topic tree to an ontology structure. The idea is to find all syntactically related noun phrases and their connections\nusing a language parser such as the Stanford NLP parser [25] and Ollie [26]. Generally, there are two types of relation triplets: \u2022 Subject-predicate-object-based relations,\ne.g., New York is the largest city in the United States \u21d2 (New York, be the largest city in, the United States);\n\u2022 Noun-based/hidden relations, e.g., Queen Elizabeth \u21d2 (Elizabeth, be, queen).\nA special type of relation triplets can be extracted from presentation documents such as those written in PowerPoint using document structures. Normally lines in a slide are not complete sentences, which means language parsing does not work. However, indentations and bullet types usually express inclusion relationships between adjacent lines. Starting with the first line in an itemized section, our algorithm scans the content in a slide line by line, and creates relations based on the current item and the item that is one level higher."}, {"heading": "C. Acquaintance Chinese Restaurant Process", "text": "As mentioned in Section 2, CRP always assigns the highest probability to the largest table, which assumes customers are more likely to sit at the table that has the largest number of customers. This ignores the social reality that a person is more willing to choose the table where his/her closest friend is sitting even though the table also seats unknown people who are actually friends of friends. Similarly with human-written documents, adjacent sentences usually describe the same topics. We consider a restaurant table as a topic, and a person sitting at any of the tables as a noun phrase. In order to penalize the largest topic and assign high probabilities to adjacent noun phrases being in the same topics, we introduce an improved partition method, Acquaintance Chinese Restaurant Process (ACRP).\nThe ultimate purposes of ACRP are to estimate K, the number of topics for rLDA, and to set the initial topic distribution states for rLDA. Suppose a document is read from top to bottom and left to right. As each noun phrase belongs to one sentence and one text chunk (e.g., section, paragraph and slide), the locations of all noun phrases in a document can be mapped to a two-dimensional space where sentence location is the x axis and text chunk location is the y axis (the first noun phrase of a document holds value (0, 0)). More specifically, every noun phrase has four attributes: content, location, one-to-many relation triplets, and document ID. Noun phrases in the same text chunk are more likely to be \u201cacquaintances;\u201d they are even closer to each other if they are in the same sentence. In contrast to CRP, ACRP assigns probabilities based on closeness, which is specified in the following procedure.\n1) Let zn be the integer-valued random variable corresponding to the index of a topic assigned to the nth\nphrase. Draw a probability P (zn+1) from Equations 2\nto 5 below for the (n+1)th noun phrase tn+1, joining each of the existing k topics and the new (k + 1)th topic given the topic assignments of previous n noun phrases, Z1:n. If a noun phrase joins any of the existing k topics, we denote the corresponding topic index by i \u2208 [1, k]. \u2022 The probability of choosing the (k + 1)th topic:\nP (zn+1 = (k + 1)|Z1:n) = \u03b3\nn+ \u03b3 . (2)\n\u2022 The probability of selecting any of the k topics: \u2013 if the content of tn+1 is synonymous with or an\nacronym of a previously analyzed noun phrase tm (m < n+ 1) in the ith topic,\nP (zn+1 = i|Z1:n) = 1\u2212 \u03b3; (3)\n\u2013 else if the document ID of tn+1 is different from all document IDs belonging to the ith\ntopic, P (zn+1 = i|Z1:n) = \u03b3; (4)\n\u2013 otherwise,\nP (zn+1 = i|Z1:n) = Ci \u2212 (1\u2212 1min(Q1:i) ) (1 +min(S1:i))n+ \u03b3 , (5)\nwhere Ci refers to the current number of noun phrases in the ith topic, Q1:i represents the vector of chunk location differences of the (n+ 1)th noun phrase and all members in the ith topic, S1:i stands for the vector of sentence location differences, and \u03b3 is a penalty factor.\nNormalize the (k + 1) probabilities to guarantee they are each in the range of [0, 1] and their sum is equal to 1. 2) Based on the probabilities 2 to 5, we sample a topic index z from {1, ..., (k + 1)} for every noun phrase, and we count the number of unique topics K in the end. We shuffle the order of documents and iterate ACRP until K is unchanged."}, {"heading": "D. Nested Acquaintance Chinese Restaurant Process", "text": "The procedure for extending ACRP to hierarchies is essential to why hrLDA outperforms hLDA. Instead of a predefined tree depth L, the tree depth for hrLDA is optional and data-driven. More importantly, clustering decisions are made given a global distribution of all current nonpartitioned phrases (leaves) in our algorithm. This means there can be multiple paths traversed down a topic tree for each document. With reference to the topic tree, every node has a noun phrase as its label and represents a topic that may have multiple sub-topics. The root node is visited by all phrases. In practice, we do not link any phrases to the root node, as it contains the entire vocabulary. An inner node\nof a topic tree contains a selected topic label. A leaf node contains an unprocessed noun phrase. We define a hashmap leaves with a document ID as the key and the current leaf nodes of the document as the value. We denote the current tree level by l. We next outline the overall algorithm.\n1) We start with the root node (l = 0) and apply rLDA to all the documents in a corpus.\na) Collect the current leaf nodes of every document. leaves initially contains all noun phrases in the corpus. Assign a cluster partition to the leaf nodes in each document based on ACRP and sample the cluster partition until the number of topics of all noun phrases in leaves is stable or the iteration reaches the predefined number of iteration times (whichever occurs first). b) Mark the number of topics (child nodes) of parent node m at level l as Klm . Build a Klm - dimensional topic proportion vector \u03b8 based on Dir(\u03b1). c) For every noun phrase {tn}Ndn=1 in document d, form the topic assignments Z{1,...,Klm} based on Multi(\u03b8). d) Generate relation triplets from Multi(\u03b2) given Dir(\u03b7) and the associated topic vector {Zk}K lm\nk=1 . e) Eliminate partitioned leaf nodes from leaves.\nUpdate the current level l by 1. 2) If phrases in leaves are not yet completely partitioned\nto the next level and l is less than L, continue the following steps. For each leaf node, we set the top phrase (i.e., the phrase having the highest probability) as the topic label of this leaf node and the leaf node becomes an inner node. We next update leaves and repeat procedures 1(a)\u2212 1(e).\nTo summarize this process more succinctly: we build the topic hierarchies with rLDA in a divisive way (see Figure 3). We start with the collection of extracted noun phrases and split them using rLDA and ACRP. Then, we apply the procedure recursively until each noun phrase is selected as a topic label. After every rLDA assignment, each inner node only contains the topic label (top phrase), and the rest of the phrases are divided into nodes at the next level using ACRP and rLDA. Hence, we build a topic tree with each node as a topic label (noun phrase), and each topic is composed of its topic labels and the topic labels of the topic\u2019s descendants. In the end, we finalize our terminological ontology by linking the extracted relation triplets with the topic labels as subjects.\nWe use collapsed Gibbs sampling [27] for inference from posterior distribution P (Z|T, \u03b1, \u03b7) based on Equation 1. Assume the nth noun phrase tn = t\u0302 in parent node m comes from document d. We denote unassigned noun phrases from document d in parent node m by dm, and unique noun\nphrases in parent node m by T\u0302m. We simplify the probability of assigning the nth noun phrase in parent node m to topic k among Klm topics as\nP (zn = k|Z\u00acn, T\u0302m, \u03b1, \u03b7) \u221d P (tn = t\u0302, zn = k|Z\u00acn, T\u0302m\u00acn, \u03b1, \u03b7)\n= \u222b P (tn = t\u0302, zn = k|Z\u00acn, T\u0302m\u00acn, \u03b8dm , \u03b2k)d\u03b8dm , d\u03b2k\n= Ck,t\u0302\u00acn + \u03b7\u2211T\u0302m\nt\u0302 (Ck,t\u0302\u00acn + \u03b7)\nCdm,k\u00acn + \u03b1\u2211Klm k=1 (Cdm,k\u00acn + \u03b1)\n(6)\nwhere Z\u00acn refers to all topic assignments other than zn, \u03b8dm is multinational document-topic distribution for unassigned noun phrases dm, \u03b2k is the multinational topic-relation distribution for topic k, Ck,t\u0302\u00acn is the number of occurrences of noun phrase t\u0302 in topic k except the nth noun phrase in m, Cdm,k\u00acn stands for the number of times that topic k occurs in dm excluding the nth noun phrase in m. The time complexity of hrLDA is O( \u2211L l=1N\n2Kl), where Kl is the number of topics at level l. The space complexity is O(N).\nIn order to build a hierarchical topic tree of a specific domain, we must generate a subset of the relation triplets using external constraints or semantic seeds via a pruning process [28]. As mentioned above, in a relation triplet, each relation connects one subject and one object. By assembling all subject and object pairs, we can build an undirected graph with the objects and the subjects constituting the nodes of the graph [29]. Given one or multiple semantic seeds as input, we first collect a set of nodes that are connected to the seed(s), and then take the relations from the set of nodes as input to retrieve associated subject and object pairs. This process constitutes one recursive step. The subject and object pairs become the input of the subsequent recursive step."}, {"heading": "IV. EMPIRICAL RESULTS", "text": "A. Implementation\nWe utilized the Apache poi library to parse texts from pdfs, word documents and presentation files; the MALLET toolbox [30] for the implementations of LDA, optimized LDA [31] and hLDA; the Apache Jena library to add relations, properties and members to hierarchical topic trees; and Stanford Protege1 for illustrating extracted ontologies. We make our code and data available 2. We used the same empirical hyper-parameter setting (i.e., \u03b1 = 1, \u03b7 = 0.1, and \u03b3 = 0.01) across all our experiments. We then demonstrate the evaluation results from two aspects: topic hierarchy and ontology rule."}, {"heading": "B. Hierarchy Evaluation", "text": "In this section, we present the evaluation results of hrLDA tested against optimized LDA, hLDA, and phrase hLDA (i.e., hLDA based on noun phrases) as well as ontology examples that hrLDA extracted from real-world text data. The entire corpus we generated contains 349,362 tokens (after removing stop words and cleaning) and is built from articles on semiconductor packaging. It includes 84 presentation files, articles from 1,782 Wikipedia pages and 3,000 research papers that were published in IEEE manufacturing conference proceedings within the last decade. In order to see the performance in data sets of different scales, we also used a smaller corpus Wiki that holds the articles collected from the Wikipedia pages only.\nWe extract a single level topic tree using each of the four models; hrLDA becomes rLDA, and phrase hLDA becomes phrase-based LDA. We have tested the average perplexity and running time performance of ten independent runs on each of the four models [32], [33]. Equation 7 defines the perplexity, which we employed as an empirical measure.\nln(perplexity) = \u2212 \u2211D\nd log(P (Td|Zd)P (Zd|d))\u2211D d Nd , (7)\nwhere Td is a vector containing the Nd relation triplets in document d, and Zd is the topic assignment for Td.\nThe comparison results on our Wiki corpus are shown in Figure 4. hrLDA yields the lowest perplexity and reasonable running time. As the running time spent on parameter optimization is extremely long (the optimized LDA requires 19.90 hours to complete one run), for efficiency, we adhere to the fixed parameter settings for hrLDA. Superiority\nFigures 5 to 7 illustrates the perplexity trends of the three hierarchical topic models (i.e., hrLDA, phrase hLDA and hLDA) applied to both the Wiki corpus and the entire corpus with seed \u201cchip\u201d given different level settings. From left to right, hrLDA retains the lowest perplexities compared\n1http://protege.stanford.edu/ 2https://github.com/XiaofengZhu/hrLDA\nwith other models as the corpus size grows. Furthermore, from top to bottom, hrLDA remains stable as the topic level increases, whereas the perplexity of phrase hLDA and especially the perplexity of hLDA become rapidly high. Figure 8 highlights the perplexity values of the three models with confidence intervals in the final state. As shown in the two types of experiments, hrLDA has the lowest average perplexities and smallest confidence intervals, followed by phrase hLDA, and then hLDA.\nOur interpretation is that hLDA and phrase hLDA tend to assign terms to the largest topic and thus do not guarantee that each topic path contains terms with similar meaning. Robustness\nFigure 9 shows exhaustive hierarchical topic trees extracted from a small text sample with topics from four domains: semiconductor, integrated circuit, Berlin, and London. hLDA tends to mix words from different domains into one topic. For instance, words on the first level of the topic tree come from all four domains. This is because the topic path drawing method in existing hLDA-based models takes words in the most important topic of every document and labels them as the main topic of the corpus. In contrast, hrLDA is able to create four big branches for the four domains from the root. Hence, it generates clean topic hierarchies from the corpus."}, {"heading": "C. Gold Standard-based Ontology Evaluation", "text": "The visualization of one concrete ontology on the semiconductor packaging domain is presented in Figure 10. For instance, Topic packaging contains topic integrated circuit packaging, and topic label jedec is associated with relation triplet (jedec, be short for, joint electron device engineering council).\nWe use KB-LDA, phrase hLDA, and LDA+GSHL as our baseline methods, and compare ontologies extracted from hrLDA, KB-LDA, phrase hLDA, and LDA+GSHL with DBpedia ontologies. We use precision, recall and F-measure\nfor this ontology evaluation. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia. A false positive case is an incorrectly identified ontology rule. A false negative case is a missed ontology rule. Table I shows the evaluation results of ontologies extracted from Wikipedia articles pertaining to European Capital Cities (Corpus E), Office Buildings in Chicago (Corpus O) and Birds of the United States (Corpus B) using hrLDA, KB-LDA, phrase hLDA (tree depth L = 3), and LDA+GSHL in contrast to these gold ontologies belonging to DBpedia. The three corpora used in this evaluation were collected from Wikipedia abstracts, the same text source of DBpedia. The seeds of hrLDA and the root concepts of LDA+GSHL are capital, building, and bird. For both KB-LDA and phrase hLDA we kept the top five tokens in each topic as each node of their topic trees is a distribution/list of phrases. hrLDA achieves the highest precision and F-measure scores in the three experiments compared to the other models. KB-LDA performs better than\nphrase hLDA and LDA+GSHL, and phrase hLDA performs similarly to LDA+GSHL. In general, hrLDA works well especially when the pre-knowledge already exists inside the corpora. Consider the following two statements taken from the corpus on Birds of the United States as an example. In order to use two short documents \u201cThe Acadian flycatcher is a small insect-eating bird.\u201d and \u201cThe Pacific loon is a medium-sized member of the loon.\u201d to infer that the Acadian flycatcher and the Pacific loon are both related to topic bird, the pre-knowledge that \u201cthe loon is a species of bird\u201d is required for hrLDA. This example explains why the accuracy of extracting ontologies from this kind of corpus is low."}, {"heading": "V. CONCLUDING REMARKS", "text": "In this paper, we have proposed a completely unsupervised model, hrLDA, for terminological ontology learning. hrLDA is a domain-independent and self-learning model, which means it is very promising for learning ontologies in new domains and thus can save significant time and effort in ontology acquisition.\nWe have compared hrLDA with popular topic models to interpret how our algorithm learns meaningful hierarchies. By taking syntax and document structures into consideration, hrLDA is able to extract more descriptive topics. In addition, hrLDA eliminates the restrictions on the fixed topic tree depth and the limited number of topic paths. Furthermore, ACRP allows hrLDA to create more reasonable topics and to converge faster in Gibbs sampling.\nWe have also compared hrLDA to several unsupervised ontology learning models and shown that hrLDA can learn applicable terminological ontologies from real world data. Although hrLDA cannot be applied directly in formal reasoning, it is efficient for building knowledge bases for information retrieval and simple question answering. Also, hrLDA is sensitive to the quality of extracted relation triplets. In order to give optimal answers, hrLDA should be embedded in more complex probabilistic modules to identify true facts from extracted ontology rules. Finally, one issue we have not addressed in our current study is\ncapturing pre-knowledge. Although a direct solution would be adding the missing information to the data set, a more advanced approach would be to train topic embeddings to extract hidden semantics."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by Intel Corporation, Semiconductor Research Corporation (SRC). We are obliged to Professor Goce Trajcevski from Northwestern University for his insightful suggestions and discussions. This work was partly conducted using the Protege resource, which is supported by grant GM10331601 from the National Institute of General Medical Sciences of the United States National Institutes of Health."}], "references": [{"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM, vol. 38, no. 11, pp. 39\u201341, 1995.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S. Auer", "C. Bizer", "G. Kobilarov", "J. Lehmann", "R. Cyganiak", "Z. Ives"], "venue": "Proceedings of the 6th International Semantic Web Conference. Springer, 2007, pp. 722\u2013735.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Yago: A core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Proceedings of the 16th international conference on World Wide Web, 2007, pp. 697\u2013 706.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["K. Bollacker", "C. Evans", "P. Paritosh", "T. Sturge", "J. Taylor"], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data, 2008, pp. 1247\u20131250.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Toward an architecture for neverending language learning", "author": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R. Hruschka", "T.M. Mitchell"], "venue": "AAAI, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Deepdive: Webscale knowledge-base construction using statistical learning and inference", "author": ["F. Niu", "C. Zhang", "C. R\u00e9", "J.W. Shavlik"], "venue": "VLDS, vol. 12, pp. 25\u201328, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Domain cartridge: Unsupervised framework for shallow domain ontology construction from corpus", "author": ["S. Mukherjee", "J. Ajmera", "S. Joshi"], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 2014, pp. 929\u2013938.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["X.L. Dong", "E. Gabrilovich", "G. Heitz", "W. Horn", "N. Lao", "K. Murphy", "T. Strohmann", "S. Sun", "W. Zhang"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 601\u2013610.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Largescale knowledge base completion: Inferring via grounding network sampling over selected instances", "author": ["Z. Wei", "J. Zhao", "K. Liu", "Z. Qi", "Z. Sun", "G. Tian"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, ser. CIKM \u201915. ACM, 2015, pp. 1331\u20131340.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Inducing space dirichlet process mixture largemargin entity relationshipinference in knowledge bases", "author": ["S.P. Chatzis"], "venue": "Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. ACM, 2015, pp. 1311\u20131320.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Neighborhood mixture model for knowledge base completion", "author": ["D.Q. Nguyen", "K. Sirts", "L. Qu", "M. Johnson"], "venue": "arXiv preprint arXiv:1606.06461, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "JOURNAL of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Data-driven approach for ontology learning", "author": ["I. Ocampo-Guzman", "I. Lopez-Arevalo", "V. Sosa-Sosa"], "venue": "Electrical Engineering, Computing Science and Automatic Control, CCE, 2009 6th International Conference on. IEEE, 2009, pp. 1\u20136.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic topic models for learning terminological ontologies", "author": ["W. Wei", "P. Barnaghi", "A. Bargiela"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 7, pp. 1028\u20131040, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "An ontology term extracting method based on latent dirichlet allocation", "author": ["Y. Jing", "W. Junli", "Z. Xiaodong"], "venue": "Multimedia Information Networking and Security (MINES), 2012 Fourth International Conference on. IEEE, 2012, pp. 366\u2013369.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Tree labeled lda: A hierarchical model for web summaries", "author": ["A. Slutsky", "X. Hu", "Y. An"], "venue": "Big Data, 2013 IEEE International Conference on. IEEE, 2013, pp. 134\u2013140.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Terminological ontology learning and population using latent dirichlet allocation", "author": ["F. Colace", "M. De Santo", "L. Greco", "F. Amato", "V. Moscato", "A. Picariello"], "venue": "Journal of Visual Languages & Computing, vol. 25, no. 6, pp. 818\u2013826, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Kb-lda: Jointly learning a knowledge base of hierarchy, relations, and facts", "author": ["D. Movshovitz-Attias", "W.W. Cohen"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 2015, pp. 1449\u20131459.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounding topic models with knowledge bases", "author": ["Z. Hu", "G. Luo", "M. Sachan", "E. Xing", "Z. Nie"], "venue": "Proceedings of the 24th International Joint Conference on Artificial Intelligence, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical topic models and the nested chinese restaurant process", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan", "J.B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems 16. MIT Press, 2004, p. 17.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["D.M. Blei", "T.L. Griffiths", "M.I. Jordan"], "venue": "Journal of the ACM (JACM), vol. 57, no. 2, p. 7, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Distance dependent chinese restaurant processes", "author": ["D.M. Blei", "P.I. Frazier"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2461\u20132488, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Nested chinese restaurant franchise process: Applications to user tracking and document modeling", "author": ["A. Ahmed", "L. Hong", "A.J. Smola"], "venue": "ICML (3), 2013, pp. 1426\u20131434.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Nested hierarchical dirichlet processes", "author": ["J. Paisley", "C. Wang", "D.M. Blei", "M.I. Jordan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 37, no. 2, pp. 256\u2013270, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard.", "D. McClosky"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014, pp. 55\u201360.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Open language learning for information extraction", "author": ["Mausam", "M. Schmitz", "R. Bart", "S. Soderland", "O. Etzioni"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, 2012, pp. 524\u2013534.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences, vol. 101, no. suppl 1, pp. 5228\u20135235, 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "A bootstrapping method for learning semantic lexicons using extraction pattern contexts", "author": ["M. Thelen", "E. Riloff"], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Association for Computational Linguistics, 2002, pp. 214\u2013221.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Large-scale learning of relation-extraction rules with distant supervision from the web", "author": ["S. Krause", "H. Li", "H. Uszkoreit", "F. Xu"], "venue": "The Semantic WebISWC 2012, pp. 263\u2013278, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Mallet: A machine learning for language toolkit", "author": ["A.K. McCallum"], "venue": "2002, http://mallet.cs.umass.edu.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2002}, {"title": "On smoothing and inference for topic models", "author": ["A. Asuncion", "M. Welling", "P. Smyth", "Y.W. Teh"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pp. 27\u201334, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "A methodology for ontology evaluation using topic models", "author": ["A. Gangopadhyay", "M. Molek", "Y. Yesha", "M. Brady", "Y. Yesha"], "venue": "Intelligent Networking and Collaborative Systems (INCoS), 2012 4th International Conference on. IEEE, 2012, pp. 390\u2013395.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Using natural language to integrate, evaluate, and optimize extracted knowledge bases", "author": ["D. Downey", "C.S. Bhagavatula", "A. Yates"], "venue": "Proceedings of the 2013 workshop on Automated knowledge base construction. ACM, 2013, pp. 61\u201366.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 134, "endOffset": 137}, {"referenceID": 1, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 172, "endOffset": 175}, {"referenceID": 4, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 195, "endOffset": 198}, {"referenceID": 6, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 217, "endOffset": 220}, {"referenceID": 7, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 238, "endOffset": 241}, {"referenceID": 8, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 250, "endOffset": 253}, {"referenceID": 9, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 261, "endOffset": 265}, {"referenceID": 10, "context": "Although researchers have made significant progress on knowledge acquisition and have proposed many ontologies, for instance, WordNet [1], DBpedia [2], YAGO [3], Freebase, [4] Nell [5], DeepDive [6], Domain Cartridge [7], Knowledge Vault [8], INS-ES [9], iDLER [10], and TransE-NMM [11], current ontology construction methods still rely heavily on manual parsing and existing knowledge bases.", "startOffset": 282, "endOffset": 286}, {"referenceID": 11, "context": "The classical topic model, latent Dirichlet allocation (LDA) [12], simplifies a document as a bag of its words and describes a topic as a distribution", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "Prior research [13], [14], [15], [16], [17], [18], [19] has shown that LDA-based approaches are adequate for (terminological) ontology learning.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "Prior research [13], [14], [15], [16], [17], [18], [19] has shown that LDA-based approaches are adequate for (terminological) ontology learning.", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Prior research [13], [14], [15], [16], [17], [18], [19] has shown that LDA-based approaches are adequate for (terminological) ontology learning.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Prior research [13], [14], [15], [16], [17], [18], [19] has shown that LDA-based approaches are adequate for (terminological) ontology learning.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "Prior research [13], [14], [15], [16], [17], [18], [19] has shown that LDA-based approaches are adequate for (terminological) ontology learning.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "Prior research [13], [14], [15], [16], [17], [18], [19] has shown that LDA-based approaches are adequate for (terminological) ontology learning.", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "Prior research [13], [14], [15], [16], [17], [18], [19] has shown that LDA-based approaches are adequate for (terminological) ontology learning.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "Among models not using unigrams, LDA-based Global Similarity Hierarchy Learning (LDA+GSHL) [14] only extracts a subset of relations: \u201cbroader\u201d and \u201crelated\u201d relations.", "startOffset": 91, "endOffset": 95}, {"referenceID": 17, "context": "In addition, the topic hierarchies of KB-LDA [18] rely on hypernym-hyponym ar X iv :1 70 8.", "startOffset": 45, "endOffset": 49}, {"referenceID": 19, "context": "Regarding the third objective, we adapt and improve the hierarchical latent Dirichlet allocation (hLDA) model [20], [21].", "startOffset": 110, "endOffset": 114}, {"referenceID": 20, "context": "Regarding the third objective, we adapt and improve the hierarchical latent Dirichlet allocation (hLDA) model [20], [21].", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "The theory of distance-dependent CRP was formerly proposed by David Blei [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "the fact that the single path was changed to multiple paths in some extensions of hLDA - the nested Chinese restaurant franchise processes [23] and the nested hierarchical Dirichlet Processes [24], - this topic path drawing strategy puts words from different domains into one topic when input data are mixed with topics from multiple domains.", "startOffset": 139, "endOffset": 143}, {"referenceID": 23, "context": "the fact that the single path was changed to multiple paths in some extensions of hLDA - the nested Chinese restaurant franchise processes [23] and the nested hierarchical Dirichlet Processes [24], - this topic path drawing strategy puts words from different domains into one topic when input data are mixed with topics from multiple domains.", "startOffset": 192, "endOffset": 196}, {"referenceID": 24, "context": "using a language parser such as the Stanford NLP parser [25] and Ollie [26].", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "using a language parser such as the Stanford NLP parser [25] and Ollie [26].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "Normalize the (k + 1) probabilities to guarantee they are each in the range of [0, 1] and their sum is equal to 1.", "startOffset": 79, "endOffset": 85}, {"referenceID": 26, "context": "We use collapsed Gibbs sampling [27] for inference from posterior distribution P (Z|T, \u03b1, \u03b7) based on Equation 1.", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "process [28].", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "By assembling all subject and object pairs, we can build an undirected graph with the objects and the subjects constituting the nodes of the graph [29].", "startOffset": 147, "endOffset": 151}, {"referenceID": 29, "context": "We utilized the Apache poi library to parse texts from pdfs, word documents and presentation files; the MALLET toolbox [30] for the implementations of LDA, optimized LDA [31] and hLDA; the Apache Jena library to add relations, properties and members to hierarchical topic trees; and Stanford Protege1 for illustrating extracted ontologies.", "startOffset": 119, "endOffset": 123}, {"referenceID": 30, "context": "We utilized the Apache poi library to parse texts from pdfs, word documents and presentation files; the MALLET toolbox [30] for the implementations of LDA, optimized LDA [31] and hLDA; the Apache Jena library to add relations, properties and members to hierarchical topic trees; and Stanford Protege1 for illustrating extracted ontologies.", "startOffset": 170, "endOffset": 174}, {"referenceID": 31, "context": "We have tested the average perplexity and running time performance of ten independent runs on each of the four models [32], [33].", "startOffset": 118, "endOffset": 122}, {"referenceID": 32, "context": "We have tested the average perplexity and running time performance of ten independent runs on each of the four models [32], [33].", "startOffset": 124, "endOffset": 128}], "year": 2017, "abstractText": "In this paper, we present hierarchical relationbased latent Dirichlet allocation (hrLDA), a data-driven hierarchical topic model for extracting terminological ontologies from a large number of heterogeneous documents. In contrast to traditional topic models, hrLDA relies on noun phrases instead of unigrams, considers syntax and document structures, and enriches topic hierarchies with topic relations. Through a series of experiments, we demonstrate the superiority of hrLDA over existing topic models, especially for building hierarchies. Furthermore, we illustrate the robustness of hrLDA in the settings of noisy data sets, which are likely to occur in many practical scenarios. Our ontology evaluation results show that ontologies extracted from hrLDA are very competitive with the ontologies created by domain experts. Keywords-terminological ontology; ontology learning; hierarchical topic modeling; knowledge acquisition", "creator": "LaTeX with hyperref package"}}}