{"id": "1708.09453", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Inference of Fine-Grained Event Causality from Blogs and Films", "abstract": "Human understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning fine-grained causal relations between everyday events. We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80% of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire.", "histories": [["v1", "Wed, 30 Aug 2017 20:12:01 GMT  (25kb)", "http://arxiv.org/abs/1708.09453v1", "Events and Stories in the News Workshop, ACL 2017"]], "COMMENTS": "Events and Stories in the News Workshop, ACL 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["zhichao hu", "elahe rahimtoroghi", "marilyn a walker"], "accepted": false, "id": "1708.09453"}, "pdf": {"name": "1708.09453.pdf", "metadata": {"source": "CRF", "title": "Inference of Fine-Grained Event Causality from Blogs and Films", "authors": ["Zhichao Hu", "Elahe Rahimtoroghi"], "emails": ["zhu@soe.ucsc.edu,", "elahe@soe.ucsc.edu,", "mawalker@ucsc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n09 45\n3v 1\n[ cs\n.C L\n] 3\n0 A\nug 2\n01 7\nmainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning finegrained causal relations between everyday events. We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80% of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire."}, {"heading": "1 Introduction", "text": "Computational models of language understanding must recognize narrative structure because many types of natural language texts are narratively structured, e.g. news, reviews, film scripts, conversations, and personal blogs (Polanyi, 1989; Jurafsky et al., 2014; Bell, 2005; Gordon et al., 2011a). Human understanding of narrative is driven by reasoning about causal relations between the events and states\nin the story (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010). Thus previous work has aimed to learn a knowledge base of semantic relations between events from text (Chklovski and Pantel, 2004; Gordon et al., 2011a; Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Do et al., 2011), with the long-term aim of using this knowledge for understanding. Some of this work explicitly models causality; other work characterizes the semantic relations more loosely as \u201cevents that tend to co-occur\u201d. Related work points out that causality is granular in nature, and that humans flexibly move back and forth between different levels of granularity of causal knowledge (Hobbs, 1985). Thus methods are needed to learn causal relations and reason about them at different levels of granularity (Mulkar-Mehta et al., 2011).\nOne limitation of prior work is that it has primarily focused on newswire, thus have only learned relations about newsworthy topics, and likely the most frequent, highly common (coarsegrained) news events. But news articles are not the only resource for learning about relations between events. Much of the content on social media in personal blogs is written by ordinary people about their daily lives (Burton et al., 2009), and these blogs contain a large variety of everyday\nevents (Gordon et al., 2012). Film scene descriptions are also action-rich and told in fine-grained detail (Beamer and Girju, 2009; Hu et al., 2013). Moreover, both of these genres typically report events in temporal order, which is a primary cue to causality. In this position paper, we claim that knowledge about fine-grained causal relations between everyday events is often not available in news, and can be better learned from other narrative genres.\nFor example, Figure 1 shows a part of a personal narrative written in a blog about a camping trip (Burton et al., 2009). The major event in this story is camping, which is contingent upon several finer-grained events, such as packing things the night before, waking up in the morning, packing frozen food, and later on at the campground, placing a tarp and setting up the tent. Similarly film scene descriptions, such as the one shown in Figure 2, typically contain fine-grained causality. In this scene from Lord of the Rings, grabbing leads to spilling, and pushing leads to stumbling and falling.\nWe show that unsupervised methods for modeling causality can learn fine-grained event relations from personal narratives and film scenes, even when the corpus is relatively small compared to those that have been used for newswire. We learn high-quality causal relations, with over 80% judged as causal by humans. We claim that these fine-grained causal relations are much closer in spirit to those motivating earlier work on scripts (Lehnert, 1981; Schank et al., 1977; Wilensky, 1982; de Jong, 1979), and we show that the causal knowledge we learn is not found in causal knowledge bases learned from news.\nSection 2 first summarizes previous work on learning causal knowledge. We then present our experiments and results on modeling event causality in blogs and film scenes in Section 3. Conclusions and future directions are discussed in Section 4."}, {"heading": "2 Background and Related Work", "text": "Cognitive theories of narrative understanding define narrative coherence in terms of four different sources of causal inferences between events A and B (Trabasso and van den Broek, 1985; Warren et al., 1979; Trabasso et al., 1989; Van den Broek, 1990). (1) Physical: A physically causes event B. (2) Motivational: A hap-\npens with B as a motivation. (3) Psychological: A brings about emotions expressed by event B. (4) Enabling: A creates a state or condition for B to happen.\nThere has been a great deal of interest in learning narrative relations or narrative schema in an unsupervised or weakly supervised manner from text. Here we focus on work where the resulting knowledge bases have been made publicly available, allowing us to compare the learned knowledge directly.\nThe VerbOcean project learned five different semantic relations between event types (verbs) from newswire, with the HAPPENS-BEFORE relation defined as \u201cindicating that the two verbs refer to two temporally disjoint intervals or instances\u201d. WordNet\u2019s cause relation, between a causative and a resultative verb (as in buy::own) is tagged as an instance of HAPPENS-BEFORE in VerbOcean, consistent with the heuristic that temporal ordering is a major component of causality. Other examples of the HAPPENS-BEFORE relation in the VerbOcean knowledge base include marry::divorce, detain::prosecute, enroll::graduate, schedule::reschedule, and tie::untie (Chklovski and Pantel, 2004).\nBalasubramanian et al. (2013) generate pairs of event relational tuples, called Rel-grams. The Rel-grams are publicly available through an online search interface1. Rel-gram tuples are extracted using a co-occurrence statistical metric, Symmetric Conditional Probability (SCP), which combines Bigram probability in both directions as follows:\nSCP (e1, e2) = P (e2|e1)\u00d7 P (e1|e2) (1)\nTheir evaluation experiments directly compared the knowledge learned in Rel-grams to the previous work on narrative schemas (Chambers and Jurafsky, 2008, 2009), showing that they achieve better results, thus our work compares directly to the tuples available in Rel-grams.\n1http://relgrams.cs.washington.edu:10000/relgrams\nOther work focuses more directly on learning causal or contingency relations between events. Beamer and Girju (2009) introduced a distributional measure called Causal Potential to assess the likelihood of a causal relation holding between two events. This measure is based on Suppes\u2019 probabilistic theory of causality (Suppes, 1970).\nCP(e1, e2) = PMI (e1, e2) + log P (e1 \u2192 e2)\nP (e2 \u2192 e1) (2)\nwhere PMI (e1, e2) = log P (e1, e2)\nP (e1)P (e2)\nwhere the arrow notation means ordered event pairs, i.e. event e1 occurs before event e2. CP consists of two terms: the first is pair-wise mutual information (PMI) and the second is relative ordering of bigrams. PMI measures how often events occur as a pair (without considering their order); whereas relative ordering accounts for the order of the event pairs because temporal order is one of the strongest cues to causality (Beamer and Girju, 2009; Riaz and Girju, 2010, 2013). This work explicitly links their definitions to research using the Penn Discourse Treebank (PDTB) definition of CONTINGENCY.\nBeamer and Girju (2009) applied the CP measure to 173 film scripts, resulting in a high correlation between human-judged causality and the CP measure. Their paper provides a list of 90 verb pairs, selected from the high, middle and low CP ranges in their learned causal pairs. We compare their 30 highest CP events with causal event pairs that we learn from film.\nRiaz and Girju (2010) apply a similar measure to topic-sorted news stories about Hurricane Katrina and the Iraq War and present ranked causality relations between events for these topics, suggesting that topic-sorted corpora can produce better causal knowledge. Other work has also used CP to measure the contingency relation between two events, reporting better results than achieved with PMI or bigrams alone (Hu et al., 2013; Rahimtoroghi et al., 2016)."}, {"heading": "3 Methods and Evaluations", "text": "Our primary goal is simply to show that finegrained causal relations can be learned from film scripts and blogs, and that these are not found in causal knowledge bases learned from newswire.\nIn this section we describe our datasets and methods, and the present two evaluations. First, we evaluate whether the relations learned are causal using human judgment HITs on Amazon Mechanical Turk. Second, we directly compare to event pair collections from other publicly available sources learned from news genre."}, {"heading": "3.1 Datasets", "text": "Topical coherence and similarity of events within the corpus used for learning event relations can be as important as the size of the corpus (Riaz and Girju, 2010; Rahimtoroghi et al., 2016). We use two datasets for learning causal event pairs: first-person narratives from blogs (Burton et al., 2009; Rahimtoroghi et al., 2016), and film scene descriptions (excluding dialogs because dialogs are not as actionrich) (Walker et al., 2012; Hu et al., 2013). Our experiment on blogs learns causal relations from a topic-sorted corpus of \u223c1000 camping stories. We also posit that the genre of a film may select for similar types of events. However genres can be defined broadly or narrowly, e.g. the Drama genre overlaps with many other genres. We thus compare two narrow film genres of Fantasy and Mystery with the Drama genre from an existing corpus (Walker et al., 2012; Hu et al., 2013). The raw numbers for each subcorpus are shown in Table 1. Note that Camping corpus consists of blog posts which are much shorter compared to movie scripts. Thus their word count is much smaller compared to films corpus despite the larger number of documents."}, {"heading": "3.2 Methods", "text": "In the blogs, related event pairs are more frequently separated by utterances that provide state descriptions or affective reactions to events (Swanson et al., 2014). As a result, we use Causal Potential (CP) measure to assess the causal relation between events and apply skip-2 bigram method for modeling event pairs. But in film\nscenes, events are very densely distributed, thus related event pairs are often adjacent to one another and therefore nearby events are more likely to be causal. So, for event pairs extracted from films we use a variant of CP measure, shown in Eq. 3, that accounts for different window sizes and punishes event pairs from larger window sizes (Riaz and Girju, 2010, 2013; Do et al., 2011; Pichotta and Mooney, 2014).\nCPvariant(e1, e2) =\nwmax\u2211\ni=1\nCPi(e1, e2)\ni (3)\nwhere wmax is the max window size (how many events after the current event are paired with the current event). CPi(e1; e2) is the CP score for event pair e1; e2 calculated using window size i."}, {"heading": "3.3 Experiments and Results", "text": "We process the data in each dataset and calculate causal potential score for each extracted event pair, resulting in a rank-ordered list of causal event pairs. We evaluate the top 100 event pairs for camping, and the top 684 event pairs for films. We take a number of event pairs from each film genre\n(proportional to the number of films in that genre, see Table 1 and 3), then remove duplicate event pairs, which result in the 684 event pairs from film. Table 2 presents examples of learned highCP event pairs from each corpus. In our following Mechanical Turk experiments, Turkers have to pass qualification tests similar to the actual HITs to be able to participate in our task.\nIn a study on each genre of films, we compare high-CP pairs to a random sample of lowCP pairs on Mechanical Turk to see if pairs with high CP score more strongly encode causal relations that ones with low CP. For every event pair in the 684 high pairs, we randomly select a low pair in order to collect human judgments on Mechanical Turk. The task first defines events and event pairs, then gives examples of event pairs with causal relations. Turkers are asked to select the event pair that is more likely to manifest a causal relation. The results, summarized in Table 3, show that humans judge a large majority of the high-CP pairs to have a causal relation and the results vary by genre. The causality rate is achieved for more focused genres, Fantasy (90.7%) and Mystery (87.7%), despite their smaller size, and the lowest for Drama (82.6%). We believe this result is further evidence that topical coherence improves causal relation learning (Rahimtoroghi et al., 2016; Riaz and Girju, 2010).\nIn our second evaluation method, we compare the learned CP event pairs to the existing causal knowledge collections. First, we compare our results to the Rel-grams data (learned from newswire) (Balasubramanian et al., 2013). For event pairs from films, we randomly sample 100 high-CP event pairs ensuring that each of the first events of the pairs are distinct. We use the publicly available search interface for Rel-grams to find tuples with the same first event for direct comparison of content of the learned knowledge. We set the co-occurrence window to 5, and se-\nlect the Rel-gram tuples with the highest # 50 (FS) (frequency of first statement occurring before second statement within a window of 50) to choose high-quality tuples. We evaluate the extracted Rel-gram tuples using the same Mechanical Turk HIT described above. Table 4 shows Mechanical Turk evaluation results for our method on films vs. Rel-grams: in 81% questions, humans judge the high-CP pairs to be more likely to manifest a causal relation. We believe this is because the fine-grained event pairs we learn do not exist in the Rel-gram collections and thus the Rel-gram tuples that matched our first events are not highly coherent, despite the filtering we applied.\nFor event pairs from camping blogs, we evaluate all 100 high-CP pairs in a Mechanical Turk study where Turkers are asked to choose whether an event pair has causal relation or not. We also evaluate Rel-gram tuples using the same task. However, Rel-grams are not sorted by topic. To find tuples relevant to Camping Trip, we use our top 10 indicative events and extracted all the Relgram tuples that included at least one event corresponding to one of the Camping indicative events, e.g. go camp. We remove any tuple with frequency less than 25 and sort the rest by the total symmetrical conditional probability. The evaluation results presented in Table 5 show that 82% of the blog paurs were labeled as causal, where as only 42% of the Rel-gram pairs were labeled as causal. We argue that this is mainly due to the limitations of the newswire data which does not contain the fine-grained everyday events that we have extracted from our corpus.\nNext, we compare our results to the event pairs in VerbOcean (learned from newswire) with the HAPPENS-BEFORE rela-\ntion (Chklovski and Pantel, 2004). We use all 6497 event pairs from VerbOcean, comparing with our 684 event pairs from films and 100 event pairs from camping blogs with high CP scores. Our result shows that there are 12 event pairs that exist in both VerbOcean and films, e.g. turn - leave and slow - stop, and there is only one event pair that exist in both VerbOcean and camping blogs: pack - leave. This confirms that most causal relations learned from other narrative genres do not exist in the currently available knowledge bases extracted from newswire. A number of event pairs from these collections share the first event, e.g. dig - find and scan - spot from films vs. dig - repair and scan - upload from VerbOcean; drive - park and pick - eat from blogs vs. drive - drag and pick - plunk from VerbOcean.\nFinally, we compare our high-CP pairs learned from film to the high-CP event pairs from Beamer and Girju (2009), learned from only 173 films. There is no public release of Beamer and Girju\u2019s event pairs, thus we take the 29 event pairs with high CP score presented in the paper. A total of 14 of their 29 pairs are also in our top 684 film pairs. These include pairs such as swerve - avoid, leave - stand and unlock - open. However on our larger genre-sorted corpus we also learn pairs such as grab - haul, scratch - claw and saddle- mount that do not exist in their collection."}, {"heading": "4 Conclusions and Future Work", "text": "Causality is often granular in nature with major events related to the occurrence of finer-grained events. In this position paper, we argue that the focus on newswire has inhibited attempts to learn fine-grained causal relations between everyday events, and that other narrative genres better support such learning. We use unsupervised methods to extract fine-grained causal event relations from films and blog posts about camping.\nWe show that more than 80% of the relations we learn are evaluated as causal, and that topical coherence plays an important role in modeling event relations. We also show that the causal knowledge we learn from other narrative genres does not exist in current event collections induced from newswire. We plan to expand our genre-specific experiments on the films corpus in future, as well as using other narrative datasets, like restaurant reviews, to extract fine-grained causal knowledge about events."}], "references": [{"title": "Generating coherent event schemas at scale", "author": ["Niranjan Balasubramanian", "Stephen Soderland", "Mausam", "Oren Etzioni."], "venue": "EMNLP. pages 1721\u20131731.", "citeRegEx": "Balasubramanian et al\\.,? 2013", "shortCiteRegEx": "Balasubramanian et al\\.", "year": 2013}, {"title": "Using a bigram event model to predict causal potential", "author": ["Brandon Beamer", "Roxana Girju."], "venue": "Computational Linguistics and Intelligent Text Processing, Springer, pages 430\u2013441.", "citeRegEx": "Beamer and Girju.,? 2009", "shortCiteRegEx": "Beamer and Girju.", "year": 2009}, {"title": "News stories as narratives", "author": ["Allan Bell."], "venue": "The Language of Time: A Reader page 397.", "citeRegEx": "Bell.,? 2005", "shortCiteRegEx": "Bell.", "year": 2005}, {"title": "The ICWSM 2009 Spinn3r dataset", "author": ["Kevin Burton", "Akshay Java", "Ian Soboroff."], "venue": "Proceedings of the Third Annual Conference on Weblogs and Social Media (ICWSM 2009).", "citeRegEx": "Burton et al\\.,? 2009", "shortCiteRegEx": "Burton et al\\.", "year": 2009}, {"title": "Unsupervised learning of narrative event chains", "author": ["Nathanael Chambers", "Dan Jurafsky."], "venue": "Proceedings of ACL-08: HLT pages 789\u2013797.", "citeRegEx": "Chambers and Jurafsky.,? 2008", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2008}, {"title": "Unsupervised learning of narrative schemas and their participants", "author": ["Nathanael Chambers", "Dan Jurafsky."], "venue": "Proceedings of the 47th Annual Meeting of the ACL. pages 602\u2013610.", "citeRegEx": "Chambers and Jurafsky.,? 2009", "shortCiteRegEx": "Chambers and Jurafsky.", "year": 2009}, {"title": "Verbocean: Mining the web for fine-grained semantic verb relations", "author": ["Timothy Chklovski", "Patrick Pantel."], "venue": "EMNLP. volume 4, pages 33\u201340.", "citeRegEx": "Chklovski and Pantel.,? 2004", "shortCiteRegEx": "Chklovski and Pantel.", "year": 2004}, {"title": "Skimming Stories in Real Time: An Experiment in Integrated Understanding", "author": ["G.F. de Jong."], "venue": "Ph.D. thesis, Computer Science Department, Yale University.", "citeRegEx": "Jong.,? 1979", "shortCiteRegEx": "Jong.", "year": 1979}, {"title": "Minimally supervised event causality identification", "author": ["Quang Xuan Do", "Yee Seng Chan", "Dan Roth."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 294\u2013", "citeRegEx": "Do et al\\.,? 2011", "shortCiteRegEx": "Do et al\\.", "year": 2011}, {"title": "Experiencing narrative worlds: On the psychological activities of reading", "author": ["R.J. Gerrig."], "venue": "Yale Univ Pr.", "citeRegEx": "Gerrig.,? 1993", "shortCiteRegEx": "Gerrig.", "year": 1993}, {"title": "Commonsense causal reasoning using millions of personal stories", "author": ["Andrew Gordon", "Cosmin Bejan", "Kenji Sagae."], "venue": "Twenty-Fifth Conference on Artificial Intelligence (AAAI-11).", "citeRegEx": "Gordon et al\\.,? 2011a", "shortCiteRegEx": "Gordon et al\\.", "year": 2011}, {"title": "Commonsense causal reasoning using millions of personal stories", "author": ["Andrew S Gordon", "Cosmin Adrian Bejan", "Kenji Sagae."], "venue": "AAAI.", "citeRegEx": "Gordon et al\\.,? 2011b", "shortCiteRegEx": "Gordon et al\\.", "year": 2011}, {"title": "Different strokes of different folks: Searching for health narratives in weblogs", "author": ["Andrew S Gordon", "Christopher Wienberg", "Sara Owsley Sood."], "venue": "Privacy, Security, Risk and Trust (PASSAT), 2012 International Conference on and 2012", "citeRegEx": "Gordon et al\\.,? 2012", "shortCiteRegEx": "Gordon et al\\.", "year": 2012}, {"title": "Automatically producing plot unit representations for narrative text", "author": ["Amit Goyal", "Ellen Riloff", "Hal Daum\u00e9 III."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. pages 77\u201386.", "citeRegEx": "Goyal et al\\.,? 2010", "shortCiteRegEx": "Goyal et al\\.", "year": 2010}, {"title": "Constructing inferences during narrative text comprehension", "author": ["Arthur C Graesser", "Murray Singer", "Tom Trabasso."], "venue": "Psychological review 101(3):371.", "citeRegEx": "Graesser et al\\.,? 1994", "shortCiteRegEx": "Graesser et al\\.", "year": 1994}, {"title": "Granularity", "author": ["Jerry R Hobbs."], "venue": "Proceedings of the 9th international joint conference on Artificial intelligence-Volume 1. Morgan Kaufmann Publishers Inc., pages 432\u2013435.", "citeRegEx": "Hobbs.,? 1985", "shortCiteRegEx": "Hobbs.", "year": 1985}, {"title": "Unsupervised induction of contingent event pairs from film scenes", "author": ["Zhichao Hu", "Elahe Rahimtoroghi", "Larissa Munishkina", "Reid Swanson", "Marilyn A Walker."], "venue": "Proceedings of Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Hu et al\\.,? 2013", "shortCiteRegEx": "Hu et al\\.", "year": 2013}, {"title": "Narrative framing of consumer sentiment in online restaurant reviews", "author": ["Dan Jurafsky", "Victor Chahuneau", "Bryan R Routledge", "Noah A Smith."], "venue": "First Monday 19(4).", "citeRegEx": "Jurafsky et al\\.,? 2014", "shortCiteRegEx": "Jurafsky et al\\.", "year": 2014}, {"title": "Plot units and narrative summarization", "author": ["Wendy G Lehnert."], "venue": "Cognitive Science 5(4):293\u2013331.", "citeRegEx": "Lehnert.,? 1981", "shortCiteRegEx": "Lehnert.", "year": 1981}, {"title": "Using granularity concepts for discovering causal relations", "author": ["Rutu Mulkar-Mehta", "Christopher Welty", "Jerry R Hoobs", "Eduard Hovy."], "venue": "Proceedings of the FLAIRS conference.", "citeRegEx": "Mulkar.Mehta et al\\.,? 2011", "shortCiteRegEx": "Mulkar.Mehta et al\\.", "year": 2011}, {"title": "Statistical script learning with multi-argument events", "author": ["Karl Pichotta", "Raymond J Mooney."], "venue": "EACL 2014 page 220.", "citeRegEx": "Pichotta and Mooney.,? 2014", "shortCiteRegEx": "Pichotta and Mooney.", "year": 2014}, {"title": "Telling the American Story: A Structural and Cultural Analysis of Conversational Storytelling", "author": ["Livia Polanyi."], "venue": "MIT Press.", "citeRegEx": "Polanyi.,? 1989", "shortCiteRegEx": "Polanyi.", "year": 1989}, {"title": "Learning fine-grained knowledge about contingent relations between everyday events", "author": ["Elahe Rahimtoroghi", "Ernesto Hernandez", "Marilyn A. Walker."], "venue": "Proceedings of SIGDIAL 2016. pages 350\u2013359.", "citeRegEx": "Rahimtoroghi et al\\.,? 2016", "shortCiteRegEx": "Rahimtoroghi et al\\.", "year": 2016}, {"title": "Another look at causality: Discovering scenario-specific contingency relationships with no supervision", "author": ["Mehwish Riaz", "Roxana Girju."], "venue": "Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on. IEEE, pages 361\u2013368.", "citeRegEx": "Riaz and Girju.,? 2010", "shortCiteRegEx": "Riaz and Girju.", "year": 2010}, {"title": "Toward a better understanding of causality between verbal events: Extraction and analysis of the causal power of verb-verb associations", "author": ["Mehwish Riaz", "Roxana Girju."], "venue": "Proceedings of the annual SIGdial Meeting on Discourse and Dialogue", "citeRegEx": "Riaz and Girju.,? 2013", "shortCiteRegEx": "Riaz and Girju.", "year": 2013}, {"title": "Scripts Plans Goals", "author": ["R Schank", "Robert Abelson", "Roger C Schank."], "venue": "Lea.", "citeRegEx": "Schank et al\\.,? 1977", "shortCiteRegEx": "Schank et al\\.", "year": 1977}, {"title": "A probabilistic theory of causal", "author": ["Patrick Suppes"], "venue": null, "citeRegEx": "Suppes.,? \\Q1970\\E", "shortCiteRegEx": "Suppes.", "year": 1970}, {"title": "The causal inference", "author": ["Paul Van den Broek"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1990}, {"title": "Points: A theory of the struc", "author": ["Robert Wilensky"], "venue": null, "citeRegEx": "Wilensky.,? \\Q1982\\E", "shortCiteRegEx": "Wilensky.", "year": 1982}], "referenceMentions": [{"referenceID": 21, "context": "news, reviews, film scripts, conversations, and personal blogs (Polanyi, 1989; Jurafsky et al., 2014; Bell, 2005; Gordon et al., 2011a).", "startOffset": 63, "endOffset": 135}, {"referenceID": 17, "context": "news, reviews, film scripts, conversations, and personal blogs (Polanyi, 1989; Jurafsky et al., 2014; Bell, 2005; Gordon et al., 2011a).", "startOffset": 63, "endOffset": 135}, {"referenceID": 2, "context": "news, reviews, film scripts, conversations, and personal blogs (Polanyi, 1989; Jurafsky et al., 2014; Bell, 2005; Gordon et al., 2011a).", "startOffset": 63, "endOffset": 135}, {"referenceID": 10, "context": "news, reviews, film scripts, conversations, and personal blogs (Polanyi, 1989; Jurafsky et al., 2014; Bell, 2005; Gordon et al., 2011a).", "startOffset": 63, "endOffset": 135}, {"referenceID": 9, "context": "in the story (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010).", "startOffset": 13, "endOffset": 85}, {"referenceID": 14, "context": "in the story (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010).", "startOffset": 13, "endOffset": 85}, {"referenceID": 18, "context": "in the story (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010).", "startOffset": 13, "endOffset": 85}, {"referenceID": 13, "context": "in the story (Gerrig, 1993; Graesser et al., 1994; Lehnert, 1981; Goyal et al., 2010).", "startOffset": 13, "endOffset": 85}, {"referenceID": 6, "context": "Thus previous work has aimed to learn a knowledge base of semantic relations between events from text (Chklovski and Pantel, 2004; Gordon et al., 2011a; Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Do et al., 2011), with the long-term aim of using this knowledge for understanding.", "startOffset": 102, "endOffset": 255}, {"referenceID": 10, "context": "Thus previous work has aimed to learn a knowledge base of semantic relations between events from text (Chklovski and Pantel, 2004; Gordon et al., 2011a; Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Do et al., 2011), with the long-term aim of using this knowledge for understanding.", "startOffset": 102, "endOffset": 255}, {"referenceID": 4, "context": "Thus previous work has aimed to learn a knowledge base of semantic relations between events from text (Chklovski and Pantel, 2004; Gordon et al., 2011a; Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Do et al., 2011), with the long-term aim of using this knowledge for understanding.", "startOffset": 102, "endOffset": 255}, {"referenceID": 0, "context": "Thus previous work has aimed to learn a knowledge base of semantic relations between events from text (Chklovski and Pantel, 2004; Gordon et al., 2011a; Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Do et al., 2011), with the long-term aim of using this knowledge for understanding.", "startOffset": 102, "endOffset": 255}, {"referenceID": 20, "context": "Thus previous work has aimed to learn a knowledge base of semantic relations between events from text (Chklovski and Pantel, 2004; Gordon et al., 2011a; Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Do et al., 2011), with the long-term aim of using this knowledge for understanding.", "startOffset": 102, "endOffset": 255}, {"referenceID": 8, "context": "Thus previous work has aimed to learn a knowledge base of semantic relations between events from text (Chklovski and Pantel, 2004; Gordon et al., 2011a; Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Pichotta and Mooney, 2014; Do et al., 2011), with the long-term aim of using this knowledge for understanding.", "startOffset": 102, "endOffset": 255}, {"referenceID": 15, "context": "causality is granular in nature, and that humans flexibly move back and forth between different levels of granularity of causal knowledge (Hobbs, 1985).", "startOffset": 138, "endOffset": 151}, {"referenceID": 19, "context": "Thus methods are needed to learn causal relations and reason about them at different levels of granularity (Mulkar-Mehta et al., 2011).", "startOffset": 107, "endOffset": 134}, {"referenceID": 3, "context": "dia in personal blogs is written by ordinary people about their daily lives (Burton et al., 2009), and these blogs contain a large variety of everyday", "startOffset": 76, "endOffset": 97}, {"referenceID": 12, "context": "events (Gordon et al., 2012).", "startOffset": 7, "endOffset": 28}, {"referenceID": 1, "context": "Film scene descriptions are also action-rich and told in fine-grained detail (Beamer and Girju, 2009; Hu et al., 2013).", "startOffset": 77, "endOffset": 118}, {"referenceID": 16, "context": "Film scene descriptions are also action-rich and told in fine-grained detail (Beamer and Girju, 2009; Hu et al., 2013).", "startOffset": 77, "endOffset": 118}, {"referenceID": 3, "context": "For example, Figure 1 shows a part of a personal narrative written in a blog about a camping trip (Burton et al., 2009).", "startOffset": 98, "endOffset": 119}, {"referenceID": 18, "context": "We claim that these fine-grained causal relations are much closer in spirit to those motivating earlier work on scripts (Lehnert, 1981; Schank et al., 1977; Wilensky, 1982; de Jong, 1979), and we show that the causal knowledge we learn is not found in", "startOffset": 120, "endOffset": 187}, {"referenceID": 25, "context": "We claim that these fine-grained causal relations are much closer in spirit to those motivating earlier work on scripts (Lehnert, 1981; Schank et al., 1977; Wilensky, 1982; de Jong, 1979), and we show that the causal knowledge we learn is not found in", "startOffset": 120, "endOffset": 187}, {"referenceID": 28, "context": "We claim that these fine-grained causal relations are much closer in spirit to those motivating earlier work on scripts (Lehnert, 1981; Schank et al., 1977; Wilensky, 1982; de Jong, 1979), and we show that the causal knowledge we learn is not found in", "startOffset": 120, "endOffset": 187}, {"referenceID": 6, "context": "Other examples of the HAPPENS-BEFORE relation in the VerbOcean knowledge base include marry::divorce, detain::prosecute, enroll::graduate, schedule::reschedule, and tie::untie (Chklovski and Pantel, 2004).", "startOffset": 176, "endOffset": 204}, {"referenceID": 0, "context": "Balasubramanian et al. (2013) generate pairs of", "startOffset": 0, "endOffset": 30}, {"referenceID": 26, "context": "This measure is based on Suppes\u2019 probabilistic theory of causality (Suppes, 1970).", "startOffset": 67, "endOffset": 81}, {"referenceID": 1, "context": "Beamer and Girju (2009) introduced a distributional measure called Causal Potential to assess the likelihood of a causal relation holding between two events.", "startOffset": 0, "endOffset": 24}, {"referenceID": 1, "context": "PMI measures how often events occur as a pair (without considering their order); whereas relative ordering accounts for the order of the event pairs because temporal order is one of the strongest cues to causality (Beamer and Girju, 2009; Riaz and Girju, 2010, 2013).", "startOffset": 214, "endOffset": 266}, {"referenceID": 16, "context": "Other work has also used CP to measure the contingency relation between two events, reporting better results than achieved with PMI or bigrams alone (Hu et al., 2013; Rahimtoroghi et al., 2016).", "startOffset": 149, "endOffset": 193}, {"referenceID": 22, "context": "Other work has also used CP to measure the contingency relation between two events, reporting better results than achieved with PMI or bigrams alone (Hu et al., 2013; Rahimtoroghi et al., 2016).", "startOffset": 149, "endOffset": 193}, {"referenceID": 23, "context": "Topical coherence and similarity of events within the corpus used for learning event relations can be as important as the size of the corpus (Riaz and Girju, 2010; Rahimtoroghi et al., 2016).", "startOffset": 141, "endOffset": 190}, {"referenceID": 22, "context": "Topical coherence and similarity of events within the corpus used for learning event relations can be as important as the size of the corpus (Riaz and Girju, 2010; Rahimtoroghi et al., 2016).", "startOffset": 141, "endOffset": 190}, {"referenceID": 3, "context": "We use two datasets for learning causal event pairs: first-person narratives from blogs (Burton et al., 2009; Rahimtoroghi et al., 2016), and film scene descriptions (excluding dialogs because dialogs are not as actionrich) (Walker et al.", "startOffset": 88, "endOffset": 136}, {"referenceID": 22, "context": "We use two datasets for learning causal event pairs: first-person narratives from blogs (Burton et al., 2009; Rahimtoroghi et al., 2016), and film scene descriptions (excluding dialogs because dialogs are not as actionrich) (Walker et al.", "startOffset": 88, "endOffset": 136}, {"referenceID": 16, "context": ", 2016), and film scene descriptions (excluding dialogs because dialogs are not as actionrich) (Walker et al., 2012; Hu et al., 2013).", "startOffset": 95, "endOffset": 133}, {"referenceID": 16, "context": "We thus compare two narrow film genres of Fantasy and Mystery with the Drama genre from an existing corpus (Walker et al., 2012; Hu et al., 2013).", "startOffset": 107, "endOffset": 145}, {"referenceID": 8, "context": "3, that accounts for different window sizes and punishes event pairs from larger window sizes (Riaz and Girju, 2010, 2013; Do et al., 2011; Pichotta and Mooney, 2014).", "startOffset": 94, "endOffset": 166}, {"referenceID": 20, "context": "3, that accounts for different window sizes and punishes event pairs from larger window sizes (Riaz and Girju, 2010, 2013; Do et al., 2011; Pichotta and Mooney, 2014).", "startOffset": 94, "endOffset": 166}, {"referenceID": 22, "context": "topical coherence improves causal relation learning (Rahimtoroghi et al., 2016; Riaz and Girju, 2010).", "startOffset": 52, "endOffset": 101}, {"referenceID": 23, "context": "topical coherence improves causal relation learning (Rahimtoroghi et al., 2016; Riaz and Girju, 2010).", "startOffset": 52, "endOffset": 101}, {"referenceID": 0, "context": "First, we compare our results to the Rel-grams data (learned from newswire) (Balasubramanian et al., 2013).", "startOffset": 76, "endOffset": 106}, {"referenceID": 6, "context": "newswire) with the HAPPENS-BEFORE relation (Chklovski and Pantel, 2004).", "startOffset": 43, "endOffset": 71}], "year": 2017, "abstractText": "Human understanding of narrative is mainly driven by reasoning about causal relations between events and thus recognizing them is a key capability for computational models of language understanding. Computational work in this area has approached this via two different routes: by focusing on acquiring a knowledge base of common causal relations between events, or by attempting to understand a particular story or macro-event, along with its storyline. In this position paper, we focus on knowledge acquisition approach and claim that newswire is a relatively poor source for learning finegrained causal relations between everyday events. We describe experiments using an unsupervised method to learn causal relations between events in the narrative genres of first-person narratives and film scene descriptions. We show that our method learns fine-grained causal relations, judged by humans as likely to be causal over 80% of the time. We also demonstrate that the learned event pairs do not exist in publicly available event-pair datasets extracted from newswire.", "creator": "LaTeX with hyperref package"}}}