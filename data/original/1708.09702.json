{"id": "1708.09702", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Human and Machine Judgements for Russian Semantic Relatedness", "abstract": "Semantic relatedness of terms represents similarity of meaning by a numerical score. On the one hand, humans easily make judgments about semantic relatedness. On the other hand, this kind of information is useful in language processing systems. While semantic relatedness has been extensively studied for English using numerous language resources, such as associative norms, human judgments, and datasets generated from lexical databases, no evaluation resources of this kind have been available for Russian to date. Our contribution addresses this problem. We present five language resources of different scale and purpose for Russian semantic relatedness, each being a list of triples (word_i, word_j, relatedness_ij). Four of them are designed for evaluation of systems for computing semantic relatedness, complementing each other in terms of the semantic relation type they represent. These benchmarks were used to organize a shared task on Russian semantic relatedness, which attracted 19 teams. We use one of the best approaches identified in this competition to generate the fifth high-coverage resource, the first open distributional thesaurus of Russian. Multiple evaluations of this thesaurus, including a large-scale crowdsourcing study involving native speakers, indicate its high accuracy.", "histories": [["v1", "Thu, 31 Aug 2017 13:33:04 GMT  (568kb,D)", "http://arxiv.org/abs/1708.09702v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alexander panchenko", "dmitry ustalov", "nikolay arefyev", "denis paperno", "natalia konstantinova", "natalia loukachevitch", "chris biemann"], "accepted": false, "id": "1708.09702"}, "pdf": {"name": "1708.09702.pdf", "metadata": {"source": "CRF", "title": "Human and Machine Judgements for Russian Semantic Relatedness", "authors": ["Alexander Panchenko", "Dmitry Ustalov", "Nikolay Arefyev", "Denis Paperno", "Natalia Konstantinova", "Natalia Loukachevitch", "Chris Biemann"], "emails": ["panchenko@lt.informatik.tu-darmstadt.de", "biem@lt.informatik.tu-darmstadt.de", "dmitry.ustalov@urfu.ru", "louk_nat@mail.ru", "denis.paperno@unitn.it", "n.konstantinova@wlv.ac.uk"], "sections": [{"heading": null, "text": "Keywords: semantic similarity \u00b7 semantic relatedness \u00b7 evaluation \u00b7 distributional thesaurus \u00b7 crowdsourcing \u00b7 language resources"}, {"heading": "1 Introduction", "text": "Semantic relatedness numerically quantifies the degree of semantic alikeness of two lexical units, such as words and multiword expressions. The relatedness score is high for pairs of words in a semantic relation (e.g., synonyms, hyponyms, free\nar X\niv :1\n70 8.\n09 70\n2v 1\n[ cs\n.C L\n] 3\n1 A\nassociations) and low for semantically unrelated pairs. Semantic relatedness and semantic similarity have been extensively studied in psychology and computational linguistics, see [1\u20134] inter alia. While both concepts are vaguely defined, similarity is a more restricted notion than relatedness, e.g. \u201capple\u201d and \u201ctree\u201d would be related but not similar. Semantically similar word pairs are usually synonyms or hypernyms, while relatedness also can also refer to meronyms, cohyponyms, associations and other types of relations. Semantic relatedness is an important building block of NLP techniques, such as text similarity [5, 6], word sense disambiguation [7], query expansion [8] and some others [9].\nWhile semantic relatedness was extensively studied in the context of the English language, NLP researchers working with Russian language could not conduct such studies due to the lack of publicly available relatedness resources. The datasets presented in this paper are meant to fill this gap. Each of them is a collection of weighted word pairs in the format (wi, wj , sij), e.g. (book, proceedings, 0.87). Here, the wi is the source word, wj is the destination word and sij \u2208 [0; 1] is the semantic relatedness score (see Table 1).\nMore specifically, we present (1) four resources for evaluation and training of semantic relatedness systems varying in size and relation type and (2) the first open distributional thesaurus for the Russian language (see Table 2). All datasets contain relations between single words.\nThe paper is organized as follows: Section 2 describes approaches to evaluation of semantic relatedness in English. Section 3 presents three datasets where semantic relatedness of word was established manually. The HJ dataset, further described in Section 3.1, is based on Human Judgements about semantic relatedness; the RuThes (RT) dataset is based on synonyms and hypernyms from a handcrafted thesaurus (see Section 3.2); the Associative Experiment (AE) dataset, introduced in Section 3.3, represents cognitive associations between words. Section 4 describes datasets where semantic relatedness between words is established automatically: the Machine Judgements (MJ) dataset, presented in Section 4.1, is based on a combination of submissions from a shared task on Russian semantic similarity; Section 4.2 describes the construction and evaluation of the Russian Distributional Thesaurus (RDT)."}, {"heading": "2 Related Work", "text": "There are three main approaches to evaluating semantic relatedness: using human judgements about word pairs, using semantic relations from lexical-semantic resources, such as WordNet [10], and using data from cognitive word association experiments. We built three evaluation datasets for Russian each based on one of these principles to enable a comprehensive comparison of relatedness models."}, {"heading": "2.1 Datasets Based on Human Judgements about Word Pairs", "text": "Word pairs labeled manually on a categorical scale by human subjects is the basis of this group of benchmarks. High scores of subjects indicate that words are semantically related, low scores indicate that they are unrelated. The HJ dataset presented in Section 3.1 belongs to this group of evaluation datasets.\nResearch on relatedness starts from the pioneering work of Rubenstein and Goodenough [11], where they aggregated human judgments on the relatedness of 65 noun pairs into the RG dataset. 51 human subjects rated the pairs on a scale from 0 to 4 according to their similarity. Later, Miller and Charles [12] replicated the experiment of Rubenstein and Goodenough, obtaining similar results on a subset of 30 noun pairs. They used 10 words from the high level (between 3 and 4), 10 from the intermediate level (between 1 and 3), and 10 from the low level (0 to 1) of semantic relatedness, and then obtained similarity judgments from 38 subjects, given the RG annotation guidelines, on those 30 pairs. This dataset is known as the MC dataset.\nA larger set of 353 word pairs was put forward by Filkenstein et al. [13] as the WordSim353 dataset. The dataset contains 353 word pairs, each associated with 13 or 16 human judgements. In this case, the subjects were asked to rate word pairs for relatedness, although many of the pairs also exemplify semantic similarity. That is why, Agirre et al. [14] subdivided the WordSim353 dataset into two subsets: the WordSim353 similarity set and the WordSim353 relatedness set. The former set consists of word pairs classified as synonyms, antonyms, identical, or hyponym-hypernym and unrelated pairs. The relatedness set contains\nword pairs connected with other relations and unrelated pairs. The similarity set contains 204 pairs and the relatedness set includes 252 pairs.\nThe three abovementioned datasets were created for English. There have been several attempts to translate those datasets into other languages. Gurevych translated the RG and MC datasets into German [15]; Hassan and Mihalcea translated them into Spanish, Arabic and Romanian [16]; Postma and Vossen [17] translated the datasets into Dutch; Jin and Wu [18] presented a shared task for Chinese semantic similarity, where the authors translated the WordSim353 dataset. Yang and Powers [19] proposed a dataset specifically for measuring verb similarity, which was later translated into German by Meyer and Gurevych [20].\nHassan and Mihalcea [16] and Postma and Vossen [17] used three stages to translation pairs: (1) disambiguation of the English word forms; (2) translation for each word; (3) ensuring that translations are in the same class of relative frequency as the English source word.\nMore recently, SimLex-999 was released by Hill et al. [21], focusing specifically on similarity and not relatedness. While most datasets are only available in English, SimLex-999 became a notable exception and has been translated into German, Russian and Italian. The Russian version of SimLex-999 is similar to the HJ dataset presented in our paper. In fact, these Russian datasets were created in parallel almost at the same time6. SimLex-999 contains 999 word pairs, which is considerably larger than the classical MC, RG and WordSim353 datasets.\nThe creators of the MEN dataset [22] went even further, annotating via crowdsourcing 3 000 word pairs sampled from the ukWaC corpus [23]. However, this dataset is also available only for English. A comprehensive list of datasets for evaluation of English semantic relatedness, featuring 12 collections, was gathered by Faruqui and Dyer [24]. This set of benchmarks was used to build a web application for evaluation and visualization of word vectors.7"}, {"heading": "2.2 Datasets Based on Lexical-Semantic Resources", "text": "Another group of evaluation datasets evaluates semantic relatedness scores with respect to relations described in lexical-semantic resources such as WordNet. The RT dataset presented in Section 3.2 belongs to this group of evaluation datasets.\nBaroni and Lenci [25] stressed that semantically related words differ in the type of relation between them, so they generated the BLESS dataset containing tuples of the form (wj , wj , type). Types of relations included co-hyponyms, hypernyms, meronyms, attributes (relation between a noun and an adjective expressing its attribute), event (relation between a noun and a verb referring to actions or events). BLESS also contains, for each target word, a number of random words that were checked to be semantically unrelated to the this word. BLESS includes 200 English concrete single-word nouns having reasonably high frequency that are not very polysemous. The destination words of the\n6 The HJ dataset was first released in November 2014 and first published in June 2015, while the SimLex-999 was first published December 2015. 7 http://wordvectors.org/suite.php\nnon-random relations are English nouns, verbs and adjectives selected and validated manually using several sources including WordNet, and collocations from the Wikipedia and the ukWaC corpora.\nVan de Cruys [26] used Dutch WordNet to evaluate distributional similarity measures. His approach uses the structure of the lexical resource, whereby distributional similarity is compared to shortest-path-based distance. Biemann and Riedl [27] follow a similar approach based on the English WordNet to assess quality of their distributional semantics framework.\nFinally, Sahlgren [28] evaluated distributional lexical similarity measures comparing them to manually-crafted thesauri, but also associative norms, such as those described in the following section."}, {"heading": "2.3 Datasets Based on Human Word Association Experiments", "text": "The third strain of research evaluates the ability of current automated systems to simulate the results of human word association experiments. Evaluation tasks based on associative relations originally captured attention of psychologists, such as Griffiths and Steyvers [29]. One such task was organized in the framework of the Cogalex workshop [30]. The participants received lists of five words (e.g. \u201ccircus\u201d, \u201cfunny\u201d, \u201cnose\u201d, \u201cfool\u201d, and \u201cCoco\u201d) and were supposed to select the word most closely associated to all of them. In this specific case, the word \u201cclown\u201d is the expected response. 2 000 sets of five input words, together with the expected target words (associative responses) were provided as a training set to participants. The test dataset contained another 2 000 sets of five input words. The training and the test datasets were both derived from the Edinburgh Associative Thesaurus (EAT) [31]. For each stimulus word, only the top five associations, i.e. the associations produced by the largest number of respondents, were retained, and all other associations were discarded. The AE dataset presented in Section 3.3 belongs to this group of evaluation datasets."}, {"heading": "3 Human Judgements about Semantic Relatedness", "text": "In this section, we describe three datasets designed for evaluation of Russian semantic relatedness measures. The datasets were tested in the framework of the shared task on RUssian Semantic Similarity Evaluation (RUSSE) [32].8 Each participant had to calculate similarities between a collection of word pairs. Then, each submission was assessed using the three benchmarks presented below, each being a subset of the input word pairs."}, {"heading": "3.1 HJ: Human Judgements of Word Pairs", "text": "Description of the Dataset. The HJ dataset is a union of three widely used benchmarks for English: RG, MC and WordSim353, see [14, 33\u201335, 35, 36] inter\n8 http://russe.nlpub.ru\nalia. The dataset contains 398 word pairs translated to Russian and re-annotated by native speakers. In addition to the complete dataset, we also provide separate parts that correspond to MC, RG and WordSim353.\nTo collect human judgements, an in-house crowdsourcing system was used. We set up a special section on the RUSSE website and asked volunteers on Facebook and Twitter to participate in the experiment. Each annotator received an assignment consisting of 15 word pairs randomly selected from the 398 pairs, and has been asked to assess the relatedness of each pair on the following scale: 0 \u2013 not related at all, 1 \u2013 weak relatedness, 2 \u2013 moderate relatedness, and 3 \u2013 high relatedness. We provided annotators with simple instructions explaining the procedure and goals of the study.9\nA pair of words was added to the annotation task with the probability inversely proportional to the number of current annotations. We obtained a total of 4 200 answers, i.e. 280 submissions of 15 judgements. Ordinal Krippendorff\u2019s alpha of 0.49 indicates a moderate agreement of annotators. The scores included in the HJ dataset are average human ratings scaled to the [0, 1] range.\nUsing the Dataset. To evaluate a relatedness measure using this dataset one should (1) calculate relatedness scores for each pair in the dataset; (2) calculate Spearman\u2019s rank correlation coefficient \u03c1 between the vector of human judgments and the scores of the system (see Table 4 for an example)."}, {"heading": "3.2 RT: Synonyms and Hypernyms", "text": "Description of the Dataset. This dataset follows the structure of the BLESS dataset [25]. Each target word has the same number of related and unrelated source words. The dataset contains 9 548 relations for 1 008 nouns (see Table 2). Half of these relations are synonyms and hypernyms from the RuThes-lite thesaurus [37] and half of them are unrelated words. To generate negative pairs we used the automatic procedure described in Panchenko et al. [32]. We filtered out false negative relations for 1 008 source words with the help of human annotators. Each negative relation in this subset was annotated by at least two annotators: Masters\u2019 students of an NLP course, native speakers of Russian.\nAs the result, we provide a dataset featuring 9 548 relations of 1 008 source words, where each source word has the same number of negative random relations and positive (synonymous or hypernymous) relations. In addition, we provide a larger dataset of 114 066 relations for 6 832 source words, where negative relations have not been verified manually.\nUsing the Dataset. To evaluate a similarity measure using this dataset one should (1) calculate relatedness scores for each pair in the dataset; (2) first sort\n9 Annotation guidelines for the HJ dataset: http://russe.nlpub.ru/task/annotate. txt\npairs by the score; and then (3) calculate the average precision metric:\nAveP =\n\u2211 r P (r)\nR ,\nwhere r is the rank of each non-random pair, R is the total number of nonrandom pairs, and P (r) is the precision of the top-r pairs. See Table 4 and [32] for examples. Besides, the dataset can be used to train classification models for predicting hypernyms and synonyms using the binary sij scores."}, {"heading": "3.3 AE: Cognitive Associations", "text": "Description of the Dataset. The structure of this dataset is the same as the structure of the RT dataset: each source word has the same number of related and unrelated target words. The difference is that, related word pairs of this dataset were sampled from a Russian web-based associative experiment.10 In the experiment, users were asked to provide a reaction to an input stimulus source word, e.g.: man \u2192 woman, time \u2192 money, and so on. The strength of association in this experiment is quantified by the number of respondents providing the same stimulus-reaction pair. Associative thesauri typically contain a mix of synonyms, hyponyms, meronyms and other types, making relations asymmetric. To build this dataset, we selected target words with the highest association with the stimulus in Sociation.org data. Like with the other datasets, we used only single-word nouns. Similarly to the RT dataset, we automatically generated negative word pairs and filtered out false negatives with help of annotators.\nAs the result, we provide a dataset featuring 3 002 relations of 340 source words (see Table 2), where each source word has the same number of negative random relations and positive associative relations. In addition, we provide the larger dataset of 86 772 relations for 5 796 source words, where negative relations were not verified manually.\nUsing the Dataset. Evaluation procedure using this dataset is the same as for the RT dataset: one should calculate the average precision AveP . Besides, the dataset can be used to train classification models for predicting associative relations using the binary sij scores."}, {"heading": "4 Machine Judgements about Semantic Relatedness", "text": ""}, {"heading": "4.1 MJ: Machine Judgements of Word Pairs", "text": "Description of the Dataset. This dataset contains 12 886 word pairs of 1 519 source words coming from HJ, RT, and AE datasets. Only 398 word pairs from the HJ dataset have continuous scores, while the other pairs which come from the RT and the AE datasets have binary relatedness scores. However, for training 10 The associations were sampled from the sociation.org database in July 2014.\nand evaluation purposes it is desirable to have continuous relatedness scores as they distinguish between the shades of relatedness. Yet, manual annotation of a big number of pairs is problematic: the largest dataset of this kind available to date, the MEN, contains 3 000 word pairs. Thus, unique feature of the MJ dataset is that it is at the same time large-scale, like BLESS, and has accurate continuous scores, like WordSim-353.\nTo estimate continuous relatedness scores with high confidence without any human judgements, we used 105 submissions of the shared task on Russian semantic similarity (RUSSE). We assumed that the top-scored systems can be used to bootstrap relatedness scores. Each run of the shared task consisted of 12 886 word pairs along with their relatedness scores. We used the following procedure to average these scores and construct the dataset:\n1. Select one best submission for each of the 19 participating teams for HJ, RT and AE datasets (total number of submissions is 105). 2. Rank the n = 19 best submissions according to their results in HJ, RT and AE: rk = n + 1 \u2212 k, where k is the place in the respective track. The best system obtains the rank r1 = 19; the worst one has the rank r19 = 1. 3. Combine scores of these 19 best submissions as follows: s\u2032ij = 1 n \u2211n k=1 \u03b1ks k ij ,\nwhere skij is the similarity between words (wi, wj) of the k-th submission; \u03b1k is the weight of the k-th submission. We considered three combination strategies each discounting differently teams with low ranks in the final evaluation. Thus the best teams impact more the combined score. In the first strategy, the \u03b1k weight is the rank rk. In the second strategy, the \u03b1k equals exponent of this rank: exp(rk). Finally, in the third strategy, the weight equals to the square root of rank: \u221a rk. We tried to use AveP and \u03c1 as weights, but this\ndid not lead to better fit. 4. Union pairs (wi, wj , s\u2032ij) of HJ, RT and AE datasets into the MJ dataset.\nTable 1 presents example of the relatedness scores obtained using this procedure.\nEvaluation of the Dataset. Combination of the submissions using any of the three methods yields relatedness scores that outperforms all single submissions of the shared task (see Table 3). Note that ranks of the systems were obtained using the HJ, RT and AE datasets. Thus we can only claim that MJ provides continuous relatedness scores that fit well to the binary scores. Among the three weightings, using inverse ranks provides the top scores on the HJ and the AE datasets and the second best scores on the RT dataset. Thus, we selected this strategy to generate the released dataset.\nUsing the Dataset. To evaluate a relatedness measure using the MJ dataset, one should (1) calculate relatedness scores for each pair in the dataset; (2) calculate Spearman\u2019s rank correlation \u03c1 between the vector of machine judgments and the scores of the evaluated system. Besides, the dataset can be used to train regression models for predicting semantic relatedness using the continuous sij scores."}, {"heading": "4.2 RDT: Russian Distributional Thesaurus", "text": "While four resources presented above are accurate and represent different types of semantic relations, their coverage (222 \u2013 1 519 source words) makes them best suited for evaluation and training purposes. In this section, we present a large-scale resource in the same (wi, wj , sij) format, the first open Russian distributional thesaurus. This resource, thanks to its coverage of 932 896 target words can be directly used in NLP systems.\nDescription of the Dataset. In order to build the distributional thesaurus, we used the Skip-gram model [38] trained on a 12.9 billion word collection of Russian texts extracted from the digital library lib.rus.ec. According to the results of the shared task on Russian semantic relatedness [32,39], this approach scored in the top 5 among 105 submissions, obtaining different ranks depending on the evaluation dataset. At the same time, this method is completely unsupervised and language independent as we do not use any preprocessing except tokenization, in contrast to other top-ranked methods e.g. [40] who used extra linguistic resources, such as dictionaries.\nFollowing our prior experiments [39], we selected the following parameters of the model: minimal word frequency \u2013 5, number of dimensions in a word vector \u2013 500, three or five iterations of the learning algorithm over the input corpus, context window size of 1, 2, 3, 5, 7 and 10 words. We calculated 250 nearest neighbours using the cosine similarity between word vectors for the 1.1 million of the most frequent tokens. Next we filtered all tokens with non-Cyrillic symbols which provided us a resource featuring 932 896 source words. In addition to the raw tokens we provide a lemmatized version based on the PyMorphy2 morphological analyzer [41]. We performed no part of speech filtering as it can be trivially performed if needed.\nFig. 1 visualizes top 20 nearest neighbours of the word \u201c\u0444\u0438\u0437\u0438\u043a\u0430\u201d (physics) from the RDT. One can observe three groups of related words: morphological variants (e.g. \u201c\u0444\u0438\u0437\u0438\u043a\u0435\u201d, \u201c\u0444\u0438\u0437\u0438\u043a\u0443\u201d), physical terms, e.g. \u201c\u043a\u0432\u0430\u043d\u0442\u043e\u0432\u0430\u044f\u201d (quantum) and \u201c\u0442\u0435\u0440\u043c\u043e\u0434\u0438\u043d\u0430\u043c\u0438\u043a\u0430\u201d (thermodynamics) and names of other scientific disciplines, e.g. \u201c\u043c\u0430\u0442\u0435\u043c\u0430\u0442\u0438\u043a\u0430\u201d (mathematics), \u201c\u0445\u0438\u043c\u0438\u044f\u201d (chemistry). Note that the thesaurus contains both raw tokens as displayed in Fig. 1 and lemmatized neighbours.\nAn important added value of our work is engineering. While our approach is straightforward, training a large-scale Skip-gram model on a 12.9 billion tokens corpus with three iterations over a corpus takes up to five days on a r3.8xlarge Amazon EC2 instance featuring 32 CPU cores and 244 GB of RAM. Furthermore, computation of the neighbours takes up to a week for only one model using the large 500 dimensional vectors, not to mention the time needed to test different configurations of the model. Besides, to use the word embeddings directly, one needs to load more than seven millions of the 500 dimensional vectors, which is only possible on a similar instance to r3.8xlarge. On the other hand, the resulting RDT resource is a CSV file that can be easily indexed in an RDBMS system or an succinct in-memory data structure and subsequently efficiently used in most NLP systems. However, we also provide the original word vectors for non-standard use-cases.\nEvaluation. We evaluated the quality of the distributional thesaurus using the HJ, RT and AE datasets presented above. Furthermore, we estimated precision of extracted relations for 100 words randomly sampled from the vocabulary of the HJ dataset. For each word we extracted the top 20 similar words according to each model under evaluation resulting in 4 127 unique word pairs. Each pair was annotated by three distinct annotators with a binary choice as opposed to a graded judgement, i.e. an annotator was supposed to indicate if a given word pair is plausibly related or not.11 In this experiment, we used an open source crowdsourcing engine [42].12 Judgements were aggregated using a majority vote. In total, 395 Russian-speaking volunteers participated in our crowdsourcing experiment with the substantial inter-rater agreement of 0.47 in terms of Krippendorff\u2019s alpha. The dataset obtained as a result of this crowdsourcing is publicly available (see download link below).\nDiscussion of the Results. Evaluation of different configurations of the distributional thesaurus are presented in Table 4 and Fig. 2. The model trained on the full 12.9 billion tokens corpus with context window size 10 outperforms other models according to HJ, RT, AE and precision at 20 metrics. We used this model to generate the thesaurus presented in Table 2. However, the model trained on the 2.5 billion tokens sample of the full lib.rus.ec corpus (20% of the full corpus) yields very similar results in terms of precision. Yet, this model show slightly lower results according to other benchmarks. Models based on other context window sizes yield lower results as compared to these trained using the context window size 10 (see Fig. 2)."}, {"heading": "5 Conclusion", "text": "In this paper, we presented five new language resources for the Russian language, which can be used for training and evaluating semantic relatedness measures, 11 Annotation guidelines are available at http://crowd.russe.nlpub.ru. 12 http://mtsar.nlpub.org\nand to create NLP applications requiring semantic relatedness. These resources were used to perform a large-scale evaluation of 105 submissions in a shared task on Russian semantic relatedness. One of the best systems identified in this evaluation campaign was used to generate the first open Russian distributional thesaurus. Manual evaluation of this thesaurus, based on a large-scale crowdsourcing with native speakers, showed a precision of 0.94 on the top 10 similar words. All introduced resources are freely available for download.13 Finally, the methodology for bootstrapping datasets for semantic relatedness presented in this paper can help to construct similar resources in other languages.\n13 http://russe.nlpub.ru/downloads\nAcknowledgements. We would like to acknowledge several funding organisations that partially supported this research. Dmitry Ustalov was supported by the Russian Foundation for Basic Research (RFBR) according to the research project no. 16-37-00354 \u043c\u043e\u043b_\u0430. Denis Paperno was supported by the European Research Council (ERC) 2011 Starting Independent Research Grant no. 283554 (COMPOSES). Natalia Loukachevitch was supported by Russian Foundation for Humanities (RFH), grant no. 15-04-12017. Alexander Panchenko was supported by the Deutsche Forschungsgemeinschaft (DFG) under the project \u201cJoining Ontologies and Semantics Induced from Text (JOIN-T)\u201d."}], "references": [{"title": "Evaluating WordNet-based Measures of Lexical Semantic Relatedness", "author": ["A. Budanitsky", "G. Hirst"], "venue": "Computational Linguistics 32(1)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Measures of semantic similarity and relatedness in the biomedical domain", "author": ["T. Pedersen", "S.V. Pakhomov", "S. Patwardhan", "C.G. Chute"], "venue": "Journal of Biomedical Informatics 40(3)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence. IJCAI\u201907, Morgan Kaufmann Publishers Inc.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "An ontology-based measure to compute semantic similarity in biomedicine", "author": ["M. Batet", "D. S\u00e1nchez", "A. Valls"], "venue": "Journal of Biomedical Informatics 44(1)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures", "author": ["D. B\u00e4r", "C. Biemann", "I. Gurevych", "T. Zesch"], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. SemEval \u201912, Association for Computational Linguistics", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Text Relatedness Based on a Word Thesaurus", "author": ["G. Tsatsaronis", "I. Varlamis", "M. Vazirgiannis"], "venue": "Journal of Artificial Intelligence Research 37(1)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Using Measures of Semantic Relatedness for Word Sense Disambiguation", "author": ["S. Patwardhan", "S. Banerjee", "T. Pedersen"], "venue": "Proceedings of the 4th International Conference on Computational Linguistics and Intelligent Text Processing. Springer Berlin Heidelberg", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Query Expansion with ConceptNet and WordNet: An Intrinsic Comparison", "author": ["M.H. Hsu", "M.F. Tsai", "H.H. Chen"], "venue": "Information Retrieval Technology: Third Asia Information Retrieval Symposium, AIRS 2006, Singapore, October 16-18, 2006. Proceedings. Springer Berlin Heidelberg", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Similarity Measures for Semantic Relation Extraction", "author": ["A. Panchenko"], "venue": "PhD thesis, UCLouvain", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "WordNet: A Lexical Database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM 38(11)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Contextual correlates of synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Communications of the ACM 8(10)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1965}, {"title": "Contextual correlates of semantic similarity", "author": ["G.A. Miller", "W.G. Charles"], "venue": "Language and Cognitive Processes 6(1)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "Placing Search in Context: The Concept Revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "Proceedings of the 10th International Conference on World Wide Web. WWW \u201901, ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches", "author": ["E. Agirre", "E. Alfonseca", "K. Hall", "J. Kravalova", "M. Pa\u015fca", "A. Soroa"], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics. NAACL \u201909, Association for Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Using the Structure of a Conceptual Network in Computing Semantic Relatedness", "author": ["I. Gurevych"], "venue": "Natural Language Processing \u2013 IJCNLP 2005: Second International Joint Conference, Jeju Island, Korea, October 11-13, 2005. Proceedings. Springer Berlin Heidelberg", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge", "author": ["S. Hassan", "R. Mihalcea"], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3. EMNLP \u201909, Association for Computational Linguistics", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "What implementation and translation teach us: the case of semantic similarity measures in wordnets", "author": ["M. Postma", "P. Vossen"], "venue": "Proceedings of the Seventh Global Wordnet Conference.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Semeval-2012 task 4: Evaluating chinese word similarity", "author": ["P. Jin", "Y. Wu"], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation. SemEval \u201912, Association for Computational Linguistics", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Verb Similarity on the Taxonomy of WordNet", "author": ["D. Yang", "D.M.W. Powers"], "venue": "Proceedings of the Third International WordNet Conference \u2014 GWC 2006, Masaryk University", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "To Exhibit is not to Loiter: A Multilingual, SenseDisambiguated Wiktionary for Measuring Verb Similarity", "author": ["C.M. Meyer", "I. Gurevych"], "venue": "Proceedings of COLING 2012: Technical Papers, The COLING 2012 Organizing Committee", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "SimLex-999: Evaluating Semantic Models With (Genuine) Similarity Estimation", "author": ["F. Hill", "R. Reichart", "A. Korhonen"], "venue": "Computational Linguistics 41(4)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal Distributional Semantics", "author": ["E. Bruni", "N.K. Tran", "M. Baroni"], "venue": "Journal of Artificial Intelligence Research 49(1)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Introducing and evaluating ukWaC, a very large Web-derived corpus of English", "author": ["A. Ferraresi", "E. Zanchetta", "S. Bernardini", "M. Baroni"], "venue": "Proceedings of the 4th Web as Corpus Workshop (WAC-4): Can we beat Google?", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Community Evaluation and Exchange of Word Vectors at wordvectors.org. In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations", "author": ["M. Faruqui", "C. Dyer"], "venue": "Association for Computational Linguistics", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "How We BLESSed Distributional Semantic Evaluation", "author": ["M. Baroni", "A. Lenci"], "venue": "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics. GEMS \u201911, Association for Computational Linguistics", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Mining for Meaning: The Extraction of Lexicosemantic Knowledge from Text", "author": ["T. Van de Cruys"], "venue": "PhD thesis, University of Groningen", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Text: Now in 2D! a framework for lexical expansion with contextual similarity", "author": ["C. Biemann", "M. Riedl"], "venue": "Journal of Language Modelling 1(1)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces", "author": ["M. Sahlgren"], "venue": "PhD thesis, Stockholm University", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Prediction and Semantic Association", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Advances in Neural Information Processing Systems 15. MIT Press", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "The CogALex-IV Shared Task on the Lexical Access Problem", "author": ["R. Rapp", "M. Zock"], "venue": "Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon (CogALex), Association for Computational Linguistics and Dublin City University", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "An associative thesaurus of English and its computer analysis", "author": ["G.R. Kiss", "C. Armstrong", "R. Milroy", "J. Piper"], "venue": "The Computer and Literary Studies. Edinburgh University Press", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1973}, {"title": "RUSSE: The First Workshop on Russian Semantic Similarity", "author": ["A. Panchenko", "N.V. Loukachevitch", "D. Ustalov", "D. Paperno", "C.M. Meyer", "N. Konstantinova"], "venue": "Computational Linguistics and Intellectual Technologies: papers from the Annual conference \u201cDialogue\u201d. Volume 2. RGGU", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Using Information Content to Evaluate Semantic Similarity in a Taxonomy", "author": ["P. Resnik"], "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence - Volume 1. IJCAI\u201995, Morgan Kaufmann Publishers Inc.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1995}, {"title": "An Information-Theoretic Definition of Similarity", "author": ["D. Lin"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning. ICML \u201998, Morgan Kaufmann Publishers Inc.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}, {"title": "Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts", "author": ["S. Patwardhan", "T. Pedersen"], "venue": "Proceedings of the Workshop on Making Sense of Sense: Bringing Psycholinguistics and Computational Linguistics Together, Association for Computational Linguistics", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Using Wiktionary for Computing Semantic Relatedness", "author": ["T. Zesch", "C. M\u00fcller", "I. Gurevych"], "venue": "Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 2. AAAI\u201908, AAAI Press", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2008}, {"title": "RuThes-Lite, a publicly available version of Thesaurus of Russian language RuThes", "author": ["N.V. Loukachevitch", "B.V. Dobrov", "I.I. Chetviorkin"], "venue": "Computational Linguistics and Intellectual Technologies: papers from the Annual conference \u201cDialogue\u201d, RGGU", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems 26. Curran Associates, Inc.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "Evaluating Three Corpus-Based Semantic Similarity Systems for Russian", "author": ["N. Arefyev", "A. Panchenko", "A. Lukanin", "O. Lesota", "P. Romanov"], "venue": "Computational Linguistics and Intellectual Technologies: papers from the Annual conference \u201cDialogue\u201d. Volume 2. RGGU", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "The Impact of Different Vector Space Models and Supplementary Techniques on Russian Semantic Similarity Task", "author": ["K.A. Lopukhin", "A.A. Lopukhina", "G.V. Nosyrev"], "venue": "Computational Linguistics and Intellectual Technologies: Papers from the Annual conference \u201cDialogue\u201d. Volume 2. RGGU", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Morphological Analyzer and Generator for Russian and Ukrainian Languages", "author": ["M. Korobov"], "venue": "Analysis of Images, Social Networks and Texts: 4th International Conference, AIST 2015, Revised Selected Papers. Springer International Publishing", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "A Crowdsourcing Engine for Mechanized Labor", "author": ["D. Ustalov"], "venue": "Proceedings of the Institute for System Programming 27(3)", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Semantic relatedness and semantic similarity have been extensively studied in psychology and computational linguistics, see [1\u20134] inter alia.", "startOffset": 124, "endOffset": 129}, {"referenceID": 1, "context": "Semantic relatedness and semantic similarity have been extensively studied in psychology and computational linguistics, see [1\u20134] inter alia.", "startOffset": 124, "endOffset": 129}, {"referenceID": 2, "context": "Semantic relatedness and semantic similarity have been extensively studied in psychology and computational linguistics, see [1\u20134] inter alia.", "startOffset": 124, "endOffset": 129}, {"referenceID": 3, "context": "Semantic relatedness and semantic similarity have been extensively studied in psychology and computational linguistics, see [1\u20134] inter alia.", "startOffset": 124, "endOffset": 129}, {"referenceID": 4, "context": "Semantic relatedness is an important building block of NLP techniques, such as text similarity [5, 6], word sense disambiguation [7], query expansion [8] and some others [9].", "startOffset": 95, "endOffset": 101}, {"referenceID": 5, "context": "Semantic relatedness is an important building block of NLP techniques, such as text similarity [5, 6], word sense disambiguation [7], query expansion [8] and some others [9].", "startOffset": 95, "endOffset": 101}, {"referenceID": 6, "context": "Semantic relatedness is an important building block of NLP techniques, such as text similarity [5, 6], word sense disambiguation [7], query expansion [8] and some others [9].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Semantic relatedness is an important building block of NLP techniques, such as text similarity [5, 6], word sense disambiguation [7], query expansion [8] and some others [9].", "startOffset": 150, "endOffset": 153}, {"referenceID": 8, "context": "Semantic relatedness is an important building block of NLP techniques, such as text similarity [5, 6], word sense disambiguation [7], query expansion [8] and some others [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 9, "context": "There are three main approaches to evaluating semantic relatedness: using human judgements about word pairs, using semantic relations from lexical-semantic resources, such as WordNet [10], and using data from cognitive word association experiments.", "startOffset": 183, "endOffset": 187}, {"referenceID": 10, "context": "Research on relatedness starts from the pioneering work of Rubenstein and Goodenough [11], where they aggregated human judgments on the relatedness of 65 noun pairs into the RG dataset.", "startOffset": 85, "endOffset": 89}, {"referenceID": 11, "context": "Later, Miller and Charles [12] replicated the experiment of Rubenstein and Goodenough, obtaining similar results on a subset of 30 noun pairs.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "[13] as the WordSim353 dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] subdivided the WordSim353 dataset into two subsets: the WordSim353 similarity set and the WordSim353 relatedness set.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Gurevych translated the RG and MC datasets into German [15]; Hassan and Mihalcea translated them into Spanish, Arabic and Romanian [16]; Postma and Vossen [17] translated the datasets into Dutch; Jin and Wu [18] presented a shared task for Chinese semantic similarity, where the authors translated the WordSim353 dataset.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Gurevych translated the RG and MC datasets into German [15]; Hassan and Mihalcea translated them into Spanish, Arabic and Romanian [16]; Postma and Vossen [17] translated the datasets into Dutch; Jin and Wu [18] presented a shared task for Chinese semantic similarity, where the authors translated the WordSim353 dataset.", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "Gurevych translated the RG and MC datasets into German [15]; Hassan and Mihalcea translated them into Spanish, Arabic and Romanian [16]; Postma and Vossen [17] translated the datasets into Dutch; Jin and Wu [18] presented a shared task for Chinese semantic similarity, where the authors translated the WordSim353 dataset.", "startOffset": 155, "endOffset": 159}, {"referenceID": 17, "context": "Gurevych translated the RG and MC datasets into German [15]; Hassan and Mihalcea translated them into Spanish, Arabic and Romanian [16]; Postma and Vossen [17] translated the datasets into Dutch; Jin and Wu [18] presented a shared task for Chinese semantic similarity, where the authors translated the WordSim353 dataset.", "startOffset": 207, "endOffset": 211}, {"referenceID": 18, "context": "Yang and Powers [19] proposed a dataset specifically for measuring verb similarity, which was later translated into German by Meyer and Gurevych [20].", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "Yang and Powers [19] proposed a dataset specifically for measuring verb similarity, which was later translated into German by Meyer and Gurevych [20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 15, "context": "Hassan and Mihalcea [16] and Postma and Vossen [17] used three stages to translation pairs: (1) disambiguation of the English word forms; (2) translation for each word; (3) ensuring that translations are in the same class of relative frequency as the English source word.", "startOffset": 20, "endOffset": 24}, {"referenceID": 16, "context": "Hassan and Mihalcea [16] and Postma and Vossen [17] used three stages to translation pairs: (1) disambiguation of the English word forms; (2) translation for each word; (3) ensuring that translations are in the same class of relative frequency as the English source word.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "[21], focusing specifically on similarity and not relatedness.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The creators of the MEN dataset [22] went even further, annotating via crowdsourcing 3 000 word pairs sampled from the ukWaC corpus [23].", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "The creators of the MEN dataset [22] went even further, annotating via crowdsourcing 3 000 word pairs sampled from the ukWaC corpus [23].", "startOffset": 132, "endOffset": 136}, {"referenceID": 23, "context": "A comprehensive list of datasets for evaluation of English semantic relatedness, featuring 12 collections, was gathered by Faruqui and Dyer [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 24, "context": "Baroni and Lenci [25] stressed that semantically related words differ in the type of relation between them, so they generated the BLESS dataset containing tuples of the form (wj , wj , type).", "startOffset": 17, "endOffset": 21}, {"referenceID": 25, "context": "Van de Cruys [26] used Dutch WordNet to evaluate distributional similarity measures.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Biemann and Riedl [27] follow a similar approach based on the English WordNet to assess quality of their distributional semantics framework.", "startOffset": 18, "endOffset": 22}, {"referenceID": 27, "context": "Finally, Sahlgren [28] evaluated distributional lexical similarity measures comparing them to manually-crafted thesauri, but also associative norms, such as those described in the following section.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "Evaluation tasks based on associative relations originally captured attention of psychologists, such as Griffiths and Steyvers [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 29, "context": "One such task was organized in the framework of the Cogalex workshop [30].", "startOffset": 69, "endOffset": 73}, {"referenceID": 30, "context": "The training and the test datasets were both derived from the Edinburgh Associative Thesaurus (EAT) [31].", "startOffset": 100, "endOffset": 104}, {"referenceID": 31, "context": "The datasets were tested in the framework of the shared task on RUssian Semantic Similarity Evaluation (RUSSE) [32].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "The HJ dataset is a union of three widely used benchmarks for English: RG, MC and WordSim353, see [14, 33\u201335, 35, 36] inter", "startOffset": 98, "endOffset": 117}, {"referenceID": 32, "context": "The HJ dataset is a union of three widely used benchmarks for English: RG, MC and WordSim353, see [14, 33\u201335, 35, 36] inter", "startOffset": 98, "endOffset": 117}, {"referenceID": 33, "context": "The HJ dataset is a union of three widely used benchmarks for English: RG, MC and WordSim353, see [14, 33\u201335, 35, 36] inter", "startOffset": 98, "endOffset": 117}, {"referenceID": 34, "context": "The HJ dataset is a union of three widely used benchmarks for English: RG, MC and WordSim353, see [14, 33\u201335, 35, 36] inter", "startOffset": 98, "endOffset": 117}, {"referenceID": 34, "context": "The HJ dataset is a union of three widely used benchmarks for English: RG, MC and WordSim353, see [14, 33\u201335, 35, 36] inter", "startOffset": 98, "endOffset": 117}, {"referenceID": 35, "context": "The HJ dataset is a union of three widely used benchmarks for English: RG, MC and WordSim353, see [14, 33\u201335, 35, 36] inter", "startOffset": 98, "endOffset": 117}, {"referenceID": 0, "context": "The scores included in the HJ dataset are average human ratings scaled to the [0, 1] range.", "startOffset": 78, "endOffset": 84}, {"referenceID": 24, "context": "This dataset follows the structure of the BLESS dataset [25].", "startOffset": 56, "endOffset": 60}, {"referenceID": 36, "context": "Half of these relations are synonyms and hypernyms from the RuThes-lite thesaurus [37] and half of them are unrelated words.", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "See Table 4 and [32] for examples.", "startOffset": 16, "endOffset": 20}, {"referenceID": 37, "context": "In order to build the distributional thesaurus, we used the Skip-gram model [38] trained on a 12.", "startOffset": 76, "endOffset": 80}, {"referenceID": 31, "context": "According to the results of the shared task on Russian semantic relatedness [32,39], this approach scored in the top 5 among 105 submissions, obtaining different ranks depending on the evaluation dataset.", "startOffset": 76, "endOffset": 83}, {"referenceID": 38, "context": "According to the results of the shared task on Russian semantic relatedness [32,39], this approach scored in the top 5 among 105 submissions, obtaining different ranks depending on the evaluation dataset.", "startOffset": 76, "endOffset": 83}, {"referenceID": 39, "context": "[40] who used extra linguistic resources, such as dictionaries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "Following our prior experiments [39], we selected the following parameters of the model: minimal word frequency \u2013 5, number of dimensions in a word vector \u2013 500, three or five iterations of the learning algorithm over the input corpus, context window size of 1, 2, 3, 5, 7 and 10 words.", "startOffset": 32, "endOffset": 36}, {"referenceID": 40, "context": "In addition to the raw tokens we provide a lemmatized version based on the PyMorphy2 morphological analyzer [41].", "startOffset": 108, "endOffset": 112}, {"referenceID": 41, "context": "11 In this experiment, we used an open source crowdsourcing engine [42].", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "5-rt-3 [40] \u2013 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 31, "context": "975 \u2013 \u2013 \u2013 \u2013 9-ae-9 [32] \u2013 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "952 \u2013 \u2013 \u2013 \u2013 9-ae-6 [32] \u2013 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "965 \u2013 \u2013 \u2013 \u2013 17-rt-1 [32] \u2013 0.", "startOffset": 20, "endOffset": 24}], "year": 2017, "abstractText": "Semantic relatedness of terms represents similarity of meaning by a numerical score. On the one hand, humans easily make judgements about semantic relatedness. On the other hand, this kind of information is useful in language processing systems. While semantic relatedness has been extensively studied for English using numerous language resources, such as associative norms, human judgements and datasets generated from lexical databases, no evaluation resources of this kind have been available for Russian to date. Our contribution addresses this problem. We present five language resources of different scale and purpose for Russian semantic relatedness, each being a list of triples (wordi,wordj , similarityij). Four of them are designed for evaluation of systems for computing semantic relatedness, complementing each other in terms of the semantic relation type they represent. These benchmarks were used to organise a shared task on Russian semantic relatedness, which attracted 19 teams. We use one of the best approaches identified in this competition to generate the fifth high-coverage resource, the first open distributional thesaurus of Russian. Multiple evaluations of this thesaurus, including a large-scale crowdsourcing study involving native speakers, indicate its high accuracy.", "creator": "LaTeX with hyperref package"}}}