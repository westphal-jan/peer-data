{"id": "1709.00541", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Patterns versus Characters in Subword-aware Neural Language Modeling", "abstract": "Words in some natural languages can have a composite structure. Elements of this structure include the root (that could also be composite), prefixes and suffixes with which various nuances and relations to other words can be expressed. Thus, in order to build a proper word representation one must take into account its internal structure. From a corpus of texts we extract a set of frequent subwords and from the latter set we select patterns, i.e. subwords which encapsulate information on character $n$-gram regularities. The selection is made using the pattern-based Conditional Random Field model with $l_1$ regularization. Further, for every word we construct a new sequence over an alphabet of patterns. The new alphabet's symbols confine a local statistical context stronger than the characters, therefore they allow better representations in ${\\mathbb{R}}^n$ and are better building blocks for word representation. In the task of subword-aware language modeling, pattern-based models outperform character-based analogues by 2-20 perplexity points. Also, a recurrent neural network in which a word is represented as a sum of embeddings of its patterns is on par with a competitive and significantly more sophisticated character-based convolutional architecture.", "histories": [["v1", "Sat, 2 Sep 2017 07:00:22 GMT  (151kb,D)", "http://arxiv.org/abs/1709.00541v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["rustem takhanov", "zhenisbek assylbekov"], "accepted": false, "id": "1709.00541"}, "pdf": {"name": "1709.00541.pdf", "metadata": {"source": "CRF", "title": "Patterns versus Characters in Subword-aware Neural Language Modeling", "authors": ["Rustem Takhanov"], "emails": ["zhassylbekov}@nu.edu.kz"], "sections": [{"heading": null, "text": "Keywords: subword-aware language modeling, pattern-based conditional random field, word representation, deep learning"}, {"heading": "1 Introduction", "text": "The goal of natural language modeling is, given a corpus of texts from a certain language, to build a probabilistic distribution over all possible sequences of words/sentences. Historically, first approaches to the problem [16,4] were highly interpretable, involving syntax and morphology, i.e. the internal structure of such models was of interest even to linguists. Nowadays the best performance is achieved by the so called recurrent neural network language models (RNNLM), which unfortunately lack the desired properties of interpretability.\nFor rich-resource languages the amount of training data, i.e. a corpus of texts, is bounded only by the computational power of the language modeling method. Due to this, most of RNNLM methods treat text as a sequence of token identifiers, where a token corresponds to either a word, or punctuation mark. Indeed, if any word appears in a text in various different contexts, a\nar X\niv :1\n70 9.\n00 54\n1v 1\n[ cs\n.C L\n] 2\nS ep\n2 01\n7\nmethod can learn high quality word representation without taking into account its morphology. This logics fails when a corpus of texts is not large enough, and the problem is aggravated for morphology-rich languages, such as, e.g., turkic or finno-ugric languages. Thus, the problem of word representation that would take into account an internal structure of a word becomes very actual \u2014 recent advances in language modeling are connected with treating words as sequences of characters or other subword units.\nMuch research has been done on character-level neural language modeling [15,6,11,9,10,20]. However, not much work exploits character n-grams that occur in a word. In [17] a word is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding; the word embeddings are then fed into neural machine translation models. In [22] a very similar technique is used and an evaluation on three other tasks (word similarity, sentence similarity, and part-of-speech tagging) is performed; they demonstrate that their method outperforms more complex architectures based on character-level recurrent and convolutional neural networks. Probably closest to ours is an approach from [2] where a word representation is a sum of terms, each term corresponding to a certain n-gram that occurs in that word. One weekness of the mentioned approaches is that all possible n-grams that occur in a corpus of texts are present there in an a priori equal way, and a difference in their value for word representation is calculated in the process of learning. Whereas we in advance select a subset of n-grams that could potentially enrich word vectors by subword information. For this purpose we use the pattern-based Conditional Random Field with l1 regularization.\nOur approach also differs in the following aspects: we (i) replace each character by a new symbol which in some way concentrates an information on previous characters, (ii) experiment with several ways of combining subword embeddings to produce word embeddings, and (iii) evaluate our methods on a ubiquitous language modeling task."}, {"heading": "2 A new alphabet for words", "text": "Throughout the paper, we will use the following notation: if X is an alphabet, then X \u2217 denotes a set of words over X ; for \u03b1, \u03b2 \u2208 X \u2217, \u03b1\u03b2 denotes the concatenation of \u03b1 and \u03b2; by \u2217 we denote an arbitrary word.\nThe key trick that we use in this paper is replacing a word a1a2 \u00b7 \u00b7 \u00b7 ak (that occurs in some context) over the initial alphabet A with a word s1s2 \u00b7 \u00b7 \u00b7 sk over a new alphabet of states S. Let us describe this substitution. We first define a finite state machine (A,S, \u03b4, s0), where s0 is an initial state and \u03b4 : S \u00d7 A \u2192 S is a state-transition function. If we are given a sentence \u03b1 = b1b2 \u00b7 \u00b7 \u00b7 bK such that every bi is a character symbol from A (it could be a punctuation mark, i.e. a symbol that marks a boundary between words) our state machine reads this sentence and produces a sequence of states: s0s1 \u00b7 \u00b7 \u00b7 sK . In the latter sequence, every si corresponds to a state of our machine after reading a symbol bi. Thus, every subsequence bibi+1 \u00b7 \u00b7 \u00b7 bj of the initial sentence \u03b1 corresponds\nto a subsequence sisi+1 \u00b7 \u00b7 \u00b7 sj where 1 \u2264 i \u2264 j \u2264 K. Therefore, if bibi+1 \u00b7 \u00b7 \u00b7 bj corresponds to a word in a sentence \u03b1, then we will substitute it with sisi+1 \u00b7 \u00b7 \u00b7 sj .\nThus, given such a finite state machine, every word of a sentence can be rewritten over another alphabet S. Let us describe now our finite state machine.\nSuppose that after an analysis of a training set, i.e. of a corpus of texts from our language L, we extract a certain finite set of sequences \u03a00 \u2286 A\u2217 that we assume not only to be frequent, but in some way statistically characterising our language. A specific way of choosing \u03a00 will be given in the following subsection. Any element \u03c0 \u2208 \u03a00 we call a pattern. Any such set defines a set of states S = {\u03b2| \u2203\n\u03c0\u2208\u03a00 \u03c0 = \u03b2\u2217}, which is, in fact, a set of all prefixes of patterns. We\nassume that an empty word \u03b5 is also in S and define s0 = \u03b5. Now we have to define a state-transition function \u03b4. Our idea is to construct it in such a way that after reading the first l symbols of the sentence b1b2 \u00b7 \u00b7 \u00b7 bl the machine should be in a state sl \u2208 S where sl is the longest word from S for which b1b2 \u00b7 \u00b7 \u00b7 bl = \u2217sl (Figure 1). The latter decription induces the following definition: for any \u03b1 \u2208 S and a \u2208 A, \u03b4(\u03b1, a) is the longest word \u03b2 \u2208 S for which \u03b1a = \u2217\u03b2.\nPatterns\nIn this subsection we will describe how we extract a set of patterns \u03a00 from a corpus of texts (Figure 2). By a corpus of texts we understand a training set T = {\u03b11, \u00b7 \u00b7 \u00b7 , \u03b1L} \u2286 A\u2217 where \u03b1i is a sentence from our language L.\nFirst we extract from our training set T a set of patterns \u03a0 \u2032 based on the following simple procedure: we fix in advance a threshold f and put to T only those words \u03b1 \u2208 A\u2217 that occur in T in more than f places. Then we apply a reduction procedure, i.e. if a) \u03b1 is a subword of \u03b2, b) \u03b1 and \u03b2 always occur together in T , then we delete \u03b1 from \u03a0 \u2032. A pattern-based conditional random field model for our language is the following probability distribution over A\u2217 [23,19]:\nPr(b1 \u00b7 \u00b7 \u00b7 bK) = A \u00b7 e\u2212E(b1\u00b7\u00b7\u00b7bK),\nwhere E(b1 \u00b7 \u00b7 \u00b7 bK) = \u2211 \u03b1\u2208\u03a0\u2032 \u2211 i<j:bi\u00b7\u00b7\u00b7bj=\u03b1 c\n\u03b1, and c\u03b1, \u03b1 \u2208 \u03a0 \u2032, are parameters to be learned from T .\nThe learning is done by the minimization of the negative log-likelihood with L1-regularization:\n\u2212 \u2211L i=1 log Pr(\u03b1i) + C \u2211 \u03b1\u2208\u03a0\u2032 |c\u03b1|. (1)\nThe latter function is convex, an efficient computation of its value and gradient is described in [19]. For the optimization we used the Limited-memory BroydenFletcher-Goldfarb-Shanno (L-BFGS) method written by Jorge Nocedal. Via the parameter C one can manage the number of patterns \u03b1 \u2208 \u03a0 \u2032 for which c\u03b1 6= 0. Finally, we define \u03a00 = {\u03b1 \u2208 \u03a0 \u2032|c\u03b1 6= 0}."}, {"heading": "3 Subword-aware neural language model", "text": "In what follows, both regular characters and patterns are referred to as subwords. The overall architecture of the subword-aware neural language model is displayed in Figure 3.\nIt consists of three main parts: (i) subword-based word embedding model, (ii) word-level recurrent neural network language model (RNNLM), and (iii) softmax layer. Below we describe each part in more detail. Subword-based word embeddings: A word w \u2208 W (in a sentence) is defined by the sequence of its subwords s1 . . . snw \u2208 X \u2217 (X = A in the case of characterbased representation, and X = S in our pattern-based approach), and each state is embedded into dX -dimensional space via an embedding matrix E in X \u2208 R|X|\u00d7dX to obtain a sequence of state vectors:\ns1, . . . , snw . (2)\nThen we try three different methods to get an embedding of the word w:\n\u2013 Concat: A simple concatenation of state vectors (2) into a single word vector:\nw = [s1; s2; . . . ; snw ;0;0; . . . ;0\ufe38 \ufe37\ufe37 \ufe38 n\u2212nw ].\nWe either truncate (if w consists of more than n symbols) or zero-pad w so that all word vectors have the same length n \u00b7 dX to allow batch processing. This approach is motivated by a desire to keep all the information regarding subwords, including the order in which they appear in the word.\n\u2013 Sum: A summation of subword vectors: w = \u2211nw t=1 st. (3)\nThis approach was used by [3] to combine a word and its morpheme embeddings into a single word vector.\n\u2013 CNN: A convolutional model of [9]:\nw = CNN(s1, . . . , snw).\nThis method has already demonstrated excellent performance for characterlevel inputs, therefore we decided to apply it to patterns as well.\nTo model interactions between subwords, we feed the resulting word embedding w into a stack of two highway layers [18] with dimensionality dHW per layer. In cases when dimensionality of w does not match dHW, we project it into RdHW . Word-level RNNLM: Once we have embeddings w1:k for a sequence of words w1:k, we can use a word-level RNN language model to produce a sequence of states h1:k \u2208 RdLM according to\nht = RNNCell(wt,ht\u22121), h0 = 0.\nThere is a big variety of RNN cells to choose from. The most advanced recurrent neural architectures, at the time of this writing, are RHN [25] and NAS [26]. However, to make our results directly comparable to the previous work of [9] on character-level language modeling we select a more conventional architecture \u2013 a stack of two LSTM cells [8].\nSoftmax: The last state hk from (4) is further used to predict the next word wk+1 according to the probability distribution\nPr(wk+1|w1:k) = softmax(hkW + b), (4)\nwhere W \u2208 RdLM\u00d7|W|, b \u2208 R|W|, and dLM is a hidden layer size of the RNN."}, {"heading": "4 Experimental Setup", "text": "Data sets: All models are trained and evaluated on the English PTB data set [12] utilizing the standard training (0-20), validation (21-22), and test (23-24) splits along with pre-processing by [14]. Since the PTB is criticized for being small nowadays, we also provide an evaluation on the WikiText-2 data set [13], which is approximately two times larger than PTB in size and three times larger in vocabulary. We do not append any additional symbols at the end of each line in WikiText-2, but remove spaces between equality signs in the sequences \u201c= =\u201d and \u201c= = =\u201d, which occur in section titles.\nHyperparameters: The regularization parameter C from (1) is set to 1600, which results in 883 unique patterns (|\u03a00| = 883, |S| = 890) for the PTB data set (cf. 48 plain characters) and 1440 unique patterns (|\u03a00| = 1440, |S| = 1471) for the WikiText-2 data set (cf. 281 plain characters). We set the threshold value f to 300 on the PTB and to 700 on the WikiText-2. We experiment with two configurations for the state size dLM of the word-level RNNLM: 300 (small models) and 650 (medium-sized models). Specification of other hyperparameters is given below.\nConcat: dA = 15 (for characters), and dS = 30 (for patterns). We give higher dimensionality to patterns as their amount significantly exceeds the amount of characters. n is set to the 95th percentile of word lengths, i.e. 95% of all words have not more than n characters1. We do not set n = maxw\u2208W nw, as this would result in excessive zero-padding. dHW = dLM.\nSum: dX = dHW = dLM \u2208 {300, 650} for both characters and patterns. We give higher dimensionality to subword vectors here (compared to other models) since the resulting word vector will have the same size as subword vectors (see (3)).\nCNN: In character-based models we choose the same values for hyperparameters as in the work of [9]. For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding\n1 word length in characters and in patterns is the same.\ndepths (number of features per width) are [100, 50, 75, 100, 100, 100] and [100, 100, 150, 200, 200, 200, 200]. dHW = \u2211 depths \u2208 {525, 1150}. Optimization is done similarly to [24,9,5]. Training the models involves minimizing the negative log-likelihood over the corpus w1:K :\n\u2212 \u2211K k=1 log Pr(wk|w1:k\u22121) \u2212\u2192 min,\nwhich is typically done by truncated BPTT [21,6]. We backpropagate for 35 time steps using stochastic gradient descent where the learning rate is initially set to 0.7 and halved if the perplexity does not decrease on the validation set after an epoch. We use a batch size of 20. We train for 65 epochs, picking the best performing model on the validation set. Parameters of the models are randomly initialized uniformly in [\u22120.05, 0.05], except the forget bias of the word-level LSTM, which is initialized to 1, and the transform bias of the highway, which is initialized to values near \u22122. For regularization we use variational dropout [5] with dropout rates for small/medium Concat, Sum/medium CNN models as follows: 0.1/0.15/0.2 for the embedding layer, 0.2/0.3/0.35 for the input to the gates, 0.1/0.15/0.2 for the hidden units, and 0.2/0.3/0.35 for the output activations. We clip the norm of the gradients (normalized by minibatch size) at 5."}, {"heading": "5 Results", "text": "The results of evaluation on PTB and WikiText-2 are reported in Tables 1 and 2 correspondingly. As one can see, models which process patterns consistently outperform those which use characters under small parameter budgets. However, the difference in performance is less pronounced when we allow more parameters.\nAlso, it is clearly seen that patterns are more beneficial for simple models, such as Concat and Sum, but have less effect on the CNN model, which shrinks\nthe gap between characters and patterns. This is quite natural as patterns carry some information on character n-grams and, hence, can be considered as \u201cdiscrete convolutions\u201d, which makes CNN over patterns not as efficient as CNN over regular characters. However, we notice that in all cases a simple sum of pattern embeddings (Pat-Sum) is on par with a more sophisticated convolution over character embeddings (Char-CNN). Faster2 training of the Pat-Sum compared to the Char-CNN makes the patterns even more advantageous. Why does Pat-Sum perform equally well as Char-CNN? As was described in Section 3 word embeddings are processed by the two highway layers before they are fed into the RNNLM. Highway is a weighted average between nonlinear and identity transformations of the incoming word embedding:\nw 7\u2192 t \u03c3(wA + b) + (1\u2212 t) w,\nwhere t, A and b are trainable parameters, \u03c3(\u00b7) is a non-linear activation, 1 is a vector whose all components are 1 and is an operation of component-wise multiplication. The ideal input for the highway is the one that does not need to undergo a nonlinear transformation, i.e the highway will then be close to an identity operator, and hence in the ideal case we shall have t = 0. But if w is rather \u201craw\u201d, then the highway should prepare it for the RNN (resulting in t 6= 0). Such extra nonlinearity can measured by the closeness of t to 1. We hypothesize that the reason why Pat-Sum performs well is that the sum of pattern embeddings is already a good word representation. Hence the highway in Pat-Sum does less nonlinear work than in Char-CNN: In Pat-Sum it is almost an identical transformation, and such a simple highway is well-trained according to [7]. To validate our hypothesis we compare the distributions of the transform\ngate t values from both highway layers of Pat-Sum and Char-CNN. The den-\n2 around 1.2x speedup on NVIDIA Titan X (Pascal)\nsity plots in Fig. 4 support our hypothesis: Pat-Sum does not utilize much of nonlinearity in the highway layers, while Char-CNN heavily relies on it. Source code: All models were implemented in TensorFlow [1] and the source code for Pat-Sum is available at https://github.com/zh3nis/pat-sum."}, {"heading": "6 Conclusion", "text": "Regular characters are rather uninformative when their embeddings are concatenated or summed to produce word vectors, but patterns, on the contrary, carry enough information to make these methods work significantly better. Convolutions over subword embeddings do capture n-gram regularities and, therefore, make the difference between characters and patterns less noticeable. It is noteworthy, that a simple and fast sum of pattern embeddings is on par with more sophisticated and slower convolutions over characters embeddings."}, {"heading": "7 Acknowledgments", "text": "We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M Devin"], "venue": "arXiv preprint arXiv:1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Enriching word vectors with subword information", "author": ["P. Bojanowski", "E. Grave", "A. Joulin", "T. Mikolov"], "venue": "arXiv preprint arXiv:1607.04606", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Compositional morphology for word representations and language modelling", "author": ["J. Botha", "P. Blunsom"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14).", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Three models for the description of language", "author": ["N. Chomsky"], "venue": "IRE Transactions on information theory 2(3)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1956}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Identity matters in deep learning", "author": ["M. Hardt", "T. Ma"], "venue": "arXiv preprint arXiv:1611.04231", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9(8)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "A character-word compositional neural language model for finnish", "author": ["M. Lankinen", "H. Heikinheimo", "P. Takala", "T. Raiko", "J. Karhunen"], "venue": "arXiv preprint arXiv:1612.03266", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["W. Ling", "C. Dyer", "A.W. Black", "I. Trancoso", "R. Fermandez", "S. Amir", "L. Marujo", "T. Luis"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, Association for Computational Linguistics", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Computational linguistics 19(2)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Pointer sentinel mixture models", "author": ["S. Merity", "C. Xiong", "J. Bradbury", "R. Socher"], "venue": "Proceedings of ICLR 2017.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "Interspeech. Volume 2.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Subword language modeling with neural networks", "author": ["T. Mikolov", "I. Sutskever", "A. Deoras", "H.S. Le", "S. Kombrink", "J. Cernocky"], "venue": "preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon", "W. Weaver"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Letter n-gram-based input encoding for continuous space language models", "author": ["H. Sperr", "J. Niehues", "A. Waibel"], "venue": "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in neural information processing systems.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Inference algorithms for pattern-based crfs on sequence data", "author": ["R. Takhanov", "V. Kolmogorov"], "venue": "ICML (3).", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Character-word lstm language models", "author": ["L. Verwimp", "J. Pelemans", "P Wambacq"], "venue": "Proceedings of EACL 2017.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["P.J. Werbos"], "venue": "Proceedings of the IEEE 78(10)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Charagram: Embedding words and sentences via character n-grams", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Conditional random fields with high-order features for sequence labeling", "author": ["N. Ye", "W.S. Lee", "H.L. Chieu", "D. Wu"], "venue": "Advances in Neural Information Processing Systems.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent highway networks", "author": ["J.G. Zilly", "R.K. Srivastava", "J. Kout\u0144\u0131k", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1607.03474", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural architecture search with reinforcement learning", "author": ["B. Zoph", "Q.V. Le"], "venue": "Proceedings of ICLR 2017.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 22, "context": "The selection is made using the patternbased Conditional Random Field model [23,19] with l1 regularization.", "startOffset": 76, "endOffset": 83}, {"referenceID": 18, "context": "The selection is made using the patternbased Conditional Random Field model [23,19] with l1 regularization.", "startOffset": 76, "endOffset": 83}, {"referenceID": 15, "context": "Historically, first approaches to the problem [16,4] were highly interpretable, involving syntax and morphology, i.", "startOffset": 46, "endOffset": 52}, {"referenceID": 3, "context": "Historically, first approaches to the problem [16,4] were highly interpretable, involving syntax and morphology, i.", "startOffset": 46, "endOffset": 52}, {"referenceID": 14, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 5, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 10, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 8, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 9, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 19, "context": "Much research has been done on character-level neural language modeling [15,6,11,9,10,20].", "startOffset": 72, "endOffset": 89}, {"referenceID": 16, "context": "In [17] a word is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding; the word embeddings are then fed into neural machine translation models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [22] a very similar technique is used and an evaluation on three other tasks (word similarity, sentence similarity, and part-of-speech tagging) is performed; they demonstrate that their method outperforms more complex architectures based on character-level recurrent and convolutional neural networks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Probably closest to ours is an approach from [2] where a word representation is a sum of terms, each term corresponding to a certain n-gram that occurs in that word.", "startOffset": 45, "endOffset": 48}, {"referenceID": 22, "context": "A pattern-based conditional random field model for our language is the following probability distribution over A\u2217 [23,19]:", "startOffset": 114, "endOffset": 121}, {"referenceID": 18, "context": "A pattern-based conditional random field model for our language is the following probability distribution over A\u2217 [23,19]:", "startOffset": 114, "endOffset": 121}, {"referenceID": 18, "context": "The latter function is convex, an efficient computation of its value and gradient is described in [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 2, "context": "This approach was used by [3] to combine a word and its morpheme embeddings into a single word vector.", "startOffset": 26, "endOffset": 29}, {"referenceID": 8, "context": "\u2013 CNN: A convolutional model of [9]:", "startOffset": 32, "endOffset": 35}, {"referenceID": 17, "context": "To model interactions between subwords, we feed the resulting word embedding w into a stack of two highway layers [18] with dimensionality dHW per layer.", "startOffset": 114, "endOffset": 118}, {"referenceID": 24, "context": "The most advanced recurrent neural architectures, at the time of this writing, are RHN [25] and NAS [26].", "startOffset": 87, "endOffset": 91}, {"referenceID": 25, "context": "The most advanced recurrent neural architectures, at the time of this writing, are RHN [25] and NAS [26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "However, to make our results directly comparable to the previous work of [9] on character-level language modeling we select a more conventional architecture \u2013 a stack of two LSTM cells [8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "However, to make our results directly comparable to the previous work of [9] on character-level language modeling we select a more conventional architecture \u2013 a stack of two LSTM cells [8].", "startOffset": 185, "endOffset": 188}, {"referenceID": 11, "context": "Data sets: All models are trained and evaluated on the English PTB data set [12] utilizing the standard training (0-20), validation (21-22), and test (23-24) splits along with pre-processing by [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "Data sets: All models are trained and evaluated on the English PTB data set [12] utilizing the standard training (0-20), validation (21-22), and test (23-24) splits along with pre-processing by [14].", "startOffset": 194, "endOffset": 198}, {"referenceID": 12, "context": "Since the PTB is criticized for being small nowadays, we also provide an evaluation on the WikiText-2 data set [13], which is approximately two times larger than PTB in size and three times larger in vocabulary.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "CNN: In character-based models we choose the same values for hyperparameters as in the work of [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 1, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 2, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 3, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 4, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 5, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 110, "endOffset": 128}, {"referenceID": 0, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 1, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 2, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 3, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 4, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 5, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 6, "context": "For pattern-based models we choose: dS = 50 and dS = 100 for small and medium-sized models; filter widths are [1, 2, 3, 4, 5, 6] and [1, 2, 3, 4, 5, 6, 7] for small and medium-sized models; the corresponding", "startOffset": 133, "endOffset": 154}, {"referenceID": 23, "context": "Optimization is done similarly to [24,9,5].", "startOffset": 34, "endOffset": 42}, {"referenceID": 8, "context": "Optimization is done similarly to [24,9,5].", "startOffset": 34, "endOffset": 42}, {"referenceID": 4, "context": "Optimization is done similarly to [24,9,5].", "startOffset": 34, "endOffset": 42}, {"referenceID": 20, "context": "which is typically done by truncated BPTT [21,6].", "startOffset": 42, "endOffset": 48}, {"referenceID": 5, "context": "which is typically done by truncated BPTT [21,6].", "startOffset": 42, "endOffset": 48}, {"referenceID": 4, "context": "For regularization we use variational dropout [5] with dropout rates for small/medium Concat, Sum/medium CNN models as follows: 0.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "Hence the highway in Pat-Sum does less nonlinear work than in Char-CNN: In Pat-Sum it is almost an identical transformation, and such a simple highway is well-trained according to [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 0, "context": "Source code: All models were implemented in TensorFlow [1] and the source code for Pat-Sum is available at https://github.", "startOffset": 55, "endOffset": 58}], "year": 2017, "abstractText": "Words in some natural languages can have a composite structure. Elements of this structure include the root (that could also be composite), prefixes and suffixes with which various nuances and relations to other words can be expressed. Thus, in order to build a proper word representation one must take into account its internal structure. From a corpus of texts we extract a set of frequent subwords and from the latter set we select patterns, i.e. subwords which encapsulate information on character n-gram regularities. The selection is made using the patternbased Conditional Random Field model [23,19] with l1 regularization. Further, for every word we construct a new sequence over an alphabet of patterns. The new alphabet\u2019s symbols confine a local statistical context stronger than the characters, therefore they allow better representations in R and are better building blocks for word representation. In the task of subword-aware language modeling, pattern-based models outperform character-based analogues by 2-20 perplexity points. Also, a recurrent neural network in which a word is represented as a sum of embeddings of its patterns is on par with a competitive and significantly more sophisticated character-based convolutional architecture.", "creator": "LaTeX with hyperref package"}}}