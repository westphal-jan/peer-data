{"id": "1709.01186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Sep-2017", "title": "Learning Neural Word Salience Scores", "abstract": "Measuring the salience of a word is an essential step in numerous NLP tasks. Heuristic approaches such as tfidf have been used so far to estimate the salience of words. We propose \\emph{Neural Word Salience} (NWS) scores, unlike heuristics, are learnt from a corpus. Specifically, we learn word salience scores such that, using pre-trained word embeddings as the input, can accurately predict the words that appear in a sentence, given the words that appear in the sentences preceding or succeeding that sentence. Experimental results on sentence similarity prediction show that the learnt word salience scores perform comparably or better than some of the state-of-the-art approaches for representing sentences on benchmark datasets for sentence similarity, while using only a fraction of the training and prediction times required by prior methods. Moreover, our NWS scores positively correlate with psycholinguistic measures such as concreteness, and imageability implying a close connection to the salience as perceived by humans.", "histories": [["v1", "Mon, 4 Sep 2017 22:52:59 GMT  (154kb,D)", "http://arxiv.org/abs/1709.01186v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["krasen samardzhiev", "andrew gargett", "danushka bollegala"], "accepted": false, "id": "1709.01186"}, "pdf": {"name": "1709.01186.pdf", "metadata": {"source": "CRF", "title": "Learning Neural Word Salience Scores", "authors": ["Krasen Samardzhiev", "Andrew Gargett"], "emails": ["krasensam@gmail.com", "andrew.gargett@stfc.ac.uk", "danushka@liverpool.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Humans can easily recognise the words that contribute to the meaning of a sentence (i.e. content words) from words that serve only a grammatical functionality (i.e. functional words). For example, functional words such as the, an, a etc. have limited contributions towards the overall meaning of a document and are often filtered out as stop words in information retrieval systems [33]. We define the salience q(w) of a word w in a given text T as the semantic contribution made by w towards the overall meaning of T . If we can accurately compute the salience of words, then we can develop better representations of texts that can be used in downstream NLP tasks such as similarity measurement [7] or text (e.g. sentiment, entailment) classification [34].\nAs described later in section 2, existing methods for detecting word salience can be classified into two groups: (a) lexicon-based filtering methods such as stop word lists, or (b) word frequency-based heuristics such as the popular term-frequency inverse document frequency (tfidf) [25] measure and its variants. Unfortunately, two main drawbacks can be identified in common to both stop words lists and frequency-based salience scores.\nFirst, such methods do not take into account the semantics associated with individual words when determining their salience. For example, consider the following two adjacent sentences extracted from a newspaper article related to the visit of the Japanese Prime Minister, Shinzo Abe, to the White House in Washington, to meet the US President Donald Trump.\n(a) Abe visited Washington in February and met Trump in the White House.\nar X\niv :1\n70 9.\n01 18\n6v 1\n[ cs\n.C L\n] 4\n(b) Because the trade relations between US and Japan have been fragile after the recent comments by the US President, the Prime Minister\u2019s visit to the US can be seen as an attempt to reinforce the trade relations.\nIn Sentence (a), the Japanese person name Abe or American person name Trump would occur less in a corpus than the US state name Washington. Nevertheless, for the main theme of this sentence, Japanese Prime minister met US President, the two person names are equally important as the location they met. Therefore, we must look into the semantics of the individual words when computing their saliences.\nSecond, words do not occur independently of one another in a text, and methods that compute word salience using frequency or pre-compiled stop words lists alone do not consider the contextual information. For example, the two sentences (a) and (b) in our previous example are extracted from the same newspaper article and are adjacent. The words in the two sentences are highly related. For example, Abe in sentence (a) refers to the Prime Minister in sentence (b), and Trump in sentence (a) is refers to the US President in sentence (b). A human reader who reads sentence (a) before sentence (b) would expect to see some relationship between the topic discussed in (a) and that in the next sentence (b). Unfortunately, methods that compute word salience scores considering each word independently from all other words in near by contexts, ignore such proximity relationships.\nTo overcome the above-mentioned disfluencies in existing word salience scores, we propose an unsupervised method that first randomly initialises word salience scores, and subsequently updates them such that we can accurately predict the words in local contexts. Specifically, we train a twolayer neural network where in the first layer we take pre-trained word embeddings of the words in a sentence Si as the input and compute a representation for Si (here onwards referred to as a sentence embedding) as the weighted average of the input word embeddings. The weights correspond to the word salience scores of the words in Si. Likewise, we apply the same approach to compute the sentence embedding for the sentence Si\u22121 preceding Si and Si+1 succeeding Si in a sentenceordered corpus. Because Si\u22121, Si and Si+1 are adjacent sentences, we would expect the sentence pairs (Si, Si\u22121) and (Si, Si+1) to be topically related.1\nWe would expect a high degree of cosine similarity between si and si\u22121, and si and si+1, where boldface symbols indicate vectors. Likewise, for a randomly selected sentence Sj /\u2208 {Si\u22121, Si, Si+1}, the expect similarity between Sj and Si would be low. We model this as a supervised similarity prediction task and use backpropagation to update the word salience scores, keeping word embeddings fixed. We refer to the word salience scores learnt by the proposed method as the Neural Word Salience (NWS) scores. We will use the contextual information of a word to learn its salience. However, once learnt, we consider salience as a property of a word that holds independently of its context. This enables us to use the same salience score for a word after training, without having to modify it considering the context in which it occurs.\nSeveral remarks can be made about the proposed method for learning NWS scores. First, we do not require labelled data for learning NWS scores. Although we require semantically similar (positive) and semantically dissimilar (negative) pairs of sentences for learning the NWS scores, both positive and negative examples are automatically extracted from the given corpus. Second, we use pre-trained word embeddings as the input, and do not learn the word embeddings as part of the learning process. This design choice differentiates our work from previously proposed sentence embedding learning methods that jointly learn word embeddings as well as sentence embeddings [23, 27, 26]. Moreover, it decouples the word salience score learning problem from word or sentence embedding learning problem, thereby simplifying the optimisation task and speeding up the learning process.\nWe use the NWS scores to compute sentence embeddings and measure the similarity between two sentences using 18 benchmark datasets for semantic textual similarity in past SemEval tasks [4]. Experimental results show that the sentence similarity scores computed using the NWS scores and pre-trained word embeddings show a high degree of correlation with human similarity ratings in those benchmark datasets. Moreover, we compare the NWS scores against the human ratings for psycholinguistic properties of words such as arousal, valence, dominance, imageability, and con-\n1Si\u22121 and Si+1 could also be topically related and produce a positive training examples in some cases. However, they are non-adjacent and possibly less related compared to adjacent sentence pairs. Because we have an abundant supply of sentences, and we want to reduce label noise in positive examples, we do not consider (Si\u22121, Si+1) as a positive example.\ncreteness. Our analysis shows that NWS scores demonstrate a moderate level of correlation with concreteness and imageability ratings, despite not being specifically trained to predict such psycholinguistic properties of words."}, {"heading": "2 Related Work", "text": "Word salience scores have long been studied in the information retrieval community [33]. Given a user query described in terms of one or more keywords, an information retrieval system must find the most relevant documents to the user query from a potentially large collection of documents. Word salience scores based on term frequency, document frequency, and document length have been proposed such as tfidf and BM25 [32].\nOur proposed method learns word salience scores by creating sentence embeddings. Next, we briefly review such sentence embedding methods and explain the differences between the sentence embedding learning problem and word salience learning problem.\nSentences have a syntactic structure and the ordering of words affects the meaning expressed in the sentence. Consequently, compositional approaches for computing sentence-level semantic representations from word-level semantic representations have used numerous linear algebraic operators such as vector addition, element-wise multiplication, multiplying by a matrix or a tensor [8, 29].\nAlternatively to applying nonparametric operators on word embeddings to create sentence embeddings, recurrent neural networks can learn the optimal weight matrix that can produce an accurate sentence embedding when repeatedly applied to the constituent word embeddings. For example, skip-thought vectors [27] use bi-directional LSTMs to predict the words in the order they appear in the previous and next sentences given the current sentence. Although skip-thought vectors have shown superior performances in supervised tasks, its performance on unsupervised tasks has been sub-optimal [7]. Moreover, training bi-directional LSTMs from large datasets is time consuming and we also need to perform LSTM inference in order to create the embedding for unseen sentences at test time, which is time consuming compared to weighted addition of the input word embeddings. FastSent [23] was proposed as an alternative lightweight approach for sentence embedding where a softmax objective is optimised to predict the occurrences of words in the next and the previous sentences, ignoring the ordering of the words in the sentence.\nSurprisingly, averaging word embeddings to create sentence embeddings has shown comparable performances to sentence embeddings that are learnt using more sophisticated word-order sensitive methods. For example, [7] proposed a method to find the optimal weights for combining word embeddings when creating sentence embeddings using unigram probabilities, by maximising the likelihood of the occurrences of words in a corpus. Siamese CBOW [26] learns word embeddings such that we can accurately compute sentence embeddings by averaging the word embeddings. Although averaging is an order insensitive operator, [1] empirically showed that it can accurately predict the content and word order in sentences. This can be understood intuitively by recalling that words that appear between two words are often different in contexts where those two words are swapped. For example, in the two sentences \u201cOstrich is a large bird that lives in Africa\u201d and \u201cLarge birds such as Ostriches live in Africa\u201d, the words that appear in between ostrich and bird are different, giving rise to different sentence embeddings even when sentence embeddings are computed by averaging the individual word embeddings. Instead of considering all words equally for sentence embedding purposes, attention-based models [19, 39, 36] learn the amount of weight (attention) we must assign to each word in a given context.\nOur proposed method for learning NWS scores are based on the prior observation that averaging is an effective heuristic for creating sentence embeddings from word embeddings. However, unlike sentence embedding learning methods that do not learn word salience scores [20, 39] , our goal in this paper is to learn word salience scores and not sentence embeddings. We compute sentence embeddings only for the purpose of evaluating the word salience scores we learn. Moreover, our work differs from Siamese CBOW [26] in that we do not learn word embeddings but take pretrained word embeddings as the input for learning word salience scores. NWS scores we learn in this paper are also different from the salience scores learnt by [7] because they do not constrain their word salience scores such that they can be used to predict the words that occur in adjacent sentences."}, {"heading": "3 Neural Word Salience Scores", "text": "Let us consider a vocabulary V of words w \u2208 V . For the simplicity of exposition, we limit the vocabulary to unigrams but note that the proposed method can be used to learn salience scores for arbitrary length n-grams. We assume that we are given d-dimensional pre-trained word embeddings w \u2208 Rd for the words in V . Let us denote the NWS score of w by q(w) \u2208 R. We learn q(w) such that the similarity between two adjacent sentences Si and Si\u22121, or Si and Si+1 in a sentence-ordered corpus C is larger than that between two non-adjacent sentences Si and Sj , where j /\u2208 {i\u22121, i, i+1}. Let us further represent the two sentence Si = {wi1, . . . , win} and Sj = {wj1, . . . , wjm} by the sets of words in those sentences. Here, we assume the corpus to contain sequences of ordered sentence such as in a newspaper article, a book chapter or a blog post.\nThe neural network we use for learning q(w) is shown in Figure 1. The first layer computes the embedding of a sentence S, s \u2208 Rd using Equation 1, which is the weighted-average of the individual word embeddings.\ns = \u2211 w\u2208S q(w)w (1)\nWe use (1) to compute embeddings for two sentences Si and Sj denoted respectively by si and sj . Here, the same set of salience scores q(w) are used for computing both si and sj , which resembles a Siamese neural network architecture.\nThe root node computes the similarity h(si, sj) between two sentence embeddings. Different similarity (alternatively dissimilarity or divergence) functions such as cosine similarity, `1 distance, `2 distance, Jenson-Shannon divergence etc. can be used as h. As a concrete example, here we use softmax of the inner-products as follows:\nh(si, sj) = exp\n( si >sj )\u2211 Sk\u2208C exp (si>sk) (2)\nIdeally, the normalisation term in the denominator in the softmax must be taken over all the sentences Sk in the corpus [6]. However, this is computationally expensive in most cases except for extremely small corpora. Therefore, following noise-contrastive estimation [18], we approximate the normalisation term using a randomly sampled set of K sentences, where K is typically less than 10. Because the similarity between two randomly sampled sentences is likely to be smaller than, for example, two adjacent sentences, we can see this sampling process as randomly sampling negative training instances from the corpus.\nFor two sentences Si and Sj we consider them to be similar (positive training instance) if j \u2208 {i\u2212 1, i + 1}, and denote this by the target label t = 1. On the other hand, if the two sentences are non-adjacent (i.e. j /\u2208 {i\u2212 1, i + 1}), then we consider the pair (Si,Sj) to form a negative training\ninstance, and denote this by t = 0.2 This assumption enables us to use a sentence-ordered corpus for selecting both positive and negative training instances required for learning NWS scores.\nUsing t and h(si, sj) above, we compute the cross-entropy error E(t, (Si,Sj)) for an instance (t, (Si,Sj)) as follows:\nE(t, (Si,Sj)) = t log (h(si, sj)) + (1\u2212 t) log (1\u2212 h(si, sj)) (3)\nNext, we backpropagate the error gradients via the network to compute the updates as follows:\n\u2202E\n\u2202q(w) = (t\u2212 h(si, sj)) h(si, sj)(1\u2212 h(si, sj)) \u2202h(si, sj) \u2202q(w) (4)\nHere, we drop the arguments of the error and simply write it as E to simplify the notation. To compute \u2202h(si,sj)\u2202q(w) let us define g(si, sj) = log (h(si, sj)) (5) From which we have,\n\u2202h(si, sj)\n\u2202q(w) = h(si, sj)\n\u2202g(si, sj)\n\u2202q(w) . (6)\nWe can then compute \u2202g\u2202q(w) as follows:\nI[w \u2208 Si]w>sj + I[w \u2208 Sj ]w>si (7)\n\u2212 log (\u2211 k exp ( si >sj )( I[w \u2208 Si]w>sk + I[w \u2208 Sk]w>si )) (8)\nHere, the indicator function I is given by (9).\nI[\u03b8] = { 1 \u03b8 is True 0 otherwise\n(9)\nSubstituting (9), (7), in (4) we compute \u2202E\u2202q(w) and use stochastic gradient descent with initial learning rate set to 0.01 and subsequently scheduled by AdaGrad [15]. The NWS scores are randomly initialised in our experiments."}, {"heading": "4 Experiments", "text": "We use the Toronto books corpus3 as our training dataset. This corpus contains 81 million sentences from 11,038 books, and has been used as a training dataset in several prior work on sentence embedding learning. We convert all sentences to lowercase and tokenise using the Python NLTK4 punctuation tokeniser. No further pre-processing is conduced beyond tokenisation. The proposed method is implemented using TensorFlow5 and executed on a NVIDIA Tesla K40c 2880 GPU. The source code is submitted as a supplementary and will be publicly released upon paper acceptance."}, {"heading": "4.1 Measuring Semantic Textual Similarity", "text": "It is difficult to evaluate the accuracy of word salience scores by direct manual inspection. Moreover, there does not exist any datasets where human annotators have manually rated words for their salience. Therefore, we resort to extrinsic evaluations, where, we first use (1) to create the sentence embedding for a given sentence using pre-trained word embeddings and the NWS scores computed using the proposed method. Next, we measure the semantic textual similarity (STS) between two sentences by the cosine similarity between the corresponding sentence embeddings. Finally, we compute the correlation between human similarity ratings for sentence pairs in benchmark datasets for STS and the similarity scores computed following the above-mentioned procedure. If there exists a high degree of correlation between the sentence similarity scores computed using the NWS\n2It is possible in theory that two non-adjacent sentences could be similar, but the likelihood of this event is small and can be safely ignored in practice.\n3http://yknzhu.wixsite.com/mbweb 4http://www.nltk.org/ 5https://www.tensorflow.org/\nscores and human ratings, then it can be considered as empirical support for the accuracy of the NWS scores. As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 [4], 2013 [5], 2014 [3], and 2015 [2]. Note that the tasks with the same name in different years actually represent different tasks.\nWe use Pearson correlation coefficient as the evaluation measure. For a list of n ordered pairs of ratings {(xi, yi)}ni=1, the Pearson correlation coefficient between the two ratings, r(x,y), is computed as follows:\nr(x,y) = \u2211n i=1(xi \u2212 x\u0304)(yi \u2212 y\u0304)\u221a\u2211n\ni=1(xi \u2212 x\u0304)2 \u221a\u2211n i=1(yi \u2212 y\u0304)2 (10)\nHere, x\u0304 = 1n \u2211n i=1 xi and y\u0304 = 1 n \u2211n i=1 yi. Pearson correlation coefficient is invariant against linear transformations of the similarity scores, which is suitable for comparing similarity scores assigned to the same set of items by two different methods (human ratings vs. system ratings).\nWe use the Fisher transformation [17] to test for the statistical significance of Pearson correlation coefficients. Fisher transformation, F (r), of the Pearson correlation coefficient r is given by (11).\nF (r) = 1\n2 log\n( 1 + r\n1\u2212 r\n) (11)\nThen, 95% confidence intervals are given by (12).\ntanh ( F (r)\u00b1 1.96\u221a\nn\u2212 3\n) (12)\nWe consider two baseline methods in our evaluations as described next.\nAveraged Word Embeddings (AVG) As a baseline that does not use any salience scores for words when computing sentence embeddings, we use Averaged Word Embeddings (AVG) where we simply add all the word embeddings of the words in a sentence and divide from the total number of words to create a sentence embedding. This baseline demonstrates the level of performance we would obtain if we did not perform any word salience-based weighting in (1).\nInverse Sentence Frequency (ISF) As described earlier in section 2, term frequency is not a useful measure for discriminating salient vs. non-salient words in short-texts because it is rare for a particular word to occur multiple times in a short text such as a sentence. However, (inverse of) the number of different sentences in which a particular word occurs is a useful method for identifying salient features because non-content stop words are likely to occur in any sentence, irrespective of the semantic contribution to the topic of the sentence. Following the success of Inverse Document Frequency (IDF) in filtering out high frequent words in text classification tasks [24], we define Inverse Sentence Frequency (ISF) of a word as the reciprocal of the number of sentences in which that word appears in a corpus. Specifically, ISF is computed as follows:\nISF(w) = log ( 1 + total no. of sentences in the corpus\nno. of sentences containing w\n) (13)\nIn Table 1, we compare NWS against AVG, ISF baselines. SMOOTH is the unigram probabilitybased smoothing method proposed by [7].6 We compute sentence embeddings for NWS, AVG and ISF using pre-trained 300 dimensional GloVe embeddings trained from the Toronto books corpus using contextual windows of 10 tokens.7 For reference purposes we show the level of performance we would obtain if we had used sentence embedding methods such as, skip-thought [27], and SiameseCBOW [26]. Note that however, sentence embedding methods do not necessarily compute word salience scores. For skip-thought, Siamese CBOW and SMOOTH methods we report the published results in the original papers. Because [27] did not report results for skip-thought on all 18 benchmark datasets used here, we report the re-evaluation of skip-thought on all 18 benchmark datasets by [38].\n6Corresponds to the GloVe-W method in the original publication. 7We use the GloVe implementation by the original authors available at https://nlp.stanford.edu/\nprojects/glove/\nStatistically significant improvements over the ISF baseline are indicated by an asterisk \u2217, whereas the best results on each benchmark dataset are shown in bold. From Table 1, we see that between the two baselines AVG and ISF, ISF consistently outperforms AVG in all benchmark datasets. In 9 out of the 18 benchmarks, the proposed NWS scores report the best performance. Moreover, in 9 datasets NWS statistically significantly outperforms the ISF baseline. Siamese-CBOW reports the best results in 5 datasets, whereas SMOOTH reports the best results in 2 datasets. Overall, NWS stands out as the best performing method among the methods compared in Table 1.\nOur proposed method for learning NWS scores does not assume any specific properties of a particular word embedding learning algorithm. Therefore, in principle, we can learn NWS scores using any pre-trained set of word embeddings. To evaluate the accuracy of the word salience scores computed using different word embeddings, we conduct the following experiment. We use SGNS, CBOW and GloVe word embedding learning algorithms to learn 300 dimensional word embeddings from the Toronto books corpus.8 The vocabulary size, cut-off frequency for selecting words, context window size are are kept fixed across different word embedding learning methods for the consistency of the evaluation. We then trained NWS with each set of word embeddings. Performance on STS benchmarks is shown in Table 2, where the best performance is bolded.\nFrom Table 2, we see that GloVe is the best among the three word embedding learning methods compared in Table 2 for producing pre-trained word embeddings for the purpose of learning NWS scores. In particular, NWS scores reports best results with GloVe embeddings in 10 out of the 18 benchmark datasets, whereas with CBOW embeddings it obtains the best results in the remaining 8 benchmark datasets.\nFigures 2a and 2b show the Pearson correlation coefficients on STS benchmarks obtained by NWS scores computed respectively for GloVe and SGNS embeddings. We plot training curves for the average correlation over each year\u2019s benchmarks as well as the overall average over the 18 benchmarks. We see that for both embeddings the training saturates after about five or six epochs. This ability to learn quickly with a small number of epochs is attractive because it reduces the training time.\n8We use the implementation of word2vec from https://github.com/dav/word2vec"}, {"heading": "4.2 Correlation with Psycholinguistic Scores", "text": "Prior work in psycholinguistics show that there is a close connection between the emotions felt by humans and the words they read in a text. Valence (the pleasantness of the stimulus), arousal (the\nintensity of emotion provoked by the stimulus), and dominance (the degree of control exerted by the stimulus) contribute to how the meanings of words affect human psychology, and often referred to as the affective meanings of words. [28] show that by using SGNS embeddings as features in a kNearest Neighbour classifier, it is possible to accurately extrapolate the affective meanings of words. Moreover, perceived psycholinguistic properties of words such as concreteness (how \u201cpalpable\u201d the object the word refers to) and imageability (the intensity with which a word arouses images) have been successfully predicted using word embeddings [35, 31]. For example, [35] used the cosine similarity between word embeddings obtained via Latent Semantic Analysis (LSA) [13] to predict the concreteness and imageability ratings of words.\nOn the other hand, prior work studying the relationship between human reading patterns using eyetracking devices show that there exist a high positive correlation between word salience and reading times [16, 19]. For example, humans pay more attention to words that carry meaning as indicated by the longer fixation times. Therefore, an interesting open question is that what psycholinguistic properties of words, if any, are related to the NWS scores we learn in a purely unsupervised manner from a large corpus? To answer this question empirically, we conduct the following experiment. We used the Affected Norms for English Words (ANEW) dataset created by Warriner et al. [37], which contains valence, arousal, and dominance ratings collected via crowd sourcing for 13,915 words. Moreover, we obtained concreteness and imageability ratings for 3364 words from the MRC psycholinguistic database. We then measure the Pearson correlation coefficient between NWS scores and each of the psycholinguistic ratings as shown in Table 3.\nWe see a certain degree of correlation between NWS scores computed for all three word embeddings and the concreteness scores. Both GloVe and SGNS show moderate positive correlations for concreteness, whereas CBOW shows a moderate negative correlation for the same. A similar trend can be observed for imageability ratings in Table 3, where GloVe and SGNS correlates positively with imageability, while CBOW correlates negatively. Moreover, no correlation could be observed for arousal, valance and dominance ratings. This result shows that NWS scores are not correlated with affective meanings of words (arousal, dominance, and valance), but show a moderate level of correlation with perceived meaning scores (concreteness and imageability)."}, {"heading": "5 Conclusion", "text": "We proposed a method for learning Neural Word Salience scores from a sentence-ordered corpus, without requiring any manual data annotations. To evaluate the learnt salience scores, we computed sentence embeddings as the linearly weighted sum over pre-trained word embeddings. Our experimental results show that the proposed NWS scores outperform baseline methods, previously proposed word salience scores and sentence embedding methods on a range of benchmark datasets selected from past SemEval STS tasks. Moreover, the NWS scores shows interesting correlations with perceived meaning of words indicated by concreteness and imageability psycholinguistic ratings."}], "references": [{"title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "author": ["Y. Adi", "E. Kermany", "Y. Belinkov", "O. Lavi", "Y. Goldberg"], "venue": "arXiv", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Semeval-2015 task 2: Semantic textual similarity", "author": ["E. Agirre", "C. Banea", "C. Cardie", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "W. Guo", "I. Lopez- Gazpio", "M. Martxalar", "R. Mihalcea", "G. Rigau", "L. Uria", "J.M. Wiebe"], "venue": "english, spanish and pilot on interpretability. In Proc. of SemEval, pages 252 \u2013 263", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Semeval-2014 task 10: Multilingual semantic textual similarity", "author": ["E. Agirre", "C. Banea", "C. Cardie", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "W. Guo", "R. Mihalcea", "G. Rigau", "J. Weibe"], "venue": "Proc. of the 8th International Workshop on Semantic Evaluation (SemEval), pages 81\u201391", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Semeval-2012 task 6: A pilot on semantic textual similarity", "author": ["E. Agirre", "D. Cer", "M. Diab", "A. Gonzalez-Agirre"], "venue": "Proc. of the first Joint Conference on Lexical and Computational Semantics (*SEM), pages 385 \u2013 393", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "shared task: Semantic textual similarity", "author": ["E. Agirre", "D. Cer", "M. Diab", "A. Gonzalez-Agirre", "W. Guo"], "venue": "Proc. of the Second Joint Conference on Lexical and Computational Semantics (*SEM):, pages 32\u201343", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "When and why are log-linear models self-normalizing? In Proc", "author": ["J. Andreas", "D. Klein"], "venue": "of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 244\u2013249", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["S. Arora", "Y. Liang", "T. Ma"], "venue": "Proc. of International Conference on Learning Representations (ICLR)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2017}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata"], "venue": "Proc. of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546\u2013556", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Websim: A web-based semantic similarity measure", "author": ["D. Bollegala", "Y. Matsuo", "M. Ishizuka"], "venue": "Proc. of 21st Annual Conference of the Japanese Society of Artitificial Intelligence, pages 757 \u2013 766", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Www sits the sat: Measuring relational similarity on the web", "author": ["D. Bollegala", "Y. Matsuo", "M. Ishizuka"], "venue": "Proc. of ECAI\u201908, pages 333\u2013337", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "A machine learning approach to sentence ordering for multidocument summarization and its evaluation", "author": ["D. Bollegala", "N. Okazaki", "M. Ishizuka"], "venue": "Proc. of International Joint Conferences in Natural Language Processing, pages 624 \u2013 635. Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "A preference learning approach to sentence ordering for multi-document summarization", "author": ["D. Bollegala", "N. Okazaki", "M. Ishizuka"], "venue": "Information Sciences, 217:78 \u2013 95", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE, 41(6):391\u2013407", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Cross-language latent relational search: Mapping knowledge across languages", "author": ["N.T. Duc", "D. Bollegala", "M. Ishizuka"], "venue": "Proc. of the Twenty-Fifth AAAI Conference on Artificial Intelligence, pages 1237 \u2013 1242", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Object-based saliency as a predictor of attention in visual tasks", "author": ["M. Dziemianko", "A. Clarke", "F. Keller"], "venue": "Proc. of the 35thth Annual Conference of the Cognitive Science Society, pages 2237\u20132242", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Frequency distribution of the values of the correlation coefficient in samples of an indefinitely large population", "author": ["R.A. Fisher"], "venue": "Biometrika, 10(4):507\u2013521", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1915}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Modeling human reading with neural attention", "author": ["M. Hahn", "F. Keller"], "venue": "Proc. of Empirical Methods in Natural Language Processing (EMNLP), pages 85\u201395", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Pairwise word interaction modeling with deep neural networks for semantic similarity measurement", "author": ["H. He", "J. Lin"], "venue": "Proc. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 937\u2013948", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension", "author": ["H. Hernault", "D. Bollegala", "M. Ishizuka"], "venue": "Empirical Methods in Natural Language Processing, pages 399 \u2013 409", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "A sequential model for discourse segmentation", "author": ["H. Hernault", "D. Bollegala", "M. Ishizuka"], "venue": "International Conference on Intelligence Text Processing and Computational Linguistics (CICLing), pages 315 \u2013 326", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning disributed representations of sentences from unlabelled data", "author": ["F. Hill", "K. Cho", "A. Korhonen"], "venue": "Proc. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1367\u20131377", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Proc. of the European Conference on Machine Learning (ECML), pages 137\u2013142", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K.S. Jones"], "venue": "Journal of Documentation, 28:11\u201321", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1972}, {"title": "and M", "author": ["T. Kenter", "A. Borisov"], "venue": "de Rijke. Siamese cbow: Optimizing word embeddings for sentence representations. In Proc, of the Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 941\u2013951", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Skipthought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "Proc. of Advances in Neural Information Processing Systems (NIPS), pages 3276\u20133284", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "How useful are corpus-based methods for extrapolating psycholinguistic variables? The Quarterly Journal of Experimental Pscychology", "author": ["P. Mandera", "E. Keuleers", "M. Brysbaret"], "venue": "68(8):1623\u20131642", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "Proc. of Annual Meeting of the Association for Computational Linguistics, pages 236 \u2013 244", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "An adaptive differential evolution algorithm", "author": ["N. Noman", "D. Bollegala", "H. Iba"], "venue": "Proc. of IEEE Congress on Evolutionary Computation (CEC), pages 2229\u20132236", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring psycholinguistic properties of words", "author": ["G.H. Paetzold", "L. Specia"], "venue": "Proc. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 435\u2013440", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Overview of the okapi projects", "author": ["S.E. Robertson"], "venue": "Journal of Documentation, 53(1):3 \u2013 7", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1997}, {"title": "Introduction to Modern Information Retreival", "author": ["G. Salton", "C. Buckley"], "venue": "McGraw-Hill Book Company", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1983}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["R. Socher", "J. Pennington", "E.H. Huang", "A.Y. Ng", "C.D. Manning"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["P.D. Turney", "Y. Neuman", "D. Assaf", "Y. Cohen"], "venue": "Proc. of Empirical Methods in Natural Language Processing (EMNLP), pages 27 \u2013 31", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Cse: Conceptutal sentence embeddings based on attention model", "author": ["Y. Wang", "H. Huang", "C. Feng", "Q. Zhou", "J. Gu", "X. Gao"], "venue": "Proc. of Annual Meeting of the Association for Computational Linguistics, pages 505\u2013515", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Norms of valence", "author": ["A.B. Warriner", "V. Kuperman", "M. Brysbaret"], "venue": "arousal, and dominance for 13,915 enlish lemmas. Behavior Research Methods, 45(4):1191\u20131207", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards universal paraphrastic sentence embeddings", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "Proc. of International Conference on Learning Representations (ICLR)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["W. Yin", "H. Sch\u00fctze", "B. Xiang", "B. Zhou"], "venue": "Transactions of Association for Computational Linguistics, pages 259\u2013272", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 32, "context": "have limited contributions towards the overall meaning of a document and are often filtered out as stop words in information retrieval systems [33].", "startOffset": 143, "endOffset": 147}, {"referenceID": 6, "context": "If we can accurately compute the salience of words, then we can develop better representations of texts that can be used in downstream NLP tasks such as similarity measurement [7] or text (e.", "startOffset": 176, "endOffset": 179}, {"referenceID": 33, "context": "sentiment, entailment) classification [34].", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "As described later in section 2, existing methods for detecting word salience can be classified into two groups: (a) lexicon-based filtering methods such as stop word lists, or (b) word frequency-based heuristics such as the popular term-frequency inverse document frequency (tfidf) [25] measure and its variants.", "startOffset": 283, "endOffset": 287}, {"referenceID": 22, "context": "This design choice differentiates our work from previously proposed sentence embedding learning methods that jointly learn word embeddings as well as sentence embeddings [23, 27, 26].", "startOffset": 170, "endOffset": 182}, {"referenceID": 26, "context": "This design choice differentiates our work from previously proposed sentence embedding learning methods that jointly learn word embeddings as well as sentence embeddings [23, 27, 26].", "startOffset": 170, "endOffset": 182}, {"referenceID": 25, "context": "This design choice differentiates our work from previously proposed sentence embedding learning methods that jointly learn word embeddings as well as sentence embeddings [23, 27, 26].", "startOffset": 170, "endOffset": 182}, {"referenceID": 3, "context": "We use the NWS scores to compute sentence embeddings and measure the similarity between two sentences using 18 benchmark datasets for semantic textual similarity in past SemEval tasks [4].", "startOffset": 184, "endOffset": 187}, {"referenceID": 32, "context": "Word salience scores have long been studied in the information retrieval community [33].", "startOffset": 83, "endOffset": 87}, {"referenceID": 31, "context": "Word salience scores based on term frequency, document frequency, and document length have been proposed such as tfidf and BM25 [32].", "startOffset": 128, "endOffset": 132}, {"referenceID": 7, "context": "Consequently, compositional approaches for computing sentence-level semantic representations from word-level semantic representations have used numerous linear algebraic operators such as vector addition, element-wise multiplication, multiplying by a matrix or a tensor [8, 29].", "startOffset": 270, "endOffset": 277}, {"referenceID": 28, "context": "Consequently, compositional approaches for computing sentence-level semantic representations from word-level semantic representations have used numerous linear algebraic operators such as vector addition, element-wise multiplication, multiplying by a matrix or a tensor [8, 29].", "startOffset": 270, "endOffset": 277}, {"referenceID": 26, "context": "For example, skip-thought vectors [27] use bi-directional LSTMs to predict the words in the order they appear in the previous and next sentences given the current sentence.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "Although skip-thought vectors have shown superior performances in supervised tasks, its performance on unsupervised tasks has been sub-optimal [7].", "startOffset": 143, "endOffset": 146}, {"referenceID": 22, "context": "FastSent [23] was proposed as an alternative lightweight approach for sentence embedding where a softmax objective is optimised to predict the occurrences of words in the next and the previous sentences, ignoring the ordering of the words in the sentence.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "For example, [7] proposed a method to find the optimal weights for combining word embeddings when creating sentence embeddings using unigram probabilities, by maximising the likelihood of the occurrences of words in a corpus.", "startOffset": 13, "endOffset": 16}, {"referenceID": 25, "context": "Siamese CBOW [26] learns word embeddings such that we can accurately compute sentence embeddings by averaging the word embeddings.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Although averaging is an order insensitive operator, [1] empirically showed that it can accurately predict the content and word order in sentences.", "startOffset": 53, "endOffset": 56}, {"referenceID": 18, "context": "Instead of considering all words equally for sentence embedding purposes, attention-based models [19, 39, 36] learn the amount of weight (attention) we must assign to each word in a given context.", "startOffset": 97, "endOffset": 109}, {"referenceID": 38, "context": "Instead of considering all words equally for sentence embedding purposes, attention-based models [19, 39, 36] learn the amount of weight (attention) we must assign to each word in a given context.", "startOffset": 97, "endOffset": 109}, {"referenceID": 35, "context": "Instead of considering all words equally for sentence embedding purposes, attention-based models [19, 39, 36] learn the amount of weight (attention) we must assign to each word in a given context.", "startOffset": 97, "endOffset": 109}, {"referenceID": 19, "context": "However, unlike sentence embedding learning methods that do not learn word salience scores [20, 39] , our goal in this paper is to learn word salience scores and not sentence embeddings.", "startOffset": 91, "endOffset": 99}, {"referenceID": 38, "context": "However, unlike sentence embedding learning methods that do not learn word salience scores [20, 39] , our goal in this paper is to learn word salience scores and not sentence embeddings.", "startOffset": 91, "endOffset": 99}, {"referenceID": 25, "context": "Moreover, our work differs from Siamese CBOW [26] in that we do not learn word embeddings but take pretrained word embeddings as the input for learning word salience scores.", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "NWS scores we learn in this paper are also different from the salience scores learnt by [7] because they do not constrain their word salience scores such that they can be used to predict the words that occur in adjacent sentences.", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "Ideally, the normalisation term in the denominator in the softmax must be taken over all the sentences Sk in the corpus [6].", "startOffset": 120, "endOffset": 123}, {"referenceID": 17, "context": "Therefore, following noise-contrastive estimation [18], we approximate the normalisation term using a randomly sampled set of K sentences, where K is typically less than 10.", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "01 and subsequently scheduled by AdaGrad [15].", "startOffset": 41, "endOffset": 45}, {"referenceID": 3, "context": "As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 [4], 2013 [5], 2014 [3], and 2015 [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 [4], 2013 [5], 2014 [3], and 2015 [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 [4], 2013 [5], 2014 [3], and 2015 [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "As shown in Table 1, we use 18 benchmark datasets from SemEval STS tasks from years 2012 [4], 2013 [5], 2014 [3], and 2015 [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 16, "context": "We use the Fisher transformation [17] to test for the statistical significance of Pearson correlation coefficients.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "Following the success of Inverse Document Frequency (IDF) in filtering out high frequent words in text classification tasks [24], we define Inverse Sentence Frequency (ISF) of a word as the reciprocal of the number of sentences in which that word appears in a corpus.", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "SMOOTH is the unigram probabilitybased smoothing method proposed by [7].", "startOffset": 68, "endOffset": 71}, {"referenceID": 26, "context": "7 For reference purposes we show the level of performance we would obtain if we had used sentence embedding methods such as, skip-thought [27], and SiameseCBOW [26].", "startOffset": 138, "endOffset": 142}, {"referenceID": 25, "context": "7 For reference purposes we show the level of performance we would obtain if we had used sentence embedding methods such as, skip-thought [27], and SiameseCBOW [26].", "startOffset": 160, "endOffset": 164}, {"referenceID": 26, "context": "Because [27] did not report results for skip-thought on all 18 benchmark datasets used here, we report the re-evaluation of skip-thought on all 18 benchmark datasets by [38].", "startOffset": 8, "endOffset": 12}, {"referenceID": 37, "context": "Because [27] did not report results for skip-thought on all 18 benchmark datasets used here, we report the re-evaluation of skip-thought on all 18 benchmark datasets by [38].", "startOffset": 169, "endOffset": 173}, {"referenceID": 27, "context": "[28] show that by using SGNS embeddings as features in a kNearest Neighbour classifier, it is possible to accurately extrapolate the affective meanings of words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Moreover, perceived psycholinguistic properties of words such as concreteness (how \u201cpalpable\u201d the object the word refers to) and imageability (the intensity with which a word arouses images) have been successfully predicted using word embeddings [35, 31].", "startOffset": 246, "endOffset": 254}, {"referenceID": 30, "context": "Moreover, perceived psycholinguistic properties of words such as concreteness (how \u201cpalpable\u201d the object the word refers to) and imageability (the intensity with which a word arouses images) have been successfully predicted using word embeddings [35, 31].", "startOffset": 246, "endOffset": 254}, {"referenceID": 34, "context": "For example, [35] used the cosine similarity between word embeddings obtained via Latent Semantic Analysis (LSA) [13] to predict the concreteness and imageability ratings of words.", "startOffset": 13, "endOffset": 17}, {"referenceID": 12, "context": "For example, [35] used the cosine similarity between word embeddings obtained via Latent Semantic Analysis (LSA) [13] to predict the concreteness and imageability ratings of words.", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "On the other hand, prior work studying the relationship between human reading patterns using eyetracking devices show that there exist a high positive correlation between word salience and reading times [16, 19].", "startOffset": 203, "endOffset": 211}, {"referenceID": 18, "context": "On the other hand, prior work studying the relationship between human reading patterns using eyetracking devices show that there exist a high positive correlation between word salience and reading times [16, 19].", "startOffset": 203, "endOffset": 211}, {"referenceID": 36, "context": "[37], which contains valence, arousal, and dominance ratings collected via crowd sourcing for 13,915 words.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Measuring the salience of a word is an essential step in numerous NLP tasks. Heuristic approaches such as tfidf have been used so far to estimate the salience of words. We propose Neural Word Salience (NWS) scores, unlike heuristics, are learnt from a corpus. Specifically, we learn word salience scores such that, using pre-trained word embeddings as the input, can accurately predict the words that appear in a sentence, given the words that appear in the sentences preceding or succeeding that sentence. Experimental results on sentence similarity prediction show that the learnt word salience scores perform comparably or better than some of the state-of-the-art approaches for representing sentences on benchmark datasets for sentence similarity, while using only a fraction of the training and prediction times required by prior methods. Moreover, our NWS scores positively correlate with psycholinguistic measures such as concreteness, and imageability implying a close connection to the salience as perceived by humans.", "creator": "LaTeX with hyperref package"}}}