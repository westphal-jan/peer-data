{"id": "1709.02828", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Globally Normalized Reader", "abstract": "Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answer's sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type. This method improves the performance of all models considered in this work and is of independent interest for a variety of NLP tasks.", "histories": [["v1", "Fri, 8 Sep 2017 18:27:50 GMT  (332kb,D)", "http://arxiv.org/abs/1709.02828v1", "Presented at EMNLP 2017"]], "COMMENTS": "Presented at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jonathan raiman", "john miller"], "accepted": true, "id": "1709.02828"}, "pdf": {"name": "1709.02828.pdf", "metadata": {"source": "CRF", "title": "Globally Normalized Reader", "authors": ["Jonathan Raiman"], "emails": ["jonathanraiman@baidu.com", "millerjohn@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "Question answering (QA) and information extraction systems have proven to be invaluable in wide variety of applications such as medical information collection on drugs and genes (Quirk and\nPoon, 2016), large scale health impact studies (Althoff et al., 2016), or educational material development (Koedinger et al., 2015). Recent progress in neural-network based extractive question answering models are quickly closing the gap with human performance on several benchmark QA tasks such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al., 2016), or NewsQA (Trischler et al., 2016a). However, current approaches to extractive question answering face several limitations:\nar X\niv :1\n70 9.\n02 82\n8v 1\n[ cs\n.C L\n] 8\nS ep\n2 01\n7\n1. Computation is allocated equally to the entire document, regardless of answer location, with no ability to ignore or focus computation on specific parts. This limits applicability to longer documents.\n2. They rely extensively on expensive bidirectional attention mechanisms (Seo et al., 2016) or must rank all possible answer spans (Lee et al., 2016).\n3. While data-augmentation for question answering have been proposed (Zhou et al., 2017), current approaches still do not provide training data that can improve the performance of existing systems.\nIn this paper we demonstrate a methodology for addressing these three limitations, and make the following claims:\n1. Extractive Question Answering can be cast as a nested search process, where sentences provide a powerful document decomposition and an easy to learn search step. This factorization enables conditional computation to be allocated to sentences and spans likely to contain the right answer.\n2. When cast as a search process, models without bi-directional attention mechanisms and without ranking all possible answer spans can achieve near state of the art results on extractive question answering.\n3. Preserving narrative structure and explicitly incorporating type and question information into synthetic data generation is key to generating examples that actually improve the performance of question answering systems.\nOur claims are supported by experiments on the SQuAD dataset where we show that the Globally Normalized Reader (GNR), a model that performs an iterative search process through a document (shown visually in Figure 1), and has computation conditionally allocated based on the search process, achieves near state of the art Exact Match (EM) and F1 scores without resorting to more expensive attention or ranking of all possible spans. Furthermore, we demonstrate that Type Swaps, a type-aware data augmentation strategy that aligns named entities with a knowledge base and swaps them out for new entities that share the same type,\nimproves the performance of all models on extractive question answering.\nWe structure the paper as follows: in Section 2 we introduce the task and our model. Section 3 describes our data-augmentation strategy. Section 4 introduces our experiments and results. In Section 5 we discuss our findings. In Section 6 we relate our work to existing approaches. Conclusions and directions for future work are given in Section 7."}, {"heading": "2 Model", "text": "Given a document d and a question q, we pose extractive question answering as a search problem. First, we select the sentence, the first word of the span, and finally the last word of the span. A example of the output of the model is shown in Figure 1, and the network architecture is depicted in Figure 2.\nMore formally, let d1, . . . , dn denote each sentence in the document, and for each sentence di, let di,1, . . . , di,mi denote the word vectors corresponding to the words in the sentence. Similarly, let q1, . . . , q` denote the word vectors corresponding to words in the question. An answer is a tuple a = (i\u2217, j\u2217, k\u2217) indicating the correct sentence i\u2217, start word in the sentence j\u2217 and end word in the sentence k\u2217. Let A(d) denote the set of valid answer tuples for document d. We now describe each stage of the model in turn."}, {"heading": "2.1 Question Encoding", "text": "Each question is encoded by running a stack of bidirectional LSTM (Bi-LSTM) over each word in the question, producing hidden states (hfwd1 , h bwd 1 ), . . . , (h fwd ` , h bwd ` ) (Graves and Schmidhuber, 2005). Following Lee et al. (2016), these hidden states are used to compute a passage-independent question embedding, qindep. Formally,\nsj = w > q MLP([h bwd j ;h fwd j ]) (1) \u03b1j = exp(sj)\u2211`\nj\u2032=1 exp(sj\u2032) (2)\nqindep = \u2211\u0300 j=1 \u03b1j [h bwd j ;h fwd j ], (3)\nwhere wq is a trainable embedding vector, and MLP is a two-layer neural network with a Relu non-linearity. The question is represented by concatenating the final hidden states of the for-\nward and backward LSTMs and the passageindependent embedding, q = [hbwd1 ;h fwd ` ; q indep]."}, {"heading": "2.2 Question-Aware Document Encoding", "text": "Conditioned on the question vector, we compute a representation of each document word that is sensitive to both the surrounding context and the question. Specifically, each word in the document is represented as the concatenation of its word vector di,j , the question vector q, boolean features indicating if a word appears in the question or is repeated, and a question-aligned embedding from Lee et al. (2016). The question-aligned embedding qaligni,j is given by\nsi,j,k = MLP(di,j)>MLP(qk) (4) \u03b1i,j,k = exp(si,j,k)\u2211`\nk\u2032=1 exp(si,j,k\u2032) (5)\nq align i,j = \u2211\u0300 k=1 \u03b1i,j,kqk. (6)\nThe document is encoded by a separate stack of Bi-LSTMs, producing a sequence of hidden states (hfwd1,1 , h bwd 1,1 ), . . . , (h fwd n,mn , h bwd n,mn). The search procedure then operates on these hidden states."}, {"heading": "2.3 Answer Selection", "text": "Sentence selection. The first phase of our search process picks the sentence that contains the answer span. Each sentence di is represented by the hidden state of the first and last word in the sentence for the backward and forward LSTM respectively, [hbwdi,1 ;h fwd i,mi\n], and is scored by passing this representation through a fully connected layer that outputs the unnormalized sentence score for sentence di, denoted \u03c6sent(di).\nSpan start selection. After selecting a sentence di, we pick the start of the answer span within the sentence. Each potential start word di,j is represented as its corresponding document encoding [hfwdi,j ;h bwd i,j ], and is scored by passing this encoding through a fully connected layer that outputs the unnormalized start word score for word j in sentence i, denoted \u03c6sw(di,j).\nSpan end selection. Conditioned on sentence di and starting word di,j , we select the end word from the remaining words in the sentence di,j , . . . , di,mi . To do this, we run a BiLSTM over the remaining document hidden states\n(hfwdi,j , h bwd i,j ), . . . , (h fwd i,mi , hbwdi,mi) to produce representations (h\u0303fwdi,j , h\u0303 bwd i,j ), . . . , (h\u0303 fwd i,mi\n, h\u0303bwdi,mi). Each end word di,k is then scored by passing [h\u0303fwdi,k ; h\u0303 bwd i,k ] through a fully connected layer that outputs the unnormalized end word score for word k in sentence i, with start word j, denoted \u03c6ew(di,j:k)."}, {"heading": "2.4 Global Normalization", "text": "The scores for each stage of our model can be normalized at the local or global level. Previous work demonstrated that locally-normalized models have a weak ability to correct mistakes made in previous decisions, while globally normalized models are strictly more expressive than locally normalized models (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).\nIn a locally normalized model each decision is made conditional on the previous decision. The probability of some answer a = (i, j, k) is decomposed as\nP(a|d, q) =Psent(i|d, q) \u00b7 Psw(j|i, d, q)\u00b7 Pew(k|j, i, d, q).\n(7)\nEach sub-decision is locally normalized by applying a softmax to the relevant selection scores:\nPsent(i|d, q) = exp(\u03c6sent(di))\u2211n\nx=1 exp(\u03c6sent(dx)) , (8)\nPsw(j|i, d, q) = exp(\u03c6sw(di,j))\u2211mi x=1 exp(\u03c6sw(di,x)) , (9)\nPew(k|j, i, d, q) = exp(\u03c6ew(di,j:k))\u2211mi x=j exp(\u03c6ew(di,j:x)) .\n(10) To allow our model to recover from incorrect sentence or start word selections, we instead globally normalize the scores from each stage of our procedure. In a globally normalized model, we define\nscore(a, d, q) = \u03c6sent(di)+\u03c6sw(di,j)+\u03c6ew(di,j:k). (11) Then, we model\nP(a | d, q) = exp(score(a, d, q)) Z , (12)\nwhere Z is the partition function Z = \u2211\na\u2032\u2208A(d)\nexp(score(a\u2032, d, q)). (13)\nIn contrast to locally-normalized models, the model is normalized over all possible search paths instead of normalizing each step of search procedure. At inference time, the problem is to find\narg max a\u2208A(d)\nP(a | d, q), (14)\nwhich can be approximately computed using beam search."}, {"heading": "2.5 Objective and Training", "text": "We minimize the negative log-likelihood on the training set using stochastic gradient descent. For a single example (a, d, q), the negative loglikelihood\n\u2212score(a, d, q) + logZ (15)\nrequires an expensive summation to compute logZ. Instead, to ensure learning is efficient, we use beam search during training and early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004). Concretely, we approximate Z by summing only over candidates on the final beam B:\nZ \u2248 \u2211 a\u2032\u2208B exp(score(a\u2032, d, q)). (16)\nAt training time, if the gold sequence falls off the beam at step t during decoding, a stochastic gradient step is performed on the partial objective computed through step t and normalized over the beam at step t."}, {"heading": "2.6 Implementation", "text": "Our best performing model uses a stack of 3 BiLSTMs for the question and document encodings, and a single Bi-LSTM for the end of span prediction. The hidden dimension of all recurrent layers is 200. I We use the 300 dimensional 8.4B token Common Crawl GloVe vectors (Pennington et al., 2014). Words missing from the Common Crawl vocabulary are set to zero. In our experiments, all architectures considered have sufficient capacity to overfit the training set. We regularize the models by fixing the word embeddings throughout training, dropping out the inputs of the Bi-LSTMs with probability 0.3 and the inputs to the fullyconnected layers with probability 0.4 (Srivastava et al., 2014), and adding gaussian noise to the recurrent weights with \u03c3 = 10\u22126. Our models are trained using Adam with a learning rate of 0.0005, \u03b21 = 0.9, \u03b22 = 0.999, = 10\u22128 and a batch size of 32 (Kingma and Ba, 2014).\nAll our experiments are implemented in Tensorflow (Abadi et al., 2016), and we tokenize using Ciseau (Raiman, 2017). Despite performing beam-search during training, our model trains to convergence in under 4 hours through the use of efficient LSTM primitives in CuDNN (Chetlur et al., 2014) and batching our computation over examples and search beams. We release our code and augmented dataset.1\nOur implementation of the GNR is 24.7\n1https://github.com/baidu-research/ GloballyNormalizedReader\ntimes faster at inference time than the official Bi-Directional Attention Flow implementation2. Specifically, on a machine running Ubuntu 14 with 40 Intel Xeon 2.6Ghz processors, 386GB of RAM, and a 12GB TitanX-Maxwell GPU, the GNR with beam size 32 and batch size 32 requires 51.58 \u00b1 0.266 seconds (mean \u00b1 std)3 to process the SQUAD validation set. By contrast, the BiDirectional Attention Flow model with batch size 32 requires 1260.23\u00b117.26 seconds. We attribute this speedup to avoiding expensive bi-directional attention mechanisms and making computation conditional on the search beams."}, {"heading": "3 Type Swaps", "text": "In extractive question answering, the set of possible answer spans can be pruned by only keeping answers whose nature (person, object, place, date, etc.) agrees with the question type (Who, What, Where, When, etc.). While this heuristic helps human readers filter out irrelevant parts of a document when searching for information, no explicit supervision of this kind is present in the dataset. Despite this absence, the distribution question representations learned by our models appear to utilize this heuristic. The final hidden state of the\n2https://github.com/allenai/ bi-att-flow\n3All numbers are averaged over 5 runs.\nquestion-encoding LSTMs naturally cluster based on question type (Table 1).\nIn other words, the task induces a question encoding that superficially respects type information. This property is a double-edged sword: it allows the model to easily weed out answers that are inapplicable, but also leads it astray by selecting a text span that shares the answer\u2019s type but has the wrong underlying entity. A similar observation was made in the error analysis of (Weissenborn et al., 2017). We propose Type Swaps, an augmentation strategy that leverages this emergent behavior in order to improve the model\u2019s ability to prune wrong answers, and make it more robust to surface form variation. This strategy has three steps:\n1. Locate named entities in document and question.\n2. Collect surface variation for each entity type:\nhuman \u2192 {Ada Lovelace, Daniel Kahnemann,...},\ncountry\u2192 {USA, France, ...}, ...\n3. Generate new document-question-answer examples by swapping each named entity in an original triplet with a surface variant that shares the same type from the collection.\nAssigning types to named entities in natural language is an open problem, nonetheless when faced\nwith documents where we can safely assume that the majority of the entities will be contained in a large knowledge base (KB) such as Wikidata Vrandec\u030cic\u0301 and Kro\u0308tzsch (2014) we find that simple string matching techniques are sufficiently accurate. Specifically, we use a part of speech tagger (Honnibal, 2017) to extract nominal groups in the training data and string-match them with entities in Wikidata. Using this technique, we are able to extract 47,598 entities in SQuAD that fall under 6,380 Wikidata instance of4 types. Additionally we assign \u201cnumber types\u201d (e.g. year, day of the week, distance, etc.) to nominal groups that contain dates, numbers, or quantities5. These extraction steps produce 84,632 unique surface variants (on average 16.93 per type) with the majority of the variation found in humans, numbers or organizations as visible in Figure 4.\nWith this method, we can generate 2.92 \u00b7 10369 unique documents (average of 3.36 \u00b7 10364 new documents for each original document). To ensure there is sufficient variation in the generated documents, we sample from this set and only keep variations where the question or answer is mutated. At each training epoch, we train on T Type Swap ex-\n4https://www.wikidata.org/wiki/ Property:P31\n5In our experiments we found that not including numerical variation in the generated examples led to an imbalanced dataset and lower final performance.\namples and the full original training data. An example output of the method is shown in Figure 3."}, {"heading": "4 Results", "text": "We evaluate our model on the 100,000 example SQuAD dataset (Rajpurkar et al., 2016) and perform several ablations to evaluate the relative importance of the proposed methods."}, {"heading": "4.1 Learning to Search", "text": "In our first experiment, we aim to quantify the importance of global normalization on the learning and search process. We use T = 104 Type Swap samples and vary beam width B between 1 and 32 for a locally and globally normalized models and summarize the Exact-Match and F1 score of the model\u2019s predicted answer and ground truth computed using the evaluation scripts from (Rajpurkar et al., 2016) (Table 3). We additionally report another metric, the Sentence score, which is a measure for how often the predicted answer came from the correct sentence. This metric provides a measure for where mistakes are made during prediction."}, {"heading": "4.2 Type Swaps", "text": "In our second experiment, we evaluate the impact of the amount of augmented data on the performance of our model. In this experiment, we use\nthe best beam sizes for each model (B = 10 for local andB = 32 for global) and vary the augmentation from T = 0 (no augmentation) to T = 5 \u00b7104. The results of this experiment are summarized in (Table 4).\nWe observe that both models improve in performance with T > 0 and performance degrades past T = 104. Moreover, data augmentation and global normalization are complementary. Combined, we obtain 1.6 EM and 2.0 F1 improvement over the locally normalized baseline.\nWe also verify that the effects of Type Swaps are not limited to our specific model by observing the impact of augmented data on the DCN+ (Xiong et al., 2016)6. We find that it strongly reduces generalization error, and helps improve F1, with potential further improvements coming by re-\n6 The DCN+ is the DCN with additional hyperparameter tuning by the same authors as submitted on the SQuAD leaderboard https://rajpurkar.github. io/SQuAD-explorer/.\nducing other forms of regularization (Table 5)."}, {"heading": "5 Discussion", "text": "In this section we will discuss the results presented in Section 4, and explain how they relate to our main claims."}, {"heading": "5.1 Extractive Question Answering as a Search Problem", "text": "Sentences provide a natural and powerful document decomposition for search that can be easily learnt as a search step: for all the models and configurations considered, the Sentence score was\nabove 88% correct (Table 3)7. Thus, sentence selection is the easy part of the problem, and the model can allocate more computation (such as the end-word selection Bi-LSTM) to spans likely to contain the answer. This approach avoids wasteful work on unpromising spans and is important for further scaling these methods to long documents."}, {"heading": "5.2 Global Normalization", "text": "The Globally Normalized Reader outperforms previous approaches and achieves the second highest EM behind (Wang et al., 2017), without using bi-directional attention and only scoring spans in its final beam. Increasing the beam width improves the results for both locally and globally normalized models (Table 3), suggesting search errors account for a significant portion of the performance difference between models. Models such as Lee et al. (2016) and Wang and Jiang (2016) overcome this difficulty by ranking all possible spans and thus never skipping a possible answer. Even with large beam sizes, the locally normalized model underperforms these approaches. However, by increasing model flexibility and performing search during training, the globally normalized model is able to recover from search errors and achieve much of the benefits of scoring all possible spans."}, {"heading": "5.3 Type-Aware Data Augmentation", "text": "Type Swaps, our data augmentation strategy, offers a way to incorporate the nature of the question and the types of named entities in the answers into the learning process of our model and reduce sensitivity to surface variation. Existing neuralnetwork approaches to extractive QA have so far ignored this information. Augmenting the dataset with additional type-sensitive synthetic examples improves performance by providing better coverage of different answer types. Growing the number of augmented samples used improves the performance of all models under study (Table 4-5). With T \u2208 [104, 5 \u00b7 104], (EM, F1) improve from (65.8 \u2192 66.7, 74.0 \u2192 75.0) for locally normalized models, and (66.6 \u2192 68.4, 75.0 \u2192 76.21)\n7The objective function difference explains the lower performance of globally versus locally normalized models on the Sentence score: local models must always assign the highest probability to the correct sentence, while global models only ensure the correct span has the highest probability. Thus global models do not need to enforce a high margin between the correct answer\u2019s sentence score and others and are more likely to keep alternate sentences around.\nfor globally normalized models. Past a certain amount of augmentation, we observe performance degradation. This suggests that despite efforts to closely mimic the original training set, there is a train-test mismatch or excess duplication in the generated examples.\nOur experiments are conducted on two vastly different architectures and thus these benefits are expected to carry over to different models (Weissenborn et al., 2017; Seo et al., 2016; Wang et al., 2017), and perhaps more broadly in other natural language tasks that contain named entities and have limited supervised data."}, {"heading": "6 Related Work", "text": "Our work is closely related to existing approaches in learning to search, extractive question answering, and data augmentation for NLP tasks.\nLearning to Search. Several approaches to learning to search have been proposed for various NLP tasks and conditional computation. Most recently, Andor et al. (2016) and Zhou et al. (2015) demonstrated the effectiveness of globally normalized networks and training with beam search for part of speech tagging and transition-based dependency parsing, while Wiseman and Rush (2016) showed that these techniques could also be applied to sequence-to-sequence models in several application areas including machine translation. These works focus on parsing and sequence prediction tasks and have a fixed computation regardless of the search path, while we show that the same techniques can also be straightforwardly applied to question answering and extended to allow for conditional computation based on the search path.\nLearning to search has also been used in context of modular neural networks with conditional computation in the work of Andreas et al. (2016) for image captioning. In their work reinforcement learning was used to learn how to turn on and off computation, while we find that conditional computation can be easily learnt with maximum likelihood and the help of early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004) to guide the training process.\nOur framework for conditional computation whereby the search space is pruned by a sequence of increasingly complex models is broadly reminiscent of the structured prediction cascades of (Weiss and Taskar, 2010). Trischler et al. (2016b)\nalso explored this approach in the context of question answering.\nExtractive Question Answering. Since the introduction of the SQuAD dataset, numerous systems have achieved strong results. Seo et al. (2016); Wang et al. (2017) and Xiong et al. (2016) make use of a bi-directional attention mechanisms, whereas the GNR is more lightweight and achieves similar results without this type of attention mechanism. The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al. (2016) and Wang and Jiang (2016) must score all O(N2) possible answer spans, making training and inference expensive. The GNR avoids this complexity by learning to search during training and outperforms both systems while scoring only O(B) spans. Weissenborn et al. (2017) is a locally normalized model that first predicts start and then end words of each span. Our experiments lead us to believe that further factorizing the problem and using global normalization along with our data augmentation would yield corresponding improvements.\nData augmentation. Several works use data augmentation to control the generalization error of deep learning models. Zhang and LeCun (2015) use a thesaurus to generate new training examples based on synonyms. Vijayaraghavan et al. (2016) employs a similar method, but uses Word2vec and cosine similarity to find similar words. Jia and Liang (2016) use a high-precision synchronous context-free grammar to generate new semantic parsing examples. Our data augmentation technique, Type Swaps, is unique in that it leverages an external knowledge-base to provide new examples that have more variation and finer-grained changes than methods that use only a thesaurus or Word2Vec, while also keeping the narrative and grammatical structure intact.\nMore recently Zhou et al. (2017) proposed a sequence-to-sequence model to generate diverse and realistic training question-answer pairs on SQuAD. Similar to their approach, our technique makes use of existing examples to produce new examples that are fluent, however we also are able to explicitly incorporate entity type information into the generation process and use the generated data to improve the performance of question answering models."}, {"heading": "7 Conclusions and Future Work", "text": "In this work, we provide a methodology that overcomes several limitations of existing approaches to extractive question answering. In particular, our proposed model, the Globally Normalized Reader, reduces the computational complexity of previous models by casting the question answering as search and allocating more computation to promising answer spans. Empirically, we find that this approach, combined with global normalization and beam search during training, leads to near state of the art results. Furthermore, we find that a type-aware data augmentation strategy improves the performance of all models under study on the SQuAD dataset. The method is general, only requiring that the training data contains named entities from a large KB. We expect it to be applicable to other NLP tasks that would benefit from more training data.\nAs future work we plan to apply the GNR to other question answering datasets such as MS MARCO (Nguyen et al., 2016) or NewsQA (Trischler et al., 2016a), as well as investigate the applicability and benefits of Type Swaps to other tasks like named entity recognition, entity linking, machine translation, and summarization. Finally, we believe there a broad range of structured prediction problems (code generation, generative models for images, audio, or videos) where the size of original search space makes current techniques intractable, but if cast as learning-to-search problems with conditional computation, might be within reach."}, {"heading": "Acknowledgments", "text": "We would like to thank the anonymous reviewers for their valuable feedback. In addition, we thank Adam Coates, Carl Case, Andrew Gibiansky, and Szymon Sidor for thoughtful comments and fruitful discussion. We also thank James Bradbury and Bryan McCann for running Type Swap experiments on the DCN+."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Influence of pok\u00e9mon go on physical activity: Study and implications", "author": ["Tim Althoff", "Ryen W White", "Eric Horvitz."], "venue": "Journal of Medical Internet Research, 18(12).", "citeRegEx": "Althoff et al\\.,? 2016", "shortCiteRegEx": "Althoff et al\\.", "year": 2016}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "arXiv preprint arXiv:1603.06042.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "arXiv preprint arXiv:1601.01705.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer."], "venue": "arXiv preprint arXiv:1410.0759.", "citeRegEx": "Chetlur et al\\.,? 2014", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 111. Association for Computational Linguistics.", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Spacy", "author": ["Matthew Honnibal."], "venue": "https:// github.com/explosion/spaCy.", "citeRegEx": "Honnibal.,? 2017", "shortCiteRegEx": "Honnibal.", "year": 2017}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.03622.", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Data mining and education", "author": ["Kenneth R Koedinger", "Sidney D\u2019Mello", "Elizabeth A McLaughlin", "Zachary A Pardos", "Carolyn P Ros\u00e9"], "venue": "Wiley Interdisciplinary Reviews: Cognitive Science,", "citeRegEx": "Koedinger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koedinger et al\\.", "year": 2015}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das."], "venue": "arXiv preprint arXiv:1611.01436.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction beyond the sentence boundary", "author": ["Chris Quirk", "Hoifung Poon."], "venue": "arXiv preprint arXiv:1609.04873.", "citeRegEx": "Quirk and Poon.,? 2016", "shortCiteRegEx": "Quirk and Poon.", "year": 2016}, {"title": "Ciseau", "author": ["Jonathan Raiman."], "venue": "https:// github.com/jonathanraiman/ciseau.", "citeRegEx": "Raiman.,? 2017", "shortCiteRegEx": "Raiman.", "year": 2017}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603.", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830.", "citeRegEx": "Trischler et al\\.,? 2016a", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "A parallel-hierarchical model for machine comprehension on sparse data", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Phillip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1603.08884.", "citeRegEx": "Trischler et al\\.,? 2016b", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Deepstance at semeval-2016 task 6: Detecting stance in tweets using character and word-level cnns", "author": ["Prashanth Vijayaraghavan", "Ivan Sysoev", "Soroush Vosoughi", "Deb Roy."], "venue": "arXiv preprint arXiv:1606.05694.", "citeRegEx": "Vijayaraghavan et al\\.,? 2016", "shortCiteRegEx": "Vijayaraghavan et al\\.", "year": 2016}, {"title": "Wikidata: a free collaborative knowledgebase", "author": ["Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch."], "venue": "Communications of the ACM, 57(10):78\u201385.", "citeRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.,? 2014", "shortCiteRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.", "year": 2014}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Gated self-matching networks for reading comprehension and question answering", "author": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Structured prediction cascades", "author": ["David Weiss", "Benjamin Taskar."], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 916\u2013923.", "citeRegEx": "Weiss and Taskar.,? 2010", "shortCiteRegEx": "Weiss and Taskar.", "year": 2010}, {"title": "Fastqa: A simple and efficient neural architecture for question answering", "author": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe."], "venue": "arXiv preprint arXiv:1703.04816.", "citeRegEx": "Weissenborn et al\\.,? 2017", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.02960.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604.", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun."], "venue": "arXiv preprint arXiv:1502.01710.", "citeRegEx": "Zhang and LeCun.,? 2015", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "ACL (1), pages 1213\u20131222.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Neural question generation from text: A preliminary study", "author": ["Qingyu Zhou", "Nan Yang", "Furu Wei", "Chuanqi Tan", "Hangbo Bao", "Ming Zhou."], "venue": "arXiv preprint arXiv:1704.01792.", "citeRegEx": "Zhou et al\\.,? 2017", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "Poon, 2016), large scale health impact studies (Althoff et al., 2016), or educational material development (Koedinger et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 10, "context": ", 2016), or educational material development (Koedinger et al., 2015).", "startOffset": 45, "endOffset": 69}, {"referenceID": 16, "context": "Recent progress in neural-network based extractive question answering models are quickly closing the gap with human performance on several benchmark QA tasks such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al.", "startOffset": 172, "endOffset": 196}, {"referenceID": 12, "context": ", 2016), MS MARCO (Nguyen et al., 2016), or NewsQA (Trischler et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 19, "context": ", 2016), or NewsQA (Trischler et al., 2016a).", "startOffset": 19, "endOffset": 44}, {"referenceID": 17, "context": "They rely extensively on expensive bidirectional attention mechanisms (Seo et al., 2016) or must rank all possible answer spans (Lee et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 11, "context": ", 2016) or must rank all possible answer spans (Lee et al., 2016).", "startOffset": 47, "endOffset": 65}, {"referenceID": 31, "context": "While data-augmentation for question answering have been proposed (Zhou et al., 2017), current approaches still do not provide training data that can improve the performance of existing systems.", "startOffset": 66, "endOffset": 85}, {"referenceID": 6, "context": ", (h fwd ` , h bwd ` ) (Graves and Schmidhuber, 2005).", "startOffset": 23, "endOffset": 53}, {"referenceID": 6, "context": ", (h fwd ` , h bwd ` ) (Graves and Schmidhuber, 2005). Following Lee et al. (2016), these hidden states are used to compute a passage-independent question embedding, qindep.", "startOffset": 24, "endOffset": 83}, {"referenceID": 11, "context": "Specifically, each word in the document is represented as the concatenation of its word vector di,j , the question vector q, boolean features indicating if a word appears in the question or is repeated, and a question-aligned embedding from Lee et al. (2016). The question-aligned embedding q i,j is given by", "startOffset": 241, "endOffset": 259}, {"referenceID": 2, "context": "Previous work demonstrated that locally-normalized models have a weak ability to correct mistakes made in previous decisions, while globally normalized models are strictly more expressive than locally normalized models (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 219, "endOffset": 283}, {"referenceID": 30, "context": "Previous work demonstrated that locally-normalized models have a weak ability to correct mistakes made in previous decisions, while globally normalized models are strictly more expressive than locally normalized models (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 219, "endOffset": 283}, {"referenceID": 5, "context": "Previous work demonstrated that locally-normalized models have a weak ability to correct mistakes made in previous decisions, while globally normalized models are strictly more expressive than locally normalized models (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 219, "endOffset": 283}, {"referenceID": 2, "context": "Instead, to ensure learning is efficient, we use beam search during training and early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 95, "endOffset": 159}, {"referenceID": 30, "context": "Instead, to ensure learning is efficient, we use beam search during training and early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 95, "endOffset": 159}, {"referenceID": 5, "context": "Instead, to ensure learning is efficient, we use beam search during training and early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 95, "endOffset": 159}, {"referenceID": 13, "context": "4B token Common Crawl GloVe vectors (Pennington et al., 2014).", "startOffset": 36, "endOffset": 61}, {"referenceID": 18, "context": "4 (Srivastava et al., 2014), and adding gaussian noise to the recurrent weights with \u03c3 = 10\u22126.", "startOffset": 2, "endOffset": 27}, {"referenceID": 9, "context": "999, = 10\u22128 and a batch size of 32 (Kingma and Ba, 2014).", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "All our experiments are implemented in Tensorflow (Abadi et al., 2016), and we tokenize using Ciseau (Raiman, 2017).", "startOffset": 50, "endOffset": 70}, {"referenceID": 15, "context": ", 2016), and we tokenize using Ciseau (Raiman, 2017).", "startOffset": 38, "endOffset": 52}, {"referenceID": 4, "context": "Despite performing beam-search during training, our model trains to convergence in under 4 hours through the use of efficient LSTM primitives in CuDNN (Chetlur et al., 2014) and batching our computation over examples and search beams.", "startOffset": 151, "endOffset": 173}, {"referenceID": 26, "context": "A similar observation was made in the error analysis of (Weissenborn et al., 2017).", "startOffset": 56, "endOffset": 82}, {"referenceID": 7, "context": "Specifically, we use a part of speech tagger (Honnibal, 2017) to extract nominal groups in the training data and string-match them with entities in Wikidata.", "startOffset": 45, "endOffset": 61}, {"referenceID": 21, "context": "with documents where we can safely assume that the majority of the entities will be contained in a large knowledge base (KB) such as Wikidata Vrande\u010di\u0107 and Kr\u00f6tzsch (2014) we find that simple string matching techniques are sufficiently accurate.", "startOffset": 142, "endOffset": 172}, {"referenceID": 16, "context": "We evaluate our model on the 100,000 example SQuAD dataset (Rajpurkar et al., 2016) and perform several ablations to evaluate the relative importance of the proposed methods.", "startOffset": 59, "endOffset": 83}, {"referenceID": 16, "context": "We use T = 104 Type Swap samples and vary beam width B between 1 and 32 for a locally and globally normalized models and summarize the Exact-Match and F1 score of the model\u2019s predicted answer and ground truth computed using the evaluation scripts from (Rajpurkar et al., 2016) (Table 3).", "startOffset": 252, "endOffset": 276}, {"referenceID": 16, "context": "Model EM F1 Human (Rajpurkar et al., 2016) 80.", "startOffset": 18, "endOffset": 42}, {"referenceID": 16, "context": "Single model Sliding Window (Rajpurkar et al., 2016) 13.", "startOffset": 28, "endOffset": 52}, {"referenceID": 23, "context": "2 Match-LSTM (Wang and Jiang, 2016) 64.", "startOffset": 13, "endOffset": 35}, {"referenceID": 28, "context": "9 DCN (Xiong et al., 2016) 65.", "startOffset": 6, "endOffset": 26}, {"referenceID": 11, "context": "6 Rasor (Lee et al., 2016) 66.", "startOffset": 8, "endOffset": 26}, {"referenceID": 17, "context": "9 Bi-Attention Flow (Seo et al., 2016) 67.", "startOffset": 20, "endOffset": 38}, {"referenceID": 24, "context": "3 R-Net(Wang et al., 2017) 72.", "startOffset": 7, "endOffset": 26}, {"referenceID": 28, "context": "We also verify that the effects of Type Swaps are not limited to our specific model by observing the impact of augmented data on the DCN+ (Xiong et al., 2016)6.", "startOffset": 138, "endOffset": 158}, {"referenceID": 24, "context": "The Globally Normalized Reader outperforms previous approaches and achieves the second highest EM behind (Wang et al., 2017), without using bi-directional attention and only scoring spans in its final beam.", "startOffset": 105, "endOffset": 124}, {"referenceID": 11, "context": "Models such as Lee et al. (2016) and Wang and Jiang (2016) overcome this difficulty by ranking all possible spans and thus never skipping a possible answer.", "startOffset": 15, "endOffset": 33}, {"referenceID": 11, "context": "Models such as Lee et al. (2016) and Wang and Jiang (2016) overcome this difficulty by ranking all possible spans and thus never skipping a possible answer.", "startOffset": 15, "endOffset": 59}, {"referenceID": 26, "context": "Our experiments are conducted on two vastly different architectures and thus these benefits are expected to carry over to different models (Weissenborn et al., 2017; Seo et al., 2016; Wang et al., 2017), and perhaps more broadly in other natural language tasks that contain named entities and have limited supervised data.", "startOffset": 139, "endOffset": 202}, {"referenceID": 17, "context": "Our experiments are conducted on two vastly different architectures and thus these benefits are expected to carry over to different models (Weissenborn et al., 2017; Seo et al., 2016; Wang et al., 2017), and perhaps more broadly in other natural language tasks that contain named entities and have limited supervised data.", "startOffset": 139, "endOffset": 202}, {"referenceID": 24, "context": "Our experiments are conducted on two vastly different architectures and thus these benefits are expected to carry over to different models (Weissenborn et al., 2017; Seo et al., 2016; Wang et al., 2017), and perhaps more broadly in other natural language tasks that contain named entities and have limited supervised data.", "startOffset": 139, "endOffset": 202}, {"referenceID": 2, "context": "Most recently, Andor et al. (2016) and Zhou et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "Most recently, Andor et al. (2016) and Zhou et al. (2015) demonstrated the effectiveness of globally normalized networks and training with beam search for part of speech tagging and transition-based dependency parsing, while Wiseman and Rush (2016) showed that these techniques could also be applied to sequence-to-sequence models in several application areas including machine translation.", "startOffset": 15, "endOffset": 58}, {"referenceID": 2, "context": "Most recently, Andor et al. (2016) and Zhou et al. (2015) demonstrated the effectiveness of globally normalized networks and training with beam search for part of speech tagging and transition-based dependency parsing, while Wiseman and Rush (2016) showed that these techniques could also be applied to sequence-to-sequence models in several application areas including machine translation.", "startOffset": 15, "endOffset": 249}, {"referenceID": 2, "context": "In their work reinforcement learning was used to learn how to turn on and off computation, while we find that conditional computation can be easily learnt with maximum likelihood and the help of early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004) to guide the training process.", "startOffset": 209, "endOffset": 273}, {"referenceID": 30, "context": "In their work reinforcement learning was used to learn how to turn on and off computation, while we find that conditional computation can be easily learnt with maximum likelihood and the help of early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004) to guide the training process.", "startOffset": 209, "endOffset": 273}, {"referenceID": 5, "context": "In their work reinforcement learning was used to learn how to turn on and off computation, while we find that conditional computation can be easily learnt with maximum likelihood and the help of early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004) to guide the training process.", "startOffset": 209, "endOffset": 273}, {"referenceID": 2, "context": "Learning to search has also been used in context of modular neural networks with conditional computation in the work of Andreas et al. (2016) for image captioning.", "startOffset": 120, "endOffset": 142}, {"referenceID": 25, "context": "Our framework for conditional computation whereby the search space is pruned by a sequence of increasingly complex models is broadly reminiscent of the structured prediction cascades of (Weiss and Taskar, 2010).", "startOffset": 186, "endOffset": 210}, {"referenceID": 19, "context": "Trischler et al. (2016b)", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Seo et al. (2016); Wang et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "Seo et al. (2016); Wang et al. (2017) and Xiong et al.", "startOffset": 0, "endOffset": 38}, {"referenceID": 16, "context": "Seo et al. (2016); Wang et al. (2017) and Xiong et al. (2016) make use of a bi-directional attention mechanisms, whereas the GNR is more lightweight and achieves similar results without this type of attention mechanism.", "startOffset": 0, "endOffset": 62}, {"referenceID": 11, "context": "The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 11, "context": "The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al. (2016) and Wang and Jiang (2016) must score all O(N2) possible answer spans, making training and inference expensive.", "startOffset": 63, "endOffset": 114}, {"referenceID": 11, "context": "The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al. (2016) and Wang and Jiang (2016) must score all O(N2) possible answer spans, making training and inference expensive.", "startOffset": 63, "endOffset": 140}, {"referenceID": 11, "context": "The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al. (2016) and Wang and Jiang (2016) must score all O(N2) possible answer spans, making training and inference expensive. The GNR avoids this complexity by learning to search during training and outperforms both systems while scoring only O(B) spans. Weissenborn et al. (2017) is a locally normalized model that first predicts start and then end words of each span.", "startOffset": 63, "endOffset": 380}, {"referenceID": 27, "context": "Zhang and LeCun (2015) use a thesaurus to generate new training examples based on synonyms.", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "Vijayaraghavan et al. (2016) employs a similar method, but uses Word2vec and cosine similarity to find similar words.", "startOffset": 0, "endOffset": 29}, {"referenceID": 8, "context": "Jia and Liang (2016) use a high-precision synchronous context-free grammar to generate new semantic parsing examples.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "More recently Zhou et al. (2017) proposed a sequence-to-sequence model to generate diverse and realistic training question-answer pairs on SQuAD.", "startOffset": 14, "endOffset": 33}, {"referenceID": 12, "context": "As future work we plan to apply the GNR to other question answering datasets such as MS MARCO (Nguyen et al., 2016) or NewsQA (Trischler et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 19, "context": ", 2016) or NewsQA (Trischler et al., 2016a), as well as investigate the applicability and benefits of Type Swaps to other tasks like named entity recognition, entity linking, machine translation, and summarization.", "startOffset": 18, "endOffset": 43}], "year": 2017, "abstractText": "Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bidirectional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answer\u2019s sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type. This method improves the performance of all models considered in this work and is of independent interest for a variety of NLP tasks.", "creator": "LaTeX with hyperref package"}}}