{"id": "1709.02984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2017", "title": "Sentiment Polarity Detection for Software Development", "abstract": "The role of sentiment analysis is increasingly emerging to study software developers' emotions by mining crowd-generated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers' communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexicon- and keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines.", "histories": [["v1", "Sat, 9 Sep 2017 17:28:10 GMT  (2157kb)", "http://arxiv.org/abs/1709.02984v1", null], ["v2", "Mon, 25 Sep 2017 14:37:55 GMT  (2157kb)", "http://arxiv.org/abs/1709.02984v2", "Cite as: Calefato, F., Lanubile, F., Maiorano, F., Novielli N. Empir Software Eng (2017).this https URLFull-text view-only version here:this http URL, Empir Software Eng (2017)"]], "reviews": [], "SUBJECTS": "cs.SE cs.CL", "authors": ["fabio calefato", "filippo lanubile", "federico maiorano", "nicole novielli"], "accepted": false, "id": "1709.02984"}, "pdf": {"name": "1709.02984.pdf", "metadata": {"source": "CRF", "title": "Sentiment Polarity Detection for Software Development", "authors": ["Fabio Calefato", "Filippo Lanubile", "Federico Maiorano", "Nicole Novielli"], "emails": ["nicole.novielli}@uniba.it,", "f.maiorano2@studenti.uniba.it"], "sections": [{"heading": null, "text": "generated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers\u2019 communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexicon- and keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines.\nKeywords - Sentiment Analysis; Communication Channels; Stack Overflow; Word Embedding; Social Software Engineering."}, {"heading": "1. Introduction", "text": "Sentiment analysis is the study of the subjectivity (neutral vs. emotionally loaded) and polarity (positive vs. negative) of a text (Pang and Lee 2008). It relies on sentiment lexicons, that is, large collections of words, each annotated with its own positive or negative orientation (i.e., prior polarity). The overall sentiment of a text is therefore computed upon the prior polarity of the contained words.\nRecent studies suggest approaches for enhancing software development, maintenance, and evolution by applying sentiment analysis on Stack Overflow (Rahman et al. 2015), app reviews (Maalej et al. 2016), and tweets containing comments on software applications (Guzman et al. 2016). Further research on developers\u2019 emotions addresses the role of affect in social software engineering, by applying sentiment analysis to the content available in collaborative development environments such as GitHub (Guzman et al. 2014, Guzman and Bruegge 2013, Sinha et al.2016), Jira (M\u00e4ntyl\u00e4 et al. 2016, Ortu et al. 2015), and Stack Overflow (Calefato et al. 2015, Novielli et al. 2015).\nWith a notable few exceptions (Blaz and Becker 2016, Panichella et al. 2015), empirical software engineering studies have exploited off-the-shelf sentiment analysis tools that have been trained on non-software engineering documents, such as movie reviews (Socher et al. 2013), or posts crawled from general-purpose social media, such as Twitter and YouTube (Thelwall et al. 2012). Jongeling et al. (2017) show how the choice of the sentiment analysis tool may impact the conclusion validity of empirical studies by performing a benchmarking study on seven datasets, including discussions and comments from Stack Overflow and issue trackers. By comparing the predictions of widely used off-the-shelf sentiment analysis tools, they show that not only these tools do not agree with human annotation of developers\u2019 communication channels, but they also disagree among themselves.\nAnother challenge to address is the negative bias of existing sentiment analysis tools, that is the misclassification of neutral technical texts as emotionally negative. It is particularly the case of bug reports or problem descriptions (Blaz and Becker 2016, Novielli et al. 2015). Novielli et al. (2015) show how sentences like \u201cWhat is the best way to kill a critical process\u201d or \u201cI am missing a parenthesis but I don\u2019t know where\u201d are erroneously classified as negative because both \u2018to kill\u2019 and \u2018missing\u2019 hold a negative polarity in the SentiStrength lexicon. This evidence is consistent with the meaning-is-use assumption that the sense of an expression is fully determined by its context of use (Wittgenstein 1965).\nIn this paper, we address the problem of applying sentiment analysis to the software engineering discipline. We propose a sentiment analysis classifier, named Senti4SD, which exploits a suite of lexicon-based, keyword-based, and semantic features (Basile and Novielli 2015) for appropriately dealing with the domain-dependent use of a lexicon. The approach implemented by Senti4SD successfully addresses the problem of misclassifying neutral sentences as negative. We observe a 19% improvement in precision for the negative class and a 25% improvement in recall for the neutral class with respect to the baseline, represented by SentiStrength. The emotion polarity classifier is publicly available 1 and represents the first contribution of this paper. To train and test Senti4SD, we built a gold standard of 4,423 posts mined from Stack Overflow. As a second contribution of this study, we release our gold standard as well as the emotion annotation guidelines to be used in further studies on emotion awareness in software engineering. Consistently with the meaning-is-use assumption, we assume that the contextual polarity of a word can be correctly inferred by its use. Thus, in order to derive our semantic features, we represent word meaning based on distributional semantics. In particular, we exploit neural-network-based approaches to distributional semantics, also known as word embedding (Levy and Goldberg 2014). Specifically, we used word2vec (Mikolov et al. 2013) to build a Distributional Semantic Model (DSM) where words are represented as high-dimensional vectors. The DSM, which builds on a collection of over 20 million questions, answers, and comments from Stack Overflow, represents a valuable resource for software engineering researchers who intend to investigate the use of word embedding in text categorization tasks. Therefore, we release the DSM as a third contribution of this study. Finally, as a fourth contribution, we provide a better understanding of the negative bias in off-the-shelf sentiment analysis tools when applied in the software engineering domain. The contribution of lexicon-based, keyword-based, and semantic features is confirmed by our empirical evaluation leveraging different feature settings. We provide empirical evidence of better performance also in presence of a minimal set of training documents.\nThe paper is structured as follows. In Section 2, we present an overview of the research methods followed by the theoretical background in Section 3. Section 4 describes the annotation study for building the gold standard. In Sections 5 and 6, we describe respectively the features used by our classifier, and then, the experimental setup and evaluation. Discussion and threats to validity are presented in Sections 7 and 8, respectively. In Section 9, we position our contribution with respect to related work. Finally, in Section 10 we draw conclusions and present future work."}, {"heading": "2. Research Methods", "text": "Our research leverages a mix of qualitative and quantitative methods, including manual coding of textual data for building a gold standard on emotion polarity in software development, natural language processing techniques for feature extraction from Stack Overflow texts, and machine learning for training our emotion polarity classifier. Fig. 1 summarizes the process we followed in the current study. The complete process is organized in four sequential phases.\n1 The full lab package including Senti4SD, the DSM and the gold standard is available for download at: https://github.com/collab-uniba/Senti4SD\nIn Phase 1 we identified the theoretical framework of the current study and chose the emotion model to adopt in our annotation (see Section 3.1). The first output is the taxonomy of emotions and its mapping with polarity. As a second output, we defined the coding guidelines to adopt in the annotation study (see Appendix A).\nIn Phase 2 (see Section 4), the annotation study was carried out. We built the annotation sample by leveraging questions, answers, and comments extracted from Stack Overflow (see Section 4.1). Overall, the annotation sample is composed of 4,800 documents including questions, answers, and comments. User-contributed contents were preprocessed to improve their readability by discarding text elements that should not be annotated for sentiment, i.e. URLs, code snippets, and HTML tags. The annotation phase included the training of coders and a pilot annotation study before the final annotation was performed (see Sections 4.2 and 4.3).\nIn Phase 3 (see Section 4.3), we used the results of the annotation phase to build our gold standard for emotion polarity in software development. The interrater agreement was computed using Kappa, to assess the reliability of the annotation procedure and schema. The gold labels were assigned to documents in the annotation sample built using a majority voting criterion.\nIn Phase 4 (see Sections 5 and 6), we used the gold standard for emotion polarity to train and evaluate our classifier, whose\nperformance was compared with off-the-shelf tools representing the state of the art for sentiment analysis on social media."}, {"heading": "3. Background", "text": "In order to fully comprehend the addressed problem and the proposed solution, some key concepts are needed. The main\npoints of such supporting concepts are presented in the following sections.\n3.1. Emotion Modeling\nPsychologists worked at decoding emotions for decades, developing theories based on cognitive psychology and natural language communication. So far, two points of view have emerged: one considers emotions as a continuous function of one or more dimensions, while the other assumes that a limited set of basic emotions exists (Carofiglio et al. 2009).\nAs regards the first viewpoint (continuous function), mining affective states from text typically involves modeling them across two dimensions: (1) the affect polarity, or valence, and (2) the level of activation, also known as arousal or intensity. It is the case of the \u2018circumplex model\u2019 of affect, which represents emotions according to a bi-dimensional representation schema capturing the emotion valence (pleasant vs. unpleasant) and arousal (activation vs. deactivation). According to this model, each emotion can be considered a \u201clabel for a fuzzy set, defined as a class without sharp boundaries\u201d (Russell 1980).\nOn the other hand, theories following the discrete viewpoint agree on the idea that a limited set of basic emotions exists, although there is no consensus about the nature and the number of these basic emotions. According to Ekman (1999), basic emotions have specific feelings, universal signals, and corresponding physiological changes. Lazarus (1991) describes nine negative (anger, fright, anxiety, guilt, shame, sadness, envy, jealousy, and disgust) and seven positive (happiness, pride, relief, love, hope, compassion, and gratitude) emotions, with their appraisal patterns: positive emotions are triggered if the situation experienced is congruent with an individual goal, otherwise negative emotions are prompted.\nShaver et al. (1987) defined a tree-structured hierarchical classification of emotions. The hierarchy organizes emotion labels in three levels of hierarchical clusters. Each level refines the granularity of the previous one, thus providing more indication on its nature. The framework includes, at the top level, six basic emotions, namely love, joy, anger, sadness, fear, and surprise. The framework is easy to understand, thanks to the intuitive nature of the emotion labels. Consistently with our goal of training a classifier for emotion polarity, we map the emotions in the model by Shaver et al. to positive, negative, and neutral polarity (see Section 4). We use this mapping as a theoretical framework to inform our annotation guidelines (see Appendix A).\n3.2. Polarity Detection with SentiStrength\nSentiStrength (Thelwall et al. 2012) is a state-of-the-art, lexicon-based classifier that exploits a sentiment lexicon built by combining entries from different linguistic resources. In the SentiStrength lexicon, each negative word receives a sentiment score ranging from -2 to -5, which represents its prior polarity (i.e., the polarity of the term out of its contextual use). Similarly, positive\nwords are associated with a score between +2 and +5, while neutral words receive scores equal to \u00b11. Positive and negative emoticons are also included in the dictionary. Based on the assumption that a sentence can convey mixed sentiment, SentiStrength outputs both positive and negative sentiment scores for any input text written in English. It determines the overall positive and negative scores to a text by considering the maximum among all the sentence scores, based on the prior polarity of their terms. Intensifiers, i.e., exclamation marks or verbs such as \u2018really\u2019, are treated as booster words and increase the word sentiment scores. Negations are also treated and determine the inversion of the polarity score for a given word. Therefore, the\noverall positive p and negative n sentiment scores issued by the tool range from \u00b11 (absence of positive/negative sentiment) to\n\u00b15 (extremely positive/negative). Based on their algebraic sum, SentiStrength can also report the overall trinary score, i.e. the overall positive (score = 1), negative (score = -1) and neutral (score = 0). Examples are provided in TABLE 1. The rationale for classification reported in the second column of the table is obtained by enabling the \u2018explain\u2019 option in SentiStrength.\nValidated on social media, SentiStrength can deal with short informal texts that include abbreviations, intensifiers, and emoticons that typically occur in online interactions. As such, it has been widely adopted in social computing (Kucuktunc et al. 2012, Thelwall et al. 2012) and social software engineering (Guzman and Bruegge 2013, Guzman et al. 2014, Maalej et al. 2016, Novielli et al 2015).\nTo overcome the limitations and threats to validity derived from the use of off-the-shelf sentiment analysis tools in empirical software engineering studies (Blaz and Becker 2016, Jongeling et al. 2017, Novielli et al 2015), we train an emotion polarity classifier in a supervised machine learning setting by leveraging a gold standard of technical texts contributed by developers in Stack Overflow.\nA customized version of SentiStrength has been developed to support sentiment analysis in software engineering (Islam and Zibran 2017). The tool is called SentiStrength-SE and is built upon the SentiStrength API. It leverages a manually adjusted version of the SentiStrength lexicon and implements ad hoc heuristics to correct the misclassifications observed when running SentiStrength on the Ortu dataset (Ortu et al. 2016). In our evaluation, we also include the performance of SentiStrength-SE for benchmarking (see Section 6).\n3.3. Distributional Semantic Models\nState-of-the-art sentiment analysis tools and lexicons rely on a dictionary-based word representation (see Novielli et al. 2015 for an overview). Words are treated as atomic units and are associated to a prior polarity expressed as a sentiment score, ranging from extremely negative to extremely positive with the absence of sentiment in the middle. Since the notion of word similarity is not taken into account, the polarity of a text is only based on the prior polarity of the words it contains and cannot be adjusted based on their contextual meaning.\nDistributional Semantic Models (DSMs) represent words as mathematical points in high-dimensional vector spaces. A DSM relies on the so-called distributional hypothesis claiming that linguistic items with similar meanings occur in the same context\n(Miller and Charles 1991). Based on the assumption that the meaning of a document is determined by the meaning of the words that appear in it, a text unit (e.g., a document, a sentence, a text fragment, etc.) can be represented as the vector sum of all the word vectors occurring in it. Thus, in a DSM, both words and documents are homogeneously represented as vectors and can be compared using similarity metrics that measure their closeness in the space, traditionally through cosine similarity (Mikolov et al. 2013b).\nTraditional approaches to distributional semantics create word vectors by counting the occurrences of terms in a corpus and then operating a dimensionality reduction of word-by-document matrices. It is the case, for example, of Latent Semantic Analysis (Landauer and Dutnais 1997), which operates a singular value decomposition on the original term-by-document matrix to a lowdimension latent vector space. Such methods are usually referred in the literature as context-counting approaches (Baroni et al. 2014).\nRecently, neural network-based approaches have been proposed (Bengio et al. 2003, Collobert and Weston 2008, Mikolov et al. 2013a) for learning distributed representation of words as continuous vectors. These approaches, also known as word embedding (Levy and Goldberg 2014), learn the vectors that maximize the probability of the contexts in which the target word appears. For this reason, they are usually referred to as context-predicting approaches (Baroni et al. 2014).\nIn our study, we leverage the approach defined by Milokov et al. (2013). They developed two models for implementing context-predicting approaches: (1) the Continuous Bag-of-Words (CBOW) model predicts the target word by considering the previous and following n words in a symmetrical context window; (2) the Skip-gram model predicts the surrounding words based on the target word. Both architectures are implemented in word2vec,2 a publicly available tool for building a DSM from a large collection of documents. Both CBOW and Skip-gram models are capable of scaling up to large data sets with billions of words and are computationally more efficient for training high-dimensional spaces than context-counting approaches (Mikolov et al. 2013a). Furthermore, they outperform traditional context-counting approaches on standard lexical semantics benchmarks (Baroni et al. 2014)."}, {"heading": "4. Dataset: A Gold Standard for Emotion Polarity in Software Development", "text": "To train and evaluate our classifier for emotion polarity we built a gold standard composed of 4,423 posts from Stack Overflow. The dataset is well-balanced: 35% of posts convey positive emotions while 27% present negative emotions. No emotions are observed for the remaining 38% of posts, thus they receive the neutral polarity label.\nIn the following, we describe the sampling and coding processes adopted for building the gold standard.\n4.1. Creating the Annotation Sample\nThe annotation sample was extracted from the official Stack Overflow dump of user-contributed content from July 2008 to September 2015. To improve their readability, we pre-processed all the posts, using regular expressions, to discard all those elements that are out of the scope of the sentiment annotation task, e.g. code snippets, URLs, and HTML tags.\nIn a previous study (Novielli et al. 2015), we found that stronger expressions of emotions are usually detected in comments rather than in question or answers. Therefore, we consider as a unit of analysis the Stack Overflow post, which includes not only questions and answers, but also comments provided by community members. Hence, conceptually we are addressing 3x4 groups\n2 https://github.com/dav/word2vec\nof posts, that is, four types of Stack Overflow posts in {question, answer, question comment, answer comment} with three possible emotion styles in {positive, negative, neutral}.\nA desirable property of a training set is that its items are equally distributed across the existing classes of values (He and Garcia, 2009). Therefore, we built the dataset for the annotation by performing opportunistic sampling of posts based on both the presence of affectively-loaded lexicon and their type. To do so, we used SentiStrength to assess the presence/absence of affective lexicon in a post. We computed the positive and negative sentiment scores for the text of all the four types of posts extracted from the StackOverflow dump. Then, we randomly selected the same number of items based on the type of post and its sentiment scores. Our sample for annotation contains 4,800 items overall, equally distributed with respect to the types of posts and polarity, i.e. one-third of posts scored as positive by SentiStrength, one-third as negative, and one-third as neutral.\n4.2. Pilot Annotation Study\nTwelve coders participated in the emotion polarity annotation task. The coders were recruited among graduate CS students at the University of Bari and trained in a joint 2-hour session by the last author. She first explained the coding guidelines and then provided a sample of 25 Stack Overflow posts to be annotated individually in 30 minutes. Then, a follow-up discussion aimed at clarifying possible ambiguities in the interpretation of the coding guidelines.\nTraining was completed with a pilot subset of 100 items to be annotated individually at home. The twelve participants were organized into four groups of three coders each. Therefore, the pilot study was performed on 400 posts overall and each item in the dataset was assigned to three coders.\nThe coders were requested to indicate the emotion polarity, with a possible value in {positive, negative, neutral, mixed} (see Appendix A for guidelines). Analogously to Murgia et al. (2014), we refer to the Shaver et al. tree-structured framework for detecting emotions in the text (see Table A in Appendix A). The main difference with their annotation study is that we explicitly requested our coders to provide a polarity label, according to the specific emotion detected. In our study, positive polarity was indicated when the coders detected either joy or love. Conversely, negative polarity should be indicated when the coders identified anger, sadness, or fear. Regarding surprise, we asked the coders to determine the polarity based on contextual information. The neutral label indicates the absence of emotion. Posts conveying multiple emotions with opposite polarity (i.e., joy and sadness) were annotated as mixed.\nThe deadline was set a week after the assignment and the results of the annotation were discussed in a 2-hour plenary meeting with the experimenter. During this discussion, the coders had to resolve the disagreements on the pilot sample. This session was also used to disambiguate unclear parts of the guidelines as well as to enrich them with borderline examples whose annotation was agreed upon during the meeting. After the disagreements were solved, the pilot annotation became the first building block of the gold standard.\n4.3. Emotion Polarity Coding: Extended Study\nOnce the training was complete, we assigned a new set of 500 posts to each coder. Once again, each item was annotated by three coders. Overall, 2,000 new items were annotated in this second step. Again, coders were required to perform this new annotation task individually. The deadline for returning the annotation was set in three weeks. We then assigned the final set of 600 posts to the coders. Overall, 2,400 additional new items were annotated in this final step.\nAs an evidence of the reliability of the coding schema and procedure, we computed the weighted Cohen\u2019s Kappa among the pairs of raters (Cohen 1968). We are interested in distinguishing between mild disagreement, that is the disagreement between negative/positive and neutral annotations, and strong disagreement, that is the disagreement between positive and negative judgments. We assigned a weight = 2 to strong disagreement and a weight = 1 to mild disagreement. We compute the inter-coder reliability for the entire set, including the pilot set annotation. The agreement is computed for all the four groups of participants (A, B, C, D) and for all pair of coders (C1, C2, and C3) in each group (see TABLE 2). We note a substantial agreement with Kappa values ranging in [.66, .80] (average .74). This evidence is confirmed also by the values of the observed agreement, which is the percentage of cases on which the raters agree, ranging in [.73, .85] (average .79).\nConsistently with previous research on emotion annotation (Blaz and Becker 2016, Murgia et al. 2014), we resolved the disagreements by applying a majority voting criterion. We excluded from the gold standard all the posts for which opposite polarity labels were provided, including mixed cases (3%), even in presence of majority agreement. The final gold standard resulted in 4,423 posts, representing 92% of 4,800 annotated items."}, {"heading": "5. Emotion Polarity Classifier: Feature Description and System Setup", "text": "Previous research shows how combining generic and domain-specific resources improves the performance of sentiment analysis (Bollegala et al. 2013). Therefore, we exploit three different kinds of features based on: (1) generic sentiment lexicons, (2) keywords (i.e., n-grams extracted from our dataset), and (3) word representation in a distributional semantic model specifically trained on software engineering data.\n5.1. Lexicon-based features\nThe first set of features exploits existing sentiment lexicons. The approach is totally independent of the lexicon chosen and simply requires that a sentiment score is provided for each entry of the input (Novielli and Basile, 2015). For example, in the lexicon used by SentiStrength, each negative word is associated with an a priori sentiment score in [-2, -5]. Similarly, positive words receive a score in [+2, +5]. A list of objective words is also provided, with scores equal to \u00b11.\nFor a given post we compute the lexicon-based features reported in TABLE 3. In particular, we compute the number of tokens with positive and negative prior polarity (Pos_words and Neg_words), the overall number of tokens with either positive or negative prior polarity (Subj_words), the score of the last emoticon (Last_emo), the sum of all the scores for positive (Sum_pos), negative (Sum_neg), and subjective (Sum_subj) tokens, and the maximum positive and negative scores observed in the post (Max_pos and Max_neg). We also capture the presence of positive/negative utterances in combination with\nexclamation marks, indicating emphasis (Pos_Emph and Neg_Emph). Finally, we capture the sentiment of the last token/emotion (End_Pos, End_Neg), and whether it is combined with an exclamation mark (End_Pos_Emph, End_Neg_Emph). All the lexicon-based features have been already used for the sentiment analysis of crowd-generated content (Mohammad et al. 2013). Our lexicon-based features are independent of the specific lexicon adopted. To enable fair comparison with the baseline, represented by SentiStrength, for this study, we use the SentiStrength lexicon. The choice of the SentiStrength lexicon is further supported by its ability to deal short informal text, as it includes also abbreviations, intensifiers, and emoticons that are typically used in the social web. Furthermore, it incorporates sentiment scores from other linguistic resources that were previously validated in the scope of empirical research in sentiment analysis (Stone et al., 1966) and psycholinguistics (Pennebaker and Francis, 2001).\nKeyword-based features include word counts for n-grams appearing in a document, in our case a Stack Overflow post. In our feature set, we consider uni- and bi-grams. Consistently with traditional approaches to text classification (Joachims 1998), each n-gram in our corpus corresponds to a feature, with the number of occurrences as its value. Other than including n-grams, we designed features able to capture aspects of micro-blogging, such as the use of uppercase and elongated words used as intensifiers, the presence of positive and negative emoticons, and the occurrence of slang expression of laughter (Basile and Novielli 2015). The total number of keyword-based features is 76,346. We report a summary in TABLE 4.\nThe semantic features capture the similarity between the vector representations of the Stack Overflow documents and prototype vectors representing the polarity classes in a DSM. Analogously to (Basile and Novielli 2015, Novielli and Strapparava 2013), we represent a Stack Overflow document (i.e., a question, answer, or comment) in the DSM as the vector sum of all the vectors of words occurring in the document, using the superposition operator (Smolensky 1990).\nThe prototype vectors are vector representation of the positive, negative, and neutral classes in the DSM, namely p_pos, p_neg, and p_neu. A prototype vector is a vector representation of the lexical profile for a given polarity class, based on a sentiment lexicon that provides prior polarity scores for words. To compute the prototype vector p_pos for the positive class we sum all the vectors for words with positive polarity score in the chosen sentiment lexicon. In a similar fashion, we compute p_neg and p_neu by summing up, respectively, all the negative and neutral words in the chosen sentiment lexicon. In this study, we used the list of positive, negative, and neutral words included in the SentiStrength lexicon. We further calculated the subjective prototype p_subj vector by summing up the positive and negative word vectors, to better capture the differences in the lexical choice of neutral sentences and affectively-loaded ones.\nWe used the four prototype vectors to compute the semantic features, that is the similarity scores between the document vector (i.e., a Stack Overflow post) and each prototype vector, namely Sim_pos, Sim_neg, Sim_neu, Sim_subj (see TABLE 5). The semantic features are computed on a DSM built on Stack Overflow data, using the CBOW architecture implemented by word2vec (Mikolov et al. 2013), as depicted in Fig. 2. We choose a configuration with 600 vector dimensions, after having repeated the 10-fold cross-validation for parameter tuning (see Section 5.4.). We ran word2vec on a corpus extracted from the Stack Overflow official dump updated to September 2015. We extracted 3.8 million questions from the dump with the associated 5.9 million answers and 11.6 million comments. We preprocessed the posts to remove the URLs, HTML codes, and code snippets, obtaining a collection of more than 20 million posts with 912,201,785 tokens overall.\n5.4. System Setup and Parameter Tuning\nBefore extracting all features, we performed tokenization using the Stanford NLP suite (Manning et al. 2014). During the tokenization, we replaced the user mentions with the meta-token @USER. We did not perform any stemming nor lemmatization since an inflected form may convey important information about polarity. It is the case, for example of \u2018ail\u2019 and \u2018ailing\u2019, which holds different prior polarity in the SentiStrength lexicon. Also, we did not remove stop words, consistently with previous research in sentiment classification tasks (Saif et al. 2014).\nWe trained Senti4SD using Support Vector Machines (SVM). SVM is able to learn and generalize even with a high dimensional feature space, which is a typical scenario in text classification tasks like ours (Joachims 1998). In particular, linear SVM is a state-of-the-art learning technique for such high-dimensional sparse datasets with a large number of items and a large number of features N, where each item has only s << N non-null features (Joachims 2006), as typical in presence of n-grams. One way to avoid dealing with such high dimensional input spaces would be to perform substantial feature selection. However, in supervised learning for text classification tasks, very few features are actually irrelevant, and feature selection results in a significant loss of information (Joachims 1998). Thus, we exploit the full set of features. Still, in order to assess the predictive value of our features, we analyze and rank them according to their information gain (Mitchell 1997). In TABLE 6, we report the top 25 features ranked by information gain.\nWe ran our experiment using the R interface (Helleputte 2015) to Liblinear (Fan et al. 2008), an open source library for largescale linear classification with SVM. For linear classification, the only parameter is the cost parameter C. Too large C values makes the cost of misclassification high, thus forcing the algorithm to better explain the training data but potentially inducing the risk of overfitting. To fine-tune the SVM parameter while still preventing overfitting, we ran the Liblinear parameter tuning utility on our training set in a 10-fold cross-validation setting. We chose the optimal value for the C parameter by maximizing the prediction accuracy. We repeated the parameter tuning with all the settings derived by combining the two available architectures in word2vec CBOW and Skip-gram, with vector space dimensions in {200, 400, 600, 800, 1000}. Furthermore, word2vec has two input parameters for rare-word pruning and frequent word sub-sampling: words appearing less than min-count times are discarded from the document collection before starting the DSM training, while frequent words (as defined by the sample input parameter) are down-sampled to increase the effective context window considered for vector prediction (Mikolov et al. 2013). Consistently with previous research (Basile and Novielli 2015), we maintained the default\nFig. 2 Building the DSM on Stack Overflow data\nvalue for the sample parameter while discarding the terms with less than 10 occurrences, thus obtaining a final vocabulary of 346,236 terms. The optimal configuration, used to train our final classifier over the training set, is reported in TABLE 7."}, {"heading": "6. Evaluation", "text": "6.1. Creation of Train and Test Sets\nWe split the gold set into training (70%) and test (30%) sets, using the R (R Development Core Team 2008) package caret (Kuhn 2016) for stratified sampling. We used the training set to seek the optimal parameter setting for our classifier (see Section 5.4). The final model was trained on the whole training set using the optimal configuration and then evaluated on the test set, to assess to what degree the trained model is able to generalize sentiment polarity classification on unseen new data from the heldout test set.\n6.2. Results\nAfter having trained Senti4SD, we evaluated the learned model on the Stack Overflow test set. TABLE 8 reports the performance obtained in terms of recall, precision, and F-measure for the single classes and overall. The overall performance is computed adopting micro-averaging as aggregated metric (Sebastiani 2002). We highlight in bold the best value for each metric.\nIn TABLE 8, we also report the performance of SentiStrength3 on the Stack Overflow test set, which we consider as a baseline for the performance assessment of Senti4SD. We choose SentiStrength because it is the most widely employed tool in sentiment analysis studies in software engineering (Calefato et al. 2015, Guzman et al. 2016, Guzman and Bruegge 2013, Ortu et al. 2015, Sinha et al. 2016). In addition, we also report the performance of SentiStrength-SE. We mapped both SentiStrength and SentiStrength-SE scores to a categorical sentiment label in {positive, neutral, negative} for each entire question, answer or comment. Consistently with the approach defined by SentiStrength authors (Thelwall et al. 2012) and already adopted\n3 The evaluations have been performed using the SentiStrength Java API obtained from http://sentistrength.wlv.ac.uk/ on December 2016.\nTABLE 8 . PERFORMANCE OF SENTI4SD AND COMPARISON WITH SENTISTRENGTH (BASELINE).\nOverall Positive Negative Neutral\nR P F R P F R P F R P F\nBaseline (SentiStrength) .82 .82 .82 .92 .89 .90 .96 .67 .79 .64 .95 .76 SentiStrength-SE .78 .78 .78 .79 .90 .84 .79 .73 .76 .77 .73 .75 Senti4SD .87 .87 .87 .92 .92 .92 .89 .80 .84 .80 .87 .83 Improvement over the SentiStrength baseline +6% +6% +6% --- +3% +2% -7% +19% +6% +25% -8% +9%\nTABLE 9. AGREEMENT BETWEEN MANUAL LABELING AND PREDICTION ON THE STACK OVERFLOW TEST SET.\nPrediction SentiStrength Senti4SD\nNegative Positive Neutral Negative Positive Neutral\nM an\nua l Negative 345 (95.8%) 7 (1.9%) 8 (2.2%) 321 (89.2%) 3 (0.8%) 36 (10.0%)\nPositive 30 (6.6%) 420 (91.7%) 8 (1.8%) 11 (2.4%) 423 (92.4%) 24 (5.2%) Neutral 140 (27.6%) 44 (8.7%) 324 (63.8%) 70 (13.8%) 32 (6.3%) 406 (79.9%)\nin previous benchmarking studies (Jongeling et al. 2017), given the positive (p) and negative (n) scores issued by the tool, we consider a text as positive when p + n > 0, negative when p + n <0, and neutral if (p = n) and (p < 4).\nTexts with a score of p = n and p \u2265 4 are considered having an undetermined sentiment and should be removed from the dataset. However, no such controversial cases are found in our dataset. Looking at the performance of Senti4SD, we observe a 19% improvement in precision for the negative class and a 25% improvement in recall for the neutral class with respect to the SentiStrength baseline, which in turn outperforms SentiStrength-SE.\nIn TABLE 9 we report the confusion matrix for both SentiStrength and Senti4D, showing the agreement between the manual labeling and the polarity predicted by each tool. We consider only SentiStrength because it outperforms SentiStregth-SE. We complement the evidence provided by the confusion matrix with Venn diagrams representing the posts correctly classified as negative, positive, and neutral by SentiStrength and Senti4SD (see Fig. 3). Looking at the predictions, we observe that the 24 negative cases recognized only by SentiStrength (see Fig. 3.a) are classified as neutral by Senti4SD. As for positive posts (see Fig.3.b), the 10 cases missed by Senti4SD are classified mostly as neutral and only one of them is classified as negative. Conversely, the 13 recognized only by Senti4SD are misclassified by SentiStrength as positive. As for neutral, the 84 cases recognized only by Senti4SD (see Fig.3.c) are classified mainly as negative (69/84).\nWe complement the previous evaluation with an assessment of the advantage of including all the features defined in Senti4SD. Our goal is to assess whether the improvement of performance, with respect to SentiStrength, is a result of the adopted machine learning technique or is rather due to the additional features (full set of keyword-based, semantic features, and lexiconbased features) we propose. We start by computing the performance with a simple model including only the uni- and bi-grams. Such a model does not include consideration of any sentiment-specific feature and represents the traditional approach to text categorization based on machine learning (Joachims 1998). By doing so, we want to assess to what extent the additional features contribute to the performance by capturing sentiment-related linguistic phenomena. Then, we evaluate the performance of Senti4SD by considering incremental feature settings, in order to assess the contribution of each feature group to the classifier performance. Results are reported in TABLE 10 and complement the evidence provided by the information gain analysis (see TABLE 6) about the role of each feature group. The last column of TABLE 10 (p-value <0.05) indicates whether we observe a statistically significant improvement in a given setting over the previous approach. Statistical significance of the difference\nbetween settings is computed by performing the Chi-squared test with a= 0.05.\nBy comparing the performance of Senti4SD leveraging different sets of features, we observe that simply training a supervised classifier based on n-grams does not yield an acceptable overall performance (F = .69). By leveraging the full set of keyboard-\nbased features the overall performance improves to F = .79. However, such a classifier would still perform poorly if compared to the Sentistrength baseline. In particular, recall for negative is unsatisfying (R = .67). Adding the four semantic features, performance is further improved (F=.81). In particular, adding only four features (i.e., the cosine similarity of the document with the sentiment prototype vectors) we observe an improvement of the negative recall from .67 to .73 and of the neutral precision from .74 to .78. However, while the improvement is statistically significant, the recall for negative is still low (R=.73). Finally, we include consideration of lexicon-based features, which further increase the recall of negative up to .89 and the precision of neutral up to .87.\nSearching for further evidence of the robustness of the approach implemented by Senti4SD, we also assess its performance by splitting our gold standard into train and test set with different percentages. Results are reported in TABLE 11 and compared with the SentiStrength performance on the same test set for each iteration. For each setting, we compare the behavior of the two classifiers by performing a Chi-square test on the predictions issued by Senti4SD and SentiStrength, observing a p-value lower than 0.05. In all the three settings, Senti4SD outperforms SentiStrength, even with a reduced training set (last row of TABLE 11), with 30% of the gold standard used as training set. Again, we highlight in bold the best value for each metric."}, {"heading": "7. Discussion", "text": "Comparison with the SentiStrength baseline. The performance of SentiStrength on our test set (see TABLES 8 and 9) confirms previous findings about its negative bias in the software engineering domain (Novielli et al. 2015). In the case of our\nTABLE 10 . PERFORMANCE OF SENTI4SD WITH INCREMENTAL FEATURE SETTINGS.\nExperimental Setting Overall Positive Negative Neutral p-value <0.05\nR P F R P F R P F R P F\nN-grams only .69 .69 .69 .84 .75 .79 .88 .57 .69 .42 .85 .56\nKeyword-based features .79 .79 .79 .84 .84 .84 .67 .80 .73 .83 .74 .79 *\nKeyword + Semantics .81 .81 .81 .86 .86 .86 .73 .80 .76 .83 .78 .81 * Keyword + Semantics + Lexicon Based (full feature set) .87 .87 .87 .92 .92 .92 .89 .80 .84 .80 .87 .83 * * p < 0.05\nStack Overflow dataset, SentiStrength erroneously classifies 28% of neutral posts as negative, with a poor recall for neutral class (.64) and a low precision for the negative one (.67). Since Stack Overflow is explicitly designed to support developers looking for help, discussions are often misclassified as conveying negative polarity because they are naturally rich in the \u2018problem\u2019 lexicon, which does not necessarily indicate the intention to show any affective state.\nSenti4SD is able to address the problem of such negative bias. TABLE 9 shows that the number of neutral documents misclassified as negative is reduced from 27.6% in SentiStrength to 13.8% in Senti4SD. As a consequence, the F-measure increases from .79 to .84 for the negative class and from .76 to .83 for the neutral class, thus depicting a more balanced classifier (see TABLE 8). In particular, our classifier improves the recall of neutral documents from .64 up to .80 (25% of improvement) and the precision of negative documents from .67 up to .80 (19% of improvement). For example, SentiStrength erroneously classifies as negative sentences that are instead neutral, as the following ones: \u2018This will help you to come back to the previous activity. As per your code, the application was completely killed.\u2019, \u2018Or if you don't want to worry about height calculation do this\u2019. On the contrary, Senti4SD correctly labels the above sentences as neutral.\nHowever, we observe that this gain in precision is obtained at the expense of the negative class recall, which decreases from .96 to .89. For example, SentiStrength correctly classifies as negative the following posts: \u2018Is it possible to prevent a user from editing the title of a node on the node edit screen? One of the things I really detest about Drupal is the rigidity of the title & body field in each node\u2019 and \u2018Ew, that sounds a bit ugly! Is it possible for an instance of a class to be created before its unit's initialization section has run? In other words, could an instance of TMyObject try to use FLogger before it's been set in the initialization section?\u2019. On the contrary, the two posts are erroneously labeled as neutral by Senti4SD even in presence of the negative lexicon (i.e., \u2018detest\u2019 and \u2018ugly\u2019). Such misclassifications are probably due to the prevalence of neutral lexicon in the posts. Specifically, in the first post the first sentence does not carry any sentiment while in the second post the second and third sentences are neutral. A possible way to overcome this limitation occurring with long posts is to perform finer-grained annotation at a sentence level in order to train a sentence-based version of Senti4SD.\nMisclassification of positive posts as negative occurs in 6.6% of the cases when the classification is performed with SentiStrength (see TABLE 9). This is what we consider a strong disagreement that should be avoided. Senti4SD reduces such misclassification to 2.4% of the cases. For example, sentences like \u2018Is in so u need not worry! Internally the data is always stored as TEXT, so even if you create table with, SQLite is going to follow the rules of data type\u2019 is erroneously classified by SentiStrength as negative due to the presence of negative lexicon (\u2018worry\u2019) even if SentiStrength is supposed to correctly deal with negations (Thelwall et al. 2012) which should determine polarity inversion.\nSurprisingly, SentiStrength-SE produces a lower performance than SentiStrength on our Stack Overflow gold standard albeit it outperformes SentiStrength on other technical texts (Islam and Zibran 2017). This might occur because SentiStrength-SE incorporates ad hoc heuristics and word polarity scores that are specifically designed to solve misclassifications observed on a small unbalanced dataset of 400 developers\u2019 comments in Jira (Ortu et al. 2016). As such, overfitting is a plausible explanation for the decay in performance of SentiStrength-SE in our study.\nImplications. Senti4SD was developed in the scope of our ongoing research on the role of emotions in social software engineering (Novielli et al. 2014, M\u00e4ntyl\u00e4 et al. 2017). More specifically, we envision the emergence of sentiment analysis tools monitoring communication between the developers as well as user-contributed technical texts (e.g., reviews in app stores), analyzing the affect expressed in this communication and translating the results into actionable insights (Gachechiladze et al 2017). Among others, negative affective states deserve attention because of their detrimental impact on developers\u2019 productivity\n(Denning 2012, Ford and Parnin, 2015, Graziotin et al. 2017). When implementing a sentiment classifier, deciding whether to optimize by precision or by recall is not a trivial decision, which depends on the application scenario. Early detection of negative sentiment towards self, such as frustration, could be useful to design tools for supporting developers experiencing cognitive difficulties (i.e., learning a new language or solving tasks with high reasoning complexity) (Ford and Parnin, 2015), as well as in their daily programming tasks (M\u00fcller and Fritz 2015). In such a scenario, a monitoring tool might suggest the intervention of an expert or provide a link to further material and documentation to support the developer. However, a sentiment analysis tool with high recall and low precision for negative sentiment as SentiStrength would produce several false positives, causing undesired, erroneous interruptions that are detrimental to developers' productivity and focus. In such cases, being able to reduce the number of false positives for negative sentiment becomes crucial and Senti4SD should be preferred to SentiStrength, due to its higher precision.\nSimilarly, timely detection of negative sentiment towards peers, such as anger and hostility (Gachechiladze et al 2017), might be exploited for detecting code of conduct violations (Tromp and Pechenizkiy 2015) or enhancing effective community management. For example, sentiment analysis may support GitHub users who want to be notified of heated conversation and lock them before flame wars break out.4 In scenarios that involve human intervention to guide the contributors\u2019 behavior towards a constructive pattern, it might be desirable to optimize negative sentiment detection by recall, thus choosing to leverage SentiStrength higher sensitivity to negative emotions. Conversely, if automatic filtering of offensive comments or conversation is envisaged, it becomes important to optimize by precision by using Senti4SD, to avoid banning neutral conversations.\nFinally, sentiment analysis is now regarded as a technique also useful for mining large software repositories, e.g., to understand the role of sentiment in security discussions (Pletea et al. 2014) and commits in GitHub (Guzman et al. 2014). In such scenarios, a sentiment classifier specifically trained and validated in the software engineering domain allows controlling for threats to validity due to inappropriate instrumentation, as argued by Jongeling et al (2017).\nContribution of features. Consistently with traditional approaches to supervised machine learning in text classification (Joachims 1998), we did not perform feature selection, thus including in our evaluation setting the full suite of lexicon-based, keyword-based, and semantic features described in Section 5. As a further evidence of the importance of each group of features, we performed an analysis based on information gain (see TABLE 6) and assessed Senti4SD performance by leveraging different feature settings (see TABLE 10). The top-ten predictive features belong to the group of lexicon-based, which is an expected result since they are based on sentiment lexicons specifically designed to represent the sentiment polarity association to words. They are immediately followed by the four semantic features that measure the similarity between a document and the linguistic profile of each polarity class. Among the top predicting keyword-based features, we find positive and negative emoticons (Pos_emo and Neg_emo, respectively). Expressions of gratitude (i.e., \u2018thanks\u2019) and appreciation (i.e., \u2018great\u2019, \u2018excellent\u2019) are also among the top uni-gram predictors, thus confirming evidence from previous research that paying gratitude for the help received as well as enthusiasm for the solution provided are the main causes for positive sentiment in the social programmer ecosystem (Calefato et al. 2015, Novielli et al. 2015, Ortu et al. 2015). Conversely, expression of anger and frustration (i.e., \u2018hate\u2019, \u2018annoying\u2019) are among the top predictors for negative sentiment. The contribution of each feature group to the classification performance is confirmed by the evaluation of Senti4SD leveraging different feature settings, as reported in TABLE 10. Furthermore, we provide empirical evidence that supervised training as implemented by Senti4SD produce similar performance also in presence of a minimal set of training documents (see TABLE 11).\n4 https://help.github.com/articles/locking-conversations\nGold standard. Our manually annotated dataset is the first resource on emotion polarity to be built upon the corpus of Stack Overflow. As such, our dataset represents a valuable resource in the scope of empirical research on emotion awareness in software engineering (SEmotion 2016). Stack Overflow is an example of an online community where programmers do networking by reading and answering others\u2019 questions, thus participating in the creation and diffusion of crowdsourced documentation. Among the non-technical factors that can influence the members of online communities, the emotional style of a technical contribution does affect its probability of success (Calefato et al. 2015). Being able to identify harsh comments towards technical matters could be useful in detecting particularly challenging questions that have not been exhaustively answered (Novielli et al. 2015), which is a goal addressed by current research on effective knowledge-sharing (Anderson et al. 2012). Similarly, detecting negative attitude towards the interlocutor could allow the community moderators to guide users towards appropriate interaction patterns. This is an open problem in the Stack Overflow community, as users complain about harsh comments coming from expert contributors (Meta 2017), which may impair successful question-answering (Asaduzzaman et al. 2013).\nThe release of our gold standard complements the effort of Ortu et al. (2016) who recently released a dataset of 2,000 issue comments and 4,000 sentences written by developers, collected by mining the repositories of four open source ecosystems, namely Apache, Spring, JBoss, and CodeHaus. Their dataset is annotated using the basic emotion labels in the framework by Shaver et al. (1987) that we also adopt in the present study."}, {"heading": "8. Threats to Validity", "text": "Our methodology could produce different results if applied outside of Stack Overflow. However, Stack Overflow is so popular among software developers (currently used by about 7 million software developers5) to be reasonably confident that the dataset is representative of developers\u2019 communication style. Nevertheless, we acknowledge that replications are needed to further increase the external validity to the entire software developer ecosystem.\nWe built our gold standard on emotion polarity through manual annotation. Emotion annotation is a subjective process since affect triggering and perception can be influenced by personality traits and personal dispositions (Scherer et al. 2004). To mitigate this threat, we provided clear guidelines (see Appendix A) grounded on a theoretical framework for emotion identification based on the model by Shaver et al. (1987). Furthermore, polarity labels were assigned using majority agreement among three coders. To be more conservative, even in presence of majority agreement, we excluded from the gold standard all the posts for which opposite polarity labels were provided. The interrater agreement (average weighted Cohen\u2019s Kappa = 0.74) confirms a good reliability of the gold standard. Nevertheless, we intend to improve coding guidelines by enriching the number of examples, especially for those more controversial that lead to coding conflicts.\nThe sample set for the emotion annotation experiment was built using SentiStrength. We built our sample set to have onethird of posts scored by SentiStrength as positive, one-third as negative, and one-third as neutral. However, as highlighted by previous research (Blaz and Becker 2016, Jongeling et al. 2017, Novielli et al. 2015), off-the-shelf tools for sentiment analysis report limited performance when detecting sentiment in the software engineering domain. In particular, SentiStrength tends to misclassify neutral sentences as negative (see TABLES 8 and 9). As a result, we ended up including in our sample set a higher proportion of neutral sentences that were originally misclassified as negative by SentiStregth and later correctly classified as neutral by our coders. Another cause of error when using SentiStrength on our data is the misclassification of positive posts as\n5 Source: http://stackexchange.com/sites#questions Last accessed: June \u201817\nnegative (see TABLE 9). As such, a small proportion of the posts originally included in the sample annotation set because rated as negative by SentiStrength were subsequently classified as positive by the coders. The distribution of the gold standard built through the annotation study confirms these issues and shows how the negative class is underrepresented in our dataset, i.e., the gold standard contains 35% positive posts, 27% negative posts, and 38% of neutral posts. Another consequence of using SentiStrength to create the sample set is that the sentences in our dataset contain emotion words included in the SentiStrength lexicon. Hence, we observe a very good performance of the tool on our gold standard (F = .82, TABLE 8), making SentiStrength a challenging baseline for our classification task.\nFinally, we excluded from the gold standard all the posts for which opposite polarity labels were provided, which represent the 3% of all annotated data. Our choice is justified by the intention to not introduce noise in the data during the supervised training phase of Senti4SD. Currently, Senti4SD would classify those posts as either positive or negative. However, we acknowledge that a minority of posts might present both positive and negative emotions. In our future work, we will fine-tune Senti4SD by training separate binary classifiers for positive and negative sentiment to be able to recognize also mixed sentiment."}, {"heading": "9. Related Work", "text": "9.1. Sentiment Analysis Resources for Software Engineering\nTrying to overcome the limitations posed by using off-the-shelf sentiment analysis tools, software engineering researchers\nrecently started to develop their own tools.\nPanichella et al. (2015) applied sentiment analysis for classifying user reviews in Google Play and Apple Store. They trained their own classifier on 2,000 manually-annotated reviews, using Na\u00efve Bayes and a bag-of-word approach. However, they do not report evaluation metrics for their classifier so we are not able to make any comparison with their method. For the sake of completeness, we also experimented with Na\u00efve Bayes, as they suggest, but we found that it is outperformed by SVM.\nM\u00e4ntyl\u00e4 et al. (2016) investigated the potential of mining developers\u2019 emotions in issue-tracking systems to prevent loss of productivity and burnout. They measured the emotions in issue comments in terms of VAD metrics, that is, scores for the Valence (i.e., the affect polarity), Arousal (i.e., the affect intensity), and Dominance (i.e., the sensation of being in control of a situation). To estimate VAD scores, they adopted the same lexicon-based approach implemented by SentiStrength, using a VAD lexicon of over 13K English words developed by psychology research. However, given the lack of a gold standard for VAD, they were not able to provide any evaluation of their approach to emotion mining.\nOrtu et al. (2015) presented an empirical study on the correlation of emotions and issue-fixing time in the Apache issuetracking system. They measure the emotion polarity in issue comments using SentiStrength. As for discrete emotion labels, they developed their own classifier for detecting the presence of four basic emotions framework by the Shaver et al. (1987), namely anger, joy, sadness, and love. Their approach exploits SVM using a suite of features based on the SentiStrength output, the politeness score (Danescu-Niculescu-Mizil et al. 2013), and the presence of affective words derived from WordNetAffect (Strapparava and Valitutti 2004). The classifier is evaluated on a gold standard of 4,000 sentences, obtaining an F-measure score ranging from .74 for anger to .82 for sadness. At the time of writing, the classifier is not yet available for research purposes.\nBlaz and Becker (2016) developed a polarity classifier for IT tickets. Their approach is based on a domain dictionary created using a semiautomatic bootstrapping approach to expanding an initial set of affectively-loaded words used as seeds. They also exploit features based on the document structure, i.e., by distinguishing between the polarity of the opening section from the polarity of the actual problem report section in the ticket. They compare different approaches with different feature settings. In\nthe best setting, they obtain an overall performance of F = .85, that is comparable to the one achieved by Senti4SD (F =.86). However, their classifier still reports a negative bias inducing the misclassification of neutral documents as negative. Their performance on the negative class (F = .70, R = .74, P = .67) reflects such bias. Senti4SD successfully addresses this problem by obtaining a more balanced performance for both the negative (F = .84, R = .87, P = .80) and neutral (F = .83, R = .80, P = .85) classes. Furthermore, Senti4SD has been trained and evaluated on a balanced and larger dataset.\nIslam and Zibran (2017) developed SentiStrength-SE, a customized version of SentiStrength for software engineering. The tool is built upon the SentiStrength API and incorporates ad hoc heuristics designed to solve the misclassifications of SentiStrength observed on a subset of about 400 comments from the Ortu dataset (Ortu et al. 2016). The sentiment scores of the lexicon have been manually adjusted to reflect the semantics and neutral polarity of domain words such as \u2018support\u2019, \u2018error\u2019, or \u2018default\u2019. The authors performed an evaluation of the tool on the remaining 5,600 comments from the Ortu dataset, showing that SentiStrength-SE (F = .78, R = .85, P = .74) outperforms SentiStrength (F = .62, R = .79, P = .62) on technical texts. However, SentiStrength-SE produces a lower performance (F = .78, R = .78, P = .78) than both SentiStrength (F = .82, R = .82, P = .82) and Senti4SD (F = .87, R = .87, P = .87) when used to classify polarity of posts from our Stack Overflow gold standard (see Section 6).\n9.2. Distributional Semantics in Software Engineering\nTo the best of our knowledge, word embedding techniques have not been applied before to sentiment analysis tasks in the software development domain. In particular, we exploit the idea of using features based on the document similarity with respect to prototype vectors modeling the lexical profile of the polarity classes. The use of prototype vectors in text classifications was successfully exploited in different domains, e.g., for unsupervised speech-act recognition in telephone conversations (Novielli and Strapparava 2013) and for sentiment analysis in micro-blogging (Basile and Novielli 2015).\nHowever, the use of distributional semantics is not entirely new in software engineering research. Traditional, contextcounting approaches to distributional semantics have already been used, including Latent Dirichlet Allocation for topic modeling in Stack Overflow (Barua et al. 2014) and Latent Semantic Analysis for recovering traceability links in software artifact (De Lucia et al. 2007). Tian et al. (2014) recently proposed the use of pointwise mutual information to represents word similarity in a high-dimensional space. They built SEWordSim, a word similarity database trained on Stack Overflow questions and answers. However, as already discussed in Section 2.2, count-based approaches suffer from the main drawback of poor scalability. In the specific case of SEWordSim, the words are represented in a high-dimensional matrix whose eij elements correspond to the pointwise mutual information between the words wi and wj, thus describing their semantic association. The fact that vector space dimensions equal the vocabulary size significantly limits the scalability of approaches based on such word models. Instead, word embedding overcomes the limitations of context-counting approaches \u2013 due to their poor scalability to large document collections (Mikolov et al. 2013a) \u2013 and provides more effective vector representation of words (Baroni et al. 2014, Mikolov et al. 2013b). Thus, in our study, we adopt word embedding for building our distributional semantic model.\nYe et al. (2016) already exploited word embedding for enhancing information retrieval in software engineering. They run word2vec on a collection of over 12K Java SE 7 documents to represent both natural language words and source code tokens in a distributional semantic model. Their final goal is to bridge the lexical gap between code fragments and natural language description that can be found in tutorials, API documentations, and bug reports. They empirically demonstrate how exploiting\nword embedding improves over state-of-the-art approaches to bug localization. Furthermore, they demonstrate the benefit of exploiting word embedding for linking API documents to Java questions posted in Stack Overflow."}, {"heading": "10. Conclusions", "text": "We presented Senti4SD, a sentiment polarity classifier for software developers\u2019 artifacts. The classifier is trained and tested on a gold standard of over 4K posts mined from Stack Overflow and manually annotated with emotion polarity. The gold standard is publicly available for further studies on emotion awareness in software engineering. We also release the guidelines for an annotation to encourage the community to extend and further validate our dataset by replicating the annotation experiment.\nThe semantic features of Senti4SD are computed based on a distributional semantic model built exploiting word embedding. We built the DSM by running word2vec on a collection of over 20 million documents from Stack Overflow, thus obtaining word vectors that are representative of developers\u2019 communication style. The DSM is released, with the replication kit, for future research on word embedding for text categorization and information retrieval in software engineering.\nBy combining lexicon-based, keyword-based and semantic features, Senti4SD successfully addresses the problem of the negative bias in off-the-shelf sentiment analysis tools. In particular, we observe a 19% improvement in precision for the negative class and a 25% improvement in recall for the neutral class with respect to the baseline represented by SentiStrength.\nAs future work, we intend to explore the contribution of additional features to capture further meaningful aspects of language use, such as part-of-speech and the rhetorical structure of sentences. We also intend to fine-tune Senti4SD to recognize content with mixed sentiment. As a further assessment of our approach, we intend to evaluate Senti4SD performance on further crowdgenerated content from other social software engineering tools and repositories (e.g., GitHub, issue tracking systems). Besides, we plan to provide further benchmarking with other sentiment analysis tools and lexicons. Finally, we are also working on an extended version of our gold standard that will include emotion labels (e.g., love, anger, sadness, joy), as a first step towards building a classifier to detect specific emotions."}, {"heading": "Acknowledgments", "text": "This work is partially supported by the project \u2018EmoQuest - Investigating the Role of Emotions in Online Question & Answer Sites\u2019, funded by the Italian Ministry of Education, University and Research (MIUR) under the program \u201cScientific Independence of young Researchers\u201d (SIR). The computational work has been executed on the IT resources made available by two projects, ReCaS and PRISMA, funded by MIUR under the program \u201cPON R&C 2007-2013\u201d. We thank Pierpaolo Basile for insightful discussions and helpful comments and the annotators involved in the gold standard building."}, {"heading": "Appendix A: Coding Guidelines", "text": "In the following, we report the task description and the guidelines used for training the coders involved in the emotion\nannotation study.\nTask Description and Annotation Guidelines. You are invited to take part in the annotation study of developers contributed texts in Stack Overflow. We are interested in annotating the presence of emotions in technical documents authored by developers during their online interactions.\nThe data source is the official Stack Overflow dump released by Stack Exchange on May \u201915. You will be required to\nannotate randomly selected posts, including questions, answers, and comments. The unit of annotation is the entire post.\nYou will use the coding schema reported in TABLE A. For each post, please indicate what emotion it conveys (if any) among the basic emotions (first column in the table), that are, love, joy, surprise, anger, sadness, and fear. Multiple Emotion labels are allowed but you should try to avoid if possible. You can use the second and third level in the schema as a reference for choosing the primary emotion, as shown in TABLE B.\nOnce you define the emotion label, please specify the emotion polarity accordingly, choosing among positive, negative, neutral, and mixed. If the post does not contain any emotion, it should be annotated as neutral. The surprise is the only emotion that could match any of the polarity value: please, carefully evaluate each post in order to determine if it conveys positive, negative, or neutral polarity. If multiple emotion labels are indicated in a given text, you should define the polarity accordingly. A text annotated with one or more positive emotions only has a positive polarity. Conversely, a post annotated with one or more negative emotions holds a negative polarity. If both positive and negative emotions are found, you should indicate both. If you wish to indicate a polarity label you are required to specify the corresponding emotion. The absence of emotion can be annotated exclusively as neutral. The list of all possible combination allowed and not allowed by our coding schema is reported in TABLE C.\nTABLE A. MAPPING THE SHAVER ET AL. EMOTION FRAMEWORK TO SENTIMENT POLARITY\nEmotion Polarity\nBasic Emotions\nSecond level Emotions Third level Emotions\nPositive\nLove\nAffection Liking, Caring, Compassion, Fondness, Affection, Love, Attraction, Tenderness, Sentimentality\nLust Desire, Passion, Infatuation, Arousal\nLonging ---\nJoy\nCheerfulness Happiness, Amusement, Satisfaction, Bliss, Gaiety, Glee, Jolliness, Joviality, Joy, Delight, Enjoyment, Gladness, Jubilation, Elation, Ecstasy, Euphoria\nZest Enthusiasm, Excitement, Thrill, Zeal, Exhilaration\nContentment Pleasure\nOptimism Hope, Eagerness\nPride Triumph\nEnthrallment Rapture\nNegative\nAnger\nIrritation Annoyance, Agitation, Grumpiness, Aggravation, Grouchiness\nExasperation Frustration\nRage Anger, Fury, Hate, Dislike, Resentment, Outrage, Wrath, Hostility, Bitterness, Ferocity, Loathing, Scorn, Spite, Vengefulness\nDisgust Revulsion, Contempt\nEnvy Jealousy\nTorment ---\nSadness\nSuffering Hurt, Anguish, Agony\nSadness Depression, Sorrow, Despair, Gloom, Hopelessness, Glumness, Unhappiness, Grief, Woe, Misery, Melancholy\nDisappointment Displeasure, Dismay\nShame Guilt, Regret, Remorse\nNeglect Embarrassment, Insecurity, Insult, Rejection, Alienation, Isolation, Loneliness, Homesickness, Defeat, Dejection, Humiliation\nSympathy Pity\nFear Horror Alarm, Fright, Panic, Terror, Fear, Hysteria, Shock, Mortification\nNervousness Anxiety, Distress, Worry, Uneasiness, Tenseness, Apprehension, Dread\nEither Positive or Negative Surprise Surprise Amazement, Astonishment\nTABLE B. EXAMPLES OF ANNOTATED POSTS.\nInput Text\nAnnotation Rationale for annotation\n(second and third level emotion found) Basic Emotion(s)\nfound Polarity\n\u201cThanks for your input! You're, like, awesome\u201d Love Positive Liking (third level), Affection (second level) indicating gratitude. \u201cI'm happy with the approach and the code looks good \u201d. Joy Positive Happiness, Satisfaction (third), Cheerfulness (second) \u201cI still question the default, which can lead to surprisingly huge memory use\u201d Surprise Negative Surprise (second) due to the unexpected undesirable behavior of the code. \u201cI will come over to your work and slap you \u201d Anger Negative Hostility (third), Rage (second) \u201cSorry for the delay Stephen\u201d Sadness Negative Guilt (third), Shame (second) \u201cI'm worried about some subtle differences between char and Character\u201d Fear Negative Worry (third)\nTABLE C. COMBINATIONS OF VALUES ALLOWED AND NOT ALLOWED BY OUR ANNOTATION SCHEMA.\nLove Joy Surprise Anger Sadness Fear Polarity Explanation\nAnnotation allowed by our schema\nx Negative Negative emotion and negative polarity\nx Positive Positive emotion and positive polarity\nx Negative\nSurprise is intrinsically ambiguous, all polarity values are allowed x Positive\nx Neutral\nx x Positive Multiple emotion labels, positive polarity\nx x Negative Multiple emotion labels, negative polarity\nx x Mixed Multiple emotion labels, mixed polarity\nNeutral Absence of emotion\nAnnotation NOT allowed by our schema\nNegative No emotion and negative polarity\nPositive No emotion and positive polarity\nMixed No emotion and mixed polarity\nx Neutral Emotion label different from surprise and neutral polarity\nx Neutral"}], "references": [{"title": "Discovering value from community activity on focused question", "author": ["A Anderson", "D Huttenlocher", "J Kleinberg", "J Leskovec"], "venue": null, "citeRegEx": "Anderson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2012}, {"title": "Answering questions about unanswered questions of stack", "author": ["M Asaduzzaman", "AS Mashiyat", "CK Roy", "KA Schneider"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting", "author": ["M Baroni", "G Dinu", "G Kruszewski"], "venue": "USA, MSR", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "What are developers talking about? an analysis of topics and trends in stack", "author": ["A Barua", "SW Thomas", "AE Hassan"], "venue": null, "citeRegEx": "Barua et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Barua et al\\.", "year": 2014}, {"title": "Uniba: Sentiment analysis of English tweets combining micro-blogging, lexicon and semantic", "author": ["P Basile", "N Novielli"], "venue": null, "citeRegEx": "Basile and Novielli,? \\Q2015\\E", "shortCiteRegEx": "Basile and Novielli", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Y Bengio", "R Ducharme", "P Vincent", "C Janvin"], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit", "author": ["London", "pp 23\u201344 Cohen J"], "venue": null, "citeRegEx": "London et al\\.,? \\Q1968\\E", "shortCiteRegEx": "London et al\\.", "year": 1968}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask", "author": ["R Bulletin Collobert", "J Weston"], "venue": null, "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "A computational approach to politeness with", "author": ["C Danescu-Niculescu-Mizil", "M Sudhof", "D Jurafsky", "J Leskovec", "C Potts"], "venue": null, "citeRegEx": "Danescu.Niculescu.Mizil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Danescu.Niculescu.Mizil et al\\.", "year": 2013}, {"title": "The Association for Computer Linguistics, pp 250\u2013259", "author": ["A De Lucia", "F Fasano", "R Oliveto", "G Tortora"], "venue": "Ekman P", "citeRegEx": "Lucia et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lucia et al\\.", "year": 1999}, {"title": "Liblinear: A library for large linear classification", "author": ["RE Fan", "KW Chang", "CJ Hsieh", "XR Wang", "CJ Lin"], "venue": "Trans Softw Eng Methodol", "citeRegEx": "Fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2012}, {"title": "Exploring causes of frustration for software developers. In CHASE, pages 115\u2013116", "author": ["D Ford", "C Parnin"], "venue": null, "citeRegEx": "Ford and Parnin,? \\Q2015\\E", "shortCiteRegEx": "Ford and Parnin", "year": 2015}, {"title": "Unhappy Developers: Bad for Themselves, Bad for Process", "author": ["D NIER.2017.18 Graziotin", "F Fagerholm", "X Wang", "P Abrahamsson"], "venue": null, "citeRegEx": "Graziotin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Graziotin et al\\.", "year": 2017}, {"title": "Towards emotional awareness in software development", "author": ["E Guzman", "B Bruegge"], "venue": "teams. In: Proceedings of the 2013", "citeRegEx": "Guzman and Bruegge,? \\Q2013\\E", "shortCiteRegEx": "Guzman and Bruegge", "year": 2013}, {"title": "Sentiment analysis of commit comments in Github: An empirical study", "author": ["E Guzman", "D Azocar", "Y Li"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "2016) A needle in a haystack: What do twitter users say about software", "author": ["E Guzman", "R Alkadhi", "N Seyff"], "venue": null, "citeRegEx": "Guzman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guzman et al\\.", "year": 2016}, {"title": "Learning from Imbalanced Data", "author": ["H He", "EA Garcia"], "venue": "Engineering Conference (RE),", "citeRegEx": "He and Garcia,? \\Q2016\\E", "shortCiteRegEx": "He and Garcia", "year": 2016}, {"title": "Using rhetorical structure in sentiment analysis", "author": ["A Hogenboom", "F Frasincar", "F de Jong", "U Kaymak"], "venue": null, "citeRegEx": "Hogenboom et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hogenboom et al\\.", "year": 2015}, {"title": "Choosing your weapons: On sentiment analysis tools for software engineering", "author": ["R Jongeling", "S Datta", "A Serebrenik"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A large- scale sentiment analysis for Yahoo! answers", "author": ["BB Cambazoglu", "I Weber", "H Ferhatosmanoglu"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Emotion and adaptation", "author": ["R Lazarus"], "venue": "PSYCHOLOGICAL", "citeRegEx": "Lazarus,? \\Q1991\\E", "shortCiteRegEx": "Lazarus", "year": 1991}, {"title": "On the automatic classification of app reviews", "author": ["Z Kurtanovic", "H Nabil", "C Stanik"], "venue": "Requirements Engineering", "citeRegEx": "Maalej et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Maalej et al\\.", "year": 2016}, {"title": "The Stanford CoreNLP natural language", "author": ["CD Manning", "M Surdeanu", "J Bauer", "J Finkel", "SJ Bethard", "D McClosky"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Mining valence, arousal, and dominance: Possibilities", "author": ["Demonstrations", "M pp 55\u201360 M\u00e4ntyl\u00e4", "B Adams", "G Destefanis", "D Graziotin", "M Ortu"], "venue": null, "citeRegEx": "Demonstrations et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Demonstrations et al\\.", "year": 2016}, {"title": "Bootstrapping a lexicon for emotional arousal in software", "author": ["MV M\u00e4ntyl\u00e4", "N Novielli", "F Lanubile", "M Claes", "M Kuutila"], "venue": null, "citeRegEx": "M\u00e4ntyl\u00e4 et al\\.,? \\Q2017\\E", "shortCiteRegEx": "M\u00e4ntyl\u00e4 et al\\.", "year": 2017}, {"title": "Efficient estimation of word representations in vector space", "author": ["T Mikolov", "K Chen", "G Corrado", "J Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2017}, {"title": "Distributed representations of words and phrases", "author": ["T Mikolov", "I Sutskever", "K Chen", "GS Corrado", "J Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "NRC-Canada: Building the state-of-the-art in sentiment analysis of tweets", "author": ["Measurement H (Ed) Emotion", "SM Elsevier Mohammad", "S Kiritchenko", "X Zhu"], "venue": null, "citeRegEx": "Emotion et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Emotion et al\\.", "year": 2013}, {"title": "Stuck and frustrated or in flow and happy: sensing developers' emotions and progress", "author": ["SC M\u00fcller", "T Fritz"], "venue": "CoRR abs/1308.6242,", "citeRegEx": "M\u00fcller and Fritz,? \\Q2015\\E", "shortCiteRegEx": "M\u00fcller and Fritz", "year": 2015}, {"title": "Towards discovering the role of emotions in Stack Overflow", "author": ["N Novielli", "F Calefato", "F Lanubile"], "venue": null, "citeRegEx": "Novielli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Novielli et al\\.", "year": 2014}, {"title": "The challenges of sentiment detection in the social programmer ecosystem", "author": ["F Calefato", "F Lanubile"], "venue": null, "citeRegEx": "Novielli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novielli et al\\.", "year": 2015}, {"title": "Are bullies more productive?: Empirical study", "author": ["M Ortu", "B Adams", "G Destefanis", "P Tourani", "M Marchesi", "R Tonelli"], "venue": null, "citeRegEx": "Ortu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ortu et al\\.", "year": 2015}, {"title": "Opinion mining and sentiment anal- ysis", "author": ["NY York"], "venue": "Found Trends Inf Retr 2(1-2):1\u2013135,", "citeRegEx": "York,? \\Q2008\\E", "shortCiteRegEx": "York", "year": 2008}, {"title": "Linguistic Inquiry and Word Count: LIWC", "author": ["J Evolution Pennebaker", "M Francis"], "venue": "Erlbaum Publishers,", "citeRegEx": "Pennebaker and Francis,? \\Q2001\\E", "shortCiteRegEx": "Pennebaker and Francis", "year": 2001}, {"title": "Recommending insightful comments for source code using crowdsourced", "author": ["MM Rahman", "CK Roy", "I Keivanloo"], "venue": null, "citeRegEx": "Rahman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rahman et al\\.", "year": 2015}, {"title": "A circumplex model of affect. Journal of personality and social psychology", "author": ["H Saif", "M Fernandez", "Y He", "H Alani"], "venue": "DOI 10.1109/SCAM.2015.7335404 Russell J", "citeRegEx": "Saif et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Saif et al\\.", "year": 2015}, {"title": "Emotions in everyday life: Probability of oc- currence, risk", "author": ["(ELRA) Association", "Reykjavik", "K Iceland Scherer", "T Wranik", "J Sangsue", "V Tran", "U Scherer"], "venue": null, "citeRegEx": "Association et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Association et al\\.", "year": 2004}, {"title": "Emotion knowledge: Further exploration of a prototype approach", "author": ["York NY", "P USA Shaver", "J Schwartz", "D Kirson", "C O\u2019Connor"], "venue": null, "citeRegEx": "NY et al\\.,? \\Q1987\\E", "shortCiteRegEx": "NY et al\\.", "year": 1987}, {"title": "Analyzing developer sentiment in commit logs", "author": ["V Sinha", "A Lazar", "B Sharif"], "venue": "Proceedings of the 13th International", "citeRegEx": "Sinha et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sinha et al\\.", "year": 2016}, {"title": "Recursive deep models for semantic", "author": ["R Socher", "A Perelygin", "J Wu", "J Chuang", "CD Manning", "AY Ng", "C Potts"], "venue": "Artificial Intelligence", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "The general inquirer: A computer approach to content analysis", "author": ["PJ 1083\u20131086 Stone", "DC Dunphy", "MS Smith", "DM Ogilvie"], "venue": null, "citeRegEx": "Stone et al\\.,? \\Q1966\\E", "shortCiteRegEx": "Stone et al\\.", "year": 1966}, {"title": "Sentiment strength detection for the social web", "author": ["Cambridge", "M MA: The MIT Press. Thelwall", "K Buckley", "G Paltoglou"], "venue": "J Am Soc Inf Sci Technol", "citeRegEx": "Cambridge et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cambridge et al\\.", "year": 2012}, {"title": "Sewordsim: Software-specific word similarity database", "author": ["Y Tian", "D Lo", "J Lawall"], "venue": "Companion Proceedings of the 36th", "citeRegEx": "Tian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2014}, {"title": "Pattern-based emotion classification on social media", "author": ["E Tromp", "M Pechenizkiy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}], "referenceMentions": [{"referenceID": 34, "context": "Recent studies suggest approaches for enhancing software development, maintenance, and evolution by applying sentiment analysis on Stack Overflow (Rahman et al. 2015), app reviews (Maalej et al.", "startOffset": 146, "endOffset": 166}, {"referenceID": 21, "context": "2015), app reviews (Maalej et al. 2016), and tweets containing comments on software applications (Guzman et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 15, "context": "2016), and tweets containing comments on software applications (Guzman et al. 2016).", "startOffset": 63, "endOffset": 83}, {"referenceID": 39, "context": "2015), empirical software engineering studies have exploited off-the-shelf sentiment analysis tools that have been trained on non-software engineering documents, such as movie reviews (Socher et al. 2013), or posts crawled from general-purpose social media, such as Twitter and YouTube (Thelwall et al.", "startOffset": 184, "endOffset": 204}, {"referenceID": 13, "context": "2014, Guzman and Bruegge 2013, Sinha et al.2016), Jira (M\u00e4ntyl\u00e4 et al. 2016, Ortu et al. 2015), and Stack Overflow (Calefato et al. 2015, Novielli et al. 2015). With a notable few exceptions (Blaz and Becker 2016, Panichella et al. 2015), empirical software engineering studies have exploited off-the-shelf sentiment analysis tools that have been trained on non-software engineering documents, such as movie reviews (Socher et al. 2013), or posts crawled from general-purpose social media, such as Twitter and YouTube (Thelwall et al. 2012). Jongeling et al. (2017) show how the choice of the sentiment analysis tool may impact the conclusion validity of empirical studies by performing a benchmarking study on seven datasets, including discussions and comments from Stack Overflow and issue trackers.", "startOffset": 6, "endOffset": 566}, {"referenceID": 4, "context": "We propose a sentiment analysis classifier, named Senti4SD, which exploits a suite of lexicon-based, keyword-based, and semantic features (Basile and Novielli 2015) for appropriately dealing with the domain-dependent use of a lexicon.", "startOffset": 138, "endOffset": 164}, {"referenceID": 26, "context": "Specifically, we used word2vec (Mikolov et al. 2013) to build a Distributional Semantic Model (DSM) where words are represented as high-dimensional vectors.", "startOffset": 31, "endOffset": 52}, {"referenceID": 22, "context": "It is particularly the case of bug reports or problem descriptions (Blaz and Becker 2016, Novielli et al. 2015). Novielli et al. (2015) show how sentences like \u201cWhat is the best way to kill a critical process\u201d or \u201cI am missing a parenthesis but I don\u2019t know where\u201d are erroneously classified as negative because both \u2018to kill\u2019 and \u2018missing\u2019 hold a negative polarity in the SentiStrength lexicon.", "startOffset": 90, "endOffset": 136}, {"referenceID": 20, "context": "Lazarus (1991) describes nine negative (anger, fright, anxiety, guilt, shame, sadness, envy, jealousy, and disgust) and seven positive (happiness, pride, relief, love, hope, compassion, and gratitude) emotions, with their appraisal patterns: positive emotions are triggered if the situation experienced is congruent with an individual goal, otherwise negative emotions are prompted.", "startOffset": 0, "endOffset": 15}, {"referenceID": 20, "context": "Lazarus (1991) describes nine negative (anger, fright, anxiety, guilt, shame, sadness, envy, jealousy, and disgust) and seven positive (happiness, pride, relief, love, hope, compassion, and gratitude) emotions, with their appraisal patterns: positive emotions are triggered if the situation experienced is congruent with an individual goal, otherwise negative emotions are prompted. Shaver et al. (1987) defined a tree-structured hierarchical classification of emotions.", "startOffset": 0, "endOffset": 404}, {"referenceID": 2, "context": "Such methods are usually referred in the literature as context-counting approaches (Baroni et al. 2014).", "startOffset": 83, "endOffset": 103}, {"referenceID": 2, "context": "For this reason, they are usually referred to as context-predicting approaches (Baroni et al. 2014).", "startOffset": 79, "endOffset": 99}, {"referenceID": 2, "context": "Furthermore, they outperform traditional context-counting approaches on standard lexical semantics benchmarks (Baroni et al. 2014).", "startOffset": 110, "endOffset": 130}, {"referenceID": 2, "context": "Such methods are usually referred in the literature as context-counting approaches (Baroni et al. 2014). Recently, neural network-based approaches have been proposed (Bengio et al. 2003, Collobert and Weston 2008, Mikolov et al. 2013a) for learning distributed representation of words as continuous vectors. These approaches, also known as word embedding (Levy and Goldberg 2014), learn the vectors that maximize the probability of the contexts in which the target word appears. For this reason, they are usually referred to as context-predicting approaches (Baroni et al. 2014). In our study, we leverage the approach defined by Milokov et al. (2013). They developed two models for implementing context-predicting approaches: (1) the Continuous Bag-of-Words (CBOW) model predicts the target word by considering the previous and following n words in a symmetrical context window; (2) the Skip-gram model predicts the surrounding words based on the target word.", "startOffset": 84, "endOffset": 652}, {"referenceID": 30, "context": "In a previous study (Novielli et al. 2015), we found that stronger expressions of emotions are usually detected in comments rather than in question or answers.", "startOffset": 20, "endOffset": 42}, {"referenceID": 40, "context": "Furthermore, it incorporates sentiment scores from other linguistic resources that were previously validated in the scope of empirical research in sentiment analysis (Stone et al., 1966) and psycholinguistics (Pennebaker and Francis, 2001).", "startOffset": 166, "endOffset": 186}, {"referenceID": 33, "context": ", 1966) and psycholinguistics (Pennebaker and Francis, 2001).", "startOffset": 30, "endOffset": 60}, {"referenceID": 4, "context": "Other than including n-grams, we designed features able to capture aspects of micro-blogging, such as the use of uppercase and elongated words used as intensifiers, the presence of positive and negative emoticons, and the occurrence of slang expression of laughter (Basile and Novielli 2015).", "startOffset": 265, "endOffset": 291}, {"referenceID": 26, "context": "The semantic features are computed on a DSM built on Stack Overflow data, using the CBOW architecture implemented by word2vec (Mikolov et al. 2013), as depicted in Fig.", "startOffset": 126, "endOffset": 147}, {"referenceID": 22, "context": "Before extracting all features, we performed tokenization using the Stanford NLP suite (Manning et al. 2014).", "startOffset": 87, "endOffset": 108}, {"referenceID": 26, "context": "defined by the sample input parameter) are down-sampled to increase the effective context window considered for vector prediction (Mikolov et al. 2013).", "startOffset": 130, "endOffset": 151}, {"referenceID": 4, "context": "Consistently with previous research (Basile and Novielli 2015), we maintained the default Fig.", "startOffset": 36, "endOffset": 62}, {"referenceID": 30, "context": "The performance of SentiStrength on our test set (see TABLES 8 and 9) confirms previous findings about its negative bias in the software engineering domain (Novielli et al. 2015).", "startOffset": 156, "endOffset": 178}, {"referenceID": 11, "context": ", learning a new language or solving tasks with high reasoning complexity) (Ford and Parnin, 2015), as well as in their daily programming tasks (M\u00fcller and Fritz 2015).", "startOffset": 75, "endOffset": 98}, {"referenceID": 28, "context": ", learning a new language or solving tasks with high reasoning complexity) (Ford and Parnin, 2015), as well as in their daily programming tasks (M\u00fcller and Fritz 2015).", "startOffset": 144, "endOffset": 167}, {"referenceID": 10, "context": "(Denning 2012, Ford and Parnin, 2015, Graziotin et al. 2017). When implementing a sentiment classifier, deciding whether to optimize by precision or by recall is not a trivial decision, which depends on the application scenario. Early detection of negative sentiment towards self, such as frustration, could be useful to design tools for supporting developers experiencing cognitive difficulties (i.e., learning a new language or solving tasks with high reasoning complexity) (Ford and Parnin, 2015), as well as in their daily programming tasks (M\u00fcller and Fritz 2015). In such a scenario, a monitoring tool might suggest the intervention of an expert or provide a link to further material and documentation to support the developer. However, a sentiment analysis tool with high recall and low precision for negative sentiment as SentiStrength would produce several false positives, causing undesired, erroneous interruptions that are detrimental to developers' productivity and focus. In such cases, being able to reduce the number of false positives for negative sentiment becomes crucial and Senti4SD should be preferred to SentiStrength, due to its higher precision. Similarly, timely detection of negative sentiment towards peers, such as anger and hostility (Gachechiladze et al 2017), might be exploited for detecting code of conduct violations (Tromp and Pechenizkiy 2015) or enhancing effective community management. For example, sentiment analysis may support GitHub users who want to be notified of heated conversation and lock them before flame wars break out. In scenarios that involve human intervention to guide the contributors\u2019 behavior towards a constructive pattern, it might be desirable to optimize negative sentiment detection by recall, thus choosing to leverage SentiStrength higher sensitivity to negative emotions. Conversely, if automatic filtering of offensive comments or conversation is envisaged, it becomes important to optimize by precision by using Senti4SD, to avoid banning neutral conversations. Finally, sentiment analysis is now regarded as a technique also useful for mining large software repositories, e.g., to understand the role of sentiment in security discussions (Pletea et al. 2014) and commits in GitHub (Guzman et al. 2014). In such scenarios, a sentiment classifier specifically trained and validated in the software engineering domain allows controlling for threats to validity due to inappropriate instrumentation, as argued by Jongeling et al (2017). Contribution of features.", "startOffset": 15, "endOffset": 2504}, {"referenceID": 30, "context": "Being able to identify harsh comments towards technical matters could be useful in detecting particularly challenging questions that have not been exhaustively answered (Novielli et al. 2015), which is a goal addressed by current research on effective knowledge-sharing (Anderson et al.", "startOffset": 169, "endOffset": 191}, {"referenceID": 0, "context": "2015), which is a goal addressed by current research on effective knowledge-sharing (Anderson et al. 2012).", "startOffset": 84, "endOffset": 106}, {"referenceID": 0, "context": "2015), which is a goal addressed by current research on effective knowledge-sharing (Anderson et al. 2012). Similarly, detecting negative attitude towards the interlocutor could allow the community moderators to guide users towards appropriate interaction patterns. This is an open problem in the Stack Overflow community, as users complain about harsh comments coming from expert contributors (Meta 2017), which may impair successful question-answering (Asaduzzaman et al. 2013). The release of our gold standard complements the effort of Ortu et al. (2016) who recently released a dataset of 2,000 issue comments and 4,000 sentences written by developers, collected by mining the repositories of four open source ecosystems, namely Apache, Spring, JBoss, and CodeHaus.", "startOffset": 85, "endOffset": 559}, {"referenceID": 0, "context": "2015), which is a goal addressed by current research on effective knowledge-sharing (Anderson et al. 2012). Similarly, detecting negative attitude towards the interlocutor could allow the community moderators to guide users towards appropriate interaction patterns. This is an open problem in the Stack Overflow community, as users complain about harsh comments coming from expert contributors (Meta 2017), which may impair successful question-answering (Asaduzzaman et al. 2013). The release of our gold standard complements the effort of Ortu et al. (2016) who recently released a dataset of 2,000 issue comments and 4,000 sentences written by developers, collected by mining the repositories of four open source ecosystems, namely Apache, Spring, JBoss, and CodeHaus. Their dataset is annotated using the basic emotion labels in the framework by Shaver et al. (1987) that we also adopt in the present study.", "startOffset": 85, "endOffset": 870}, {"referenceID": 8, "context": "Their approach exploits SVM using a suite of features based on the SentiStrength output, the politeness score (Danescu-Niculescu-Mizil et al. 2013), and the presence of affective words derived from WordNetAffect (Strapparava and Valitutti 2004).", "startOffset": 110, "endOffset": 147}, {"referenceID": 23, "context": "M\u00e4ntyl\u00e4 et al. (2016) investigated the potential of mining developers\u2019 emotions in issue-tracking systems to prevent loss of productivity and burnout.", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "M\u00e4ntyl\u00e4 et al. (2016) investigated the potential of mining developers\u2019 emotions in issue-tracking systems to prevent loss of productivity and burnout. They measured the emotions in issue comments in terms of VAD metrics, that is, scores for the Valence (i.e., the affect polarity), Arousal (i.e., the affect intensity), and Dominance (i.e., the sensation of being in control of a situation). To estimate VAD scores, they adopted the same lexicon-based approach implemented by SentiStrength, using a VAD lexicon of over 13K English words developed by psychology research. However, given the lack of a gold standard for VAD, they were not able to provide any evaluation of their approach to emotion mining. Ortu et al. (2015) presented an empirical study on the correlation of emotions and issue-fixing time in the Apache issuetracking system.", "startOffset": 0, "endOffset": 724}, {"referenceID": 23, "context": "M\u00e4ntyl\u00e4 et al. (2016) investigated the potential of mining developers\u2019 emotions in issue-tracking systems to prevent loss of productivity and burnout. They measured the emotions in issue comments in terms of VAD metrics, that is, scores for the Valence (i.e., the affect polarity), Arousal (i.e., the affect intensity), and Dominance (i.e., the sensation of being in control of a situation). To estimate VAD scores, they adopted the same lexicon-based approach implemented by SentiStrength, using a VAD lexicon of over 13K English words developed by psychology research. However, given the lack of a gold standard for VAD, they were not able to provide any evaluation of their approach to emotion mining. Ortu et al. (2015) presented an empirical study on the correlation of emotions and issue-fixing time in the Apache issuetracking system. They measure the emotion polarity in issue comments using SentiStrength. As for discrete emotion labels, they developed their own classifier for detecting the presence of four basic emotions framework by the Shaver et al. (1987), namely anger, joy, sadness, and love.", "startOffset": 0, "endOffset": 1071}, {"referenceID": 8, "context": "Their approach exploits SVM using a suite of features based on the SentiStrength output, the politeness score (Danescu-Niculescu-Mizil et al. 2013), and the presence of affective words derived from WordNetAffect (Strapparava and Valitutti 2004). The classifier is evaluated on a gold standard of 4,000 sentences, obtaining an F-measure score ranging from .74 for anger to .82 for sadness. At the time of writing, the classifier is not yet available for research purposes. Blaz and Becker (2016) developed a polarity classifier for IT tickets.", "startOffset": 111, "endOffset": 495}, {"referenceID": 4, "context": ", for unsupervised speech-act recognition in telephone conversations (Novielli and Strapparava 2013) and for sentiment analysis in micro-blogging (Basile and Novielli 2015).", "startOffset": 146, "endOffset": 172}, {"referenceID": 3, "context": "Traditional, contextcounting approaches to distributional semantics have already been used, including Latent Dirichlet Allocation for topic modeling in Stack Overflow (Barua et al. 2014) and Latent Semantic Analysis for recovering traceability links in software artifact (De Lucia et al.", "startOffset": 167, "endOffset": 186}, {"referenceID": 2, "context": "Traditional, contextcounting approaches to distributional semantics have already been used, including Latent Dirichlet Allocation for topic modeling in Stack Overflow (Barua et al. 2014) and Latent Semantic Analysis for recovering traceability links in software artifact (De Lucia et al. 2007). Tian et al. (2014) recently proposed the use of pointwise mutual information to represents word similarity in a high-dimensional space.", "startOffset": 168, "endOffset": 314}, {"referenceID": 2, "context": "2013a) \u2013 and provides more effective vector representation of words (Baroni et al. 2014, Mikolov et al. 2013b). Thus, in our study, we adopt word embedding for building our distributional semantic model. Ye et al. (2016) already exploited word embedding for enhancing information retrieval in software engineering.", "startOffset": 69, "endOffset": 221}], "year": 2017, "abstractText": "The role of sentiment analysis is increasingly emerging to study software developers\u2019 emotions by mining crowdgenerated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers\u2019 communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexiconand keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines.", "creator": "Word"}}}