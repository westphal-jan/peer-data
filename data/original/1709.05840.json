{"id": "1709.05840", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2017", "title": "Autoencoder-Driven Weather Clustering for Source Estimation during Nuclear Events", "abstract": "Emergency response applications for nuclear or radiological events can be significantly improved via deep feature learning due to the hidden complexity of the data and models involved. In this paper we present a novel methodology for rapid source estimation during radiological releases based on deep feature extraction and weather clustering. Atmospheric dispersions are then calculated based on identified predominant weather patterns and are matched against simulated incidents indicated by radiation readings on the ground. We evaluate the accuracy of our methods over multiple years of weather reanalysis data in the European region. We juxtapose these results with deep classification convolution networks and discuss advantages and disadvantages.", "histories": [["v1", "Mon, 18 Sep 2017 09:55:31 GMT  (1284kb,D)", "http://arxiv.org/abs/1709.05840v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["i a klampanos", "a davvetas", "s", "ronopoulos", "c pappas", "a ikonomopoulos", "v karkaletsis"], "accepted": false, "id": "1709.05840"}, "pdf": {"name": "1709.05840.pdf", "metadata": {"source": "CRF", "title": "Autoencoder-Driven Weather Clustering for Source Estimation during Nuclear Events", "authors": ["Iraklis A. Klampanos", "Athanasios Davvetas", "Spyros Andronopoulos", "Charalambos Pappas", "Andreas Ikonomopoulos", "Vangelis Karkaletsis"], "emails": ["iaklampanos@iit.demokritos.gr"], "sections": [{"heading": null, "text": "via deep feature learning due to the hidden complexity of the data and models involved. In this paper we present a novel methodology for rapid source estimation during radiological releases based on deep feature extraction and weather clustering. Atmospheric dispersions are then calculated based on identified predominant weather patterns and are matched against simulated incidents indicated by radiation readings on the ground. We evaluate the accuracy of our methods over multiple years of weather reanalysis data in the European region. We juxtapose these results with deep classification convolution networks and discuss advantages and disadvantages."}, {"heading": "1 Introduction", "text": "In the field of atmospheric dispersion modelling and its applications for supporting decision making during events of atmospheric releases of hazardous substances (e.g. radioactive material), inverse source-term estimation and source inversion refer to computational methods aiming at estimating the location and/or the emitted quantities of the hazardous material using both observations (readings on the ground) and results of dispersion models. Such methods are typically used when the presence of a hazardous substance above the background levels in the air is detected by an existing monitoring network, while its origin is unknown.\nThe most characteristic example of a real case involving radioactive elements that have been detected before the release was officially announced is the Chernobyl Nuclear Power Plant accident1. The Acerinox (or Algeciras) incident is another example of an unknown radioactive release that was traced back after radioactivity levels higher than the background had been observed at very long distances from the release location[20].\nDepending on various factors, such as the spatial resolution desired, traditional inverse modelling can be computationally expensive, and therefore time-consuming, rendering its application problematic when timing is critical. In addition, atmospheric dispersion models (e.g. HYSPLIT[7, 21]) are complex pieces of software that require expert training and case-by-case application. In this paper we present a\n\u2217Corresponding author. Email: iaklampanos@iit.demokritos.gr 1http://www.irsn.fr/EN/publications/thematic-safety/chernobyl/\nar X\niv :1\n70 9.\n05 84\n0v 1\n[ cs\n.L G\nnovel, complementary approach based on data analytics and deep learning. Our goals are to effectively transfer the computational bulk before the time of such an event taking place, create reusable data byproducts of value and complement existing emergency response methodologies.\nFocusing on the European region, we cluster re-analysis weather data in order to derive weather circulation patterns, which affect plume dispersion. We then calculate plume dispersions for a number of European nuclear power plant sites of interest, based on representative cluster descriptors. Last, we match previously unseen weather to the weather patterns we have learned and rank the nuclear sites of interest according to how well their plume dispersion for the closest weather patterns match current hypothetical radiation measurements.\nThis application combines into a framework data analytical and machine learning methods, e.g. deep autoencoders and k-means clustering, large weather datasets and the effective approximation of weather and atmospheric dispersion models using big data technologies.\nWe make the following contributions:\n1. we propose and evaluate a novel methodology for release origin estimation;\n2. we evaluate a series of autoencoder-based configurations, followed by k-means clustering applied on weather re-analysis data;\n3. we juxtapose our method with deep convolution networks for classification, discussing their respective advantages and disadvantages; and\n4. we present an application prototype for the rapid estimation of release origin and its implementation.\nIn the following section we provide a succinct discussion of related approaches, technologies and methods. In Section 3 we present the proposed methodology and rationale, while in Section 4 we provide a discussion of our evaluation methodology and discuss results and findings. In Section 5 we present a pilot application showcasing the proposed approach. Last, in Section 6 we conclude the paper and discuss directions for future work."}, {"heading": "2 Related Work", "text": "This study combines algorithms and methods spanning a number of disciplines, namely machine learning, weather circulation clustering, atmospheric dispersion and weather modelling. In this section we provide an overview of relevant work."}, {"heading": "2.1 Discovering Weather Patterns", "text": "Over the last decades there have been several studies attempting to automatically discover weather patterns via unsupervised hierarchical and iterative clustering[14], as part of synoptic climatology[29]. Many of these studies have attempted to establish statistically robust representations based on the calculation of principal components in, what this body of literature refers to as, S- and T-modes. S-mode refers to the typical application of principal component analysis (PCA) per weather sample used for feature reduction, while T-mode refers to the application of PCA per grid point. (In our evaluation, in Section 4, we refer to the T-mode also as PCAT , to indicate cases where PCA has been applied on the \u201ctransposed\u201d form of the data samples.) Indicatively, Huth[13] summarizes and compares correlation, sums-of-squares, agglomerative hierarchical, PCA, k-means and hierarchical agglomerative clustering algorithms. These algorithms were executed on geopotential height (GHT) at a pressure level of 700hPa (more information on pressure levels is provided in Section 3.1) and were evaluated using internal metrics, such as consistency and robustness. According to Huth, there is no clear winner for clustering weather patterns, however T-mode PCA appears to produce clusters that resemble manually identified weather patterns. Methods based on more recent advances in neural networks, employing self-organizing maps, have also been reported[5, 10].\nOther classification and clustering approaches tailored to specific applications or geographical regions have been published. Teixeira de Lima and Stephany[24] evaluate a multitude of weather variables for extreme weather classification. Straus et.al.[22] use a modified version of k-means clustering for the pressure level of 200hPa and zonal winds to discover and analyze winter circulation regimes over the Pacific-North American region. Hsu and Cheng[12] evaluate daily average surface wind measurements to discover weather patterns affecting air pollution in western Taiwan. Focusing on the urban heat island, the phenomenon in which an urban area is significantly warmer than its surrounding rural areas due to human activity, Hoffmann and Schl\u00fcnzen[11] study k-means on a number of variables such as GHT, relative humidity, vorticity, and others. Far from being exhaustive, the above studies are indicative of the multitude of applications, clustering and classification approaches which have been reported in this field.\nWhile part of our study involves discovering useful weather patterns, our application of these clusters is specific to capturing the conditions leading to similar plume dispersions. Our experiments, reported in Section 4, focus on GHT as a feature predictive of circulation patterns and consequently, plume dispersion."}, {"heading": "2.2 Autoencoders for Feature Reduction", "text": "An autoencoder[16, 2, 8] is an unsupervised feed-forward neural network designed to approximate the identity function, i.e. one that attempts to learn a function h(x) = x\u0302 \u2248 x, where x and x\u0302 denote the input and output vectors respectively. Post-training, applications typically disregard the output of autoencoders, instead making use of the activation values of the hidden layers. In the case of a simple, single-layer auto-encoder, the hidden activations are equivalent to PCA[4].\nIn its simplest form, when there is a single hidden layer and the number of hidden units equals the number of inputs, the auto-encoder is too successful in replicating the input, leading to overfitting. Various methods have been suggested to avoid overfitting, e.g. having fewer hidden than input units, by enforcing activation sparsity or by introducing noise which the auto-encoder learns to compensate for[27, 26]. An alternative approach is to use deeper configurations of stacked autoencoders, where inner\nlayers encode and decode previously encoded vectors. Encodings generated by stacked autoencoders can capture deeper statistical representations of the input data.\nAutoencoders have been augmented by different types of deep neural networks, such as convolutional networks, or convnets. Convnets[17, 8] are designed to discover multi-dimensional patterns of varying sizes and have been applied, stand-alone or as part of more complex configurations, primarily in classification-based image recognition. It has been shown (e.g. by Gutstein et.al.[9]) that convnets allow for knowledge to be transferred from one-another, which is useful for continuous, parallel learning. Convolutional autoencoders are formed by stacking convolutional layers, fully connected layers and de-convolutional layers in a single configuration and are designed to capture feature hierarchies in the input space[18].\nIn this study we evaluate simple, stacked and convolutional denoising autoencoders[18] in an attempt to reach smaller and statistically robust representations of weather variables. We then use these encoded representations to discover weather patterns in Europe using the k-means algorithm."}, {"heading": "3 Methodology and Rationale", "text": "Our methodology comprises two main steps: (1) the derivation of weather patterns to be used for driving reference plume dispersion patterns, and (2) the subsequent estimation of the release origin. An overview of our methodology is shown in Fig. 2."}, {"heading": "3.1 Weather Re-analysis Data and Features", "text": "To discover weather patterns we use weather re-analysis data. This is gridded data that originates from optimal combinations of Numerical Weather Prediction (NWP) data and observations, therefore representing the best available description of past atmospheric conditions. Weather re-analysis data cover a sufficiently large period (40 years) and are of fine enough spatial (less than 1\u25e6) and temporal (up to 6hr) resolution over the region of interest, i.e. the European continent. This dataset is freely available online as part of the ERA-Interim project[6] and is provided by the European Centre for Medium range Weather Forecast (ECMWF)2.\nFor this study we used global ERA-Interim data covering 11 years (1986-1993 and 1996-1998) with a 6hr temporal resolution provisioned by NCAR in the GRIB format3. The default spatial resolution\n2http://www.ecmwf.int/en/research/climate-reanalysis/era-interim 3http://rda.ucar.edu/datasets/ds627.0/\nof the data is \u2248 0.7\u25e6\u00d7 0.7\u25e6 (\u2248 50\u00d7 50km). These data contain several atmospheric variables expressed across 37 vertical pressure levels ranging from 1hPa to 1000hPa \u2013 pressure levels can be seen as a more robust altitude representation. In this study we focus on the geopotential height (GHT) variable, a \u201cgravity-adjusted\u201d height, and more specifically to either GHT700hPa or a combination of GHT@500, 700 and 900hPa. GHT has been shown to be predictive of weather circulation patterns[13, 14]. Raw weather reanalysis data is downscaled using the Weather Research and Forecasting (WRF) modelling system[19] as a pre-processing step and to make the data compatible with the atmospheric dispersion model.\nThe resulting data used to derive weather patterns are either 2D grids of 64\u00d764 cells, corresponding to approx. 105km\u00d7105km, or 3D grids of 64\u00d764\u00d73 cells. An example of GHT at the 700hPA pressure level is shown in Fig. 3. The weather patterns discovered as a result of feature extraction and analysis are used for the calculation of forward dispersions. The resulting atmospheric dispersion grid has a size of 500\u00d7500 cells, corresponding to approx. 15km\u00d715km, and a temporal resolution of 1hr."}, {"heading": "3.2 Dispersion and Inverse Models", "text": "Atmospheric dispersion models are computational codes that simulate the processes of transport and diffusion of air pollutants, as well as other physical processes that occur during dispersion, such as deposition on the ground and transformations (chemical reactions, radioactive decay, etc.). Dispersion model calculations are based on meteorological data, such as these calculated by WRF, described above.\nDispersion models can operate in \u201cforward\u201d in time (prognostic) mode when the pollutant emission locations and the related emitted quantities (as functions of time) are known. They can also be used in \u201cinverse\u201d computations, where in combination with available measured pollutant concentrations, can estimate unknown pollutant emission locations and emission rates. There are several types of atmospheric dispersion models. In this work, which involves dispersions in long spatial ranges over complex topography and high meteorological variability, we use the so-called \u201cLagrangian\u201d models. These models simulate the emission of pollutants by a series of virtual \u201cpuffs\u201d or particles that are transported by the wind field and diffused by the atmospheric turbulence. Air concentration\nof pollutants and deposition on the ground are computed on a grid defined independently of the meteorological data grid.\nIn our experiments we make use of the widely used NOAA HYSPLIT atmospheric dispersion model[7, 21], which works well with data produced by WRF. Other dispersion models, such as the DIPCOT model[1, 25], can also be used. A visual of a 2D dispersion as used by our application is shown in Fig. 1."}, {"heading": "3.3 Weather Patterns and Cluster Descriptors for Plume Dispersion", "text": "k-means clustering: Data clustering[15, 3] represents a broad field dealing with the unsupervised discovery of groupings in multidimensional data based on their statistical similarity. A multitude of clustering algorithms has been proposed in the literature, many of which being used in operational settings. In this work we derive weather patterns via k-means clustering. The calculation of these patterns and their associated atmospheric dispersions for a number of nuclear facilities of interest are used for subsequent origin estimation.\nk-means, essentially an expectation-maximization (E-M) type of algorithm, is one of the most widely-used algorithms for data clustering. The algorithm is initialized by k typically random cluster centroids {\u00b5i} and its goal is to assign data points to a set of k corresponding sets C = {ci} so as to optimize an objective function; During the expectation step each data point is assigned to the closest cluster based on its distance from the clusters\u2019 centroids. During the maximization step the centroids {\u00b5i} get updated to reflect the new assignments. The algorithm stops when either there are no changes in the cluster assignments, or when a predefined threshold on the acceptable number of iterations has been reached. k-means is guaranteed to eventually converge, however not necessarily towards the global minimum.\nAutoencoders for dimensionality reduction: Working with long sequences of dense high-dimensional vectors can be impractical. More importantly, weather exhibits spatiotemporal patterns which may not be immediately exploitable by dealing with the raw data. We perform dimensionality reduction on the raw weather data using autoencoders in order to obtain reduced, statistically robust summaries. We then derive weather patterns by feeding the encoded versions of weather snapshots to k-means.\nSeveral dimensionality reduction techniques can be applied on weather data, such as PCA. We base our choice to use autoencoders on two premises (1) a shallow, linear activation auto-encoder is equivalent to PCA therefore covering our evaluation requirements and (2) given enough training, stacked autoencoders may lead to more robust representations spanning multiple variables and therefore lead to better quality clustering. In Section 4 we evaluate different configurations of autoencoders and discuss their effectiveness further.\nCluster descriptors: The clusters derived are hypothesized to consist of weather snapshots representative of specific weather patterns observed in Europe and are described by cluster descriptors or summaries. A clustering outcome C is a set of clusters {ci \u2208 C|ci = [wt \u2208 Rxyzm]}, where wt is a value at time t. Variables x, y and z indicate the geographical location and geopotential height respectively, and m denotes the physical variable, e.g. the geopotential, wind velocity, etc. By default k-means uses centroids as descriptors. Even though this is sensible for fitting additional weather snapshots to the model, it is not useful for producing synthetic atmospheric dispersions as it does not contain any temporal information. Taking into account errors introduced by unsupervised clustering as well as the dispersion models requiring time information, cluster descriptors should achieve two goals: they should reflect dominant weather conditions within the clusters and lend themselves to temporal and physical interpretation.\nIn this work we derive cluster descriptors, which can be used by dispersion models as sequences of representative weather snapshots, following two distinct approaches:\n1. The first approach is to derive these snapshots by a second application of k-means within each cluster, taking k to be the number of desired snapshots (in this case k = 13 \u2013 since we work on 3-day intervals we take 12 6-hourly snapshots plus 1 snapshot to inform the dispersion model\nof the end time of the dispersion run). The centroids of the resulting clusters form the original cluster\u2019s descriptor. This approach should be able to capture all weather trends within each cluster effectively, however it does not retail meaningful temporal information. We refer to this cluster descriptor approach as km2 \u2013 the k-means of the k-means result.\n2. A second approach is based on temporal densities within each cluster. After collapsing the weather snapshots of each cluster from multiple years where they typically originate from into a single-year period, we calculate its temporal densities. (The collapsing into a single-year period is to enforce time continuity and therefore improve the subsequent calculation of the dispersion pattern.) We then choose the weather snapshot corresponding to the time maximizing density and select a temporally continuous period around it for as many items as we require the descriptor to have. If the cluster has more than one snapshots at this offset from the beginning of the year (originally coming from multiple years) the mean weather is taken into account. If the cluster lacks continuous snapshots we use copies of neighbouring ones. Whereas cluster descriptors created by this approach have a sound temporal and physical interpretation, they fail to capture all characteristics of poorly formed clusters. We refer to these descriptors as density-based or dense.\nAfter weather snapshots have been clustered and weather patterns have emerged, we simulate hypothetical releases from 20 European nuclear facilities. Using the HYSPLIT atmospheric dispersion model[7, 21] we calculate hypothetical dispersions of radioactive material into the atmosphere for each of these locations. For the purposes of this application we assume that the emitted particles have a fixed atomic weight. These atmospheric dispersions have a finer geospatial resolution and form the basis for origin estimation."}, {"heading": "3.4 Source Estimation", "text": "To estimate the release origin we follow a two-step matching procedure. After an event has taken place we first identify the weather pattern that best represents the current weather. For this calculation several similarity (or distance) metrics can be used. Treating this matching as an additional k-means assignment step, we choose the cluster which minimizes the Euclidean distance between the observed weather vector w and the centroid of the cluster in question.\nBased on our choice of a representative weather pattern we then consider the dispersions previously calculated for our set of fixed candidate release origins. The likelihood of a location being the release origin can be calculated by comparing the release distributions of the cluster-based dispersions against hypothetical detection readings.\nA hypothetical event is detected by a sequence of readings at certain locations. We model these point readings as a discrete probability distribution across the geographical area of study r(x, y) : R (after dividing each reading by their total sum). This evaluates to a reading at all locations where radiation has been detected and to 0 at all other locations. The readings distribution is compared against a number of dispersion distributions given a weather pattern and a point of origin.\nSince r(x, y) is typically sparse we pass it through an isotropic 2D Gaussian smoothing filter obtaining R = rg(x, y). Based on the assumption that cells neighbouring readings are likely to also be contaminated, albeit with a decreasing probability, applying such a filter increases the number of cells with non-zero radiation values and therefore the likelihood to obtain meaningful positive matches between readings and dispersions.\nFinally, we take the inverse order of the dispersions corresponding to the weather pattern in question D according to their distance from the detection distribution R. Due to their sparsity, we compare the readings and dispersion vectors using the cosine distance metric."}, {"heading": "4 Evaluation and Discussion", "text": "The autoencoders of our cluster-based system were trained using 6-hourly weather samples spanning 11 years (1986-1993 and 1996-19984). Weather data used for training and evaluation were represented by a 64\u00d764 grid. In order to obtain more samples to train with, we introduced \u00b110% element-wise random noise to the weather data and increased the number of samples five-fold. This resulted in a total of 96,432 training samples.\nOur cluster-based system was evaluated using as a basis 3-day dispersions spanning 2 years of European weather (1994-1995), for a choice of 20 nuclear facilities, resulting in 4,480 samples. We increased the evaluation sample size five-fold by introducing the same type of element-wise random noise, as with the weather samples.\nFor creating training and evaluation datasets we approximated radiation readings by randomly choosing 10 and 30 geographical points \u2013 identifiable by (latitude, longitude) pairs \u2013 from each sample\u2019s dispersion pattern. This simulates receiving readings from arbitrary sensors on the ground and assesses how accuracy changes with the amount of available information. In a real situation it is expected that readings are not randomly scattered across the geographical area affected by a plume. The choice for this training and evaluation strategy was deliberate, as it makes no assumptions regarding the actual locations of radiation detectors5, while it is more readily generalizable to different use-cases, such as chemical and other releases from unknown potential origins.\nThe quality of the clustering is crucial to the performance of this application as a whole, which in turn depends on the quality of the latent features exposed by the autoencoder. In this paper we study denoising autoencoders of different depths and complexities, shown in Fig. 4 \u2013 the shallow autoencoder approximates PCA[4], while deeper configurations are expected to lead to more performant clustering solutions. Further fine-tuning the networks in order to improve the clustering outcome, e.g. by using\n4This split in time is a result of incrementally upscaling our experiment with additional years. The dimensionality reduction algorithms evaluated do not take temporal continuity into account, as they operate on discrete weather snapshots.\n5European countries are equipped with stationary networks of radiation detectors.\nthe DEC algorithm[28], should further increase performance, however it is left as future work. Cluster analysis on both raw and encoded weather data was inconclusive as to the optimal number of clusters. Based on cluster consistency scores and subjective expert opinions regarding observed weather patterns in Europe we set k=15.\nWith the clustering algorithm and the choice of atmospheric variables remaining the same the accuracy of our system depends on the following parameters: (1) the dimensionality reduction approach applied \u2013 in this case on the configuration of the autoencoder; (2) the choice of the distance (or similarity) metric between synthetic dispersions and radiation readings; (3) the choice of cluster descriptors for deriving synthetic dispersions; and (4) the number of available readings.\nTables 1 and 2 summarize results obtained for different choices of dimensionality reduction algorithms and cluster descriptors for 10 and 30 readings respectively. We have experimented both with the cosine and the correlation distances between vectors of readings and synthetic dispersions, with the former being consistently better. Euclidean-based distances are not performant due to the sparsity of the vectors involved. Here, we report results obtained via the cosine distance metric.\nAccording to the tables of results, we observe that the km2 descriptor performs best with the shallow autoencoder (Shallow A/E). This can be attributed to the fact that the km2 descriptor effectively counters cluster inconsistencies averaging over often meaningful sub-clusters. However, the sequence in which the averages are produced is indeterminable, yet time is meaningful to the dispersion model. For the multi-channel convolutional autoencoder density-based descriptors work best. Because Deep MC Conv A/E leads to better clustering outcomes, temporal density better represents weather clusters.\nThis observation is also supported by the difference in accuracy between the two choices of cluster descriptors \u2013 Fig. 5. Here, for the most performant configuration (4, Deep MC Conv A/E) we observe a significant and intuitive ranking in accuracy difference, with the difference increasing with the accuracy threshold (1 to 3) and the number of readings (10 to 30). This suggests that the use of multiple geopotential heights leads to more robust data representations favoring the density-based descriptor.\nSummarizing, although dimensionality reduction appears to benefit the application, and with the exception of Deep MC Conv A/E, the depth and type of the autoencoder does not appear to be making any notable or predictable difference on the final accuracy scores. This can be attributed to clustering errors propagated through the workflow to the final accuracy scores caused by the inherent difficulty\nto cluster weather, the choice of physical variables and their relationship to the dispersion as well as to the low resolution of the original data.\nHaving considered the above, in this application domain the timeliness of results is paramount. This, combined with visual aids (maps and dispersion patterns), user expertise in atmospheric physics and a top-3 accuracy of approximately 75% with increasing trends with the number of readings, constitutes an arguably useful system."}, {"heading": "4.1 Origin Estimation as a Classification Task", "text": "As a supplement to this study, we experimented with deep convolutional networks[16] treating the estimation of release origin as a classification problem. As this was intended as a preliminary study, we made use of the data produced as an evaluation dataset for the cluster-based approach, covering two years of weather: 1994 and 1995. For these two years we had previously calculated dispersions for 20 nuclear facilities in Europe and for 3-day periods. This resulted in 4480 samples which include weather and their corresponding dispersion patterns. We increased our weather and dispersion samples ten-fold. The weather dataset was increased by introducing random noise of \u00b110% on GHT values, while its dispersion counterpart was increased by randomly sampling 30 points from each dispersion pattern and applying a Gaussian filter, similarly to the evaluation dataset used in the cluster-based evaluation. This resulted in a total of 49,280 samples. For evaluation, we used a separate set of 22,400 samples created from the same time period applying the same procedure.\nWe trained our networks to classify weather and reading patterns into one of the 20 nuclear facilities. All networks exhibit two consecutive convolution layers per input type (weather and readings/dispersion pattern) followed by maxpool layers. They are then reshaped and concatenated before they are jointly fed into a series of four fully connected layers with the last one being a softmax. The simplest network uses a single size of filters (4\u00d74 for the 64\u00d764 weather data, and 10\u00d710 for the 167\u00d7167 dispersion data \u2013 dispersion data are resized to reduce the network\u2019s memory footprint). The other two networks use two parallel convolutional threads for each input type, each with different filter sizes (1\u00d71 \u2013 4\u00d74 in parallel to 1\u00d71 \u2013 6\u00d76 for weather data and 1\u00d71 \u2013 10\u00d710 in parallel to 1\u00d71 \u2013 16\u00d716 for readings/dispersion data). The purpose of this choice was to capture spatial features at different scales. The inclusion of 1\u00d71 convolutions before larger ones was inspired by the Inception configuration[23]. The more complex one catered for three channels of GHT at 500, 700 and 900hPa.\nAs shown in Table 3, all three classification networks outperformed the cluster-based approach significantly. Even though the evaluation task was arguably easier than the cluster-based one \u2013 the time period was smaller and both training and evaluation sets came from the same original dataset \u2013 the difference in performance is large enough that it warrants further investigation. A notable downside to treating this problem as a classification task is that it does not yield useful and reusable data byproducts, such as weather patterns and synthetic dispersions.\nThe source code for the experiments and evaluation described in this section can be found at https://github.com/davidath/ncsr-atmo-learn."}, {"heading": "5 Application Prototype", "text": "Enabling decision-making in the context of high-impact events, apart from the effectiveness of the designed algorithms, depends on the visualization functionality offered, the comprehensibility of data and results and often the application responsiveness. We address the requirements of decision makers by implementing a user-friendly and efficient tool that offers visualization of the available information combining clustering byproducts and dispersions6. The application prototype exploits technology integrated by the Big Data Europe integrator platform (BDI)7. The user interface makes use of the Sextant platform for visualising and exploring linked geospatial data8.\nThe application prototype enables decision makers by simulating hypothetical scenarios, while, if integrated with a service providing the current weather it can also be used during real events. An event is instantiated by the presence of radioactivity at certain geographical locations. By entering the marking mode, users can mark the location of readings. The marking mode allows the user to freely place the detection reading by hand (longitude and latitude of the mouse pointer is always shown to the user). This mode offers the ability to decision makers to mimic the location of detection stations\n6Prototype source code: https://github.com/iaklampanos/bde-pilot-2 7https://www.big-data-europe.eu/platform/ 8http://sextant.di.uoa.gr\nas well as the location of portable radiation detection devices. To estimate the origin\u2019s location the application needs to know, apart from the detection readings, the state of the current weather. The current weather is visualised as arrows by combining the west-east and north-south wind magnitudes. The wind direction can be currently displayed for 3 different pressure levels (500, 700 and 900hPa).\nThe concentration of radioactivity based on the clustering descriptors has been precomputed for two radionuclides typically emitted during nuclear accidents: Caesium-137 (Cs-137) and Iodine-131 (I-131). The selection of pollutants to be simulated as well as the source estimation method (various clustering methods) are part of the prototype\u2019s configuration parameters and is achieved via the control panel. The control panel is a key component of the user interface and user experience. Allowing the users to experiment using different configurations may lead to a plethora of results useful in different conditions. A video demonstration of the prototype has been posted at https://vimeo.com/227245883, also including extensions, such as the drawing and selection of known fixed radioactivity detectors and the depiction of inhabited locations below the estimated plume.\nThe overall usefulness and the quality of user experience are amplified by fast response times. Responsiveness is primarily a result of the methodology adopted, combined with its implementation using efficient big data technologies. Efficient and reliable access to neural network models and weather data as well as caching for visualisation are provided by the BDI platform."}, {"heading": "6 Conclusions and Future Work", "text": "In this work we presented a novel machine-learning-driven, cluster-based approach for rapid origin estimation during nuclear incidents. Our approach is inspired by image processing, deep learning and data mining, extending useful techniques to weather and atmospheric dispersion data. Incentivized by the need to create ML-ready reusable data products to enable the creation of sophisticated applications, we discussed the derivation of usable weather patterns via clustering. We employed deep autoencoders to derive robust latent features on which to apply clustering. By thoroughly evaluating our application we provide evidence of effectiveness and usefulness in the area of nuclear emergency response and in bordering areas making use of complex physics models and big geospatial data.\nDirections for future research and development may include the following: (1) evaluation of alternative classification approaches that learn origins, similar to the ones presented here, but also dispersion patterns (2) the formulation of novel clustering algorithms that take into account features such as periodicity and temporal density for cluster formation; (3) the inclusion of additional atmospheric dispersion models and learning model ensembles; (4) evaluation of alternative learning algorithms, such as recurrent neural networks (RNNs), to explicitly account for the temporal component of such analyses; (5) extension of the application to include all nuclear facilities in Europe and further to consider all possible source locations (i.e. all grid cells as potential sources), to be applicable in all radiological incidents; and (6) the evolution of the application to an operational system that learns continuously with new meteorological data."}, {"heading": "7 Acknowledgements", "text": "This work has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 644564. For more details, please visit https://www. big-data-europe.eu"}], "references": [{"title": "RODOS meteorological pre-processor and atmospheric dispersion model DIPCOT: A model suite for radionuclides dispersion in complex terrain", "author": ["S. Andronopoulos", "E. Davakis", "J. Bartzis", "I. Kovalets"], "venue": "Radioprotection, 45(5):S77\u2013S84", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Deep Architectures for AI, volume", "author": ["Y. Bengio"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Pattern Recognition and Machine Learning, volume", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological Cybernetics, 59(4-5):291\u2013294", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1988}, {"title": "Using self-organizing maps to investigate extreme climate events: An application to wintertime precipitation in the Balkans", "author": ["T. Cavazos"], "venue": "Journal of Climate, 13(10):1718\u20131732", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "The ERA-Interim reanalysis: configuration and performance of the data assimilation system", "author": ["D.P. Dee", "S.M. Uppala", "A.J. Simmons", "P. Berrisford", "P. Poli", "S. Kobayashi", "U. Andrae", "M.A. Balmaseda", "G. Balsamo", "P. Bauer", "P. Bechtold", "A.C.M. Beljaars", "L. van de Berg", "J. Bidlot", "N. Bormann", "C. Delsol", "R. Dragani", "M. Fuentes", "A.J. Geer", "L. Haimberger", "S.B. Healy", "H. Hersbach", "E.V. H\u00f3lm", "L. Isaksen", "P. K\u00e5llberg", "M. K\u00f6hler", "M. Matricardi", "A.P. McNally", "B.M. Monge-Sanz", "J.-J. Morcrette", "B.-K. Park", "C. Peubey", "P. de Rosnay", "C. Tavolato", "J.-N. Th\u00e9paut", "F. Vitart"], "venue": "Quarterly Journal of the Royal Meteorological Society,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "HYSPLIT4 user\u2019s guide", "author": ["R. Draxler"], "venue": "NOAA Tech. Memo. ERL ARL-230, (June):NOAA Air Resources Laboratory, Silver Spring, MD", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": "MIT Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Knowledge transfer in deep convolutional neural nets", "author": ["S. Gutstein", "O. Fuentes", "E. Freudenthal"], "venue": "International Journal on Artificial Intelligence Tools, 17(3):555\u2013567", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Self-organizing maps: Applications to synoptic climatology", "author": ["B.C. Hewitson", "R.G. Crane"], "venue": "Climate Research, 22(1):13\u201326", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Weather pattern classification to represent the urban heat island in present and future climate", "author": ["P. Hoffmann", "K. Heinke Schl\u00fcnzen"], "venue": "Journal of Applied Meteorology and Climatology, 52(12):2699\u20132714", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Classification of weather patterns to study the influence of meteorological characteristics on PM2.5 concentrations in Yunlin County, Taiwan", "author": ["C.-H. Hsu", "F.-Y. Cheng"], "venue": "Atmospheric Environment,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "An Intercomparison of Computer-assisted Circulation Classification Methods", "author": ["R. Huth"], "venue": "International Journal of Climatology, 16(8):893\u2013922", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Classifications of Atmospheric Circulation Patterns", "author": ["R. Huth", "C. Beck", "A. Philipp", "M. Demuzere", "Z. Ustrnul", "M. Cahynov\u00e1", "J. Kysel\u00fd", "O.E. Tveito"], "venue": "Annals of the New York Academy of Sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Data clustering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Surveys, 31(3):264\u2013323", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1999}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 86(11):2278\u20132323", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Stacked convolutional auto-encoders for hierarchical feature extraction", "author": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "The weather research and forecast model: Software architecture and performance", "author": ["J. Michalakes", "J. Dudhia", "D. Gill", "T. Henderson", "J. Klemp", "W. Skamarock", "W. Wang"], "venue": "Proceedings of the 11th ECMWF Workshop on the Use of High Performance Computing In Meteorology, volume 25, pages 156\u2013168", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Evaluation of the radiological situation in algeria after the algeciras incident", "author": ["A. Noureddine", "A. Hammadi", "R. Boudjenoun", "M. Menacer", "A. Allalou", "M. Benkrid", "M. Maache"], "venue": "Mediterranean Marine Science, 4(2):59\u201363", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "and F", "author": ["A.F. Stein", "R.R. Draxler", "G.D. Rolph", "B.J.B. Stunder", "M.D. Cohen"], "venue": "Ngan. Noaa\u2019s hysplit atmospheric transport and dispersion modeling system", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Circulation regimes: Chaotic variability versus SST-forced predictability", "author": ["D.M. Straus", "S. Corti", "F. Molteni"], "venue": "Journal of Climate, 20(10):2251\u20132272", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A new classification approach for detecting severe weather patterns", "author": ["G.R. Teixeira de Lima", "S. Stephany"], "venue": "Computers & Geosciences,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Assimilation of gamma dose rate and concentration measurements in Lagrangian model DIPCOT", "author": ["V. Tsiouri", "I. Kovalets", "S. Andronopoulos", "J. Bartzis"], "venue": "HARMO 2010 - Proceedings of the 13th International Conference on Harmonisation within Atmospheric Dispersion Modelling for Regulatory Purposes, pages 896\u2013899", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion Pierre-Antoine Manzagol", "author": ["P. Vincent", "H. Larochelle"], "venue": "Journal of Machine Learning Research, 11:3371\u20133408", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning - ICML \u201908, pages 1096\u20131103", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "and A", "author": ["J. Xie", "R. Girshick"], "venue": "Farhadi. Unsupervised deep embedding for clustering analysis", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Developments and prospects in synoptic climatology", "author": ["B. Yarnal", "A.C. Comrie", "B. Frakes", "D.P. Brown"], "venue": "International Journal of Climatology, 21(15):1923\u20131950", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 19, "context": "The Acerinox (or Algeciras) incident is another example of an unknown radioactive release that was traced back after radioactivity levels higher than the background had been observed at very long distances from the release location[20].", "startOffset": 231, "endOffset": 235}, {"referenceID": 6, "context": "HYSPLIT[7, 21]) are complex pieces of software that require expert training and case-by-case application.", "startOffset": 7, "endOffset": 14}, {"referenceID": 20, "context": "HYSPLIT[7, 21]) are complex pieces of software that require expert training and case-by-case application.", "startOffset": 7, "endOffset": 14}, {"referenceID": 13, "context": "Over the last decades there have been several studies attempting to automatically discover weather patterns via unsupervised hierarchical and iterative clustering[14], as part of synoptic climatology[29].", "startOffset": 162, "endOffset": 166}, {"referenceID": 28, "context": "Over the last decades there have been several studies attempting to automatically discover weather patterns via unsupervised hierarchical and iterative clustering[14], as part of synoptic climatology[29].", "startOffset": 199, "endOffset": 203}, {"referenceID": 12, "context": ") Indicatively, Huth[13] summarizes and compares correlation, sums-of-squares, agglomerative hierarchical, PCA, k-means and hierarchical agglomerative clustering algorithms.", "startOffset": 20, "endOffset": 24}, {"referenceID": 4, "context": "Methods based on more recent advances in neural networks, employing self-organizing maps, have also been reported[5, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 9, "context": "Methods based on more recent advances in neural networks, employing self-organizing maps, have also been reported[5, 10].", "startOffset": 113, "endOffset": 120}, {"referenceID": 23, "context": "Teixeira de Lima and Stephany[24] evaluate a multitude of weather variables for extreme weather classification.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "[22] use a modified version of k-means clustering for the pressure level of 200hPa and zonal winds to discover and analyze winter circulation regimes over the Pacific-North American region.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Hsu and Cheng[12] evaluate daily average surface wind measurements to discover weather patterns affecting air pollution in western Taiwan.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "Focusing on the urban heat island, the phenomenon in which an urban area is significantly warmer than its surrounding rural areas due to human activity, Hoffmann and Schl\u00fcnzen[11] study k-means on a number of variables such as GHT, relative humidity, vorticity, and others.", "startOffset": 175, "endOffset": 179}, {"referenceID": 15, "context": "An autoencoder[16, 2, 8] is an unsupervised feed-forward neural network designed to approximate the identity function, i.", "startOffset": 14, "endOffset": 24}, {"referenceID": 1, "context": "An autoencoder[16, 2, 8] is an unsupervised feed-forward neural network designed to approximate the identity function, i.", "startOffset": 14, "endOffset": 24}, {"referenceID": 7, "context": "An autoencoder[16, 2, 8] is an unsupervised feed-forward neural network designed to approximate the identity function, i.", "startOffset": 14, "endOffset": 24}, {"referenceID": 3, "context": "In the case of a simple, single-layer auto-encoder, the hidden activations are equivalent to PCA[4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 26, "context": "having fewer hidden than input units, by enforcing activation sparsity or by introducing noise which the auto-encoder learns to compensate for[27, 26].", "startOffset": 142, "endOffset": 150}, {"referenceID": 25, "context": "having fewer hidden than input units, by enforcing activation sparsity or by introducing noise which the auto-encoder learns to compensate for[27, 26].", "startOffset": 142, "endOffset": 150}, {"referenceID": 16, "context": "Convnets[17, 8] are designed to discover multi-dimensional patterns of varying sizes and have been applied, stand-alone or as part of more complex configurations, primarily in classification-based image recognition.", "startOffset": 8, "endOffset": 15}, {"referenceID": 7, "context": "Convnets[17, 8] are designed to discover multi-dimensional patterns of varying sizes and have been applied, stand-alone or as part of more complex configurations, primarily in classification-based image recognition.", "startOffset": 8, "endOffset": 15}, {"referenceID": 8, "context": "[9]) that convnets allow for knowledge to be transferred from one-another, which is useful for continuous, parallel learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Convolutional autoencoders are formed by stacking convolutional layers, fully connected layers and de-convolutional layers in a single configuration and are designed to capture feature hierarchies in the input space[18].", "startOffset": 215, "endOffset": 219}, {"referenceID": 17, "context": "In this study we evaluate simple, stacked and convolutional denoising autoencoders[18] in an attempt to reach smaller and statistically robust representations of weather variables.", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "This dataset is freely available online as part of the ERA-Interim project[6] and is provided by the European Centre for Medium range Weather Forecast (ECMWF)2.", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "GHT has been shown to be predictive of weather circulation patterns[13, 14].", "startOffset": 67, "endOffset": 75}, {"referenceID": 13, "context": "GHT has been shown to be predictive of weather circulation patterns[13, 14].", "startOffset": 67, "endOffset": 75}, {"referenceID": 18, "context": "Raw weather reanalysis data is downscaled using the Weather Research and Forecasting (WRF) modelling system[19] as a pre-processing step and to make the data compatible with the atmospheric dispersion model.", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "In our experiments we make use of the widely used NOAA HYSPLIT atmospheric dispersion model[7, 21], which works well with data produced by WRF.", "startOffset": 91, "endOffset": 98}, {"referenceID": 20, "context": "In our experiments we make use of the widely used NOAA HYSPLIT atmospheric dispersion model[7, 21], which works well with data produced by WRF.", "startOffset": 91, "endOffset": 98}, {"referenceID": 0, "context": "Other dispersion models, such as the DIPCOT model[1, 25], can also be used.", "startOffset": 49, "endOffset": 56}, {"referenceID": 24, "context": "Other dispersion models, such as the DIPCOT model[1, 25], can also be used.", "startOffset": 49, "endOffset": 56}, {"referenceID": 14, "context": "k-means clustering: Data clustering[15, 3] represents a broad field dealing with the unsupervised discovery of groupings in multidimensional data based on their statistical similarity.", "startOffset": 35, "endOffset": 42}, {"referenceID": 2, "context": "k-means clustering: Data clustering[15, 3] represents a broad field dealing with the unsupervised discovery of groupings in multidimensional data based on their statistical similarity.", "startOffset": 35, "endOffset": 42}, {"referenceID": 6, "context": "Using the HYSPLIT atmospheric dispersion model[7, 21] we calculate hypothetical dispersions of radioactive material into the atmosphere for each of these locations.", "startOffset": 46, "endOffset": 53}, {"referenceID": 20, "context": "Using the HYSPLIT atmospheric dispersion model[7, 21] we calculate hypothetical dispersions of radioactive material into the atmosphere for each of these locations.", "startOffset": 46, "endOffset": 53}, {"referenceID": 3, "context": "4 \u2013 the shallow autoencoder approximates PCA[4], while deeper configurations are expected to lead to more performant clustering solutions.", "startOffset": 44, "endOffset": 47}, {"referenceID": 27, "context": "the DEC algorithm[28], should further increase performance, however it is left as future work.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "As a supplement to this study, we experimented with deep convolutional networks[16] treating the estimation of release origin as a classification problem.", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "The inclusion of 1\u00d71 convolutions before larger ones was inspired by the Inception configuration[23].", "startOffset": 96, "endOffset": 100}], "year": 2017, "abstractText": "Emergency response applications for nuclear or radiological events can be significantly improved via deep feature learning due to the hidden complexity of the data and models involved. In this paper we present a novel methodology for rapid source estimation during radiological releases based on deep feature extraction and weather clustering. Atmospheric dispersions are then calculated based on identified predominant weather patterns and are matched against simulated incidents indicated by radiation readings on the ground. We evaluate the accuracy of our methods over multiple years of weather reanalysis data in the European region. We juxtapose these results with deep classification convolution networks and discuss advantages and disadvantages.", "creator": "LaTeX with hyperref package"}}}