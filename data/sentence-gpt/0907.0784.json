{"id": "0907.0784", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jul-2009", "title": "Cross-Task Knowledge-Constrained Self Training", "abstract": "We present an algorithmic framework for learning multiple related tasks. Our framework exploits a form of prior knowledge that relates the output spaces of these tasks. We present PAC learning results that analyze the conditions under which such learning is possible for those tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Sat, 4 Jul 2009 18:42:01 GMT  (62kb,D)", "http://arxiv.org/abs/0907.0784v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["hal daum\u00e9 iii"], "accepted": true, "id": "0907.0784"}, "pdf": {"name": "0907.0784.pdf", "metadata": {"source": "CRF", "title": "Cross-Task Knowledge-Constrained Self Training", "authors": ["Hal Daum\u00e9 III"], "emails": ["me@hal3.name"], "sections": [{"heading": "1 Introduction", "text": "When two NLP systems are run on the same data, we expect certain constraints to hold between their outputs. This is a form of prior knowledge. We propose a self-training framework that uses such information to significantly boost the performance of one of the systems. The key idea is to perform self-training only on outputs that obey the constraints.\nOur motivating example in this paper is the task pair: named entity recognition (NER) and shallow parsing (aka syntactic chunking). Consider a hidden sentence with known POS and syntactic structure below. Further consider four potential NER sequences for this sentence.\nPOS: NNP NNP VBD TO NNP NN Chunk: [- NP -][-VP-][-PP-][-NP-][-NP-] NER1: [- Per -][- O -][-Org-][- 0 -] NER2: [- Per -][- O -][- O -][- O -][- O -] NER3: [- Per -][- O -][- O -][- Org -] NER4: [- Per -][- O -][- O -][-Org-][- O -]\nWithout ever seeing the actual sentence, can we guess which NER sequence is correct? NER1 seems\nwrong because we feel like named entities should not be part of verb phrases. NER2 seems wrong because there is an NNP1 (proper noun) that is not part of a named entity (word 5). NER3 is amiss because we feel it is unlikely that a single name should span more than one NP (last two words). NER4 has none of these problems and seems quite reasonable. In fact, for the hidden sentence, NER4 is correct2.\nThe remainder of this paper deals with the problem of formulating such prior knowledge into a workable system. There are similarities between our proposed model and both self-training and cotraining; background is given in Section 2. We present a formal model for our approach and perform a simple, yet informative, analysis (Section 3). This analysis allows us to define what good and bad constraints are. Throughout, we use a running example of NER using hidden Markov models to show the efficacy of the method and the relationship between the theory and the implementation. Finally, we present full-blown results on seven different NER data sets (one from CoNLL, six from ACE), comparing our method to several competitive baselines (Section 4). We see that for many of these data sets, less than one hundred labeled NER sentences are required to get state-of-the-art performance, using a discriminative sequence labeling algorithm (Daume\u0301 III and Marcu, 2005)."}, {"heading": "2 Background", "text": "Self-training works by learning a model on a small amount of labeled data. This model is then evalu-\n1When we refer to NNP, we also include NNPS. 2The sentence is: \u201cGeorge Bush spoke to Congress today\u201d\nar X\niv :0\n90 7.\n07 84\nv1 [\ncs .L\nG ]\n4 J\nul 2\nated on a large amount of unlabeled data. Its predictions are assumed to be correct, and it is retrained on the unlabeled data according to its own predictions. Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community. Its success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006). In some cases, self-training takes into account model confidence.\nCo-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident. The original, and simplest analysis of cotraining is due to Blum and Mitchell (1998). It does not take into account confidence (to do so requires a significantly more detailed analysis (Dasgupta et al., 2001)), but is useful for understanding the process."}, {"heading": "3 Model", "text": "We define a formal PAC-style (Valiant, 1994) model that we call the \u201chints model\u201d3. We have an instance space X and two output spaces Y1 and Y2. We assume two concept classes C1 and C2 for each output space respectively. Let D be a distribution over X , and f1 \u2208 C1 (resp., f2 \u2208 C2) be target functions. The goal, of course, is to use a finite sample of examples drawn from D (and labeled\u2014perhaps with noise\u2014 by f1 and f2) to \u201clearn\u201d h1 \u2208 C1 and h2 \u2208 C2, which are good approximations to f1 and f2.\nSo far we have not made use of any notion of constraints. Our expectation is that if we constrain h1 and h2 to agree (vis-a-vis the example in the Introduction), then we should need fewer labeled examples to learn either. (The agreement should \u201cshrink\u201d the size of the corresponding hypothesis spaces.) To formalize this, let \u03c7 : Y1 \u00d7 Y2 \u2192 {0, 1} be a con-\n3The name comes from thinking of our knowledge-based constraints as \u201chints\u201d to a learner as to what it should do.\nstraint function. We say that two outputs y1 \u2208 Y1 and y2 \u2208 Y2 are compatible if \u03c7(y1, y2) = 1. We need to assume that \u03c7 is correct: Definition 1. We say that \u03c7 is correct with respect to D, f1, f2 if whenever x has non-zero probability under D, then \u03c7(f1(x), f2(x)) = 1.\nRUNNING EXAMPLE\nIn our example, Y1 is the space of all POS/chunk sequences and Y2 is the space of all NER sequences. We assume that C1 and C2 are both represented by HMMs over the appropriate state spaces. The functions we are trying to learn are f1, the \u201ctrue\u201d POS/chunk labeler and f2, the \u201ctrue\u201d NER labeler. (Note that we assume f1 \u2208 C1, which is obviously not true for language.)\nOur constraint function \u03c7 will require the following for agreement: (1) any NNP must be part of a named entity; (2) any named entity must be a subsequence of a noun phrase. This is precisely the set of constraints discussed in the introduction.\nThe question is: given this additional source of knowledge (i.e., \u03c7), has the learning problem become easier? That is, can we learn f2 (and/or f1) using significantly fewer labeled examples than if we did not have \u03c7? Moreover, we have assumed that \u03c7 is correct, but is this enough? Intuitively, no: a function \u03c7 that returns 1 regardless of its inputs is clearly not useful. Given this, what other constraints must be placed on \u03c7. We address these questions in Sections 3.3. However, first we define our algorithm."}, {"heading": "3.1 One-sided Learning with Hints", "text": "We begin by considering a simplified version of the \u201clearning with hints\u201d problem. Suppose that all we care about is learning f2. We have a small amount of data labeled by f2 (call thisD) and a large amount of data labeled by f1 (call this Dunlab\u2013\u201dunlab\u201d because as far as f2 is concerned, it is unlabeled).\nRUNNING EXAMPLE\nIn our example, this means that we have a small amount of labeled NER data and a large amount of labeled POS/chunk data. We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences). We are only interested in learning to do NER. Details of the exact HMM setup are in Section 4.2.\nWe call the following algorithm \u201cOne-Sided Learning with Hints,\u201d since it aims only to learn f2:\n1: Learn h2 directly on D 2: For each example (x, y1) \u2208 Dunlab 3: Compute y2 = h2(x) 4: If \u03c7(y1, y2), add (x, y2) to D 5: Relearn h2 on the (augmented) D 6: Go to (2) if desired\nRUNNING EXAMPLE\nIn step 1, we train an NER HMM on CoNLL. On test data, this model achieves an F -score of 50.8. In step 2, we run this HMM on all the WSJ data, and extract 3145 compatible examples. In step 3, we retrain the HMM; the F -score rises to 58.9."}, {"heading": "3.2 Two-sided Learning with Hints", "text": "In the two-sided version, we assume that we have a small amount of data labeled by f1 (call this D1), a small amount of data labeled by f2 (call thisD2) and a large amount of unlabeled data (call this Dunlab). The algorithm we propose for learning hypotheses for both tasks is below:\n1: Learn h1 on D1 and h2 on D2. 2: For each example x \u2208 Dunlab: 3: Compute y1 = h1(x) and y2 = h2(x) 4: If \u03c7(y1, y2) add (x, y1) to D1, (x, y2) to D2 5: Relearn h1 on D1 and h2 on D2. 6: Go to (2) if desired\nRUNNING EXAMPLE\nWe use 3500 examples from NER and 1000 from WSJ. We use the remaining 18447 examples as unlabeled data. The baseline HMMs achieve F - scores of 50.8 and 76.3, respectively. In step 2, we add 7512 examples to each data set. After step 3, the new models achieve F -scores of 54.6 and 79.2, respectively. The gain for NER is lower than before as it is trained against \u201cnoisy\u201d syntactic labels."}, {"heading": "3.3 Analysis", "text": "Our goal is to prove that one-sided learning with hints \u201cworks.\u201d That is, if C2 is learnable from large amounts of labeled data, then it is also learnable from small amounts of labeled data and large amounts of f1-labeled data. This is formalized in Theorem 1 (all proofs are in Appendix A). However, before stating the theorem, we must define an\n\u201cinitial weakly-useful predictor\u201d (terminology from Blum and Mitchell(1998)), and the notion of noisy PAC-learning in the structured domain. Definition 2. We say that h is a weakly-useful predictor of f if for all y: PrD [h(x) = y] \u2265 and PrD [f(x) = y | h(x) = y\u2032 6= y] \u2265 PrD [f(x) = y] + .\nThis definition simply ensures that (1) h is nontrivial: it assigns some non-zero probability to every possible output; and (2) h is somewhat indicative of f . In practice, we use the hypothesis learned on the small amount of training data during step (1) of the algorithm as the weakly useful predictor. Definition 3. We say that C is PAC-learnable with noise (in the structured setting) if there exists an algorithm with the following properties. For any c \u2208 C, any distribution D over X , any 0 \u2264 \u03b7 \u2264 1/ |Y|, any 0 < < 1, any 0 < \u03b4 < 1 and any \u03b7 \u2264 \u03b70 < 1/ |Y|, if the algorithm is given access to examples drawn EX\u03b7SN (c,D) and inputs , \u03b4 and \u03b70, then with probability at least 1 \u2212 \u03b4, the algorithm returns a hypothesis h \u2208 C with error at most . Here, EX\u03b7SN (c,D) is a structured noise oracle, which draws examples from D, labels them by c and randomly replaces with another label with prob. \u03b7.\nNote here the rather weak notion of noise: entire structures are randomly changed, rather than individual labels. Furthermore, the error is 0/1 loss over the entire structure. Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss. While not stated directly in terms of PAC learnability, it is clear that his results apply. Taskar et al. (2005) establish tighter bounds for the case of Hamming loss. This suggests that the requirement of 0/1 loss is weaker.\nAs suggested before, it is not sufficient for \u03c7 to simply be correct (the constant 1 function is correct, but not useful). We need it to be discriminating, made precise in the following definition. Definition 4. We say the discrimination of \u03c7 for h0 is PrD[\u03c7(f1(x), h0(x))]\u22121.\nIn other words, a constraint function is discriminating when it is unlikely that our weakly-useful predictor h0 chooses an output that satisfies the constraint. This means that if we do find examples (in our unlabeled corpus) that satisfy the constraints, they are likely to be \u201cuseful\u201d to learning.\nRUNNING EXAMPLE\nIn the NER HMM, let h0 be the HMM obtained by training on the small labeled NER data set and f1 is the true syntactic labels. We approximate PrD by an empirical estimate over the unlabeled distribution. This gives a discrimination is 41.6 for the constraint function defined previously. However, if we compare against \u201cweaker\u201d constraint functions, we see the appropriate trend. The value for the constraint based only on POS tags is 39.1 (worse) and for the NP constraint alone is 27.0 (much worse).\nTheorem 1. Suppose C2 is PAC-learnable with noise in the structured setting, h02 is a weakly useful predictor of f2, and \u03c7 is correct with respect to D, f1, f2, h02, and has discrimination \u2265 2(|Y| \u2212 1). Then C2 is also PAC-learnable with one-sided hints.\nThe way to interpret this theorem is that it tells us that if the initial h2 we learn in step 1 of the onesided algorithm is \u201cgood enough\u201d (in the sense that it is weakly-useful), then we can use it as specified by the remainder of the one-sided algorithm to obtain an arbitrarily good h2 (via iterating).\nThe dependence on |Y| is the discrimination bound for \u03c7 is unpleasant for structured problems. If we wish to find M unlabeled examples that satisfy the hints, we\u2019ll need a total of at least 2M(|Y| \u2212 1) total. This dependence can be improved as follows. Suppose that our structure is represented by a graph over vertices V , each of which can take a label from a set Y . Then, |Y| =\n\u2223\u2223Y V \u2223\u2223, and our result requires that \u03c7 be discriminating on an order exponential in V . Under the assumption that \u03c7 decomposes over the graph structure (true for our example) and that C2 is PAC-learnable with per-vertex noise, then the discrimination requirement drops to 2 |V | (|Y | \u2212 1).\nRUNNING EXAMPLE\nIn NER, |Y | = 9 and |V | \u2248 26. This means that the values from the previous example look not quite so bad. In the 0/1 loss case, they are compared to 1025; in the Hamming case, they are compared to only 416. The ability of the one-sided algorithm follows the same trends as the discrimination values. Recall the baseline performance is 50.8. With both constraints (and a discrimination value of 41.6), we obtain a score of 58.9. With just the POS constraint (discrimination of 39.1), we obtain a score of 58.1. With just the NP constraint (discrimination of 27.0, we obtain a score of 54.5.\nThe final question is how one-sided learning relates to two-sided learning. The following definition and easy corollary shows that they are related in the obvious manner, but depends on a notion of uncorrelation between h01 and h 0 2. Definition 5. We say that h1 and h2 are uncorrelated if PrD [h1(x) = y1 | h2(x) = y2, x] = PrD [h1(x) = y1 | x]. Corollary 1. Suppose C1 and C2 are both PAClearnable in the structured setting, h01 and h 0 2 are weakly useful predictors of f1 and f2, and \u03c7 is correct with respect to D, f1, f2, h01 and h02, and has discrimination \u2265 4(|Y| \u2212 1)2 (for 0/1 loss) or \u2265 4 |V |2 (|Y |\u22121)2 (for Hamming loss), and that h01 and h02 are uncorrelated. Then C1 and C2 are also PAC-learnable with two-sided hints.\nUnfortunately, Corollary 1 depends quadratically on the discrimination term, unlike Theorem 1."}, {"heading": "4 Experiments", "text": "In this section, we describe our experimental results. We have already discussed some of them in the context of the running example. In Section 4.1, we briefly describe the data sets we use. A full description of the HMM implementation and its results are in Section 4.2. Finally, in Section 4.3, we present results based on a competitive, discriminativelylearned sequence labeling algorithm. All results for NER and chunking are in terms of F-score; all results for POS tagging are accuracy."}, {"heading": "4.1 Data Sets", "text": "Our results are based on syntactic data drawn from the Penn Treebank (Marcus et al., 1993), specifically the portion used by CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). Our NER data is from two sources. The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). The ACE data constitute six separate data sets from six domains: weblogs (wl), newswire (nw), broadcast conversations (bc), United Nations (un), direct telephone speech (dts) and broadcast news (bn). Of these, bc, dts and bn are all speech data sets. All the examples from the previous sections have been limited to the CoNLL data."}, {"heading": "4.2 HMM Results", "text": "The experiments discussed in the preceding sections are based on a generative hidden Markov model for both the NER and syntactic chunking/POS tagging tasks. The HMMs constructed use first-order transitions and emissions. The emission vocabulary is pruned so that any word that appears\u2264 1 time in the training data is replaced by a unique *unknown* token. The transition and emission probabilities are smoothed with Dirichlet smoothing, \u03b1 = 0.001 (this was not-aggressively tuned by hand on one setting). The HMMs are implemented as finite state models in the Carmel toolkit (Graehl and Knight, 2002).\nThe various compatibility functions are also implemented as finite state models. We implement them as a transducer from POS/chunk labels to NER labels (though through the reverse operation, they can obviously be run in the opposite direction). The construction is with a single state with transitions: \u2022 (NNP,?) maps to B-* and I-* \u2022 (?,B-NP) maps to B-* and O \u2022 (?,I-NP) maps to B-*, I-* and O \u2022 Single exception: (NNP,x), where x is not an NP\ntag maps to anything (this is simply to avoid empty composition problems). This occurs in 100 of the 212k words in the Treebank data and more rarely in the automatically tagged data."}, {"heading": "4.3 One-sided Discriminative Learning", "text": "In this section, we describe the results of one-sided discriminative labeling with hints. We use the true syntactic labels from the Penn Treebank to derive the constraints (this is roughly 9000 sentences). We use the LaSO sequence labeling software (Daume\u0301 III and Marcu, 2005), with its built-in feature set.\nOur goal is to analyze two things: (1) what is the effect of the amount of labeled NER data? (2) what is the effect of the amount of labeled syntactic data from which the hints are constructed?\nTo answer the first question, we keep the amount of syntactic data fixed (at 8936 sentences) and vary the amount of NER data in N \u2208 {100, 200, 400, 800, 1600}. We compare models with and without the default gazetteer information from the LaSO software. We have the following models for comparison: \u2022 A default \u201cBaseline\u201d in which we simply train\nthe NER model without using syntax.\n\u2022 In \u201cPOS-feature\u201d, we do the same thing, but we first label the NER data using a tagger/chunker trained on the 8936 syntactic sentences. These labels are used as features for the baseline. \u2022 A \u201cSelf-training\u201d setting where we use the\n8936 syntactic sentences as \u201cunlabeled,\u201d label them with our model, and then train on the results. (This is equivalent to a hints model where \u03c7(\u00b7, \u00b7) = 1 is the constant 1 function.) We use model confidence as in Blum and Mitchell (1998).4\nThe results are shown in Figure 1. The trends we see are the following: \u2022 More data always helps. \u2022 Self-training usually helps over the baseline\n(though not always: for instance in wl and parts of cts and bn). \u2022 Adding the gazetteers help. \u2022 Adding the syntactic features helps. \u2022 Learning with hints, especially for \u2264 1000\ntraining data points, helps significantly, even over self-training.\nWe further compare the algorithms by looking at how many training setting has each as the winner. In particular, we compare both hints and self-training to the two baselines, and then compare hints to selftraining. If results are not significant at the 95% level (according to McNemar\u2019s test), we call it a tie. The results are in Table 1.\nIn our second set of experiments, we consider the role of the syntactic data. For this experiment, we hold the number of NER labeled sentences constant (at N = 200) and vary the amount of syntactic data in M \u2208 {500, 1000, 2000, 4000, 8936}. The results of these experiments are in Figure 2. The trends are:\n\u2022 The POS feature is relatively insensitive to the amount of syntactic data\u2014this is most likely because it\u2019s weight is discriminatively adjusted\n4Results without confidence were significantly worse.\n0.7 wl\n0.8 nw\n0.9 conll\n0.8 bc\nby LaSO so that if the syntactic information is bad, it is relatively ignored.\n\u2022 Self-training performance often degrades as the amount of syntactic data increases.\n\u2022 The performance of learning with hints increases steadily with more syntactic data.\nAs before, we compare performance between the different models, declaring a \u201ctie\u201d if the difference is not statistically significant at the 95% level. The results are in Table 2.\nIn experiments not reported here to save space, we experimented with several additional settings. In one, we weight the unlabeled data in various ways: (1) to make it equal-weight to the labeled data; (2) at 10% weight; (3) according to the score produced by the first round of labeling. None of these had a\nsignificant impact on scores; in a few cases performance went up by 1, in a few cases, performance went down about the same amount."}, {"heading": "4.4 Two-sided Discriminative Learning", "text": "In this section, we explore the use of two-sided discriminative learning to boost the performance of our syntactic chunking, part of speech tagging, and named-entity recognition software. We continue to use LaSO (Daume\u0301 III and Marcu, 2005) as the sequence labeling technique.\nThe results we present are based on attempting to improve the performance of a state-of-the-art system train on all of the training data. (This is in contrast to the results in Section 4.3, in which the effect of using limited amounts of data was explored.) For the POS tagging and syntactic chunking, we being with all 8936 sentences of training data from CoNLL. For the named entity recognition, we limit our presentation to results from the CoNLL 2003 NER shared task. For this data, we have roughly 14k sentences of training data, all of which are used. In both cases, we reserve 10% as development data. The development data is use to do early stopping in LaSO.\nAs unlabeled data, we use 1m sentences extracted from the North American National Corpus of En-\nwl\n0.8 nw\n0.9 conll\n0.9 bc\nglish (previously used for self-training of parsers (McClosky et al., 2006)). These 1m sentences were selected by dev-set relativization against the union of the two development data sets.\nFollowing similar ideas to those presented by Blum and Mitchell (1998), we employ two slight modifications to the algorithm presented in Section 3.2. First, in step (2b) instead of adding all allowable instances to the labeled data set, we only add the top R (for some hyper-parameter R), where \u201ctop\u201d is determined by average model confidence for the two tasks. Second, Instead of using the full unlabeled set to label at each iteration, we begin with a random subset of 10R unlabeled examples and another add random 10R every iteration.\nWe use the same baseline systems as in one-sided learning: a Baseline that learns the two tasks independently; a variant of the Baseline on which the output of the POS/chunker is used as a feature for the NER; a variant based on self-training; the hintsbased method. In all cases, we do use gazetteers. We run the hints-based model for 10 iterations. For selftraining, we use 10R unlabeled examples (so that it had access to the same amount of unlabeled data as the hints-based learning after all 10 iterations). We used three values of R: 50, 100, 500. We select the\nbest-performing model (by the dev data) over these ten iterations. The results are in Table 3.\nAs we can see, performance for syntactic chunking is relatively stagnant: there are no significant improvements for any of the methods over the baseline. This is not surprising: the form of the constraint function we use tells us a lot about the NER task, but relatively little about the syntactic chunking task. In particular, it tells us nothing about phrases other than NPs. On the other hand, for NER, we see that both self-training and learning with hints improve over the baseline. The improvements are not\nenormous, but are significant (at the 95% level, as measured by McNemar\u2019s test). Unfortunately, the improvements for learning with hints over the selftraining model are only significant at the 90% level."}, {"heading": "5 Discussion", "text": "We have presented a method for simultaneously learning two tasks using prior knowledge about the relationship between their outputs. This is related to joint inference (Daume\u0301 III et al., 2006). However, we do not require that that a single data set be labeled for multiple tasks. In all our examples, we use separate data sets for shallow parsing as for named-entity recognition. Although all our experiments used the LaSO framework for sequence labeling, there is noting in our method that assumes any particular learner; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc.\nOur approach, both algorithmically and theoretically, is most related to ideas in co-training (Blum and Mitchell, 1998). The key difference is that in co-training, one assumes that the two \u201cviews\u201d are on the inputs; here, we can think of the two output spaces as being the difference \u201cviews\u201d and the compatibility function \u03c7 being a method for reconciling these two views. Like the pioneering work of Blum and Mitchell, the algorithm we employ in practice makes use of incrementally augmenting the unlabeled data and using model confidence. Also like that work, we do not currently have a theoretical framework for this (more complex) model.5 It would also be interesting to explore soft hints, where the range of \u03c7 is [0, 1] rather than {0, 1}.\nRecently, Ganchev et al. (2008) proposed a coregularization framework for learning across multiple related tasks with different output spaces. Their approach hinges on a constrained EM framework and addresses a quite similar problem to that addressed by this paper. Chang et al. (2008) also propose a \u201csemisupervised\u201d learning approach quite similar to our own model. The show very promising results in the context of semantic role labeling.\n5Dasgupta et al. (2001) proved, three years later, that a formal model roughly equivalent to the actual Blum and Mitchell algorithm does have solid theoretical foundations.\nGiven the apparent (very!) recent interest in this problem, it would be ideal to directly compare the different approaches.\nIn addition to an analysis of the theoretical properties of the algorithm presented, the most compelling avenue for future work is to apply this framework to other task pairs. With a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (Marcu, 2000), parsing and word alignment, or summarization and information extraction."}, {"heading": "Acknowledgments", "text": "Many thanks to three anonymous reviewers of this papers whose suggestions greatly improved the work and the presentation. This work was partially funded by NSF grant IIS 0712764."}, {"heading": "A Proofs", "text": "The proof of Theorem 1 closes follows that of Blum and Mitchell (1998).\nProof (Theorem 1, sketch). Use the following notation: ck = PrD[h(x) = k], pl = PrD[f(x) = l], ql|k = PrD[f(x) = l | h(x) = k]. Denote by A the set of outputs that satisfy the constraints. We are interested in the probability that h(x) is erroneous, given that h(x) satisfies the constraints:\np (h(x) \u2208 A\\{l} | f(x) = l) = \u2211\nk\u2208A\\{l}\np (h(x) = k | f(x) = l) = \u2211\nk\u2208A\\{l}\nckql|k/pl\n\u2264 \u2211 k\u2208A ck(|Y| \u2212 1 + \u2211 l 6=k 1/pl) \u2264 2 \u2211 k\u2208A ck(|Y| \u2212 1)\nHere, the second step is Bayes\u2019 rule plus definitions, the third step is by the weak initial hypothesis assumption, and the last step is by algebra. Thus, in order to get a probability of error at most \u03b7, we need\u2211\nk\u2208A ck = Pr[h(x) \u2208 A] \u2264 \u03b7/(2(|Y| \u2212 1)).\nThe proof of Corollary 1 is straightforward.\nProof (Corollary 1, sketch). Write out the probability of error as a double sum over true labels y1, y2 and predicted labels y\u03021, y\u03022 subject to \u03c7(y\u03021, y\u03022). Use the uncorrelation assumption and Bayes\u2019 to split this into the product two terms as in the proof of Theorem 1. Bound as before."}], "references": [{"title": "Combining labeled and unlabeled data with cotraining", "author": ["Blum", "Mitchell1998] Avrim Blum", "Tom Mitchell"], "venue": "In Proceedings of the Conference on Computational Learning Theory (COLT),", "citeRegEx": "Blum et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blum et al\\.", "year": 1998}, {"title": "Learning and inference with constraints", "author": ["Chang et al.2008] Ming-Wei Chang", "Lev Ratinov", "Nicholas Rizzolo", "Dan Roth"], "venue": "In Proceedings of the National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Chang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2008}, {"title": "Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods", "author": ["Michael Collins"], "venue": "In International Workshop on Parsing Technologies (IWPT)", "citeRegEx": "Collins.,? \\Q2001\\E", "shortCiteRegEx": "Collins.", "year": 2001}, {"title": "PAC generalization bounds for co-training", "author": ["Michael Littman", "David McAllester"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Dasgupta et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2001}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["III Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Marcu. Daniel"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Multi-view learning over structured and non-identical outputs", "author": ["Joao Graca", "John Blitzer", "Ben Taskar"], "venue": "In Proceedings of the Converence on Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "Ganchev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2008}, {"title": "Carmel finite state transducer package. http://www.isi.edu/licensed-sw/ carmel", "author": ["Graehl", "Knight2002] Jonathan Graehl", "Kevin Knight"], "venue": null, "citeRegEx": "Graehl et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Graehl et al\\.", "year": 2002}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML)", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "The Theory and Practice of Discourse Parsing and Summarization", "author": ["Daniel Marcu"], "venue": null, "citeRegEx": "Marcu.,? \\Q2000\\E", "shortCiteRegEx": "Marcu.", "year": 2000}, {"title": "Building a large annotated corpus of English: The Penn Treebank", "author": ["Marcus et al.1993] Mitch Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "The use of classifiers in sequential inference", "author": ["Punyakanok", "Roth2001] Vasin Punyakanok", "Dan Roth"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Punyakanok et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Punyakanok et al\\.", "year": 2001}, {"title": "Text chunking using transformation-based learning", "author": ["Ramshaw", "Marcus1995] Lance A. Ramshaw", "Mitchell P. Marcus"], "venue": "In Proceedings of the Third ACL Workshop on Very Large Corpora", "citeRegEx": "Ramshaw et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ramshaw et al\\.", "year": 1995}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Taskar et al.2005] Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "Taskar et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Introduction to the CoNLL-2000 shared task: Chunking", "author": ["Tjong Kim Sang", "Sabine Buchholz"], "venue": "In Proceedings of the Conference on Natural Language Learning (CoNLL)", "citeRegEx": "Sang et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2000}, {"title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of Conference on Computational Natural Language Learning,", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Self-training for machine translation", "author": ["Nicola Ueffing"], "venue": "In NIPS workshop on Machine Learning for Multilingual Information Access", "citeRegEx": "Ueffing.,? \\Q2006\\E", "shortCiteRegEx": "Ueffing.", "year": 2006}, {"title": "A theory of the learnable", "author": ["Leslie G. Valiant"], "venue": "Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Valiant.,? \\Q1994\\E", "shortCiteRegEx": "Valiant.", "year": 1994}, {"title": "Unsupervised word sense disambiguation rivaling supervised methods", "author": ["David Yarowsky"], "venue": "In Proceedings of the Conference of the Association for Computational Linguistics (ACL)", "citeRegEx": "Yarowsky.,? \\Q1995\\E", "shortCiteRegEx": "Yarowsky.", "year": 1995}], "referenceMentions": [{"referenceID": 15, "context": ", 2006) to machine translation (Ueffing, 2006).", "startOffset": 31, "endOffset": 46}, {"referenceID": 17, "context": "Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions.", "startOffset": 12, "endOffset": 53}, {"referenceID": 3, "context": "It does not take into account confidence (to do so requires a significantly more detailed analysis (Dasgupta et al., 2001)), but is useful for understanding the process.", "startOffset": 99, "endOffset": 122}, {"referenceID": 16, "context": "Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident. The original, and simplest analysis of cotraining is due to Blum and Mitchell (1998). It does not take into account confidence (to do so requires a significantly more detailed analysis (Dasgupta et al.", "startOffset": 13, "endOffset": 706}, {"referenceID": 16, "context": "We define a formal PAC-style (Valiant, 1994) model that we call the \u201chints model\u201d3.", "startOffset": 29, "endOffset": 44}, {"referenceID": 9, "context": "We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences).", "startOffset": 116, "endOffset": 163}, {"referenceID": 2, "context": "Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss.", "startOffset": 0, "endOffset": 15}, {"referenceID": 2, "context": "Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss. While not stated directly in terms of PAC learnability, it is clear that his results apply. Taskar et al. (2005) establish tighter bounds for the case of Hamming loss.", "startOffset": 0, "endOffset": 212}, {"referenceID": 9, "context": "Our results are based on syntactic data drawn from the Penn Treebank (Marcus et al., 1993), specifically the portion used by CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000).", "startOffset": 69, "endOffset": 90}, {"referenceID": 7, "context": "Although all our experiments used the LaSO framework for sequence labeling, there is noting in our method that assumes any particular learner; alternatives include: conditional random fields (Lafferty et al., 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al.", "startOffset": 191, "endOffset": 214}, {"referenceID": 12, "context": ", 2001), independent predictors (Punyakanok and Roth, 2001), maxmargin Markov networks (Taskar et al., 2005), etc.", "startOffset": 87, "endOffset": 108}, {"referenceID": 4, "context": "Recently, Ganchev et al. (2008) proposed a coregularization framework for learning across multiple related tasks with different output spaces.", "startOffset": 10, "endOffset": 32}, {"referenceID": 1, "context": "Chang et al. (2008) also propose a \u201csemisupervised\u201d learning approach quite similar to our own model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 8, "context": "With a little thought, one can imagine formulating compatibility functions between tasks like discourse parsing and summarization (Marcu, 2000), parsing and word alignment, or summarization and information extraction.", "startOffset": 130, "endOffset": 143}], "year": 2013, "abstractText": "We present an algorithmic framework for learning multiple related tasks. Our framework exploits a form of prior knowledge that relates the output spaces of these tasks. We present PAC learning results that analyze the conditions under which such learning is possible. We present results on learning a shallow parser and named-entity recognition system that exploits our framework, showing consistent improvements over baseline methods.", "creator": "LaTeX with hyperref package"}}}