{"id": "0909.5457", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2009", "title": "Guaranteed Rank Minimization via Singular Value Projection", "abstract": "Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization with affine constraints (ARMP) and show that SVP recovers the minimum rank solution for affine constraints that satisfy the \"restricted isometry property\". Our results improve upon a recent breakthrough by Recht, Fazel and Parillo (RFP07) in three significant ways: SVP recovers the minimum rank solution for affine constraints that satisfy the \"restricted isometry property\".\n\n\n\n\nThe main feature of this paper is that SVP recovers the minimum rank solution for affine constraints that satisfy the \"restricted isometry property\". Here, we demonstrate that SVP recovers the minimum rank solution for affine constraints that satisfy the \"restricted isometry property\". We propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization with affine constraints (ARMP) and show that SVP recovers the minimum rank solution for affine constraints that satisfy the \"restricted isometry property\". We propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization with affine constraints (ARMP) and show that SVP recovers the minimum rank solution for affine constraints that satisfy the \"restricted isometry property\".\nThe key aspect of the paper is the following:\nOur algorithm is based on a small number of basic concepts:\nLifting to a set of equations\n\nA simple SVP to find an algorithm that yields a solution for affine constraints, but the algorithm is not directly able to find an algorithm that yields a solution for affine constraints.\nWe use the following principles to apply SVP to all basic concepts:\nSVP:\nAn algorithm for generating and calculating random algorithms for SVP\nLifting to a set of equations\nAn algorithm for generating and calculating random algorithms for SVP\nLifting to a set of equations\nSVP:\nAn algorithm for generating and calculating random algorithms for SVP\nMining for the first generation of SVP\nOur algorithm is based on the following principles:\nThe initial algorithm is based on a large number of basic concepts. The first generation is based on a large number of basic concepts. The first generation is based on a large number of basic concepts. The first generation is based on a large number of basic concepts. The first generation is based on", "histories": [["v1", "Wed, 30 Sep 2009 14:44:54 GMT  (41kb)", "http://arxiv.org/abs/0909.5457v1", null], ["v2", "Mon, 19 Oct 2009 19:32:44 GMT  (49kb)", "http://arxiv.org/abs/0909.5457v2", "An earlier version of this paper was submitted to NIPS-2009 on June 5, 2009"], ["v3", "Mon, 19 Oct 2009 21:21:57 GMT  (49kb)", "http://arxiv.org/abs/0909.5457v3", "An earlier version of this paper was submitted to NIPS-2009 on June 5, 2009"]], "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT", "authors": ["prateek jain 0002", "raghu meka", "inderjit s dhillon"], "accepted": true, "id": "0909.5457"}, "pdf": {"name": "0909.5457.pdf", "metadata": {"source": "CRF", "title": "Guaranteed Rank Minimization via Singular Value Projection", "authors": ["Raghu Meka", "Prateek Jain", "Inderjit S. Dhillon"], "emails": ["inderjit}@cs.utexas.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n90 9.\n54 57\nv1 [\ncs .L\nG ]\n3 0\nSe p"}, {"heading": "1 Introduction", "text": "In this paper we study the general affine rank minimization problem (ARMP),\nmin rank(X) s.t A(X) = b, X \u2208 Rm\u00d7n, b \u2208 Rd, (ARMP)\nwhere A is an affine transformation from Rm\u00d7n to Rd. The general affine rank manimization problem is of considerable practical interest and many important machine learning problems such as matrix completion, low-dimensional metric embedding, low-rank kernel learning can be viewed as instances of the above problem. Unfortunately, ARMP is NP-hard in general and is also NP-hard to approximate ([MJCD08]).\nUntil recently, most known methods for ARMP were heuristic in nature with few known rigorous guarantees. The most commonly used heuristic for the problem is to assume a factorization of X and optimize the resulting non-convex problem by alternating minimization [Bra03, Kor08, MB07], alternative projections [GB00] or alternating LMIs [SIG97]. Another common approach is to relax the rank constraint to a convex function such as the trace-norm or the log determinant [FHB01], [FHB03]. However, most of these methods do not have any optimality guarantees. Recently, Meka et al. [MJCD08] proposed online learning based methods for ARMP. However, their methods can only guarantee at best a logarithmic approximation for the minimum rank.\nIn a recent breakthrough, Recht, Fazel and Parillo [RFP07] obtained the first non-trivial exactrecovery results for ARMP obtaining guaranteed rank minimization for affine transformations A that satisfy a restricted isometry property (RIP). Define the isometry constant of A, \u03b4k to be the smallest number such that for all X \u2208 Rm\u00d7n of rank at most k,\n(1\u2212 \u03b4k)\u2016X\u20162F \u2264 \u2016A(X)\u201622 \u2264 (1 + \u03b4k)\u2016X\u20162F . (1)\nRecht et al. show that for affine constraints with bounded isometry constants (specifically, \u03b45k < 1/10), finding the minimum trace-norm solution recovers the minimum rank solution. However, their results only address the case of exact measurements and are hard to analyse. Moreover, even the best existing optimization algorithms for their convex relaxation problem are relatively inefficient in practice.\nIn this paper we propose a simple and fast algorithm SVP (Singular Valur Projection) based on the classical projected gradient algorithm. We present a simple analysis showing that SVP recovers the minimum rank solution for affine constraints that satisfy RIP even in the presence of noise and prove the following guarantees.\nTheorem 1.1. Suppose the isometry constant of A satisfies \u03b42k \u2264 1/3 and let b = A(X\u2217) for a rank-k matrix X\u2217. Then, SVP (Algorithm 1) with step-size \u03b7t = 1/(1 + \u03b42k) converges to X\n\u2217. Furthermore, SVP outputs a matrix X of rank at most k such that \u2016A(X) \u2212 b\u201622 \u2264 \u01eb in at most \u2308\n1 log((1\u2212\u03b42k)/2\u03b42k) log \u2016b\u2016 2 2\u01eb\n\u2309\niterations.\nTheorem 1.2 (Main). Suppose the isometry constant of A satisfies \u03b42k \u2264 1/3 and let b = A(X\u2217)+e for a rank k matrix X\u2217 and an error vector e \u2208 Rd. Then, SVP with step-size \u03b7t = 1/(1 + \u03b42k) outputs a matrix X of rank at most k such that \u2016A(X) \u2212 b\u201622 \u2264 (C2 + \u01eb)\u2016e\u20162, \u01eb \u2265 0, in at most \u2308\n1 log(1/D) log\n\u2016b\u20162\n(C2+\u01eb)\u2016e\u20162\n\u2309\niterations for universal constants C,D.\nOur analysis of SVP is motivated by the recent work in the field of conpressed sensing by Blumensath and Davies [BD09], Garg and Khandekar [GK09]. Our results improve the results of Recht et al. as follows.\n1. SVP is considerably simpler to analyze than minimizing the trace-norm. Further, we need weaker isometry assumptions on A than those of Recht et al. [RFP07]: we only require \u03b42k < 1/3 as opposed to \u03b45k < 1/10 required by Recht et al.\n2. We show robustness of SVP to noise, whereas the results of Recht et al. [RFP07] only address the case of exact measurements.\n3. SVP has a strong geometric convergence rate and is faster than using the best trace-norm optimization algorithms by an order of magnitude.\nWe also remark that to the best of our knowledge ours is the only work with provable guarantees for rank minimization for ARMP in the presence of noise.\nThough restricted isometry property is natural in settings where the affine constraints contain information about all the entries of the unknown matrix, in several cases of considerable practical interest the affine constraints only contain local information and may not satisfy RIP directly.\nOne such important problem where RIP does not hold directly is the low-rank matrix completion problem. In the matrix completion problem we are given the entries of an unknown low-rank matrix X\u2217 for ordered pairs (i, j) \u2208 \u2126 \u2286 [m]\u00d7 [n] and the goal is to complete the missing entries of X\u2217. A highly popular application of the matrix completion problem is in the field of collaborative filtering, where typically the task is to predict user ratings given past ratings of the users. Recently, a lot of attention has been given to the problem due to the Netflix Challenge [Net]. Other applications of matrix completion include triangulation from incomplete data, link prediction in social networks etc.\nSimilar to ARMP, the low-rank matrix completion is also NP-hard in general and most methods are heuristic in nature with no theoretical guarantees. The alternating least squares minimization heuristic and its variants [Kor08, MB07] perform the best in practice but are notoriously hard to analyze.\nRecently, Candes and Recht [CR08], Candes and Tao [CT09] and Keshavan et al. [KOM09] obtained the first non-trivial results for low-rank matrix completion under a few additional assumptions. Broadly, these works give exact-recovery guarantees when the optimal solution X\u2217 is \u00b5-incoherent (see Definition 4.1), and the entries \u2126 are chosen uniformly at random with |\u2126| \u2265 C(\u00b5, k)n poly log n. However, the algorithms of the above works, even when using methods taylored specifically for matrix-completion such as those of Cai et al. [CCS08], are quite expensive in practice and not very tolerant to noise.\nAs low-rank matrix completion is a special case of ARMP, we can naturally adapt our algorithm SVP for matrix completion. We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise. Furthermore, our experiments strongly suggest (see Figure 1) that guarantees similar to those of [CT09], [KOM09] hold for SVP, achieving exact recovery for incoherent matrices from an almost optimal number of entries1.\nAlthough we do not provide a rigorous proof of exact-recovery for SVP applied to matrix completion, we make partial progress in this direction and give strong intuition for the performance of SVP. We prove that though the affine constraints defining the matrix-completion problems do not obey the restricted isometry property, they obey the restricted isometry property over incoherent matrices. This weaker RIP condition along with a hypothesis bounding the incoherence of the iterates of SVP imply exact-recovery of a low-rank incoherent matrix from an almost optimal number of entries. We also provide strong empirical evidence supporting our hypothesis bounding the incoherence of the iterates of SVP (see Figure 2).\n1It follows from a coupon collector argument that exact-recovery from random samples requires nk log n samples.\nWe first present our algorithm SVP in Section 2 and present its analysis for affine constraints satisfying RIP in Section 3. In Section 4, we specialize our algorithm SVP to the task of lowrank matrix completion and prove a more restricted isometry property for the matrix completion problem. In Section 6, we give empirical results for SVP applied to ARMP and matrix-completion on real-world and synthetic problems."}, {"heading": "2 Singular Value Projection (SVP)", "text": "Consider the following robust formulation of ARMP (RARMP),\nmin X\n\u03c8(X) = 1\n2 \u2016A(X)\u2212 b\u201622 s.t X \u2208 C(k) = {X : rank(X) \u2264 k}. (1)\nThe hardness of the above problem mainly comes from the non-convexity of the set of low-rank matrices C(k). However, in spite of the hardness of the rank constraint, the Euclidean projection onto the non-convex set C(k) can be computed efficiently using singular value decomposition. Our algorithm uses this observation along with the projected gradient method for efficiently minimizing the objective function specified in problem (1).\nLet Pk : Rm\u00d7n \u2192 Rm\u00d7n denote the orthogonal projection on to the set C(k). That is, Pk(X) = argminY {\u2016Y \u2212 X\u2016F : Y \u2208 C(k)}. It is well known that Pk(X) can be computed efficiently by computing the top k singular values and vectors of X.\nIn SVP, a candidate solution to ARMP is computed iteratively by starting from the all-zero matrix and adapting the classical projected gradient descent update as follows (Observe that \u2207\u03c8(X) = AT (A(X) \u2212 b)) :\nXt+1 \u2190 Pk ( Xt \u2212 \u03b7t\u2207\u03c8(Xt) ) = Pk ( Xt \u2212 \u03b7tAT (A(Xt)\u2212 b) ) . (2)\nAlgorithm 1 presents our SVP algorithm. Note that the iterates Xt are always low-rank, facilitating faster computation of the SVD. See Section 5 for a more detailed discussion of the computational issues.\n3 Analysis for Affine Constraints Satisfying RIP\nWe now show that SVP solves exact rank minimization for affine constraints that satisfy RIP and prove our main results, Theorems 1.1 and 1.2. We first present a lemma that bounds the error at\nAlgorithm 1 Singular Value Projection (SVP) Algorithm Require: A, b, tolerance \u03b5, \u03b7t, t = 0, 1, 2, . . . 1: Initialize: X0 = 0 and t = 0 2: repeat 3: Y t+1 \u2190 Xt \u2212 \u03b7tAT (A(Xt)\u2212 b) 4: Compute top k singular vectors of Y t+1: Uk, \u03a3k, Vk 5: Xt+1 \u2190 Uk\u03a3kV Tk 6: until \u2016A(Xt+1)\u2212 b\u201622 \u2264 \u03b5\n(t + 1)-th iteration (\u03c8(Xt+1)) w.r.t. the error incurred by the optimal solution (\u03c8(X\u2217)) and the t-th iteration.\nLemma 3.1. Let Xt be the iterate obtained by SVP algorithm at t-th iteration. Then,\n\u03c8(Xt+1) \u2264 \u03c8(X\u2217) + \u03b42k (1\u2212 \u03b42k) \u2016A(X\u2217 \u2212Xt)\u201622,\nwhere \u03b42k is the rank 2k isometry constant of A.\nProof. Recall that \u03c8(X) = 12\u2016A(X)\u2212 b\u201622. Since \u03c8(\u00b7) is a quadratic function, we have\n\u03c8(Xt+1)\u2212 \u03c8(Xt) = \u3008\u2207\u03c8(Xt),Xt+1 \u2212Xt\u3009+ 1 2 \u2016A(Xt+1 \u2212Xt)\u201622\n\u2264 \u3008AT (A(Xt)\u2212 b),Xt+1 \u2212Xt\u3009+ 1 2 \u00b7 (1 + \u03b42k) \u00b7 \u2016Xt+1 \u2212Xt\u20162F , (1)\nwhere inequality (1) follows from RIP applied to the matrix Xt+1 \u2212 Xt of rank at most 2k. Let Y t+1 = Xt \u2212 11+\u03b42kA T (A(Xt)\u2212 b) and\nft(X) = \u3008AT (A(Xt)\u2212 b),X \u2212Xt\u3009+ 1\n2 \u00b7 (1 + \u03b42k) \u00b7 \u2016X \u2212Xt\u20162F .\nThen,\nft(X) = 1\n2 (1 + \u03b42k)\n[ \u2016X \u2212Xt\u20162F + 2 \u2329AT (A(Xt)\u2212 b)\n1 + \u03b42k ,X \u2212Xt\n\u232a]\n= 1\n2 (1 + \u03b42k)\u2016X \u2212 Y t+1\u20162F \u2212\n1\n2(1 + \u03b42k) \u00b7 \u2016AT (A(Xt)\u2212 b)\u20162F .\nNow, by definition, Pk(Y t+1) = Xt+1 is the minimizer of ft(X) over all matrices X \u2208 C(k) of rank at most k. In particular, ft(X t+1) \u2264 ft(X\u2217). Thus,\n\u03c8(Xt+1)\u2212 \u03c8(Xt) \u2264 ft(Xt+1) \u2264 ft(X\u2217) = \u3008AT (A(Xt)\u2212 b),X\u2217 \u2212Xt\u3009+ 1\n2 (1 + \u03b42k)\u2016X\u2217 \u2212Xt\u20162F\n\u2264 \u3008AT (A(Xt)\u2212 b),X\u2217 \u2212Xt\u3009+ 1 2 \u00b7 1 + \u03b42k 1\u2212 \u03b42k \u2016A(X\u2217 \u2212Xt)\u201622 (2) = \u03c8(X\u2217)\u2212 \u03c8(Xt) + \u03b42k (1\u2212 \u03b42k) \u2016A(X\u2217 \u2212Xt)\u201622,\nwhere inequality (2) follows from RIP applied to X\u2217 \u2212Xt.\nWe now prove that SVP obtains the optimal solution for ARMP with restricted isometry property.\nProof of Theorem 1.1. Using Lemma 3.1 and the fact that \u03c8(X\u2217) = 0 for the noise-less case,\n\u03c8(Xt+1) \u2264 \u03b42k (1\u2212 \u03b42k) \u2016A(X\u2217 \u2212Xt)\u201622 = 2\u03b42k (1\u2212 \u03b42k) \u03c8(Xt).\nAlso, note that for \u03b42k < 1/3, 2\u03b42k (1\u2212\u03b42k) < 1. Hence, \u03c8(X\u03c4 ) \u2264 \u01eb where \u03c4 =\n\u2308\n1 log((1\u2212\u03b42k)/2\u03b42k) log \u03c8(X 0) \u01eb\n\u2309\n.\nNow, the SVP algorithm is initialized usingX0 = 0, i.e., \u03c8(X0) = \u2016b\u2016 2 2 . Hence, \u03c4 = \u2308 1 log((1\u2212\u03b42k)/2\u03b42k) log \u2016b\u2016 2 2\u01eb \u2309 .\nNext, we prove the noisy version of Theorem 1.1.\nProof of Theorem 1.2. Let the current solution Xt satisfy \u03c8(Xt) \u2265 C2\u2016e\u20162/2, where C \u2265 0 is a universal constant. Using Lemma 3.1 and the fact that b\u2212A(X\u2217) = e,\n\u03c8(Xt+1) \u2264 \u2016e\u2016 2 2\n2 + \u03b42k (1\u2212 \u03b42k) \u2016b\u2212A(Xt)\u2212 e\u201622,\n\u2264 \u2016e\u2016 2 2\n2 + 2\u03b42k (1\u2212 \u03b42k)\n(\n\u03c8(Xt)\u2212 eT (b\u2212A(Xt)) + \u2016e\u2016 2\n2\n)\n,\n\u2264 \u03c8(X t)\nC2 + 2\u03b42k (1\u2212 \u03b42k)\n(\n\u03c8(Xt) + 2\nC \u03c8(Xt) +\n1\nC2 \u03c8(Xt)\n)\n,\n\u2264 ( 1\nC2 + 2\u03b42k (1\u2212 \u03b42k)\n(\n1 + 1\nC\n)2 )\n\u03c8(Xt)\n= D\u03c8(Xt),\nwhere D = (\n1 C2 + 2\u03b42k(1\u2212\u03b42k) ( 1 + 1C )2 )\n. Recall that \u03b42k < 1/3. Hence, selecting C > (1 + \u03b42k)/(1 \u2212 3\u03b42k), we get D < 1. Also, \u03c8(X\n0) = \u03c8(0) = \u2016b\u20162/2. Hence, \u03c8(X\u03c4 ) \u2264 (C2 + \u01eb)\u2016e\u20162/2 where \u03c4 = \u2308\n1 log(1/D) log\n\u2016b\u20162\n(C2+\u01eb)\u2016e\u20162\n\u2309\n."}, {"heading": "4 Matrix Completion", "text": "We first describe the low-rank matrix completion problem formally. Let P\u2126 : Rm\u00d7n \u2192 Rm\u00d7n denote the projection onto the index set \u2126. That is, (P\u2126(X))ij = Xij for (i, j) \u2208 \u2126 and (P\u2126(X))ij = 0 otherwise. Then, the low-rank matrix completion problem (MCP) can be formulated as follows,\nmin X\nrank(X) s.t P\u2126(X) = P\u2126(X\u2217), X \u2208 Rm\u00d7n. (MCP)\nObserve that the matrix completion problem is a special case of ARMP. However, the affine constraints that define MCP, P\u2126, do not satisfy RIP in general. Thus Theorems 1.1, 1.2 above and the results of Recht et al. [RFP07] do not directly apply to MCP. The first non-trivial results for MCP were obtained recently by Candes and Recht [CR08], Keshavan et al. [KOM09] and Candes and Tao [CT09]. These works show exact recovery of the unknown matrix X\u2217 when the observed entries are sampled uniformly and X\u2217 is incoherent in the sense defined below.\nDefinition 4.1 (Incoherence). A rank-k matrix X \u2208 Rm\u00d7n with singular value decomposition X = U\u03a3V T is \u00b5-incoherent if\nmax i,j |Uij| \u2264 \u221a \u00b5\u221a m , max i,j |Vij | \u2264 \u221a \u00b5\u221a n .\nIntuitively, high incoherence implies that the non-zero entries of X are not concentrated in a small number of entries. Hence, a random sampling of the matrix should provide enough information to reconstruct the entire matrix.\nAs matrix completion is a special case of ARMP, we can apply SVP for matrix completion. We apply SVP to matrix-completion with step-size \u03b7t = 1/(1 + \u03b4)p, where p is the density of sampled entries, leading to the update\nXt+1 \u2190 Pk ( Xt \u2212 1 (1 + \u03b4)p (P\u2126(Xt)\u2212 P\u2126(X\u2217)) ) . (1)\nWe now provide some intuition for our choice of step-size \u03b7t and make partial progress towards proving that SVP achieves exact recovery for low-rank incoherent matrices. We show that though the affine constraints defining MCP, P\u2126, do not satisfy RIP for all low-rank matrices, they satisfy RIP for all low-rank incoherent matrices. Thus, if the iterates appearing in SVP remain incoherent throughout the execution of the algorithm, then Theorem 1.1 would imply recovery of the unknown entries of the matrix. Empirical evidence strongly supports our hypothesis that the incoherence of the iterates arising in SVP remains bounded.\nFigure 1 plots the threshold sampling density beyond which matrix completion for randomly generated matrices is solved exactly by SVP for fixed k and varying matrix sizes n. Note that the density threshold matches the optimal bound of O(k log n/n) with the constant being C = 1.28. Figure 2 plots the maximum incoherence maxt \u00b5(X t) = \u221a n maxt,i,j |U tij |, where U t are the left singular vectors of the intermediate iterates Xt computed by SVP. The figure clearly shows that the incoherence \u00b5(Xt) of the iterates is bounded by a constant independent of the matrix size n and density p throughout the execution of SVP.\nFix an incoherent matrix X \u2208 Rm\u00d7n of rank at most k and let \u2126 be sampled according to the Bernoulli model with each (i, j) \u2208 \u2126 independently with probability p. Then, E[\u2016P\u2126(X)\u20162F ] = p\u2016X\u20162F . Further, by Chernoff bounds, for \u03b4 > 0, p \u2265 Ck2 log n/m for a universal constant C, with high probability\n(1\u2212 \u03b4)p \u2016X\u20162F \u2264 \u2016P\u2126(X)\u20162F \u2264 (1 + \u03b4)p \u2016X\u20162F . (2) Combining the above Chernoff bound estimate with a union bound over low-rank incoherent matrices, we obtain the following restricted isometry property for the projection operator P\u2126 restricted to low-rank incoherent matrices.\nTheorem 4.2. There exists a constant C \u2265 0 such that the following holds for all 0 < \u03b4 < 1, \u00b5 \u2265 1, n \u2265 m \u2265 3: For \u2126 \u2286 [m] \u00d7 [n] chosen according to the Bernoulli model with density p \u2265 C\u00b52k2 log n/\u03b42m, with probability at least 1 \u2212 exp(\u2212n log n), the restricted isometry property in (2) holds for all \u00b5-incoherent matrices X of rank at most k.\nMotivated by the above theorem and supported by empirical evidence (Figures 1, 2) we hypothesize that SVP achieves exact recovery from an almost optimal number of samples.\nConjecture 4.3. Fix \u00b5, k and \u03b4 \u2264 1/3. Then, there exists a constant C such that for a \u00b5incoherent matrix X\u2217 of rank at most k and \u2126 sampled from the Bernoulli model with density p \u2265 C\u00b52k2 log n/m, SVP with step-size \u03b7t = 1/(1 + \u03b4)p converges to X\u2217 with high probability. Moreover, SVP outputs a matrix X of rank at most k such that \u2016P\u2126(X) \u2212 P\u2126(X\u2217)\u20162F \u2264 \u01eb after O\u00b5,k (\u2308 log ( 1 \u01eb )\u2309) iterations.\n4.1 RIP for Matrix Completion on Incoherent Matrices\nWe now prove the RIP property of Theorem 4.2 for the projection operator P\u2126. To prove Theorem 4.2 we first show the theorem for a discrete collection of matrices using Chernoff type large-deviation bounds and use standard quantization arguments to generalize to the continuous case. We first introduce some notation.\nDefinition 4.4. For a matrix X \u2208 Rm\u00d7n, let \u2016X\u2016mx = maxi,j |Xij | and call X \u03b1-regular if\n\u2016X\u2016mx \u2264 \u03b1\u221a mn \u00b7 \u2016X\u2016F .\nWe need Bernstein\u2019s inequality [Wik09] stated below.\nLemma 4.5 (Bernstein\u2019s inequality). Let X1,X2, . . . ,Xn be independent random variables with E[Xi] = 0,\u2200i. Furthermore, let |Xi| \u2264 M . Then,\nP [ \u2211\ni\nXi > t] \u2264 exp ( \u2212 t 2/2 \u2211\nV ar(Xi) +Mt/3\n)\n.\nLemma 4.6. Fix an \u03b1-regular X \u2208 Rm\u00d7n and 0 < \u03b4 < 1. Then, for \u2126 \u2286 [m]\u00d7 [n] chosen according to the Bernoulli model, with each pair (i, j) \u2208 \u2126 chosen independently with probability p,\nPr[ \u2223 \u2223\u2016P\u2126(X)\u20162F \u2212 p\u2016X\u20162F \u2223 \u2223 \u2265 \u03b4p\u2016X\u20162F ] \u2264 2 exp ( \u2212\u03b4 2pmn\n3\u03b12\n)\n.\nProof. For (i, j) \u2208 [m] \u00d7 [n], let \u03c9ij be the indicator variables with \u03c9ij = 1 if (i, j) \u2208 \u2126 and 0 otherwise. Then, \u03c9ij are independent random variables with Pr[\u03c9ij = 1] = p. Let random variable Zij = \u03c9ijX 2 ij . Note that,\nE[Zij ] = pX 2 ij, V ar(Zij) = p(1\u2212 p)X4ij .\nObserve that |Zij \u2212 E[Zij ]| < |Xij |2 < (\u03b12/mn) \u00b7 \u2016X\u20162F . Thus,\nM = max i,j\n|Zij \u2212 E[Zij ]| \u2264 \u03b12\nmn \u2016X\u20162F . (3)\nNow, define random variable S = \u2211 i,j Zij = \u2211 i,j \u03c9ijX 2 ij = \u2016P\u2126(X)\u20162F . Note that, E[S] = p\u2016X\u20162F . Since, Zij are independent random variables,\nV ar(S) = \u2211\ni,j\np(1\u2212 p)X4ij \u2264 p (max i,j X2ij) \u00b7 \u2211\ni,j\nX2ij \u2264 p\u03b12\nmn \u2016X\u20164F . (4)\nUsing Bernstein\u2019s inequality (Lemma 4.5) for S with t = \u03b4p\u2016X\u20162F and Equations (3) and (4) we get,\nPr[|S \u2212 E[S]| > t] \u2264 2 exp ( \u2212t2/2 V ar(Z) +Mt/3 )\n\u2264 2 exp ( \u2212\u03b4 2pmn\n3\u03b12\n)\n.\nWe now discretize the space of low-rank incoherent matrices so as to be able to use the above lemma with a union bound. We need the following simple lemmas. Lemma 4.7. Let X \u2208 Rm\u00d7n be a \u00b5-incoherent matrix of rank at most k. Then X is \u00b5 \u221a k-regular.\nProof. Let X = U\u03a3V T be the singular value decomposition of X. Then, Xij = Ui\u03a3V T j , where Ui, Vj are the i\u2019th and j\u2019th rows of U, V respectively. Now,\n|Xij | = |eTi U\u03a3V T ej | = | k \u2211\nl=1\nUil\u03a3llVjl| \u2264 k \u2211\nl=1\n\u03a3ll|Uil||Vjl|.\nSince X is \u00b5-incoherent,\n|Xij | \u2264 k \u2211\nl=1\n\u03a3ll|Uil||Vjl| \u2264 \u00b5\u221a mn\n\u00b7 ( k \u2211\nl=1\n\u03a3ll) \u2264 \u00b5\u221a mn\n\u00b7 \u221a k \u00b7 ( k \u2211\nl=1\n\u03a32ll) 1/2 =\n\u00b5 \u221a k\u221a\nmn \u00b7 \u2016X\u2016F .\nLemma 4.8. Let a, b, c, x, y, z \u2208 [\u22121, 1]. Then,\n|abc\u2212 xyz| \u2264 |a\u2212 x|+ |b\u2212 y|+ |c\u2212 z|.\nThe following lemma shows that the space of low-rank \u00b5-incoherent matrices can be discretized into a reasonably small set of regular matrices such that every low-rank \u00b5-incoherent matrix is close to a matrix from the set.\nLemma 4.9. For all 0 < \u01eb < 1/2, \u00b5 \u2265 1, m,n \u2265 3 and k \u2265 1, there exists a set S(\u00b5, \u01eb) \u2286 Rm\u00d7n with |S(\u00b5, \u01eb)| \u2264 (mnk/\u01eb)3 (m+n)k such that the following holds. For any \u00b5-incoherent X \u2208 Rm\u00d7n of rank k with \u2016X\u20162 = 1, there exists Y \u2208 S(\u00b5, \u01eb) such that \u2016Y \u2212X\u2016F < \u01eb and Y is (4\u00b5 \u221a k)-regular.\nProof. We construct S(\u00b5, \u01eb) by discretizing the space of low-rank incoherent matrices. Let \u03c1 = \u01eb/ \u221a 9k2mn and D(\u03c1) = {\u03c1 i : i \u2208 Z, |i| \u2264 \u230a1/\u03c1\u230b}. Let\nU(\u03c1) = {U \u2208 Rm\u00d7k : Uij \u2208 ( \u221a \u00b5/m) \u00b7D(\u03c1) },\nV (\u03c1) = {V \u2208 Rn\u00d7k : Vij \u2208 ( \u221a \u00b5/n) \u00b7D(\u03c1) }, \u03a3(\u03c1) = {\u03a3 \u2208 Rk\u00d7k : \u03a3ij = 0, i 6= j, \u03a3ii \u2208 D(\u03c1)},\nS(\u00b5, \u01eb) = {U\u03a3V T : U \u2208 U(\u03c1),\u03a3 \u2208 \u03a3(\u03c1), V \u2208 V (\u03c1) }. We will show that S(\u00b5, \u01eb) satisfies the conditions of the Lemma. Observe that |D(\u03c1)| < 1/\u03c1. Thus,\n|U(\u03c1)| < (1/\u03c1)mk, |V (\u03c1)| < (1/\u03c1)nk, |\u03a3(\u03c1)| < (1/\u03c1)k .\nHence, |S(\u00b5, \u01eb)| < (1/\u03c1)mk+nk+k < (mnk/\u01eb)3(m+n)k . Fix a \u00b5-incoherent X \u2208 Rm\u00d7n of rank at most k with \u2016X\u20162 = 1. Let the singular value decomposition of X be X = U\u03a3V T . Let U1 be the matrix obtained by rounding entries of U to integer multiples of \u221a \u00b5 \u03c1/ \u221a m as follows: for (i, l) \u2208 [m]\u00d7 [k], let\n(U1)il = \u221a \u00b5\u03c1\u221a m \u00b7 \u230a Uil \u221a m\u221a \u00b5\u03c1 \u230b .\nNow, since Uil \u2264 \u221a \u00b5/ \u221a m, it follows that U1 \u2208 U(\u03c1). Further, for all i \u2208 [m], l \u2208 [k],\n|(U1)il \u2212 Uil| < \u221a \u00b5\u221a m \u03c1 \u2264 \u03c1.\nSimilarly, define V1,\u03a31 by rounding entries of V,\u03a3 to integer multiples of \u221a \u00b5 \u03c1/ \u221a n and \u03c1 respectively. Then, V1 \u2208 V (\u03c1), \u03a31 \u2208 \u03a3(\u03c1) and for (j, l) \u2208 [n]\u00d7 [k],\n|(V1)jl \u2212 Vjl| < \u221a \u00b5\u03c1\u221a n \u2264 \u03c1, |(\u03a31)ll \u2212 \u03a3ll| < \u03c1.\nLet X(\u03c1) = U1\u03a31V T 1 . Then, by the above equations and Lemma 4.8, for i \u2208 [m], l \u2208 [k], j \u2208 [n],\n|(U1)il(\u03a31)ll(V1)jl \u2212 Uil\u03a3llVjl| < 3\u03c1.\nThus, for i, j \u2208 [m]\u00d7 [n],\n|X(\u03c1)ij \u2212Xij | = | k \u2211\nl=1\n(U1)il(\u03a31)ll(V1)jl \u2212 Uil\u03a3llVjl|\n\u2264 k \u2211\nl=1\n|(U1)il(\u03a31)ll(V1)jl \u2212 Uil\u03a3llVjl|\n< 3k\u03c1. (5)\nUsing Lemma 4.7 and Equation (5)\n\u2016X(\u03c1)\u2016mx < \u2016X\u2016mx + 3k\u03c1 \u2264 \u00b5 \u221a k\u221a\nmn \u00b7 \u2016X\u2016F + \u01eb\u221a mn .\nAlso, using (5),\n\u2016X(\u03c1) \u2212X\u20162F = \u2211\ni,j\n|X(\u03c1)ij \u2212Xij |2 < 9k2mn\u03c12 = \u01eb2.\nFurthermore, using triangular inequality, \u2016X(\u03c1)\u2016F > \u2016X\u2016F \u2212 \u01eb > \u2016X\u2016F /2. Since, \u01eb < 1 and \u00b5 \u221a k\u2016X\u2016F \u2265 1,\n\u2016X(\u03c1)\u2016mx < 2\u00b5 \u221a k\u221a\nmn \u00b7 \u2016X\u2016F <\n4\u00b5 \u221a k\u221a\nmn \u00b7 \u2016X(\u03c1)\u2016F .\nThus, X(\u03c1) is 4\u00b5 \u221a k-regular. The lemma now follows by taking Y = X(\u03c1).\nWe now prove Theorem 4.2 by combining Lemmas 4.6 and 4.9.\nProof of Theorem 4.2. Let m \u2264 n, \u01eb = \u03b4/9mnk and\nS\u2032(\u00b5, \u01eb) = {Y : Y \u2208 S(\u00b5, \u01eb), Y is 4\u00b5 \u221a k-regular},\nwhere S(\u00b5, \u01eb) is as in Lemma 4.9. Then, by Lemma 4.2 and union bound,\nPr [ \u2223 \u2223\u2016P\u2126(Y )\u20162F \u2212 p\u2016Y \u20162F \u2223 \u2223 \u2265 \u03b4p\u2016Y \u20162F for some Y \u2208 S\u2032(\u00b5, \u01eb) ]\n\u2264 2 ( mnk\n\u01eb\n)3(m+n)k\nexp (\u2212\u03b42pmn 16\u00b52k )\n\u2264 exp(C1nk log n) \u00b7 exp (\u2212\u03b42pmn\n16\u00b52k\n)\n,\nwhere C1 \u2265 0 is a constant independent of m,n, k. Thus, if p > C\u00b52k2 log n/\u03b42m, where C = 16(C1+1), with probability at least 1\u2212exp(\u2212n log n), the following holds \u2200Y \u2208 S\u2032(\u00b5, \u01eb), |\u2016P\u2126(Y )\u20162F \u2212 p\u2016Y \u20162F | \u2264 \u03b4p\u2016Y \u20162F . (6)\nAs the statement of the theorem is invariant under scaling, it is enough to show the statement for all \u00b5-incoherent matrices X of rank at most k and \u2016X\u20162 = 1. Fix such a X and suppose that Equation (6) holds. Now, by Lemma 4.9 there exists Y \u2208 S\u2032(\u00b5, \u01eb) such that \u2016Y \u2212X\u2016F \u2264 \u01eb. Moreover,\n\u2016Y \u20162F \u2264 (\u2016X\u2016F + \u01eb)2 \u2264 \u2016X\u20162F + 2\u01eb\u2016X\u2016F + \u01eb2 \u2264 \u2016X\u20162F + 3\u01ebk.\nProceeding similarly, we can show that\n|\u2016X\u20162F \u2212 \u2016Y \u20162F | \u2264 3\u01ebk. (7)\nFurther, starting with \u2016P\u2126(Y \u2212X)\u2016F \u2264 \u2016Y \u2212X\u2016F \u2264 \u01eb and arguing as above we get that\n|\u2016P\u2126(Y )\u20162F \u2212 \u2016P\u2126(X)\u20162F | \u2264 3\u01ebk. (8)\nCombining inequalities (7), (8) above, we have\n|\u2016P\u2126(X)\u20162F \u2212 p\u2016X\u20162F | \u2264 |\u2016P\u2126(X)\u20162F \u2212 \u2016P\u2126(Y )\u20162F |+ p |\u2016X\u20162F \u2212 \u2016Y \u20162F |+ |\u2016P\u2126(Y )\u20162F \u2212 p\u2016Y \u20162F | \u2264 6\u01ebk + \u03b4p\u2016Y \u20162F Equations (6), (7), (8) \u2264 6\u01ebk + \u03b4p(\u2016X\u20162F + 3\u01ebk) Equation (7) \u2264 9\u01ebk + \u03b4p\u2016X\u20162F \u2264 2\u03b4p\u2016X\u20162F . Since \u2016X\u20162F \u2265 1\nThe theorem now follows."}, {"heading": "5 Computational Issues and Related Work", "text": "Minimizing the trace-norm of a matrix subject to affine constraints can be cast as a semi-definite programming problem. However, algorithms for semi-definite programming, as used by most methods for minimizing trace-norm, are prohibitively expensive even for moderately large datasets. Recently, a variety of methods mostly based on iterative soft-thresholding have been proposed to solve the trace-norm minimization problem efficiently. For instance, Cai et al. [CCS08] proposed a Singular Value Thresholding (SVT) algorithm which is based on Uzawa\u2019s algorithm[AHU58]. A related approach based on linearized Bregman iteration was proposed by Ma et al. [MGC09].\nToh and Yun [TY09], Ji and Ye [JY09] proposed Nesterov\u2019s projected gradient based methods for optimizing the trace-norm.\nWhile the soft-thresholding based methods for trace-norm minimization are significantly faster than semi-definite programming approaches, they suffer from an important bottleneck: though the final solution to the trace-norm minimization is a low-rank matrix, the rank of the iterate in intermediate iterations can be large. In contrast, the rank of the iterates in our method is always equal to the rank of the optimal solution.\nAlso, though minimizing the trace-norm does most likely approximate the low-rank solution even in the presence of noise (see [CP09] for instance), noise poses considerable computational challenges for trace-norm optimization. Cai et al. propose a variant of SVT for handling noise that performs moderately well for uniformly bounded noise. However, the performance of SVT worsens considerably in the presence of outlier noise. SVP on the other hand is robust to both outlier and uniformly bounded noise as it minimizes the cumulative loss function \u2016A(X) \u2212 b\u201622.\nFor the case of low-rank matrix completion, Candes and Recht [CR08] obtained the first nontrivial results for the problem obtaining guaranteed completion for incoherent matrices X\u2217 and randomly sampled entries \u2126. Candes and Recht show that for X\u2217 \u00b5-incoherent and \u2126 chosen at random with |\u2126| \u2265 C(\u00b5) k2n1.2, trace-norm relaxation recovers the optimal solution. Building on the work of Candes and Recht, Candes and Tao [CT09] obtained the near-optimal bound of |\u2126| \u2265 min(C\u00b54k2n log2 n,C\u00b52kn log6 n) for exact-recovery via trace-norm minimization. However, the analysis of Candes and Recht, Candes and Tao is considerably complicated and minimizing trace-norm, even when using methods taylored for matrix-completion such as those of [CCS08], is relatively expensive in practice.\nFor the case of matrix completion, SVT has the important property that the intermediate iterations of the algorithm only require computing the singular value decomposition of a sparse matrix. This facilitates the use of fast SVD computing package such as PROPACK [Lar] that only require subroutines that compute matrix-vector products.\nOur SVP algorithm has a similar property facilitating fast computation of the update in equation (1); each iteration of SVP involves computing the SVD of the matrix Y = Xt+P\u2126(Xt\u2212X\u2217), where Xt is a matrix of rank at most k whose SVD we know and P\u2126(Xt \u2212X\u2217) is a sparse matrix. Thus, we can compute matrix-vector products of the form Y x in time O((m+ n)k + |\u2126|).\nIn a different line of work, Keshavan et al. [KOM09] obtained exact-recovery from uniformly sampled \u2126 with |\u2126| \u2265 C(\u00b5, k)n log n using different technqiues. The first iteration of SVP is similar to the first step of Keshavan et al. However, after the first iteration, Keshavan et al. use a sophisticated alternating minimization algorithm based on gradient descent on the Grassmannian manifold of low-rank matrices. However, convergence of their alternating minimization algorithm is slow. The simplicity of the updates in SVP makes it both easier to implement and significantly less computationally intensive than the alternating minimization algorithm of Keshavan et al.\nA related problem to the matrix completion problem is the problem of low-rank plus sparse decomposition of a matrix addressed by Chandrasekaran et al. [CSPW09] andWright et al. [WGRM09]. Interestingly, Wright et al. [WGRM09] show that the low-rank matrix completion problem can be reduced to the low-rank plus sparse decomposition problem. Here again, their method relies on the trace-norm relaxation and is significantly more computationally intensive than our algorithm."}, {"heading": "6 Experimental Results", "text": "In this section, we empirically evaluate our SVP method for the affine rank minimization and lowrank matrix completion problems. For both problems we present empirical results on synthetic as well as real-world datasets. For ARMP we compare our method against the trace-norm based\nsingular value thresholding (SVT) method [CCS08]. Note that although [CCS08] presents SVT method in the context of matrix completion problem, however it can be easily adapted for ARMP. For matrix completion we compare against SVT, the spectral matrix completion (SMC) method of [KOM09], and regularized alternating least squares minimization (ALS). We use our own implentation of SVT for ARMP and ALS, while for matrix completion we use the code provided by the respective authors for SVT and SMC. We report results averaged over 20 runs. All the methods are implemented in Matlab and uses mex files."}, {"heading": "6.1 Affine Rank Minimization", "text": "We first compare our method against SVT on random instances of ARMP. We generate random matrices X \u2208 Rn\u00d7n of different sizes n and fixed rank k = 5. We then generate d = 6kn random affine constraint matrices Ai, 1 \u2264 i \u2264 d and compute b = A(X). Figure 3 (a) compares the computational time required by SVP and SVT for achieving a relative error (\u2016A(X) \u2212 b\u20162/\u2016b\u20162) of 10\u22123. Our method requires many fewer iterations and is signficantly faster than SVT.\nNext we evaluate our method for the problem of matrix reconstruction from random measurements. As in Recht et al. [RFP07], we use the MIT logo as the test image for reconstruction. the MIT logo we use is a 38 \u00d7 73 image and has rank four. For reconstruction, we generate random measurement matrices Ai and measure bi = Tr(AiX). Figure 3 (a) shows that our method incurs significantly smaller reconstruction error than SVT with lower number of iterations."}, {"heading": "6.2 Matrix Completion", "text": "Next, we evaluate our method against various matrix completion methods for random low-rank matrices and uniform samples. We generate a random rank k matrix X \u2208 Rn\u00d7n and generate random Bernoulli samples with probability p. Figure 4 compares the time required by various methods to obtain a root mean square error (RMSE) of 10\u22123. Clearly, our method is substantially faster than both SVT and SMC, and is competitive with ALS.\nNext, we study the behavior of our method when there are outliers in the sampled entries. For this experiment, we generate random matrices of different size and corrupt around 10% of the sampled entries by adding large amplitude Gaussian noise. Figure 5 plots error incurred and time required by various methods as n increases from 500 to 5000. Note that SVT is particularly sensitive to outlier noise and incurs high RMSE. Also, for noisy samples the computational cost of\nboth SVT and SMC increases considerably.\nMatrix Completion: Movie-Lens Dataset Finally, we evaluate our method on the Movie-Lens dataset [Mov], which contains 1 million ratings for 3900 movies by 6040 users. For SVP and ALS, we fix the rank of the matrix to be k = 15. For SVP, we set the step size \u03b7t to be 5/ \u221a t. SVP incurs RMSE of 1.01 in 64.85 seconds, while SVT incurs RMSE of 1.21 in 1214.78 seconds. In contrast, ALS achieves RMSE of 0.90 in 195.34 seconds. We attribute the relatively poor performance of SVP and SVT as compared with ALS to the fact that the ratings matrix is not sampled uniformly, thus violating a crucial assumption of both our method and SVT. Similar to Figure 5 (b), SVT converges much slower than SVP on the Movie-Lens data."}, {"heading": "7 Conclusion and Future Work", "text": "There has been a significant amount of work recently in the area of low-rank approximations. Examples include minimizing rank subject to affine constraints, low-rank matrix completion, low-rank plus sparse decomposition. Most of these works, with the exception of Keshavan et al. [KOM09], rely on relaxing the rank constraint with trace-norm and give guarantees for recovering the optimal\nsolution under certain additional assumptions. However, trace-norm relaxation based methods are typically hard to analyse and are relatively expensive in practice.\nIn this paper, we proposed a simple and natural algorithm based on iterative hard-thresholding. We give a simple anlaysis of our algorithm for the affine rank minimization problem satisfying the restricted isometry property and give geometric convergence guarantees even in the presence of noise. The intermediate steps in our algorithm are less computationally demanding than those of current state-of-the-art methods. We empirically demonstrate that our method is significantly faster and more robust to both uniformly bounded and outlier noise than most existing methods.\nAn immediate question arising out of our work is to prove our hypothesis bounding the incoherence of the iterates of SVP for low-rank matrix completion, or otherwise directly prove Conjecture 4.3. Another interesting direction is to study parallels between our work and the iterative hardthresholding and matching pursuit lines of work in compressed sensing. Such a connection can be contransted with the natural relation between trace-norm minimization methods for rank minimization and l1-norm minimization methods in compressed sensing. Other directions include application of our methods to other problems of similar flavour such as the low-rank plus sparse matrix decomposition [CSPW09], or other matrix completion type problems like minimum dimensionality embedding using partial distance observations [FHB03] and low-rank kernel learning [MJCD08]."}, {"heading": "Acknowledgments", "text": "This research was supported by NSF grant CCF-0431257, NSF-ITR award IIS-0325116 and NSF grant CCF-0728879."}], "references": [{"title": "Studies in Linear and Nonlinear Programming", "author": ["K. Arrow", "L. Hurwicz", "H. Uzawa"], "venue": "Stanford University Press, Stanford", "citeRegEx": "AHU58", "shortCiteRegEx": null, "year": 1958}, {"title": "Applied and Computational Harmonic Analysis", "author": ["Thomas Blumensath", "Mike E. Davies. Iterative hard thresholding for compressed sensing"], "venue": "27(3):265 \u2013 274,", "citeRegEx": "BD09", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast online svd revisions for lightweight recommender systems", "author": ["Matthew Brand"], "venue": "In SIAM International Conference on Data Mining.", "citeRegEx": "Bra03", "shortCiteRegEx": null, "year": 2003}, {"title": "and Zuowei Shen", "author": ["Jian-Feng Cai", "Emmanuel J. Candes"], "venue": "A singular value thresholding algorithm for matrix completion,", "citeRegEx": "CCS08", "shortCiteRegEx": null, "year": 2008}, {"title": "Cand\u00e8s and Yaniv Plan", "author": ["J Emmanuel"], "venue": "Matrix completion with noise,", "citeRegEx": "CP09", "shortCiteRegEx": null, "year": 2009}, {"title": "Cand\u00e8s and Benjamin Recht", "author": ["J Emmanuel"], "venue": "Exact matrix completion via convex optimization,", "citeRegEx": "CR08", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse and low-rank matrix decompositions", "author": ["V. Chandrasekaran", "S. Sanghavi", "P. Parrilo", "A. Willsky"], "venue": "IFAC Symposium on System Identification.", "citeRegEx": "CSPW09", "shortCiteRegEx": null, "year": 2009}, {"title": "Cand\u00e8s and Terence Tao", "author": ["J Emmanuel"], "venue": "The power of convex relaxation: Near-optimal matrix completion,", "citeRegEx": "CT09", "shortCiteRegEx": null, "year": 2009}, {"title": "Arlington", "author": ["M. Fazel", "H. Hindi", "S. Boyd. A rank minimization heuristic with application to minimum order system approximation. In American Control Conference"], "venue": "Virginia.", "citeRegEx": "FHB01", "shortCiteRegEx": null, "year": 2001}, {"title": "Advances in linear matrix inequality methods in control: advances in design and control", "author": ["Karolos M. Grigoriadis", "Eric B. Beran. Alternating projection algorithms for linear matrix inequalities problems with rank constraints"], "venue": "pages 251\u2013267,", "citeRegEx": "GB00", "shortCiteRegEx": null, "year": 2000}, {"title": "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property", "author": ["Rahul Garg", "Rohit Khandekar"], "venue": "ICML.", "citeRegEx": "GK09", "shortCiteRegEx": null, "year": 2009}, {"title": "An accelerated gradient method for trace norm minimization", "author": ["Shuiwang Ji", "Jieping Ye"], "venue": "ICML.", "citeRegEx": "JY09", "shortCiteRegEx": null, "year": 2009}, {"title": "and Andrea Montanari", "author": ["Raghunandan H. Keshavan", "Sewoong Oh"], "venue": "Matrix completion from a few entries,", "citeRegEx": "KOM09", "shortCiteRegEx": null, "year": 2009}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Yehuda Koren"], "venue": "KDD, pages 426\u2013434.", "citeRegEx": "Kor08", "shortCiteRegEx": null, "year": 2008}, {"title": "In ICDM", "author": ["Yehuda Koren M. Bell. Scalable collaborative filtering with jointly derived neighborhood interpolation weights"], "venue": "pages 43\u201352.", "citeRegEx": "MB07", "shortCiteRegEx": null, "year": 2007}, {"title": "and L", "author": ["S. Ma", "D. Goldfarb"], "venue": "Chen. Fixed point and bregman iterative methods for matrix rank minimization,", "citeRegEx": "MGC09", "shortCiteRegEx": null, "year": 2009}, {"title": "In ICML", "author": ["Raghu Meka", "Prateek Jain", "Constantine Caramanis", "Inderjit S. Dhillon. Rank minimization via online learning"], "venue": "pages 656\u2013663.", "citeRegEx": "MJCD08", "shortCiteRegEx": null, "year": 2008}, {"title": "Parrilo", "author": ["Benjamin Recht", "Maryam Fazel", "Pablo A"], "venue": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization,", "citeRegEx": "RFP07", "shortCiteRegEx": null, "year": 2007}, {"title": "Taylor & Francis", "author": ["Robert E. Skelton", "T. Iwasaki", "K.M. Grigoriadis. A Unified Algebric Approach to Control Design"], "venue": "Inc., Bristol, PA, USA,", "citeRegEx": "SIG97", "shortCiteRegEx": null, "year": 1997}, {"title": "and Y", "author": ["J. Wright", "A. Ganesh", "S. Rao"], "venue": "Ma. Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization,", "citeRegEx": "WGRM09", "shortCiteRegEx": null, "year": 2009}, {"title": "Bernstein inequalities (probability theory) \u2014 wikipedia", "author": ["Wikipedia"], "venue": "the free encyclopedia,", "citeRegEx": "Wik09", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 17, "context": "Our results improve upon a recent breakthrough by Recht, Fazel and Parillo [RFP07] in three significant ways: 1) our method (SVP) is significantly simpler to analyse and easier to implement, 2) we give geometric convergence guarantees for SVP and, as demonstrated empiricially, SVP is significantly faster on real-world and synthetic problems, 3) we give optimality and geometric convergence guarantees even for the noisy version of ARMP.", "startOffset": 75, "endOffset": 82}, {"referenceID": 16, "context": "Unfortunately, ARMP is NP-hard in general and is also NP-hard to approximate ([MJCD08]).", "startOffset": 78, "endOffset": 86}, {"referenceID": 9, "context": "The most commonly used heuristic for the problem is to assume a factorization of X and optimize the resulting non-convex problem by alternating minimization [Bra03, Kor08, MB07], alternative projections [GB00] or alternating LMIs [SIG97].", "startOffset": 203, "endOffset": 209}, {"referenceID": 18, "context": "The most commonly used heuristic for the problem is to assume a factorization of X and optimize the resulting non-convex problem by alternating minimization [Bra03, Kor08, MB07], alternative projections [GB00] or alternating LMIs [SIG97].", "startOffset": 230, "endOffset": 237}, {"referenceID": 8, "context": "Another common approach is to relax the rank constraint to a convex function such as the trace-norm or the log determinant [FHB01], [FHB03].", "startOffset": 123, "endOffset": 130}, {"referenceID": 16, "context": "[MJCD08] proposed online learning based methods for ARMP.", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "In a recent breakthrough, Recht, Fazel and Parillo [RFP07] obtained the first non-trivial exactrecovery results for ARMP obtaining guaranteed rank minimization for affine transformations A that satisfy a restricted isometry property (RIP).", "startOffset": 51, "endOffset": 58}, {"referenceID": 1, "context": "Our analysis of SVP is motivated by the recent work in the field of conpressed sensing by Blumensath and Davies [BD09], Garg and Khandekar [GK09].", "startOffset": 112, "endOffset": 118}, {"referenceID": 10, "context": "Our analysis of SVP is motivated by the recent work in the field of conpressed sensing by Blumensath and Davies [BD09], Garg and Khandekar [GK09].", "startOffset": 139, "endOffset": 145}, {"referenceID": 17, "context": "[RFP07]: we only require \u03b42k < 1/3 as opposed to \u03b45k < 1/10 required by Recht et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[RFP07] only address the case of exact measurements.", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "Recently, Candes and Recht [CR08], Candes and Tao [CT09] and Keshavan et al.", "startOffset": 27, "endOffset": 33}, {"referenceID": 7, "context": "Recently, Candes and Recht [CR08], Candes and Tao [CT09] and Keshavan et al.", "startOffset": 50, "endOffset": 56}, {"referenceID": 12, "context": "[KOM09] obtained the first non-trivial results for low-rank matrix completion under a few additional assumptions.", "startOffset": 0, "endOffset": 7}, {"referenceID": 3, "context": "[CCS08], are quite expensive in practice and not very tolerant to noise.", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise.", "startOffset": 103, "endOffset": 109}, {"referenceID": 7, "context": "We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise.", "startOffset": 111, "endOffset": 117}, {"referenceID": 3, "context": "We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise.", "startOffset": 119, "endOffset": 126}, {"referenceID": 12, "context": "We demonstrate empirically that for a suitable step-size, SVP significantly outperforms the methods of [CR08], [CT09], [CCS08], [KOM09] in accuracy, computational time and tolerance to noise.", "startOffset": 128, "endOffset": 135}, {"referenceID": 7, "context": "Furthermore, our experiments strongly suggest (see Figure 1) that guarantees similar to those of [CT09], [KOM09] hold for SVP, achieving exact recovery for incoherent matrices from an almost optimal number of entries1.", "startOffset": 97, "endOffset": 103}, {"referenceID": 12, "context": "Furthermore, our experiments strongly suggest (see Figure 1) that guarantees similar to those of [CT09], [KOM09] hold for SVP, achieving exact recovery for incoherent matrices from an almost optimal number of entries1.", "startOffset": 105, "endOffset": 112}, {"referenceID": 17, "context": "[RFP07] do not directly apply to MCP.", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "The first non-trivial results for MCP were obtained recently by Candes and Recht [CR08], Keshavan et al.", "startOffset": 81, "endOffset": 87}, {"referenceID": 12, "context": "[KOM09] and Candes and Tao [CT09].", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[KOM09] and Candes and Tao [CT09].", "startOffset": 27, "endOffset": 33}, {"referenceID": 20, "context": "We need Bernstein\u2019s inequality [Wik09] stated below.", "startOffset": 31, "endOffset": 38}, {"referenceID": 3, "context": "[CCS08] proposed a Singular Value Thresholding (SVT) algorithm which is based on Uzawa\u2019s algorithm[AHU58].", "startOffset": 0, "endOffset": 7}, {"referenceID": 0, "context": "[CCS08] proposed a Singular Value Thresholding (SVT) algorithm which is based on Uzawa\u2019s algorithm[AHU58].", "startOffset": 98, "endOffset": 105}, {"referenceID": 15, "context": "[MGC09].", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "Toh and Yun [TY09], Ji and Ye [JY09] proposed Nesterov\u2019s projected gradient based methods for optimizing the trace-norm.", "startOffset": 30, "endOffset": 36}, {"referenceID": 4, "context": "Also, though minimizing the trace-norm does most likely approximate the low-rank solution even in the presence of noise (see [CP09] for instance), noise poses considerable computational challenges for trace-norm optimization.", "startOffset": 125, "endOffset": 131}, {"referenceID": 5, "context": "For the case of low-rank matrix completion, Candes and Recht [CR08] obtained the first nontrivial results for the problem obtaining guaranteed completion for incoherent matrices X\u2217 and randomly sampled entries \u03a9.", "startOffset": 61, "endOffset": 67}, {"referenceID": 7, "context": "Building on the work of Candes and Recht, Candes and Tao [CT09] obtained the near-optimal bound of |\u03a9| \u2265 min(C\u03bc4k2n log n,C\u03bc2kn log n) for exact-recovery via trace-norm minimization.", "startOffset": 57, "endOffset": 63}, {"referenceID": 3, "context": "However, the analysis of Candes and Recht, Candes and Tao is considerably complicated and minimizing trace-norm, even when using methods taylored for matrix-completion such as those of [CCS08], is relatively expensive in practice.", "startOffset": 185, "endOffset": 192}, {"referenceID": 12, "context": "[KOM09] obtained exact-recovery from uniformly sampled \u03a9 with |\u03a9| \u2265 C(\u03bc, k)n log n using different technqiues.", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[CSPW09] andWright et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[WGRM09].", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[WGRM09] show that the low-rank matrix completion problem can be reduced to the low-rank plus sparse decomposition problem.", "startOffset": 0, "endOffset": 8}, {"referenceID": 3, "context": "singular value thresholding (SVT) method [CCS08].", "startOffset": 41, "endOffset": 48}, {"referenceID": 3, "context": "Note that although [CCS08] presents SVT method in the context of matrix completion problem, however it can be easily adapted for ARMP.", "startOffset": 19, "endOffset": 26}, {"referenceID": 12, "context": "For matrix completion we compare against SVT, the spectral matrix completion (SMC) method of [KOM09], and regularized alternating least squares minimization (ALS).", "startOffset": 93, "endOffset": 100}, {"referenceID": 17, "context": "[RFP07], we use the MIT logo as the test image for reconstruction.", "startOffset": 0, "endOffset": 7}, {"referenceID": 12, "context": "[KOM09], rely on relaxing the rank constraint with trace-norm and give guarantees for recovering the optimal", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "Other directions include application of our methods to other problems of similar flavour such as the low-rank plus sparse matrix decomposition [CSPW09], or other matrix completion type problems like minimum dimensionality embedding using partial distance observations [FHB03] and low-rank kernel learning [MJCD08].", "startOffset": 143, "endOffset": 151}, {"referenceID": 16, "context": "Other directions include application of our methods to other problems of similar flavour such as the low-rank plus sparse matrix decomposition [CSPW09], or other matrix completion type problems like minimum dimensionality embedding using partial distance observations [FHB03] and low-rank kernel learning [MJCD08].", "startOffset": 305, "endOffset": 313}], "year": 2017, "abstractText": "Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization with affine constraints (ARMP) and show that SVP recovers the minimum rank solution for affine constraints that satisfy the restricted isometry property. We show robustness of our method to noise with a strong geometric convergence rate even for noisy measurements. Our results improve upon a recent breakthrough by Recht, Fazel and Parillo [RFP07] in three significant ways: 1) our method (SVP) is significantly simpler to analyse and easier to implement, 2) we give geometric convergence guarantees for SVP and, as demonstrated empiricially, SVP is significantly faster on real-world and synthetic problems, 3) we give optimality and geometric convergence guarantees even for the noisy version of ARMP. In addition, we address the practically important problem of low-rank matrix completion, which can be seen as a special case of ARMP. However, the affine constraints defining the matrix-completion problem do not obey the restricted isometry property in general. We empirically demonstrate that our algorithm recovers low-rank incoherent matrices from an almost optimal number of uniformly sampled entries. We make partial progress towards proving exact recovery and provide some intuition for the performance of SVP applied to matrix completion by showing a more restricted isometry property. Our algorithm outperforms existing methods, such as those of [RFP07, CR08, CT09, CCS08, KOM09], for ARMP and the matrix-completion problem by an order of magnitude and is also significantly more robust to noise.", "creator": "LaTeX with hyperref package"}}}