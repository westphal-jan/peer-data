{"id": "1001.1257", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jan-2010", "title": "Decisional Processes with Boolean Neural Network: the Emergence of Mental Schemes", "abstract": "Human decisional processes result from the employment of selected quantities of relevant information, generally synthesized from environmental incoming data and stored memories. Their main goal is the production of an appropriate and adaptive response to a cognitive or behavioral task. Different strategies of response production can be adopted, among which haphazard trials, formation of mental schemes and heuristics. In this paper, we propose a model of Boolean neural network that incorporates these strategies by recurring to global optimization strategies during the learning session. The model characterizes as well the passage from an unstructured/chaotic attractor neural network typical of data-driven processes to a faster one, forward-only and representative of schema-driven processes. Moreover, a simplified version of the Iowa Gambling Task (IGT) is introduced in order to test the model. Our results match with experimental data and point out some relevant knowledge coming from psychological domain. Moreover, the model incorporates the following data (see Supplementary Fig. 2). We apply the following to network design: We employ a single input-output model, a random-state state model, a random-state model and the domain-local data model. We use a random-state model for estimating the effect of an input on the outcome of the current task. This model is a single-output model, the least-parameter model. We use a random-state model for calculating the effect of a stimulus on the outcome of the current task. This model is a single-output model, the least-parameter model. We show that the general shape of the network is determined by the local stimulus (Fig. 2). A single-output model for estimating the effect of an input on the outcome of the current task (e.g., a random-state model) is presented in Figure 2. In Figure 2, the distribution of the state-local data model is derived from a random-state model, and can be analyzed with any given input. We show the total number of inputs of the random-state model to be distributed at a given time in each case. In contrast, the overall number of input models to be distributed at a given time is reduced to only one input. Finally, we show that all inputs are independent of the input model. In order to perform multiple experiments, we also use the following data for a regression-control model that is based on the input model (i.e., the one with the least input-output model and the one with the most input-output model). We first introduce the state", "histories": [["v1", "Fri, 8 Jan 2010 12:34:04 GMT  (104kb)", "http://arxiv.org/abs/1001.1257v1", "11 pages, 7 figures"]], "COMMENTS": "11 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["graziano barnabei", "franco bagnoli", "ciro conversano", "elena lensi"], "accepted": false, "id": "1001.1257"}, "pdf": {"name": "1001.1257.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Graziano Barnabei", "Franco Bagnoli", "Ciro Conversano", "Elena Lensi"], "emails": ["grbarnabei@gmail.com;", "franco.bagnoli@unifi.it;"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 1.\n12 57\nv1 [\ncs .A\nI] 8\nJ an\n2 01\n0"}, {"heading": "Decisional Processes with Boolean Neural Network: the Emergence of Mental Schemes \u2217", "text": ""}, {"heading": "Graziano Barnabei", "text": "Dept. of Systems and Computer Science, University of Florence, via S. Marta 3. 50139 Firenze, Italy; CSDC, Via G. Sansone, 1. 50019 Sesto F.no, Firenze, Italy.\nemail: grbarnabei@gmail.com;"}, {"heading": "Franco Bagnoli", "text": "Dept. of Energy, University of Florence, via S. Marta 3. 50139 Firenze, Italy;\nCSDC and INFN, sez. Firenze. email: franco.bagnoli@unifi.it;"}, {"heading": "Ciro Conversano, Elena Lensi", "text": "Dept. of Psychiatry, Neurobiology, Pharmacology and Biotechnology, University\nof Pisa, Via Roma, 67. 56126 Pisa, Italy.\nHuman decisional processes result from the employment of selected quantities of relevant information, generally synthesized from environmental incoming data and stored memories. Their main goal is the production of an appropriate and adaptive response to a cognitive or behavioral task. Different strategies of response production can be adopted, among which haphazard trials, formation of mental schemes and heuristics. In this paper, we propose a model of Boolean neural network that incorporates these strategies by recurring to global optimization strategies during the learning session. The model characterizes as well the passage from an unstructured/chaotic attractor neural network typical of data-driven processes to a faster one, forward-only and representative of schema-driven processes. Moreover, a simplified version of the Iowa Gambling Task (IGT) is introduced in order to test the model. Our results match with experimental data and point out some relevant knowledge coming from psychological domain.\nPACS numbers: 87.85.dq\n\u2217 Presented at Summer Solstice 2009 International Conference on Discrete Models of Complex Systems Gdansk, Poland, 22-24 June 2009\n(1)\n2"}, {"heading": "1. Introduction", "text": "Humans usually categorize incoming information into stable concepts which can be upgraded, related and nested one into another. The characteristics of information are analyzed and classified into (i.e. they activate) existing concepts but, whenever they would represent a novelty, they will induce the formation of a new concept or the upgrade of the existing ones. This adaptive modality of knowledge organization makes cognitive system able to classify, store and employ at best incoming information, in order to solve the eventual cognitive demand during next steps of processing. Subsequently, human cognitive or behavioral responses to a given set of inputs are built following several and different decisional strategies. The role of context, the kind of information and past experiences are central for the choice of what kind of decision making will be made. In general, we can take into consideration at least five strategies of output production:\n1) Reflexive responses: direct associations of inputs with an output pattern. They require no attentional resources and are out of possible controls. Typical examples are reflexive genetically implemented motor responses (e.g. the evade reflex) and associative behaviors (e.g. the Pavlov\u2019s dog salivation reflex).\n2) Automatic processes: standardized quick responses associated to a frequent activation of simple concepts while the behavioral relevance of the input/event is under an \u201calert red-line\u201d (i.e. it requires a behavioral response but not a direct attentional monitoring, for instance during the vehicle driving).\n3) Routine processes: processes triggered by several related concepts or complex events sufficiently frequent to constitute a stereotypical routine. Routines can be solved by a script [1, 2] and need the emergence of mental schemes [2, 3, 4], namely representations of complex concepts or events easily connectable to a fast cognitive or behavioral response. Note that even if the strategy requires an attentive control, it doesn\u2019t involve the same set of cognitive ability needed during a problem solving task, like the resolution of a syllogism or the Wason selection task [5].\n4) Reasoning: higher cognitive strategy of understanding and production, mainly used during the problem solving. Given a set of premises, humans seem to employ rules like those involved in formal logic [6], which establish the correct formal solution. The propositional reasoning makes no distinctions about the contents of a statement, but deals only with its syntactical structure. Unfortunately, human judgements are sometimes very far from correct formal solutions. For these reasons, a theory of mental models [7, 8] has been proposed, which claims that hypothetical-deductive reasoning have three stages of thought: an understanding of the premises\n3 which leads up to model construction, a formulation of provisional conclusions, a revision procedure that verifies if other models are possible. Errors occur because of working memory limitedness: the bigger is the number of models that we have to menage, the harder the problem becomes. So, errors are conclusions not rigorously verified.\n5) Heuristic behavior: modus operandi typical of situations in which there is either a lack of information, or different mental schemes run in conflict, or there is no time to reasoning, or the task is too difficult. In these cases, individuals adopt strategies more similar to an attempt rather than to the formal solution. Note that, in some cases, the use of heuristics is mandatory and constitutes a cognitive bias1. Most important heuristics in psychology are anchoring/adjustment, availability, and representativeness [10].\nWhile strategies 1-4 belong to a hierarchy of use and exploitation of cognitive resources, heuristics take place only after strategy 3, and if there are no conditions to apply the other ones or their application fails. Finally, it is possible to bring back the aforementioned considerations into the simple cognitive model reported in Fig. 1. Neglecting earlier input systems and concept stages, our main purpose is to formalize into a neural model the cognitive constructs of mental scheme and heuristic, showing how these can modify the production of a response. Besides we will propose a simplified version of the IGT [11, 12] in order to test our model.\nThe paper is organized as follow. In the next section we introduce the neural model, fourth section is devoted to model fitting, and in the final sections we show the main results of our simulations and discuss about future perspectives."}, {"heading": "2. The Model", "text": "The model assumes that cognitive activities during a task resolution can be represented as a Boolean neural network, whose nodes do not necessarily correspond to single biological neurons but rather to organized sets of neurons, named functional areas. We choose this formalization in order to remain within the framework of neural domain.\nThe basic computational entities, namely the formal neurons, are described by the following parameters:\n1. \u03c3: the internal and external state of activation \u2208 {-1, +1}\n2. b: the threshold \u2208 [0, 1]\n1 While the abstract version of the Wason selection task leads to a correct performance of 3.9%, the concrete one leads to a correct performance of 91%. These results are interpreted recurring to the availability heuristic applied on past experiences.\n4\n3. c: the connectivity degree, i.e. the number of afferent synapses of the neuron\nThe N bipolar neurons are linked by connections, named synapses, each bearing a weight \u2208 {-1, 0, +1}2. At time t, the i-th node computes incoming signals from afferent neurons and, at time t+ 1, produce a signal, i.e. fires, according to the following update law:\n\u03c3t+1i = sgn\n\n\nN \u2211\nj=1\nwij \u00b7 \u03c3 t i\nci \u2212 bi\n\n (1)\nwhere sgn(x) returns the sign of real number x, wij is the incoming weighted synapse of i-th neuron from the j-th one, and ci = \u2211\nj |Wij|. This formalization makes the adopted formal neuron similar to that of McCulloch & Pitts [9].\nDynamics: search of an asymptotic configuration. By generating an arbitrary ~\u03c3 and ~b, and a connection structure W with entries wij uniformly distributed in {-1, 0, +1}, we are able to define the start-\n2 The case 0-weight corresponds to the absence of link. We don\u2019t allow auto-synapses, or self-recurring links.\n5 ing configuration \u03b6t0 , composed by (W,~b, ~\u03c3t0)3, i.e. the initial condition of the dynamics. Neurons are synchronously updated by applying iteratively Eq. (1) for a sufficiently large time tmeas, the maximum convergence time allowed4. If W is asymmetric and according to its asymmetry degree, a periodical orbit of length l is reached after a transient \u03c4 , giving the asymptotic configuration \u03b6tmeas , composed by (W,~b, ~\u03c3tmeas). In the following, all the procedures will make use of this concept of asymptotic configuration and its eventual distance from the correct behavior. This is motivated from the assumption that only stable asymptotical configurations can account for stability and invariance of response typical of human cognitive processes. In principle, tc can be viewed as the time needed by the cognitive elaboration.\nTraining phase: a problem of global optimization. We choose a subset of n Boolean functions as possible instances of the training problem. For each \u03b3-th function we select N\u03b3I input neurons from which the remaining ones take entries for the update dynamics and just one, that is N\u03b3O = 1, output neuron from which, after the attainment of a \u03b6tmeas , we will measure the computed output.\nMoreover, for each \u03b3-th function we generate the corresponding training set \u03b5\u03b3 , for which all possible examples are given by coupling one of the inputs \u03be\u03b3,inp\u00b5 with the respective wanted output \u03be \u03b3,out \u00b5 , for \u00b5 = 1, ..., P \u03b3 . The presentation of the entire training set \u03b5 is consequently just one training epoch5. At the end of each epoch, the error signal E(\u03b6tmeas) will be:\n3 We have just introduced the vector notation for the matrix of synaptic weights W =\n(wij)ij , the state vector ~\u03c3 t = (\u03c3t1, . . . , \u03c3 t N ), the threshold vector ~b = (b1, . . . , bN ) and\nthe connectivity vector ~c = (c1, . . . , cN ). 4 In principle the longest convergence time tc should be 2\nN , the maximal periodical orbit of a finite size discrete system of N unities having 2 possible values. But for reasons of feasibility of simulations, fixing a reasonable tmax, tmeas will correspond to min{tc, tmax}.\n5 Consequently, given n functions to learn, the input vector I will have NI =\nn\u2211\n\u03b3=1\nN \u03b3 I\nelements, the output vector O will have NO =\nn\u2211\n\u03b3=1\nN \u03b3 O = n elements, and the size of\n\u03b5 will be P = n\u2211\n\u03b3=1\nP \u03b3 , where P \u03b3 = 2N \u03b3 I . Notably, the input vector I cannot change\nentries during dynamical update \u21d2 T becomes 2N\u2212NI . We notice that, since we do not need to test our network on the complementary subset of \u03b5\u03b3 , we can freely present, for each \u03b3-th instance of the problem, the entire correspondent training set \u03b5\u03b3 during the training phase.\n6 E =    \n  \n1\n4Plw\nN \u2211\ni=N\u2212NO+1\nP \u2211\n\u00b5=1\nlw \u2211\nj=1\n(\nSouti,\u00b5 \u2212 \u03be out i,\u00b5 )2 +\n\u03c4w \u2212 1\n10 if tc \u2264 tmax\n10 \u00b7NO otherwise\n(2)\nwhere Souti,\u00b5 and \u03be out i,\u00b5 are respectively the computed and the wanted output for the \u00b5-th pattern of \u03b56. The addiction of a term for transient will justify the passage from a computationally slow attractor neural network, prototypical of data-driven processes, to a faster and oriented one representative of schema-driven processes. Each asymptotic configuration having E = 0, is one of the possible configurations able to solve the n incoming instances of the problem and therefore the presented task.\nAs there aren\u2019t clues as the way \u03b6tmeas have to be modified, we choose to proceed by trial and error. Consequently, the learning problem can be associated with a problem of global optimization, where E is the object function to be minimized. The choice of the optimization strategy will produce different behaviors of the network during the learning phase and consequently, we will be able to associate them to different cognitive strategies of task resolution.\nHaphazard trials. No optimization strategy is applied. Starting from an arbitrary generated condition, a series of local perturbations are produced, by modifying just one entry either in W or b selected at random with uniformly distributed probability. For each perturbation, the corresponding E(\u03b6tmeas) is calculated. The resulting trend is a random walk of E. This non strategic behavior is directly affected by the N involved functional areas, namely by the size of the solution space. Consequently this strategy can need a very long time to reach the solution of the problem and to produce the correct response to the task.\nEmergence of mental schemes. The optimized asymptotical configuration must be able to store the n presented instances as mental schemes, which future activation will produce a fast and cognitively cheap response. We choose to formalize the emergence of mental schemes as a simulated annealing procedure with geometrical cooling ratio cl, fixed once for all at 0.6, and by using E as energy. By starting from the arbitrary generated \u03b6tmeas , a local perturbation is produced with same modalities of the haphazard trials. The resulting \u03b6\n\u2032tmeas differs in E of a value \u2206E from the previous one. For the acceptation of the new configuration, we refer to the Metropolis\n6 We remark that the first term in first case of Eq. (2) is just the normalized average Hamming distance generalized for the entire orbital length.\n7 algorithm. The new configuration is kept with probability:\np =\n{\n1 if \u2206E < 0 exp(\u2212\u2206E/T ) otherwise (3)\nwhere T will modulate the cooling schedule7. The perturbation procedure continues until E(\u03b6tmeas) = 0 for a reasonable number of epochs. At this point the system have inferred and stored the n instances. When a future behavioral situation will pose the same set of input, the stable reached configuration will be reactivated and, having respectively \u03c4 and E equals to zero and l equal to 1, it will produce one of the fastest and correct responses admissible by the task."}, {"heading": "3. Results", "text": "Figure 2 shows how the attainment of a configuration having E = 0 can depend on the size of the feasible region, in our model function of N . During a series of local perturbations all accepted, the greater is N the more difficult becomes the search of a configuration able to solve the task. Anyhow, this dependency is also found by applying the algorithms of global\na b\noptimization during the learning phase. Results of the optimization procedure are presented in Fig. 3. Comparing Fig. 3a with Fig. 2b, it is clear what happens to the slow dynamics during optimization. Having at the beginning a large temperature T , all moves can be accepted in spite of their respective error E, allowing the\n7 The starting T0 is fixed once for all at 5. Each 10 epochs we sample the acceptance probability ap. If ap = 0.5 \u00b1 0.2 then Tk+1 = cl \u00b7 Tk.\n8 passage among basins. By decreasing T , only moves that decrease the error E begin to be accepted, see Eq. (4), causing a more exhaustive exploration of the small-E of the basin up to the reaching of the global minimum. The dependence of E from n and N shown in Fig. 3b,c can be easily reported to the task difficulty, typically correlated with the number of instances of the learning problem and the number of involved functional areas.\na b c\nFigure 4 shows the passage from an initial unconstrained dynamics to an optimized one, ruled by the learning of a scheme. This transition also corresponds to a passage from a \u03b6t0 having one of possible \u03c4 and l (Fig. 4a), to a \u03b6tmeas having \u03c4 and l respectively equal to zero and one (Fig. 4b).\na b\n9"}, {"heading": "4. Testing the model", "text": "In this section we introduce a simplified version of IGT8 in order to test qualitatively the predictions of the model.\nThe task consists of trials where a subject must select a card, reporting both a term of winning and and a term of loss, from 4 decks (A\u2019, B\u2019, C\u2019 and D\u2019, respectively) of 60 cards. Main goal of the subject is to maximize its budget after a 100 trials session. The temporal series of decks are different; decks A\u2019 and B\u2019 promise strong winning in the short period but stronger losses in the long period, while C\u2019 and D\u2019, promising small winnings but also smaller losses, assure a better budget at the end of the task.\nFor our purpose, the task to perform by our network take in consideration only the two native decks B\u2019 (min = -2330; max = 170; mean = -62.5) and D\u2019 (min = -310; max = 95; mean = -31.25), which length is maintained to 60. While the choice of the first card is random, the following ones are given by the output of the optimized network, composed by 5 neurons, one of which is the input and one the output. At each trial Tr, the network computes all the Tr\u22121 previous trials as examples into a simulated annealing optimization step, using the previous computed choices as computed outputs Sout and the terms of winning or loss in the corresponding temporal series as wanted outputs \u03beout. This procedure assumes in psychological terms an infinite working memory, and explicitly makes use of the heuristic of availability; in absence of relevant information about the covered cards, humans tend to use available information stored from past trials. Once optimized, the output computed by the asymptotical configuration will become the choice of the new trial, and its value is registered for future choices.\nFigure 5 shows typical behavior of the network while performing the simplified IGT. The winnings early promised by B\u2019 prematurely influences the network response in favor of deck B\u2019 but, after the first severe losses, the functional E associated to B\u2019 becomes too large and the computed output switch in favor of D\u2019. From this moment on, almost all the perturbations to the asymptotical configuration \u03b6tmeas are rejected by the simulated annealing and the network quickly produce its choice trial by trial. It is interesting to point out that the transition from a regime distinguished by choices in B\u2019 to one distinguished by choice in D\u2019 happens approximately at the same time of humans [12, 13], implying that the time scales in which the mental model becomes effective are comparable between humans and our model.\nMoreover after the first great loss given by B\u2019, network tends to persevere with choices in the same deck, as both normal subjects and pathological\n8 The original IGT is a psychological task used into larger test batteries to study qualitatively behavior of normal subjects during simulated real-life decision making. In neuropsychological practice, it is administered to pathological gamblers.\n10\ngamblers. This strange behavior can be interpreted from a psychological point of view as a persistence of the use of the heuristics of anchoring and adjustment during earlier trials after the loss, while in our network is due to the fact that the error quotas related to B\u2019 and D\u2019 becomes comparable."}, {"heading": "5. Conclusion and future perspectives", "text": "We have presented preliminary results of application of a Boolean model of neural network to relevant cognitive strategies involved in decision making tasks. At moment only mental schemes have been studied. The choice of a such formalization is due to the possibilities that Boolean neural networks offers in terms of robustness, ease of simulation and easy generation of samples for data fitting.\nThe model appears to capture the most relevant psychological knowledge regarding the domain of application. By shifting from an unstructured slow attractor neural network to a quicker forward-only one, it hold in respect of learning studies about task complexity and the number of employed cognitive resources. As defined, mental schemes become fast and adaptive cognitive strategies of behavioral response.\nRegarding the model fitting, the behavior of our network on the simplified version of the IGT produces results qualitatively comparable with those of humans.\nFuture studies will be targeted to include into the model aspects regarding probabilistic and hypothetical-deductive reasoning, while future applications will take in consideration pathological gambling.\n11"}, {"heading": "Appendix A", "text": ""}, {"heading": "Implementation algorithm", "text": "The model above described has been implemented with Matlab R2007a. For sakes of space, the codes and the temporal series employed for the model fitting are available by directly contacting the authors."}], "references": [{"title": "Psychological status of the script concept", "author": ["R.P. Abelson"], "venue": "American Psychologist,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1981}, {"title": "Scripts, plans, goals and understanding", "author": ["R.C. Schank", "R.P. Abelson"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1977}, {"title": "Piaget\u2019s theory", "author": ["J. Piaget"], "venue": "In Carmichael\u2019s manual of child psychology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1970}, {"title": "Conceptual dependency: a theory of natural language understanding", "author": ["R.C. Schank"], "venue": "Cognitive Psychology,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1972}, {"title": "Reasoning. In New horizons in psychology", "author": ["P.C. Wason"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1966}, {"title": "Models of deduction. In Reasoning: representation and process in childreen and adults", "author": ["P.N. Jhonson-Laird"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1975}, {"title": "A logical calculus of the ideas immanent in nervous activity", "author": ["W. McCulloch", "W. Pitts"], "venue": "Bulletin of Mathematical Biophysics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1943}, {"title": "Judgment under Uncertainty: Heuristics and Biases", "author": ["A. Tversky", "D. Kahneman"], "venue": "Science, New Series,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1974}, {"title": "Insensitivity to future consequences following damage to human prefrontal cortex", "author": ["A. Bechara", "A.R. Damasio", "H. Damasio", "S.W. Anderson"], "venue": "Cognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "The Iowa Gambling Task and the somatic marker hypothesis: some questions and answers", "author": ["A. Bechara", "H. Damasio", "D. Tranel", "A.R. Damasio"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Routines can be solved by a script [1, 2] and need the emergence of mental schemes [2, 3, 4], namely representations of complex concepts or events easily connectable to a fast cognitive or behavioral response.", "startOffset": 35, "endOffset": 41}, {"referenceID": 1, "context": "Routines can be solved by a script [1, 2] and need the emergence of mental schemes [2, 3, 4], namely representations of complex concepts or events easily connectable to a fast cognitive or behavioral response.", "startOffset": 35, "endOffset": 41}, {"referenceID": 1, "context": "Routines can be solved by a script [1, 2] and need the emergence of mental schemes [2, 3, 4], namely representations of complex concepts or events easily connectable to a fast cognitive or behavioral response.", "startOffset": 83, "endOffset": 92}, {"referenceID": 2, "context": "Routines can be solved by a script [1, 2] and need the emergence of mental schemes [2, 3, 4], namely representations of complex concepts or events easily connectable to a fast cognitive or behavioral response.", "startOffset": 83, "endOffset": 92}, {"referenceID": 3, "context": "Routines can be solved by a script [1, 2] and need the emergence of mental schemes [2, 3, 4], namely representations of complex concepts or events easily connectable to a fast cognitive or behavioral response.", "startOffset": 83, "endOffset": 92}, {"referenceID": 4, "context": "Note that even if the strategy requires an attentive control, it doesn\u2019t involve the same set of cognitive ability needed during a problem solving task, like the resolution of a syllogism or the Wason selection task [5].", "startOffset": 216, "endOffset": 219}, {"referenceID": 5, "context": "Given a set of premises, humans seem to employ rules like those involved in formal logic [6], which establish the correct formal solution.", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "Most important heuristics in psychology are anchoring/adjustment, availability, and representativeness [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "Besides we will propose a simplified version of the IGT [11, 12] in order to test our model.", "startOffset": 56, "endOffset": 64}, {"referenceID": 0, "context": "b: the threshold \u2208 [0, 1] 1 While the abstract version of the Wason selection task leads to a correct performance of 3.", "startOffset": 19, "endOffset": 25}, {"referenceID": 6, "context": "This formalization makes the adopted formal neuron similar to that of McCulloch & Pitts [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "It is interesting to point out that the transition from a regime distinguished by choices in B\u2019 to one distinguished by choice in D\u2019 happens approximately at the same time of humans [12, 13], implying that the time scales in which the mental model becomes effective are comparable between humans and our model.", "startOffset": 182, "endOffset": 190}], "year": 2013, "abstractText": "Human decisional processes result from the employment of selected quantities of relevant information, generally synthesized from environmental incoming data and stored memories. Their main goal is the production of an appropriate and adaptive response to a cognitive or behavioral task. Different strategies of response production can be adopted, among which haphazard trials, formation of mental schemes and heuristics. In this paper, we propose a model of Boolean neural network that incorporates these strategies by recurring to global optimization strategies during the learning session. The model characterizes as well the passage from an unstructured/chaotic attractor neural network typical of data-driven processes to a faster one, forward-only and representative of schema-driven processes. Moreover, a simplified version of the Iowa Gambling Task (IGT) is introduced in order to test the model. Our results match with experimental data and point out some relevant knowledge coming from psychological domain.", "creator": "LaTeX with hyperref package"}}}