{"id": "1001.2605", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2010", "title": "An Explicit Nonlinear Mapping for Manifold Learning", "abstract": "Manifold learning is a hot research topic in the field of computer science and has many applications in the real world. A main drawback of manifold learning methods is, however, that there is no explicit mappings from the input data manifold to the output embedding. This prohibits the application of manifold learning methods in many practical problems such as classification and target detection. Previously, in order to provide explicit mappings for manifold learning methods, many methods have been proposed to get an approximate explicit representation mapping with the assumption that there exists a linear projection between the high-dimensional data samples and their low-dimensional embedding. However, this linearity assumption may be too restrictive. In this paper, an explicit nonlinear mapping is proposed for manifold learning, based on the assumption that there exists a polynomial mapping between the high-dimensional data samples and their low-dimensional representations. As far as we know, this is the first time that an explicit nonlinear mapping for manifold learning is given. In particular, we apply this to the method of Locally Linear Embedding (LLE) and derive an explicit nonlinear manifold learning algorithm, named Neighborhood Preserving Polynomial Embedding (NPPE). Experimental results on both synthetic and real-world data show that the proposed mapping is much more effective in preserving the local neighborhood information and the nonlinear geometry of the high-dimensional data samples than previous work. This is because of the fact that, since LLE does not map the global data samples on an array, the initial distribution of the local coordinates for the local map is a bit more flexible than the expected distribution in the sample.\n\n\n\nTo further illustrate, we want to introduce a method named Neighborhood Preserving Polynomial Embedding (NPPE) to our application to the data collection. The program will only have the following set of parameters:\n1) The mapping procedure is defined as follows:\n1) a variable map (a map), called the algorithm, to which the resulting data are generated.\n2) The mapping procedure is defined as follows:\n1) a parameter (a map), called the algorithm, to which the resulting data are generated.\n3) a parameters (a map), called the algorithm, to which the resulting data are generated.\n4) a parameter (a map), called the algorithm, to which the resulting data are generated.\n5) a parameter (a map), called the algorithm, to which the resulting data are generated.\nIf all the parameters are the", "histories": [["v1", "Fri, 15 Jan 2010 03:03:24 GMT  (1416kb)", "http://arxiv.org/abs/1001.2605v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["hong qiao", "peng zhang", "di wang", "bo zhang"], "accepted": false, "id": "1001.2605"}, "pdf": {"name": "1001.2605.pdf", "metadata": {"source": "CRF", "title": "An Explicit Nonlinear Mapping for Manifold Learning", "authors": ["Hong Qiao", "Bo Zhang"], "emails": ["hong.qiao@ia.ac.cn", "wangdi}@amss.ac.cn", "b.zhang@amt.ac.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 1.\n26 05\nv1 [\ncs .C\nV ]\n1 5\nJa n\n20 10\nIndex Terms\u2014Manifold learning, nonlinear dimensionality reduction, machine learning, data mining.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "MANIFOLD learning has drawn great interests sinceit was first proposed in 2000 ( [1], [2], [4]) as a promising nonlinear dimensionality reduction (NDR) method for high-dimensional data manifolds. Its basic assumption is that high-dimensional input data samples lie on or close to a low-dimensional smooth manifold embedded in the ambient Euclidean space. For example, by rotating the camera around the same object with fixed radius, images of the object can be viewed as a one-dimensional curve embedded in a high-dimensional Euclidean space, whose dimension equals to the number of pixels in the image. With the manifold assumption, manifold learning methods aim to extract the intrinsic degrees of freedom underlying the input highdimensional data samples, by preserving local or global geometric characteristics of the manifold from which\n\u2022 H. Qiao is with the Lab of Complex Systems and Intelligent Science, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. E-mail: hong.qiao@ia.ac.cn \u2022 P. Zhang and D. Wang are with the Graduate School and the Institute of Applied Mathematics, AMSS, Chinese Academy of Sciences, Beijing, 100190, China. E-mail: {zhangpeng, wangdi}@amss.ac.cn \u2022 B. Zhang is with the State Key Lab of Scientific and Engineering Computing and the Institute of Applied Mathematics, AMSS, Chinese Academy of Sciences, Beijing 100190, China. E-Mail: b.zhang@amt.ac.cn\nThe work of H. Qiao was supported in part by the National Natural Science Foundation (NNSF) of China under Grants 60675039 and 60621001 and by the Outstanding Youth Fund of the NNSF of China under Grant 60725310. The work of B. Zhang was supported in part by the 863 Program of China under Grant 2007AA04Z228, by the 973 Program of China under Grant 2007CB311002 and by the NNSF of China under Grant 90820007.\ndata samples are drawn. In recent years, various manifold learning algorithms have been proposed, such as locally linear embedding (LLE) [2], [3], ISOMAP [4], [5], Laplacian eigenmap (LE) [12], diffusion maps (DM) [14], local tangent space alignment (LTSA) [11], and Riemannian manifold learning [13]. They have achieved great success in finding meaningful low-dimensional embeddings for high-dimensional data manifolds. Meanwhile, manifold learning also has many important applications in real-world problems, such as human motion detection [17], human face recognition [18], classification and compressed expression of hyper-spectral imageries [19], dynamic shape and appearance classification [20], and visual tracking [21]\u2013[23]. However, a main drawback of the manifold learning methods is that they learn the low-dimensional representations of the high-dimensional input data samples implicitly. No explicit mapping relationship from the input data manifold to the output embedding can be obtained after the training process. Therefore, in order to obtain the low-dimensional representations of the new coming samples, the learning procedure, containing all previous samples and new samples as inputs, has to be repeatedly implemented. It is obvious that such a strategy is extremely time-consuming for sequentially arrived data, which greatly limits the application of the manifold learning methods to many practical problems, such as classification, target detection, visual tracking and detection. In order to address the issue of lacking explicit mappings, many linear projection based methods have been proposed for manifold learning by assuming that there exists a linear projection between the high-dimensional\n2 input data samples and their low-dimensional representations, such as Locality Preserving Projections (LPP) [24], [25], Neighborhood Preserving Embedding (NPE) [26], Neighborhood Preserving Projections (NPP) [27], Orthogonal Locality Preserving Projections (OLPP) [28], Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30], and Graph Embedding [31]. Although these methods have achieved their success in many problems, the linearity assumption may still be too restrictive.\nOn the other hand, several kernel-based methods have also been proposed to give nonlinear but implicit mappings for manifold learning (see, e.g. [32]\u2013[35]). These methods reformulate the manifold learning methods as kernel learning problems and then utilize the existing kernel extrapolation techniques to find the location of new data samples in the low-dimensional space. The mappings provided by the kernel-based methods are nonlinear and implicit. Furthermore, the performance of these methods depends on the choice of the kernel functions, and their computational complexity is extremely high for very large data sets.\nIn this paper, an explicit nonlinear mapping for manifold learning is proposed for the first time, based on the assumption that there exists a polynomial mapping from the high-dimensional input data samples to their lowdimensional representations. The proposed mapping has the following main features.\n1) The mapping is explicit, so it is straightforward to locate any new data samples in the lowdimensional space. This is different from the traditional manifold learning methods such as like LLE, LE, and ISOMAP [4] in which the mapping is implicit and it is not clear how new data samples can be embedded in the low-dimensional space. Compared with kernel-based mappings, the proposed mapping does not depend on the specific kernels in finding the low-dimensional representations of new data samples. 2) The mapping is nonlinear. In contrast to the linear projection-based methods which find a linear projection mapping from the input high-dimensional samples to their low-dimensional representations, the proposed mapping provides a nonlinear polynomial mapping from the input space to the reduced space. Clearly, it is more reasonable to use a polynomial mapping to handle with data samples lying on nonlinear manifolds. Meanwhile, our analysis and experiments show that the proposed mapping is of similar computational complexity with the linear projection-based methods.\nCombining this explicit nonlinear mapping with existing manifold learning methods (e.g. LLE, LE, Isomap) can give explicit manifold learning algorithms. In this paper, we concentrate on the LLE manifold learning method and propose an explicit nonlinear manifold learning algorithm called Neighborhood Preserving Polynomial Embedding (NPPE) algorithm. Experiments\non both synthetic and real-world data have been conducted to illustrate the validity and effectiveness of the proposed mapping. The remaining part of the paper is organized as follows. Section 2 gives a brief review of the existing manifold learning methods including those based on linear projections and kernel-based nonlinear mappings. Details of the explicit nonlinear mapping for manifold learning are presented in Section 3, whilst the NPPE algorithm is given in Section 4. In Section 5, experiments are conducted on both synthetic and real-world data sets to demonstrate the validity of the proposed algorithm. Conclusion is given in Section 6."}, {"heading": "2 RELATED WORKS", "text": "In this section, we briefly review existing manifold learning algorithms including those based on linear projections and out-of-sample nonlinear extensions for learned manifolds. For convenience of presentation, the main notations used in this paper are summarized in Table 1. Throughout this paper, all data samples are in the form of column vectors. Matrices are expressed using normal capital letters and data vectors are represented using lowercase letters. The superscript of a data vector is the index of its component."}, {"heading": "2.1 Manifold Learning Methods", "text": "According to the geometric characteristics which are preserved, existing manifold learning methods can be cast into two categories: local or global approaches. As local approaches, Locally Linear Embedding (LLE) [2], [3] preserves local reconstruction weights. Locally Multidimensional Scaling (LMDS) [9] preserves local pairwise Euclidean distances among data samples. Maximum Variance Unfolding (MVU) [10] also preserves pairwise Euclidean distances in each local neighborhood, but it maximizes the variance of the low-dimensional representations at the same time. Local Tangent Space\n3 Alignment (LTSA) [11] keeps the local tangent structure. Diffusion Maps [14] preserves local pairwise diffusion distances from high-dimensional data to the lowdimensional representations. Laplacian Eigenmap (LE) [12] preserves the local adjacency relationship.\nAs global approaches, Isometric Feature Mapping (ISOMAP) [4], [5] preserves the pairwise geodesic distances among the high-dimensional data samples and their low-dimensional representations. Hessian Eigenmaps (HLLE) [15] extends ISOMAP to more general cases where the set of intrinsic degrees of freedom may be non-convex. In Riemannian Manifold Learning (RML) [13], the coordinates of data samples in the tangential space are preserved to be their low-dimensional representations."}, {"heading": "2.2 Linear Projections for Manifold Learning", "text": "Manifold learning algorithms based on linear projections assume that there exists a linear projection which maps the high-dimensional samples into a low-dimensional space, that is,\nyi = U Txi, where U \u2208 R n\u00d7m, (1)\nwhere xi is a high-dimensional sample and yi is its low-dimensional representation. Denote by ui the i-th column of U . Then from a geometric point of view, data samples in Rn are projected into anm-dimensional linear subspace spanned by {ui} n i=1. The low-dimensional representation yi is the coordinate of xi in R m with respect to the basis {ui} n i=1."}, {"heading": "2.2.1 LPP", "text": "Locality Preserving Projections (LPP) [24], [25] provides a linear mapping for Laplacian Eigenmaps (LE), by applying (1) into the training procedure of LE. The LE method aims to train a set of low-dimensional representations Y which can best preserve the adjacency relationship among high-dimensional inputs X . If xi and xj are \u201cclose\u201d to each other, then yi and yj should also be so. This property is achieved by solving the following constrained optimization problem\nmin \u2211N\ni,j=1 Wij\u2016yi \u2212 yj\u2016\n2 L2\n(2)\ns. t. \u2211N\ni=1 Diyiy\nT i = Im , (3)\nwhere the penalty weights Wij are given by the heat kernel Wij = exp(\u2212\u2016xi \u2212 xj\u2016 2 2/t) and Di = \u2211 j Wij .\nIn LPP, equation (1) is applied to (2) and (3), that is, each xi is replaced with U\nT yi. By a straightforward algebraic calculation, equations (2) and (3) are transformed into\nmin Tr(UTXLXTU) (4)\ns. t. UTXDXTU = Im , (5)\nwhere W = (Wij), L = D \u2212 W and D is the diagonal matrix whose (i, i)-th entry is Di. This optimization problem leads to a generalized eigenvalue problem\nXLXTui = \u03bbiXDX Tui ,\nand the optimal solutions u1, u2, . . . , um are the eigenvectors corresponding to the m smallest eigenvalues. Once {ui} n i=1 are computed, the linear projection matrix provided by LPP is given by U = [u1 u2 \u00b7 \u00b7 \u00b7 um]. For any new data sample x from the high-dimensional space Rn, LPP finds its low-dimensional representation y simply by y = UTx."}, {"heading": "2.2.2 NPP and NPE", "text": "The linear projection mapping for Locally Linear Embedding (LLE) is independently provided by Neighborhood Preserving Embedding (NPE) [26] and Neighborhood Preserving Projections (NPP) [27]. Similarly to LPP, NPE and NPP apply the linear projection assumption (1) to the training process of LLE and reformulate the optimization problem in LLE as to compute the linear projection matrix. During the training procedure of LLE, a set of linear reconstruction weights {Wij} N i,j=1 are first computed by solving a convex optimization problem\nmin\nN\u2211\ni=1\n\u2016xi \u2212\nN\u2211\nj=1\nWijxj\u2016 2 2\ns. t. Wij = 0, if j 6\u2208 N(i) N\u2211\nj=1\nWij = 1 ,\nwhere N(i) is the index set of the k nearest neighbors of xi. Then LLE aims to preserve {Wij} N i,j=1 from X to Y . This is achieved by solving the following optimization problem\nmin\nN\u2211\ni=1\n\u2016yi \u2212\nN\u2211\nj=1\nWijyj\u2016 2 2 (6)\ns. t. 1\nN\nN\u2211\ni=1\nyiy T i = Im (7)\nIn NPE and NPP, the linear projection assumption (1) is used in the above optimization problem, so (6) and (7) become\nmin Tr(UTXMXTU) (8)\ns. t. UTXXTU = Im (9)\nwhere M = (IN \u2212 W ) T (IN \u2212 W ) with W = (Wij). The optimal solutions u1, u2, \u00b7 \u00b7 \u00b7 , um are the eigenvectors of the following generalized eigenvalue problem corresponding to the m smallest eigenvalues\nXMXTui = \u03bbiXX Tui .\n4 After finding the linear projection matrix U = [u1 u2 \u00b7 \u00b7 \u00b7 um], any new data sample x from the highdimensional space Rn can be easily mapped into the lower dimensional space Rm by y = UTx."}, {"heading": "2.2.3 OLPP and ONPP", "text": "Orthogonal Locality Preserving Projections (OLPP) [28] and Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30] are the same as LPP and NPE (or NPP), respectively, except that the linear projection matrix provided by LPP and NPE (or NPP) is restricted to be orthogonal. This is achieved by replacing the constraints (5) and (9) with UTU = Im. Then the optimization problems in OLPP and ONPP become\nOLPP: UOLPP = argmin UTU=Im\nTr(UTXLXTU) (10)\nONPP: UONPP = argmin UTU=Im\nTr(UTXMXTU) . (11)\nUnlike in the cases of LPP and NPE (or NPP), these two optimization problems lead to eigenvalue problems which are much easier to solve numerically than a generalized eigenvalue problem. The column vectors of UOLPP are given by the eigenvectors of XLX\nT corresponding to the m smallest eigenvalues. The same result holds for UONPP by replacing XLX\nT with XMXT . The reader is referred to [28] and [29], [30] for details of these two algorithms."}, {"heading": "2.3 Out-of-Sample Nonlinear Extensions for Manifold Learning", "text": "Besides linear projections for manifold learning, several out-of-sample nonlinear extensions are also proposed for manifold learning in order to get low-dimensional representations of unseen data samples from the learned manifold. These methods are based on kernel functions and extrapolation techniques. A common strategy taken by these methods is to reformulate manifold learning methods as kernel learning problems. Then extrapolation techniques are employed to find the location of new coming samples in the low-dimensional space from the learned manifold. Bengio et al. [32], [36] proposed a unified framework for extending LLE, ISOMAP and LE, in which these methods are seen as learning eigenfunctions of operators defined from data-dependent kernels. The data-dependent kernels are implicitly defined by LLE, ISOMAP LE and are used together with the Nystro\u0308m formula [38] to extrapolate the embedding of a manifold learned from finite training samples to new coming samples for LLE, ISOMAP and LE (see [32], [36]). Chin and Suter [35] investigated the equivalence between MVU and Kernel Principal Component Analysis (KPCA) [39], by which extending MVU to new samples is reduced to extending a kernel matrix. In their work [35], the kernel matrix is generated from an unknown kernel eigenfunction which is approximated using Gaussian basis functions. A framework was proposed in [33]\nfor efficient kernel extrapolation which is based on a matrix approximation theorem and an extension of the representer theorem. Under this framework, LLE was reformulated and the issue of extending LLE to new data samples was addressed in [33]."}, {"heading": "3 EXPLICIT NONLINEAR MAPPINGS FOR MANIFOLD LEARNING", "text": "In this section, we propose an explicit nonlinear mapping for manifold learning, based on the assumption that there is a polynomial mapping between the high-dimensional data samples and their lower dimensional representations. Precisely, given input samples x1, x2, . . . , xN and their low dimensional representations y1, y2, . . . , yN , we assume that there exists a polynomial mapping which maps X to Y , that is, the k-th component yki of yi is a polynomial of degree p with respect to xi in the following manner:\nyki = \u2211\nl1,l2,...,ln\u22650\n1\u2264l1+l2+\u00b7\u00b7\u00b7+ln\u2264p\nvlk(x 1 i ) l1(x2i ) l2 \u00b7 \u00b7 \u00b7 (xni ) ln , (12)\nwhere l1, l2, . . . , ln are all integers. The superscript l stands for the n-tuple indexing array (l1, l2, . . . , ln) and vk is the vector of polynomial coefficients which is defined by\nvk =\n\n        \nvlk|l1=p,l2=0,\u00b7\u00b7\u00b7 ln=0 vlk|l1=p\u22121,l2=1,\u00b7\u00b7\u00b7 ln=0\n... vlk|l1=1,l2=0,\u00b7\u00b7\u00b7 ln=0 ... vlk|l1=0,l2=0,\u00b7\u00b7\u00b7 ln=1\n\n         . (13)\nBy assuming the polynomial mapping relationship, we aim to find a polynomial approximation to the unknown mapping from the high-dimensional data samples into their low-dimensional embedding space. Compared with the linear projection assumption used previously, a polynomial mapping provides high-order approximation to the unknown nonlinear mapping and therefore is more accurate for data samples lying on nonlinear manifolds. In order to apply this explicit nonlinear mapping to manifold learning algorithms, we need two definitions from matrix analysis [40]. Definition 3.1: The Kronecker product of an m \u00d7 n matrix A and a p\u00d7 q matrix B is defined as\nA\u2297B =\n\n  a11B \u00b7 \u00b7 \u00b7 a1nB ...\n... am1B \u00b7 \u00b7 \u00b7 amnB\n\n \nwhich is an mp\u00d7 nq matrix. Definition 3.2: The Hadamard product of two m \u00d7 n matrices A and B is defined as\nA\u2299 B =\n\n  a11b11 \u00b7 \u00b7 \u00b7 a1nb1n ...\n... am1bm1 \u00b7 \u00b7 \u00b7 amnbmn\n\n \n5 Recently, it was proved in [31] that most manifold learning methods, including LLE, LE, and ISOMAP, can be cast into the framework of spectral embedding. Under this framework, finding the low-dimensional embedding representations of the high-dimensional data samples is reduced to solving the following optimization problem\nmin yi\n1\n2\nN\u2211\ni,j=1\nWij\u2016yi \u2212 yj\u2016 2 2 (14)\ns. t.\nN\u2211\ni=1\nDiyiy T i = Im (15)\nwhere Wij , i, j = 1, 2, . . . , N , are positive weights which can be defined by using the input data samples andDi = \u2211N\nj=1 Wij . Applying the polynomial assumption (12) to the above general model of manifold learning gives a general manifold learning algorithm with an explicit nonlinear mapping. Denote (x1i ) l1(x2i ) l2 \u00b7 \u00b7 \u00b7 (xni )\nln by xli and substitute (12) into (14). Then the objective function becomes\n1\n2\n\u2211\ni,j\nWij \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225    \u2211 l vl1x l i ... \u2211\nl vlmx l i\n\n \u2212\n\n \n\u2211\nl vl1x l j\n... \u2211\nl vlmx l j\n\n  \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 \u2225 2\n2\n= 1\n2\n\u2211\ni,j\nWij \u2211\nk\n(( \u2211\nl\nvlkx l i\n)\n\u2212\n( \u2211\nl\nvlkx l j\n))2\n= \u2211\ni,j\nWij \u2211\nk\n\n\n( \u2211\nl\nvlkx l i\n)2\n\u2212\n( \u2211\nl\nvlkx l i\n) ( \u2211\nl\nvlkx l j\n))\n= \u2211\nk\n  \u2211\ni\n( \u2211\nl\nvlkx l i\n)\n \u2211\nj\nWij\n\n\n( \u2211\nl\nvlkx l i\n)\n\n\u2212 \u2211\nk\n  \u2211\ni,j\n( \u2211\nl\nvlkx l i\n)\nWij\n( \u2211\nl\nvlkx l j\n)\n\n= \u2211\nk\n( \u2211\ni\n( \u2211\nl\nvlkx l i\n)\nDi\n( \u2211\nl\nvlkx l i\n))\n\u2212 \u2211\nk\n  \u2211\ni,j\n( \u2211\nl\nvlkx l i\n)\nWij\n( \u2211\nl\nvlkx l j\n)\n (16)\nSubstitute (12) into (15), so the constraint is transformed into\n\u2211\ni\nDi\n\n \n\u2211\nl vl1x l i\n... \u2211\nl vlmx l i\n\n \n( \u2211\nl\nvl1x l i \u00b7 \u00b7 \u00b7 \u2211\nl\nvlmx l i\n)\n= Im\nThis is equivalent to\n\u2211\ni\nDi\n( \u2211\nl\nvljx l i\n)( \u2211\nl\nvlkx l i\n)\n= \u03b4jk (17)\nwhere \u03b4jk = 1 for j = k and = 0 otherwise.\nIn order to simplify (16) and (17), we define X (i) p by\nX(i)p =\n\n     \np \ufe37 \ufe38\ufe38 \ufe37 xi \u2297 xi \u2297 \u00b7 \u00b7 \u00b7 \u2297 xi ...\nxi \u2297 xi xi\n\n      . (18)\nThen \u2211\nl vlkx l i = v T k X (i) p , so (16) and (17) are reduced,\nrespectively, to\nmin vk\n\u2211\nk\nvTk\n{ \u2211\ni\nX(i)p Di(X (i) p ) T\n\u2212 \u2211\nij\nX(i)p Wij(X (j) p ) T\n   vk (19)\ns. t. vTj\n{ \u2211\ni\nX(i)p Di(X (i) p ) T\n}\nvk = \u03b4jk (20)\nBy writing Xp = [X (1) p X (2) p \u00b7 \u00b7 \u00b7 X (N) p ], (19) and (20) can be further simplified to\nmin vk\n\u2211\nk\nvTk XpWXpvk (21)\ns. t. vTj XpDXpvk = \u03b4jk , (22)\nwhere W = (Wij) and D is a diagonal matrix whose i-th diagonal entry is Di.\nBy the Rayleigh-Ritz Theorem [40], the optimal solutions vk, k = 1, 2, . . . ,m, are the eigenvectors of the following generalized eigenvalue problem corresponding to the m smallest eigenvalues\nXp(D\u2212W )X T p vi = \u03bbXpDX T p vi, v T i XpDX T p vj = \u03b4ij (23)\nOnce vk, k = 1, 2, . . . ,m, are computed, the explicit nonlinear mapping from the high-dimensional data samples to the low-dimensional embedding space Rm can be given as\ny =\n\n \n\u2211\nl vl1(x 1)l1(x2)l2 \u00b7 \u00b7 \u00b7 (xn)ln\n... \u2211\nl vlm(x 1)l1(x2)l2 \u00b7 \u00b7 \u00b7 (xn)ln\n\n  , (24)\nwhere x is a high-dimensional data sample and y is its low-dimensional representation. For a new coming sample xnew , its location in the low-dimensional embedding manifold can be simply obtained by\nynew = (v T 1 X (new) p , v T 2 X (new) p , \u00b7 \u00b7 \u00b7 , v T mX (new) p ) T , (25)\nwhere X (new) p is defined in the same way as in (18).\nIn the next section, we will make use of a similar method as in LLE to define the weights Wij , i, j = 1, 2, . . . , N, so that the geometry of the neighborhood of each data point can be captured.\n6"}, {"heading": "4 NEIGHBORHOOD PRESERVING POLYNOMIAL EMBEDDING", "text": "In this section, we propose a newmanifold learning algorithm with an explicit nonlinear mapping, named Neighborhood Preserving Polynomial Embedding (NPPE), which is obtained by defining the weights Wij , i, j = 1, 2, . . . , N, in a way similar to the LLE method and combining them with the explicit nonlinear mapping as in the preceding Section 3."}, {"heading": "4.1 NPPE", "text": "Consider a data set {x1, x2, . . . , xN} from the highdimensional space Rn. NPPE starts with finding a set of linear reconstruction weights which can best reconstruct each data point xi by its k-nearest neighbors (k-NNs). This step is identical with that of LLE [2], [3]. The weights Rij , i, j = 1, 2, . . . , N , which are defined to be nonzero only if xj is among the k-NNs of xi, are computed by solving the following optimization problem\nRij = argmin\u2211 N j=1 Rij=1\nN\u2211\ni=1\n\u2016xi \u2212 N\u2211\nj=1\nRijxj\u2016 2 2 . (26)\nThe weights Rij represent the linear coefficients for reconstructing the sample xi from its neighbors {xj}, whilst the constraint \u2211N\nj=1 Rij = 1 means that xi is approximated by a convex combination of its neighbors. The weight matrix, R = (Rij), has a closed-form solution given by\nri = G\u22121e\neTG\u22121e , (27)\nwhere ri is a column vector formed by the k non-zero entries in the i-th row of R and e is a column vector of all ones. The (j, l)-th entry of the k \u00d7 k matrix G is (xj\u2212xi)\nT (xl\u2212xi), where xj and xl are among the k-NNs of xi. NPPE aims to preserve the reconstruction weights Rij from the high-dimensional input data samples to their low-dimensional representations under the polynomial mapping assumption. This is achieved by solving the following optimization problem\nY = argmin \u2211\nN i=1 yiy T i =Im\nN\u2211\ni=1\n\u2016yi \u2212\nN\u2211\nj=1\nRijyj\u2016 2 2 , (28)\nwhere each yi satisfies (12). By a simple algebraic calculation, it can be shown that (28) is equivalent to (14) and (15) with\nWij = Rij +Rji \u2212\nN\u2211\nk=1\nRikRkj , and Di = 1 . (29)\nBy the result in Section 3, the explicit nonlinear mapping can be obtained by solving (23) and the low-dimensional representations Y of X can be computed by applying (24) to X . For a new coming sample xnew, its lowdimensional representation can be simply given by (25). We conclude this section by summarizing the NPPE algorithm in Algorithm 1.\nAlgorithm 1: The NPPE Algorithm\nInput: Data matrix X , the number k of nearest neighbors and the polynomial degree p. Output: Vectors of polynomial coefficients vi, i = 1, 2, . . . ,m. Compute Rij by (27). Compute W and D by (29). Generate Xp according to (18). Solve the generalized eigenvalue problem (23) to get vi, i = 1, 2, . . . ,m.\nAlgorithm 2: The Simplified NPPE Algorithm\nInput: Data matrix X , the number k of nearest neighbors and the polynomial degree p. Output: Vectors of polynomial coefficients vi, i = 1, 2, . . . ,m. Compute Rij by (27). Compute W and D by (29). Generate Xp according to (30). Solve the generalized eigenvalue problem (23) to get vi, i = 1, 2, . . . ,m."}, {"heading": "4.2 Computational Complexity and Simplified NPPE", "text": "In the training procedure of NPPE, the computational complexity of generating Xp is O(N \u2211p i=2 n\ni). Computing XpWX T p and XpDX T p takes O(kN 2 \u2211p i=1 n i) and O(N2 \u2211p\ni=1 n i) operations, respectively, since there are\nonly k non-zero entries in each column of W and D is a diagonal matrix. The computational complexity of the final eigen-decomposition is O(m(\n\u2211p i=1 n\ni)3), which is the most time-consuming step. In the procedure of locating new samples with NPPE, generatingX (new) p takesO( \u2211p i=2 n\ni) operations and computing ynew takes O(m( \u2211p i=1 n\ni)2) operations. From the above analysis, it can be seen that, as the polynomial order p increases, the overall computational complexity increases exponentially with p, which would be extremely time-consuming when the data dimension is very high. To address this issue, we simplify NPPE by removing the crosswise items. This is achieved by replacing the Kronecker product in (18) with the Hadamard product\nX(i)p =\n\n     \np \ufe37 \ufe38\ufe38 \ufe37 xi \u2299 xi \u2299 \u00b7 \u00b7 \u00b7 \u2299 xi ...\nxi \u2299 xi xi\n\n      . (30)\nWith this strategy, the computational complexity of generating Xp is reduced to O(np(p + 1)/2), whilst the computational complexity computing ynew is reduced to O(mn2p2). The Simplified NPPE (SNPPE) is summarized in Algorithm 2. Finally, the computational complexity of SNPPE, linear methods and kernel methods on computing ynew is\n7\nsummarized in Table 2. The computational complexity of different kernel methods varies. Here we only state the computational complexity of the common step of computing the inner products. It is obvious that the total complexity in computing ynew is not less than this value."}, {"heading": "4.3 Discussion", "text": "In this subsection, we briefly explain why NPPE or SNPPE has a better performance than its linear counterparts for nonlinearly distributed data sets. Let f = (f1, f2, \u00b7 \u00b7 \u00b7 , fm) be a nonlinear map from a manifold M \u2282 Rn to Rm such that yki = f k(xi), where fk is at least pth-order differentiable. For simplicity, and without loss of generality we may assume that 0 \u2208 M and that f(0) = 0. Then the Taylor expansion of fk(x) at zero is given by\nfk(x) = (\u2207fk(0))Tx+ 1\n2 xTHfk(0)x+ o(\u2016x\u2016 2) , (31)\nwhere \u2207fk and Hfk are the gradient and Hessian of fk, respectively. From (31), it can be seen that the linear methods only use the first-order approximation provided by \u2207fk(0) to approximate the nonlinear mapping fk(x), while the proposed polynomial mapping contains the extra high-order terms. Therefore, the explicit nonlinear mapping based on the polynomial assumption gives a better approximation to the true nonlinear mapping f than the explicit linear one."}, {"heading": "5 EXPERIMENTAL TESTS", "text": "In this section, experiments on both synthetic and real world data sets are conducted to illustrate the validity and effectiveness of the proposed NPPE algorithm. In Section 5.1, NPPE is tested on recovering geometric structures of surfaces embedded in R3. In Section 5.2, NPPE is applied to locating new coming data samples in the learned low-dimensional space. In Section 5.3, NPPE is used to extract intrinsic degrees of freedom underlying two image manifolds. In the experiments, the simplified version of NPPE is implemented and compared with NPP [27] and ONPP [30] (which apply the linear and orthogonal linear projection mapping to the training procedure for LLE, respectively) as well as the kernel extrapolation (KE) method proposed in [33]. There are two parameters in the NPPE algorithm, the number k of nearest neighbors and the polynomial\ndegree p. k is usually set to be 1% of the number of training samples, and the experimental tests show that NPPE is stable around this number. The choice of p depends on the dimension m. When m is small, p can be large to make NPPE more accurate. When m is large, p should be small to make NPPE computationally efficient. Experiments show that NPPE with p = 2 is already accurate enough."}, {"heading": "5.1 Learning Surfaces in R3 with NPPE", "text": "In the first experiment, NPPE, NPP, ONPP and LLE are applied to the task of unfolding surfaces embedded in R3. The surfaces are the SwissRoll, SwissHole, and Gaussian, all of which are generated by the Matlab Demo available at http://www.math.umn.edu/\u223cwittman/mani/. On each manifold, 1000 data samples are randomly generated for training. The number of nearest neighbors is k = 10 and the polynomial degree p = 2. The experimental results are shown in Fig. 1. In each sub-figure, Z = [z1 z2 \u00b7 \u00b7 \u00b7 zN ] stands for the generating data such that xi = \u03c6(zi), where \u03c6 is the nonlinear mapping that embeds Z in R3. It can be seen from Fig. 1 that NPPE outperforms all the other three methods, even the LLE method itself. NPP and ONPP fail to unfold these nonlinear manifolds (except for ONPP on Gaussian). Furthermore, in order to estimate the similarity between the learned low-dimensional representations and the generating data, the residual variance [4] \u03c1(Y, Z) = 1\u2212R2(Y, Z) is computed, where R is the standard linear correlation coefficient taken over all entries of Y and Z . The lower \u03c1(Y, Z) is, the more similar Y and Z are. The estimation results are shown in Fig. 1(d). It can be seen that the embedding given by NPPE is the most similar one."}, {"heading": "5.2 Locating New Data Samples with NPPE", "text": "In the second experiment, we apply NPPE, NPP, ONPP and KE to locating new coming samples in the learned low-dimensional space. First, 2000 data samples which evenly distribute on the SwissRoll manifold are generated. Then 1000 samples are randomly selected as the training data to learn the mapping relationship from R\n3 to R2 by NPPE, NPP, ONPP and KE. The learned mappings are used to provide the low-dimensional representations for the rest 1000 samples. The time cost of computing the low-dimensional representations of the testing samples is also recorded. Experimental results are shown in Fig. 2. It can be seen that NPPE not only gives the best locating result but also has much lower time cost than KE. NPP and ONPP are faster for computation but fail to give the correct embedding result. The same experiment is also conducted on data samples randomly selected from SwissRoll. The results are shown in Fig. 3. NPPE also outperforms the other three methods. To further validate the performance of NPPE, we randomly generate 11000 samples on the SwissRoll\n8 manifold, 1000 for training and 10000 for testing. The experimental procedure is just the same as the preceding one. Time cost versus number of testing samples is shown in Fig. 4(a). The residual variances between the generating data of the testing samples and their lowdimensional representations given by the four methods, are illustrated in Fig. 4(b). The experimental results show that NPPE is more accurate than all the other three methods with a similar computational cost with NPP and ONPP. Note that, in all the above experiments, the time cost of KE is increasing linearly with the number of testing samples increasing, whilst that of NPP, ONPP and NPPE is almost the same with the increase of the number of testing samples."}, {"heading": "5.3 Learning Image Manifolds with NPPE", "text": "In the last experiment, NPPE is applied to extract intrinsic degrees of freedom underlying two image manifolds, the lleface [2] and usps-0.\nThe lleface consists of 1965 face images of the same person at resolution 28\u00d720, and the two intrinsic degrees of freedom underlying the face images are rotation of the head and facial emotion. We randomly select 1500 samples as the training data and 400 samples as the testing data. The number of nearest neighbors is set to be 15. The experimental results are shown in Fig. 5. The training and testing results are shown on the left and right columns, respectively, in Fig. 5. 100 training samples and 40 testing samples are randomly selected and attached to the learned embedding. It can be seen that NPPE and NPP have successfully recovered the underlying structure of lleface, while the result given by KE is not satisfactory. The rotation degree is not extracted by the learned embedding with KE. Time cost on locating new data samples by these three methods is shown in Fig. 7(a). The time cost of NPPE is higher than that of NPP but lower than that of KE, which supports the analysis of computational complexity in Section 4.2.\nThe usps-0 data set consists of 765 images of handwritten digit \u20180\u2019 at resolution 16 \u00d7 16, and the two underlying intrinsic degrees of freedom are the line width and the shape of \u20180\u2019. 600 samples are randomly selected as training data and 150 samples are chosen to be testing data. The number of nearest neighbors is set to be 5. Fig. 6 illustrates the experimental results. Training and testing results are shown on the left and right columns, respectively. 100 training samples and 20 testing samples are randomly selected and shown in the learned embedding. It can be seen that NPPE has successfully recovered the underlying structure, while it is hard to see the changes of line width and shape in the embedding given by KE and ONPP. Time cost on locating new data samples by these three methods is shown in Fig. 7(b). The time cost of NPPE is higher than ONPP but much lower than KE."}, {"heading": "6 CONCLUSION", "text": "In this paper, an explicit nonlinear mapping for manifold learning is proposed for the first time. Based on the assumption that there is a polynomial mapping from the high-dimensional input samples to their low-dimensional representations, an explicit polynomial mapping is obtained by applying this assumption to a generic model of manifold learning. Furthermore, the NPPE algorithm is a nonlinear dimensionality reduction technique with a explicit nonlinear mapping, which tends to preserve not only the locality but also the nonlinear geometry of the high-dimensional data samples. NPPE can provide convincing embedding results and locate new coming data samples in the reduced lowdimensional space simply and quickly at the same time. Experimental tests on both synthetic and real-world data have validated the effectiveness of the proposed NPPE algorithm."}], "references": [{"title": "The manifold ways of perception,", "author": ["H.S. Seung", "D.D. Lee"], "venue": "Science, vol. 290,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding,", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Think globally, fit locally: unsupervised learning of low dimensional manifold,", "author": ["L.K. Saul", "S.T. Roweis"], "venue": "J. Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "A global geometric framework for nonlinear dimensionality reduction,", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science, vol. 290,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "Global versus local methods in nonlinear dimensionality reduction,", "author": ["V. de Silva", "J. Tenenbaum"], "venue": "Proc. Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Eigenfaces for recognition,", "author": ["M. Turk", "A. Pentland"], "venue": "J. Cognitive Neuroscience,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Multidimensional Scaling", "author": ["T. Cox", "M. Cox"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Alignment of overlapping locally scaled patches for multidimensional scaling and dimensionality reduction,", "author": ["L. Yang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Unsupervised learning of image manifolds by semidefinite programming,", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Principal manifolds and nonlinear dimension reduction via local tangent space alignment,", "author": ["Z. Zhang", "H. Zha"], "venue": "SIAM J. Sci. Comput.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation,", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput.", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Riemannian manifold learning,", "author": ["T. Lin", "H. Zha"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Hessian eigenmaps: new locally linear embedding techniques for high-dimensional data,", "author": ["D. Donoho", "C. Grimes"], "venue": "Proc. Nat. Acad. Sci.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Learning and matching of dynamics shape manifolds for human action recognition,", "author": ["L. Wang", "D. Suter"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Enhancing human face detection by resampling examples through manifolds,", "author": ["J. Chen", "R. Wang", "S. Yan", "S. Shan", "X. Chen", "W. Gao"], "venue": "IEEE Trans. Syst. Man Cybern. Part A,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Fusina, \u201cExploiting manifold geometry in hyperspectral imagery,", "author": ["C.M. Bachmann", "T.L. Ainsworth", "R.A"], "venue": "IEEE Trans. Geosci. Remote Sensing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Nonlinear manifold learning for dynamic shape and dynamic appearance,", "author": ["A. Elgammal", "C.S. Lee"], "venue": "Comput. Vis. Image Underst.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Learning object intrinsic structure for robust visual tracking,", "author": ["Q. Wang", "G. Xu", "H. Ai"], "venue": "in Proc. IEEE Int. Conf. Comput. Vis. Pattern Recog.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "Learning an intrinsic variable preserving manifold for dynamic visual tracking", "author": ["H. Qiao", "P. Zhang", "B. Zhang", "S. Zheng"], "venue": "IEEE. Trans. Syst. Man. Cybern. Part B, in press,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Tracking feature extraction based on manifold learning framework", "author": ["H. Qiao", "P. Zhang", "B. Zhang", "S. Zheng"], "venue": "J. Exp. Theor. Artif. Intell., in press,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Locality preserving projections", "author": ["X. He", "P. Niyogi"], "venue": "in Proc. Advances Neural Inf. Process. Syst.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Face recognition using Laplacianfaces,", "author": ["X. He", "S. Yan", "Y. Hu", "P. Niyogi", "H.J. Zhang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Neighborhood preserving embedding,", "author": ["X. He", "D. Cai", "S. Yan", "H.J. Zhang"], "venue": "in: Proc. IEEE Int. Conf. Comput. Vis.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Neighborhood preserving projections (NPP): A novel linear dimension reduction method", "author": ["Y. Pang", "L. Zhang", "Z. Liu", "N. Yu", "H. Li"], "venue": "in Proc. ICIC", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Orthogonal neighborhood preserving projections", "author": ["E. Kokiopoulou", "Y. Saad"], "venue": "Proc. Fifth IEEE Int\u2019l Conf. Data Mining,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Orthogonal neighborhood preserving projections: A projection-based dimensionality reduction technique", "author": ["E. Kokiopoulou", "Y. Saad"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Graph embedding and extensions: a general framework for dimensionality reduction,", "author": ["S. Yan", "D. Xu", "B.Y. Zhang", "H.J. Zhang", "Q. Yang", "S. Lin"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Out-of-sample extions for LLE, Isomap, MDS, Eigenmaps and spectral clustering,", "author": ["Y. Bengio", "J.F. Paiement", "P. Vincent", "O. Delalleau", "N.L. Roux", "M. Ouimet"], "venue": "in Proc. Advances Neural Inf. Process. Syst.,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Manifold regularization: a geometric framework for learning from labelled and unlabelled examples,", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Out-of-sample extrapolation of learned manifolds", "author": ["T. Chin", "D. Suter"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Incremental nonlinear dimensionality reduction by manifold learning", "author": ["M. Law", "A. Jain"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "The Numerical Treatment of Intergral Equations", "author": ["C. Baker"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1977}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION MANIFOLD learning has drawn great interests since it was first proposed in 2000 ( [1], [2], [4]) as a promising nonlinear dimensionality reduction (NDR) method for high-dimensional data manifolds.", "startOffset": 97, "endOffset": 100}, {"referenceID": 1, "context": "1 INTRODUCTION MANIFOLD learning has drawn great interests since it was first proposed in 2000 ( [1], [2], [4]) as a promising nonlinear dimensionality reduction (NDR) method for high-dimensional data manifolds.", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "1 INTRODUCTION MANIFOLD learning has drawn great interests since it was first proposed in 2000 ( [1], [2], [4]) as a promising nonlinear dimensionality reduction (NDR) method for high-dimensional data manifolds.", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "In recent years, various manifold learning algorithms have been proposed, such as locally linear embedding (LLE) [2], [3], ISOMAP [4], [5], Laplacian eigenmap (LE) [12], diffusion maps (DM) [14], local tangent space alignment (LTSA) [11], and Riemannian manifold learning [13].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "In recent years, various manifold learning algorithms have been proposed, such as locally linear embedding (LLE) [2], [3], ISOMAP [4], [5], Laplacian eigenmap (LE) [12], diffusion maps (DM) [14], local tangent space alignment (LTSA) [11], and Riemannian manifold learning [13].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "In recent years, various manifold learning algorithms have been proposed, such as locally linear embedding (LLE) [2], [3], ISOMAP [4], [5], Laplacian eigenmap (LE) [12], diffusion maps (DM) [14], local tangent space alignment (LTSA) [11], and Riemannian manifold learning [13].", "startOffset": 130, "endOffset": 133}, {"referenceID": 4, "context": "In recent years, various manifold learning algorithms have been proposed, such as locally linear embedding (LLE) [2], [3], ISOMAP [4], [5], Laplacian eigenmap (LE) [12], diffusion maps (DM) [14], local tangent space alignment (LTSA) [11], and Riemannian manifold learning [13].", "startOffset": 135, "endOffset": 138}, {"referenceID": 10, "context": "In recent years, various manifold learning algorithms have been proposed, such as locally linear embedding (LLE) [2], [3], ISOMAP [4], [5], Laplacian eigenmap (LE) [12], diffusion maps (DM) [14], local tangent space alignment (LTSA) [11], and Riemannian manifold learning [13].", "startOffset": 164, "endOffset": 168}, {"referenceID": 9, "context": "In recent years, various manifold learning algorithms have been proposed, such as locally linear embedding (LLE) [2], [3], ISOMAP [4], [5], Laplacian eigenmap (LE) [12], diffusion maps (DM) [14], local tangent space alignment (LTSA) [11], and Riemannian manifold learning [13].", "startOffset": 233, "endOffset": 237}, {"referenceID": 11, "context": "In recent years, various manifold learning algorithms have been proposed, such as locally linear embedding (LLE) [2], [3], ISOMAP [4], [5], Laplacian eigenmap (LE) [12], diffusion maps (DM) [14], local tangent space alignment (LTSA) [11], and Riemannian manifold learning [13].", "startOffset": 272, "endOffset": 276}, {"referenceID": 13, "context": "Meanwhile, manifold learning also has many important applications in real-world problems, such as human motion detection [17], human face recognition [18], classification and compressed expression of hyper-spectral imageries [19], dynamic shape and appearance classification [20], and visual tracking [21]\u2013[23].", "startOffset": 121, "endOffset": 125}, {"referenceID": 14, "context": "Meanwhile, manifold learning also has many important applications in real-world problems, such as human motion detection [17], human face recognition [18], classification and compressed expression of hyper-spectral imageries [19], dynamic shape and appearance classification [20], and visual tracking [21]\u2013[23].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "Meanwhile, manifold learning also has many important applications in real-world problems, such as human motion detection [17], human face recognition [18], classification and compressed expression of hyper-spectral imageries [19], dynamic shape and appearance classification [20], and visual tracking [21]\u2013[23].", "startOffset": 225, "endOffset": 229}, {"referenceID": 16, "context": "Meanwhile, manifold learning also has many important applications in real-world problems, such as human motion detection [17], human face recognition [18], classification and compressed expression of hyper-spectral imageries [19], dynamic shape and appearance classification [20], and visual tracking [21]\u2013[23].", "startOffset": 275, "endOffset": 279}, {"referenceID": 17, "context": "Meanwhile, manifold learning also has many important applications in real-world problems, such as human motion detection [17], human face recognition [18], classification and compressed expression of hyper-spectral imageries [19], dynamic shape and appearance classification [20], and visual tracking [21]\u2013[23].", "startOffset": 301, "endOffset": 305}, {"referenceID": 19, "context": "Meanwhile, manifold learning also has many important applications in real-world problems, such as human motion detection [17], human face recognition [18], classification and compressed expression of hyper-spectral imageries [19], dynamic shape and appearance classification [20], and visual tracking [21]\u2013[23].", "startOffset": 306, "endOffset": 310}, {"referenceID": 20, "context": "input data samples and their low-dimensional representations, such as Locality Preserving Projections (LPP) [24], [25], Neighborhood Preserving Embedding (NPE) [26], Neighborhood Preserving Projections (NPP) [27], Orthogonal Locality Preserving Projections (OLPP) [28], Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30], and Graph Embedding [31].", "startOffset": 108, "endOffset": 112}, {"referenceID": 21, "context": "input data samples and their low-dimensional representations, such as Locality Preserving Projections (LPP) [24], [25], Neighborhood Preserving Embedding (NPE) [26], Neighborhood Preserving Projections (NPP) [27], Orthogonal Locality Preserving Projections (OLPP) [28], Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30], and Graph Embedding [31].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "input data samples and their low-dimensional representations, such as Locality Preserving Projections (LPP) [24], [25], Neighborhood Preserving Embedding (NPE) [26], Neighborhood Preserving Projections (NPP) [27], Orthogonal Locality Preserving Projections (OLPP) [28], Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30], and Graph Embedding [31].", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "input data samples and their low-dimensional representations, such as Locality Preserving Projections (LPP) [24], [25], Neighborhood Preserving Embedding (NPE) [26], Neighborhood Preserving Projections (NPP) [27], Orthogonal Locality Preserving Projections (OLPP) [28], Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30], and Graph Embedding [31].", "startOffset": 208, "endOffset": 212}, {"referenceID": 24, "context": "input data samples and their low-dimensional representations, such as Locality Preserving Projections (LPP) [24], [25], Neighborhood Preserving Embedding (NPE) [26], Neighborhood Preserving Projections (NPP) [27], Orthogonal Locality Preserving Projections (OLPP) [28], Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30], and Graph Embedding [31].", "startOffset": 324, "endOffset": 328}, {"referenceID": 25, "context": "input data samples and their low-dimensional representations, such as Locality Preserving Projections (LPP) [24], [25], Neighborhood Preserving Embedding (NPE) [26], Neighborhood Preserving Projections (NPP) [27], Orthogonal Locality Preserving Projections (OLPP) [28], Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30], and Graph Embedding [31].", "startOffset": 330, "endOffset": 334}, {"referenceID": 26, "context": "input data samples and their low-dimensional representations, such as Locality Preserving Projections (LPP) [24], [25], Neighborhood Preserving Embedding (NPE) [26], Neighborhood Preserving Projections (NPP) [27], Orthogonal Locality Preserving Projections (OLPP) [28], Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30], and Graph Embedding [31].", "startOffset": 356, "endOffset": 360}, {"referenceID": 27, "context": "[32]\u2013[35]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[32]\u2013[35]).", "startOffset": 5, "endOffset": 9}, {"referenceID": 3, "context": "This is different from the traditional manifold learning methods such as like LLE, LE, and ISOMAP [4] in which the mapping is implicit and it is not clear how new data samples can be embedded in the low-dimensional space.", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "As local approaches, Locally Linear Embedding (LLE) [2], [3] preserves local reconstruction weights.", "startOffset": 52, "endOffset": 55}, {"referenceID": 2, "context": "As local approaches, Locally Linear Embedding (LLE) [2], [3] preserves local reconstruction weights.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Locally Multidimensional Scaling (LMDS) [9] preserves local pairwise Euclidean distances among data samples.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "Maximum Variance Unfolding (MVU) [10] also preserves pairwise Euclidean distances in each local neighborhood, but it maximizes the variance of the low-dimensional representations at the same time.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "Alignment (LTSA) [11] keeps the local tangent structure.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "Laplacian Eigenmap (LE) [12] preserves the local adjacency relationship.", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "As global approaches, Isometric Feature Mapping (ISOMAP) [4], [5] preserves the pairwise geodesic distances among the high-dimensional data samples and their low-dimensional representations.", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "As global approaches, Isometric Feature Mapping (ISOMAP) [4], [5] preserves the pairwise geodesic distances among the high-dimensional data samples and their low-dimensional representations.", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "Hessian Eigenmaps (HLLE) [15] extends ISOMAP to more general cases where the set of intrinsic degrees of freedom may be non-convex.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "In Riemannian Manifold Learning (RML) [13], the coordinates of data samples in the tangential space are preserved to be their low-dimensional representations.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "1 LPP Locality Preserving Projections (LPP) [24], [25] provides a linear mapping for Laplacian Eigenmaps (LE), by applying (1) into the training procedure of LE.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "1 LPP Locality Preserving Projections (LPP) [24], [25] provides a linear mapping for Laplacian Eigenmaps (LE), by applying (1) into the training procedure of LE.", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "2 NPP and NPE The linear projection mapping for Locally Linear Embedding (LLE) is independently provided by Neighborhood Preserving Embedding (NPE) [26] and Neighborhood Preserving Projections (NPP) [27].", "startOffset": 148, "endOffset": 152}, {"referenceID": 23, "context": "2 NPP and NPE The linear projection mapping for Locally Linear Embedding (LLE) is independently provided by Neighborhood Preserving Embedding (NPE) [26] and Neighborhood Preserving Projections (NPP) [27].", "startOffset": 199, "endOffset": 203}, {"referenceID": 24, "context": "3 OLPP and ONPP Orthogonal Locality Preserving Projections (OLPP) [28] and Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30] are the same as LPP and NPE (or NPP), respectively, except that the linear projection matrix provided by LPP and NPE (or NPP) is restricted to be orthogonal.", "startOffset": 129, "endOffset": 133}, {"referenceID": 25, "context": "3 OLPP and ONPP Orthogonal Locality Preserving Projections (OLPP) [28] and Orthogonal Neighborhood Preserving Projections (ONPP) [29], [30] are the same as LPP and NPE (or NPP), respectively, except that the linear projection matrix provided by LPP and NPE (or NPP) is restricted to be orthogonal.", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "The reader is referred to [28] and [29], [30] for details of these two algorithms.", "startOffset": 35, "endOffset": 39}, {"referenceID": 25, "context": "The reader is referred to [28] and [29], [30] for details of these two algorithms.", "startOffset": 41, "endOffset": 45}, {"referenceID": 27, "context": "[32], [36] proposed a unified framework for extending LLE, ISOMAP and LE, in which these methods are seen as learning eigenfunctions of operators defined from data-dependent kernels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "The data-dependent kernels are implicitly defined by LLE, ISOMAP LE and are used together with the Nystr\u00f6m formula [38] to extrapolate the embedding of a manifold learned from finite training samples to new coming samples for LLE, ISOMAP and LE (see [32], [36]).", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "The data-dependent kernels are implicitly defined by LLE, ISOMAP LE and are used together with the Nystr\u00f6m formula [38] to extrapolate the embedding of a manifold learned from finite training samples to new coming samples for LLE, ISOMAP and LE (see [32], [36]).", "startOffset": 250, "endOffset": 254}, {"referenceID": 29, "context": "Chin and Suter [35] investigated the equivalence between MVU and Kernel Principal Component Analysis (KPCA) [39], by which extending MVU to new samples is reduced to extending a kernel matrix.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "In their work [35], the kernel matrix is generated from an unknown kernel eigenfunction which is approximated using Gaussian basis functions.", "startOffset": 14, "endOffset": 18}, {"referenceID": 26, "context": "Recently, it was proved in [31] that most manifold learning methods, including LLE, LE, and ISOMAP, can be cast into the framework of spectral embedding.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "This step is identical with that of LLE [2], [3].", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "This step is identical with that of LLE [2], [3].", "startOffset": 45, "endOffset": 48}, {"referenceID": 23, "context": "In the experiments, the simplified version of NPPE is implemented and compared with NPP [27] and ONPP [30] (which apply the linear and orthogonal linear projection mapping to the training procedure for LLE, respectively) as well as the kernel extrapolation (KE) method proposed in [33].", "startOffset": 88, "endOffset": 92}, {"referenceID": 25, "context": "In the experiments, the simplified version of NPPE is implemented and compared with NPP [27] and ONPP [30] (which apply the linear and orthogonal linear projection mapping to the training procedure for LLE, respectively) as well as the kernel extrapolation (KE) method proposed in [33].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "Furthermore, in order to estimate the similarity between the learned low-dimensional representations and the generating data, the residual variance [4] \u03c1(Y, Z) = 1\u2212R(Y, Z) is computed, where R is the standard linear correlation coefficient taken over all entries of Y and Z .", "startOffset": 148, "endOffset": 151}], "year": 2010, "abstractText": "Manifold learning is a hot research topic in the field of computer science and has many applications in the real world. A main drawback of manifold learning methods is, however, that there is no explicit mappings from the input data manifold to the output embedding. This prohibits the application of manifold learning methods in many practical problems such as classification and target detection. Previously, in order to provide explicit mappings for manifold learning methods, many methods have been proposed to get an approximate explicit representation mapping with the assumption that there exists a linear projection between the high-dimensional data samples and their low-dimensional embedding. However, this linearity assumption may be too restrictive. In this paper, an explicit nonlinear mapping is proposed for manifold learning, based on the assumption that there exists a polynomial mapping between the highdimensional data samples and their low-dimensional representations. As far as we know, this is the first time that an explicit nonlinear mapping for manifold learning is given. In particular, we apply this to the method of Locally Linear Embedding (LLE) and derive an explicit nonlinear manifold learning algorithm, named Neighborhood Preserving Polynomial Embedding (NPPE). Experimental results on both synthetic and real-world data show that the proposed mapping is much more effective in preserving the local neighborhood information and the nonlinear geometry of the high-dimensional data samples than previous work.", "creator": "LaTeX with hyperref package"}}}