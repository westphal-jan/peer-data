{"id": "1002.2044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2010", "title": "On the Stability of Empirical Risk Minimization in the Presence of Multiple Risk Minimizers", "abstract": "Recently Kutin and Niyogi investigated several notions of algorithmic stability--a property of a learning map conceptually similar to continuity--showing that training-stability is sufficient for consistency of Empirical Risk Minimization while distribution-free CV-stability is necessary and sufficient for having finite VC-dimension. This paper concerns a phase transition in the training stability of ERM, conjectured by the same authors. Kutin and Niyogi proved that ERM on finite hypothesis spaces containing a unique risk minimizer has training stability that scales exponentially with sample size, and conjectured that the existence of multiple risk minimizers prevents even super-quadratic convergence. We prove this result for the strictly weaker notion of CV-stability, positively resolving the conjecture.", "histories": [["v1", "Wed, 10 Feb 2010 09:08:56 GMT  (102kb,DS)", "http://arxiv.org/abs/1002.2044v1", "4 pages"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["benjamin i p rubinstein", "aleksandr simma"], "accepted": false, "id": "1002.2044"}, "pdf": {"name": "1002.2044.pdf", "metadata": {"source": "CRF", "title": "On the Stability of Empirical Risk Minimization in the Presence of Multiple Risk Minimizers", "authors": ["Benjamin I. P. Rubinstein", "Aleksandr Simma"], "emails": ["benr@eecs.berkeley.edu).", "asimma@eecs.berkeley.edu)."], "sections": [{"heading": null, "text": "Index Terms\u2014empirical risk minimization, algorithmic stability, threshold phenomena\nI. INTRODUCTION\nDEVROYE and Wagner [3] first studied the effect ofalgorithmic stability on the statistical generalization of learning. Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6]. While the learnability of a concept class can now be characterized by the admission of a stable learner, finding natural definitions of stability with such properties is still open [5], [6]. As part of their general investigation into algorithmic stability in [5], Kutin and Niyogi observed that Empirical Risk Minimization (ERM) on hypothesis spaces of cardinality two experiences a phase transition in achievable rates of stability as the number of risk minimizers increases from one to two. Under a unique minimizer stability scales exponentially with sample size, while in the presence of multiple minimizers no rate faster than quadratic is possible. Kutin and Niyogi extended the unique risk minimizer result to finite spaces, and conjectured that ERM on a finite hypothesis space containing multiple risk minimizers is (0, \u03b4)-trainingstable for no \u03b4 = o ( m\u22121/2 ) [5, Conjecture 10.11]. We\npositively resolve this conjecture for weaker CV-stability and show furthermore that ERM on such spaces is (0, \u03b4)-CV-stable for \u03b4 = O ( m\u22121/2 ) .\nStudying the effects of multiple risk minimizers is well motivated by the practical consideration of feature selection. Learning from irrelevant features can lead to multiple risk\nManuscript received February 10, 2010. This work was supported by the Siebel Scholars Foundation and the National Science Foundation under grant DMS-0707060.\nThe authors are with the Computer Science Division, University of California, Berkeley (e-mail: {benr,asimma}@eecs.berkeley.edu).\nminimizers, for example when learning in a symmetric hypothesis class with positive minimum risk. Results showing poor generalization in such cases, as implied by slow rates of stability, help to justify the use of feature selection. It is also noteworthy that restricting focus to finite hypothesis classes is far from unreasonable\u2014practitioners often attempt to learn on extracted features, which frequently involves discretization, and when learning on a finite domain any hypothesis space becomes effectively finite."}, {"heading": "II. PRELIMINARIES", "text": "We follow the setting of [5] closely. X denotes the input space, Y = {\u22121, 1} the output space and Z = X \u00d7 Y their product example space. D is a distribution on Z . Unless stated otherwise we assume that m examples are drawn i.i.d. according to D. A classifier or hypothesis is a function h : X \u2192 {\u22121, 1};H denotes a set of classifiers or a hypothesis space. Whenever we refer to a finite H we assume, without loss of generality, that no h1, h2 \u2208 H exist such that h1 6= h2 and h1(X) = h2(X) a.s.\nA learning algorithm on hypothesis space H is a function A : \u22c3 m>0Zm \u2192 H. When A is understood from the context, we write fs = A(s) for s \u2208 Zm. The loss incurred by classifier h on example z \u2208 Z is denoted by `(h, z). We will assume the 0-1 loss throughout, where `(h, (x, y)) = 1 [h(x) 6= y]. Define the risk of h with respect to D as RD(h) = EZ\u223cD[`(h, Z)]; and with a slight abuse of notation the empirical risk as Rs(h) = m\u22121 \u2211m i=1 `(h, zi) for s = (z1, . . . , zm) \u2208 Zm. The risk minimizers of space H with respect to distribution D is the set H? = arg minh\u2208HRD(h). A learning algorithm A is said to implement Empirical Risk Minimization if A(s) \u2208 arg minh\u2208HRs(h) for s \u2208 Zm.\nBased on sequence s = (z1, . . . , zm) \u2208 Zm, index i \u2208 [m] and example u \u2208 Z , we define sequences si = (z1, . . . , zi\u22121, zi+1, . . . , zm) \u2208 Zm\u22121 and si,u = (z1, . . . , zi\u22121, u, zi+1, . . . , zm) \u2208 Zm. The following definitions capture the relevant notions of stability.\nDefinition 2.1: A learning algorithm A is weakly (\u03b2, \u03b4)hypothesis-stable or has weak hypothesis stability (\u03b2, \u03b4) if for any i \u2208 [m],\nPr(S,U)\u223cDm+1 ( max z\u2208Z |`(fS , z)\u2212 `(fSi,U , z)| \u2264 \u03b2 ) \u2265 1\u2212 \u03b4 .\nThe notion of weak hypothesis stability was first used by Devroye and Wagner in [3] under the name of stability. Kearns and Ron [4] then used the term hypothesis stability for the same concept.\nar X\niv :1\n00 2.\n20 44\nv1 [\ncs .L\nG ]\n1 0\nFe b\n20 10\n2 Definition 2.2: A learning algorithm A is (\u03b2, \u03b4)-crossvalidation-stable, or (\u03b2, \u03b4)-CV-stable, if, for any i \u2208 [m],\nPr(S,U)\u223cDm+1(|`(fS , U)\u2212 `(fSi,U , U)| \u2264 \u03b2) \u2265 1\u2212 \u03b4 .\nDefinition 2.3: A learning algorithm A is (\u03b2, \u03b4)-overlapstable, or has overlap stability (\u03b2, \u03b4), if, for any i \u2208 [m],\nPr(S,U)\u223cDm+1(|RSi(fS)\u2212 RSi(fSi,U )| \u2264 \u03b2) \u2265 1\u2212 \u03b4 .\nCombining CV and overlap stability, Kutin and Niyogi arrive at the next definition.\nDefinition 2.4: A learning algorithm A is (\u03b2, \u03b4)-trainingstable or, has training stability (\u03b2, \u03b4), if A has\ni. CV stability (\u03b2, \u03b4); and ii. Overlap stability (\u03b2, \u03b4).\nTrivially weak hypothesis stability (\u03b2, \u03b4) implies training stability (\u03b2, \u03b4). Kutin and Niyogi show that training stability is sufficient for good bounds on generalization error (for ERM, CV stability is sufficient). They also consider the stability of ERM on a two-classifier hypothesis class [5, Theorems 9.2 and 9.5], showing that the achievable rate on \u03b4 undergoes a phase transition as the number of risk minimizers increases from one to two.\nTheorem 2.5: Consider the class H consisting of the two constant classifiers mapping X to \u22121 and 1 respectively, and let A implement ERM on H (i.e., A outputs a majority label of training set S). Let p = Pr(X,Y )\u223cD(Y = 1). 1. If p \u2265 12 then A is (0, \u03b4)-training-stable for \u03b4(m) \u2248\n(2\u03c0m)\u22121/2. 2. If p > 12 then A is weakly (0, \u03b4)-hypothesis-stable for \u03b4 = exp ( \u2212 ( 2\u2212 p\u22121 )2 m/8 + O(1) ) .\n3. If p = 12 then A is not (\u03b2, \u03b4)-CV-stable, or (\u03b2, \u03b4)-overlapstable for any \u03b2 < 1 and any \u03b4 = o ( m\u22121/2 ) .\nThe case of finite hypothesis spaces is then considered. In particular the authors show that fast rates are achieved in the presence of a unique risk minimizer.\nTheorem 2.6: Let H be a finite collection of classifiers, and let A be a learning algorithm which performs ERM over H. Suppose there exists a unique risk minimizer h? \u2208 H, then A is weakly (0, \u03b4)-hypothesis-stable for \u03b4 = exp(\u2212\u2126(m)).\nThe authors then conjecture that the analogue of Theorem 2.5.(3) holds for general |H| < \u221e: [5, Conjecture 10.11] predicts that under any distribution D inducing multiple risk minimizers, ERM is not (0, \u03b4)-training-stable for any \u03b4 = o ( m\u22121/2 ) ."}, {"heading": "III. FINITE CLASSES WITH MULTIPLE RISK MINIMIZERS", "text": "Lemma 3.1: Let H be a finite hypothesis space with |H| \u2265 2 and with risk minimizers satisfying |H?| = 2. Then ERM on H with respect to the 0-1 loss, on a sample of m examples, is not (0, \u03b4)-CV-stable for any \u03b4 = o ( m\u22121/2 ) . Furthermore,\nERM on H is (0, \u03b4)-CV-stable for \u03b4 = O ( m\u22121/2 ) .\nProof: Let H? = {h1, h2}. We proceed by first showing that fS lies in H? with exponentially increasing probability and then that switching within H? occurs often. Let = minh\u2208H\\H? RD(h) \u2212 RD(h1), which exists and is positive by the finite cardinality of H. Then by the union bound,\nRD(h) \u2265 RD(h1) + for all h \u2208 H\\H?, and Chernoff\u2019s bound\nPr(fS \u2208 H?) \u2265 Pr(\u2200h \u2208 H\\H?,RS(h1) < RS(h)) \u2265 Pr(\u2200h \u2208 H\\H?,RS(h1) < RD(h1) + /2 \u2264 RS(h)) \u2265 1\u2212 Pr(RS(h1) \u2265 RD(h1) + /2) \u2212\n\u2211 h\u2208H\\H? Pr(RD(h1) + /2 > RS(h))\n\u2265 1\u2212 Pr(RS(h1) \u2265 RD(h1) + /2) \u2212 \u2211 h\u2208H\\H? Pr(RD(h)\u2212 /2 > RS(h))\n\u2265 1\u2212 exp ( \u2212 2m/2 ) \u2212 \u2211 h\u2208H\\H? exp ( \u2212 2m/2 ) = 1\u2212 (|H| \u2212 1) exp ( \u2212 2m/2 ) . (1)\nConsider ERM on H?. Without loss of generality assume that ERM on H?, when h1 and h2 have equal empirical risk, selects h1. Let Z1 = {z \u2208 Z | `(h1, z) < `(h2, z)} and Z2 be defined analogously. Let p = Pr(Z \u2208 Z1 \u222a Z2), which is positive by assumption that h\u2032(X) = h\u2032\u2032(X) a.s. implies h\u2032 = h\u2032\u2032. Then for all i \u2208 [m]\nPrD2((Zi, U) \u2208 (Z1 \u00d7Z2) \u222a (Z2 \u00d7Z1)) = p2/2 . (2)\nFor the moment, assume that for all i \u2208 [m] Pr ( |RSi(h1)\u2212 RSi(h2)| \u2264 (m\u2212 1)\u22121 ) = O ( m\u22121/2 ) . (3)\nConditioned on the events of (2) and (3), the probability of ERM on S outputting a different hypothesis than on Si,U is at least 1/2. This fact can be proved by cases on |RSi(h1) \u2212 RSi(h2) |, while assuming that (Zi, U) \u2208 (Z1 \u00d7Z2) \u222a (Z2 \u00d7Z1). If RSi(h1) = RSi(h2), then fS 6= fSi,U trivially. If |RSi(h1) \u2212 RSi(h2) | = (m \u2212 1)\u22121 then {|RSi,Z (h1)\u2212 RSi,Z (h2) | : Z \u2208 {Zi, U}} = { 0, 2m\u22121\n} and it follows that |RS(h1) \u2212 RS(h2) | 6= |RSi,U (h1) \u2212 RSi,U (h2) |; and since by symmetry the probability that RSi,Z (h2) < RSi,Z (h1) conditioned on the corresponding difference being 2m\u22121 is 1/2, it follows that fS 6= fSi,U with probability 1/2. Thus we have shown that on H?, for all i \u2208 [m]\nPr(S,U)\u223cDm+1(fS 6= fSi,U | A \u2229B) \u2265 1/2 , (4)\nwhere\nA = {(S,U) : (Zi, U) \u2208 (Z1 \u00d7Z2) \u222a (Z2 \u00d7Z2)} B = { (S,U) : |RSi(h1)\u2212 RSi(h2)| \u2264 (m\u2212 1)\u22121 } .\nNotice that since fS , fSi,U \u2286 H? and U \u2208 Z1 \u222a Z2, fS 6= fSi,U implies that `(fS , U) 6= `(fSi,U , U). Then together (1)\u2013 (4) lead to the following statement about ERM over H:\nPr(S,U)\u223cDm+1(|`(fS , U)\u2212 `(fSi,U , U)| > 0) \u2265 Pr(|`(fS , U)\u2212 `(fSi,U , U)| > 0, fS , fSi,U \u2208 H?) \u2265 O ( p2m\u22121/2 ) ( 1\u2212 2(|H| \u2212 1) exp ( \u2212 2m/2\n)) = O ( m\u22121/2 ) .\n3 And so ERM is not (0, \u03b4)-CV-stable for any \u03b4 = o ( m\u22121/2\n) as claimed. Furthermore ERM is (0, \u03b4)-CV-stable for \u03b4 = O ( m\u22121/2 ) since\nPr(S,U)\u223cDm+1(|`(fS , U)\u2212 `(fSi,U , U)| > 0) \u2264 Pr(|`(fS , U)\u2212 `(fSi,U , U)| > 0, fS , fSi,U \u2208 H?)\n+ Pr(fS /\u2208 H? \u2228 fSi,U /\u2208 H?) \u2264 O ( m\u22121/2 ) + 2(|H| \u2212 1) exp ( \u2212 2m/2 ) = O ( m\u22121/2 ) .\nAll that remains is to verify (3). Consider Xn,q \u223c Bin (n, q) for n \u2208 N, q \u2208 [0, 1] and note that for k \u2208 N \u222a {0}\nPr (\u2223\u2223\u2223\u2223Xk, 12 \u2212 k2 \u2223\u2223\u2223\u2223 \u2264 12 ) = Pr ( Xk, 12 = k 2 ) , k even\n2Pr ( Xk, 12 = k\u22121 2 ) , k odd\n\u2265 Pr ( X2b k2 c+1, 12 = \u230a k\n2\n\u230b) .\nSplitting on \u2211m\nj=1:j 6=i 1 [Zj \u2208 Z1 \u222a Z2]\u2014the Bin (m\u2212 1, p) number of examples in Si on which h1, h2 disagree\u2014and noting that when conditioned on this sum being equal to k, the random variable m\u22121 2 (RSi(h1)\u2212 RSi(h2)) + k 2 \u223c Bin (k, 1/2), leads to the lower-bound\nPr ( |RSi(h1)\u2212 RSi(h2)| \u2264 (m\u2212 1)\u22121 ) =\nm\u22121\u2211 k=0 Pr(Xm\u22121,p = k) Pr (\u2223\u2223\u2223\u2223Xk, 12 \u2212 k2 \u2223\u2223\u2223\u2223 \u2264 12 )\n\u2265 min 0\u2264k\u2264m\u22121 Pr (\u2223\u2223\u2223\u2223Xk, 12 \u2212 k2 \u2223\u2223\u2223\u2223 \u2264 12 )m\u22121\u2211 j=0 Pr(Xm\u22121,p = j)\n= min 0\u2264k\u2264m\u22121 Pr (\u2223\u2223\u2223\u2223Xk, 12 \u2212 k2 \u2223\u2223\u2223\u2223 \u2264 12 ) \u2265 Pr ( X2bm2 c+1, 12 = \u230am 2\n\u230b) = O ( m\u22121/2 ) ,\nwhere the last relation is a consequence of Stirling\u2019s approximation. Let c = dp(m\u2212 1)/2e. The upper-bound follows similarly:\nPr ( |RSi(h1)\u2212 RSi(h2)| \u2264 (m\u2212 1)\u22121 ) =\nc\u2211 k=0 Pr(Xm\u22121,p = k) Pr (\u2223\u2223\u2223\u2223Xk, 12 \u2212 k2 \u2223\u2223\u2223\u2223 \u2264 12 )\n+ m\u22121\u2211 k=c+1 Pr(Xm\u22121,p = k) Pr (\u2223\u2223\u2223\u2223Xk, 12 \u2212 k2 \u2223\u2223\u2223\u2223 \u2264 12 ) \u2264 Pr(Xm\u22121,p \u2264 c)\n+ sup k\u2208{c+1,...,m\u22121} Pr (\u2223\u2223\u2223\u2223Xk, 12 \u2212 k2 \u2223\u2223\u2223\u2223 \u2264 12 ) \u2264 O ( e\u2212pm ) + O ( m\u22121/2\n) = O ( m\u22121/2 ) ,\nwhere the penultimate relation follows from an application of Chernoff\u2019s inequality to the first term and Stirling\u2019s approximation to the second.\nTheorem 3.2: Let H be a finite hypothesis space with |H| \u2265 2 and with risk minimizers |H?| > 1. Then any learning algorithm A implementing ERM on H with respect to the 0-1 loss, on a sample of m examples, is not (0, \u03b4)-CV-stable for any \u03b4 = o ( m\u22121/2 ) . Furthermore, A is (0, \u03b4)-CV-stable for\n\u03b4 = O ( m\u22121/2 ) .\nProof: Define as before. Arbitrarily order the risk minimizersH? = {h1, . . . , hn} where n = |H?|, and let pij = Pr(hi(X) 6= hj(X)) > 0 by assumption that h\u2032(X) = h\u2032\u2032(X) a.s. implies h\u2032 = h\u2032\u2032 for each h\u2032, h\u2032\u2032 \u2208 H. Let f ijT be the result of running A on sample T in the hypothesis space {hi, hj}, where ties between empirical risk minimizers are broken as in ERM over H?. Let f?T be the result of running A on T in H?. It follows that for all {j, k} \u2286 [n], conditional upon event Cjk = {f?S = hj , f?Si,U = hk}, f jk S = f ? S = hj and f jk Si,U = f?Si,U = hk, and so by (2)\u2013(4)\nPr(S,U)\u223cDm+1(|`(f?S , U)\u2212 `(f?Si,U , U)| > 0 | Cjk) = Pr (\u2223\u2223\u2223`(f jkS , U)\u2212 `(f jkSi,U , U)\u2223\u2223\u2223 > 0 \u2223\u2223\u2223 Cjk)\n\u2265 O (\nmin {j,k}\u2286[n]\np2jkm \u22121/2\n) . (5)\nIt follows that\nPr(S,U)\u223cDm+1(|`(f?S , U)\u2212 `(f?Si,U , U)| > 0) \u2265 min {j,k}\u2286[n] Pr(|`(f?S , U)\u2212 `(f?Si,U , U)| > 0 | Cjk)\n\u2265 O (\nmin {j,k}\u2286[n]\np2jkm \u22121/2\n) . (6)\nObserving that Pr(fS \u2208 H?) \u2265 1\u2212|H| exp(\u2212 2m/2) by the same argument that lead to (1), we have as before\nPr(|`(fS , U)\u2212 `(fSi,U , U)| > 0) \u2265 Pr(|`(fS , U)\u2212 `(fSi,U , U)| > 0, fS , fSi,U \u2208 H?) \u2265 Pr(|`(f?S , U)\u2212 `(f?Si,U , U)| > 0) \u00d7 (1\u2212 Pr(fS /\u2208 H? \u2228 fSi,U /\u2208 H?))\n= O ( min\n{j,k}\u2286[n] p2jkm\n\u22121/2 )( 1\u2212 2|H| exp ( \u2212 2m/2 )) = O ( m\u22121/2 ) . (7)\nThis implies that ERM on H is not (0, \u03b4)-CV-stable for any \u03b4 = o ( m\u22121/2 ) . Similarly we derive an upper-bound analogous\nto (7) by the same technique used in the proof of Lemma 3.1:\nPr(|`(fS , U)\u2212 `(fSi,U , U)| > 0) \u2264 Pr(|`(fSi,U , U)\u2212 `(fS , U)| > 0, fS , fSi,U \u2208 H?)\n+ Pr(fS /\u2208 H? \u2228 fSi,U /\u2208 H?)\n= O ( max {j,k}\u2286[n] p2jkm \u22121/2 ) + 2|H| exp ( \u2212 2m/2 ) = O ( m\u22121/2 ) .\nThis implies that ERM on H is (0, \u03b4)-CV-stable for \u03b4 = O ( m\u22121/2 ) .\n4"}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Peter Bartlett for his helpful feedback on this research."}], "references": [{"title": "Theory of Classification: a Survey of Recent Advances", "author": ["S. Boucheron", "O. Bousquet", "G. Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Stability and Generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Distribution-free performance bounds for potential function rules", "author": ["L. Devroye", "T. Wagner"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1979}, {"title": "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation", "author": ["M. Kearns", "D. Ron"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Almost-Everywhere Algorithmic Stability and Generalization Error", "author": ["S. Kutin", "P. Niyogi"], "venue": "Technical report TR-2002-03,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Statistical Learning: stability is sufficient and generalization is necessary and sufficient for consistency of Empirical Risk Minimization", "author": ["S. Mukherjee", "P. Niyogi", "T. Poggio", "R. Rifkin"], "venue": "AI Memo", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}], "referenceMentions": [{"referenceID": 2, "context": "DEVROYE and Wagner [3] first studied the effect of algorithmic stability on the statistical generalization of learning.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 1, "context": "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6].", "startOffset": 188, "endOffset": 191}, {"referenceID": 5, "context": "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 4, "context": "While the learnability of a concept class can now be characterized by the admission of a stable learner, finding natural definitions of stability with such properties is still open [5], [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "While the learnability of a concept class can now be characterized by the admission of a stable learner, finding natural definitions of stability with such properties is still open [5], [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 4, "context": "As part of their general investigation into algorithmic stability in [5], Kutin and Niyogi observed that Empirical Risk Minimization (ERM) on hypothesis spaces of cardinality two experiences a phase transition in achievable rates of stability as the number of risk minimizers increases from one to two.", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "PRELIMINARIES We follow the setting of [5] closely.", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "The notion of weak hypothesis stability was first used by Devroye and Wagner in [3] under the name of stability.", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "Kearns and Ron [4] then used the term hypothesis stability for the same concept.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "Consider Xn,q \u223c Bin (n, q) for n \u2208 N, q \u2208 [0, 1] and note that for k \u2208 N \u222a {0}", "startOffset": 42, "endOffset": 48}], "year": 2010, "abstractText": "Recently Kutin and Niyogi investigated several notions of algorithmic stability\u2014a property of a learning map conceptually similar to continuity\u2014showing that training-stability is sufficient for consistency of Empirical Risk Minimization while distribution-free CV-stability is necessary and sufficient for having finite VC-dimension. This paper concerns a phase transition in the training stability of ERM, conjectured by the same authors. Kutin and Niyogi proved that ERM on finite hypothesis spaces containing a unique risk minimizer has training stability that scales exponentially with sample size, and conjectured that the existence of multiple risk minimizers prevents even super-quadratic convergence. We prove this result for the strictly weaker notion of CV-stability, positively resolving the conjecture.", "creator": "LaTeX with hyperref package"}}}