{"id": "1005.0027", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2010", "title": "Learning from Multiple Outlooks", "abstract": "We consider semi-supervised learning from multiple outlooks of the same learning task, that is, learning from different representations of the same type of data. As opposed to learning from multiple views where it is assumed that the exact same instances have multiple representations, we only assume the availability of samples of the same learning task in different domains. We develop an algorithmic framework that is based on mapping the (unlabeled) data followed by adjusting the mapping using the scarcer labeled data set. Our approach requires explicit models of the data in the dataset of the two models. We demonstrate the potential for statistical advantage in learning from different outcomes of the same learning task, the scarcer labeled data set, and an algorithm to measure the likelihood that each of these two models is suitable for each of the two models. Our approach incorporates the existing model in the dataset of the same learning task with the same dataset as the two methods of learning in the dataset. We demonstrate that the model models are suitable for these two datasets because we know that the correct model (the scarcer labeled data set) is not all that far from optimal. Moreover, the models of the same learning task, where the scarcer labeled data set and the scarcer labeled data set, can be used to model the distribution of the mean-square (e.g., the mean squares) of the data. The results of this analysis are summarized in the following table: The distribution of the mean squares on the different tasks and the differences in mean-square (e.g., the mean squares) of the data, or the mean squares on the different tasks and the difference in mean-square (e.g., the mean squares) of the data, or the mean squares of the data, and the mean squares of the data, as well as the mean squares of the data. The mean square of the data is the distribution of the mean squares in the data, and the mean squares of the data are the distribution of the mean squares in the data, and the mean squares of the data are the distribution of the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or the mean squares in the data, or", "histories": [["v1", "Fri, 30 Apr 2010 21:52:17 GMT  (319kb)", "https://arxiv.org/abs/1005.0027v1", "with full proofs of theorems"], ["v2", "Tue, 14 Jun 2011 06:56:25 GMT  (64kb)", "http://arxiv.org/abs/1005.0027v2", "with full proofs of theorems and all experiments"]], "COMMENTS": "with full proofs of theorems", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["maayan harel", "shie mannor"], "accepted": true, "id": "1005.0027"}, "pdf": {"name": "1005.0027.pdf", "metadata": {"source": "META", "title": "Learning from Multiple Outlooks", "authors": ["Maayan Harel"], "emails": ["maayanga@tx.technion.ac.il", "shie@ee.technion.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 5.\n00 27\nv2 [\ncs .L\nG ]\n1 4\nJu n\n20 11"}, {"heading": "1. Introduction", "text": "It is often the case that a learning task relates to multiple representations, to which we refer as outlooks. Samples belonging to different outlooks may have varying feature representations and distinct distributions. Furthermore, the outlooks are not related through corresponding instances, but just by the common task.\nMultiple outlooks may be found in many real life problems. For example, in activity recognition when data from different users, representing the outlooks, are collected from different sensors. Note that each outlook may have a totally different feature representations,\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\nwhile the recognition task is common to all outlooks. The ability to learn from these different representations is formulated by multiple outlook learning. A different example for multiple outlooks learning is classification of document corpora written in different languages. In this case, each language represents a different outlook. In these situations, the transformations between the outlooks are unknown and feature or sample correspondence is not available. Consequently, it is rather difficult to learn the task at hand while exploiting the information in different representations.\nThe goal of multiple outlook learning is to use the information in all available outlooks to improve the learning performance of the task. We propose to approach this learning problem in a two step procedure. First, we map the empirical distributions of the different outlooks one to another. After the outlooks\u2019 distributions are matched, a generic classification algorithm can be applied using the available examples from all the outlooks.\nThis approach allows to transfer an outlook of which we have little information to another where we have more information. That is, mapping the data to the same space effectively enlarges our sample size and may also give us a better representation of the problem. We show that a classifier learned in the resulting space may outperform each single classifier.\nIn general, matching multiple distributions, without feature alignment or assuming a parametric model, is a difficult task. Therefore, we propose to match the empirical moments of the distributions as an approximation. We present an algorithm for finding one such mapping. The algorithm\u2019s objective is to find the optimal affine transformations of the outlooks\u2019 spaces, while maintaining isometry within classes. From a geometric point of view, our algorithm is based on matching the centers and the main directions of the outlooks\u2019 sample distributions. One virtue of the algorithm is its simple closed form solution."}, {"heading": "2. Related work", "text": "Learning from multiple outlooks is related to other setups such as domain adaptation, multiple view learning and manifold alignment. The main challenge in these setups, as in ours, is that the training and test data are drawn from different distributions.\nDomain adaptation tries to resolve a common scenario when some changes have been made to the test distribution, while the labeling function of the domains remains more or less the same. Some authors portray this situation by assuming a single hypothesis may classify both domains well (Blitzer et al., 2007), while others assume the target\u2019s posterior probability is equal for the domains (Shimodair, 2000; Huang et al., 2007). The latter assumption is also referred to as the covariate shift problem.\nAlgorithms for domain adaptation may be roughly divided to three categories. One approach is to reweigh the training instances so they better resemble the test distribution (Shimodair, 2000; Huang et al., 2007). Such algorithms are derived from the covariate shift assumption, which is in some sense one of the outlook mapping goals. A different approach is to combine the classifiers learnt in each domain (Mansour et al., 2009). Last, some works suggest to change the feature representation of the domains. This may be carried out by choosing a subset of features (Satpal & Sarawagi, 2007), combination of features (Daume\u0301 III, 2007), or by finding some structural correspondence between features in different domains (Blitzer et al., 2006). All the described approaches entail an initial common feature representation for the domains. Thus domain adaptation is a special case of the multiple outlook problem, for the case of outlooks with a common feature space. In Section 6 we show that our approach can also be applied to this problem.\nMultiple outlook learning is also closely related to the multi-view setup (Ru\u0308ping & Scheffer, 2005). In this setup, each view contains the same set of samples represented by different features. Clearly, any multiple view data is also some instance of a multiple outlook data with the added requirement that each sample has observations from multiple outlooks. One common approach is to map a pattern matrix of each view to a consensus pattern by matching corresponding instances (Long et al., 2008; Hou et al., 2010). Note that in the multiple outlook framework each outlook contains a unique set of samples, thus sample to sample correspondence is impossible. Amini et al. (2009) considers the case when correspondence is missing for some instances, but assumes the existence of a mapping functions between the views.\nMulti-view learning is sometimes referred to as manifold alignment. In manifold alignment we look for a transformation of two data sets with sample pairwise correspondence that minimizes the distance between them, in an unsupervised (Wang & Mahadevan, 2008) or a semi-supervised (Ham et al., 2005) manner. Wang & Mahadevan (2009) present manifold alignment without pairwise correspondence. To our knowledge, this is the only work on manifold alignment that does not assume a pairwise matching of the samples. The algorithm presented in this work is not originally suited for classification as our algorithm."}, {"heading": "3. Mapping Two Outlooks", "text": ""}, {"heading": "3.1. Problem Setting", "text": "The learner is given two outlooks belonging to separate input spaces X1 and X2 of dimension d1 and d2 respectively, with a common target Y = {1, ..., c}. We assume that all example pairs of a given outlook j = 1, 2 are independently drawn from an unknown distribution Dj , which is unique to each outlook. Denote by X\n(1) i and X (2) i the data matrices of class i of\noutlook 1 and 2, respectively. We use superscripts to denote the outlooks\u2019 index, and subscripts to denote the classification class."}, {"heading": "3.2. Multiple Outlook MAPping algorithm", "text": "In this section we present our main Multiple Outlook MAPping algorithm (MOMAP) for matching the representations of two outlooks. Throughout the derivations outlook 2 is mapped to outlook 1, which is sometimes referred to as the final outlook. Our goal is to map an outlook where we have ample labeled data, to an outlook where little labeled information is available.\nAs a preliminary step to the mapping algorithm scaling is applied. The scaling is applied to each of the outlooks separately, and aims to normalize the features of all outlooks to the same range. Note that this stage may be done using unlabeled data when available.\nNext, we use the labeled samples to match the two outlooks. The goal of this stage is to map the scaled representations by rotation and translation. Specifically, the mapping is performed by translating the means of each class to zero, rotating the classes to fit each other well, and then translating the means of the mapped outlook to the final outlook.\nLet {\n\u00b5\u0302 (1) i , \u00b5\u0302 (2) i\n}c\ni=1 be the set of empirical means of\nthe outlooks. We translate the empirical means of each\nclass of both outlooks to zero:\nX\u0302 (j) i = X (j) i \u2212 \u00b5\u0302 (j) i i = 1, ..., c, j = 1, 2. (1)\nNext, we turn to matching the main directions of the classes by rotation. Note that a rotation matrix may be defined in many manners. We search for mappings in the set of all orthonormal matrices (rotation and reflection). Our choice of mapping by rotation is motivated by its isometry property, which allows us to maintain the relative distance between the samples. We construct utilization matrices for each of the outlooks as follows. Define D\n(j) i as the utilization matrix\nof outlook j and class i. D (1) i and D (2) i are concatenated matrices constructed from the h \u2264 min(d1, d2) principal directions of the corresponding outlook and class. That is, the h eigenvectors of the empirical covariance matrices \u03a3\u0302 (1) i , \u03a3\u0302 (2) i corresponding to the h largest eigenvalues.\nUsing the utilization matrices we find the rotation matching the outlooks by solving the following optimization problem:\n{Ri} = argmin {Ri}\nc \u2211\ni=1\n\u2225 \u2225 \u2225 RiD (2) i \u2212D (1) i \u2225 \u2225 \u2225 2\nF (2)\nsubject to: RTi Ri = I i = 1, ..., c,\nwhere \u2016\u00b7\u2016F is the Frobenius norm. To gain some intuition on Problem (2) we disassemble a term in the sum of the objective function\nargmin \u2225 \u2225\n\u2225 RiD (2) i \u2212D (1) i\n\u2225 \u2225 \u2225 2\nF = argmax\nh \u2211\nl=1\nv (1)T il Rv (2) il ,\nwhere v (j) il (l = 1, ..., h) are the principal directions of the ith class of outlook j. We obtain that Problem (2) is equivalent to maximization of the sum of inner products between the principal directions of outlook 1 and the rotated principal directions of outlook 2, which in turn implies minimization of the first h principal angles between the classes of both outlooks.\nAlthough Problem (2) is not convex it can be solved in closed form. For the solutions constructed in this stage we borrow techniques from the literature of Procrustes Analysis (Gower & Dijksterhuis, 2004). Problem (2) is equivalent to\nargmax Ri\nc \u2211\ni=1\ntr (\nRiD (2) i D (1)T i\n)\n(3)\nsubject to: RTi Ri = I i = 1, ..., c.\nProblem (3) is separable, thus each component in the sum may be optimized separately. In the following derivations we drop the subscript i for brevity.\nAlgorithm 1 Matching two outlooks\nInput: empirical moments \u00b5\u0302 (j) i \u2200i, j. for i = 1 to c do X\u0302\n(j) i = X (j) i \u2212 \u00b5\u0302 (j) i j = 1, 2. X\u0303 (2) i = MatchByRotation(X\u0302 (1) i , X\u0302 (2) i ). X (2) Mappedi = X\u0303 (2) i + \u00b5\u0302 (1) i .\nend for Output: X (2) Mappedi \u2200i\nAlgorithm 2 MatchByRotation\nInput: matrices X\u0302(1), X\u0302(2). Construct matrices D(1), D(2). Compute SVD factorization D(2)D(1)T = USV T . R = V UT . Output: X\u0303(2) = X\u0302(2)RT .\nLet USV T be the singular value decomposition (SVD) of D(2)D(1)T . Define Z = V TRU . Then,\ntr ( RD(2)D(1)T ) = tr ( RUSV T ) =\ntr (ZS) = d \u2211\nk=1\nzkk\u03c3k \u2264 d \u2211\ni=k\n\u03c3k,\nwhere \u03c3k is the k-th singular value of D (2)D(1)T . The upper bound is attained for R = V UT since in that case Z = I (Algorithm 2).\nAfter the rotation, we translate the classes to match the original means of the final outlook. The above derivation gives rise to an algorithm that matches two given outlooks. The algorithm is described in Algorithm 1.\nRemark 1. Each outlook need not have the same dimension. In this case, the orthonormal constraint can not be obtained as R is no longer a square matrix. However, this problem can be easily solved. Suppose that D (1) i and D (2) i have different numbers of rows. Then, simply add rows of zeros to the smaller dimensional configuration until the dimensions are equalized. In this manner, we embed the smaller configuration in the space of the larger one.\nRemark 2. Algorithm 1 does not rely on any corresponding instances in both outlooks . However, when available, such instances may aid the mapping accuracy and can be easily incorporated into the algorithm. It is possible to do so by adding columns of the corresponding instances to the utilization matrices."}, {"heading": "4. Extension to Multiple outlooks", "text": "We present an extension of Algorithm 1 to the case of multiple outlooks. The multiple outlook scenario allows us to use the information available in all the outlooks to allow better learning of each one. To do so, we transform all the outlooks one to another. As for two outlooks, we begin by translating the means of each class of all the outlooks to zero. In the rotation step, the optimal rotations are found by solving\nmin {R\n(j) i }\nc \u2211\ni=1\n\u2211\nk<j\n\u2225 \u2225 \u2225 R (k) i D (k) i \u2212R (j) i D (j) i \u2225 \u2225 \u2225 2\nF (4)\nsubject to: R (j)T i R (j) i = I \u2200i, j.\nObserve that Algorithm 2 produces an optimal solution with zero error, as there is always a perfect rotation between two sets of h orthogonal vectors. Therefore, one optimal solution of (4), which attains an objective value of zero, is to rotate all outlooks to a chosen final outlook. Namely, for m outlooks m \u2212 1 rotation matrices are computed for each class. Finally, shift the means of the rotated outlooks to those of the final outlook.\nIf we want to switch the choice of final outlook, all we need to do is apply the inverse mapping of the relevant outlook to all mapped outlooks. For example, to switch from outlook s to k one needs to apply the following transformation:\nX (k) i = R (k)\u22121 i\n(\nX (s) i \u2212 \u00b5\u0302 (s) i\n)\n+ \u00b5\u0302 (k) i \u2200i."}, {"heading": "5. Analysis", "text": "In this section we give a probabilistic robust interpretation of the rotation process, and prove a sample complexity bound on the convergence of the estimated rotation matrix ."}, {"heading": "5.1. Probabilistic Interpretation", "text": "In this section we discuss the effect of adding random noise to the utility matrices on the optimal rotation between two outlooks (Problem (2)). We do not assume knowledge of the probability distribution of the noise. Instead, we use its bounded total value for some chosen confidence level. We show that the solution to the noised problem is bounded by the sum of the solution to the original problem and a constant value that depends on the noise. Notably, the noise only has an additive effect to the bound.\nLet \u2206 be the additive random uncertainty to the utility matrix D (2) i for some class i. Suppose that\nthis uncertainty follows an unknown joint distribution \u2206 \u223c P . This uncertainty may be portrayed by a chance-constrained extension of Problem (2) 1 :\nmin RT R=I,\u03c4 \u03c4 (5)\nPr\u2206\u223cP\n{\u2225\n\u2225 \u2225 R(D(2) +\u2206)\u2212D(1)\n\u2225 \u2225 \u2225\nF \u2264 \u03c4\n}\n\u2265 1\u2212 \u03b7,\nwhere \u03b7 \u2208 [0, 1] is the desired confidence level. Optimization of the chance constrained problem is natural, as it obtains, with high probability, the optimal rotation. However, despite their intuitive probabilistic form, chance constrained problems are generally intractable (Shapiro et al., 2009), thus we approximate Problem (5) as follows. We define \u03c1\u2217 = inf\u03b1 {Pr\u2206\u223cP (\u2016\u2206\u2016F \u2264 \u03b1) \u2265 1\u2212 \u03b7} and obtain that with probability at least 1\u2212 \u03b7 \u2225 \u2225 \u2225 R(D(2) +\u2206)\u2212D(1) \u2225 \u2225 \u2225\nF \u2264 max \u2016\u2206\u2016F\u2264\u03c1\u2217\n\u2225 \u2225 \u2225 R(D(2) +\u2206)\u2212D(1) \u2225 \u2225 \u2225\nF .\nTherefore, Problem (5) is upper bounded by the following minmax problem\nmin RTR=I max \u2016\u2206\u2016F\u2264\u03c1\u2217\n\u2225 \u2225 \u2225 R(D(2) +\u2206)\u2212D(1) \u2225 \u2225 \u2225\nF . (6)\nThis is the robust version to the original rotation problem, with the uncertainty set U = {\u2206 | \u2016\u2206\u2016F \u2264 \u03c1\u2217} 2. Next, we construct the robust counterpart of (6).\nTheorem 1. Problem (6) is equivalent to\nmin RTR=I\n( \u2225\n\u2225 \u2225 RD(2) \u2212D(1)\n\u2225 \u2225 \u2225\nF\n)\n+ \u03c1\u2217.\nThe proof is provided in A.1. The theorem shows that Problem (2) is robust to a perturbation of a total bounded value. That is, for a bounded noise, the only difference between the solution to the original problem and its robust version (Problem (6)) is an additive constant \u03c1\u2217. From a probabilistic point of view, the solution of this problem also provides a bound on the chance constrained problem in (5)."}, {"heading": "5.2. Sample complexity bounds", "text": "We next provide a bound for the sample complexity of the rotation step of the algorithm.\n1Since Problem (2) is separable, the extension is done to each class separately. We drop the subscript i, representing the class, from the following derivations for brevity.\n2The original rotation problem was actually the square of the Frobenius error. However, the two problems are equivalent since taking the square does not change the solution.\nAssumption 1. (Gaussian Mixture) Each outlook is generated by a unique mixture of c Gaussian distributions, where c is the number of classes. The samples of each outlook are realizations of x \u223c \u2211ci=1 wifi(x), where fi(x) \u223c N (\u00b5i,\u03a3i) and \u2211c i=1 wi = 1. We further assume that \u2016ExxT \u2016 \u2264 1 for each component. Theorem 2. Suppose that Assumption 1 holds. For each outlook, let \u03b4, \u01ebi, \u01eb \u2208 (0, 1), (i = 1, .., c) and suppose that the number of samples for each class i satisfies:\nni \u2265 C dh2\n\u01eb2i log2\n(\n32dh2\n\u01eb2i\n) log2 ( 4hd\n\u03b4\n)\n.\nThen P (\u2225 \u2225 \u2225 R\u0302\u2212R \u2225 \u2225 \u2225 \u2264 \u01eb ) \u2265 1\u2212 \u03b4,\nwhere, R\u0302 is the estimated rotation matrix found by Algorithm 2, d is the dimension and C is a constant.\nThe proof of the theorem is provided in A.2. Note that the sample complexity of the mapping algorithm is dominated by the rotation stage. In practice, the number of chosen principal directions h is usually small. Also note that the bound on the norm of the second moment in Assumption 1 is achieved by the scaling stage."}, {"heading": "6. Experiments", "text": "In this section we demonstrate our framework on activity recognition data, in which different users represent different outlooks. In this application, the multiple outlooks setup allows for valuable flexibility in real life recordings. For example, some users may use a simple sensor configuration for recordings, while others use a complex sensor board of multiple sensors. Also, this setup may resolve problems of varying sampling rates when using different hardware and workloads.\nIn our experiments we test two setups: a domain adaptation setup and a multiple outlook setup. For the domain adaptation setup a common feature representation is used, while for the multiple outlook setup a unique feature space is used for each user."}, {"heading": "6.1. Data set description and feature extraction", "text": "The data set used for the experiments was collected by Subramanya et al. (2006) using a customized wearable sensor system. The system includes a 3-axis accelerometer, phototransistors for measuring light, barometric pressure sensors, and GPS data. The data consist of recordings from 6 participants who were asked to perform a variety of activities and record the labels. We used the following labels: walking, running, going upstairs, going downstairs and lingering.\nAfter removing data with obvious annotation errors the data consists of about 50 hours of recording, divided approximately evenly among the 6 users. For each user the activities are roughly divided into 40% walking, 40 \u2212 50% lingering, 2 \u2212 5% running, 2 \u2212 3% going upstairs, and 2 \u2212 3% going downstairs. See (Subramanya et al., 2006) for further details on the sensor system and the recordings.\nFrom the raw data we extracted windowed samples as follows. From the accelerometer data we used the x-axes measurements sampled at 512Hz, which we decimated to 32Hz. The barometric pressure sampled at 7.1Hz, was smoothed and interpolated to 32Hz. Next, we applied a two-second sliding window over each signal using a window of appropriate length. From each window a feature vector is extracted containing the Fourier coefficients of the accelerometer data, the mean of the gradient of the barometric pressure, and the mean values of the light signals. All together we obtained 20-35 thousand samples for each user with 37 features.\nAs explained in Section 3.2, before mapping the outlooks scaling should be applied to all the outlooks. For all the experiments, we scale the data to [0,1]. To reduce the sensitivity of the scaling to outliers we first collapse the extreme two percentile of the data to the value of the extreme remaining values (also known as Winsorization). Scaling parameters are chosen on the training data and applied to the test data. This preprocessing was applied to all baseline classifiers."}, {"heading": "6.2. Domain Adaptation Setup", "text": "As mentioned above, multiple outlook learning may also be applied for domain adaptation. We tested both standard domain adaptation of two domains, as well as multiple source domain adaptation.\nFor the two domain problem we adopted the commonly used terminology in domain adaptation of source and target domains. We applied Algorithm 1 for different fractions of target labeled data and fully labeled source data. The performance was computed by 10-fold crossvalidation, each fold containing random samples from each class according to its fraction in the complete set. The only parameter of the algorithm h was chosen on a random split.\nWe test the success of the mapping algorithm by classification of the target test data with a classifier trained on the mapped source data, denoted as the MOMAP classifier (no target data was used for training). This is a multi-class classification problem, with five possible labels. We use a multi-class SVM classifier with an\nRBF-kernel (C = 64, \u03b3 = 0.25 3) obtained by LIBSVM software (Chang & Lin, 2001). The data are unevenly distributed among the five classes, therefore we use the balanced error rate (BER) as a performance measure: BER = 1c \u2211c i=1 1 ni ei, where ei and ni are the numbers of errors and number of samples in class i respectively, and c is the number of classes.\nWe compare the MOMAP classifier to the following baselines: a target only classifier, trained on the available labeled target data (TRG); a source only classifier, trained on the source data (SRC); a classifier trained on all available labeled data of target and source (ALL); and the domain adaptation algorithm presented in (Daume\u0301 III, 2007) (FEDA). We also add the \u201doptimal\u201d error, obtained by training on the fully labeled target data (OPT).\nThe results are presented in Figure 1. It can be observed that the MOMAP classifier outperforms the baseline classifiers for most fractions of target labeled data. The algorithm performs well across all sets of users, for example, for 5% labeled data it is significantly better (p-value< 0.05) than the TRG, SRC and FEDA classifiers for all sets, and significantly better than the ALL classifier for 18 out of 30 possible sets (see Table 1 in A.3).\nIn the next experiment we consider mixtures of m source domains with some labeled data (both training and test sets are mixtures). We use the extension to multiple outlooks presented in Section 4 to find the mappings of the sources to each outlook. We test the classification performance on each component of the mixture with a classifier trained on all the mapped sources. The final performance measure is the mean BER averaged on all the sources. As in the previous experiment, the evaluation was done by 10-fold cross-validation, with the same classifier. The baselines are similar, with the change of the TRG to the mean value of multiple classifiers trained in each domain, and the ALL baseline to a classifier trained on all sources (the SRC classifier was not relevant). The experiment was performed on all 20 triplet combinations. Sample results are presented in Figure 2. These trends were consistent across users, for example, for 15% of labeled data the MOMAP algorithm outperforms all other classifiers for 15 of the combinations (p-value< 0.05). In the 5 remaining combinations, the algorithm performed significantly better than the TRG and FEDA algorithms, and equally well as the ALL classifier (see Table 2 in A.3). For larger portions\n3The parameters were chosen on the target classification problem. Common parameters were chosen for clear performance comparison of the different classifiers.\nof labeled data the MOMAP algorithm also obtained smaller error than the ALL classifier (p-value< 0.05). The effect of the ALL classifier may be a result of some regularization obtained from training on data from similar yet different domains."}, {"heading": "6.3. Multiple Outlook Setup", "text": "We conducted three types of experiments for the multiple outlook setup, each with a different feature representation. The experiments\u2019 setup was similar to the previous experiments with some adjustments to the baselines: the SRC, ALL and FEDA baselines were no longer relevant, as the outlooks\u2019 features differ.\nIn the first experiment we tested the multiple outlook algorithm on two outlooks for the case of different sen-\nsors and added noise features. For the mapped outlook we used full feature representation (37 features). For the target outlook we used the accelerometer\u2019s and pressure features, and excluded the light measurements. Instead of the light features we added features with Gaussian random noise (N (0, 1)). The experiment was performed on all pair combinations. For 5% labeled data of the learned outlook, the mean BER of the MOMAP was 4.5% (\u00b12.7%) lower than that of the TRG classifier. The results for four user pairs are presented in Figure 3. These results show that the mapping was successful, as training on the mapped data outperforms training on partial data in the target outlook. In Fig. 3(c) the MOMAP algorithm has lower error than the OPT classifier for some fractions; this may be a result of the added information in the light features.\nIn the second experiment we tried to learn from two outlooks with a different number of features resulting from different sampling rates. Specifically, for the learned outlook we kept the full feature representation as described in Section 6.1, while for the mapped outlook we used the same type of features but with 30Hz sampling rate instead of 32Hz. This resulted in 37 features in the target outlook and 35 in the mapped one. Note that our algorithm may be easily modified for this scenario; see Remark 1 in Section 3.2. For 5% labeled data the MOMAP algorithm had on aver-\nage 5.9% (\u00b12.4%) lower BER than the TRG classifier. Figure 4 presents the results on four user pairs. In Figs. 4(a) and 4(c) the MOMAP algorithm has lower error than the OPT classifier. Observe that this is possible since the balanced error rate is presented, which treats the error in different classes equally (namely, the MOMAP classifier does not outperform the nonbalanced error).\nIn the third experiment we constructed the feature representation of each outlook from the 33 accelerometer\u2019s features to which we added 10 features of Gaussian noise (N (0, 1)). We then randomly permuted the order of the features of each outlook. For this experiment, we used samples belonging to the walking, running and lingering classes, as we did not use the full feature set. The experiment was performed for two outlooks as well as for multiple outlooks. The results indicate the performance boost from MOMAP especially for the running activity. Due to space limitations we provide the results in A.4."}, {"heading": "7. Future Work", "text": "Our proposed approach is a first step in developing the methodology for learning from multiple outlooks. This approach may be extended to many interesting directions. First, in this paper we only considered affine mappings between the outlooks and a natural extension is to consider richer classes of transformations such as piecewise linear mappings. Also, our ap-\nproach is batch in the sense that first all the data have to be processed and then the classification algorithm can be used. A different extension of practical interest would be to develop an online version of the proposed approach that takes samples one by one and gradually improves the mapping. Finally, a major application domain, of independent interest, is natural language processing. Here the challenge would be to use a language where labels are abundant to better classify in a different language. The main obstacle here seems to be the nature of representation: language data are often represented as sparse vectors which may call for a different type of transformations between the outlooks."}, {"heading": "A. Appendix", "text": "A.1. Proof of Theorem 1\nThe next theorem presents the robust counterpart of Problem (6); the robust version of the optimization for the two outlooks rotation problem (each component in Problem 2). We restate the theorem for clarity:\nTheorem 1. Problem (6) is equivalent to\nmin RTR=I\n(\u2225\n\u2225 \u2225 RD(2) \u2212D(1)\n\u2225 \u2225 \u2225\nF\n)\n+ \u03c1\u2217.\nProof. We obtain an explicit expression for the maximization in (6). By definition, the norm may be written as\nmax \u2016\u2206\u2016F\u2264\u03c1\u2217\n\u2225 \u2225 \u2225 R(D(2) +\u2206)\u2212D(1) \u2225 \u2225 \u2225\nF =\nmax \u2016\u2206\u2016F\u2264\u03c1\u2217,\u2016V \u2016F\u22641\ntr ( V T (R(D(2) +\u2206)\u2212D(1)) ) =\nmax \u2016V \u2016F\u22641\n{\ntr ( V T (RD(2) \u2212D(1)) )\n+ max \u2016\u2206\u2016F\u2264\u03c1\u2217\ntr ( V T R\u2206 )\n}\n.\n(7)\nNext, we develop an explicit representation of the inner maximization over \u2206. By applying the CauchySchwartz inequality and the unitary invariance of the Frobenius norm we obtain an upper bound:\nmax \u2016\u2206\u2016F\u2264\u03c1\u2217\ntr ( V T R\u2206 )\n\u2264 max \u2016\u2206\u2016F\u2264\u03c1\u2217 \u2016V \u2016F \u2016R\u2206\u2016F = \u03c1\u2217\u2016V \u2016F .\nLet \u2206\u2217 = RTV/\u2016V \u2016F . Observe that\nmax \u2016\u2206\u2016F\u2264\u03c1 \u2217\ntr ( V T R\u2206 ) \u2265 tr ( V T R\u2206\u2217 ) = \u03c1\u2217\u2016V \u2016F .\nWe conclude, that max\u2016\u2206\u2016F\u2264\u03c1\u2217 tr ( V TR\u2206 )\n= \u03c1\u2217\u2016V \u2016F . Inserting this equation into (7) we obtain:\nmax \u2016\u2206\u2016F\u2264\u03c1\u2217\n\u2225 \u2225 \u2225R(D (2) +\u2206)\u2212D(1) \u2225 \u2225 \u2225\nF =\nmax \u2016V \u2016F\u22641\n[ tr ( V T (RD(2) \u2212D(1)) ) + \u03c1\u2217\u2016V \u2016F ] =\n\u2225 \u2225 \u2225 RD (2) \u2212D(1) \u2225 \u2225 \u2225\nF + \u03c1\u2217,\nwhich concludes the proof.\nA.2. Proof of Theorem 2\nWe restate the theorem for clarity:\nTheorem 2. (Sample complexity of rotation for two outlooks) Suppose that Assumption 1 hold. Then, for \u03b4, \u01ebi, \u01eb \u2208 (0, 1), if the number of samples for each class and outlook i satisfies:\nni \u2265 C dh2\n\u01eb2i log2\n(\n32dh2\n\u01eb2i\n) log2 ( 4hd\n\u03b4\n)\nthen\nP (\u2225 \u2225 \u2225 R\u0302\u2212R \u2225 \u2225 \u2225 \u2264 \u01eb ) \u2265 1\u2212 \u03b4,\nwhere, R\u0302 is the estimated rotation matrix found by algorithm 2, d is the dimension and C is a constant.\nBefore providing the proof we present the following lemmas:\nLemma 3 (Sample complexity of estimating mean). Let Assumption 1 hold. Then for \u03b4, \u01eb \u2208 (0, 1) if each class and outlook satisfies: n \u2265 2d\u01eb2 log ( d \u03b4 ) then\nP (\u2016\u00b5\u0302\u2212 \u00b5\u2016 \u2264 \u01eb) \u2265 1\u2212 \u03b4,\nwhere \u00b5\u0302 and \u00b5 are the empirical and true mean of each component of the mixture.\nProof. We use \u03c32max = maxk ( \u03c32k )\nas the maximal directional variance of the jth mixture , and \u03c3k as the standard deviation of the samples kth coordinate. By applying Chernoff\u2019s method on each coordinate of |\u00b5\u0302k \u2212 \u00b5k|, k = 1, ..., d and then applying the union bound we obtain that for n \u2265 2\u03c3\n2 maxd \u01eb2 log ( d \u03b4 )\n\u2016\u00b5\u0302\u2212 \u00b5\u20162 \u2264 \u01eb holds with probability of at least 1 \u2212 \u03b4. The bound is obtained by applying \u03c32max \u2264 1, which is implied from Assumption 1\nLemma 4. Let X be a set of n points drawn from a one dimensional Gaussian with mean \u00b5 and variance \u03c32. With probability 1\u2212 \u03b4,\n|x\u2212 \u00b5| \u2264 \u03c3 \u221a 2 log (n\n\u03b4\n)\n\u2200x \u2208 X.\nLemma 5. Let x1, ..., xn be a set of independent realizations of random vectors from a multivariate normal distribution in Rd. Then with probability of at least 1\u2212 \u03b4,\n\u2016xi\u2016 \u2264 \u2016\u00b5\u2016+ \u03c3 \u221a 2d log ( nd\n\u03b4\n)\n.\nProof. By the reverse triangle inequality we have that\n\u2016xi\u2016 \u2212 \u2016\u00b5\u2016 \u2264 |\u2016xi\u2016 \u2212 \u2016\u00b5\u2016| \u2264 \u2016xi \u2212 \u00b5\u2016 .\nBy applying Lemma 4 on a single coordinate of the random vectors xi we get\nP\n(\n\u2223 \u2223 \u2223x (k) i \u2212 \u00b5k \u2223 \u2223 \u2223 \u2265 \u01eb\u221a d\n) \u2264 nexp (\n\u22121 2\n\u01eb2\n\u03c32d\n)\n\u2264 \u03b4 d .\nTaking the union bound over the d coordinates we get that with probability at least 1\u2212 \u03b4\n\u2016xi\u2016 \u2212 \u2016\u00b5\u2016 \u2264 \u2016xi \u2212 \u00b5\u2016 \u2264 \u03c3 \u221a 2d log ( nd\n\u03b4\n)\n.\nLemma 6 (Sample complexity of covariance estimation). Let X be a set of random samples generated from a Gaussian distribution with covariance \u03a3 and zero mean \u00b5 = 0. Define \u03a3\u0302, \u00b5\u0302 as the estimated covariance matrix and mean of the sample. Then for \u03b4, \u01eb1, \u01eb2 \u2208 (0, 1), for a sample size of\nn \u2265 C d \u01eb22 log2 ( 2d \u01eb22 ) log2 ( 2d \u03b4 )\nwe have that\nP (\u2225 \u2225 \u2225 \u03a3\u0302\u2212 \u03a3 \u2225 \u2225 \u2225 \u2264 \u01eb1 + \u01eb2 ) \u2265 1\u2212 \u03b4.\nProof. The concentration bound is obtained by dividing the error to two components,\n\u2225 \u2225 \u2225\u03a3\u0302\u2212 \u03a3 \u2225 \u2225 \u2225 \u2264 \u2225 \u2225 \u2225\u00b5\u00b5 T \u2212 \u00b5\u0302\u00b5\u0302T \u2225 \u2225 \u2225+\n\u2225 \u2225 \u2225 \u2225 \u2225 1 n n \u2211\ni=1\nxix T i \u2212 ExxT\n\u2225 \u2225 \u2225 \u2225 \u2225 , (8)\nWe begin by bounding the first component: Recall that \u00b5 = 0, so the first component is bounded by \u2016\u00b5\u0302\u20162. We apply Lemma 3 and obtain that with probability at least 1\u2212 \u03b42 :\nn1 \u2265 2d\n\u01eb log\n(\n2d\n\u03b4\n)\n, (9)\n\u2016\u00b5\u0302\u20162 \u2264 \u01eb1.\nThe second component is bounded by a concentration inequality for covariance matrices presented by Rudelson & Vershynin (2007). For completeness we add the relevant theorem; see Theorem 7. The second moment condition holds by Assumption 1. The second condition, of bounded sample norm is obtained as follows. By applying Lemma 5 and bounding the variance according to Assumption 1, we get that \u2016xi\u2016 \u2264 \u221a 2d log ( n2d \u03b4 ) .\nNext, we apply Theorem 3.1 of (Rudelson & Vershynin, 2007) with t2 = a2 log (\n2 \u03b4\n)\n/c\nand a = \u01eb2\n\u221a\nc/ log ( 2 \u03b4 ) . This results in the condition\na = \u01eb2c \u221a\nlog(2\u03b4 ) \u2265 C\n\u221a\n2d log ( nd \u03b4 )\nlog(n) \u221a n ,\nwhich is satisfied for the choice of\nn2 \u2265 C d\n\u01eb22 log2\n(\n2d \u01eb22\n) log2 ( 2d\n\u03b4\n)\n. (10)\nWe get the final sample bound by taking the maximum between the sample complexity of the mean (9) and the covariance estimation (10).\nProof of Theorem 2. Observe that by applying Equation (1) to each class and outlook we have that each component has zero mean. By Lemma 3, the sample complexity of this step is ni \u2265 2d\u01eb2 log ( d \u03b4 )\n(for each class and outlook i). In the following derivations we assume zero mean of the components\u2019 distribution. We show that the sample complexity of both stages is dominated by the rotation.\nBy substituting the finite and infinite sample rotation matrices with the values defined in Alg. 2 and applying the triangular inequality twice we have that\n\u2225 \u2225 \u2225 R\u0302\u2212R \u2225 \u2225 \u2225 F = \u2225 \u2225 \u2225 V\u0302 U\u0302T \u2212 V UT \u2225 \u2225 \u2225 F \u2264 \u2016V \u2016 \u2016\u2206U\u2016+ \u2016\u2206V \u2016 \u2016\u2206U\u2016+ \u2016\u2206V \u2016 \u2016U\u2016 , (11)\nwhere \u2206V = V\u0302 \u2212 V and \u2206U = U\u0302 \u2212 U . Recall that the matrices U, U\u0302, V, V\u0302 are the matrices of singular vectors resulting from the SVD decompositions D\u0302(2)D\u0302(1)T = U\u0302 S\u0302V\u0302 T and D(2)D(1)T = USV T . We apply the perturbation theory of the SVD decomposition presented in (Stewart & Sun, 1990) and bound Eq. (11) by\n\u2225 \u2225 \u2225 R\u0302\u2212R \u2225 \u2225 \u2225\nF \u2264 C\n\u2225 \u2225 \u2225 D(2)D(1)T \u2212 D\u0302(2)D\u0302(1)T \u2225 \u2225 \u2225\nF ,(12)\nwhere C is a constant. Observe that \u2225\n\u2225 \u2225 D(2)D(1)T \u2212 D\u0302(2)D\u0302(1)T\n\u2225 \u2225 \u2225\nF\n\u2264 \u2225 \u2225 \u2225 D(2)D(1)T \u2212D(2)D\u0302(1)T \u2225 \u2225 \u2225 F + \u2225 \u2225 \u2225 D(2)D\u0302(1)T \u2212 D\u0302(2)D\u0302(1)T \u2225 \u2225 \u2225 F \u2264 \u221a h \u2225 \u2225 \u2225 D(1)T \u2212 D\u0302(1)T \u2225 \u2225 \u2225\nF + \u221a h \u2225 \u2225 \u2225 D(2) \u2212 D\u0302(2) \u2225 \u2225 \u2225\nF . = \u221a h (\u2016\u2206D1\u2016F + \u2016\u2206D2\u2016F ) . (13)\nThe second inequality holds by the sub-multiplicative property of the Frobenius norm. When the number of\ncolumns h < d, columns of zeros need to be added to make the matrices square.\nDefine v (i) l and v\u0302 (i) l ( l = 1, ..., h) to be the h eigenvectors of matrices D(i) and D\u0302(i) respectively. The\nfollowing holds \u2016\u2206Di\u20162F = \u2211h l=1\n\u2225 \u2225 \u2225 v\u0302 (i) l \u2212 v (i) l \u2225 \u2225 \u2225 2\n2 by def-\ninition. Define the perturbation of the covariance matrix of mixture i by Ei = \u03a3i \u2212 \u03a3\u0302i. By applying the perturbation theory of the eigen decomposition on the perturbed covariance matrices (Stewart & Sun, 1990) (p.240) we get that \u2225 \u2225\n\u2225 v\u0302 (i) l \u2212 v (i) l\n\u2225 \u2225 \u2225 \u2264 C \u2016Ei\u2016 .\nLast, we use Lemma 6 to bound Ei for each outlook (i = 1, 2). If the number of samples for each outlook is\nni \u2265 C dh2\n\u01eb2i2 log2\n(\n32dh2\n\u01eb2i2\n) log2 ( 4hd\n\u03b4\n)\nthen\nP\n(\n\u2225 \u2225 \u2225 \u03a3\u0302i \u2212 \u03a3i \u2225 \u2225 \u2225 \u2264 \u01ebi,1 + \u01ebi,2\n4h\n)\n\u2265 1\u2212 \u03b4 2h ,\nwhich implies\nP\n(\n\u2016\u2206Di\u2016F \u2264 \u01ebi,1 + \u01ebi,2\n4 \u221a h\n)\n\u2265 1\u2212 \u03b4 2 .\nPlugging in the bound to (13) we get the final bound:\nP (\u2225 \u2225 \u2225 D(2)D(1)T \u2212 D\u0302(2)D\u0302(1)T \u2225 \u2225 \u2225 F \u2264 \u01eb ) \u2265 1\u2212 \u03b4,\nfor some \u01eb = 14 \u2211 i=1,2 \u01ebi,1 + \u01ebi,2 \u2208 (0, 1).\nTheorem 7 (Theorem 3.1 from (Rudelson & Vershynin, 2007)). Let x be a random vector in Rd from distribution D, which is uniformly bounded almost everywhere: \u2016x\u2016 \u2264 M , and \u2225 \u2225ExxT \u2225\n\u2225 \u2264 1. Let x1...xn be independent samples generated from D. Define\na = CM\n\u221a\nlogn\nn ,\nwhere C is an absolute constant. Then, for every t \u2208 (0, 1),\nP\n(\u2225\n\u2225 \u2225 \u2225 \u2225 1 n\nn \u2211\ni=1\nxix T i \u2212 E\n( xxT )\n\u2225 \u2225 \u2225 \u2225 \u2225 > t ) \u2264 2e\u2212ct2/a2 .\nA.3. Domain Adaptation setup - Results\nFollowing are results obtained on all users for the domain adaptation experiment. Table 1 presents the results for two users obtained on 5% labeled target data. Table 2 presents the results for multi-source domain adaptation with three users, each with 15% labeled data. Both tables contain the balanced error rate (BER) on the five class classification task. Highlighted results represent significance of the result with p-value< 0.05.\nA.4. Multiple outlook setup - Experiment 3\nIn the third experiment we constructed the feature representation of each outlook from the 33 accelerometer\u2019s Fourier coefficients to which we added 10 features of random Gaussian noise N (0, 1). We then randomly permuted the order of the features of each outlook. For this experiment, we used samples belonging to the walking, running and lingering classes, as we did not use the full feature set. The experiment was performed for the two outlook scenario as well as for multiple outlooks.\nFigure 4 shows the results for 5% labeled target data for different users couples. It can be observed, that for the walking and lingering activities the mapped outlook performs similarly to the TRG classifier. For all cases, the mapped outlook classifies the running activity with least errors. Among all user pairs the MOMAP classifier obtained smaller error for the running activity (3.5%\u221245% smaller for 5% labeled data). The results show the boosting power of the mapping, which, as may be expected, is most powerful for the classes with less labeled data. An interesting behavior is that even when all labeled data is available the MOMAP algorithm sometimes outperforms the classifier learned in the target outlook (OPT). This may be caused by some regularization obtained by the mapping. Note, however, that for the total error, on all three classes, the MOMAP classifier does not outperform OPT classifier. The results for multiple outlooks are presented in Figure 6. It can be observed that the\nmapping aids in learning the mixture."}], "references": [{"title": "Learning from Multiple Partially Observed Views\u2013an Application to Multilingual Text Categorization", "author": ["M. Amini", "N. Usunier", "C. Goutte"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Amini et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Amini et al\\.", "year": 2009}, {"title": "Domain adaptation with structural correspondence learning", "author": ["J. Blitzer", "R. McDonald", "F. Pereira"], "venue": "In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Blitzer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Learning bounds for domain adaptation", "author": ["J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J. Wortman"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "LIBSVM: a library for support vector machines", "author": ["C. Chang", "C. Lin"], "venue": "Daume\u0301 III, H. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting", "citeRegEx": "Chang and Lin,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin", "year": 2001}, {"title": "Semisupervised alignment of manifolds", "author": ["J. Ham", "D. Lee", "L. Saul"], "venue": "In Proceedings of the Annual Conference on Uncertainty in Artificial Intelligence, Z. Ghahramani and R. Cowell, Eds,", "citeRegEx": "Ham et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2005}, {"title": "Multiple view semi-supervised dimensionality reduction", "author": ["C. Hou", "C. Zhang", "Y. Wu", "F. Nie"], "venue": "Pattern Recognition,", "citeRegEx": "Hou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2010}, {"title": "A general model for multiple view unsupervised learning", "author": ["B. Long", "P.S. Yu", "Z.M. Zhang"], "venue": "In Proceedings of the 8th SIAM International Conference on Data Mining (SDM08),", "citeRegEx": "Long et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Long et al\\.", "year": 2008}, {"title": "Domain adaptation with multiple sources", "author": ["Y. Mansour", "M. Mohri", "A. Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mansour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Sampling from large matrices: An approach through geometric functional analysis", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Rudelson and Vershynin,? \\Q2007\\E", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2007}, {"title": "Learning with multiple views", "author": ["S. R\u00fcping", "T. Scheffer"], "venue": "In Proceeding of the International Conference on Machine Learning Workshop on Learning with Multiple Views,", "citeRegEx": "R\u00fcping and Scheffer,? \\Q2005\\E", "shortCiteRegEx": "R\u00fcping and Scheffer", "year": 2005}, {"title": "Domain adaptation of conditional probability models via feature subsetting", "author": ["S. Satpal", "S. Sarawagi"], "venue": "In Proceedings of Principles of Data Mining and Knowledge Discovery,", "citeRegEx": "Satpal and Sarawagi,? \\Q2007\\E", "shortCiteRegEx": "Satpal and Sarawagi", "year": 2007}, {"title": "Lectures on stochastic programming: modeling and theory", "author": ["A. Shapiro", "D. Dentcheva", "A. Ruszczy\u0144ski", "A.P. Ruszczy\u0144ski"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "Shapiro et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shapiro et al\\.", "year": 2009}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodair"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Shimodair,? \\Q2000\\E", "shortCiteRegEx": "Shimodair", "year": 2000}, {"title": "Matrix Perturbation Theory", "author": ["Stewart", "G.W", "J.G. Sun"], "venue": null, "citeRegEx": "Stewart et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 1990}, {"title": "Recognizing activities and spatial context using wearable sensors", "author": ["A. Subramanya", "A. Raj", "J. Bilmes", "D. Fox"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence. Citeseer,", "citeRegEx": "Subramanya et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Subramanya et al\\.", "year": 2006}, {"title": "Manifold alignment using Procrustes analysis", "author": ["C. Wang", "S. Mahadevan"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Wang and Mahadevan,? \\Q2008\\E", "shortCiteRegEx": "Wang and Mahadevan", "year": 2008}, {"title": "Manifold alignment without correspondence", "author": ["C. Wang", "S. Mahadevan"], "venue": "In Proceedings of the 21st International Joint Conferences on Artificial Intelligence,", "citeRegEx": "Wang and Mahadevan,? \\Q2009\\E", "shortCiteRegEx": "Wang and Mahadevan", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "Some authors portray this situation by assuming a single hypothesis may classify both domains well (Blitzer et al., 2007), while others assume the target\u2019s posterior probability is equal for the domains (Shimodair, 2000; Huang et al.", "startOffset": 99, "endOffset": 121}, {"referenceID": 12, "context": ", 2007), while others assume the target\u2019s posterior probability is equal for the domains (Shimodair, 2000; Huang et al., 2007).", "startOffset": 89, "endOffset": 126}, {"referenceID": 12, "context": "One approach is to reweigh the training instances so they better resemble the test distribution (Shimodair, 2000; Huang et al., 2007).", "startOffset": 96, "endOffset": 133}, {"referenceID": 7, "context": "A different approach is to combine the classifiers learnt in each domain (Mansour et al., 2009).", "startOffset": 73, "endOffset": 95}, {"referenceID": 1, "context": "This may be carried out by choosing a subset of features (Satpal & Sarawagi, 2007), combination of features (Daum\u00e9 III, 2007), or by finding some structural correspondence between features in different domains (Blitzer et al., 2006).", "startOffset": 210, "endOffset": 232}, {"referenceID": 6, "context": "One common approach is to map a pattern matrix of each view to a consensus pattern by matching corresponding instances (Long et al., 2008; Hou et al., 2010).", "startOffset": 119, "endOffset": 156}, {"referenceID": 5, "context": "One common approach is to map a pattern matrix of each view to a consensus pattern by matching corresponding instances (Long et al., 2008; Hou et al., 2010).", "startOffset": 119, "endOffset": 156}, {"referenceID": 4, "context": "In manifold alignment we look for a transformation of two data sets with sample pairwise correspondence that minimizes the distance between them, in an unsupervised (Wang & Mahadevan, 2008) or a semi-supervised (Ham et al., 2005) manner.", "startOffset": 211, "endOffset": 229}, {"referenceID": 0, "context": "Amini et al. (2009) considers the case when correspondence is missing for some instances, but assumes the existence of a mapping functions between the views.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Amini et al. (2009) considers the case when correspondence is missing for some instances, but assumes the existence of a mapping functions between the views. Multi-view learning is sometimes referred to as manifold alignment. In manifold alignment we look for a transformation of two data sets with sample pairwise correspondence that minimizes the distance between them, in an unsupervised (Wang & Mahadevan, 2008) or a semi-supervised (Ham et al., 2005) manner. Wang & Mahadevan (2009) present manifold alignment without pairwise correspondence.", "startOffset": 0, "endOffset": 488}, {"referenceID": 11, "context": "However, despite their intuitive probabilistic form, chance constrained problems are generally intractable (Shapiro et al., 2009), thus we approximate Problem (5) as follows.", "startOffset": 107, "endOffset": 129}, {"referenceID": 14, "context": "See (Subramanya et al., 2006) for further details on the sensor system and the recordings.", "startOffset": 4, "endOffset": 29}, {"referenceID": 14, "context": "Data set description and feature extraction The data set used for the experiments was collected by Subramanya et al. (2006) using a customized wearable sensor system.", "startOffset": 99, "endOffset": 124}], "year": 2011, "abstractText": "We propose a novel problem formulation of learning a single task when the data are provided in different feature spaces. Each such space is called an outlook, and is assumed to contain both labeled and unlabeled data. The objective is to take advantage of the data from all the outlooks to better classify each of the outlooks. We devise an algorithm that computes optimal affine mappings from different outlooks to a target outlook by matching moments of the empirical distributions. We further derive a probabilistic interpretation of the resulting algorithm and a sample complexity bound indicating how many samples are needed to adequately find the mapping. We report the results of extensive experiments on activity recognition tasks that show the value of the proposed approach in boosting performance.", "creator": "LaTeX with hyperref package"}}}