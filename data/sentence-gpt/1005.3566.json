{"id": "1005.3566", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2010", "title": "Evolution with Drifting Targets", "abstract": "We consider the question of the stability of evolutionary algorithms to gradual changes, or drift, in the target concept. We define an algorithm to be resistant to drift if, for some inverse polynomial drift rate in the target function, it converges to accuracy 1 -- \\epsilon , with polynomial resources, and then stays within that accuracy indefinitely, except with probability \\epsilon , at any one time. We show that every evolution algorithm, in the sense of Valiant (2007; 2009), can be converted using the Correlational Query technique of Feldman (2008), into such a drift resistant algorithm for the first time, by performing a systematic analysis to test the reliability of the method.\n\n\n\n\n\n\nOur predictions assume that we have an edge for our model of evolution because of the fact that the evolution model does not perform well with the method (see Fig. 2 ) but also because the way in which we think about the evolution model is inconsistent with the methods. However, with the assumption that the evolution model does not perform well with the method (see Fig. 3 ) we can also show that the mutation rate, in fact, is not particularly significant at all. Although the mutation rate is somewhat higher in most cases than in most cases, it is very low in a very weak way (see Fig. 4), and a very weak approach can be found in the case of the mutation rate in the field of phylogeny.\n\nIn the first paper of this paper, we have calculated the mutation rate in the form of the mutation rate in a new mutation model. We show that mutation rate has an exponential decay rate of 4.0 (with the exponential decay rate of 2.0 ) which is 3.3 (with the exponential decay rate of 2.0 ) with the exponential decay rate of 2.0 (with the exponential decay rate of 2.0 ) (with the exponential decay rate of 2.0 ) with the exponential decay rate of 2.0 (with the exponential decay rate of 2.0 ) with the exponential decay rate of 2.0 (with the exponential decay rate of 2.0 ) with the exponential decay rate of 2.0 (with the exponential decay rate of 2.0 ) with the exponential decay rate of 2.0 (with the exponential decay rate of 2.0 ) with the exponential decay rate of 2.0 (with the exponential decay rate of 2.0 ) with the exponential decay rate of 2.0 (with the exponential decay rate of 2", "histories": [["v1", "Wed, 19 May 2010 22:58:53 GMT  (160kb,S)", "http://arxiv.org/abs/1005.3566v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["varun kanade", "leslie g valiant", "jennifer wortman vaughan"], "accepted": false, "id": "1005.3566"}, "pdf": {"name": "1005.3566.pdf", "metadata": {"source": "CRF", "title": "Evolution with Drifting Targets", "authors": ["Varun Kanade", "Leslie G. Valiant", "Jennifer Wortman Vaughan"], "emails": ["vkanade@fas.harvard.edu", "valiant@seas.harvard.edu", "jenn@seas.harvard.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 5.\n35 66\nv1 [\ncs .L\nG ]\n1 9\nM ay\n2 01\nThe above translation result can be also interpreted as one on the robustness of the notion of evolvability itself under changes of definition. As a second result in that direction we show that every evolution algorithm can be converted to a quasi-monotonic one that can evolve from any starting point without the performance ever dipping significantly below that of the starting point. This permits the somewhat unnatural feature of arbitrary performance degradations to be removed from several known robustness translations."}, {"heading": "1 Overview", "text": "The evolvability model introduced by Valiant [19] was designed to provide a quantitative theory for studying mechanisms that can evolve in populations of realistic size, in a reasonable number of generations through the Darwinian process of variation and selection. It models evolving mechanisms as functions of many arguments, where the value of a function represents the outcome of the mechanism, and the arguments the controlling factors. For example, the function might determine the expression level of a particular protein given the expression levels of related proteins. Evolution is then modeled as a restricted form of learning from examples, in which the learner observes only the empirical performance of a set of functions that are feasible variants of the current function. The performance of a function is defined as its correlation with the ideal function, which specifies for every possible circumstance the behavior that is most beneficial in the current environment for the evolving entity.\nThe evolution process consists of repeated applications of a random variation step followed by a selection step. In the variation step of round i, a polynomial number of variants of the algorithm\u2019s current hypothesis ri are generated, and their performance empirically tested. In the selection step, one of the variants with high performance is chosen as ri+1. An algorithm therefore consists of both a procedure for describing possible variants and as well as a selection mechanism for choosing among the variants. The algorithm succeeds if it produces a hypothesis with performance close to the ideal function using only a polynomial amount of resources (in terms of number of generations and population size).\n\u2217This work was supported in part by NSF-CCF-04-27129. \u2020Vaughan is supported by NSF under grant CNS-0937060 to the CRA for the CIFellows Project. Any\nopinions, findings, conclusions, or recommendations expressed in this material are those of the authors alone.\nThe basic model as defined in Valiant [19] is concerned with the evolution of Boolean functions using representations that are randomized Boolean functions. This has been shown by Feldman [10] to be a highly robust class under variations in definition, as is necessary for any computational model that aims to capture the capabilities and limitations of a natural phenomenon. This model has also been extended to allow for representations with real number values, in which case a range of models arise that differ according to whether the quadratic loss or some other metric is used in evaluating performance [17, 10]. Our interest here remains with the original Boolean model, which is invariant under changes of this metric.\nIn this paper we consider the issue of stability of an evolution algorithm to gradual changes, or drift, in the target or ideal function. Such stability is a desirable property of evolution algorithms that is not explicitly captured in the original definition. We present two main results in this paper. First, for specific evolution algorithms we quantify how resistant they are to drift. Second, we show that evolutionary algorithms can be transformed to stable ones, showing that the evolutionary model is robust also under modifications that require resistance to drift.\nThe issue of resistance to drift has been discussed informally before in the context of evolution algorithms that are monotone in the sense that their performance is increasing, or at least nondecreasing, at every stage [17, 10]. We shall therefore start by distinguishing among three notions of monotonicity in terms of properties that need to hold with high probability: (i) quasi-monotonic, where for any \u01eb the performance never goes more than \u01eb below that of the starting hypothesis r0, (ii) monotonic, where the performance never goes below that of r0, and (iii) strictly monotonic, where performance increases by at least an inverse polynomial amount at each step. Definition (ii) is essentially Feldman\u2019s [10] and definition (iii) is implicit in Michael [17].\nWe define a notion of an evolution algorithm being stable to drift in the sense that for some inverse polynomial amount of drift, using only polynomial resources, the algorithm will converge to performance 1\u2212 \u01eb, and will stay with such high performance in perpetuity in the sense that at every subsequent time, except with probability \u01eb, its performance will be at least 1\u2212 \u01eb.\nAs our main result demonstrating the robustness of the evolutionary model itself, we show, through the simulation of query learning algorithms [9], that for every distribution D, every function class that is evolvable in the original definition, is also evolvable by an algorithm that is both (i) quasi-monotonic, and (ii) stable to some inverse polynomial amount of drift. While the definitions allow any small enough inverse polynomial drift rate, they require good performance in perpetuity, and with the same representation class for all \u01eb. Some technical complications arise as a result of the latter two requirements.\nAs a vehicle for studying the stability of specific algorithms, we show that there are natural evolutionary algorithms for linear separators over symmetric distributions and over the more general product normal distributions. Further we formulate a general result that states that for any strictly monotonic evolution algorithm, where the increase in performance at every step is defined by an inverse polynomial b, one can determine upper bounds on the polynomial parameters of the evolution algorithm, namely those that bound the generation numbers, population sizes, and sample sizes, and also a lower bound on the drift that can be resisted. We illustrate the usefulness of this formulation by applying it to show that our algorithms for linear separators can resist a significant amount of drift. We also apply it to existing algorithms for evolving conjunctions over the uniform distribution, with or without negations. We note that the advantages of evolution algorithms that use natural representations, over those obtained through simulations of query learning algorithms, may be quantified in terms of how moderate the degrees are of the polynomials that bound the generation number, population size, sample size and (inverse) drift rate of these algorithms. These results appear in Sections 6 and 7 and may be read independently of Section 5.\nAll omitted details and proofs appear in the appendix."}, {"heading": "2 The Computational Model of Evolution", "text": "In this section, we provide an overview of the original computational model of evolution (Valiant [19], where further details can be found). Many of these notions will be familiar to readers who are acquainted with the PAC model of learning [18]."}, {"heading": "2.1 Basic Definitions", "text": "Let X be a space of examples. A concept class C over X is a set of functions mapping elements in X to {\u22121, 1}. A representation class R over X consists of a set of (possibly randomized) functions from X to {\u22121, 1} described in a particular language. Throughout this paper, we think of C as the class of functions from which the ideal target f is selected, and R as a class of representations from which the evolutionary algorithm chooses an r to approximate f . We consider only classes of\nrepresentations that can be evaluated efficiently, that is, classes R such that for any r \u2208 R and any x \u2208 X , r(x) can be evaluated in time polynomial in the size of x.\nWe associate a complexity parameter n with X , C, andR. This parameter indicates the number of dimensions of each element in the domain. For example, we might define Xn to be {\u22121, 1}n, Cn to be the class of monotone conjunctions over n variables, and Rn to be the class of monotone conjunctions over n variables with each conjunction represented as a list of variables. Then C = {Cn}\u221en=1 and R = {Rn}\u221en=1 are really ensembles of classes.1 Many of our results depend on this complexity parameter n. However, we drop the subscripts when the meaning is clear from context.\nThe performance of a representation r with respect to the ideal target f is measured with respect to a distribution D over examples. This distribution represents the relative frequency with which the organism faces each set of conditions in X . Formally, for any pair of functions f : X \u2192 {\u22121, 1}, r : X \u2192 {\u22121, 1}, and distribution D over X , we define the performance of r with respect to f as\nPerff (r,D) = Ex\u223cD[f(x)r(x)] = 1\u2212 2errD(f, r) , where errD(f, r) = Prx\u223cD(f(x) 6= r(x)) is the 0/1 error between f and r. The performance thus measures the correlation between f and r and is always between \u22121 and 1.\nA new mutation is selected after each round of variation based in part on the observed fitness of the variants, i.e., their empirical correlations with the target on a polynomial number of examples. Formally, the empirical performance of r with respect to f on a set of examples x1, \u00b7 \u00b7 \u00b7 , xs chosen independently according to D is a random variable defined as (1/s)\u2211si=1 f(xi)r(xi).\nWe denote by \u01eb an accuracy parameter specifying how close to the ideal target a representation must be to be considered good. A representation r is a good approximation of f if Perff (r,D) \u2265 1\u2212\u01eb (or equivalently, if errD(f, r) \u2264 \u01eb/2). We allow the evolution algorithm to use resources that are polynomial in both 1/\u01eb and the dimension n."}, {"heading": "2.2 Model of Variation and Selection", "text": "An evolutionary algorithm E determines at each round i which set of mutations of the algorithm\u2019s current hypothesis ri\u22121 should be evaluated as candidates for ri, and how the selection will be made. The algorithm E = (R, Neigh, \u00b5, t, s) is specified by the following set of components:\n\u2022 The representation class R = {Rn}\u221en=1 specifies the space of representations over X from which the algorithm may choose functions r to approximate the target f .\n\u2022 The (possibly randomized) function Neigh(r, \u01eb) specifies for each r \u2208 Rn the set of representations r\u2032 \u2208 Rn into which r can randomly mutate. This set of representations is referred to as the neighborhood of r. For all r and \u01eb, it is required that r \u2208 Neigh(r, \u01eb) and that the size of the neighborhood is upper bounded by a polynomial.\n\u2022 The function \u00b5(r, r\u2032, \u01eb) specifies for each r \u2208 Rn and each r\u2032 \u2208 Neigh(r, \u01eb) the probability that r mutates into r\u2032. It is required that for all r and \u01eb, for all r\u2032 \u2208 Neigh(r, \u01eb), \u00b5(r, r\u2032, \u01eb) \u2265 1/p(n, 1/\u01eb) for a polynomial p.\n\u2022 The function t(r, \u01eb), referred to as the tolerance of E , determines the difference in performance that a mutation in the neighborhood of r must exhibit in order to be considered a \u201cbeneficial\u201d, \u201cneutral\u201d, or \u201cdeleterious\u201d mutation. The tolerance is required to be bounded from above and below, for all representations r, by a pair of inverse polynomials in n and 1/\u01eb.\n\u2022 Finally, the function s(r, \u01eb), referred to as the sample size, determines the number of examples used to evaluate the empirical performance of each r\u2032 \u2208 Neigh(r, \u01eb). The sample size must also be polynomial in n and 1/\u01eb.\nThe functions Neigh, \u00b5, t, and s must all be computable in time polynomial in n and 1/\u01eb. We are now ready to describe a single round of the evolution process. For any ideal target f \u2208 C, distribution D, evolutionary algorithm E = (R, Neigh, \u00b5, t, s), accuracy parameter \u01eb, and representation ri\u22121, the mutator M(f,D, E , \u01eb, ri\u22121) returns a random mutation ri \u2208 Neigh(ri\u22121, \u01eb) using the following selection procedure. First, for each r \u2208 Neigh(ri\u22121, \u01eb), the mutator computes the empirical performance of r with respect to f on a sample of size s.2 Call this v(r). Let\nBene = { r \u2223 \u2223 r \u2208 Neigh(ri\u22121, \u01eb), v(r) \u2265 v(ri\u22121) + t(ri\u22121, \u01eb) }\n1As in the PAC model, n should additionally upper bound the size of representation of the function to be learned, but for brevity we shall omit this aspect here.\n2We assume a single sample is used to evaluate the performance of all neighbors and ri\u22121, but one could interpret the model as using independent samples for each representation. This would not change our results.\nbe the set of \u201cbeneficial\u201d mutations and\nNeut = { r \u2223 \u2223 r \u2208 Neigh(ri\u22121, \u01eb), |v(r) \u2212 v(ri\u22121)| < t(ri\u22121, \u01eb) }\nbe the set of \u201cneutral\u201d mutations. If at least one beneficial mutation exists, then a mutation r is chosen from Bene as the survivor ri with relative probability \u00b5(ri\u22121, r, \u01eb). If no beneficial mutation exists, then a mutation r is chosen from Neut as the survivor ri, again with probability proportional to \u00b5(ri\u22121, r, \u01eb). Notice that, by definition, ri\u22121 is always a member of Neut, and hence a neutral mutation is guaranteed to exist."}, {"heading": "2.3 Putting It All Together", "text": "A concept class C is said to be evolvable by algorithm E over distribution D if for every target f \u2208 C, starting at any r0 \u2208 R, the sequence of mutations defined by E converges in polynomial time to a representation r whose performance with respect to f is close to 1. This is formalized as follows.\nDefinition 1 (Evolvability [19]) For a concept class C, distribution D, and evolutionary algorithm E = (R, Neigh, \u00b5, t, s), we say that C is evolvable over D by E if there exists a polynomial g(n, 1/\u01eb) such that for every n \u2208 N, f \u2208 Cn, r0 \u2208 Rn, and \u01eb > 0, with probability at least 1 \u2212 \u01eb, a sequence r0, r1, r2, \u00b7 \u00b7 \u00b7 generated by setting ri = M(f,D, E , \u01eb, ri\u22121) for all i satisfies Perff (rg(n,1/\u01eb),D) \u2265 1\u2212 \u01eb.\nWe say that the class C is evolvable over D if there exists a valid evolution algorithm E = (R, Neigh, \u00b5, t, s) such that C is evolvable over D by E . The polynomial g(n, 1/\u01eb), referred to as the generation polynomial, is an upper bound on the number of generations required for the evolution process to converge. If the above definition holds only for a particular value (or set of values) for r0, then we say that C is evolvable with initialization."}, {"heading": "2.4 Alternative Models", "text": "Various alternative formulations of the basic computational model of evolution described here have been studied. Many have been proved equivalent to the basic model in the sense that any concept class C evolvable in the basic model is evolvable in the alternative model and vice versa. Here we briefly discuss some of the variations that have been considered.\nThe performance measure Perff (r,D) is defined in terms of the 0/1 loss. Alternative performance measures based on squared loss or other loss functions have been studied in the context of evolution [10, 11, 17]. However, these alternative measures are identical to the original when f and r are (possibly randomized) binary functions, as we have assumed. (When the model is extended to allow real-valued function output, evolvability with a performance measure based on any nonlinear loss function is strictly more powerful than evolvability with the standard correlation-based performance measure [10]. We do not consider that extension in this work.)\nAlternate rules for determining how a mutation is selected have also been considered. In particular, Feldman [10] showed that evolvability using a selection rule that always chooses among the mutations with the highest or near highest empirical performance in the neighborhood is equivalent to evolvability with the original selection rule based on the classes Bene and Neut. He also discussed the performance of \u201csmooth\u201d selection rules, in which the probability of a given mutation surviving is a smooth function of its original frequency and the performance of mutations in the neighborhood.\nFinally, Feldman [9, 10] showed that fixed-tolerance evolvability, in which the tolerance t is a function of only n and 1/\u01eb but not the representation ri\u22121, is equivalent to the basic model."}, {"heading": "3 Notions of Monotonicity", "text": "Feldman [10, 11] introduced the notion of monotonic evolution in the computational model described above. His notion of monotonicity, restated here in Definition 2, requires that with high probability, the performance of the current representation ri never drops below the performance of the initial representation r0 during the evolution process.\nDefinition 2 (Monotonic Evolution) An evolution algorithm E monotonically evolves a class C over a distribution D if E evolves C over D and with probability at least 1\u2212 \u01eb, for all i \u2264 g(n, 1/\u01eb), Perff (ri,D) \u2265 Perff (r0,D), where g(n, 1/\u01eb) and r0, r1, \u00b7 \u00b7 \u00b7 are defined as in Definition 1.\nWhen explicit initialization of the starting representation r0 is prohibited, this is equivalent to requiring that Perff (ri,D) \u2265 Perff (ri\u22121,D) for all i \u2264 g(n, 1/\u01eb). In other words, it is equivalent to requiring that with high probability, performance never decreases during the evolution process.\n(Feldman showed that if representations may produce real-valued output and an alternate performance measure based on squared loss in considered, then any class C that is efficiently SQ learnable over a known, efficiently samplable distribution D is monotonically evolvable over D.)\nA stronger notion of monotonicity was used by Michael [17], who, in the context of real-valued representations and quadratic loss functions, developed an evolution algorithm for learning 1-decision lists in which only beneficial mutations are allowed. In this spirit, we define the notion of strict monotonic evolution, which requires a significant (inverse polynomial) performance increase at every round of evolution until a representation with sufficiently high performance is found.\nDefinition 3 (Strict Monotonic Evolution) An evolution algorithm E strictly monotonically evolves a class C over a distribution D if E evolves C over D and, for a polynomial m, with probability at least 1 \u2212 \u01eb, for all i \u2264 g(n, 1/\u01eb), either Perff (ri\u22121,D) \u2265 1 \u2212 \u01eb or Perff (ri,D) \u2265 Perff (ri\u22121,D) + 1/m(n, 1/\u01eb), where g(n, 1/\u01eb) and r0, r1, \u00b7 \u00b7 \u00b7 are defined as in Definition 1.\nBelow we show that a class C is strictly monotonically evolvable over a distribution D using representation class R if and only if it is possible to define a neighborhood function satisfying the property that for any r \u2208 R and f \u2208 C, if Perff (r,D) is not already near optimal, there exists a neighbor r\u2032 of r such that r\u2032 has a noticeable (again, inverse polynomial) performance improvement over r. We call such a neighborhood function strictly beneficial. The idea of strictly beneficial neighborhood functions plays an important role in developing our results in Sections 6 and 7. Feldman [11] uses a similar notion to show monotonic evolution under square loss.\nDefinition 4 (Strictly Beneficial Neighborhood Function) For a concept class C, distribution D, and representation class R, we say that a (possibly randomized) function Neigh is a strictly beneficial neighborhood function if the size of Neigh(r, \u01eb) is upper bounded by a polynomial p(n, 1/\u01eb), and there exists a polynomial b(n, 1/\u01eb) such that for every n \u2208 N, f \u2208 Cn, r \u2208 Rn, and \u01eb > 0, if Perff (r,D) < 1 \u2212 \u01eb/2, then there exists a r\u2032 \u2208 Neigh(r, \u01eb) such that Perff (r \u2032,D) \u2265 Perff (r,D) + 1/b(n, 1/\u01eb). We refer to b(n, 1/\u01eb) as the benefit polynomial.\nLemma 5 For any concept class C, distribution D, and representation class R, if Neigh is a strictly beneficial neighborhood function for C, D, and R, then there exist valid functions \u00b5, t, and s such that C is strictly monotonically evolvable over D by E = (R, Neigh, \u00b5, t, s). If a concept class C is strictly monotonically evolvable over D by E = (R, Neigh, \u00b5, t, s), then Neigh is a strictly beneficial neighborhood function for C, D, and R.\nThe proof of the second half of the lemma is immediate; the definition of strictly monotonic evolvability requires that for any initial representation r0 \u2208 R, with high probability either Perff (r0,D) \u2265 1 \u2212 \u01eb/2 or Perff (r1,D) \u2265 Perff (r0,D) + 1/m(n, 2/\u01eb) for a polynomial m. Thus if Perff (r0,D) < 1 \u2212 \u01eb/2 there must exist an r1 in the neighborhood of r0 such that Perff (r1,D) \u2265 Perff (r0,D) + 1/m(n, 2/\u01eb). The key idea behind the proof of the first half is to show that it is possible to set the tolerance t(r, \u01eb) in such a way that with high probability, Bene is never empty and there is never a representation in Bene with performance too much worse than that of the beneficial mutation guaranteed by the definition of the strictly beneficial neighborhood function. This implies that the mutation algorithm is guaranteed to choose a new representation with a significant increase in performance at each round.\nFinally, we define quasi-monotonic evolution. This is similar to the monotonic evolution, except that the performance is allowed to go slightly below that of r0. In Section 5.7, we show that this notion can be made universal, in the sense that every evolvable class is also evolvable quasimonotonically.\nDefinition 6 (Quasi-Monotonic Evolution) An evolution algorithm quasi-monotonically evolves a class C over D if E evolves C over D and with probability at least 1\u2212 \u01eb, for all i \u2264 g(n, 1/\u01eb), Perff (ri,D) \u2265 Perff (r0,D)\u2212 \u01eb, where g(n, 1/\u01eb) and r0, r1, \u00b7 \u00b7 \u00b7 are defined as in Definition 1."}, {"heading": "4 Resistance to Drift", "text": "There are many ways one could choose to formalize the notion of drift resistance. Our formalization is closely related to ideas from the work on tracking drifting concepts in the computational learning literature. The first models of concept drift were proposed around the same time by Helmbold and Long [12] and Kuh et al. [16]. In both of these models, at each time i, an input point xi is drawn from a fixed but unknown distribution D and labeled by a target function fi \u2208 C. It is assumed\nthat the error of fi with respect to fi\u22121 on D is less than a fixed value \u2206. Helmbold and Long [12] showed that a simple algorithm that chooses a concept to (approximately) minimize error over recent time steps achieves an average error of O\u0303( \u221a \u2206d) where d is the VC dimension of C.3 More\ngeneral models of drift have also been proposed [2, 3]. Let fi \u2208 C denote the ideal function on round i of the evolution process. Following Helmbold and Long [12], we make the assumption that for all i, errD(fi\u22121, fi) \u2264 \u2206 for some value \u2206. This is equivalent to assuming that Perffi\u22121(fi,D) \u2265 1 \u2212 2\u2206. Call a sequence of functions satisfying this condition a \u2206-drifting sequence. We make no other assumptions on the sequence of ideal functions.\nDefinition 7 (Evolvability with Drifting Targets) For a concept class C, distribution D, and evolution algorithm E = (R, Neigh, \u00b5, t, s), we say that C is evolvable with drifting targets over D by E if there exist polynomials g(n, 1/\u01eb) and d(n, 1/\u01eb) such that for every n \u2208 N, r0 \u2208 Rn, and \u01eb > 0, for any \u2206 \u2264 1/d(n, 1/\u01eb), and every \u2206-drifting sequence f1, f2, . . . (with fi \u2208 Cn for all i), if r0, r1, . . . is generated by E such that ri = M(fi\u22121,D, E , \u01eb, ri\u22121), then for all \u2113 \u2265 g(n, 1/\u01eb), with probability at least 1\u2212 \u01eb, Perff\u2113(r\u2113,D) \u2265 1\u2212 \u01eb. We refer to d(n, 1/\u01eb) as the drift polynomial.\nAs in the basic definition, we say that the class C is evolvable with drifting targets over D if there exists a valid evolution algorithm E = (R, Neigh, \u00b5, t, s) such that C is evolvable with drifting targets over D by E . The drift polynomial specifies how much drift the algorithm can tolerate.\nOur first main technical result, Theorem 8, relates the idea of monotonicity described above to drift resistance by showing that given a strictly beneficial neighborhood function for a class C, distribution D, and representation class R, one can construct a mutation algorithm E such that C is evolvable with drifting targets over D by E . The tolerance t and sample size s of E and the resulting generation polynomial g and drift polynomial d directly depend only on the benefit polynomial b as described below. The proof is very similar to the proof of the first half of Lemma 5. Once again the key idea is to show that it is possible to set the tolerance such that with high probability, Bene is never empty and there is never a representation in Bene with performance too much worse than the guaranteed beneficial mutation. This implies that the mutation algorithm is guaranteed to choose a new representation with a significant increase in performance with respect to the previous target fi\u22121 at each round i with high probability. As long as fi\u22121 and fi are sufficiently close, the chosen representation is also guaranteed to have good performance with respect to fi.\nTheorem 8 For any concept class C, distribution D, and representation class R, if Neigh is a strictly beneficial neighborhood function for C, D, and R, then there exist valid functions \u00b5, t, and s such that C is evolvable with drifting targets over D by E = (R, Neigh, \u00b5, t, s). In particular, if Neigh is strictly beneficial with benefit polynomial b(n, 1/\u01eb), and p(n, 1/\u01eb) is an arbitrary polynomial upper bound on the size of Neigh(r, \u01eb), then C is evolvable with drifting targets over D with \u2022 any distributions \u00b5 that satisfy \u00b5(r, r\u2032, \u01eb) \u2265 1/p(n, 1/\u01eb) for all r \u2208 Rn, \u01eb, and r\u2032 \u2208 Neigh(r, \u01eb), \u2022 tolerance function t(r, \u01eb) = 1/(2b(n, 1/\u01eb)) for all r \u2208 Rn, \u2022 any generation polynomial g(n, 1/\u01eb) \u2265 16b(n, 1/\u01eb), \u2022 any sample size s(n, 1/\u01eb) \u2265 128(b(n, 1/\u01eb))2 ln ( 2p(n, 1/\u01eb)g(n, 1/\u01eb)/\u01eb ) , and\n\u2022 any drift polynomial d(n, 1/\u01eb) \u2265 16b(n, 1/\u01eb), which allows drift \u2206 \u2264 1/(16b(n, 1/\u01eb)).\nIn Sections 6 and 7, which can be read independent of Section 5, we appeal to this theorem in order to prove that some common concept classes are evolvable with drifting targets with relatively large values of \u2206. Using Lemma 5, we also obtain the following corollary.\nCorollary 9 If a concept class C is strictly monotonically evolvable over D, then C is evolvable with drifting targets over D."}, {"heading": "5 Robustness Results", "text": "Feldman [9] proved that the original model of evolvability is equivalent to a restriction of the statistical query model of learning [15] known as learning by correlational statistical queries (CSQ) [5]. We extend Feldman\u2019s analysis to show that CSQ learning is also equivalent to both evolvability with drifting targets and quasi-monotonic evolvability, and so the notion of evolvability is robust to these changes in definition. We begin by briefly reviewing the CSQ model.\n3Throughout the paper, we use the notation O\u0303 to suppress logarithmic factors."}, {"heading": "5.1 Learning from Correlational Statistical Queries", "text": "The statistical query (SQ) model was introduced by Kearns [15] and has been widely studied due to its connections to learning with noise [1, 4]. Like the PAC model, the goal of an SQ learner is to produce a hypothesis h that approximates the behavior of a target function f with respect to a fixed but unknown distribution D. Unlike the PAC model, the learner is not given direct access to labeled examples \u3008x, f(x)\u3009, but is instead given access to a statistical query oracle. The learner submits queries of the form (\u03c8, \u03c4) to the oracle, where \u03c8 : X \u00d7{\u22121, 1} \u2192 [\u22121, 1] is a query function and \u03c4 \u2208 [0, 1] is a tolerance parameter. The oracle responds to each query with any value v such that |Ex\u223cD[\u03c8(x, f(x))] \u2212 v| \u2264 \u03c4 . An algorithm is said to efficiently learn a class C in the SQ model if for all n \u2208 N, \u01eb > 0, and f \u2208 Cn, and every distribution Dn over Xn, the algorithm, given access to \u01eb and the SQ oracle for f and Dn, outputs a polynomially computable hypothesis h in polynomial time such that err(f, h) \u2264 \u01eb. Furthermore it is required that each query (\u03c8, \u03c4) made by the algorithm can be evaluated in polynomial time given access to f and Dn. It is known that any class efficiently learnable in the SQ model is efficiently learnable in the PAC model with label noise [15].\nA query (\u03c8, \u03c4) is called a correlational statistical query (CSQ) [5] if \u03c8(x, f(x)) = \u03c6(x)f(x) for some function \u03c6 : X \u2192 [\u22121, 1]. An algorithm A is said to efficiently learn a class C in the CSQ model if A efficiently learns C in the SQ model using only correlational statistical queries.\nIt is useful to consider one additional type of query, the CSQ> query [9]. A CSQ> query is specified by a triple (\u03c6, \u03b8, \u03c4), where \u03c6 : X \u2192 [\u22121, 1] is a query function, \u03b8 is a threshold, and \u03c4 \u2208 [0, 1] is a tolerance parameter. When presented with such a query, a CSQ> oracle for target f and distribution D returns 1 if Ex\u223cD[\u03c6(x)f(x)] \u2265 \u03b8+ \u03c4 , 0 if Ex\u223cD[\u03c6(x)f(x)] \u2264 \u03b8\u2212 \u03c4 , and arbitrary value of either 1 or 0 otherwise. Feldman [9] showed that if there exists an algorithm for learning C over D that makes CSQs, then there exists an algorithm for learning C over D using CSQ>s of the form (\u03c6, \u03b8, \u03c4) where \u03b8 \u2265 \u03c4 for all queries. Furthermore the number of queries made by this algorithm is at most O(log(1/\u03c4)) times the number of queries made by the original CSQ algorithm."}, {"heading": "5.2 Overview of the Reduction", "text": "The construction we present uses Feldman\u2019s simulation [9] repeatedly. Fix a concept class C and a distribution D such that C is learnable over D in the CSQ model. As mentioned above, this implies that there exists a CSQ> algorithm A for learning C over D. Let H be the class of hypotheses from which the output of A is chosen. In the analysis that follows, we restrict our attention to the case in which A is deterministic. However, the extension of our analysis to randomized algorithms is straightforward using Feldman\u2019s ideas (see Lemma 4.7 in his paper [9]).\nFirst, we present a high level outline of our reduction. Throughout this section we will use randomized Boolean functions. If \u03c8 : X \u2192 [\u22121, 1] is a real valued function, let \u03a8 denote the randomized Boolean function such that for every x, E[\u03a8(x)] = \u03c8(x). It can be easily verified that for any function \u03c6(x), Ex,\u03a8[\u03c6(x)\u03a8(x)] = Ex[\u03c6(x)\u03c8(x)]. For the rest of this section, we will abuse notation and simply write real-valued functions in place of the corresponding randomized Boolean functions.\nOur representation is of the form r = (1\u2212\u01eb/2)h+(\u01eb/2)\u03a6. Here h is a hypothesis from H and \u03a6 is function that encodes the state of the CSQ> algorithm that is being simulated. Feldman\u2019s simulation only uses the second part. Our simulation runs in perpetuity, restarting Feldman\u2019s simulation each time it has completed. Since the target functions are drifting over time, if h has high performance with respect to the current target function, it will retain the performance for some time steps in the future, but not forever. During this time, Feldman\u2019s simulation on the \u03a6 part produces a new hypothesis h\u2032 which has high performance at the time this simulation is completed. At this time, we will transition to a representation r\u2032 = (1\u2212 \u01eb/2)h\u2032 + (\u01eb/2)\u03a6, where \u03a6 is reset to start Feldman\u2019s simulation anew. Thus, although the target drifts, our simulation will continuously run Feldman\u2019s simulation to find a hypothesis that has a high performance with respect to the current target.\nThe rest of section 5 details the reduction. First, we show how a single run of A is simulated, which is essentially Feldman\u2019s reduction with minor modifications. Then we discuss how to restart this simulation once it has completed. This requires the addition of certain intermediate states to keep the reduction feasible in the evolution model. We also show that our reduction can be made quasi-monotonic. Finally, we show how all this can be done using a representation class that is independent of \u01eb, as is required. This last step is shown in the appendix."}, {"heading": "5.3 Construction of the Evolutionary Algorithm", "text": "We describe the construction of our evolutionary algorithm E . Let \u03c4 = \u03c4(n, 1/\u01eb) be a polynomial lower bound on the tolerance of the queries made by A when run with accuracy parameter \u01eb/4. Without loss of generality, we may assume all queries are made with this tolerance. Let q = q(n, 1/\u01eb)\nbe a polynomial upper bound on the number of queries made by A, and assume that Amakes exactly q queries (if not, redundant queries can be added). Here, we allow our representation class to be dependent on \u01eb. However, this restriction may be removed (cf. Appendix A.7.1) In the remainder of this section we drop the subscripts n and \u01eb, except where there is a possibility of confusion.\nFollowing Feldman\u2019s notation, let z denote a bit string of length q which records the oracle responses to the queries made by A; that is, the ith bit of z is 1 if and only if the answer to the ith query is 1. Let |z| denote the length of z, zi the prefix of z of length i, and zi the ith bit of z. Since A is deterministic, the ith query made by A depends only on responses to the previous i\u22121 queries. We denote this query by (\u03c6zi\u22121 , \u03b8zi\u22121 , \u03c4), with \u03b8zi\u22121 \u2265 \u03c4 , as discussed in Section 5.1. Let hz denote the final hypothesis output by A given query responses z. Since we have chosen to simulate A with accuracy parameter \u01eb/4, hz is guaranteed to satisfy Perff (hz ,D) \u2265 1 \u2212 \u01eb/4 for any function f for which the query responses in z are valid. Finally, let \u03c3 denote the empty string.\nFor every i \u2208 {1, \u00b7 \u00b7 \u00b7 , q} and z \u2208 {0, 1}i, we define \u03a6z = (1/q) \u2211i\nj=1 I(zj = 1)\u03c6zj\u22121 (x), where I is an indicator function that is 1 if its input is true and 0 otherwise. For any h \u2208 H, define r\u01eb[h, z] = (1\u2212 \u01eb/2)h(x) + (\u01eb/2)\u03a6z(x). Recall that each of these real-valued functions can be treated as a randomized Boolean function as required by the evolution model. The performance of this function, which we use as our basic representation, is mainly determined by the performance of h, but by setting the tolerance parameter low enough, the \u03a6z part can learn useful information about the (drifting) targets by simulating A.\nLet R\u0303\u01eb = {r\u01eb[h, z] | h \u2208 H, 0 \u2264 |z| \u2264 q \u2212 1}. The representations in R\u0303\u01eb will be used for simulating one round of A. To reach a state where we can restart the simulation, we will need to add intermediate representations. These are defined below.\nLet tu(n, 1/\u01eb) be an upper bound on \u01eb\u03b8zi/(8q) for all i and z i. (This will be a polynomial upper bound on all tolerances t that we define below.) Assume for simplicity that K = 2/tu(n, 1/\u01eb) is an integer. Let w0 = r\u01eb[h, z], for some h \u2208 H and |z| = q (w0 depends on h and z, but to keep notation simple we will avoid subscripts). For k = 1, . . . ,K, define wk = (1\u2212 k(tu(n, 1/\u01eb)/2))w0. Notice that wK = 0, where 0 is a function that can be realized by a randomized function that ignores its input and predicts +1 or \u22121 randomly. Let W\u01eb = {wi | w0 = r\u01eb[h, z], h \u2208 H, |z| = q, i \u2208 {0, . . . ,K}}. Finally define R\u01eb = R\u0303\u01eb \u222aW\u01eb. For every representation r\u01eb[h, z] \u2208 R\u0303\u01eb, we set \u2022 Neigh(r\u01eb[h, z], \u01eb) = {r\u01eb[h, z], r\u01eb[h, z0], r\u01eb[h, z1]}, \u2022 \u00b5(r\u01eb[h, z], r\u01eb[h, z], \u01eb) = \u03b7 and \u00b5(r\u01eb[h, z], r\u01eb[h, z0], \u01eb)=\u00b5(r, r\u01eb[h, z1], \u01eb)=(1\u2212 \u03b7)/2, \u2022 t(r\u01eb[h, z], \u01eb) = \u01eb\u03b8zi/(8q). For the remaining representations wk \u2208 W\u01eb, with w0 = r\u01eb[h, z], we set \u2022 Neigh(wK , \u01eb) = {wK , r\u01eb[0, \u03c3]} and Neigh(wk, \u01eb) = {wk, wk+1, r\u01eb[hz,\u01eb, \u03c3]} for all k < K, \u2022 \u00b5(wK , wK) = \u03b7 and \u00b5(wK , r\u01eb[0, \u03c3]) = 1\u2212 \u03b7, and \u00b5(wk, wk, \u01eb) = \u03b72, \u00b5(wk, wk+1, \u01eb) = \u03b7\u2212 \u03b72, and \u00b5(wk, r\u01eb[hz,\u01eb, \u03c3]) = 1\u2212 \u03b7 for all k < K,\n\u2022 t(wk, \u01eb) = tu(n, 1/\u01eb). Finally, let \u03b7 = \u01eb/(4q+2K), \u03c4 \u2032 = min{(\u01eb\u03c4)/(2q), tu(n, 1/\u01eb)/8}, and s = 1/(2(\u03c4 \u2032)2) log((6q+3K)/\u01eb). Let E = (R\u01eb, Neigh, \u00b5, t, s) with components defined as above. We show that E evolves C over D tolerating drift of \u2206 = (\u01eb\u03c4)/(4q+2K +2). This value of drift, while small, is an inverse polynomial in n and 1/\u01eb as required. The point to note is that the evolutionary algorithm runs perpetually, while still maintaining high performance on any given round with high probability.\nFor any representation r, we denote by LPE the union of the low probability events that some estimates of performance are not within \u03c4 \u2032 of their true value, or that a mutation with relative probability less than 2\u03b7 (either in Bene or Neut) is selected over other mutations.\n5.4 Simulating the CSQ> Algorithm for Drifting Targets\nWe now show that it is possible to simulate a CSQ> algorithm using an evolution algorithm E even when the target is drifting. However, if we simulate a query (\u03c6, \u03b8, \u03c4) on round i, there is no guarantee that the answer to this query will remain valid in future rounds. The following lemma shows that by lowering the tolerance of the simulated query below the tolerance that is actually required by the CSQ> algorithm, we are able to generate a sequence of query answers that remain valid over many rounds. Specifically, it shows that if v is a valid response for the query (\u03c6, \u03b8, \u03c4/2) with respect to fi, then v is also a valid response for the query (\u03c6, \u03b8, \u03c4) with respect to fj for any j \u2208 [i \u2212 \u03c4/(2\u2206), i+ \u03c4/(2\u2206)]. Lemma 10 Let f1, f2, \u00b7 \u00b7 \u00b7 be a \u2206-drifting sequence with respect to the distribution D over X . For any tolerance \u03c4 , any threshold \u03b8, any indices i and j such that |i \u2212 j| \u2264 \u03c4/(2\u2206), and any function \u03c6 : X \u2192 [\u22121, 1], if Ex\u223cD[\u03c6(x)fj(x)] \u2265 \u03b8 + \u03c4 , then Ex\u223cD[\u03c6(x)fi(x)] \u2265 \u03b8 + \u03c4/2. Similarly, if Ex\u223cD[\u03c6(x)fj(x)] \u2264 \u03b8 \u2212 \u03c4 , then Ex\u223cD[\u03c6(x)fi(x)] \u2264 \u03b8 \u2212 \u03c4/2.\nWe say that a string z is consistent with a target function f , if for all 1 \u2264 i \u2264 |z|, zi is a valid response to the query (\u03c6zi\u22121 , \u03b8zi\u22121 , \u03c4), with respect to f . Suppose that the algorithm E starts with representation r0 = r\u01eb[h, \u03c3]. (Recall that \u03c3 denotes the empty string.) The following lemma shows that after q time steps, with high probability it will reach a representation r\u01eb[h, z] where |z| = q and z is consistent with the target function fq, implying that z is a proper simulation of A on fq.\nLemma 11 If \u2206 \u2264 \u03c4/(2q), then for any \u2206-drifting sequence f0, f1, . . . , fq, if r0, r1, . . . , rq is the sequence of representations of E starting at r0 = r\u01eb[h, \u03c3], and if the LPE does not occur for q rounds, then rq = r\u01eb[h, z] where |z| = q and z is consistent with fq.\nThe proof uses the following ideas: If the LPE does not occur, there are no mutations of the form r \u2192 r, so the length of z increases by 1 every round, and also all estimates of performance are within \u03c4 \u2032 of their true value. When this is the case, and after observing that r\u01eb[h, zi0] is always neutral, it is possible to show that for any round i, (i) if r\u01eb[h, z\ni1] is beneficial, then 1 is a valid answer to the ith query with respect to fi, (ii) if r\u01eb[h, z\ni1] is deleterious then 0 is a valid answer for the ith query with respect to fi, and (c) if r\u01eb[h, z\ni1] is neutral, then both 0 and 1 are valid answers to the ith query. This implies that zi+1 is always a valid answer to the ith query with respect to fi, and by Lemma 10, with respect to fq."}, {"heading": "5.5 Restarting the Simulation", "text": "We now discuss how to restart Feldman\u2019s simulation once it completes. Suppose we are in a representation of the form r\u01eb[h, z], where |z| = q, and z is consistent with the current target function f . Then if hz is the hypothesis output by A using query responses in z, we are guaranteed that (with high probability) Perff (hz ,D) \u2265 1\u2212\u01eb/4. At this point, we would like the algorithm to choose a new representation r\u01eb[hz, \u03c3], where \u03c3 is the empty string. The intuition behind this move is as follows. The performance of r\u01eb[hz, \u03c3] is guaranteed to be high (and to remain high for many generations) because much of the weight is on the hz term. Thus we can use the second term (\u03a6\u03c3) to restart the learning process. After q more time steps have passed, it may be the case that the performance of hz is no longer as high with respect to the new target, but the simulated algorithm will have already found a different hypothesis that does have high performance with respect to this new target.\nThere is one tricky aspect of this approach. In some circumstances, we may need to restart the simulation by moving from r\u01eb[h, z] to r\u01eb[hz, \u03c3] even though z is not consistent with f . This situation can arise for two reasons. First, we might be near the beginning of the evolution process when E has not had enough generations to correctly determine the query responses (starting state may be r\u01eb[h, z0] where z0 has wrong answers). Second, there is some small probability of failure on any given round and we would like the evolutionary algorithm to recover from such failures smoothly. In either case, to handle the situation in which hz may have performance below zero (or very close), we will also allow r\u01eb[h, z] to mutate to r\u01eb[0, \u03c3].\nThe required changes from r\u01eb[h, z] to either r\u01eb[hz, \u03c3] or r\u01eb[0, \u03c3] described above may be deleterious. To handle this, we employ a technique of Feldman [9], where we first decrease the performance gradually (through neutral mutations) until these mutations are no longer deleterious. The representations defined in W\u01eb achieve this. The claim is that starting from any representation of the form wk, we reach either r\u01eb[hz, \u03c3] or r\u01eb[0, \u03c3] in at most K \u2212 k + 1 steps, with high probability. Furthermore, since the probability of moving to r\u01eb[hz, \u03c3] is very high, this representation will be reached if it is ever a neutral mutation (i.e., the LPE does not happen). Thus, the performance always stays above the performance of r\u01eb[hz, \u03c3]. Lemma 12 formalizes this claim.\nLemma 12 If \u2206 \u2264 tu(n, 1/\u01eb)/4, then for any \u2206-drifting sequence f0, f1, . . . , fq, if r0, r1, . . . , rq is the sequence of mutations of E starting at r0 = wk, then if the LPE does not happen at any time-step, there exists a j \u2264 K\u2212k+1 such that rj = r\u01eb[hz,\u01eb, \u03c3] or rj = r\u01eb[0, \u03c3]. Furthermore, for all 1 \u2264 i < j, Perffi(ri,D) \u2265 Perffi(r\u01eb[hz,\u01eb, \u03c3],D)."}, {"heading": "5.6 Equivalence to Evolvability with Drifting Targets", "text": "Combining these results, we prove the equivalence between evolvability and evolvability with drifting targets starting from any representation in R\u01eb. The proof we give here uses the representation class R\u01eb and therefore assumes that the value of \u01eb is known. For the needed generalization to the case where R = \u222a\u01ebR\u01eb, Feldman\u2019s backsliding trick [9] can be used to first reach a representation with zero performance, and then move to a representation in R\u01eb. Theorem 13 shows that every concept class that is learnable using CSQs (and thus every class that is evolvable) is evolvable with drifting targets.\nTheorem 13 If C is evolvable over distribution D, then C is evolvable with drifting targets over D. Proof: Let A be a CSQ> algorithm for learning C over D with accuracy \u01eb/4. A makes q = q(n, 1/\u01eb) queries of tolerance \u03c4 and outputs h satisfying Perff (h,D) \u2265 1 \u2212 \u01eb/4. Let E be the evolutionary algorithm derived fromA as described in Section 5.3. Recall thatK = 2/tu(n, 1/\u01eb), let g = 2q+K+1. We show that starting from an arbitrary representation r0 \u2208 R\u01eb, with probability at least 1 \u2212 \u01eb, Perffg (rg,D) \u2265 1 \u2212 \u01eb. This is sufficient to show that for all \u2113 \u2265 g, with probability at least 1 \u2212 \u01eb, Perff\u2113(r\u2113,D) \u2265 1\u2212 \u01eb, since we can consider the run of E starting from r\u2113\u2212g.\nWith the setting of parameters as described in Section 5.3, with probability at least 1 \u2212 \u01eb, the LPE does not occur for g time steps, i.e., all estimates are within \u03c4 \u2032 = min{(\u03c4\u01eb)/(2q), tu(n, 1/\u01eb)/8} of their true value and unlikely mutations (those with relative probabilities less that 2\u03b7) are not chosen. Thus, we can apply the results of Lemmas 11 and 12. We assume that this is the case for the rest of the proof. When \u2206 = (\u01eb\u03c4)/(4q+2K+2), the assumption of Lemmas 11 and 12 hold and we can apply them.\nFirst, we argue that starting from an arbitrary representation, in at most q +K steps, we will have reached a representation of the form r\u01eb[h, \u03c3], for some h \u2208 H. If the start representation is r\u01eb[h, z] for |z| \u2264 q \u2212 1, then in at most q \u2212 1 steps we reach a representation of the form r\u01eb[h, z\u2032] with |z\u2032| = q, in which case by Lemma 12, the algorithm will transition to representation r\u01eb[h, \u03c3] in at most K + 1 additional steps. Alternately, if the start representation is wk for k \u2208 {0, . . . ,K} as defined in Section 5.3, then by Lemma 12, we reach a representation of the form r\u01eb[h, \u03c3] in at most K + 1 steps.\nLet m be the time step when E first reaches the representation of the form r\u01eb[h, \u03c3]. Then using Lemma 11, rm+q = r\u01eb[h, z\n\u2217], where z\u2217 is consistent with fm+q. Let h\u2217 = hz\u2217,\u01eb be the hypothesis output by the simulated run of A. Then Perffm+q (h\u2217,D) \u2265 1\u2212 \u01eb/4, and hence Perffm+q (r\u01eb[h\u2217, \u03c3],D) \u2265 1\u2212 3\u01eb/4. For the value of \u2206 we are using, for all i \u2264 g, Perffi(r\u01eb[h\u2217, \u03c3],D) \u2265 1\u2212 \u01eb.\nFrom such a representation, when all estimates of performance are within \u03c4 \u2032 of their true value and unlikely mutations (those with relative probability \u2264 2\u03b7) do not occur, the performance will remain above 1 \u2212 \u01eb. By Lemma 12, the algorithm will move from rm+q = r\u01eb[h, z\u2217] to r\u01eb[h\u2217, \u03c3] in at most K + 1 steps, and during these time steps for any time step i it holds that Perffi(ri,D) \u2265 Perffi(r\u01eb[h\n\u2217, \u03c3],D). Once r\u01eb[h\u2217, \u03c3] is reached, for q steps the representations will be of the form r\u01eb[h\n\u2217, z]. For any such time step i, Perffi(ri,D) \u2265 Perffi(r\u01eb[h\u2217, \u03c3],D). This is because if the answers in z are correct (and they will be since the LPE does not happen at any time step), the term \u03a6z is made up of only those functions \u03c6zj\u22121 for which z\nj = 1, which are those for which \u03c6zj\u22121 has a correlation greater than \u03b8zj\u22121 \u2212 \u03c4 \u2265 0 with the target fi (using Lemma 10). Since as observed above the performance of r\u01eb[h\n\u2217, \u03c3] does not degrade below 1\u2212 \u01eb in the time horizon we are interested in Perffi(ri,D) \u2265 Perffi [r\u01eb[h\u2217, \u03c3]) \u2265 1\u2212 \u01eb."}, {"heading": "5.7 Equivalence to Quasi-Monotonic Evolution", "text": "Finally, we show that all evolvable classes are also evolvable quasi-monotonically. In the proof of Theorem 13, we showed that for all \u2113 \u2265 g = 2q+K+1, with high probability Perff\u2113(r\u2113,D) \u2265 1\u2212\u01eb, so quasi-monotonicity is satisfied trivially. Thus we only need to show quasi-monotonicity for the first g steps. We will use the same construction as defined in Section 5.3, with modifications. However, this assumes that the representation knows \u01eb, since now the trick of having the performance slide back to zero would violate quasi-monotonicity. To make the representation class independent of \u01eb a more complex construction is needed. Details can be found in the appendix.\nTheorem 14 If C is evolvable over distribution D, then C is quasi-monotonically evolvable over D with drifting targets."}, {"heading": "6 Evolving Hyperplanes with Drifting Targets", "text": "In this section, we present two alternative algorithms for evolving n-dimensional hyperplanes with drifting targets. The first algorithm, which generates the neighbors of a hyperplane by rotating it a small amount in one of 2(n \u2212 1) directions, tolerates drift on the order of \u01eb/n, but only over spherically symmetric distributions. The second algorithm, which generates the neighbors of a hyperplane by shifting single components of its normal vector, tolerates a smaller drift, but works when the distribution is an unknown product normal distribution. To our knowledge, these are the first positive results on evolving hyperplanes in the computational model of evolution.\nFormally, let Cn be the class of all n-dimensional homogeneous linear separators.4 For notational convenience, we reference each linear separator in Cn by the hyperplane\u2019s n-dimensional unit length\n4A homogeneous linear separator is one that passes through the origin. [6]\nnormal vector f \u2208 Rn. For every f \u2208 Cn and x \u2208 Rn, we then have that f(x) = 1 if f \u00b7 x \u2265 0, and f(x) = \u22121 otherwise. The evolution algorithms we consider in this section use a representation class Rn also consisting of n-dimensional unit vectors, where r \u2208 Rn is the normal vector of the hyperplane it represents.5 Then R = {r | \u2016r\u20162 = 1}. We describe the two algorithms in turn."}, {"heading": "6.1 An Evolution Algorithm Based on Rotations", "text": "For the rotation-based algorithm, we define the neighborhood function of r \u2208 Rn as follows. Let {u1 = r,u2, \u00b7 \u00b7 \u00b7 ,un} be an orthonormal basis for Rn. This orthonormal basis can be chosen arbitrarily (and potentially randomly) as long as u1 = r. Then\nNeigh(r, \u01eb) = r \u222a { r\u2032 \u2223 \u2223 r\u2032 = cos ( \u01eb/(\u03c0 \u221a n) ) r\u00b1 sin ( \u01eb/(\u03c0 \u221a n) ) ui , i \u2208 {2, \u00b7 \u00b7 \u00b7 , n} } . (1)\nIn other words, each r\u2032 \u2208 Neigh(r, \u01eb) is obtained by rotating r by an angle of \u01eb/(\u03c0\u221an) in some direction. The size of this neighbor set is clearly 2n\u2212 1. We obtain the following theorem.\nTheorem 15 Let C be the class of homogeneous linear separators, R be the class of homogeneous linear separators represented by unit length normal vectors, and D be an arbitrary spherically symmetric distribution. Define Neigh as in Equation 1 and let p be any polynomial satisfying p(n, 1/\u01eb) \u2265 2n\u22121. Then C is evolvable with drifting targets over D by algorithm A = (R, Neigh, \u00b5, t, s) with \u2022 any distributions \u00b5 that satisfy \u00b5(r, r\u2032, \u01eb) \u2265 1/p(n, 1/\u01eb) for all r \u2208 Rn, \u01eb, and r\u2032 \u2208 Neigh(r, \u01eb), \u2022 tolerance function t(r, \u01eb) = \u01eb/(\u03c03n) for all r \u2208 Rn, \u2022 any generation polynomial g(n, 1/\u01eb) \u2265 8\u03c03n/\u01eb, \u2022 a sample size s(n, 1/\u01eb) = O\u0303(n2/\u01eb2), and \u2022 any drift polynomial d(n, 1/\u01eb) \u2265 8\u03c03n/\u01eb, which allows drift \u2206 \u2264 \u01eb/(8\u03c03n).\nTo prove this, we need only to show that Neigh is a strictly beneficial neighborhood function for C, D, and R with b(n, 1/\u01eb) = \u03c03n/(2\u01eb). The theorem then follows from Theorem 8. The analysis relies on the fact that under any spherically symmetric distribution D (for example, the uniform distribution over a sphere), errD(u,v) = arccos(u \u00b7v)/\u03c0, where arccos(u \u00b7v) is the angle between u and v [6]. This allows us to reason about the performance of one function with respect to another by analyzing the dot product between their normal vectors."}, {"heading": "6.2 A Component-Wise Evolution Algorithm", "text": "We now describe the alternate algorithm for evolving homogeneous linear separators. The guarantees we achieve are inferior to those described in the previous section. However, this algorithm applies when D is any unknown product normal distribution (with polynomial variance) over Rn.\nLet ri and fi denote the ith components of r and f respectively (not the values of the representation and ideal function at round i as in previous sections). The alternate algorithm is based on the following observations. First, whenever there exists some i for which ri and fi have different signs and aren\u2019t too close to 0, we can obtain a new representation with a non-trivial increase in performance by flipping the sign of ri. Second, if there are no beneficial sign flips, if there is some i for which ri is not too close to fi, we can obtain a new representation with a significant increase in performance by adjusting ri a little and renormalizing. The amount we must adjust ri depends on the standard deviation of D in the ith dimension, so we must try many values when D is unknown. Finally, if the above conditions do not hold, then the performance of r is already good enough.\nDenote by {ei}ni=1 the basis of Rn. Let \u03c31, . . . , \u03c3n be the standard deviation of the distribution D in the n dimensions. We assume that 1 \u2265 \u03c3i \u2265 (1/n)k for some constant k for all i, and that the algorithm is given access to the value of k, but not the particular values \u03c3i. We define the neighborhood function as Neigh(r, \u01eb) = Nfl \u222aNsl, where Nfl = {r\u2212 2riei | i = 1, . . . , d} is the set of representations obtained by flipping the sign of one component of r, and\nNsl =\n\n\n\nr \u00b1 j\u01eb2 12nk \u221a n ei\n\u2016r \u00b1 j\u01eb2 12nk \u221a n ei\u20162\n\u2223\n\u2223\n\u2223\n\u2223\n\u2223\n\u2223\ni \u2208 {1, \u00b7 \u00b7 \u00b7 , d}, j \u2208 {1, \u00b7 \u00b7 \u00b7 , 4nk}\n\n\n\nis the set obtained by shifting each component by various amounts. We obtain the following.\n5Technically we must assume that the representations r \u2208 Rn and input points x \u2208 R n are expressed to a fixed finite precision so that r \u00b7x is guaranteed to be computable in polynomial time, but for simplicity, in the analysis that follows, we treat both as simply vectors of real numbers.\nTheorem 16 Let C be the class of homogeneous linear separators, and R be the class of homogeneous linear separators represented by unit length normal vectors, and D be a product normal distribution with (unknown) standard deviations \u03c31, \u00b7 \u00b7 \u00b7 , \u03c3n such that 1 \u2265 \u03c3i \u2265 (1/n)k for all i for a constant k. Define Neigh as above and let p be any polynomial such that p(n, 1/\u01eb) \u2265 8n2k+1 + 2n. Then C is evolvable with drifting targets over D by algorithm A = (R, Neigh, \u00b5, t, s) with \u2022 any distribution \u00b5 satisfying \u00b5(r, r\u2032, \u01eb) \u2265 1/p(n, 1/\u01eb) for all r \u2208 Rn and r\u2032 \u2208 Neigh(r, \u01eb), \u2022 tolerance function t(r, \u01eb) = \u01eb6/(288n), \u2022 any generation polynomial g(n, 1/\u01eb) \u2265 2304n/\u01eb6, \u2022 a sample size s(n, 1/\u01eb) = O\u0303(n2/\u01eb12), and \u2022 any drift polynomial d(n, 1/\u01eb) \u2265 2304n/\u01eb6, which allows drift \u2206 \u2264 \u01eb6/(2304n).\nThe proof formalizes the set of observations described above, using them to show that Neigh is a strictly beneficial neighborhood function for C, D, and R with b(n, 1/\u01eb) = 144n/\u01eb6. The theorem is then an immediate consequence of Theorem 8."}, {"heading": "7 Evolving Conjunctions with Drifting Targets", "text": "We now show that conjunctions are evolvable with drifting targets over the uniform distribution with a drift of O(\u01eb2), independent of n. We begin by examining monotone conjunctions and prove that the neighborhood function defined by Valiant [19] is a strictly beneficial neighborhood function with b(n, 1/\u01eb) = \u01eb2/9. Our proof uses techniques similar to those used in the simplified analysis of Valiant\u2019s algorithm presented by Diochnos and Tura\u0301n [8]. By building on ideas from Jacobson [14], we extend this result to show that general conjunctions are evolvable with the same rate of drift."}, {"heading": "7.1 Monotone Conjunctions", "text": "We represent monotone conjunctions using a representation class R where each r \u2208 R is a subset of {1, \u00b7 \u00b7 \u00b7 , n} such that |r| \u2264 log2(3/\u01eb), representing the conjunction of the variables xj for all j \u2208 r. We therefore allow the representation class to depend on \u01eb in our analysis. This dependence is easy to remove (e.g., using Valiant\u2019s technique of allowing an initial phase in which the length of the representation decreases until it is below log2(3/\u01eb) [19]), but simplifies presentation.\nThe neighborhood of a representation r consists of the set of conjunctions that are formed by adding a variable to r, removing a variable from r, and swapping a variable in r with a variable not in r, plus the representation r itself. Formally, define the following three sets of conjunctions: N+(r) = {r \u222a {j}|j 6\u2208 r}, N\u2212(r) = {r \\ {j}|j \u2208 r}, and N\u00b1(r) = {r \\ {j} \u222a {k}|j \u2208 S, k 6\u2208 S}. The neighborhood Neigh(r, \u01eb) is then defined as follows. Let q = \u2308log2(3/\u01eb)\u2309. If r is the empty set, then Neigh(r, \u01eb) = N+(r) \u222a r. If 0 < |r| < q, then Neigh(r, \u01eb) = N+(r) \u222a N\u2212(r) \u222aN\u00b1(r) \u222a r. Finally, if |r| = q, then Neigh(r, \u01eb) = N\u2212(r) \u222a N\u00b1(r) \u222a r. Note that the size of the neighborhood is bounded by 1+n+n2/4 in the worst case; the combined size of the sets N+(r) and N\u2212(r) is at most n, and the size of N\u00b1(r) is at most n2/4. We obtain the following theorem.\nTheorem 17 Let C be the class of monotone conjunctions, R be the class of monotone conjunctions of size at most q = \u2308log2(3/\u01eb)\u2309 represented as subsets of indices, and D be the uniform distribution. Define Neigh as above and let p be any polynomial satisfying p(n, 1/\u01eb) \u2265 1 + n + n2/4. Then C is evolvable with drifting targets over D by algorithm A = (R, Neigh, \u00b5, t, s) with \u2022 any distributions \u00b5 that satisfy \u00b5(r, r\u2032, \u01eb) \u2265 1/p(n, 1/\u01eb) for all r \u2208 Rn, \u01eb, and r\u2032 \u2208 Neigh(r, \u01eb), \u2022 tolerance function t(r, \u01eb) = \u01eb2/18 for all r \u2208 Rn, \u2022 any generation polynomial g(n, 1/\u01eb) \u2265 144/\u01eb2, \u2022 a sample size s(n, 1/\u01eb) = O\u0303(1/\u01eb2), and \u2022 any drift polynomial d(n, 1/\u01eb) \u2265 144/\u01eb2, which allows drift \u2206 \u2264 \u01eb2/144.\nTo prove the theorem, we show that Neigh is a strictly beneficial target function with benefit polynomial b(n, 1/\u01eb) = 9/\u01eb2 and once again appeal to Theorem 8. The proof is then essentially just a case-by-case analysis of the performance of the best r\u2032 \u2208 Neigh(r, \u01eb) for an exhaustive set of conditions on r and f ."}, {"heading": "7.2 General Conjunctions", "text": "Jacobson [14] proposed an extension to the algorithm above that applies to general conjunctions. The key innovation in his algorithm is the addition of a fourth set N \u2032(r) to the neighborhood or r,\nwhere each r\u2032 \u2208 N \u2032(r) is obtained by negating a subset of the literals in r. We show here that the drift rate of his construction can be analyzed in a similar way to the monotone case.\nWe represent general conjunctions using a representation class R where each r \u2208 R is a subset of {1, \u00b7 \u00b7 \u00b7 , n} \u222a {\u22121, \u00b7 \u00b7 \u00b7 ,\u2212n} such that |r| \u2264 log2(3/\u01eb). Here each r represents the conjunction of literals xj for all positive j \u2208 r and negated literals x\u2212j for all negative j \u2208 r, and we restrict R so that it is never the case that both j \u2208 r and \u2212j \u2208 r. The dependence of this representation class on \u01eb can be removed as before.\nAs before, the neighborhood of a representation r includes the set of conjunctions that are formed by adding a variable to r, removing a variable from r, and swapping a variable in r with a variable not in r, plus the representation r itself. However, it now also includes a fourth set N \u2032(r) of all conjunctions that can be obtained by negating a subset of the literals of r. The size of the set N \u2032(r) is at most 2q \u2264 6/\u01eb, so by a similar argument to the one above, the size of the neighborhood is bounded by 1 + 2n+ n2 + 6/\u01eb. We obtain the following theorem.\nTheorem 18 Let C be the class of conjunctions, R be the class of conjunctions of at most q = \u2308log2(3/\u01eb)\u2309 literals represented as above, and D be the uniform distribution. Define Neigh as above and let p be any polynomial satisfying p(n, 1/\u01eb) \u2265 1+2n+n2+6/\u01eb. Then C is evolvable with drifting targets over D by A = (R, Neigh, \u00b5, t, s) with \u00b5, t, g, s, and d as specified in Theorem 17.\nThe proof uses many of the same ideas as the proof of Theorem 17. However, there are a few extra cases that need to be considered. First, if f is a \u201clong\u201d conjunction, and r contains at least one literal that is the negation of a literal in f , then we show that adding another literal to r leads to a significant increase in performance. (If r is already of maximum size, then the performance is already good enough.) Second, we show that if f is \u201cshort\u201d and r contains at least one literal that is the negation of a literal in f , then there exists an r\u2032 \u2208 N \u2032(r) with significantly better performance. All other cases are identical to the monotone case."}, {"heading": "A Additional Proofs", "text": ""}, {"heading": "A.1 Accuracy of the Empirical Performance", "text": "In order to prove Lemma 5 and Theorem 8, it is necessary to examine how close the empirical performance of a representation r is to the representation\u2019s true performance. The following simple lemma shows that as long as the sample size s(n, 1/\u01eb) is sufficiently large, the empirical performance of each representation will be close to the true performance with high probability.\nLemma 19 Consider any r \u2208 R and f \u2208 C and fix any Z > 0 and \u03b4 > 0. Let N be an upper bound on the size of the neighborhood Neigh(r, 1/\u01eb). For each r\u2032 \u2208 Neigh(r, \u01eb), let v(r\u2032) be the empirical performance of r\u2032 with respect to f on a sample of size s \u2265 2 ln(2N/\u03b4)/Z2. With probability 1 \u2212 \u03b4, for all r\u2032 \u2208 Neigh(r, \u01eb), |v(r\u2032)\u2212 Perff (r\u2032,D)| \u2264 Z. Proof: Consider a particular r\u2032 \u2208 Neigh(r, \u01eb). By Hoeffding\u2019s inequality [13], for any Z, Pr (|v(r\u2032)\u2212 Perff (r\u2032,D)| \u2265 Z) \u2264 2 exp ( \u2212sZ2/2 ) . The right hand side of this inequality is upper bounded by \u03b4/N as long as s \u2265 2 ln(2N/\u03b4)/Z2, as we have assumed. The lemma then follows from a standard application of the union bound."}, {"heading": "A.2 Proof of Lemma 5", "text": "Suppose that Neigh is a strictly beneficial neighborhood function for C, D, and R with benefit polynomial b(n, 1/\u01eb). To prove the first half of the lemma, we will construct an algorithm for strictly monotonically evolving C over D. First, for any r \u2208 Rn and \u01eb > 0, we set the tolerance at t(r, \u01eb) = 1/(2b(n, 1/\u01eb)). We then set s(n, 1/\u01eb) = 128(b(n, 1/\u01eb))2 ln(2p(n, 1/\u01eb)/\u03b4) for a choice of \u03b4 that will be specified below. By Lemma 19, this guarantees that on a particular round i, with probability at least 1\u2212 \u03b4, for all r \u2208 Neigh(ri\u22121, \u01eb), |v(r) \u2212 Perff (r,D)| \u2264 1/(8b(n, 1/\u01eb)). For the remainder of this proof, we refer to this high probability event as the HPE.\nFor any fixed round i, consider first the case that Perff (ri\u22121,D) \u2265 1 \u2212 \u01eb/2. Since ri\u22121 \u2208 Neigh(ri\u22121, \u01eb), there is always at least one neutral mutation available and there could be a beneficial mutation, so ri will always be chosen from either Bene or Neut. Consider an arbitrary ri chosen from Bene \u222a Neut. If the HPE occurs, then\n(Perff (ri\u22121,D)\u2212 Perff (ri,D)) \u2264 ( v(ri\u22121) + 1\n8b(n, 1/\u01eb)\n) \u2212 ( v(ri)\u2212 1\n8b(n, 1/\u01eb)\n)\n= (v(ri\u22121)\u2212 v(ri)) + 1\n4b(n, 1/\u01eb)\n\u2264 t(r, \u01eb) + 1 4b(n, 1/\u01eb) = 3 4b(n, 1/\u01eb) \u2264 3\u01eb 8 .\nThe last inequality uses the fact that 1/b(n, 1/\u01eb) \u2264 \u01eb/2. This must be the case to guarantee that an improvement of 1/b(n, 1/\u01eb) is possible when the performance is arbitrarily close to (but still less than) 1 \u2212 \u01eb/2; otherwise, the definition of strictly beneficial neighborhood would not be satisfied. We then have\nPerff (ri,D) \u2265 1\u2212 \u01eb 2 \u2212 3\u01eb 8 > 1\u2212 \u01eb . (2)\nNow consider the case in which Perff (ri\u22121,D) < 1 \u2212 \u01eb/2. Since Neigh is a strictly beneficial neighborhood function, it must be the case that there exists a representation r \u2208 Neigh(ri\u22121, \u01eb) such that Perff (r,D) \u2265 Perff (ri\u22121,D) + 1/b(n, 1/\u01eb). Call this representation r\u2217. If the HPE occurs, then\nv(r\u2217)\u2212 v(ri\u22121) \u2265 ( Perff (r \u2217,D)\u2212 1\n8b(n, 1/\u01eb)\n) \u2212 ( Perff (ri\u22121,D) + 1\n8b(n, 1/\u01eb)\n)\n= (Perff (r \u2217,D)\u2212 Perff (ri\u22121,D))\u2212\n1 4b(n, 1/\u01eb) \u2265 3 4b(n, 1/\u01eb) > t(r, \u01eb) ,\nand so r\u2217 \u2208 Bene. Since the set Bene is non-empty, a representation in this set will be chosen for ri. Consider an arbitrary ri chosen from Bene. If the HPE occurs, then\n(Perff (ri,D)\u2212 Perff (ri\u22121,D)) \u2265 ( v(r) \u2212 1 8b(n, 1/\u01eb) ) \u2212 ( v(ri\u22121) + 1 8b(n, 1/\u01eb) )\n= (v(r) \u2212 v(ri\u22121))\u2212 1\n4b(n, 1/\u01eb)\n\u2265 t(r, \u01eb)\u2212 1 4b(n, 1/\u01eb) = 1 4b(n, 1/\u01eb) . (3)\nNow, let g(n, 1/\u01eb) = 8b(n, 1/\u01eb). Setting the parameter \u03b4 = \u01eb/g(n, 1/\u01eb) above and applying the union bound again, we have that with probability at least 1 \u2212 \u01eb, the HPE occurs at all round i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , g(n, 1/\u01eb)}. Suppose this is the case. From the argument leading up to Equation 3, we know that the performance of the current representation is monotonically increasing as long as the performance is less than 1 \u2212 \u01eb/2, and furthermore increases by at least 1/(4b(n, 1/\u01eb)) on each around. It remains to show that the algorithm evolves C, that is, that Perff (rg(n,1/\u01eb),D) \u2265 1\u2212 \u01eb.\nFrom the monotonic improvement when the performance is less than 1\u2212 \u01eb/2 and the argument leading up to Equation 2, it is clear that if the performance ever reaches 1 \u2212 \u01eb/2, it will not fall below 1 \u2212 \u01eb again before round g(n, 1/\u01eb). It is easy to see that the performance reaches 1 \u2212 \u01eb/2 at some point during these g(n, 1/\u01eb) rounds. In the worst case, the performance starts at \u22121. It is guaranteed to increase by at least 1/(4b(n, 1/\u01eb)) on each round. Thus it must reach 1 \u2212 \u01eb/2 in no more than g(n, 1/\u01eb) = 8b(n, 1/\u01eb) rounds.\nTo prove the second half of the lemma, note that the definition of strictly monotonic evolvability requires that for any initial representation r0 \u2208 R, with high probability either Perff (r0,D) \u2265 1\u2212\u01eb/2 or Perff (r1,D) \u2265 Perff (r0,D) + 1/m(n, 2/\u01eb). This implies that if Perff (r0,D) < 1 \u2212 \u01eb/2 there must exist an r1 \u2208 Neigh(r1, \u01eb) such that Perff (r1,D) \u2265 Perff (r0,D) + 1/m(n, 2/\u01eb)."}, {"heading": "A.3 Proof of Theorem 8", "text": "Suppose that Neigh is a strictly beneficial neighborhood function for C, D, and R with benefit polynomial b(n, 1/\u01eb). For any r \u2208 Rn and \u01eb > 0, we set the tolerance at t(r, \u01eb) = 1/(2b(n, 1/\u01eb)). We then set s(n, 1/\u01eb) = 128(b(n, 1/\u01eb))2 ln(2p(n, 1/\u01eb)/\u03b4) for a choice of \u03b4 that will be specified below. This guarantees that on a particular round i, with probability at least 1 \u2212 \u03b4, for all r \u2208 Neigh(ri\u22121, \u01eb), \u2223 \u2223v(r) \u2212 Perffi\u22121(r,D) \u2223\n\u2223 \u2264 1/(8b(n, 1/\u01eb)). (See Lemma 19 in Appendix A.1 for details.) For the remainder of this proof, we refer to this high probability event as the HPE.\nFix an i. Suppose \u2206 \u2264 1/(16b(n, 1/\u01eb)). If f1, f2, \u00b7 \u00b7 \u00b7 is a \u2206-drifting sequence, then for any r \u2208 R, \u2223\n\u2223Perffi\u22121(r,D)\u2212 Perffi(r,D) \u2223 \u2223 \u2264 Ex\u223cD [ |fi\u22121(x) \u2212 fi(x)| \u00b7 |r(x)| ] \u2264 2errD(fi\u22121, fi) \u2264 2\u2206 \u2264 1/(8b(n, 1/\u01eb)) . (4)\nConsider the case that Perffi\u22121(ri\u22121,D) \u2265 1\u2212 \u01eb/2. Since ri\u22121 \u2208 Neigh(ri\u22121, \u01eb), there is at least one neutral mutation available and there could be a beneficial mutation, so ri will be chosen from either Bene or Neut. Consider an arbitrary ri chosen from Bene\u222a Neut. If the HPE occurs, then\nPerffi\u22121(ri\u22121,D)\u2212 Perffi\u22121(ri,D) \u2264 ( v(ri\u22121) + 1\n8b(n, 1/\u01eb)\n) \u2212 ( v(ri)\u2212 1\n8b(n, 1/\u01eb)\n)\n\u2264 t(r, \u01eb) + 1 4b(n, 1/\u01eb) = 3 4b(n, 1/\u01eb) .\nThen from Equation 4 and the assumption that \u2206 \u2264 1/(16b(n, 1/\u01eb)),\nPerffi(ri,D) \u2265 Perffi\u22121 (ri,D)\u2212 1\n8b(n, 1/\u01eb)\n\u2265 ( Perffi\u22121(ri\u22121,D)\u2212 3\n4b(n, 1/\u01eb)\n)\n\u2212 1 8b(n, 1/\u01eb) \u2265 ( 1\u2212 \u01eb 2 ) \u2212 7\u01eb 16 > 1\u2212 \u01eb . (5)\nThe last line uses the fact that 1/b(n, 1/\u01eb) \u2264 \u01eb/2. This must be the case to guarantee that an improvement of 1/b(n, 1/\u01eb) is possible when the performance is arbitrarily close to (but still less than) 1\u2212 \u01eb/2; otherwise, the definition of strictly beneficial neighborhood would not be satisfied.\nNow consider the case in which Perffi\u22121(ri\u22121,D) < 1\u2212 \u01eb/2. Since Neigh is a strictly beneficial neighborhood function, it must be the case that there exists a representation r \u2208 Neigh(ri\u22121, \u01eb) such that Perffi\u22121(r,D) \u2265 Perffi\u22121(ri\u22121,D) + 1/b(n, 1/\u01eb). Call this r\u2217. If the HPE occurs, then\nv(r\u2217)\u2212 v(ri\u22121) \u2265 ( Perffi\u22121(r \u2217,D)\u2212 1\n8b(n, 1/\u01eb)\n) \u2212 ( Perffi\u22121(ri\u22121,D) + 1\n8b(n, 1/\u01eb)\n)\n= ( Perffi\u22121(r \u2217,D)\u2212 Perffi\u22121(ri\u22121,D) ) \u2212 1 4b(n, 1/\u01eb) \u2265 3 4b(n, 1/\u01eb) > t(r, \u01eb) ,\nand so r\u2217 \u2208 Bene. Since the set Bene is non-empty, a representation in this set will be chosen for ri. Consider an arbitrary ri chosen from Bene. If the HPE occurs, then\nPerffi\u22121(ri,D)\u2212 Perffi\u22121(ri\u22121,D) \u2265 (v(r) \u2212 1/(8b(n, 1/\u01eb)))\u2212 (v(ri\u22121) + 1/(8b(n, 1/\u01eb))) \u2265 t(r, \u01eb)\u2212 1/(4b(n, 1/\u01eb)) = 1/(4b(n, 1/\u01eb)) .\nThen from Equation 4,\nPerffi(r,D) \u2212 Perffi\u22121(ri\u22121,D) \u2265 Perffi\u22121(r,D)\u2212 Perffi\u22121(ri\u22121,D)\u2212 2\u2206 \u2265 1/(4b(n, 1/\u01eb))\u2212 1/(8b(n, 1/\u01eb)) = 1/(8b(n, 1/\u01eb)) . (6)\nNow, let g(n, 1/\u01eb) = 16b(n, 1/\u01eb) and consider any round \u2113 \u2265 g(n, 1/\u01eb). Setting the parameter \u03b4 = \u01eb/g(n, 1/\u01eb) above and applying the union bound again, we have that with probability at least 1\u2212 \u01eb, the HPE occurs at all rounds i \u2208 {\u2113\u2212 g(n, 1/\u01eb), \u00b7 \u00b7 \u00b7 , \u2113\u2212 1}. Suppose this is the case.\nFrom the argument leading up to Equation 6, we know that the performance of the current representation with respect to the current target is monotonically increasing as long as the performance is less than 1\u2212 \u01eb/2, and increases by at least 1/(8b(n, 1/\u01eb)) on each round. Combining this with the argument leading up to Equation 5, it is clear that if the performance ever reaches 1 \u2212 \u01eb/2 during this period of time, it will never again fall below 1\u2212 \u01eb before round \u2113. It remains to show that the performance reaches 1\u2212 \u01eb/2 at some point during these g(n, 1/\u01eb) rounds. This is also easy to see. In the worst case, the performance starts at \u22121. It is guaranteed to increase by at least 1/(8b(n, 1/\u01eb)) on each round, so it must reach 1\u2212 \u01eb/2 in no more than g(n, 1/\u01eb) = 16b(n, 1/\u01eb) rounds.\nThis shows that for any \u2113 \u2265 g(n, 1/\u01eb), with probability at least 1 \u2212 \u01eb, Perff\u2113(r\u2113,D) \u2265 1 \u2212 \u01eb and so C is evolvable with drifting targets."}, {"heading": "A.4 Proof of Lemma 10", "text": "Assume that i < j. The proof for the case in which i > j is nearly identical, and the result is trivial if i = j. For any \u03c4 and any function \u03c6 : X \u2192 [\u22121, 1],\n|E[\u03c6(x)fi(x)] \u2212 E[\u03c6(x)fj(x)]| = |E [\u03c6(x)(fi(x)\u2212 fj(x))]| = \u2223 \u2223 \u2223 \u2223\n\u2223\nE\n[\n\u03c6(x)\nj\u2212i \u2211\nk=1\n(fi+k\u22121(x)\u2212 fi+k(x)) ]\u2223 \u2223 \u2223 \u2223\n\u2223\n\u2264 E [ |\u03c6(x)| j\u2212i \u2211\nk=1\n|fi+k\u22121(x)\u2212 fi+k(x)| ]\n\u2264 j\u2212i \u2211\nk=1\nE [|fi+k\u22121(x)\u2212 fi+k(x)|] \u2264 (j \u2212 i)\u2206 \u2264 \u03c4\n2 ,\nwhere all expectations are taken with respect to x \u223c D. Therefore if Ex\u223cD[\u03c6(x)fj(x)] \u2265 \u03b8+ \u03c4 , then Ex\u223cD[\u03c6(x)fi(x)] \u2265 Ex\u223cD[\u03c6(x)fj(x)]\u2212 \u03c4\n2 \u2265 \u03b8 + \u03c4 \u2212 \u03c4 2 = \u03b8 + \u03c4 2 .\nSimilarly, if Ex\u223cD[\u03c6(x)fj(x)] \u2264 \u03b8 \u2212 \u03c4 , then Ex\u223cD[\u03c6(x)fi(x)] \u2264 Ex\u223cD[\u03c6(x)fj(x)] + \u03c4\n2 \u2264 \u03b8 \u2212 \u03c4 + \u03c4 2 = \u03b8 \u2212 \u03c4 2 ."}, {"heading": "A.5 Proof of Lemma 11", "text": "Under the assumption that the LPE does not occur at any time step, after q time steps if rq = r\u01eb[h, z], then |z| = q, since we add one bit at each step. Let ri = r\u01eb[h, zi]. We consider the possible mutations of ri. Observe that r\u01eb[h, z i0] is always neutral for all i. The cases we need to consider are (a) r\u01eb[h, z i1] is beneficial, and therefore chosen as the next representation implying that zi+1 = 1, (b) r\u01eb[h, z i1] is deleterious, and therefore r\u01eb[h, z i0] is chosen as the next representation, implying that zi+1 = 0, and (c) r\u01eb[h, z i1] is neutral, which implies that either r\u01eb[h, z i1] or r\u01eb[h, z i0] can be chosen as the next representation, implying that zi+1 = 0 or 1. Suppose we are in case (a), then we show that 1 is a valid answer to the query (\u03c6zi,\u01eb, \u03b8zi,\u01eb, \u03c4/2) with respect to fi. Consider,\nt\n(\nri, 1\n\u01eb\n)\n\u2264 v(r\u01eb[h, zi1])\u2212 v(ri) \u2264 Perffi(r\u01eb[h, zi1],D)\u2212 Perffi(ri,D) + 2\u03c4 \u2032 = \u01eb 2q E[\u03c6zi,\u01eb \u00b7 fi] + 2\u03c4 \u2032 .\nRe-arranging the terms, we get:\nE[\u03c6zi,\u01eb \u00b7 fi] \u2265 2q\n\u01eb\n(\nt\n(\nri, 1\n\u01eb\n) \u2212 2\u03c4 \u2032 ) \u2265 \u03b8zi,\u01eb \u2212 \u03c4\n2 .\nSimilarly one can show in case (b), that E[\u03c6zi,\u01eb \u00b7 f ] \u2264 \u03b8zi,\u01eb \u2212 \u03c4/2 and hence 0 is a valid answer to the query (\u03c6zi,\u01eb, \u03b8zi,\u01eb, \u03c4/2) and in case (c), that \u03b8zi,\u01eb \u2212 \u03c4/2 \u2264 E[\u03c6zi,\u01eb \u00b7 f ] \u2264 \u03b8zi,\u01eb + \u03c4/2, and hence both 0 and 1 are valid answers.\nBy Lemma 10, if zi+1 is a valid answer to the query (\u03c6zi,\u01eb, \u03b8zi,\u01eb, \u03c4/2), with respect to fi it is a valid answer to the (\u03c6zi,\u01eb, \u03b8zi,\u01eb, \u03c4) with respect to fq (since \u2206 \u2264 \u03c4/(2q)). Thus z is consistent with fq."}, {"heading": "A.6 Proof of Lemma 12", "text": "Let \u03c4 \u2032 = tu(n, 1/\u01eb)/8. Assuming that the LPE does not occur at any time step, wj+1 is always a neutral mutation for wj , and mutations of the form wj \u2192 wj will not occur. Also r\u01eb[hz,\u01eb, \u03c3] will always be chosen if it is a neutral mutation. Then in K \u2212 k rounds we will reach wK (if we had not already gone to r\u01eb[hz,\u01eb, \u03c3]) and hence on the next round we will move to r\u01eb[0, \u03c3]. This implies that the number of steps is at most K \u2212 k + 1.\nNow, suppose that if at some stage Perffi(ri,D) < Perffi(r\u01eb[hz,\u01eb, \u03c3],D). Then Perffi\u22121(r\u01eb[hz,\u01eb, \u03c3],D)\u2212 Perffi\u22121(ri\u22121,D)\n\u2265 Perffi(r\u01eb[hz,\u01eb, \u03c3],D) \u2212 Perffi\u22121(ri,D)\u2212 tu(n, 1\u01eb ) 2 \u2212\u2206 \u2265 \u2212 tu(n, 1 \u01eb ) 2 \u2212\u2206,\nand so r\u01eb[hz,\u01eb, \u03c3] is a neutral mutation for ri\u22121. By the assumption above, ri = r\u01eb[hz,\u01eb, \u03c3], proving the lemma."}, {"heading": "A.7 Proof Sketch for Theorem 14", "text": "We apply pieces of analysis of Theorem 13 here. We omit some details since the arguments are very similar; in fact, the argument that this algorithm is resistant to drift is nearly identical. To start, we let the representation class be R\u01eb which depends on \u01eb. Here, backsliding is not allowed since it degrades performance arbitrarily. We discuss how to encode all values of \u01eb in the same representation class in the Section A.7.1 below.\nWe will use the same construction as defined in Section 5.3, with only a small modification. For the representations in W\u01eb, say w0 = r\u01eb[h, z] with |z| = q, and in the neighborhood of wk we will also add r\u01eb[h, \u03c3] in addition to the existing r\u01eb[hz, \u03c3], r\u01eb[0, \u03c3] and wk+1. Thus, even if hz\u2217 has poor performance, we can ensure that the performance goes more than \u01eb lower than the starting state. Formally, \u2022 Neigh\u2032(wK , \u01eb) = {wK , r\u01eb[0, \u03c3]} and Neigh\u2032(wk, \u01eb) = {wk, wk+1, r\u01eb[hz, \u03c3], r\u01eb[h, \u03c3]} for k < K, \u2022 \u00b5\u2032(wK , wK) = \u03b7 and \u00b5\u2032(wK , r\u01eb[0, \u03c3]) = 1 \u2212 \u03b7, and \u00b5\u2032(wk, wk) = \u03b72, \u00b5\u2032(wk, wk+1) = \u00b5\u2032(wk, r\u01eb[hz, \u03c3] = (\u03b7 \u2212 \u03b72)/2, and \u00b5\u2032(wk, r\u01eb[h, \u03c3]) = 1\u2212 \u03b7 for k < K, and\n\u2022 t\u2032(wk, \u01eb) = tu(n, 1/\u01eb) for all k. Let \u03b7 = \u01eb/(4q + 2K), \u03c4 \u2032 = min{(\u01eb\u03c4)/(2q), tu(n, 1/\u01eb)/8}, and s = 1/(2(\u03c4 \u2032)2) log((6q + 3K)/\u01eb), as defined earlier in section 5.3. Let Neigh\u2032 = Neigh, \u00b5\u2032 = \u00b5 and t\u2032 = t for the representations in R\u0303\u01eb. We will show that E = (R\u01eb, Neigh\u2032, \u00b5\u2032, t\u2032, s) evolves C quasi-monotonically.\nThe intuition of the proof is as follows. Any two representations r\u01eb[h, z] and r\u01eb[h, z \u2032] are within performance \u01eb of each other (by definition). Using a similar argument as that for Lemma 12, one can show that while we start decreasing performance from w0 = r\u01eb[h, z], the performance never dips below the performance of r\u01eb[h, \u03c3] (and since this has the highest probability, this will be chosen whenever it is neutral). If r\u01eb[hz, \u03c3] is chosen earlier, it will be because its performance was higher than that of r\u01eb[h, \u03c3] and quasi-monotonicity is maintained. Just as in Theorem 13, one can show that in at most 2q +K + 1 steps, a representation with performance 1\u2212 \u01eb is reached.\nFor our setting of parameters, for g time steps LPE does not occur. And we will assume that this is the case. Recall that \u2206 = (\u01eb\u03c4)/(4q + 2K + 2).\nThere are two distinct types of starting representations: (i) r\u01eb[h, z] with |z| < q, or (ii) wk for some k where w0 = r\u01eb[h, z] with |z| = q. Suppose first that the starting representation is r\u01eb[h, z]. Since LPE events don\u2019t occur, we will reach r\u01eb[h, z\n\u2217] with |z\u2217| = q in q \u2212 |z| steps. Note that for all z\u2032 for any f , |Perff (r\u01eb[h, z\u2032],D) \u2212 Perff (r\u01eb[h, z],D)| \u2264 \u01eb. So during this phase quasi-monotonicity is maintained.\nConsider the case in which the starting representation is instead wk for some k, with w0 = r\u01eb[h, z \u2217] and |z\u2217| = q, or the case in which we reach such a representation wk after starting at r\u01eb[h, z]. The algorithm then transitions to either representation r\u01eb[h, \u03c3] or representation r\u01eb[hz\u2217 , \u03c3]. Furthermore, the transition to r\u01eb[hz\u2217 , \u03c3] happens only if Perff (hz\u2217 ,D) \u2265 Perff (h,D). This happens in at most K steps (using argument similar to that of Lemma 12) and during this time the performance never goes below that of r\u01eb[h, \u03c3], and so never goes more than \u01eb/2 below that of the starting representation.\nLet h\u2032 be either h or hz\u2217 depending on which was chosen as described in the above paragraph. Since \u2206 is low, the performance of h\u2032 will never go significantly below that of h (even if h\u2032 = hz\u2217) for the next g steps, hence it is sufficient to prove then that the performance will not drop significantly below that of r\u01eb[h\n\u2032, \u03c3]. From r\u01eb[h\u2032, \u03c3] in at most q steps we reach a representation r\u01eb[h\u2032, z\u2032] where z\u2032 is consistent with f . During this time the performance never goes more than \u01eb/2 below that of Perf(r\u01eb[h\n\u2032, \u03c3],D). From r\u01eb[h\u2032, z\u2032] we reach a representation with performance greater than 1 \u2212 \u01eb in one step, or r\u01eb[h \u2032, z\u2032] already has performance at least 1\u2212 \u01eb. Thus, the evolution is quasi-monotonic."}, {"heading": "A.7.1 Removing the Need to Know \u01eb", "text": "In Section A.7, we showed that any CSQ algorithm can be converted into an evolutionary algorithm that is drift-resistant and quasi-monotonic, provided we are allowed to fix \u01eb and encode its value in the representation. Here, we describe in some detail how a representation class that simultaneously encodes all values of \u01eb can be constructed. Note that the definition of evolvability allows the neighborhood to depend on \u01eb, but not the starting representation.\nWe assume that the parameter \u01eb provided to the algorithm is a power of 2. If this were not the case we could simply run the algorithm with \u01eb\u2032, setting \u01eb\u2032 = 2\u230alog \u01eb\u230b. The performance guarantees would only be better since \u01eb\u2032 \u2264 \u01eb. Furthermore, since \u01eb\u2032 \u2265 \u01eb/2, the running time would not be affected, except up to a constant factor. The representations will encode values of \u01eb ranging over the set S\u01eb = {1/2, 1/4, . . . , 2\u2212n}. It is not necessary to consider values of \u01eb smaller than this, since this would allow the algorithm to take time exponential in n, and hence an exhaustive search over all functions of polynomial-sized representations would be feasible in just one round of evolution. For the rest of this section, assume that \u01eb can only take values from this set.\nRecall the notation used in Section 5. In particular, A is a CSQ> algorithm that takes parameter \u01eb, makes q = q(n, 1/\u01eb) queries of tolerance \u03c4(n, 1/\u01eb), returns a hypothesis h with Perff (h,D) \u2265 1\u2212\u01eb. Similar to the definitions in 5.3, let \u03a6z,\u01eb = (1/q) \u2211q i=1 I(zj = 1)\u03c6zj\u22121,\u01eb(x).\nDefine a term as follows:\n\u2022 Every h \u2208 H is a term, and h is said to encode no \u01eb. \u2022 For any \u01eb1, let T1 be a term that either encodes no \u01eb or encodes only \u01eb\u2032 > \u01eb1. Then T = (1\u2212 \u01eb1/2)T1 + (\u01eb1/2)\u03a6z,\u01eb1 is a term if |z| \u2264 q(n, 1/\u01eb1). Furthermore, T is said to encode all of the values of \u01eb that T1 encodes plus \u01eb1.\nThus any term T may encode up to n values of \u01eb, and the values of \u01eb will increase as we get deeper in the term. This ensures that all terms have polynomial-sized (in n) representations, and the number of terms is finite.\nLet list(T ) denote the list of all \u01eb \u2208 S\u01eb that are encoded in T . Observe that the definition of term implies that the smallest \u01eb \u2208 list(T ) is encoded at the outermost level, and the values increase as we move to the interior. In particular if \u01eb1 is the smallest value in list(T ), then T = (1\u2212\u01eb1/2)T1+ (\u01eb1/2)\u03a6z,\u01eb1 for some z and T1 and list(T1) = list(T ) \\ {\u01eb1}. Denote by out(T ) the smallest value of \u01eb in list(T ) and let next(T ) be T1 such that T = (1\u2212 out(T )/2)T1 + (out(T )/2)\u03a6z,out(T ).\nWe consider all terms except those of the form h \u2208 H to be valid representations. The representation class will also contain more representations, that we shall define shortly.\nConsider the following three cases.\n(a) The evolutionary algorithm is in a state T , such that out(T ) = \u01eb, where \u01eb is the true parameter of the algorithm. Then (pretending as if T is in H) the results from Section 5 will apply directly. In particular, let T = r\u01eb[T1, z] = (1\u2212\u01eb/2)T1+(\u01eb/2)\u03a6z,\u01eb. Then Neigh(T, \u01eb) = {T, r\u01eb[T, z0], r\u01eb[T, z1]} if |z| \u2264 q(n, 1/\u01eb). When |z| = q(n, 1/\u01eb), we again define states similar to those in W\u01eb in 5.3, which allow the algorithm to gradually slide to move to r\u01eb[hz, \u03c3], r\u01eb[T, \u03c3] or r\u01eb[0, \u03c3] (but the performance will never go more than \u01eb lower than T with high probability).\n(b) The case when out(T ) \u2264 \u01eb. Let T0 = T , and define Ti = next(Ti\u22121) for all i, let k be the smallest such that out(Tk) \u2265 \u01eb. (It may happen that Tk = h for some h \u2208 H. Note that,\nT1 = (1\u2212\u01eb1/2) ( (1\u2212\u01eb2/2) ( \u00b7 \u00b7 \u00b7 (1\u2212 \u01ebk\u22121/2)Tk + (\u01ebk\u22121/2)\u03a6zk\u22121,\u01ebk\u22121 \u00b7 \u00b7 \u00b7 ) +(\u01eb2/2)\u03a6z2,\u01eb2 ) +(\u01eb1/2)\u03a6z1,\u01eb1 .\nThen since \u01eb1 < \u01eb2 < \u00b7 \u00b7 \u00b7 < \u01ebk\u22121 \u2264 \u01eb/2, and every \u01ebi is a power of 2, Perff (Tk,D) \u2265 Perf(T1,D)\u2212 2(\u01eb1 + \u00b7 \u00b7 \u00b7 \u01ebk\u22121) \u2265 4(\u01eb/2) .\nThen Perff (Tk,D) \u2265 Perff (T,D) \u2212 2\u01eb (because \u01ebk\u22121 \u2264 \u01eb/2 ). If out(Tk) = \u01eb, let rb = Tk. Otherwise, let rb = (1\u2212 \u01eb/2)Tk + (\u01eb/2)\u03a6\u03c3,\u01eb. Note that rb is always a valid term (by the above definition) and hence it is in the representation class. Also Perff (rb,D) \u2265 Perf(T,D)\u2212 3\u01eb.\n(c) The case, when out(T ) > \u01eb. Let rc = (1 \u2212 \u01eb/2)T + (\u01eb/2)\u03a6\u03c3,\u01eb. Again, rc is a valid term and hence in the representation class. Also in this case Perff (rc,D) \u2265 Perff (T,D)\u2212 \u01eb. In cases (b) and (c), if we can transition to the representations rb and rc respectively, we will\nhave reduced to case (a). However since the moves themselves may be deleterious, we need to add intermediate representations similar to those defined in W\u01eb in Section 5.3. In particular let w0 = T (where T may be that of case (b) or (c)). Define wk = (1 \u2212 k(tu(n, 1/\u01eb)/2))w0, where tu(n, 1/\u01eb) is the polynomial upper bound on the tolerances. Define Neigh(wk, \u01eb) = {wk, wk+1, rb}\n(or Neigh(wk, \u01eb) = {wk, wk+1, rc}). The idea is the same that the performance reduces gradually until the jump to rb (or rc) is no longer deleterious. During this time the performance never goes below that of rb (respectively rc) and hence quasi-monotonicity is maintained. (Although in some cases degradation may be 3\u01eb, we could just run with higher accuracy (say \u01eb/4) to begin with.)\nSo far we have ignored the drift. However, notice that the number of time steps to get to a representation which encodes the correct value of \u01eb is, with high probability, polynomial (in fact just 2/tu(n, 1/\u01eb)). Thus by making the drift small enough (though still an inverse polynomial), the function can be made to look essentially unchanging to the evolution algorithm."}, {"heading": "A.8 Proof of Theorem 15", "text": "We show that Neigh is a strictly beneficial neighborhood function for C, D, and R with b(n, 1/\u01eb) = \u03c03n/(2\u01eb). The theorem is then an immediate consequence of Theorem 8.\nThe analysis relies heavily on a couple of useful trigonometric facts. First, it is well known (see, for example, Dasgupta [6]) that under any spherically symmetric distribution D (for example, the uniform distribution over a sphere), errD(u,v) = arccos(u \u00b7 v)/\u03c0, where arccos(u \u00b7 v) is the angle between u and v. We will use this fact repeatedly. We also make use of the following inequalities from Dasgupta et al. [7]. For any \u03b8 \u2208 [0, \u03c0/2], 2\u03b8/\u03c0 \u2264 sin(\u03b8) \u2264 \u03b8, and 4\u03b82/\u03c02 \u2264 1\u2212 cos(\u03b8) \u2264 \u03b82/2.\nConsider an arbitrary r \u2208 Rn and f \u2208 Cn. To simplify presentation, assume that r1 = 1 and ri = 0 for i \u2208 {2, \u00b7 \u00b7 \u00b7 , n}. (Here and for the remainder of this proof, we use the notation ri and fi to denote the ith components of r and f , not the values of the representation and ideal function at round i as in previous sections.) This assumption is without loss of generality since we are considering only spherically symmetric distributions. Furthermore, assume that the axes are oriented such that for any r\u2032 \u2208 Neigh(r, \u01eb) (except for r itself), r\u20321 = cos(\u01eb/(\u03c0 \u221a n)), r\u2032i = \u00b1 sin(\u01eb/(\u03c0 \u221a n)) for some i \u2208 {2, \u00b7 \u00b7 \u00b7 , n}, and r\u2032j = 0 for all other j. This change in basis is also without loss of generality. Suppose that Perff (r,D) < 1\u2212 \u01eb/2 since otherwise there is nothing to prove. The condition that we need to prove can be stated as\nmax r\u2032\u2208Neigh(r,\u01eb)\nPerff (r \u2032,D) \u2265 Perff (r,D) +\n1\nb(n, 1/\u01eb) = Perff (r,D) +\n2\u01eb\n\u03c03n .\nUsing the facts that for any unit vectors u and v, Perfv(u,D) = 1\u2212 2err(u,v) and errD(u,v) = arccos(u \u00b7v)/\u03c0, this condition is equivalent to arccos(maxr\u2032\u2208Neigh(r,\u01eb) r\u2032 \u00b7 f) \u2264 arccos (r \u00b7 f)\u2212 \u01eb/(\u03c02n). By definition of the neighborhood function, there exists a r\u2032 \u2208 Neigh(r, \u01eb) such that\nr\u2032 \u00b7 f \u2265 f1 cos ( \u01eb\n\u03c0 \u221a n\n)\n+ max i\u2208{2,\u00b7\u00b7\u00b7 ,n}\n|fi| sin ( \u01eb\n\u03c0 \u221a n\n) \u2265 f1 cos ( \u01eb\n\u03c0 \u221a n\n)\n+\n\u221a\n1\u2212 f21 n sin ( \u01eb \u03c0 \u221a n ) .\nUsing the standard trigonometric equality that for any \u03b8 and \u03c6, arccos(\u03b8)\u2212 arccos(\u03c6) = arccos(\u03b8\u03c6+ \u221a\n(1\u2212 \u03b82)(1\u2212 \u03c62)), we have\narccos(r \u00b7 f)\u2212 \u01eb \u03c02n = arccos(f1)\u2212 arccos ( cos ( \u01eb \u03c02n ))\n= arccos\n(\nf1 cos ( \u01eb\n\u03c02n\n) + \u221a 1\u2212 f21 sin ( \u01eb\n\u03c02n\n)\n)\n.\nThen since arccos is decreasing in [0, \u03c0], to prove the result, it is sufficient to show that\narccos\n(\nf1 cos\n(\n\u01eb\n\u03c0 \u221a n\n)\n+\n\u221a\n1\u2212 f21 n sin ( \u01eb \u03c0 \u221a n )\n)\n\u2264 arccos ( f1 cos ( \u01eb\n\u03c02n\n) + \u221a 1\u2212 f21 sin ( \u01eb\n\u03c02n\n)\n)\nor taking the cosine of both sides and rearranging terms,\nf1\n(\ncos ( \u01eb\n\u03c02n\n) \u2212 cos ( \u01eb\n\u03c0 \u221a n\n))\n\u2264 \u221a 1\u2212 f21 ( 1\u221a n sin ( \u01eb \u03c0 \u221a n ) \u2212 sin ( \u01eb \u03c02n ) ) . (7)\nFirst consider the case in which f1 < 0. In this case, it is sufficient to show that the difference of cosines on the left hand side of the equation and the difference in sines on the right hand side are both positive. This can be verified easily using the inequalities for sines and cosines given above.\nFor the rest of this proof, assume that f1 > 0. Since we have assumed that Perff (r,D) < 1\u2212\u01eb/2, it follows that err(r, f) = arccos(f1)/\u03c0 > \u01eb/4, or equivalently, f1 < cos(\u01eb\u03c0/4). Then \u221a\n1\u2212 f21 > \u221a\n1\u2212 (cos(\u01eb\u03c0/4))2 = sin(\u01eb\u03c0/4), and for Equation 7 to hold, it is sufficient to show that\ncos ( \u01eb\u03c0\n4\n)\n(\ncos ( \u01eb\n\u03c02n\n) \u2212 cos ( \u01eb\n\u03c0 \u221a n\n))\n\u2264 sin ( \u01eb\u03c0\n4\n)\n(\n1\u221a n sin\n(\n\u01eb\n\u03c0 \u221a n\n)\n\u2212 sin ( \u01eb\n\u03c02n\n)\n)\n. (8)\nUsing the inequalities for sines and cosines given above, we have that\ncos ( \u01eb\u03c0\n4\n)\n(\ncos ( \u01eb\n\u03c02n\n) \u2212 cos ( \u01eb\n\u03c0 \u221a n\n)) \u2264 1 ( 1\u2212 cos ( \u01eb\n\u03c0 \u221a n\n))\n\u2264 \u01eb 2\n2\u03c02n ,\nand\nsin ( \u01eb\u03c0\n4\n)\n(\n1\u221a n sin\n(\n\u01eb\n\u03c0 \u221a n\n)\n\u2212 sin ( \u01eb\n\u03c02n\n)\n)\n\u2265 \u01eb 2\n(\n2\u01eb \u03c02n \u2212 \u01eb \u03c02n\n)\n= \u01eb2\n2\u03c02n .\nTherefore Equation 8 holds, Neigh is a strictly beneficial neighbor function, and C is evolvable with drifting targets."}, {"heading": "A.9 Proof of Theorem 16", "text": "We start by analyzing the simpler case in which D is known to be a spherical Gaussian distribution. In this case, a simpler neighborhood function can be used in which set of \u201cshift\u201d neighbors Nsl is greatly reduced. Below, we explain how to extend this analysis to the case in which D is an unknown product normal distributions over Rn and more complex neighborhood function defined in Section 6.2 is used.\nThroughout this proof, we use the notation ri and fi to denote the ith components of r and f respectively, and denote by ei the basis of R\nn. We define the simplified neighborhood function as Neigh\u2032(r, \u01eb) = Nfl \u222a N \u2032sl, where Nfl = {r \u2212 2riei | i = 1, . . . , d} is still the set of representations obtained by flipping the sign of one component of r, and N \u2032\nsl = {r\u2032i/\u2016r\u2032i\u20162 | r\u2032i = r\u00b1\u03b2ei, i = 1, . . . , d}\nis the set obtained by shifting each component a small amount and renormalizing, with \u03b2 satisfying \u01eb2/(6 \u221a n) \u2264 \u03b2 \u2264 \u01eb2/(3\u221an).\nWe first show that for any target function f , increasing Perff (r,D) is the equivalent to increasing f \u00b7 r. The following two lemmas establish this. These lemmas rely on the same trigonometric facts used in the proof of Theorem 15. In particular, under any spherically symmetric distribution D, errD(u,v) = arccos(u \u00b7 v)/\u03c0, where arccos(u \u00b7 v) is the angle between u and v, and for any \u03b8 \u2208 [0, \u03c0/2], 2\u03b8/\u03c0 \u2264 sin(\u03b8) \u2264 \u03b8, and 4\u03b82/\u03c02 \u2264 1\u2212 cos(\u03b8) \u2264 \u03b82/2.\nLemma 20 Let D be a spherical Gaussian distribution. For unit vectors v, f and \u03b1 \u2208 (0, 1), if f \u00b7 v \u2265 1\u2212 \u03b1, then Perf f (v,D) \u2265 1\u2212\u221a\u03b1\nProof: Since f and v are unit vectors and \u03b1 \u2208 (0, 1), i.e., f \u00b7 v > 0, we may write f \u00b7 v = cos(\u03b8) for \u03b8 \u2208 [0, \u03c0/2]. Thus we have that 1 \u2212 4\u03b82/\u03c02 \u2265 cos(\u03b8) \u2265 1 \u2212 \u03b1, and hence \u03b8/\u03c0 \u2264 \u221a\u03b1/2. But \u03b8/\u03c0 = err(f ,v) and Perff (v,D) = 1\u2212 2err(f ,v) \u2265 1\u2212 \u221a \u03b1.\nLemma 21 Let D be a spherical Gaussian distribution. For unit vectors u, v, f , if f \u00b7u\u2212f \u00b7v \u2265 \u03c9 > 0, then Perf\nf (u,D) \u2212 Perf f (v,D) \u2265 \u03c9/2.\nProof: Since u, v and f are unit vectors, we may assume that there exit angles \u03c6, \u03b8 \u2208 [0, \u03c0] such that f \u00b7 u = cos(\u03c6) and f \u00b7 v = cos(\u03b8), and that \u03c6 < \u03b8. Since the derivative of the cosine function is lower bounded by \u22121, cos(\u03c6) \u2212 cos(\u03b8) \u2264 \u03b8 \u2212 \u03c6. Finally observe that Perff (u,D) \u2212 Perff (v,D) = 2(err(f ,v)\u2212 err(f ,u)) = 2\u03c0 (\u03b8 \u2212 \u03c6) \u2265 \u03c9/2.\nThe next lemma shows that Neigh\u2032 is a strictly beneficial neighborhood function. More than the lemma statement itself, it is the analysis of this lemma that is important to us. Below we will show how to extend this analysis to the case in which D is a product normal distribution.\nLemma 22 Let C be the class of homogeneous linear separators, R be the class of homogeneous linear separators represented by unit length normal vectors, and D be a spherical Gaussian distribution. Define Neigh\u2032 as in the previous paragraph and let p be any polynomial such that p(n, 1/\u01eb) \u2265 3n. Then Neigh\u2032 is a strictly beneficial neighborhood function for C, D, and R, with b(n, 1/\u01eb) = 144n/\u01eb6.\nProof: Let \u03c1 = \u01eb3/(12 \u221a n) and \u03b7 = \u01eb2/(3 \u221a n). By assumption, \u03b2 then satisfies \u03b7/2 \u2264 \u03b2 \u2264 \u03b7.\nConsider an arbitrary r \u2208 Rn and f \u2208 Cn. If there exists r\u2032 \u2208 Nfl such that Perff (r\u2032,D) \u2212 Perff (r,D) \u2265 1/b(n, 1/\u01eb), then we are done, so assume that there is no element in Nfl with this property. In this case, we then claim that one of the following must hold for all i = 1, . . . , n: (i) ri and fi have the same sign, (ii) |ri| \u2264 \u03c1, or (iii) |fi| \u2264 \u03c1. If none of these properties hold, then f \u00b7 (r \u2212 2riei) \u2212 f \u00b7 r = \u22122rifi \u2265 2\u03c12 and by Lemma 21, the change in performance is at least \u03c12 = 1/b(n, 1/\u01eb).\nIn the rest of the analysis we assume that if no flip is a beneficial mutation (by at least 1/b(n, 1/\u01eb)), all ri and fi are in the interval [\u2212\u03c1, 1]. The reason this does not affect generality is this: Suppose one of them is smaller than \u2212\u03c1, we know that the other one then lies in the interval [\u22121, \u03c1]. We can now assume that the basis we were working with actually contained \u2212ei rather than ei. (This is useful for analysis, so that we can only consider mutations which increase the value of any component.) However, the neighborhood contains both mutations. Thus, we may assume that all fi and ri are in [\u2212\u03c1, 1], and hence f \u00b7 r \u2265 \u2212\u03c1(\u2016f\u20161 + \u2016r\u20161) \u2265 \u22122\u03c1 \u221a n.\nIn this situation if there is no i for which ri \u2264 fi\u2212 \u03b7, then f \u00b7 r \u2265 1\u2212 \u01eb2, and we are already close to optimal. (as shown below)\nf \u00b7 r = \u2211\ni\nfiri = \u2211\ni fi\u2208[\u2212\u03c1,0)\nfiri + \u2211\ni fi\u2208[0,1]\nfiri \u2265 \u2212\u03c1\u2016r\u20161 + \u2211\ni fi\u2208[0,1]\nfi(fi \u2212 \u03b7)\n\u2265 \u2211\ni fi\u2208[0,1]\nf2i \u2212 \u03c1\u2016r\u20161 \u2212 \u03b7\u2016f\u20161 \u2265 1\u2212 n\u03c12 \u2212 \u221a n\u03c1\u2212\u221an\u03b7\nSuppose there exists an i for which ri \u2264 fi\u2212 \u03b7, then fi \u2265 \u03b7\u2212\u03c1 \u2265 \u03b7/2 > 0. Let r\u2032 = r+\u03b2ei, with \u03b7/2 \u2264 \u03b2 \u2264 \u03b7. Then \u2016r\u2032\u20162 = \u221a 1 + 2\u03b2ri + \u03b22. From elementary algebra, we get the inequality that 1 + \u03b2ri \u2264 \u221a\n1 + 2\u03b2ri + \u03b22 \u2264 1 + \u03b2ri + \u03b22/2 (assuming \u03b2ri \u2208 (\u22121, 1), which is true). Then consider the following quantity of interest:\nf \u00b7 r \u2032 \u2016r\u2032\u20162 \u2212 f \u00b7 r = f \u00b7 r \u2032 \u2212 \u221a 1 + \u03b2ri + \u03b22(f \u00b7 r) \u2016r\u2032\u20162\nSince 1/2 \u2264 \u2016r\u2032\u20162 \u2264 2, if the quantity in the numerator is positive (as we will show) we have\n2\n( f \u00b7 r \u2032\n\u2016r\u2032\u20162 \u2212 f \u00b7 r\n)\n\u2265 f \u00b7 (r+ \u03b2ei)\u2212 \u221a 1 + 2\u03b2ri + \u03b22(f \u00b7 r)\n= \u03b2fi + f \u00b7 r ( 1\u2212 \u221a 1 + 2\u03b2ri + \u03b22 ) . (9)\nNotice that by our setting of parameters, ri \u2265 \u2212\u03c1 \u2265 \u2212\u03b2/2, thus the quantity under the square root sign is greater than 1. When f \u00b7 r < 0, the second term in the above expression is actually positive, and hence the total quantity is at least as much as the first term which is at least \u03b2\u03b7/2 = \u03b72/4 \u2265 2/b(n, 1/\u01eb). Thus we will consider the case when f \u00b7 r \u2265 0. In that case continuing from equation (9) and using the fact that \u221a 1 + 2\u03b2ri + \u03b22 \u2264 1 + \u03b2ri + \u03b22/2, we get\nf \u00b7 r \u2032\n\u2016r\u2032\u20162 \u2212 f \u00b7 r \u2265 1 2 ( \u03b2fi \u2212 (1\u2212 \u01eb2)(\u03b2ri + \u03b22/2) ) .\nSince ri + \u03b2/2 \u2264 ri + \u03b7 \u2264 fi, this is greater than \u03b2fi\u01eb2/2 \u2265 2/b(n, 1/\u01eb). Hence Neigh\u2032 is a strictly beneficial neighborhood function."}, {"heading": "Product Normal Distributions", "text": "We now describe how the analysis above can be adjusted to handle product normal distributions with polynomial variances. Recall that \u03c31, . . . , \u03c3n are the standard deviations of the distribution D in each of the n dimensions, and that 1 \u2265 \u03c3i \u2265 (1/n)k for all i for some constant k (which is known by the algorithm). Assume without loss of generality that 1 = \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3n \u2265 (1/n)k.\nDefine \u03c4(x1, . . . , xn) = (x1/\u03c31, \u00b7 \u00b7 \u00b7 , xn/\u03c3n) and \u03bb(x1, . . . , xn) = (\u03c31x1, \u00b7 \u00b7 \u00b7 , \u03c3nxn). Note that the transformations \u03c4 and \u03bb are inverses. Let N [0,\u03a3] denote the distribution with covariance matrix \u03a3 = diag(\u03c321 , . . . , \u03c3 2 n), and let N [0, I] denote the spherical normal distribution with variance 1. Note that if x is distributed according to N [0,\u03a3], \u03c4(x) is distributed according to N [0, I]. For any vector f and r, we have\nerrN [0,\u03a3](f , r) = Prx\u223cN [0,\u03a3][sign(\u03bb(f) \u00b7 \u03c4(x)) 6= sign(\u03bb(r) \u00b7 \u03c4(x)] = Prz\u223cN [0,I][sign(\u03bb(f) \u00b7 z) 6= sign(\u03bb(r) \u00b7 z)] = errN [0,I](\u03bb(f), \u03bb(r)) .\nWe assume that \u2016f\u20162 = 1 and that our representations consist of vectors also of unit norm. Then, for all r we have, (1/n)k \u2264 \u2016\u03bb(r)\u20162 \u2264 1. Observe that the \u201cflips\u201d are invariant with respect to \u03bb, i.e., \u03bb(r \u2212 2riei) = \u03bb(r) + 2\u03bb(r)iei. Because of this, we can consider the same set Nfl of flips as in the spherical distribution case.\nUnfortunately, it does not suffice to use the same set of \u201cshift\u201d mutationsN \u2032 sl . Let r be our current representation and let r\u2032 = r + \u03b3ei. Consider the two vectors \u03bb(r)/\u2016\u03bb(r)\u20162 and \u03bb(r\u2032)/\u2016\u03bb(r)\u20162, and consider their ith components, which are \u03c3iri/\u2016\u03bb(r)\u20162 and \u03c3i(ri + \u03b3)/\u2016\u03bb(r)\u20162 respectively. The difference between the two is \u03c3i\u03b3/\u2016\u03bb(r)\u20162. As in the proof of Lemma 22, let \u03b7 = \u01eb2/(3 \u221a n). If \u03b3 took a certain value such that \u03b7/2 \u2264 \u03c3i\u03b3/\u2016\u03bb(r)\u20162 \u2264 \u03b7, then by the same analysis in the proof of Lemma 22, this would be a beneficial mutation.\nSince we don\u2019t know the values of \u03c3i and \u2016\u03bb(r)\u20162, we use the following trick: Let\nNi =\n{ r\u00b1 ( j\u03b7\n4nk\n) ei | 1 \u2264 j \u2264 4nk } .\nNow consider the quantity\n\u03b3j = \u03c3i \u2016\u03bb(r)\u20162 \u03b7j 4nk .\nObserve that (1/n)k \u2264 \u03c3i/\u2016\u03bb(r)\u20162 \u2264 nk. Thus \u03b31 \u2264 \u03b7/4, \u03b34n2k \u2265 \u03b7, and finally \u03b3j \u2212 \u03b3j\u22121 \u2264 \u03b7/4; at least one j satisfies \u03b7/2 \u2264 \u03b3j \u2264 \u03b7. Let Nsl = {r\u2032/\u2016r\u2032\u20162|r\u2032 \u2208 Ni, i \u2208 {1, . . . , n}}. (This is the same set Nsl defined in Section 6.2, only in slightly different notation.) With Neigh(r, \u01eb) = Nfl \u222aNsl, Neigh is then a strictly beneficial neighborhood function with respect to any product normal distribution with variance lower bounded by (1/n)k as desired. The benefit polynomial remains the same as in Lemma 22, b(n, 1/\u01eb) = 144n/\u01eb6, though the neighborhood size is larger."}, {"heading": "A.10 Proof of Theorem 17", "text": "We show that Neigh is a strictly beneficial neighborhood function for C, D, and R with benefit polynomial b(n, 1/\u01eb) = 9/\u01eb2. The theorem is then an immediate consequence of Theorem 8.\nSince q = \u2308log2(3/\u01eb)\u2309, it follows that \u01eb/6 \u2264 2\u2212q \u2264 \u01eb/3. We make use of this repeatedly. Consider an arbitrary r \u2208 Rn and f \u2208 Cn. As in Diochnos and Tura\u0301n [8], we define m to be the number of \u201cmutual\u201d variables shared between f and r, u to be the number of \u201cundiscovered\u201d variables that appear in f but not r, and w to be the number of \u201cwrong\u201d variables that appear in r but not f . Thus |f | = m+ u and |r| = m+w. The functions r and f disagree if and only if all m mutual variables are true and either all u undiscovered variables are true while some wrong variable is false, or all w wrong variables are true while some undiscovered variable is false. Therefore if D is uniform, errD(f, r) = 2\u2212m (2\u2212u(1\u2212 2\u2212w) + 2\u2212w(1\u2212 2\u2212u)) = 2\u2212m\u2212u + 2\u2212m\u2212w \u2212 21\u2212m\u2212u\u2212w, so\nPerff (r,D) = 1\u2212 21\u2212m\u2212u \u2212 21\u2212m\u2212w + 22\u2212m\u2212u\u2212w = 1\u2212 21\u2212|f | \u2212 21\u2212|r| + 22\u2212m\u2212u\u2212w . (10) We start by considering the case in which the target is \u201clong\u201d, that is, |f | = m + u \u2265 q + 1. If |r| = m+w = q, then Perff (r,D) > 1\u221221\u2212|f |\u221221\u2212|r| \u2265 1\u22122\u2212q\u221221\u2212q = 1\u22123 \u00b72\u2212q \u2265 1\u2212 \u01eb, and the performance of r with respect to f is already good enough. This is because both f and r are almost always false under the uniform distribution. On the other hand, if |r| < q, then there must exist a neighbor r\u2032 in the set N+(r) such that the variable contained in r\u2032 but not r is an undiscovered variable of f . Then Perff (r\n\u2032,D)\u2212 Perff (r,D) = 2\u2212|r| > 2\u2212q = \u01eb/6. Now consider the case in which the target is \u201cshort\u201d, that is, f = m + u \u2264 q. Suppose that\nu = 0 (so the variables in f are a strict subset of the variables in r). If w = 0, then f and r must be identical, so assume w > 0. Then there must exist a neighbor r\u2032 in the set N\u2212 such that the variable contained in r but not r\u2032 is a wrong variable. From Equation 10, for this neighbor r\u2032, Perff (r\u2032,D)\u2212 Perff (r,D) = 21\u2212|r| \u2265 21\u2212q \u2265 \u01eb/3. On the other hand, suppose that u > 0. If |r| = m+w < q, then there must exist a neighbor r\u2032 in the set N+(r) such that the variable contained in r\u2032 but not r is an undiscovered variable of f . As above, Perff (r\n\u2032,D)\u2212 Perff (r,D) = 2\u2212|r| > 2\u2212q = \u01eb/6. Finally, if |r| = m+w = q, then there must exist a neighbor r\u2032 in the set N\u00b1 such that the variable contained in r but not r\u2032 is wrong and the variable contained in r\u2032 but not r is an undiscovered variable of f . In this case, from Equation 10, Perff (r\n\u2032,D)\u2212 Perff (r,D) = 22\u2212m\u2212u\u2212w \u2265 22\u22122q \u2265 \u01eb2/9. We have shown that whenever Perff (r,D) < 1 \u2212 \u01eb, there exists an r\u2032 \u2208 Neigh(r, \u01eb) such that\nPerff (r \u2032,D)\u2212 Perff (r,D) \u2265 \u01eb2/9, so Neigh is a strictly beneficial neighborhood function."}, {"heading": "A.11 Proof of Theorem 18", "text": "We show that Neigh is a strictly beneficial neighborhood function for C, D, and R with benefit polynomial b(n, 1/\u01eb) = 9/\u01eb2. The theorem is then an immediate consequence of Theorem 8.\nConsider an arbitrary r \u2208 Rn and f \u2208 Cn. As in the proof of Theorem 17, we start by considering the case in which the target is \u201clong\u201d, that is, |f | \u2265 q+ 1, where q = \u2308log2(3/\u01eb)\u2309. If |r| = q, then as before, Perff (r,D) > 1\u221221\u2212|f |\u221221\u2212|r| \u2265 1\u2212\u01eb, and the performance of r with respect to f is already good enough. If r does not contain a literal that is a negation of a literal in f , then Equation 10\nholds and just as in the proof of Theorem 17, there must exist a neighbor r\u2032 in the set N+(r) such that Perff (r\n\u2032,D) \u2212 Perff (r,D) = 2\u2212|r| > 2\u2212q = \u01eb/6. On the other hand, if r does contain at least one literal that is a negation of a literal in f , then f and r are never simultaneously true and so Perff (r,D) = 1\u22122(2\u2212|f |+2\u2212|r|) = 1\u221221\u2212|f |\u221221\u2212|r|. In this case, by a similar argument, a neighbor r\u2032 \u2208 N+ has performance 1\u2212 21\u2212|f | \u2212 2\u2212|r|, so Perff (r\u2032,D)\u2212 Perff (r,D) = 2\u2212|r| > 2\u2212q \u2265 \u01eb/6.\nNow consider the case in which f is \u201cshort\u201d. If r does not contain a literal that is a negation of a literal in f , then Equation 10 holds and the case-by-case analysis is identical to the analysis in the proof of Theorem 17. Suppose r contains at least one literal that is the negation of a literal in f . In this case, as above, Perff (r,D) = 1 \u2212 21\u2212|f | \u2212 21\u2212|r|. Let r\u2032 \u2208 N \u2032(r) be the conjunction obtained by starting with r and negating all literals in S. From Equation 10, we have that Perff (r\n\u2032,D) \u2265 1\u2212 21\u2212|f | \u2212 21\u2212|r| + 22\u2212|f |\u2212|r|, and so Perff (r\u2032,D)\u2212 Perff (r,D) \u2265 22\u2212|f |\u2212|r| \u2265 22\u22122q \u2265 \u01eb2/9. The lemma statement follows."}], "references": [], "referenceMentions": [], "year": 2010, "abstractText": "We consider the question of the stability of evolutionary algorithms to gradual changes,<lb>or drift, in the target concept. We define an algorithm to be resistant to drift if, for<lb>some inverse polynomial drift rate in the target function, it converges to accuracy 1 \u2212 \u01eb<lb>with polynomial resources, and then stays within that accuracy indefinitely, except with<lb>probability \u01eb at any one time. We show that every evolution algorithm, in the sense of<lb>Valiant [19], can be converted using the Correlational Query technique of Feldman [9], into<lb>such a drift resistant algorithm. For certain evolutionary algorithms, such as for Boolean<lb>conjunctions, we give bounds on the rates of drift that they can resist. We develop some<lb>new evolution algorithms that are resistant to significant drift. In particular, we give an<lb>algorithm for evolving linear separators over the spherically symmetric distribution that is<lb>resistant to a drift rate of O(\u01eb/n), and another algorithm over the more general product<lb>normal distributions that resists a smaller drift rate. The above translation result can be also interpreted as one on the robustness of the notion of<lb>evolvability itself under changes of definition. As a second result in that direction we show<lb>that every evolution algorithm can be converted to a quasi-monotonic one that can evolve<lb>from any starting point without the performance ever dipping significantly below that of<lb>the starting point. This permits the somewhat unnatural feature of arbitrary performance<lb>degradations to be removed from several known robustness translations.", "creator": "LaTeX with hyperref package"}}}