{"id": "1006.0475", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2010", "title": "Prediction with Advice of Unknown Number of Experts", "abstract": "In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales of all the experts, and this is a useful application in a wide range of situations.\n\n\nThis is also the case where we need to add:\nA strong-valued model of predicting an actual outcome is one which requires an initial assumption: a prediction with this positive model is the probability that it will occur in the next 10 years or so (or in the following 20 years or so) and has a positive model (which does not contain any actual predictions). This is an application of this kind to predictions for all the experts on the probability of a future future. In particular, this is the probability that it will occur in the next 10 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so in the following 20 years or so", "histories": [["v1", "Wed, 2 Jun 2010 19:41:27 GMT  (81kb,S)", "http://arxiv.org/abs/1006.0475v1", "22 pages; draft version"]], "COMMENTS": "22 pages; draft version", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexey chernov", "vladimir vovk"], "accepted": false, "id": "1006.0475"}, "pdf": {"name": "1006.0475.pdf", "metadata": {"source": "CRF", "title": "Prediction with Advice of Unknown Number of Experts", "authors": ["Alexey Chernov"], "emails": ["chernov@cs.rhul.ac.uk", "vovk@cs.rhul.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 6.\n04 75\nv1 [\ncs .L\nG ]\n2 J\nIn the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales."}, {"heading": "1 Introduction", "text": "We consider the problem of prediction with expert advice (PEA) and its variant, decision-theoretic online learning (DTOL). In the PEA framework (see [3] for details, references and historical notes), at each step Learner gets decisions (also called predictions) of several Experts and must make his own decision. Then the environment generates an outcome and a (real-valued) loss is calculated for each decision as a known function of decision and outcome. The difference between cumulative losses of Learner and one of Experts is the regret to this Expert. Learner aims at minimizing his regret to Experts, for any sequence of Expert decisions and outcomes.\nIn DTOL, introduced in [8], Learner\u2019s decision is a probability distribution on a finite set of actions. Then each action incurs a loss (the vector of the losses can be regarded as the outcome), and Learner suffers the loss equal to the expected loss over all actions (according to the probabilities from his decision). The regret is the difference between the cumulative losses of Learner and one of the actions. One can interpret each action as a rigid Expert that always suggests this action. A precise connection between the DTOL and PEA frameworks will be described in Section 2.\nUsually Learner is required to have small regret to all Experts. In other words, a strategy for Learner must have a guaranteed upper bound on Learner\u2019s regret to the best Expert (one with the minimal loss). In this paper we deal with another kind of bound, recently introduced in [4]. It captures the following\nintuition. Generally speaking, the more Experts (or actions, in the DTOL terminology) Learner must take into account, the worse his performance will be. However, assume that each Expert has several different names, so Learner is given a lot of identical advice. It seems natural that the loss of Learner is big if there is a real controversy between Experts (or a real difference between actions), and small if most of the Experts agree with each other. So a competent regret bound should depend on the real number of Experts instead of the nominal one. Another example: assume that all the actions are different, but many of them are good \u2014 there are many ways to achieve some goal. Then Learner has less space to make a mistake and to select a bad action. Again it seems that a competent regret bound should depend on the fraction of the good actions rather than the nominal number of actions.\nIf the effective number of actions (Experts) is significantly less than the nominal one, one can loosely say that the number of actions is unknown in this setting. The following regret bound obtained in [4] for their NormalHedge algorithm holds for this case:\nLT \u2264 L\u01ebT +O ( \u221a T ln 1\n\u01eb + ln2 N\n)\n, (1)\nwhere N is the nominal number of actions, LT is the cumulative loss of Learner after step T and L\u01ebT is the value such that at least \u01eb-fraction of actions have smaller or equal cumulative loss after step T (or L\u01ebT can be interpreted as the loss of \u01ebN -th best action). It is important that the bound holds uniformly for all \u01eb and T and the algorithm does not need to know them in advance. The number 1\u01eb plays the role of the effective number of actions. The bound shows, in a sense, that the NormalHedge algorithm can work even if the number of actions is not known.\nOur main result (Theorem 9) is the following bound for a new algorithm:\nLT \u2264 L\u01ebT + 2 \u221a T ln 1\n\u01eb + 7\n\u221a T .\nThis bound is also uniform in T and \u01eb. In contrast to (1), our bound does not depend on the nominal number of actions, whereas (1) contains a term O(ln2 N). So it is the first (as far as we know) bound strictly in terms of the effective number of actions. Our bound has a simpler structure, but it is generally incomparable to the (precise) bound for Normal Hedge from [4] (see Subsection 4.2 for discussion of different known bounds). Also our bound can be easily adapted to internal regret (see [12] for definition). We describe the application to internal regret in Subsection 4.3.\nOur bound is obtained with the help of the defensive forecasting method (DF). The DF is based on bounding the growth of some supermartingale (a kind of potential function). In [5], the DF was used to obtain bounds of the form LT \u2264 cLnT + a, where c and a are some constants. For our form of bounds, we need a new variation of the DF and a new sort of supermartingales. So we introduce the notion of multivalued supermartingale and prove a boundedness result for them (Lemmas 2 and 3). (This result is of certain independent interest: for example, it helps to get rid of additional Assumption 3 in Theorem 3 in [5].)\nThe paper is organized as follows. In Section 2 we describe the setup of prediction with expert advice and of decision-theoretic framework online learn-\ning, and define the \u01eb-quantile regret. In Section 3 we describe the Defensive Forecasting Algorithm, define multivalued supermartingales and discuss their properties, and introduce supermartingales of a specific form that are based on Hoeffding inequality. In Subsection 4.1 we prove two loss bounds on the \u01ebquantile regret, and in Subsection 4.2 we compare them with the bound for the NormalHedge algorithm and with other known bounds. In Subsection 4.3 we show how these bounds can be transformed into bounds on the internal regret. In the last subsection we describe a toy example of an algorithm that guarantees bounds for two very different loss functions simultaneously."}, {"heading": "2 Notation and Setup", "text": "Vectors with coordinates p1, . . . , pN are denoted by an arrow over the letter: ~p = (p1, . . . , pN ). For any natural N , by \u2206N we denote the standard simplex in R N : \u2206N = {~p \u2208 [0, 1]N | \u2211N n=1 pn = 1}. By ~p \u00b7 ~q we denote the scalar product: ~p \u00b7 ~q = \u2211Nn=1 pnqn.\nProtocol 1 Decision-theoretic framework for learning\nL0 := 0. Ln0 := 0, n = 1, . . . , N . for t = 1, 2, . . . do Learner announces ~\u03b3t \u2208 \u2206N . Reality announces ~\u03c9t \u2208 [0, 1]N . Lt := Lt\u22121 + ~\u03b3t \u00b7 ~\u03c9t. Lnt := L n t\u22121 + \u03c9t,n, n = 1, . . . , N . end for\nThe decision-theoretic framework for online learning (DTOL) was introduced in [8]. DTOL protocol is given as Protocol 1. The Learner has N available actions, and at each step t he must assign probability weights \u03b3t,1, . . . , \u03b3t,N to these actions. Then each action suffers a loss \u03c9t,n, and Learner\u2019s loss is the expected loss over all actions according to the weights he assigned. Learner\u2019s goal is to keep small his regret Rnt = Lt \u2212 Lnt to any action n, independent of the losses.\nProtocol 2 Prediction with Expert Advice\nL0 := 0. Ln0 := 0, n = 1, . . . , N . for t = 1, 2, . . . do Expert n announces \u03b3nt \u2208 \u0393, n = 1, . . . , N . Learner announces \u03b3t \u2208 \u0393. Reality announces \u03c9t \u2208 \u2126. Lt := Lt\u22121 + \u03bb(\u03b3t, \u03c9t). Lnt := L n t\u22121 + \u03bb(\u03b3 n t , \u03c9t), n = 1, . . . , N . end for\nDTOL can be regarded as a special case of prediction with expert advice (PEA), as explained below. The PEA protocol is given as Protocol 2. The\ngame is specified by the set of outcomes \u2126, the set of decisions \u0393 and the loss function \u03bb : \u0393 \u00d7 \u2126 \u2192 R. The game is played repeatedly by Learner having access to decisions made by a pool of Experts. At each step, Learner is given N Experts\u2019 decisions and is required to come out with his own decision. The loss \u03bb(\u03b3, \u03c9) measures the discrepancy between the decision \u03b3 and the outcome \u03c9. Lt is Learner\u2019s cumulative loss over the first t steps, and Lnt is the n-th Expert\u2019s cumulative loss over the first t steps. The goal of Learner is the same: to keep small his regret Rnt = Lt \u2212Lnt to any Expert n, independent of Experts\u2019 moves and the outcomes.\nAs defined in [4] (for DTOL), the regret to the top \u01eb-quantile (at step T ) is the value R\u01ebT such that there are at least \u01ebN actions (the fraction at least \u01eb of all Experts) with RnT \u2265 R\u01ebT . Or, equivalently, R\u01ebT = LT \u2212 L\u01ebT where L\u01ebT is a value such that at least \u01ebN actions (the fraction at least \u01eb of all Experts) has the loss LnT less than L \u01eb T .\nA uniform bound on R\u01ebT (in other words, a bound on Learner\u2019s loss LT in terms of L\u01ebT ) that holds for all \u01eb is more general than the standard best Expert bounds. The latter can be obtained as a special case for \u01eb = 1/N . For this reason, it is natural to call the value 1/\u01eb the effective number of actions: a bound on R\u01ebT can be considered as the best Expert bound in an imaginary game against 1/\u01eb Experts.\nLet us say what games (\u2126,\u0393,\u039b) we consider in this paper. For any game (\u2126,\u0393, \u03bb), we call \u039b = {g \u2208 R\u2126 | \u2203\u03b3 \u2208 \u0393 \u2200\u03c9 \u2208 \u2126 g(\u03c9) = \u03bb(\u03b3, \u03c9)} the prediction set. The prediction set captures most of the information about the game. The prediction set is assumed to be non-empty. In this paper, we consider bounded convex compact games only. This means that we assume that the set \u039b is bounded and compact, and the superprediction set \u039b + [0,\u221e]\u2126 is convex, that is, for any g1, . . . , gK \u2208 \u039b and for any p1, . . . , pK \u2208 [0, 1]K , \u2211K k=1 pk = 1, there exists g \u2208 \u039b such that g(\u03c9) \u2264 \u2211Kk=1 pkgk(\u03c9) for all \u03c9 \u2208 \u2126. For such games, we assume without loss of generality that \u039b \u2286 [0, 1]\u2126 (we always can scale the loss function).\nFor DTOL as a special case of PEA, the outcome space is \u2126 = [0, 1]N , the decision space is \u0393 = \u2206N , and the loss function is \u03bb(~\u03b3, ~\u03c9) = ~\u03b3 \u00b7 ~\u03c9. Experts play fixed strategies always choosing ~\u03b3nt such that \u03b3 n t,n = 1 and \u03b3 n t,k = 0 for k 6= n (see e. g. [13, Example 7] for more details about this game). In an important sense the general PEA protocol for the bounded convex games is equivalent to DTOL. Obviously, if some upper bound on regret is achievable in any PEA game then it is achievable in the special case of the DTOL game. To see how to transfer an upper bound from DTOL to a PEA game, let us interpret the decisions \u03b3nt of Experts and the outcome \u03c9t in the PEA game as the outcome ~\u03c9\u2032t in DTOL: \u03c9 \u2032 t,n = \u03bb(\u03b3 n t , \u03c9t). If Learner\u2019s decision \u03b3t satisfies \u03bb(\u03b3t, \u03c9t) \u2264 \u2211N n=1 \u03b3 \u2032 t,n\u03bb(\u03b3 n t , \u03c9t), where ~\u03b3 \u2032 t is Learner\u2019s decision in DTOL, then the regret (at step t) in the PEA game will be not greater than the regret in DTOL. It remains to note that, since the game is convex, for any ~\u03b3\u2032t there exists \u03b3t such that \u03bb(\u03b3t, \u03c9) \u2264 \u2211N n=1 \u03b3 \u2032 t,n\u03bb(\u03b3 n t , \u03c9), for any \u03c9 \u2208 \u2126.\nHowever, the equivalence between DTOL and PEA is limited. In particular, we can obtain PEA bounds that hold for specific loss functions or classes of loss functions (such as mixable loss functions [13]), and these bounds may be much stronger than the general bounds induced by DTOL.\nIn this paper, we consider PEA and DTOL in parallel for another reason.\nIt is sometimes useful to consider a more general variant of Protocol 2 where the number of Experts is infinite (and maybe uncountably infinite): then PEA can be applied to large families of functions as Experts. With the help of our method, we can cope either with DTOL, where the number of actions is finite, or with PEA when \u2126 is finite and the number of Experts is arbitrary. So we cannot infer a bound for infinitely many Experts from a DTOL result, but we can obtain a PEA result directly. In the sequel, we will write about N experts, but always allow N to be infinite in the PEA case.\nMost of the presentation below is in the terms of PEA but applicable to DTOL as well. We normally hide the difference between PEA and DTOL behind the common notation (DTOL is considered as the game described above). When the difference is important, we give two parallel fragments of a statement or proof."}, {"heading": "3 Defensive Forecasting and Supermartingales", "text": "This section contains the technical results we need to construct our prediction algorithm. They are used in the proofs but not in the theorem statements and discussions in the next section."}, {"heading": "3.1 Defensive Forecasting", "text": "The general structure of the Defensive Forecasting Algorithm (DFA) is quite simple. At step t, we define a function ft : \u0393\u00d7\u2126 \u2192 R (with special properties \u2014 see below) and look for \u03b3 \u2208 \u0393 such that\n\u2200\u03c9 \u2208 \u2126 ft(\u03b3, \u03c9) \u2264 ft\u22121(\u03b3t\u22121, \u03c9t\u22121) , (2)\nwhere ft\u22121 is the function defined at the previous step, \u03b3t\u22121 is Learner\u2019s decision at the previous step, and \u03c9t\u22121 is the outcome at the previous step. Then \u03b3 with this property is announced as the next decision of Learner \u03b3t.\nThe choice of ft may depend on all the previous decisions, outcomes, and on this step Experts\u2019 decisions (for PEA), so ft = F({\u03b3n1 }Nn=1, \u03b31, \u03c91, . . . , {\u03b3nt }Nn=1). Having specified F , we call this strategy of Learner an application of the DFA to F .\nThe algorithm guarantees that the values of ft do not increase, in particular, after each step the value ft(\u03b3t, \u03c9t) is not greater than some initial value f0. We will choose F so that the inequality F({\u03b3n1 }Nn=1, \u03b31, \u03c91, . . . , {\u03b3nt }Nn=1)(\u03b3t, \u03c9t) \u2264 f0 implies a loss bound we need.\nAlso we need to guarantee that the algorithm always can find \u03b3 satisfying (2). To this end we will choose F so that the sequence ft will be a (multivalued) supermartingale as defined in the next subsection."}, {"heading": "3.2 Multivalued Supermartingales", "text": "Let \u2126 be a compact metric space. Any finite set \u2126 is considered as a metric space with the discrete metric. Let P(\u2126) be the space of all measures on \u2126 supplied with the weak topology.\nFor any measurable function g \u2208 R\u2126 and any \u03c0 \u2208 P(\u2126), denote\nE\u03c0g =\n\u222b\n\u2126\ng(\u03c9)\u03c0(d\u03c9) .\nFor finite \u2126, this definition reduces to the scalar product:\nE\u03c0g = \u2211\n\u03c9\u2208\u2126 g(\u03c9)\u03c0(\u03c9) .\nLet S be an operator that to any sequence e1, \u03c01, \u03c91, . . . , eT\u22121, \u03c0T\u22121, \u03c9T\u22121, eT , where \u03c9t \u2208 \u2126, \u03c0t \u2208 P(\u2126), t = 1, . . . , T \u2212 1, and et, t = 1, . . . , T are some arbitrary values, assigns a function ST : P(\u2126) \u2192 R\u2126. To simplify notation, we will hide the dependence of ST on all the long argument sequence in the index T . We call S a (game-theoretic) supermartingale if for any sequence of arguments, for any \u03c0 \u2208 P(\u2126), for gT\u22121 = ST\u22121(\u03c0T\u22121) and for g = ST (\u03c0) it holds\nE\u03c0g \u2264 gT\u22121(\u03c9T\u22121) . (3)\nThis definition of supermartingale is equivalent to the one given in [5]. We say that supermartingale S is forecast-continuous if every ST is a continuous function.\nThe main property of forecast-continuous supermartingales that makes them useful in our context is given by Lemma 1. Originally, a variant of the lemma was obtained by Leonid Levin in 1976. The proof is based on fixed-point considerations, see [10, Theorem 16.1] or [6, Lemma 8] for details.\nLemma 1. Let \u2126 be a compact metric space. Let a function q : P(\u2126)\u00d7\u2126 \u2192 R be continuous as function from P(\u2126) to R\u2126. If for all \u03c0 \u2208 P(\u2126) it holds that\nE\u03c0q(\u03c0, \u00b7) \u2264 C ,\nwhere C \u2208 R is some constant, then\n\u2203\u03c0 \u2208 P(\u2126)\u2200\u03c9 \u2208 \u2126 q(\u03c0, \u03c9) \u2264 C .\nThe lemma guarantees that for any forecast-continuous supermartingale S we can always choose gt \u2208 St such that gt(\u03c9) \u2264 gt\u22121(\u03c9t\u22121) for all \u03c9. This is exactly the kind of condition we need for the DFA.\nUnfortunately, for the loss bounds we want to obtain, we did not find a suitable forecast-continuous supermartingale. So we define a more general notion of multivalued supermartingale, and prove an appropriate variant of Levin\u2019s lemma.\nTo get the definition of a multivalued supermartingale, we make just three changes in the definition of supermartingale above: operator S depends additionally on gt \u2208 St(\u03c0t); ST is function from P(\u2126) to non-empty subsets of R\n\u2126; the condition (3) holds for any g \u2208 ST (\u03c0). Namely, let S be an operator that to any sequence e1, \u03c01, g1, \u03c91, . . . , eT\u22121, \u03c0T\u22121, gT\u22121, \u03c9T\u22121, eT , where \u03c9t \u2208 \u2126, \u03c0t \u2208 P(\u2126), gt \u2208 R\u2126, t = 1, . . . , T \u2212 1, and et, t = 1, . . . , T are some arbitrary values, assigns a function ST : P(\u2126) \u2192 2R \u2126\nsuch that ST (\u03c0) is a nonempty subset of R\u2126 for all \u03c0 \u2208 P(\u2126). S is called a multivalued supermartingale\nif for any sequence of arguments where gt \u2208 St(\u03c0t), for any \u03c0 \u2208 P(\u2126), ST (\u03c0) 6= \u2205 and for all g \u2208 ST (\u03c0) it holds\nE\u03c0g \u2264 gT\u22121(\u03c9T\u22121) . (4)\nA multivalued supermartingale is called forecast-continuous if for every ST , the set {(\u03c0, g) | \u03c0 \u2208 P(\u2126), g \u2208 ST (\u03c0)} is closed and additionally for every \u03c0 \u2208 P(\u2126) the set ST (\u03c0)+[0,\u221e]\u2126 = {g \u2208 R\u2126 | \u2203g\u2032 \u2208 ST (\u03c0)\u2200\u03c9 \u2208 \u2126g\u2032(\u03c9) \u2264 g(\u03c9)} is convex.\nNote that if S is a forecast-continuous multivalued supermartingale and St(\u03c0) always consists of exactly one element, S is (equivalent to) a forecastcontinuous supermartingale in the former sense: closedness of the graph of ST means that ST (\u03c0) is a continuous function of \u03c0 and the convexity requirement becomes trivial."}, {"heading": "3.3 Levin\u2019s Lemma for Multivalued Supermartingales", "text": "Here we prove two version of Levin\u2019s lemma suitable for multivalued supermartingales. The first variant (it is simpler) will be used for PEA with finite outcome set \u2126. The second variant will be used for DTOL.\nLemma 2. Let \u2126 be a finite set. Let X be a compact subset of R\u2126. Let q \u2286 P(\u2126) \u00d7 X be a relation. Denote q(\u03c0) = {g | (\u03c0, g) \u2208 q} and ran q = \u222a\u03c0\u2208P(\u2126)q(\u03c0) \u2286 X. Suppose that q is closed, for every \u03c0 \u2208 P(\u2126) the set q(\u03c0) is non-empty and the set q(\u03c0) + [0,\u221e]\u2126 is convex. If for some real constant C it holds that for every \u03c0 \u2208 P(\u2126)\n\u2200g \u2208 q(\u03c0) E\u03c0g \u2264 C ,\nthen there exists g \u2208 ran q such that\n\u2200\u03c9 \u2208 \u2126 g(\u03c9) \u2264 C .\nWe derive the lemma from Lemma 1 similarly to the derivation of Kakutani\u2019s fixed point theorem for multi-valued mappings (see, e. g. [1, Theorem 11.9]) from Brouwer\u2019s fixed point theorem. Unfortunately, we did not find a way just to refer to Kakutani\u2019s theorem and have to repeat the whole construction with appropriate changes.\nProof. Note first that P(\u2126) is compact for finite \u2126, hence q is compact as a closed subset of a compact set. Let Mq = maxg\u2208ran q,\u03c9\u2208\u2126 |g(\u03c9)|.\nFor every natural m > 0, let us take any (1/m)-net {\u03c0mk } on P(\u2126) such that for every \u03c0 \u2208 P(\u2126) there is at least one net element \u03c0mk at the distance less than 1/m from \u03c0 and for every \u03c0 \u2208 P(\u2126) there are at most 4|\u2126|2 elements of the net at the distances less than 1/m from \u03c0. (One can use here any reasonable distance on P(\u2126), for example, the maximum absolute value of the coordinates of the difference.) For every \u03c0mk in the net, fix any g m k \u2208 q(\u03c0mk ) (recall that q(\u03c0mk ) is non-empty). Now let us define a function qm : P(\u2126)\u00d7\u2126 \u2192 R as a linear interpolation of the points (\u03c0mk , g m k ). Namely, let {umk } be a partition of unity of P(\u2126) subordinate to U1/m(\u03c0 m k ), the (1/m)-neighborhoods of \u03c0 m k (that is, u m k (\u03c0) are non-negative,\numk (\u03c0) = 0 if the distance between \u03c0 and \u03c0 m k is 1/m or more, and the sum over k of all umk (\u03c0) is 1 at any \u03c0). Let q m(\u03c0, \u03c9) = \u2211 k u m k (\u03c0)g m k (\u03c9).\nThe function qm is forecast-continuous. Let us find an upper bound on E\u03c0q m(\u03c0, \u00b7):\nE\u03c0q m(\u03c0, \u00b7) =\n\u2211\nk\numk (\u03c0)E\u03c0g m k\n= \u2211\nk\numk (\u03c0)E\u03c0mk g m k +\n\u2211\nk\numk (\u03c0) \u2211 \u03c9\u2208\u2126 (\u03c0(\u03c9)\u2212 \u03c0mk (\u03c9))gmk (\u03c9) \u2264 C +Mq|\u2126|/m\n(the bound on the first term holds since gmk \u2208 q(\u03c0mk ) and hence E\u03c0mk gmk \u2264 C). By Lemma 1 we can find a point \u03c0m \u2208 P(\u2126) such that\n\u2200\u03c9 \u2208 \u2126 qm(\u03c0m, \u03c9) \u2264 C +Mq|\u2126|/m .\nRecalling that qm(\u03c0m, \u03c9) = \u2211 k u m k (\u03c0 m)gmk (\u03c9) and that there are at most 4|\u2126|2 non-zero values among umk (\u03c0\nm), we get the following statement. There exist some \u03b1mk \u2265 0, k = 1, . . . , 4|\u2126|2, \u2211 k \u03b1 m k = 1, and some g m k \u2208 q(\u03c0mk ) with \u03c0mk at the distance at most 1/m from \u03c0m such that\n\u2200\u03c9 \u2208 \u2126 4|\u2126|2 \u2211\nk=1\n\u03b1mk g m k (\u03c9) \u2264 C +Mq|\u2126|/m . (5)\nSince P(\u2126) is compact, we can find a limit point \u03c0\u2217 of \u03c0m. It will be a limit point of \u03c0mk as well. Since q is compact, we can find g \u2217 k \u2208 q(\u03c0\u2217) such that (\u03c0\u2217, g\u2217k) are limit points of (\u03c0 m k , g m k ) for each k. Finally, since P({1, . . . , 4|\u2126|2}) is compact, we can find limit points \u03b1\u2217k (corresponding to the points g \u2217 k).\nTaking the limits as m \u2192 \u221e over the convergent subsequences in (5), we get\n\u2200\u03c9 \u2208 \u2126 4|\u2126|2 \u2211\nk=1\n\u03b1\u2217kg \u2217 k(\u03c9) \u2264 C .\nSince q(\u03c0\u2217)+ [0,\u221e]\u2126 is convex, the convex combination \u22114|\u2126| 2 k=1 \u03b1 \u2217 kg \u2217 k belongs to q(\u03c0\u2217)+[0,\u221e]\u2126. In other words, the combination is minorized by some g\u2217 \u2208 q(\u03c0\u2217) and\ng\u2217(\u03c9) \u2264 4|\u2126|2 \u2211\nk=1\n\u03b1\u2217kg \u2217 k(\u03c9) \u2264 C\nfor all \u03c9 \u2208 \u2126.\nNow let us prove a variant of the lemma suitable for the DTOL framework, where the set of outcomes is infinite. Here we make a strong assumption: the supermartingale values ST (\u03c0) depend on \u03c0 in a very limited way: just on the mean of \u03c0.\nLemma 3. Let \u2126 be [0, 1]N . Let X be a compact subset of R\u2126. Let q \u2286 P(\u2126)\u00d7X be a relation. Denote q(\u03c0) = {g | (\u03c0, g) \u2208 q} and ran q = \u222a\u03c0\u2208P(\u2126)q(\u03c0) \u2286 X. Assume that if \u222b \u03c9\u03c01(d\u03c9) = \u222b \u03c9\u03c02(d\u03c9) then q(\u03c01) = q(\u03c02). Suppose that q is\nclosed, for every \u03c0 \u2208 P(\u2126) the set q(\u03c0) is non-empty and the set q(\u03c0) + [0,\u221e]\u2126 is convex. If for some real constant C it holds that for every \u03c0 \u2208 P(\u2126)\n\u2200g \u2208 q(\u03c0) E\u03c0g \u2264 C , then there exists g \u2208 ran q such that\n\u2200\u03c9 \u2208 \u2126 g(\u03c9) \u2264 C . Proof. Since [0, 1]N is a compact metric space, the space P([0, 1]N) with weak topology is compact too (see, e. g. [10, Prop. B.28]). Hence q is compact as a closed subset of a compact set. Let Mq = maxg\u2208ran q,\u03c9\u2208\u2126 |g(\u03c9)|.\nWe consider P(\u2126) as a metric space with Wasserstein distance W (\u03c0, \u03c0\u2032) = supf |E\u03c0f \u2212 E\u03c0\u2032f |, where the supremum is taken over 1-Lipschitz functions (see [10, Def. B.20]). For every natural m > 0, let us construct a (1/m)-net {\u03c0mk } on P(\u2126) with the following property. Let \u03c9mi be a (1/(2m))-net on \u2126 such that at most 4N2 its elements are at the distance less than 1/(2m) from any \u03c9 \u2208 \u2126. For any \u03c0mk there exists \u03c9mi such that \u222b \u03c9\u03c0mk (d\u03c9) = \u03c9 m i . (A net with this property exists: note that for any \u03c0 there is a \u03c0\u2032 at the distance at most 1/(2m) such that \u222b\n\u03c9\u03c0\u2032(d\u03c9) = \u03c9mi ; it remains to consider a cover of 1/(2m)-neighborhoods centered in all \u03c0 with the given expected values.) For every \u03c0mk in the net, let us take any g m k \u2208 q(\u03c0mk ).\nNow let us define a function qm : P(\u2126)\u00d7\u2126 \u2192 R as a linear interpolation of the points (\u03c0mk , g m k ). Namely, let {umk } be a partition of unity of P(\u2126) subordinate to U1/m(\u03c0 m k ), the (1/m)-neighborhoods of \u03c0 m k (that is, u m k (\u03c0) are non-negative, umk (\u03c0) = 0 if the distance between \u03c0 and \u03c0 m k is 1/m or more, and the sum over k of all umk (\u03c0) is 1 at any \u03c0). Let q m(\u03c0, \u03c9) = \u2211 k u m k (\u03c0)g m k (\u03c9).\nThe function qm is forecast-continuous. Let us find an upper bound on E\u03c0q m(\u03c0, \u00b7):\nE\u03c0q m(\u03c0, \u00b7) =\n\u2211\nk\numk (\u03c0)E\u03c0g m k\n= \u2211\nk\numk (\u03c0)E\u03c0mk g m k +\n\u2211\nk\numk (\u03c0)\n( \u222b\n\u2126\ngmk (\u03c9)\u03c0(d\u03c9) \u2212 \u222b\n\u2126\ngmk (\u03c9)\u03c0 m k (d\u03c9)\n)\n\u2264 C +Mq/m (the bound on the first term holds since gmk \u2208 q(\u03c0mk ) and hence E\u03c0mk gmk \u2264 C).\nBy Lemma 1 we can find a point \u03c0m \u2208 P(\u2126) such that \u2200\u03c9 \u2208 \u2126 qm(\u03c0m, \u03c9) \u2264 C +Mq/m .\nAmong \u03c0mk such that u m k (\u03c0 m) is non-zero, there are at most 4N2 different expected values. Let us group all gmk corresponding to \u03c0 m k with a certain expected value. They belong to the same set q(\u03c0mk ), thus their convex combination is minorized by another element g\u0303mi of the same set. Thus we arrive at the following statement: there are some \u03b1mi \u2265 0, i = 1, . . . , 4N2, \u2211 i \u03b1 m i = 1, and some g\u0303mi \u2208 q(\u03c0mi ) with \u03c0mi at the distance at most 1/m from \u03c0m such that\n\u2200\u03c9 \u2208 \u2126 4N2 \u2211\ni=1\n\u03b1mi g\u0303 m i (\u03c9) \u2264 C +Mq/m . (6)\nThe rest of the proof is the same as in Lemma 2."}, {"heading": "3.4 Hoeffding Supermartingale", "text": "Here we introduce a specific multivalued supermartingale, or rather a family of supermartingales, that will be used for our main results.\nFor technical convenience, our definition of supermartingale St consists of two parts: a function G : P(\u2126) \u2192 2\u0393, which assigns a set of decisions G(\u03c0) \u2286 \u0393 to every \u03c0 \u2208 P(\u2126), and a function ft : \u0393\u00d7\u2126 \u2192 R. The values of St are defined by the formula:\nSt(\u03c0) = {g \u2208 R\u2126 | \u2203\u03b3 \u2208 G(\u03c0)\u2200\u03c9 \u2208 \u2126 g(\u03c9) = ft(\u03b3, \u03c9)} . (7)\nThe part G(\u03c0) depends on the game (\u2126,\u0393, \u03bb) only and does not change from step to step:\nG(\u03c0) = argmin \u03b3\u2208\u0393\nE\u03c0\u03bb(\u03b3, \u00b7) = {\u03b3 \u2208 \u0393 | \u2200\u03b3\u2032 \u2208 \u0393 E\u03c0\u03bb(\u03b3, \u00b7) \u2264 E\u03c0\u03bb(\u03b3\u2032, \u00b7)} . (8)\nLemma 4. Let (\u2126,\u0393, \u03bb) be a game such that its prediction set \u039b = {g \u2208 R\u2126 | \u2203\u03b3 \u2208 \u0393 \u2200\u03c9 \u2208 \u2126 g(\u03c9) = \u03bb(\u03b3, \u03c9)} is a non-empty compact subset of R\u2126 and \u039b + [0,\u221e]\u2126 is convex. Then the set\nG\u039b = {(\u03c0, g) \u2208 P(\u2126)\u00d7 \u039b | \u2203\u03b3 \u2208 G(\u03c0)\u2200\u03c9 \u2208 \u2126 g(\u03c9) = \u03bb(\u03b3, \u03c9)}\nis closed and for every \u03c0 \u2208 P(\u2126) the sets G(\u03c0) and G\u039b(\u03c0) = {g | (\u03c0, g) \u2208 G\u039b} are non-empty and the sets G\u039b(\u03c0) + [0,\u221e]\u2126 are convex. Proof. Since \u039b is non-empty and compact, the minimum of E\u03c0g is attained for every \u03c0, and hence G(\u03c0) and also G\u039b(\u03c0) is non-empty.\nAssume that g1, g2 \u2208 G\u039b(\u03c0) \u2286 \u039b and \u03b1 \u2208 [0, 1]. Then \u03b1g1+(1\u2212\u03b1)g2 \u2265 g \u2208 \u039b since \u039b + [0,\u221e]\u2126 is convex, and E\u03c0g \u2264 E\u03c0(\u03b1g1 + (1 \u2212 \u03b1)g2) = E\u03c0g1 = E\u03c0g2. Hence g \u2208 G\u039b(\u03c0) and thus G\u039b(\u03c0) + [0,\u221e]\u2126 is convex.\nIt remains to show that G\u039b is closed. Let gi \u2208 G\u039b(\u03c0i) and (\u03c0i, gi) converges to (\u03c0, g); we need to show that g \u2208 G\u039b(\u03c0). Indeed, g \u2208 \u039b since \u039b is compact and gi \u2192 g. Hence g = \u03bb(\u03b3, \u00b7) for some \u03b3 \u2208 \u0393. To show that \u03b3 \u2208 G(\u03c0), let us take any \u03b3\u2032 \u2208 \u0393 and check that E\u03c0g \u2264 E\u03c0g\u2032, where g\u2032 = \u03bb(\u03b3\u2032, \u00b7). Clearly, E\u03c0ig\u2032 converges to E\u03c0g\n\u2032 since \u03c0i \u2192 \u03c0. Also E\u03c0igi converges to E\u03c0g. Then for any \u01eb > 0 we can find sufficiently large i so that E\u03c0g \u2264 E\u03c0igi+\u01eb and E\u03c0ig\u2032 \u2264 E\u03c0g\u2032+\u01eb. We have E\u03c0igi \u2264 E\u03c0ig\u2032 since gi \u2208 L(\u03c0i). These three inequalities imply that E\u03c0g \u2264 E\u03c0g\u2032 + 2\u01eb. Since \u01eb is arbitrary, we have E\u03c0g \u2264 E\u03c0g\u2032.\nNote that for convex bounded compact games the conditions of the lemma are satisfied by definition. For DTOL, the set \u039b = {g \u2208 R[0,1]N | \u2203~p \u2208 \u2206N\u2200~\u03c9 \u2208 [0, 1]N g(\u03c9) = ~p \u00b7 ~\u03c9} is obviously non-empty and it is compact and convex as a linear image of simplex \u2206N .\nNow consider a function H : \u0393\u00d7 \u2126 \u2192 R of the form\nH(\u03b3, \u03c9) = e\u03b7(\u03bb(\u03b3,\u03c9)\u2212\u03bb(\u03b3 \u2032,\u03c9))\u2212\u03b72/2 , (9)\nwhere parameter \u03b3\u2032 \u2208 \u0393 and \u03b7 \u2265 0. Lemma 5. Let (\u2126,\u0393, \u03bb) be a game, the range of \u03bb is included in [0, 1] and G(\u03c0) is defined by (8). Then for all \u03b3\u2032 \u2208 \u0393, for all \u03b7 \u2264 0, for all \u03c0 \u2208 P(\u2126), and for all \u03b3 \u2208 G(\u03c0) it holds\nE\u03c0e \u03b7(\u03bb(\u03b3,\u00b7)\u2212\u03bb(\u03b3\u2032,\u00b7))\u2212\u03b72/2 \u2264 1 .\nProof. Since \u03bb(\u03b3, \u03c9) \u2212 \u03bb(\u03b3\u2032, \u03c9) \u2208 [\u22121, 1] for any \u03b3, \u03b3\u2032 and \u03c9, the Hoeffding inequality (see e. g. [3, Lemma A.1]) implies that\nE\u03c0e \u03b7(\u03bb(\u03b3,\u00b7)\u2212\u03bb(\u03b3\u2032,\u00b7)) \u2264 e\u03b7E\u03c0(\u03bb(\u03b3,\u00b7)\u2212\u03bb(\u03b3\u2032,\u00b7))+\u03b72/2 .\nIt remains to note that E\u03c0(\u03bb(\u03b3, \u00b7)\u2212 \u03bb(\u03b3\u2032, \u00b7)) \u2264 0 by definition of G(\u03c0).\nNow we can explain what ft will be used in (7):\nft(\u03b3, \u03c9) =\nK \u2211\nk=1\npt,kHt,k(\u03b3, \u03c9) , (10)\nwhere pt,k \u2265 0 are some weights and Ht,k are functions of the form (9) with some parameters \u03b7t,k and \u03b3t,k, cf. (11), (15), (18), (21), (22), and (23). The sum may be infinite or it can be even an integral over some measure pt(k). As in the definition of supermartingale, the index t may hide the dependence on a long sequence of arguments.\nLemma 6. St defined by (7), (8), and (10) satisfies the conditions of Lemma 2 if (\u2126,\u0393, \u03bb) is a bounded convex compact game with finite \u2126 or the conditions of Lemma 3 if (\u2126,\u0393, \u03bb) is DTOL, where St(\u03c0) is taken for q(\u03c0) and \u2211K\nk=1 pt,k is taken for C.\nProof. If g \u2208 St(\u03c0) then g = ft(\u03b3, \u00b7) for some \u03b3 \u2208 G(\u03c0). Thus we have E\u03c0g = \u2211K\nk=1 pt,kE\u03c0Ht,k(\u03b3, \u00b7) \u2264 \u2211K\nk=1 pt,k by Lemma 5. Clearly, St(\u03c0) \u2286 \u039b and \u039b is compact, as remarked after Lemma 4. The set\nSt(\u03c0) is non-empty since G(\u03c0) is non-empty by Lemma 4.\nLet \u03c6t(g) = \u2211K k=1 pt,kE\u03c0e \u03b7t,k(g\u2212\u03bb(\u03b3\u2032t,k,\u00b7))\u2212\u03b72t,k/2. Note that g \u2208 G\u039b(\u03c0) if and only if \u03c6(g) \u2208 St(\u03c0). Note also that \u03c6t is a convex (and hence continuous) function of g. Thus, the graph of St is closed since G\u039b closed and St(\u03c0)+[0,\u221e]\u2126 is convex since G\u039b(\u03c0) + [0,\u221e]\u2126 is convex.\nThe condition St(\u03c01) = St(\u03c02) when \u222b \u03c9\u03c01(d\u03c9) = \u222b\n\u03c9\u03c02(d\u03c9) for DTOL follows from the equality E\u03c0\u03bb(~\u03b3, ~\u03c9) = E\u03c0(~\u03b3 \u00b7 ~\u03c9) = ~\u03b3 \u00b7 E\u03c0~\u03c9."}, {"heading": "4 Loss Bounds", "text": "In this section, we consider applications of the supermartingale technique to obtaining the loss bounds in several different settings. Let us begin with a simple theorem that shows a clean application of the DFA.\nTheorem 7. If T is known in advance then the DFA achieves the bound\nLT \u2264 min n\nLnT + \u221a 2T lnN\n(for DTOL with N actions as well as for PEA with N experts).\nProof. Let \u03b7 = \u221a 2(lnN)/T and\nft(\u03b3, \u03c9) =\nN \u2211\nn=1\n1\nN e\u03b7(Lt\u22121\u2212L n t\u22121)\u2212\u03b72/2 \u00d7 e\u03b7(\u03bb(\u03b3,\u03c9)\u2212\u03bb(\u03b3nt ,\u03c9))\u2212\u03b72/2 . (11)\nAt each step t, the DFA finds \u03b3t such that ft(\u03b3t, \u03c9) \u2264 ft\u22121(\u03b3t\u22121, \u03c9t\u22121) for all \u03c9 \u2208 \u2126. Such a \u03b3t exists due to Lemma 6 combined with Lemma 3 for DTOL or Lemma 2 for PEA. Clearly, ft\u22121(\u03b3t\u22121, \u03c9t\u22121) = \u2211N n=1 1 N exp(\u03b7(Lt\u22121 \u2212 Lnt\u22121) \u2212 \u03b72/2), and we get that the DFA applied to {ft} guarantees that\nft(\u03b3T , \u03c9T ) =\nN \u2211\nn=1\n1\nN e\u03b7(LT\u2212L n T )\u2212\u03b72/2 \u2264 1 .\nBounding the sum from below by one additive term, we get the bound.\nThis bound is twice as large as the best bound obtained in [3] (see their Theorems 2.2 and 3.7). Our bound is the same as that in Corollary 2.2 in [3]."}, {"heading": "4.1 Bounds on \u01eb-Quantile Regret", "text": "The bound in Theorem 7 is guaranteed only once, at step T specified in advance. The next bound is uniform, that is, holds for any T , and it holds for \u01eb-quantile regret for all \u01eb > 0.\nTheorem 8. For DTOL with N actions, the DFA achieves the bound\n\u222b 1/e\n0\ne(LT\u2212L \u01eb T )\u03b7\u2212T\u03b72/2 d\u03b7\n\u03b7 (\nln 1\u03b7\n)2 \u2264 1\n\u01eb , (12)\nfor any T and any \u01eb > 0, where L\u01ebT is a value such that at least \u01eb-fraction of actions has the loss after step T not greater than L\u01ebT . In particular, (12) implies for any \u03b4 \u2208 (0, 1/4)\nLT \u2264 L\u01ebT + 2\u221a 2\u2212 \u03b4\n\u221a\nT ln 1\n\u01eb +\n1 2 T ln 1 \u03b4 + 2T ln lnT +max\n{\n4, 400 ln 1\n\u01eb\n}\n, (13)\nwhich can be further reduced to\nR\u01ebT \u2264 ( 1 + 1\nlnT\n)\n\u221a\n2T ln 1\n\u01eb + 5T ln lnT +O\n(\nln 1\n\u01eb\n)\n. (14)\nThe bound holds also for PEA; if each of finitely or infinitely many Experts is assigned some positive weight pn, the sum of all weights being not greater than 1, the DFA achieves (12)\u2013(14) with L\u01ebT being a value such that the total weight of Experts that have the loss after step T not greater than L\u01ebT is at least \u01eb. Proof. We mix all the supermartingales used in (11) over \u03b7 \u2208 [0, 1/e] according to the probability measure\n\u00b5(d\u03b7) = d\u03b7\n\u03b7 (\nln 1\u03b7\n)2 , \u03b7 \u2208 [0, 1/e] .\nWe apply the DFA (that is, at each step t, find \u03b3t such that ft(\u03b3t, \u03c9) \u2264 ft\u22121(\u03b3t\u22121, \u03c9t\u22121) for all \u03c9 \u2208 \u2126) to\nft(\u03b3, \u03c9) =\nN \u2211\nn=1\n1\nN\n\u222b 1/e\n0\nd\u03b7\n\u03b7 (\nln 1\u03b7\n)2 e \u03b7(Lt\u22121\u2212Lnt\u22121)\u2212\u03b72/2 \u00d7 e\u03b7(\u03bb(\u03b3,\u03c9)\u2212\u03bb(\u03b3nt ,\u03c9))\u2212\u03b72/2\n(15)\n(for PEA with weighed Experts, the term 1/N is replaced by pn) and achieve fT (\u03b3T , \u03c9T ) \u2264 1 for all T . Bounding the sum from below by the sum of terms where LnT \u2264 L\u01ebT , we get\n\u222b 1/e\n0\ne\u03b7(LT\u2212L \u01eb T )\u2212T\u03b72/2 d\u03b7\n\u03b7 (\nln 1\u03b7\n)2 \u2264 1\n\u01eb . (16)\nLet us estimate the integral. Notice that the exponent R\u03b7\u2212T\u03b72/2 is positive when 0 \u2264 \u03b7 \u2264 2R/T and attains its maximum R2/(2T ) at the mid-point of this interval, \u03b7 = R/T . Solving the quadratic inequality\nR\u03b7 \u2212 T\u03b72/2 \u2265 (1/2\u2212 \u03b4)R2/T\ngives\n\u03b7 \u2208 [ R\nT\n( 1\u2212 \u221a 2\u03b4 ) , R\nT\n( 1 + \u221a 2\u03b4 ) ]\n(0 < \u03b4 < 1/2) and so (16) implies\ne(1/2\u2212\u03b4)R 2/T ln(1 +\n\u221a 2\u03b4)\u2212 ln(1\u2212 \u221a 2\u03b4)\n(ln(T/R)\u2212 ln(1 + \u221a 2\u03b4))(ln(T/R)\u2212 ln(1\u2212 \u221a 2\u03b4)) \u2264 1 \u01eb\nwhen ( 1 + \u221a 2\u03b4 )\nR/T \u2264 1/e. If the last condition does not hold and hence R is close to T , one can get from (16) that T < 400 ln(1/\u01eb). Assuming \u03b4 < 1/4, we can obtain\ne(2\u2212\u03b4)R 2/T \u2264 1\n\u01eb \u221a 2\u03b4\nln2 4T\nR .\nFor R \u2265 4, we further obtain\n(2\u2212 \u03b4)R2/T \u2264 ln 1 \u01eb + 1 2 ln 1 \u03b4 + 2 ln lnT ,\nwhich finally leads to (13). Substituting \u03b4 = 1/ lnT , we get (14).\nThe bound (14) is not optimal asymptotically in T : it grows as O( \u221a T ln lnT )\nas T \u2192 \u221e, instead of O( \u221a T ). The next theorem gives an asymptotically optimal bound but using a \u201cfake\u201d DFA.\nTheorem 9. For DTOL with N actions, there exists a strategy that achieves the bound\nLT \u2264 L\u01ebT + 2 \u221a T ln 1\n\u01eb + 7\n\u221a T (17)\nfor any T and any \u01eb, where L\u01ebT is a value such that at least \u01eb-fraction of actions has the loss after step T not greater than L\u01ebT . The bound holds also for PEA; if each of finitely or infinitely many Experts is assigned some positive weight pn, the sum of all weights being not greater than 1, the strategy achieves (17) with L\u01ebT being a value such that the total weight of Experts that have the loss after step T not greater than L\u01ebT is at least \u01eb.\nProof. The algorithm in this theorem is not the DFA and does not use supermartingales properly: we use values ft(\u03b3t, \u03c9t) that may increase at some steps and ft(\u03b3t, \u03c9t) \u2264 ft\u22121(\u03b3t\u22121, \u03c9t\u22121) does not hold. Nevertheless, the increases of ft stay bounded so that it always holds ft(\u03b3t, \u03c9t) \u2264 1.\nLet 1/c = \u2211\u221e\ni=1 1 i2 . At step T , our algorithm finds \u03b3T such that fT (\u03b3T , \u03c9) \u2264\nCT for all \u03c9, where\nfT (\u03b3, \u03c9) =\nN \u2211\nn=1\n1\nN\n\u221e \u2211\ni=1\nc i2 e(i/\n\u221a T)(LT\u22121\u2212LnT\u22121)\u2212(i/2 \u221a T) \u2211T\u22121 t=1 (i/ \u221a t)\n\u00d7 e(i/ \u221a T)(\u03bb(\u03b3,\u03c9)\u2212\u03bb(\u03b3nT ,\u03c9))\u2212(i/ \u221a T)2/2 (18)\nand\nCT =\nN \u2211\nn=1\n1\nN\n\u221e \u2211\ni=1\nc i2 e(i/\n\u221a T)(LT\u22121\u2212LnT\u22121)\u2212(i/2 \u221a T) \u2211T\u22121 t=1 (i/ \u221a t) .\nFor PEA with weighed experts, it is sufficient to replace 1/N by pn in the definitions of fT and CT .\nNote that fT has the form (10), hence Lemma 6 applies, and due to Lemma 3 or Lemma 2 such a \u03b3T exists.\nLet us prove by induction over T that CT \u2264 1. It is trivial for T = 0, since L0 = L n 0 = 0 and \u22110 t=1 = 0. Assume that CT \u2264 1 and prove that CT+1 \u2264 1. By the choice of \u03b3T , we know that fT (\u03b3T , \u03c9T ) \u2264 CT \u2264 1. Since the function x\u03b1 is concave for 0 < \u03b1 < 1, we have\n1 \u2265 ( fT (\u03b3T , \u03c9T ) )\n\u221a T/ \u221a T+1\n=\n(\nN \u2211\nn=1\n1\nN\n\u221e \u2211\ni=1\nc i2 e(i/\n\u221a T)(LT\u2212LnT )\u2212(i/2 \u221a T) \u2211 T t=1(i/ \u221a t)\n) \u221a T/ \u221a T+1\n\u2265 N \u2211\nn=1\n1\nN\n\u221e \u2211\ni=1\nc i2 (\ne(i/ \u221a T )(LT\u2212LnT )\u2212(i/2 \u221a T) \u2211 T t=1(i/ \u221a t) )\n\u221a T/ \u221a T+1\n= CT+1 .\nNow it is easy to get the loss bound. Assume that for an \u01eb-fraction of Experts their losses LnT are smaller than or equal to L \u01eb T . Then fT (\u03b3T , \u03c9T ) can be bounded from below by\n\u01eb\n\u221e \u2211\ni=1\nc i2 e(i/\n\u221a T)(LT\u2212L\u01ebT )\u2212(i/2 \u221a T) \u2211 T t=1(i/ \u221a t) .\nFurther, bounding the infinite sum by one of the terms, we get\ne(i/ \u221a T)(LT\u2212L\u01ebT )\u2212(i/2 \u221a T) \u2211T t=1(i/ \u221a t) \u2264 1\n\u01eb\ni2 c .\nTaking the logarithm, using \u2211T\nt=1\n( 1/ \u221a t ) \u2264 2 \u221a T and rearranging the terms,\nwe get\nLT \u2264 L\u01ebT + \u221a T\ni\n(\ni2 + ln 1\n\u01eb + 2 ln i+ ln(1/c)\n)\n.\nLetting i = \u2308 \u221a ln(1/\u01eb) \u2309 + 1 and using the estimates i \u2264 \u221a\nln(1/\u01eb) + 2, 1/i \u2264 1, (ln i)/i \u2264 2, (ln(1/\u01eb))/i \u2264 \u221a\nln(1/\u01eb), and ln(1/c) = ln(\u03c02/6) \u2264 1, we obtain the final bound.\nRemark 1. For DTOL and for PEA with the finite number of Experts, the infinite sum over i in the proof can be replaced by the sum up to \u2308 \u221a lnN) \u2309 +1.\nHowever, one should keep decreasing weights c/i2: for uniform weights the bound will have an additional term of the form O((ln lnN)/ ln(1/\u01eb)).\nRemark 2. Probably, the first bound for \u01eb-quantile regret was stated (implicitly) in [9]. More precisely, that paper considered even more general regret notion: Theorem 1 in [9] gives a bound for PEA with weighed experts under the logarithmic loss of the form\nLT \u2264 N \u2211\nn=1\nunL n T +\nN \u2211\nn=1\nun ln un pn\nfor any ~u \u2208 \u2206N ; p1, . . . , pN are weights of Experts. Here pn are known to the algorithm in advance, whereas un are not known and the bound holds uniformly for all un. Taking un = 0 for Experts not from the \u01eb-quantile of the best Experts, and uniform un over Experts from the \u01eb-quantile, we get the bound in terms of L\u01ebT . It can be easily checked that the strategy in Theorem 9 also achieves the following bound:\nLT \u2264 N \u2211\nn=1\nunL n T + 2\n\u221a \u221a \u221a \u221aT ( N \u2211\nn=1\nun ln un pn\n)\n+ 7 \u221a T\nfor any ~u \u2208 \u2206N and any T . In Theorem 8 one can replace L\u01ebT by \u2211N n=1 unL n T and ln(1/\u01eb) by \u2211N\nn=1 un ln(un/pn) as well.\nRemark 3. Theorem 9 can be also adapted to discounted regrets of the form LT = \u2211T t=1(1\u2212 \u03b1)T\u2212t\u03bb(\u03b3t, \u03c9t) for a known \u03b1. Then \u01eb in the bound is replaced by \u03b1, and L\u01ebT by L n T = \u2211T t=1(1\u2212 \u03b1)T\u2212t\u03bb(\u03b3nt , \u03c9t)."}, {"heading": "4.2 Discussion of the Bounds", "text": "For a game with N Experts, the best bound, uniform in T , is given by [3, Theorem 2.3]:\nLT \u2264 LnT + \u221a 2T lnN + \u221a lnN\n8 . (19)\nThe bounds (14) and (17) with \u01eb = 1/N are always worse than (19). In the bound (17) the leading coefficient at \u221a T lnN is \u221a 2 times as much. In the\nbound (14) the coefficient at \u221a T lnN is the same, but the other terms are larger, and even the asymptotics is worse when N is fixed and T \u2192 \u221e. However, it appears that the bound (19) cannot be transferred to \u01eb-quantile regret R\u01ebT = LT \u2212L\u01ebT . The proof of Theorem 2.3 in [3] heavily relies on tracking the loss of only one best Expert, and it is unclear whether the existence of several good (or identical) Experts can be exploited in this proof. The experiments\nreported in [4] show that algorithms with good best Expert bounds may have rather bad performance when the nominal number of Experts is much greater than the effective number of Experts.\nThe first (and the only, as far as we know) bound specifically formulated for \u01eb-quantile regret is proven for the NormalHedge algorithm in [4, Theorem 1]:\nLT \u2264 L\u01ebT + \u221a ( 1 + ln 1\n\u01eb\n)( 3(1 + 50\u03b4)T + 16 ln2 N\n\u03b4\n(\n10.2\n\u03b42 + lnN\n))\n, (20)\nwhich holds uniformly for all \u03b4 \u2208 (0, 1/2]. Note that this bound depends on the effective number of actions 1/\u01eb and at the same time on the nominal number of actions N . The latter dependence is weak, but probably prevents the use of NormalHedge with infinitely many Experts.\nThe main advantage of our bounds in Theorems 8 and 9 is that they are perfectly in terms of the effective number of Experts. In a sense, the DFA does not need to know in advance the number of Experts.\nRemark 4. To obtain a precise statement about the unknown number of Expert, one can consider the setting where Experts may come at some later steps; the regret to a late Expert is accumulated over the steps after his coming \u2014 it is a simple time selection function (see Subsection 4.3), which switches from 0 to 1 only once. Our algorithms and bounds can be easily adapted for this setting: we must consider infinitely many Experts almost all of which are inactive; and then proceed similarly to Theorem 11.\nBoth our bounds are worse than (20) asymptotically when \u01eb and N are fixed and T \u2192 \u221e. In this case, the regret term in (20) grows as \u221a 3T ln(1/\u01eb) + 3T , whereas in (17) it grows as \u221a 4T ln(1/\u01eb) + 7 \u221a T and in (14), the worst bound, it grows as \u221a\n5T ln lnT + 2T ln(1/\u01eb). On the other hand, our bounds are better when T is relatively small. The term ln lnT is small for any reasonable practical application (e. g., ln lnT < 4 if T is the age of the universe expressed in microseconds), and then the main term in (14) is \u221a 2T ln(1/\u01eb), which even fits the optimal bound (19). Bound (17) improves over (20) for T \u2264 106 ln4 N . Now let us say a few words about known algorithms for which an \u01eb-quantile regret bounds were not formulated explicitly, but can easily be obtained. The Weighted Average Algorithm, which is used to obtain bound (19), can be analysed in a manner different from [3, Theorem 2.3], see [11]. Then one can obtain the following bound for \u01eb-quantile regret:\nLT \u2264 L\u01ebT + 1\nc\n\u221a T ln 1\n\u01eb + c\n\u221a T ,\nwhere the constant c > 0 is arbitrary but must be fixed in advance. If \u01eb is not known and hence c cannot be adapted to \u01eb, the leading term is O( \u221a T ln 1\u01eb ), which is worse than (17) for small \u01eb (that is, if we consider a large effective number of actions).\nFor the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to \u01eb-quantile regret:\nLT \u2264 cL\u01ebT + a ,\nwhere the possible constants c \u2265 1 and a depend on the loss function. However, in the case of DTOL or arbitrary convex games, the constant c is strictly greater that 1 and the bound may be much worse than (14) and (17) (when L\u01ebT grows\nsignificantly faster than \u221a T ). At the same time, this bound is much better when L\u01ebT \u2248 0 (there is at least \u01eb fraction of \u201cperfect\u201d Experts ). For the standard setting with the known number of Experts, other \u201csmall loss\u201d bounds, of the form LT \u2264 LnT + O( \u221a\nLnT ), were obtained. The authors of [4] posed an open question whether similar bounds can be obtained if the (effective) number of actions is not known. We left the question open."}, {"heading": "4.3 Internal Regret and Time Selection Functions", "text": "It was shown in [5] and in [7] that the loss bounds obtained by the DFA can be easily transferred to second-guessing experts and sleeping experts models. A second-guessing expert is a (known) function of Learner\u2019s decision. Informally, a second-guessing expert explains how Learner could improve (hopefully) his performance. Sleeping experts (or specialists) introduced in [9] may be inactive at some steps, abstaining from announcing their decision (a specialist may decide that the current problem is outside her expertize area). The regret of Learner to a sleeping expert is counted over the steps when the expert was active.\nThe models similar to second-guessing experts and sleeping experts were studied in DTOL as internal (or wide range) regret and time selection (or activation) functions respectively (see [12] for a review). The internal regret compares Learner\u2019s loss not to the loss of a fixed action, but to the loss of a modification rule of the form \u201cEvery time Learner selected action n he should have selected n\u2032 instead\u201d (more formally, all the weight \u03b3t,n assigned to action n should have been appended to \u03b3t,n\u2032). The wide range regret deals with more general modification rules which may replace each action by some other action. Note that a fixed action n is also a modification rule that suggests to use n instead of any other action.\nA time selection function attached to a modification rule assigns a scaling factor from [0, 1] to each step. The regret of Learner to this rule is a sum of the regrets at each step weighed by these factors. This weight can be regarded as a degree of specialist\u2019s certainty: when the rule is known to be inapplicable for some reason, the weight is zero; and when the rule is partially relevant, the rule agrees for some partial responsibility only.\nAs has been recently shown [12], an algorithm achieving in DTOL with N action some regret bound with respect to N can be transformed into an algorithm that achieves the same bound with respect to K for K modification rules with attached time selection functions. This gives the best regret bound O( \u221a T lnK).\nWe show how to extend the results of Theorems 8 and 9 to internal regret and time selection settings. We do not apply the general method of [12], but directly modify our supermartingales and proofs. Remarkably, we need very modest changes.\nA modification rule is represented by N\u00d7N stochastic matrixM : the matrix elements are non-negative and the sum of every column is 1. The (one-step) regret of Learner\u2019s decision ~\u03b3 \u2208 \u2206N to the modification rule M on the outcome ~\u03c9 \u2208 [0, 1]N is ~\u03b3 \u00b7~\u03c9\u2212 (M~\u03b3) \u00b7~\u03c9, where M~\u03b3 is the product of matrix M and vectorcolumn ~\u03b3. The total regret after step T on the sequence of outcomes ~\u03c91, ~\u03c92, . . .\nof Learner predicting ~\u03b31, ~\u03b32, . . . with respect to a modification rule M(t) with attached time selection function I(t) is\nRT =\nT \u2211\nt=1\nI(t) ( ~\u03b3t \u00b7 ~\u03c9t \u2212 (M(t)~\u03b3t) \u00b7 ~\u03c9t )\n(cf. RH,I,f in [12]).\nRemark 5. The definition above reflects a slightly more general notion of a modification rule, which allows, for example, the rules that mean \u201cinstead of n select at random n\u2032 or n\u2032\u2032 equiprobably\u201d. Khot and Ponnuswami [12] do not discuss such rules explicitly, but it appears that their method works for them as well (unless we miss some subtlety in the proof).\nFirst let us obtain an analogue of Theorem 9. We formulate the bound with respect to the effective number of modification rules. It is very probable that the method of [12] also transforms a bound in terms of the effective number of actions into a bound in terms of the effective number of modification rules, but we did not check.\nTheorem 10. In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N \u00d7N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1]. (The modification rule numbered k may arbitrarily change in time and may depend on the whole history, and so is the time selection function.) There is a strategy that achieves the bound\nR\u01ebT \u2264 2 \u221a T ln 1\n\u01eb + 7\n\u221a T\nfor any T and any \u01eb, where R\u01ebT is a value such that for at least \u01eb-fraction of the rules the regret RkT of rule k after step T is not less than R \u01eb T .\nProof. The proof is very similar to the proof of theorem 9. The only change in the algorithm is that in (18) we replace (\u03bb(~\u03b3, ~\u03c9) \u2212 \u03bb(~\u03b3nT , ~\u03c9)) = ~\u03b3 \u00b7 ~\u03c9 \u2212 \u03c9n by Ik(t) ( ~\u03b3 \u00b7 ~\u03c9 \u2212 (Mk(t)~\u03b3) \u00b7 ~\u03c9 ) and thus apply the same algorithm with\nfT (\u03b3, \u03c9) =\nK \u2211\nk=1\n1\nK\n\u221e \u2211\ni=1\nc i2 e(i/\n\u221a T)RkT\u22121\u2212(i/2 \u221a T) \u2211T\u22121 t=1 (i/ \u221a t)\n\u00d7 e(i/ \u221a T)(Ik(T )(~\u03b3\u00b7~\u03c9\u2212(Mk(T )~\u03b3)\u00b7~\u03c9))\u2212(i/ \u221a T)2/2 . (21)\nWe need to check that the conditions of Lemma 3 are satisfied. It is enough to observe that I(T ) \u2264 1 and that exp (( i/ \u221a T ) (Ik(T )(~\u03b3 \u00b7 ~\u03c9 \u2212 (Mk(T )~\u03b3) \u00b7 ~\u03c9)) ) is\nconvex in ~\u03b3, then the proof of the Lemma 6 applies without changes. The loss bound is obtained as in Theorem 9.\nTheorem 8 can be adapted in a similar way. But we formulate another analogue of the theorem: The bound includes the total number of modification rules instead of the the effective number of them, but the regret of each rule k is bounded in terms of the actual activity time (or awake time)\n\u2211T t=1 Ik(t) of\nthe rule, not the total time T . We do not know whether bounds referring to the awake time were explicitly stated anywhere; however, a bound of this kind can be obtained from bounds that depend on the loss of the rule (or action), as in [2, Theorem 16] or [12, Theorem 5].\nTheorem 11. In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N \u00d7N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1]. The DFA achieves the bound\n\u222b 1/e\n0\ne\u03b7R k T\u2212Tk(T )\u03b72/2 d\u03b7\n\u03b7 (\nln 1\u03b7\n)2 \u2264 K ,\nwhere Tk(T ) = \u2211T\nt=1 Ik(t), for any T and k = 1, . . . ,K. In particular, the above bound implies for any \u03b4 \u2208 (0, 1/4)\nRkT \u2264 2\u221a 2\u2212 \u03b4\n\u221a\nTk(T ) ln 1\nK +\n1 2 Tk(T ) ln 1 \u03b4 + 2Tk(T ) ln lnTk(T )\n+ max {4, 400 lnK} , which can be further reduced to\nR\u01ebT \u2264 ( 1 + 1\nlnTk(T )\n)\n\u221a\n2Tk(T ) lnK + 5Tk(T ) ln lnTk(T ) +O (lnK) .\nProof. We change the supermartingale used for Theorem 8, similarly to the proof of Theorem 10. Namely, we apply the DFA to the supermartingale\nft(\u03b3, \u03c9) =\nK \u2211\nk=1\n1\nK\n\u222b 1/e\n0\nd\u03b7\n\u03b7 (\nln 1\u03b7\n)2 e \u03b7Rkt\u22121\u2212Tk(t\u22121)\u03b72/2\n\u00d7 e\u03b7Ik(T )(~\u03b3\u00b7~\u03c9\u2212(Mk(T )~\u03b3)\u00b7~\u03c9)\u2212(\u03b7Ik(t))2/2 . (22) Note that in contrast to the proof of Theorem 10, Ik(t) appears also in the \u201cHoeffding correction term\u201d e\u2212\u03b7 2/2. The rest of the proof does not change much. To get the loss bound we observe that \u2211T\nt=1(Ik(t)) 2 \u2264 Tk(T ) since\nIk(t) \u2208 [0, 1]."}, {"heading": "4.4 A Toy Example of a Multiobjective Bound", "text": "In this subsection, we discuss bounds with respect to two loss functions. In [7], we showed how to cope with several mixable loss functions. Here we combine a mixable loss function (the square loss) with a non-mixable one (the absolute loss).\nLet us describe an informal prediction setting where such a combination of loss functions can make sense. We want to predict the probability of rain. We have two groups of Experts. The first group consists of Metoffices that give the probability and evaluate the result according to the Brier (square) loss function. The second group is Simpletons, they give a boolean (\u2018rain\u2019/\u2018no rain\u2019) prediction and count the number of errors (the simple prediction game). We must provide a pair, a probability and a boolean prediction, and the two components of our prediction must agree in the following sense: if we give probability of rain more than one half, we must predict \u2018rain\u2019; if we give probability of rain less than one half, we must predict \u2018no rain\u2019; only if we give the probability 1/2, we may choose the boolean prediction arbitrary (so we can randomize here). In the theorem below we bound both Learner\u2019s Brier loss and Learner\u2019s expected (over the internal randomizer) number of errors.\nTheorem 12. Assume that we are given K Experts that give predictions pk \u2208 [0, 1] and M Experts that give predictions bm \u2208 {0, 1}. Learner is allowed to give predictions (p, p\u0303) \u2208 [0, 1]\u00d7 [0, 1], with the following restriction: if p < 1/2 then p\u0303 = 0 and if p > 1/2 then p\u0303 = 1. Then there exists a strategy for Learner guaranteeing for any sequence of outcomes \u03c91, \u03c92, . . . that for any T and for any k it holds\nT \u2211\nt=1\n(pt \u2212 \u03c9t)2 \u2264 T \u2211\nt=1\n(pkt \u2212 \u03c9t)2 + 1\n2 ln(K +M) ,\nand for any T and for any m it holds\nT \u2211\nt=1\n|p\u0303t \u2212 \u03c9t| \u2264 T \u2211\nt=1\n[bmt 6= \u03c9t] +O( \u221a T ln(K +M) + T ln lnT ) ,\nwhere [bmt 6= \u03c9t] = 1 if bmt 6= \u03c9t and [bmt 6= \u03c9t] = 0 otherwise.\nProof. Let A = {(p, p\u0303) \u2208 [0, 1]2 | p\u0303 = 0 if p < 1/2 and p\u0303 = 1 if p > 1/2 and } = {(p, 0) | p \u2208 [0, 1/2)} \u222a {(1/2, p\u0303) | p\u0303 \u2208 [0, 1]} \u222a {(p, 1) | p \u2208 (1/2, 1]}. We apply the DFA to supermatingale ST on \u2126 = {0, 1} defined by (7) with\nfT (p, p\u0303, \u03c9) = 1\nK +M\nK \u2211\nk=1\ne2 \u2211T t=1 ((pt\u2212\u03c9t)2\u2212(pkt \u2212\u03c9t)2) \u00d7 e2((p\u2212\u03c9)2\u2212(pkT\u2212\u03c9)2)\n+ 1\nK +M\nM \u2211\nm=1\n\u222b 1/e\n0\nd\u03b7\n\u03b7 (\nln 1\u03b7\n)2 e \u03b7 \u2211 T t=1(|p\u0303t\u2212\u03c9t|\u2212[bmt 6=\u03c9t])\u2212\u03b72/2\u00d7e\u03b7(|p\u0303\u2212\u03c9|\u2212[bmT 6=\u03c9])\u2212\u03b72/2\n(23)\nand G(\u03c0) = {(p, p\u0303) \u2208 A | p = \u03c0(1)}. To ensure that ST is a supermartingale, we need to check that E\u03c0 ( |p\u0303\u2212 \u03c9| \u2212 [bmT 6= \u03c9] )\n\u2264 0 if (\u03c0(1), p\u0303) \u2208 G(\u03c0). Then we can refer to Lemma 6 and [5, Lemma 2].\nIndeed, E\u03c0 ( |p\u0303\u2212 \u03c9| \u2212 [bmT 6= \u03c9] ) = \u03c0(1)(1 \u2212 p\u0303\u2212 (1\u2212 bmT )) + \u03c0(0)(p\u0303\u2212 bmT ) = (\u03c0(0)\u2212 \u03c0(1))(p\u0303\u2212 bmT ). If \u03c0(1) > 1/2 then \u03c0(0) < 1/2 and p\u0303 = 1 \u2265 bmT . If \u03c0(1) < 1/2 then \u03c0(0) > 1/2 and p\u0303 = 0 \u2264 bmT . If \u03c0(1) = 1/2 then \u03c0(0) = 1/2. Obviously, in all the cases (\u03c0(0)\u2212 \u03c0(1))(p\u0303\u2212 bmT ) \u2264 0.\nThe bound follows in the usual way (cf. Theorem 8).\nRemark 6. Let us discuss how to find the numbers p and p\u0303 such that fT (p, p\u0303, 0) \u2264 1 and fT (p, p\u0303, 1) \u2264 1. Consider x \u2208 [0, 2] and two functions\np(x) =\n\n \n \nx, if x < 1/2, 1/2, if x \u2208 [1/2, 3/2], x\u2212 1, if x > 3/2,\nand p\u0303(x) = min{1,max{x\u2212 1/2, 0}} .\nClearly, p(x) and p\u0303(x) are continuous functions of x. Let\ng(x, \u03c9) = fT (p(x), p\u0303(x), \u03c9)\u2212 1 .\nIt is obvious that if g(x0, 0) \u2264 0 and g(x0, 1) \u2264 0 then we can take p(x0) and p\u0303(x0) as p and p\u0303 we are looking for. The supermartingale property of ST and the definition of ST imply that\np(x)g(x, 1) + (1\u2212 p(x))g(x, 0) \u2264 0 .\nSubstituting x = 0, we get g(0, 0) \u2264 0. Substituting x = 2, we get g(2, 1) \u2264 0. If g(0, 1) \u2264 0 or g(2, 0) \u2264 0, we can take x0 = 0 or x0 = 2 respectively. Otherwise, consider the function \u03c6(x) = g(x, 1) \u2212 g(x, 0). It is continuous, \u03c6(0) > 0 and \u03c6(2) < 0, hence there exists x0 such that \u03c6(x0) = 0. Clearly, g(x0, 0) = g(x0, 1) \u2264 0."}, {"heading": "Acknowledgements", "text": "This work was supported by EPSRC, grant EP/F002998/1. We are grateful to Yura Kalnishkan for discussions."}, {"heading": "1244 (2008).", "text": "[12] Subhash Khot and Ashok Kumar Ponnuswami. Minimizing Wide Range Regret with Time Selection Functions. In Rocco A. Servedio and Tong Zhang, editors, 21st Annual Conference on Learning Theory - COLT 2008, Helsinki, Finland, July 9-12, 2008, pages 81\u201386, Omnipress, 2008.\n[13] V. Vovk. A game of prediction of expert advice. Journal of Computer and System Sciences, 56:153-173, 1998."}], "references": [{"title": "Fixed Point Theory and Applications , volume 141 of Cambridge Tracts in Mathematics", "author": ["R. Agarwal", "M. Meehan", "D. O\u2019Regan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "From External to Internal Regret", "author": ["Avrim Blum", "Yishay Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A Parameter-free Hedging Algorithm", "author": ["Kamalika Chaudhuri", "Yoav Freund", "Daniel Hsu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Supermartingales in prediction with expert advice", "author": ["Alexey Chernov", "Yuri Kalnishkan", "Fedor Zhdanov", "Vladimir Vovk"], "venue": "Proceedings of the Nineteenth International Conference on Algorithmic Learning Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Supermartingales in prediction with expert advice", "author": ["Alexey Chernov", "Yuri Kalnishkan", "Fedor Zhdanov", "Vladimir Vovk"], "venue": "Theoretical Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Prediction with expert evaluators", "author": ["Alexey Chernov", "Vladimir Vovk"], "venue": "Proceedings of the 20th International Conference on Algorithmic Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E. Schapire", "Yoram Singer", "Manfred K. Warmuth"], "venue": "In Proceedings of the Twenty Ninth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "The weak aggregating algorithm and weak mixability", "author": ["Yuri Kalnishkan", "Michael V. Vyugin"], "venue": "Proc. COLT\u201905,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Minimizing Wide Range Regret with Time Selection Functions", "author": ["Subhash Khot", "Ashok Kumar Ponnuswami"], "venue": "21st Annual Conference on Learning Theory - COLT", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "A game of prediction of expert advice", "author": ["V. Vovk"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "In the PEA framework (see [3] for details, references and historical notes), at each step Learner gets decisions (also called predictions) of several Experts and must make his own decision.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "In DTOL, introduced in [8], Learner\u2019s decision is a probability distribution on a finite set of actions.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "In this paper we deal with another kind of bound, recently introduced in [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "The following regret bound obtained in [4] for their NormalHedge algorithm holds for this case:", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "Our bound has a simpler structure, but it is generally incomparable to the (precise) bound for Normal Hedge from [4] (see Subsection 4.", "startOffset": 113, "endOffset": 116}, {"referenceID": 10, "context": "Also our bound can be easily adapted to internal regret (see [12] for definition).", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "In [5], the DF was used to obtain bounds of the form LT \u2264 cLT + a, where c and a are some constants.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "(This result is of certain independent interest: for example, it helps to get rid of additional Assumption 3 in Theorem 3 in [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "For any natural N , by \u2206N we denote the standard simplex in R N : \u2206N = {~ p \u2208 [0, 1] | \u2211N n=1 pn = 1}.", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "Reality announces ~\u03c9t \u2208 [0, 1] .", "startOffset": 24, "endOffset": 30}, {"referenceID": 7, "context": "The decision-theoretic framework for online learning (DTOL) was introduced in [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "As defined in [4] (for DTOL), the regret to the top \u01eb-quantile (at step T ) is the value R T such that there are at least \u01ebN actions (the fraction at least \u01eb of all Experts) with R T \u2265 R T .", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": ", pK \u2208 [0, 1] , \u2211K k=1 pk = 1, there exists g \u2208 \u039b such that g(\u03c9) \u2264 \u2211Kk=1 pkgk(\u03c9) for all \u03c9 \u2208 \u03a9.", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "For such games, we assume without loss of generality that \u039b \u2286 [0, 1] (we always can scale the loss function).", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "For DTOL as a special case of PEA, the outcome space is \u03a9 = [0, 1] , the decision space is \u0393 = \u2206N , and the loss function is \u03bb(~\u03b3, ~\u03c9) = ~\u03b3 \u00b7 ~ \u03c9.", "startOffset": 60, "endOffset": 66}, {"referenceID": 11, "context": "In particular, we can obtain PEA bounds that hold for specific loss functions or classes of loss functions (such as mixable loss functions [13]), and these bounds may be much stronger than the general bounds induced by DTOL.", "startOffset": 139, "endOffset": 143}, {"referenceID": 4, "context": "(3) This definition of supermartingale is equivalent to the one given in [5].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "Let \u03a9 be [0, 1] .", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "Since [0, 1] is a compact metric space, the space P([0, 1]) with weak topology is compact too (see, e.", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "Since [0, 1] is a compact metric space, the space P([0, 1]) with weak topology is compact too (see, e.", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "Assume that g1, g2 \u2208 G\u039b(\u03c0) \u2286 \u039b and \u03b1 \u2208 [0, 1].", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "For DTOL, the set \u039b = {g \u2208 R[0,1]N | \u2203~ p \u2208 \u2206N\u2200~\u03c9 \u2208 [0, 1] g(\u03c9) = ~ p \u00b7 ~\u03c9} is obviously non-empty and it is compact and convex as a linear image of simplex \u2206N .", "startOffset": 28, "endOffset": 33}, {"referenceID": 0, "context": "For DTOL, the set \u039b = {g \u2208 R[0,1]N | \u2203~ p \u2208 \u2206N\u2200~\u03c9 \u2208 [0, 1] g(\u03c9) = ~ p \u00b7 ~\u03c9} is obviously non-empty and it is compact and convex as a linear image of simplex \u2206N .", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "Let (\u03a9,\u0393, \u03bb) be a game, the range of \u03bb is included in [0, 1] and G(\u03c0) is defined by (8).", "startOffset": 54, "endOffset": 60}, {"referenceID": 2, "context": "This bound is twice as large as the best bound obtained in [3] (see their Theorems 2.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "2 in [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "Probably, the first bound for \u01eb-quantile regret was stated (implicitly) in [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "More precisely, that paper considered even more general regret notion: Theorem 1 in [9] gives a bound for PEA with weighed experts under the logarithmic loss of the form", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "3 in [3] heavily relies on tracking the loss of only one best Expert, and it is unclear whether the existence of several good (or identical) Experts can be exploited in this proof.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "reported in [4] show that algorithms with good best Expert bounds may have rather bad performance when the nominal number of Experts is much greater than the effective number of Experts.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "3], see [11].", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "For the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to \u01eb-quantile regret: LT \u2264 cLT + a ,", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "For the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to \u01eb-quantile regret: LT \u2264 cLT + a ,", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "The authors of [4] posed an open question whether similar bounds can be obtained if the (effective) number of actions is not known.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "3 Internal Regret and Time Selection Functions It was shown in [5] and in [7] that the loss bounds obtained by the DFA can be easily transferred to second-guessing experts and sleeping experts models.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "3 Internal Regret and Time Selection Functions It was shown in [5] and in [7] that the loss bounds obtained by the DFA can be easily transferred to second-guessing experts and sleeping experts models.", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "Sleeping experts (or specialists) introduced in [9] may be inactive at some steps, abstaining from announcing their decision (a specialist may decide that the current problem is outside her expertize area).", "startOffset": 48, "endOffset": 51}, {"referenceID": 10, "context": "The models similar to second-guessing experts and sleeping experts were studied in DTOL as internal (or wide range) regret and time selection (or activation) functions respectively (see [12] for a review).", "startOffset": 186, "endOffset": 190}, {"referenceID": 0, "context": "A time selection function attached to a modification rule assigns a scaling factor from [0, 1] to each step.", "startOffset": 88, "endOffset": 94}, {"referenceID": 10, "context": "As has been recently shown [12], an algorithm achieving in DTOL with N action some regret bound with respect to N can be transformed into an algorithm that achieves the same bound with respect to K for K modification rules with attached time selection functions.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "We do not apply the general method of [12], but directly modify our supermartingales and proofs.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "The (one-step) regret of Learner\u2019s decision ~\u03b3 \u2208 \u2206N to the modification rule M on the outcome ~\u03c9 \u2208 [0, 1] is ~\u03b3 \u00b7~\u03c9\u2212 (M~\u03b3) \u00b7~\u03c9, where M~\u03b3 is the product of matrix M and vectorcolumn ~\u03b3.", "startOffset": 99, "endOffset": 105}, {"referenceID": 10, "context": "RH,I,f in [12]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "Khot and Ponnuswami [12] do not discuss such rules explicitly, but it appears that their method works for them as well (unless we miss some subtlety in the proof).", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "It is very probable that the method of [12] also transforms a bound in terms of the effective number of actions into a bound in terms of the effective number of modification rules, but we did not check.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N \u00d7N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1].", "startOffset": 190, "endOffset": 196}, {"referenceID": 0, "context": "In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N \u00d7N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1].", "startOffset": 190, "endOffset": 196}, {"referenceID": 0, "context": "To get the loss bound we observe that \u2211T t=1(Ik(t)) 2 \u2264 Tk(T ) since Ik(t) \u2208 [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 6, "context": "In [7], we showed how to cope with several mixable loss functions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Assume that we are given K Experts that give predictions p \u2208 [0, 1] and M Experts that give predictions b \u2208 {0, 1}.", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "Learner is allowed to give predictions (p, p\u0303) \u2208 [0, 1]\u00d7 [0, 1], with the following restriction: if p < 1/2 then p\u0303 = 0 and if p > 1/2 then p\u0303 = 1.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "Learner is allowed to give predictions (p, p\u0303) \u2208 [0, 1]\u00d7 [0, 1], with the following restriction: if p < 1/2 then p\u0303 = 0 and if p > 1/2 then p\u0303 = 1.", "startOffset": 57, "endOffset": 63}, {"referenceID": 0, "context": "Let A = {(p, p\u0303) \u2208 [0, 1] | p\u0303 = 0 if p < 1/2 and p\u0303 = 1 if p > 1/2 and } = {(p, 0) | p \u2208 [0, 1/2)} \u222a {(1/2, p\u0303) | p\u0303 \u2208 [0, 1]} \u222a {(p, 1) | p \u2208 (1/2, 1]}.", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "Let A = {(p, p\u0303) \u2208 [0, 1] | p\u0303 = 0 if p < 1/2 and p\u0303 = 1 if p > 1/2 and } = {(p, 0) | p \u2208 [0, 1/2)} \u222a {(1/2, p\u0303) | p\u0303 \u2208 [0, 1]} \u222a {(p, 1) | p \u2208 (1/2, 1]}.", "startOffset": 120, "endOffset": 126}, {"referenceID": 1, "context": "Consider x \u2208 [0, 2] and two functions", "startOffset": 13, "endOffset": 19}], "year": 2013, "abstractText": "In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales.", "creator": "LaTeX with hyperref package"}}}