{"id": "1006.2945", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2010", "title": "Two-Timescale Learning Using Idiotypic Behaviour Mediation For A Navigating Mobile Robot", "abstract": "A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to solving mobile-robot navigation problems is presented and tested in both the real and virtual domains. The LTL phase consists of rapid simulations that use a Genetic Algorithm to derive diverse sets of behaviours, encoded as variable sets of attributes, and the STL phase is an idiotypic Artificial Immune System. Results from the LTL phase show that sets of behaviours develop very rapidly, and significantly greater diversity is obtained when multiple autonomous populations are used, rather than a single one.", "histories": [["v1", "Tue, 15 Jun 2010 10:17:21 GMT  (326kb)", "http://arxiv.org/abs/1006.2945v1", "40 pages, 12 tables, Journal of Applied Soft Computing"]], "COMMENTS": "40 pages, 12 tables, Journal of Applied Soft Computing", "reviews": [], "SUBJECTS": "cs.AI cs.NE cs.RO", "authors": ["amanda whitbrook", "uwe aickelin", "jonathan m garibaldi"], "accepted": false, "id": "1006.2945"}, "pdf": {"name": "1006.2945.pdf", "metadata": {"source": "CRF", "title": "TWO-TIMESCALE LEARNING USING IDIOTYPIC BEHAVIOUR MEDIATION FOR A NAVIGATING MOBILE ROBOT", "authors": ["Amanda M. Whitbrook", "Uwe Aickelin", "Jonathan M. Garibaldi"], "emails": ["jmg}@cs.nott.ac.uk"], "sections": [{"heading": null, "text": "A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to solving mobile-robot navigation problems is presented and tested in both the real and virtual domains. The LTL phase consists of rapid simulations that use a Genetic Algorithm to derive diverse sets of behaviours, encoded as variable sets of attributes, and the STL phase is an idiotypic Artificial Immune System. Results from the LTL phase show that sets of behaviours develop very rapidly, and significantly greater diversity is obtained when multiple autonomous populations are used, rather than a single one. The architecture is assessed under various scenarios, including removal of the LTL phase and switching off the idiotypic mechanism in the STL phase. The comparisons provide substantial evidence that the best option is the inclusion of both the LTL phase and the idiotypic system. In addition, this paper shows that structurally different environments can be used for the two phases without compromising transferability.\nKEYWORDS: Mobile-robot navigation, genetic algorithm, artificial immune systems, idiotypic network, reinforcement learning, behaviour mediation."}, {"heading": "1 INTRODUCTION", "text": "An important decision when designing effective controllers for mobile robots is how much a priori knowledge should be imparted to them. Should they attempt to learn all behaviours during the task, or should they begin with a set of pre-engineered actions? Both of these alternatives have considerable drawbacks; starting with no prior knowledge increases task time substantially because the robot has to undergo a learning period during which it is also at risk of damage. However, if it is solely reliant on designer-prescribed behaviours, it has no capacity for learning and adaptation. The architecture described in this paper takes inspiration from the vertebrate immune system in order to attempt to overcome these problems. The immune system learns to recognize antigens over the lifetime of the individual, which constitutes Short-Term Learning (STL), but it also knows how to build successful antibodies from gene libraries that have evolved over the lifetime of the species. This represents Long-Term Learning (LTL), defined as that which evolves and develops as a species interacts with its environment and reproduces itself. Here, this \u201ctwo timescale\u201d approach is mimicked by using an Artificial Immune System (AIS) to represent the STL phase, and a Genetic Algorithm (GA) to represent the LTL phase. The GA rapidly evolves sets of behaviours in simulation to seed the AIS, which removes the need for using pre-engineered behaviours, and prevents robots from having to begin a task with no knowledge. The GA and AIS are run consecutively, with the GA running first. Once the GA has converged, the evolved antibody information is stored in a database for AIS initialization, and the GA does not run again. An idiotypic network that uses Reinforcement Learning (RL) to update antibody-\nantigen matching is selected for the AIS system, and Farmer\u2019s computational model (Farmer 1986) of Jerne\u2019s idiotypic-network theory (Jerne 1974) is adopted. This model uses the analogy of antibodies as robot behaviours and antigens as environmental stimuli, and, in theory, has great potential to create very flexible and dynamic robots that can adapt to their environment. However, most previous implementations of the method exhibit rather limited self-discovery and learning properties, since their designs use very small numbers of pre-engineered behaviours, and only the network connections between them are evolved. In the architecture described here, the actual behaviours themselves are evolved, which permits delivery of novel and diverse sets of antibodies for seeding the AIS. This paper demonstrates the importance of seeding (i.e. including an LTL-phase) by comparing schemes that employ the GA with those that do not. It also investigates the benefits of idiotypic selection by comparing idiotypic systems with AIS schemes that rely on RL only. In addition, it examines whether antibody replacement is necessary in seeded AIS systems. Finally, as a result of all these investigations, an attempt is made to represent the antigen space (i.e. the environment) more fully. Here, comparisons with the previous results show that a more complex representation does not enhance performance."}, {"heading": "2 BACKGROUND AND MOTIVATION", "text": "Throughout the lifetime of an individual, the adaptive immune system learns to recognize antigens by building up high concentrations of antibodies that have proved useful in the past, and by eliminating those deemed redundant. This is a form of STL. However, the antibody repertoire is not random at birth and the mechanism by which\nantibodies are replaced is not a random process. Antibodies are built from gene libraries that have evolved over the lifetime of the species. This suggests that the immune system depends on both STL and LTL in order to achieve its purpose. AIS algorithms take inspiration from the natural immune system (de Castro and Timmis 2002), and a variety of different models have been applied to both hardware (Canham et al. 2003) and software (Neal and Timmis 2003) robotics. However, the most popular robotics software model has been the idiotypic network, based on Farmer\u2019s model of continuous antibody-concentration change. In this model the concentrations are not only dependent upon past matching to antigens, they also depend on the other antibodies present in the system, i.e. antibodies are continually suppressed and stimulated by each other as well as being stimulated by antigens. In theory this design permits great variability of robot behaviour since the antibody that best matches the invading antigen is not necessarily selected for execution; the complex dynamics of stimulation and suppression ensure that suitable alternative antibodies are tried when the need arises (see Whitbrook et al. 2007). However, past work in this area has mostly focused on how the antibodies in the network should be connected and, for simplicity, has used a single set of preengineered behaviours for the antibodies, which limits the potential of the method. For example, Watanabe et al. (1998a and 1998b) use an idiotypic network to control a garbage-collecting robot. Their antibodies are composed of a precondition, a behaviour, and an idiotope part that defines antibody connection. However, the sets of possible behaviours and preconditions are fixed, and only the idiotope part is evolved. Michelan and Von Zuben (2002) and Vargas et al. (2003) also use GAs, but again only the idiotypic-network connections are derived. Krautmacher and Dilger (2004) apply the idiotypic method to robot navigation, but their emphasis is on the use of a\nvariable set of antigens; they do not change or develop the initial set of handcrafted antibodies, as only the network links are evolved. Luh and Liu (2004) address targetfinding using an idiotypic system, modelling their antibodies as steering directions. However, although many behaviours are technically possible since any angle can be selected, the method is limited because a behaviour is defined only as a steering angle. Hart et al. (2003) update their network links dynamically using RL and a skill hierarchy, so that more complex tasks are achieved by building on basic ones, but the initial behaviours are hand-designed at the start. It is clear that the idiotypic AIS methodology holds great promise for providing a system that can adapt to change, but its potential has never been fully explored because of the limits imposed on the fundamental behaviour-set. This research aims to widen the scope of the idiotypic network by combining LTL with STL, as in the natural immune system. The LTL consists of a GA in which six basic antibody-types are encoded with a set of six variable attributes that can take many different values, meaning that the system can evolve complete sets of simple but very diverse antibodies. These can then be passed to the STL phase, as a form of seeding or intelligent initialization for the AIS. In addition, the seeding provides the potential to bestow much greater flexibility to the idiotypic system, as an evolved set of distinct behaviours is available for each known antigen, providing a degree of choice. LTL in simulation coupled with an idiotypic AIS in the real world represents a novel combination for robot-control systems, and should provide definite advantages, not only for AIS initialization, but also for evolutionary robotics. In the past, much evolutionary work has been carried out serially on physical robots, which requires a long time for convergence and puts the robot and its environment at risk of damage. For example, Floreano and Mondada (1996) adopt this approach and report a\nconvergence time of ten days. More recent evolutionary experiments with physical robots, for example Marocca and Floreano (2002,) Hornby et al. (2000), and Zykov et al. (2004) have produced reliable and robust systems, but have not overcome the problems of potential damage and slow, impractical convergence times. Parallel evolution with a number of robots (for example Watson et al. 1999) reduces the time required, but can still be extremely prohibitive in terms of time and logistics. Simulated robots provide a definite advantage for speed of convergence, but the tradeoff is the huge difference between the simulated and real domains (Brooks 1992). Systems that employ an evolutionary training period (LTL) and some form of lifelong adaptation (STL) have been used to try to address the problem of domain differences, for example, Walker et al. (2006) use a GA in the simulated LTL phase and an evolutionary strategy (ES) on the physical robot. They note improved performance when the LTL phase is implemented, and remark that the ES provides continued adaptation to the environment, but they deal with a limited number of behaviour parameters in the GA, and do not state the duration of the LTL phase. Keymeulen et al. (1998) run their LTL and STL phases simultaneously, as the physical robot maps its environment at the same time as carrying out its goal-seeking task, thus creating the simulated world. They report the rapid evolution of adaptable and fit controllers, but these results apply only to simple, structured environments where the robot can always detect the coloured target, and the obstacles are few. For example, they observe the development of obstacle avoidance in five minutes, but this applies to an environment with only one obstacle, and the results imply that the real robot was unable to avoid the obstacle prior to this. The method described here aims to capitalize on the fast convergence speeds that a simulator can achieve, but will also address the domain compatibility issues by\ntransferring the behaviours to an adaptive AIS that runs on a real robot. In theory the method should be entirely practical for real world situations, in terms of delivering a short training-period, safe starting-behaviours, and a fully-dynamic and adaptable system. The aims of this paper are to investigate whether there are distinct advantages to integrating LTL strategies with STL strategies (for this purpose unseeded systems that use random behaviour sets are also trialed), and to establish the role of the idiotypic network in providing flexibility. The important questions are whether the evolved antibodies can be used effectively in real world environments, or whether there is a need to replace the original antibodies with new ones. In addition, trials using a slightly more complex environmental model are conducted to determine whether this enhances performance. The paper thus aims to investigate the following hypotheses:\nH1 Seeded STL systems outperform unseeded STL systems. H2 Seeded STL systems that employ idiotypic effects outperform seeded systems\nthat rely on RL only.\nH3 As long as the LTL-derived behaviours are sufficiently diverse, antibody\nreplacement should not be necessary in the STL phase.\nH4 Task performance is enhanced by increasing the number of antigens from\neight to nine.\nWhitbrook et al. (2007) provides statistical evidence that idiotypic AIS systems are more effective than similar non-idiotypic techniques, but this is restricted to a single robotic platform (Pioneer 3), the simulated domain, and only two different environments. The work presented here will hence also extend this research to include\na different type of robot (e-puck), more environments, different problems, the real domain, an alternative RL strategy (see sections 4.4 and 5.3), and a variable idiotope (see section 5.1)."}, {"heading": "3 TEST ENVIRONMENTS AND PROBLEMS", "text": "The LTL phase requires accelerated simulations in order to produce the initial sets of antibodies as rapidly as possible. For this reason the Webots simulator (Michel 2004) is selected as it is able to run simulations up to 600 times faster than real time, depending on computer power, graphics card, world design and the number and complexity of the robots used. The chosen robot is the e-puck (see Fig 1), since the Webots c++ environment natively supports it. It is a miniature mobile-robot equipped with a ring of eight noisy, nonlinear, infra-red (IR) sensors that can detect the presence of objects up to a distance of about 0.1 m. It also has a small frontal camera and receives the raw RGB values of the images from this. Blob-finding software is created to translate the RGB data into groups of like-coloured pixels (blobs). The GA runs in a test environment that consists of a virtual e-puck navigating around a building with three rooms (see Figs 2 and 3) by tracking blue markers painted on the walls. These markers are intended to guide the robot through the doors, which close automatically once the robot has passed through. A run ends when the robot has crossed the finish-line in the third room, and its performance is measured according to speed of task completion L T, and number of collisions recorded L C. Two variations of the test environments are used; World 1 (see Fig 2) has fewer obstacles and no other\nrobots. World 2 (see Fig 3) includes many more obstacles, and there is also a dummy wandering-robot in each room. The STL is tested in both the virtual and real domains, and the simulated environments are named World 3 and World 4 (see Figs 4 and 5). In these the robot begins south of the central row of pillars and must detect and travel to the blue targetblock in the north, avoiding collisions. In addition, a wandering e-puck acts as a dynamic obstacle. Once the robot has arrived at the target, the number of collisions S C and task completion time S T are recorded. The starting positions of the robots and target block are changed automatically after each run. The real environment consists of a square wooden pen with sides 1.26 m long and 0.165 m high (see Fig 6), and the mission robot must find and travel to a blue ball located inside it, avoiding collisions. Once the ball is found it must come to a complete stop. The obstacles, robots and ball are randomly placed in different starting positions after each run, to create a slightly different environment each time. A hand-designed controller is also used for comparison with the seeded idiotypic system. This uses a simple random wander for target searching, a backward turning motion to escape collisions, and it steers the robot in the opposite direction to any detected obstacles. The simulations are run with Webots version 5.1.10 using GNU/Linux 2.6.9 (CentOS distribution) with a Pentium 4 processor (clock speed 3.6 GHz). Fast mode is used for the LTL, and real time for the STL. The graphics card is an NVIDIA GeForce 7600GS, which affords average simulation speeds of approximately 200-times realtime for World 1 and 100-times real-time for World 2. The camera field-of-view is set at 0.3 radians, the pixel width and height at 15 and 3 pixels respectively and the speed unit for the wheels is set to 0.00683 radians/s."}, {"heading": "4 LONG-TERM LEARNING (GA) SYSTEM ARCHITECTURE", "text": ""}, {"heading": "4.1 Antigens and Antibodies", "text": "The antigens model the environmental information as perceived by the sensors. There are only two basic types of antigen, whether the target is visible (a \u201ctarget\u201d type) and whether an obstacle is near (an \u201cobstacle\u201d type), the latter taking priority over the former. An obstacle is detected if the IR sensor with the maximum reading Imax has value Vmax equal to 250 or more. If this is the case then the antigen is of type \u201cobstacle\u201d, and the antigen is further classified in terms of the obstacle\u2019s distance from and its orientation toward the robot. The distance is \u201cnear\u201d if Vmax is between 250 (about 0.03 m) and 2400 (about 0.01 m), and \u201ccollision\u201d if Vmax is 2400 or more. The IR sensors correspond to the quantity of reflected light, so higher readings mean closer obstacles. The orientation is \u201cright\u201d if Imax is sensor 0, 1 or 2, \u201crear\u201d if it is 3 or 4 and \u201cleft\u201d if it is 5, 6 or 7 (see Fig 1). If no obstacles are detected then the perceived antigen is of type \u201ctarget\u201d and there are two varieties, \u201ctarget seen\u201d and \u201ctarget unseen\u201d, depending on whether appropriate-coloured pixel-clusters have been recognized by the blob-finding software. There are thus eight possible antigens, which are coded 0\u20137, see Table 1. Six basic types of behaviour are employed; wandering using either a left or right turn, wandering using both left and right turns, turning forwards, turning on the spot, turning backwards, and tracking the door-markers. Behaviours hence possess an attribute type U, and a further six attributes are encoded to enable behaviour diversity. These are speed S, frequency of turn F, angle of turn A, direction of turn D, frequency of right turn Rf, and angle of right turn Ra. The fusion of the basic behaviour-types\nwith a number of attributes that can take many values means that the GA has the potential to select from a huge number of possible robot actions. However, some behaviour types do not use a particular attribute and there are limits to the values that the attributes can take. These limits (see Table 2) are carefully selected in order to strike a balance between reducing the size of the search space, which increases speed of convergence, and maintaining diversity."}, {"heading": "4.2 GA System Structure", "text": "The GA control program uses the two-dimensional array of behaviours Bij, i = 0, \u2026, x-1, j = 0, \u2026, y-1, where x is the number of robots in the population (x \u2265 5) and y is\nthe number of antigens, i.e. eight. When the program begins i is equal to zero, and the array is initialized to null. The infra-red sensors are read every 192 milliseconds, but the camera is only read if no obstacles are found as this increases computational efficiency. Once an antigen code is determined, a behaviour or antibody is created to deal with it by randomly choosing a behaviour type and its attribute values. For example, the behaviour WANDER_SINGLE (605, 50, 90, LEFT, NULL, NULL) may be constructed. This behaviour consists of travelling forwards with a speed of 605 Speed Units/s, but turning left 50% of the time by reducing the speed of the left wheel by 90%. (Note that wheel speed reductions of more than 100% represent the wheels turning backwards.) The newly created action is executed and the sensor values are read again to determine the next antigen code. If the antigen has been encountered before, then the behaviour assigned previously is used, otherwise a new behaviour is created. The algorithm proceeds in this manner, creating new behaviours for antigens that have not been seen before and reusing the behaviours allotted to those that have. However, the behaviour\u2019s cumulative reinforcement-learning score E, which is a measure of how well it is thought to have performed, is adjusted after every sensor reading. If E falls below the threshold value of -14 then the behaviour is replaced with a new one. Behaviour replacement also occurs when the antigen has not changed in any 60-second period, as this most likely means that the robot has not undergone any translational movement. A separate supervisor-program is responsible for returning the virtual robot back to its start-point once it has passed the finish-line, for opening and closing the doors as necessary, and for repositioning the wandering dummy-robot, so that it is always in the same room as the mission robot. Another of the supervisor\u2019s functions is to assess\nthe time taken to complete the task L T. Each robot is given 1250 seconds to reach the end-point; those that fail receive a 1000-second penalty if they do not pass through any doors. Reduced penalties of 750 or 500 seconds are awarded to failing robots that pass through one door or two doors respectively. When the whole population has completed the course, the relative-fitness L \u00b5 of each individual is calculated. Since high values in terms of both L T and L C should yield a low relative-fitness, the following formula is used:\n,\n][\n1 1 0 1\u2211 \u2212 = \u2212 = x k ki i L ff \u00b5 (1)\nwhere L f is the absolute-fitness given by:\n.i L i L i L CTf \u03c1+= (2)\nIn this phase, \u03c1 is set to 1 to give greater weight to the task time, otherwise robots that constantly turn on the spot, and hence endure no collisions, would receive good relative-fitness values and the GA would take too long to converge. The five fittest robots from each generation are selected, and their mean task time and mean number of collisions are calculated. The mean absolute-fitness is derived from these using (2) and compared with that of the previous generation to assess rate-ofconvergence. The GA terminates when any of the four conditions shown in Table 3 are reached. These are selected in order to achieve fast convergence, but also to maintain a high solution quality. (Note that the convergence criteria are relaxed for World 2, as it is a more cluttered environment requiring a longer task completion\ntime.) If the stopping criteria are met, the attribute values representing the behaviours of the five fittest robots are saved for seeding the AIS system, otherwise the GA proceeds as described in section 4.3. Note that when adopting the scenario of five separate populations that never interbreed, the five robots that are assessed for convergence are the single fittest from each of the autonomous populations. In this case, convergence is dependent upon the single best L T, L C, and L f values, and the final five robots that pass their behaviours to the AIS system are the single fittest from each population after the GA is complete."}, {"heading": "4.3 GA Details", "text": "Two different parent robots are selected through the roulette-wheel method and each of the x pairs interbreeds to create x child robots, (x is the number of robots in the population). The process is concerned with assigning behaviour attribute-values to each of the child robots for each of the eight antigens in the system. It can take the form of complete antibody replacement, adoption of the attribute values of only one parent or crossover from both parents, and attribute-value mutation.\n\u2022 Complete antibody replacement occurs according to the prescribed mutation\nrate \u03b5. Here, a completely new random behaviour is assigned to the child robot\nfor the particular antigen, i.e. both the parent behaviours are ignored.\n\u2022 Crossover is used when there has been no complete replacement, and the\nmethod used depends on whether the parent behaviours are of the same antibody type U.\no If the types are different then the child adopts the complete set of\nattribute values of one parent only, which is selected at random.\no If the types are the same, then crossover can occur by taking the\naverages of the two parent values, by randomly selecting a parent value, or by taking an equal number from each parent according to set patterns. In these cases, the type of crossover is determined randomly with equal probability. The purpose behind this approach is to attempt to replicate nature, where the offspring of the same two parents may differ considerably each time they reproduce.\n\u2022 Mutation of an attribute value may also take place according to the mutation\nrate \u03b5, provided that complete replacement has not already occurred. Here, the individual attribute-values (except D) of a child robot may be increased or decreased by between 20% and 50%, but must remain within the prescribed limits."}, {"heading": "4.4 Reinforcement Learning in the Long-Term Learning Phase", "text": "Reinforcement Learning (RL) is used to accelerate GA convergence, and works by comparing current and previous antibody codes to determine behaviour effectiveness. Ten points are awarded for every positive change in the environment, and ten are deducted for each negative change. Table 4 shows the possible antigen code\ncombinations and column 3 shows the points added or deducted in the LTL-phase. For example, 20 points are awarded if the antigen code changes from an \u201cobstacle\u201d type to \u201ctarget seen\u201d, because the robot has moved away from an obstacle as well as gaining or keeping sight of the target. In the case where the antigen code remains at 1 (the target is kept in sight), the score awarded depends upon how the orientation of the target has moved with respect to the robot. In addition, when an obstacle is detected both in the current and previous iteration, then the score awarded depends upon several factors, including changes in the position of Imax and in the reading Vmax, the current and previous distance-type (\u201ccollision\u201d or \u201cnear\u201d) and the tallies of consecutive \u201cnears\u201d and \u201ccollisions\u201d. Further details on the LTL architecture are provided in Whitbrook et al. (2008a)."}, {"heading": "5 SHORT-TERM LEARNING (AIS) SYSTEM ARCHITECTURE", "text": ""}, {"heading": "5.1 Creating the Paratope and Idiotope Matrices", "text": "The GA selects the five fittest robots from the final generation, so five distinct sets of antibodies are used, each set consisting of eight behaviours, i.e. one antibody for each antigen. The 40 antibodies in the system can hence be represented as Aij, i = 0, \u2026, v-1, j = 0, \u2026, y-1, where v is the number of sets and y is the number of antigens. The evolved antibody types and their associated attribute values, task completion times L Ti and numbers of collisions L Ci are taken directly from the file created in the LTL phase. The STL phase calculates the relative fitness of each antibody set S \u00b5i from:\n,\n][\n1 1 0 1\u2211 \u2212 = \u2212 = v k ki i S ff \u00b5 (3)\nusing (2) with \u03c1 set to 8 to give the collisions approximate equal weight compared to the task time. (This is permissible here because it is assumed that evolution will not have selected robots that constantly turn on the spot.) Once the relative fitness values are calculated, a matrix of RL scores Pij can be derived by multiplying the antibody\u2019s final RL score Eij by the relative fitness S \u00b5i of its set, and scaling approximately to between 0.00 and 1.00 using:\n. \u03d5\n\u00b5i S\nij ij\nE P = (4)\nTaking \u03c6 as 20 achieves the required scaling in (4) since the maximum value Eij S \u00b5i can take is approximately 20. The matrix P is analogous to an antibody paratope as the scores represent a comparative estimate of how well each antibody matches its antigen. For the unseeded systems the five antibody sets are generated at the start of the STL phase, by randomly choosing behaviour types and their attribute values. The initial elements of P are also randomly generated, but always lie between 0.25 and 0.75 to try to limit any initial biasing of the selection. For both seeded and unseeded systems, a matrix I (analogous to a matrix of idiotope values) is created by comparing the individual paratope matrix elements Pij with the mean element value for each of the antigens \u03c3j. This is given by:\n.\n1 0\nv\nP\nv i ij\nj\n\u2211 \u2212\n==\u03c3 (5)\nIf Pij (i = 0, \u2026, v-1) is less than \u03c3j, then an idiotope value Iij of 1.0 is assigned, otherwise a value of zero is given. However, only one antibody in each set may have a non-zero idiotope. If more than one has a non-zero value, then one is selected at random and the others are set back to zero. This avoids over-stimulation or oversuppression of antibodies. The paratope matrix is adjusted after every iteration; first, because the active antibody\u2019s paratope value either increases or decreases, depending on the RL score awarded, and second, because the paratope values are re-calculated, so that each \u03c3j is returned to its initial mean value. The adjustment is given by:\n,0 1\ntj\nj\ntijtij PP\n\u03c3\n\u03c3 =\n+ (6)\nwhere \u03c3j0 represents the initial mean and \u03c3jt represents the temporary mean obtained after scoring of the active antibody. The adjustment helps to eliminate the problems that occur when useful antibodies acquire zero Pij values. The idiotope is recalculated, based on the latest Pij values, after every 120 sensor readings."}, {"heading": "5.2 Antibody Selection Process", "text": "At the start of the STL phase each antibody has 1000 clones in the system, but the numbers fluctuate according to a variation of Farmer\u2019s equation:\n),1( 3)()()1( kNbSN ttt imimim \u2212+=+ (7)\nwhere Nim represents the number of clones of each antibody matching the invading antigen m, Sim is the current strength-of-match of each of these antibodies to m, b is a scaling constant and k3 is the death rate constant. The concentration Cij of every antibody in the system consequently changes according to:\n, 1 0 1 0 \u2211\u2211 \u2212 = \u2212 =\n\u03a6 =\nx k y l\nkl\nij\nij\nN\nN C (8)\nwhere \u03a6 is another scaling factor that can be used to control the levels of interantibody stimulation and suppression (25 is used here).\nThe antibody selection process comprises three stages for idiotypic selection, but only one stage if idiotypic selection is not used. First, the sensors are read to determine the index of the presenting antigen m, and an appropriate antibody is selected from those available for that antigen. More specifically, the system chooses from antibodies Aim, i = 0, \u2026, 4, by examining the paratope values Pim. The antibody \u03b1 with the highest of these paratope values is chosen as the first-stage winner. If the index of the winning antibody set is denoted as n, then \u03b1 = Anm. If idiotypic effects are not considered \u03b1 carries out its action, and is assessed by RL, see section 5.3. If an idiotypic system is used, then the stimulatory and suppressive effects of \u03b1 on all the antibodies in the repertoire must be considered. This involves comparing the idiotope of \u03b1 with the paratopes of the other antibodies to determine how much each is stimulated, and comparing the paratope of \u03b1 with the idiotopes of the others to calculate how much each should be suppressed. Here, idiotypic selection is governed by equations (9)-(12), which are based on those in Whitbrook et al. (2007). Equation (9) concerns the increase in strength-of-match value \u03b5im when stimulation occurs,\n,)1( 1\n0 1\u2211\n\u2212\n= \u2212=\ny\nj njijnjijim CCIPk\u03b5 (9)\nwhere k1 is a constant that determines the magnitude of any stimulatory effects. The formula for the reduction in strength-of-match value \u03b4im when suppression occurs is given by:\n\u2211 \u2212\n= =\n1\n0 2 ,\ny\nj njijijnjim CCIPk\u03b4 (10)\nwhere k2 governs the suppression magnitude. Hence, the strength-of-match after the second selection-stage (Sim)2 is given by:\n,)()( 12 imimimim SS \u03b4\u03b5 \u2212+= (11)\nwhere the initial strength-of-match (Sim)1 for each antibody is taken as the current Pim value. After the (Sim)2 values are calculated, the numbers of clones Nim are adjusted using (7) and all concentrations Cij are re-evaluated using (8). The third stage calculates the activation \u03bb of each antibody in the sub-set Aim from:\n.)( 2imimim SC=\u03bb (12)\nThe third-stage winning antibody \u03b2 is that with the highest \u03bb value in the sub-set. If p is the index of \u03b2\u2019s antibody set, then \u03b2 = Apm. When idiotypic selection is used, \u03b2 carries out its action and it is \u03b2 that is scored using RL rather than \u03b1, although \u03b1 and \u03b2 are the same when n = p."}, {"heading": "5.3 Reinforcement Learning within the Short-Term Learning Phase", "text": "In the STL phase the RL scores are scaled to one hundredth of the values used for the LTL phase (see column 4 of Table 4), since the RL is intimately linked with the idiotypic selection process, and larger values would lead to over-stimulation and oversuppression. In addition, a reward is given when no obstacles are encountered, and penalties are issued when they are. This is in contrast to the LTL case, where no reward or penalty is issued, and is necessary to increase the flux of the system. In the\nLTL, neutral scores are permissible as there is ample time to develop good strategies, but in the STL, the idiotypic system needs to remain in a state of flux if suppression and stimulation are to occur at all. The maximum cumulative-RL-score (or Pij value) allowed is 1.00, and the minimum Pij value is 0.00. The Pij values are also adjusted when the antigen code has remained at 0 for more than 250 iterations, as this means that the robot is spending too much time wandering and has not found anything. It is important to recognize this behaviour as negative, as otherwise robots may be circling around on the spot, never achieving anything, but receiving constant rewards. The non-idiotypic case reduces the cumulative-RL-score by 1.0, and the idiotypic case reduces it by 0.5, as pre-trials have shown that non-idiotypic robots require a more drastic change to break out of repeated behaviour cycles. The same Pij adjustments are also made if there have been more than 15 consecutive obstacle encounters, as this may indicate that a robot is trapped. Following RL, the paratope values are scaled using (6). In the case of the unseeded trials, replacement occurs for all antibodies with a cumulative- RL-score less than 0.1. The successor is created by randomly choosing a behaviour type and its attribute values. Replacement does not occur in the seeded systems, since H3 is directly concerned with establishing whether this is necessary. Further details on the STL architecture are provided in Whitbrook et al. (2008b)."}, {"heading": "6 EXPERIMENTAL PROCEDURES AND RESULTS", "text": ""}, {"heading": "6.1 Long-Term Learning General Procedures", "text": "The GA is run in Worlds 1 and 2 using single populations of 25, 40, and 50 robots, and using five autonomous populations of five, eight, and ten. A mutation rate \u03b5 of 5% is used throughout, as previous trials have shown that this provides a good compromise between fast convergence, high diversity and good solution-quality. Solution quality L q is taken as half of the absolute-fitness value (2) with \u03c1 = 8, to give approximate equal weighting to the collisions. For each scenario, ten repeats are performed and the means of the convergence time \u03c4, solution quality L q, and diversity in type ZU and speed ZS (see Section 6.2) are recorded. Two\u2013tailed standard t-tests are conducted on the result sets, and differences are accepted as significant at the 99% level only."}, {"heading": "6.2 Measuring Antibody Diversity", "text": "Antibody diversity is measured using the type U and the speed S attributes, since these are the only action-controlling attributes common to all behaviours. The final antibodies are grouped by antigen number and the groups are assessed by comparison of each of the five members with the others, i.e. ten pair-wise comparisons are made in each group. A point is awarded for each comparison if the attribute values are different; if they are the same no points are awarded. For example, the set of behaviour types [1 3 4 4 1] has two pair-wise comparisons with the same value, so\neight points are given. Table 5 summarizes possible attribute-value combinations and the result of conducting the pair-wise comparisons on them. The y individual diversity-scores for each of U and S are summed and divided by \u03c3y to yield a diversity score for each attribute. Here \u03c3 is the expected diversity-score for a large number of randomly-selected sets of five antibodies. This is approximately 8.333 for U (see Table 5) and 10.000 for S. It is lower for U since there are only six behaviours to select from, whereas the speed is selected from 751 possible values, so there is a much higher probability of producing unique values in a random selection of five. The adjustment effectively means that a random selection yields a diversity of 1 for both S and U. The diversity calculation is given by:\n,1\ny\nz\nZ\ny\ni\ni\n\u03c3\n\u2211 == (13)\nwhere Z represents the overall diversity-score and z represents the individual score awarded to each antigen."}, {"heading": "6.3 Long-Term Learning Phase Results", "text": "Table 6 presents mean \u03c4, L q, ZU, and ZS values, and Table 7 summarises the significant difference levels when comparing single and multiple populations of robots. The schemes that are compared use the same number of robots, for example a single population of 25 is compared with five populations of five.\nThe tables show that, for both worlds, there are no significant differences between convergence times when comparing the single and multiple populations. In addition, speed diversity is significantly better for the multiple populations in all cases. Multiple populations always demonstrate a speed diversity of 100%, indicating that the final-selected genes are completely unrelated to each other, as expected. In contrast, single-population speed-diversity never reaches 100% as there are always repeated genes in the final-selected robots. Evidence from previous experiments with single populations of five, ten and 20 suggests that the level of gene duplication\ndecreases as the single population size increases. This explains the lower ZU and ZS values for a population of 25 robots. Type diversity is consistently higher for the multiple populations, but only significantly higher when comparing a single population of 25 with five populations of five robots. For the multiple populations, mean type-diversity ratings never reach 100%, even though there are no repeated genes. The reduced type-diversity must occur because there are only six types to choose from, and these are not randomly selected but chosen in a more intelligent way. However, speed diversity can remain at 100% because there are many different speeds to choose from and convergence is rapid. It is likely that both intelligent selection and repeated genes decrease the typediversity scores for the single populations, but in the multiple populations, the phenomenon is caused by intelligent selection only. In World 1, solution quality is consistently significantly better for the multiple populations, but this is not the case in World 2. This may indicate that using multiple populations helps to improve solution quality for simpler problems, but the phenomenon diminishes as the problem becomes more difficult. The best option in terms of population model appears to be five autonomous populations, since this elicits significantly-higher antibody diversity. In addition, one can run the GA without significantly increasing the convergence time or reducing solution quality, and the fast convergence times (ten minutes in World 1 and 25 minutes in World 2) satisfy the requirement for a practical training-period."}, {"heading": "6.4 Short-Term Learning General Procedures", "text": "Pre-trials have shown that the antibody sets taken from the above LTL experiments produce a higher number of collisions compared with a hand-designed controller, when used to seed the AIS in Worlds 3 and 4. Since the hand-designed controller deals with much slower speeds, the GA is run again in World 1 with five autonomous populations of ten robots and with the static-turn antibody\u2019s upper speed-limit reduced to 100 speed units/s, all other speed limits reduced to 400 speed units/s, and the reverse antibody\u2019s lower speed-limit reduced to 300 speed units/s. The stopping criteria is also simplified to g > 0 AND L Tg < 500 AND L Cg < 25 OR g > 30 to allow for the general increase in task time. Thirty STL trials are performed in each of the two simulated worlds, World 3 and World 4, and 20 are completed in the real world. This is done for each of the following systems; seeded with idiotypic effects, seeded with RL only, unseeded with idiotypic effects, unseeded with RL only, and the hand-designed controller. In the unseeded simulated-worlds two separate sets of experiments are conducted with two different initially-random behaviour sets R1 and R2. The real-world unseeded experiments use only R1 since they have to run in real time and are hence much more time consuming to carry out. In the idiotypic systems b is set to 100, k3 is set to zero, and k1 and k2 are set at 0.85 and 1.10 respectively. These values are chosen in order to yield a mean idiotypic difference rate of approximately 20%, as suggested in Whitbrook et al. (2007). Note that an idiotypic difference occurs when the antibodies \u03b1 and \u03b2 are different. A run finishes when the robot has detected three consecutive instances of more than 40 blue pixels in the ball image, so that it is \u201caware\u201d of having found its target. For all\nexperiments, the time taken S T and the number of collisions S C are capped at 4000 s and 100 respectively. Any runs that exceed either of these limits are counted as failures. The solution quality, S q is calculated in the same way as for the LTL, i.e:\n2\nCT q SS S \u03c1+= (14)\nwhere \u03c1 = 8 as before. Standard two-tailed t-tests are applied to compare the various systems, and differences are accepted as significant at the 99% level only."}, {"heading": "6.5 Short-Term Learning Phase Results", "text": "Table 8 shows the mean S C, S T, and S q values for each of the systems in each of the worlds, and Table 9 presents the significant difference levels when the systems are compared. Table 10 highlights the failure rates, indicating the percentage of failures due to an excessive number of collisions, running out of time, and overall.\nIn all of the worlds, both simulated and real, the seeded idiotypic system proves better in terms of fewer collisions, a faster completion time, and a higher solution quality.\nWhen compared with the unseeded systems it is significantly better in all cases, i.e. for all of the metrics, in all the worlds, and irrespective of whether the unseeded systems use idiotypic effects, or which random behaviour set is used.\nThe seeded idiotypic system also surpasses the hand-designed controller in all cases (except for a tie in S C in World 4), and more than half of these differences are significant overall. Moreover, in the real world all of the differences are significant. It appears that the hand-designed controller performs very well in the simulator in terms\nof S C, but poorly for S T, whereas in the real world it performs badly for both of these metrics. Although it has built-in initial knowledge, it probably proves inferior in the real world because of its inability to change the way it responds to an antigen. The seeded idiotypic system works well in the real world and in the simulator for both S C and S T. In fact, in the real world it proves significantly better than all of the other systems trialled, for all metrics. When the non-idiotypic seeded system is compared with the unseeded systems, although its performance is better in all cases, it is not always significantly better. Most of the significant differences arise when comparing seeded and unseeded systems that do not use idiotypic effects. When the unseeded system employs idiotypic effects and the seeded system does not, there is a marked drop in the percentage of significant differences. When the seeded idiotypic system is compared with the seeded non-idiotypic system, the idiotypic system performs better in all cases, and significantly better in most. However, when the unseeded systems are compared in this way, although the idiotypic system consistently performs better, none of the differences are significant. The seeded idiotypic system is the only scheme that displays an overall failure rate of 0%. Failure rates are reasonably low (7% overall) for the non-idiotypic seeded system, but reach unacceptable proportions for the hand-designed controller (24% overall) and the idiotypic unseeded system (49% and 40% overall). The non-idiotypic unseeded system is clearly the worst option with overall fail rates of 68% and 50%. Moreover, the actual number of collisions for failing robots is of the order of thousands for unseeded real-world systems, which renders the method entirely unsuitable.\nA general observation is that both the hand-designed controller and the non-idiotypic seeded system exhibit repeated behaviour patterns, particularly when obstacles are both to the left and to the right of the robot. Under these circumstances the robot often moves away from one obstacle, only to encounter the other, and the sequence continues, sometimes indefinitely. The phenomenon is also observed with the seeded idiotypic system, but to a much lesser extent, and the robot is always able to free itself quite promptly."}, {"heading": "6.6 Representation of the Antigen Space", "text": "The seeded idiotypic robot still sometimes exhibits a repeated behaviour pattern when there are objects both to its left and right. Although this is observed rarely, and the robot is always able to free itself quite quickly, it may be that the antigen coverage is not represented adequately. This raises the question as to whether there are potential benefits to introducing an additional environmental scenario \u201cobstacle left and right\u201d. Its inclusion might also improve the performance of the non-idiotypic and handdesigned systems, reducing or even eliminating any idiotypic advantage. In order to investigate these matters, a single new antigen is created, and the antigens are recoded as shown in Table 11. The new antigen (coded 5) presents itself when IR sensors 5 and 2 (those directly to the left and right of the robot respectively) both exhibit readings between 140 and 2400. However, if Vmax is above 2400, antigen 6, 7, or 8 (i.e. one of the collision antigens) is invoked, as before. There is no \u201ccollision left and right\u201d antigen as a series of simulated and real world trials suggests that it simply does not appear. In addition, the mixed antigens \u201cobject near left and collision right\u201d, and \u201ccollision left and object near right\u201d are not included as they occur very rarely and\nwould produce too many new antigens, probably increasing the execution time of the LTL phase too much. Pre-trials also show that, when the previous antigen is the newly introduced one (i.e. antigen 5) and any object is detected, it is necessary to use only positive RL scores, and to boost them. This is because the new antigen does not occur as often as the others, and so any negative RL scores effectively put the assigned behaviours out of business very quickly. The required changes are accomplished by ignoring the actual Vmax values and the object distances, and by making all scores awarded for changes in orientation positive and tripling them, see Table 12.\nFollowing these adjustments, the GA is re-run with five autonomous populations of ten, as before, in order to obtain the new sets of starting antibodies. The STL experimental procedures used for the eight-antigen structure are repeated for the nineantigen structure, except that only R1 is used for the unseeded systems. When compared with the results from the eight-antigen structure, the new results show no significant difference for the seeded and unseeded idiotypic systems, but there are some significant differences for the non-idiotypic schemes. With nine antigens the seeded non-idiotypic system performs significantly better for all of the metrics in World 3, and the unseeded non-idiotypic network performs significantly better for collisions in World 3, but significantly worse for time and solution quality in the real world. These results translate to the following changes when comparing the nine-antigen systems with each other:\n\u2022 The seeded idiotypic system is still always significantly better than the\nunseeded systems in terms of time and solution quality, but in the simulator the collisions show less significance now.\n\u2022 The seeded idiotypic system still consistently out performs the seeded non-\nidiotypic system, but the only significant differences are in World 4 for time and solution quality.\n\u2022 The unseeded idiotypic system is now mostly significantly better than the\nunseeded non-idiotypic system."}, {"heading": "6.7 Discussion", "text": "The observations detailed in section 6.5 provide very strong statistical evidence in\nsupport of H1, i.e. they defend the notion that seeded schemes outperform unseeded ones. The results also uphold H2, since robot performance appears to be further enhanced by incorporating an idiotypic network into the STL architecture. In the seeded idiotypic system, the evolved antibody set provides immediate knowledge of how to begin the task, and the idiotypic AIS permits it to change and adapt its behaviour as the need arises. Without idiotypic effects, the seeded system has the same initial knowledge, but relies only on RL for adaptation, so it is less flexible. However, when the unseeded systems are compared in this way, no significant difference is apparent. This is because the unseeded systems have no initial knowledge, and must acquire their abilities during the STL phase. This is a very slow process, even when idiotypic selection is used, because the search space is probably much too large given the time frame for completing the task. Moreover, the mechanism by which antibodies are replaced is not well developed; the robot is forced to select a random behaviour when it rejects an antibody, and could hence still be using random antibodies during the latter stages of task completion. Further evidence in favour of coupling LTL seeding with STL idiotypic mechanisms lies in the fact that the seeded idiotypic system is the only scheme that consistently displays a 0% failure rate. This upholds H3, i.e. it suggests that antibody replacement is not necessary when adequate seeding and a sufficiently adaptive strategy are in place. The effect of the extra antigen is to reduce the number of collisions for the unseeded systems, but only in simulation. It also brings about reduced numbers of collisions, and an all-round better performance for the seeded non-idiotypic systems, especially for the simpler simulated world, but the improvement is not consistent throughout all the environments, and the system still exhibits an overall fail rate of 3% compared\nwith 0% again for the seeded idiotypic system. As the collision reduction translates poorly to the real world, and the improvement in non-idiotypic systems is inconsistent, there is not much support for H4, and so use of the extra antigen is not recommended. In addition, its use increases the convergence time of the GA, and based on performance, one would always opt for the seeded idiotypic system, which shows no significant change when the new antigen is introduced."}, {"heading": "7 CONCLUSIONS", "text": "This paper has described merging LTL (an accelerated GA), with STL (an idiotypic AIS), in order to seed the AIS with sets of very diverse behaviours that can work together to solve a mobile-robot target-finding problem. It has described the unique antibody encoding and the GA method used for evolving the initial set of antibodies, and has shown that significantly higher antibody diversity can be obtained when a number of autonomous populations are used, rather than a single one. Furthermore, for five autonomous populations, one can run the GA without significantly increasing convergence time or reducing solution quality, and the diversity ratings do not appear to be affected by the difficulty of the problem. The LTL system has proved itself capable of delivering the starting antibodies within a realistic time frame, i.e. within about ten minutes in a static world, and within about 25 minutes in a dynamic world. The STL phase architecture has also been described and a number of experiments performed that show that seeded systems consistently perform significantly better than unseeded systems in both the real world and different simulated worlds. Strong statistical evidence that the idiotypic selection process contributes towards this improvement has also been demonstrated, and the experiments further imply that\nantibody replacement is not necessary within the STL-phase as long as adequate seeding is in place. In addition, trials have been conducted with an extra antigen, but these have shown no significant benefit, suggesting that eight antigens may already be optimal in terms of balancing LTL convergence-time and STL performance. The fusion of the two learning timescales has hence provided an adaptable and robust system for carrying out navigation activities in structured real-world environments. This shows that, given the right conditions, behaviours derived in GA simulations can transfer extremely well to the real world, even when the nature and layout of the environments are quite different."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was funded by the UK Government\u2019s Engineering and Physical Sciences Research Council (EPSCRC)."}], "references": [{"title": "Artificial life and real robots", "author": ["R.A. Brooks"], "venue": "Toward a Practice of Autonomous Systems, Proc. of the First European Conf. on Artificial Life,", "citeRegEx": "Brooks,? \\Q1992\\E", "shortCiteRegEx": "Brooks", "year": 1992}, {"title": "Robot Error Detection Using an Artificial Immune System", "author": ["R. Canham", "A.H. Jackson", "A. Tyrell"], "venue": "in: Proc. of the NASA/DoD Conf. on Evolvable Hardware,", "citeRegEx": "Canham et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Canham et al\\.", "year": 2003}, {"title": "The immune system, adaptation, and machine learning, Physica, D", "author": ["J.D. Farmer", "N.H. Packard", "A. S", "Perelson"], "venue": null, "citeRegEx": "Farmer et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Farmer et al\\.", "year": 1986}, {"title": "Evolution of homing navigation in a real mobile robot", "author": ["D. Floreano", "F. Mondada"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics- Part B: Cybernetics,", "citeRegEx": "Floreano and Mondada,? \\Q1996\\E", "shortCiteRegEx": "Floreano and Mondada", "year": 1996}, {"title": "A role for immunology in \u2018next generation\u2019 robot controllers", "author": ["E. Hart", "P. Ross", "A. Webb", "A. Lawson"], "venue": "in: Proc. of the 2nd International Conf. on Artificial Immune Systems,", "citeRegEx": "Hart et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hart et al\\.", "year": 2003}, {"title": "Evolving robust gaits with AIBO", "author": ["G. Hornby", "S. Takamura", "J. Yokono", "O. Hanagata", "T. Yamamoto", "M. Fujita"], "venue": "in: Proc. of the IEEE International Conf. on Robotics and Automation (ICRA),", "citeRegEx": "Hornby et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Hornby et al\\.", "year": 2000}, {"title": "Towards a network theory of the immune system", "author": ["N.K. Jerne"], "venue": "Ann. Immunol. (Inst Pasteur),", "citeRegEx": "Jerne,? \\Q1974\\E", "shortCiteRegEx": "Jerne", "year": 1974}, {"title": "Comparison between an offline model-free and an on-line model-based evolution applied to a robotics navigation system using evolvable hardware", "author": ["D. Keymeulen", "M. Iwata", "Y. Kuniyoshi", "T. Higuchi"], "venue": "in: Proc. of the 6th International Conf. on Artificial Life,", "citeRegEx": "Keymeulen et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Keymeulen et al\\.", "year": 1998}, {"title": "AIS based robot navigation in a rescue", "author": ["M. Krautmacher", "W. Dilger"], "venue": "scenario, in: Proc. of the 3rd International Conf. on Artificial Immune Systems,", "citeRegEx": "Krautmacher and Dilger,? \\Q2004\\E", "shortCiteRegEx": "Krautmacher and Dilger", "year": 2004}, {"title": "Reactive immune network based mobile robot navigation", "author": ["G.C. Luh", "Gand W.W. Liu"], "venue": "in: Proc. of the 3rd International Conf. on Artificial Immune Systems,", "citeRegEx": "Luh and Liu,? \\Q2004\\E", "shortCiteRegEx": "Luh and Liu", "year": 2004}, {"title": "Cyberbotics Ltd \u2013 WebotsTM: Professional Mobile Robot Simulation", "author": ["O. Michel"], "venue": "International Journal of Advanced Robotic Systems,", "citeRegEx": "Michel,? \\Q2004\\E", "shortCiteRegEx": "Michel", "year": 2004}, {"title": "Decentralized control system for autonomous navigation based on an evolved artificial immune network", "author": ["R. Michelan", "F.J. Von Zuben"], "venue": "in: Proc. of the 2002 Congress on Evolutionary Computation,", "citeRegEx": "Michelan and Zuben,? \\Q2002\\E", "shortCiteRegEx": "Michelan and Zuben", "year": 2002}, {"title": "Timidity: A useful mechanism for robot control", "author": ["M.J. Neal", "J. Timmis"], "venue": null, "citeRegEx": "Neal and Timmis,? \\Q2003\\E", "shortCiteRegEx": "Neal and Timmis", "year": 2003}, {"title": "The balance between initial training and life-long adaptation in evolving robot controllers", "author": ["J.H. Walker", "S.M. Garrett", "M.S. Wilson"], "venue": "IEEE Transactions on Systems, Man and Cybernetics- Part B: Cybernetics,", "citeRegEx": "Walker et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Walker et al\\.", "year": 2006}, {"title": "Emergent construction of behavior arbitration mechanism based on the immune system", "author": ["Y. Watanabe", "A. Ishiguro", "Y. Shirai", "Y. Uchikawa"], "venue": "in: Proc. of the 1998 IEEE International Conf. on Evolutionary Computation,", "citeRegEx": "Watanabe et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Watanabe et al\\.", "year": 1998}, {"title": "Evolutionary construction of an immune network-based behavior arbitration mechanism for autonomous mobile robots, Electrical Engineering in Japan", "author": ["Y. Watanabe", "T. Kondo", "A. Ishiguro", "Y. Shirai", "Y. Uchikawa"], "venue": null, "citeRegEx": "Watanabe et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Watanabe et al\\.", "year": 1998}, {"title": "Idiotypic Immune Networks in Mobile Robot Control", "author": ["A.M. Whitbrook", "U. Aickelin", "J.M. Garibaldi"], "venue": "IEEE Transactions on Systems, Man and CyberneticsPart B: Cybernetics,", "citeRegEx": "Whitbrook et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Whitbrook et al\\.", "year": 2007}, {"title": "Genetic-Algorithm Seeding of Idiotypic Networks for Mobile-Robot Navigation", "author": ["A.M. Whitbrook", "U. Aickelin", "J.M. Garibaldi"], "venue": "in: Proc. the 5th International Conf. on Informatics in Control, Automation and Robotics,", "citeRegEx": "Whitbrook et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Whitbrook et al\\.", "year": 2008}, {"title": "An Idiotypic Immune Network as a Short-Term Learning Architecture for Mobile Robots", "author": ["A.M. Whitbrook", "U. Aickelin", "J.M. Garibaldi"], "venue": "in: Proc. of the 7th International Conf. on Artificial Immune Systems,", "citeRegEx": "Whitbrook et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Whitbrook et al\\.", "year": 2008}, {"title": "Evolving dynamic gaits on a physical robot", "author": ["V. Zykov", "J. Bongard", "H. Lipson"], "venue": "in: Proc. of The Genetic and Evolutionary Computation Conf. (GECCO),", "citeRegEx": "Zykov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zykov et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 6, "context": "antigen matching is selected for the AIS system, and Farmer\u2019s computational model (Farmer 1986) of Jerne\u2019s idiotypic-network theory (Jerne 1974) is adopted.", "startOffset": 132, "endOffset": 144}, {"referenceID": 1, "context": "(Canham et al. 2003) and software (Neal and Timmis 2003) robotics.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "2003) and software (Neal and Timmis 2003) robotics.", "startOffset": 19, "endOffset": 41}, {"referenceID": 1, "context": "(Canham et al. 2003) and software (Neal and Timmis 2003) robotics. However, the most popular robotics software model has been the idiotypic network, based on Farmer\u2019s model of continuous antibody-concentration change. In this model the concentrations are not only dependent upon past matching to antigens, they also depend on the other antibodies present in the system, i.e. antibodies are continually suppressed and stimulated by each other as well as being stimulated by antigens. In theory this design permits great variability of robot behaviour since the antibody that best matches the invading antigen is not necessarily selected for execution; the complex dynamics of stimulation and suppression ensure that suitable alternative antibodies are tried when the need arises (see Whitbrook et al. 2007). However, past work in this area has mostly focused on how the antibodies in the network should be connected and, for simplicity, has used a single set of preengineered behaviours for the antibodies, which limits the potential of the method. For example, Watanabe et al. (1998a and 1998b) use an idiotypic network to control a garbage-collecting robot. Their antibodies are composed of a precondition, a behaviour, and an idiotope part that defines antibody connection. However, the sets of possible behaviours and preconditions are fixed, and only the idiotope part is evolved. Michelan and Von Zuben (2002) and Vargas et al.", "startOffset": 1, "endOffset": 1415}, {"referenceID": 1, "context": "(Canham et al. 2003) and software (Neal and Timmis 2003) robotics. However, the most popular robotics software model has been the idiotypic network, based on Farmer\u2019s model of continuous antibody-concentration change. In this model the concentrations are not only dependent upon past matching to antigens, they also depend on the other antibodies present in the system, i.e. antibodies are continually suppressed and stimulated by each other as well as being stimulated by antigens. In theory this design permits great variability of robot behaviour since the antibody that best matches the invading antigen is not necessarily selected for execution; the complex dynamics of stimulation and suppression ensure that suitable alternative antibodies are tried when the need arises (see Whitbrook et al. 2007). However, past work in this area has mostly focused on how the antibodies in the network should be connected and, for simplicity, has used a single set of preengineered behaviours for the antibodies, which limits the potential of the method. For example, Watanabe et al. (1998a and 1998b) use an idiotypic network to control a garbage-collecting robot. Their antibodies are composed of a precondition, a behaviour, and an idiotope part that defines antibody connection. However, the sets of possible behaviours and preconditions are fixed, and only the idiotope part is evolved. Michelan and Von Zuben (2002) and Vargas et al. (2003) also use GAs, but again only the idiotypic-network connections are derived.", "startOffset": 1, "endOffset": 1440}, {"referenceID": 1, "context": "(Canham et al. 2003) and software (Neal and Timmis 2003) robotics. However, the most popular robotics software model has been the idiotypic network, based on Farmer\u2019s model of continuous antibody-concentration change. In this model the concentrations are not only dependent upon past matching to antigens, they also depend on the other antibodies present in the system, i.e. antibodies are continually suppressed and stimulated by each other as well as being stimulated by antigens. In theory this design permits great variability of robot behaviour since the antibody that best matches the invading antigen is not necessarily selected for execution; the complex dynamics of stimulation and suppression ensure that suitable alternative antibodies are tried when the need arises (see Whitbrook et al. 2007). However, past work in this area has mostly focused on how the antibodies in the network should be connected and, for simplicity, has used a single set of preengineered behaviours for the antibodies, which limits the potential of the method. For example, Watanabe et al. (1998a and 1998b) use an idiotypic network to control a garbage-collecting robot. Their antibodies are composed of a precondition, a behaviour, and an idiotope part that defines antibody connection. However, the sets of possible behaviours and preconditions are fixed, and only the idiotope part is evolved. Michelan and Von Zuben (2002) and Vargas et al. (2003) also use GAs, but again only the idiotypic-network connections are derived. Krautmacher and Dilger (2004) apply the idiotypic method to robot navigation, but their emphasis is on the use of a", "startOffset": 1, "endOffset": 1546}, {"referenceID": 9, "context": "Luh and Liu (2004) address targetfinding using an idiotypic system, modelling their antibodies as steering directions.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "For example, Floreano and Mondada (1996) adopt this approach and report a", "startOffset": 13, "endOffset": 41}, {"referenceID": 5, "context": "More recent evolutionary experiments with physical robots, for example Marocca and Floreano (2002,) Hornby et al. (2000), and Zykov et al.", "startOffset": 100, "endOffset": 121}, {"referenceID": 5, "context": "More recent evolutionary experiments with physical robots, for example Marocca and Floreano (2002,) Hornby et al. (2000), and Zykov et al. (2004) have produced reliable and robust systems, but have not overcome the problems of potential damage and slow, impractical convergence times.", "startOffset": 100, "endOffset": 146}, {"referenceID": 0, "context": "Simulated robots provide a definite advantage for speed of convergence, but the tradeoff is the huge difference between the simulated and real domains (Brooks 1992).", "startOffset": 151, "endOffset": 164}, {"referenceID": 0, "context": "Simulated robots provide a definite advantage for speed of convergence, but the tradeoff is the huge difference between the simulated and real domains (Brooks 1992). Systems that employ an evolutionary training period (LTL) and some form of lifelong adaptation (STL) have been used to try to address the problem of domain differences, for example, Walker et al. (2006) use a GA in the simulated LTL phase and an evolutionary strategy (ES) on the physical robot.", "startOffset": 152, "endOffset": 369}, {"referenceID": 0, "context": "Simulated robots provide a definite advantage for speed of convergence, but the tradeoff is the huge difference between the simulated and real domains (Brooks 1992). Systems that employ an evolutionary training period (LTL) and some form of lifelong adaptation (STL) have been used to try to address the problem of domain differences, for example, Walker et al. (2006) use a GA in the simulated LTL phase and an evolutionary strategy (ES) on the physical robot. They note improved performance when the LTL phase is implemented, and remark that the ES provides continued adaptation to the environment, but they deal with a limited number of behaviour parameters in the GA, and do not state the duration of the LTL phase. Keymeulen et al. (1998) run their LTL and STL phases simultaneously, as the physical robot maps its environment at the same time as carrying out its goal-seeking task, thus creating the simulated world.", "startOffset": 152, "endOffset": 744}, {"referenceID": 10, "context": "For this reason the Webots simulator (Michel 2004) is selected as it is able to run simulations up to 600 times faster than real time, depending on computer power, graphics card, world design and the number and complexity of the robots used.", "startOffset": 37, "endOffset": 50}, {"referenceID": 16, "context": "Further details on the LTL architecture are provided in Whitbrook et al. (2008a).", "startOffset": 56, "endOffset": 81}, {"referenceID": 16, "context": "Here, idiotypic selection is governed by equations (9)-(12), which are based on those in Whitbrook et al. (2007). Equation (9) concerns the increase in strength-of-match value \u03b5im when stimulation occurs,", "startOffset": 89, "endOffset": 113}, {"referenceID": 16, "context": "Further details on the STL architecture are provided in Whitbrook et al. (2008b).", "startOffset": 56, "endOffset": 81}, {"referenceID": 16, "context": "These values are chosen in order to yield a mean idiotypic difference rate of approximately 20%, as suggested in Whitbrook et al. (2007). Note that an idiotypic difference occurs when the antibodies \u03b1 and \u03b2 are different.", "startOffset": 113, "endOffset": 137}], "year": 2010, "abstractText": "A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to solving mobile-robot navigation problems is presented and tested in both the real and virtual domains. The LTL phase consists of rapid simulations that use a Genetic Algorithm to derive diverse sets of behaviours, encoded as variable sets of attributes, and the STL phase is an idiotypic Artificial Immune System. Results from the LTL phase show that sets of behaviours develop very rapidly, and significantly greater diversity is obtained when multiple autonomous populations are used, rather than a single one. The architecture is assessed under various scenarios, including removal of the LTL phase and switching off the idiotypic mechanism in the STL phase. The comparisons provide substantial evidence that the best option is the inclusion of both the LTL phase and the idiotypic system. In addition, this paper shows that structurally different environments can be used for the two phases without compromising transferability.", "creator": "PrimoPDF http://www.primopdf.com/"}}}