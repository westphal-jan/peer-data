{"id": "1006.5278", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2010", "title": "A Survey Paper on Recommender Systems", "abstract": "Recommender systems apply data mining techniques and prediction algorithms to predict users' interest on information, products and services among the tremendous amount of available items. The vast growth of information on the Internet as well as number of visitors to websites add some key challenges to recommender systems. These are: producing accurate recommendation, handling many recommendations efficiently and coping with the vast growth of number of participants in the system.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 28 Jun 2010 07:20:28 GMT  (1267kb,D)", "https://arxiv.org/abs/1006.5278v1", "11 pages. 5 figures. project report"], ["v2", "Wed, 30 Jun 2010 07:37:17 GMT  (0kb,I)", "http://arxiv.org/abs/1006.5278v2", "This paper needs completion"], ["v3", "Mon, 13 Sep 2010 04:16:30 GMT  (0kb,I)", "http://arxiv.org/abs/1006.5278v3", "This paper needs completion"], ["v4", "Fri, 24 Dec 2010 07:22:48 GMT  (1267kb,D)", "http://arxiv.org/abs/1006.5278v4", "This paper has some typos in it"]], "COMMENTS": "11 pages. 5 figures. project report", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["dhoha almazro", "ghadeer shahatah", "lamia albdulkarim", "mona kherees", "romy martinez", "william nzoukou"], "accepted": false, "id": "1006.5278"}, "pdf": {"name": "1006.5278.pdf", "metadata": {"source": "CRF", "title": "A Survey Paper on Recommender Systems", "authors": ["Dhoha Almazro", "Ghadeer Shahatah", "Lamia Albdulkarim", "Mona Kherees", "Romy Martinez", "William Nzoukou"], "emails": ["d_almaz@encs.concordia.ca", "g_shaha@encs.concordia.ca", "l_alabd@encs.concordia.ca", "m_khere@encs.concordia.ca", "romy.martinez@polymtl.ca", "w_nzouko@encs.concordia.ca"], "sections": [{"heading": null, "text": "To address these issues we have explored several collaborative filtering techniques such as the item based approach, which identify relationship between items and indirectly compute recommendations for users based on these relationships. The user based approach was also studied, it identifies relationships between users of similar tastes and computes recommendations based on these relationships.\nIn this paper, we introduce the topic of recommender system. It provides ways to evaluate efficiency, scalability and accuracy of recommender system. The paper also analyzes different algorithms of user based and item based techniques for recommendation generation. Moreover, a simple experiment was conducted using a data mining application -Wekato apply data mining algorithms to recommender system. We conclude by proposing our approach that might enhance the quality of recommender systems.\nCategories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval: Clustering\nKeywords Recommender Systems, Collaborative filtering, User Based, Item Based\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 2010 Team #6 : INSE6180 ."}, {"heading": "1. INTRODUCTION", "text": "Nowadays the amount of information we are retrieving have become increasingly enormous. Back in 1982, John Naisbitt observed that: \u201cwe are drowning in information but starved for knowledge.\u201d [13]. This \u201cstarvation\u201d caused by having many ways people pour data into the Internet but not many techniques to process the data to knowledge. For example, digital libraries contain tens of thousands of journals and articles. However, it is difficult for users to pick the valuable resources they want. What we really need is new technologies that can assist us find resources of interest among the overwhelming items available.\nOne of the most successful such technologies is the Recommender system; as defined by M. Deshpande and G. Karypis: \u201ca personalized information filtering technology used to either predict whether a particular user will like a particular item (prediction problem) or to identify a set of N items that will be of interest to a certain user (top-N recommendation problem)\u201d [1].\nOver the years, various approaches for building recommender systems have been created [3]; collaborative filtering has been a very successful approach in both research and practice, and in information filtering and e-commerce applications [22]. Collaborative filtering works by creating a matrix of all items and users\u2019 preferences. In order to recommend items for the target user, similarities between him and other users are computed based on their common taste. This approach is called user-based approach. A different way to recommend items is by computing the similarities between items in the matrix. This approach is called item based approach."}, {"heading": "1.1 Background", "text": "Recommender system can be built with many approaches. Below are some of them:\n\u2022 Random prediction algorithm is an algorithm that randomly chooses items from the set of available items and recommends them to the user. Since the item\u2019s selection is done randomly, the accuracy of the algorithm is based on luck; the greater the number of items is, the chance of good selection lowers. Random prediction has a great probability of failure. Thus, it has never been taken seriously by any researcher or vendor and only serves as reference point1, helping to compare the\n1A similar algorithm to the Random-based algorithm is the\nar X\niv :1\n00 6.\n52 78\nv4 [\ncs .I\nR ]\n2 4\nD ec\n2 01\n0\nquality of the results obtained by the utilization of a more sophisticated algorithm [16].\n\u2022 Frequent sequences can help build recommender systems. For example, if a customer frequently rates items we can use the frequent pattern to recommend other items to him. The only problem is that this method will only be efficient after the customer makes minimum purchases.\n\u2022 Collaborative filtering algorithms (CF ) are algorithms that require the recommendation seekers to express their preferences by rating items. In this algorithm, the roles of recommendation seeker (a user) and preference provider2 are merged; the more users rate items (or categories), the more accurate the recommendation becomes. In most CF approaches, there is a list of users U = u1, u2, . . . , um and a list of items I = i1, i2, . . . , in. Each user ui has a list of item Iui on which he has expressed his opinion [19].\n\u2022 Content based algorithms are algorithms that attempt to recommend items that are similar to items the user liked in the past. They treat the recommendation\u2019s problem as a search for related items. Information about each item is stored and used for the recommendations. Items selected for recommendation are items that content correlates the most with the user\u2019s preferences [22]. For example, whenever a user rated an items, the algorithm constructs a search query to find other popular items by the same author, artist, or director, or with similar keywords or subjects [14]. Content based algorithms analyze item descriptions to identify items that are of particular interest to the user [18].\nMany others approaches for recommender system exist. However, collaborative filtering algorithms have come to be the best of recommendation algorithms. As stated by Papagelis, collaborative filtering algorithms have \u201c been extensively adopted by both research and e-commerce recommendation systems in order to provide an intelligent mechanism to filter out the excess of information available and to provide customers with the prospect to effortlessly find out items that they will probably like according to their logged history of prior transactions\u201d [16]. CF algorithms have significant advantages over traditional content-based filtering; they can filter any type of content, e.g. text, artwork, music, mutual funds. They can also filter based on complex and hard to represent concepts such as taste and quality. They can to make serendipitous3 recommendations. CF algorithms do not depend on error-prone machine analysis of content [8]."}, {"heading": "1.2 Contribution", "text": "This paper has three primary research contributions:\n1. Analysis of the user-based and item-based prediction algorithms.\nso-called Rating-based Algorithm where instead of recommending items randomly, the system will recommend only the most popular items 2It is users who provide ratings for items 3serendipity: the lucky tendency to find interesting or valuable things by chance (Cambridge Advanced Learner\u2019s Dictionary, 2010)\n2. Formulation of a hybrid model that uses both item based and user algorithm for more accurate prediction.\n3. An experiment to show how from data mining we can deduce rules and make predictions."}, {"heading": "1.3 Organization", "text": "Our work will be primarily based on CF algorithms. First, we introduce and describe collaborative filtering. Afterward, we talk two of the most used collaborative filtering algorithms; user-based and items-based algorithms. We continue by giving an example of an algorithm that is used in a commercial recommender system: the Amazon.com item-toitem CF algorithm. Following that, we discuss about some of the privacy and security issues related to recommender systems and also describe the metrics used to evaluate recommender. Subsequently, we continue by describing our own hybrid method. Finally we present the results of the experiments we did."}, {"heading": "2. COLLABORATIVE FILTERING ALGORITHMS", "text": "The term\u201ccollaborative filtering\u201dwas first coined by Goldberg to describe an email filtering system called Tapestry4. Tapestry was an electronic messaging system that allowed users to rate messages (\u201cgood\u201d or \u201cbad\u201d) or associate text annotations with those messages. Annotations and ratings could then be shared between users. Afterward a user could write some queries (on the annotations and on the ratings) to filter his messages. Although Tapestry provided good recommendations, it had one major drawback; the user was required to write complicated queries [3]. The first system to generate automated recommendations was the GroupLens5 system (Resnick et al. 1994; Konstan et al. 1997). The GroupLens system provided users with personalized recommendation on Usenet6 postings. It recommended articles found interesting by users similar to the target user.\nMost of the CF algorithms are based on the concept of similarity. Some algorithms (like the GroupLens system) compute the similarity between users, others look at the similarity between items, others at the similarity between categories of items. Before we can understand how CF algorithms work, we need to understand this similarity."}, {"heading": "2.1 Similarity", "text": "Similarity (closeness) is define by data analysis in a term of a distance7 function such as the Euclidean (Equation 1) and the Manhattan (Equation 2).\nd(i, j) = \u221a\n(xi1 \u2212 xj1)2 + . . .+ (xin \u2212 xjn)2 (1) d(i, j) = |xi1 \u2212 xj1|+ . . .+ |xin \u2212 xjn| (2)\nIn those distance functions, the difference between corresponding values of attributes in tuples i and j are taken.\n4Tapestry was the first recommendation support system to be made. It was build at Xerox R\u00a9Parc which also famous for inventing graphical operating system[21] 5GroupLens Research is a research lab at the University of Minnesota that specialize in recommender systems.(Wikipedia , 2010) 6Usenet is a worldwide distributed Internet discussion system where users read and post public messages to one or more categories, known as newsgroups. (Wikipedia 2010) 7the more distant objects are, the less similar they become\nTypically attributes are normalized so that attributes with larges values do not outweigh attributes with smaller values.\nThe Euclidean and the Manhattan distance trivially work well and can help us compute the similarity for tuples that have attributes with numerical values. However if the attribute is categorical such as color, we need more sophisticated methods to differentiate the grading (for example color blue vs black) [7]. Some of those methods are cosinebased similarity, Conditional Probability-Based Similarity and Pearson correlation Similarity.\n2.1.1 Cosine-Based Similarity In this approach, items are thought of as vectors in the m\ndimensional user-space where the dimension is the attribute by which the item are rated. The cosine of the angle between the vectors that represent two items is their similarity (see Figure 1). We know from calculus the dot\u2013product formula:\n~i \u00b7~j = ||~i|| \u00b7 ||~j|| \u00b7 cos \u0398\n=\u21d2 sim(i, j) = cos \u0398 = ~i \u00b7~j ||~i|| \u00b7 ||~j||\n2.1.2 Conditional Probability-Based Similarity Another way to compute the similarity is to use a mea-\nsure that is based on the conditional probability of liking (or rating) an item given that the user already showed his interest for another item. If an item i has a good chance of being purchased after an item j was purchased then i and j are similar. The similarity is given by sim(i, j) = P (i|j)\u00d7\u03b1 where \u03b1 is a factor dependent on the problem [3].\n2.1.3 Pearson correlation Similarity The similarity is given by the amount of correlation be-\ntween the items or users. The correlation is computed with the Pearson formula (equation (3)). If the set of users who both rated i and j are denoted by U then the correlation similarity is given by:\nsim(i, j) = corrij (3)\n= \u2211 u\u2208U (Ru,i \u2212Ri)(Ru,j \u2212Rj)\u221a\u2211\nu\u2208U (Ru,i \u2212Ri)2 \u221a\u2211 u\u2208U (Ru,j \u2212Rj)2\nExperimentations have shown that Pearson correlation function performs better than cosine vector similarity (Breese et al. 1998), Spearman correlation, entropy-based uncertainty (Herlock et al. 1999). Pearson correlation is the most\nused similarity function in the two approaches of CF based recommender; user-based or memory-based and item-based or model based [3]."}, {"heading": "2.2 User-Based algorithms", "text": "User based algorithms are CF algorithms that work on the assumption that each user belongs to a group of similar behaving users. The basis for the recommendation is composed by items that are liked by users. Items are recommended based on users tastes (in term of their preference on items). The algorithm considers that users who are similar (have similar attributes) will be interested on same items [4]. User based algorithms are three steps algorithm; the first step is to profile every user in order to find which ones are similar to the target user, the second step is to compute the union of the items selected by these users and associate a weight with each item based on its importance in the set and the third and final step is to select and recommend items that have the highest weight and have not been already selected by the active user [3]. The most important step is the first one; creating the union of items liked by others or selecting the most important of them is easily done when the set of similar users is known [6]. Thus the overall performance of the algorithm will depend on the method used to find users that are similar to the target user. There are many methods by which it can be done. the k-Nearest Neighbors algorithm is the most used because of its efficiency [3]. k-Nearest Neighbors algorithm is a lazy learner classification algorithm. The algorithm requires to be provided with a training data set; a set of users who are well categorized. Then for a given user, it will compare that user\u2019s attribute with all the user in the training data set to find which ones are similar to him. The similarity between users can then be calculated using Pearson correlation (see the above sections to see why Pearson correlation is better than cosine-based and other similarity functions). Two approaches can be used to compute the similarity between users; explicitly and implicitly.\n2.2.1 Prediction based on explicit ratings In this case, users are required to express their ratings on\nitems. This process sometimes happens through a form or a control panel. Let I \u2032 = ix : x = 1, 2, . . . , n\n\u2032 \u2227 n\u2032 \u2264 n where n is the total number of items in the database the set of items that users ux and uy have both rated. The similarity between ux and uy is given by [16]:\n\u03bax,y = sim(ux, uy)\n= \u2211n\u2032 h=1(rux,ih \u2212 rux)(ruy,ih \u2212 ruy )\u221a\u2211n\u2032\nh=1(rux,ih \u2212 rux)2 \u221a\u2211m\u2032 h=1(ruy,ih \u2212 ruy )2\n2.2.2 Prediction based on implicit ratings Implicit rating does not mean that a user will not show\nhis appreciation toward an item, it simply means that he does not do it directly or explicitly as with the preceding approach. The rating of each item is captured implicitly. For example, if a user spend more time looking on an item, the item get an high rating. Another example is that an item will also get a high rating if an user repeatedly come look it.\nM. Papagelisa and D. Plexousakis define a Pearson Correlation function for a recommender where the item\nrating is captured by looking at the explicit rating the users gave to the categories.\nLet C\u2032 = cx : x = 1, 2, . . . , n \u2032 \u2227 n\u2032 \u2264 n where n is the total number of categories in the database that users ux and uy have both rated. The similarity between ux and uy is given by [16]:\n\u03bbx,y = sim(ux, uy)\n= \u2211n\u2032 h=1(rux,ch \u2212 rux)(ruy,ch \u2212 ruy )\u221a\u2211n\u2032\nh=1(rux,ch \u2212 rux)2 \u221a\u2211m\u2032 h=1(ruy,ch \u2212 ruy )2\nwhere cx;x = 1, 2, . . . , p are the available categories. After users have been clustered, the algorithms pursue by finding popular items between those users and recommend them[5].\nAlthough user based algorithms are very efficient and give good results, they suffer some drawbacks;\n\u2022 sparsity: The number of users and items in major e\u2013 commerce website is very large. Most of the users however only rated a small portion of the total items available; even very popular items result in having been rated by only a few of the total number of users. This means that the user-item matrix is very sparse and has a lot of 0 element. Because of that it is possible that the similarity between two users cannot be defined thus making the algorithm useless [17]. Even when the evaluation of similarity is possible, it may not be very reliable, because of insufficient information processed.\n\u2022 scalability: Because finding the optimal clusters of users over large data sets is impractical. Most user-based recommenders use various forms of greedy cluster generation algorithms such as Lazy learner k-nearest neighbors. These cluster generation algorithms require a lot of computations8 that grow linearly with the numbers of users and cannot be precomputed because users and items are changing over time in the database. Moreover, since the user based algorithms must compute\n8For example in the k-nearest neighbors algorithm, finding the optimal k requires a large amount of computations\nthe k-nearest neighbors for every users browsing the system, the latency (waiting time) for each recommendation will increase and may it affect real-time performance of the system [3]. The conclusion is that userbased algorithm do not scale well and are not suitable for large databases of users and items."}, {"heading": "2.3 Item-Based algorithms", "text": "Because of the problems mentioned above with the userbased recommender systems, item-based (or model-based) recommender were developed. Item-based recommender are a type of collaboration filtering (CF) algorithms that look at the similarity between items to make a prediction. The idea is that a user is most likely to purchase items that are similar to the one he already bought in the past; so by analyzing the purchasing information we can have an idea about what he may want in the future (Deshpande, Karypis 2004). Analyzing the historical information can be done explicitly (by looking at the explicit ratings users made on the items) or implicitly (for example through the user browsing information or the rating on categories of item).\nItem-based algorithms are two steps algorithms; in the first step, the algorithms scan the past informations of the users; the ratings they gave to items are collected during this step. From these ratings, similarities between items are built and inserted into an item-to-item matrix M . The element xi,j of the matrix M represents the similarity between the item in row i and the item in column j. Afterward, in the final step, the algorithms selects items that are most similar to the particular item a user is rating. Deshpande and Karypis give a method to construct M (Algorithm 1) after computing the similarities between the items. For each\nAlgorithm 1\nfor j \u2212\u2192 1 to m do for i \u2212\u2192 1 to m do\nif i 6= j then Mi,j \u2212\u2192 sim(R\u2217,j , R\u2217,i) else Mi,j \u2212\u2192 0\nend if end for for i \u2212\u2192 1 to m do\nif i 6= among the k largest values in M,j then Mi,j \u2212\u2192 0\nend if end for\nend for\nitem j, the algorithm computes the similarity between j and the other items and stores the results in the jth column of M (line 1). After that it zero-all the entries in M that less similarity than the kth largest similarity. The second inner for-loop makes sure that an item does not recommend itself.\nSimilarity in item based collaborative filtering can also be computed following two approaches: implicit or explicit.\n2.3.1 Prediction based on explicit ratings As stated before, this approach requires users to specifi-\ncally rate (give their opinion) on items. Let U \u2032 = ux : x = 1, 2, . . . ,m\n\u2032 \u2227m\u2032 \u2264 m where m is the total number of users in database, the set of users that have\nboth rated item i and item j, the Pearson correlation coefficient of their associated columns in the user-item matrix and is given by the following formula [16].\nsim(i, j) = \u2211m\u2032 h=1(Ruh,i \u2212Ri)(Ruh,j \u2212Rj)\u221a\u2211m\u2032\nh=1(Ruh,i \u2212Ri)2 \u221a\u2211m\u2032 h=1(Ruh,j \u2212Rj)2\nRuh,i is the explicit rating given by an user uh to an item i. And Ri is the average of the ratings given on item i.\n2.3.2 Prediction based on implicit ratings As with the implicit user based algorithm (see section\n2.2.2), the ratings given to items can be implicitly captured. M. Papagelisa and D. Plexousakis computes the similarity between two items as the Pearson correlation coefficient of their associated rows in the item-category bitmap matrix9.\nsim(i, j) = \u2211p h=1(vch,i \u2212 vi)(vch,j \u2212 vj)\u221a\u2211p\nh=1(vch,i \u2212 vi)2 \u221a\u2211p h=1(vch,j \u2212 vj)2\np is the number of categories and vch,i is a Boolean value that equals to 1 if the item i belongs to the category h or equals to 0 otherwise.\nCompared to the user-based algorithms, item-based algorithms sparse better and scale well. Their major disadvantage is the cost to build the item-to-item matrix M . If we recall section 2.3, then we see that in other to construct M , we need to compute the similarity between every pair of items. Once this is done, item-based algorithms perform more rapidly and scale better than the user-based algorithms. Despite their slowness, experiments have shown that user-based algorithm produce more accurate recommendation than item-based algorithms [3].\nThe choice of the algorithm will then be based on how much trade-off can be made between the prediction performance and the scalability."}, {"heading": "2.4 The Amazon.com example", "text": "Amazon.com is a e-commerce website in which users can buy books, music and others goods. It has a databases containing more than 29 million customers and several million catalog items.\nAmazon.com use a algorithm based on item-based collaborative filtering to make their recommendations. Their algorithm, called item-to-item collaborative filtering, works by first matching each of the user\u2019s purchased and rated items to similar items (as with the item based CF, this is use to create an item-to-item matrix where elements are the similarities between items). Afterward, it combines those similar items into a recommendation list [14]. The most similar items are found using algorithm 2.4 (G. Linden, B.Smith, and J. York, 2003).\nTo improve the scalability and the performance, Amazon.com has built its recommender as two components. An offline component that creates the expensive and costly itemto-item matrix offline. The other component is the online component that look at the item-to-item matrix to produce the recommendations. The online component is dependent only on how many titles the user has purchased or rated [14].\n9\u201citem-category bitmap matrix is a matrix of items against categories that have as elements the value 1 if the item belongs to the specific category and the value 0 otherwise.\u201d (M. Papagelisa, D. Plexousakis, 2005)\nAlgorithm 2 The most similar items algorithm. Amazon.com computes the similarity using cosine measure\nfor each item in product catalog, I1 do for each customer C who purchased I1 do\nfor each item I2 purchased by customer C do Record that a customer purchased I1 and I2\nend for end for for each item I2 do\nCompute the similarity between I1 and I2 end for\nend for"}, {"heading": "3. EVALUATION", "text": "User satisfaction is the most important factor of the success of a recommender system which is an accurate recommendation within a reasonable time. In commercial systems, it is measured by number of recommended items that has been bought (and of course not returned!)[9]. For noncommercial systems, it is measured by asking for users\u2019 feedback. To properly employ a recommender system, it is important to study the domain for which it is being used [9].\nThis section will focus on evaluating recommender systems for different systems. We will then introduce three important metrics for evaluating the quality of recommender systems. Finally, we will address the challenges of employing recommender systems."}, {"heading": "3.1 Different Systems, Different Algorithms", "text": "Recommender systems differ based on the type of application used. Therefore, a certain algorithm may work very well on a dataset and work poorly on different data set. In other words, some algorithms work well in situations where items are more than users (e.g. a recommender system that suggest tens of thousands of research papers to thousands of users). Other algorithms are designed for the opposite situation where users are more than items (e.g. MovieLens, a system for recommending movies, has a data set of 65000 users and 5000 movies) [9]. Furthermore, Recommender Systems varies according to the nature of data sets. The static nature of items allows us to pre compute and store some of the values of the algorithm. However, the same technique is not efficient for items with a dynamic nature [19]. In some cases where similarities are way more than the dissimilarity, it is efficient to compute the dissimilarity and extract the similarity afterwards [6]."}, {"heading": "3.2 Recommendation Metrics", "text": "Items and users are getting increased in systems where recommender systems utilization is crucial. To ensure user satisfaction all the time, algorithms must not work on thousands, but millions of item within reasonable time [23]. Therefore, recommender systems must cope with the growth by making the suggestions more accurate, efficient and scalable.\nAccuracy This is measured by how close the result of a recommendation matches a user\u2019s preference. Accuracy is the most important metric in evaluating the quality if a recommender system because this is what all is about: understanding the user and suggesting what he really likes\nor what he is looking for precisely to gain the user\u2019s trust [23]. There are two measures for evaluating the accuracy of a recommender system:\n\u2022 Statistical Accuracy Metric: This compares the numerical recommendation scores against the actual user rating. One of the widely used metrics is the Mean Absolute Error (MAE); the lower the value of MAE the more accurate the result is [19].\n\u2022 Decision Support Accuracy: which measure how effective the prediction engine is at helping a user selecting high- quality item from the set of all items. Receiver Operating Characteristic (ROC) is one of the metrics that help assessing the accuracy of predictions [19].\nAn interesting point regarding accuracy was pointed out by many researchers. Very accurate recommender systems are not always good! An example is an online travel agency that recommends destinations that has already been visited by a user. Yes the recommendation was accurate enough but it was not useful she already visited these places [15]. This means that recommendation must be accurate in predicting the upcoming actions of a user not only knowing him. Moreover, the recommendations that are most accurate according to the standards metrics are sometimes not the recommendations that are useful to users [15].\nEfficiency In order for a recommender system to be reliable, not only it must be accurate, but also it must process within a reasonable time, make good use of the available resources, and handle hundred requests per second [23]. Memory and Computation time are two important metrics that evaluate the efficiency of a recommender system. Algorithms that work with item sets that has a static nature tend to pre-compute item similarity and stores a matrix of similarities. The more the items, the bigger the matrix will grow. Therefore, we will end up with a quick look up table that speeds up the recommendation process; however, an O(n2) space is needed for n items [19]. Because of the space problem we may not consider all the n items of a system. Instead, we only consider a small fraction of the most similar items k where (k < n). This attempt will reduce the size of the lookup table but we will have a trade-off: smaller model size means a reduced quality [19]. Another approach to efficiently allocate space needed is to give each item a space according to the amount of rating. In other words, the more an item has ratings, the more space I allocate [10].\nIn some situation, the knowledge of customer preferences changes, memory consumption reduces and the time used for computation increases, therefore the efficiency of the recommender system in dynamic datasets depends on the amount of calculation required in an algorithm [23]. In this situation, two calculations must be performed: learning time and running time. In some cases, running time was fast but learning time was 8 hours. To speed up the calculation, we consider a relevant dataset rather than the whole database; again, a trade-off between the accuracy and efficiency. Another approach to speed up the calculation is to use data\nstructure or other data mining techniques such as hierarchical clustering since searching for neighbors is faster than scanning the whole tree [23].\nScalability A good recommendation algorithm that handles thousands of request, must also handle hundred of thousand requests in the future. Despite the accuracy and efficiency of many algorithms, they are not coping with the growth of data sets. Therefore, in order to manage the vast increase in number of users and items, a trade-off between the prediction performance and scalability is inevitable. Again, this is done by considering a portion of the whole dataset with similar characteristics.\nOne of the best approaches for maintaining accuracy, efficiency, and scalability is to use hashing techniques. It compresses large data sets, scale very large number of users, and obtain a good performance within a reasonable time [10]."}, {"heading": "3.3 Challenges of Recommender Systems", "text": "If recommender systems rely only on items that have been rated, then it is missing a lot of good items for recommendation that are hidden because no one has rated them. This is called the Coverage metrics which is the percentage of items for which a recommender agent can provide predictions [16]. This is one of the problems that face systems that employ recommender systems. Another challenge is the sparsity issue which is rating few of the total number of items [16]. For systems that has just established, they are facing the cold start problem where the recommender system is unable to accurately recommend items due to the fact that only few rating has been performed on items. Noise, data redundancy, and overfitting are also other challenges of recommendation agent [23].\nIn order to reduce the sparsity problem, some researchers have proposed a compensation system by which users are rewarded for providing ratings to items. Others have proposed to capture the ratings by implicitly look at the user\u2019s behavior [20]. Another approach to solve the sparsity problem is to rely filtering agent called filterbots or dynamic agents to automatically rate items [20]."}, {"heading": "4. SECURITY AND PRIVACY ISSUES", "text": "Collaborative filtering CF recommender requires personal information from a user to give personalized recommendations. The more users express their preferences on items, the more accurate the recommendation they receive become. As with any data mining systems, users must trust the recommender to protect their information appropriately. Moreover, since the user does not know how the recommendation is performed, he should trust the accuracy of the recommender[11]. The recommender should not violate the trust of the users."}, {"heading": "4.1 Privacy Risks", "text": "In most systems, users need to register before they can enjoy personalized recommendation. The registration process often requires them to provide some personal informations like their names, birth dates, postal code and email. Combinations of those required fields (attributes) may be highly identifying (Quasi-identifier10). Personal preferences\n10Quasi-identifier: \u201cVariable values or combinations of vari-\nlike those expressed to many recommender systems may become quasi-identifier, especially if some users express unusual preferences (S. Lam, D. Frankowski, and J. Riedl, 2006). User\u2019s preferences could then be used to re-identify him in another system. For example, a company like Netflix could use the preferences some users saved in its system to find them on a competitor website.\nSince not every users want their information to be disclosed or misused, the recommender should then protect itself against exposition of users informations or misuse of those informations. Recommender systems are also confronted with other type of problems such as security\u2019s related problem. Since being recommended is often promise of good selling, recommender are often target of manipulation from producers or malicious users [2]. For example, a book writer may try to alter the recommendation so that his book get recommended. Recent research by Dellarocas and others have shown that even popular systems such as Amazon and eBay have (and are) being manipulated [12]. Shilling attacks are one of the most discussed method by which the prediction of a recommender can be bias."}, {"heading": "4.2 Shilling Attacks", "text": "A shilling attack is an attack in which the system\u2019s recommendations for a particular item is manipulated by submitting misrepresented opinions to the system (S. Lam, D. Frankowski, and J. Riedl, 2006). The attack can have two objectives: decrease the ratings of all the items outside its target item-set (push attack) to make them more recommended. He may also increase the ratings (nuke attack) of other items to make its target item-set less recommended. Two simple types of shilling attacks are RandomBot and AverageBot.\n\u2022 A RandomBot is filterbot who randomly rate items outside of the target item-set with either the minimum rating (for nuke attack) or maximum rating (for push attack).\n\u2022 An AverageBot is a filterbot where the rating is based on the average rating of each item following a normal distribution with a mean equal to the average rating for that item.\nAnother type of attack that may affect recommender are the so called Sybil attack in which a dishonnest user may create multiples users account in other to improve the recommendation of another user or another item.\nRecommender shall then provides ways to protect itself against those attacks since they are well known. Some systems provided CAPTCHA11 to stop filterbots from corrupting the ratings."}, {"heading": "5. OUR APPROACH", "text": "After studying collaborative filtering with its both approaches item-based and user-based, we found that each approach has its advantages and disadvantages. Thus, we are\nable values within a dataset that are not structural uniques but might be empirically unique and therefore in principle uniquely identify a population unit.\u201d(OECD, Glossary of statistical term, 2010) 11\u201cA CAPTCHA or Captcha is a type of challenge-response test used in computing to ensure that the response is not generated by a computer\u201d (Wikipedia, 2010)\nproposing a new hybrid technique that combines the two approaches and trying to come up with a new approach that is more accurate and efficient.\nThe proposed approach starts by clustering all items and users based on demographic information. In other word, items and users will be categorized based on users personal attributes and make recommendations based on demographic categorization. Clustering techniques work by identifying groups of users and groups of items which appear to have similar preferences.\nAfter applying the clustering technique, the next step is to extract the suitable clusters for both item based algorithm and user based algorithm. The item based algorithm will measure the similarities between a target user\u2019s preference and the items we have in the cluster. The user based algorithm will measure the similarities between the target user and other users in its cluster.\nThe results of both algorithms are listed it terms of items. These items will be ranked from the most appropriate to the least appropriate for the target user. Then, the items in both item sets will be merged in one item set also depending on the rank that each item got in the step before. Finally, the recommendation of top-k items will be generated to the user. Figure 3 illustrates the new hybrid approach.\nAs mentioned earlier, recommender systems must cope with the growth of items and users by making suggestions more accurate, efficient and scalable. Hopefully our approach is able to handle the massive growth in a way that ensures user satisfaction.\nIn terms of accuracy, by employing item based and user based together and ranking both results, we are extracting\nthe best of both methods and suggesting the most accurate items to users.\nIn terms of efficiency, the proposed approach will not going to deal with the whole database and will only deal with a portion of it due to the clustering technique that will be implemented before applying the proposed technique. Therefore, the amount of computation and memory will be much less, and it will speed up the calculation of the recommendation.\nIn terms of scalability, the proposed approach will not have a problem with scalability since item based algorithms is still going to be implemented and is able to handle the scalability issue. Moreover, applying a hashing technique will make the proposed system able to absorb the growth of users and items."}, {"heading": "6. EXPERIMENTS", "text": "We were able to implement a recommender system based on a user\u2019s profile as well as on an item based profile. To do so, we used the Java open-source program named Weka. Weka provides environment for comparing learning algorithms, graphical user interface, comprehensive set of data pre-processing tools, learning algorithms and evaluation methods. Furthermore Weka provides implementation of Regression, Clustering, Classification, Association rules and feature selection12. As part of our experiment we used the classification algorithm J48 which is an open source Java implementation of the C4.513 algorithm in Weka (Wikipedia,2010). Weka also provides many methods for loading data such as (ARFF) or (CSV) file, in our experiment we use a file in CSV format."}, {"heading": "6.1 Dataset", "text": "In the first part of our experiment, we inputted a\u201c.csv\u201dfile containing the following parameters: UserID, Age, Gender, Student, Have children, Movie category. The table below (Table 1 ) provides further details of each parameter.\nIn the second part , the \u201c.csv\u201d file contained the following parameters: UserID, Movie title, Movie categories: Action, Adventure, Animation, Children\u2019s, Comedy, Crime, Documentary, Drama, Fantasy, Film-Noir, Horror, Musical, Mystery, Romance, Sci-Fi, Thriller, War. Table 2 provides further details of each parameter."}, {"heading": "6.2 Data cleaning and preparation", "text": "Both datasets were populated using the spawner14 program. After establishing which parameters we wanted to generate, we inputed them in spawner and we got some sample datasets. Each datasets contained 300 data. Each datasets were modified manually in order to make sure that the dataset were coherent and logical. The movies titles in the item datasets were identical to titles provided by the MovieLens website. It is important to note that the same movie title can be viewed and rated by different users.\n12IBM R\u00a9developerworks website 13C4.5 is an algorithm used to generate a decision tree that was developed by Ross Quinlan from his earlier ID3 algorithm. (Wikipedia,2010) 14Spawner is a software that can generate sample based on certain criteria\nOnce we generated the datasets, user and item sets, we made sure that some entries were blanks in order for weka to perform some predictions. For the user data set, weka will predict a movie category whereas for the item data set, it will predict a movie title according to which category the movie belong to. Once this step was performed, the pre-processing steps consisted of three main steps: opening the file with weka, selecting all the other attributes and finally choosing the attribute as a class attribute. These pre-processing steps were important in order to insure that the appropriate data were used in our experiments."}, {"heading": "6.3 Results", "text": "The implementation of the recommender system was done using two diffeerent input files. Indeed, our objective was to recommend a movie title to a user according to his profile and also based on the movie category that movie belong to.\nUsing this result (Figure 4), we are able in the first part of the experiment, to predict what kind of movie a certain user with specific characteristics would like. For instance, an adult, who is also a female student with children would most likely like an animation type movie. On the other hand, a male teenager who is a student without children would rather an action movie. Therefore, we are able to recommend a certain movie category based on a user\u2019s profile. We used a user based algorithm based on demographics to generate our data.The accuracy for this experiment was 61.43% of correctly classified.\nUsing this result (Figure 5), we are able in the second part of the experiment, to recommend a movie title according to what type of movie category that movie belongs to. For instance, if a user normally selects movies that are comedies, animations and a children\u2019s movies, we would recommend \u201cAlladin and the king of thieves\u201d. Furthermore, if a user rated a movie as drama and science fiction but not as a comedy and adventure, we would recommend \u201cTwelve Monkeys\u201d. Again, here we were able to recommend movie titles based on what users normally selects. The accuracy for this experiment was 66.01%.\nWith these experiments we were able to demonstrate recommender systems using weka and trying to reproduce the user based algorithm (based on demographics) and item based algorithms."}, {"heading": "7. CONCLUSION", "text": "This report presented some of the algorithms used to build recommender systems. Each of these algorithm has its advantages and disadvantages; user-based algorithms are accurate but not scalable, item-based algorithms are scalable but not precise as the user-based. Research on recommender system is mainly focused on finding ways to improve the performance, scalability or accuracy of the algorithms. Hybrid algorithms that combine features of user-based and itembased algorithms have been created. Other approaches using Rough Set Prediction, Slope One Scheme Smoothing and other methods have been developed to build item-based and user-based algorithms. As with any systems that contains data on users, recommender systems have some privacy and security issues to deal with.\nIn conclusion, recommender systems are powerful systems that give an added-value to business and corporation. They are a relatively recent technology and they will only keep improving in the future."}, {"heading": "8. ACKNOWLEDGMENTS", "text": "The authors would like to thank Dr. Benjamin Fung for his insightful help during the writing of this paper. This paper was produced using the ACM SAC 2010 LATEX2 template."}, {"heading": "9. REFERENCES", "text": "[1] Bigdeli, E., and Bahmani, Z. Comparing accuracy\nof cosine-based similarity and correlation-based similarity algorithms in tourism recommender systems. In Management of Innovation and Technology, 2008. ICMIT 2008. 4th IEEE International Conference on (21-24 2008), pp. 469 \u2013474.\n[2] Chirita, P.-A., Nejdl, W., and Zamfir, C. Preventing shilling attacks in online recommender systems. In WIDM \u201905: Proceedings of the 7th annual ACM international workshop on Web information and data management (New York, NY, USA, 2005), ACM, pp. 67\u201374.\n[3] Deshpande, M., and Karypis, G. Item-based top-<i>n</i> recommendation algorithms. ACM Trans. Inf. Syst. 22, 1 (2004), 143\u2013177.\n[4] Ghazanfar, M. A., and Prugel-Bennett, A. A scalable, accurate hybrid recommender system. International Workshop on Knowledge Discovery and Data Mining 0 (2010), 94\u201398.\n[5] Godoy, D., and Amandi, A. An agent-based recommender system to support collaborative web search based on shared user interests. In CRIWG (2007), pp. 303\u2013318.\n[6] Group, G. R., Sarwar, B., Karypis, G., Konstan, J., and Riedl, J. Analysis of recommendation algorithms for e-commerce. ACM Press, pp. 158\u2013167.\n[7] Han, J., and Kamber, M. Data Mining: Concepts and Techniques, second ed. Elsevier, 2006.\n[8] Herlocker, J. L., Konstan, J. A., and Riedl, J. Explaining collaborative filtering recommendations. In CSCW \u201900: Proceedings of the 2000 ACM conference on Computer supported cooperative work (New York, NY, USA, 2000), ACM, pp. 241\u2013250.\n[9] Herlocker, J. L., Konstan, J. A., Terveen, L. G., and Riedl, J. T. Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst. 22, 1 (2004), 5\u201353.\n[10] Karatzoglou, A., Smola, A., and Weimer, M. Collaborative filtering on a budget, 2010.\n[11] Lam, S. K., Frankowski, D., and Riedl, J. Do you trust your recommendations? an exploration of security and privacy issues in recommender systems. In ETRICS (2006), pp. 14\u201329.\n[12] Lam, S. K., and Riedl, J. Shilling recommender systems for fun and profit. In WWW \u201904: Proceedings of the 13th international conference on World Wide Web (New York, NY, USA, 2004), ACM, pp. 393\u2013402.\n[13] Larose, D. Discovering knowledge in Data. Wiley Interscience, 2008.\n[14] Linden, G., Smith, B., and York, J. Amazon.com recommendations: Item-to-item collaborative filtering. IEEE Internet Computing 7 (2003), 76\u201380.\n[15] McNee, S. M., Riedl, J., and Konstan, J. A. Being accurate is not enough: how accuracy metrics\nhave hurt recommender systems. In CHI \u201906: CHI \u201906 extended abstracts on Human factors in computing systems (New York, NY, USA, 2006), ACM, pp. 1097\u20131101.\n[16] Papagelis, M., and Plexousakis, D. Qualitative analysis of user-based and item-based prediction algorithms for recommendation agents. Engineering Applications of Artificial Intelligence 18, 7 (2005), 781 \u2013 789.\n[17] Papagelis, M., Plexousakis, D., and Kutsuras, T. Alleviating the sparsity problem of collaborative filtering using trust inferences. 2005, pp. 224\u2013239.\n[18] Pazzani, M., and Billsus, D. Content-based recommendation systems. 2007, pp. 325\u2013341.\n[19] Sarwar, B., Karypis, G., Konstan, J., and Reidl, J. Item-based collaborative filtering recommendation algorithms. In WWW \u201901: Proceedings of the 10th international conference on World Wide Web (New York, NY, USA, 2001), ACM, pp. 285\u2013295.\n[20] Sarwar, B. M. Using semi-intelligent filtering agents to improve prediction quality in collaborative filtering systems, 1998.\n[21] Terveen, L., and Hill, W. Beyond recommender systems: Helping people help each other, 2001.\n[22] van Meteren, R., and van Someren, M. Using content-based filtering for recommendation.\n[23] Yu, K., Xu, X., Tao, J., Ester, M., and Kriegel, H.-P. Instance selection techniques for memory-based collaborative filtering, 2002."}], "references": [{"title": "Comparing accuracy of cosine-based similarity and correlation-based similarity algorithms in tourism recommender systems", "author": ["E. Bigdeli", "Z. Bahmani"], "venue": "In Management of Innovation and Technology,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Preventing shilling attacks in online recommender systems", "author": ["Chirita", "P.-A", "W. Nejdl", "C. Zamfir"], "venue": "Proceedings of the 7th annual ACM international workshop on Web information and data management (New York, NY,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Item-based top-<i>n</i> recommendation algorithms", "author": ["M. Deshpande", "G. Karypis"], "venue": "ACM Trans. Inf. Syst. 22,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "A scalable, accurate hybrid recommender system", "author": ["M.A. Ghazanfar", "A. Prugel-Bennett"], "venue": "International Workshop on Knowledge Discovery and Data Mining", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "An agent-based recommender system to support collaborative web search based on shared user interests", "author": ["D. Godoy", "A. Amandi"], "venue": "CRIWG", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Data Mining: Concepts and Techniques, second ed", "author": ["J. Han", "M. Kamber"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Explaining collaborative filtering recommendations", "author": ["J.L. Herlocker", "J.A. Konstan", "J. Riedl"], "venue": "Proceedings of the 2000 ACM conference on Computer supported cooperative work (New York, NY,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["J.L. Herlocker", "J.A. Konstan", "L.G. Terveen", "J.T. Riedl"], "venue": "ACM Trans. Inf. Syst. 22,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Do you trust your recommendations? an exploration of security and privacy issues in recommender systems", "author": ["S.K. Lam", "D. Frankowski", "J. Riedl"], "venue": "ETRICS", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Shilling recommender systems for fun and profit", "author": ["S.K. Lam", "J. Riedl"], "venue": "Proceedings of the 13th international conference on World Wide Web (New York, NY,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Discovering knowledge in Data", "author": ["D. Larose"], "venue": "Wiley Interscience,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Amazon.com recommendations: Item-to-item collaborative filtering", "author": ["G. Linden", "B. Smith", "J. York"], "venue": "IEEE Internet Computing", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Being accurate is not enough: how accuracy metrics  have hurt recommender systems. In CHI \u201906: CHI \u201906 extended abstracts on Human factors in computing systems", "author": ["S.M. McNee", "J. Riedl", "J.A. Konstan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Qualitative analysis of user-based and item-based prediction algorithms for recommendation agents", "author": ["M. Papagelis", "D. Plexousakis"], "venue": "Engineering Applications of Artificial Intelligence 18,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Alleviating the sparsity problem of collaborative filtering using trust inferences", "author": ["M. Papagelis", "D. Plexousakis", "T. Kutsuras"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Content-based recommendation systems", "author": ["M. Pazzani", "D. Billsus"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Reidl"], "venue": "Proceedings of the 10th international conference on World Wide Web (New York, NY,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Using semi-intelligent filtering agents to improve prediction quality in collaborative filtering", "author": ["B.M. Sarwar"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Beyond recommender systems: Helping people help", "author": ["L. Terveen", "W. Hill"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Instance selection techniques for memory-based collaborative", "author": ["K. Yu", "X. Xu", "J. Tao", "M. Ester", "Kriegel", "H.-P"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}], "referenceMentions": [{"referenceID": 10, "context": "\u201d [13].", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "Karypis: \u201ca personalized information filtering technology used to either predict whether a particular user will like a particular item (prediction problem) or to identify a set of N items that will be of interest to a certain user (top-N recommendation problem)\u201d [1].", "startOffset": 263, "endOffset": 266}, {"referenceID": 2, "context": "Over the years, various approaches for building recommender systems have been created [3]; collaborative filtering has been a very successful approach in both research and practice, and in information filtering and e-commerce applications [22].", "startOffset": 86, "endOffset": 89}, {"referenceID": 13, "context": "quality of the results obtained by the utilization of a more sophisticated algorithm [16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "Each user ui has a list of item Iui on which he has expressed his opinion [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "For example, whenever a user rated an items, the algorithm constructs a search query to find other popular items by the same author, artist, or director, or with similar keywords or subjects [14].", "startOffset": 191, "endOffset": 195}, {"referenceID": 15, "context": "Content based algorithms analyze item descriptions to identify items that are of particular interest to the user [18].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "As stated by Papagelis, collaborative filtering algorithms have \u201c been extensively adopted by both research and e-commerce recommendation systems in order to provide an intelligent mechanism to filter out the excess of information available and to provide customers with the prospect to effortlessly find out items that they will probably like according to their logged history of prior transactions\u201d [16].", "startOffset": 401, "endOffset": 405}, {"referenceID": 6, "context": "CF algorithms do not depend on error-prone machine analysis of content [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "Although Tapestry provided good recommendations, it had one major drawback; the user was required to write complicated queries [3].", "startOffset": 127, "endOffset": 130}, {"referenceID": 18, "context": "It was build at Xerox R \u00a9Parc which also famous for inventing graphical operating system[21] GroupLens Research is a research lab at the University of Minnesota that specialize in recommender systems.", "startOffset": 88, "endOffset": 92}, {"referenceID": 5, "context": "However if the attribute is categorical such as color, we need more sophisticated methods to differentiate the grading (for example color blue vs black) [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "The similarity is given by sim(i, j) = P (i|j)\u00d7\u03b1 where \u03b1 is a factor dependent on the problem [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Pearson correlation is the most used similarity function in the two approaches of CF based recommender; user-based or memory-based and item-based or model based [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 3, "context": "The algorithm considers that users who are similar (have similar attributes) will be interested on same items [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "User based algorithms are three steps algorithm; the first step is to profile every user in order to find which ones are similar to the target user, the second step is to compute the union of the items selected by these users and associate a weight with each item based on its importance in the set and the third and final step is to select and recommend items that have the highest weight and have not been already selected by the active user [3].", "startOffset": 444, "endOffset": 447}, {"referenceID": 2, "context": "the k-Nearest Neighbors algorithm is the most used because of its efficiency [3].", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "The similarity between ux and uy is given by [16]:", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "The similarity between ux and uy is given by [16]:", "startOffset": 45, "endOffset": 49}, {"referenceID": 4, "context": "After users have been clustered, the algorithms pursue by finding popular items between those users and recommend them[5].", "startOffset": 118, "endOffset": 121}, {"referenceID": 14, "context": "Because of that it is possible that the similarity between two users cannot be defined thus making the algorithm useless [17].", "startOffset": 121, "endOffset": 125}, {"referenceID": 2, "context": "For example in the k-nearest neighbors algorithm, finding the optimal k requires a large amount of computations the k-nearest neighbors for every users browsing the system, the latency (waiting time) for each recommendation will increase and may it affect real-time performance of the system [3].", "startOffset": 292, "endOffset": 295}, {"referenceID": 13, "context": "both rated item i and item j, the Pearson correlation coefficient of their associated columns in the user-item matrix and is given by the following formula [16].", "startOffset": 156, "endOffset": 160}, {"referenceID": 2, "context": "Despite their slowness, experiments have shown that user-based algorithm produce more accurate recommendation than item-based algorithms [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 11, "context": "Afterward, it combines those similar items into a recommendation list [14].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "The online component is dependent only on how many titles the user has purchased or rated [14].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "In commercial systems, it is measured by number of recommended items that has been bought (and of course not returned!)[9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "To properly employ a recommender system, it is important to study the domain for which it is being used [9].", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "MovieLens, a system for recommending movies, has a data set of 65000 users and 5000 movies) [9].", "startOffset": 92, "endOffset": 95}, {"referenceID": 16, "context": "However, the same technique is not efficient for items with a dynamic nature [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 19, "context": "To ensure user satisfaction all the time, algorithms must not work on thousands, but millions of item within reasonable time [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 19, "context": "or what he is looking for precisely to gain the user\u2019s trust [23].", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "One of the widely used metrics is the Mean Absolute Error (MAE); the lower the value of MAE the more accurate the result is [19].", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "Receiver Operating Characteristic (ROC) is one of the metrics that help assessing the accuracy of predictions [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 12, "context": "Yes the recommendation was accurate enough but it was not useful she already visited these places [15].", "startOffset": 98, "endOffset": 102}, {"referenceID": 12, "context": "Moreover, the recommendations that are most accurate according to the standards metrics are sometimes not the recommendations that are useful to users [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "Efficiency In order for a recommender system to be reliable, not only it must be accurate, but also it must process within a reasonable time, make good use of the available resources, and handle hundred requests per second [23].", "startOffset": 223, "endOffset": 227}, {"referenceID": 16, "context": "Therefore, we will end up with a quick look up table that speeds up the recommendation process; however, an O(n) space is needed for n items [19].", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "This attempt will reduce the size of the lookup table but we will have a trade-off: smaller model size means a reduced quality [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 19, "context": "In some situation, the knowledge of customer preferences changes, memory consumption reduces and the time used for computation increases, therefore the efficiency of the recommender system in dynamic datasets depends on the amount of calculation required in an algorithm [23].", "startOffset": 271, "endOffset": 275}, {"referenceID": 19, "context": "Another approach to speed up the calculation is to use data structure or other data mining techniques such as hierarchical clustering since searching for neighbors is faster than scanning the whole tree [23].", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "This is called the Coverage metrics which is the percentage of items for which a recommender agent can provide predictions [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "Another challenge is the sparsity issue which is rating few of the total number of items [16].", "startOffset": 89, "endOffset": 93}, {"referenceID": 19, "context": "Noise, data redundancy, and overfitting are also other challenges of recommendation agent [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "Others have proposed to capture the ratings by implicitly look at the user\u2019s behavior [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "Another approach to solve the sparsity problem is to rely filtering agent called filterbots or dynamic agents to automatically rate items [20].", "startOffset": 138, "endOffset": 142}, {"referenceID": 8, "context": "Moreover, since the user does not know how the recommendation is performed, he should trust the accuracy of the recommender[11].", "startOffset": 123, "endOffset": 127}, {"referenceID": 1, "context": "Since being recommended is often promise of good selling, recommender are often target of manipulation from producers or malicious users [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "Recent research by Dellarocas and others have shown that even popular systems such as Amazon and eBay have (and are) being manipulated [12].", "startOffset": 135, "endOffset": 139}], "year": 2016, "abstractText": "Recommender systems apply data mining techniques and prediction algorithms to predict users\u2019 interest on information, products and services among the tremendous amount of available items. The vast growth of information on the Internet as well as number of visitors to websites add some key challenges to recommender systems. These are: producing accurate recommendation, handling many recommendations efficiently and coping with the vast growth of number of participants in the system. Therefore, new recommender system technologies are needed that can quickly produce high quality recommendations even for huge data sets. To address these issues we have explored several collaborative filtering techniques such as the item based approach, which identify relationship between items and indirectly compute recommendations for users based on these relationships. The user based approach was also studied, it identifies relationships between users of similar tastes and computes recommendations based on these relationships. In this paper, we introduce the topic of recommender system. It provides ways to evaluate efficiency, scalability and accuracy of recommender system. The paper also analyzes different algorithms of user based and item based techniques for recommendation generation. Moreover, a simple experiment was conducted using a data mining application -Wekato apply data mining algorithms to recommender system. We conclude by proposing our approach that might enhance the quality of recommender systems.", "creator": "LaTeX with hyperref package"}}}