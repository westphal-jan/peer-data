{"id": "1010.5511", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Oct-2010", "title": "Efficient Minimization of Decomposable Submodular Functions", "abstract": "Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems with more complicated linear functions than the general monomorphic monomorphic monomorphic monomorphic monomorphism. The monomorphic monomorphism can be identified by the following properties:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 26 Oct 2010 20:23:39 GMT  (229kb,DS)", "http://arxiv.org/abs/1010.5511v1", "Expanded version of paper for Neural Information Processing Systems 2010"]], "COMMENTS": "Expanded version of paper for Neural Information Processing Systems 2010", "reviews": [], "SUBJECTS": "cs.LG math.OC", "authors": ["peter stobbe", "andreas krause 0001"], "accepted": true, "id": "1010.5511"}, "pdf": {"name": "1010.5511.pdf", "metadata": {"source": "CRF", "title": "Efficient Minimization of Decomposable Submodular Functions", "authors": ["Peter Stobbe", "Andreas Krause"], "emails": ["stobbe@caltech.edu", "krausea@caltech.edu"], "sections": [{"heading": "1 Introduction", "text": "Convex optimization has become a key tool in many machine learning algorithms. Many seemingly multimodal optimization problems such as nonlinear classification, clustering and dimensionality reduction can be cast as convex programs. When minimizing a convex loss function, we can rest assured to efficiently find an optimal solution, even for large problems. Convex optimization is a structural property of continuous optimization problems. However, many machine learning problems, such as structure learning, variable selection, MAP inference in discrete graphical models, require solving discrete, combinatorial optimization problems.\nIn recent years, another fundamental problem structure, which has similar beneficial properties, has emerged as very useful in many combinatorial optimization problems arising in machine learning: Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Similarly to convexity, submodularity allows one to efficiently find provably (near-)optimal solutions. In particular, the minimum of a submodular function can be found in strongly polynomial time [11]. Unfortunately, while polynomial-time solvable, exact techniques for submodular minimization require a number of function evaluations on the order of n5 [12], where n is the number of variables in the problem (e.g., number of random variables in the MAP inference task), rendering the algorithms impractical for many real-world problems.\nFortunately, several submodular minimization problems arising in machine learning have structure that allows solving them more efficiently. Examples include symmetric functions that can be solved in O(n3) evaluations using Queyranne\u2019s algorithm [19], and functions that decompose into attractive, pairwise potentials, that can be solved using graph cutting techniques [7]. In this paper, we introduce a novel class of submodular minimization problems that can be solved efficiently. In particular, we develop an algorithm SLG, that can minimize a class of submodular functions that we call decomposable: These are functions that can be decomposed into sums of concave functions applied to modular (additive) functions. Our algorithm is based on recent techniques of smoothed convex minimization [18] applied to the Lova\u0301sz extension. We demonstrate the usefulness of\nar X\niv :1\n01 0.\n55 11\nv1 [\ncs .L\nG ]\n2 6\nO ct\nour algorithm on a joint classification-and-segmentation task involving tens of thousands of variables, and show that it outperforms state-of-the-art algorithms for general submodular function minimization by several orders of magnitude."}, {"heading": "2 Background on Submodular Function Minimization", "text": "We are interested in minimizing set functions that map subsets of some base set E to real numbers. I.e., given f : 2E \u2192 R we wish to solve for A\u2217 \u2208 arg minA f(A). For simplicity of notation, we use the base set E = {1, . . . n}, but in an application the base set may consist of nodes of a graph, pixels of an image, etc. Without loss of generality, we assume f(\u2205) = 0. If the function f has no structure, then there is no way solve the problem other than checking all 2n subsets. In this paper, we consider functions that satisfy a key property that arises in many applications: submodularity (c.f., [16]). A set function f is called submodular iff, for all A,B \u2208 2E , we have\nf(A \u222aB) + f(A \u2229B) \u2264 f(A) + f(B). (1) Submodular functions can alternatively, and perhaps more intuitively, be characterized in terms of their discrete derivatives. First, we define \u2206kf(A) = f(A\u222a{k})\u2212f(A) to be the discrete derivative of f with respect to k \u2208 E at A; intuitively this is the change in f \u2019s value by adding the element k to the set A. Then, f is submodular iff:\n\u2206kf(A) \u2265 \u2206kf(B), for all A \u2286 B \u2286 E and k \u2208 E \\B. Note the analogy to concave functions; the discrete derivative is smaller for larger sets, in the same way that \u03c6(x+h)\u2212\u03c6(x) \u2265 \u03c6(y+h)\u2212\u03c6(y) for all x \u2264 y, h \u2265 0 if and only if \u03c6 is a concave function on R. Thus a simple example of a submodular function is f(A) = \u03c6(|A|) where \u03c6 is any concave function. Yet despite this connection to concavity, it is in fact \u2018easier\u2019 to minimize a submodular function than to maximize it1, just as it is easier to minimize a convex function. One explanation for this is that submodular minimization can be reformulated as a convex minimization problem.\nTo see this, consider taking a set function minimization problem, and reformulating it as a minimization problem over the unit cube [0, 1]n \u2282 Rn. Define eA \u2208 Rn to be the indicator vector of the set A, i.e.,\neA[k] = { 0 if k /\u2208 A 1 if k \u2208 A\nWe use the notation x[k] for the kth element of the vector x. Also we drop brackets and commas in subscripts, so ekl = e{k,l} and ek = e{k} as with the standard unit vectors. A continuous extension of a set function f is a function f\u0303 on the unit cube f\u0303 : [0, 1]n \u2192 R with the property that f(A) = f\u0303(eA). In order to be useful, however, one needs the minima of the set function to be related to minima of the extension:\nA\u2217 \u2208 arg min A\u22082E f(A)\u21d2 eA\u2217 \u2208 arg min x\u2208[0,1]n f\u0303(x). (2)\nA key result due to Lova\u0301sz [16] states that each submodular function f has an extension f\u0303 that not only satisfies the above property, but is also convex and efficient to evaluate. We can define the Lova\u0301sz extension in terms of the submodular polyhedron Pf :\nPf = {v \u2208 Rn : v \u00b7 eA \u2264 f(A), for all A \u2208 2E}, f\u0303(x) = sup v\u2208Pf v \u00b7 x.\nThe submodular polyhedron Pf is defined by exponentially many inequalities, and evaluating f\u0303 requires solving a linear program over this polyhedron. Perhaps surprisingly, as shown by Lova\u0301sz, f\u0303 can be very efficiently computed as follows. For a fixed x let \u03c3 : E \u2192 E be a permutation such that x[\u03c3(1)] \u2265 . . . \u2265 x[\u03c3(n)], and then define the set Sk = {\u03c3(1), . . . , \u03c3(k)}. Then we have a formula for f\u0303 and a subgradient:\nf\u0303(x) = n\u2211 k=1 x[\u03c3(k)](f(Sk)\u2212 f(Sk\u22121)), \u2202f\u0303(x) 3 n\u2211 k=1 e\u03c3(k)(f(Sk)\u2212 f(Sk\u22121)).\nNote that if two components of x are equal, the above formula for f\u0303 is independent of the permutation chosen, but the subgradient is not unique.\n1With the additional assumption that f is nondecreasing, maximizing a submodular function subject to a cardinality constraint |A| \u2264M is \u2018easy\u2019; a greedy algorithm is known to give a near-optimal answer [17].\nEquation (2) was used to show that submodular minimization can be achieved in polynomial time [16]. However, algorithms which directly minimize the Lovasz extension are regarded as impractical. Despite being convex, the Lova\u0301sz extension is non-smooth, and hence a simple subgradient descent algorithm would need O(1/ 2) steps to achieve O( ) accuracy.\nRecently, Nesterov showed that if knowledge about the structure of a particular non-smooth convex function is available, it can be exploited to achieve a running time of O(1/ ) [18]. One way this is done is to construct a smooth approximation of the non-smooth function, and then use an accelerated gradient descent algorithm which is highly effective for smooth functions. Connections of this work with submodularity and combinatorial optimization are also explored in [4] and [2]. In fact, in [2], Bach shows that computing the smoothed Lova\u0301sz gradient of a general submodular function is equivalent to solving a submodular minimization problem. In this paper, we do not treat general submodular functions, but rather a large class of submodular minimization functions that we call decomposable. (To apply the smoothing technique of [18], special structural knowledge about the convex function is required, so it is natural that we would need special structural knowledge about the submodular function to leverage those results.) We further show that we can exploit the discrete structure of submodular minimization in a way that allows terminating the algorithm early with a certificate of optimality, which leads to drastic performance improvements."}, {"heading": "3 The Decomposable Submodular Minimization Problem", "text": "In this paper, we consider the problem of minimizing functions of the following form: f(A) = c \u00b7 eA + \u2211 j \u03c6j(wj \u00b7 eA), (3)\nwhere c,wj \u2208 Rn and 0 \u2264 wj \u2264 1 and \u03c6j : [0,wj \u00b7 1] \u2192 R are arbitrary concave functions. It is shown in the Appendix that functions of this form are submodular. We call this class of functions decomposable submodular functions, as they decompose into a sum of concave functions applied to nonnegative modular functions2. Below, we give examples of decomposable submodular functions arising in applications.\nWe first focus on the special case where all the concave functions are of the form \u03c6j(\u00b7) = dj min(yj , \u00b7) for some yj , dj > 0. Since these potentials are of key importance, we define the submodular functions \u03a8w,y(A) = min(y,w \u00b7 eA) and call them threshold potentials. In Section 5, we will show in how to generalize our approach to arbitrary decomposable submodular functions.\nExamples. The simplest example is a 2-potential, which has the form \u03c6(|A\u2229{k, l}|), where \u03c6(1)\u2212 \u03c6(0) \u2265 \u03c6(1)\u2212 \u03c6(2). It can be expressed as a sum of a modular function and a threshold potential:\n\u03c6(|A \u2229 {k, l}|) = \u03c6(0) + (\u03c6(2)\u2212 \u03c6(1))ekl \u00b7 eA + (2\u03c6(1)\u2212 \u03c6(0)\u2212 \u03c6(2))\u03a8ekl,1(A) Why are such potential functions interesting? They arise, for example, when finding the Maximum a Posteriori configuration of a pairwise Markov Random Field model in image classification schemes such as in [20]. On a high level, such an algorithm computes a value c[k] that corresponds to the log-likelihood of pixel k being of one class vs. another, and for each pair of adjacent pixels, a value dkl related to the log-likelihood that pixels k and l are of the same class. Then the algorithm classifies pixels by minimizing a sum of 2-potentials: f(A) = c \u00b7eA + \u2211 k,l dkl(1\u2212 |1\u2212ekl \u00b7eA|). If the value dkl is large, this encourages the pixels k and l to be classified similarly.\nMore generally, consider a higher order potential function: a concave function of the number of elements in some activation set S, \u03c6(|A \u2229 S|) where \u03c6 is concave. It can be shown that this can be written as a sum of a modular function and a positive linear combination of |S| \u2212 1 threshold potentials. Recent work [14] has shown that classification performance can be improved by adding terms corresponding to such higher order potentials \u03c6j(|Rj\u2229A|) to the objective function where the functions \u03c6j are piecewise linear concave functions, and the regions Rj of various sizes generated from a segmentation algorithm. Minimization of these particular potential functions can then be reformulated as a graph cut problem [13], but this is less general than our approach.\nAnother canonical example of a submodular function is a set cover function. Such a function can be reformulated as a combination of concave cardinality functions (details in appendix). So all\n2A function is called modular if (1) holds with equality. It can be written as A 7\u2192 w \u00b7eA for some w \u2208 Rn.\nfunctions which are weighted combinations of set cover functions can be expressed as threshold potentials. However, threshold potentials with nonuniform weights are strictly more general than concave cardinality potentials. That is, there existsw and y such that \u03a8w,y(A) cannot be expressed as \u2211 j \u03c6j(|Rj \u2229A|) for any collection of concave \u03c6j and sets Rj .\nAnother example of decomposable functions arises in multiclass queuing systems [10]. These are of the form f(A) = c \u00b7 eA + u \u00b7 eA\u03c6(v \u00b7 eA), where u,v are nonnegative weight vectors and \u03c6 is a nonpositive nonincreasing concave function. With the proper choice of \u03c6j and wj (again details are in appendix), this can in fact be reformulated as sum of the type in Eq. 3 with n terms.\nIn our own experiments, shown in Section 6, we use an implementation of TextonBoost [20] and augment it with quadratic higher order potentials. That is, we use TextonBoost to generate per-pixel scores c, and then minimize f(A) = c \u00b7eA+ \u2211 j |A\u2229Rj ||Rj \\A|, where the regionsRj are regions of pixels that we expect to be of the same class (e.g., by running a cheap region-growing heuristic). The potential function |A\u2229Rj ||Rj\\A| is smallest whenA contains all ofRj or none of it. It gives the largest penalty when exactly half of Rj is contained in A. This encourages the classification scheme to classify most of the pixels in a region Rj the same way. We generate regions with a basic regiongrowing algorithm with random seeds. See Figure 1(a) for an illustration of examples of regions that we use. In our experience, this simple idea of using higher-order potentials can dramatically increase the quality of the classification over one using only 2-potentials, as can be seen in Figure 2.\n4 The SLG Algorithm for Threshold Potentials We now present our algorithm for efficient minimization of a decomposable submodular function f based on smoothed convex minimization. We first show how we can efficiently smooth the Lova\u0301sz extension of f . We then apply accelerated gradient descent to the gradient of the smoothed function. Lastly, we demonstrate how we can often obtain a certificate of optimality that allows us to stop early, drastically speeding up the algorithm in practice."}, {"heading": "4.1 The Smoothed Extension of a Threshold Potential", "text": "The key challenge in our algorithm is to efficiently smooth the Lova\u0301sz extension of f , so that we can resort to algorithms for accelerated convex minimization. We now show how we can efficiently smooth the threshold potentials \u03a8w,y(A) = min(y,w \u00b7 eA) of Section 3, which are simple enough to allow efficient smoothing, but rich enough when combined to express a large class of submodular functions. For x \u2265 0, the Lova\u0301sz extension of \u03a8w,y is \u03a8\u0303w,y(x) = supv \u00b7 x s.t. v \u2264 w,v \u00b7 eA \u2264 y for all A \u2208 2E . Note that when x \u2265 0, the arg max of the above linear program always contains a point v which satisfies v \u00b7 1 = y, and v \u2265 0. So we can restrict the domain of the dual variable v to those points which satisfy these two conditions, without changing the value of \u03a8\u0303(x):\n\u03a8\u0303w,y(x) = max v\u2208D(w,y)\nv \u00b7 x where D(w, y) = {v : 0 \u2264 v \u2264 w,v \u00b7 1 = y}.\nRestricting the domain of v allows us to define a smoothed Lova\u0301sz extension (with parameter \u00b5) that is easily computed:\n\u03a8\u0303\u00b5w,y(x) = max v\u2208D(w,y) v \u00b7 x\u2212 \u00b5 2 \u2016v\u20162\nTo compute the value of this function we need to solve for the optimal vector v\u2217, which is also the gradient of this function, as we have the following characterization:\n\u2207\u03a8\u0303\u00b5w,y(x) = arg max v\u2208D(w,y) v \u00b7 x\u2212 \u00b5 2 \u2016v\u20162 = arg min v\u2208D(w,y) \u2225\u2225\u2225\u2225x\u00b5 \u2212 v \u2225\u2225\u2225\u2225 . (4)\nTo derive an expression for v\u2217, we begin by forming the Lagrangian and deriving the dual problem:\n\u03a8\u0303\u00b5w,y(x) = min t\u2208R,\u03bb1,\u03bb2\u22650 ( max v\u2208Rn v \u00b7 x\u2212 \u00b5 2 \u2016v\u20162 + \u03bb1 \u00b7 v + \u03bb2 \u00b7 (w \u2212 v) + t(y \u2212 v \u00b7 1) ) = min\nt\u2208R,\u03bb1,\u03bb2\u22650\n1\n2\u00b5 \u2016x\u2212 t1 + \u03bb1 \u2212 \u03bb2\u20162 + \u03bb2 \u00b7w + ty.\nIf we fix t, we can solve for the optimal dual variables \u03bb\u22171 and \u03bb \u2217 2 componentwise. By strong duality, we know the optimal primal variable is given by v\u2217 = 1\u00b5 (x\u2212 t \u22171 + \u03bb\u22171 \u2212 \u03bb\u22172). So we have: \u03bb\u22171 = max(t \u22171\u2212 x,0), \u03bb\u22172 = max(x\u2212 t\u22171\u2212 \u00b5w,0)\u21d2 v\u2217 = min (max ((x\u2212 t\u22171)/\u00b5,0) ,w) .\nThis expresses v\u2217 as a function of the unknown optimal dual variable t\u2217. For the simple case of 2-potentials, we can solve for t\u2217 explicitly and get a closed form expression:\n\u2207\u03a8\u0303\u00b5ekl,1(x) =  ek if x[k] \u2265 x[l] + \u00b5 el if x[l] \u2265 x[k] + \u00b5 1 2 (ekl + 1 \u00b5 (x[k]\u2212 x[l])(ek \u2212 el)) if |x[k]\u2212 x[l]| < \u00b5\nHowever, in general to find t\u2217 we note that v\u2217 must satisfy v\u2217 \u00b7 1 = y. So define \u03c1\u00b5x,w(t) as:\n\u03c1\u00b5x,w(t) = min(max((x\u2212 t1)/\u00b5,0),w) \u00b7 1 Then we note this function is a monotonic continuous piecewise linear function of t, so we can use a simple root-finding algorithm to solve \u03c1\u00b5x,w(t\n\u2217) = y. This root finding procedure will take no more than O(n) steps in the worst case.\n4.2 The SLG Algorithm for Minimizing Sums of Threshold Potentials Stepping beyond a single threshold potential, we now assume that the submodular function to be minimized can be written as a nonnegative linear combination of threshold potentials and a modular function, i.e.,\nf(A) = c \u00b7 eA + \u2211 j dj\u03a8wj ,yj (A).\nThus, we have the smoothed Lova\u0301sz extension, and its gradient: f\u0303\u00b5(x) = c \u00b7 x+ \u2211 j dj\u03a8\u0303 \u00b5 wj ,yj (x) and\u2207f\u0303 \u00b5(x) = c+ \u2211 j dj\u2207\u03a8\u0303\u00b5wj ,yj (x).\nWe now wish to use the accelerated gradient descent algorithm of [18] to minimize this function. This algorithm requires that the smoothed objective has a Lipschitz continuous gradient. That is, for some constant L, it must hold that \u2016\u2207f\u0303\u00b5(x1) \u2212 \u2207f\u0303\u00b5(x2)\u2016 \u2264 L\u2016x1 \u2212 x2\u2016, for all x1,x2 \u2208 Rn. Fortunately, by construction, the smoothed threshold extensions \u03a8\u0303\u00b5wj ,yj (x) all have 1/\u00b5 Lipschitz gradient, a direct consequence of the characterization in Equation 4. Hence we have a loose upper bound for the Lipschitz constant of f\u0303\u00b5: L \u2264 D\u00b5 , where D = \u2211 j dj . Furthermore, the smoothed threshold extensions approximate the threshold extensions uniformly: |\u03a8\u0303\u00b5wj ,yj (x)\u2212 \u03a8\u0303wj ,yj (x)| \u2264 \u00b5 2 for all x, so |f\u0303 \u00b5(x)\u2212 f\u0303(x)| \u2264 \u00b5D2 .\nOne way to use the smoothed gradient is to specify an accuracy \u03b5, then minimize f\u0303\u00b5 for sufficiently small \u00b5 to guarantee that the solution will also be an approximate minimizer of f\u0303 . Then we simply apply the accelerated gradient descent algorithm of [18]. See also [3] for a description. Let PC(x) = arg minx\u2032\u2208C \u2016x \u2212 x\u2032\u2016 be the projection of x onto the convex set C. In particular, P[0,1]n(x) = min(max(x,0),1). Algorithm 1 formalizes our Smoothed Lova\u0301sz Gradient (SLG) algorithm:\nAlgorithm 1: SLG: Smoothed Lova\u0301sz Gradient Input: Accuracy \u03b5; decomposable function f . begin\n\u00b5 = \u03b52D , L = D \u00b5 , x\u22121 = z\u22121 = 1 21; for t = 0, 1, 2, . . . do gt = \u2207f\u0303\u00b5(xt\u22121)/L; zt = P[0,1]n ( z\u22121 \u2212 \u2211t s=0 ( s+1 2 ) gs ) ; yt = P[0,1]n(xt \u2212 gt);\nif gapt \u2264 \u03b5/2 then stop; xt = (2zt + (t+ 1)yt)/(t+ 3);\nx\u03b5 = yt; Output: \u03b5-optimal x\u03b5 to minx\u2208[0,1]n f\u0303(x)\nThe optimality gap of a smooth convex function at the iterate yt can be computed from its gradient:\ngapt = max x\u2208[0,1]n (yt \u2212 x) \u00b7 \u2207f\u0303\u00b5(yt) = yt \u00b7 \u2207f\u0303\u00b5(yt) + max(\u2212\u2207f\u0303\u00b5(yt),0) \u00b7 1.\nIn summary, as a consequence of the results of [18], we have the following guarantee about SLG:\nTheorem 1 SLG is guaranteed to provide an \u03b5-optimal solution after running forO(D\u03b5 ) iterations.\nSLG is only guaranteed to provide an \u03b5-optimal solution to the continuous optimization problem. Fortunately, once we have an \u03b5-optimal point for the Lova\u0301sz extension, we can efficiently round it to set which is \u03b5-optimal for the original submodular function using Alg. 2 (see [9] for more details).\nAlgorithm 2: Set generation by rounding the continuous solution Input: Vector x \u2208 [0, 1]n; submodular function f . begin\nBy sorting, find any permutation \u03c3 satisfying: x[\u03c3(1)] \u2265 . . . \u2265 x[\u03c3(n)]; Sk = {\u03c3(1), . . . , \u03c3(k)}; K\u2217 = arg mink\u2208{0,1,...,n} f(Sk); C = {Sk : k \u2208 K\u2217};\nOutput: Collection of sets C, such that f(A) \u2264 f\u0303(x) for all A \u2208 C"}, {"heading": "4.3 Early Stopping based on Discrete Certificates of Optimality", "text": "In general, if the minimum of f is not unique, the output of SLG may be in the interior of the unit cube. However, if f admits a unique minimum A\u2217, then the iterates will tend toward the corner eA\u2217 . One natural question one may ask, if a trend like this is observed, is it necessary to wait for the iterates to converge all the way to the optimal solution of the continuous problem minx\u2208[0,1]n f\u0303(x), when one is actually iterested in solving the discrete problem minA\u22082E f(A)? Below, we show that it is possible to use information about the current iterates to check optimality of a set and terminate the algorithm before the continuous problem has converged.\nTo prove optimality of a candidate set A, we can use a subgradient of f\u0303 at eA. If g \u2208 \u2202f\u0303(eA), then we can compute an optimality gap:\nf(A)\u2212 f\u2217 \u2264 max x\u2208[0,1]n (eA \u2212 x) \u00b7 g = \u2211 k\u2208A max(0, g[k](eA[k]\u2212 eE\\A[k])). (5)\nIn particular if g[k] \u2264 0 for k \u2208 A and g[k] \u2265 0 for k \u2208 E \\ A, then A is optimal. But if we only have knowledge of candidate set A, then finding a subgradient g \u2208 \u2202f\u0303(eA) which demonstrates optimality may be extremely difficult, as the set of subgradients is a polyhedron with exponentially many extreme points. But our algorithm naturally suggests the subgradient we could use; the gradient of the smoothed extension is one such subgradient \u2013 provided a certain condition is satisfied, as described in the following Lemma.\nLemma 1 Suppose f is a decomposable submodular function, with Lova\u0301sz extension f\u0303 , and smoothed extension f\u0303\u00b5 as in the previous section. Suppose x \u2208 Rn and A \u2208 2E satisfy the following property:\nmin k\u2208A,l\u2208E\\A x[k]\u2212 x[l] \u2265 2\u00b5 Then \u2207f\u0303\u00b5(x) \u2208 \u2202f\u0303(eA) This is a consequence of our formula for\u2207\u03a8\u0303\u00b5, but see the appendix for a detailed proof. Lemma 1 states that if the components of point x corresponding to elements of A are all larger than all the other components by at least 2\u00b5, then the gradient at x is a subgradient for f\u0303 at eA (which by Equation 5 allows us to compute an optimality gap). In practice, this separation of components naturally occurs as the iterates move in the direction of the point eA, long before they ever actually reach the point eA. But even if the components are not separated, we can easily add a positive multiple of eA to separate them and then compute the gradient there to get an optimality gap. In summary, we have the following algorithm to check the optimality of a candidate set: Of critical\nAlgorithm 3: Set Optimality Check Input: Set A; decomposable function f ; scale \u00b5; x \u2208 Rn. begin\n\u03b3 = 2\u00b5+ maxk\u2208A,l\u2208E\\A x[l]\u2212 x[k]; g = \u2207f\u0303\u00b5(x+ \u03b3eA); gap = \u2211 k\u2208A max(0, g[k](eA[k]\u2212 eE\\A[k]));\nOutput: gap, which satisfies gap \u2265 f(A)\u2212 f\u2217\nimportance is how to choose the candidate set A. But by Equation 5, for a set to be optimal, we want the components of the gradient \u2207f\u0303\u00b5(A + \u03b3eA)[k] to be negative for k \u2208 A and positive for k \u2208 E \\ A. So it is natural to choose A = {k : \u2207f\u0303\u00b5(x)[k] \u2264 0}. Thus, if adding \u03b3eA does not change the signs of the components of the gradient, then in fact we have found the optimal set. This stopping criterion is very effective in practice, and we use it in all of our experiments."}, {"heading": "5 Extension to General Concave Potentials", "text": "To extend our algorithm to work on general concave functions, we note that an arbitrary concave function can be expressed as an integral of threshold potential functions. This is a simple consequence of integration by parts, which we state in the following lemma:\nLemma 2 For \u03c6 \u2208 C2([0, T ]), \u03c6(x) = \u03c6(0) + \u03c6\u2032(T )x\u2212 \u222b T 0 min(x, y)\u03c6\u2032\u2032(y)dy, \u2200x \u2208 [0, T ]\nThis means that for a general sum of concave potentials as in Equation (3), we have:\nf(A) = c \u00b7 eA + \u2211 j ( \u03c6j(0) + \u03c6 \u2032(wj \u00b7 1)wj \u00b7 eA \u2212 \u222b wj \u00b71 0 \u03a8wj ,y(A)\u03c6 \u2032\u2032 j (y)dy ) .\nThen we can define f\u0303 and f\u0303\u00b5 by replacing \u03a8 with \u03a8\u0303 and \u03a8\u0303\u00b5 respectively. Our SLG algorithm is essentially unchanged, the conditions for optimality still hold, and so on. Conceptually, we just use a different smoothed gradient, but calculating it is more involved. We need to compute the integrals of the form \u222b \u2207\u03a8\u0303\u00b5w,y(x)\u03c6\u2032\u2032(y)dy. Since \u2207\u03a8\u0303\u00b5w,y(x) is a piecewise linear function with repect to y which we can compute, we can evaluate the integral by parts so that we need only evaluate \u03c6, but not its derivatives. We leave this formula for the appendix."}, {"heading": "6 Experiments", "text": "Synthetic Data. We reproduce the experimental setup of [8] designed to compare submodular minimization algorithms. Our goal is to find the minimum cut of a randomly generated graph (which requires submodular minimization of a sum of 2-potentials) with the graph generated by the specifications in [1]. We compare against the state of the art combinatorial algorithms (LEX2, HYBRID, SFM3, PR [6]) that are guaranteed to find the exact solution in polynomial time, as well as the Minimum Norm algorithm of [8], a practical alternative with unknown running time. Figures 1(b) and 1(c) compare the running time of SLG against the running times reported in [8]. In some cases, SLG was 6 times faster than the MinNorm algorithm. However the comparison to the MinNorm algorithm is inconclusive in this experiment, since while we used a faster machine, we also used a simple MATLAB implementation. What is clear is that SLG scales at least as well as MinNorm on these problems, and is practical for problem sizes that the combinatorial algorithms cannot handle.\nImage Segmentation Experiments. We also tested our algorithm on the joint image segmentation-and-classification task introduced in Section 3. We used an implementation of TextonBoost [20], then trained on and tested subsampled images from [5]. As seen in Figures 2(e) and 2(g), using only the per-pixel score from our TextonBoost implementation gets the general area of the object, but does not do a good job of identifying the shape of a classified object. Compare to the ground truth in Figures 2(b) and 2(d). We then perform MAP inference in a Markov Random Field with 2-potentials (as done in [20]). While this regularization, as shown in Figures 2(f) and 2(h), leads to improved performance, it still performs poorly on classifying the boundary.\nFinally, we used SLG to regularize with higher order potentials. To generate regions for our potentials, we randomly picked seed pixels and grew the regions based on HSV channels of the image. We picked our seed pixels with a preference for pixels which were included in the least number of previously generated regions. Figure 1(a) shows what the regions typically looked like. For our experiments, we used 90 total regions. We used SLG to minimize f(A) = c\u00b7eA+ \u2211 j |A\u2229Rj ||Rj\\A|, where c was the output from TextonBoost, scaled appropriately. Figures 2(i) and 2(k) show the classification output. The continuous variables x at the end of each run are shown in Figures 2(j) and 2(l); while it has no formal meaning, in general one can interpret a very high or low value of x[k] to correspond to high confidence in the classification of the pixel k. To generate the result shown in Figure 2(k), a problem with 104 variables and 90 concave potentials, our MATLAB/mex implementation of SLG took 71.4 seconds. In comparison, the MinNorm implementation of the SFO toolbox [15] gave the same result, but took 6900 seconds. Similar problems on an image of twice the resolution (4\u00d7 104 variables) were tested using SLG, resulting in a runtimes of roughly 1600 seconds."}, {"heading": "7 Conclusion", "text": "We have developed a novel method for efficiently minimizing a large class of submodular functions of practical importance. We do so by decomposing the function into a sum of threshold potentials, whose Lova\u0301sz extensions are convenient for using modern smoothing techniques of convex optimization. This allows us to solve submodular minimization problems with thousands of variables, that cannot be expressed using only pairwise potentials. Thus we have achieved a middle ground between graph-cut-based algorithms which are extremely fast but only able to handle very specific types of submodular minimization problems, and combinatorial algorithms which assume nothing but submodularity but are impractical for large-scale problems.\nAcknowledgements This research was partially supported by NSF grant IIS-0953413, a gift from Microsoft Corporation and an Okawa Foundation Research Grant. Thanks to Alex Gittens and Michael McCoy for use of their TextonBoost implementation."}, {"heading": "A Submodularity of Decomposable Functions", "text": "Since the sum of submodular functions is submodular, we need only prove that the submodularity of f(A) = \u03c6(w \u00b7 eA), where \u03c6 is an arbitrary concave function on R and w \u2265 0. By definition of concavity, for all \u03b8 \u2208 [0, 1], we have:\n\u03c6(\u03b8(y + h) + (1\u2212 \u03b8)x) + \u03c6((1\u2212 \u03b8)(y + h) + \u03b8x) \u2265 \u03c6(y + h) + \u03c6(x) If x \u2264 y and h \u2265 0, then setting \u03b8 = h/(y \u2212 x+ h) in the above gives us:\n\u03c6(x+ h)\u2212 \u03c6(x) \u2265 \u03c6(y + h)\u2212 \u03c6(y) (6) Then, for all k \u2208 E \\A, we compute the the discrete derivative \u2206kf(A): \u2206kf(A) = \u03c6(w \u00b7 eA +w[k])\u2212 \u03c6(w \u00b7 eA) (7) So if A \u2286 B \u2286 E and k \u2208 E \\B, thenw \u00b7 eA \u2264 w \u00b7 eB , so by Eqs. 6 and 7, \u2206kf(A) \u2265 \u2206kf(B), and hence f is submodular."}, {"heading": "B Reformulation of Set Cover Functions", "text": "A set cover function can be formulated as the function: f(A) = |\u222ai\u2208ABi|\nWhere Bi are subsets of some base set F , and the Bi form some collection of subsets indexed by E. For every k \u2208 F , we define the vectors wk \u2208 R|E| as follows:\nwk[i] = { 0 k /\u2208 Bi 1 k \u2208 Bi\nWe claim: f(A) = \u2211 k\u2208F min(1,wk \u00b7 eA) The kth term in the sum equals 1 if k \u2208 Bi for some i \u2208 A and 0 otherwise. The sum of all such terms will give the cardinality of the union of the Bi with i \u2208 A, which is exactly the set cover function."}, {"heading": "C Strict Generality of Threshold Potentials", "text": "As mentioned in the text, any concave cardinality function can be decomposed into the sum of several threshold potentials. This is effectively the discrete version of Lemma 2:\n\u03c6(|A \u2229 S|) = \u03c6(0) + (\u03c6(|S|)\u2212 \u03c6(|S| \u2212 1))eS \u00b7 eA + |S|\u22121\u2211 k=1 (2\u03c6(k)\u2212 \u03c6(k \u2212 1)\u2212 \u03c6(k + 1)) min(k, eS \u00b7 eA)\nSince \u03c6 is concave, the coefficients (2\u03c6(k) \u2212 \u03c6(k \u2212 1) \u2212 \u03c6(k + 1)) are nonnegative. So without loss of generality, any sum of concave cardinality functions can be expressed as a sum of a modular function and nonnegative linear combination of threshold potentials:\u2211\nj\n\u03c6j(|Rj \u2229A|) = c \u00b7 eA + \u2211\nSk\u2282E,|S|>1\n|S|\u22121\u2211 k=1 dkl min(k, eSk \u00b7 eA)\nThere are \u2211 m=2 ( n m ) (m \u2212 1) coefficients dkl and they all must be nonnegative. So to check if a submodular function f(A) can be expressed as such a sum, we can just write out the 2n constraints for each subset:\nf(A) = c \u00b7 eA + \u2211\nSk\u2282E,|S|>1\n|S|\u22121\u2211 k=1 dkl min(k, eSk \u00b7 eA) for all A \u2208 2E (8)\nIf n = 4, we have 24 linear constraints, 4 unconstrained variables from c, and 19 nonnegative variables dkl. This is small enough that one can check for feasibility using a linear algebra package. We discovered that simple threshold potential f(A) = min(y,w \u00b7 eA) with w = [1, 2, 3, 4]/4 and y = 1 does not have a feasible solution to Eq. 8."}, {"heading": "D Reformulation of a Class of Functions", "text": "Another example of decomposable functions are the problems under consideration in [10], which are of the following form:\nf(A) = c \u00b7 eA + (u \u00b7 eA)\u03c6(v \u00b7 eA)\nWhere u,v are nonnegative weight vectors and \u03c6 is a nonincreasing concave function. Suppose we can choose vectors wj and concave \u03c6\u0303 to satisfy:\n\u03c6\u0303(wj \u00b7 eA) = {\n0 if j /\u2208 A \u03c6(v \u00b7 eA)\u2212 \u03c6(0) if j \u2208 A\n(9)\nThen we claim the following is an equivalent formulation for f in decomposable form:\nf \u2032(A) = (c+ \u03c6(0)u) \u00b7 eA + n\u2211 j=1 u[j]\u03c6\u0303(wj \u00b7 eA) (10)\nIndeed, plugging Eq. 9 to the above gives: f \u2032(A) = (c+ \u03c6(0)u) \u00b7 eA + \u2211 j\u2208A u[j](\u03c6(v \u00b7 eA)\u2212 \u03c6(0)) = f(A)\nTo satisfy Eq. 9 we define \u03c6\u0303 as follows:\n\u03c6\u0303(t) = { 0 if t \u2264 1 \u00b7 v \u03c6(t\u2212 1 \u00b7 v)\u2212 \u03c6(0) if t > 1 \u00b7 v\nAnd let wj = v + (1 \u00b7 v)ej . It is straightforward to check that these definitions satisfy Eq. 9. Note \u03c6\u0303 is concave because \u03c6 is nonincreasing concave. Incidentally, the decomposition in Eq. 10 proves that f is submodular."}, {"heading": "E Proof of Lemma 1", "text": "By linearity, it is sufficient to consider the case f = \u03a8\u00b5w,y . First we claim that if the hypothesis of the Lemma holds, adding a positive multiple of eA will not change the gradient. That is, \u2207\u03a8\u0303\u00b5w,y(x) = \u2207\u03a8\u0303\u00b5w,y(x+ \u03b1eA) for \u03b1 > 0. Recall the formula for the gradient:\n\u2207\u03a8\u0303\u00b5w,y(x) = min(max((x\u2212 t\u22171)/\u00b5,0) where t\u2217 satisfies min(max((x\u2212 t\u22171)/\u00b5,0),w) \u00b7 1 = y\nConsider the effect of adding \u03b1eA to x in this formula; either t\u2217 is increased by \u03b1 or it is unchanged; in either case the gradient itself is unchanged. Next, note the following scale relationship which follows directly from the definition of \u03a8\u0303\u00b5w,y:\n\u03a8\u0303\u00b5w,y(\u03b1x) = \u03b1\u03a8\u0303 \u00b5/\u03b1 w,y (x).\nBut combined with our first observation this implies\n\u2207\u03a8\u0303\u00b5w,y(x) = \u2207\u03a8\u0303\u00b5/\u03b1w,y (x/\u03b1+ eA)\nBut the right-hand side of that equation must converge to a subgradient of the nonsmooth function as \u03b1\u2192\u221e:\nlim \u03b1\u2192\u221e\n\u2207\u03a8\u0303\u00b5/\u03b1w,y (x/\u03b1+ eA) \u2208 \u2202\u03a8\u0303w,y(eA)\nwhich gives the result."}, {"heading": "F Proof of Lemma 2", "text": "This is straightforward calculation:\u222b T 0 min(x, y)\u03c6\u2032\u2032(y)dy = \u222b x 0 y\u03c6\u2032\u2032(y)dy + \u222b T x x\u03c6\u2032\u2032(y)dy\n= (y\u03c6\u2032(y)\u2212 \u03c6(y)) \u2223\u2223x 0 + x\u03c6\u2032(y) \u2223\u2223T x = x\u03c6\u2032(x)\u2212 \u03c6(x) + \u03c6(0) + x\u03c6\u2032(T )\u2212 x\u03c6\u2032(x) = \u03c6(0) + x\u03c6\u2032(T )\u2212 \u03c6(x)\nIntuitively, this is a consequence of integration by parts and the fact that \u2202 2\n\u2202x2 min(x, y) = \u2212\u03b4(x\u2212y) (the Dirac delta)."}, {"heading": "G General Smoothed Gradient Formula", "text": "Let f(A) = \u03c6(w \u00b7 eA) be a general concave potential. For ease of notation, in the following let g(y) = \u2207\u03a8\u0303\u00b5w,y(x) be the gradient of the smoothed extension of a threshold potential. Then by Lemma 2, we have this formula for the gradient of smoothed extention of f :\n\u2207f\u0303\u00b5(x) = \u03c6\u2032(w \u00b7 1)w \u2212 \u222b w\u00b71 0 g(y)\u03c6\u2032\u2032(y)dy\nNote that g is a piecewise linear function of y. Let the intervals [yi, yi+1] with 0 = y0 \u2264 . . . \u2264 yN = w \u00b7 1 be the intervals that g is linear on. Let \u03b8i = g(yi), so then 0 = \u03b80 \u2264 . . . \u2264 \u03b8N = w. Finally let gi(y) be the linear functions that g equals on these intervals,:\ng(y) = gi(y) for y \u2208 [yi\u22121, yi]\nDenote by g\u2032i = (\u03b8i \u2212 \u03b8i\u22121)/(yi \u2212 yi\u22121) the vector which is derivative of gi(y) with respect to y. So then our smoothed gradient can be evaluated:\n\u2207f\u0303\u00b5(x) = \u03c6\u2032(yN )w \u2212 N\u2211 i=1 \u222b yi yi\u22121 gi(y)\u03c6 \u2032\u2032(y)dy\n= \u03c6\u2032(yN )w + N\u2211 i=1 (g\u2032i\u03c6(y)\u2212 gi(y)\u03c6\u2032(y)) \u2223\u2223yi yi\u22121\n= \u03c6\u2032(yN )w + N\u2211 i=1 g\u2032i(\u03c6(yi)\u2212 \u03c6(yi\u22121))\u2212 N\u2211 i=1 (\u03b8i\u03c6 \u2032(yi)\u2212 \u03b8i\u22121\u03c6\u2032(yi\u22121))\n= N\u2211 i=1 (g(yi)\u2212 g(yi\u22121))(\u03c6(yi)\u2212 \u03c6(yi\u22121)) yi \u2212 yi\u22121\nNote there are at most 2n points yi, and they can be found all in O(n log n) time, since it requires a sort. So the overall operation count of evaluating this formula is O(n2) since it requires adding up O(n) n-dimensional vectors."}], "references": [{"title": "Structured sparsity-inducing norms through submodular functions", "author": ["F. Bach"], "venue": "Advances in Neural Information Processing Systems ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Nesta: A fast and accurate first-order method for sparse recovery", "author": ["S. Becker", "J. Bobin", "E.J. Candes"], "venue": "Arxiv preprint arXiv 904 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient solutions to relaxations of combinatorial problems with submodular penalties via the Lov\u00e1sz extension and non-smooth convex optimization", "author": ["F.A. Chudak", "K. Nagano"], "venue": "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied Mathematics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "and A", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn"], "venue": "Zisserman, The PASCAL Visual Object Classes Challenge 2009 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "A push-relabel framework for submodular function minimization and applications to parametric optimization", "author": ["L. Fleischer", "S. Iwata"], "venue": "Discrete Applied Mathematics 131 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Energy minimization via graph cuts: Settling what is possible", "author": ["D. Freedman", "P. Drineas"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2005. CVPR 2005, vol. 2", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "The Minimum-Norm-Point Algorithm Applied to Submodular Function", "author": ["Satoru Fujishige", "Takumi Hayashi", "Shigueo Isotani"], "venue": "Minimization and Linear Programming,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Beyond convexity: Online submodular minimization, Advances in Neural Information Processing Systems", "author": ["Elad Hazan", "Satyen Kale"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Computational geometric approach to submodular function minimization for multiclass queueing systems", "author": ["T. Itoko", "S. Iwata"], "venue": "Integer Programming and Combinatorial Optimization ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "A combinatorial strongly polynomial algorithm for minimizing submodular functions", "author": ["S. Iwata", "L. Fleischer", "S. Fujishige"], "venue": "Journal of the ACM (JACM) 48 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "A simple combinatorial algorithm for submodular function minimization", "author": ["S. Iwata", "J.B. Orlin"], "venue": "Proceedings of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms, Society for Industrial and Applied Mathematics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust Higher Order Potentials for Enforcing Label Consistency", "author": ["Pushmeet Kohli", "Lubor Ladick\u00fd", "Philip H.S. Torr"], "venue": "International Journal of Computer Vision", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "SFO: A Toolbox for Submodular Function Optimization", "author": ["A. Krause"], "venue": "The Journal of Machine Learning Research 11 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Submodular functions and convexity", "author": ["L. Lov\u00e1sz"], "venue": "Mathematical programming: the state of the art, Bonn ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1982}, {"title": "An analysis of the approximations for maximizing submodular set functions", "author": ["G. Nemhauser", "L. Wolsey", "M. Fisher"], "venue": "Mathematical Programming 14 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1978}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Minimizing symmetric submodular functions", "author": ["Maurice Queyranne"], "venue": "Mathematical Programming", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}], "referenceMentions": [{"referenceID": 9, "context": "In particular, the minimum of a submodular function can be found in strongly polynomial time [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "Unfortunately, while polynomial-time solvable, exact techniques for submodular minimization require a number of function evaluations on the order of n [12], where n is the number of variables in the problem (e.", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Examples include symmetric functions that can be solved in O(n) evaluations using Queyranne\u2019s algorithm [19], and functions that decompose into attractive, pairwise potentials, that can be solved using graph cutting techniques [7].", "startOffset": 104, "endOffset": 108}, {"referenceID": 5, "context": "Examples include symmetric functions that can be solved in O(n) evaluations using Queyranne\u2019s algorithm [19], and functions that decompose into attractive, pairwise potentials, that can be solved using graph cutting techniques [7].", "startOffset": 227, "endOffset": 230}, {"referenceID": 15, "context": "Our algorithm is based on recent techniques of smoothed convex minimization [18] applied to the Lov\u00e1sz extension.", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": ", [16]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "A key result due to Lov\u00e1sz [16] states that each submodular function f has an extension f\u0303 that not only satisfies the above property, but is also convex and efficient to evaluate.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "With the additional assumption that f is nondecreasing, maximizing a submodular function subject to a cardinality constraint |A| \u2264M is \u2018easy\u2019; a greedy algorithm is known to give a near-optimal answer [17].", "startOffset": 201, "endOffset": 205}, {"referenceID": 13, "context": "Equation (2) was used to show that submodular minimization can be achieved in polynomial time [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 15, "context": "Recently, Nesterov showed that if knowledge about the structure of a particular non-smooth convex function is available, it can be exploited to achieve a running time of O(1/ ) [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 2, "context": "Connections of this work with submodularity and combinatorial optimization are also explored in [4] and [2].", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "Connections of this work with submodularity and combinatorial optimization are also explored in [4] and [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "In fact, in [2], Bach shows that computing the smoothed Lov\u00e1sz gradient of a general submodular function is equivalent to solving a submodular minimization problem.", "startOffset": 12, "endOffset": 15}, {"referenceID": 15, "context": "(To apply the smoothing technique of [18], special structural knowledge about the convex function is required, so it is natural that we would need special structural knowledge about the submodular function to leverage those results.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Recent work [14] has shown that classification performance can be improved by adding terms corresponding to such higher order potentials \u03c6j(|Rj\u2229A|) to the objective function where the functions \u03c6j are piecewise linear concave functions, and the regions Rj of various sizes generated from a segmentation algorithm.", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "Another example of decomposable functions arises in multiclass queuing systems [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "We now wish to use the accelerated gradient descent algorithm of [18] to minimize this function.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "Then we simply apply the accelerated gradient descent algorithm of [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "See also [3] for a description.", "startOffset": 9, "endOffset": 12}, {"referenceID": 15, "context": "In summary, as a consequence of the results of [18], we have the following guarantee about SLG:", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "2 (see [9] for more details).", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "We reproduce the experimental setup of [8] designed to compare submodular minimization algorithms.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "We compare against the state of the art combinatorial algorithms (LEX2, HYBRID, SFM3, PR [6]) that are guaranteed to find the exact solution in polynomial time, as well as the Minimum Norm algorithm of [8], a practical alternative with unknown running time.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "We compare against the state of the art combinatorial algorithms (LEX2, HYBRID, SFM3, PR [6]) that are guaranteed to find the exact solution in polynomial time, as well as the Minimum Norm algorithm of [8], a practical alternative with unknown running time.", "startOffset": 202, "endOffset": 205}, {"referenceID": 6, "context": "Figures 1(b) and 1(c) compare the running time of SLG against the running times reported in [8].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "We used an implementation of TextonBoost [20], then trained on and tested subsampled images from [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 12, "context": "In comparison, the MinNorm implementation of the SFO toolbox [15] gave the same result, but took 6900 seconds.", "startOffset": 61, "endOffset": 65}], "year": 2010, "abstractText": "Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.", "creator": "TeX"}}}