{"id": "1102.5597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2011", "title": "Fast and Faster: A Comparison of Two Streamed Matrix Decomposition Algorithms", "abstract": "With the explosion of the size of digital dataset, the limiting factor for decomposition algorithms is the \\emph{number of passes} over the input, as the input is often stored out-of-core or even off-site. Moreover, we're only interested in algorithms that operate in \\emph{constant memory} w.r. However, the problem with these algorithms is that in the event of a large number of collisions we might have to be very careful that the data will not be kept out of the main processing area.\n\nThe following comparison of the same data and the corresponding size of the data (see Fig. 5b).\nNow we know that our algorithms are capable of generating a large number of simultaneous data streams, and that the resulting values will be the same as the previous numbers. That's because these data streams are very large and have different processing values that each stream represents. To solve this problem we need a different algorithm that does both.\nIn this example we have a 32 bit algorithm which can fetch and calculate an image with the same size and bandwidth. This could be used to make it possible to store and display images in different formats on different types of display devices, and also to make the image faster than the data stream.\nTo solve this problem, we need two separate functions called c_d , which can fetch and calculate the data stream. It's basically the same, but it uses different functions to compute and compute the data stream.\nIn order to create a machine that can store and retrieve image images, we need to calculate a number of distinct data streams. The C++32 process generates images from both c_d and c_d . C++32 is a very simple C++ implementation. All images are in one single byte, whereas the C++ runtime provides the following function to compile a few different C++ programs: c_d . The C++ runtime provides the following function for calculating the number of consecutive images.\nc_d . The C++ runtime provides the following function for calculating the number of consecutive images.\nc_d . The C++ runtime provides the following function for calculating the number of consecutive images. c_d . The C++ runtime provides the following function for calculating the number of consecutive images. c_d . The C++ runtime provides the following function for calculating the number of consecutive images. c_d . The C++ runtime provides the following function for calculating the number of consecutive images. c_d . The C++ runtime provides", "histories": [["v1", "Mon, 28 Feb 2011 05:26:58 GMT  (159kb)", "http://arxiv.org/abs/1102.5597v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["radim \\v{r}eh{\\r{u}}\\v{r}ek"], "accepted": false, "id": "1102.5597"}, "pdf": {"name": "1102.5597.pdf", "metadata": {"source": "CRF", "title": "Fast and Faster: A Comparison of Two Streamed Matrix Decomposition Algorithms", "authors": ["Radim \u0158eh\u016f\u0159ek"], "emails": ["radimrehurek@seznam.cz"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 2.\n55 97\nv1 [\ncs .N\nA ]\n2 8\nFe b"}, {"heading": "1 Introduction", "text": "Matrix decomposition algorithms are commonly used in a variety of domains across much of the field of Computer Science1. Research has traditionally focused on optimizing the number of FLOPS (floating point operations) and numerical robustness of these algorithms [Comon and Golub, 1990, Golub and Van Loan, 1996]. However, modern datasets are too vast to be stored in main memory, or even on a single computer, so that communication itself quickly becomes a bottleneck.\nOne of the oldest and most widely known matrix decomposition algorithms is the Singular Value Decomposition (SVD), or its closely related eigen decomposition, which produce a provably optimal (in the least-squares sense) rank-k factorizations when truncated. In the following, n will denote the number of observations (matrix columns), m the number of features (matrix rows) and k the truncated target rank, k \u226a m \u226a n. In practise, the optimal decompositions are notoriously expensive to compute and truly large-scale applications are rare. The most common remedy is a) approximation (subsampling the input), b) some sort of incremental updating scheme which avoids recomputing the truncated models from scratch every time an observation/feature is updated, or c) giving up on a globally optimal solution and using another, heuristic algorithm. One way or another, the algorithm must avoid asking for O(n) memory, as the number of observations is assumed to be too large in modern problems. Table 1 summarizes available algorithms (and their implementations) with respect to several interesting characteristics, such as whether or not they are distributed, whether they can be incrementally updated, how many input passes are required or whether they realize subspace tracking (infinite input stream, gradual model decay).\nThis paper compares two particular modern approaches to large-scale eigen decomposition: a onepass streamed distributed algorithm from [R\u030cehu\u030ar\u030cek, 2010] and a modified stochastic streamed two-\n1Examples include Latent Semantic Analysis in Natural Language Processing; (discrete) Karhunen\u2013Loe\u0300ve Transform in Image Processing or Recommendation Systems in Information Retrieval. SVD is also used in solving shift-invariant Differential Equations, in Geophysics, in Signal Processing, in Antenna Array Processing, . . .\npass algorithm from [Halko et al., 2009]. They require one and two passes over the input respectively; we will call them P1 and P2 from now on. Both are streamed, meaning no random access to observations is required and their memory requirements are constant in the number of observations. Some modifications to the original P2 algorithm were necessary to achieve this; these are described below. Apart from the practical side-by-side comparison, we also present a hybrid of the two methods here, a novel algorithm which takes advantage of the speed of P2 while retaining the one-pass quality of P1."}, {"heading": "1.1 Stochastic two-pass algorithm, P2", "text": "The one-pass stochastic algorithm as described in [Halko et al., 2009] is unsuitable for large-scale decompositions, because the computation requires O(nk +mk) memory. We can reduce this to a managable O(mk), i.e. independent of the input stream size n, at the cost of running two passes over the input matrix instead of one2. This is achieved by two optimizations: 1) the sample matrix is constructed piece-by-piece from the stream, instead of a direct matrix multiplication, and 2) the final dense decomposition is performed on a smaller k \u00d7 k eigenproblem BBT instead of the full k \u00d7 n matrix B. These two \u201ctricks\u201d allow us to compute the decomposition in constant memory, by processing the observations one after another, or, preferrably, in as large chunks as fit into core memory. The intuition behind these optimizations if fairly straightforward, so we defer fleshing out the full algorithm to Appendix 1."}, {"heading": "1.2 One-pass algorithm, P1", "text": "Streamed one-pass algorithms are fundamentally different from the 2-pass algorithm above (or any other multi-pass algorithm), in that as long as they manage to keep their memory requirements constant, they allow us to process infinite input streams. In environments where the input cannot be persistently stored, this may be the only option.\nIn [R\u030cehu\u030ar\u030cek, 2010], I describe one such algorithm. It works by computing in-core decompositions of document chunks, possibly on different machines, and efficiently merging these dense partial decompositions into one. The partial in-core decomposition algorithm is viewed as \u201cblack box\u201d and chosen to be Douglas Rohde\u2019s SVDLIBC. The coarsely-grained parallelism of this algorithm makes it suitable for distributing the computation over a cluster of commodity computers connected by a high-latency network.\n1.3 Hybrid algorithm, P12\nIn this work, we also explore combining the two above approaches. We consider using the incore stochastic decomposition of [Halko et al., 2009] instead of SVDLIBC in the one-pass merging framework of [R\u030cehu\u030ar\u030cek, 2010]. This hybrid approach is labelled P12 in the experiments below.\n2Actually, 2 + q passes are needed when using q power iterations."}, {"heading": "2 Experiments", "text": "We will be comparing the algorithms on an implicit 100,000\u00d7 3,199,665 sparse matrix with 0.5 billion non-zero entries (0.15% density). This matrix represents the entire English Wikipedia3, with the vocabulary (number of features) clipped to the 100,000 most frequent word types4. In all experiments, the number of requested eigen factors is arbitrarily set to k = 400.\nThe experiments used three 2.0GHz Intel Xeon workstations with 4GB of RAM, connected by Ethernet on a single network segment. The machines were not dedicated; due to the large amount of experiments, we only managed to run each experiment twice. We report the better of the two times."}, {"heading": "2.1 Oversampling", "text": "In this set of experiments, we examine the relative accuracy of the three algorithms. P2 has two parameters which affect accuracy: the oversampling factor l and the number of power iterations q. In the one-pass algorithms P1 and P12, we improve accuracy by asking for extra factors l during intermediate computations, to be truncated at the very end of the decomposition.\nFigure 1 summarizes both the relative accuracy and runtime performance of the algorithms, for multiple choices of l and q. We see that although all methods are very accurate for the greatest factors, without oversampling the accuracy quickly degrades. This is especially true of the P2 algorithm, where no amount of oversampling helps and power iterations are definitely required.\nThe \u201cground-truth\u201d decomposition is unknown, so we cannot give absolute errors. However, according to our preliminary experiments on a smaller corpus, the stochastic algorithm with extra power iterations and oversampling gives the most accurate results; we will therefore plot it in all subsequent figures, in magenta colour, as a frame of reference. Note that all algorithm consistently err on the side of underestimating the magnitude of the singular values\u2014as a rule of thumb, the greater the singular values in each plot, the more accurate the result."}, {"heading": "2.2 Chunk size", "text": "The one-pass algorithms P1 and P12 proceed in document chunks that fit into core memory. A natural question is, what effect does the size of these chunks have on performance and accuracy? With smaller chunks, the algorithm requires less memory; with larger chunks, it performs fewer merges, so we might expect better performance. This intuition is quantified in Figure 2, which lists accuracy and performance results for chunk sizes of 10,000, 20,000 and 40,000 documents.\nWe see that chunk sizes in this range have little impact on accuracy, and that performance gradually improves with increasing chunk size. This speed-up is inversely proportional to the efficiency of the decomposition merge algorithm: with a hypothetical zero-cost merge algorithm, there would be no improvement at all, and runtime would be strictly dominated by costs of the in-core decompositions. On the other hand, a very costly merge routine would imply a linear relationship."}, {"heading": "2.3 Input stream order", "text": "In the Wikipedia input stream, observations are presented in lexicographic order\u2014observation corresponding to the Wikipedia entry on anarchy comes before the entry on bible, which comes before censorship etc. This order is of course far from random, so we are naturally interested in how it affects the resulting decomposition of the single-pass algorithms (the two-pass algorithm is orderagnostic by construction).\nTo test this, we randomly shuffled the input stream and re-ran the experiments on P1. Ideally, the results should be identical, no matter how we permute the input stream. Results in Figure 3 reveal that this is not the case: singular values coming from the shuffled runs are distinctly different to the ones coming from the original, alphabetically ordered sequence. This likely shows that the one-pass truncated scheme has some difficulties adjusting to gradual subspace drift. With the shuffled input,\n3Static dump as downloaded from http://download.wikimedia.org/enwiki/latest, June 2010.\n4The corpus preprocessing setup is described in more detail online.\nno significant drift can occur thanks to the completely random observation order, and a much higher accuracy is retained even without oversampling."}, {"heading": "2.4 Distributed computing", "text": "The two single pass algorithms, P1 and P12, lend themselves to easy parallelization. In Figure 4, we evaluate them on a cluster of 1, 2 and 4 computing nodes. The scaling behaviour is linear in the number of machines, as there is virtually no communication going on except for dispatching the input data and collecting the results. As with chunk size, the choice of cluster size does not affect accuracy much.\nThe P2 algorithm can be distributed too, but is already dominated by the cost of accessing data in its q+2 passes. Routing data around the network gives no performance boost, so we omit the results from the figure. We note that distributing P2 would still make sense under the condition that the data is already predistributed to the computing nodes, perhaps by means of a distributed filesystem."}, {"heading": "3 Conclusion", "text": "We presented a streamed version of a two-pass stochastic eigen decomposition algorithm and compared it to two streamed one-pass algorithms, one of which is a novel one-pass distributed algorithm. The comparison was done in the context of Latent Semantic Analysis, on a corpus of 3.2 million documents comprising the English Wikipedia.\nOn a single 2GHz machine, the top achieved decomposition times were 4 hours and 42 minutes for the one-pass P12 algorithm and 3 hours 6 minutes for the stochastic multi-pass algorithm. Without power iterations and with reduced amount of oversampling, we recorded even lower times, but at the cost of a serious loss of accuracy. On a cluster of four computing nodes on three physical machines, the single pass P12 decomposition was completed in 1 hour and 41 minutes.\nWe observed that the lightning-fast stochastic algorithm suffers from serious accuracy issues, which can be remedied by increasing the number of passes over the input (power iterations), as suggested in [Halko et al., 2009]. But, as the number of passes is the most precious resource in streaming environments, the otherwise slower one-pass algorithms become quickly competitive. The one-pass algorithms, one the other hand, suffer from dependency on the order of observations in the input stream; we will return to this behaviour in future work.\nA practical and perhaps even more exciting contribution is a modern implementation of these algorithms that we release into open-source as gensim. Written in Python, it still manages to get top performance thanks to the use of Python\u2019s NumPy library with fast BLAS calls under the hood."}, {"heading": "Acknowledgments", "text": "This study was partially supported by the LC536 grant of MS\u030cMT C\u030cR."}, {"heading": "A Streamed Stochastic Eigen Decomposition", "text": "Algorithm 1: Two-pass Stochastic Decomposition in Constant Memory with Streamed Input\nInput: m\u00d7 n input matrix A, presented as a stream of observation chunks A = [C1, C2, . . . , CC ]. Truncation factor k. Oversampling factor l. Number of power iterations q. Output: U , S2 spectral decomposition of A (i.e, US2UT = AAT ) truncated to the k greatest factors. Data: Intermediate matrices require O(m(k + l)) memory; in particular, the algorithm avoids materializing any O(n) or O(m2) matrices.\n// Construct the m\u00d7 (k + l) sample matrix Y = AO, in one pass over the input stream.\nY \u2190 sum(CiOi for Ci in A) ; // each Oi is a random |Ci| \u00d7 (k + l) gaussian matrix\n// Run q power iterations to improve accuracy (optional), Y = (AAT )qAO. Needs q extra passes.\nfor iteration \u2190 1 to q do Y \u2190 sum(Ci(CTi Y ) for Ci in A);\n// Construct the m\u00d7 (k + l) orthonormal action matrix Q, in-core. Q \u2190 orth(Y );\n// Construct (k + l)\u00d7 (k + l) covariance matrix X = BBT in one pass, where B = QTA.\nX \u2190 sum((QTCi)(QTCi)T for Ci in A) ; // BLAS rank-k update routine SYRK\n// Compute U , S by means of the small (k + l)\u00d7 (k + l) matrix X . UX , SX \u2190 eigh(X);\n// Go back from the eigen values of X to the eigen values of B (= eigen values of A). S2 \u2190 first k values of \u221a SX ; U \u2190 first k columns of QUX ;"}, {"heading": "B Wikipedia LSA Topics", "text": "First ten topics coming from the P2 decomposition with three power iterations and 400 extra samples. The top ten topics are apparently dominated by meta-topics of Wikipedia administration and by robots importing large databases of countries, films, sports, music etc.\nTopic i Singular Ten most salient words for topic i, with their weights\nvalue si 1. 201.118 -0.474*\u201cdelete\u201d + -0.383*\u201cdeletion\u201d + -0.275*\u201cdebate\u201d + -0.223*\u201ccomments\u201d + -\n0.220*\u201cedits\u201d + -0.213*\u201cmodify\u201d + -0.208*\u201cappropriate\u201d + -0.194*\u201csubsequent\u201d +\n-0.155*\u201cwp\u201d + -0.117*\u201cnotability\u201d\n2. 143.479 0.340*\u201cdiff\u201d + 0.325*\u201clink\u201d + 0.190*\u201cimage\u201d + 0.179*\u201cwww\u201d + 0.169*\u201cuser\u201d\n+ 0.157*\u201cundo\u201d + 0.154*\u201ccontribs\u201d + -0.145*\u201cdelete\u201d + 0.116*\u201calbum\u201d + -\n0.111*\u201cdeletion\u201d\n3. 136.235 0.421*\u201cdiff\u201d + 0.386*\u201clink\u201d + 0.195*\u201cundo\u201d + 0.182*\u201cuser\u201d + -0.176*\u201cimage\u201d\n+ 0.174*\u201cwww\u201d + 0.170*\u201ccontribs\u201d + -0.111*\u201calbum\u201d + 0.105*\u201cadded\u201d + -\n0.101*\u201ccopyright\u201d\n4. 125.436 0.346*\u201cimage\u201d + -0.246*\u201cage\u201d + -0.223*\u201cmedian\u201d + -0.208*\u201cpopulation\u201d +\n0.208*\u201ccopyright\u201d + -0.200*\u201cincome\u201d + 0.190*\u201cfair\u201d + -0.171*\u201ccensus\u201d + -\n0.168*\u201ckm\u201d + -0.165*\u201chouseholds\u201d\n5. 117.243 0.317*\u201cimage\u201d + -0.196*\u201cplayers\u201d + 0.190*\u201ccopyright\u201d + 0.176*\u201cmedian\u201d\n+ 0.174*\u201cage\u201d + 0.173*\u201cfair\u201d + 0.155*\u201cincome\u201d + 0.144*\u201cpopulation\u201d + -\n0.134*\u201cfootball\u201d + 0.129*\u201chouseholds\u201d\n6. 100.451 -0.504*\u201cplayers\u201d + -0.319*\u201cfootball\u201d + -0.284*\u201cleague\u201d + -0.194*\u201cfootballers\u201d\n+ -0.141*\u201cimage\u201d + -0.132*\u201cseason\u201d + -0.117*\u201ccup\u201d + -0.113*\u201cclub\u201d + -\n0.110*\u201cbaseball\u201d + -0.103*\u201cf\u201d\n7. 92.376 0.411*\u201calbum\u201d + 0.275*\u201calbums\u201d + 0.217*\u201cband\u201d + 0.215*\u201csong\u201d +\n0.184*\u201cchart\u201d + 0.164*\u201csongs\u201d + 0.160*\u201csingles\u201d + 0.149*\u201cvocals\u201d + 0.139*\u201cgui-\ntar\u201d + 0.129*\u201ctrack\u201d\n8. 84.024 0.246*\u201cwikipedia\u201d + 0.183*\u201ckeep\u201d + -0.179*\u201cdelete\u201d + 0.167*\u201carticles\u201d +\n0.153*\u201cyour\u201d + 0.150*\u201cmy\u201d + -0.141*\u201cfilm\u201d + 0.129*\u201cwe\u201d + 0.123*\u201cthink\u201d +\n0.121*\u201cuser\u201d\n9. 79.548 word \u201ccategory\u201d in ten different languages (and their exotic un-TEX-able scripts)\n10. 79.074 -0.587*\u201cfilm\u201d + -0.459*\u201cfilms\u201d + 0.129*\u201calbum\u201d + 0.127*\u201cstation\u201d + -\n0.121*\u201ctelevision\u201d + -0.119*\u201cposter\u201d + -0.112*\u201cdirected\u201d + -0.109*\u201cactors\u201d +\n0.095*\u201crailway\u201d + -0.085*\u201cmovie\u201d"}], "references": [{"title": "Tracking a few extreme singular values and vectors in signal processing", "author": ["Comon", "Golub", "P. 1990] Comon", "G. Golub"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Comon et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Comon et al\\.", "year": 1990}, {"title": "Generalized hebbian algorithm for incremental Latent Semantic Analysis", "author": ["Gorrell", "Webb", "G. 2005] Gorrell", "B. Webb"], "venue": "In Ninth European Conference on Speech Communication and Technology", "citeRegEx": "Gorrell et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gorrell et al\\.", "year": 2005}, {"title": "Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions", "author": ["Halko et al", "N. 2009] Halko", "P. Martinsson", "J. Tropp"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Sequential Karhunen\u2013Loeve basis extraction and its application to images", "author": ["Levy", "Lindenbaum", "A. 2000] Levy", "M. Lindenbaum"], "venue": "IEEE Transactions on Image processing,", "citeRegEx": "Levy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2000}, {"title": "On updating problems in Latent Semantic Indexing", "author": ["Zha", "Simon", "H. 1999] Zha", "H. Simon"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Zha et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Zha et al\\.", "year": 1999}], "referenceMentions": [], "year": 2016, "abstractText": "With the explosion of the size of digital dataset, the limiting factor for decomposition algorithms is the number of passes over the input, as the input is often stored out-of-core or even off-site. Moreover, we\u2019re only interested in algorithms that operate in constant memory w.r.t. to the input size, so that arbitrarily large input can be processed. In this paper, we present a practical comparison of two such algorithms: a distributed method that operates in a single pass over the input vs. a streamed two-pass stochastic algorithm. The experiments track the effect of distributed computing, oversampling and memory trade-offs on the accuracy and performance of the two algorithms. To ensure meaningful results, we choose the input to be a real dataset, namely the whole of the English Wikipedia, in the application settings of Latent Semantic Analysis.", "creator": "LaTeX with hyperref package"}}}