{"id": "1104.3929", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2011", "title": "Understanding Exhaustive Pattern Learning", "abstract": "Pattern learning in an important problem in Natural Language Processing (NLP). Some exhaustive pattern learning (EPL) methods (Bod, 1992) were proved to be flawed (Johnson, 2002), while similar algorithms (Och and Ney, 2004) showed great advantages on other tasks, such as machine translation. In this article, we first formalize EPL, and then show that the probability given by an EPL model is constant-factor approximation of the probability given by an ensemble method that integrates exponential number of models obtained with various segmentations of the training data. This work for the first time provides theoretical justification for the widely used EPL algorithm in NLP, which was previously viewed as a flawed heuristic method. Better understanding of EPL may lead to improved pattern learning algorithms in future.\n\n\n\n\nNLP can be used to improve understanding of human language learning and improve performance of both natural language processing and natural language processing. In this paper, we show how EPL can improve the accuracy and accuracy of many data processing tasks and understand the underlying processes of learning in NLP.\n\nIn the last two papers, we present how we can increase the accuracy and accuracy of several of the various EPL training methods.", "histories": [["v1", "Wed, 20 Apr 2011 02:49:59 GMT  (26kb)", "http://arxiv.org/abs/1104.3929v1", "15 pages, 3 figures"]], "COMMENTS": "15 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["libin shen"], "accepted": false, "id": "1104.3929"}, "pdf": {"name": "1104.3929.pdf", "metadata": {"source": "CRF", "title": "Understanding Exhaustive Pattern Learning", "authors": ["Libin Shen"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 4.\n39 29\nv1 [\ncs .A\nI] 2\n0 A\npr 2"}, {"heading": "1. Introduction", "text": "Pattern learning is the crux of many natural language processing (NLP) problems. It is usually solved as grammar induction for these problems. For parsing, we learn a statistical grammar with respect to certain linguistic formalism, such as Context Free Grammar (CFG), Dependency Grammar (DG), Tree Substitution Grammar (TSG), Tree Adjoining Grammar (TAG), and Combinatory Categorial Grammar (CCG) etc. For machine translation (MT), we learn a bilingual grammar that transfer a string or tree structure in a source language into a corresponding string or tree structure in a target language.\nWhat is embarrassing is that many of the grammar induction algorithms that provide stateof-the-art performance are usually regarded as less principled in the aspect of statistical modeling. Johnson (2002); Prescher et al. (2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them. Similar heuristic methods have also been used in many other pattern learning tasks, for example, like semantic parsing as in Zettlemoyer and Collins (2005) and chunking as in Daume\u0301 III and Marcu (2005) in an implicit way.\nIn all these heuristic algorithms, one needs to extract overlapping structures from training data in an exhaustive way. Therefore, in the article, we call them exhaustive pattern learning (EPL) methods. The use of EPL methods is intended to cope with the uncertainty of building blocks used in statistical models. As far as MT is concerned, Koehn et al. (2003) found that it was better to define a translation model on phrases than on words, but there was no obvious way to define what phrases\nwere. DeNero et al. (2006) observed that exhaustive pattern learning outperforms generative models with fixed building blocks.\nIn EPL algorithms, one needs to collect statistics of overlapping structures from training data, so that they are not valid generative models. Thus, the EPL algorithms for grammar induction were viewed as heuristic methods DeNero et al. (2006); Daume\u0301 III (2008). Recently, DeNero et al. (2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al. (2009); Post and Gildea (2009) investigated various sampling methods for grammar induction, which were believed to be more principled than EPL. However, there was no convincing empirical evidence showing that these new methods provided better performance on large-scale data sets.\nIn this article, we will show that there exists a mathematically sound explanation for the EPL approach. We will first introduce a likelihood function based on ensemble learning, which marginalizes all possible building block segmentations on the training data. Then, we will show that the probability given by an EPL grammar is constant-factor approximation of an ensemble method that integrates exponential number of models. Therefore, with an EPL grammar induction algorithm, we learn a model with much more diversity from the training data. This may explain why EPL methods provide state-of-the-art performance in many NLP pattern learning problems.\nThe rest of the article is organized as follows. We will first formalize EPL in Section 2. In Section 3, we introduce the ensemble method, and then show the approximation theorem and its corollaries. We discuss a few important problems in Section 4, and conclude our work in Section 5."}, {"heading": "2. Formalizing Exhaustive Pattern Learning", "text": "For the purpose of formalizing the core idea of EPL, we hereby introduce a task called monotonic translation. Analysis on this task can be extended to other pattern learning problems. Then, we will define segmentation on training data, and introduce the EPL grammar, which will later be used in Section 3, theoretical justification of EPL."}, {"heading": "2.1 Monotonic Translation", "text": "Monotonic translation is defined as follows. The input x \u2208 X is a string of words x1x2...xi in the source language. The monotonic translation of x is y \u2208 Y , a string of words, y1y2...yi, of the same length in the target language, where yj is the translation of xj , 1 \u2264 j \u2264 i.\nIn short, monotonic translation is a simplified version of machine translation. There is no word reordering, insertion or deletion. In this way, we ignore the impact of word level alignment, so as to focus our effort on the study of building blocks. We leave the incorporation of alignments for future work. In fact, we can simply view alignments as constraints on building blocks. Monotonic translation is already general enough to model many NLP tasks such as labelling and chunking."}, {"heading": "2.2 Training Data Segmentation and MLE Grammars", "text": "Without losing generality, we assume that the training data D contains a single pair of word strings, xD and yD, which could be very long. Let xD = x1x2...xn, and yD = y1y2...yn. Source word xi is aligned to target word yi. Let the length of the word strings be |D| = n. Figure 1 shows a simple example of training data. Here |D| = 4.\nWe assume that there exists a hidden segmentation on the training data, which segments xD and yD into tokens. A token consists of a string of words, either on source or target, and it contains\nat least one word. As for monotonic translation, the source side and the target side share the same topology of segmentation. Tokens are the building blocks of the statistical model to be presented, which means that the parameters for the model are defined on tokens instead of words.\nA segmentation sD of D, or s for short, is represented as a vector of n \u2212 1 Boolean values, s1s2...sn\u22121. si = 0 if and only if xi and xi+1 belong to the same token. s applies onto both the source and the target in the same way, which means xi and xi+1 belong to the same token if and only if yi and yi+1 belong to the same token.\nIf we segment D with s, we obtain a tokenized training set Ds. Ds contains a pair of token strings \u3008us,vs\u3009. us = u1u2...u|Ds|, and vs = v1v2...v|Ds|, where |Ds| is the total number of tokens in us or vs. Figure 2 shows an example of segmentation on training data. Here, s2 = 0, so that we have a token pair that spans two words, (u2, v2) = (LEFT FOR, went to).\nGiven training data D and a segmentation s on D, there is a unique joint probabilistic model obtained by the MLE on Ds. Each parameter of this model contains a source token and target token. Since each token represents a string of words, we call this model a string-to-string grammar GDs. Specifically, for any pair of tokens (u, v), we have\nPr(u, v|GDs) = #s(u, v)\n|Ds| , (1)\nwhere #s(u, v) is the number of times that this token pair appears in the segmented data Ds. As for the example segmentation s in Figure 2, its MLE grammar is simply as follows.\nPr(SOPHIE, sophia |GDs) = 1/3\nPr(LEFT FOR, went to |GDs) = 1/3\nPr(PHILLY, philadelphia |GDs) = 1/3\nHowever, for any given training data, its segmentation is unknown to us. One way to cope with this problem is to consider all possible segmentations. String distribution on the training data will lead us to a good estimation of the hidden segmentation and tokens. In Section 3, we will introduce an ensemble method to incorporate MLE grammars obtained from all possible segmentations. Segmentations are generated with certain prior distribution."}, {"heading": "2.3 Exhaustive Pattern Learning for Monotonic Translation", "text": "Now we present an EPL solution. We follow the widely-used heuristic method to generate a grammar by applying various segmentations at the same time. We build a heuristic grammar GD,d out of the training data D by counting all possible token pairs (u, v) with at most d words on each side, where d \u226a |D| is a given parameter\nPr(u, v|GD,d) = #(u, v)\nZd ,\nwhere #(u, v) is the number of times that the string pair encoded in (u, v) appears in D,1 and\nZd = \u2211\n(u\u2032,v\u2032)\n#(u\u2032, v\u2032) = \u2211\ni=1...d\n(|D| \u2212 i+ 1) = (1\u2212 d\u2212 1\n2|D| )d|D|.\nTherefore,\nPr(u, v|GD,d) = #(u, v)\n(1\u2212 d\u221212|D|)d|D| (2)\nFor example, the heuristic grammar for the training data in Figure 1 is as follows if we set d = 2.\nPr(SOPHIE, sophia |GD,2) = 1/7\nPr(LEFT, went |GD,2) = 1/7\nPr(FOR, to |GD,2) = 1/7\nPr(PHILLY, philadelphia |GD,2) = 1/7\nPr(SOPHIE LEFT, sophia went |GD,2) = 1/7\nPr(LEFT FOR, went to |GD,2) = 1/7\nPr(FOR PHILLY, to philadelphia |GD,2) = 1/7\nA desirable translation rule \u2018LEFT FOR \u21d2 went to\u2019 is in this heuristic grammar, although its weight is diluted by noise. The hope is that, good translation rules will appear more often in the training data, so that they can be distinguished from noisy rules.\nIn the decoding phase, we use grammar GD,d as if it is a regular MLE grammar. Let x = x1x2...xi be an input source string. For any segmentation a on the test sentence x, let ua = u1u2...uk be the resultant string of source tokens. The length of the string is |x| = i, and the length of the token string is |ua| = k. The translation that we are looking for is given by the target token vector v\u0302, such that\n\u3008v\u0302, a\u0302\u3009 = argmax \u3008v,a\u3009 Pr(ua,v|GD,d), where\nPr(ua,v|GD,d) = \u220f\nj=1...|ua|\nPr(uj, vj |GD,d) (3)\n= \u220f\nj=1...|ua|\nmj\n(1\u2212 d\u221212|D|)d|D|\n1. For the sake of convenience, in the rest of this article, we no longer distinguish a token and the string contained in this token unless necessary. We use symbols u and v to represent both. The meaning is clear in context.\nwhere mj = #(uj , vj). As in previous work of structure-based MT, we do not calculate the marginal probability that sums up all possible target tokens generating the same word string, due to the concern of computational complexity.\nObviously, with GD,d, we can take advantage of larger context of up to d words. However, a common criticism against the EPL approach is that a grammar like GD,d is not mathematically sound. The probabilities are simply heuristics, and there is no clear statistical explanation. In the next section, we will show that GD,d is mathematically sound."}, {"heading": "3. Theoretical Justification of Exhaustive Pattern Learning", "text": "In this section, we will first introduce an ensemble model and a prior distribution of segmentation. Then we will show the theorem of approximation, and present corollaries on conditional probabilities and tree structures."}, {"heading": "3.1 An Ensemble Model", "text": "Let D be the training data of |D| words. Let s be an arbitrary token segmentation on D, where s is unknown to us. Given D and s, we can obtain a model/grammar GDs with maximum likelihood estimation. Thus, we can calculate joint probability of (uj , vj) given grammar GDs, Pr(uj, vj |GDs).\nThere are potentially exponential number of distinct segmentations for D. Here, we use an ensemble method to sum over all possible segmentations. This method would provide desirable coverage and diversity of translation rules to be learned from the training data. For each segmentation s, we have a fixed prior probability Pr(s) which we will shown in Section 3.2. Thus, we define the ensemble probability L(uj, vj) as follows.\nL(uj, vj) = \u2211\ns\nPr(uj, vj |GDs)Pr(s). (4)\nPrior segmentation probabilities Pr(s) serve as model probabilities in (4). Having the model probabilities fixed in this way could avoid over-fitting of the training data DeNero et al. (2006).\nIn decoding, we search for the best hypothesis v\u0302 given training data D and input x as follows.\n\u3008v\u0302, a\u0302\u3009 = argmax \u3008v,a\u3009 L(ua,v), where\nL(ua,v) = \u220f\nj=1...|ua|\nL(uj , vj)\nWhat is interesting is that there turns out to be a prior distribution for s, such that, under certain conditions, the limit of L(ua,v)/Pr(ua,v|GD,d) as |D| \u2192 \u221e is a value that depends only on |x| and a parameter of the prior distribution Pr(s), to be shown in Theorem 3. |x| is a constant for all hypotheses for the same input. Therefore, Pr(ua,v|GD,d) is constant-factor approximation of L(ua,v). Using GD,d is, to some extent, equivalent to using all possible MLE grammars at the same time via an ensemble method."}, {"heading": "3.2 Prior Distribution of Segmentation", "text": "Now we define a probabilistic model to generate segmentation. s = \u3008s1, s2, ..., s|D|\u22121\u3009 is a vector of |D| \u2212 1 independent Bernoulli variables. si represents whether xi and xi+1 belong to separated\ntokens. 1 means yes and 0 means no. All the individual separating variables have the same distribution, Pq(si = 0) = q and Pq(si = 1) = 1 \u2212 q, for a given real value q, 0 \u2264 q \u2264 1. Since L(ua,v) depends on q now, we rewrite it as Lq(ua,v).\nBased on the definition, Lemma 1 immediately follows, which will be used later.\nLemma 1 For each string pair (u, v), the probability that an appearance of (u, v) in D is exactly tokenized as u and v by s is q|u|\u22121(1\u2212 q)2."}, {"heading": "3.3 Theorem of Approximation", "text": "Let x = x1x2...xi be an input source string. Let a be a segmentation on x, and the resultant token string be ua = u1u2...uk. Let v = v1v2...vk be a hypothesis translation of ua. Let mj = #(uj , vj), the number of times that string pair (uj , vj) appears in the training data D, 1 \u2264 j \u2264 k. Let mj,s = #s(uj , vj), the number of times that this token pair appears in the segmented data Ds. In order to prove Theorem 2, we assume that the following two assumptions are true for any pair of tokens (uj , vj).\nAssumption 1 Any two of the mj appearances in D are neither overlapping nor consecutive.\nThis assumption is necessary for the calculation of E[mj,s], 1 \u2264 j \u2264 k. Based on Lemma 1, the number of times that (uj , vj) is exactly tokenized as in this way with segmentation s is in a binomial distribution B(mj, q|uj |\u22121(1\u2212 q)2), so that\nE[mj,s] = mjq |uj |\u22121(1\u2212 q)2,\nwhere |uj | is the number of words in uj . In addition, since there is no overlap, these appearances cover a total of |uj |mj source words.\nAssumption 2 Let \u03b7j = (|uj |+1)mj\n|D| . We have lim|D|\u2192\u221e \u03b7j = 0.\nIn fact, as we will see it in Section 4.1, we do not have to rely on Assumption 2 to bound the ratio of Pr(ua,v|GD,d) and Lq(ua,v). We know that \u03b7j is a very small positive number, and we can build the upper and lower bounds of the ratio based on \u03b7j . However, with this assumption, it will be much easier to see the big picture, so we assume that it is true in the rest of this section.\nTheorem 2 Suppose Assumptions 1 and 2 hold for a given pair of tokens (uj , vj), then we have\nlim |D|\u2192\u221e\nLq(uj , vj)\nPr(uj , vj |GD,d) = q|uj |,\nwhere q = d/(d+ 1).\nLater in the section, we will show Theorem 2 with Lemmas 4 and 5. Theorem 3 immediately follows Theorem 2.\nTheorem 3 Suppose Assumptions 1 and 2 hold for any j, then we have\nlim |D|\u2192\u221e\nLq(ua,v)\nPr(ua,v|GD,d) = q|x|,\nwhere q = d/(d+ 1).\nHere, |x| is a constant for hypotheses of the same input. An interesting observation is that the prior segmentation model to fit into this theorem tends to generate longer tokens, if we have a larger value for d.\nWe will show Theorem 2 by bounding it from above and below via Lemmas 4 and 5 respectively. Now, we introduce the notations to be use the proofs of Lemmas 4 and 5 in Appendixes A and B respectively.\nFirst, we combine (1) and (4), and obtain\nLq(uj , vj) = Es( mj,s |Ds| ). (5)\nWith Assumption 1, we know the value of E[mj,s]. However, |Ds| depends on mj,s, and this prevents us from computing the expected value on each individual item.\nWe solve it by bounding |Ds| with values independent of mj,s, or the separating variables related to the mj appearances in D. We divide D into two parts, H and I , based on the mj appearances of (uj , vj) pairs. H is the part that contains and only contains all mj appearances, and I is the rest of D, so that the internal separating variables of I are independent of mj,s. An example is shown in Figure 3. Black boxes represent the mj appearances.\nWe concatenate fragments in I and keep the I-internal separating variables as in s. There are two variants of the segmentation for I , depending on how we define the separating variables between the fragments. So we have the following two segmented sub-sets.\n\u2022 I s0(I): inter-fragment separating variable = 0.\n\u2022 I s1(I): inter-fragment separating variable = 1.\nHere, s0(I) and s1(I) represent the two segmentation vectors on I respectively, each of which has |I| \u2212 mj \u2212 1 changeable separating variables, where |I| is the number words contained in I . The number of changeable variables that set to 1 follows a binomial distribution B(|I|\u2212mj \u2212 1, 1\u2212 q). In Figure 3, fixed inter-fragment separating variables are represented in the bold italic font.\nIf there are s changeable variables set to 1 in I s0(I), the number of tokens in Is0(I) is |Is0(I)| = s+ 1. Similarly, if there are s changeable variables set to 1 in I s1(I), the number of tokens in Is1(I) is |I s1(I)| = s+mj + 1.\nIn addition, it is easy to verify that\n|I s0(I)| \u2264 |Ds| (6)\n|Ds| \u2264 |Is1(I)|+ |uj |mj (7)\nCombining (5), (6) and the two assumptions, we have the upper bound in Lemma 4.\nLemma 4 If Assumptions 1 and 2 hold,\nlim |D|\u2192\u221e\nLq(uj , vj)\nPr(uj , vj |GD,d) \u2264 q|uj |,\nwhere q = d/(d+ 1).\nSimilarly, combining (5), (7) and the two assumptions, we obtain the lower bound in Lemma 5.\nLemma 5 If Assumptions 1 and 2 hold,\nlim |D|\u2192\u221e\nLq(uj , vj)\nPr(uj , vj |GD,d) \u2265 q|uj |,\nwhere q = d/(d+ 1).\nThe proofs of Lemmas 4 and 5 are given in the Appendixes A and B respectively. The proof of Lemma 4 also depends on Lemma 8. Lemma 8 and its proof are given in Appendix C.\nTherefore, Theorem 2 holds."}, {"heading": "3.4 Corollaries on Conditional Probabilities", "text": "Theorem 2 is for joint distribution of token pairs. In previous work of using EPL, conditional probabilities were ofter used, for example, like P (u|v) and P (v|u). Starting from Theorem 2, we can easily obtain the following corollaries for conditional probabilities.\nCorollary 6 Suppose Assumptions 1 and 2 hold for a given pair of tokens (uj , vj), then we have\nlim |D|\u2192\u221e Pr(uj |vj , GD,d)/ Lq(uj , vj)\u2211 u Lq(u, vj) = 1,\nwhere q = d/(d+ 1).\nProof According to the definition, Pr(u, vj |GD,d) = 0 and Lq(u, vj) = 0, if |u| 6= |vj |. Therefore, we only need to consider all pairs of (u, vj), such that |u| = |vj | = |uj |. The number of distinct u is a finite number, since source vocabulary is a finite set. Therefore, according to Theorem 2, for any small positive number \u01eb, there exists a positive number n, such that, if |D| > n, we have\n(1\u2212 \u01eb) Lq(u, vj)\nq|uj | \u2264 Pr(u, vj|GD,d) \u2264 (1 + \u01eb)\nLq(u, vj)\nq|uj | (8)\nTherefore, we have\nPr(uj |vj , GD,d) = Pr(uj, vj |GD,d)\u2211\nu:|u|=|uj| Pr(u, vj |GD,d)\n\u2264 (1 + \u01eb)Lq(u, vj)/q\n|uj |\n\u2211 u:|u|=|uj| (1\u2212 \u01eb)Lq(u, vj)/q|uj | {Eqn. (8)}\n= 1 + \u01eb\n1\u2212 \u01eb Lq(uj , vj)\u2211 u Lq(u, vj)\nThus,\nPr(uj|vj , GD,d)/ Lq(uj , vj)\u2211 u Lq(u, vj) \u2264 1 + \u01eb 1\u2212 \u01eb\nSimilarly, we have\nPr(uj|vj , GD,d)/ Lq(uj , vj)\u2211 u Lq(u, vj) \u2265 1\u2212 \u01eb 1 + \u01eb\nTherefore,\nlim |D|\u2192\u221e Pr(uj|vj , GD,d)/ Lq(uj , vj)\u2211 u Lq(u, vj) = 1\nCorollary 7 Suppose Assumptions 1 and 2 hold for a given pair of tokens (uj , vj), then we have\nlim |D|\u2192\u221e Pr(vj|uj , GD,d)/ Lq(uj , vj)\u2211 v Lq(uj , v) = 1,\nwhere q = d/(d+ 1).\nThe proof of Corollary 7 is similar to that of Corollary 6. Therefore, conditional probabilities in EPL are reasonable approximation of the conditional ensemble probability functions.\nThe proofs for the conditional probabilities depend on a special property of monotonic translation; the length of uj is the same as the length of vj . However, this is not true in real application of phrase-based translation. The source and the target sides may have different segmentations. We leave the modeling of real phrase-based translation for future work."}, {"heading": "3.5 Extension to Tree Structures", "text": "Now we try to extend Theorem 2 to the string-to-tree grammar. First, we define a prior distribution on tree segmentation. We assign a Bernoulli variable to each tree node, representing the probabilities that we separate the tree at this node, i.e, with probabilities of 1 \u2212 q, we choose to separate each node.\nLet (uj , vj) be a string\u2013tree pair, where uj is a source string and vj is a target tree. Let tj be the number of words in uj , and let nj be the number of non-terminals in uj , where tj + nj \u2264 d, and\u2211\ntj is the length of the input sentence, |x|. Thus, the probability that an appearance of (uj , vj) in D is exactly tokenized as in this way is qtj\u22121(1\u2212 q)nj+1.\nWith similar methods used in the proofs for string structures, we can show that, if Assumptions 1 and 2 hold,\nlim |D|\u2192\u221e\nLq(uj , vj)\nPr(uj, vj |GD,d) = c qtj\u22121(1\u2212 q)nj ,\nwhere c = \u2211\ni: ti+ni\u2264d #(ui, vi)/|D| is a constant, and q is a free parameter. We skip the proof\nhere to avoid duplication of similar procedure. We define\nPq,d(uj , vj) = c q tj\u22121(1\u2212 q)njPr(uj, vj |GD,d).\nThus, Pq,d(uj , vj) approximates Lq(uj , vj), where\nlim |D|\u2192\u221e\nLq(uj , vj)\nPq,d(uj , vj) = 1.\nThis result shows a theoretically better way of using heuristic grammar in string-to-tree models."}, {"heading": "4. Discussion", "text": "In this section, we will focus on three facts that need more explanation."}, {"heading": "4.1 On the Use of Assumption 2", "text": "In the proofs of Lemmas 4 and 5, Assumption 2 is only used in the very last steps. Therefore, we could build the upper and lowers bounds of the ratio without Assumption 2 by connecting Inequalities (12) and (13) in Appendixes A and B respectively."}, {"heading": "4.2 On the Ensemble Probability", "text": "The ensemble probability in (4) can be viewed as simplification of a Bayesian model in (9).\nL(uj , vj) = Pr(uj, vj |D)\n= \u2211\nG\u2208G(D)\nPr(uj, vj |G)Pr(G|D) (9)\nIn (9), we marginalize all possible token-based grammars G from D, G(D). Furthermore,\nPr(G|D) = \u2211\ns\nPr(G|D, s)Pr(s|D)\nThen, we approximate the posterior probability of G given D and s with point estimation. Thus, Pr(G|D, s) = 1 if and only if G is the MLE grammar of Ds, which means all the distribution mass is assigned to GDs, the MLE grammar for Ds. We also assume that s is independent of D. Thus,\nPr(G|D) = \u2211\ns\n1(G = GDs)Pr(s), (10)\nwhere Pr(s) is a prior distribution of segmentation for any string of |D| words. With (10), we can rewrite (9) as follows.\nL(uj , vj) = \u2211\nG\u2208G(D)\nPr(uj, vj |G) \u2211\ns\n1(G = GDs)Pr(s)\n= \u2211\ns\n\u2211\nG\u2208G(D)\nPr(uj , vj |G)1(G = GDs)Pr(s)\n= \u2211\ns\nPr(uj, vj |GDs)Pr(s) (11)\nEquation (11) is exactly the ensemble probability in Equation (4)."}, {"heading": "4.3 On the DOP Model", "text": "The EPL method investigated in this article may date back to Data Oriented Parsing (DOP) by Bod (1992). What is special with DOP is that the DOP model uses overlapping treelets of various sizes in an exhaustive way as building blocks of a statistical tree grammar.\nIn our framework, for each pair (uj , vj), we can use uj to represent the input text, and vj to represent its tree structure. Thus, it would be similar to the string-to-tree model in Section 3.5. Joint probability of (uj , vj) stands for unigram probability Pr(treelet).\nHowever, the original DOP estimator (DOP1) is quite different from our monotonic translation model. The conditional probability in DOP1 is defined as Pr(treelet|subroot-label), so that there is no obvious way to model DOP1 with monotonic translation. Therefore, theoretical justification of DOP1 is still an open problem."}, {"heading": "5. Conclusion", "text": "In this article, we first formalized exhaustive pattern learning (EPL), which is widely used in grammar induction in NLP. We showed that using an EPL heuristic grammar is equivalent to using an ensemble method to cope with the uncertainty of building blocks of statistical models.\nBetter understanding of EPL may lead to improved pattern learning algorithms in future. This work will affect the research in various fields of natural language processing, including machine translation, parsing, sequence classification etc. EPL can also been applied to other research fields outside NLP."}, {"heading": "Acknowledgments", "text": "This work was inspired by enlightning discussion with Scott Miller, Rich Schwartz and Spyros Matsoukas when the author was at BBN Technologies. Reviewers of ACL 2010, CoNLL 2010, EMNLP 2010, ICML 2011, CLJ and JMLR helped to sharpen the focus of this work. However, all the mistakes belong to me."}, {"heading": "Appendix A. Proof for Lemma 4", "text": "Lq(uj, vj) = Es[ mj,s |Ds| ] {Eqn. (5) }\n\u2264 Es[ mj,s |I s0(I)| ] {Eqn. (6) } = E[mj,s]E[ 1\n|I s0(I)|\n]\n{Independence of mj,s and Is0(I)}\n= E[mj,s]Es0(I)[ 1\n|I s0(I)|\n]\n= E[mj,s]\n(1\u2212 q)(|I| \u2212mj) (1\u2212 q|I|\u2212mj) {Lemma 8}\n\u2264 E[mj,s]\n(1\u2212 q)(|I| \u2212mj)\n= mjq\n|uj |\u22121(1\u2212 q)2\n(1\u2212 q)(|I| \u2212mj) {Binomial Dist., Assumption 1}\n= mjq\n|uj |\u22121(1\u2212 q)2\n(1\u2212 q)(|D| \u2212 |uj |mj \u2212mj)\n= mjq\n|uj |\u22121(1\u2212 q)2\n(1\u2212 q)(1 \u2212 \u03b7j)|D|\n= mj\n(1\u2212 d\u221212|D|)d|D|\nq|uj|\u22121(1\u2212 q)2(1\u2212 d\u221212|D|)d\n(1\u2212 q)(1\u2212 \u03b7j)\n= Pr(uj, vj |GD,d) q|uj |\u22121(1\u2212 q)2(1\u2212 d\u221212|D|)d\n(1\u2212 q)(1\u2212 \u03b7j)\n= Pr(uj, vj |GD,d)q |uj |\n(1\u2212 q)(1 \u2212 d\u221212|D|)d\n(1\u2212 \u03b7j)q (12)\nlim |D|\u2192\u221e\nLq(uj , vj)\nPr(uj, vj |GD,d)\n\u2264 q|uj | (1\u2212 q)d\nq {Assumption 2}\n= q|uj |."}, {"heading": "Appendix B. Proof for Lemma 5", "text": "Lq(uj , vj) = Es[ mj,s |Ds| ] {Eqn. (5) }\n\u2265 Es[ mj,s\n|I s1(I)|+ |uj |mj\n] {Eqn. (7)}\n= E[mj,s]E[ 1\n|I s1(I)|+ |uj|mj\n]\n{Independence of mj,s and Is1(I)}\n\u2265 E[mj,s]\nE[|I s1(I)|+ |uj |mj ]\n{Jensen\u2019s inequality}\n= E[mj,s]\nE s1(I)[|Is1(I)|] + |uj |mj\n= mjq\n|uj |\u22121(1\u2212 q)2\n(1\u2212 q)(|I| \u2212 1\u2212mj) +mj + 1 + |uj |mj {Binomial Dist., Assumption 1}\n= mjq\n|uj |\u22121(1\u2212 q)2\n(1\u2212 q)(|D| \u2212 |uj |mj) + q(1 +mj) + |uj |mj\n= mjq\n|uj|\u22121(1\u2212 q)2\n(1\u2212 q)|D|+ q(|uj |mj +mj) + q\n= mjq\n|uj |\u22121(1\u2212 q)2\n(1\u2212 q + q\u03b7j + q |D|)|D|\n= mj\n(1\u2212 d\u221212|D|)d|D|\nq|uj |\u22121(1\u2212 q)2(1\u2212 d\u221212|D|)d\n1\u2212 q + q\u03b7j + q |D|\n= Pr(uj, vj |GD,d) q|uj |\u22121(1\u2212 q)2(1\u2212 d\u221212|D|)d\n1\u2212 q + q\u03b7j + q |D|\n= Pr(uj, vj |GD,d)q |uj |\n(1\u2212 q)2(1\u2212 d\u221212|D|)d\n(1\u2212 q + q\u03b7j + q |D|)q\n(13)\nlim |D|\u2192\u221e\nLq(uj , vj)\nPr(uj, vj |GD,d)\n\u2265 q|uj | (1\u2212 q)d\nq {Assumption 2}\n= q|uj |."}, {"heading": "Appendix C. Lemma 8 and its Proof", "text": "Lemma 8 Let X be a random variable of Binomial distribution B(n, 1\u2212 q), then\nE[ 1\nX + 1 ] =\n1\u2212 qn+1\n(1\u2212 q)(n+ 1)\nE[ 1\nX + 1 ] =\nn\u2211\nk=0\n1\nk + 1\nn!\nk!(n\u2212 k)! qn\u2212k(1\u2212 q)k\n= 1\n(1\u2212 q)(n + 1) n\u2211\nk=0\n(n+ 1)!\n(k + 1)!(n \u2212 k)! qn\u2212k(1\u2212 q)k+1\n= 1\n(1\u2212 q)(n + 1) ((q + 1\u2212 q)n+1 \u2212 qn+1)\n= 1\u2212 qn+1\n(1\u2212 q)(n + 1)"}], "references": [{"title": "A gibbs sampler for phrasal synchronous grammar induction", "author": ["Phil Blunsom", "Trevor Cohn", "Chris Dyer", "Miles Osborne"], "venue": "In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Blunsom et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2009}, {"title": "A computational model of language performance: Data Oriented Parsing", "author": ["Rens Bod"], "venue": "In Proc. of COLING92,", "citeRegEx": "Bod.,? \\Q1992\\E", "shortCiteRegEx": "Bod.", "year": 1992}, {"title": "Non-projective parsing for statistical machine translation", "author": ["Xavier Carreras", "Michael Collins"], "venue": "In Proceedings of the 2009 Conference of Empirical Methods in Natural Language Processing,", "citeRegEx": "Carreras and Collins.,? \\Q2009\\E", "shortCiteRegEx": "Carreras and Collins.", "year": 2009}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "A Bayesian model of syntax-directed tree to string grammar induction", "author": ["Trevor Cohn", "Phil Blunsom"], "venue": "In Proceedings of the 2009 Conference of Empirical Methods in Natural Language Processing,", "citeRegEx": "Cohn and Blunsom.,? \\Q2009\\E", "shortCiteRegEx": "Cohn and Blunsom.", "year": 2009}, {"title": "Inducing compact but accurate tree-substitution grammars", "author": ["Trevor Cohn", "Sharon Goldwater", "Phil Blunsom"], "venue": "In Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Cohn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2009}, {"title": "Natural language processing blog: Teaching machine translation", "author": ["Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "III.,? \\Q2008\\E", "shortCiteRegEx": "III.", "year": 2008}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "III and Marcu.,? \\Q2005\\E", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Why generative phrase models underperform surface heuristics", "author": ["John DeNero", "Dan Gillick", "James Zhang", "Dan Klein"], "venue": "In Proceedings of the Workshop on Statistical Machine Translation,", "citeRegEx": "DeNero et al\\.,? \\Q2006\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2006}, {"title": "Sampling alignment structure under a Bayesian translation model", "author": ["John DeNero", "Alexandre Bouchard-C\u00f4t\u00e9", "Dan Klein"], "venue": "In Proceedings of the 2008 Conference of Empirical Methods in Natural Language Processing,", "citeRegEx": "DeNero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2008}, {"title": "Scalable inference and training of context-rich syntactic models", "author": ["Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer"], "venue": "COLINGACL", "citeRegEx": "Galley et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2006}, {"title": "The DOP estimation method is biased and inconsistent", "author": ["Mark Johnson"], "venue": "Computational Linguistics,", "citeRegEx": "Johnson.,? \\Q2002\\E", "shortCiteRegEx": "Johnson.", "year": 2002}, {"title": "Statistical phrase based translation", "author": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "The alignment template approach to statistical machine translation", "author": ["Franz J. Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney.,? \\Q2004\\E", "shortCiteRegEx": "Och and Ney.", "year": 2004}, {"title": "Bayesian learning of a tree substitution grammar", "author": ["Matt Post", "Daniel Gildea"], "venue": "In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Post and Gildea.,? \\Q2009\\E", "shortCiteRegEx": "Post and Gildea.", "year": 2009}, {"title": "On the statistical consistency of dop estimators", "author": ["Detlef Prescher", "Remko Scha", "Khalil Sima\u2019an", "Andreas Zollmann"], "venue": "In Proceedings of the 14th Meeting of Computational Linguistics in the Netherlands,", "citeRegEx": "Prescher et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Prescher et al\\.", "year": 2004}, {"title": "Dependency treelet translation: Syntactically informed phrasal SMT", "author": ["Chris Quirk", "Arul Menezes", "Colin Cherry"], "venue": "In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Quirk et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2005}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "author": ["Libin Shen", "Jinxi Xu", "Ralph Weischedel"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Shen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke Zettlemoyer", "Michael Collins"], "venue": null, "citeRegEx": "Zettlemoyer and Collins.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "Some exhaustive pattern learning (EPL) methods Bod (1992) were proved to be flawed Johnson (2002), while similar algorithms Och and Ney (2004) showed great advantages on other tasks, such as machine translation.", "startOffset": 47, "endOffset": 58}, {"referenceID": 1, "context": "Some exhaustive pattern learning (EPL) methods Bod (1992) were proved to be flawed Johnson (2002), while similar algorithms Och and Ney (2004) showed great advantages on other tasks, such as machine translation.", "startOffset": 47, "endOffset": 98}, {"referenceID": 1, "context": "Some exhaustive pattern learning (EPL) methods Bod (1992) were proved to be flawed Johnson (2002), while similar algorithms Och and Ney (2004) showed great advantages on other tasks, such as machine translation.", "startOffset": 47, "endOffset": 143}, {"referenceID": 5, "context": "Johnson (2002); Prescher et al.", "startOffset": 0, "endOffset": 15}, {"referenceID": 5, "context": "Johnson (2002); Prescher et al. (2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent.", "startOffset": 0, "endOffset": 39}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent.", "startOffset": 18, "endOffset": 29}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al.", "startOffset": 18, "endOffset": 271}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al.", "startOffset": 18, "endOffset": 291}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al.", "startOffset": 18, "endOffset": 306}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al.", "startOffset": 18, "endOffset": 327}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al.", "startOffset": 18, "endOffset": 349}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them.", "startOffset": 18, "endOffset": 369}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them.", "startOffset": 18, "endOffset": 398}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them. Similar heuristic methods have also been used in many other pattern learning tasks, for example, like semantic parsing as in Zettlemoyer and Collins (2005) and chunking as in Daum\u00e9 III and Marcu (2005) in an implicit way.", "startOffset": 18, "endOffset": 578}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them. Similar heuristic methods have also been used in many other pattern learning tasks, for example, like semantic parsing as in Zettlemoyer and Collins (2005) and chunking as in Daum\u00e9 III and Marcu (2005) in an implicit way.", "startOffset": 18, "endOffset": 624}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them. Similar heuristic methods have also been used in many other pattern learning tasks, for example, like semantic parsing as in Zettlemoyer and Collins (2005) and chunking as in Daum\u00e9 III and Marcu (2005) in an implicit way. In all these heuristic algorithms, one needs to extract overlapping structures from training data in an exhaustive way. Therefore, in the article, we call them exhaustive pattern learning (EPL) methods. The use of EPL methods is intended to cope with the uncertainty of building blocks used in statistical models. As far as MT is concerned, Koehn et al. (2003) found that it was better to define a translation model on phrases than on words, but there was no obvious way to define what phrases", "startOffset": 18, "endOffset": 1005}, {"referenceID": 4, "context": "DeNero et al. (2006) observed that exhaustive pattern learning outperforms generative models with fixed building blocks.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "DeNero et al. (2006) observed that exhaustive pattern learning outperforms generative models with fixed building blocks. In EPL algorithms, one needs to collect statistics of overlapping structures from training data, so that they are not valid generative models. Thus, the EPL algorithms for grammar induction were viewed as heuristic methods DeNero et al. (2006); Daum\u00e9 III (2008).", "startOffset": 0, "endOffset": 365}, {"referenceID": 3, "context": "(2006); Daum\u00e9 III (2008). Recently, DeNero et al.", "startOffset": 14, "endOffset": 25}, {"referenceID": 3, "context": "(2006); Daum\u00e9 III (2008). Recently, DeNero et al. (2008); Blunsom et al.", "startOffset": 14, "endOffset": 57}, {"referenceID": 0, "context": "(2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "(2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al.", "startOffset": 8, "endOffset": 55}, {"referenceID": 0, "context": "(2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al. (2009); Post and Gildea (2009) investigated various sampling methods for grammar induction, which were believed to be more principled than EPL.", "startOffset": 8, "endOffset": 75}, {"referenceID": 0, "context": "(2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al. (2009); Post and Gildea (2009) investigated various sampling methods for grammar induction, which were believed to be more principled than EPL.", "startOffset": 8, "endOffset": 99}, {"referenceID": 8, "context": "Having the model probabilities fixed in this way could avoid over-fitting of the training data DeNero et al. (2006). In decoding, we search for the best hypothesis v\u0302 given training data D and input x as follows.", "startOffset": 95, "endOffset": 116}, {"referenceID": 1, "context": "3 On the DOP Model The EPL method investigated in this article may date back to Data Oriented Parsing (DOP) by Bod (1992). What is special with DOP is that the DOP model uses overlapping treelets of various sizes in an exhaustive way as building blocks of a statistical tree grammar.", "startOffset": 111, "endOffset": 122}], "year": 2013, "abstractText": "Pattern learning in an important problem in Natural Language Processing (NLP). Some exhaustive pattern learning (EPL) methods Bod (1992) were proved to be flawed Johnson (2002), while similar algorithms Och and Ney (2004) showed great advantages on other tasks, such as machine translation. In this article, we first formalize EPL, and then show that the probability given by an EPL model is constant-factor approximation of the probability given by an ensemble method that integrates exponential number of models obtained with various segmentations of the training data. This work for the first time provides theoretical justification for the widely used EPL algorithm in NLP, which was previously viewed as a flawed heuristic method. Better understanding of EPL may lead to improved pattern learning algorithms in future.", "creator": "LaTeX with hyperref package"}}}