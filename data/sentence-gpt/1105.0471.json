{"id": "1105.0471", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2011", "title": "Suboptimal Solution Path Algorithm for Support Vector Machine", "abstract": "We consider a suboptimal solution path algorithm for the Support Vector Machine. The solution path algorithm is an effective tool for solving a sequence of a parametrized optimization problems in machine learning. The path of the solutions provided by this algorithm are very accurate and they satisfy the optimality conditions more strictly than other SVM optimization algorithms. In many machine learning application, however, this strict optimality is often unnecessary, and it adversely affects the computational efficiency. Our algorithm can generate the path of suboptimal solutions within an arbitrary user-specified tolerance level. It allows us to control the trade-off between the accuracy of the solution and the computational cost. Moreover, We also show that our suboptimal solutions can be interpreted as the solution of a \\emph{perturbed optimization problem} from the original one. We provide some theoretical analyses of our algorithm based on this novel interpretation. The experimental results also demonstrate the effectiveness of our algorithm.", "histories": [["v1", "Tue, 3 May 2011 03:14:14 GMT  (159kb)", "http://arxiv.org/abs/1105.0471v1", "A shorter version of this paper is submitted to ICML 2011"]], "COMMENTS": "A shorter version of this paper is submitted to ICML 2011", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["masayuki karasuyama", "ichiro takeuchi"], "accepted": true, "id": "1105.0471"}, "pdf": {"name": "1105.0471.pdf", "metadata": {"source": "CRF", "title": "Suboptimal Solution Path Algorithm for Support Vector Machine", "authors": ["Masayuki Karasuyama", "Ichiro Takeuchi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 5.\n04 71\nv1 [\ncs .L\nG ]\n3 M"}, {"heading": "1 Introduction", "text": "Recently, the solution path algorithm (Efron et al., 2004; Hastie et al., 2004; Cauwenberghs & Poggio, 2001) has been widely recognized as one of the effective tools in machine learning. It can efficiently compute a sequence of the solutions of a parametrized optimization problem. This technique is originally developed as parametric programming in the optimization community (Best, 1982).\nIn a class of parametric quadratic programs (QPs), the solution path is represented as a piecewise-linear function of the problem parameters. If we regard the regularization parameter of the Support Vector Machine (SVM) as problem parameter, the optimization problem for the SVM is categorized in this class. Therefore, the SVM solutions are represented as piecewise-linear functions of the regularization parameter.\nThe solutions of these parametric QPs are characterized by active constraint set in the current solution. The linearity of the path comes from the fact that the Karush-Khun-Tucker (KKT) optimality conditions of these problems are represented as a linear system defined by the current active set, while the \u201cpiecewise-\nness\u201d is the consequence of the changes in the active set. The piecewise-linear solution path algorithm repeatedly updates the linear system and active set. The point of active set change is called breakpoint in the literature. The path of solutions generated by this algorithm is very accurate and they satisfy the optimality conditions more strictly than other algorithms.\nMany machine learning problems, however, do not require strict optimality of the solution. In fact, one of the popular SVM optimization algorithm, called sequential minimal optimization (SMO) Platt (1999), is known to produce suboptimal (approximated) solution, where the tolerance to the optimality (degree of approximated) can be specified by users. In many experimental studies, it has been demonstrated that the generalization performances of these suboptimal solutions are not significantly different from those of strictly optimal ones.\nTherefore, the strict optimality of the solution path algorithm is often unnecessary. Furthermore, it adversely affects the computational efficiency of the algorithm. In fact, the solution path algorithm can be very slow when it encounters a large number of (seemingly redundant) breakpoints. Although some empirical studies suggest that the number of breakpoints grows linearly in the input size, in the worst case, it can grow exponentially (Ga\u0308rtner et al., 2009). Another difficulty is in starting the solution path algorithm from an approximated solution, for example obtained by SMO, because it does not satisfy the strict optimality requirement.\nIn order to address these issues in the current solution path algorithm, we introduce a suboptimal solution path algorithm. Our algorithm also generates piecewise-linear solution path, but the optimality tolerance (approximation level) can be arbitrary controlled by users. It allows to control the trade-off between the accuracy of the solution and the computational cost.\nThe presented suboptimal solution path algorithm has the following properties.\n\u2022 First, the algorithm can reduce the number of breakpoints (which is the main computational bottleneck in solution path algorithm) by allowing multiple active set changes at one breakpoint. Although this modification causes what is called degeneracy problem, we provide an efficient and accurate way to solve this issue. We empirically show that reducing the number of breakpoints can work effectively to the computational efficiency.\n\u2022 Second, the suboptimal solutions obtained by the algorithm can be interpreted as the solution of a perturbed optimization problem from the original one. This novel interpretation provides several insights into the properties of our suboptimal solutions. We present some theoretical analyses of our suboptimal solutions using this interpretation.\nWe also empirically investigate several practical properties of our approach. Although, our algorithm updates multiple active constraints at one breakpoint, we observe that the entire changing patterns of the active sets are very similar to those of the exact path. Moreover, despite its computational efficiency, the generalization performance of our suboptimal path is comparable to conventional\none. To the best of our knowledge, there are no previous works for suboptimal solution path algorithm with controllable optimality tolerance that can be applicable to standard SVM formulation 1. Although many authors mimic the solution path by just repeating the warm-start on finely grid points (e.g., Friedman et al., 2007), this approach does not provide any guarantee about the intermediate solutions between grid points. In this paper we focus our attention to the solution path algorithm for standard SVM, but the presented approach can be applied to other problems in the aforementioned QP class."}, {"heading": "2 Solution Path for Support Vector Machine", "text": "In this section, we describe the solution path algorithm for regularization parameters of Support Vector Machine (SVM)."}, {"heading": "2.1 Support Vector Machine", "text": "Suppose we have a set of training data {(xi, yi)} n i=1, where xi \u2208 X \u2286 R p is the input and yi \u2208 {\u22121,+1} is the output class label. SVM learns a linear discriminant function f(x) = w\u22a4\u03a6(x) + \u03b10 in a feature space F , where \u03a6 : X \u2192 F is a map from the input space X to the feature space F , w \u2208 F is a coefficient vector and \u03b10 \u2208 R is a bias term.\nIn this paper, we consider the optimization problem of the following form:\nmin w,\u03b10,{\u03bei}ni=1\n1 2\u2016w\u2016 2 2 + \u2211n i=1 Ci\u03bei, (1)\ns.t. yif(xi) \u2265 1\u2212 \u03bei, \u03bei \u2265 0, i = 1, . . . , n,\nwhere {Ci} n i=1 denotes regularization parameters. This formulation reduces to the standard formulation of the SVM when all Ci\u2019s are the same. Our discussion in this paper holds for arbitrary choice of Ci\u2019s.\nWe formulate the dual problem of (1) as:\nmax \u03b1\n\u2212 1\n2 \u03b1\u22a4Q\u03b1+ 1\u22a4\u03b1\ns.t. y\u22a4\u03b1 = 0, 0 \u2264 \u03b1 \u2264 c, (2)\nwhere \u03b1 = [\u03b11, . . . , \u03b1n] \u22a4, c = [C1, . . . , Cn] \u22a4 and (i, j) element of Q \u2208 Rn\u00d7n is Qij = yiyj\u03a6(xi)\n\u22a4\u03a6(xj). Note that, we use inequalities between vectors as the element-wise inequality (i.e., \u03b1 \u2264 c \u21d4 \u03b1i \u2264 Ci for i = 1, . . . , n ). Using kernel function K(xi,xj) = \u03a6(xi) \u22a4\u03a6(xj), discriminant function f is represented as:\nf(x) = n\u2211\ni=1\n\u03b1iyiK(x,xi) + \u03b10.\n1 Giesen et al. (2010) proposed approximated path algorithm with some optimality guarantee that can be applicable to L2-SVM without bias term.\nIn what follows, the subscript by an index set such as vI for a vector v = [v1, \u00b7 \u00b7 \u00b7 , vn]\n\u22a4 indicates a sub-vector of v whose elements are indexed by I = {i1, . . . , i|I|}. For example, for v = [a, b, c] \u22a4 and I = {1, 3}, vI = [a, c] \u22a4. Similarly, the subscript by two index sets such as MI1,I2 for a matrix M \u2208 R\nn\u00d7n denotes a sub-matrix whose rows and columns are indexed by I1 and I2, respectively. The principal sub-matrix such as MI,I is abbreviated as MI ."}, {"heading": "2.2 Solution Path Algorithm for SVM", "text": "In this paper, we consider the solution path with respect to the regularization parameter vector c. To follow the path, we parametrized c in the following form:\nc(\u03b8) = c(0) + \u03b8d,\nwhere c(0) = [C (0) 1 , . . . , C (0) n ]\u22a4 is some initial parameter, d = [d1, . . . , dn] \u22a4 is a direction of the path and \u03b8 \u2265 0. We trace the change of the optimal solution of the SVM when \u03b8 increases from 0.\nLet {\u03b1 (\u03b8) i } n i=0 be the optimal parameters and {f (\u03b8) i } n i=1 be the outputs f(xi)\nat \u03b8. The KKT optimality conditions are summarized as:\nyif (\u03b8) i \u2265 1, if \u03b1 (\u03b8) i = 0, (3a) yif (\u03b8) i = 1, if 0 < \u03b1 (\u03b8) i < C (\u03b8) i , (3b) yif (\u03b8) i \u2264 1, if \u03b1 (\u03b8) i = C (\u03b8) i , (3c)\ny\u22a4\u03b1 = 0. (3d)\nWe separate data points into three index sets M,O, I \u2286 {1, . . . , n} in such a way that these sets satisfy\ni \u2208 O \u21d2 yif (\u03b8) i \u2265 1, \u03b1 (\u03b8) i = 0, (4a)\ni \u2208 M \u21d2 yif (\u03b8) i = 1, \u03b1 (\u03b8) i \u2208 [0, Ci], (4b)\ni \u2208 I \u21d2 yif (\u03b8) i \u2264 1, \u03b1 (\u03b8) i = Ci, (4c)\nand we denote these partitions altogether as \u03c0 := (O,M, I). If every data point belongs to one of the three index sets and equality (3d) holds, the KKT conditions (3) are satisfied. As long as these index sets are unchanged, we have analytical expression of the optimal solution in the form of \u03b1 (\u03b8+\u2206\u03b8) i = \u03b1 (\u03b8) i +\u2206\u03b8\u03b2i, i = 0, . . . , n, where \u2206\u03b8 is the change of \u03b8 and {\u03b2i} n i=0 are constants derived from sensitivity analysis theory:\nTheorem 1. Let \u03c0 = (O,M, I) be the partition at the optimal solution at \u03b8 and assume that\nM =\n[ 0 y\u22a4M\nyM QM\n]\nis non-singular2. Then, as long as \u03c0 is unchanged, {\u03b2i} n i=0 is given by\n[ \u03b20 \u03b2M ] =\u2212M\u22121 [ y\u22a4I QM,I ] dI , \u03b2O = 0, \u03b2I = dI . (5)\nThe proof is in Appendix A. This theorem can be viewed as one of the specific forms of the sensitivity theorem Fiacco (1976). It can be derived from the KKT conditions (3) and the similar properties are repeatedly used in various solution path algorithms in machine learning (Cauwenberghs & Poggio, 2001; Hastie et al., 2004).\nUsing the above theorem, we can update the solution by \u03b1 (\u03b8+\u2206\u03b8) i = \u03b1 (\u03b8) i + \u2206\u03b8\u03b2i as long as \u03c0 is unchanged. However, if we changes \u03b8, the optimal partition \u03c0 could also changes. Those change points are called breakpoints. In the solution path algorithm, the optimality conditions are always kept satisfied by precisely detecting the breakpoints and updating \u03c0 properly."}, {"heading": "3 Suboptimal Solution Path", "text": "In this section, we develop a suboptimal solution path algorithm for the SVM, where the tolerance to the optimality conditions can be arbitrary controlled by users. The basic idea is to relax the KKT optimality conditions and allow multiple data points to move among the partition \u03c0 at the same time. Note that it reduces the number of breakpoints and leads to the improvement in its computational efficiency: allowing us to control the balance between the accuracy of the solution and the computational cost."}, {"heading": "3.1 Approximate Optimality Conditions", "text": "First, we relax the conditions (4) as\ni \u2208 O \u21d2 yif (\u03b8) i \u2265 1\u2212 \u03b51, \u03b1 (\u03b8) i \u2208 [\u2212\u03b52, 0], (6a)\ni \u2208 M \u21d2 yif (\u03b8) i \u2208 [1\u2212\u03b51, 1+\u03b51], \u03b1 (\u03b8) i \u2208 [\u2212\u03b52, C (\u03b8) i +\u03b52], (6b)\ni \u2208 I \u21d2 yif (\u03b8) i \u2264 1+\u03b51, \u03b1 (\u03b8) i \u2208 [C (\u03b8) i , C (\u03b8) i +\u03b52], (6c)\nwhere \u03b51 \u2265 0 and \u03b52 \u2265 0 specify the degree of approximation. If we set \u03b51 = \u03b52 = 0, these conditions reduce to (4).\nOur algorithm changes \u03b8 while keeping the above conditions (6) satisfied. Let \u03b80 = 0 be the initial value of \u03b8 and the non-decreasing sequence \u03b80 \u2264 \u03b81 \u2264 \u03b82 \u2264 . . ., be the breakpoints. Suppose we are currently at \u03b8k, the next breakpoint \u03b8k+1 is characterized as the point that we can not increase \u03b8 without violating the conditions (6) or changing index sets \u03c0.\n2The invertibility of the matrix M is assured if and only if the submatrix QM is positive definite in subspace {z \u2208 R|M| | y\u22a4Mz = 0}.\nIf we set {\u03b2i} n i=0 by (5), then yif (\u03b8) i , i \u2208 M, and \u03b1 (\u03b8) i , i \u2208 O\u222aI, are constants.\nTo increase \u03b8 from \u03b8k, we only need to check the following inequalities:\nyif (\u03b8k) i +\u2206\u03b8gi \u2265 1\u2212 \u03b51, i \u2208 O,\n\u03b1 (\u03b8k) i +\u2206\u03b8\u03b2i \u2208 [\u2212\u03b52, C (\u03b8k) i + \u03b52], i \u2208 M,\nyif (\u03b8k) i +\u2206\u03b8gi \u2264 1\u2212 \u03b51, i \u2208 I,\nwhere gi is the change of output yifi which is defined by g = Q\u03b2 + y\u03b20. We want to know the maximum \u2206\u03b8 which satisfies all of the above inequalities. We can easily calculate the maximum \u2206\u03b8 for each inequality as follows:\n\u0398O = { (1\u2212 \u03b51 \u2212 yif (\u03b8k) i )/gi \u2223\u2223\u2223i \u2208 O, gi < 0 } ,\n\u0398M\u2113 = { \u2212(\u03b1 (\u03b8k) i + \u03b52)/\u03b2i \u2223\u2223\u2223 i \u2208 M, \u03b2i < 0 } , \u0398Mu = { (C (\u03b8k) i + \u03b52 \u2212 \u03b1\n(\u03b8k) i )/(\u03b2i \u2212 di)\u2223\u2223\u2223i \u2208 M, \u03b2i > di } ,\n\u0398I = { (1 + \u03b51 \u2212 yif (\u03b8k) i )/gi \u2223\u2223\u2223i \u2208 I, gi > 0 } ,\nSince we have to keep all of the inequalities satisfied, we take the minimum of these values: \u2206\u03b8 = min\u0398, where \u0398 = {\u0398O,\u0398M\u2113 ,\u0398Mu ,\u0398I}. Then we can find \u03b8k+1 = \u03b8k +\u2206\u03b8.\nAlthough we detect \u03b8k+1, it is necessary to update \u03c0 to go beyond the breakpoint. Conventional solution path algorithms allow only one data point to move between the partition \u03c0 at each breakpoint. For example, \u03b1i, i \u2208 M, reaches 0, the algorithm transfers the index i from M to O (Figure 1(a)). In our algorithm, multiple data points are allowed to move between the partitions \u03c0 at the same time in order to reduce the number of breakpoints."}, {"heading": "3.2 Update Index Sets", "text": "At a breakpoint, our algorithm handles all the data points that violate the strict inequality conditions (4) rather than the relaxed ones (6) (Figure 1(b)). This situation can be interpreted as what is called degeneracy in the parametric programming (Ritter, 1984). Here, degeneracy means that multiple constraints hit their boundaries of inequalities simultaneously. Although degenerate situation rarely happens in conventional solution path algorithms, it is not the case in ours. The simultaneous change of multiple data points inevitably brings about \u201chighly\u201d degenerate situations involved with many constraints. In degenerate case, we have a problem called the cycling. For example, if we move two indices i and j from M to O at the breakpoint, then both or either of them may immediately return to M. To avoid the cycling, we need to design an update strategy for \u03c0 that can circumvent cycling.\nThe degeneracy can be handled by several approaches which are known in the parametric programming literature. Ritter (1984) showed that the cycling can be dealt with through the well-known Bland\u2019s minimum index rule in the\nlinear programming (Bland, 1977). However, in the worst case, this approach must go through all the possible patterns of next \u03c0. Since we need to evaluate {\u03b2i} n i=0 in each iteration, a large number of iterations may cause additional computational cost. In this paper, we provide more essential solution to this problem based on (Berkelaar et al., 1997).\nSuppose we are currently on the breakpoint \u03b8k. Let\nBO = {i | \u03b1 (\u03b8k) i \u2264 0, \u03b2i < 0, i \u2208 M}\u222a\n{i | yif (\u03b8k) i \u2264 1, gi < 0, i \u2208 O},\nBI = {i | \u03b1 (\u03b8k) i \u2265 C (\u03b8k) i , \u03b2i > di, i \u2208 M}\u222a\n{i | yif (\u03b8k) i \u2265 1, gi > 0, i \u2208 I}.\nBO is the set of indices which satisfy the conditions (6a) and (6b) for being the member of M and O simultaneously at \u03b8k. Similarly, indices in BI satisfy the conditions (6b) and (6c) for being the member of M and I at \u03b8k. Moreover, let us define sum of these two sets as\nB = BO \u222a BI .\nOur task is to partition these indices to O, M and I correctly so that it does not cause the cycling.\nIn our formulation, due to the approximation by \u03b51 and \u03b52, the cycling may not occur at \u2206\u03b8 = 0 immediately. For example, suppose that i move to M from O and its parameter is \u03b1i = 0. In the next iteration, we need to check \u03b1i + \u2206\u03b8\u03b2i \u2265 \u2212\u03b52. If \u03b2i < 0, then we obtain \u2206\u03b8 \u2264 \u2212\u03b52/\u03b2i > 0. Although it allows \u2206\u03b8 > 0, the index i may return back to O. This situation can also be considered as cycling.\nLet \u03c0k = (Ok,Mk, Ik) be \u03c0 in [\u03b8k, \u03b8k+1]. At \u03b8k+1, if and only if the cycling does not occur, it can be shown that the following conditions hold:\n\u03b2i \u2265 0, gi = 0, for i \u2208 Mk+1 \u2229 BO, (7a)\n\u03b2i = 0, gi \u2265 0, for i \u2208 Ok+1 \u2229 BO, (7b)\n\u03b2i \u2264 di, gi = 0, for i \u2208 Mk+1 \u2229 BI , (7c)\n\u03b2i = di, gi \u2264 0, for i \u2208 Ik+1 \u2229 BI . (7d)\nAlthough \u03b2i and gi are usually calculated using \u03c0, our approach allows us to calculate \u03b2i and gi without knowing \u03c0 so that they can satisfy the above conditions. If the gradient \u03b2, which is defined in (5), satisfies the following conditions, we can find the next partition \u03c0k+1 to satisfy (7). The conditions are:\ngi\u03b2i = 0, gi \u2265 0, \u03b2i \u2265 0, i \u2208 BO,\ngi(di \u2212 \u03b2i) = 0, gi \u2264 0, \u03b2i \u2264 di, i \u2208 BI , (8)\nIf we know such \u03b2 and g, using the following update rule, we can determine \u03c0k+1 as:\nMk = Mk+ 1 2 \u222a {i | \u03b2i > 0, gi = 0, i \u2208 BO}\n\u222a {i | \u03b2i < di, gi = 0, i \u2208 BI}, Ok = Ok+ 1\n2 \u222a {i | \u03b2i = 0, gi \u2265 0, i \u2208 BO},\nIk = Ik+ 1 2 \u222a {i | \u03b2i = di, gi \u2264 0, i \u2208 BI},\n(9)\nwhere Ok+ 1 2 = Ok \\ B, Mk+ 1 2 = Mk \\ B and Ik+ 1 2 = Ik \\ B.\nRemark 1. By definition, the update rule (9) guarantees that the non-cycling conditions (7) hold.\nTo use (9), we need \u03b2 (5) which satisfies (8). The following theorem shows that it can be obtained from a quadratic programming problem (QP):\nTheorem 2. Let \u03b2\u03020, \u03b2\u0302 and g\u0302 be the optimal solutions of the following QP\nproblem:\nmin \u03b2\u03020, \u0302\u03b2,g\u0302\n\u2211\ni\u2208BO\ng\u0302i\u03b2\u0302i + \u2211\ni\u2208BI\ng\u0302i(\u03b2\u0302i \u2212 di) (10)\ns.t.    g\u0302BO \u2265 0, \u03b2\u0302BO \u2265 0, g\u0302BI \u2264 0, \u03b2\u0302BI \u2264 dI , g\u0302M k+1 2 = 0, \u03b2\u0302O k+1 2 = 0, \u03b2\u0302I k+1 2 = dI k+1 2 ,\ny\u22a4\u03b2\u0302 = 0, g\u0302 = Q\u03b2\u0302 + y\u03b2\u03020,\nand \u03c0 is determined by (9) using \u03b2\u0302 and g\u0302. Then \u03b2\u03020, \u03b2\u0302 and g\u0302 satisfy (8) and they are equal to the gradient \u03b20, \u03b2 and g, respectively.\nAlthough the detailed proof is in Appendix, we can provide clear interpretation of this optimization problem. The objective function and inequality constraints corresponds to (8) and the other constraints correspond to the linear system (5). It can be shown that the optimal value of the objective function is 0. Given the non-negativity of each term in the objective, we see that (8) holds (see Appendix B for detail).\nThe optimization problem (10) has 2n+ 1 variables and 2|B|+ 2n+ 1 constraints. However, we can reduce these sizes to |B| variables and 2|B| constraints by arranging the equality constraints3. The detailed formulation of the reduced problem is in Appendix C. If the size of |B| is large, it may take large computational cost to solve (10). To avoid this, we set the upper bound B for the number of elements of B. In the case of |B| > B, we choose top B elements from the original B by increasing order of \u0398 as the elements of B."}, {"heading": "3.3 Algorithm and Computational Complexity", "text": "Here, we summarize our algorithm and analyze its computational complexity. At the k-th breakpoint, our algorithm performs the following procedure:\nstep1 Using \u03c0k, calculate \u03b20,\u03b2 and g by (5)\nstep2 Calculate the next breakpoint \u03b8k+1 and update \u03b1 (\u03b8) 0 ,\u03b1 (\u03b8), c(\u03b8);\nstep3 Solve (10) and calculate \u03c0k+1 by (9)\nIn step1, we need to solve the linear system (5). In conventional solution path algorithms, we can update it using rank-one-update of an inverse matrix or a Cholesky factor from previous iteration by O(|M|2) computations. In our case, we need rank-m-update at each breakpoint, where 1 \u2264 m \u2264 B. When we set B as some small constant, the computational cost still remains O(|M|2). Including the other processes in this step, the computational cost becomes O(n|M|). In step2, given \u03b2 and g, we can calculate all the possible step length \u0398 by O(n). In step3, since the optimization problem (10) becomes convex QP problem with\n3\nIn the case of |M k+ 1\n2\n| = 0, the reduced problem has |B|+ 1 variables 2|B|+ 1 constraints.\n|B| variables, it can be solved efficiently by some standard QP solvers in the situation |B| is relatively small compared to n. When we set B as some constant, the time for solving this optimization problem is then independent of n.\nPut it all together, in the case of constant B, the computational cost of each breakpoint is O(n|M|). This is the same as the conventional solution path algorithm. However, as we will see later in experiments, our algorithm drastically reduces the number of breakpoints especially when we use large \u03b51 and \u03b52."}, {"heading": "4 Analysis", "text": "In this section, we provide some theoretical analyses of our suboptimal solution path."}, {"heading": "4.1 Interpretation as Perturbed Problem", "text": "An interesting property of our approach is that the solutions always keep the optimality of an optimization problem which is slightly perturbed from the original one. The following theorem gives the formulation of the perturbed problem:\nTheorem 3. Every solution \u03b1(\u03b8) in the suboptimal solution path is the optimal solution of the following optimization problem:\nmax \u03b1\n\u2212 1\n2 \u03b1\u22a4Q\u03b1+ (1+ p)\u22a4\u03b1\ns.t. y\u22a4\u03b1 = 0, \u2212q \u2264 \u03b1 \u2264 c(\u03b8) + q. (11)\nwhere perturbation parameters p, q \u2208 Rn are in \u2212\u03b511 \u2264 p \u2264 \u03b511 and 0 \u2264 q \u2264 \u03b521, respectively.\nProof. Let \u03be+, \u03be\u2212 \u2208 Rn+ and \u03ba \u2208 R be the Lagrange multipliers. The Lagrangian is\nL = \u2212 12\u03b1 \u22a4Q\u03b1+ (1+ p)\u22a4\u03b1\n+(\u03b1+ q)\u22a4\u03be\u2212 + (c(\u03b8) + q \u2212\u03b1)\u22a4\u03be+ + \u03bay\u22a4\u03b1,\nand the KKT conditions are\n\u2202L \u2202\u03b1 = \u2212Q\u03b1+ 1+ p+ \u03be \u2212 \u2212 \u03be+ + \u03bay = 0, (12a)\n\u03be+, \u03be\u2212 \u2265 0, (12b)\n\u03be\u2212i (\u03b1i + qi) = 0, i = 1, . . . , n, (12c)\n\u03be+i (C (\u03b8) i + qi \u2212 \u03b1i) = 0, i = 1, . . . , n, (12d)\n\u2212q \u2264 \u03b1 \u2264 c(\u03b8) + q. (12e)\ny\u22a4\u03b1 = 0, (12f)\nSubstituting \u03b1 = \u03b1(\u03b8) and \u03ba = \u2212\u03b1 (\u03b8) 0 , i-th element of (12a) can be written as\nyif (\u03b8) i = 1 + pi + \u03be \u2212 i \u2212 \u03be + i . Considering this and the conditions of suboptimal solution \u03b1(\u03b8) (6), there exist pi \u2208 [\u2212\u03b51, \u03b51] and \u03be \u00b1 i which satisfy \u03be + i = \u03be \u2212 i = 0 for i \u2208 M, \u03be+i = 0, \u03be \u2212 i \u2265 0, for i \u2208 O and \u03be + i \u2265 0, \u03be \u2212 i = 0, for i \u2208 I. These \u03be\u00b1i \u2019s satisfy the non-negativity constraint (12b). The complementary conditions (12c) and (12d) for i \u2208 M hold from \u03be+i = \u03be\u2212i = 0. For i \u2208 O, since \u03be + i = 0, we don\u2019t have to check (12d). In this case, if we set qi = \u2212\u03b1 (\u03b8) i \u2208 [0, \u03b52], then (12c) holds. It can be shown in a similar way that (12c) and (12d) hold for i \u2208 I. Our suboptimal solution path algorithm always satisfies the equality constraint of the dual (2) and the box constraint (12e) satisfied. Therefore, we see (12) holds.\nThe problem (11) can be interpreted as the dual problem of the following form of the SVM:\nmin w,\u03b10\n1 2 w\u22a4w +\nn\u2211\ni=1\n\u2113(1 + pi \u2212 yifi), (13)\nwhere\n\u2113(\u03bei) =\n{ (C\n(\u03b8) i + qi)\u03bei, for \u03bei \u2265 0, \u2212qi\u03bei, for \u03bei < 0,\nis a loss function. We see that the perturbations present in the loss term."}, {"heading": "4.2 Error Analysis", "text": "We have shown that the solution of the suboptimal solution path can be interpreted as the optimal solution of the perturbed problem (13). Here, we consider how close the optimal solution of the perturbed problem to the solution of the original problem in terms of the optimal objective value.\nLet D(\u03b1) and D\u0303(\u03b1) be the dual objective functions of the original optimization problem (2) and the perturbed problem (11), respectively. From the affine\nlower bound of D\u0303(\u03b1), we obtain\nD\u0303(\u03b1) \u2264 D(\u03b1\u2217) + p\u22a4\u03b1\u2217 + (\u2212Q\u03b1\u2217 + 1+ p)\u22a4(\u03b1\u2212\u03b1\u2217),\nwhere \u03b1\u2217 is the optimal solution of the original problem. Let \u03b1\u0303 be the optimal solution of the perturbed problem. Substituting \u03b1 = \u03b1\u0303 and adding \u03b1\u22170y\n\u22a4(\u03b1\u0303\u2212 \u03b1\u2217) = 0 to the right hand side, we obtain\nD\u0303(\u03b1\u0303)\u2212D(\u03b1\u2217) \u2264 p\u22a4\u03b1\u2217 + (\u03be\u2217 + p)\u22a4(\u03b1\u0303\u2212\u03b1\u2217), (14)\nwhere \u03be\u2217 = \u2212Q\u03b1\u2217 \u2212 y\u03b1\u22170 + 1. Note that \u03be \u2217 I \u2265 0, \u03be \u2217 M = 0 and \u03be \u2217 O \u2264 0, where I, M and O represent the optimal partition of the original problem (2). Here,\nwe define I\u0303 = {i | \u03be\u2217i + pi \u2265 0, i \u2208 I}, O\u0303 = {i | \u03be \u2217 i + pi \u2264 0, i \u2208 O} and M\u0303 = {1, . . . , n} \\ (O\u0303 \u222a I\u0303). From the right hand side of (14), we obtain\nD\u0303(\u03b1\u0303)\u2212D(\u03b1\u2217) \u2264 \u2211\ni\u2208M\u222aI |pi| C (\u03b8) i +\n\u2211 i\u2208I\u0303\u222aO\u0303 |\u03be \u2217 i + pi| qi + \u2211 i\u2208M\u0303 |pi| (C (\u03b8) i + qi)\nFrom the duality theorem, this also bounds the difference of the primal objective value. Comparing the original objective function (1), this bound can be considered small when pi and qi is enough small compared to \u03be \u2217 i and Ci. In this view point, this bound gives theoretical justification for our intuitive interpretation. The bound for D(\u03b1\u2217)\u2212 D\u0303(\u03b1\u0303) can be also derived in the same manner."}, {"heading": "5 Experiments", "text": "In this section, we illustrate the empirical performance of the proposed approach compare to the conventional exact solution path algorithm. Our task is to trace the solution path from c(0) = 10\u22121/n \u00d7 1 to c(1) = 106/n \u00d7 1. Since all the elements of c(\u03b8) takes the same value in this case, we sometimes refer to this common value as C(\u03b8) (i.e., c(\u03b8) = C(\u03b8) \u00d7 1). The RBF kernel K(xi,xj) = exp(\u2212\u03b3\u2016xi \u2212 xj\u2016 2 2) is used with \u03b3 = 1/p where p is the number of features. To circumvent possible numerical instability in the solution path, we add small positive constant 10\u22126 to the diagonals of the matrix Q.\nLet e \u2265 0 be a parameter which controls the degree of approximations. In this paper, using e, we set \u03b51 and \u03b52 as \u03b51 = e and \u03b52 = e\u00d7C\n(\u03b8k), respectively, where \u03b8k is the previous breakpoint. We set \u03b52 using relative scale to C\n(\u03b8k). Table 1 lists the statistics of data sets. These data sets are available from LIBSVM site (Chang & Lin, 2001) and UCI data repository (Asuncion & Newman, 2007). We randomly sampled n data points from the original data set 10 times (we set n be approximately 80% of the original number of data points in the table). The input x of each data set is linearly scaled to [0, 1]p.\nFigure 2 shows the comparison of the CPU time and the number of breakpoints. To make fair comparison, the initialization is not included in the CPU time. In these results, we set B = 10 and we investigated the relationship between the computational cost and the degree of approximation by examining several settings of e \u2208 {0.001, 0.01, 0.1, 0.5}. The results indicate that our approach can reduce the CPU time especially when e is large. The number of\nbreakpoints were also reduced, in the same way as the CPU time. In our approach, since we need rank-m-update of matrix in each breakpoint (1 \u2264 m \u2264 B), an update in a breakpoint may take longer time than rank-one-update which is needed in the conventional solution path algorithm. We conjecture that this is why the decrease in the number of breakpoints was slightly faster than the CPU time. However, since the maximum value of |B| was set as B = 10 in this experiment, this additional cost was relatively small compared to the effect of the reduction of the number of breakpoints.\nNext, we investigated the effect of B. Figure 3 shows the CPU time and the number of breakpoints for w1a data (n = 2477, p = 300) with B = 10 and B = n. When B = n, there are no upper bounds for |B|. In the left plot, when B = n, we see that the CPU time is longer than the case of B = 10. In this data set, this difference of the CPU time mainly comes from the cost of the matrix update and QP (10) whose size is proportional to |B| (data not shown). On the\nother hand, in the left plot, the number of breakpoints is stable in the both case of B = n and B = 10, and interestingly, the number itself is almost the same in these two settings. Our results suggest that too many B does not contribute to reduce the number of breakpoint. Although these unstable results in B = n is not always happen, we observed that it is more stable to use B = 10 or B = 100 in several other data sets.\nWe also compared the difference of \u03c0 between the exact solution path and the suboptimal path in order to see the degree of approximation in terms of the active set. Let Ii \u2208 {0, 1} be an indicator variable which has 1 when a data point i belongs to different set among M, O and I between two solution paths. Figure 4(a) shows plots of 10 runs average of \u2211n i=1 Ii/n for e = 0.5 in a5a data set. We see that the difference is at most about 10%. Figure 4(b) shows the size of each index set (this plot is one of 10 runs). Although the small differences exist, the changing patterns are similar each other.\nTable 2 shows results of test error rate comparison for e = 0.5. We used 60% of the data for training, 20% for validation and 20% for testing. In each data set, we see that the performances of our suboptimal solutions are comparable to the exact solution path."}, {"heading": "6 Conclusion", "text": "In this paper, we have developed a suboptimal solution path algorithm which traces the changes of solutions under the relaxed optimality conditions. Our algorithm can reduce the number of breakpoints by moving multiple indices in \u03c0 at one breakpoint. Another interesting property of our approach is that the suboptimal solutions exactly correspond to the optimal solutions of the perturbed problems from the original SVM optimization problems. The experimental results demonstrate that our algorithm efficiently follows the path and it has similar patterns of active sets and classification performances compared to the exact path."}, {"heading": "A Proof of Theorem 1", "text": "Here, we provide a proof of the following theorem:\nTheorem 1. Let \u03c0 = (O,M, I) be the partition at the optimal solution at \u03b8 and assume that\nM =\n[ 0 y\u22a4M\nyM QM\n]\nis non-singular. Then, as long as \u03c0 is unchanged, {\u03b2i} n i=0 is given by\n[ \u03b20 \u03b2M ] = \u2212M\u22121 [ y\u22a4I QM,I ] dI ,\n\u03b2O = 0, \u03b2I = dI .\n(A.1)\nProof. As long as \u03c0 is unchanged, \u03b1i for i \u2208 O and i \u2208 I must be\n\u03b1i = 0, i \u2208 O, \u03b1i = C (\u03b8) i , i \u2208 I.\nTherefore, we see that \u03b2O = 0 and \u03b2I = dI . From the definition of M, at the optimal, the following linear system holds\nQM\u03b1 (\u03b8) M +QM,Ic (\u03b8) I + yM\u03b1 (\u03b8) 0 = 1.\nCombining with the equality constraint of the dual problem y\u22a4\u03b1 = 0, we obtain the following linear system:\nM\n[ \u03b1 (\u03b8) 0\n\u03b1 (\u03b8) M\n] + [ y\u22a4I\nQM,I\n] c (\u03b8) I = [ 0 1 ] .\nSolving this, we obtain\n[ \u03b1 (\u03b8) 0\n\u03b1 (\u03b8) M\n]\n= \u2212M\u22121 [\ny\u22a4I QM,I\n] c (\u03b8) I +M \u22121 [ 0 1 ] .\nUsing c(\u03b8+\u2206\u03b8) = c(\u03b8) + \u03b8d, we can write\n[ \u03b1 (\u03b8+\u2206\u03b8) 0\n\u03b1 (\u03b8+\u2206\u03b8) M\n] = [ \u03b1 (\u03b8) 0\n\u03b1 (\u03b8) M\n] \u2212 \u03b8M\u22121 [ y\u22a4I\nQM,I\n] dI\nThen, we obtain (A.1)."}, {"heading": "B Proof of Theorem 2", "text": "Here, we provide a proof of Theorem 2. First, we prove the following lemma.\nLemma 1. Suppose \u03b2\u0302 \u2208 Rn, \u03b2\u03020 \u2208 R and g\u0302 = Q\u03b2\u0302 + y\u03b2\u03020 satisfy the following conditions:\ng\u0302i\u03b2\u0302i = 0, g\u0302i \u2265 0, \u03b2\u0302i \u2265 0, i \u2208 BO , (B.1a)\ng\u0302i(di \u2212 \u03b2\u0302i) = 0, g\u0302i \u2264 0, \u03b2\u0302i \u2264 di, i \u2208 BI , (B.1b)\ng\u0302M k+1\n2\n= 0, \u03b2\u0302O k+1\n2\n= 0, \u03b2\u0302I k+ 1\n2\n= dI , (B.1c)\ny \u22a4 \u03b2\u0302 = 0. (B.1d)\nThen, \u03b2\u03020, \u03b2\u0302 and g\u0302 are equal to \u03b20, \u03b2 and g, respectively, where \u03c0 is determined by the update rule\nMk = Mk+ 1 2 \u222a {i | \u03b2i > 0, gi = 0, i \u2208 BO}\n\u222a {i | \u03b2i < di, gi = 0, i \u2208 BI},\nOk = Ok+ 1 2 \u222a {i | \u03b2i = 0, gi \u2265 0, i \u2208 BO},\nIk = Ik+ 1 2 \u222a {i | \u03b2i = di, gi \u2264 0, i \u2208 BI},\n(B.2)\nusing \u03b2\u0302 and g\u0302.\nProof. Since the conditions (B.1a) and (B.1b) hold, all of the elements of B is assigned to one of the three index sets by (B.2). From the definitions of Mk+1, Ok+1 and Ik+1 (B.2), we see g\u0302Mk+1 = 0, \u03b2\u0302Ok+1 = 0 and \u03b2\u0302Ik+1 = dIk+1 . Using these three equations and (B.1d), we can easily obtain the same linear system as (A.1).\nNext, we consider theorem 2.\nTheorem 2. Let \u03b2\u03020, \u03b2\u0302 and g\u0302 be the optimal solutions of the following QP problem:\nmin \u03b2\u03020, \u0302\u03b2,g\u0302\n\u2211\ni\u2208BO\ng\u0302i\u03b2\u0302i + \u2211\ni\u2208BI\ng\u0302i(\u03b2\u0302i \u2212 di) (B.3)\ns.t.\n \n\ng\u0302BO \u2265 0, \u03b2\u0302BO \u2265 0, g\u0302BI \u2264 0, \u03b2\u0302BI \u2264 dI ,\ng\u0302M k+1\n2\n= 0, \u03b2\u0302O k+1\n2\n= 0, \u03b2\u0302I k+1\n2\n= dI k+ 1\n2\n,\ny \u22a4 \u03b2\u0302 = 0, g\u0302 = Q\u03b2\u0302 + y\u03b2\u03020,\nand \u03c0 is determined by (B.2) using \u03b2\u0302 and g\u0302. Then \u03b2\u03020, \u03b2\u0302 and g\u0302 satisfy (B.1) and they are equal to the gradient \u03b20, \u03b2 and g, respectively.\nProof. In this proof, we omit subscript k+ 1 2 to simplify the notation. First, we rewrite the optimization problem (B.3) as follows:\nmin \u03b2\u03020, \u0302\u03b2,g\u0302\n\u2211\ni\u2208BO\u222aM\u222aO\ngi\u03b2\u0302i + \u2211\ni\u2208BI\u222aI\ngi(\u03b2\u0302i \u2212 di)\ns.t.\n \n\ng\u0302 = Q\u03b2\u0302 + y\u03b2\u03020,\ng\u0302BO \u2265 0, \u03b2\u0302BO \u2265 0,\ng\u0302BI \u2264 0, \u03b2\u0302BI \u2264 dI ,\ng\u0302M = 0, \u03b2\u0302O = 0, \u03b2\u0302I = dI , y \u22a4 \u03b2\u0302 = 0.\nAlthough we slightly modified the expression of the objective function, its value is the same as (B.3) as long as the equality constraints hold. From the inequality constraints, we see that the objective value is always non-negative in the feasible region.\nTo simplify the notation, we introduce the following new variables:\n\u03b2\u0303 = E(2)d+E\u03b2\u0302, g\u0303 = Eg\u0302,\ny\u0303 = Ey,\nQ\u0303 = EQE,\nwhere\nE (1) ij = { 1 for {(i, j) | i = j, i \u2208 BO \u222aM\u222aO}, 0 others ,\nE (2) ij = { 1 for {(i, j) | i = j, i \u2208 BI \u222a I}, 0 others ,\nE = E(1) \u2212E(2).\nMoreover, if we set T = O \u222a I, the optimization problem (B.3) is written as\nmin \u03b20, \u02dc\u03b2,g\u0303 \u03b2\u0303\n\u22a4 Q\u0303\u03b2\u0303 + r0\u03b20 + r \u22a4 \u03b2\u0303\ns.t.\n \n\nQ\u0303\u03b2\u0303 + y\u0303\u03b20 + r \u2212 g\u0303 = 0,\ng\u0303M = 0, g\u0303B \u2265 0,\n\u03b2\u0303T = 0, \u03b2\u0303B \u2265 0, y\u0303 \u22a4 \u03b2\u0303 = r0,\nwhere\nr = \u2212Q\u0303:,IdI \u2212 Q\u0303:,BcIdBI , r0 = \u2212y\u0303 \u22a4 I dI \u2212 y\u0303BcIdBI ,\nare constants. Let \u03be \u2208 Rn,\u00b5M \u2208 R |M|,\u00b5B \u2208 R |B|,\u03bdT \u2208 R |T |,\u03bdB \u2208 R |B|, \u03c1 \u2208 R be the Lagrange multipliers. Then, the Lagrangian is\nL = \u03b2\u0303 \u22a4 Q\u0303\u03b2\u0303 + r0\u03b20 + r \u22a4 \u03b2\u0303 + \u03be\u22a4 ( Q\u0303\u03b2\u0303 + y\u0303\u03b20 + r \u2212 g\u0303 )\n+\u00b5\u22a4Mg\u0303M \u2212 \u00b5 \u22a4 B g\u0303B + \u03bd \u22a4 T \u03b2\u0303T \u2212 \u03bd \u22a4 B \u03b2\u0303B +\u03c1 ( y\u0303 \u22a4 \u03b2\u0303 \u2212 r0 ) ,\nwhere \u00b5B \u2265 0, \u03bdB \u2265 0. Differentiating L, we obtain\n\u2202L \u2202\u03b2\u0303 = 2Q\u0303\u03b2\u0303 + r + Q\u0303\u03be + \u03bd\u0303 + \u03c1y\u0303 = 0,\n\u2202L \u2202\u03b20 = r0 + \u03be \u22a4 y\u0303 = 0,\n\u2202L \u2202g\u0303 = \u2212\u03be + \u00b5\u0303 = 0,\nwhere \u03bd\u0303 \u2208 Rn is a vector whose components are \u03bd\u0303M = 0, \u03bd\u0303B = \u2212\u03bdB, \u03bd\u0303T = \u03bdT and \u00b5\u0303 \u2208 Rn has \u00b5\u0303M = \u00b5M, \u00b5\u0303B = \u2212\u00b5B, \u00b5\u0303T = 0. Using these equations, we obtain the following dual problem:\nmax \u02dc\u03b2,\u03be,\u03bd\u0303 ,\u00b5\u0303,\u03c1\n\u2212\u03b2\u0303 \u22a4 Q\u0303\u03b2\u0303 \u2212 \u03c1r0 + \u03be \u22a4 r (B.4)\ns.t.\n \n\n2Q\u0303\u03b2\u0303 + r + Q\u0303\u03be + \u03bd\u0303 + \u03c1y\u0303 = 0,\nr0 + \u03be \u22a4 y\u0303 = 0,\n\u2212 \u03be + \u00b5\u0303 = 0,\n\u03bd\u0303M = 0, \u03bd\u0303B \u2264 0,\n\u00b5\u0303T = 0, \u00b5\u0303B \u2264 0.\n(B.5)\nUsing the constraints of this problem (B.5), we can derive the following bound of the objective function (B.4):\n\u2212 \u03b2\u0303 \u22a4 Q\u0303\u03b2\u0303 \u2212 \u03c1r0 + \u03be \u22a4 r = \u2212\u03b2\u0303 \u22a4 Q\u0303\u03b2\u0303 + \u03c1\u03be\u22a4y\u0303 + \u03be\u22a4r\n= \u2212\u03b2\u0303 \u22a4 Q\u0303\u03b2\u0303 \u2212 2\u03b2\u0303 \u22a4 Q\u0303\u03be \u2212 \u03be\u22a4Q\u0303\u03be \u2212 \u03be\u22a4\u03bd\u0303\n= \u2212(\u03b2\u0303 + \u03be)\u22a4Q\u0303(\u03b2\u0303 + \u03be)\u2212 \u03be\u22a4\u03bd\u0303\n= \u2212(\u03b2\u0303 + \u03be)\u22a4Q\u0303(\u03b2\u0303 + \u03be)\u2212 \u00b5\u0303\u22a4\u03bd\u0303 \u2264 0.\nFrom this we see that the dual objective function is less than or equal to 0. Thus, the optimal objective value of the optimization problem is 0. Then the conditions (B.1) is satisfied. From lemma 1, the claim is proved."}, {"heading": "C Reformulate the Optimization Problem (10)", "text": "We reformulate the optimization problem (10) to reduce the number of variables and constraints. Here again, we omit subscript of M, O and I to simplify the notation.\nDefine B = {b1, . . . , b|B|}, SO = {i \u2208 {1, . . . , |B|} | bi \u2208 BO} and SI = {i \u2208 {1, . . . , |B|} | bi \u2208 BI}. When |B| 6= 0, the optimization problem (10) can be reformulated as\nmin \u03b2\nB\n\u03b2 \u22a4 BQ \u2032 \u03b2B + (vB \u2212Q \u2032 :,SI dBI ) \u22a4 \u03b2B\ns.t.\n \n\nQ \u2032 SO ,: \u03b2B + vBO \u2265 0, Q \u2032 SI ,: \u03b2B + vBI \u2264 0, \u03b2BO \u2265 0, \u03b2BI \u2264 dBI ,\nwhere\nQ \u2032 = QB \u2212 [ yB QB,M ] M \u22121\n[ y\u22a4B\nQB,M\n] ,\nu = \u2212M\u22121 [\ny\u22a4I QM,I\n] dI ,\nv = [ y Q:,M ] u+Q:,IdI .\nOn the other hand, when |B| = 0, (10) becomes\nmin \u03b2\nB ,\u03b20\n\u03b2 \u22a4 BQB\u03b2B + (QB,IdI \u2212QB,BIdBI ) \u22a4 \u03b2B\n\u2212(y\u22a4I dI + y \u22a4 BI dBI )\u03b20\ns.t.\n \n\ny \u22a4 B\u03b2B + y \u22a4 I dI = 0 QBO ,B\u03b2B +QBO ,IdI + yBO\u03b20 \u2265 0 QBI ,B\u03b2B +QBI ,IdI + yBI\u03b20 \u2264 0 \u03b2BO \u2265 0, \u03b2BI \u2264 dBI ."}], "references": [{"title": "The optimal set and optimal partition approach to linear and quadratic programming", "author": ["A.B. Berkelaar", "K. Roos", "T. Terl\u00e1ky"], "venue": "Advances in Sensitivity Analysis and Parametric Programming,", "citeRegEx": "Berkelaar et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Berkelaar et al\\.", "year": 1997}, {"title": "An algorithm for the solution of the parametric quadratic programming problem", "author": ["M.J. Best"], "venue": "Technical Report 82-24,", "citeRegEx": "Best,? \\Q1982\\E", "shortCiteRegEx": "Best", "year": 1982}, {"title": "New finite pivoting rules for the simplex method", "author": ["R.G. Bland"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bland,? \\Q1977\\E", "shortCiteRegEx": "Bland", "year": 1977}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "L. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Sensitivity analysis for nonlinear programming using penalty methods", "author": ["A.V. Fiacco"], "venue": "Mathematical Programming,", "citeRegEx": "Fiacco,? \\Q1976\\E", "shortCiteRegEx": "Fiacco", "year": 1976}, {"title": "Pathwise coordinate optimization", "author": ["J. Friedman", "T. Hastie", "H. H\u00f6fling", "R. Tibshirani"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Friedman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2007}, {"title": "An exponential lower bound on the complexity of regularization", "author": ["B. G\u00e4rtner", "J. Giesen", "M. Jaggi"], "venue": "paths. CoRR,", "citeRegEx": "G\u00e4rtner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "G\u00e4rtner et al\\.", "year": 2009}, {"title": "Approximating parameterized convex optimization problems", "author": ["J. Giesen", "M. Jaggi", "S. Laue"], "venue": "18th European Symposium on Algorithms,", "citeRegEx": "Giesen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Giesen et al\\.", "year": 2010}, {"title": "The entire regularization path for the support vector machine", "author": ["T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hastie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2004}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J.C. Platt"], "venue": "Advances in Kernel Methods \u2014 Support Vector Learning,", "citeRegEx": "Platt,? \\Q1999\\E", "shortCiteRegEx": "Platt", "year": 1999}, {"title": "On parametric linear and quadratic programming problems", "author": ["K. Ritter"], "venue": "Mathematical Programming: Proceedings of the International Congress on Mathematical Programming,", "citeRegEx": "Ritter,? \\Q1984\\E", "shortCiteRegEx": "Ritter", "year": 1984}], "referenceMentions": [{"referenceID": 4, "context": "Recently, the solution path algorithm (Efron et al., 2004; Hastie et al., 2004; Cauwenberghs & Poggio, 2001) has been widely recognized as one of the effective tools in machine learning.", "startOffset": 38, "endOffset": 108}, {"referenceID": 9, "context": "Recently, the solution path algorithm (Efron et al., 2004; Hastie et al., 2004; Cauwenberghs & Poggio, 2001) has been widely recognized as one of the effective tools in machine learning.", "startOffset": 38, "endOffset": 108}, {"referenceID": 1, "context": "This technique is originally developed as parametric programming in the optimization community (Best, 1982).", "startOffset": 95, "endOffset": 107}, {"referenceID": 7, "context": "Although some empirical studies suggest that the number of breakpoints grows linearly in the input size, in the worst case, it can grow exponentially (G\u00e4rtner et al., 2009).", "startOffset": 150, "endOffset": 172}, {"referenceID": 9, "context": "In fact, one of the popular SVM optimization algorithm, called sequential minimal optimization (SMO) Platt (1999), is known to produce suboptimal (approximated) solution, where the tolerance to the optimality (degree of approximated) can be specified by users.", "startOffset": 101, "endOffset": 114}, {"referenceID": 8, "context": "1 Giesen et al. (2010) proposed approximated path algorithm with some optimality guarantee that can be applicable to L2-SVM without bias term.", "startOffset": 2, "endOffset": 23}, {"referenceID": 9, "context": "It can be derived from the KKT conditions (3) and the similar properties are repeatedly used in various solution path algorithms in machine learning (Cauwenberghs & Poggio, 2001; Hastie et al., 2004).", "startOffset": 149, "endOffset": 199}, {"referenceID": 5, "context": "This theorem can be viewed as one of the specific forms of the sensitivity theorem Fiacco (1976). It can be derived from the KKT conditions (3) and the similar properties are repeatedly used in various solution path algorithms in machine learning (Cauwenberghs & Poggio, 2001; Hastie et al.", "startOffset": 83, "endOffset": 97}, {"referenceID": 11, "context": "This situation can be interpreted as what is called degeneracy in the parametric programming (Ritter, 1984).", "startOffset": 93, "endOffset": 107}, {"referenceID": 10, "context": "This situation can be interpreted as what is called degeneracy in the parametric programming (Ritter, 1984). Here, degeneracy means that multiple constraints hit their boundaries of inequalities simultaneously. Although degenerate situation rarely happens in conventional solution path algorithms, it is not the case in ours. The simultaneous change of multiple data points inevitably brings about \u201chighly\u201d degenerate situations involved with many constraints. In degenerate case, we have a problem called the cycling. For example, if we move two indices i and j from M to O at the breakpoint, then both or either of them may immediately return to M. To avoid the cycling, we need to design an update strategy for \u03c0 that can circumvent cycling. The degeneracy can be handled by several approaches which are known in the parametric programming literature. Ritter (1984) showed that the cycling can be dealt with through the well-known Bland\u2019s minimum index rule in the", "startOffset": 94, "endOffset": 869}, {"referenceID": 2, "context": "linear programming (Bland, 1977).", "startOffset": 19, "endOffset": 32}, {"referenceID": 0, "context": "In this paper, we provide more essential solution to this problem based on (Berkelaar et al., 1997).", "startOffset": 75, "endOffset": 99}], "year": 2013, "abstractText": "We consider a suboptimal solution path algorithm for the Support Vector Machine. The solution path algorithm is an effective tool for solving a sequence of a parametrized optimization problems in machine learning. The path of the solutions provided by this algorithm are very accurate and they satisfy the optimality conditions more strictly than other SVM optimization algorithms. In many machine learning application, however, this strict optimality is often unnecessary, and it adversely affects the computational efficiency. Our algorithm can generate the path of suboptimal solutions within an arbitrary user-specified tolerance level. It allows us to control the trade-off between the accuracy of the solution and the computational cost. Moreover, We also show that our suboptimal solutions can be interpreted as the solution of a perturbed optimization problem from the original one. We provide some theoretical analyses of our algorithm based on this novel interpretation. The experimental results also demonstrate the effectiveness of our algorithm.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}