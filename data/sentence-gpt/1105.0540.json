{"id": "1105.0540", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2011", "title": "Pruning nearest neighbor cluster trees", "abstract": "Nearest neighbor (k-NN) graphs are widely used in machine learning and data mining applications, and our aim is to better understand what they reveal about the cluster structure of the unknown underlying distribution of points. Moreover, is it possible to identify spurious structures that might arise due to sampling variability?\n\n\n\n\nWe are working on an algorithm for detecting signal shifts within a given cluster in a very similar way to the previous work of this paper. We believe that it is very unlikely that it will solve many problems, including the problem of random number generation, but it can be a useful tool in understanding the distribution of points.\nThe cluster structure of the unknown underlying distribution of points is a natural progression as you are working on a new problem. For example, when the probability of a cluster has declined dramatically, you might discover that the chances of a cluster has risen dramatically as you do not have a large number of clusters and fewer clusters. But for such a problem, this is the case. We have now established that the probability of a cluster having a small number of points has decreased, and therefore the probability of a cluster having a large number of points has decreased.\nIn our original paper, we examined the clustering of the unknown underlying distribution of points, but in this paper we have used the same set of clustering techniques to detect clustering. In order to detect signal shifts in different clusters, we propose that the probability of a cluster having a small number of points has increased, and thus you are able to identify distinct clusters where the probability of a cluster having a large number of points has decreased. For example, if a cluster has a large number of points, then you might have found that there are many clusters within a cluster with low probability of a cluster having a large number of points. In fact, the probability of a cluster having a large number of points has decreased, and therefore the probability of a cluster having a large number of points has increased. We are exploring this possibility in a previous paper in which we used the same set of clustering techniques to detect clustering, but in this paper we have also used the same set of clustering techniques to detect clustering. The clustering technique is similar in scope to the method of clustering in other parts of the world, with different features and approaches. For example, we have also used the clustering technique in other parts of the world, with different features and approaches. In contrast, there is a significant number of points on a cluster that is not related to each", "histories": [["v1", "Tue, 3 May 2011 10:34:25 GMT  (85kb)", "https://arxiv.org/abs/1105.0540v1", null], ["v2", "Thu, 5 May 2011 14:13:49 GMT  (85kb)", "http://arxiv.org/abs/1105.0540v2", null]], "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["samory kpotufe", "ulrike von luxburg"], "accepted": true, "id": "1105.0540"}, "pdf": {"name": "1105.0540.pdf", "metadata": {"source": "META", "title": "Pruning nearest neighbor cluster trees", "authors": ["Samory Kpotufe"], "emails": ["SAMORY@TUEBINGEN.MPG.DE", "ULRIKE.LUXBURG@TUEBINGEN.MPG.DE"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 5.\n05 40\nv2 [\nst at\n.M L\n] 5\nM ay\n2 01\nOur first contribution is a statistical analysis that reveals how certain subgraphs of a k-NN graph form a consistent estimator of the cluster tree of the underlying distribution of points. Our second and perhaps most important contribution is the following finite sample guarantee. We carefully work out the tradeoff between aggressive and conservative pruning and are able to guarantee the removal of all spurious cluster structures at all levels of the tree while at the same time guaranteeing the recovery of salient clusters. This is the first such finite sample result in the context of clustering."}, {"heading": "1. Introduction", "text": "In this work, we consider the nearest neighbor (k-NN) graph where each sample point is linked to its nearest neighbors. These graphs are widely used in machine learning and data mining applications, and interestingly there is still much to understand about their expressiveness. In particular we would like to better understand what such a graph on a finite sample of points might reveal about the cluster structure of the underlying distribution of points. More importantly we are interested in whether one can identify spurious structures that are artifacts of sampling variability, i.e. spurious structures that are not representative of the true cluster structure of the distribution.\nOur first contribution is in exposing more of the richness\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\nFigure 1. A density f (black line) and its cluster tree (dashed). The CCs of 3 level sets are shown in lighter color at the bottom.\nof k-NN graphs. Let Gn be a k-NN graph over an nsample from a distributionF with density f . Previous work (Maier et al., 2009) has shown that the connected components (CC) of a given level set of f can be approximated by the CCs of some subgraph of Gn, provided the level set satisfies certain boundary conditions. However it remained unclear whether or when all level sets of f might satisfy these conditions, in other words, whether the CCs of any level set can be recovered. We show under mild assumptions on f that CCs of any level set can be recovered by subgraphs of Gn for n sufficiently large. Interestingly, these subgraphs are obtained in a rather simple way: just remove points from the graph in decreasing order of their k-NN radius (distance to the k\u2019th nearest neighbor), and we obtain a nested hierarchy of subgraphs which approximates the cluster tree of F , i.e. the nested hierarchy formed by the level sets of f (see Figure 1, also Section 2.1).\nOur second, and perhaps more important contribution is in providing the first concrete approach in the context of clustering that guarantees the pruning of all spurious cluster structures at any tree level. We carefully work out the tradeoff between pruning \u201caggressively\u201d (and potentially removing important clusters) and pruning \u201cconservatively\u201d (with the risk of keeping spurious clusters) and derive tuning settings that require no knowledge of the underlying distribution beyond an upper bound on f . We can thus guarantee in a finite sample setting that (a) all clusters remaining at any level of the pruned tree correspond to CCs of some level set of f , i.e. all spurious clusters are pruned away, and (b) salient clusters are still discovered, where the\ndegree of saliency depends on the sample size n. We can show furthermore that the pruned tree remains a consistent estimator of the underlying cluster tree, i.e. the CCs of any level set of f are recovered for sufficiently large n. Interestingly, the pruning procedure is not tied to the k-NN method, but is based on a simple intuition that can be applied to other cluster tree methods (see Section 3).\nOur results rely on a central \u201cconnectedness\u201d lemma (Section 5.2) that identifies which CCs of f remain connected in the empirical tree. This is done by analizing the way in which k-NN radii vary along a path in a dense region."}, {"heading": "1.1. Related work", "text": "Recovering the cluster tree of the underlying density is a clean formalism of hierarchical clustering proposed in 1981 by J. A. Hartigan (Hartigan, 1981). Hartigan showed in the same seminal paper that the single-linkage algorithm is a consistent estimator of the cluster tree for densities on R. For Rd, d > 1 it is known that the empirical cluster tree of a consistent density estimate is a consistent estimator of the underlying cluster tree (see e.g. (Wong & Lane, 1983)), unfortunately there is no known algorithm for computing this empirical tree. Nonetheless, the idea has led to the development of interesting heuristics based on first estimating density, then approximating the cluster tree of the density estimate in high dimension (Wong & Lane, 1983; Stueltze & Nugent, 2010).\nMany other related work such as (Rigollet & Vert, 2009; Singh et al., 2009; Maier et al., 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al., 2009) which uses a k-NN graph for level set estimation. As previously discussed, level set estimation however never led to a consistent estimator of the cluster tree, since these results typically impose technical requirements on the level set being recovered but do not work out how or when these requirements might be satisfied by all level sets of a distribution.\nA recent insightful paper of Chaudhuri & Dasgupta (2010) presents the first provably consistent algorithm for estimating the cluster tree. At each level of the empirical cluster tree, they retain only those samples whose k-NN radii are below a scale parameter r which indexes the level; CCs at this level are then discovered by building an rneighborhood graph on the retained samples. This is similar to an earlier generalization of single-linkage by Wishart (1969) which however was given without a convergence analysis. The k-NN tree studied here differs in that, at an equivalent level r, points are connected to the subset of their k-nearest neighbors retained at that level. One practical appeal of our method is its simplicity: we need only remove points from an initial k-NN graph to obtain the var-\nious levels of the empirical cluster tree.\n(Chaudhuri & Dasgupta, 2010) provides finite sample results for a particular setting of k \u2248 logn. In contrast our finite sample results are given for a wide range of values of k, namely for logn . k . n1/O(d). In both cases the finite sample results establish natural separation conditions under which the CCs of level sets are recovered (see Theorem 1). The result of (Chaudhuri & Dasgupta, 2010) however allows the possibility that some empirical clusters are just artifacts of sampling variability. We provide a simple pruning procedure that ensures that clusters discovered empirically at any level correspond to true clusters at some level or the underlying cluster tree. Note that this can be trivially guaranteed by returning a single cluster at all levels, so we additionally guarantee that the algorithm discovers salient modes of the density, where the saliency depends on empirical quantities (see Theorem 2).\nA recent archived paper (Rinaldo et al., 2010) also treats the problem of false clusters in cluster tree estimation, but the result is not algorithmic as they only consider the cluster tree of an empirical density estimate, and do not provide a way to compute this cluster tree.\nThere exist many pruning heuristics in the literature which typically consist of removing small clusters (Maier et al., 2009; Stueltze & Nugent, 2010) using some form of thresholding. The difficulty with these approaches is in how to define small without making strong assumptions on the unknown underlying distribution, or on the tree level being pruned (levels correspond to different resolutions or cluster sizes). Moreover, even the assumption that spurious clusters must be small does not necessarily hold. Consider for example a cluster made up of two large regions connected by a thin bridge of low mass; the two large regions can easily appear as two separate clusters in a finite sample. Some more sophisticated methods such as (Stueltze & Nugent, 2009) do not rely on cluster size for pruning, instead they return confidence values for the empirical clusters based on various notions of cluster stability; unfortunately they do not provide finite sample guarantees. Our pruning guarantees the removal of all spurious clusters, large and small (see Figure 2); we make no assumption on the shape of clusters beyond a smoothness assumption on the density; we provide a simple tuning parameter whose setting requires just an upper bound on the density."}, {"heading": "2. Preliminaries", "text": "Assume the finite dataset X = {Xi}ni=1 is drawn i.i.d. from a distribution F over Rd with density function f . We start with some simple definitions related to k-NN operations. All balls, unless otherwise specified, denote closed balls in Rd.\nDefinition 1 (k-NN radii). For x \u2208 X , let rk,n(x) denote the radius of the smallest ball centered at x containing k points from X \\ {x}. Also, let rk(x) denote the radius of the smallest ball centered at x of F -mass k/n. Definition 2 (k-NN and mutual k-NN graphs). The kNN graph is that whose vertices are the points in X, and where Xi is connected to Xj iff Xi \u2208 B(Xj , \u03b8rk(Xj)) or Xj \u2208 B(Xi, \u03b8rk(Xi)) for some \u03b8 > 0. The mutual k-NN graph is that where Xi is connected to Xj iff Xi \u2208 B(Xj , \u03b8rk(Xj)) and Xj \u2208 B(Xi, \u03b8rk(Xi))."}, {"heading": "2.1. Cluster tree", "text": "Definition 3 (Connectedness). We say A \u2282 Rd is connected if for every x, x\u2032 \u2208 A there exists a continuous 1\u22121 function P : [0, 1] 7\u2192 A where P (0) = x and P (1) = x\u2032. P is called a path in A between x and x\u2032.\nThe cluster tree of f will be denoted {G(\u03bb)}\u03bb>0, where G(\u03bb) are the CCs of the level set {x : f(x) \u2265 \u03bb}. Notice that {G(\u03bb)}\u03bb>0 forms a (infinite) tree hierarchy where for any two components A,A\u2032, either A \u2229 A\u2032 = \u2205 or one is a descendant of the other, i.e A \u2282 A\u2032 or A\u2032 \u2282 A."}, {"heading": "3. Algorithm", "text": "Definition 4 (k-NN density estimate). Define the density estimate at x \u2208 Rd as :\nfn(x) . =\nk\nn \u00b7 vol (B(x, rk,n(x))) =\nk\nn \u00b7 vdrdk,n(x) ,\nwhere vd is the volume of the unit ball in Rd.\nLet Gn be the k-NN or mutual k-NN graph. For \u03bb > 0 define Gn(\u03bb) as the subgraph of Gn containing only vertices in {Xi : fn(Xi) \u2265 \u03bb} and corresponding edges. The CCs of {Gn(\u03bb)}\u03bb>0 form a tree: let An and A\u2032n be two such CCs, either An \u2229 A\u2032n = \u2205 or one is a descendant of the other, i.e. An is a subgraph of A\u2032n or vice versa. To simplify notation, we let the set {Gn(\u03bb)}\u03bb>0 denote the empirical cluster tree before pruning.\nPruning\nThe pruning procedure (Algorithm 1) consists of simple lookups: it reconnects CCs at level \u03bb if they are part of the same CC at level \u03bb \u2212 \u01eb\u0303 where the tuning parameter \u01eb\u0303 \u2265 0 controls how aggressively we prune. We show its behavior on a finite sample in Figure 2.\nThe intuition behind the procedure is the following. Suppose An, A\u2032n \u2282 X are disconnected at some level \u03bb in the empirical tree before pruning. However, they ought to be connected, i.e. their vertices belong to the same CC A at the highest level where they are all contained in the underlying cluster tree. Then, key sample points from A that\nwould have kept them connected are missing at level \u03bb in the empirical tree. These key points have fn values lower than \u03bb, but probably not much lower. By looking down to a lower level near \u03bb we find that An, A\u2032n are connected and thus detect the situation. Notice that this intuition is not tied to the k-NN cluster tree but can be applied to any other cluster tree procedure. All that is required is that all points from A (as discussed above) be connected at some level in the tree close to \u03bb.\nAlgorithm 1 Prune Gn(\u03bb) Given: tuning parameter \u01eb\u0303 \u2265 0, same for all levels. G\u0303n(\u03bb)\u2190 Gn(\u03bb). if \u03bb > \u01eb\u0303 then\nConnect components An, A\u2032n of G\u0303n(\u03bb) if they are part of the same component of Gn(\u03bb\u2212 \u01eb\u0303).\nelse Connect all G\u0303n(\u03bb). end if\nIt is not hard to see that the CCs of the pruned subgraphs{ G\u0303n(\u03bb) }\n\u03bb>0 still form a tree. We will hence denote the\npruned empirical tree by { G\u0303n(\u03bb) }\n\u03bb>0 ."}, {"heading": "4. Results Overview", "text": "We make the following assumptions on the density f .\n(A.1) \u2203F > 0, supx\u2208Rd f(x) \u2264 F . (A.2) f is Hoelder-continuous, i.e. there exists L, \u03b1 > 0\nsuch that for all x, x\u2032 \u2208 Rd,\n|f(x)\u2212 f(x\u2032)| \u2264 L \u2016x\u2212 x\u2032\u2016\u03b1 .\nTheorem 1 below is a finite sample result that establishes conditions under which samples from a connected subset\nof Rd remain connected in the empirical cluster tree, and samples from two disconnected subsets of Rd remain disconnected even after pruning. Essentially, for k sufficiently large, points from connected subsets A remain connected below some level. Also, provided k is not too large, disjoint subsets A and A\u2032 which are separated by a large enough region of low density (relative to n, k and \u01eb\u0303), remain disconnected above some level.\nWe require the following two definitions.\nDefinition 5 (Envelope of A \u2282 Rd). Let A \u2282 Rd and for r > 0, define: A+r . = {y : \u2203x \u2208 A, y \u2208 B(x, r)} .\nDefinition 6 ((\u01eb, r)-separated sets ). A,A\u2032 \u2282 Rd are (\u01eb, r)separated if there exists a separating set S such that every path in Rd between A and A\u2032 intersects S, and\nsup x\u2208S+r f(x) \u2264 inf x\u2208A\u222aA\u2032 f(x)\u2212 \u01eb.\nTheorem 1. Suppose f satisfies (A.1) and (A.2). Let Gn be the k-NN or mutual k-NN graph. Let \u03b4 > 0 and define \u01ebk . = 11F \u221a ln(2n/\u03b4)/k. There exist C and C\u2032 = C\u2032(F) such that, for\nC ( max { 1, \u221a 2/\u03b8 })d d ln(n/\u03b4)\n\u2264 k \u2264 C\u2032 ( F \u221a ln(n/\u03b4) )2(\u03b1+d)/(3\u03b1+d) n2\u03b1/(3\u03b1+d) (1)\nthe following holds with probability at least 1 \u2212 3\u03b4 simultaneously for subsets A of Rd.\n(a) Let A be a connected subset of Rd, and let \u03bb . =\ninfx\u2208A f(x) > 2\u01ebk. All points in A \u2229 X belong to the same CC of G\u0303n(\u03bb\u2212 2\u01ebk).\n(b) Let A and A\u2032 be two disjoints subsets of Rd, and define \u03bb = infx\u2208A\u222aA\u2032 f(x). Recall that \u01eb\u0303 \u2265 0 is the tuning parameter. Suppose A and A\u2032 are (\u01eb, r)-separated for\n\u01eb = 6\u01ebk + 2\u01eb\u0303 and r = \u03b82 (4k/vdn\u03bb) 1/d. Then A \u2229X and A\u2032 \u2229X are disconnected in G\u0303n(\u03bb\u2212 2\u01ebk).\nTheorem 1 above, although written in terms of G\u0303n, applies also to Gn by just setting \u01eb\u0303 = 0. The theorem implies consistency of both pruned and unpruned k-NN trees under mild additional conditions. Some such conditions are illustrated in the corollary below. A nice practical aspect of the pruning procedure is that consistency is obtained for a wide range of settings of \u01eb\u0303 and k as functions of n.\nCorollary 1 (Consistency). Suppose that f satisfies (A.1) and (A.2) and that, in addition, F is supported on a compact set, and for any \u03bb > 0, there are finitely many components in G(\u03bb). Assume that, as n \u2192 \u221e, \u01eb\u0303 = \u01eb\u0303(n) \u2192 0 and k/ logn \u2192 0 while k = k(n) satisfies (1).\nFor any A \u2282 Rd, let An denote the smallest component of { G\u0303n(\u03bb) }\n\u03bb>0 containing A \u2229 X. Fix \u03bb > 0. We have\nlimn\u2192\u221e P (\u2200A,A\u2032 \u2208 G(\u03bb), An is disjoint from A\u2032n) = 1.\nProof. Let A and A\u2032 be separate components of G(\u03bb). The assumptions ensure that all paths between A and A\u2032 traverse a compact set S satisfying \u03bb\u2212maxx\u2208S f(x) .= \u01ebS > 0 (see Lemma 14 of (Chaudhuri & Dasgupta, 2010)). Let \u01eb = 6\u01ebk + 2\u01eb\u0303 and r = \u03b82 (4k/vdn\u03bb)\n1/d. By uniform continuity of f , there exists N1 such that for n > N1, r is small enough so that \u03bb \u2212 maxx\u2208S+r f(x) > \u01ebS/2. Also, there exists N2 > N1 such that for n > N2, \u01eb < \u01ebS/2, in other words supx\u2208S+r f(x) \u2264 \u03bb\u2212 \u01eb. Since Gn(\u03bb) is finite, there exists N such that for n > N , all pairs A,A\u2032 have a suitable (\u01eb, r)-separating set S. Thus by Theorem 1, for n > N , with probability at least 1\u2212 3\u03b4, \u2200A,A\u2032 \u2208 G(\u03bb), A \u2229X and A\u2032 \u2229X are fully contained in G\u0303n(\u03bb\u2212 2\u01ebk) and are disjoint. They are thus disjoint at any higher level, so An and A\u2032n are also disjoint.\nThe above holds for all \u03b4 > 0, so the statement follows.\nWhile Theorem 1 establishes that a connected set A remains connected below some level, it does not guarantee against parts of A becoming disconnected at higher levels, creating spurious clusters. Note that the removal of spurious clusters can be trivially guaranteed by just letting the parameter \u01eb\u0303 very large, but the ability of the algorithm to discover true clusters is necessarily affected. We are interested in how to set \u01eb\u0303 in order to guarantee the removal of spurious clusters while still recovering important ones.\nTheorem 2 guarantees that, by setting \u01eb\u0303 as \u2126(\u01ebk) (recall \u01ebk from Theorem 1), separate CCs of the empirical cluster tree correspond to actual clusters of the (unknown) underlying distribution, i.e. all spurious clusters are removed. The setting of \u01eb\u0303 only requires an upper-boundF on the density f 1. Note that, under such a setting, consistency is maintained per Corollary 1, and in light of Theorem 1 (b), we can expect that interesting clusters are discovered. In particular the following salient modes of f are discovered.\nDefinition 7 ((\u01eb, r)-salient mode). An (\u01eb, r)-salient mode is a leaf node A of the cluster tree {G(\u03bb)}\u03bb>0 which has an ancestor Ak \u2283 A (possibly A itself) satisfying:\n(i) Ak is the ancestor of a single leaf of {G(\u03bb)}\u03bb>0, namely A.\n(ii) Ak is large: \u2203x \u2208 Ak, B(x, rk(x)) \u2282 Ak. 1We might just use maxi\u2208[n] fn(Xi) in practice, which in\nlight of Lemma 1 can be a good surrogate for F (see Figure 3).\n(iii) Ak is sufficiently separated from other components at its level: let \u03bb . = infx\u2208Ak f(x); Ak and\n({x : f(x) \u2265 \u03bb} \\Ak) are (\u01eb, r)-separated.\nNotice that, under the assumptions of Corollary 1, every mode of f is (\u01eb, r)-salient for sufficiently large k and 1/\u01eb\u0303.\nTheorem 2 (Pruning guarantees). Let \u03b4 > 0. Under the assumptions of Theorem 1, the following holds with probability at least 1\u2212 3\u03b4.\n(a) Suppose the tuning parameter \u01eb\u0303 \u2265 3\u01ebk. Consider two disjoint CCs An and A\u2032n at the same level in{ G\u0303n(\u03bb) }\n\u03bb>0 . Let V be the union of vertices of An\nand A\u2032n, and define \u03bb . = infx\u2208V f(x). The vertices of An and those of A\u2032n are in separate CCs of G(\u03bb).\n(b) Let \u01eb = 6\u01ebk + 2\u01eb\u0303 and r = \u03b82 (4k/vdn\u03bb) 1/d. There\nexists a 1 \u2212 1 map from the set of (\u01eb, r)-salient modes to the leaves of the empirical tree { G\u0303n(\u03bb) }\n\u03bb>0 .\nThe behavior of both the k-NN and mutual k-NN tree, as guaranteed in Theorem 2, is illustrated in Figure 3."}, {"heading": "5. Analysis", "text": "Theorem 1 follows from lemmas 3 and 6 below. These two lemmas depend on the events described by lemmas 1, 2 and 4 which happen with a combined probability of at least 1\u2212 3\u03b4 for a confidence parameter \u03b4 > 0.\nTheorem 2 follows from lemmas 5 and 7 below. These two lemmas also depend on the events described by lemmas 1, 2 and 4 which happen with a combined probability of at least 1\u2212 3\u03b4."}, {"heading": "5.1. Maintaining Separation", "text": "In this section we establish conditions under which points from two disconnected subsets of Rd remain disconnected in the empirical tree, even after pruning.\nThe following is an important lemma which establishes the estimation error of fn relative to f on the sample X. Interestingly, although of independent interest, we could not find this sort of finite sample statement in the literature on k-NN2, at least not under our assumptions. The proof, presented as supplement in the appendix, is a bit involved and starts with some intuition from an asymptotic analysis of (Devroye & Wagner, 1977) combined with a form of the Chernoff bound found in (Angluin & Valiant, 1979).\nLemma 1. Suppose f satisfies (A.1) and (A.2). There exists C = C(F) such that for \u03b4 > 0, for \u01eb = 11F \u221a ln(2n/\u03b4)/k and\n121 ln(2n/\u03b4)\n\u2264 k \u2264 C ( F \u221a ln(2n/\u03b4) )2(\u03b1+d)/(3\u03b1+d) n2\u03b1/(3\u03b1+d),\nwe have with probability at least 1 \u2212 \u03b4 that supXi\u2208X |fn(Xi)\u2212 f(Xi)| \u2264 \u01eb.\nThe next lemma bounds rk,n(Xi) in terms of rk(Xi), and hence, in terms of the density at Xi. The proof is provided as supplement in the appendix.\nLemma 2. Suppose f satisfies (A.1) and (A.2). Fix \u03bb > 0 and let L\u03bb .= {x : f(x) \u2265 \u03bb}.\n(a) Let r . = 12 (\u03bb/2L)\n1/\u03b1. We have \u2200x, x\u2032 \u2208 Rd, \u2016x\u2212 x\u2032\u2016 \u2264 2r =\u21d2 |f(x)\u2212 f(x\u2032)| \u2264 \u03bb/2. If in addition x \u2208 L\u03bb, it follows that f(x)/2 \u2264 f(x\u2032) \u2264 2f(x).\n(b) Suppose k \u2264 2\u2212(d+3)vd(2L)\u2212d/\u03b1\u03bb(d+\u03b1)/\u03b1n. We have\n\u2200x \u2208 L\u03bb, rk(x) \u2264 min { 2\u22123/dr, ( 2k\nvdnf(x)\n)1/d} .\nFor \u03b4 > 0, if in addition k \u2265 192 ln(2n/\u03b4), we have with probability at least 1\u2212 \u03b4 that for all Xi \u2208 X\u2229L\u03bb\n2\u22123/drk(Xi) \u2264 rk,n(Xi) \u2264 23/drk(Xi).\nThe main separation lemma is next. It says that if A and A\u2032 are separated by a sufficiently large low density region, then they remain separated in the empirical tree.\n2There are however many asymptotic analyses of k-NN methods such as (Devroye & Wagner, 1977).\nLemma 3 (Separation). Suppose f satisfies (A.1) and (A.2). Let Gn be the k-NN or mutual k-NN graph. Define \u01ebk . = 11F \u221a ln(2n/\u03b4)/k, and let \u03b4 > 0. There exists C = C(F) such that, for 192 ln(2n/\u03b4) \u2264 k\n\u2264 C ( F \u221a ln(n/\u03b4) )2(\u03b1+d)/(3\u03b1+d) n2\u03b1/(3\u03b1+d),\nthe following holds with probability at least 1 \u2212 2\u03b4 simultaneously for any two disjoint subsets A,A\u2032 of Rd.\nLet \u03bb = infx\u2208A\u222aA\u2032 f(x). If A and A\u2032 are (\u01eb, r)-separated for \u01eb = 6\u01ebk+2\u01eb\u0303 and r = \u03b82 (4k/vdn\u03bb)\n1/d, then A\u2229X and A\u2032 \u2229X are disconnected in Gn(\u03bb\u2212 2\u01ebk \u2212 \u01eb\u0303) and therefore in G\u0303n(\u03bb\u2212 2\u01ebk).\nProof. Applying Lemma 1, it\u2019s immediate that, with probability at least 1 \u2212 \u03b4, all points of any A \u222a A\u2032 \u2229 X are in Gn(\u03bb \u2212 \u01ebk) and lower levels, and no point from S+r \u2229X is in Gn(\u03bb \u2212 5\u01ebk \u2212 2\u01eb\u0303) or higher levels. Thus any path between A and A\u2032 in Gn(\u03bb \u2212 2\u01ebk \u2212 \u01eb\u0303) must have an edge through the center x \u2208 S of a ball B(x, r) \u2282 S+r. This edge must therefore have length greater than 2r. We just need to show that no such edge exists in Gn(\u03bb\u2212 2\u01ebk \u2212 \u01eb\u0303). Let V be the set of points (vertices) in Gn(\u03bb\u22122\u01ebk\u2212 \u01eb\u0303). By Lemma 1, minXi\u2208V f(Xi) \u2265 \u03bb\u22123\u01ebk\u2212\u01eb\u0303. Given the density assumption on S, \u03bb \u2265 6\u01ebk + 2\u01eb\u0303 so minXi\u2208V f(Xi) \u2265 \u03bb/2 and V \u2282 L\u01ebk . Now, given the range of k, Lemma 2 holds for the level set L\u01ebk . It follows that with probability at least 1\u2212\u03b4 (uniform over any such choice of A,A\u2032 since the event is a function of L\u01ebk ),\nmax Xi\u2208V rk,n(Xi) \u2264 23/d max Xi\u2208V\nrk(Xi) \u2264 2r\n\u03b8 .\nThus, edge lengths in Gn(\u03bb\u2212 2\u01ebk \u2212 \u01eb\u0303) are at most 2r."}, {"heading": "5.1.1. IDENTIFYING MODES", "text": "As a corollary to Lemma 3, we can guarantee in Lemma 5 that certain salient modes are recovered by the empirical cluster tree. For this to happen, we require in Definition 7 (ii) that an (\u01eb, r)-salient mode A is contained in a sufficiently large set Ak so that we sample points near the mode.\nWe start with the following VC lemma establishing conditions under which subsets of Rd contain samples from X.\nLemma 4 (Lemma 5.1 of (Bousquet et al., 2004)). Suppose C is a class of subsets of Rd. Let SC(2n) denote the 2n-shatter coefficient of C. Let Fn denote the empirical distribution over n samples drawn i.i.d from F . For \u03b4 > 0, with probability at least 1\u2212 \u03b4,\nsup A\u2208C F(A)\u2212Fn(A)\u221a F(A)\n\u2264 2 \u221a\nlogSC(2n) + log 4/\u03b4 n .\nLemma 5 (Modes). Suppose f satisfies (A.1) and (A.2). Let Gn be the k-NN or mutual k-NN graph. Let \u03b4 > 0. There exist C and C\u2032 = C\u2032(F) such that, for\nCd ln(n/\u03b4)\n\u2264 k \u2264 C\u2032 ( F \u221a ln(n/\u03b4) )2(\u03b1+d)/(3\u03b1+d) n2\u03b1/(3\u03b1+d)\nthe following holds with probability at least 1 \u2212 3\u03b4. Let \u01eb = 6\u01ebk + 2\u01eb\u0303 and r = \u03b82 (4k/vdn\u03bb)\n1/d. There exists a 1\u2212 1 map from the set of (\u01eb, r)-salient modes to the leaves of the empirical tree { G\u0303n(\u03bb) }\n\u03bb>0 .\nProof. First, with probability at least 1 \u2212 \u03b4, for any (\u01eb, r)salient mode A, there are samples in X from the containing set Ak (as defined in Definition 7). To arrive at this we apply Lemma 4 for the class C of all possible balls B \u2208 Rd, (for this class SC(2n) \u2264 (2n)d+1). We have with probability at least 1\u2212 \u03b4 that for all B, Fn(B) > 0 whenever\nF(B) \u2265 Cd ln(n/\u03b4) n > 4 (d+ 1) log(2n) + log(4/\u03b4) n ,\nwhere C is appropriately chosen to satisfy the last inequality. Now, from the definition of Ak, there exists x such that B(x, rk(x)) \u2282 Ak, while we have F(B(x, rk(x))) = k/n \u2265 Cd ln(n/\u03b4)/n, implying that Fn(Ak) \u2265 Fn(B(x, rk(x))) \u2265 1/n. As a consequence of the above argument, there is a finite number m of (\u01eb, r)-salient modes since each contributes some points to the final sample X. We can therefore arrange them as { Ai\n}m i=1\nso that for i < j, we have \u03bbi \u2264 \u03bbj where \u03bbi = infx\u2208Ai\nk f(x). An injective map can now be constructed iteratively as follows.\nStarting with i = 1, we have by Lemma 3 that, with probability at least 1 \u2212 2\u03b4, Aik \u2229 X is disconnected in G\u0303n(\u03bbi \u2212 2\u01ebk) from all Ajk, j > i. Let U be the union of those CCs of G\u0303n(\u03bbi\u22122\u01ebk) containing points from Aik\u2229X. We\u2019ve already established that U contains no point from any Ajk, j > i. For i > 1, U also contains no point from any Ajk, j < i. This is because, again by Lemma 3, A j k \u2229X is disconnected in G\u0303n(\u03bbj \u2212 2\u01ebk) from Aik \u2229 X, therefore disconnected from U since all CCs in U remain connected at lower levels. Now, since U is disconnected from all Ajk, j 6= i, we can just map Ai to any leaf rooted in U , Ai being the unique image of such a leaf."}, {"heading": "5.2. Maintaining Connectedness", "text": "In this section we show that sample points from a connected subset A of Rd remain connected in the empirical cluster tree before pruning (therefore also after pruning).\nSimilar to (Chaudhuri & Dasgupta, 2010), for any two points x, x\u2032 \u2208 A \u2229 X we uncover a path in Gn near\na path P in A that connects the two. The path in Gn (the dashed path depicted below) consists of a sequence x1 = x, x2, . . . , xi = x\n\u2032 of sample points from balls centered on the path P in A (the solid path depicted below). The intuition is that P is a high density route near which we can find enough sample points to connect x and x\u2032.\nx\nx \u2032\nThe balls centered on P must be chosen sufficiently small and consecutively close so that consecutive terms xi, xi+1 are adjacent in Gn. In (Chaudhuri & Dasgupta, 2010), points are adjacent (at any particular level) whenever they are less than some scale r apart; one can therefore choose balls of the same radius o(r) and consecutively o(r) close. In our particular case, no single scale determines adjacency. Adjacency is determined by the various nearest-neighbor radii and this creates a multiscale effect that complicates the analysis. One way to handle (and effectively get rid of) this multiscale effect is to choose balls on P of the same radius r corresponding to the smallest possible nearestneighbor radius in Gn (restricted to A\u2229X). However, in order to get samples in such small balls one would need rather large sample size n, so the idea results in weak bounds. We instead use an inductive argument which keeps track of the various scales, the intuition being that nearest-neighborradii have to change slowly along the path P from x to x\u2032.\nLemma 6 (Connectedness). Suppose f satisfies (A.1) and (A.2). Let Gn be the k-NN or mutual k-NN graph. Define \u01ebk . = 11F \u221a ln(2n/\u03b4)/k and let \u03b4 > 0. There exist C and C\u2032 = C\u2032(F) such that, for\nC ( max { 1, \u221a 2/\u03b8 })d d ln(n/\u03b4)\n\u2264 k \u2264 C\u2032 ( F \u221a ln(n/\u03b4) )2(\u03b1+d)/(3\u03b1+d) n2\u03b1/(3\u03b1+d),\nthe following holds with probability at least 1 \u2212 3\u03b4 simultaneously for all connected subsets A of Rd.\nLet \u03bb . = infx\u2208A f(x) > 2\u01ebk. All points in A \u2229X belong to the same CC of Gn(\u03bb\u2212 2\u01ebk), therefore of G\u0303n(\u03bb\u2212 2\u01ebk).\nProof. First, let C and C\u2032 be large enough for lemmas 1 and 2 to hold. Define r\n. = 12 (\u01ebk/2L) 1/\u03b1. By Lemma 2 (a), we have that f(x) \u2265 \u03bb\u2212 \u01ebk/2 for any x \u2208 A+r. Applying Lemma 1, it follows that with probability at least 1 \u2212 \u03b4 (uniform over choices of A), all points of A+r \u2229X are in Gn(\u03bb \u2212 2\u01ebk). We will show that A \u2229 X is connected in Gn(\u03bb \u2212 2\u01ebk) possibly through points in A+r \\A. In particular, any x, x\u2032 \u2208 A \u2229 X are connected through a sequence {xi}i>1 , xi \u2208 A+r \u2229 X built according to the\nfollowing procedure. Let P be a path in A between x and x\u2032. Define \u03c4 . = min { 1, \u03b8/ \u221a 2 } .\nStarting at i = 1 (x1 = x), set xi+1 = x\u2032 if \u2016xi \u2212 x\u2032\u2016 \u2264 \u03b8min {rk,n(xi), rk,n(x\u2032)}, and we\u2019re done, otherwise: Let yi be the point in P\u2229B ( xi, \u03c42 \u22129/drk,n(xi) ) farthest along the path P from x, i.e. P\u22121(yi) is highest in the set. Define the half-ball\nH(yi) . = {z : \u2016z \u2212 y\u2016 < \u03c42\u221218/drk,n(xi),\n(z \u2212 yi) \u00b7 (xi \u2212 yi) \u2265 0}.\nPick xi+1 in H(yi) \u2229X, and continue.\nThe rest of the argument will proceed inductively as follows. First, assume that xi \u2208 A+r and that yi exists. This is necessarily the case for x1, y1. Assume xi+1 6= x\u2032. We will show that xi+1 exists, is also in A+r, and is adjacent to xi in Gn. It will follow that yi+1 must exist (if the process does not end) and is distinct from y1, . . . , yi. We\u2019ll then argue that the process must also end.\nTo see that xi+1 exists (under the aforementioned assumptions), we apply Lemma 4 for the class C of all possible half-balls H(y) centered at y \u2208 Rd (for this class SC(2n) \u2264 (2n)2d+1). We have with probability at least 1\u2212 \u03b4 that for all H(y), Fn(H(y)) > 0 whenever\nF(H(y)) \u2265 C0d ln( n \u03b4 )\nn >\n(8d+ 4) log(2n) + 4 log(4\u03b4 )\nn ,\nwhere C0 is appropriately chosen to satisfy the last inequality. We next show F(H(yi)) satisfies the first inequality. We first apply Lemma 2 on L\u01ebk \u2283 A+r (this inclusion was established earlier). We have with probability at least 1 \u2212 \u03b4 (uniform over all A) that for xi \u2208 A+r, rk,n(xi) \u2264 23/drk(xi) \u2264 r. Thus, for all z \u2208 H(yi),\n\u2016z \u2212 xi\u2016 \u2264 2 \u00b7 \u03c42\u22129/drk,n(xi) \u2264 2 \u00b7 \u03c42\u22129/dr \u2264 2r, (2)\nimplying by the same Lemma 2 that f(z) \u2265 f(xi)/2. Now, from Lemma 1, fn(xi) \u2264 f(xi) + \u01ebk \u2264 2f(xi). We can thus write\nF(H(yi)) \u2265 1\n4 vol\n( B(yi, \u03c42 \u221218/drk,n(xi)) ) f(xi)\n= \u03c4d2\u221220 vol (B(xi, rk,n(xi))) f(xi)\n\u2265 \u03c4d2\u221221 vol (B(xi, rk,n(xi))) fn(xi)\n= \u03c4d2\u221221 k n \u2265 C0d ln(n/\u03b4) n , for C \u2265 221C0.\nTherefore there is a point xi+1 in H(yi) \u2229X. In addition xi+1 \u2208 A+r since it is within r of yi \u2208 A.\nNext we establish that there is an edge between xi and xi+1 in Gn. To this end we relate rk,n(xi+1) to rk,n(xi) by first relating rk(xi+1) to rk(xi). Remember that for z \u2208 A+r we have rk(z) < r so that for any z\u2032 \u2208 B(z, rk(z)) we have f(z)/2 \u2264 f(z\u2032) \u2264 2f(z). Also recall that we always have \u2016xi \u2212 xi+1\u2016 \u2264 2r (see (2)), implying f(xi+1) < 2f(xi). We then have\nvdr d k(xi) \u00b7\n1 2 f(xi) \u2264 k n \u2264 vdrdk(xi+1) \u00b7 2f(xi+1)\n\u2264 vdrdk(xi+1) \u00b7 4f(xi), where for the first two inequalities we used the fact that both balls B(xi, rk(xi)) and B(xi+1, rk(xi+1)) have the same mass k/n. It follows that\nrk,n(xi+1) \u2265 2\u22123/drk(xi+1) \u2265 2\u22126/drk(xi) \u2265 2\u22129/drk,n(xi), (3)\nimplying 2\u22129/drk,n(xi) \u2264 min {rk,n(xi), rk,n(xi+1)}. We then get\n\u2016xi \u2212 xi+1\u20162 = \u2016xi \u2212 yi\u20162 + \u2016xi+1 \u2212 yi\u20162\n\u2212 (xi \u2212 yi) \u00b7 (xi+1 \u2212 yi) \u2264 \u2016xi \u2212 yi\u20162 + \u2016xi+1 \u2212 yi\u20162 \u2264 2\u03c42 \u00b7min { r2k,n(xi), r 2 k,n(xi+1) } \u2264 \u03b82 min { r2k,n(xi), r 2 k,n(xi+1) } ,\nmeaning xi and xi+1 are adjacent in Gn.\nFinally we argue that yi+1 must exist. By (3) above we have\n\u2016xi+1 \u2212 yi\u2016 < \u03c42\u221218/drk,n(xi) \u2264 \u03c42\u22129/drk,n(xi+1),\nin other words the ball B ( xi+1, \u03c42 \u22129/drk,n(xi+1) )\ncontains yi \u2208 P in its interior. It follows by continuity of P that there is a point yi+1 in this ball further along the path from xi than yi. Thus, recursively all yi\u2019s must be distinct, implying that all xi\u2019s must be distinct. Since all xi\u2019s belong to the finite sample X the process must eventually terminate."}, {"heading": "5.2.1. PRUNING OF SPURIOUS BRANCHES", "text": "As a corollary to Lemma 6 we can guarantee in Lemma 7 that the pruning procedure will remove all spurious branchings, and hence, all spurious clusters.\nLemma 7 (Pruning). Let \u03b4 > 0. Under the assumptions of Lemma 6, the following holds with probability at least 1\u2212 3\u03b4, provided \u01eb\u0303 \u2265 3\u01ebk. Consider two disjoint CCs An and A\u2032n at the same level in{ G\u0303n(\u03bb) }\n\u03bb>0 . Let V be the union of vertices of An and\nA\u2032n, and define \u03bb . = infx\u2208V f(x). The vertices of An and those of A\u2032n are in separate CCs of G(\u03bb).\nProof. Let \u03bbn = minx\u2208V fn(x) be the level in the empirical tree containing An, A\u2032n. By Lemma 1, supx\u2208X |fn(x)\u2212 f(x)| \u2264 \u01ebk so \u03bbn \u2264 \u03bb + \u01ebk. Thus, we must have \u03bb > 2\u01ebk, since otherwise \u03bbn \u2264 \u01eb\u0303 implying G\u0303n(\u03bbn) must have a single connected component.\nNow suppose points in V were in the same componentA of G(\u03bb). By Lemma 6, all of A \u2229X is connected in Gn(\u03bb \u2212 2\u01ebk) and at lower levels. By the last argument\u03bbn\u2212 \u01eb\u0303 \u2264 \u03bb\u2212 2\u01ebk so the pruning procedure reconnects An and A\u2032n."}, {"heading": "Acknowledgements", "text": "We thank Sanjoy Dasgupta for interesting discussions which helped improve presentation."}, {"heading": "A. Proof of Lemma 1", "text": "Lemma 1 follows as a corollary to Lemma 9 below.\nWe\u2019ll often make use of the following form of the Chernoff bound.\nLemma 8 ((Angluin & Valiant, 1979)). Let N \u223c Bin(n, p). Then for all 0 < t \u2264 1,\nP (N > (1 + t)np) \u2264 exp ( \u2212t2np/3 ) , P (N < (1\u2212 t)np) \u2264 exp ( \u2212t2np/3 ) .\nLemma 9. Suppose the density function f satisfies:\n(a) f is uniformly continuous on Rd. In other words, \u2200\u01eb > 0, \u2203c\u01eb s.t. for all balls B where vol (B) \u2264 c\u01eb we have supx,x\u2032\u2208B |f(x)\u2212 f(x\u2032)| < \u01eb/2.\n(b) \u2203F , supx\u2208Rd f(x) = F .\nFix 0 < \u01eb < F , let n \u2265 2, and k < n. If k/n\u01eb \u2264 c\u01eb/4 then\nP ( sup Xi\u2208X |f(Xi)\u2212 fn(Xi)| > \u01eb ) \u2264 2n exp ( \u2212 \u01eb 2k 120F 2 ) .\nProof. We\u2019ll be using the short-hand notation Bk,n(x) . = B(x, rk,n(x)) for readability in what follows.\nWe start with the simple bound:\nP ( sup Xi\u2208X |f(Xi)\u2212 fn(Xi)| > \u01eb ) \u2264 P (\u2203Xi \u2208 X, fn(Xi) > f(Xi) + \u01eb)+ P (\u2203Xi \u2208 X, fn(Xi) < f(Xi)\u2212 \u01eb)\n= P ( \u2203Xi \u2208 X, vol (Bk,n(Xi)) <\nk\nn(f(Xi) + \u01eb)\n) +\n(4)\nP ( \u2203Xi \u2208 X, f(Xi) > \u01eb, vol (Bk,n(Xi)) >\nk\nn(f(Xi)\u2212 \u01eb)\n)\n(5)\nWe handle (4) and (5) by first fixing i and conditioning on Xi = x. We start with (4):\nP ( \u2203Xi \u2208 X, vol (Bk,n(Xi)) <\nk\nn(f(Xi) + \u01eb)\n)\n\u2264 n \u222b\nx\nP ( vol (Bk,n(x)) <\nk\nn(f(x) + \u01eb)\n) dF(x), (6)\nwhere the inner probability is over the choice of X \\ {Xi = x} for i fixed. In what follows we use the notation Fn\u22121 to denote the empirical distribution over X \\ {Xi = x}. Assume vol (Bk,n(x)) < k/n(f(x) + \u01eb) < k/n\u01eb < c\u01eb. Then by the uniform continuity assumption on f we have\nF (Bk,n(x)) < (f(x) + \u01eb/2) k\nn(f(x) + \u01eb)\n= ( 1\u2212 \u01eb\n2(f(x) + \u01eb)\n) k\nn \u2264\n( 1\u2212 \u01eb\n4F ) k n\nNow let B(x) be the ball centered at x with F -mass (1\u2212 \u01eb/4F ) (k/n). Since by the above, F (Bk,n(x)) < F (B(x)), we also have that Fn (Bk,n(x)) < Fn (B(x)). This implies that\nF (B(x)) < ( 1\u2212 \u01eb\n4F ) k n\u2212 1 = ( 1\u2212 \u01eb 4F ) Fn (Bk,n(x))\n\u2264 ( 1\u2212 \u01eb\n4F\n) Fn (B(x)) .\nIn other words, let t = \u01eb/(4F \u2212 \u01eb), applying the Chernoff bound of Lemma 8, we have\nP (vol (Bk,n(x)) < k/n(f(x) + \u01eb))\n\u2264 P (Fn (B(x)) > (1 + t)F (B(x))) \u2264 exp ( \u2212t2(n\u2212 1)F (B(x)) /3 ) \u2264 exp ( \u2212\u01eb2k/96F 2 ) .\nCombine with (6) to complete the bound on (4).\nWe now turn to bounding (5). We proceed as before by fixing i and integrating over Xi = x where f(x) > \u01eb, that is\nP ( \u2203Xi \u2208 X, f(Xi) > \u01eb, vol (Bk,n(Xi)) >\nk\nn(f(Xi)\u2212 \u01eb)\n)\n\u2264 n \u222b\nx,f(x)>\u01eb\nP ( vol (Bk,n(x)) >\nk\nn(f(x)\u2212 \u01eb)\n) dF(x),\n(7)\nwhere again the probability is over the choice of X \\ {Xi = x}. Now, we can no longer infer how much f deviates within Bk,n(x) from just the event in question (as we did for the other direction). The trick (inspired by (Devroye & Wagner, 1977)) is to consider a related ball.\nLet B(x) be the ball centered at x of volume k/n(f(x) \u2212 3\u01eb/4). Then\nvol (Bk,n(x)) > k\nn(f(x)\u2212 \u01eb) > vol (B(x))\n=\u21d2 Fn\u22121 (B(x)) \u2264 k \u2212 1 n\u2212 1 < k n .\nSince vol (B(x)) < 4k/\u01eb < c\u01eb, we have by the uniform continuity of f that\nF (B(x)) > k(f(x)\u2212 \u01eb/2) n(f(x)\u2212 3\u01eb/4) >\n( 1 + \u01eb\n4F ) k n\n> ( 1 + \u01eb\n4F\n) Fn\u22121 (B(x)) .\nwe thus have for t = \u01eb/(4F + \u01eb), and using Lemma 8 that\nP ( vol (Bk,n(x)) >\nk\nn(f(x) \u2212 \u01eb)\n)\n\u2264 P (Fn (B(x)) \u2264 (1\u2212 t)F (B(x))) \u2264 exp ( \u2212t2(n\u2212 1)F (B(x)) ) \u2264 exp ( \u2212\u01eb2k/120F 2 ) .\nCombine with (7) to complete the bound on (5).\nThe final result is proved by then combining the bounds on (4) and (5).\nProof of Lemma 1. For any 0 < \u01eb < 1, let c\u01eb = vd2\n\u2212d (\u01eb/2L)d/\u03b1 so that whenever for balls B, vol (B) < c\u01eb, the radius r of B is less than 12 (\u01eb/2L)\n1/\u03b1. Thus, supx,x\u2032\u2208B |f(x)\u2212 f(x\u2032)| \u2264 L(2r)\u03b1 < \u01eb/2. Now, for the settings of \u01eb and k in the lemma statement, we have\n0 < \u01eb < F and 4k\nn\u01eb < vd2 \u2212d ( \u01eb 2L )d/\u03b1 = c\u01eb,\nso we can apply Lemma 9 to get\nP ( sup Xi\u2208X |f(Xi)\u2212 fn(Xi)| > \u01eb ) \u2264 2n exp ( \u2212 \u01eb 2k 120F 2 )\n< \u03b4."}, {"heading": "B. Proof of Lemma 2", "text": "Lemma 2 follows as a corollary to Lemma 10 below.\nLemma 10. Consider a subset A of Rd such that there exists r, satisfying\n\u2200x \u2208 A, \u2016x\u2212 x\u2032\u2016 < 2r =\u21d2 1 2 f(x) \u2264 f(x\u2032) \u2264 2f(x).\nAssume Xi \u2208 X \u2229 A. We have\nP ( rk,n(Xi) \u2265 23/drk(Xi) | rk(Xi) < 2\u22123/dr )\n\u2264 exp (\u2212k/12) , P ( rk,n(Xi) \u2264 2\u22123/drk(Xi) | rk(Xi) < 2\u22123/dr )\n\u2264 exp (\u2212k/192) .\nProof. Let Xi \u2208 X, and fix Xi = x \u2208 A such that rk(x) < 2\u22123/dr. We automatically have\n1 2 vol (B(x, rk(x))) f(x) \u2264 F (B(x, rk(x)))\n\u2264 2 vol (B(x, rk(x))) f(x).\nWe similarly have\nF ( B(x, 23/drk(x)) ) \u2265 vol ( B(x, 23/drk(x)) ) f(x) 2\n\u2265 8 vol (B(x, rk(x))) f(x)\n2\n\u2265 2F (B(x, rk(x))) = 2 k\nn .\nAgain, similarly\nk\n32n =\n1\n32 F (B(x, rk(x))) \u2264 F\n( B(x, 2\u22123/drk(x)) )\n\u2264 1 2 F (B(x, rk(x))) = k 2n .\nThus by Lemma 8,\nP ( rk,n(x) > 2 3/drk(x) ) \u2264\nP ( Fn\u22121 ( B(x, 23/drk(x)) ) < k\nn \u2264 1 2 F ( B(x, 23/drk(x))\n))\n\u2264 exp ( \u2212(n\u2212 1)F ( B(x, 23/drk(x)) ) /12 ) \u2264 exp (\u2212k/12) ,\nand\nP ( rk,n(x) < 2 \u22123/drk(x) ) \u2264\nP ( Fn\u22121 ( B(x, 2\u22123/drk(x)) ) > k\nn \u2265 2F\n( B(x, 2\u22123/drk(x))\n))\n\u2264 exp ( \u2212(n\u2212 1)F ( B(x, 2\u22123/drk(x)) ) /3 ) \u2264 exp (\u2212k/192) .\nConclude by integrating these probabilities over possible values of Xi = x \u2208 A.\nProof of Lemma 2. Part (a) follows directly from the Holder assumption on f . For part (b), notice that\nsup x\u2208L\u03bb\nvdr d k(x)\u03bb \u2264 inf x\u2208L\u03bb F (B(x, rk(x))) =\nk\nn\nso that supx\u2208L\u03bb rk(x) \u2264 2\u22123/dr for the setting of k. Now using part (a) again we have for all x \u2208 L\u03bb\nvdr d k(x) \u00b7\nf(x)\n2 \u2264 F (B(x, rk(x))) =\nk n ,\nso rk(x) \u2264 (2k/vdnf(x))1/d. Finally, the probabilistic statement is obtained by applying Lemma 10 and a union-bound over X \u2229 L\u03bb."}], "references": [{"title": "Fast probabilistic algorithms for Hamiltonian circuits and matchings", "author": ["D. Angluin", "L.G. Valiant"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Angluin and Valiant,? \\Q1979\\E", "shortCiteRegEx": "Angluin and Valiant", "year": 1979}, {"title": "Introduction to statistical learning theory", "author": ["O. Bousquet", "S. Boucheron", "G. Lugosi"], "venue": "Lecture Notes in Artificial Intelligence,", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Rates of convergence for the cluster tree", "author": ["K. Chaudhuri", "S. Dasgupta"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Chaudhuri and Dasgupta,? \\Q2010\\E", "shortCiteRegEx": "Chaudhuri and Dasgupta", "year": 2010}, {"title": "The strong uniform consistency of nearest neighbor density estimates", "author": ["L.P. Devroye", "T.J. Wagner"], "venue": "The Annals of Statistics,", "citeRegEx": "Devroye and Wagner,? \\Q1977\\E", "shortCiteRegEx": "Devroye and Wagner", "year": 1977}, {"title": "Consistency of single linkage for high-density clusters", "author": ["J.A. Hartigan"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Hartigan,? \\Q1981\\E", "shortCiteRegEx": "Hartigan", "year": 1981}, {"title": "Optimal construction of k-nearest neighbor graphs for identifying noisy clusters", "author": ["M. Maier", "M. Hein", "U. von Luxburg"], "venue": "Theoretical Computer Science,", "citeRegEx": "Maier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Maier et al\\.", "year": 2009}, {"title": "Fast rates for plug-in estimators of density level", "author": ["P. Rigollet", "R. Vert"], "venue": "sets. Bernouilli,", "citeRegEx": "Rigollet and Vert,? \\Q2009\\E", "shortCiteRegEx": "Rigollet and Vert", "year": 2009}, {"title": "Generalized density clustering", "author": ["A. Rinaldo", "L. Wasserman"], "venue": "Annals of Statistics,", "citeRegEx": "Rinaldo and Wasserman,? \\Q2010\\E", "shortCiteRegEx": "Rinaldo and Wasserman", "year": 2010}, {"title": "Stability of density based clustering", "author": ["A. Rinaldo", "A. Singh", "R. Nugent", "L. Wasserman"], "venue": null, "citeRegEx": "Rinaldo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rinaldo et al\\.", "year": 2010}, {"title": "Adaptive hausdorff estimation of density level sets", "author": ["A. Singh", "C. Scott", "R. Nowak"], "venue": "Annals of Statistics,", "citeRegEx": "Singh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2009}, {"title": "Clustering with confidence: A binning approach", "author": ["W. Stueltze", "R. Nugent"], "venue": "International Federation Classification Societies Conference,", "citeRegEx": "Stueltze and Nugent,? \\Q2009\\E", "shortCiteRegEx": "Stueltze and Nugent", "year": 2009}, {"title": "A generalized single linkage method for estimating the cluster tree of a density", "author": ["W. Stueltze", "R. Nugent"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Stueltze and Nugent,? \\Q2010\\E", "shortCiteRegEx": "Stueltze and Nugent", "year": 2010}, {"title": "Mode analysis: A generalization of nearest neighbor which reduces chaining effects", "author": ["D. Wishart"], "venue": "Numerical Taxonomy,", "citeRegEx": "Wishart,? \\Q1969\\E", "shortCiteRegEx": "Wishart", "year": 1969}, {"title": "A kth nearest neighbor clustering procedure", "author": ["M. Wong", "T. Lane"], "venue": "Journal of the Royal Statistical Society Series B,", "citeRegEx": "Wong and Lane,? \\Q1983\\E", "shortCiteRegEx": "Wong and Lane", "year": 1983}], "referenceMentions": [{"referenceID": 5, "context": "Previous work (Maier et al., 2009) has shown that the connected components (CC) of a given level set of f can be approximated by the CCs of some subgraph of Gn, provided the level set satisfies certain boundary conditions.", "startOffset": 14, "endOffset": 34}, {"referenceID": 4, "context": "Hartigan (Hartigan, 1981).", "startOffset": 9, "endOffset": 25}, {"referenceID": 9, "context": "Many other related work such as (Rigollet & Vert, 2009; Singh et al., 2009; Maier et al., 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al.", "startOffset": 32, "endOffset": 122}, {"referenceID": 5, "context": "Many other related work such as (Rigollet & Vert, 2009; Singh et al., 2009; Maier et al., 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al.", "startOffset": 32, "endOffset": 122}, {"referenceID": 5, "context": ", 2009; Rinaldo & Wasserman, 2010) consider the task of recovering the CCs of a single level set, the closest to the present work being (Maier et al., 2009) which uses a k-NN graph for level set estimation.", "startOffset": 136, "endOffset": 156}, {"referenceID": 12, "context": "This is similar to an earlier generalization of single-linkage by Wishart (1969) which however was given without a convergence analysis.", "startOffset": 66, "endOffset": 81}, {"referenceID": 8, "context": "A recent archived paper (Rinaldo et al., 2010) also treats the problem of false clusters in cluster tree estimation, but the result is not algorithmic as they only consider the cluster tree of an empirical density estimate, and do not provide a way to compute this cluster tree.", "startOffset": 24, "endOffset": 46}, {"referenceID": 5, "context": "There exist many pruning heuristics in the literature which typically consist of removing small clusters (Maier et al., 2009; Stueltze & Nugent, 2010) using some form of thresholding.", "startOffset": 105, "endOffset": 150}, {"referenceID": 1, "context": "1 of (Bousquet et al., 2004)).", "startOffset": 5, "endOffset": 28}], "year": 2011, "abstractText": "Nearest neighbor (k-NN) graphs are widely used in machine learning and data mining applications, and our aim is to better understand what they reveal about the cluster structure of the unknown underlying distribution of points. Moreover, is it possible to identify spurious structures that might arise due to sampling variability? Our first contribution is a statistical analysis that reveals how certain subgraphs of a k-NN graph form a consistent estimator of the cluster tree of the underlying distribution of points. Our second and perhaps most important contribution is the following finite sample guarantee. We carefully work out the tradeoff between aggressive and conservative pruning and are able to guarantee the removal of all spurious cluster structures at all levels of the tree while at the same time guaranteeing the recovery of salient clusters. This is the first such finite sample result in the context of clustering.", "creator": "LaTeX with hyperref package"}}}