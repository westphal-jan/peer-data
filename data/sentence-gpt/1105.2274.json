{"id": "1105.2274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2011", "title": "Data-Distributed Weighted Majority and Online Mirror Descent", "abstract": "In this paper, we focus on the question of the extent to which online learning can benefit from distributed computing. We focus on the setting in which $N$ agents online-learn cooperatively, where each agent only has access to its own data. We propose a generic data-distributed online learning meta-algorithm called the BOLD learning meta-algorithm. In the study, we show that $N$ agents online-learn cooperatively in the same dataset, but without access to data from the same dataset. For example, the $N$ agent online-learn cooperatively in a similar dataset, but without access to data from the same dataset, it can have access to data from the same dataset. The BOLD learning meta-algorithm is distributed in a separate set of experiments, with each experiment separately, so that all participants in the dataset share data from the same dataset. To make this possible, we use a simple but sophisticated statistical algorithm (CFA) that is similar to the BOLD learning meta-algorithm in its original implementation. The BOLD learning meta-algorithm is distributed in a single dataset; for example, the $N$ agents online-learn cooperatively in a similar dataset, but without access to data from the same dataset. We then analyze the sample for a single, distributed data-distributed data-distributed dataset, which is distributed as a single data-distributed dataset. For each participant in the dataset, each participant in each experiment has access to the same dataset, but without access to data from the same dataset. The BOLD learning meta-algorithm is distributed in a separate set of experiments, with each experiment separately, so that all participants in the dataset share data from the same dataset. To make this possible, we use a simple and sophisticated statistical algorithm (CFA) that is similar to the BOLD learning meta-algorithm in its original implementation. The BOLD learning meta-algorithm is distributed in a single dataset; for example, the $N$ agents online-learn cooperatively in a same dataset, but without access to data from the same dataset. For example, the $N$ agents online-learn cooperatively in a same dataset, but without access to data from the same dataset. To make this possible, we use a simple and sophisticated statistical algorithm (CFA) that is similar to the BOLD learning meta-algorithm in its original implementation. The BOLD learning meta-algorithm is distributed in a single dataset; for example, the", "histories": [["v1", "Wed, 11 May 2011 18:59:13 GMT  (863kb)", "http://arxiv.org/abs/1105.2274v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["hua ouyang", "alexander gray"], "accepted": false, "id": "1105.2274"}, "pdf": {"name": "1105.2274.pdf", "metadata": {"source": "META", "title": "Data-Distributed Weighted Majority and Online Mirror Descent", "authors": [], "emails": ["houyang@cc.gatech.edu", "agray@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 5.\n22 74\nv1 [\ncs .L\nG ]\n1 1\nM ay"}, {"heading": "1. Introduction", "text": "The real world can be viewed as a gigantic distributed system that evolves over time. An intelligent agent in this system can learn from two sources: examples from the environment, as well as information from other agents. One way to state the question addressed by the Data-Distributed Online Learning (DDOL) schemes we introduce can be informally described as follows: within an interconnected network of learning agents, although an agent only receives m samples of input data, can it be made to perform as if it has received M > m samples? Here the performance is measured by generalization abilities (prediction error or regret for the online setting). In other words, to what extent can an agent make fewer generalization errors by uti-\nPreliminary work.\nlizing information from other online-learning agents?\nThis question can also be phrased another way. In recent years, the increasing ubiquity of massive datasets as well as the opportunities for distributed computing (cloud computing, multi-core, etc.), have conspired to spark much interest in developing distributed algorithms for machine learning (ML) methods. While it is easy to see how parallelism can be obtained for most of the computational problems in ML, the question arises whether online learning, which appears at first glance to be inherently serial, can be fruitfully parallelized to any significant degree. While several recent papers have proposed distributed schemes, the question of whether significant speedups over the default serial scheme can be achieved has remained fairly open. Theory establishing or disallowing such a possibility is particularly to be desired. To the best of our knowledge, this paper is the first work that answers these questions for the general online learning setting.\nIn this paper we show both theoretically and experimentally that significant speedups are possible in online learning by utilizing parallelism. We introduce a general framework for data-distributed online learning which encapsulates schemes such as weighted majority, online subgradient descent, and online exponentiated gradient descent."}, {"heading": "1.1. Related Work", "text": "In an empirical study (Delalleau & Bengio, 2007), the authors proposed to make a trade-off between batch and stochastic gradient descent by using averaged mini-batches of size 10 \u223c 100. A parameter averaging scheme was proposed in (Mann et al., 2009) to solve a batch regularized conditional max entropy optimization problem, where the distributed parameters from each agent is averaged in the final stage. A distributed subgradient descent method was proposed in\n(Nedic & Ozdaglar, 2009) and an incremental subgradient method using a Markov chain was proposed in (Johansson et al., 2009). In (Duchi et al., 2010), a distributed dual averaging algorithm was proposed for minimizing convex empirical risks via decentralized networks. Convergence rates were reported for various network topologies. The same idea of averaged subgradients was extended to centralized online learning settings in (Dekel et al., 2010). The problem of multi-agent learning has been an active research topic in reinforcement learning. In this paper, we will focus on the online supervised learning setting."}, {"heading": "2. Setup and DDOL Meta-Algorithm", "text": "In this paper, we assume that each agent only has access to a portion of the data locally and communications with other agents. Suppose we have N learning agents. At round t, the ith learning agent Ai : i = 1, . . . , N receives an example xti \u2208 RD from the environment and makes a prediction yti . The environment then reveals the correct answer lti corresponding to x t i and the agent suffer some loss L(xti, y t i , l t i). The parameter set of an agent Ai at time t is wti \u2208 W . Each agent is a vertex of a connected graph G = (A, E). There will be a bidirectional communication between Ai and Aj if they are neighbors (connected by an edge eij). Ai has Ni \u2212 1 neighbors. The generic meta-algorithm for data-distributed online learning (DDOL) is very simple: each agent Ai works according to the following procedure:\nAlgorithm 1 Distributed Online Learning (DDOL)\nfor t = 1, 2, . . . do Ai makes local prediction(s) on example(s) xti; Ai Update wti using local correct answer(s) lti ; Ai Communicate wti with its neighbors and do Weighted Average over wtj , j = 1, . . . , Ni; end for\nTo derive a distributed online learning algorithm, one need to specify the Update, Communicate andWeighted Average schemes. In the following sections, we will discuss how to use two classic online learning methods as the basic Update scheme, and how the combination with different Communicate/Weighted Average schemes leads to different performance guarantees."}, {"heading": "3. Distributed Weighted Majority", "text": "We firstly propose two expert-advise-based online classification algorithms which can be regarded as dis-\ntributed versions of the classic Weighted Majority algorithm (WMA) (Littlestone & Warmuth, 1989). For simplicity, we assume that in both algorithms, all the experts are shared by all the agents, and each agent is adjacent to all the other agents (G is a complete graph).\nAlg. 2 is named Distributed Weighted Majority by Imitation (DWM-I). In the communication step, each Ai mimics other agent\u2019s operations by penalizing each expert p in the same way as any other agents do, and then makes a geometric averaging. The following re-\nAlgorithm 2 DWM-I: agent Ai 1: Initialize all weights w0i1 , . . . , w 0 iP\nof P shared experts for agent Ai to 1.\n2: for t = 1, 2, . . . do 3: Given experts\u2019 predictions yti1 , . . . , y t iP\nover xti, Ai predicts\n4:\n{ 1, if \u2211 p:yip=1 wt\u22121ip \u2265 \u2211 p:yip=0 wt\u22121ip\n\u22121, otherwise. 5: Environment reveals lti for Ai.\n6: \u2200p : w\u0303tip \u2190 { \u03b1wt\u22121ip , if y t ip\n6= lti (0 < \u03b1 < 1) wt\u22121ip , otherwise.\n7: wtip \u2190 (\u220fN j=1 w\u0303 t jp )1/N . 8: end for\nsult gives the upper bound of the average number of mistakes by each agent, assuming that each agent is receiving information from all the other agents.\nTheorem 3.1. For Algorithm 2 with N agents and P shared experts, maxiMi \u2264 1log 2\n1+\u03b1\n( m\u2217 N log 1 \u03b1 + logP ) ,\nwhere Mi is the number of mistakes by agent Ai and m\u2217 is the minimum number of mistakes by the best expert over all agents so far.\nProof. The proof essentially follows that of WMA. The best expert E\u2217 makes m\u2217 mistakes over all agents so far. So for any Ai, its weight of E\u2217 is \u03b1 m\u2217 N . Upon each\nmistake made by any agent, the total weights \u2211\np w t ip\nof Ai decreases by a factor of at least 12 (1 \u2212 \u03b1). So the total weights for Ai is at most P [ 1\u2212 12 (1\u2212 \u03b1) ]Mi . Therefore for any i, \u03b1 m\u2217 N \u2264 P ( 1+\u03b1 2 )Mi . It follows that\nMi \u2264 1\nlog 21+\u03b1\n( m\u2217 N log 1 \u03b1 + logP ) . (1)\nTaking \u03b1 = 1/2,\nMi < 2.41 (m\u2217 N + logP ) . (2)\nComparing (2) with the result of WMA: M < 2.41(m\u2217 + logP ), in the most optimistic case, if m\u2217 is of the same order as the number of mistakes made by the best expert E\u2217 in the single agent scheme, in other words, if E\u2217 makes 0 error over all agents other than Ai, then the upper bound is decreased by a factor of 1/N . In the most pessimistic case, if E\u2217 makes exactly the same number of mistakes over all N agents, then the upper bound is the same as with a single agent. This happens when all agents are receiving the same inputs and answers from the environment, hence there is no new information being communicated among the network and no communications are needed. In reality, m\u2217 falls between these two extremes.\nTheorem 3.1 is stated from an individual agent point of view. From the social point of view, the total number of mistakes made by all agents \u2211N i=1Mi is upper bounded by\n1\nlog 21+\u03b1\n( m\u2217 log 1\n\u03b1 +N logP\n) , (3)\nwhich is not larger than that in a single agent scheme (N logP can be ignored in comparing with the first term m\u2217 which could be very large in practice). Imagine that NT samples are processed in the single agent scheme, while in the N agents scheme, each Ai process T samples. In the most pessimistic case, upper bound (3) is the same for these two schemes. This is a very good property for parallel computing, since the proposed online DWM can achieve the same generalization capacity, while being N times faster than a serial algorithm. This property is verified by the experiments in Section 5.\nAs in the Randomized Weighted Majority algorithm (RWM) (Littlestone & Warmuth, 1989), we can introduce some randomness to our choices of experts by giving each expert a probability of being chosen depending on the performance of this expert in the past. Specifically, in each round we choose an expert with probability pi = wi/ \u2211 iwi. We can have a Distributed Randomized Weighted Majority and obtain a similar upper bound as that of RWM with a constant of 1/N as in Theorem 3.1.\nThe upper bound (1) can be further improved by an alternative algorithm (Alg. 3), which differs from Alg. 2 only in the way that each agent utilizes information received from others. Instead of mimicking other agents\u2019 operations, an agent now updates its weights by arithmetically averaging together with all the weights it received from its neighbors.\nTheorem 3.2. For Algorithm 3 with N agents and P shared experts, maxiMi \u2264\nAlgorithm 3 DWM-A: agent Ai 1: \u00b7 \u00b7 \u00b7 2: wtip \u2190 1 N \u2211N j=1 w\u0303 t jp .\n3: \u00b7 \u00b7 \u00b7\n1 log 2\n1+\u03b1\n(\u2211 t (1\u2212\u03b1)mt \u2217\nN\u2212(1\u2212\u03b1)mt \u2217\n+ logP ) , where Mi is\nthe number of mistakes by agent Ai so far and mt\u2217 is the minimum number of mistakes by the best expert at round t over all agents.\nProof. Denote the weight of expert p for agent Ai at round t as wtip . Indeed,\nN\u2211\ni=1\nwtip =\n( N\u2211\ni=1\nwt\u22121ip )( N \u2212mtp N + \u03b1 mtp N )\n=\n( N\u2211\ni=1\nwt\u22122ip\n)[ 1\u2212\nmtp(1\u2212 \u03b1) N\n][ 1\u2212\nmt\u22121p (1\u2212 \u03b1) N\n]\n= \u00b7 \u00b7 \u00b7 = N [ 1\u2212\nmtp(1\u2212 \u03b1) N\n] \u00b7 \u00b7 \u00b7 [ 1\u2212\nm1p(1 \u2212 \u03b1) N\n] .\nUsing 1 \u2212 x \u2265 exp(\u2212x/(1 \u2212 x)), \u2200x \u2208 (0, 1) and the fact that 0 \u2264 mtp \u2264 N , we have for any agent Ai,\nwtip = \u220f\nt\n[ 1\u2212\nmtp(1\u2212 \u03b1) N\n] \u2265 exp (\u2211\nt\n\u2212mtp(1\u2212 \u03b1) N \u2212mtp(1\u2212 \u03b1)\n) .\n(4)\nOn the other hand, for any Ai, P\u2211\np=1\nwtip \u2264 P [ 1\u2212 1\n2 (1\u2212 \u03b1)\n]Mi . (5)\nSince wtip \u2264 \u2211P p=1 w t ip , combining (4) and (5),\nP [ 1\u2212 1\n2 (1\u2212 \u03b1)\n]Mi \u2265 exp ( \u2212 \u2211\nt\nmtp(1\u2212 \u03b1) N \u2212mtp(1 \u2212 \u03b1)\n) .\nIt follows that \u2200i = 1 . . .N, p = 1 . . . P\nMi \u2264 1\nlog 21+\u03b1\n(\u2211\nt\n(1\u2212 \u03b1)mtp N \u2212 (1\u2212 \u03b1)mtp + logP\n) . (6)\nNow we are ready to compare the refined bound (6) with (1) using m\u2217 \u2265 \u2211 tm t \u2217. Without considering the logP part of the bounds which is much smaller than the m\u2217 part, it is easy to verify that if 1/2 \u2264 \u03b1 < 1, then\nm\u2217 log 1 \u03b1\nN >\n\u2211\nt\n(1\u2212 \u03b1)mt\u2217 N \u2212 (1 \u2212 \u03b1)mt\u2217\n(7)\nwithout any assumption on mt\u2217; If 0 < \u03b1 < 1/2, then the above inequality holds when\nmt\u2217 \u2264 N ( 1\n1\u2212 \u03b1 \u2212 1 log(1/\u03b1)\n) . (8)\nThe RHS of (8) is lower bounded by 0.81N . Specifically, when mt\u2217 = O(N/2) and by taking \u03b1 = 1/2, the difference in (7) is\nm\u2217 N\n\u2212 \u2211\nt\nmt\u2217 2N \u2212mt\u2217 = O (m\u2217 N ) .\nHence the error bound in Theorem 3.2 is much lower. Experimental evidence will be provided in Section 5."}, {"heading": "4. Distributed Online Mirror Descent", "text": "In this section we extend the idea of distributed online learning to Online Convex Optimization (OCO) problems. OCO is an online variant of the convex optimization, which is ubiquitous in many machine learning problems such as support vector machines, logistic regression and sparse signal reconstruction tasks. Each of these learning algorithms has a convex loss function to be minimized.\nOne can consider OCO as a repeated game between an algorithm A and the environment. At each round t, A chooses a strategy wt \u2208 W and the environment reveals a convex function ft. Here we assume that all convex functions share the same feasible set W . The goal of A is to minimize the difference between the cumulation \u2211 t ft(wt) and that of the best strategyw \u2217 it can play in hindsight. This difference is commonly known as external regret, defined as below.\nDefinition 4.1. The regret of convex functions f = {ft} for t = 1, 2, . . . , T is defined as R(T ) =\u2211T\nt=1 ft(wt)\u2212 infw\u2208W \u2211T t=1 ft(w).\nIn the distributed setting, this game is played by every agent Ai, i = 1, . . . , N . The goal of Ai is to minimize its own regret Ri(T ), named the individual regret. We call the sum of individual regrets R(T ) = \u2211N\ni=1 Ri(T ) social regret.\nWe will present the online mirror descent (OMD) framework which generalize many OCO algorithms such as online subgradient descent (Zinkevich, 2003), Winnow (Littlestone, 1988), online exponentiated gradient (Kivinen & Warmuth, 1997), online Newton\u2019s method (Hazan et al., 2006).\nWe firstly introduce some notations used in this section. A distance generating function \u03c9(u) is a continuously differentiable function that is a-strongly convex w.r.t. some norm \u2016 \u00b7 \u2016 associated with an inner\nproduct \u3008\u00b7, \u00b7\u3009. Using Bregman divergence \u03c8(u,v) = \u03c9(u) \u2212 \u03c9(v) \u2212 \u3008\u2207\u03c9(v),u \u2212 v\u3009 as a proximity function, the update rule of OMD can be expressed as wt+1 \u2190 argminz\u2208W \u03b7t \u3008gt, z\u2212wt\u3009 + \u03c8(z,wt), where gt is a subgradient of ft at wt and \u03b7t is a learning rate which plays an important role in the regret bound. Denote the dual norm of \u2016 \u00b7 \u2016 as \u2016 \u00b7 \u2016\u2217. Suppose agent Ai has Ni \u2212 1 neighbors. We propose Distributed Online Mirror Descent algorithm in Alg. 4. In this algorithm, the update rule has explicit expressions for some special proximity functions \u03c8(\u00b7, \u00b7). Next we derive distributed update rules for two well-known OMD examples: Online Gradient Descent (OGD) and Online Exponentiated Gradient (OEG).\nAlgorithm 4 DOMD: agent Ai Initialize w1i \u2208 W for t = 1, 2, . . . do Local prediction using wti .\nwt+1i \u2190 arg min z\u2208W\nNi\u2211\nj=1\n[ \u03b7t \u2329 gtj , z\u2212wtj \u232a + \u03c8(z,wtj) ] .\nend for"}, {"heading": "Distributed OGD", "text": "Taking \u03c8(u,v) = 12\u2016u \u2212 v\u201622 (i.e. the proximity is measured by squared Euclidean distance), an agent Ai needs to solve the minimization minz \u2211Ni i=1 [ \u03b7t \u3008gti, z\u2212wti\u3009+ 12\u2016z\u2212wti\u201622 ] , which leads to a simple DOGD updating rule\nwt+1i \u2190 1\nNi\nNi\u2211\nj=1\n( wtj \u2212 \u03b7tgtj ) . (9)"}, {"heading": "Distributed OEG", "text": "Taking the unnormalized relative entropy as the proximity function \u03c8(u,v) = \u2211D d=1 ud lnud \u2212\u2211D\nd=1 vd ln vd\u2212(lnv+I)T (u\u2212v), we can solve the minimization minz \u2211Ni j=1 [ \u03b7t \u2329 gtj, z\u2212wtj \u232a + \u2211D\nd=1 zd ln zd\u2212\u2211D d=1(w t j)d ln(w t j)d\u2212(lnwtj+I)T (z\u2212wtj) ] , and obtain the update rule for DOEG:\nwt+1i \u2190 ( Ni\u220f\nj=1\nwtje \u2212\u03b7tgtj\n)1/Ni . (10)\nIf the feasible setW is a simplex ball \u2016w\u20161 \u2264 S instead of RD, one only needs to do an extra normalization: \u2200d = 1, . . . , D, (wt+1i )d \u2190 S(wt+1 i )d\n\u2211 d (wt+1 i )d\nif \u2016wt+1i \u20161 > S.\nUpdating rules (9) and (10) share the same spirit as stated in the meta-algorithm 1: each agent updates\nits parameters wi individually, then it averages with its neighbors\u2019 parameters, either arithmetically, or geometrically. The following results shows how this simple averaging scheme works, in terms of average individual regrets 1N \u2211 iRi(T ). As in theorem 3.1 and theorem 3.2, for simplicity, we assume that the graph G is complete, i.e. each agent has N \u2212 1 neighbors. Lemma 4.2. (Nemirovsky & Yudin, 1983) Let Pw(u) = argminz\u2208W \u3008u, z\u2212w\u3009 + \u03c8(z,w), for any v,w \u2208 W and u \u2208 RD one has\n\u3008u,w\u2212 v\u3009 \u2264 \u03c8(w,v) \u2212 \u03c8(Pw(u),v) + \u2016u\u20162\u2217 2a . (11)\nTheorem 4.3. If N agents in Algorithm 4 are connected via a complete graph, ft are convex, distances between two parameter vectors are upper bounded supi,j,t \u03c8(w t i ,w t j) = D, let \u03b7t = 1\u221a t , then the average individual regret\n1\nN\nN\u2211\ni=1\nRi(T ) \u2264 D \u221a T + 1\n2aN2\nT\u2211\nt=1\n( 1\u221a t \u2225\u2225 N\u2211\nj=1\ngtj \u2225\u22252 \u2217\n) .\n(12)\nProof. Since G is complete, at a fixed t, wti is the same for any i. Hence w t+1 i = argminz\u2208W \u2211Ni\nj=1\n[ \u03b7t \u2329 gtj, z\u2212wtj \u232a + \u03c8(z,wtj) ] =\nargminz\u2208W \u2329 \u03b7t N \u2211N j=1 g t j , z \u2212 wti \u232a + \u03c8(z,wti) = Pwt i (\u03b7tN \u2211N j=1 g t j). Let u = \u03b7t N \u2211N j=1 g t j , v = w \u2217, w = wti in (11), we have\n\u2329 1 N\nN\u2211\nj=1\ngtj , w t i \u2212w\u2217\n\u232a\n\u2264 1 \u03b7t [ \u03c8(wti ,w \u2217)\u2212 \u03c8(wt+1i ,w\u2217) ] + \u03b7t 2a \u2225\u2225 1 N N\u2211\nj=1\ngtj \u2225\u22252 \u2217.\nUsing the convexity of ft and summing the above inequality over t we have\n1\nN\nN\u2211\ni=1\nRi(T ) =\nT\u2211\nt=1\n1\nN\nN\u2211\nj=1\n[ f tj (w t j)\u2212 f tj (w\u2217) ]\n\u2264 T\u2211\nt=1\n\u2329 1 N N\u2211\nj=1\ngtj, w t i \u2212w\u2217 \u232a \u2264 1 \u03b71 \u03c8(w1i ,w \u2217)\u2212\n1\n\u03b7T \u03c8(wT+1i ,w\n\u2217) + \u2211\n2\u2264t\u2264T\n( 1\n\u03b7t \u2212 1 \u03b7t\u22121\n) \u03c8(wti ,w \u2217)+\nT\u2211\nt=1\n( \u03b7t\n2aN2 \u2225\u2225 N\u2211\nj=1\ngtj \u2225\u22252 \u2217\n) .\nSetting \u03b7t = 1/ \u221a t and using the assumption on the upper bound of \u03c8(\u00b7, \u00b7) we reach the result.\nTo appreciate the above theorem, we further assume that the subgradient is upper bounded: sup\nw\u2208W,t=1,2,... \u2016gt(w)\u2016\u2217 = G. In the most optimistic case, at a given round t, if the subgradients gtj , j = 1, . . . , N are mutually orthogonal, then the second term of the upper bound (12) can be bounded by 12aNG 2 \u221a T , which is 1/N times smaller than using a single agent. In the most pessimistic case, if all the subgradients gtj, j = 1, . . . , N are exactly the same,\nthen the second term is bounded by 12aG 2 \u221a T , which is the same as in a single agent scheme.\nAccording to the regret bound (12), the social regret\u2211N i=1Ri(T ) \u2264 ND2 \u221a T + 12aN \u2211T t=1 ( \u03b7t \u2225\u2225\u2211N j=1 g t j \u2225\u22252 \u2217 ) .\nIn the most optimistic case, the bound is ND2 \u221a T + G2\n2a \u2211T t=1 \u03b7t \u2264 (ND2+G 2 a ) \u221a T . In the most pessimistic\ncase, the bound becomes (ND2 + NG 2 a ) \u221a T .\nImagine that NT samples need to be processed. In the single agent scheme, they are accessed by only 1 agent, while in the N agents scheme, these NT samples are evenly distributed with each Ai processing T samples. In the most optimistic case, the bound for the N agent scheme is (ND2 + G 2 a ) \u221a T , while in the most pessimistic case, it is (ND2 + N G 2 a ) \u221a T . In comparison, the bound for the single agent scheme is (D2 \u221a N + G 2 \u221a N a ) \u221a T . We cannot draw an immediate conclusion of which one is better, since it depends on the correlations of examples, as well as D and G. But it is clear that the N agent scheme is at most \u221a N times larger in its social regret bound, while being N times faster."}, {"heading": "5. Experimental Study", "text": "In this section, several sets of online classification experiments will be used to evaluate the theories and the proposed distributed online learning algorithms. Three real-world binary-class datasets 1 from various application domains are adopted. Table 1 summarizes these datasets and the parameters used in section 5.2.\nTo simulate the behavior of multi-agents, we use Pthreads (POSIX Threads) for multi-threaded programming, where each thread is an agent, and they communicate with each other via the shared memory. Barriers are used for synchronizations. All exper-\n1www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\niments are carried out on a workstation with a 4-core 2.7GHz Intel Core 2 Quad CPU."}, {"heading": "5.1. Distributed Weighted Majority", "text": "To evaluate the proposed DWM algorithms, the simplest decision stumps are chosen as experts, and all the experts are trained off-line. We randomly choose P \u2264 D dimensions. Within each dimension, 200 probes are evenly placed between the min and max values of this dimension. The probe with the minimum training error over the whole dataset is selected as the decision threshold. In all the following weighted majority experiments, we choose the penalty factor \u03b1 = 0.9.\nThe first set of experiments report the behaviors of DWM-I and DWM-A from the individual agent point of view. Each agent share the same P = 4 experts, and communicates with all the others. Fig. 1 and 2 depict the cumulative number of mispredictions made by each thread as a function of the number of samples accessed by a single agent, where 1, 2, 3 and 4 agents are compared. Each plot in a subfigure represents an agent. It is clear that an agent Ai makes fewer mistakes Mi as it receives more information from its neighbors. With 4 agents, Mi is reduced by half comparing with the single agent case. This provides some evidence for the 1/N error reduction as stated in Theorem 3.1.\nAs discussed in Section 3, from the social point of view, with the same number of samples accessed, the bound (3) of the total number of mistakes made by all agents ( \u2211\niMi) is almost as small as that in a single agent case. The comparisons for both DWM-I and DWMA are illustrated in Fig.3. This result is not surprising, since no more information is provided for multiple agents, and one should not hope that \u2211 iMi is much lower thanM . But on the other hand, the DWM algorithms achieve the same level of mistakes, while they\nare N times faster. It can also be observed from Fig. 1, 2 and 3 that DWM-A makes slightly fewer mistakes than DWM-I.\nTo verify the tightness of bound (1) and the refined (6), we compare the number of mistakes m\u2217 make by the best expert E\u2217 over all agents with that of a single agent Mi. Fig. 4 shows that with N = 2 or 5 agents, m\u2217 is around 2 or 5 times larger thanMi, which means Mi \u2248 m\u2217/N . However, choosing \u03b1 = 0.9 in bound (1) leads to Mi \u2264 2.05m\u2217/N . This shows that the bound in Theorem 3.2 is indeed tighter than Theorem 3.1."}, {"heading": "5.2. Distributed Online Mirror Descent", "text": "In this section, several online classification experiments will be carried out using the proposed DOGD and DOEG algorithms. For DOGD, we choose the L2regularized instance hinge loss function as our convex objective function:\nft(w) = Cmax { 0, 1\u2212 ltwTxt } + \u2016w\u201622/2.\nFor DOEG, we take ft(w) = max { 0, 1\u2212 ltwTxt } and W = {w : \u2016w\u20161 \u2264 S}. Since the update rule (10) cannot change the signs of wt, we use a similar trick likeEG\u00b1 proposed in (Kivinen & Warmuth, 1997), i.e. letting w = w+ \u2212w\u2212, where w+,w\u2212 > 0. Since we will not compare the generalization capacities between these two algorithms, in all the following experiments, the parameters of C and S are chosen according to Table 1 without any further tuning. The subgradient of the non-smooth hinge loss is take as gt = \u2212ltxt if 1\u2212 liwTxt > 0 and 0 otherwise. We firstly illustrate the generalization capacities of DOGD and DOEG. Since we do not know infw\u2208W \u2211T t=1 ft(w), it is not easy to calculate the individual regret or social regret. Hence we only compare the number of mispredictions and the average accumulated objective function values as functions of the number of samples accessed by a single agent. The results are shown in Fig. 5 \u223c 8. It is clear that for both DOGD and DOEG, the number of mispredictions decreases when more agents communicate with each other. The average objective values 1 N f i t (w i t) also decrease with the increasing number of agents N . However, as shown in Fig. 8, when N = 32, the averaged 1N f i t (w i t) is larger than N = 16. This might be due to the insufficient number of samples of the dataset cod-rna. This conjecture is experimentally verified in Fig. 9, where the size of covtype is 522910.\nAs discussed at the end of Section 4, the social regret bound of N agents is at most \u221a N times larger than that of a single agent scheme. The next set of experiments will be used to verify this claim. Fig. 10 depicts the result. We can see that the total loss \u2211N i=1 f i t (w i t)\nfor N = 8, 16, 32 is even lower than using a single agent. N = 64 is slightly higher, but the difference is still much lower than the theoretical \u221a 64. This suggests that there might exist a bound tighter than (12)."}, {"heading": "6. Conclusions and Future Work", "text": "We proposed a generic data-distributed online learning meta-algorithm. As concrete examples, two sets of distributed algorithms were derived. One is for distributed weighted majority, and the other is for distributed online convex optimization. Their effectiveness is supported by both analysis and experiments.\nThe analysis shows that with N agents, DWM can have an upper error bound that is 1/N lower than using a single agent. From the social point of view, the bound of total number of errors made by all N agents is the same as using 1 agent, while processing the same amount of examples. This indicates that DWM attains the same level of generalization error as\nWM, but is N times faster.\nThe average individual regret for DOMD algorithms is also much lower than OMD, although it is not 1/N lower as in DWM. In the worst case, the bound of social regret is at most \u221a N higher than using a single agent.\nIn follow-on work, two assumptions made in this paper will be removed to make the proposed algorithms more robust in practical applications. Firstly, as discussed in (Duchi et al., 2010), the connected graph G does not need to be complete. We are working on distributed active learning and active teaching, which might lead to a data-dependent communication topology. Secondly, the learning process should be fully asynchronous. This brings up the problem of \u2018delays\u2019 in label feedbacks (Mesterharm, 2005; Langford et al., 2009). Moreover, for OCO, with more structural information on ft rather than the black-box model, we might be able to find better distributed algorithms and achieve tighter bounds."}, {"heading": "Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao,", "text": "L. Optimal distributed online prediction using minibatches. In NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds, 2010.\nDelalleau, O. and Bengio, Y. Parallel stochastic gradient descent. In CIAR Summer School, 2007."}, {"heading": "Duchi, J., Agarwal, A., and Wainwright, M. Distributed", "text": "dual averaging in networks. In NIPS, 2010."}, {"heading": "Hazan, Elad, Agarwal, Amit, Kalai, Adam, and Kale,", "text": "Satyen. Logarithmic regret algorithms for online convex optimization. In COLT, 2006."}, {"heading": "Johansson, Bjorn, Rabi, Maben, and Johansson, Mikael.", "text": "A randomized incremental subgradient method for distributed optimization in networked systems. SIAM J. Optim., 20(3):1157\u20131170, 2009."}, {"heading": "Kivinen, Jyrki and Warmuth, Manfred K. Exponentiated", "text": "gradient versus gradient descent for linear predictors. Information and Computation, (132):1\u201363, 1997.\nLangford, John, Smola, Alexander J., and Zinkevich, Martin. Slow learners are fast. arXiv Submitted, 2009.\nLittlestone, Nick. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285\u2013318, 1988."}, {"heading": "Littlestone, Nick and Warmuth, Manfred K. The weighted", "text": "majority algorithm. In Foundations of Computer Science, 30th Annual Symposium on, pp. 256\u2013261, 1989."}, {"heading": "Mann, G., McDonald, R., Mohri, M., Silberman, N., and", "text": "Walker, D. D. Efcient large-scale distributed training of conditional maximum entropy models. In NIPS, 2009.\nMesterharm, Chris. Online learning with delayed label feedback. In Algorithmic Learning Theory, pp. 399\u2013413, 2005.\nNedic, Angelia and Ozdaglar, Asuman. Distributed subgradient methods for multi-agent optimization. IEEE Trans. on Automatic Control, 54(1):48\u201361, 2009.\nNemirovsky, A. S. and Yudin, D. B. Problem Complexity and Method Efficiency in Optimization. John Wiley & Sons, 1983.\nZinkevich, Martin. Online convex programming and generalized infinitesimal gradient ascent. In ICML, 2003."}], "references": [{"title": "Optimal distributed online prediction using minibatches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "In NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Parallel stochastic gradient descent", "author": ["O. Delalleau", "Y. Bengio"], "venue": "In CIAR Summer School,", "citeRegEx": "Delalleau and Bengio,? \\Q2007\\E", "shortCiteRegEx": "Delalleau and Bengio", "year": 2007}, {"title": "Distributed dual averaging in networks", "author": ["J. Duchi", "A. Agarwal", "M. Wainwright"], "venue": "In NIPS,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Hazan", "Elad", "Agarwal", "Amit", "Kalai", "Adam", "Kale", "Satyen"], "venue": "In COLT,", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "A randomized incremental subgradient method for distributed optimization in networked systems", "author": ["Johansson", "Bjorn", "Rabi", "Maben", "Mikael"], "venue": "SIAM J. Optim.,", "citeRegEx": "Johansson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2009}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["Kivinen", "Jyrki", "Warmuth", "Manfred K"], "venue": "Information and Computation,", "citeRegEx": "Kivinen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 1997}, {"title": "Slow learners are fast", "author": ["Langford", "John", "Smola", "Alexander J", "Zinkevich", "Martin"], "venue": "arXiv Submitted,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["Littlestone", "Nick"], "venue": "Machine Learning,", "citeRegEx": "Littlestone and Nick.,? \\Q1988\\E", "shortCiteRegEx": "Littlestone and Nick.", "year": 1988}, {"title": "The weighted majority algorithm", "author": ["Littlestone", "Nick", "Warmuth", "Manfred K"], "venue": "In Foundations of Computer Science, 30th Annual Symposium on,", "citeRegEx": "Littlestone et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Littlestone et al\\.", "year": 1989}, {"title": "Efcient large-scale distributed training of conditional maximum entropy models", "author": ["G. Mann", "R. McDonald", "M. Mohri", "N. Silberman", "D.D. Walker"], "venue": "In NIPS,", "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Online learning with delayed label feedback", "author": ["Mesterharm", "Chris"], "venue": "In Algorithmic Learning Theory, pp", "citeRegEx": "Mesterharm and Chris.,? \\Q2005\\E", "shortCiteRegEx": "Mesterharm and Chris.", "year": 2005}, {"title": "Distributed subgradient methods for multi-agent optimization", "author": ["Nedic", "Angelia", "Ozdaglar", "Asuman"], "venue": "IEEE Trans. on Automatic Control,", "citeRegEx": "Nedic et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nedic et al\\.", "year": 2009}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A.S. Nemirovsky", "D.B. Yudin"], "venue": null, "citeRegEx": "Nemirovsky and Yudin,? \\Q1983\\E", "shortCiteRegEx": "Nemirovsky and Yudin", "year": 1983}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML,", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "A parameter averaging scheme was proposed in (Mann et al., 2009) to solve a batch regularized conditional max entropy optimization problem, where the distributed parameters from each agent is averaged in the final stage.", "startOffset": 45, "endOffset": 64}, {"referenceID": 4, "context": "(Nedic & Ozdaglar, 2009) and an incremental subgradient method using a Markov chain was proposed in (Johansson et al., 2009).", "startOffset": 100, "endOffset": 124}, {"referenceID": 2, "context": "In (Duchi et al., 2010), a distributed dual averaging algorithm was proposed for minimizing convex empirical risks via decentralized networks.", "startOffset": 3, "endOffset": 23}, {"referenceID": 0, "context": "The same idea of averaged subgradients was extended to centralized online learning settings in (Dekel et al., 2010).", "startOffset": 95, "endOffset": 115}, {"referenceID": 3, "context": "We will present the online mirror descent (OMD) framework which generalize many OCO algorithms such as online subgradient descent (Zinkevich, 2003), Winnow (Littlestone, 1988), online exponentiated gradient (Kivinen & Warmuth, 1997), online Newton\u2019s method (Hazan et al., 2006).", "startOffset": 257, "endOffset": 277}], "year": 2011, "abstractText": "In this paper, we focus on the question of the extent to which online learning can benefit from distributed computing. We focus on the setting in whichN agents online-learn cooperatively, where each agent only has access to its own data. We propose a generic datadistributed online learning meta-algorithm. We then introduce the Distributed Weighted Majority and Distributed Online Mirror Descent algorithms, as special cases. We show, using both theoretical analysis and experiments, that compared to a single agent: given the same computation time, these distributed algorithms achieve smaller generalization errors; and given the same generalization errors, they can be N times faster.", "creator": "LaTeX with hyperref package"}}}