{"id": "1105.6041", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2011", "title": "The Perceptron with Dynamic Margin", "abstract": "The classical perceptron rule provides a varying upper bound on the maximum margin, namely the length of the current weight vector divided by the total number of updates up to that time. Requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin (PDM). We demonstrate that PDM converges in a finite number of steps and derive an upper bound on them with a constant weight of up to 0.5, in the form of the maximum weight of the weights, on a matrix of tensors. We show that PDM has a low degree of linearized distribution, and that all of these features fit into the PDM classifier, allowing us to evaluate the entire model at a much higher level than previously assumed. We then show that the model's weight distribution (from the PDM classifier) was significantly higher than the expected degree of linearized distribution with the weight of up to 0.25, in the form of the max weight of the weights, on a matrix of tensors.\n\n\nFigure 2. The current model for the PDM classifier. PPT PowerPoint slide\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 30 May 2011 17:02:09 GMT  (59kb)", "http://arxiv.org/abs/1105.6041v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["constantinos panagiotakopoulos", "petroula tsampouka"], "accepted": false, "id": "1105.6041"}, "pdf": {"name": "1105.6041.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 5.\n60 41\nv1 [\ncs .L\nG ]\n3 0\nM ay\nKeywords: Online learning, classification, maximum margin."}, {"heading": "1 Introduction", "text": "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2]. Typically, SVMs obtain large margin solutions by solving a constrained quadratic optimization problem using dual variables. In their native form, however, efficient implementation is hindered by the quadratic dependence of their memory requirements in the number of training examples a fact which renders prohibitive the processing of large datasets. To overcome this problem decomposition methods [15, 6] were developed that apply optimization only to a subset of the training set. Although such methods led to improved convergence rates, in practice their superlinear dependence on the number of examples, which can be even cubic, can still lead to excessive runtimes when large datasets are processed. Recently, the so-called linear SVMs [7, 8, 13] made their appearance. They take advantage of linear kernels in order to allow parts of them to be written in primal notation and were shown to outperform decomposition SVMs when dealing with massive datasets.\nThe above considerations motivated research in alternative large margin classifiers naturally formulated in primal space long before the advent of linear SVMs. Such algorithms are mostly based on the perceptron [16, 12], the simplest online learning algorithm for binary linear classification. Like the perceptron, they focus on the primal problem by updating a weight vector which represents\nat each step the current state of the algorithm whenever a data point presented to it satisfies a specific condition. It is the ability of such algorithms to process one example at a time1 that allows them to spare time and memory resources and consequently makes them able to handle large datasets. The first algorithm of that kind is the perceptron with margin [3] which is much older than SVMs. It is an immediate extension of the perceptron which provably achieves solutions with only up to 1/2 of the maximum margin [10]. Subsequently, various algorithms succeeded in approximately attaining maximum margin by employing modified perceptron-like update rules. Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20]. Very recently, the same goal was accomplished by a generalized perceptron with margin, the margitron [14].\nThe most straightforward way of obtaining large margin solutions through a perceptron is by requiring that the weight vector be updated every time the example presented to the algorithm has (normalized) margin which does not exceed a predefined value [17, 18, 1]. The obvious problem with this idea, however, is that the algorithm with such a fixed margin condition will definitely not converge unless the target value of the margin is smaller than the unknown maximum margin. In an earlier work [14] we noticed that the upper bound \u2016at\u2016 /t on the maximum margin, with \u2016at\u2016 being the length of the weight vector and t the number of updates, that comes as an immediate consequence of the perceptron update rule is very accurate and tends to improve as the algorithm achieves larger margins. In the present work we replace the fixed target margin value with a fraction 1\u2212 \u01eb of this varying upper bound on the maximum margin. The hope is that as the algorithm keeps updating its state the upper bound will keep approaching the maximum margin and convergence to a solution with the desired accuracy \u01eb will eventually occur. Thus, the resulting algorithm may be regarded as a realizable implementation of the perceptron with fixed margin condition.\nThe rest of this paper is organized as follows. Section 2 contains some preliminaries and a motivation of the algorithm based on a qualitative analysis. In Sect. 3 we give a formal theoretical analysis. Section 4 is devoted to implementational issues. Section 5 contains our experimental results while Sect. 6 our conclusions."}, {"heading": "2 Motivation of the Algorithm", "text": "Let us consider a linearly separable training set {(xk, lk)}mk=1, with vectors xk \u2208 IRd and labels lk \u2208 {+1,\u22121}. This training set may either be the original dataset or the result of a mapping into a feature space of higher dimensionality [21, 2]. Actually, there is a very well-known construction [4] making linear separability always possible, which amounts to the adoption of the 2-norm soft margin. By placing xk in the same position at a distance \u03c1 in an additional dimension, i.e. by extending xk to [xk, \u03c1], we construct an embedding of our data into the socalled augmented space [3]. This way, we construct hyperplanes possessing bias\n1 The conversion of online algorithms to the batch setting is done by cycling repeatedly through the dataset and using the last hypothesis for prediction.\nin the non-augmented feature space. Following the augmentation, a reflection with respect to the origin of the negatively labeled patterns is performed by multiplying every pattern with its label. This allows for a uniform treatment of both categories of patterns. Also, R \u2261 max\nk \u2016yk\u2016 with yk \u2261 [lkxk, lk\u03c1] the kth\naugmented and reflected pattern. Obviously, R \u2265 \u03c1. The relation characterizing optimally correct classification of the training patterns yk by a weight vector u of unit norm in the augmented space is\nu \u00b7 yk \u2265 \u03b3d \u2261 max u \u2032 : \u2225 \u2225 \u2225 u \u2032 \u2225 \u2225 \u2225 = 1 min i {u\u2032 \u00b7 yi} \u2200k . (1)\nWe shall refer to \u03b3d as the maximum directional margin. It coincides with the maximum margin in the augmented space with respect to hyperplanes passing through the origin. For the maximum directional margin \u03b3d and the maximum geometric margin \u03b3 in the non-augmented feature space, it holds that 1 \u2264 \u03b3/\u03b3d \u2264 R/\u03c1. As \u03c1 \u2192 \u221e, R/\u03c1 \u2192 1 and, consequently, \u03b3d \u2192 \u03b3 [17, 18].\nWe consider algorithms in which the augmented weight vector at is initially set to zero, i.e. a0 = 0, and is updated according to the classical perceptron rule\nat+1 = at + yk (2)\neach time an appropriate misclassification condition is satisfied by a training pattern yk. Taking the inner product of (2) with the optimal direction u and using (1) we get u \u00b7 at+1 \u2212 u \u00b7 at = u \u00b7 yk \u2265 \u03b3d a repeated application of which gives [12]\n\u2016at\u2016 \u2265 u \u00b7 at \u2265 \u03b3dt . (3) From (3) we readily obtain\n\u03b3d \u2264 \u2016at\u2016 t\n(4)\nprovided t > 0. Notice that the above upper bound on the maximum directional margin \u03b3d is an immediate consequence of the classical perceptron rule and holds independent of the misclassification condition.\nIt would be very desirable that \u2016at\u2016 /t approaches \u03b3d with t increasing since this would provide an after-run estimate of the accuracy achieved by an algorithm employing the classical perceptron update. More specifically, with \u03b3\u2032d being the directional margin achieved upon convergence of the algorithm in tc updates, it holds that\n\u03b3d \u2212 \u03b3\u2032d \u03b3d \u2264 1\u2212 \u03b3 \u2032 dtc \u2016atc\u2016 . (5)\nIn order to understand the mechanism by which \u2016at\u2016 /t evolves we consider the difference between two consecutive values of \u2016at\u20162 /t2 which may be shown to be given by the relation\n\u2016at\u20162 t2 \u2212\u2016at+1\u2016 2 (t+ 1)2 =\n1\nt(t+ 1)\n{(\n\u2016at\u20162 t \u2212 at \u00b7 yk\n)\n+\n(\n\u2016at+1\u20162 t+ 1 \u2212 at+1 \u00b7 yk\n)}\n.\n(6)\nLet us assume that satisfaction of the misclassification condition by a pattern yk has as a consequence that \u2016at\u20162/t > at \u00b7 yk (i.e., the normalized margin ut \u00b7 yk of yk (with ut \u2261 at/ \u2016at\u2016) is smaller than the upper bound (4) on \u03b3d). Let us further assume that after the update has taken place yk still satisfies the misclassification condition and therefore \u2016at+1\u20162/(t+ 1) > at+1 \u00b7 yk. Then, the r.h.s. of (6) is positive and \u2016at\u2016 /t decreases as a result of the update. In the event, instead, that the update leads to violation of the misclassification condition, \u2016at+1\u20162/(t+ 1) is not necessarily larger than at+1 \u00b7 yk and \u2016at\u2016 /t may not decrease as a result of the update. We expect that statistically, at least in the early stages of the algorithm, most updates do not lead to correctly classified patterns (i.e., patterns which violate the misclassification condition) and as a consequence \u2016at\u2016 /t will have the tendency to decrease. Obviously, the rate at which this will take place depends on the size of the difference \u2016at\u20162 /t\u2212at \u00b7 yk which, in turn, depends on the misclassification condition.\nIf we are interested in obtaining solutions possessing margin the most natural choice of misclassification condition is the fixed (normalized) margin condition\nat \u00b7 yk \u2264 (1\u2212 \u01eb)\u03b3d \u2016at\u2016 (7)\nwith the accuracy parameter \u01eb satisfying 0 < \u01eb \u2264 1. This is an example of a misclassification condition which if it is satisfied ensures that \u2016at\u20162/t > at\u00b7yk. Moreover, by making use of (4) and (7) it may easily be shown that \u2016at+1\u20162/(t+ 1) \u2265 at+1 \u00b7 yk for t \u2265 \u01eb\u22121R2/\u03b32d. Thus, after at most \u01eb\u22121R2/\u03b32d updates \u2016at\u2016 /t decreases monotonically. The perceptron algorithm with fixed margin condition (PFM) is known to converge in a finite number of updates to an \u01eb-accurate approximation of the maximum directional margin hyperplane [17, 18, 1]. Although it appears that PFM demands exact knowledge of the value of \u03b3d, we notice that only the value of \u03b2 \u2261 (1 \u2212 \u01eb)\u03b3d, which is the quantity entering (7), needs to be set and not the values of \u01eb and \u03b3d separately. That is why the after-run estimate (5) is useful in connection with the algorithm in question. Nevertheless, in order to make sure that \u03b2 < \u03b3d a priori knowledge of a fairly good lower bound on \u03b3d is required and this is an obvious defect of PFM.\nThe above difficulty associated with the fixed margin condition may be remedied if the unknown \u03b3d is replaced for t > 0 with its varying upper bound \u2016at\u2016 /t\nat \u00b7 yk \u2264 (1\u2212 \u01eb) \u2016at\u20162\nt . (8)\nCondition (8) ensures that \u2016at\u20162/t \u2212 at \u00b7 yk \u2265 \u01eb\u2016at\u20162/t > 0. Moreover, as in the case of the fixed margin condition, \u2016at+1\u20162/(t+ 1) \u2212 at+1 \u00b7 yk \u2265 0 for t \u2265 \u01eb\u22121R2/\u03b32d. As a result, after at most \u01eb\u22121R2/\u03b32d updates the r.h.s. of (6) is bounded from below by \u01eb \u2016at\u20162 /t2(t + 1) \u2265 \u01eb\u03b32d/(t + 1) and \u2016at\u2016 /t decreases monotonically and sufficiently fast. Thus, we expect that \u2016at\u2016 /t will eventually approach \u03b3d close enough, thereby allowing for convergence of the algorithm to an \u01eb-accurate approximation of the maximum directional margin hyperplane. It is also apparent that the decrease of \u2016at\u2016 /t will be faster for larger values of \u01eb.\nThe Perceptron with Dynamic Margin\nInput: A linearly separable augmented dataset S = (y1, . . . ,yk, . . . ,ym) with reflection assumed Fix: \u01eb Define: qk = \u2016yk\u2016\n2 , \u01eb\u0304 = 1\u2212 \u01eb Initialize: t = 0, a0 = 0, \u21130 = 0, \u03b80 = 0 repeat\nfor k = 1 to m do ptk = at \u00b7 yk if ptk \u2264 \u03b8t then\nat+1 = at + yk \u2113t+1 = \u2113t + 2ptk + qk t \u2190 t+ 1 \u03b8t = \u01eb\u0304 \u2113t/t\nuntil no update made within the for loop\nThe perceptron algorithm employing the misclassification condition (8) (with its threshold set to 0 for t = 0), which may be regarded as originating from (7) with \u03b3d replaced for t > 0 by its dynamic upper bound \u2016at\u2016 /t, will be named the perceptron with dynamic margin (PDM).\n3 Theoretical Analysis\nFrom the discussion that led to the formulation of PDM it is apparent that if the algorithm converges it will achieve by construc-\ntion a solution possessing directional margin at least as large as (1 \u2212 \u01eb)\u03b3d. (We remind the reader that convergence assumes violation of the misclassification condition (8) by all patterns. In addition, (4) holds.) The same obviously applies to PFM. Thus, for both algorithms it only remains to be demonstrated that they converge in a finite number of steps. This has already been shown for PFM [17, 18, 1] but no general \u01eb-dependent bound in closed form has been derived. Our purpose in this section is to demonstrate convergence of PDM and provide explicit bounds for both algorithms.\nBefore we proceed with our analysis we will need the following result.\nLemma 1. Let the variable t \u2265 e\u2212C satisfy the inequality\nt < \u03b4(1 + C + ln t) , (9)\nwhere \u03b4, C are constants and \u03b4 > e\u2212C. Then\nt \u2264 t0 \u2261 (1 + e\u22121)\u03b4 (C + ln ((1 + e)\u03b4)) . (10)\nProof. If t \u2265 e\u2212C then (1 + C + ln t) \u2265 1 and inequality (9) is equivalent to f(t) = t/(1 + C + ln t) \u2212 \u03b4 < 0. For the function f(t) defined in the interval [e\u2212C ,+\u221e) it holds that f(e\u2212C) < 0 and df/dt = (C + ln t)/(1 + C + ln t)2 > 0 for t > e\u2212C . Stated differently, f(t) starts from negative values at t = e\u2212C and increases monotonically. Therefore, if f(t0) \u2265 0 then t0 is an upper bound of all t for which f(t) < 0. Indeed, it is not difficult to verify that t0 > \u03b4 > e \u2212C and\nf(t0) = \u03b4\n(\n(1 + e\u22121)\n(\n1 + ln ln(eC(1 + e)\u03b4)\nln(eC(1 + e)\u03b4)\n)\u22121 \u2212 1 ) \u2265 0\ngiven that ln lnx/ lnx \u2264 e\u22121. \u2293\u2294\nNow we are ready to derive an upper bound on the number of steps of PFM.\nTheorem 1. The number t of updates of the perceptron algorithm with fixed margin condition satisfies the bound\nt \u2264 (1 + e \u22121)\n2\u01eb\nR2 \u03b32d\n{\n4 \u03b3d R ( 1\u2212 \u03b3d R (1\u2212 \u01eb) ) + ln\n(\n(1 + e)\n\u01eb\nR\n\u03b3d\n( 1\u2212 \u03b3d R (1\u2212 \u01eb) )\n)}\n.\nProof. From (2) and (7) we get\n\u2016at+1\u20162 = \u2016at\u20162 + \u2016yk\u20162 + 2at \u00b7 yk \u2264 \u2016at\u20162 ( 1 + R2\n\u2016at\u20162 + 2(1\u2212 \u01eb)\u03b3d \u2016at\u2016\n)\n.\nThen, taking the square root and using the inequality \u221a 1 + x \u2264 1+x/2 we have\n\u2016at+1\u2016 \u2264 \u2016at\u2016 ( 1 + R2\n\u2016at\u20162 + 2(1\u2212 \u01eb)\u03b3d \u2016at\u2016\n) 1 2\n\u2264 \u2016at\u2016 ( 1 + R2\n2 \u2016at\u20162 + (1\u2212 \u01eb)\u03b3d \u2016at\u2016\n)\n.\nNow, by making use of \u2016at\u2016 \u2265 \u03b3dt, we observe that\n\u2016at+1\u2016 \u2212 \u2016at\u2016 \u2264 R2\n2 \u2016at\u2016 + (1\u2212 \u01eb)\u03b3d \u2264\nR2\n2\u03b3d\n1 t + (1\u2212 \u01eb)\u03b3d .\nA repeated application of the above inequality t\u2212N times (t > N \u2265 1) gives\n\u2016at\u2016 \u2212 \u2016aN\u2016 \u2264 R2\n2\u03b3d\nt\u22121 \u2211\nk=N\nk\u22121 + (1 \u2212 \u01eb)\u03b3d(t\u2212N)\n< R2\n2\u03b3d\n(\n1\nN +\n\u222b t\nN\nk\u22121dk\n)\n+ (1\u2212 \u01eb)\u03b3d(t\u2212N)\nfrom where using the obvious bound \u2016aN\u2016 \u2264 RN we get an upper bound on \u2016at\u2016\n\u2016at\u2016 < R2\n2\u03b3d\n(\n1\nN + ln\nt\nN\n)\n+ (1\u2212 \u01eb)\u03b3d(t\u2212N) +RN .\nCombining the above upper bound on \u2016at\u2016, which holds not only for t > N but also for t = N , with the lower bound from (3) we obtain\nt < 1\n2\u01eb\nR2 \u03b32d\n{\n1 N \u2212 lnN + 2\u03b3d R ( 1\u2212 \u03b3d R (1\u2212 \u01eb) ) N + ln t\n}\n.\nSetting\n\u03b4 = 1\n2\u01eb\nR2 \u03b32d , \u03b1 = 2 \u03b3d R ( 1\u2212 \u03b3d R (1\u2212 \u01eb) )\nand choosing N = 1+ [\u03b1\u22121], with [x] being the integer part of x \u2265 0, we finally get\nt < \u03b4(1 + 2\u03b1+ ln\u03b1+ ln t) . (11)\nNotice that in deriving (11) we made use of the fact that \u03b1N +N\u22121 \u2212 lnN < 1 + 2\u03b1 + ln\u03b1. Inequality (11) has the form (9) with C = 2\u03b1 + ln\u03b1. Obviously, e\u2212C < \u03b1\u22121 < N \u2264 t and e\u2212C < \u03b1\u22121 \u2264 \u03b4. Thus, the conditions of Lemma 1 are satisfied and the required bound, which is of the form (10), follows from (11). \u2293\u2294\nFinally, we arrive at our main result which is the proof of convergence of PDM in a finite number of steps and the derivation of the relevant upper bound.\nTheorem 2. The number t of updates of the perceptron algorithm with dynamic margin satisfies the bound\nt \u2264\n\n      \n       \nt0\n(\n1\u2212 11\u22122\u01eb R 2\n\u03b32 d\nt\u221210\n) 1 2\u01eb\n, t0 \u2261 [\u01eb\u22121] (\nR \u03b3d\n) 1 \u01eb (\n1 + [\u01eb \u22121]\u22121\n1\u22122\u01eb\n) 1 2\u01eb\nif \u01eb < 12\n(1 + e\u22121)R 2\n\u03b32 d\nln ( (1 + e)R 2\n\u03b32 d\n)\nif \u01eb = 12\nt0 ( 1\u2212 2(1\u2212 \u01eb)t1\u22122\u01eb0 ) , t0 \u2261 \u01eb(3\u22122\u01eb)2\u01eb\u22121 R 2\n\u03b32 d\nif \u01eb > 12 .\nProof. From (2) and (8) we get\n\u2016at+1\u20162 = \u2016at\u20162 + 2at \u00b7 yk + \u2016yk\u20162 \u2264 \u2016at\u20162 ( 1 + 2(1\u2212 \u01eb)\nt\n)\n+R2 . (12)\nLet us assume that \u01eb < 1/2. Then, using the inequality (1+ x)\u03b6 \u2265 1+ \u03b6x for x \u2265 0, \u03b6 = 2(1\u2212 \u01eb) \u2265 1 in (12) we obtain\n\u2016at+1\u20162 \u2264 \u2016at\u20162 ( 1 + 1\nt\n)2(1\u2212\u01eb)\n+R2\nfrom where by dividing both sides with (t+ 1)2(1\u2212\u01eb) we arrive at\n\u2016at+1\u20162 (t+ 1)2(1\u2212\u01eb) \u2212 \u2016at\u2016 2 t2(1\u2212\u01eb) \u2264 R 2 (t+ 1)2(1\u2212\u01eb) .\nA repeated application of the above inequality t\u2212N times (t > N \u2265 1) gives\n\u2016at\u20162 t2(1\u2212\u01eb) \u2212 \u2016aN\u2016 2 N2(1\u2212\u01eb) \u2264 R2 t \u2211\nk=N+1\nk\u22122(1\u2212\u01eb) \u2264 R2 \u222b t\nN\nk\u22122(1\u2212\u01eb)dk\n= R2N2\u01eb\u22121\n2\u01eb\u2212 1\n(\n(\nt\nN\n)2\u01eb\u22121 \u2212 1 ) .(13)\nNow, let us define\n\u03b1t \u2261 \u2016at\u2016 Rt\nand observe that the bounds \u2016at\u2016 \u2264 Rt and \u2016at\u2016 \u2265 \u03b3dt confine \u03b1t to lie in the range\n\u03b3d R \u2264 \u03b1t \u2264 1 .\nSetting \u2016aN\u2016 = \u03b1NRN in (13) we get the following upper bound on \u2016at\u20162\n\u2016at\u20162 \u2264 t2(1\u2212\u01eb)\u03b12NR2N2\u01eb { 1 + \u03b1\u22122N N \u22121\n2\u01eb\u2212 1\n(\n(\nt\nN\n)2\u01eb\u22121 \u2212 1 )}\nwhich combined with the lower bound \u2016at\u20162 \u2265 \u03b32dt2 leads to\nt2\u01eb \u2264 \u03b12N R2\n\u03b32d N2\u01eb\n{\n1 + \u03b1\u22122N N \u22121\n2\u01eb\u2212 1\n(\n(\nt\nN\n)2\u01eb\u22121 \u2212 1 )} . (14)\nFor \u01eb < 1/2 the term proportional to (t/N)2\u01eb\u22121 in (14) is negative and may be dropped to a first approximation leading to the looser upper bound t0\nt0 \u2261 N ( \u03b1N R\n\u03b3d\n) 1 \u01eb (\n1 + \u03b1\u22122N N \u22121\n1\u2212 2\u01eb\n) 1 2\u01eb\n(15)\non the number t of updates. Then, we may replace t with its upper bound t0 in the r.h.s. of (14) and get the improved bound\nt \u2264 t0 ( 1\u2212 1 1\u2212 2\u01eb R2 \u03b32d t\u221210 )\n1 2\u01eb\n.\nThis is allowed given that the term proportional to (t/N)2\u01eb\u22121 in (14) is negative and moreover t is raised to a negative power. Choosing N = [\u01eb\u22121] and \u03b1N = 1 (i.e., setting \u03b1N to its upper bound which is the least favorable assumption) we obtain the bound stated in Theorem 2 for \u01eb < 1/2.\nNow, let \u01eb > 1/2. Then, using the inequality (1+x)\u03b6 + \u03b6(1\u2212 \u03b6)x2/2 \u2265 1+ \u03b6x for x \u2265 0, 0 \u2264 \u03b6 = 2(1\u2212 \u01eb) \u2264 1 in (12) and the bound \u2016at\u2016 \u2264 Rt we obtain\n\u2016at+1\u20162 \u2264 \u2016at\u20162 ( 1 + 1\nt\n)2(1\u2212\u01eb) + (1 \u2212 \u01eb)(2\u01eb\u2212 1)\u2016at\u2016 2\nt2 +R2\n\u2264 \u2016at\u20162 ( 1 + 1\nt\n)2(1\u2212\u01eb)\n+ \u01eb(3\u2212 2\u01eb)R2 .\nBy dividing both sides of the above inequality with (t+ 1)2(1\u2212\u01eb) we arrive at\n\u2016at+1\u20162 (t+ 1)2(1\u2212\u01eb) \u2212 \u2016at\u2016 2 t2(1\u2212\u01eb) \u2264 \u01eb(3\u2212 2\u01eb) R 2 (t+ 1)2(1\u2212\u01eb) (16)\na repeated application of which, using also \u2016a1\u20162 \u2264 R2 \u2264 \u01eb(3\u2212 2\u01eb)R2, gives\n\u2016at\u20162 t2(1\u2212\u01eb) \u2264 \u01eb(3\u2212 2\u01eb)R2 t \u2211\nk=1\nk\u22122(1\u2212\u01eb) \u2264 \u01eb(3\u2212 2\u01eb)R2 ( 1 + \u222b t\n1\nk\u22122(1\u2212\u01eb)dk\n)\n= \u01eb(3\u2212 2\u01eb)R2 ( 1 + t2\u01eb\u22121 \u2212 1 2\u01eb\u2212 1 ) .\nCombining the above bound with the bound \u2016at\u20162 \u2265 \u03b32dt2 we obtain\nt2\u01eb \u2264 \u01eb(3\u2212 2\u01eb)R 2\n\u03b32d\n(\n1 + t2\u01eb\u22121 \u2212 1 2\u01eb\u2212 1\n)\n(17)\nor\nt \u2264 \u01eb(3\u2212 2\u01eb) 2\u01eb\u2212 1 R2\n\u03b32d\n( 1\u2212 2(1\u2212 \u01eb)t1\u22122\u01eb ) . (18)\nFor \u01eb > 1/2 the term proportional to t1\u22122\u01eb in (18) is negative and may be dropped to a first approximation leading to the looser upper bound t0\nt0 \u2261 \u01eb(3\u2212 2\u01eb) 2\u01eb\u2212 1 R2\n\u03b32d\non the number t of updates. Then, we may replace t with its upper bound t0 in the r.h.s. of (18) and get the improved bound stated in Theorem 2 for \u01eb > 1/2. This is allowed given that the term proportional to t1\u22122\u01eb in (18) is negative and moreover t is raised to a negative power.\nFinally, taking the limit \u01eb \u2192 1/2 in (14) (with N = 1, \u03b1N = 1) or in (17) we get\nt \u2264 R 2\n\u03b32d (1 + ln t)\nwhich on account of Lemma 1 leads to the bound of Theorem 2 for \u01eb = 1/2. \u2293\u2294\nRemark 1. The bound of Theorem 2 holds for PFM as well on account of (4).\nThe worst-case bound of Theorem 2 for \u01eb \u226a 1 behaves like \u01eb\u22121(R/\u03b3d) 1 \u01eb which suggests an extremely slow convergence if we require margins close to the maximum. From expression (15) for t0, however, it becomes apparent that a more favorable assumption concerning the value of \u03b1N (e.g., \u03b1N \u226a 1 or even as low as \u03b1N \u223c \u03b3d/R) after the first N \u226b \u03b1\u22122N updates does lead to tremendous improvement provided, of course, that N is not extremely large. Such a sharp decrease of \u2016at\u2016 /t in the early stages of the algorithm, which may be expected from relation (6) and the discussion that followed, lies behind its experimentally exhibited rather fast convergence.\nIt would be interesting to find a procedure by which the algorithm will be forced to a guaranteed sharp decrease of the ratio \u2016at\u2016 /t. The following two observations will be vital in devising such a procedure. First, we notice that when PDM with accuracy parameter \u01eb has converged in tc updates the threshold (1\u2212\u01eb)\u2016atc\u20162/tc of the misclassification condition must have fallen below \u03b3d \u2016atc\u2016. Otherwise, the normalized margin utc \u00b7yk of all patterns yk would be larger than \u03b3d. Thus, \u03b1tc < (1\u2212 \u01eb)\u22121\u03b3d/R. Second, after convergence of the algorithm with accuracy parameter \u01eb1 in tc1 updates we may lower the accuracy parameter from the value \u01eb1 to the value \u01eb2 and continue the run from the point where convergence with parameter \u01eb1 has taken place since for all updates that took place during the first run the misclassified patterns would certainly satisfy (at that time) the condition associated with the smaller parameter \u01eb2. This way, the first run is legitimately fully incorporated into the second one and the tc1 updates required for convergence during the first run may be considered the first tc1 updates of the second run under this specific policy of presenting patterns to the algorithm. Combining the above two observations we see that by employing\na first run with accuracy parameter \u01eb1 we force the algorithm with accuracy parameter \u01eb2 < \u01eb1 to have \u03b1t decreased from a value \u223c 1 to a value \u03b1tc1 < (1\u2212 \u01eb1)\u22121\u03b3d/R in the first tc1 updates.\nThe above discussion suggests that we consider a decreasing sequence of parameters \u01ebn such that \u01ebn+1 = \u01ebn/\u03b7 (\u03b7 > 1) starting with \u01eb0 = 1/2 and ending with the required accuracy \u01eb and perform successive runs of PDM with accuracies \u01ebn until convergence in tcn updates is reached. According to our earlier discussion tcn includes the updates that led the algorithm to convergence in the current and all previous runs. Moreover, at the end of the run with parameter \u01ebn we will have ensured that \u03b1tcn < (1\u2212 \u01ebn)\u22121\u03b3d/R. Therefore, tcn+1 satisfies tcn+1 \u2264 t0 or\ntcn+1 \u2264 tcn ( 1\n1\u2212 \u01ebn\n)\u03b7/\u01ebn (\n1 + (1\u2212 \u01ebn)2 1\u2212 2\u01ebn/\u03b7 R2 \u03b32d t\u22121cn\n)\u03b7/2\u01ebn\n.\nThis is obtained by substituting in (15) the values \u01eb = \u01ebn+1 = \u01ebn/\u03b7, N = tcn and \u03b1N = (1\u2212 \u01ebn)\u22121\u03b3d/R which is the least favorable choice for \u03b1tcn . Let us assume that \u01ebn \u226a 1 and set tcn = \u03be\u22121n R2/\u03b32d with \u03ben \u226a 1. Then, 1/(1\u2212 \u01ebn)\u03b7/\u01ebn \u2243 e\u03b7 and\n(\n1 + (1\u2212 \u01ebn)2 1\u2212 2\u01ebn/\u03b7 R2 \u03b32d t\u22121cn\n)\u03b7/2\u01ebn\n\u2243 (1 + \u03ben)\u03b7/2\u01ebn \u2243 e\u03b7\u03ben/2\u01ebn .\nFor \u03ben \u2243 \u01ebn the term above becomes approximately e\u03b7/2 while for \u03ben \u226a \u01ebn approaches 1. We see that under the assumption that PDM with accuracy parameter \u01ebn converges in a number of updates \u226b R2/\u03b32d the ratio tcn+1/tcn in the successive run scenario is rather tightly constrained. If, instead, our assumption is not satisfied then convergence of the algorithm is fast anyway. Notice, that the value of tcn+1/tcn inferred from the bound of Theorem 2 is \u223c \u03b7 (R/\u03b3d)(\u03b7\u22121)/\u01ebn which is extremely large. We conclude that PDM employing the successive run scenario (PDM-succ) potentially converges in a much smaller number of steps."}, {"heading": "4 Efficient Implementation", "text": "To reduce the computational cost involved in running PDM, we extend the procedure of [14, 13] and construct a three-member nested sequence of reduced \u201cactive sets\u201d of data points. As we cycle once through the full dataset, the (largest) first-level active set is formed from the points of the full dataset satisfying at \u00b7 yk \u2264 c1(1 \u2212 \u01eb) \u2016at\u20162 /t with c1 = 2.2. Analogously, the second-level active set is formed as we cycle once through the first-level active set from the points which satisfy at \u00b7 yk \u2264 c2(1 \u2212 \u01eb) \u2016at\u20162 /t with c2 = 1.1. The third-level active set comprises the points that satisfy at \u00b7 yk \u2264 (1\u2212 \u01eb) \u2016at\u2016 2 /t as we cycle once through the second-level active set. The third-level active set is presented repetitively to the algorithm for Nep3 mini-epochs. Then, the second-level active set is presented Nep2 times. During each round involving the second-level set, a new third-level set is constructed and a new cycle of Nep3 passes begins. When the number of Nep2 cycles involving the second-level set is reached the first-level\nset becomes active again leading to the population of a new second-level active set. By invoking the first-level set for the (Nep1+1)\nth time, we trigger the loading of the full dataset and the procedure starts all over again until no point is found misclassified among the ones comprising the full dataset. Of course, the Nep1 , Nep2 and Nep3 rounds are not exhausted if no update takes place during a round. In all experiments we choose Nep1 = 9, Nep2 = Nep3 = 12. In addition, every time we make use of the full dataset we actually employ a permuted instance of it. Evidently, the whole procedure amounts to a different way of sequentially presenting the patterns to the algorithm and does not affect the applicability of our theoretical analysis. A completely analogous procedure is followed for PFM.\nAn additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14, 13] once a data point is presented to the algorithm. It is understood, of course, that in order for a multiple update to be compatible with our theoretical analysis it should be equivalent to a certain number of updates occuring as a result of repeatedly presenting to the algorithm the data point in question. For PDM when a pattern yk is found to satisfy the misclassification condition (8) we perform \u03bb = [\u00b5+] + 1 updates at once. Here, \u00b5+ is the smallest non-negative root of the quadratic equation in the variable \u00b5 derivable from the relation (t+ \u00b5)at+\u00b5 \u00b7 yk \u2212 (1 \u2212 \u01eb) \u2016at+\u00b5\u20162 = 0 in which at+\u00b5 \u00b7yk = at \u00b7yk +\u00b5 \u2016yk\u20162 and \u2016at+\u00b5\u20162 = \u2016at\u20162 +2\u00b5at \u00b7yk +\u00b52 \u2016yk\u20162. Thus, we require that as a result of the multiple update the pattern violates the misclassification condition. Similarly, we perform multiple updates for PFM.\nFinally, in the case of PDM (no successive runs) when we perform multiple updates we start doing so after the first full epoch. This way, we avoid the excessive growth of the length of the weight vector due to the contribution to the solution of many aligned patterns in the early stages of the algorithm which hinders the fast decrease of \u2016at\u2016 /t. Moreover, in this scenario when we select the first-level active set as we go through the full dataset for the first time (first full epoch) we found it useful to set c1 = c2 = 1.1 instead of c1 = 2.2."}, {"heading": "5 Experimental Evaluation", "text": "We compare PDM with several other large margin classifiers on the basis of their ability to achieve fast convergence to a certain approximation of the \u201coptimal\u201d hyperplane in the feature space where the patterns are linearly separable. For linearly separable data the feature space is the initial instance space whereas for inseparable data (which is the case here) a space extended by as many dimensions as the instances is considered where each instance is placed at a distance \u2206 from the origin in the corresponding dimension2 [4]. This extension generates a margin of at least \u2206/ \u221a m. Moreover, its employment relies on the well-known\n2 yk = [y\u0304k, lk\u2206\u03b41k, . . . , lk\u2206\u03b4mk], where \u03b4ij is Kronecker\u2019s \u03b4 and y\u0304k the projection of the kth extended instance yk (multiplied by its label lk) onto the initial instance space. The feature space mapping defined by the extension commutes with a possible augmentation (with parameter \u03c1) in which case y\u0304k = [lkx\u0304k, lk\u03c1]. Here x\u0304k represents the kth data point.\nequivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function \u2016w\u20162 +\u2206\u22122 \u2211\ni\u03be\u0304 2 i involving the weight vector w and the 2-norm of the slacks \u03be\u0304i\n[2]. Of course, all algorithms are required to solve identical hard margin problems.\nThe datasets we used for training are: the Adult (m = 32561 instances, n = 123 attributes) and Web (m = 49749, n = 300) UCI datasets as compiled by Platt [15], the training set of the KDD04 Physics dataset (m = 50000, n = 70 after removing the 8 columns containing missing features) obtainable from http://kodiak.cs.cornell.edu/kddcup/datasets.html, the Real-sim (m = 72309, n = 20958), News20 (m = 19996, n = 1355191) and Webspam (unigram treatment with m = 350000, n = 254) datasets all available at http:// www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets, the multiclass Covertype UCI dataset (m = 581012, n = 54) and the full Reuters RCV1 dataset (m = 804414, n = 47236) obtainable from http://www.jmlr.org/papers/ volume5/lewis04a/lyrl2004_rcv1v2_README.htm. For the Covertype dataset we study the binary classification problem of the first class versus rest while for the RCV1 we consider both the binary text classification tasks of the C11 and CCAT classes versus rest. The Physics and Covertype datasets were rescaled by a multiplicative factor 0.001. The experiments were conducted on a 2.5 GHz Intel Core 2 Duo processor with 3 GB RAM running Windows Vista. Our codes written in C++ were compiled using the g++ compiler under Cygwin.\nThe parameter \u2206 of the extended space is chosen from the set {3, 1, 0.3, 0.1} in such a way that it corresponds approximately to R/10 or R/3 depending on the size of the dataset such that the ratio \u03b3d/R does not become too small (given that the extension generates a margin of at least \u2206/ \u221a m). More specifically, we have chosen \u2206 = 3 for Covertype, \u2206 = 1 for Adult, Web and Physics, \u2206 = 0.3 for Webspam, C11 and CCAT and \u2206 = 0.1 for Real-sim and News20. We also verified that smaller values of \u2206 do not lead to a significant decrease of the training error. For all datasets and for algorithms that introduce bias through augmentation the associated parameter \u03c1 was set to the value \u03c1 = 1.\nWe begin our experimental evaluation by comparing PDM with PFM. We run PDM with accuracy parameter \u01eb = 0.01 and subsequently PFM with the fixed margin \u03b2 = (1\u2212 \u01eb)\u03b3d set to the value \u03b3\u2032d of the directional margin achieved by PDM. This procedure is repeated using PDM-succ with step \u03b7 = 8 (i.e., \u01eb0 = 0.5, \u01eb1 = 0.0625, \u01eb2 = \u01eb = 0.01). Our results (the value of the directional margin \u03b3\u2032d achieved, the number of required updates (upd) for convergence and the CPU time for training in seconds (s)) are presented in Table 1. We see that PDM is considerably faster than PFM as far as training time is concerned in spite of the fact that PFM needs much less updates for convergence. The successive run scenario succeeds, in accordance with our expectations, in reducing the number of updates to the level of the updates needed by PFM in order to achieve the same value of \u03b3\u2032d at the expense of an increased runtime. We believe that it is fair to say that PDM-succ with \u03b7 = 8 has the overall performance of PFM without the defect of the need for a priori knowledge of the value of \u03b3d. We also notice that although the accuracy \u01eb is set to the same value for both scenarios\nthe margin achieved with successive runs is lower. This is an indication that PDM-succ obtains a better estimate of the maximum directional margin \u03b3d.\nWe also considered other large margin classifiers representing classes of algorithms such as perceptron-like algorithms, decomposition SVMs and linear SVMs with the additional requirement that the chosen algorithms need only specification of an accuracy parameter. From the class of perceptron-like algorithms we have chosen (aggressive) ROMMA which is much faster than ALMA in the light of the results presented in [9, 14]. Decomposition SVMs are represented by SVMlight [7] which, apart from being one of the fastest algorithms of this class, has the additional advantage of making very efficient use of memory, thereby making possible the training on very large datasets. Finally, from the more recent class of linear SVMs we have included in our study the dual coordinate descent (DCD) algorithm [8] and the margin perceptron with unlearning (MPU)3 [13]. We considered the DCD versions with 1-norm (DCDL1) and 2-norm (DCD-L2) soft margin which for the same value of the accuracy parameter produce identical solutions if the penalty parameter is C = \u221e for DCD-L1 and C = 1/(2\u22062) for DCD-L2. The source for SVMlight (version 6.02) is available at http://smvlight.joachims.org and for DCD at http: //www.csie.ntu.edu.tw/~cjlin/liblinear. The absence of publicly available implementations for ROMMA necessitated the writing of our own code in C++ employing the mechanism of active sets proposed in [9] and incorporating a mechanism of permutations performed at the beginning of a full epoch. For MPU the implementation followed closely [13] with active set parameters c\u0304 = 1.01, Nep1 = Nep2 = 5, gap parameter \u03b4b = 3R 2 and early stopping.\nThe experimental results (margin values achieved and training runtimes) involving the above algorithms with the accuracy parameter set to 0.01 for all of\n3 MPU uses dual variables but is not formulated as an optimization. It is a perceptron incorporating a mechanism of reduction of possible contributions from \u201cvery-well classified\u201d patterns to the weight vector which is an essential ingredient of SVMs.\nthem are summarized in Table 2. Notice that for SVMlight we give the geometric margin \u03b3\u2032 instead of the directional one \u03b3\u2032d because SVM\nlight does not introduce bias through augmentation. For the rest of the algorithms considered, including PDM and PFM, the geometric margin \u03b3\u2032 achieved is not listed in the tables since it is very close to the directional margin \u03b3\u2032d if the augmentation parameter \u03c1 is set to the value \u03c1 = 1. Moreover, for DCD-L1 and DCD-L2 the margin values coincide as we pointed out earlier. From Table 2 it is apparent that ROMMA and SVMlight are orders of magnitude slower than DCD and MPU. Comparing the results of Table 1 with those of Table 2 we see that PDM is orders of magnitude faster than ROMMA which is its natural competitor since they both belong to the class of perceptron-like algorithms. PDM is also much faster than SVMlight but statistically a few times slower than DCD, especially for the larger datasets. Moreover, PDM is a few times slower than MPU for all datasets. Finally, we observe that the accuracy achieved by PDM is, in general, closer to the beforerun accuracy 0.01 since in most cases PDM obtains lower margin values. This indicates that PDM succeeds in obtaining a better estimate of the maximum margin than the remaining algorithms with the possible exception of MPU.\nBefore we conclude our comparative study it is fair to point out that PDM is not the fastest perceptron-like large margin classifier. From the results of [14] the fastest algorithm of this class is the margitron which has strong before-run guarantees and a very good after-run estimate of the achieved accuracy through (5). However, its drawback is that an approximate knowledge of the value of \u03b3d (preferably an upper bound) is required in order to fix the parameter controlling the margin threshold. Although there is a procedure to obtain this information, taking all the facts into account the employment of PDM seems preferable."}, {"heading": "6 Conclusions", "text": "We introduced the perceptron with dynamic margin (PDM), a new approximate maximum margin classifier employing the classical perceptron update, demonstrated its convergence in a finite number of steps and derived an upper bound on them. PDM uses the required accuracy as the only input parameter. Moreover, it is a strictly online algorithm in the sense that it decides whether to perform an update taking into account only its current state and irrespective of whether the pattern presented to it has been encountered before in the process of cycling repeatedly through the dataset. This certainly does not hold for linear SVMs. Our experimental results indicate that PDM is the fastest large margin classifier enjoying the above two very desirable properties."}, {"heading": "1. Blum, A.: Lectures on machine learning theory. Carnegie Mellon University, USA.", "text": "Available at http://www.cs.cmu.edu/ avrim/ML09/lect0126.pdf 2. Cristianini, N., Shawe-Taylor, J.: An introduction to support vector machines (2000) Cambridge, UK: Cambridge University Press 3. Duda, R.O., Hart, P.E.: Pattern classsification and scene analysis (1973) Wiley 4. Freund, Y., Shapire, R.E.: Large margin classification using the perceptron algorithm. Machine Learning 37(3) (1999) 277\u2013296 5. Gentile, C.: A new approximate maximal margin classification algorithm. Journal of Machine Learning Research 2 (2001) 213\u2013242 6. Joachims, T.: Making large-scale SVM learning practical. In Advances in kernel methods-support vector learning (1999) MIT Press 7. Joachims, T.: Training linear SVMs in linear time. KDD (2006) 217\u2013226 8. Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S.S., Sundararajan, S.: A dual coordinate descent method for large-scale linear SVM. ICML (2008) 408\u2013415 9. Ishibashi, K., Hatano, K., Takeda, M.: Online learning of maximum p-norm margin classifiers. COLT (2008) 69-80. 10. Krauth, W., Me\u0301zard, M.: Learning algorithms with optimal stability in neural networks. Journal of Physics A20 (1987) L745\u2013L752 11. Li, Y., Long, P.: The relaxed online maximummargin algorithm. Machine Learning, 46(1-3) (2002) 361\u2013387 12. Novikoff, A.B.J.: On convergence proofs on perceptrons. In Proc. Symp. Math. Theory Automata, Vol. 12 (1962) 615\u2013622 13. Panagiotakopoulos, C., Tsampouka, P.: The margin perceptron with unlearning. ICML (2010) 855-862 14. Panagiotakopoulos, C., Tsampouka, P.: The margitron: A generalized perceptron with margin. IEEE Transactions on Neural Networks 22(3) (2011) 395-407 15. Platt, J.C.: Sequential minimal optimization: A fast algorithm for training support vector machines. Microsoft Res. Redmond WA, Tech. Rep. MSR-TR-98-14 (1998) 16. Rosenblatt, F.: The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6) (1958) 386\u2013408 17. Tsampouka, P., Shawe-Taylor, J.: Perceptron-like large margin classifiers. Tech. Rep., ECS, University of Southampton, UK (2005). Obtainable from http://eprints.ecs.soton.ac.uk/10657"}, {"heading": "18. Tsampouka, P., Shawe-Taylor, J.: Analysis of generic perceptron-like large margin", "text": "classifiers. ECML (2005) 750\u2013758 19. Tsampouka, P., Shawe-Taylor, J.: Constant rate approximate maximum margin algorithms. ECML (2006) 437\u2013448 20. Tsampouka, P., Shawe-Taylor, J.: Approximate maximum margin algorithms with rules controlled by the number of mistakes. ICML (2007) 903\u2013910 21. Vapnik, V.: Statistical learning theory (1998) Wiley"}], "references": [{"title": "An introduction to support vector machines", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Pattern classsification and scene analysis", "author": ["R.O. Duda", "P.E. Hart"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1973}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Y. Freund", "R.E. Shapire"], "venue": "Machine Learning 37(3)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "A new approximate maximal margin classification algorithm", "author": ["C. Gentile"], "venue": "Journal of Machine Learning Research 2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "In Advances in kernel methods-support vector learning", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "KDD", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["Hsieh", "C.-J.", "Chang", "K.-W.", "Lin", "C.-J.", "S.S. Keerthi", "S. Sundararajan"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning of maximum p-norm margin classifiers", "author": ["K. Ishibashi", "K. Hatano", "M. Takeda"], "venue": "COLT", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning algorithms with optimal stability in neural networks", "author": ["W. Krauth", "M. M\u00e9zard"], "venue": "Journal of Physics A20", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1987}, {"title": "The relaxed online maximummargin algorithm", "author": ["Y. Li", "P. Long"], "venue": "Machine Learning, 46(1-3)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "On convergence proofs on perceptrons", "author": ["A.B.J. Novikoff"], "venue": "In Proc. Symp. Math. Theory Automata, Vol. 12", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1962}, {"title": "The margin perceptron with unlearning", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "ICML", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "The margitron: A generalized perceptron with margin", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "IEEE Transactions on Neural Networks 22(3)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["J.C. Platt"], "venue": "Microsoft Res. Redmond WA, Tech. Rep. MSR-TR-98-14", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65(6)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1958}, {"title": "Perceptron-like large margin classifiers", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "Tech. Rep., ECS, University of Southampton, UK", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Analysis of generic perceptron-like large margin classifiers", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "ECML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Constant rate approximate maximum margin algorithms", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "ECML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximate maximum margin algorithms with rules controlled by the number of mistakes", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistical learning theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}], "referenceMentions": [{"referenceID": 19, "context": "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].", "startOffset": 220, "endOffset": 227}, {"referenceID": 0, "context": "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].", "startOffset": 220, "endOffset": 227}, {"referenceID": 13, "context": "To overcome this problem decomposition methods [15, 6] were developed that apply optimization only to a subset of the training set.", "startOffset": 47, "endOffset": 54}, {"referenceID": 4, "context": "To overcome this problem decomposition methods [15, 6] were developed that apply optimization only to a subset of the training set.", "startOffset": 47, "endOffset": 54}, {"referenceID": 5, "context": "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.", "startOffset": 36, "endOffset": 46}, {"referenceID": 6, "context": "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.", "startOffset": 36, "endOffset": 46}, {"referenceID": 11, "context": "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.", "startOffset": 36, "endOffset": 46}, {"referenceID": 14, "context": "Such algorithms are mostly based on the perceptron [16, 12], the simplest online learning algorithm for binary linear classification.", "startOffset": 51, "endOffset": 59}, {"referenceID": 10, "context": "Such algorithms are mostly based on the perceptron [16, 12], the simplest online learning algorithm for binary linear classification.", "startOffset": 51, "endOffset": 59}, {"referenceID": 1, "context": "The first algorithm of that kind is the perceptron with margin [3] which is much older than SVMs.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "It is an immediate extension of the perceptron which provably achieves solutions with only up to 1/2 of the maximum margin [10].", "startOffset": 123, "endOffset": 127}, {"referenceID": 9, "context": "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Very recently, the same goal was accomplished by a generalized perceptron with margin, the margitron [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "The most straightforward way of obtaining large margin solutions through a perceptron is by requiring that the weight vector be updated every time the example presented to the algorithm has (normalized) margin which does not exceed a predefined value [17, 18, 1].", "startOffset": 251, "endOffset": 262}, {"referenceID": 16, "context": "The most straightforward way of obtaining large margin solutions through a perceptron is by requiring that the weight vector be updated every time the example presented to the algorithm has (normalized) margin which does not exceed a predefined value [17, 18, 1].", "startOffset": 251, "endOffset": 262}, {"referenceID": 12, "context": "In an earlier work [14] we noticed that the upper bound \u2016at\u2016 /t on the maximum margin, with \u2016at\u2016 being the length of the weight vector and t the number of updates, that comes as an immediate consequence of the perceptron update rule is very accurate and tends to improve as the algorithm achieves larger margins.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "This training set may either be the original dataset or the result of a mapping into a feature space of higher dimensionality [21, 2].", "startOffset": 126, "endOffset": 133}, {"referenceID": 0, "context": "This training set may either be the original dataset or the result of a mapping into a feature space of higher dimensionality [21, 2].", "startOffset": 126, "endOffset": 133}, {"referenceID": 2, "context": "Actually, there is a very well-known construction [4] making linear separability always possible, which amounts to the adoption of the 2-norm soft margin.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "by extending xk to [xk, \u03c1], we construct an embedding of our data into the socalled augmented space [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 15, "context": "As \u03c1 \u2192 \u221e, R/\u03c1 \u2192 1 and, consequently, \u03b3d \u2192 \u03b3 [17, 18].", "startOffset": 44, "endOffset": 52}, {"referenceID": 16, "context": "As \u03c1 \u2192 \u221e, R/\u03c1 \u2192 1 and, consequently, \u03b3d \u2192 \u03b3 [17, 18].", "startOffset": 44, "endOffset": 52}, {"referenceID": 10, "context": "Taking the inner product of (2) with the optimal direction u and using (1) we get u \u00b7 at+1 \u2212 u \u00b7 at = u \u00b7 yk \u2265 \u03b3d a repeated application of which gives [12] \u2016at\u2016 \u2265 u \u00b7 at \u2265 \u03b3dt .", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "The perceptron algorithm with fixed margin condition (PFM) is known to converge in a finite number of updates to an \u01eb-accurate approximation of the maximum directional margin hyperplane [17, 18, 1].", "startOffset": 186, "endOffset": 197}, {"referenceID": 16, "context": "The perceptron algorithm with fixed margin condition (PFM) is known to converge in a finite number of updates to an \u01eb-accurate approximation of the maximum directional margin hyperplane [17, 18, 1].", "startOffset": 186, "endOffset": 197}, {"referenceID": 15, "context": "This has already been shown for PFM [17, 18, 1] but no general \u01eb-dependent bound in closed form has been derived.", "startOffset": 36, "endOffset": 47}, {"referenceID": 16, "context": "This has already been shown for PFM [17, 18, 1] but no general \u01eb-dependent bound in closed form has been derived.", "startOffset": 36, "endOffset": 47}, {"referenceID": 12, "context": "To reduce the computational cost involved in running PDM, we extend the procedure of [14, 13] and construct a three-member nested sequence of reduced \u201cactive sets\u201d of data points.", "startOffset": 85, "endOffset": 93}, {"referenceID": 11, "context": "To reduce the computational cost involved in running PDM, we extend the procedure of [14, 13] and construct a three-member nested sequence of reduced \u201cactive sets\u201d of data points.", "startOffset": 85, "endOffset": 93}, {"referenceID": 12, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14, 13] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 142}, {"referenceID": 11, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14, 13] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 142}, {"referenceID": 2, "context": "For linearly separable data the feature space is the initial instance space whereas for inseparable data (which is the case here) a space extended by as many dimensions as the instances is considered where each instance is placed at a distance \u2206 from the origin in the corresponding dimension [4].", "startOffset": 293, "endOffset": 296}, {"referenceID": 0, "context": "equivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function \u2016w\u2016 +\u2206 \u2211 i\u03be\u0304 2 i involving the weight vector w and the 2-norm of the slacks \u03be\u0304i [2].", "startOffset": 238, "endOffset": 241}, {"referenceID": 13, "context": "The datasets we used for training are: the Adult (m = 32561 instances, n = 123 attributes) and Web (m = 49749, n = 300) UCI datasets as compiled by Platt [15], the training set of the KDD04 Physics dataset (m = 50000, n = 70 after removing the 8 columns containing missing features) obtainable from http://kodiak.", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "From the class of perceptron-like algorithms we have chosen (aggressive) ROMMA which is much faster than ALMA in the light of the results presented in [9, 14].", "startOffset": 151, "endOffset": 158}, {"referenceID": 12, "context": "From the class of perceptron-like algorithms we have chosen (aggressive) ROMMA which is much faster than ALMA in the light of the results presented in [9, 14].", "startOffset": 151, "endOffset": 158}, {"referenceID": 5, "context": "Decomposition SVMs are represented by SVM [7] which, apart from being one of the fastest algorithms of this class, has the additional advantage of making very efficient use of memory, thereby making possible the training on very large datasets.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "Finally, from the more recent class of linear SVMs we have included in our study the dual coordinate descent (DCD) algorithm [8] and the margin perceptron with unlearning (MPU) [13].", "startOffset": 125, "endOffset": 128}, {"referenceID": 11, "context": "Finally, from the more recent class of linear SVMs we have included in our study the dual coordinate descent (DCD) algorithm [8] and the margin perceptron with unlearning (MPU) [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 7, "context": "The absence of publicly available implementations for ROMMA necessitated the writing of our own code in C++ employing the mechanism of active sets proposed in [9] and incorporating a mechanism of permutations performed at the beginning of a full epoch.", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "For MPU the implementation followed closely [13] with active set parameters c\u0304 = 1.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "From the results of [14] the fastest algorithm of this class is the margitron which has strong before-run guarantees and a very good after-run estimate of the achieved accuracy through (5).", "startOffset": 20, "endOffset": 24}], "year": 2013, "abstractText": "The classical perceptron rule provides a varying upper bound on the maximum margin, namely the length of the current weight vector divided by the total number of updates up to that time. Requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin (PDM). We demonstrate that PDM converges in a finite number of steps and derive an upper bound on them. We also compare experimentally PDM with other perceptron-like algorithms and support vector machines on hard margin tasks involving linear kernels which are equivalent to 2-norm soft margin.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}