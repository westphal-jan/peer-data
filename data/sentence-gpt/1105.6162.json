{"id": "1105.6162", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2011", "title": "A statistical learning algorithm for word segmentation", "abstract": "In natural speech, the speaker does not pause between words, yet a human listener somehow perceives this continuous stream of phonemes as a series of distinct words. The detection of boundaries between spoken words is an instance of a general capability of the human neocortex to remember and to recognize recurring sequences. This paper describes a computer algorithm that is designed to solve the problem of locating word boundaries in blocks of English text from which the spaces have been removed. This problem avoids the complexities of processing speech but requires similar capabilities for detecting recurring sequences. The algorithm that is described in this paper relies entirely on statistical relationships between letters in the input stream to infer the locations of word boundaries. The source code for a C++ version of this algorithm is presented in an appendix. The paper is available online at http://www.acrossm.edu/research/nc/cjs.aspx.\n\n\nThe paper was supported by the National Institute of Education (NIE) and The Center for Science and Technology (CIIT) in the USA (with grant N1 R8E01).", "histories": [["v1", "Tue, 31 May 2011 05:03:06 GMT  (732kb)", "http://arxiv.org/abs/1105.6162v1", "28 pages, 5 figures"], ["v2", "Sun, 26 Jun 2011 22:52:32 GMT  (745kb)", "http://arxiv.org/abs/1105.6162v2", "30 pages, 5 figures"]], "COMMENTS": "28 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jerry r van aken"], "accepted": false, "id": "1105.6162"}, "pdf": {"name": "1105.6162.pdf", "metadata": {"source": "CRF", "title": "A Statistical Learning Algorithm for Word Segmentation", "authors": ["Jerry R. Van Aken"], "emails": [], "sections": [{"heading": null, "text": "KEYWORDS: Word boundary detection, sequence memory, text segmentation, temporal pattern recognition, word splitting, Viterbi algorithm.\nIn natural speech, spoken words blend together to form a continuous stream of sounds, yet humans perceive speech as a sequence of distinct words. How the human neocortex accomplishes this feat is an open question.\nEven experienced listeners apparently rely, to some extent, on prosodic cues in speech\u2014 timing, pauses, stress, and changes in intonation\u2014to separate words and phrases. Is prosody the primary mechanism by which humans learn to identify individual words? Perhaps prosody merely augments a more fundamental learning mechanism that relies on the statistical properties of continuous speech to identify individual words. (See Cutler [3], Kuhl [8], and Saffran, Aslin & Newport [9].) If so, it should be possible to design a computer algorithm to do something similar.\nSpeech processing introduces complexities that are beyond the scope of this paper. Instead, this paper presents a statistical inference algorithm that addresses a simplified version of this problem: how to detect the word boundaries in a block of English text from which all spaces (and punctuation) are removed. This problem is described as follows."}, {"heading": "Problem Statement", "text": "Take a block of text, such as the following:\nThe quick brown fox jumped over the lazy sleeping dog.\nThis example contains ten words, nine of which are unique.\n Send correspondence to: Jerry Van Aken, One Microsoft Way, Redmond, WA 98052.\nEliminate all spaces and punctuation from this text, and convert all letters to lower case, as follows:\nthequickbrownfoxjumpedoverthelazysleepingdog\nDesign a word-segmentation algorithm that recognizes the individual words in the preceding character stream, and that marks the boundaries between words. The algorithm should produce the following output stream:\nthe_quick_brown_fox_jumped_over_the_lazy_sleeping_dog\nA shortcut to achieving such a result is to incorporate a ready-made dictionary that contains some or all of the words that the algorithm will encounter. However, no such shortcut is used here. The algorithm never observes any word in isolation, and it receives no a priori information about which combinations of letters are valid words. Instead, the algorithm must discover the words by observing a character stream that is composed of words but that contains no explicit word-boundary information.\nA small portion of a character stream that contains words selected at random from the previous example might look like the following:\n...pingjumpedthefoxdogfoxbrownthethedogquickjumpeddoglazyqui...\nThis character stream might be generated by a program that randomly selects words from a hidden dictionary and appends them to the stream. The following C++ program generates such a stream:\n// // Stochastic stream generator // #include <stdlib.h> #include <stdio.h>\nchar *test[] = {\n\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"sleeping\", \"dog\"\n}; const int NUM_TEST_WORDS = sizeof(test)/sizeof(test[0]); const int NUM_LEARNING_WORDS = 5000;\nvoid main() {\nfor (int ix = 0; ix < NUM_LEARNING_WORDS; ++ix) {\nint jx = rand() % NUM_TEST_WORDS; printf(\"%s\", test[jx]);\n}\n}\nThis stochastic stream generator is written in C++ but uses only language features that should be readily understood by a C programmer.\nThe problem is to design a word-segmentation algorithm that takes, as its input, the character stream that is produced by the stochastic stream generator. This algorithm uses the statistical relationships between the characters in the stream to infer the locations of the word boundaries in the input stream. The algorithm produces, as its output, a modified version of the input stream into which markers are inserted at word boundaries.\nThe word-segmentation algorithm described in this paper operates in on-line mode. The characters from the input stream appear in the output stream with only a small delay. In contrast, a batch-mode algorithm might not send any characters to the output stream until the algorithm has made one or more passes through the entire input stream.\nIn the previous C++ program, the state of the stochastic stream generator during each forloop iteration is a combination of a randomly selected word (selected by index jx) and the current offset within this word (indicated by index ix). The internal state (ix, jx) is hidden. The word-segmentation algorithm must learn to infer the transitions between hidden states by observing only the characters that are generated by these states.\nThe problem that is addressed by this algorithm is a simplified version of the word boundary problem in continuous speech. The first simplification is to replace the practical problem of finding word boundaries in speech with the somewhat artificial problem of finding word boundaries in English text from which spaces and punctuation are removed. The second simplification is that the words in the input stream to the algorithm occur in random sequence and not according to the rules of grammar. Finally, the probabilities that particular words appear in the character stream are static over the length of the stream. In a real environment, these probabilities might change dynamically over time. An open issue is the extent to which these simplifications might limit the usefulness of this algorithm in real-world applications."}, {"heading": "Relationship to Prior Work", "text": "Brent [2] provides an overview of word-segmentation algorithms that are primarily published in the speech-processing literature. Gambell & Yang [6] provide a performance comparison many of these algorithms. Goldsmith [7] provides a recent overview of wordsegmentation algorithms that are primarily from the text-processing literature.\nWord-segmentation algorithms for speech and text frequently rely on the same statistical regularities to detect words in character streams. However, in speech processing, the focus is primarily on on-line algorithms, whereas the text-processing literature tends to focus more on batch algorithms. The following is a brief description of several representative on-line algorithms from the speech-processing literature.\nElman [4] trained a simple recurrent network (SRN) to predict the next symbol in an input stream based on the symbols that immediately precede this symbol. The input\nstream consisted of contiguous words selected from a hidden dictionary. The boundaries between words in the input stream were unmarked. The SRN was most successful in predicting the symbols toward the ends of words. The greatest inaccuracy occurred in predicting the first symbol in a new word after reaching the end of the previous word. Thus, increases in the error rate were highly correlated with words, and these increases could be used as hints to locate the word boundaries in the stream.\nSaffran, Aslin & Newport [9] studied language learning in infants, and showed that statistical relationships between phonemes in speech are an important source of information about word boundaries. This research suggests that infants can use transitional probabilities between adjacent phonemes to aid the detection of word boundaries. Saffran, et al, defined the transitional probability that phoneme y will immediately follow phoneme x to be the frequency of the sequence xy divided by the frequency of x. The transitional probability between two adjacent phonemes is typically high if the phonemes are part of the same word, and tends to be low if the two phonemes are separated by a word boundary.\nBrent [2] proposed that the accuracy of word boundary predictions can be improved by using mutual information between phonemes instead of transitional probabilities. The mutual information between two phonemes x and y is calculated as log2(Pxy / (Px . Py)), where Px is the frequency of x, Py is the frequency of y, and is the frequency of xy.\nIntuitively, there seems to be a good deal more statistical information in a contiguous stream of words than is being exploited by the approaches described in the preceding paragraphs. For example, if one or more familiar words can be successfully identified in a part of the stream, this information might help to reduce uncertainty in identifying other words that share some of the same word boundaries. Additionally, some of the described techniques look at only two adjacent phonemes at a time (in principle, an SRN can look more deeply than this into the stream history). The accuracy of the transitional probability of the next symbol in an input stream can frequently be improved by evaluating this probability in the context of a greater number of the symbols that immediately precede it.\nFinally, if there are several competing hypotheses about how to assign word boundaries to a part of an input stream, all currently active hypotheses should be updated and evaluated in parallel as each new symbol arrives in the input stream. The Viterbi algorithm [5][10] provides a useful model of the kind of processing that the wordsegmentation algorithm requires to simultaneously evaluate multiple competing paths. In the Viterbi algorithm, each path represents a hypothetical sequence of hidden states to account for the input stream observed by the algorithm. The algorithm must limit the number of possible paths that are simultaneously active, and the depth of the history for these paths must be truncated at frequent intervals. Otherwise, the storage and processing requirements will quickly become unmanageable for an input stream of any significant length.\nThe word-segmentation algorithm described in this paper differs from previous algorithms in several ways. The algorithm considers all plausible segmentations of a\nblock of letters that is wide enough to contain several medium-size words. Each such segmentation is a path in a Viterbi trellis. Only recurring sequences that occur above a threshold frequency can be a part of a segmentation path. A controlled-growth policy limits the number of sequences that the algorithm must store in memory. The formulas used for transition probabilities more directly predict the locations of word boundaries. In contrast, previous algorithms directly calculate next-symbol probabilities based on one (bigram) or more preceding symbols, and then indirectly infer word boundaries where these probabilities fall beneath some threshold value."}, {"heading": "Sequence Memory", "text": "To detect recurring sequences, the word-segmentation algorithm must store sequences that have previously been observed in the input stream and compare these stored sequences to new sequences that arrive in the stream. The algorithm stores selected sequences in a sequence memory, and counts the number of times that each stored sequence is observed.\nTo be practical, the algorithm cannot store every possible sequence that occurs in the input stream. Most are junk sequences, which are randomly occurring combinations of parts of adjacent words. A particular junk sequence should repeat only rarely. In contrast, valid sequences are actual words or subwords, and these sequences occur relatively frequently. (A subword is a part of a word that is not combined with parts of any other words.)\nOn this basis, the algorithm assumes that a sequence is likely to be valid if this sequence is observed to occur above some threshold frequency, fT. The frequency of a sequence X in the input stream is calculated as the number of instances of X divided by the total number of letters in the stream. In the current version of the algorithm, fT is a tuned (set by the programmer) parameter.\nFor example, the test data for the stochastic stream generator in an earlier section consists of 10 words (including two instances of the) and 44 letters. The expected frequency of the word quick in the generated stream should be about one in 44. However, the junk sequence ickbro occurs only when the word brown immediately follows the word quick, and this junk sequence should have a frequency of about one in 440. A reasonable value for fT is one half divided by the number of letters in the test data, which, for this example, is one in 88.\nThe algorithm uses a controlled-growth policy to prevent the sequence memory from filling up with junk sequences. If a sequence X of length n is determined, based on its frequency, to be a valid sequence, the algorithm permits X to serve as the base sequence for successor sequences of length n+1. Therafter, if an instance of X in the input stream is immediately followed by letter x, the new sequence, X' = X + x (where the plus sign indicates concatenation), is added to the sequence memory but is tentatively classified as\na junk sequence. However, roughly one in ten new sequences are, after a period of monitoring, reclassified as valid sequences, after which they can serve as base sequences for yet longer sequences.\nAlthough most of the sequences in sequence memory are, in fact, junk sequences, every such junk sequence consists of a valid sequence to which one letter is appended. This restriction prevents the algorithm from having to store longer junk sequences.\nInitially, before the algorithm receives the first letter in the input stream, the sequence memory contains 26 one-letter sequences for the letters a to z. The algorithm always treats a one-letter sequence as valid, regardless of its frequency.\nIn the following diagram, each node in the tree represents a sequence. The characters in the sequence are contained in this node and in the predecessor nodes. In this example, the sequence them is formed by appending one tree node (for the character m) to the three tree nodes that form the sequence the. The sequence their is formed by appending two nodes to the sequence the. The leftmost tree node in the diagram is the root of the tree that contains all stored sequences that begin with the character t.\nIn this diagram, each tree node is a SEQUENCE structure that contains three members. The event member contains the letter (character code) at the end of the sequence that is represented by this node. The inCount member is a counter that records the number of times this sequence has been observed in the input stream. This member is used to determine the frequency of the sequence. The prevSeq member of this structure is a pointer to the node that represents the predecessor sequence. (A SEQUENCE structure has additional members that are described in Appendix A.)\nA sequence X in the tree has a single predecessor sequence, which is pointed to by X.prevSeq, but X might have numerous successors. The word-segmentation algorithm does a hash-table lookup to find a successor. If x is the character that immediately follows an instance of X in the input stream, the algorithm finds the successor X' = X + x by calling a NextSequence function, which takes X and x as arguments and returns X'. For\nexample, if the two arguments are the sequence the and the character m, this function returns the sequence them.\nIn the preceding diagram, the leftmost node in the tree represents the one-character sequence t. This node is the root of the tree that contains all sequences that begin with the character t."}, {"heading": "Word Boundary Detection", "text": "To what extent can a learning algorithm that relies on statistical inference alone successfully detect word boundaries in a long sequence of letters that contains no explicit word boundary markers?\nTypically, the predictability of the next letter xn in a sequence X = x1\u2026xn-1 can be improved by taking into account one or more of the letters that precede xn in the sequence. Moreover, a letter at or near the end of a word is easier to predict than a letter at or near the beginning of a word.\nIn the following diagram, xk is the last letter in a word of length N. As a general rule, xk is most strongly predicted by the transition probability from the N-1 letters that immediately precede it. In contrast, the probability of the transition to xk from the N or N+1 letters that precede xk tends to be much weaker because these letters cross a word boundary. Similarly, the probability of the transition to xk+1 from the letters that precede xk+1 is much weaker because they are separated from xk+1 by a word boundary.\nAlthough the transition probabilities within words can be used with some success to identify individual words in isolation, there are frequent exceptions to the general rule just described. As a result, a misidentified word boundary might cause a word to be split into two pieces, or parts of two adjacent words might be misidentified as a word.\nTo avoid such misidentifications, a more reliable strategy is to concurrently identify the word boundaries in a block of several adjacent words. In this way, the probable starting and ending boundaries of a word can be reinforced by the probable boundaries of the adjacent words on either side of this word. This block should start and end on word boundaries so that the first and last words in the block can be reliably identified. The block should be wide enough to contain several words of medium length.\nTransition probabilities within words are less useful for identifying very short words. However, if a very short word is sandwiched between a pair of relatively long words, and the probable boundaries of these longer words can be determined from their internal transition probabilities, the extent of the short word can then be inferred from these boundaries. This technique is especially helpful for identifying a one-letter word, such as the English word I or a, that contains no internal transitions.\nTo construct such a block of words, the word-segmentation algorithm uses a variablewidth storage structure called an event window. As indicated in the following diagram, the algorithm is event-driven. The arrival of a new letter in the input stream is an event that triggers a new cycle of processing. This cycle may or may not result in a word being detached from the window and sent to the output stream. For each such event, the algorithm appends the new letter to the right side of the event window. Thus, the oldest letters lie toward the left side of the window.\nThe event window is populated solely with references to valid sequences (words and subwords) that are stored in the sequence memory. A sequence cannot appear in the window until the algorithm has classified the sequence as valid based on the frequency of the sequence.To identify the word boundaries within a block, the algorithm matches sets of adjacent letters in the block to valid sequences. The algorithm constructs one or more paths through the block, where each path is a set of contiguous sequences that cover the letters in the block, without gaps or overlaps, from one side of the block to the other. Typically, the algorithm can construct a number of such paths through a block from the sequences stored in sequence memory. The sequences in each path represent a hypothetical set of words, and the seams between adjacent sequences in the path are the hypothetical word boundaries.\nAs shown in the following diagram, the event window is a two-dimensional structure. Event time increases from left to right (though not in equal time intervals), and sequence length increases from bottom to top. Each solid black dot represents a valid sequence in a hypothetical path. Each column in this diagram contains some number of sequences (hypothetical words) of different lengths. For example, the event window contains three sequences that end with the letter r. These are r, er, and her.\nEach solid arrow represents a transition from a valid predecessor sequence to a valid successor sequence. The algorithm assigns a probability to each such transition. A path is a connected set of transitions that spans the width of the window.\nA dashed arrow represents a transition to a junk sequence that is stored in sequence memory but that fails the frequency threshold test. The algorithm assigns a probability of zero to such a transition.\nIn the preceding diagram, the event window contains many hypothetical paths. All paths start at the one-letter sequence, h, at the left side of the window. Each path defines a hypothetical set of words. A downward transition in a path represents a hypothetical word boundary. The algorithm assigns a score to each path. The score is the product of the transition probabilities in the path. In this example, the highest-scoring path (highlighted) contains the words here and have.\nThe word-segmentation algorithm uses the Viterbi algorithm to calculate the path scores in the event window. As shown in the preceding diagram, the requirements of the wordsegmentation algorithm transform of the Viterbi graph into an irregular sawtooth shape instead of the familiar trellis shape.\nWhen a new letter is added to the right edge of the event window, and the new column of the window is populated with valid sequences, the scores for the sequences in the new column are calculated as the product of the scores for the previous column and the transition probabilities to the new column. The formulas for estimating the transition probabilities are derived in a later section.\nIn an event window, a sequence X of length n can have only two possible successor sequences. A successor successor sequence X' of length n+1 supports the hypothesis that the letter x that immediately follows an instance of X in the input stream is a part of the same word as X. This is a same-word transition. A successor sequence X' of length one supports the hypothesis that the letter x that immediately follows X in the input stream is the start of a new word. This is a new-word transition.\nA sequence X in an event window always has a new-word successor, but may or may not have a same-word successor. The algorithm always adds a new-word successor because all sequences of length one are valid by definition. However, the algorithm adds a sameword successor only if this successor is found in sequence memory and is a valid sequence.\nIn the preceding diagram, each sequence of length n > 1 always has just one predecessor, which has a length of n-1. In contrast, a sequence of length one can have several predecessors, but only the highest-scoring path survives the transition to the start of the new word (by Bellman\u2019s principle of optimality), and the algorithm remembers only the predecessor that belongs to this path. Thus, the algorithm needs to assign just one score to each sequence in the event window.\nAfter the event window accumulates a sufficient number of letters, the algorithm uses a word score (described in a later section) to identify the probable word boundary that is nearest the right side of the event window. A block of contiguous words is defined by this word boundary and by the word boundary at the left edge of the window. Next, the algorithm identifies the probable word boundaries inside this block, detaches the oldest (leftmost) word in the block, and sends this word to the output stream. Thus, the width of the event window shrinks each time a word is detached and sent to the output stream, and grows each time a new letter from the input stream is added to the window. Because the algorithm always detaches a (probable) word from the left side of the window, the left edge of the window is always aligned to a (probable) word boundary.\nThe word boundary at the right edge of the block cannot be identified as reliably as the boundary at the left edge. The boundary on the left edge was previously defined in the context of the words on either side of this boundary, whereas the context for the boundary on the right edge is still incomplete.\nThe newer words in the block provide sufficient context to determine the likely word boundaries for the older words in the block. However, after the algorithm detaches one or more of the older words from the block, the remaining words typically have little or no context and cannot be identified with the same high certainty. The algorithm must wait until more letters arrive in the input stream (or the stream ends) to detach the next word.\nThe sequence memory always contains the 26 one-letter sequences a,b,\u2026,z. If the algorithm encounters an input sequence that is completely novel, this sequence might, in the extreme case, be sent to the output stream as a series of individual letters, with a word-boundary marker (space ) inserted between each pair of letters.\nAn event window might contain two or more instances of the same sequence. If so, all instances refer to the same stored sequence in sequence memory."}, {"heading": "Transition Probabilities", "text": "To calculate the probability of a path through an event window, the algorithm must first calculate the probabilities of the individual sequence-to-sequence transitions in the path. The probability of the full path, from start to end, is the product of the individual transition probabilities in the path.\nAs discussed previously, if an instance of sequence X in the input stream is immediately followed by the letter x, the two possible transitions from this sequence are the sameword transition to sequence X' = X + x (where the plus sign indicates concatenation) and the new-word transition to the one-word sequence X' = 0 + x, where X' is the next sequence in the path after X, and 0 represents the null sequence. Thus, only the following two transition probabilities need to be calculated:\n TPsame(X, x), which is the probability of the same-word transition from X given that x is the value of the letter that immediately follows this instance of X.\n TPnew(X, x), which is the probability of the new-word transition from X given x.\nFirst, TPsame can be expressed as follows:\nTPsame(X, x) = P(X' = X + x | X)\n= P(X' = X + x | X' = X + \u25a1) . P(X' = X + \u25a1 | X)\nwhere\n P(X' = X + \u25a1 | X) is the prior probability that whatever letter immediately follows an instance of X in the input stream (before this letter value is known) is a part of the\nsame word as X, where the symbol \u25a1 is a placeholder for whatever letter follows X.\n P(X' = X + x | X' = X + \u25a1) is the conditional probability that x is the value of the letter that immediately follows an instance of X in the input stream, given that whatever\nletter follows an instance of X is a part of the same word as X.\nSecond, TPnew(X, x) can be expressed as follows:\nTPnew(X, x) = P(X' = 0 + x | X)\n= P(X' = 0 + x | X' = 0 + \u25a1) . P(X' = 0 + \u25a1 | X)\nwhere\n P(X' = 0 + \u25a1 | X) is the prior probability that whatever letter immediately follows an instance of X in the input stream is the first letter in a new word, where X' = 0 + \u25a1 is\nthe one-letter sequence that contains this first letter. However, we can immediately relate P(X' = 0 + \u25a1 | X) to P(X' = X + \u25a1 | X) as follows:\nP(X' = 0 + \u25a1 | X) = 1.0 \u2013 P(X' = X + \u25a1 | X)\n P(X' = 0 + x | X' = 0 + \u25a1) is the conditional probability that x is the value of the first letter in a new word, given that whatever letter follows X is the start of a new word.\nThus, to calculate the transition probabilities TPsame and TPnew, the algorithm must first calculate the prior probability P(X' = X + \u25a1 | X), and the conditional probabilities P(X' = X + x | X' = X + \u25a1) and P(X' = 0 + x | X' = 0 + \u25a1). The word-segmentation algorithm uses approximations for these three probabilities. The accuracy of the algorithm largely depends on the accuracy of these approximations.\nTo approximate the prior probability P(X' = X + \u25a1 | X), the algorithm must gather some additional statistics for each sequence. For this purpose, two new counters are added to the previously described SEQUENCE structure. In addition to the inCount and createCount members, which were previously discussed, this structure is augmented with two new members, outCount and succCount.\nFor a stored sequence X, the outCount member counts the number of valid instances of X. Each time the algorithm observes an instance of X in the input stream, and the calculated frequency of X at the time of this observation exceeds the threshold frequency fT, X.outCount is incremented by one. If the length of X is greater than one, the succCount member of the predecessor of X is incremented by one at the same time X.outCount is incremented.\nThe succCount member of sequence X counts the number of times that a same-word successor to X is a valid sequence. As discussed previously, a valid sequence in sequence memory can have several successors (for example, the word the can have successors them, they, and so on). If the k successors to X are = X + xi, i = 1,\u2026,k, then X.succCount is equal to the following sum:\nIf x is the letter value that immediately follows an instance of sequence X, and the algorithm determines that the successor X' = X + x is a valid sequence, both X.succCount and X'.outCount are incremented by one, and this occurrence indicates that X and x are probably parts of the same word. If X.outCount is incremented, but X.succCount and X'.outCount are not incremented, this occurrence probably indicates that the letter x that follows X is the start of a new word, which implies that a word boundary separates X from X'.\nConceptually, incrementing X.outCount is analogous to the firing of a neuron that represents sequence X. Each time X.inCount is incremented by an input event, the algorithm must determine, based on the frequency of X, whether X will fire. If X fires, the algorithm must then determine whether the successor sequence X' will fire, and so on.\nAfter X fires, a successor X' = X + x will fire only if the following conditions are satisfied:\n The letter that immediately follows this instance of X in the input stream is x. (This condition always causes X'.inCount to increment regardless of whether X'.outCount\nincrements.)\n The frequency of X' exceeds threshold frequency fT.\nHowever, the one-letter sequence X' = 0 + x, which has no predecessor, fires unconditionally each time an instance of x is observed in the input stream.\nIdeally, X'.outCount counts the number of times that a particular successor X' = X + x is part of the same word as X, and X.succCount counts the number of times any successor to X is part of the same word as X. However, both of these counters are prone to overcounting. The reason is that the algorithm determines whether a successor sequence is valid solely based on its frequency. Thus, if X' = X + x is recognized as a valid sequence but, for the current instance of X and x, the letter x is, in fact, the start of a new word, X'.outCount and X.succCount are still incremented.\nFor example, if the input stream contains instances of the words the, them, and one or more words that begin with the letter m, then, immediately after m is observed to follow an instance of the, there is uncertainty about whether this m is part of the same word as the, or is the start of a new word. In either case, the algorithm increments x.succCount for X = the and X'.outCount for X' = them because the algorithm classifies X' as a valid sequence based solely on its frequency.\nIn the following discussion, \u03b4 represents the overcount amount in X.succCount, and \u03b5 represents the overcount amount in X'.outCount. The following is the prior probability that X and whatever letter follows X are part of the same word:\nThe following is the conditional probability that x is the value of the letter that immediately follows an instance of sequence X, given that whatever letter follows an instance of X is part of the same word as X:\nFinally, if the input stream, thus far, contains a total of N letters, and nx of these letters are letter x, the following approximation is used for the probability that a word starts with x:\nThis approximation is based on the assumption that the probability that a letter occurs at the start of a word is the same as the probability that a letter occurs anywhere in the input stream. The validity of this assumption varies according to the characteristics of the hidden dictionary that is used to generate the input stream to the algorithm.\nEstimates of the overcount amounts \u03b4 and \u03b5 are required to calculate the probabilities P(X' = X + \u25a1 | X) and P(X' = X + x | X' = X + \u25a1). The following difference is the number of times that whatever letter follows x is the start of a new word:\ndiff = X.outCount \u2013 (X.succCount - \u03b4)\nThe ratio \u03b5 / diff must be very close to the ratio nx / N, where, as before, nx is the number of instances of letter x in the input stream and N is the total number of letters in the stream. If we assume that \u03b4 << diff, the value of \u03b5 can be estimated as follows:\nThe value of \u03b4 is more difficult to estimate, although it must be at least as large as \u03b5. However, the ratio of \u03b4 to X.succCount tends to be much smaller than the ratio of \u03b5 to X'.outCount, so \u03b4 has significantly less impact than \u03b5 on the transition probabilities. Based on this rationale, the algorithm sets \u03b4 = 0.\nBy substituting these values for \u03b4 and \u03b5 into the previous formulas for P(X' = X + \u25a1 | X) and P(X' = X + x | X' = X + \u25a1), the following approximate formulas are derived for the transition probabilities:\nThese formulas are used in the program listing for the word-segmentation algorithm in Appendix B. Occasionally, the value of Psame is slightly less than zero because of the subtraction of Pnew, but this error seems to have little or no effect on performance, and the algorithm does not bother to clamp Psame to zero."}, {"heading": "Two-Level Scoring System", "text": "The transition probabilities that were derived in the previous section are used to calculate the first-level path scores in what is a two-level scoring system. As explained previously, the (first-level) score for a path is calculated as the product of the transition probabilities in the path. These first-level scores, by themselves, are somewhat context-sensitive, which can cause occasional errors.\nAlthough the first-level scores are usually reliable, they are sometimes degraded by word combinations that create ambiguities, and these ambiguities can cause the algorithm to incorrectly identify word boundaries. In particular, the letters at the end of a word in a block might occasionally combine with the letters at the start of the next word in the block to match a stored sequence, and thereby fool the algorithm into misidentifying the word boundaries in the block.\nA second scoring level is introduced to produce more consistent and reliable results. The second-level scores are essentially a refined version of the first-level scores. To support the two-level scoring system, the first-level scores in each column in the event window must be normalized.\nThe following diagram indicates how the first-level scores are calculated when a new letter arrives in the input stream and a column is added to the right side of the event window.\nPart (a) of this diagram shows the same-word transitions from the previous column in the event window to the new column. Dashed arrows represent transitions to junk sequences, and these transitions are assigned a probability of zero (and discarded). Solid arrows represent transitions to valid sequences, and the probability of each such transition is calculated by using the formula for TPsame that was derived in the previous section. The raw score for a transition from sequence X in the previous column to a sequence X' in the new column is the product of the transition probability and the score that was previously calculated for the path that ends at X. Before the raw scores can be normalized, the raw score for the new-word transition must be obtained.\nPart (b) of the preceding diagram shows the new-word transitions. Several transitions are shown from sequences in the previous column to the one-letter sequence X' in the new column, but only the highest-scoring path survives this transition. In this case, dashed\narrows represent transitions in the discarded paths, and the solid arrow represents the transition from X to X' in the survivor. The probability of the transition from X to X' is calculated by using the formula for TPnew that was derived in the previous section. The raw score for the path that includes this transition is the product of the transition probability and the score that was previously calculated for the path that ends at X.\nAfter the raw scores are calculated for all of the sequences in the new column, these scores are normalized so that the sum of the scores in the column is one. After normalization, these first-level scores are ready to propagate to the second level of the two-level scoring system.\nTo provide the basis for the second-level scores, each SEQUENCE structure in sequence memory has an accumScores member that indicates the likelihood\u2014based on all observed instances of the sequence\u2014that a sequence is a word. In part (b) of the previous diagram, the first-level score calculated for the one-letter sequence X' is the relative probability that a word boundary occurs between X and X', and that X is, therefore, a word. To record this probability, the algorithm adds the first-level score at X' to the accumScores member of stored sequence X. (X is the only sequence in the previous column to be so incremented.)\nAfter NX instances of a sequence X have been observed in the input stream, the average relative probability that X is a word is simply X.accumScores / NX. This word probability is a value between zero and one. To calculate the second-level score for a path, the algorithm first calculates the partial score for each hypothetical word X in the path as the product of the word probability of X and the length of X. The second-level score for the path is then the product of these partial scores."}, {"heading": "Discussion", "text": "The program listing in Appendix B contains a complete implementation of the wordsegmentation algorithm and a test program. The test data for this program is Lincoln's Gettysburg Address. This speech is 271 words in length and contains a total of 1,149 characters.\nThe output stream generated by this program contains several errors. First, the algorithm fails to insert word boundary markers (spaces) between two instances of the words in_a and one instance of on_a. These errors are symptoms of an inherent weakness in the algorithm. To identify a very short word like in, on or a, the algorithm relies on the longer words that surround the short word to help to establish the word boundaries around the short word. However, when two very short words are adjacent to each other, this strategy sometimes fails.\nThere are additional errors, and these errors vary with the seed value used for the random number generator (and, of course, with the algorithm used by the random number\ngenerator). For the seed value in the program listing (and for the srand and rand functions in the Microsoft C/C++ runtime library), there is just one additional error: the word highly is incorrectly segmented as high_ly. The problem here is that this word is the only one in the speech that begins with . In addition, the speech contains only one instance of the word highly. Because of the algorithm's policy of controlled growth, the representation of this word in the sequence memory grows very slowly and is still incomplete after the algorithm has processed an input stream of 175,000 words. This error can be eliminated by increasing the value of NUM_LEARNING_WORDS from 175,000 to 300,000.\nIn Appendix B, the SetFirstLevelScores function calculates the first-level scores for a new column, and the SetSecondLevelScores function calculates the second-level scores for an entire event window. As previously explained, the algorithm selects word boundaries based on the second-level scores. The first-level scores are used as intermediate values in the calculation of the second-level scores. However, the algorithm is easily modified to use just the first-level scores to select the word boundaries\u2014simply comment out the call to SetSecondLevelScores. With this change, the accuracy of the algorithm declines noticeably.\nThe PopulateNewColumn function populates a new column in the event window with pointers to valid sequences. In addition, this function updates the statistical counts for the sequences in sequence memory, and adds new sequences to this memory. PopulateNewColumn is responsible for nearly the entire construction and maintenance of the sequence memory. The only exception is that the accumScores values in sequence memory are updated by the SetFirstLevelScores function so that these values are available for use by the SetSecondLevelScores function. Thus, virtually all of the learning done by the algorithm occurs in PopulateNewColumn.\nFor the Gettysburg Address example, the word-segmentation algorithm stores 14,489 sequences in sequence memory. Of these, only 1,527 are classified as valid sequences. Some applications might require the algorithm\u2019s storage requirements to be reduced. A strategy for reclaiming sequence memory is to delete a stored junk sequence after it has been monitored for long enough to confirm that it cannot be a valid sequence. The storage for this sequence can then be recycled. Another storage-reduction strategy is to use a smaller version of the SEQUENCE structure for newly created sequences. A stored sequence that is later confirmed to be valid can be upgraded to use a larger SEQUENCE structure that has a full set of counters."}, {"heading": "Acknowledgment", "text": "A careful reading by Dana M. Van Aken of an early manuscript resulted in a number of improvements in the presentation of the concepts in this paper."}, {"heading": "Appendix A: The SEQUENCE Structure", "text": "The SEQUENCE structure represents a sequence of characters. Each sequence that is stored in sequence memory is represented by a SEQUENCE structure. This structure is defined as follows:\ntypedef struct _SEQUENCE {\nstruct _SEQUENCE *_link; struct _SEQUENCE *_prevSeq; struct {\nBYTE _event; BYTE _length; WORD _allocNum;\n} _info; DWORD _createCount; DWORD _outCount; DWORD _inCount; DWORD _succCount; float _accumScores;\n} SEQUENCE;"}, {"heading": "Members", "text": "link\nA pointer to the next SEQUENCE structure in a linked list. A hash table contains all of the sequences in sequence memory. Each bucket in the hash table contains a linked list of sequences.\nprevSeq\nA pointer to the SEQUENCE structure that represents the predecessor sequence in sequence memory.\nevent\nSpecifies the ANSI character code at the end of the sequence that is represented by this SEQUENCE structure. The other characters in this sequence are contained in the predecessor sequence that is pointed to by the prevSeq member.\nlength\nSpecifies the number of characters in the sequence that is represented by this SEQUENCE structure.\nallocNum\nSpecifies the allocation number that is assigned to this structure. Each created instance of the SEQUENCE structure is assigned an allocation number to uniquely identify the instance.\ncreateCount\nSpecifies the number of characters that had been received in the input stream at the time that this SEQUENCE structure instance was created. The frequency at which instances of the sequence represented by this structure are observed in the input stream is measured relative to the createCount value.\ninCount\nSpecifies the number of instances of the sequence represented by this structure that have been observed in the input stream.\noutCount\nSpecifies the number of observed instances of the sequence represented by this structure that have been determined to be valid sequences, based on the frequency at which the sequence appears in the input stream. This structure's outCount value is always less than its inCount value.\nsuccCount\nSpecifies the number of times that the sequence represented by this structure is immediately followed by a character that is part of the same word as this sequence. This structure's succCount value is incremented by one each time an immediate successor's outCount value is incremented by one. Thus, this structure's succCount value equals the sum of the outCount values of all of its immediate successors. This structure's succCount value is always less than its outCount value.\naccumScores\nSpecifies the accumulated first-level scores for the sequence that is represented by this structure. Each time an instance of this sequence is the most likely sequence in an event window column to be a complete word, the first-level score for this sequence is added to accumScores. (The accumScores members of the other sequences in the column remain unchanged.) The average value of these accumulated scores, which is obtained by dividing this structure's accumScores value by its inCount value, indicates the relative probability that this sequence is a word."}, {"heading": "Appendix B: Program Listing", "text": "The following source code listing contains an implementation of the word-segmentation algorithm, and includes a test program and test data. This listing is a complete program that is written in C++ but uses only language features that should be readily understood by a C programmer. To run this program, first copy all of the source code into a file that has a .cpp file name extension. Then compile the program and run it.\n// // Word-segmentation algorithm //\n#include <stdlib.h> #include <string.h> #include <memory.h> #include <stdio.h> #include <assert.h> #include <math.h>\n#define ARRAY_LENGTH(x) (sizeof(x)/sizeof(x[0]))\nconst int MAX_WINDOW_WIDTH = 32;\ntypedef unsigned char UINT8; typedef unsigned short UINT16; typedef unsigned long UINT32;\n// A sequence stored in sequence memory typedef struct _SEQUENCE {\nstruct _SEQUENCE *_link; struct _SEQUENCE *_prevSeq; struct {\nUINT8 _event; UINT8 _length; UINT16 _allocNum;\n} _info; UINT32 _createCount; UINT32 _inCount; UINT32 _outCount; UINT32 _succCount; float _accumScores;\n} SEQUENCE;\n// A cell in a column in an event window typedef struct _CELL {\nSEQUENCE *_seq; float _score;\n} CELL;\n// A column in an event window typedef struct _COLUMN {\nstruct _COLUMN *_prevCol; struct _COLUMN *_nextCol; UINT8 _event; UINT8 _bestLength; float _bestScore; int _numCells; CELL _cell[MAX_WINDOW_WIDTH];\n} COLUMN;\n// Global variables used by word-segmentation algorithm const float THRESHOLD_BIAS = 4.567; SEQUENCE _rootSequence['z' - 'a' + 1]; SEQUENCE *_hashTable[12577]; COLUMN _column[MAX_WINDOW_WIDTH]; COLUMN *_headColumn = NULL;\nUINT16 _allocCount = 0; int _eventCount = 0; int _fireCount = 0; int _minColumns = MAX_WINDOW_WIDTH / 2; int _numColumns = 0; int _enableWrite = 0;\n// // Test data and tuned parameters // #if 0\nconst int NUM_LEARNING_WORDS = 5000; const float THRESHOLD_PROB = 1.0 / 100.0; char *_testData[] = {\n\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"sleeping\", \"dog\"\n};\n#else\nconst int NUM_LEARNING_WORDS = 175000; const float THRESHOLD_PROB = 1.0 / 1500.0; char *_testData[] = {\n\"four\", \"score\", \"and\", \"seven\", \"years\", \"ago\", \"our\", \"fathers\", \"brought\", \"forth\", \"on\", \"this\", \"continent\", \"a\", \"new\", \"nation\", \"conceived\", \"in\", \"liberty\", \"and\", \"dedicated\", \"to\", \"the\", \"proposition\", \"that\", \"all\", \"men\", \"are\", \"created\", \"equal\", \"now\", \"we\", \"are\", \"engaged\", \"in\", \"a\", \"great\", \"civil\", \"war\", \"testing\", \"whether\", \"that\", \"nation\", \"or\", \"any\", \"nation\", \"so\", \"conceived\", \"and\", \"so\", \"dedicated\", \"can\", \"long\", \"endure\", \"we\", \"are\", \"met\", \"on\", \"a\", \"great\", \"battlefield\", \"of\", \"that\", \"war\", \"we\", \"have\", \"come\", \"to\", \"dedicate\", \"a\", \"portion\", \"of\", \"that\", \"field\", \"as\", \"a\", \"final\", \"resting\", \"place\", \"for\", \"those\", \"who\", \"here\", \"gave\", \"their\", \"lives\", \"that\", \"that\", \"nation\", \"might\", \"live\", \"it\", \"is\", \"altogether\", \"fitting\", \"and\", \"proper\", \"that\", \"we\", \"should\", \"do\", \"this\", \"but\", \"in\", \"a\", \"larger\", \"sense\", \"we\", \"can\", \"not\", \"dedicate\", \"we\", \"can\", \"not\", \"consecrate\", \"we\", \"can\", \"not\", \"hallow\", \"this\", \"ground\", \"the\", \"brave\", \"men\", \"living\", \"and\", \"dead\", \"who\", \"struggled\", \"here\", \"have\", \"consecrated\", \"it\", \"far\", \"above\", \"our\", \"poor\", \"power\", \"to\", \"add\", \"or\", \"detract\", \"the\", \"world\", \"will\", \"little\", \"note\", \"nor\", \"long\", \"remember\", \"what\", \"we\", \"say\", \"here\", \"but\", \"it\", \"can\", \"never\", \"forget\", \"what\", \"they\", \"did\", \"here\", \"it\", \"is\", \"for\", \"us\", \"the\", \"living\", \"rather\", \"to\", \"be\", \"dedicated\", \"here\", \"to\", \"the\", \"unfinished\", \"work\", \"which\", \"they\", \"who\", \"fought\", \"here\", \"have\", \"thus\", \"far\", \"so\", \"nobly\", \"advanced\", \"it\", \"is\", \"rather\", \"for\", \"us\", \"to\", \"be\", \"here\", \"dedicated\", \"to\", \"the\", \"great\", \"task\", \"remaining\", \"before\", \"us\", \"that\", \"from\", \"these\", \"honored\", \"dead\", \"we\", \"take\", \"increased\", \"devotion\", \"to\", \"that\", \"cause\", \"for\", \"which\", \"they\", \"gave\", \"the\", \"last\", \"full\", \"measure\", \"of\", \"devotion\", \"that\", \"we\", \"here\", \"highly\", \"resolve\", \"that\", \"these\", \"dead\", \"shall\", \"not\", \"have\", \"died\", \"in\", \"vain\", \"that\", \"this\", \"nation\", \"under\", \"god\", \"shall\", \"have\", \"a\", \"new\", \"birth\", \"of\", \"freedom\", \"and\", \"that\", \"government\", \"of\", \"the\", \"people\", \"by\", \"the\", \"people\", \"for\", \"the\", \"people\", \"shall\", \"not\", \"perish\", \"from\", \"the\", \"earth\"\n};\n#endif\n// // Initialize data structures used by the word-segmentation algorithm. // void Initialize() {\nint ix;\nmemset(_hashTable, 0, sizeof(_hashTable)); memset(_rootSequence, 0, sizeof(_rootSequence)); for (ix = 0; ix < ARRAY_LENGTH(_rootSequence); ++ix) {\n_rootSequence[ix]._info._event = ix + 'a'; _rootSequence[ix]._info._length = 1; _rootSequence[ix]._info._allocNum = ++_allocCount; _rootSequence[ix]._accumScores = 0.0;\n}\nmemset(_column, 0, sizeof(_column)); for (ix = 1; ix < MAX_WINDOW_WIDTH; ++ix) {\n_column[ix-1]._nextCol = &_column[ix]; _column[ix]._prevCol = &_column[ix-1];\n} _column[MAX_WINDOW_WIDTH - 1]._nextCol = &_column[0]; _column[0]._prevCol = &_column[MAX_WINDOW_WIDTH - 1];\n}\n// // Use the ELF hash algorithm to generate a hash table index. // UINT32 GetHashIndex(SEQUENCE *seq, UINT8 event) {\nconst UINT32 count = sizeof(event) + sizeof(seq->_info); UINT8 *p = (UINT8*)(&seq->_info); UINT32 h = 0, g;\nfor (int ix = 0; ix < count; ++ix) {\nh = (h << 4) + event; if (g = h & 0xF0000000)\nh ^= g >> 24;\nh &= ~g; event = *p++;\n} return (h % ARRAY_LENGTH(_hashTable));\n}\n// // Look up the sequence that is formed by appending a character // to the specified input sequence. // SEQUENCE* NextSequence(SEQUENCE *seq, UINT8 event) {\nSEQUENCE *p, *q = NULL; int index;\nassert('a' <= event && event <= 'z'); if (!seq)\nreturn &_rootSequence[event - 'a'];\nindex = GetHashIndex(seq, event); p = _hashTable[index]; if (p) {\nif (p->_prevSeq == seq && p->_info._event == event)\nreturn p;\nfor (q = p, p = p->_link; p; q = p, p = p->_link)\nif (p->_prevSeq == seq && p->_info._event == event)\nbreak;\nif (p)\nq->_link = p->_link;\n} if (!p) {\np = (SEQUENCE*)malloc(sizeof(SEQUENCE)); assert(p); memset(p, 0, sizeof(SEQUENCE)); p->_prevSeq = seq; p->_info._event = event; p->_info._length = 1 + seq->_info._length; p->_info._allocNum = ++_allocCount; p->_createCount = _eventCount;\n} assert(p); p->_link = _hashTable[index]; _hashTable[index] = p; return p;\n}\n// // Populate a new column in the event window with pointers to valid // sequences that match the latest characters to arrive in the input // stream. Additionally, update the statistics in the sequence memory. // int PopulateNewColumn(UINT8 event, CELL inCell[],\nint numInCells, CELL outCell[])\n{\nint ix, numOutCells; SEQUENCE *inSeq, *outSeq;\n++_eventCount; outSeq = NextSequence(NULL, event); outSeq->_inCount += 1; outSeq->_outCount += 1; outCell[0]._seq = outSeq; outCell[0]._score = 0.0; numOutCells = 1; for (ix = 0; ix < numInCells; ++ix) {\nfloat prob;\ninSeq = inCell[ix]._seq; outSeq = NextSequence(inSeq, event); outSeq->_inCount += 1; prob = (float)(outSeq->_inCount - THRESHOLD_BIAS) /\n(_eventCount - outSeq->_createCount);\nif (prob < THRESHOLD_PROB)\nbreak;\nif (!outSeq->_outCount)\n++_fireCount;\noutSeq->_outCount += 1; inSeq->_succCount += 1; outCell[numOutCells]._seq = outSeq; ++numOutCells; assert(numOutCells < MAX_WINDOW_WIDTH); assert(outSeq->_info._length == numOutCells);\n}\n// Update the statistics on these probable junk sequences, // because a few of them will turn out to be valid sequences. for (++ix ; ix < numInCells; ++ix) {\ninSeq = inCell[ix]._seq; outSeq = NextSequence(inSeq, event); outSeq->_inCount += 1;\n} return numOutCells;\n}\n// // Calculate first-level scores for the valid sequences in the new column. // void SetFirstLevelScores() {\nCOLUMN *col = _headColumn->_prevCol; CELL *inCell = col->_cell; CELL *outCell = _headColumn->_cell; float frac = (float)outCell[0]._seq->_inCount / _eventCount; float sum = 0.0;\nassert(_headColumn->_numCells > 0); for (int ix = 0; ix < col->_numCells; ++ix) {\nSEQUENCE *inSeq = inCell[ix]._seq; float score = inCell[ix]._score; float eowCount = inSeq->_outCount - inSeq->_succCount; float Pdown = frac * eowCount / inSeq->_outCount;\nif (ix + 1 < _headColumn->_numCells) {\nSEQUENCE *outSeq = outCell[ix+1]._seq; float Pup = (float)outSeq->_outCount / inSeq->_outCount - Pdown;\noutCell[ix+1]._score = score * Pup; sum += outCell[ix+1]._score;\n} score *= Pdown; if (!ix || outCell[0]._score < score) {\noutCell[0]._score = score; col->_bestLength = ix + 1;\n}\n}\n// Normalize first-level scores in new column. if (sum != 0.0) {\nsum += outCell[0]._score; for (int jx = 0; jx < _headColumn->_numCells; ++jx)\noutCell[jx]._score /= sum;\n}\nelse\noutCell[0]._score = 1.0;\ninCell[col->_bestLength-1]._seq->_accumScores += outCell[0]._score;\n}\n// // Calculate second-level scores for the paths in the event window. // void SetSecondLevelScores() {\nCOLUMN *col, *prevCol;\nprevCol = _headColumn - _numColumns; if (prevCol < _column)\nprevCol += MAX_WINDOW_WIDTH;\nprevCol->_bestScore = 1.0; col = prevCol->_nextCol; assert(col->_numCells == 1); for (int ix = 1; ix < _numColumns;\n++ix, prevCol = col, col = col->_nextCol)\n{\nfor (int jx = 0; jx < col->_numCells;\n++jx, prevCol = prevCol->_prevCol)\n{\nSEQUENCE *seq = col->_cell[jx]._seq; float Pword = seq->_accumScores / seq->_inCount; float score = prevCol->_bestScore;\nscore *= pow(Pword, jx + 1); if (!jx || col->_bestScore < score) {\ncol->_bestScore = score; col->_bestLength = jx + 1;\n}\n}\n}\n}\n// // Detach one or more words from the left side of the event window // and send these words to the output stream. // void DetachWords() {\nCOLUMN *col; SEQUENCE *stack[MAX_WINDOW_WIDTH]; SEQUENCE **seq = &stack[ARRAY_LENGTH(stack)]; int len, offset = 1;\n// Determine which words to detach from event window. *--seq = NULL; while (offset < _numColumns) {\ncol = _headColumn - offset; if (col < _column)\ncol += MAX_WINDOW_WIDTH;\nlen = col->_bestLength; if (offset > _minColumns / 2)\n*--seq = col->_cell[len-1]._seq;\noffset += len;\n} if (!*seq) {\nif (_numColumns != MAX_WINDOW_WIDTH)\nreturn;\nprintf(\"-\"); // Indicate character is forced out. *--seq = _headColumn->_nextCol->_cell[0]._seq;\n}\n// Send detached words to output stream. while (*seq) {\nUINT8 buffer[MAX_WINDOW_WIDTH]; UINT8 *str = &buffer[ARRAY_LENGTH(buffer)];\n*--str = '\\0'; _numColumns -= (*seq)->_info._length; for (SEQUENCE *p = *seq++; p; p = p->_prevSeq)\n*--str = p->_info._event;\nprintf(\"%s%c\", str, *seq ? ' ' : '\\n');\n} col = _headColumn - _numColumns + 1; if (col < _column)\ncol += MAX_WINDOW_WIDTH;\n// Align left edge of event window to new word boundary. for (len = 1;\nlen <= _numColumns && col->_numCells > len; ++len, col = col->_nextCol)\n{\ncol->_numCells = len; if (col->_bestLength > len)\ncol->_bestLength = len;\n}\n}\n// // A new character just arrived in the input stream. Process this event. // void ProcessEvent(UINT8 event) {\nCOLUMN *col; SEQUENCE *seq;\nif (!_headColumn) {\n_numColumns = 1; _headColumn = &_column[0]; _headColumn->_numCells = PopulateNewColumn(event, NULL,\n0, _headColumn->_cell);\nreturn;\n}\ncol = _headColumn; _headColumn = _headColumn->_nextCol; memset(_headColumn->_cell, 0, sizeof(_headColumn->_cell)); _headColumn->_event = event; _headColumn->_numCells = PopulateNewColumn(event, col->_cell,\ncol->_numCells, _headColumn->_cell);\nassert(_headColumn->_numCells < MAX_WINDOW_WIDTH); SetFirstLevelScores(); if (!_enableWrite)\nreturn;\nif (++_numColumns <= _minColumns)\nreturn;\nif (_minColumns) {\nif (_numColumns != MAX_WINDOW_WIDTH) {\nseq = col->_cell[col->_bestLength - 1]._seq; if (seq->_accumScores / seq->_inCount < 0.50)\nreturn;\n} else\nprintf(\"+\"); // Indicate event window overflow.\n} SetSecondLevelScores(); DetachWords();\n}\n// // Main program: Send stream of test data to word-segmentation algorithm. // int main() {\nint ix, charCount = 0; char *s;\nsrand(123456); Initialize();\n// Learning phase for (ix = 0; ix < NUM_LEARNING_WORDS; ++ix) {\nint index = rand() % ARRAY_LENGTH(_testData);\nfor (s = _testData[index]; *s != '\\0'; ++s)\nProcessEvent(*s);\n}\n_enableWrite = 1; // Enable output stream. _headColumn = NULL; // Reset event window.\n// Output phase for (ix = 0; ix < ARRAY_LENGTH(_testData); ++ix) {\nfor (s = _testData[ix]; *s != '\\0'; ++s) {\nProcessEvent(*s); ++charCount;\n}\n}\n// Flush words in event window to output stream. _minColumns = 0; ProcessEvent('x');\nprintf(\"\\nSEQUENCES STORED = %d\", _allocCount); printf(\"\\nSEQUENCES FIRING = %d\", _fireCount); printf(\"\\nTOTAL EVENT COUNT = %d\", _eventCount);\nprintf(\"\\nSAMPLE LENGTH = %d words, %d chars\\n\",\nARRAY_LENGTH(_testData), charCount);\nreturn 1;\n}\nThe GetHashIndex function in the preceding program uses the ELF hash algorithm, as described by Binstock [1]."}], "references": [{"title": "Hashing Rehashed", "author": ["Binstock", "Andrew"], "venue": "Dr. Dobb's Journal, 4(2), April 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "An Efficient, Probabilistically Sound Algorithm for Segmentation and Word Discovery", "author": ["Brent", "Michael"], "venue": "Machine Learning, 34, 71-106.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 0}, {"title": "Prosody and the Word Boundary Problem", "author": ["Cutler", "Anne"], "venue": "Signal to Syntax: Bootstrapping from Speech to Grammar in Early Acquisition, ed. James Morgan & Kathering Demuth, Lawrence Erlbaum Associates, 1996, 87-99.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Finding Structure in Time", "author": ["Elman", "Jeffrey"], "venue": "Cognitive Science, 14, 1990, 179-211.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "The Viterbi Algorithm", "author": ["Forney", "David"], "venue": "Proceedings of the IEEE, 61(3), March 1973, 268-278.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1973}, {"title": "Mechanisms and Constraints in Word Segmentation", "author": ["Gambell", "Timothy", "Charles Yang"], "venue": "Manuscript, Yale University, 2005, 31 pages.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Segmentation and Morphology", "author": ["Goldsmith", "John"], "venue": "The Handbook of Computational Linguistics and Natural Language Processing, ed. Alexander Clark, et al, Blackwell Publishing, 2010, 364-393.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Early Language Acquisition: Cracking the Speech Code", "author": ["Kuhl", "Patricia"], "venue": "Nature Reviews, 5, November 2004, 831-842.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Statistical Learning by 8-Month- Old Infants", "author": ["Saffran", "Jenny", "Richard Aslin", "Elissa Newport"], "venue": "Science, 274, December 1996, 1926-1928.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm", "author": ["Viterbi", "Andrew"], "venue": "IEEE Transactions on Information Theory, 13(2), April 1967, 260-269.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1967}], "referenceMentions": [{"referenceID": 2, "context": "(See Cutler [3], Kuhl [8], and Saffran, Aslin & Newport [9].", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "(See Cutler [3], Kuhl [8], and Saffran, Aslin & Newport [9].", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "(See Cutler [3], Kuhl [8], and Saffran, Aslin & Newport [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Brent [2] provides an overview of word-segmentation algorithms that are primarily published in the speech-processing literature.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Gambell & Yang [6] provide a performance comparison many of these algorithms.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "Goldsmith [7] provides a recent overview of wordsegmentation algorithms that are primarily from the text-processing literature.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Elman [4] trained a simple recurrent network (SRN) to predict the next symbol in an input stream based on the symbols that immediately precede this symbol.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "Saffran, Aslin & Newport [9] studied language learning in infants, and showed that statistical relationships between phonemes in speech are an important source of information about word boundaries.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Brent [2] proposed that the accuracy of word boundary predictions can be improved by using mutual information between phonemes instead of transitional probabilities.", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "The Viterbi algorithm [5][10] provides a useful model of the kind of processing that the wordsegmentation algorithm requires to simultaneously evaluate multiple competing paths.", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "The Viterbi algorithm [5][10] provides a useful model of the kind of processing that the wordsegmentation algorithm requires to simultaneously evaluate multiple competing paths.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "The GetHashIndex function in the preceding program uses the ELF hash algorithm, as described by Binstock [1].", "startOffset": 105, "endOffset": 108}], "year": 2011, "abstractText": "In natural speech, the speaker does not pause between words, yet a human listener somehow perceives this continuous stream of phonemes as a series of distinct words. The detection of boundaries between spoken words is an instance of a general capability of the human neocortex to remember and to recognize recurring sequences. This paper describes a computer algorithm that is designed to solve the problem of locating word boundaries in blocks of English text from which the spaces have been removed. This problem avoids the complexities of processing speech but requires similar capabilities for detecting recurring sequences. The algorithm that is described in this paper relies entirely on statistical relationships between letters in the input stream to infer the locations of word boundaries. The source code for a C++ version of this algorithm is presented in an appendix.", "creator": "Microsoft\u00ae Office Word 2007"}}}