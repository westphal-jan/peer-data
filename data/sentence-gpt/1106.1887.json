{"id": "1106.1887", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "Learning the Dependence Graph of Time Series with Latent Factors", "abstract": "This paper considers the problem of learning, from samples, the dependency structure of a system of linear stochastic differential equations, when some of the variables are latent. In particular, we observe the time evolution of some variables, and never observe other variables; from this, we would like to find the dependency structure between the observed variables -- separating out the spurious interactions caused by the (marginalizing out of the) latent variables' time series. We develop a new method, based on convex optimization, to do so in the case when the number of latent variables is smaller than the number of observed ones.", "histories": [["v1", "Thu, 9 Jun 2011 19:34:29 GMT  (33kb)", "http://arxiv.org/abs/1106.1887v1", null], ["v2", "Sat, 25 Feb 2012 02:03:33 GMT  (1654kb,D)", "http://arxiv.org/abs/1106.1887v2", null], ["v3", "Fri, 16 Mar 2012 16:18:16 GMT  (1656kb,D)", "http://arxiv.org/abs/1106.1887v3", null], ["v4", "Tue, 1 May 2012 04:30:11 GMT  (1656kb,D)", "http://arxiv.org/abs/1106.1887v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ali jalali", "sujay sanghavi"], "accepted": true, "id": "1106.1887"}, "pdf": {"name": "1106.1887.pdf", "metadata": {"source": "CRF", "title": "Learning with Latent Factors in Time Series", "authors": ["Ali Jalali", "Sujay Sanghavi"], "emails": ["alij@mail.utexas.edu", "sanghavi@mail.utexas.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 6.\n18 87\nv1 [\ncs .L\nG ]\n9 J\nun 2"}, {"heading": "1 Introduction", "text": "Linear stochastic dynamical systems are classic processes that are widely used to model time series data in a huge number of fields: financial data [8], biological networks of species [17] or genes [1], chemical reactions [11, 12], control systems with noise [27], etc. An important task in several of these domains is learning the model from data [25]; doing so is often the first step in both data interpretation, and making predictions of future values or the effect of perturbations. Often one is interested in learning the dependency structure [15]; i.e. identifying, for each variable, which set of other variables it directly interacts with. For stock market data, for example, this can reveal which other stocks most directly affect a given stock.\nWe consider model structure learning in a particularly challenging yet widely prevalent setting: where (the time series of) some state variables are observed, and others are unobserved/latent. We are interested in learning the dependency structure between the observed variables. However, the presence of latent time series, if not properly accounted for by the model learning procedure, will result in the appearance of spurious interactions between observed variables \u2013 two observed variables that interact with the same unobserved variable may now be reported to be interacting. This happens, for example, if one uses the classic maximum-likelihood estimator [9], and persists even if we have observations over a long time horizon.\nSuppose, for illustration, that we are interested in learning the dependency structure between the prices of a set of stocks via a linear stochastic model. Clearly, stock prices depend not only on each other, but are also jointly influenced by several variables that may not be part of our model, for example, currency markets, commodity prices etc.; these are latent time series. Their presence means that a naive structure learning algorithm (say max-likelihood) that takes as input only the stock prices, will report several spurious interactions; say, e.g. between all stocks that fluctuate with the price of oil.\nClearly there are several issues with regards to fundamental identifiability, and sample and computational complexity, that need to be defined and resolved. We do so below in the specific context of our model setting. We provide both theoretical characterization and guarantees of the problem, as well as numerical illustrations for both synthetic data and some real data extracted from stock market.\nThe rest of the paper is organized as follows: We define the problem in Section 2. Section 3 reveals our main result followed by a high-level outline of the proof in Section 4. The simulation results are included in Section 5."}, {"heading": "2 Problem Setting and Main Idea", "text": "This paper considers the problem of structure learning in linear stochastic dynamical systems, in a setting where only a subset of the time series are observed, and others are unobserved/latent. In particular, we consider a system with state vectors x(t) \u2208 Rp and u(t) \u2208 Rr, for t \u2208 R+ and dynamics described by\nd\ndt\n[ x(t) u(t) ] = [ A\u2217 B\u2217 C\u2217 D\u2217 ]\n\ufe38 \ufe37\ufe37 \ufe38 A\u2217\n[ x(t) u(t) ] + d dt w(t), (1)\nwhere, w(t) \u2208 Rp+r is an independent standard Brownian motion vector and A\u2217, B\u2217, C\u2217, D\u2217 are system parameters. We observe the process x(t) for some time horizon 0 \u2264 t \u2264 T , but not the process u(\u00b7). We are interested in learning the matrix A\u2217, which captures the interactions between the observed variables.\nWe will also be interested in a similar objective for an analogous discrete time system with parameter 0 < \u03b7 < 2\n\u03c3max(A\u2217) : [\nx(n+ 1) u(n+ 1)\n] \u2212 [ x(n) u(n) ] = \u03b7 [ A\u2217 B\u2217 C\u2217 D\u2217 ] [ x(n) u(n) ] +w(n) n \u2208 N0, (2)\nwhere, w(n) is a zero-mean Gaussian noise vector with covariance matrix \u03b7I(p+r)\u00d7(p+r). Here \u03b7 can be thought of as the sampling step; in particular notice that as \u03b7 \u2192 0, we recover model (1) from model (2). The upper bound on \u03b7 ensures the stability of the discrete time system as required by our theorem. Intuitively, \u03c3max(A\u2217) corresponds to the fastest convergence rate in the system and the upper bound on \u03b7 corresponds to the Nyquist minimum sampling rate required for the reconstruction of the signal. As done in [2], our proofs will initially focus on the discrete case (2), and derive results for (1) afterwards.\n(A1) Stable Overall System: We only consider stable systems. In fact, we impose an assumption slightly stronger than the stability on the overall system. For the continuous system (1), we require D := \u2212\u03bbmax(A \u2217+A\u2217T 2 ) > 0. With slightly abuse of notation, for the discrete system (2), we require D := 1\u2212\u03a3 2 max\n\u03b7 > 0, where, \u03a3max := \u03c3max(I + \u03b7A\u2217). As a consequence of this assumption, by Lyapunov theory, the continuous system (1) has a unique stationary measure which is a zero-mean Gaussian distribution with positive definite (otherwise, it is not unique) covariance matrix Q\u2217 \u2208 R(p+r)\u00d7(p+r) given by the solution of A\u2217Q\u2217+Q\u2217A\u2217T +I = 0. Similarly, for the discrete time system (2), we have A\u2217Q\u2217+Q\u2217A\u2217T + \u03b7A\u2217Q\u2217A\u2217T + I = 0. This matrix Q\u2217 has the form Q\u2217 = [Q\u2217 R\u2217T ; R\u2217 P \u2217], where, Q\u2217 and P \u2217 are the steady-state covariance matrix of the observed and latent variables, respectively and R\u2217 is the steady-state cross-covariance between observed and latent variables. By stability, we have Cmin := \u039bmin(Q\u2217) > 0 and Dmax := \u039bmax(Q\u2217) < \u221e. Identifiability: Clearly, the above objective of identifying A\u2217 is in general impossible without some additional assumptions on the model; in particular, several different choices of the overall model (including different choices of A\u2217) can result in the same effective model for the x(\u00b7) process. x(\u00b7) would then be statistically identical under both models, and correct identification would not be possible even over an infinite time horizon. Additionally, it would in general be impossible to achieve identification if the number of latent variables is comparable to or exceeds the number of observed variables. Thus, to make the problem\nwell-defined, we need to restrict (via appropriate assumptions) the set of models we are interested in."}, {"heading": "2.1 Main Idea", "text": "Consider the discrete-time system (2) in steady state and suppose, for a moment, that we ignored the fact that there may be latent time series; in this case, we would be back in the classical setting, for which the (population version of) the likelihood is\nL(A) = 1 2\u03b72\nE [ \u2016x(i+ 1)\u2212 x(i)\u2212 \u03b7Ax(i)\u201622 ] .\nLemma 1. For x(\u00b7) generated by (2), the the optimum A\u0302 := maxA L(A) is given by\nA\u0302 = A\u2217 +B\u2217R\u2217(Q\u2217)\u22121.\nThus, the optimal A\u0302 is a sum of the original A\u2217 (which we want to recover) and the matrix B\u2217R\u2217(Q\u2217)\u22121 that captures the spurious interactions obtained due to the latent time series. Notice that the matrix B\u2217R\u2217(Q\u2217)\u22121 has the rank at most equal to number r of latent time series. We will assume that the number of latent time series is smaller than the number of observed ones \u2013 i.e. r < p \u2013 and hence B\u2217R\u2217(Q\u2217)\u22121 is a low-rank matrix."}, {"heading": "2.2 Identifiability", "text": "Besides identifying the effect of the latent time series, the above realization also allows us to quantify the assumptions we would need to impose to ensure identifiability. In particular, the true model should be such that A\u2217 should be uniquely identifiable from B\u2217R\u2217(Q\u2217)\u22121. We choose to study models that have a local-global structure where (a) each of the observed time series xi(t) interacts with only a few other observed series, while (b) each of the latent series interacts with a (relatively) large number of observed series. In the stock market example, for instance, this would model the case where the latent series corresponds to macro-economic factors, like currencies or the price of oil, that affect a lot of stock prices.\nIn particular, let s be the maximum number of non-zero entries in each row, and d be the maximum number of non-zero entries in any column, of A\u2217. Parameters s and d control the number of other observed variables any given observed variable interacts with. Note that this means A\u2217 is a sparse matrix. Let L\u2217 := B\u2217R\u2217(Q\u2217)\u22121 and assume it has SVD L\u2217 = U\u2217\u03a3\u2217V \u2217T , and recall that its rank is r. Then, following [7], L\u2217 is said to be (\u00b51, \u00b52)incoherent if \u00b51, \u00b52 > 0 are smallest real numbers satisfying\nmax i\n\u2016U\u2217T ei\u2016 \u2264 \u221a \u00b52r\np , max i \u2016V \u2217Tei\u2016 \u2264\n\u221a \u00b51r\np , \u2016U\u2217V \u2217T \u2016\u221e \u2264\n\u221a r \u221a \u00b51\u00b52\np2 ,\nwhere, ei\u2019s are standard basis vectors with proper length. Here \u2016 \u00b7 \u2016 represents the Euclidean norm of the vector. Smaller values of \u00b5 mean the row and column spaces make larger angles with the standard bases, and hence the resulting matrix is more dense.\n(A2) Identifiability: We require that the s, d of the sparse matrix A\u2217 and the \u00b51, \u00b52 of the low-rank L\u2217, which has rank r, satisfy \u03b1 := (\u221a \u00b51s+ \u221a \u00b52d )\u221a r p < 1."}, {"heading": "2.3 Algorithm", "text": "Recall that our task is to recover the matrix A\u2217 given observations of the x(\u00b7) process. We saw that the max-likelihood estimate (in the population case) was the sum of A\u2217 and a lowrank matrix; we subsequently assumed that A\u2217 is sparse. In light of this development, it is natural to use the max-likelihood as the loss function for the sum of a sparse and low-rank matrix, and separate appropriate regularizers for each of the components. Thus, for the continuous-time system observed up to time T , we propose solving\n(A\u0302, L\u0302) = argmin A,L\n1\n2T\n\u222b T\nt=0\n\u2016(A+ L)x(t)\u201622 dt\u2212 1\nT\n\u222b T\nt=0\nx(t)T (A+ L)T dx(t) + \u03bbA\u2016A\u20161 + \u03bbL\u2016L\u2016\u2217, (3)\nand for the discrete-time system given n samples, we propose solving\n(A\u0302, L\u0302) = argmin A,L\n1\n2\u03b72n\nn\u22121\u2211\ni=0\n\u2016x(i+ 1)\u2212 x(i)\u2212 \u03b7(A+ L)x(i)\u201622 + \u03bbA\u2016A\u20161 + \u03bbL\u2016L\u2016\u2217. (4)\nHere \u2016 \u00b7 \u20161 is the \u21131 norm (a convex surrogate for sparsity), and \u2016 \u00b7 \u2016\u2217 is the nuclear norm (i.e. sum of singluar values, a convex surrogate for low-rank). The optimum A\u0302 of (4) or (3) is our estimate of A\u2217, and our main result provides conditions under which we recover the support of A\u2217, as well as a bound on the error in values \u2016A\u0302 \u2212 A\u2217\u2016\u221e (maximum absolute value). We provide a bound on the error \u2016L\u0302 \u2212 L\u2217\u20162 (spectral norm) for the low-rank part. Notice that the discrete objective function goes to the continuous one as \u03b7 \u2192 0."}, {"heading": "2.4 High-dimensional setting", "text": "Note that when A\u2217 is a sparse matrix, the actual degrees of freedom between the observed variables is smaller than that evinced by the ambient dimension p. Indeed, we will be interested in recovering A\u2217 with a number of samples n that is potentially much smaller than p (for small s and d). In the special case when we are in steady state and L = 0 (i.e. \u03bbL large) the recovery of each row of A\u2217 is akin to a LASSO [24] problem (of sparse vector recovery from noisy linear measurements) with Q\u2217 being the covariance of the design matrix. We thus require Q\u2217 to satisfy incoherence conditions that are akin to those in LASSO (see e.g. [26] for the necessity of such conditions).\n(A3) Incoherence: To control the effect of the irrelevant (not latent) variables on the set of relevant variables, we require \u03b8 := 1 \u2212 maxk \u2016Q\u2217Sc\nk Sk ( Q\u2217SkSk )\u22121 \u2016\u221e,1 > 0, where, Sk is the support of the kth row of A\u2217 and Sck is the complement of that. The norm \u2016 \u00b7 \u2016\u221e,1 is the maximum of the \u21131-norm of the rows."}, {"heading": "2.5 Related Work", "text": "A comprehensive body of work is done on sparse and low-rank decomposition of matrices both for noiseless [6, 4, 7] and noisy [28, 3] cases, as well as on graphical model learning [22, 14, 20], all via convex optimization. Recently, Chandrasekaran et. al. [5] studied the problem of learning latent variables in graphical models via sparse and low-rank decomposition. In most of these results the emphasis is on the recovery of the low-rank component, where as in our work, we try to recover the sparse component. Moreover, in the graphical model learning works, the samples are assumed to be independent and certainly, we can not make that assumption.\nLearning graphical model for time series has been recently studied in [2]. However, they do not consider latent time series and hence just used LASSO to get a sparse graphical model. Our method can handle latent time series if they exist. Notice that if there is no latent time series, taking \u03bbL large enough will effectively result into solving just LASSO."}, {"heading": "3 Main Results", "text": "In this section, we present our main result for both Continuous and Discrete time systems. We start by imposing some assumptions on the regularizers and the sample complexity.\n(A4) Regularizers: We need to impose some assumptions on the regularizers to be able to guarantee our result. Let m = max ( 80\u221a D \u2016B\u2217\u2016\u221e,1, \u221a \u2016x(0)\u201622 + \u2016u(0)\u201622 + ( \u221a \u03b7 + 1)2 ) , be the constant capturing the effect of initial condition and latent variables through matrix B\u2217. We impose the following assumptions on the regularizers:\n\u2022 (A4-1) \u03bbA \u2265 16m(4\u2212\u03b8)\u03b8\u221aD\n\u221a log ( 4((s+2r)p+r2)\n\u03b4\n)\nn\u03b7 .\n\u2022 (A4-2) \u03bbL\u03bbA\u221ap \u2265 1 (1\u2212\u03b1)(4\u2212\u03b8)\n(( 3\u03b1(4\u2212\u03b8)\u221as\n4 + (8\u2212\u03b8)s \u03b8\n)( \u03b8 \u221a p\n9s \u221a s + 1 ) + 12 ) .\n(A5) Sample Complexity: As we mentioned before, \u03b7 plays the role of the step size when we sample the continuous time system, hence, we present our sample complexity in terms of \u03b7n \u2248 T . Our result is stated in terms of the probability of failure \u03b4. We require two sets of constraints on the sample complexity T = \u03b7n as follows:\n\u2022 (A5-1) Covariance matrix concentration: To get a consistent estimate of the covariance matrix, we require\nT = n\u03b7 \u2265 3\u00d710 6s3\nD2\u03b82C2min log\n( 4((s+ 2r)p+ r2)\n\u03b4\n) .\nNote that our guarantee is for the worst possible sample complexity since L\u2217 has infinite (order p2) support size and we provide \u2113\u221e error bound to get the sign support recovery. Clearly, one can provide much stronger guarantee if n can scale with p (as opposed to log(p) in our case); but here, our intension is to provide guarantees in low sample complexity regime.\n\u2022 (A5-2) Minimum Energy: To recover the exact sign support of A\u2217, we require\nT = n\u03b7 \u2265 256m 2\u03bd2(4\u2212 \u03b8)2\nD2\u03b82A2min log\n( 4((s+ 2r)p+ r2)\n\u03b4\n) ,\nwhere, \u03bd = \u03b1\u03b8 2Dmax + (8\u2212\u03b8)\u221as Cmin(4\u2212\u03b8)\nand Amin := min | (A\u2217)(k)j | . The reason behind this scaling is the way we show the exact sign support recovery. First, we show that \u2016A\u0302\u2212 A\u2217\u2016\u221e \u2264 \u03bd\u03bbA (See equation (11)). Second, we take T = \u03b7n so large that \u03bbA is small enough to satisfy \u03bd\u03bbA < Amin and hence, Sign(A\u0302) = Sign(A\u2217).\nWe identify the distance between the spaces L\u0302 and L\u2217 come from with a parameter\ndefined as \u03c1 := min (\n\u03b1 4 , \u03b8\u03b1\u03bbA 5\u03b8\u03b1\u03bbA+16Dmax\u2016L\u2217\u20162\n) . The following (unified) theorem states our\nmain result for both discrete and continuous time systems.\nTheorem 1. If assumptions (A1)-(A5) are satisfied, then with probability 1\u2212\u03b4, our algorithm outputs a pair (A\u0302, L\u0302) satisfying\n\u2022 (a) Sign Support Recovery: Sign(Supp(A\u0302)) = Sign(Supp(A\u2217)).\n\u2022 (b) Error Bounds: \u2016A\u0302\u2212A\u2217\u2016\u221e \u2264 \u03bd\u03bbA and \u2016L\u0302\u2212 L\u2217\u20162 \u2264 \u03c11\u22125\u03c1\u2016L\u2217\u20162.\nRemark 1: Our result has a significant difference with other results on sparse and low-rank decomposition. In a typical sparse and low-rank decomposition, the number of observations has the same order as number of variables, i.e., n = \u0398(p), mainly because the typical goal is to recover the low-rank component (either the rank [5] or values [4]) accurately. In contrast, we are interested primarily on the sign support recovery of the sparse component. Our result shows that we can significantly reduce the number of observation to n = \u0398(log(p)) for sparse component recovery.\nRemark 2: Notice that \u03bd2 = \u0398(s) and hence our result have the same sample complexity (up to a constant) as [2], even though we recover the sign support in the presence of latent variables. The difference (in constants) between our result and [2] is summarized in parameter m. As sample complexity grows, the parameter \u03c1 goes to zero and hence we get a better estimation of L\u2217. Moreover, if the identifiability parameter \u03b1 is small, we get a better estimation of L\u0302 (see expression of \u03c1 and Assumption (A4-2)). If \u03b1 is close to 1, then we can get a really bad estimate of L\u0302. Finally, choosing \u03b7 to be small enough (increasing the sampling frequency), so that the overall system has small singular values, is enough to guarantee the sign support recovery.\nRemark 3: As a technical remark, we would like to mention the three key ingredients of the proof enabling us to get this low sample complexity. The first ingredient comes from our new set of optimality conditions inspired by [4]. This optimality conditions enable us to certify an approximation of L\u2217 while certifying the exact sign support of A\u2217. The second ingredient\ncomes from the bounds on the Schur complement of the perturbation of positive semidefinite matrices [23]. This result enables us to get a bound on the Schur complement of a perturbation of a positive semi-definite matrix of size p with only log(p) samples. The third and last key ingredient is taking advantage of the structure of L\u2217. Since L\u2217 = B\u2217R\u2217(Q\u2217)\u22121 and Q\u2217 is a symmetric positive semi-definite matrix, the row-space of L\u2217 is a subset of the row-space of Q\u2217. This property enables us to bound the projection of the matrices of the form FQ\u2217 (for some matrix F ) into the null space of L\u2217 with low sample complexity."}, {"heading": "4 Proof Outline", "text": "In this section, we first introduce some notations and definitions and then, provide a three step proof technique to prove the main theorem for the discrete time system. The proof of the continuous time system is done via a coupling argument in the appendix.\nSparse Matrix Notation: For any matrix A \u2208 Rp\u00d7p, define Supp(A) = {(j, k) : A(k)j 6= 0}, and let \u2126 = {A \u2208 Rp\u00d7p : Supp(A) \u2286 Supp(A\u2217)} be the subspace of matrices whose their support is the subset of the matrix A\u2217. The orthogonal projection to the subspace \u2126 can be defined such that (P\u2126(M))(k)j is equal to M (k) j if (j, k) \u2208 Supp(A\u2217) and zero otherwise. Denote the orthogonal complement space of \u2126 with \u2126c. The orthogonal projection to this space can be defined as P\u2126c(M) = M \u2212 P\u2126(M). Low-Rank Matrix Notation: For any matrix L \u2208 Rp\u00d7p with singular value decomposition U\u03a3V T , where, U, V \u2208 Rp\u00d7r and \u03a3 \u2208 Rr\u00d7r, let Span(L) = {UXT + Y V T : X,Y \u2208 Rp\u00d7r}. Let T (L) = {N \u2208 Rp\u00d7p : Span(N) \u2286 Span(L)} denote the subspace of matrices whose column and row spans are included in those of L. The orthogonal projection to the subspace T (L) can be characterized by PT (N) = UUTN+NV V T \u2212UUTNV V T . Denote the orthogonal complement space of T with T c with orthogonal projection PT c(\u00b7). We define a metric to measure the closeness of two tangent spaces T1 and T2 as follows\n\u03c1 (T1, T2) = max N\u2208Rp\u00d7p \u2016PT1(N) \u2212 PT2(N)\u20162 \u2016N\u20162 .\nAn immediate consequence of this definition is that \u2016PT2(U1V T1 ) \u2212 U2V T2 \u20162 \u2264 \u03c1 (T1, T2), because otherwise, let N = vvT to get the contradiction, where, v is the eigenvector corresponding to the eigenvalue, exceeding \u03c1 (T1, T2). Finally, let T = T (L\u2217) for the shorter notation.\nProof Technique: We outline the proof in three steps as follows:\nSTEP 1: Constructing a candidate primal optimal solution (A\u0303, L\u0303) with the desired sparsity pattern using the restricted support optimization problem, called oracle problem:\n(A\u0303, L\u0303)= arg min L:\u03c1(T (L),T )\u2264\u03c1 A:P\u2126c (A)=0\n1\n2\u03b72n\nn\u22121\u2211\ni=0\n\u2016x(i+ 1)\u2212x(i)\u2212\u03b7(A+ L)x(i)\u201622+\u03bbA\u2016A\u20161+\u03bbL\u2016L\u2016\u2217. (5)\nThis oracle is similar to the one used in [5]. It ensures that the right sparsity pattern is chosen for A\u0303 and the tangent spaces L\u0303 and L\u2217 come from are close with parameter \u03c1. Note that this is a proof technique, not a method to construct the solution.\nSTEP 2: Writing down a set of sufficient (stationary) optimality conditions for (A\u0303, L\u0303) to be the unique solution of the (unrestricted) optimization problem (4):\nLemma 2. If \u2126\u2229 T = {0}, then (A\u0303, L\u0303), the solution to the oracle problem (5), is the unique solution of the problem (4) if there exists a matrix Z\u0303 \u2208 Rp\u00d7p such that (C1) P\u2126(Z\u0303) = \u03bbASign ( A\u0303 ) . (C2)\n\u2225\u2225\u2225P\u2126c(Z\u0303) \u2225\u2225\u2225 \u221e < \u03bbA.\n(C3) \u2225\u2225\u2225PT (Z\u0303)\u2212 \u03bbLU\u2217V \u2217T \u2225\u2225\u2225 2 \u2264 4\u03c1\u03bbL. (C4) \u2225\u2225\u2225PT c(Z\u0303) \u2225\u2225\u2225 2 < (1\u2212 \u03b1)\u03bbL.\n(C5) \u2212 1 \u03b7n\nn\u2211\ni=1\n( x(i+ 1)\u2212 x(i)\u2212 \u03b7(A\u0303+ L\u0303)x(i) ) x(i)T + Z\u0303 = 0 =: Jn + Z\u0303.\nSTEP 3: Constructing a dual variable Z\u0303 that satisfies the sufficient optimality conditions stated in Lemma 2. For matrices M \u2208 \u2126 and N \u2208 T , let\nHM = M \u2212 PT (M) + P\u2126PT (M)\u2212PT P\u2126PT (M) + . . . GN = N \u2212 P\u2126(N) + PT P\u2126(N)\u2212 P\u2126PT P\u2126(N) + . . . .\nIt has been shown in [7] that if \u03b1 < 1 then both infinite sums converge. Suppose we have the SVD decomposition L\u0303 = U\u0303\u03a3\u0303V\u0303 T . Let\nZ\u0303 = H\u03bbASign(A\u0303) + GPT (\u03bbLU\u0303 V\u0303 T ) +\u2206,\nwhere, \u2206 is a matrix such that (C5) is satisfied. As a result of this construction, we have P\u2126(Z\u0303 \u2212 \u2206) = \u03bbASign(A\u0303) and since PT (Z\u0303 \u2212 \u2206) = PT (\u03bbLU\u0303 V\u0303 T ), by Lemma 3, we have PT\u0303 (Z\u0303 \u2212\u2206) = \u03bbLU\u0303 V\u0303 T . Notice that P\u2126(Jn) = P\u2126(Z\u0303 \u2212\u2206) and PT\u0303 (Jn) = PT\u0303 (Z\u0303 \u2212\u2206) (and hence by Lemma 3, PT (Jn) = PT (Z\u0303 \u2212 \u2206)); thus, P\u2126(\u2206) = PT (\u2206) = 0. Now, we can establish P\u2126(Z\u0303) = \u03bbASign(A\u0303) and PT (Z\u0303) = PT (\u03bbLU\u0303 V\u0303 T ) and consequently the conditions (C1) and (C3) in Lemma 2 are satisfied. It suffices to show that (C2) and (C4) are satisfied with high probability. This has been shown in Lemma 6."}, {"heading": "5 Experimental Results", "text": "In this section, we numerically illustrate the power of our theoretical result for both synthetic and real data collected from the stock market."}, {"heading": "5.1 Synthetic Data", "text": "We generate a sparse matrix A\u2217 with values taken from a standard normal distribution and generate B\u2217, C\u2217 and D\u2217 with i.i.d. entries drawn from a standard normal distribution scaled down by a factor r (to make sure that Amin is large enough statistically). To make the matrix A\u2217 negative definite (hence, stable), using Gers\u030cgorin disk theorem [10], we put a large-enough negative value on the diagonal. We simulate the discrete time system for different values of \u03b7 = C\u03b7 2\u03c3max(A\u2217) for the stability parameter C\u03b7 \u2208 (0, 1) and solve (4) using accelerated proximal gradient method [18]. Notice that the value of \u03b7 by itself is not a good criterion to judge the performance of these types of algorithms because the rate of changes in the dynamic system depend on A\u2217. That is why, unlike [2], we use C\u03b7 for performance analysis. Thus, we plot our result with respect to the control parameter\n\u0398 = \u03b7n\n2 \u03c3max(A\u2217)\ns3 log ((s+ 2r)p+ r2) .\nWe pick the values of \u03bbA and \u03bbL by dividing the training data into chunks each having consecutive samples of the time series and do the cross validation over those chunks. Note\nthat this is different from the standard cross validation technique due to the dependency of samples.\nFigure 1(a) shows that as long as \u03b7 < 2\u03c3max(A\u2217) , the number of samples n remains almost constant. This can be seen (but not pointed out) in Figure 2 of [2] as well. This phenomenon matches our intuition that the amount of information we get from each sample remains the same as long as the Nyquist sampling criterion is met. Figure 1(b) indicates that the phase transition point of the control parameter is not sensitive to the value of r, which means the control parameter is the right scaling of the sample complexity."}, {"heading": "5.2 Stock Market Data", "text": "We take the end-of-the-day closing stock prices for 50 different companies in the period of May 17, 2010 - May 13, 2011 (255 business days). These companies (among them, Amazon, eBay, Pepsi, etc) are consumer goods companies traded either at NASDAQ or NYSE in USD. The data is collected from Google Finance website. Our goal is to observe the stock prices for a period of time and predict it for the entire days of the next month with small error.\nWe apply our algorithm to this data and try to learn the model using the data for n (consecutive) days and then compute the mean squared error in the prediction of the following month (25 business days). We randomly pick an starting day n0 between day 1 and day 255 \u2212 25 \u2212 n. Then we learn the model using the data from the day n0 to the day n0 + n (total of n days). Then, we test our data on the consecutive 25 days. Finally, we average the error over 10 different starting points n0 for each value of n. We pick the regularizers by the semi-cross validation process explained in the previous section. The ratio n25 shows the ratio of training sample size to the testing sample size.\nFigure 2(b) shows the prediction error for both our method and pure LASSO [2] method as the train/test ratio increases. It can be seen that our method not only have better prediction, but also is more robust. Our algorithm requires only 3 months of the past data to give a robust estimation of the next month; in contrast with almost 6 months requirement of LASSO. However, the error of our algorithm is much smaller (by a factor of 6) than LASSO even in the steady state. Figure 2(a) shows the sparsity level for our model and the LASSO model. The number of latent variables our model finds varies from 8 \u2212 12 for different train/test ratios. As Figure 2(a) illustrates, our estimated A\u0302 is order of magnitude sparser than the one estimated by LASSO."}, {"heading": "A Proof of Lemma 1", "text": "Proof. Ignoring the term \u2016x(n+1)\u2212x(n)\u201622 which is independent of A, minimization of L(A) with this infinite sample size is equivalent to\nmin A E\n[ x(n)TATAx(n)\u2212 2\n\u03b7 (x(n + 1)\u2212 x(n))TAx(n)\n]\n= min A E\n[ trace ( Ax(n)x(n)TAT ) \u2212 2 (A\u2217x(n) +B\u2217u(t))T Ax(n) ]\n= min A\ntrace ( AQ\u2217AT ) \u2212 2trace ( A\u2217Q\u2217AT ) \u2212 2trace ( B\u2217R\u2217AT )\n= min A\ntrace (( A\u2212 2 ( A\u2217 +B\u2217R\u2217(Q\u2217)\u22121 ) ) Q\u2217AT ) .\nHere we ignored the term w(n) due to the fact that it is zero mean and independent of x(n) and u(n). This implies that the asympotatic optimizer of L(\u00b7) satisfies A\u0302 = A\u2217 + B\u2217R\u2217(Q\u2217)\u22121."}, {"heading": "B Proof of Lemma 2", "text": "General Notation: For a matrix X \u2208 Ra\u00d7b, we use X(1), . . . , X(a) to denote rows, X1, . . . , Xb to denote columns and X (1) 1 , . . . , X (a) b to denote entries. Also, for the sets of indecies S1 \u2286 {1, \u00b7 \u00b7 \u00b7, a} and S2 \u2286 {1, \u00b7 \u00b7 \u00b7, b}, the matrix XS1S2 \u2208 R|S1|\u00d7|S2| represents the sub-matrix of X consisting of the rows and columns corresponding to index sets S1 and S2.\nProof. Suppose A\u0303 = A\u0302 +DA and L\u0303 = L\u0302 +DL for some matrices DA and DL. Let ZA = \u03bbASign(A\u0303) \u2212 F0 with P\u2126(F0) = 0 and \u3008F0, DA\u3009 = \u03bbA\u2016P\u2126c(DA)\u20161. Let T\u0303 = T (L\u0303) with SVD decomposition L\u0303 = U\u0303\u03a3\u0303V\u0303 T . Now, let ZL = \u03bbLU\u2217V \u2217T + W1 \u2212 W0 with PT (W0) = 0 and \u2016W0\u20162 \u2264 (1 \u2212 \u03b1)\u03bbL and \u3008W0, DL\u3009 = (1 \u2212 \u03b1)\u03bbL\u2016PT c(DL)\u2016\u2217 and with PT c(W1) = 0 and \u2016W1\u20162 \u2264 4\u03c1\u03bbL such that \u03bbLU\u2217V \u2217T + W1 \u2212 W0 = \u03bbLU\u0303 V\u0303 T + W2 for some matrix W2 with PT\u0303 c(W2) = 0 and \u2016W2\u20162 < \u03bbL. By zero duality gap between dual norms, matrices F0 and W0 exist and the existence of matrices W1 and W2 is guaranteed by Lemma 3. Hence, ZA and ZL are subgradients of \u03bbA\u2016A\u0303\u20161 and \u03bbL\u2016L\u0303\u2016\u2217. By convexity of the program, we have\nL(A\u0302+ L\u0302) + \u03bbA\u2016A\u0302\u20161 + \u03bbL\u2016L\u0302\u2016\u2217 \u2265 L(A\u0303 + L\u0303) + \u03bbA\u2016A\u0303\u20161 + \u03bbL\u2016L\u0303\u2016\u2217 + \u2329 Z\u0303,DA +DL \u232a \u2212 \u3008ZA, DA\u3009 \u2212 \u3008ZL, DL\u3009 .\nIt suffices to show that \u2329 Z\u0303,DA +DL \u232a \u2212 \u3008ZA, DA\u3009 \u2212 \u3008ZL, DL\u3009 \u2265 0 to conclude by the\noptimality of (A\u0302, L\u0302) that the result holds. Notice that (C5) ensures that P\u2126(Z\u0303) = \u03bbASign(A\u0303) and PT\u0303 (Z\u0303) = \u03bbLU\u0303 V\u0303 T . By Lemma 3, we conclude that PT (Z\u0303) = \u03bbLU\u2217V \u2217T +W1. Thus, for some \u03b3 < 1, we have \u2329 Z\u0303,DA +DL \u232a \u2212 \u3008ZA, DA\u3009 \u2212 \u3008ZL, DL\u3009\n= \u03bbA\u2016P\u2126c(DA)\u20161 + (1\u2212 \u03b1)\u03bbL\u2016PT c(DL)\u2016\u2217 + \u2329 Z\u0303,DA +DL \u232a \u2212 \u2329 \u03bbASign(A\u0303),P\u2126(DA) \u232a \u2212 \u2329 \u03bbLU \u2217V \u2217T +W1,PT (DL) \u232a = \u03bbA\u2016P\u2126c(DA)\u20161 + (1\u2212 \u03b1)\u03bbL\u2016PT c(DL)\u2016\u2217 + \u2329 P\u2126c(Z\u0303),P\u2126c(DA) \u232a + \u2329 PT c(Z\u0303),PT c(DL) \u232a\n+ \u2329 PT (Z\u0303)\u2212 \u03bbLU\u2217V \u2217T \u2212W1,PT (DL) \u232a + \u2329 P\u2126(Z\u0303)\u2212 \u03bbASign(A\u0303),P\u2126(DA) \u232a\n\u2265 (1\u2212 \u03b3)\u03bbA\u2016P\u2126c(DA)\u20161 + (1\u2212 \u03b3)(1\u2212 \u03b1)\u03bbL\u2016PT c(DL)\u2016\u2217 \u2265 0.\nThis concludes the proof of the lemma.\nLemma 3. For any two matrices with SVD L\u2217 = U\u2217\u03a3\u2217V \u2217T and L\u0303 = U\u0303 \u03a3\u0303V\u0303 T and corresponding tanget spaces T and T\u0303 , if \u03c1(T , T\u0303 ) = \u03c1 \u2264 \u03b14 , then, for any matrix W0 with PT (W0) = 0 and \u2016W0\u20162 \u2264 (1\u2212 \u03b1)\u03bbL, there exist matrices W1 and W2 such that\nZ\u0303L := \u03bbLU \u2217V \u2217T +W1 \u2212W0 = \u03bbLU\u0303 V\u0303 T +W2,\nwhere, PT\u0303 (W2) = 0 with \u2016W2\u20162 < \u03bbL and PT c(W1) = 0 with \u2016W1\u20162 \u2264 4\u03c1\u03bbL.\nProof. Let W2 = \u2212PT c(\u03bbLU\u0303 V\u0303 T )\u2212W0 and W1 = PT (\u03bbLU\u0303 V\u0303 T )\u2212\u03bbLU\u2217V \u2217T . For this choice, the equality constraints hold. First, notice that \u2016PT (M)\u20162 \u2264 2 \u2016M\u20162 and hence,\n\u2225\u2225\u2225Z\u0303L \u2225\u2225\u2225 2 = \u2225\u2225\u2225\u03bbLU\u0303 V\u0303 T \u2212 PT c(\u03bbLU\u0303 V\u0303 T )\u2212W0 \u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225PT (\u03bbLU\u0303 V\u0303 T ) \u2225\u2225\u2225 2 + \u2016W0\u20162 \u2264 2 \u2225\u2225\u2225\u03bbLU\u0303 V\u0303 T\n\u2225\u2225\u2225 2 + \u2016W0\u20162 \u2264 (3\u2212 \u03b1)\u03bbL.\nUsing this, we can bound both W1 and W2. For W2, we have\n\u2016W2\u20162 \u2264 \u2225\u2225\u2225PT c(\u03bbLU\u0303 V\u0303 T ) \u2225\u2225\u2225 2 + \u2016W0\u20162\n= \u2225\u2225\u2225PT c(\u03bbLU\u0303 V\u0303 T \u2212 \u03bbLU\u2217V \u2217T \u2212W1) \u2225\u2225\u2225 2 + \u2016W0\u20162 \u2264 \u2225\u2225\u2225\u03bbLU\u0303 V\u0303 T \u2212 \u03bbLU\u2217V \u2217T \u2212W1\n\u2225\u2225\u2225 2 + \u2016W0\u20162\n= \u2225\u2225\u2225PT\u0303 (Z\u0303L)\u2212 PT (Z\u0303L) \u2225\u2225\u2225 2 + \u2016W0\u20162 \u2264 \u03c1 \u2225\u2225\u2225Z\u0303L\n\u2225\u2225\u2225 2 + \u2016W0\u20162\n\u2264 ((3 \u2212 \u03b1)\u03c1+ (1\u2212 \u03b1)) \u03bbL < \u03bbL.\nNote that PT\u0303 (Z\u0303L) = \u03bbLU\u0303 V\u0303 T and hence, we can establish\n\u2016W1\u20162 = \u2225\u2225\u2225PT (\u03bbLU\u0303 V\u0303 T )\u2212 \u03bbLU\u2217V \u2217T \u2225\u2225\u2225 2\n= \u2225\u2225\u2225PT (Z\u0303L)\u2212 PT\u0303 (Z\u0303L) \u2225\u2225\u2225 2 + \u2225\u2225\u2225\u03bbLU\u0303 V\u0303 T \u2212 \u03bbLU\u2217V \u2217T \u2225\u2225\u2225 2 \u2264 \u03c1 \u2225\u2225\u2225Z\u0303L\n\u2225\u2225\u2225 2 + \u03c1\u03bbL \u2264 ((3\u2212 \u03b1)\u03c1+ \u03c1)\u03bbL.\nThis concludes the proof of the lemma."}, {"heading": "C Auxiliary Optimality Lemmas", "text": "Lemma 4 (Convex Optimality). If A\u0302 is a solution of (4) then there exists a matrix Z\u0302 \u2208 Rp\u00d7p, called dual variable, such that Z\u0302 \u2208 \u03bbA\u2202\u2016A\u0302\u20161 and Z\u0302 \u2208 \u03bbL\u2202\u2016L\u0302\u2016\u2217 and\n\u2212 1 \u03b7n\nn\u2211\ni=1\n( x(i + 1)\u2212 x(i)\u2212 \u03b7(A\u0302+ L\u0302)x(i) ) x(i)T + Z\u0302 = 0. (6)\nProof. The proof follows from the standard first order optimality argument.\nLemma 5. If \u03b1 < 1 then \u2126 \u2229 T = {0}.\nProof. The proof follows the same technique used in [7]. Trivially, {0} \u2208 \u2126 \u2229 T . Assume that there exists a non-zero matrix M \u2208 \u2126 \u2229 T . By idempotency of orthogonal projections, we have M = P\u2126(M) = PT (P\u2126(M)) and hence\n\u2016PT (P\u2126(M))\u2016\u221e = \u2225\u2225U\u2217U\u2217TP\u2126(M) + P\u2126(M)V \u2217V \u2217 \u2212 U\u2217U\u2217TP\u2126(M)V \u2217V \u2217T \u2225\u2225 \u221e\n\u2264 \u2225\u2225U\u2217U\u2217TP\u2126(M) \u2225\u2225 \u221e + \u2225\u2225(I \u2212 U\u2217U\u2217T )P\u2126(M)V \u2217V \u2217T \u2225\u2225 \u221e \u2264 max i \u2225\u2225U\u2217U\u2217Tei \u2225\u2225max j \u2016ejP\u2126(M)\u2016\n+ \u2225\u2225I \u2212 U\u2217U\u2217T \u2225\u2225 2 max j \u2016P\u2126(M)ej\u2016max i \u2225\u2225V \u2217V \u2217Tei \u2225\u2225\n\u2264 max i\n\u2225\u2225U\u2217U\u2217Tei \u2225\u2225\u221ad1 \u2016P\u2126(M)\u2016\u221e + \u221a d2 \u2016P\u2126(M)\u2016\u221e maxi \u2225\u2225V \u2217V \u2217T ei \u2225\u2225\n\u2264 \u03b1\u2016P\u2126(M)\u2016\u221e = \u03b1\u2016PT (P\u2126(M)) \u2016\u221e.\nHence, \u2016M\u2016\u221e = 0 or equivalently, M = 0. This is a contradiction.\nLemma 6. For constructed dual variable, conditions (C2) and (C4) are satisfied.\nProof. Let Q(n) = 1n \u2211n i=1 x(i)x(i) T and R(n) = 1n \u2211n i=1 u(i)x(i)\nT . Substituting x(i + 1) \u2212 x(i) = \u03b7A\u2217x(i) + \u03b7B\u2217u(i) + w(i) and L\u2217 = B\u2217R\u2217(Q\u2217)\u22121 in (C5), we equivalently get\n(A\u0303\u2212A\u2217)Q(n)+(L\u0303\u2212 L\u2217)Q(n)\u2212B\u2217 ( R(n) \u2212R\u2217(Q\u2217)\u22121Q(n) )\n\ufe38 \ufe37\ufe37 \ufe38 Y (n)\n\u2212 1 n\u03b7\nn\u2211\ni=1\nw(i)x(i)T\n\ufe38 \ufe37\ufe37 \ufe38 W (n)\n+Z\u0303=0. (7)\nWe can rewrite this equation as\nP\u2126c(L\u0303\u2212 L\u2217)Q(n) + (A\u0303\u2212A\u2217 + P\u2126(L\u0303\u2212 L\u2217))Q(n)\u2212 Y (n) \u2212W (n) + Z\u0303 = 0. (8)\nLet us only focus on the kth row of the system of equation (7). We can break down (7) on the kth row into two sets of linear equations as follows:\n(A\u0303\u2212A\u2217 + L\u0303\u2212 L\u2217)(k)Sk Q (n) SkSk = \u2212(L\u0303\u2212 L \u2217) (k) Sc k Q (n) Sc k Sk + Y (n) Sk +W (n) Sk \u2212 Z\u0303Sk (A\u0303\u2212A\u2217 + L\u0303\u2212 L\u2217)(k)Sk Q (n) SkSck = \u2212(L\u0303\u2212 L\u2217)(k)Sc k Q (n) Sc k Sc k + Y (n) Sc k +W (n) Sc k \u2212 Z\u0303Sc k .\n(9)\nBy Lemma 7, we have \u2225\u2225\u2225\u2225(L\u0303\u2212 L\u2217) (k) Sc k Q (n) Sc k Sk ( Q (n) SkSk )\u22121\u2225\u2225\u2225\u2225 \u221e \u2264 ( 1\u2212 \u03b8 2 )\u2225\u2225\u2225P\u2126c(L\u0303 \u2212 L\u2217) \u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225L\u0303\u2212 L\u2217 \u2225\u2225\u2225 \u221e . Since Z\u0303 satisfies (C3), we have \u2225\u2225\u2225PT (U\u0303 V\u0303 T )\u2212 U\u2217V \u2217T\n\u2225\u2225\u2225 2 \u2264 4\u03c1. By the properties of the\noracle problem (closeness of spaces T and T\u0303 ), we have \u2225\u2225\u2225L\u0303\u2212 L\u2217\n\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225PT\u0303 (L\u0303\u2212 L\u2217)\u2212 PT (L\u0303 \u2212 L\u2217) \u2225\u2225\u2225 2 + \u2225\u2225PT\u0303 (L\u2217)\u2212 PT (L\u2217) \u2225\u2225 2 + \u2225\u2225\u2225PT (L\u0303)\u2212 L\u2217 \u2225\u2225\u2225 2\n\u2264 \u03c1 \u2225\u2225\u2225L\u0303\u2212 L\u2217 \u2225\u2225\u2225 2 + \u03c1 \u2016L\u2217\u20162 + \u2225\u2225\u2225PT (U\u0303 V\u0303 T )\u2212 U\u2217V \u2217T \u2225\u2225\u2225 2 \u2225\u2225\u2225L\u0303\u2212 L\u2217 \u2225\u2225\u2225 2 \u2264 \u03c1 \u2225\u2225\u2225L\u0303\u2212 L\u2217\n\u2225\u2225\u2225 2 + \u03c1 \u2016L\u2217\u20162 + 4\u03c1 \u2225\u2225\u2225L\u0303\u2212 L\u2217 \u2225\u2225\u2225 2 .\nHence, \u2225\u2225\u2225L\u0303\u2212 L\u2217 \u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225L\u0303\u2212 L\u2217 \u2225\u2225\u2225 2 \u2264 \u03c1 1\u2212 5\u03c1 \u2016L \u2217\u20162 . (10)\nThus, from the first equation in (9) and Lemma 8, we get\n\u2225\u2225\u2225A\u0303\u2212A\u2217 \u2225\u2225\u2225 \u221e \u2264 2\u03c1 1\u2212 5\u03c1 \u2016L \u2217\u20162 + \u221a s Cmin (\u2225\u2225\u2225Y (n) \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225W (n) \u2225\u2225\u2225 \u221e + \u03bbA )\n\u2264 2\u03c1 1\u2212 5\u03c1 \u2016L \u2217\u20162 + (8\u2212 \u03b8)\u03bbA\n\u221a s\nCmin(4\u2212 \u03b8)\n\u2264   \u03b1\u03b8 Dmax ( 1 + DmaxCmin ) + (8\u2212 \u03b8) \u221a s Cmin(4\u2212 \u03b8)  \u03bbA.\n(11)\nThe last inequality follows from Lemmas 9 and 10. Substituting (A\u0303\u2212 A\u2217 + L\u0303\u2212 L\u2217)(k)Sk from the first equation in the second in (9), we get\nZ\u0303Sc k = \u2212(L\u0303\u2212 L\u2217)(k)Sc k Q (n) Sc k Sc k + Y (n) Sc k +W (n) Sc k\n\u2212 ( \u2212(L\u0303\u2212 L\u2217)(k)Sc\nk Q (n) Sc k Sk + Y (n) Sk +W (n) Sk \u2212 Z\u0303Sk\n)( Q\n(n) SkSk\n)\u22121 Q\n(n) SkSck .\nTaking maximum absolute value from both sides, using Lemmas 7,9 and 10, we get\n\u2225\u2225\u2225P\u2126c(Z\u0303) \u2225\u2225\u2225 \u221e \u2264 max k \u2225\u2225\u2225\u2225(L\u0303\u2212 L \u2217) (k) Sc k ( Q (n) Sc k Sc k \u2212Q(n)Sc k Sk ( Q (n) SkSk )\u22121 Q (n) SkSck )\u2225\u2225\u2225\u2225 \u221e + \u2225\u2225\u2225Y (n) \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225W (n) \u2225\u2225\u2225 \u221e\n+max k \u2225\u2225\u2225\u2225Q (n) Sc k Sk ( Q (n) SkSk )\u22121\u2225\u2225\u2225\u2225 \u221e,1 (\u2225\u2225\u2225Y (n) \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225W (n) \u2225\u2225\u2225 \u221e + \u03bbA )\n\u2264 max k\n\u2225\u2225\u2225\u2225(L\u0303\u2212 L \u2217) (k) Sc k ( Q (n) Sc k Sc k \u2212Q(n)Sc k Sk ( Q (n) SkSk )\u22121 Q (n) SkSck )\u2225\u2225\u2225\u2225 \u221e + \u03b8\u03bbA 4(4\u2212 \u03b8) + \u03b8\u03bbA 4(4\u2212 \u03b8)\n+ ( 1\u2212 \u03b8\n2\n)( \u03b8\u03bbA\n4(4\u2212 \u03b8) + \u03b8\u03bbA 4(4\u2212 \u03b8) + \u03bbA )\n\u2264 2\u03c1 1\u2212 5\u03c1\n( 1 +\nDmax Cmin\n) Dmax \u2016L\u2217\u20162 + ( 1\u2212 \u03b8\n4\n) \u03bbA \u2264 ( 1\u2212 (1\u2212 \u03b1)\u03b8\n4\n) \u03bbA.\nThe one to the last inequality follows from perturbation theory for the Schur complement of semi-definite matrices (See Lemma 13). The last inequality holds for our choice of \u03c1. Hence, condition (C2) is satisfied.\nTo show (C3) also holds, notice that from (8), we have \u2225\u2225\u2225PT (Z\u0303)\n\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225PT c ( (A\u0303+ L\u0303\u2212A\u2217 \u2212 L\u2217)Q(n) )\u2225\u2225\u2225 2 + \u2225\u2225\u2225Y (n) \u2225\u2225\u2225 2 + \u2225\u2225\u2225W (n) \u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225PT c ( (A\u0303+ L\u0303\u2212A\u2217 \u2212 L\u2217)Q(n) )\u2225\u2225\u2225 2 + \u03b8\u03bbA \u221a p 2(4\u2212 \u03b8) .\nThe last inequality follows from Lemmas 9 and 10 and the fact that Q(n) on the support is invertible for the given sample complexity due to Lemma 8.\nNext, notice that L\u2217 = B\u2217R\u2217(Q\u2217)\u22121 and hence the row-space of L\u2217 is the column/row space of Q\u2217 and consequently, for any matrix F \u2208 T , we have PT c(FQ\u2217) = 0. Thus, we have \u2225\u2225\u2225PT c ( (A\u0303+ L\u0303\u2212A\u2217 \u2212 L\u2217)Q(n)\n)\u2225\u2225\u2225 2\n= \u2225\u2225\u2225PT c ( (A\u0303+ L\u0303\u2212A\u2217 \u2212 L\u2217) ( Q(n) \u2212Q\u2217 ))\u2225\u2225\u2225 2 + \u2225\u2225\u2225PT c ( A\u0303+ L\u0303\u2212A\u2217 \u2212 L\u2217 ) Q\u2217 \u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225(A\u0303+ L\u0303\u2212A\u2217 \u2212 L\u2217) ( Q(n) \u2212Q\u2217\n)\u2225\u2225\u2225 2 + \u2225\u2225\u2225A\u0303+ L\u0303\u2212A\u2217 \u2212 L\u2217 \u2225\u2225\u2225 2 \u2016Q\u2217\u20162 \u221a p\n\u2264 (\u221a s \u2225\u2225\u2225A\u0303\u2212A\u2217 \u2225\u2225\u2225 \u221e + \u2225\u2225\u2225L\u0303\u2212 L\u2217 \u2225\u2225\u2225 2 )(\u221a p \u2225\u2225\u2225Q(n) \u2212Q\u2217 \u2225\u2225\u2225 \u221e +Dmax )\u221a p.\nFinally, from (11), (10) and Lemma 12, we get\n\u2225\u2225\u2225PT c(Z\u0303) \u2225\u2225\u2225 2 \u2264 ( \u03b8Cmin\u221ap 9s \u221a s +Dmax )  3\u03b1\u03b8 \u221a s\n2Dmax ( 1 + DmaxCmin\n) + (8\u2212 \u03b8)sCmin(4\u2212 \u03b8)\n \u03bbA \u221a p+ \u03b8\u03bbA \u221a p\n2(4\u2212 \u03b8)\n\u2264 \u03b8(1\u2212 \u03b1)\u03bbL.\nHence, condition (C4) is also satisfied. This concludes the proof of the lemma."}, {"heading": "D Concentration Results", "text": "In this section we prove the concentration results used throughout the paper. Before, we state the results, we want to introduce some useful inequalities used to get the results. By the dynamics of the system, we have\n[ x(i) u(i) ] = (I + \u03b7A\u2217)i [ x(0) u(0) ] + i\u22121\u2211\nl=0\n(I + \u03b7A\u2217)i\u2212l\u22121 w(l).\nLemma 7. For any S \u2286 {1, 2, . . . , p} with |S| \u2264 s and sample complexity n\u03b7 \u2265 3\u00d7106 s3 D2 \u03b82 C2min log ( 4((s+2r)p+r2) \u03b4 ) with high probability, we have \u2225\u2225\u2225\u2225Q (n) ScS ( Q(n)SS\n)\u22121\u2225\u2225\u2225\u2225 \u221e,1 \u2264 1\u2212 \u03b8 2 ,\nprovided that \u2225\u2225\u2225Q\u2217ScS (Q\u2217SS) \u22121 \u2225\u2225\u2225 \u221e,1 \u2264 1\u2212 \u03b8.\nProof. Using Lemma 8, it can be shown (see Lemma A.1 in [2] for example) that \u2225\u2225\u2225\u2225Q (n) ScS ( Q(n)SS )\u22121\u2225\u2225\u2225\u2225 \u221e,1 \u2264 \u2225\u2225\u2225Q\u2217ScS (Q\u2217SS)\u22121 \u2225\u2225\u2225 \u221e,1\n+ 3|S|\n\u221a |S|\nCmin \u2225\u2225\u2225Q(n) \u2212Q\u2217 \u2225\u2225\u2225 \u221e + 2|S|2\n\u221a |S|\nC2min\n\u2225\u2225\u2225Q(n) \u2212Q\u2217 \u2225\u2225\u2225 2\n\u221e .\nThe result follows from Lemma 11. This concludes the proof of the lemma.\nLemma 8. For any S \u2286 {1, 2, . . . , p} with |S| \u2264 s and sample complexity n\u03b7 \u2265 3\u00d7106 s3 D2 \u03b82 C2min log ( 4((s+2r)p+r2) \u03b4 ) with high probability, we have\n\u039bmin ( Q(n)SS ) \u2265 Cmin\n2 .\nProof. By the Courant-Fischer variational representation [13], we have\n\u039bmin ( Q(n)SS ) \u2265 \u039bmin (Q\u2217SS)\u2212 \u039bmax ( Q\u2217SS \u2212Q(n)SS )\n\u2265 Cmin \u2212 \u221a s \u2225\u2225\u2225Q\u2217 \u2212Q(n) \u2225\u2225\u2225 \u221e .\nThe last inequality follows from Lemma 11. This concludes the proof of the lemma.\nLemma 9. For \u03bbA \u2265 16(4\u2212\u03b8) \u221a \u2016x(0)\u201622+\u2016u(0)\u201622+( \u221a \u03b7+1)2\n\u03b8 \u221a D\n\u221a log ( 4((s+2r)p+r2)\n\u03b4\n)\nn\u03b7 with high probability, we have \u2225\u2225\u2225W(n)\n\u2225\u2225\u2225 \u221e \u2264 \u03b8\u03bbA 4(4\u2212 \u03b8) .\nProof. Let X(i) = [x(i) u(i)]T . According to the dynamics of the system, we have\nW(n) = 1 \u03b7n\nn\u22121\u2211\ni=0\nw(i)X(0)T ( (I + \u03b7A\u2217)i )T \ufe38 \ufe37\ufe37 \ufe38\nE1(i)\n+ 1\n\u03b7n\nn\u22121\u2211\ni=1\nw(i) i\u22121\u2211\nl=0\nw(l)T ( (I + \u03b7A\u2217)i\u2212l\u22121 )T\n\ufe38 \ufe37\ufe37 \ufe38 E2(i)\n.\nWe bound these two terms separately. Notice that w(i) is distributed N (0, \u03b7I) independent of x(0) and w(j)\u2019s. Given x(0), we have\nw(i)jE1(i) (k) \u223c N ( 0, \u03b7 ( E1(i) (k) )2) .\nBy stability assumption, we have ( E1(i) (k) )2 \u2264 \u03a32imax(\u2016x(0)\u201622 + \u2016u(0)\u201622) and hence,\nVAR\n( 1\n\u03b7n\nn\u22121\u2211\ni=0\nw(i)jE1(i) (k) ) \u2264 1\n\u03b72n2\nn\u22121\u2211\ni=0\nVAR ( w(i)jE1(i) (k) )\n\u2264 \u2016x(0)\u2016 2 2 + \u2016u(0)\u201622\n\u03b7n(1\u2212 \u03a32max) .\nConsequently, by standard concentration of Gaussian random variables and union bound, we get\nP [\u2225\u2225\u2225\u2225\u2225 1 \u03b7n n\u22121\u2211\ni=0\nw(i)E1(i) \u2225\u2225\u2225\u2225\u2225 \u221e \u2265 \u01eb ] \u2264 p\u2211 j=1 p\u2211 k=1 P [\u2223\u2223\u2223\u2223\u2223 1 \u03b7n n\u22121\u2211 i=0 w(i)jE1(i) (k) \u2223\u2223\u2223\u2223\u2223 \u2265 \u01eb ]\n\u2264 2 exp ( \u2212 \u01eb\n2(1\u2212 \u03a32max) 2 (\u2016x(0)\u201622 + \u2016u(0)\u201622) \u03b7n+ log((s+ 2r)p+ r2)\n) .\nWith similar analysis, we get\nVAR\n( 1\n\u03b7n\nn\u22121\u2211\ni=0\nw(i)jE2(i) (k) ) \u2264 1\n\u03b72n2\nn\u22121\u2211\ni=0\nVAR ( w(i)jE2(i) (k) )\n\u2264 (\u221a \u03b7 + 1 )2\n\u03b7n(1\u2212 \u03a32max) .\nThe last inequality follows from the concentration of \u03c72 random variables [16], in particular,\nP\n[ 1\n\u03b7n\nn\u22122\u2211\nl=0\n\u2016w(l)jE2(l)(k)\u201622 \u2265 ( 1 + \u221a \u03b7 )2\n1\u2212 \u03a32max\n] \u2264 exp ( \u22121 2 \u03b7n+ log((s+ 2r)p+ r2) ) .\nFinally, we get\nP [\u2225\u2225\u2225\u2225\u2225 1 \u03b7n n\u22121\u2211\ni=0\nw(i)E2(i) \u2225\u2225\u2225\u2225\u2225 \u221e \u2265 \u01eb ] \u2264 p\u2211 j=1 p\u2211 k=1 P [\u2223\u2223\u2223\u2223\u2223 1 \u03b7n n\u22121\u2211 i=0 w(i)jE2(i) (k) \u2223\u2223\u2223\u2223\u2223 \u2265 \u01eb ]\n\u2264 2 exp ( \u2212 \u01eb\n2(1\u2212 \u03a32max) 2 (\u221a \u03b7 + 1 )2 \u03b7n+ log((s+ 2r)p+ r2)\n) .\nThe result follows for \u01eb = \u03b8\u03bbA8(4\u2212\u03b8) . This concludes the proof of the lemma.\nLemma 10. For \u03bbA \u2265 640(4\u2212\u03b8)\u2016B\u2217\u2016\u221e,1\n( Dmax Cmin +1 )\n\u03b8D\n\u221a log ( 4((s+2r)p+r2)\n\u03b4\n)\nn\u03b7 with high probability, we have \u2225\u2225\u2225Y (n)\n\u2225\u2225\u2225 \u221e \u2264 \u03b8\u03bbA 4(4\u2212 \u03b8) .\nProof. We can establish\nY (n) = B\u2217 ( R(n) \u2212R\u2217 )\n\ufe38 \ufe37\ufe37 \ufe38 +B\u2217R\u2217(Q\u2217)\u22121\n( Q\u2217 \u2212Q(n) )\n\ufe38 \ufe37\ufe37 \ufe38 .\nWe bound these two terms separately. For the first term, we have \u2225\u2225\u2225B\u2217 ( R\u2217 \u2212R(n)\n)\u2225\u2225\u2225 \u221e \u2264 \u2016B\u2217\u2016\u221e,1 \u2225\u2225\u2225Q\u2217 \u2212Q(n) \u2225\u2225\u2225 \u221e .\nFor the second term, we have \u2225\u2225\u2225B\u2217R\u2217(Q\u2217)\u22121 ( Q\u2217 \u2212Q(n)\n)\u2225\u2225\u2225 \u221e \u2264 \u2225\u2225B\u2217R\u2217(Q\u2217)\u22121 \u2225\u2225 \u221e,1 \u2225\u2225\u2225Q\u2217 \u2212Q(n) \u2225\u2225\u2225 \u221e\n\u2264 \u2016B\u2217\u2016\u221e,1 \u03c3max ( R\u2217(Q\u2217)\u22121 ) \u2225\u2225\u2225Q\u2217 \u2212Q(n) \u2225\u2225\u2225 \u221e \u2264 \u2016B\u2217\u2016\u221e,1 Dmax Cmin \u2225\u2225\u2225Q\u2217 \u2212Q(n) \u2225\u2225\u2225 \u221e .\nThe result follows from Lemma 12. This concludes the proof of the lemma.\nLemma 11. For sample complexity n\u03b7 \u2265 3\u00d7106 s3 D2 \u03b82 C2min\nlog (\n4((s+2r)p+r2) \u03b4\n) with high probability,\nwe have \u2225\u2225\u2225Q\u2217 \u2212Q(n) \u2225\u2225\u2225 \u221e \u2264 \u03b8 Cmin 9 s \u221a s .\nProof. Let X(i) = [x(i) u(i)]T . Let \u00b5(i) = E [X(i)] (clearly, \u00b5(\u221e) = 0). We have\nQ(n) \u2212Q\u2217 = 1 n\nn\u22121\u2211\ni=0\n\u00b5(i)\u00b5(i)T\n\ufe38 \ufe37\ufe37 \ufe38 +\n1\nn\nn\u22121\u2211\ni=0\nE [ (X(i)\u2212 \u00b5(i)) (X(i)\u2212 \u00b5(i))T ] \u2212Q\u2217\n\ufe38 \ufe37\ufe37 \ufe38 E1\n+ 1\nn\nn\u22121\u2211\ni=0\nE [ (X(i)\u2212 \u00b5(i)) (X(i)\u2212 \u00b5(i))T ] \u2212 1\nn\nn\u22121\u2211\ni=0\n(X(i)\u2212 \u00b5(i)) (X(i)\u2212 \u00b5(i))T\n\ufe38 \ufe37\ufe37 \ufe38 E2\n.\nWe bound these three terms, separately. For the first term, we have \u2225\u2225\u2225\u2225\u2225 1 n n\u22121\u2211\ni=0\n\u00b5(i)\u00b5(i)T \u2225\u2225\u2225\u2225\u2225 \u221e \u2264 1 n n\u22121\u2211 i=0 \u03a32imax ( \u2016x(0)\u201622 + \u2016u(0)\u2016 2 2 )\n\u2264 \u2016x(0)\u2016 2 2 + \u2016u(0)\u2016 2 2\nn(1\u2212 \u03a32max) .\nFor the second term, notice that by independency assumption on w, we have\n1\nn\nn\u22121\u2211\ni=0\nE [ (X(i)\u2212 \u00b5(i)) (X(i)\u2212 \u00b5(i))T ] = \u03b7\nn\nn\u22121\u2211\ni=0\ni\u22121\u2211\nl=0\n(I + \u03b7A\u2217)2l\n= \u03b7\nn\nn\u22121\u2211\ni=0\n( I \u2212 (I + \u03b7A\u2217)2i )( I \u2212 (I + \u03b7A\u2217)2 )\u22121\n= \u03b7 ( n\u2212 1 n I \u2212 (I + \u03b7A\u2217)2 + 1 n (I + \u03b7A\u2217)2n )( I \u2212 (I + \u03b7A\u2217)2 )\u22122 .\nOn the other hand, we have Q\u2217 = E [ lim i\u2192\u221e (X(i)\u2212 \u00b5(i)) (X(i)\u2212 \u00b5(i))T ] = lim i\u2192\u221e E [ (X(i)\u2212 \u00b5(i)) (X(i)\u2212 \u00b5(i))T ]\n= lim i\u2192\u221e \u03b7\ni\u22121\u2211\nl=0\n(I + \u03b7A\u2217)2l\n= lim i\u2192\u221e\n\u03b7 ( I \u2212 (I + \u03b7A\u2217)2i )( I \u2212 (I + \u03b7A\u2217)2 )\u22121\n= \u03b7 ( I \u2212 (I + \u03b7A\u2217)2 )\u22121 .\nIn the above inequalities, we interchanged limit and expectation as a result of Gaussianity assumption and the stability of the system. Finally we get\n\u2016E1\u2016\u221e \u2264 \u03b7(1\u2212 \u03a32nmax) n(1\u2212 \u03a32max)2 \u2264 \u03b7 n(1\u2212 \u03a32max)2 .\nTo bound the third term, notice that\n1\nn\nn\u22121\u2211\ni=0\n(X(i)\u2212 \u00b5(i)) (X(i)\u2212 \u00b5(i))T = n\u22121\u2211\nj=0\n(I+\u03b7A\u2217)j   n\u2212 j n 1 n\u2212 j n\u2212j\u22121\u2211\ni=0\nw(i)w(i)T\n\ufe38 \ufe37\ufe37 \ufe38 Vj\n  ( (I + \u03b7A\u2217)j )T .\nBy Lemma 1 in [21], we have\nP [ \u2016Vj \u2212 \u03b7I\u2016\u221e > n n\u2212 j \u01eb ] \u2264 4 exp ( \u2212 \u01eb 2n 3200\u03b7(n\u2212 j)n+ log ( (s+ 2r)p+ r2 )) .\nConsequently, we get\nP [ n\u2212 j n \u03a32(n\u2212j\u22121)max \u2016Vj \u2212 \u03b7I\u2016\u221e > \u03a32(n\u2212j\u22121)max \u01eb ] \u2264 4 exp ( \u2212 \u01eb 2n 3200\u03b7(n\u2212 j)n+ log ( (s+ 2r)p+ r2 )) .\nThus, we conclude\nP [ \u2016E2\u2016\u221e >\n1\n1\u2212 \u03a32max \u01eb\n] \u2264 4 exp ( \u2212 \u01eb 2\n3200\u03b7 n+ log\n( (s+ 2r)p+ r2 )) .\nWe want this probability to be less than \u03b4. Putting all thre parts together, we get\n\u2225\u2225\u2225Q\u2217 \u2212Q(n) \u2225\u2225\u2225 \u221e \u2264 1 1\u2212 \u03a32max\n( \u03b7(1 \u2212 \u03a32max)\u22121 + \u2016x(0)\u201622 + \u2016u(0)\u2016 2 2\nn + \u01eb\n) . (12)\nFor n\u03b7 \u2265 18 s \u221a s\nD \u03b8 Cmin\n( D\u22121 + \u2016x(0)\u201622 + \u2016u(0)\u2016 2 2 ) and \u01eb = \u03b7D\u03b8Cmin 18 s \u221a s , the result follows, provide\nthat the probabilities go to zero, i.e.,\nn\u03b7 \u2265 3\u00d7 10 6 s3\nD2 \u03b82 C2min log\n( 4((s+ 2r)p+ r2)\n\u03b4\n) .\nFor large enough values of p, this lower bound dominates the earlier lower bound of n\u03b7, hence, we ignore that one. This concludes the proof of the lemma.\nLemma 12. For \u03bbA \u2265 640(4\u2212\u03b8)\u2016B\u2217\u2016\u221e,1\n( Dmax Cmin +1 )\n\u03b8D\n\u221a log ( 4((s+2r)p+r2)\n\u03b4\n)\nn\u03b7 , with high probability, we have \u2225\u2225\u2225Q\u2217 \u2212Q(n)\n\u2225\u2225\u2225 \u221e\n\u2264 \u03b8\u03bbA 4(4\u2212 \u03b8) \u2016B\u2217\u2016\u221e,1 ( Dmax Cmin + 1 ) .\nProof. According to (12), the result follows if \u01eb = \u03b8\u03bbA D 8(4\u2212\u03b8)\u2016B\u2217\u2016\u221e,1 ( Dmax Cmin +1 ) assuming p is large enough.\nLemma 13. For sample complexity n\u03b7 \u2265 3\u00d710 6(Dmax+2Cmin) D2(Dmax+Cmin) log ( 4((s+2r)p+r2) \u03b4 ) with high probability, we have \u2225\u2225\u2225\u2225Q (n) Sc k Sc k \u2212Q(n)Sc k Sk ( Q (n) SkSk )\u22121 Q (n) SkSck\n\u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nS(n)\n\u2264 2(1 + DmaxCmin )Dmax.\nProof. Since Q\u2217 and Q(n) are positive semi-definite matrices and \u2225\u2225\u2225S(n)\n\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225S(n) \u2212 S\u2217 \u2225\u2225\u2225 2 + \u2016S\u2217\u20162\nThe result directly follows from Theorem in [23] for \u01eb := \u2016Q(n) \u2212 Q\u2217\u2016\u221e = Dmax+Cmin4(Dmax+2Cmin) considering the fact that \u2016S\u2217\u20162 \u2264 Dmax ( 1 + DmaxCmin ) ."}, {"heading": "E Proof of the Continuous Time Theorem", "text": "Proof. Denote X(t) = [x(t)u(t)]T and let\nQ\u0302 = 1 T\n\u222b T\nt=0\nX(t)X(t)Tdt W\u0302 = 1 T\n\u222b T\nt=0\ndw(t)X(t)T .\nHaving the result for the discrete time system, it suffices (see proof of Theorem 1.1 in [2] for more details) to show that for a given continuous time system, there exists a discrete time system with Q(n) and W(n) such that almost surely,\nQ(n) \u2212\u2192 Q\u0302 W(n) \u2212\u2192 W\u0302 , as n \u2192 \u221e for a fixed T = n\u03b7 (and hence, \u03b7 \u2192 0). Let Q\u2217 be the matrix satisfying the continuous time Lyapunov stability equation A\u2217Q\u2217 + Q\u2217A\u2217T + I = 0 and Q\u2217(\u03b7) be the matrix satisfying the discrete time Lyapunov stability equation A\u2217Q\u2217(\u03b7) +Q\u2217(\u03b7)A\u2217T + \u03b7A\u2217Q\u2217(\u03b7)A\u2217T + I = 0. It is easy to see that Q\u2217(\u03b7) \u2192 Q\u2217 as \u03b7 \u2192 0 by the uniqueness of the stationary distribution. Moreover, by Lemma 11, we know that Q(n) \u2192 Q\u2217(\u03b7) as n \u2192 \u221e. Now, let the initial state of the discrete time system be\nX(i = 0) = (Q\u2217(\u03b7))1/2 (Q\u2217)\u22121/2 X(t = 0), and the noise w(i) = w(t = i\u03b7) \u2212 w(t = (i \u2212 1)\u03b7). It can be easily checked that w(i) \u223c N (0, \u03b7I) if the continuous time w(t) is a Brownian motion. Thus, x(i) and x(t) are coupled and the almost sure convergence, follows from the convergence of random walks to Brownian motions [19]. This concludes the proof of the theorem for continuous time systems."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper considers the problem of learning, from samples, the depen-<lb>dency structure of a system of linear stochastic differential equations,<lb>when some of the variables are latent. In particular, we observe the time<lb>evolution of some variables, and never observe other variables; from this,<lb>we would like to find the dependency structure between the observed vari-<lb>ables \u2013 separating out the spurious interactions caused by the (marginal-<lb>izing out of the) latent variables\u2019 time series. We develop a new method,<lb>based on convex optimization, to do so in the case when the number of<lb>latent variables is smaller than the number of observed ones. For the case<lb>when the dependency structure between the observed variables is sparse,<lb>we theoretically establish a high-dimensional scaling result for structure re-<lb>covery. We verify our theoretical result with both synthetic and real data<lb>(from the stock market).", "creator": "LaTeX with hyperref package"}}}