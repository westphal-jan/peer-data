{"id": "1106.4574", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2011", "title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods", "abstract": "Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization problems. We study how such algorithms can be improved using accelerated gradient methods. We provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this deficiency, enjoys a uniformly superior guarantee and works well in practice. The best-known approach for optimizing stochastic convex optimization is to generate a high-throughput optimization approach to efficiently parallelize the computation and transfer the computation to new solutions. The simplest method for using the high-throughput algorithm is to generate a low-throughput optimization method in the same way as the standard gradient method. The best-known approach is to generate a high-throughput algorithm to efficiently parallelize the computation and transfer the computation to new solutions. The best-known approach for optimizing stochastic convex optimization is to generate a low-throughput optimization method in the same way as the standard gradient method. The best-known approach is to generate a high-throughput algorithm to efficiently parallelize the computation and transfer the computation to new solutions. The best-known approach is to generate a high-throughput algorithm to efficiently parallelize the computation and transfer the computation to new solutions.\n\n\n\n\n\nThe current research using a gradient method was conducted in the absence of a high-throughput algorithm to achieve the best performance. The proposed high-throughput algorithm could be used to accelerate linear optimization, by applying gradient methods on the basis of the normal gradient, thereby maximizing its precision. The high-throughput algorithm is a direct and powerful way to maximize the performance and performance of linear optimization.\nThe following calculations (1) have been proposed, by authors:\nThe following calculations (1) were performed in a single linear solution, based on the number of steps in which the method is applied.\nIn a simple approach to minimizing a single linear solution, the method is applied by the factor of the initial input. The calculation has a nonlinear time of 1.25 ms, with a constant of 0.2 ms. The method has a nonlinear time of 0.1 ms, with a constant of 0.2 ms. The method can be applied by a nonlinear time of 1.25 ms, with a constant of 0.2 ms. The method can be applied by a nonlinear time of 1.25 ms, with a constant of 0.2 ms", "histories": [["v1", "Wed, 22 Jun 2011 20:59:20 GMT  (37kb,D)", "http://arxiv.org/abs/1106.4574v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew cotter", "ohad shamir", "nati srebro", "karthik sridharan"], "accepted": true, "id": "1106.4574"}, "pdf": {"name": "1106.4574.pdf", "metadata": {"source": "CRF", "title": "Better Mini-Batch Algorithms via Accelerated Gradient Methods", "authors": ["Andrew Cotter", "Ohad Shamir", "Karthik Sridharan"], "emails": ["cotter@ttic.edu", "ohadsh@microsoft.com", "nati@ttic.edu", "karthik@ttic.edu"], "sections": [{"heading": null, "text": "1 Introduction\nWe consider a stochastic convex optimization problem of the form\nmin w\u2208W L(w),\nwhere L(w) = Ez [`(w, z)],\nand optimization is based on an empirical sample of instances z1, . . . , zm. We focus on objectives `(w, z) that are non-negative, convex and smooth in their first argument (i.e. have a Lipschitz-continuous gradient). The classical learning application is when z = (x, y) and `(w, (x, y)) is a prediction loss. In recent years, there has been much interest in developing efficient first-order stochastic optimization methods for these problems, such as stochastic mirror descent [2, 6] and stochastic dual averaging [9, 16]. These methods are characterized by incremental updates based on subgradients \u2202`(w, zi) of individual instances, and enjoy the advantages of being highly scalable and simple to implement.\nAn important limitation of these methods is that they are inherently sequential, and so problematic to parallelize. A popular way to speed-up these algorithms, especially in a parallel setting, is via mini-batching, where the incremental update is performed on an average of the subgradients with respect to several instances at a time, rather than a single instance (i.e., 1b \u2211b j=1 \u2202`(w, zi+j)). The gradient computations for each minibatch can be parallelized, allowing these methods to perform faster in a distributed framework (see for\nar X\niv :1\n10 6.\n45 74\nv1 [\ncs .L\nG ]\nAlgorithm 1 Stochastic Gradient Descent with Mini-Batching (SGD)\nParameters: Step size \u03b7, mini-batch size b. Input: Sample z1, . . . , zm w1 = 0 for i = 1 to n = m/b do\nLet `i(wi) = 1 b \u2211bi t=b(i\u22121)+1 `(wi, zt) w\u2032i+1 := wi \u2212 \u03b7\u2207`i(wi)) wi+1 := PW(w \u2032 i+1)\nend for Return w\u0304 = 1n \u2211n i=1 wi\ninstance [11]). Recently, [10] has shown that a mini-batching distributed framework is capable of attaining asymptotically optimal speed-up in general (see also [1]).\nA parallel development has been the popularization of accelerated gradient descent methods [7, 8, 15, 5]. In a deterministic optimization setting and for general smooth convex functions, these methods enjoy a rate of O(1/n2) (where n is the number of iterations) as opposed to O(1/n) using standard methods. However, in a stochastic setting (which is the relevant one for learning problems), the rate of both approaches have an O(1/ \u221a n) dominant term in general, so the benefit of using accelerated methods for learning problems is not obvious.\nAlgorithm 2 Accelerated Gradient Method (AG)\nParameters: Step sizes (\u03b3i, \u03b2i), mini-batch size b Input: Sample z1, . . . , zm w = 0 for i = 1 to n = m/b do\nLet `i(wi) := 1 b \u2211bi t=b(i\u22121)+1 `(w, zt) wmdi := \u03b2 \u22121 i wi + (1\u2212 \u03b2 \u22121 i )w ag i w\u2032i+1 := w md i \u2212 \u03b3i\u2207`i(wmdi ) wi+1 := PW(w \u2032 i+1) wagi+1 \u2190 \u03b2 \u22121 i wi+1 + (1\u2212 \u03b2 \u22121 i )w ag i\nend for Return wagn\nIn this paper, we study the application of accelerated methods for mini-batch algorithms, and provide theoretical results, a novel algorithm, and empirical experiments. The main resulting message is that by using an appropriate accelerated method, we obtain significantly better stochastic optimization algorithms in terms of convergence speed. Moreover, in certain regimes acceleration is actually necessary in order to allow a significant speedups. The potential benefit of acceleration to mini-batching has been briefly noted in [4], but here we study this issue in much more depth. In particular, we make the following contributions:\n\u2022 We develop novel convergence bounds for the standard gradient method, which refines the result of [10, 4] by being dependent on L(w?) = infw\u2208W L(w), the expected loss of the best predictor in our class. For example, we show that in the regime where the desired suboptimality is comparable or larger than L(w?), including in the separable case L(w?) = 0, mini-batching does not lead to significant speed-ups with standard gradient methods.\n\u2022 We develop a novel variant of the stochastic accelerated gradient method [5], which is optimized for a mini-batch framework and implicitly adaptive to L(w?).\n\u2022 We provide an analysis of our accelerated algorithm, refining the analysis of [5] by being dependent on L(w?), and show how it always allows for significant speed-ups via mini-batching, in contrast to standard gradient methods. Moreover, its performance is uniformly superior, at least in terms of theoretical upper bounds.\n\u2022 We provide an empirical study, validating our theoretical observations and the efficacy of our new method.\n2 Preliminaries\nWe consider stochastic convex optimization problems over some convex domainW. Here, we takeW to be a convex subset of a Euclidean space, and use \u2016w\u2016 to denote the standard Euclidean norm. In the Appendix, we state and prove the result in a more general setting, where W is a convex subset of a Banach space, and \u2016w\u2016 can be an arbitrary norm.(subset of Euclidean case, see Appendix for the more general Banach space case), using an i.i.d. sample z1, . . . , zm \u2208 Z drawn from some fixed distribution.\nThroughout this paper we assume that the instantaneous loss ` :W\u00d7Z 7\u2192 R is convex in its first argument and non-negative. We further assume that the loss is H-smooth in its first argument for each z \u2208 Z. That is for every z \u2208 Z and w,w\u2032 \u2208 W,\n\u2016\u2207`(w, z)\u2212\u2207`(w\u2032, z)\u2016 \u2264 H \u2016w \u2212w\u2032\u2016\n(for more general Banach space case, the norm on the left hand side is the dual norm). Let us denote\nL(w) := Ez [`(w, z)]\nWe wish to minimize L(w) over convex domain W. We will provide guarantees on L(w) relative to L(w?) at some w? \u2208 W, where the guarantees also depend on \u2016w?\u2016. We could choose w? := arg minw\u2208W L(w), though our results hold for any w? \u2208 W, and in some cases we might choose to compete with a low-norm w? that is not optimal in W.\nThe behavior of the accelerated gradient method also depends on the radius of W, defined as:\nD := sup w\u2208W\n\u2016w\u2016\nWe discuss two stochastic optimization approaches to deal with this problem: stochastic gradient descent (SGD), and accelerated gradient methods (AG). In a mini-batch setting, both approaches iteratively average sub-gradients with respect to several instances, and use this average to update the predictor. However, the update is done in different ways. In the Appendix, we also provide the form of the update in the more general mirror descent setting, where \u2016w\u2016 is an arbitrary norm.\nThe stochastic gradient descent algorithm is summarized as Algorithm 1. In the pseudocode, PW refers to the projection on to the ball W (under the Euclidean distance). The accelerated gradient method (e.g., [5]) is summarized as Algorithm 2.\nIn terms of existing results, for the SGD algorithm we have [4, Section 5.1]\nE [L(w\u0304)]\u2212 L(w?) \u2264 O\n(\u221a 1\nm +\nb\nm\n) ,\nwhereas for an accelerated gradient algorithm, we have [5]\nE [L(wagn )]\u2212 L(w?) \u2264 O\n(\u221a 1\nm +\nb2\nm2\n) ,\nwhere in both cases the dependence on D,H and \u2016w?\u2016 is suppressed. The above bounds suggest that, as long as b = o( \u221a m), both methods allow us to use a large mini-batch size b without significantly degrading the performance of either method. This allows the number of iterations n = m/b to be smaller, potentially resulting in faster convergence speed. However, these bounds do not show that accelerated methods have a significant advantage over the SGD algorithm, at least when b = o( \u221a m), since both have the same first-order term 1/ \u221a m. To understand the differences between these two methods better, we will need a more refined analysis, to which we now turn.\n3 Convergence Guarantees\nThe following theorems provide a refined convergence guarantee for the SGD algorithm and the AG algorithm, which improves on the analysis of [10, 4, 5] by being explicitly dependent on L(w?), the expected loss of the best predictor w? in W.\nTheorem 1. For any w? \u2208 W, using Stochastic Gradient Descent with a step size of \u03b7 = min  12H , \u221a b\u2016w\u2217\u20162 L(w?)Hn\n1+ \u221a H\u2016w\u2217\u20162 L(w?)bn , we have:\nE [L(w\u0304)]\u2212 L(w?) \u2264\n\u221a 64H \u2016w?\u20162 L(w?)\nbn +\n4L(w?) + 4H \u2016w?\u20162\nn +\n8H \u2016w?\u20162\nbn\nNote that the radius D does not appear in the above bound, which depends only on \u2016w?\u2016. This means that W could be unbounded, perhaps even the entire space, and a projection step for SGD is not really crucial. The step size, of course, still depends on \u2016w?\u2016.\nTheorem 2. For any w? \u2208 W, using Accelerated Gradient with step size parameters \u03b2i = i+12 , \u03b3i = \u03b3i p where\n\u03b3 = min\n{ 1\n4H ,\n\u221a b\u2016w\u2217\u20162 348HL(w?)(n\u22121)2p+1 , ( b 1044H(n\u22121)2p ) p+1 2p+1 ( \u2016w\u2217\u20162 4H\u2016w\u2217\u20162+ \u221a 4H\u2016w\u2217\u20162L(w?) ) p 2p+1 } (1)\nand\np = min { max { log(b)\n2 log(n\u2212 1) ,\nlog log(n)\n2 (log(b(n\u2212 1))\u2212 log log(n))\n} , 1 } , (2)\nas long as n \u2265 783, we have:\nE [L(wagn )]\u2212 L(w?) \u2264 117\n\u221a H \u2016w?\u20162 L(w?)\nbn + 367H \u2016w?\u20164/3D 23\u221a bn + 546HD2\n\u221a log(n)\nbn +\n5H \u2016w?\u20162\nn2 \u2264 117 \u221a HD2L(w?)\nbn + 367HD2\u221a bn + 546HD2\n\u221a log(n)\nbn +\n5HD2\nn2\nUnlike for SGD, notice that the bound for the AG method above does depend on D, and a projection step is necessary for our analysis. However it is worth noting that D only appears in terms of order at least 1/n, and appears only mildly in the 1/( \u221a bn) term, suggesting some robustness to the radius D.\nWe emphasize that Theorem 2 gives more than a theoretical bound: it actually specifies a novel accelerated gradient strategy, where the step size \u03b3i scales polynomially in i, in a way dependent on the minibatch size b and L(w?). While L(w?) may not be known in advance, it does have the practical implication that choosing \u03b3i \u221d ip for some p < 1, as opposed to just choosing \u03b3i \u221d i as in [5]), might yield superior results.\nWe now provide a proof sketch of Theorems 1 and 2. A more general statement of the Theorems as well as a complete proof can be found in the Appendix.\nThe key observation used for analyzing the dependence on L(w?) is that for any non-negative H-smooth convex function f :W 7\u2192 R, we have [13]:\n\u2016\u2207f(w)\u2016 \u2264 \u221a 4Hf(w) (3)\nThis self-bounding property tells us that the norm of the gradient is small at a point if the loss is itself small at that point. This self-bounding property has been used in [14] in the online setting and in [13] in the stochastic setting to get better (faster) rates of convergence for non-negative smooth losses. The implication of this observation are that for any w \u2208 W, \u2016\u2207L(w)\u2016 \u2264 \u221a 4HL(w) and \u2200z \u2208 Z, \u2016`(w, z)\u2016 \u2264 \u221a 4H`(w, z).\nProof sketch for Theorem 1. The proof for the stochastic gradient descent bound is mainly based on the proof techniques in [5] and its extension to the mini-batch case in [10]. Following the line of analysis in [5], one can show that\nE [ 1 n n\u2211 i=1 L(wi) ] \u2212 L(w?) \u2264 \u03b7n\u22121 n\u22121\u2211 i=1 E [ \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162 ] + D 2 2\u03b7(n\u22121)\nIn the case of [5], E [\u2016\u2207L(wi)\u2212\u2207`i(wi)\u2016] is bounded by the variance, and that leads to the final bound provided in [5] (by setting \u03b7 appropriately). As noticed in [10], in the minibatch setting we have \u2207`i(wi) = 1 b \u2211bi t=b(i\u22121)+1 `(wi, zt) and so one can further show that\nE [ 1 n n\u2211 i=1 L(wi) ] \u2212 L(w?) \u2264 \u03b7b2(n\u22121) n\u22121\u2211 i=1 ib\u2211 t=\n(i\u22121)b+1\nE \u2016\u2207L(wi)\u2212\u2207`(wi, zt)\u20162 + D 2\n2\u03b7(n\u22121) (4)\nIn [10], each of \u2016\u2207L(wi)\u2212\u2207`(wi, zt)\u2016 is bounded by \u03c30 and so setting \u03b7, the mini-batch bound provided there is obtained. In our analysis we further use the self-bounding property to (4) and get that\nE [ 1 n n\u2211 i=1 L(wi) ] \u2212 L(w?) \u2264 16H\u03b7b(n\u22121) n\u22121\u2211 i=1 E [L(wi)] + D 2 2\u03b7(n\u22121)\nrearranging and setting \u03b7 appropriately gives the final bound.\nProof sketch for Theorem 2. The proof of the accelerated method starts in a similar way as in [5]. For the \u03b3i\u2019a and \u03b2i\u2019s mentioned in the theorem, following similar lines of analysis as in [5] we get the preliminary bound\nE [L(wagn )]\u2212 L(w?) \u2264 2\u03b3 (n\u2212 1)p+1 n\u22121\u2211 i=1 i2p E [\u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252]+ D2\u03b3(n\u2212 1)p+1\nIn [5] the step size \u03b3i = \u03b3(i+ 1)/2 and \u03b2i = (i+ 1)/2 which effectively amounts to p = 1 and further similar to the stochastic gradient descent analysis. Furthermore, each E [\u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252] is assumed to be bounded by some constant, and thus leads to the final bound provided in [5] by setting \u03b3 appropriately. On the other hand, we first notice that due to the mini-batch setting, just like in the proof of stochastic gradient descent,\nE [L(wagn )]\u2212 L(w?) \u2264 2\u03b3\nb2(n\u22121)p+1 n\u22121\u2211 i=1 i2p ib\u2211 t=\nb(i\u22121)+1\nE [\u2225\u2225\u2207L(wmdi )\u2212\u2207`(wmdi , zt)\u2225\u22252]+ D2\u03b3(n\u22121)p+1\nUsing smoothness, the self bounding property some manipulations, we can further get the bound\nE [L(wagn )]\u2212 L(w?) \u2264 64H\u03b3 b(n\u22121)1\u2212p n\u22121\u2211 i=1 (E [L(wagi )]\u2212 L(w ?)) + 64H\u03b3L(w ?)(n\u22121)p b\n+ D 2 \u03b3(n\u22121)p+1 + 32HD2 b(n\u22121)\nNotice that the above recursively bounds E [L(wagn )]\u2212L(w?) in terms of \u2211n\u22121 i=1 (E [L(w ag i )]\u2212 L(w?)). While unrolling the recursion all the way down to 2 does not help, we notice that for any w \u2208 W, L(w)\u2212L(w?) \u2264 12HD2 + 3L(w?). Hence we unroll the recursion to M steps and use this inequality for the remaining sum. Optimizing over number of steps up to which we unroll and also optimizing over the choice of \u03b3, we get the bound,\nE [L(wagn )]\u2212 L(w?) \u2264 \u221a 1648HD2L(w?) b(n\u22121) + 348(6HD2+2L(w?)) b(n\u22121) (b(n\u2212 1)) p p+1 + 32HD 2 b(n\u22121)\n+ 4HD 2 (n\u22121)p+1 + 36HD2 b(n\u22121) log(n)\n(b(n\u22121)) p 2p+1\nUsing the p as given in the theorem statement, and few simple manipulations, gives the final bound.\n4 Optimizing with Mini-Batches\nTo compare our two theorems and understand their implications, it will be convenient to treat H and D as constants, and focus on the more interesting parameters of sample size m, minibatch size b, and optimal expected loss L(w?). Also, we will ignore the logarithmic factor in Theorem 2, since we will mostly be interested in significant (i.e. polynomial) differences between the two algorithms, and it is quite possible that this logarithmic factor is merely an artifact of our analysis. Using m = nb, we get that the bound for the SGD algorithm is\nE [L(w\u0304)]\u2212 L(w?) \u2264 O\u0303\n(\u221a L(w?)\nbn +\n1\nn\n) = O\u0303 (\u221a L(w?)\nm +\nb\nm\n) , (5)\nand the bound for the accelerated gradient method we propose is\nE [L(wagn )]\u2212 L(w?) \u2264 O\u0303\n(\u221a L(w?)\nbn + 1\u221a bn + 1 n2\n) = O\u0303 (\u221a L(w?)\nm +\n\u221a b m + b2 m2\n) . (6)\nTo understand the implication these bounds, we follow the approach described in [3, 12] to analyze large-scale learning algorithms. First, we fix a desired suboptimality parameter , which measures how close to L(w?) we want to get. Then, we assume that both algorithms are ran till the suboptimality of their outputs is at most . Our goal would be to understand the runtime each algorithm needs, till attaining suboptimality , as a function of L(w?), , b.\nTo measure this runtime, we need to discern two settings here: a parallel setting, where we assume that the mini-batch gradient computations are performed in parallel, and a serial setting, where the gradient computations are performed one after the other. In a parallel setting, we can take the number of iterations n as a rough measure of the runtime (note that in both algorithms, the runtime of a single iteration is comparable). In a serial setting, the relevant parameter is m, the number of data accesses.\nTo analyze the dependence on m and n, we upper bound (5) and (6) by , and invert them to get the bounds on m and n. Ignoring logarithmic factors, for the SGD algorithm we get\nn \u2264 1\n( L(w?)\n\u00b7 1 b + 1\n) m \u2264 1 ( L(w?) + b ) , (7)\nand for the AG algorithm we get\nn \u2264 1\n( L(w?)\n\u00b7 1 b + 1\u221a b + \u221a\n) m \u2264 1 ( L(w?) + \u221a b+ b \u221a ) . (8)\nFirst, let us compare the performance of these two algorithms in the parallel setting, where the relevant parameter to measure runtime is n. Analyzing which of the terms in each bound dominates, we get that for the SGD algorithm, there are 2 regimes, while for the AG algorithm, there are 2-3 regimes depending on the relationship between L(w?) and . The following two tables summarize the situation (again, ignoring constants):\nSGD Algorithm\nRegime n b \u2264 \u221a L(w?)m L(w\n?) 2b b \u2265 \u221a L(w?)m 1\nAG Algorithm\nRegime n\n\u2264 L(w?)2 b \u2264 L(w?)1/4m3/4 L(w ?) 2b\nb \u2265 L(w?)1/4m3/4 1\u221a\n\u2265 L(w?)2 b \u2264 L(w?)m L(w ?) 2b\nL(w?)m \u2264 b \u2264 m2/3 1 \u221a b\nb \u2265 m2/3 1\u221a\nFrom the tables, we see that for both methods, there is an initial linear speedup as a function of the minibatch size b. However, in the AG algorithm, this linear speedup regime holds for much larger minibatch sizes1. Even beyond the linear speedup regime, the AG algorithm still maintains a \u221a b speedup, for the reasonable case where \u2265 L(w?)2. Finally, in all regimes, the runtime bound of the AG algorithm is equal or significantly smaller than that of the SGD algorithm.\nWe now turn to discuss the serial setting, where the runtime is measured in terms of m. Inspecting (7) and (8), we see that a larger size of b actually requires m to increase for both algorithms. This is to be expected, since mini-batching does not lead to large gains in a serial setting. However, using mini-batching in a serial setting might still be beneficial for implementation reasons, resulting in constant-factor improvements in runtime (e.g. saving overhead and loop control, and via pipelining, concurrent memory accesses etc.). In that case, we can at least ask what is the largest mini-batch size that won\u2019t degrade the runtime guarantee by more than a constant. Using our bounds, the mini-batch size b for the SGD algorithm can scale as much as L/ , vs. a larger value of L/ 3/2 for the AG algorithm.\nFinally, an interesting point is that the AG algorithm is sometimes actually necessary to obtain significant speed-ups via a mini-batch framework (according to our bounds). Based on the table above, this happens when the desired suboptimality is not much bigger then L(w?), i.e. = \u2126(L(w?)). This includes the \u201cseparable\u201d case, L(w?) = 0, and in general a regime where the \u201cestimation error\u201d and \u201capproximation error\u201d L(w?) are roughly the same\u2014an arguably very relevant one in machine learning. For the SGD algorithm, the critical mini-batch value \u221a L(w?)m can be shown to equal L(w?)/ , which is O(1) in our case. So with SGD we get no non-constant parallel speedup. However, with AG, we still enjoy a speedup of at least \u0398( \u221a b), all the way up to mini-batch size b = m2/3.\n5 Experiments\nWe implemented both the SGD algorithm (Algorithm 1) and the AG algorithm (Algorithm 2, using step-sizes of the form \u03b3i = \u03b3i p as suggested by Theorem 2) on two publicly-available binary classification problems,\n1Since it is easily verified that \u221a L(w?)m is generally smaller than both L(w?)1/4m3/4 and L(w?)m\nastro-physics and CCAT. We used the smoothed hinge loss `(w; x, y), defined as 0.5\u2212 yw>x if yw>x \u2264 0; 0 if yw>x > 1, and 0.5(1\u2212 yw>x)2 in between.\nWhile both datasets are relatively easy to classify, we also wished to understand the algorithms\u2019 performance in the \u201cseparable\u201d case L(w?) = 0, to see if the theory in Section 4 holds in practice. To this end, we created an additional version of each dataset, where L(w?) = 0, by training a classifier on the entire dataset and removing margin violations.\nIn all of our experiments, we used up to half of the data for training, and one-quarter each for validation and testing. The validation set was used to determine the step sizes \u03b7 and \u03b3i. We justify this by noting that our goal is to compare the performance of the SGD and AG algorithms, independently of the difficulties in choosing their stepsizes. In the implementation, we neglected the projection step, as we found it does not significantly affect performance when the stepsizes are properly selected.\nIn our first set of experiments, we attempted to determine the relationship between the performance of the AG algorithm and the p parameter, which determines the rate of increase of the step sizes \u03b3i. Our experiments are summarized in Figure 5. Perhaps the most important conclusion to draw from these plots is that neither the \u201ctraditional\u201d choice p = 1, nor the constant-step-size choice p = 0, give the best performance in all circumstances. Instead, there is a complicated data-dependent relationship between p, and the final classifier\u2019s performance. Furthermore, there appears to be a weak trend towards higher p performing better for larger minibatch sizes b, which corresponds neatly with our theoretical predictions.\nIn our next experiment, we directly compared the performance of the SGD and AG methods. To do so, we varied the minibatch size b while holding the total amount of data used for training, m = nb, fixed. When L(w?) > 0 (top row of Figure 5), the total sample size m is high and the suboptimality is low (red and black plots), we see that for small minibatch size, both methods do not degrade as we increase b, corresponding to a linear parallel speedup. In fact, SGD is actually overall better, but as b increases, its performance degrades more quickly, eventually performing worse than AG. That is, even in the least favorable scenario for AG (high L(w?) and small , see the tables in Section 4), it does give benefits with large enough minibatch sizes. Also, we see that even here, once the suboptimality is roughly equal to L(w?), AG significantly outperforms SGD, even with small minibatches, agreeing with our the theory.\nTurning to the case L(w?) = 0 (bottom two rows of Figure 5), which is theoretically more favorable to AG, we see it is indeed mostly better, in terms of retaining linear parallel speedups for larger minibatch sizes, even for large data set sizes corresponding to small suboptimality values, and might even be advantageous with small minibatch sizes.\n6 Summary\nIn this paper, we presented novel contributions to the theory of first order stochastic convex optimization (Theorems 1 and 2, generalizing results of [4] and [5] to be sensitive to L (w?)), developed a novel step size strategy for the accelerated method that we used in order to obtain our results and we saw works well in practice, and provided a more refined analysis of the effects of minibatching which paints a different picture then previous analyses [4, 1] and highlights the benefit of accelerated methods.\nA remaining open practical and theoretical question is whether the bound of Theorem 2 is tight. Following [5], the bound is tight for b = 1 and b \u2192 \u221e, i.e. the first and third terms are tight, but it is not clear whether the 1/( \u221a bn) dependence is indeed necessary. It would be interesting to understand whether with a more refined analysis, or perhaps different step-sizes, we can avoid this term, whether an altogether different algorithm is needed, or whether this term does represent the optimal behavior for any method based on b-aggregated stochastic gradient estimates.\nReferences\n[1] A. Agarwal and J. Duchi. Distributed delayed stochastic optimization. Technical report, arXiv, 2011.\n[2] A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. Operations Research Letters, 31(3):167 \u2013 175, 2003.\n[3] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In NIPS, 2007.\n[4] O. Dekel, R. Gilad Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using mini-batches. Technical report, arXiv, 2010.\n[5] G. Lan. An optimal method for stochastic convex optimization. Technical report, Georgia Institute of Technology, 2009.\n[6] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574\u20131609, 2009.\n[7] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady AN SSSR, 269:543\u2013547, 1983.\n[8] Y. Nesterov. Smooth minimization of non-smooth functions. Math. Program., 103(1):127\u2013152, 2005.\n[9] Y. Nesterov. Primal-dual subgradient methods for convex problems. Mathematical Programming, 120(1):221\u2013259, August 2009.\n[10] O. Shamir O. Dekel, R. Gilad-Bachrach and L. Xiao. Optimal distributed online prediction. In ICML, 2011.\n[11] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: primal estimated sub-gradient solver for SVM. Math. Program., 127(1):3\u201330, 2011.\n[12] S. Shalev-Shwartz and N. Srebro. SVM optimization: inverse dependence on training set size. In ICML, 2008.\n[13] N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In NIPS, 2010.\n[14] S.Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis, Hebrew University of Jerusalem, 2007.\n[15] P. Tseng. On accelerated proximal gradient methods for convex-concave optimization. Submitted to SIAM Journal on Optimization, 2008.\n[16] L. Xiao. Dual averaging methods for regularized stochastic learning and online optimization. Journal of Machine Learning Research, 11:2543\u20132596, 2010.\nA Generalizing to Different Norms\nWe now turn to general norms and discuss the generic Mirror Descent and Accelerated Mirror Descent algorithms. In this more general case we let domain W be some closed convex set of a Banach space equipped with norm \u2016\u00b7\u2016. We will use \u2016\u00b7\u2016\u2217 to represent the dual norm of \u2016\u00b7\u2016. Further the H-smoothness of the loss function in this general case is takes the form that for any z \u2208 Z and any w,w\u2032 \u2208 W,\n\u2016\u2207`(w, z)\u2212\u2207`(w\u2032, z)\u2016\u2217 \u2264 H \u2016w \u2212w \u2032\u2016\nThe key to generalizing the algorithms and result is to find a non-negative function R : W 7\u2192 R that is strongly convex on the domain W w.r.t. to the norm \u2016\u00b7\u2016, that is:\nDefinition 1. A function R :W 7\u2192 R is said to be 1-strongly convex w.r.t. norm \u2016\u00b7\u2016 if for any w,w\u2032 \u2208 W and any \u03b1 \u2208 [0, 1],\nR(\u03b1w + (1\u2212 \u03b1)w\u2032) \u2264 \u03b1R(w) + (1\u2212 \u03b1)R(w\u2032)\u2212 \u03b1(1\u2212\u03b1)2 \u2016w \u2212w \u2032\u20162\nWe also denote more generally\nD := \u221a\n2 sup w\u2208W R(w) .\nThe generalizations of the SGD and AG methods are summarized in Algorithms 3 and 4 respectively. The key difference between these and the Euclidean case is that the gradient descent step is replaced by a descent step involving gradient mappings of R and its conjugate R\u2217 and the projection step is replaced by Bregman projection (projection to set minimizing the Bregman divergence to the point).\nAlgorithm 3 Stochastic Mirror Descent with Mini-Batching (SMD)\nParameters: Step size \u03b7, mini-batch size b. Input: Sample z1, . . . , zm w1 = argmin\nw R(w)\nfor i = 1 to n = m/b do\nLet `i(wi) = 1 b \u2211bi t=b(i\u22121)+1 `(wi, zt) w\u2032i+1 := \u2207R\u2217 (\u2207R (wi)\u2212 \u03b3i\u2207`i(wi)) wi+1 := argmin\nw\u2208W \u2206R\n( w \u2223\u2223w\u2032i+1)\n} wi+1 = argmin\nw\u2208W {\u03b7\u3008\u2207`i(wi),w \u2212wi\u3009+ \u2206R (w|wi)}\nend for Return w\u0304 = 1n \u2211n i=1 wi\nTheorem 3. Let R : W 7\u2192 R be a non-negative strongly convex function on W w.r.t. norm \u2016\u00b7\u2016. Let K = \u221a 2 supw:\u2016w\u2016\u22641R(w). For any w ? \u2208 W, using Stochastic Mirror Descent with a step size of\n\u03b7 = min  12H , b32HK2 , \u221a 32bR(w?) L(w?)HK2n 16 ( 1 + \u221a\n32HK2R(w?) L(w?)bn\n)  ,\nwe have that, E [L(w\u0304)]\u2212 L(w?) \u2264 \u221a 128HK2R(w?) L(w?)\nbn +\n4L(w?) + 8HR(w?)\nn +\n16HK2R(w?)\nbn\nAlgorithm 4 Accelerated Mirror Descent Method (AMD)\nParameters: Step sizes (\u03b3i, \u03b2i), mini-batch size b Input: Sample z1, . . . , zm w1 = argmin\nw R(w)\nfor i = 1 to n = m/b do\nLet `i(wi) := 1 b \u2211bi t=b(i\u22121)+1 `(w, zt) wmdi := \u03b2 \u22121 i wi + (1\u2212 \u03b2 \u22121 i )w ag i\nw\u2032i+1 := \u2207R\u2217 ( \u2207R ( wmdi ) \u2212 \u03b3i\u2207`i(wmdi ) ) wi+1 := argmin\nw\u2208W \u2206R\n( w \u2223\u2223w\u2032i+1)\n} wi+1 = argmin\nw\u2208W\n{ \u03b3i \u2329 \u2207`i(wmdi ),w \u2212wmdi \u232a + \u2206R ( w \u2223\u2223wmdi )}\nwagi+1 \u2190 \u03b2 \u22121 i wi+1 + (1\u2212 \u03b2 \u22121 i )w ag i\nend for Return wagn\nTheorem 4. Let R : W 7\u2192 R be a non-negative strongly convex function on W w.r.t. norm \u2016\u00b7\u2016. Also let K = \u221a 2 supw:\u2016w\u2016\u22641R(w). For any w\n? \u2208 W, using Accelerated Mirror Descent with step size parameters \u03b2i = i+1 2 , \u03b3i = \u03b3i p where\n\u03b3 = min\n{ 1\n4H ,\n\u221a bR(w?)\n174HK2L(w?)(n\u2212 1)2p+1 ,\n( b\n1044HK2(n\u2212 1)2p\n) p+1 2p+1 ( 6R(w?)\n3 2HD 2 + L(w?)\n) p 2p+1 } and\np = min { max { log(b)\n2 log(n\u2212 1) ,\nlog log(n)\n2 (log(b(n\u2212 1))\u2212 log log(n))\n} , 1 } ,\nas long as n \u2265 max{783K2, 87K 2L(w?) HD2 }, we have that :\nE [L(wagn )]\u2212 L(w?) \u2264 164\n\u221a HK2R(w?)L(w?)\nb(n\u2212 1) +\n580HK2(R(w?))2/3D 2 3\n\u221a b(n\u2212 1)\n+ 545HK2D2\n\u221a log(n)\nb(n\u2212 1) +\n8HR(w?)\n(n\u2212 1)2\nB Complete Proofs\nWe provide complete proofs of Theorems 3 and 4, noting how Theorems 1 and 2 are specializations to the Euclidean case.\nB.1 Stochastic Mirror Descent\nProof of Theorem 3. Due to H-smoothness of convex function L we have that,\nL(wi+1) \u2264 L(wi) + \u3008\u2207L(wi),wi+1 \u2212wi\u3009+ H\n2 \u2016wi+1 \u2212wi\u20162\n= L(wi) + \u3008\u2207L(wi)\u2212\u2207`i(wi),wi+1 \u2212wi\u3009+ H\n2 \u2016wi+1 \u2212wi\u20162 + \u3008\u2207`i(wi),wi+1 \u2212wi\u3009\nby Holder\u2019s inequality we get,\n\u2264 L(wi) + \u2016\u2207L(wi)\u2212\u2207`i(wi)\u2016\u2217\u2016wi+1 \u2212wi\u2016+ H\n2 \u2016wi+1 \u2212wi\u20162 + \u3008\u2207`i(wi),wi+1 \u2212wi\u3009\nsince for any \u03b1 > 0, ab \u2264 a 2 2\u03b1 + \u03b1b2 2 ,\n\u2264 L(wi) + \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217\n2(1/\u03b7 \u2212H) + (1/\u03b7 \u2212H) 2 \u2016wi+1 \u2212wi\u20162 + H 2 \u2016wi+1 \u2212wi\u20162 + \u3008\u2207`i(wi),wi+1 \u2212wi\u3009\n= L(wi) + \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 2(1/\u03b7 \u2212H) + \u2016wi+1 \u2212wi\u20162 2\u03b7 + \u3008\u2207`i(wi),wi+1 \u2212wi\u3009\nWe now note that the update step can be written equivalently as\nwi+1 = argmin w\u2208W\n{\u03b7\u3008\u2207`i(wi),w \u2212wi\u3009+ \u2206R(w,wi)} .\nIt can be shown that (see for instance Lemma 1 of [5])\n\u03b7\u3008\u2207`i(wi),wi+1 \u2212wi\u3009 \u2264 \u03b7\u3008\u2207`i(wi),w? \u2212wi\u3009+ \u2206R(w?,wi)\u2212\u2206R(w?,wi+1)\u2212\u2206R(wi,wi+1)\nPlugging this we get that,\nL(wi+1) \u2264 L(wi) + \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 2(1/\u03b7 \u2212H) + \u2016wi \u2212wi+1\u20162 2\u03b7 + \u3008\u2207`i(wi),w? \u2212wi\u3009\n+ 1\n\u03b7 (\u2206R(w\n?,wi)\u2212\u2206R(w?,wi+1)\u2212\u2206R(wi,wi+1))\n= L(wi) + \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 2(1/\u03b7 \u2212H) + \u2016wi \u2212wi+1\u20162 2\u03b7 + \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009+ \u3008\u2207L(wi),w? \u2212wi\u3009\n+ 1\n\u03b7 (\u2206R(w\n?,wi)\u2212\u2206R(w?,wi+1)\u2212\u2206R(wi,wi+1))\n\u2265 L(wi) + \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 2(1/\u03b7 \u2212H) + \u2016wi \u2212wi+1\u20162 2\u03b7 + \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009 \u2212 \u3008\u2207L(wi),wi \u2212w?\u3009\n+ 1\n\u03b7 (\u2206R(w\n?,wi)\u2212\u2206R(w?,wi+1)\u2212\u2206R(wi,wi+1))\nby strong convexity, \u2206R(wi,wi+1) \u2265 \u2016wi\u2212wi+1\u20162 and so,\n\u2264 L(wi) + \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217\n2(1/\u03b7 \u2212H) + \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009 \u2212 \u3008\u2207L(wi),wi \u2212w?\u3009\n+ 1\n2\u03b7 (\u2206R(w\n?,wi)\u2212\u2206R(w?,wi+1))\nsince \u03b7 \u2264 12H ,\n\u2264 L(wi) + \u03b7\u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 + \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009 \u2212 \u3008\u2207L(wi),wi \u2212w?\u3009\n+ 1\n\u03b7 (\u2206R(w\n?,wi)\u2212\u2206R(w?,wi+1))\nby convexity, L(wi)\u2212 \u3008\u2207L(wi),wi \u2212w?\u3009 \u2264 L(w?) and so\n\u2264 L(w?) + \u03b7\u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 + \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009\n+ 1\n\u03b7 (\u2206R(w\n?,wi)\u2212\u2206R(w?,wi+1))\nHence we conclude that :\n1\nn\u2212 1 n\u22121\u2211 i=1 L(wi+1)\u2212 L(w?) \u2264 \u03b7 (n\u2212 1) n\u22121\u2211 i=1 \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 + 1 n\u2212 1 n\u22121\u2211 i=1 \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009\n+ 1\nn\u2212 1 n\u22121\u2211 i=1 \u2206R(w ?,wi)\u2212\u2206R(w?,wi+1) \u03b7\n= \u03b7\n(n\u2212 1) n\u22121\u2211 i=1 \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 + 1 n\u2212 1 n\u22121\u2211 i=1 \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009\n+ \u2206R (w ?|w1)\u2212\u2206R (w?|wn\u22121) \u03b7(n\u2212 1)\n\u2264 \u03b7 (n\u2212 1) n\u22121\u2211 i=1 \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 + 1 n\u2212 1 n\u22121\u2211 i=1 \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009\n+ R(w?)\n\u03b7(n\u2212 1)\n\u2264 \u03b7 (n\u2212 1) n\u22121\u2211 i=1 \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 + 1 n\u2212 1 n\u22121\u2211 i=1 \u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009\n+ R(w?)\n\u03b7(n\u2212 1)\nTaking expectation with respect to sample on both sides and noticing that E [\u3008\u2207`i(wi)\u2212\u2207L(wi),w? \u2212wi\u3009] = 0, we get that,\nE\n[ 1\nn\u2212 1 n\u22121\u2211 i=1 L(wi+1)\u2212 L(w?)\n] \u2264 \u03b7\n(n\u2212 1) n\u22121\u2211 i=1 E [ \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 ] + R(w?) \u03b7(n\u2212 1)\nNow note that\n\u2207L(wi)\u2212\u2207`i(wi) = 1\nb bi\u2211 t=(i\u22121)b+1 (\u2207L(wi)\u2212\u2207`(wi, zt))\nand that (\u2207L(wi)\u2212 `(wi, zt)) is a mean zero vector drawn i.i.d. Also note that wi only depends on the first (i\u2212 1)b examples and so when we consider expectation w.r.t. z(i\u22121)b+1, . . . , zib alone, wi is fixed. Hence by Corollary B.2 we have that,\nE [ \u2016\u2207L(wi)\u2212\u2207`i(wi)\u20162\u2217 ] \u2264 K 2\nb2 E  \u2225\u2225\u2225\u2225\u2225\u2225 bi\u2211 t=(i\u22121)b+1 (\u2207L(wi)\u2212\u2207`(wi, zt)) \u2225\u2225\u2225\u2225\u2225\u2225 2\n\u2217  = K2\nb2 bi\u2211 t=(i\u22121)b+1 E [ \u2016(\u2207L(wi)\u2212\u2207`(wi, zt))\u20162\u2217 ] Plugging this back we get that\nE\n[ 1\nn\u2212 1 n\u22121\u2211 i=1 L(wi+1)\u2212 L(w?)\n] \u2264 K 2\u03b7\nb2(n\u2212 1) n\u22121\u2211 i=1 bi\u2211 t=(i\u22121)b+1 E [ \u2016(\u2207L(wi)\u2212\u2207`(wi, zt))\u20162\u2217 ] + R(w?) \u03b7(n\u2212 1)\n\u2264 2K 2\u03b7\nb2(n\u2212 1) n\u22121\u2211 i=1 ib\u2211 t=(i\u22121)b+1 E [ \u2016\u2207L(wi)\u20162 + \u2016\u2207`(wi, zt)\u20162\u2217 ] + R(w?) \u03b7(n\u2212 1)\nfor any non-negative H-smooth convex function f , we have the self-bounding property that \u2016\u2207f(w)\u2016\u2217 \u2264\u221a 4Hf(w). Using this,\n\u2264 8HK 2\u03b7\nb2(n\u2212 1) n\u22121\u2211 i=1 ib\u2211 t=(i\u22121)b+1 E [L(wi) + `(wi, zt)] + R(w?) \u03b7(n\u2212 1)\n= 16\u03b7HK2\nb E\n[ 1\nn\u2212 1 n\u22121\u2211 i=1 L(wi)\n] + R(w?)\n\u03b7(n\u2212 1)\nAdding 1n\u22121L(w1) on both sides and removing L(wn) on the left we conclude that\nE\n[ 1\nn\u2212 1 n\u22121\u2211 i=1 L(wi)\n] \u2212 L(w?) \u2264 16\u03b7HK 2\nb E\n[ 1\nn\u2212 1 n\u22121\u2211 i=1 L(wi)\n] + R(w?)\n\u03b7(n\u2212 1) + L(w1) n\u2212 1\nHence we conclude that\nE\n[ 1\nn n\u2211 i=1 L(wi)\n] \u2212 L(w?) \u2264 1( 1\u2212 16\u03b7HK2b ) (16\u03b7HK2 b L(w?) + L(w1) n + R(w?) \u03b7n )\n=\n( 1\n1\u2212 16\u03b7HK2b \u2212 1\n) L(w?) +\n1\n1\u2212 16\u03b7HK2b\n( L(w1)\nn + R(w?) \u03b7n\n)\n=\n( 1\n1\u2212 16\u03b7HK2b \u2212 1\n) L(w?) + ( 1\n1\u2212 16\u03b7HK2b\n) L(w1)\nn\n+\n( 1\n1\u2212 16\u03b7HK2b\n) b\n16\u03b7HK2 16HK2R(w?) bn\nWriting \u03b1 = 1 1\u2212 16\u03b7HK2b\n\u2212 1, so that \u03b7 = b16HK2 ( 1\u2212 1\u03b1+1 ) we get,\nE\n[ 1\nn n\u2211 i=1 L(wi)\n] \u2212 L(w?) \u2264 \u03b1L(w?) + (\u03b1+ 1)L(w1)\nn +\n16H(\u03b1+ 1)2\n\u03b1\nR(w?)\nbn\n\u2264 \u03b1L(w?) + (\u03b1+ 1)L(w1) n +\n( \u03b1+ 1\n\u03b1\n) 32HR(w?)\nbn\nNow we shall always pick \u03b7 \u2264 b32HK2 so that \u03b1 \u2264 1 and so\nE\n[ 1\nn n\u2211 i=1 L(wi)\n] \u2212 L(w?) \u2264 \u03b1L(w?) + 32HK 2R(w?)\n\u03b1 bn +\n2L(w1)\nn +\n16HK2R(w?)\nbn\nPicking\n\u03b7 = min  12H , b32HK2 , \u221a 32bR(w?) L(w?)HK2n 16 ( 1 + \u221a\n32HK2R(w?) L(w?)bn\n)  ,\nor equivalently \u03b1 = min { 1, \u221a\n32HK2R(w?) L(w?)bn\n} we get,\nE\n[ 1\nn n\u2211 i=1 L(wi)\n] \u2212 L(w?) \u2264 \u221a 128HK2R(w?) L(w?)\nbn +\n2L(w1)\nn +\n16HK2R(w?)\nbn\nFinally note that by smoothness,\nL(w1) \u2264 L(w?) + \u3008\u2207L(w1)\u2212\u2207L(w?),w1 \u2212w?\u3009+ \u3008\u2207L(w?),w1 \u2212w?\u3009 \u2264 L(w?) + \u2016\u2207L(w1)\u2212\u2207L(w?)\u2016\u2217 \u2016w1 \u2212w ?\u2016+ \u2016\u2207L(w?)\u2016\u2217 \u2016w1 \u2212w ?\u2016\n\u2264 L(w?) +H \u2016w1 \u2212w?\u20162 + \u221a 4HL(w?) \u2016w1 \u2212w?\u2016\nSince R is 1-strongly convex and w1 = argmin w R(w),\n\u2264 L(w?) + 2HR(w?) + \u221a 8HL(w?)R(w?) \u2264 2L(w?) + 4HR(w?)\nHence we conclude that\nE\n[ 1\nn n\u2211 i=1 L(wi)\n] \u2212 L(w?) \u2264 \u221a 128HK2R(w?) L(w?)\nbn +\n4L(w?) + 8HR(w?)\nn +\n16HK2R(w?)\nbn\nUsing Jensen\u2019s inequality concludes the proof.\nProof of Theorem 1. For Euclidean case R(w) = 12 \u2016w\u2016 2 2 and K = \u221a supw:\u2016w\u20162\u22641 \u2016w\u2016 2 = 1. Plugging these in the previous theorem concludes the proof.\nB.2 Accelerated Mirror Descent\nLemma B.1. For the accelerated update rule, if the step sizes \u03b2i \u2208 [1,\u221e) and \u03b3i \u2208 (0,\u221e) are chosen such that \u03b21 = 1 and for all i \u2208 [n]\n0 < \u03b3i+1(\u03b2i+1 \u2212 1) \u2264 \u03b2i\u03b3i and 2H\u03b3i \u2264 \u03b2i\nthen we have that\nE [L(wagn )]\u2212 L(w?) \u2264 \u03b31(\u03b21 \u2212 1) \u03b3n(\u03b2n \u2212 1) L(wag1 ) + 32H b\u03b3n(\u03b2n \u2212 1) n\u22121\u2211 i=1 \u03b32i E [L(w ag i )] +\nD2\n2\u03b3n(\u03b2n \u2212 1) +\n16H2D2\nb\u03b3n(\u03b2n \u2212 1) n\u22121\u2211 i=1 \u03b32i \u03b22i\nProof. First note that for any i,\nwagi+1 \u2212w md i = \u03b2 \u22121 i wi+1 + (1\u2212 \u03b2 \u22121 i )w ag i \u2212w md i\n= \u03b2\u22121i wi+1 + (1\u2212 \u03b2 \u22121 i )w ag i \u2212 \u03b2 \u22121 i wi \u2212 (1\u2212 \u03b2 \u22121 i )w ag i = \u03b2\u22121i (wi+1 \u2212wi) (9)\nNow by smoothness we have that\nL(wagi+1) \u2264 L(w md i ) + \u2329 \u2207L(wmdi ),w ag i+1 \u2212w md i \u232a + H\n2 \u2016wagi+1 \u2212w md i \u20162\n= L(wmdi ) + \u2329 \u2207L(wmdi ),w ag i+1 \u2212w md i \u232a + H\n2\u03b22i \u2016wi+1 \u2212wi\u20162\n= L(wmdi ) + \u2329 \u2207L(wmdi ),w ag i+1 \u2212w md i \u232a + 1\n2\u03b2i\u03b3i \u2016wi+1 \u2212wi\u20162 \u2212 \u03b2i/\u03b3i \u2212H 2\u03b22i \u2016wi+1 \u2212wi\u20162\nsince wagi+1 = \u03b2 \u22121 i wi+1 + (1\u2212 \u03b2 \u22121 i )w ag i ,\n= L(wmdi ) + \u2329 \u2207L(wmdi ), \u03b2\u22121i wi+1 + (1\u2212 \u03b2 \u22121 i )w ag i \u2212w md i \u232a + \u2016wi \u2212wi+1\u20162\n2\u03b2i\u03b3i \u2212 \u03b2i/\u03b3i \u2212H 2\u03b22i \u2016wi+1 \u2212wi\u20162\n= L(wmdi ) + (1\u2212 \u03b2\u22121i ) \u2329 \u2207L(wmdi ),w ag i \u2212w md i \u232a +\n\u2329 \u2207L(wmdi ),wi+1 \u2212wmdi \u232a \u03b2i + \u2016wi \u2212wi+1\u20162 2\u03b2i\u03b3i\n\u2212 \u03b2i/\u03b3i \u2212H 2\u03b22i \u2016wi+1 \u2212wi\u20162\n= (1\u2212 \u03b2\u22121i ) ( L(wmdi ) + \u2329 \u2207L(wmdi ),w ag i \u2212w md i \u232a) + L(wmdi ) +\n\u2329 \u2207L(wmdi ),wi+1 \u2212wmdi \u232a \u03b2i + \u2016wi \u2212wi+1\u20162 2\u03b2i\u03b3i\n\u2212 \u03b2i/\u03b3i \u2212H 2\u03b22i \u2016wi+1 \u2212wi\u20162\n= (1\u2212 \u03b2\u22121i )L(w ag i ) +\nL(wmdi ) + \u2329 \u2207L(wmdi ),wi+1 \u2212wmdi \u232a \u03b2i + \u2016wi \u2212wi+1\u20162 2\u03b2i\u03b3i \u2212 \u03b2i/\u03b3i \u2212H 2\u03b22i \u2016wi+1 \u2212wi\u20162\n= (1\u2212 \u03b2\u22121i )L(w ag i )\u2212 \u03b2i/\u03b3i \u2212H 2\u03b22i \u2016wi+1 \u2212wi\u20162 + \u2016wi \u2212wi+1\u20162 2\u03b2i\u03b3i\n+ L(wmdi ) +\n\u2329 \u2207`i(wmdi ),wi+1 \u2212wmdi \u232a + \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi+1 \u2212wmdi \u232a \u03b2i\n= (1\u2212 \u03b2\u22121i )L(w ag i )\u2212 \u03b2i/\u03b3i \u2212H 2\u03b22i \u2016wi+1 \u2212wi\u20162 + \u2016wi \u2212wi+1\u20162 2\u03b2i\u03b3i +\n\u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi+1 \u2212wi \u232a \u03b2i\n+ L(wmdi ) +\n\u2329 \u2207`i(wmdi ),wi+1 \u2212wmdi \u232a + \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212wmdi \u232a \u03b2i\nby Holder\u2019s inequality,\n\u2264 (1\u2212 \u03b2\u22121i )L(w ag i )\u2212 \u03b2i/\u03b3i \u2212H 2\u03b22i \u2016wi+1 \u2212wi\u20162 + \u2016wi \u2212wi+1\u20162 2\u03b2i\u03b3i + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u2225\u2217 \u2016wi+1 \u2212wi\u2016 \u03b2i\n+ L(wmdi ) +\n\u2329 \u2207`i(wmdi ),wi+1 \u2212wmdi \u232a + \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212wmdi \u232a \u03b2i\nsince for any a, b and \u03b1 > 0, ab \u2264 a 2 2\u03b1 + \u03b1b2 2\n\u2264 (1\u2212 \u03b2\u22121i )L(w ag i ) + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H) + \u2016wi \u2212wi+1\u20162 2\u03b2i\u03b3i\n+ L(wmdi ) +\n\u2329 \u2207`i(wmdi ),wi+1 \u2212wmdi \u232a + \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212wmdi \u232a \u03b2i\nWe now note that the update step 2 of accelerated gradient can be written equivalently as\nwi+1 = argmin w\u2208W\n{ \u03b3i \u2329 \u2207`i(wmdi ),w \u2212wmdi \u232a + \u2206R ( w \u2223\u2223wmdi )} .\nIt can be shown that (see for instance Lemma 1 of [5]) \u03b3i \u2329 \u2207`i(wi),wi+1 \u2212wmdi \u232a \u2264 \u03b3i \u2329 \u2207`i(wmdi ),w? \u2212wmdi \u232a + \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1)\u2212\u2206R (wi|wi+1)\nPlugging this we get that,\nL(wagi+1) \u2264 (1\u2212 \u03b2 \u22121 i )L(w ag i ) + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H) + \u2016wi \u2212wi+1\u20162 2\u03b2i\u03b3i + \u2329 \u2207`i(wmdi ),w? \u2212wmdi \u232a \u03b2i\n+ L(wmdi ) +\n\u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212wmdi \u232a \u03b2i + \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1)\u2212\u2206R (wi|wi+1) \u03b3i\u03b2i\nby strong-convexity of R, \u2206R (wi|wi+1) \u2265 12 \u2016wi \u2212wi+1\u2016 2 and so,\n= (1\u2212 \u03b2\u22121i )L(w ag i ) + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H) + \u2329 \u2207`i(wmdi ),w? \u2212wmdi \u232a \u03b2i\n+ L(wmdi ) +\n\u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212wmdi \u232a \u03b2i + \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1) \u03b3i\u03b2i\n= (1\u2212 \u03b2\u22121i )L(w ag i ) + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H) + \u2329 \u2207`i(wmdi )\u2212\u2207L(wmdi ),w? \u2212wmdi \u232a \u03b2i\n+\n\u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212wmdi \u232a \u03b2i + \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1) \u03b3i\u03b2i\n+ L(wmdi ) +\n\u2329 \u2207L(wmdi ),w? \u2212wmdi \u232a \u03b2i\nby convexity, L(w?) \u2265 L(wmdi ) + \u2329 \u2207L(wmdi ),w? \u2212wmdi \u232a , hence\n\u2264 (1\u2212 \u03b2\u22121i )L(w ag i ) + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H) + \u2329 \u2207`i(wmdi )\u2212\u2207L(wmdi ),w? \u2212wmdi \u232a \u03b2i\n+\n\u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212wmdi \u232a \u03b2i + \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1) \u03b3i\u03b2i + L(w?) \u03b2i\n= (1\u2212 \u03b2\u22121i )L(w ag i ) + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H) + \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a \u03b2i\n+ \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1) \u03b3i\u03b2i + \u03b2\u22121i L(w ?)\n= L(w?) + (1\u2212 \u03b2\u22121i ) (L(w ag i )\u2212 L(w ?)) + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H) + \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a \u03b2i\n+ \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1) \u03b3i\u03b2i\nThus we conclude that\nL(wagi+1)\u2212 L(w ?) \u2264 (1\u2212 \u03b2\u22121i ) (L(w ag i )\u2212 L(w ?)) + \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H) + \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a \u03b2i\n+ \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1) \u03b2i\u03b3i\nMultiplying throughout by \u03b2i\u03b3i we get\n\u03b3i\u03b2i ( L(wagi+1)\u2212 L(w ?) ) \u2264 \u03b3i(\u03b2i \u2212 1) (L(wagi )\u2212 L(w ?)) + \u03b3i\u03b2i \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H)\n+ \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1) + \u03b3i \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a\nOwing to the condition that \u03b3i+1(\u03b2i+1 \u2212 1) \u2264 \u03b3i\u03b2i we have that \u03b3i+1(\u03b2i+1 \u2212 1) ( L(wagi+1)\u2212 L(w ?) ) \u2264 \u03b3i(\u03b2i \u2212 1) (L(wagi )\u2212 L(w ?)) + \u03b3i\u03b2i \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 2(\u03b2i/\u03b3i \u2212H)\n+ \u2206R (w ?|wi)\u2212\u2206R (w?|wi+1) + \u03b3i \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a Using the above inequality repeatedly we conclude that\n\u03b3n(\u03b2n \u2212 1) (L(wagn )\u2212 L(w?)) \u2264 \u03b31(\u03b21 \u2212 1) (L(w ag 1 )\u2212 L(w?)) + n\u22121\u2211 i=1\n\u03b3i\u03b2i \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217\n2(\u03b2i/\u03b3i \u2212H)\n+ \u2206R (w ?|w1)\u2212\u2206R (w?|wn) + n\u22121\u2211 i=1 \u03b3i \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a \u2264 \u03b31(\u03b21 \u2212 1) (L(wag1 )\u2212 L(w?)) +\nn\u22121\u2211 i=1\n\u03b3i\u03b2i \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217\n2(\u03b2i/\u03b3i \u2212H)\n+R(w?) + n\u22121\u2211 i=1 \u03b3i \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a = \u03b31(\u03b21 \u2212 1) (L(wag1 )\u2212 L(w?)) +\nn\u22121\u2211 i=1\n\u03b3i\u03b2i \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217\n2(\u03b2i/\u03b3i \u2212H)\n+R(w?) + n\u22121\u2211 i=1 \u03b3i \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a since 2H\u03b3i \u2264 \u03b2i,\n\u2264 \u03b31(\u03b21 \u2212 1) (L(wag1 )\u2212 L(w?)) + n\u22121\u2211 i=1 \u03b32i \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2225\u22252\u2217 +R(w?)\n+ n\u22121\u2211 i=1 \u03b3i \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a \u2264 \u03b31(\u03b21 \u2212 1)L(wag1 ) +\nn\u22121\u2211 i=1 2\u03b32i \u2016\u2207L(w ag i )\u2212\u2207`i(w ag i )\u2016 2 \u2217 +R(w ?)\n+ n\u22121\u2211 i=1 \u03b3i \u2329 \u2207L(wmdi )\u2212\u2207`i(wmdi ),wi \u2212w? \u232a +\nn\u22121\u2211 i=1 2\u03b32i \u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2212\u2207L(wagi ) +\u2207`i(wagi )\u2225\u22252\u2217\nTaking expectation we get that\n\u03b3n(\u03b2n \u2212 1) (E [L(wagn )]\u2212 L(w?)) \u2264 \u03b31(\u03b21 \u2212 1)L(w ag 1 ) + n\u22121\u2211 i=1 2\u03b32i E [ \u2016\u2207L(wagi )\u2212\u2207`i(w ag i )\u2016 2 \u2217 ] +R(w?)\n+ n\u22121\u2211 i=1 2\u03b32i E [\u2225\u2225\u2207L(wmdi )\u2212\u2207`i(wmdi )\u2212\u2207L(wagi ) +\u2207`i(wagi )\u2225\u22252\u2217]\n(10)\nNow note that\n\u2207L(wagi )\u2212\u2207`i(w ag i ) =\n1\nb bi\u2211 t=(i\u22121)b+1 (\u2207L(wagi )\u2212 `(w ag i , zt)) and\n\u2207L(wagi )\u2212\u2207`i(w ag i )\u2212\u2207L(w md i )+\u2207`i(wmdi ) =\n1\nb bi\u2211 t=(i\u22121)b+1 ( \u2207L(wagi )\u2212 `(w ag i , zt)\u2212\u2207L(w md i ) + `(w md i , zt) ) Further (\u2207L(wi)\u2212 `(wi, zt)) and ( \u2207L(wagi )\u2212 `(w ag i , zt)\u2212\u2207L(wmdi ) + `(wmdi , zt) ) are mean zero vectors drawn i.i.d. Also note that wagi only depends on the first (i \u2212 1)b examples and so when we consider expectation w.r.t. z(i\u22121)b+1, . . . , zib, wi is fixed. Hence by Corollary B.2 we have that,\nE [ \u2016\u2207L(wagi )\u2212\u2207`i(w ag i )\u2016 2 \u2217 ] = K2\nb2 E  \u2225\u2225\u2225\u2225\u2225\u2225 bi\u2211 t=(i\u22121)b+1 (\u2207L(wagi )\u2212\u2207`(w ag i , zt)) \u2225\u2225\u2225\u2225\u2225\u2225 2\n\u2217  \u2264 K 2\nb2 bi\u2211 t=(i\u22121)b+1 E [ \u2016(\u2207L(wagi )\u2212\u2207`(w ag i , zt))\u2016 2 \u2217 ] and similarly\nE [\u2225\u2225\u2207L(wagi )\u2212\u2207`i(wagi )\u2212\u2207L(wmdi ) +\u2207`i(wmdi )\u2225\u22252\u2217]\n\u2264 K 2\nb2 bi\u2211 t=(i+1)b+1 E [\u2225\u2225\u2207L(wagi )\u2212\u2207`(wagi , zt)\u2212\u2207L(wmdi ) +\u2207`(wmdi , zt)\u2225\u22252\u2217]\nPlugging these back in Equation 10 we get : \u03b3n(\u03b2n \u2212 1) (E [L(wagn )]\u2212 L(w?)) \u2264 \u03b31(\u03b21 \u2212 1)L(wag1 ) + n\u22121\u2211 i=1 2K2\u03b32i b2 bi\u2211 t=(i\u22121)b+1 E [ \u2016(\u2207L(wagi )\u2212\u2207`(w ag i , zt))\u2016 2 \u2217 ] +R(w?)\n+ n\u22121\u2211 i=1 2K2\u03b32i b2 bi\u2211 t=(i+1)b+1 E [\u2225\u2225\u2225\u2207L(wagi )\u2212\u2207`(wagi , zt)\u2212\u2207L(wmdi ) +\u2207`(wmdi , zt)\u2225\u2225\u22252\u2217 ]\n\u2264 \u03b31(\u03b21 \u2212 1)L(wag1 ) + n\u22121\u2211 i=1 4K2\u03b32i b2 bi\u2211 t=(i\u22121)b+1 E [ \u2016\u2207L(wagi )\u2016 2 \u2217 + \u2016\u2207`(w ag i , zt)\u2016 2 \u2217 ] +R(w?)\n+ n\u22121\u2211 i=1 4K2\u03b32i b2 bi\u2211 t=(i+1)b+1 E [\u2225\u2225\u2225\u2207L(wagi )\u2212\u2207L(wmdi )\u2225\u2225\u22252\u2217 + \u2225\u2225\u2225\u2207`(wmdi , zt)\u2212\u2207`(wagi , zt)\u2225\u2225\u22252\u2217 ]\nfor any non-negative H-smooth convex function f , we have the self-bounding property that \u2016\u2207f(w)\u2016 \u2264 \u221a\n4Hf(w). Using this,\n\u2264 \u03b31(\u03b21 \u2212 1)L(wag1 ) + n\u22121\u2211 i=1 16HK2\u03b32i b2 bi\u2211 t=(i\u22121)b+1 E [L(wagi ) + `(w ag i , zt)] +R(w ?)\n+ n\u22121\u2211 i=1 4K2\u03b32i b2 bi\u2211 t=(i+1)b+1 E [\u2225\u2225\u2225\u2207L(wagi )\u2212\u2207L(wmdi )\u2225\u2225\u22252\u2217 + \u2225\u2225\u2225\u2207`(wmdi , zt)\u2212\u2207`(wagi , zt)\u2225\u2225\u22252\u2217 ]\n= \u03b31(\u03b21 \u2212 1)L(wag1 ) + n\u22121\u2211 i=1 32HK2\u03b32i b E [L(wagi )] +R(w ?)\n+ n\u22121\u2211 i=1 4K2\u03b32i b2 bi\u2211 t=(i+1)b+1 E [\u2225\u2225\u2225\u2207L(wagi )\u2212\u2207L(wmdi )\u2225\u2225\u22252\u2217 + \u2225\u2225\u2225\u2207`(wmdi , zt)\u2212\u2207`(wagi , zt)\u2225\u2225\u22252\u2217 ]\nby H-smoothness of L and ` we have that \u2225\u2225\u2207L(wagi )\u2212\u2207L(wmdi )\u2225\u2225\u2217 \u2264 H \u2225\u2225wagi \u2212wmdi \u2225\u2225. Similarly we also have that\u2225\u2225\u2207`(wagi , zt)\u2212\u2207`(wmdi , zt)\u2225\u2225\u2217 \u2264 H \u2225\u2225wagi \u2212wmdi \u2225\u2225. Hence,\n\u2264 \u03b31(\u03b21 \u2212 1)L(wag1 ) + n\u22121\u2211 i=1 32HK2\u03b32i b E [L(wagi )] +R(w ?)\n+ n\u22121\u2211 i=1 8H2K2\u03b32i b E [\u2225\u2225\u2225wagi \u2212wmdi \u2225\u2225\u22252]\nHowever, wmdi \u2190 \u03b2\u22121i wi + (1 \u2212 \u03b2 \u22121 i )w ag i . Hence \u2225\u2225wagi \u2212wmdi \u2225\u22252 \u2264 \u2016wi\u2212wagi \u20162\u03b22i \u2264 2\u2016wi\u2212w1\u20162+2\u2016w1\u2212wagi \u20162\u03b22i \u2264 4D2\u03b22i . Hence,\n\u2264 \u03b31(\u03b21 \u2212 1)L(wag1 ) + n\u22121\u2211 i=1 32HK2\u03b32i b E [L(wagi )] +R(w ?) + 32H2K2D2 b n\u22121\u2211 i=1 \u03b32i \u03b22i\nDividing throughout by \u03b3n(\u03b2n \u2212 1) concludes the proof.\nProof of Theorem 4. First note that the for any i,\n2H\u03b3i = 2H\u03b3i p \u2264 i\np 2 \u2264 \u03b2i\nAlso note that since p \u2208 [0, 1],\n\u03b3i+1(\u03b2i+1 \u2212 1) = \u03b3 i(i+ 1)p 2 \u2264 \u03b3 i p(i+ 1) 2 = \u03b3i\u03b2i\nThus we have verified that the step sizes satisfy the conditions required by previous lemma. From the previous lemma we have that\nE [L(wagn )]\u2212 L(w?) \u2264 \u03b31(\u03b21 \u2212 1) \u03b3n(\u03b2n \u2212 1) L(wag1 ) + 32HK2 b\u03b3n(\u03b2n \u2212 1) n\u22121\u2211 i=1 \u03b32i E [L(w ag i )] + R(w?) \u03b3n(\u03b2n \u2212 1) + 32H2K2D2 b\u03b3n(\u03b2n \u2212 1) n\u22121\u2211 i=1 \u03b32i \u03b22i\n= 64HK2\u03b3\nbnp(n\u2212 1) n\u22121\u2211 i=1 i2p E [L(wagi )] + 2R(w?) \u03b3np(n\u2212 1) + 256H2K2D2\u03b3 bnp(n\u2212 1) n\u22121\u2211 i=1 i2p (i+ 1)2\n\u2264 64HK 2\u03b3(n\u2212 1)2p\nbnp(n\u2212 1)\nn\u22121\u2211 i=1 E [L(wagi )] + 2R(w?) \u03b3(n\u2212 1)p+1 + 256H2K2D2\u03b3 b(n\u2212 1)p+1 n\u22121\u2211 i=1 1 i2(1\u2212p)\n\u2264 64HK 2\u03b3 b(n\u2212 1)1\u2212p n\u22121\u2211 i=1 E [L(wagi )] + 2R(w?) \u03b3(n\u2212 1)p+1 + 256H2K2D2\u03b3 b(n\u2212 1)p+1 n\u22121\u2211 i=1 1 i2(1\u2212p)\n\u2264 64HK 2\u03b3 b(n\u2212 1)1\u2212p n\u22121\u2211 i=1 E [L(wagi )] + 2R(w?) \u03b3(n\u2212 1)p+1 + 256H2K2D2\u03b3 b(n\u2212 1)\n\u2264 64HK 2\u03b3 b(n\u2212 1)1\u2212p n\u22121\u2211 i=1 (E [L(wagi )]\u2212 L(w ?)) + 64HK2\u03b3L(w?)(n\u2212 1)p b + 2R(w?) \u03b3(n\u2212 1)p+1\n+ 256H2K2D2\u03b3\nb(n\u2212 1)\nsince \u03b3 \u2264 1/4H,\n\u2264 64HK 2\u03b3 b(n\u2212 1)1\u2212p n\u22121\u2211 i=1 (E [L(wagi )]\u2212 L(w ?)) + 64HK2\u03b3L(w?)(n\u2212 1)p b + 2R(w?) \u03b3(n\u2212 1)p+1\n+ 64HK2D2\nb(n\u2212 1)\nThus we have shown that\nE [L(wagn )]\u2212 L(w?) \u2264 64HK2\u03b3 b(n\u2212 1)1\u2212p n\u22121\u2211 i=1 (E [L(wagi )]\u2212 L(w ?)) + 64HK2\u03b3L(w?)(n\u2212 1)p b + 2R(w?) \u03b3(n\u2212 1)p+1\n+ 64HK2D2\nb(n\u2212 1)\nNow if we use the notation ai = E [L(wagi )]\u2212 L(w?), A(i) = 64HK2\u03b3 b(i\u22121)1\u2212p and\nB(i) = 64HK2\u03b3L(w?)(i\u2212 1)p\nb +\n2R(w?)\n\u03b3(i\u2212 1)p+1 +\n64HK2D2\nb(i\u2212 1)\nNote that for any i by smoothness, ai \u2264 L0 := 32HD 2 + L(w?) Also notice that\nn\u2211 i=n\u2212M\u22121 A(i) = 64HK2\u03b3 b n\u2211 i=n\u2212M\u22121\n1 (i\u2212 1)1\u2212p \u2264 64HK\n2\u03b3np\nb\nHence as long as\n\u03b3 \u2264 b 64HK2np\n, (11)\u2211n i=n\u2212M\u22121A(i) \u2264 1. We shall ensure that the \u03b3 we choose will satisfy the above condition. Now applying lemma B.3 we get that for any M ,\nan \u2264 eA(n) ( a0(n\u2212M) +\nn\u2211 i=n\u2212M\u22121 B(i)\n) +B(n) (12)\nNow notice that\nn\u2211 i=n\u2212M\u22121 B(i) = 64HK2\u03b3L(w?) b n\u2211 i=n\u2212M\u22121\n1\n(i\u2212 1)p +\n2R(w?)\n\u03b3\nn\u2211 i=n\u2212M\u22121\n1\n(i\u2212 1)p+1 +\n64HK2D2\nb\nn\u2211 i=n\u2212M\u22121 1 (i\u2212 1)\n\u2264 64HK 2\u03b3L(w?)(n\u2212M \u2212 2)p\nb +\n2R(w?)\n\u03b3(n\u2212M \u2212 2)p+1 +\n64HK2D2\nb(n\u2212M \u2212 2) +\n64HK2\u03b3L(w?)(n\u2212 1)p+1\nb\n+ 2R(w?)\n\u03b3(n\u2212M \u2212 2)p +\n64HK2D2 log n\nb\nPlugging this back in Equation 12 we conclude that\nan \u2264 64eHK2\u03b3 b(n\u2212 1)1\u2212p ( L0(n\u2212M) + 64HK2\u03b3L(w?)(n\u2212M \u2212 2)p b +\n2R(w?)\n\u03b3(n\u2212M \u2212 2)p+1\n+ 64HK2D2\nb(n\u2212M \u2212 2) +\n64HK2\u03b3L(w?)(n\u2212 1)p+1\nb +\n2R(w?)\n\u03b3(n\u2212M \u2212 2)p +\n64HK2D2 log n\nb ) + 64HK2\u03b3L(w?)(n\u2212 1)p\nb +\n2R(w?)\n\u03b3(n\u2212 1)p+1 +\n64HK2D2\nb(n\u2212 1)\n\u2264 64eHK 2\u03b3 b(n\u2212 1)1\u2212p ( L0(n\u2212M \u2212 2) + 64HK2D2 b(n\u2212M \u2212 2) +\n4R(w?)\n\u03b3(n\u2212M \u2212 2)p +\n64HK2D2 log(n)\nb\n+ 256HK2\u03b3L(w?)(n\u2212 1)p+1\nb\n) + 64HK2\u03b3L(w?)(n\u2212 1)p\nb +\n4R(w?)\n\u03b3(n\u2212 1)p+1 +\n64HK2D2\nb(n\u2212 1)\nsince \u03b3 \u2264 b64HK2np and 64HK2D2 b(n\u2212M\u22122) \u2264 4R(w?) \u03b3(n\u2212M\u22122)p ,\n\u2264 64eHK 2\u03b3 b(n\u2212 1)1\u2212p ( L0(n\u2212M \u2212 2) +\n6R(w?)\n\u03b3(n\u2212M \u2212 2)p +\n64HK2D2 log n\nb\n+ 256HK2\u03b3L(w?)(n\u2212 1)p+1\nb\n) + 64HK2\u03b3L(w?)(n\u2212 1)p\nb +\n4R(w?)\n\u03b3(n\u2212 1)p+1 +\n64HK2D2\nb(n\u2212 1)\nWe now optimize over the choice of M above by using\n(n\u2212M \u2212 2) = ( 6R(w?)\n\u03b3L0\n) 1 p+1\nOfcourse for the choice of M to be valid we need that n\u2212M \u2212 2 \u2264 n which gives our second condition on \u03b3 which is\n\u03b3 \u2265 6R(w ?)\nnp+1L0 (13)\nPlugging in this M we get,\nan \u2264 64eHK2\u03b3\nb(n\u2212 1)1\u2212p\n( 2L p p+1\n0\n( 6R(w?)\n\u03b3\n) 1 p+1\n+ 128HK2\u03b3L(w?)(n\u2212 1)p+1\nb +\n64HK2D2 log n\nb\n)\n+ 64HK2\u03b3L(w?)(n\u2212 1)p\nb +\n2R(w?)\n\u03b3(n\u2212 1)p+1 +\n64HK2D2\nb(n\u2212 1)\n= 128eHK2\u03b3\np p+1L\np p+1 0 (6R(w ?)) 1 p+1\nb(n\u2212 1)1\u2212p +\n2e(64HK2\u03b3)2L(w?)(n\u2212 1)2p\nb2 +\n2R(w?)\n\u03b3(n\u2212 1)p+1\n+ 2e(64HK2)2D2\u03b3 log n\nb2(n\u2212 1)1\u2212p +\n64HK2\u03b3L(w?)(n\u2212 1)p\nb +\n64HK2D2\nb(n\u2212 1)\nhowever by condition in Equation 11, \u03b3 \u2264 b64HK2np , hence\n\u2264 348HK 2\u03b3\np p+1L\np p+1 0 (6R(w ?)) 1 p+1\nb(n\u2212 1)1\u2212p +\n2e(64HK2)2D2\u03b3 log n\nb2(n\u2212 1)1\u2212p\n+ 348HK2\u03b3L(w?)(n\u2212 1)p\nb +\n2R(w?)\n\u03b3(n\u2212 1)p+1 +\n64HK2D2\nb(n\u2212 1) (14)\nWe shall try to now optimize the above bound w.r.t. \u03b3, To this end set\n\u03b3 = min\n{ 1\n4H ,\n\u221a bR(w?)\n174HK2L(w?)(n\u2212 1)2p+1 ,\n( b\n1044HK2(n\u2212 1)2p\n) p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1 } (15)\nWe first need to verify that this choice of \u03b3 satisfies the conditions in Equation 11 and 13. To this end, note that as for the condition in Equation 11,\n\u03b3 \u2264 (\nb\n1044HK2(n\u2212 1)2p\n) p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1\nand hence it can be easily verified that for n \u2265 3, \u03b3 \u2264 b64HK2np . On the other hand to verify the condition in Equation 13, we need to show that\n\u03b3 = min\n{ 1\n4H ,\n\u221a bR(w?)\n174HK2L(w?)(n\u2212 1)2p+1 ,\n( b\n1044HK2(n\u2212 1)2p\n) p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1 }\n\u2265 6R(w ?)\nnp+1 (L0)\nIt can be verified that this condition is satisfied as long as, n \u2265 max { 3, 87K2L(w?)\nb ,\n783K2\nb } So in effect as long as n \u2265 3 and sample size nb \u2265 max{783K2, 87K\n2L(w?) HD2 } the conditions are satisfied. Now\nplugging in this choice of \u03b3 into the bound in Equation 14, we get\nan \u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n2\n3\n( 6264HK2R(w?)L p p+1\n0\nb(n\u2212 1)\n) p+1 2p+1\n+ 64HK2D2\nb(n\u2212 1)\n+ 8HR(w?)\n(n\u2212 1)p+1 +D2 log(n)\n( 64HK2\nb(n\u2212 1)\n) 3p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1\n\u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n2\n3\n( 6264HK2R(w?)L p p+1\n0\nb(n\u2212 1)\n) p+1 2p+1\n+ 64HK2D2\nb(n\u2212 1)\n+ 8HR(w?)\n(n\u2212 1)p+1 +D2 log(n)\n( 64HK2\nb(n\u2212 1)\n) 3p+1 2p+1 ( 6R(w?)\nL0\n) p 2p+1\n\u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n2\n3\n( 6264HK2R(w?)L p p+1\n0\nb(n\u2212 1)\n) p+1 2p+1\n+ 64HK2D2\nb(n\u2212 1)\n+ 8HR(w?)\n(n\u2212 1)p+1 +\n(( 96K2 ) p p+1 D2\nR(w?)\n) p+1 2p+1 (\n64HK2R(w?)\nb(n\u2212 1)\n) log(n)\n(b(n\u2212 1)) p 2p+1\n\u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n4176HK2R(w?)\nb(n\u2212 1)\n( L0\n6264HK2R(w?)\n) p 2p+1\n(b(n\u2212 1)) p 2p+1 + 64HK2D2\nb(n\u2212 1)\n+ 8HR(w?)\n(n\u2212 1)p+1 +\n( 64HK2D2\nb(n\u2212 1)\n) log(n)\n(b(n\u2212 1)) p 2p+1\n( 384HK2R(w?)\nL0\n) p 2p+1\n\u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n4176HK2R(w?)\nb(n\u2212 1)\n( L0\n6264HK2R(w?)\n) p 2p+1\n(b(n\u2212 1)) p 2p+1 + 64HK2D2\nb(n\u2212 1)\n+ 8HR(w?)\n(n\u2212 1)p+1 +\n( 64HK2D2\nb(n\u2212 1)\n) log(n)\n(b(n\u2212 1)) p 2p+1\nPicking\np = min { max { log(b)\n2 log(n\u2212 1) ,\nlog log(n)\n2 (log(b(n\u2212 1))\u2212 log log(n))\n} , 1 }\nwe get the bound,\nan \u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n( 4176HK2R(w?)\u221a\nb(n\u2212 1) +\n4176HK2R(w?) \u221a log(n)\nb(n\u2212 1)\n)( L0\n6264HK2R(w?)\n) 1 3\n+ 120HK2D2\nb(n\u2212 1) +\n8HR(w?)\n(n\u2212 1)2 + 8HR(w?)\u221a b(n\u2212 1) + 64HK2D2\n\u221a log(n)\nb(n\u2212 1)\n\u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n( 4176HK2R(w?)\u221a\nb(n\u2212 1) +\n4176HK2R(w?) \u221a log(n)\nb(n\u2212 1)\n)( L0\n6264HK2R(w?)\n) 1 3\n+ 120HK2D2\nb(n\u2212 1) +\n8HR(w?)\n(n\u2212 1)2 + 8HR(w?)\u221a b(n\u2212 1) + 64HK2D2\n\u221a log(n)\nb(n\u2212 1)\n\u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n454(HK2R(w?))2/3L 1 3 0\u221a\nb(n\u2212 1) +\n454(HK2R(w?))2/3L 1 3 0 \u221a log(n)\nb(n\u2212 1)\n+ 120HK2D2\nb(n\u2212 1) +\n8HR(w?)\n(n\u2212 1)2 + 8HR(w?)\u221a b(n\u2212 1) + 64HK2D2\n\u221a log(n)\nb(n\u2212 1)\nRecall that L0 = 3 2HD 2 + L(w?). Now note that if L(w?) \u2264 HK2D2/2 then L0 \u2264 2HK2D2, on the other hand if L(w?) > HK2D2/2 then (HK2R(w?))2/3L 1 3 0 \u2264 \u221a 4HK2R(w?)L(w?). Hence we can conclude that,\nan \u2264\n\u221a 2784HK2R(w?)L(w?)\nb(n\u2212 1) +\n454HK2(R(w?))2/3(2D2) 1 3\n\u221a b(n\u2212 1)\n+ 454HK2(R(w?))2/3(2D2)\n1 3 \u221a log(n)\nb(n\u2212 1)\n+ 120HK2D2\nb(n\u2212 1) +\n8HR(w?)\n(n\u2212 1)2 + 8HR(w?)\u221a b(n\u2212 1) + 64HK2D2\n\u221a log(n)\nb(n\u2212 1) +\n908 \u221a HK2R(w?)L(w?)\u221a\nb(n\u2212 1) + 908 \u221a HK2R(w?)L(w?) log(n)\nb(n\u2212 1)\nSince n > 783K2 and R(w?) \u2264 D2/2 we can conclude that\nan \u2264 164\n\u221a HK2R(w?)L(w?)\nb(n\u2212 1) +\n580HK2(R(w?))2/3D 2 3\n\u221a b(n\u2212 1)\n+ 545HK2D2\n\u221a log(n)\nb(n\u2212 1) +\n8HR(w?)\n(n\u2212 1)2\nThis concludes the proof.\nProof of Theorem 2. For Euclidean case R(w) = 12 \u2016w\u2016 2 2 and K = \u221a supw:\u2016w\u20162\u22641 \u2016w\u2016 2 = 1. Plugging these in the previous theorem (along with appropriate step size) we get\nE [L(wagn )]\u2212 L(w?) \u2264 116\n\u221a H \u2016w?\u20162 L(w?)\nb(n\u2212 1) + 366H \u2016w?\u20164/3D 23\u221a b(n\u2212 1) + 545HD2\n\u221a log(n)\nb(n\u2212 1) +\n4H \u2016w?\u20162\n(n\u2212 1)2\nThe second inequality is a direct consequence of the fact that \u2016w?\u2016 \u2264 D.\nB.3 Some Technical Lemmas Lemma B.2. Denote K := \u221a\n2 supw:\u2016w\u2016\u22641R(w), then for any x1, . . . ,xb mean zero vectors drawn iid from\nany fixed distribution,\nE \u2225\u2225\u2225\u2225\u22251b b\u2211 t=1 xt \u2225\u2225\u2225\u2225\u2225 2\n\u2217\n \u2264 K2 b2 b\u2211 t=1 E [ \u2016xt\u20162\u2217 ]\nProof. We start by noting that\u2225\u2225\u2225\u2225\u22251b i\u2211 t=1 xt \u2225\u2225\u2225\u2225\u2225 2\n\u2217\n= ( sup\nw:\u2016w\u2016\u22641\n\u2329 w, 1\nb i\u2211 t=1 xt\n\u232a)2\n= ( inf \u03b1 1 \u03b1 sup\nw:\u2016w\u2016\u22641\n\u2329 w, \u03b1\nb i\u2211 t=1 xt\n\u232a)2\n\u2264 ( inf \u03b1 { 1 \u03b1 sup w:\u2016w\u2016\u22641 R(w) + 1 \u03b1 R\u2217 ( \u03b1 b i\u2211 t=1 xt )})2\n= ( inf \u03b1 { K2 2\u03b1 + 1 \u03b1 R\u2217 ( \u03b1 b i\u2211 t=1 xt )})2 (16)\nwhere the step before last was due to Fenchel-Young inequality and R\u2217 is simply the convex conjugate of R. Now For any i \u2208 [b] define Si = R\u2217 ( \u03b1 b \u2211i t=1 xt ) . We claim that\nE [Si] \u2264 E [Si\u22121] + \u03b12 2b2 E [ \u2016xi\u20162\u2217 ] To see this note that since R is 1-strongly convex w.r.t. \u2016\u00b7\u2016, by duality R\u2217 is 1-strongly smooth w.r.t. \u2016\u00b7\u2016\u2217 and so for any i \u2208 [b],\nR\u2217\n( 1\nb i\u2211 t=1 xt\n) \u2264 R\u2217 ( 1\nb i\u22121\u2211 t=1 xt\n) + 1\n2b\n\u2329 \u2207R\u2217 ( 1\nb i\u22121\u2211 t=1 xt\n) ,xi \u232a + \u03b12\n2b2 \u2016xi\u20162\u2217\ntaking expectation w.r.t. xi and noting that E [xi] = 0 by assumption we see that\nExb [Si] \u2264 Si\u22121 + \u03b12\n2b2 Exi\n[ \u2016xi\u20162\u2217 ] Taking expectation we get as claimed that :\nE [Si] \u2264 E [Si\u22121] + \u03b12 2b2 E [ \u2016xi\u20162\u2217 ] Now using this above recursively (and noting that S0 = 0 ) we conclude that\nE [Si] \u2264 \u03b12\n2b2 i\u2211 t=1 E [ \u2016xt\u20162\u2217 ] Plugging this back in Equation 16 we get\nE \u2225\u2225\u2225\u2225\u22251b b\u2211 t=1 xt \u2225\u2225\u2225\u2225\u2225 2\n\u2217\n \u2264 (inf \u03b1 { K2 2\u03b1 + \u03b1 2b2 i\u2211 t=1 E [ \u2016xt\u20162\u2217 ]})2\n= ( inf \u03b1 { K2 2\u03b1 + \u03b1 2b2 i\u2211 t=1 E [ \u2016xi\u20162\u2217 ]})2 = K2 b2 i\u2211 t=1 E [ \u2016xt\u20162\u2217 ]\nLemma B.3. Consider a sequence of non-negative number a1, . . . , an \u2208 [0, a0] that satisfy\nan \u2264 A(n) n\u22121\u2211 i=1 ai +B(n)\nwhere A is decreasing in n. For such a sequence, for any m \u2208 [n], as long as A(i) \u2264 1/2 for any i \u2265 n\u2212m\u22121 and \u2211n i=n\u2212m\u22121A(i) \u2264 1 then\nan \u2264 eA(n) ( a0(n\u2212m) +\nn\u2211 i=n\u2212m\u22121 B(i)\n) +B(n)\nProof. We shall unroll this recursion. Note that\nan \u2264 A(n) n\u22121\u2211 i=1 ai +B(n)\n= A(n) ( n\u22122\u2211 i=1 ai + an\u22121 ) +B(n)\n\u2264 A(n) ( n\u22122\u2211 i=1 ai +A(n\u2212 1) n\u22122\u2211 i=1 ai +B(n\u2212 1) ) +B(n)\n= A(n)(1 +A(n\u2212 1)) n\u22122\u2211 i=1 ai +B(n) +A(n)B(n\u2212 1)\n\u2264 A(n)(1 +A(n\u2212 1)) ( n\u22123\u2211 i=1 ai +A(n\u2212 2) n\u22123\u2211 i=1 ai +B(n\u2212 2) ) + +B(n) +A(n)B(n\u2212 1)\n= A(n)(1 +A(n\u2212 1))(1 +A(n\u2212 2)) n\u22123\u2211 i=1 ai +B(n) +A(n)B(n\u2212 1) +A(n)(1 +A(n\u2212 1))B(n\u2212 2)\nContinuing so upto m steps we get\nan \u2264 A(n) ( m\u22121\u220f i=1 (1 +A(n\u2212 i)) ) n\u2212m\u2211 i=1 ai +B(n) +A(n) m\u22121\u2211 i=1 i\u22121\u220f j=1 (1 +A(n\u2212 j)) B(n\u2212 i)  (17)\nWe would now like to bound in general the term \u220fm\u22121 i=1 (1 +A(n\u2212 i)). To this extant note that,\nm\u22121\u220f i=1 (1 +A(n\u2212 i)) = exp ( m\u22121\u2211 i=1 log(1 +A(n\u2212 i)) )\nNow assume A(i) \u2264 1/2 for all i \u2265 n\u2212m\u2212 1 so that log(1 +A(n\u2212 i)) \u2264 A(n\u2212 i). We get\nm\u22121\u220f i=1 (1 +A(n\u2212 i)) \u2264 exp ( m\u22121\u2211 i=1 A(n\u2212 i) )\nNow if \u2211n i=n\u2212m\u22121A(i) \u2264 1 then we can conclude that\nm\u22121\u220f i=1 (1 +A(n\u2212 i)) \u2264 e\nPlugging this in Equation B.3 we get\nan \u2264 eA(n) ( n\u2212m\u2211 i=1 ai + m\u22121\u2211 i=1 B(n\u2212 i) ) +B(n)\n= eA(n) ( n\u2212m\u2211 i=1 ai + n\u2211 i=n\u2212m\u22121 B(i) ) +B(n)\nNow if for each i \u2264 n, ai \u2264 a0 then we see that\nan \u2264 eA(n) ( a0(n\u2212m) +\nn\u2211 i=n\u2212m\u22121 B(i)\n) +B(n)\nHence we conclude that as long as \u2211n i=n\u2212m\u22121A(i) \u2264 1\nan \u2264 eA(n) ( a0(n\u2212m) +\nn\u2211 i=n\u2212m\u22121 B(i)\n) +B(n)"}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "<lb>Mini-batch algorithms have been proposed as a way to speed-up stochastic convex optimization<lb>problems. We study how such algorithms can be improved using accelerated gradient methods. We<lb>provide a novel analysis, which shows how standard gradient methods may sometimes be insufficient to<lb>obtain a significant speed-up and propose a novel accelerated gradient algorithm, which deals with this<lb>deficiency, enjoys a uniformly superior guarantee and works well in practice.", "creator": "LaTeX with hyperref package"}}}