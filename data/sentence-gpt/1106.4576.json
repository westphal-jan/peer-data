{"id": "1106.4576", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2011", "title": "Expert-Guided Subgroup Discovery: Methodology and Application", "abstract": "This paper presents an approach to expert-guided subgroup discovery. The main step of the subgroup discovery process, the induction of subgroup descriptions, is performed by a heuristic beam search algorithm, using a novel parametrized definition of rule quality which is analyzed in detail. The other important steps of the proposed subgroup discovery process are the detection of statistically significant properties of selected subgroups and subgroup visualization: statistically significant properties are used to enrich the descriptions of induced subgroups, while the visualization shows subgroup properties in the form of distributions of the numbers of examples in the subgroups.\n\n\n\n\nThe current paper provides a useful and well-explained technique to describe the methods used to determine the effect of a given subgroup discovery method. In this paper we will introduce a new subgroup discovery method (subgroup discovery), which is called subgroup discovery. The new subgroup discovery method is used in conjunction with the existing method, which is called subgroup discovery.\nThe basic approach is to create the following subgroup discovery methods:\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSubgroup discovery\nSub", "histories": [["v1", "Wed, 22 Jun 2011 20:59:50 GMT  (149kb)", "http://arxiv.org/abs/1106.4576v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["d gamberger", "n lavrac"], "accepted": false, "id": "1106.4576"}, "pdf": {"name": "1106.4576.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Dragan Gamberger"], "emails": [], "sections": [{"heading": null, "text": "Expert-Guided Subgroup Dis overy:\nMethodology and Appli ation\nDragan Gamberger dragan.gamberger irb.hr Rudjer Bo skovi Institute, Bijeni ka 54 10000 Zagreb, Croatia\nNada Lavra nada.lavra ijs.si Jo zef Stefan Institute, Jamova 39 1000 Ljubljana, Slovenia\nAbstra t\nThis paper presents an approa h to expert-guided subgroup dis overy. The main step of the subgroup dis overy pro ess, the indu tion of subgroup des riptions, is performed by a heuristi beam sear h algorithm, using a novel parametrized de nition of rule quality whi h is analyzed in detail. The other important steps of the proposed subgroup dis overy pro ess are the dete tion of statisti ally signi ant properties of sele ted subgroups and subgroup visualization: statisti ally signi ant properties are used to enri h the des riptions of indu ed subgroups, while the visualization shows subgroup properties in the form of distributions of the numbers of examples in the subgroups. The approa h is illustrated by the results obtained for a medi al problem of early dete tion of patient risk groups."}, {"heading": "1. Introdu tion", "text": "This paper addresses the problem of subgroup dis overy whi h an be de ned as: given a population of individuals and a property of those individuals we are interested in, nd population subgroups that are statisti ally `most interesting', e.g., are as large as possible and have the most unusual statisti al (distributional) hara teristi s with respe t to the property of interest (Kl osgen, 1996; Wrobel, 1997, 2001). Its main ontribution is a new methodology supporting the pro ess of expert-guided subgroup dis overy. Spe i ally, we introdu e a novel parametrized de nition of rule quality used in a heuristi beam sear h algorithm, a rule subset sele tion algorithm in orporating example weights, the dete tion of statisti ally signi ant properties of sele ted subgroups, and a novel subgroup visualization method. An in-depth analysis of the proposed quality measure is provided as well. The proposed methodology has been applied to the medi al problem of dete ting and des ribing patient groups with high risk for artheros leroti oronary heart disease (CHD). 1\nThe paper organization is as follows. Algorithms for subgroup dete tion and sele tion, whi h are the main ingredients of the expert-guided subgroup dis overy methodology, are des ribed in Se tion 2. Se tion 3 presents: the oronary heart disease risk group dete tion problem, the dis overed patient risk groups, their statisti al hara terization, visualization, medi al interpretation and evaluation, in luding a dis ussion on the expert's role in\n1. Algorithms for subgroup dete tion and sele tion have been implemented in the on-line Data Mining Server\n(Gamberger & Smu , 2001), publi ly available at http://dms.irb.hr whi h an be tested in domains with up to 250 examples. A more sophisti ated implementation of the algorithms is not available for publi use.\n2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nthe subgroup dis overy pro ess. Se tion 4 provides an in-depth analysis of the proposed rule quality measure for subgroup dis overy in luding an experimental omparison with a sele ted ost-based quality measure. Finally, Se tion 5 provides links to the related work."}, {"heading": "2. Subgroup Dis overy: Rule Indu tion and Sele tion", "text": "This se tion des ribes the two main steps of the overall subgroup dis overy pro ess: indu - tion and sele tion of interesting subgroups. These two steps, as well as the whole des riptive indu tion pro ess assume a tive expert involvement."}, {"heading": "2.1 The Task of Expert-Guided Subgroup Dis overy", "text": "The task of expert-guided subgroup dis overy addressed in this work di ers slightly from the subgroup dis overy task de ned in Se tion 1 and proposed by (Kl osgen, 1996; Wrobel, 1997). Instead of de ning an optimal measure for automated subgroup sear h and sele tion, here the goal is to support the expert in performing exible and e e tive sear h of a broad range of optimal solutions. As a onsequen e, the de ision of whi h subgroups will be sele ted to form the nal solution is left to the expert. The task of the subgroup dis overy algorithm is to enable the dete tion of rules des ribing potentially optimal subgroups, whi h are hara terized by the property that they are orre t for many target lass ases (patients with oronary heart disease, in the example domain used in this work) and in orre t for all, or most of, non-target lass ases (healthy subje ts). Target lass ases in luded into a subgroup are alled true positives while non-target lass ases in orre tly in luded into a subgroup are alled false positives.\nThe parti ular expert-guided subgroup dis overy task addressed in this work assumes the ollaboration of the expert and the data analyst in repeatedly running a subgroup dis overy algorithm with a goal of nding rules des ribing population subgroups whi h:\nhave su\u00c6 iently large overage,\nhave a positive bias towards target lass ase overage (have a su\u00c6 iently large true positive/false positive ratio)\nare su\u00c6 iently diverse for dete ting most of the target population, and\nful ll other experts' subje tive measures of a eptability: understandability, simpli ity and a tionability.\nIn ea h iteration, the task of the subgroup dis overy algorithm is to suggest one or more potentially optimal solutions. Se tion 2.2 des ribes a heuristi sear h algorithm SD, whi h an be used to onstru t many rules that are optimal with respe t to an expert sele ted generalization parameter. Sin e many of the indu ed rules an be very similar, both in terms of their overage and the sele ted features, the RSS algorithm des ribed in Se tion 2.3 an be used to sele t a small number of distin t rules that are o ered to the expert as potentially optimal solutions. Alternatively, subgroup dis overy an be implemented within a `weighted' overing algorithm DMS, as is the ase in the publi ly available Data Mining Server (Gamberger & Smu , 2001), whi h generates up to three best subgroups in every iteration."}, {"heading": "2.2 The Subgroup Dis overy Algorithm", "text": "The goal of the subgroup dis overy algorithm SD, outlined in Figure 1, is to sear h for rules that maximize q\ng\n=\nTP\nFP+g\n, where TP are true positives, FP are false positives, and g is a\ngeneralization parameter. High quality rules over many target lass examples and a low number of non-target examples. The number of tolerated non-target lass ases, relative to the number of overed target lass ases, is determined by parameter g. For low g (g 1), indu ed rules will have high spe i ity (low false alarm rate) sin e overing of every single non-target lass example is made relatively very `expensive'. On the other hand, by sele ting a high g value (g > 10 for small domains), more general rules will be generated, overing also non-target lass instan es.\nAlgorithm SD: Subgroup Dis overy Input: E = P [N (E training set, jEj training set size,\nP positive (target lass) examples, N negative (non-target lass) examples)\nL set of all de ned features (attribute values), l 2 L\nParameter: g (generalization parameter, 0:1 < g, default value 1)\nmin support (minimal support for rule a eptan e) beam width (maximal number of rules in Beam and New Beam)\nOutput: S = fTargetClass Condg (set of rules formed of beam width best onditions Cond) (1) for all rules in Beam and New Beam (i = 1 to beam width) do\ninitialize ondition part of the rule to be empty, Cond(i) fg initialize rule quality, q\ng\n(i) 0\n(2) while there are improvements in Beam do (3) for all rules in Beam (i = 1 to beam width) do (4) for all l 2 L do (5) form a new rule by forming a new ondition as a onjun tion of the\nVarying the value of g enables the expert to guide subgroup dis overy in the TP=FP spa e, in whi h FP (plotted on the X-axis) needs to be minimized, and TP (plotted on the Y -axis) needs to be maximized. The TP=FP spa e is similar to the ROC (Re eiver Operating Chara teristi ) spa e (Provost & Faw ett, 2001). The omparison of the ROC and TP=FP spa e and the g\nq\nheuristi are analyzed in detail in Se tions 2.4 and 4, respe tively.\nAlgorithm SD takes as its input the omplete training set E and the feature set L, where features l 2 L are logi al onditions onstru ted from attribute values des ribing the examples in E. For dis rete ( ategori al) attributes, features have the form Attribute = value or Attribute 6= value, for numeri al attributes they have the form Attribute > value or Attribute < value. To formalize feature onstru tion, let values v\nix\n(x = 1::k\nip\n) denote\nthe k\nip\ndi erent values of attribute A\ni\nthat appear in the positive examples and w\niy\n(y =\n1::k\nin\n) the k\nin\ndi erent values of A\ni\nappearing in the negative examples. A set of features\nL is onstru ted as follows:\nFor dis rete attributes A\ni\n, features of the form A\ni\n= v\nix\nand A\ni\n6= w\niy\nare generated.\nFor ontinuous attributes A\ni\n, similar to Fayyad and Irani (1992), features of the form\nA\ni\n(v\nix\n+ w\niy\n)=2 are reated for all neighboring value pairs (v\nix\n; w\niy\n), and features\nA\ni\n> (v\nix\n+ w\niy\n)=2 for all neighboring pairs (w\niy\n; v\nix\n).\nFor integer valued attributes A\ni\n, features are generated as if A\ni\nwere both dis rete\nand ontinuous, resulting in features of four di erent forms: A\ni\n(v\nix\n+ w\niy\n)=2,\nA\ni\n> (v\nix\n+ w\niy\n)=2, A\ni\n= v\nix\n, and A\ni\n6= w\niy\n.\nThere is no theoreti al upper value for the user-re ned g parameter, but in pra ti e the suggested upper limit should not ex eed the number of training examples. For instan e, suggested g values in the Data Mining Server are in the range between 0.1 and 100, for analysing data sets of up to 250 examples. The hoi e of g should be adjusted both to the size of the data set and to the proportion of positive examples in the set.\nAlgorithm SD has two additional parameters whi h are typi ally not adjusted by the\nuser. The rst is min support (default value is\np\nP=E, where P is the number of target\nlass examples in E) whi h indire tly de nes the minimal number of target lass examples whi h must be overed by every subgroup. The se ond is beam width (default value is 20) whi h de nes the number of solutions kept in ea h iteration. The output of the algorithm is set S of beam width di erent rules with highest q\ng\nvalues. The rules have the form of\nonjun tions of features from L.\nThe algorithm initializes all the rules in Beam and New beam by empty rule onditions.\nTheir quality values q\ng\n(i) are set to zero (step 1). Rule initialization is followed by an in nite\nloop (steps 2{12) that stops when, for all rules in the beam, it is no longer possible to further improve their quality. Rules an be improved only by onjun tively adding features from L. After the rst iteration, a rule ondition onsists of a single feature, after the se ond iteration up to two features, and so forth. The sear h is systemati in the sense that for all rules in the beam (step 3) all features from L (step 4) are tested in ea h iteration. For every new rule, onstru ted by onjun tively adding a feature to rule body (step 5) quality q\ng\nis omputed (step 6). If the support of the new rule is greater than min support and\nif its quality q\ng\nis greater than the quality of any rule in New beam, the worst rule in\nNew beam is repla ed by the new rule. The rules are reordered in New beam a ording to their quality q\ng\n. At the end of ea h iteration, New beam is opied into Beam (step 11).\nWhen the algorithm terminates, the rst rule in Beam is the rule with maximum q\ng\n.\nA ne essary ondition (in step 7) for a rule to be in luded in New beam is that it must be relevant. The new rule is irrelevant if there exists a rule R in New beam su h that true positives of the new rule are a subset of true positives of R and false positives of the new rule\nare a superset of false positives of R. A detailed analysis of relevan e, presented by Lavra , Gamberger, and Turney (1998), is out of the main s ope of this paper. After the new rule is in luded in New beam it may happen that some of the existing rules in New beam be ome irrelevant with respe t to this new rule. Su h rules are eliminated from New beam during its reordering (in step 8). The testing of relevan e ensures that New beam ontains only di erent and relevant rules.\nIn Algorithm SD, rule quality measure q\ng\nserves two purposes: rst, rule evaluation, and\nse ond, evaluation of features and their onjun tions with high potential for the onstru tion of high quality rules in subsequent iterations. The analysis of this quality measure in Se tion 4 shows that for the rst purpose, a measure assigning di erent osts to false positives and false negatives ould perform equally well, but for the purpose of guiding the sear h the q\ng\nmeasure is advantageous."}, {"heading": "2.3 Rule Subset Sele tion", "text": "This se tion des ribes how to redu e the number of generated rules to a relatively small number of diverse rules. Redu ing the rule set is desirable be ause expe ting experts to evaluate a large set of rules is unfeasible, and se ond, experiments demonstrate that there are subsets of very similar rules whi h use almost the same attribute values and have similar predi tion properties.\nThe weighted overing approa h proposed for on rmation rule subset sele tion (Gamberger & Lavra , 2000) de nes diverse rules as those that over diverse sets of target lass examples. The approa h, implemented in Algorithm RSS outlined in Figure 2, an not guarantee statisti al independen e of the sele ted rules, but it ensures the diversity of generated subsets.\nAlgorithm RSS: Rule Subset Sele tion Input: S set of rules for the target lass\nP target lass examples\nParameter: number (required number of sele ted rules in output set SS) Output: SS set of relatively independent rules for the target lass (1) initialize SS fg (empty set of sele ted rules) (2) for every e 2 P do (e) 1 (3) repeat number times (4) sele t from S the rule with the highest weight P 1= (e) where summation is\nover the set P\n0\nP of target lass examples overed by the rule\n(5) for every e 2 P\n0\novered by the sele ted rule\ndo (e) (e) + 1\n(6) eliminate the sele ted rule from S (7) add the sele ted rule into set SS (8) end repeat\nFigure 2: Heuristi rule subset sele tion algorithm.\nInput to Algorithm RSS are the set of all target lass examples P and the set of rules S. Its output is a redu ed set of rules SS, SS S. The user adjustable parameter number determines how many rules will be sele ted for in lusion in output set SS. For every example e 2 P there is a ounter (e). Initially, the output set of sele ted rules is empty (step 1) and all ounter values are set to 1 (step 2). Next, in ea h iteration of the loop (steps 3 to 8), one rule is added to the output set (step 7). From set S, the rule with the highest weight value is sele ted. For ea h rule, weight is omputed so that 1= (e) values are added for all target lass examples overed by this rule (step 4). After rule sele tion, the rule is eliminated from set S (step 6) and (e) values for all target lass examples overed by the sele ted rule are in remented by 1 (step 5). This is the entral part of the algorithm whi h ensures that in the rst iteration all target lass examples ontribute the same value 1= (e) = 1 to the weight, while in the following iterations the ontributions of examples are inverse proportional to their overage by previously sele ted rules. In this way the examples already overed by one or more sele ted rules de rease their weights while rules overing many yet un overed target lass examples whose weights have not been de reased will have a greater han e to be sele ted in the following iterations.\nIn the publi ly available Data Mining Server, RSS is implemented in an outer loop for SD. Figure 3 gives the pseudo ode of algorithm DMS. In its inner loop, DMS alls SD and sele ts from its beam the single best rule to be in luded into the output set SS. To enable SD to indu e a di erent solution at ea h iteration, example weights (e) are introdu ed and used in the quality measure whi h is de ned as follows:\nq\ng\n=\nP\nTP\n1 (e)\nFP + g\n:\nThis is the same quality measure as in SD ex ept that the weights of true positive examples are not onstant and equal to 1 but de ned by expression 1\n(e)\n, hanging from iteration to\niteration.\nThe main reason for the des ribed implementation is to ensure the diversity of indu ed subgroups even though, be ause of the short exe ution time limit on the publi ly available server, a low beam width parameter value in Algorithm SD had to be set (the default value is 20). Despite the favorable diversity of rules a hieved through Algorithm DMS, the approa h has also some drawba ks. The rst drawba k is that the same rule an be dete ted in di erent iterations of Algorithm DMS, despite of the hanges in the (e) values. The more important drawba k is that heuristi sear h with a small beam width value may prevent the dete tion of some good quality subgroups. Therefore during exploratory appli ations, applying a single SD exe ution with a large beam width followed by a single run of RSS appears to be a better approa h."}, {"heading": "2.4 Subgroup Sear h and Evaluation in the ROC and TP/FP Spa e", "text": "The goal of this se tion is to larify the relation between the ROC spa e whi h is usually used for evaluating lassi er performan e, and the TP=FP spa e whi h is being sear hed by the q\ng\nheuristi in the SD algorithm.\nEvaluation of indu ed subgroups in the ROC spa e (ROC: Re eiver Operating Chara teristi , Provost & Faw ett, 2001) shows their performan e in terms of TPr and FPr,\nAlgorithm DMS: Data Mining Server subgroup onstru tion Input: E = P [N (E training set, jEj training set size,\nP positive (target lass) examples, N negative (non-target lass) examples)\nL set of all de ned features (attribute values), l 2 L\nParameter: number (required number of sele ted rules\nin output set SS) g (generalization parameter, 0:1 < g < 100, default value 1) min support (minimal support for rule a eptan e) beam width (number of rules in the beam)\nOutput: SS set of relatively independent rules for the target lass (1) initialize SS fg (empty set of sele ted rules) (2) for every e 2 P do (e) 1 (3) repeat number times (4) all Algorithm SD to onstru t a rule with maximal\nquality q\ng\n=\nP\nTP\n1 (e)\nFP+g\n(5) for every e 2 P\n0\novered by the onstru ted rule\ndo (e) (e) + 1\n(6) add the onstru ted rule into set SS (7) end repeat\nFigure 3: Iterative subgroup onstru tion in the Data Mining Server.\nwhere TPr is the sensitivity of a lassi er measuring the fra tion of positive ases that are lassi ed as positive, and FPr is the false alarm measuring the fra tion of in orre tly\nlassi ed negative ases: TPr =\nTP\nTP+FN\n=\nTP Pos , and FPr = FP TN+FP = FP Neg . A point in the\nROC spa e shows lassi er performan e in terms of false alarm rate FPr (plotted on the X-axis) that should be as low as possible, and sensitivity TPr (plotted on the Y -axis) that should be as high as possible (see Figure 5 in Se tion 3.2).\nThe ROC spa e is appropriate for measuring the su ess of subgroup dis overy, sin e subgroups whose TPr=FPr tradeo is lose to the diagonal an be dis arded as uninteresting. Conversely, interesting rules/subgroups are those su\u00c6 iently distant from the diagonal. Those rules whi h are most distant from the diagonal de ne the points in the ROC spa e from whi h a onvex hull is onstru ted. The area under the ROC urve de ned by subgroups with the best TPr=FPr tradeo an be used as a quality measure for omparing the su ess of di erent learners or subgroup miners. In subgroup onstru tion, the data analyst an try to a hieve the desired TPr=FPr tradeo by building rules using di erent data mining algorithms, by di erent parameter settings of a sele ted data mining algorithm or by applying a ost-sensitive data mining algorithm that takes into the a ount di erent mis lassi ation osts.\nThe q\ng\nmeasure in the SD algorithm that needs to be maximized, tries to nd subgroups\nthat are as far as possible from the diagonal of the ROC spa e in the dire ion of the left upper orner (with TPr equal to 100% and FPr equal to 0%). Note, however, that the a tual omputation, as implemented in Algorithm SD, is not performed in terms of TPr and\nFPr, as assumed in the ROC analysis, but rather in terms of TP and FP in the so- alled TP=FP spa e. The reason is the improved omputational e\u00c6 ien y of omputing the q\ng\nvalue whi h is used as a sear h heuristi for omparing the quality of rules for a given, xed domain. For a xed domain, the TP=FP spa e is as appropriate as the ROC spa e: the ROC spa e is namely equivalent to the normalized TP=FP spa e where Pos and Neg are normalization onstants for Y and X axes, respe tively. The TP=FP spa e and the ROC spa e are illustrated in Se tion 3.2 by Figures 4 and 5, respe tively."}, {"heading": "3. The Des riptive Indu tion Pro ess", "text": "The indu tion of subgroups, des ribed in Se tion 2.2, represents the main step of the proposed des riptive indu tion pro ess. This step orresponds to the data mining step of the standard pro ess of knowledge dis overy in databases (KDD). The overall des riptive indu - tion pro ess, proposed in this paper, is omparable to the standard KDD pro ess (Fayyad, Piatetsky-Shapiro, & Smyth, 1996), with some parti ularities of the task of subgroup dis-\novery.\nThe proposed expert-guided subgroup dis overy pro ess onsists of the following steps:\n1. problem understanding\n2. data understanding and preparation\n3. subgroup dete tion\n4. subgroup subset sele tion\n5. statisti al hara terization of subgroups\n6. subgroup visualization\n7. subgroup interpretation\n8. subgroup evaluation\nSe tion 3.1, illustrating steps 1 and 2, presents a medi al problem used as a ase study for applying the proposed des riptive indu tion methodology. Tools for supporting subgroup dete tion and sele tion in steps 3 and 4 were des ribed in detail in Se tions 2.2 and 2.3, while the results of expert-guided subgroup dete tion and sele tion are outlined in Se - tion 3.2. Methods and results of steps 5{8 for this domain are outlined in Se tions 3.3{3.6, respe tively.\nThe proposed des riptive indu tion pro ess is iterative and intera tive. It is iterative, sin e many steps may need to be repeated before a satisfa tory solution is found. It is also intera tive, assuming expert's involvement in most of the phases of the proposed des riptive indu tion pro ess. The expert's role in the patient risk group dete tion appli ation is des ribed in Se tion 3.7."}, {"heading": "3.1 The Problem of Patient Risk Group Dete tion", "text": "Early dete tion of artheros leroti oronary heart disease (CHD) is an important and dif-\nult medi al problem. CHD risk fa tors in lude artheros leroti attributes, living habits, hemostati fa tors, blood pressure, and metaboli fa tors (Goldman et al., 1996). Their s reening is performed in general pra ti e by data olle tion in three di erent stages.\nA Colle ting anamnesti information and physi al examination results, in luding risk fa -\ntors like age, positive family history, weight, height, igarette smoking, al ohol onsumption, blood pressure, and previous heart and vas ular diseases.\nB Colle ting results of laboratory tests, in luding information about risk fa tors like lipid\npro le, glu ose toleran e, and trombogeni fa tors.\nC Colle ting ECG at rest test results, in luding measurements of heart rate, left ven-\ntri ular hypertrophy, ST segment depression, ardia arrhythmias and ondu tion disturban es.\nOur goal was to onstru t at least one relevant and interesting subgroup, alled a pattern in the rest of the work, for ea h stage, A, B, and C, respe tively.\nA database with 238 patients representing typi al medi al pra ti e in CHD diagnosis, olle ted at the Institute for Cardiovas ular Prevention and Rehabilitation, Zagreb, Croatia, was used for subgroup dis overy. The database is in no respe t a good epidemiologi al CHD database re e ting a tual CHD o urren e in a general population, sin e about 50% of gathered patient re ords represent CHD patients. Nevertheless, the database is very valuable sin e it in ludes re ords of di erent types of the disease. Moreover, the in luded negative ases (patients who do not have CHD) are not randomly sele ted persons but individuals with some subje tive problems or those onsidered by general pra titioners as potential CHD patients, and hen e sent for further investigations to the Institute. This biased data set is appropriate for CHD risk group dis overy, but it is inappropriate for measuring the su ess of CHD risk dete tion and for subgroup performan e estimation in general medi al pra ti e."}, {"heading": "3.2 Results of Expert-Guided Subgroup Dete tion and Sele tion", "text": "The pro ess of expert-guided subgroup dis overy was performed as follows. For every data stage A, B and C, the DMS algorithm was run for values g in the range 0.5 to 100, and a\nxed number of sele ted output rules equal to 3. The rules indu ed in this iterative pro ess were shown to the expert for sele tion and interpretation. The inspe tion of 15{20 rules for ea h data stage triggered further experiments. Con rete suggestions of the medi al expert involved in this study were to limit the number of features in the rule body and to try to avoid the generation of rules whose features would involve expensive and/or unreliable laboratory tests. Consequently, we have performed the further experiments by intentionally limiting the feature spa e and the number of iterations in the main loop of the SD algorithm (steps 2-12 of Algorithm SD).\nIn this iterative pro ess, the expert has sele ted ve interesting CHD risk groups. Table 1 shows the indu ed subgroups, together with the values of g and the rule signi an e. In the subgroup dis overy terminology proposed in this paper, the features appearing in the\nonditions of rules des ribing the subgroups are alled the prin ipal fa tors. The des ribed\niterative pro ess was su essful for data at stages B and C, but it turned out that anamnesti data on its own (stage A data) is not informative enough for indu ing subgroups, i.e., it failed to ful l the expert's riteria of interestingness. Only after engineering the domain, by separating male and female patients, were interesting subgroups dis overed. See Se tion 3.7 for more details on the expert's involvement in this subgroup dis overy pro ess."}, {"heading": "C1 CHD left ventri ular hypertrophy 10 99.9%", "text": "ipal fa tors. Subgroup A1 is for male patients, subgroup A2 for female patients, while subgroups B1, B2, and C1 are for male and female patients. The subgroups are indu ed from di erent attribute subsets with orresponding g parameter values given in olumn g. The last olumn Sig ontains information about the signi an e of the rules omputed by the 2 test.\nSeparately for ea h data stage, we have investigated whi h of the indu ed rules are the best in terms of the ROC spa e, i.e., whi h of them are used to de ne the ROC onvex hull. At stage B, for instan e, seven rules are on the onvex hull shown in Figures 4 and 5 for the TP=FP and the ROC spa e, respe tively. Two of these rules, X1 and X2, indi ated in the gures, are listed in Table 2. Noti e that the expert-sele ted subgroups B1 and B2 are signi ant, but are not among those lying on the onvex hull. The reason for sele ting exa tly those two rules at stage B are their simpli ity ( onsisting of three features only), their generality ( overing relatively many positive ases) and the fa t that the used features are, from the medi al point of view, inexpensive laboratory tests."}, {"heading": "3.3 Statisti al Chara terization of Subgroups", "text": "The next step in the proposed des riptive indu tion pro ess starts from the dis overed subgroups. In this step, statisti al di eren es in distributions are omputed for two populations, the target and the referen e population. The target population onsists of true positive ase (CHD patients in luded into the analyzed subgroup), whereas the referen e population are all available non-target lass examples (all the healthy subje ts).\nTP=(FP + g) at data stage B. Labels B1 and B2 denote positions of subgroups sele ted by the medi al expert, and X1 and X2 two of the seven subgroups forming the TP=FP onvex hull.\nStatisti al di eren es in distributions for all the des riptors (attributes) between these\ntwo populations is tested using the\n2\ntest with 95% on den e stage (p = 0:05). For this\npurpose numeri al attributes have been partitioned in up to 30 intervals so that in every interval there are at least 5 instan es. Among the attributes with signi antly di erent distributions there are always those that form the features des ribing the subgroups (the prin ipal fa tors), but usually there are also other attributes with signi antly di erent value distributions. These attributes are alled supporting attributes, and the features formed of their values that are hara teristi for the dis overed subgroups are alled supporting fa tors.\nSupporting fa tors are very important to a hieve pattern des riptions that are reasonably omplete and a eptable for medi al pra ti e, as medi al experts dislike short rules and prefer rules in luding as mu h supportive eviden e as possible (Kononenko, 1993).\nIn this work, the role of statisti al analysis is to dete t meaningful supporting fa tors, whereas the de ision whether they will be used to support user's on den e in the subgroup des ription is left to the expert. In the CHD appli ation the expert has de ided whether the proposed fa tors are indeed interesting, how reliable they are or how easily they an be measured in pra ti e. In Table 3, expert sele ted supporting fa tors are listed next to the individual CHD risk groups, ea h des ribed by a list of prin ipal fa tors."}, {"heading": "3.4 Subgroup Visualization", "text": "A novel visualization method an be used to visualize the output of any subgroup dis overy algorithm, provided that the output has the form of rules with a target lass in their\nonsequent. It an also be used as a method for visualizing standard lassi ation rules.\nSubgroup visualization, as des ribed in this se tion, allows us to ompare distributions of di erent subgroups. The approa h assumes the existen e of at least one numeri (or ordered dis rete) attribute of expert's interest for subgroup analysis. The sele ted attribute is plotted on the X-axis of the diagram. The Y -axis represents a lass, or more pre isely, the number of instan es of a given lass. Both dire tions of the Y -axis (Y + and Y ) are used to indi ate the number of instan es. In Figure 6, for instan e, the X-axis represents age, the Y + -axis denotes lass oronary heart disease (CHD) and Y denotes lass `healthy'\n(non-CHD). Out of four graphs at the Y\n+\nside, three represent indu ed subgroups (A1, A2\nand C1) of CHD patients, and the fourth shows the age distribution of the entire population of CHD (all CHD) patients. The graphs at the Y side show the distribution of non-CHD (all healthy) patients in the training set and the distribution of healthy subje ts in luded into the subgroup A2 (dashed line).\nOn purpose, the graphs of subgroups A1 and C1 in Figure 6 show only the overage of positive ases (CHD patients), and in Figure 7 the graph of subgroup B2 shows only\nthe overage of positive ases, whereas the graphs of A2 in Figure 6 and B1 in Figure 7 indi ate that the des riptions of subgroups over positive ases (CHD patients) as well as some negative ases (healthy individuals). Ex ept for the orre t visualization of subgroups A2 and B1 and of the entire CHD and non-CHD distribution, Figures 6 and 7 have been simpli ed in order to enable a better understanding of the visualization method, by showing just the overage of positive ases.\nIn medi al domains we typi ally use the Y\n+\nside to represent the number of positive\nases (CHD patients, in this paper) in order to reveal properties of indu ed patterns for\nsubgroups of these patients. On the other hand, the Y side is reserved to reveal properties of these same patterns (or other patterns) for the negative ases (patients without CHD). One of the advantages of using Y + and Y as proposed above is that in binary lassi ation problems the omparison of the area under the graph of a subgroup and the graph of the entire population visualizes the fra tions of TP\nPos\n=\nTP\nTP+FN\nat the Y\n+\nside (sensitivity TPr),\nand\nFP Neg = FP TN+FP at the Y side (false alarm rate FPr), where Pos and Neg stand for the\nnumbers of positive and negative ases in the entire population, respe tively. For instan e, in the visualization of subgroup B1 in Figure 7 the area under the dashed line on the Y side represents the numbers of mis lassi ed training instan es of subgroup B1. In this way, the sensitivity and false alarm rate an be estimated for pattern B1 from Figure 7. The same information for pattern B2 an be found in Figure 8, showing subgroups A1 and B2 in terms of attribute `total holesterol value'.\nThe proposed visualization method an be adapted to visualize subgroups also in terms of value distributions of dis rete/nominal attributes. An approa h to su h visualization is presented in Figure 9. However, due to bar hart representation, it is more di\u00c6 ult to\nompare several subgroups in one visualization.\nIn general, it is not ne essary that Y\n+\nand Y denote two opposite lasses. If appro-\npriate, they may denote any two lasses, or even any two di erent attribute values, whi h the expert would like to ompare."}, {"heading": "3.5 Subgroup Interpretation through Visualization", "text": "Subgroup visualization is very valuable for expert interpretation of subgroup dis overy results. From Figures 6 and 7 it an be seen that there is no signi ant di eren e between\nCHD patients and healthy subje ts regarding their age, but that there are signi ant differen es among the dete ted patterns. Figure 8 illustrates a similar e e t for the total\nholesterol values although it is known that total holesterol is an important risk fa tor for the CHD disease. This observation shows that the problem of CHD risk group dete tion an typi ally not be solved by onsidering single features and demonstrates the appropriateness of the suggested approa h whi h tries to generate subgroup des riptions whi h are a logi al\nonjun tion of a few orrelated features.\nFigure 10 is also interesting, sin e it is very di erent from other gures. Noti e that exer ise ECG ST segment depression was not used as an attribute in the training data (whi h ontained only attributes that are available at stages A, B and C); exer ise ECG ST segment depression, long term ECG re ording and e ho hardiography are not available for early risk group dete tion sin e they an be olle ted/measured only in spe ialized medi al institutions. Figure 10 learly demonstrates signi ant di eren es between all CHD and all healthy subje ts in terms of exer ise ECG ST segment depression values, demonstrating that this measurement, if available, is an ex ellent disease indi ator. But it also shows that, although it is known that patterns A1 and C1 over di erent disease subpopulations, they behave very similarly in terms of the exer ise ECG ST segment depression property."}, {"heading": "3.6 Subgroup Evaluation", "text": "In order to evaluate the dis overed risk groups, the medi al expert has tested the indu ed subgroup patterns on an independent set of 70 people (50 CHD patients and 20 non-CHD\nases from the same hospital). The results for these patients, summarized in Table 4, show that the patterns are su essful in dete ting CHD patients. About 90% of CHD patients were in luded into at least one of the ve patterns. The dete ted sensitivity values (TPr) for patterns A1, B2, and C1 are signi antly higher than the values omputed on the set of patients used for subgroup dis overy. For the other two patterns the values do not di er signi antly. Note that the a ura y values are relatively high, despite the relatively high false positive rate (FPr): a lower FPr ould have been a hieved by sele ting lower values of the generalization parameter g, at a ost of dete ting subgroups with lower overage of positive ases."}, {"heading": "3.7 The Expert's Role in Subgroup Dis overy", "text": "The CHD ase study illustrates that expert-guided indu tion is an iterative pro ess in whi h the expert an hange the requested generality of the indu ed subgroups and the subset of attributes (features) that are made available for rule onstru tion. In this way it is possible to indu e di erent patterns (subgroups) from the same data set. The sele tion of one or more subgroups representing the nal solution is left to the expert; the de ision depends both on rule predi tion properties (like the number of true positives and the tolerated number of false positives), as well as subje tive properties like the understandability, unexpe tedness and a tionability of indu ed subgroup des riptions (Silbers hatz & Tuzhilin, 1995), whi h depend on the features used in the onditions of indu ed rules. In the appli ation des ribed in this paper, the main subje tive a eptability riteria were understandability, simpli ity and a tionability.\nPartitioning the CHD risk group problem into three data stages A{C was ompletely based on the expert's understanding of the typi al diagnosti pro ess. From the ma hine learning point of view this a e ts the sele tion of subsets of attributes that are used in di erent experiments. Moreover, at data stage A the partitioning of the example set has been used as well. At this data stage there are only a few attributes that ould have been used for rule indu tion. The expert's understanding of the domain suggested that the CHD population be partitioned into two subpopulations based on the sex of patients, making it signi antly easer to indu e interesting subgroups. This partitioning resulted in patterns A1 and A2.\nAlternatively, partitioning an be performed also in the phase of performing statisti al hara terization of dis overed subgroups, by further splitting the dete ted subgroups in several parts (e.g., di erentiating between male and female patients that are true positive\nases for the subgroup) and then omparing attribute value distributions for these parts. Any signi ant di eren e in this distribution may be potentially interesting as part of the subgroup des ription. As a basis for subgroup partitioning one may use either some dete ted supporting risk fa tor or any other attribute or attribute ombination whi h is potentially interesting based on the existing expert knowledge.\nThere has been some e ort devoted also to automating the pro ess of partitioning example sets by a method of unsupervised learning, but its presentation is out of the main s ope of this work ( Smu , Gamberger, & Krsta i , 2001).\nFrom the methodologi al point of view it is interesting to noti e that the expert appreiated the indu ed subgroups overing many target lass ases (with true positive rate of at least 20%) and with false positive rate as low as possible, with the intention to keep it below 10%. But in sele ting a rule, its predi tion quality has not been the most important fa tor. The ne essary ondition for sele ting a rule was that the expert was able to re ognize\nonne tions among features building the rule that are medi ally reasonable. In this sense, short rules are signi antly more intuitive; it an be noti ed from Table 1 that all rules sele ted by the expert have at most three features de ning the prin ipal risk fa tors. The fa t that the expert did not sele t subgroups with an optimal TP=FP ratio is illustrated by\nFigures 16{18 in Se tion 4.2, whi h show the positions of the patterns A1{C1 in the TP=FP spa e and the TP=FP onvex hulls indu ed for data stages A{C, onne ting points with\nthe optimal overage properties. It an be noti ed that none of the expert sele ted patterns is lying on the TP=FP onvex hull but the sele ted patterns are lose to the onvex hull."}, {"heading": "4. Analysis of the Proposed Rule Quality Measure Used in Heuristi", "text": "Sear h\nIt is well known from the ROC analysis, that in order to a hieve the best results, the dis overed rules should be as lose as possible to the top-left orner of the ROC spa e. This means that in the TPr=FPr tradeo , TPr should be as large as possible, and FPr as small as possible. Similarly, in the TP=FP spa e, TP should be as large as possible, and FP as small as possible.\nIn this work the quality measure q\ng\n= TP=(FP + g) using generalization parameter\ng has been de ned. This se tion explains why this quality measure has been sele ted, in\nomparison with other more intuitive quality measure like a ost-based measure q involving\n` ost' parameter .\n4.1 Comparison of the q\ng\nand q Heuristi s\nOur experien e in di erent medi al appli ations indi ates that intuitions like \\how expensive is every FP predi tion in terms of additional TP predi tions made by a rule\" are useful for understanding the problem of dire ting the sear h in the TP=FP spa e. Suppose that the de nition of ost parameter is based on the following argument: \\For every additional FP , the rule should over more than additional TP examples in order to be better.\" Based on su h reasoning, it is possible to de ne a quality measure q , using the following TP=FP tradeo :\nq = TP FP:\nQuality measure q is easy to use be ause of the intuitive interpretation of parameter . It has also a ni e property when used for subgroup dis overy: by hanging the value we an move in the TP=FP spa e and sele t the optimal point based on parameter .\nIn Algorithm SD, the quality measure q\ng\n, using a di erent TP=FP tradeo is used:\nq\ng\n= TP=(FP + g), where g is the generalization parameter.\nIf a subgroup dis overy algorithm employs exhaustive sear h (or if all points in the\nTP=FP spa e are known in advan e) then the two measures q\ng\nand q are equivalent in\nthe sense that every optimal solution lying on the onvex hull an be dete ted by using any of the two heuristi s; only the values that must be sele ted for parameters g and are di erent. In this ase, q might be even better be ause its interpretation is more intuitive.\nHowever, sin e Algorithm SD is a heuristi beam sear h algorithm, the situation is di erent. Subgroup dis overy is an iterative pro ess, performing one or more iterations (typi ally 2{5) until good rules are onstru ted by forming onjun tions of features in the rule body. In this pro ess, a rule quality measure is used for rule sele tion (for whi h the two measures q\ng\nand q are equivalent) as well as for the sele tion of features and their\nonjun tions that have high potential for the onstru tion of high quality rules in subsequent\niterations; for this use, rule quality measure q\ng\nis better than q . Let us explain why.\nSuppose that we have a point (a rule) R in the TP=FP spa e, where TP and FP are\nits true and false positives, respe tively. For a sele ted g value, q\ng\nan be determined for\nthis rule R. It an be shown that all points that have the same quality q\ng\nas rule R lie on\na line de ned by the following fun tion:\ntp =\nTP fp\nFP + g\n+\nTP g\nFP + g\n=\nTP (fp+ g)\nFP + g\n:\nIn this fun tion, tp represents the number of true positives of a rule with quality q\ng\nwhi h\novers exa tly fp negative examples. By sele ting a di erent fp value, the orresponding tp value an be determined by this fun tion. The line, determined by this fun tion, rosses the tp axis at point TP\n0\n= TP g=(FP + g) and the fp axis at point g. This is shown in\nFigure 11. The slope of this line is equal to the quality of rule R, whi h equals TP=(FP+g).\nfp\ntp\nR\nFP\nTP TP0\n-g\npoints with same quality qg=TP/(FP+g)\nFigure 11: Properties of rules with the same quality q g :\nFigure 12: Rules with highest quality inluded into the beam for q g = TP=(FP + g).\nIn the TP=FP spa e, points with higher quality than q\ng\nare above this line, in the\ndire tion of the upper left orner. Noti e that in the TP=FP spa e the top-left is the preferred part of the spa e: points in that part represent rules with the best TP=FP tradeo . This reasoning indi ates that points that will be in luded in the beam must all lie above the line of equal weights q\nbeam\nwhi h is de ned by the last rule in the beam. If\nrepresented graphi ally, rst beam width number of rules, found in the TP=FP spa e when rotating the line from point (0; P os) in the lo kwise dire tion, will be in luded in the beam. The enter of rotation is point ( g; 0). This is illustrated in Figure 12.\nOn the other hand, for the q quality measure de ned by q = TP FP the situation is similar but not identi al. Points with the same quality lie on a line tp = (fp FP )+TP , but its slope is onstant and equal to . Points with higher quality lie above the line in the dire tion of the left upper orner. The points that will be in luded into the beam are the\nrst beam width points in the TP=FP spa e found by a parallel movement of the line with slope , starting from point (0; P os) in the dire tion towards the lower right orner. This is illustrated in Figure 13.\nLet us now assume that we are looking for an optimal rule whi h is very spe i . In this ase, parameter will have a high value while parameter g will have a very small value. The intention is to nd the same optimal rule in the TP=FP spa e. At the rst stage of rule onstru tion only single features are onsidered and most probably their quality as the\nFP .\nnal solution is rather poor. See Figure 14 for a typi al pla ement of potentially interesting\nfeatures in the TP=FP spa e.\nThe primary fun tion of these features is to be good building blo ks so that by onjun - tively adding other features, high quality rules an be onstru ted. By adding onjun tions, solutions generally move in the dire tion of the left lower orner. The reason is that onjun tions an redu e the number of FP predi tions. However, they redu e the number of TP 's as well. Consequently, by onjun tively adding features to rules that are lose to the left lower orner, the algorithm will not be able to nd their spe ializations nearer to the left upper orner. Only the rules that have high TP value, and are in the upper part of the TP=FP spa e, have a han e to take part in the onstru tion of interesting new rules.\nFigure 15 illustrates the main di eren e between quality measures q g and q : the former tends to sele t more general features from the right upper part of the TP=FP spa e (points in the so- alled `g spa e'), while the later `prefers' spe i features from the left lower orner (points in the so- alled ` spa e'). In ases when is very large and g is very small, the e e t an be so important that it may prevent the algorithm from nding the optimal solution even with a large beam width. Noti e, however, that Algorithm SD is heuristi in its nature and no statements are true for all ases. This means that in some, but very rare ases, a quality measure based on parameter may result in a better nal solution."}, {"heading": "4.2 Experimental Evaluation of the Heuristi s", "text": "For the purpose of omparing the q\ng\nand q measures, a TP=FP onvex hull for ea h of\nthe two measures has been onstru ted. The pro edure was repeated for stages A{C. The TP=FP onvex hulls for the q\ng\nmeasure were onstru ted so that for di erent g values\nmany subgroups were onstru ted. Among them those lying on the onvex hull in the TP=FP spa e were sele ted: this resulted in onvex hulls presented by the thi k lines in\nFigures 16{18. The thin lines represent the TP=FP onvex hulls obtained in the same way for subgroups indu ed by the q measure, for values between 0.1 and 50.\nFigures 16-18 for stages A{C demonstrate that both urves agree in the largest part of the TP=FP spa e, but that for small FP values the q g measure is able to nd subgroups\nFigure 15: The quality q employing the\nparameter tends to sele t patterns (points) with small TP values, while quality q\ng\nemploy-\ning the g parameter will in lude also many patterns with large TP values (from the right part of the TP=FP spa e) that have a han e to be used in building onjun tions of high quality rules.\novering more positive examples. A ording to the analysis in the previous se tion, this was the expe ted result. In order to make the di eren e more obvious only the left part of the TP=FP spa e is shown in these gures.\nThe di eren es between the TP=FP onvex hulls for q\ng\nand q measures may seem small\nand insigni ant, but in reality it is not so. The majority of interesting subgroups (this\nlaim is supported also by patterns A1{C1 sele ted by the domain expert) are subgroups\nwith a small false positive rate whi h lie in the range in whi h q\ng\nworks better. In addition,\nfor subgroups with FP = 0 the true positive rate in our examples was about two times larger for subgroups indu ed with q\ng\nthan with q . Furthermore, note that for stages A and\nB there are two out of ve subgroups (A2 and C1) whi h lie in the gap between the TP=FP\nonvex hulls. If the q measure instead of q\ng\nmeasure were used in the experiments with\nCHD domain, at least subgroup A2 ould not have been dete ted."}, {"heading": "5. Related Work", "text": "This se tion provides omparisons and links to related work in subgroup dis overy, measures of interestingness, evaluation measures and visualization."}, {"heading": "5.1 Subgroup Dis overy", "text": "The need for user intera tivity in subgroup dis overy is addressed by Wrobel S. et al. (1996), des ribing a system developed in the KESO European resear h proje t (Knowledge Extra - tion for Statisti al O\u00c6 es) and in the systems EXPLORA (Kl osgen, 1996) and MIDOS (Wrobel, 1997, 2001). EXPLORA treats the learning task as a single relation problem, i.e., all the data are assumed to be available in one table (relation), whereas MIDOS extends this task to multi-relation databases, whi h is related to a number of other learning tasks (De Raedt & Dehaspe, 1997; Mannila & Toivonen, 1996; Wrobel & D zeroski, 1995), mostly in the eld of Indu tive Logi Programming (D zeroski & Lavra , 2001; Lavra & D zeroski, 1994).\nThe most important features of EXPLORA and MIDOS, related to this paper, on ern the use of heuristi s for subgroup dis overy; the measures of interestingness and the sear h heuristi s are outlined in separate se tions below. A related approa h to our approa h to rule subset sele tion, presented in Se tion 2.3, is Gebhardt's (1991) work on subgroup suppression.\nNote that some approa hes to asso iation rule indu tion an also be used for subgroup dis overy. For instan e, the APRIORI-C algorithm (Jovanoski & Lavra , 2001), whi h applies asso iation rule indu tion to lassi ation rule indu tion, outputs lassi ation rules with guaranteed support and on den e with respe t to a target lass. If a rule satis es also a user-de ned signi an e threshold, an indu ed APRIORI-C rule is an independent ` hunk' of knowledge about the target lass, whi h an be viewed as a subgroup des ription with guaranteed signi an e, support and on den e. Similarly, the on rmation rule on ept, introdu ed by Gamberger and Lavra (2000) and used as a basis for the subgroup dis overy algorithm in this paper, utilizes the minimal support requirement as a measure whi h must be satis ed by every rule in order to be in luded in the indu ed on rmation rule set.\nBoth above mentioned approa hes to subgroup dis overy exploit the information about lass membership. One of the main reasons why these approa hes are of interest for subgroup dis overy is that, unlike the lassi al lassi ation rule indu tion algorithms su h as CN2 (Clark & Niblett, 1989) and AQ (Mi halski, Mozeti , Hong, & Lavra , 1986), they do not use the overing algorithm. In overing algorithms only the rst few indu ed rules may\nbe of interest as subgroup des riptors with su\u00c6 ient overage. Subsequently indu ed rules are indu ed from biased example subsets, e.g., subsets in luding only positive examples not overed by previously indu ed rules. This bias onstrains the population for subgroup dis overy in a way that is unnatural for the subgroup dis overy pro ess whi h is, in general, aimed at dis overing interesting properties of subgroups of the entire population.\nRe ent approa hes to subgroup dis overy aim at over oming the problem of this inappropriate bias of the standard overing algorithm. The re ently developed subgroup dis-\novery algorithms CN2-SD (Lavra , Fla h, Kav sek, & Todorovski, 2002) and RSD (Lavra , Zelezn y, & Fla h, 2002) use the so- alled weighted overing algorithm, similar to the one implemented in Algorithm DMS des ribed in this paper.\nInstan e weights play an important role in boosting (Freund & Shapire, 1996) and alternating de ision trees (S hapire & Singer, 1999). Instan e weights have been used also in variants of the overing algorithm implemented in rule learning approa hes su h as SLIPPER (Cohen, 1999), RL (Lee, Bu hanan, & Aronis, 1998) and DAIRY (Hsu, Soderland, & Etzioni, 1998). A variant of the weighted overing algorithm has been used also in the\nontext of on rmation rule subset sele tion (Gamberger & Lavra , 2000), used as a basis\nfor the rule subset sele tion algorithm RSS des ribed in this paper."}, {"heading": "5.2 Measures of Interestingness", "text": "Various rule evaluation measures and heuristi s have been studied for subgroup dis overy, aimed at balan ing the size of a group (referred to as fa tor g by Kl osgen, 1996) with its distributional unusualness (referred to as fa tor p). The properties of fun tions that\nombine these two fa tors have been extensively studied (the so- alled \\p-g-spa e\").\nSimilarly, the weighted relative a ura y heuristi , de ned asWRA (Class Cond) = p(Cond) (p(ClassjCond) p(Class)) and used by Todorovski, Fla h, and Lavra (2000), trades o generality of the rule (p(Cond), i.e., rule overage) and relative a ura y p(ClassjCond) p(Class). This heuristi is a reformulation of one of the measures used in EXPLORA.\nBesides su h `obje tive' measures of interestingness, some `subje tive' measure of interestingness of dis overed patterns an be taken into the a ount, su h as a tionability (`a pattern is interesting if the user an do something with it to his or her advantage') and unexpe tedness (\\a pattern is interesting to the user if it is surprising to the user\") (Silbers hatz & Tuzhilin, 1995)."}, {"heading": "5.3 Subgroup Evaluation Measures", "text": "Evaluation of indu ed subgroups in the ROC spa e (Provost & Faw ett, 2001) shows lassi er performan e in terms of false alarm or false positive rate FPr = FP\nTN+FP\n(plotted on\nthe X-axis) that needs to be minimized, and sensitivity or true positive rate TPr =\nTP\nTP+FN\n(plotted on the Y -axis) that needs to be maximized. The ROC spa e is appropriate for measuring the su ess of subgroup dis overy, sin e subgroups whose TPr=FPr tradeo is\nlose to the diagonal an be dis arded as insigni ant. An appropriate approa h to evaluating a set of indu ed subgroups is by using the area under the ROC onvex hull de ned by subgroups with the best TPr=FPr tradeo as a quality measure for omparing the su ess of di erent learners.\nAlternatives to the area under the ROC onvex hull omputation are other standard evaluation measures used in rule learning, su h as predi tive a ura y or, in the ase of time/e\u00c6 ien y onstraints that need to be taken into the a ount, the tradeo measures DEA (Keller, Paterson, & Berrer, 2000) and Adjusted Ratio of Ratios (ARR) (Brazdil, Soares, & Pereira, 2001) that ombine a ura y and time to assess relative performan e.\nOptimized a ura y is, however, not the ultimate goal of subgroup dis overy. In addition to the area under the ROC onvex hull quality measure, other important su ess measures are rule signi an e (measuring the distributional unusualness of a subgroup), rule overage (measuring how large is a dis overed subgroup), rule size and size of a rule set (measuring the simpli ity and understandability of dis overed knowledge). These measures were used to evaluate the results of the CN2-SD subgroup dis overy algorithm (Lavra et al., 2002)."}, {"heading": "5.4 Subgroup Visualization", "text": "Data visualization methods have been part of statisti s and data analysis resear h for many years. This resear h on entrated primarily on plotting one or more independent variables against a dependent variable in support of exploratory data analysis (Tukey, 1977; Lee, Ong, & Quek, 1995; Unwin, 2000).\nThe visualization of analysis results has, however, gained only re ently some attention with the proliferation of data mining (Card, Ma kinlay, & Shneidermann, 1999; Fayyad, Grinstein, & Wierse, 2002; Keim & Kriegel, 1996; Simo , Noirhomme-Fraiture, & Boehlen, 2001). The visualization of analysis results primarily serves four purposes: better illustrate the pattern to the end user, enable the omparison of patterns, in rease pattern a eptan e, and enable pattern editing and support for \\what-if questions\". The re ent interest in the visualization of analysis results was spawned by the often overwhelming number and\nomplexity of data mining results.\nReaders interested in omparing the visualization method proposed in this paper with other subgroups visualization methods an nd the visualization of subgroups A1{C1 in the joint work by Gamberger, Lavra , and Wetts here k (2002)."}, {"heading": "6. Con lusions", "text": "This paper presents a novel subgroup dis overy algorithm integrated into the end to end knowledge dis overy pro ess. The dis ussion and empiri al results point out the importan e of e e tive expert-guided subgroup dis overy in the TP=FP spa e. Its main advantages are the possibility to indu e knowledge at di erent levels of generalization (a hieved by tuning the g parameter of the subgroup dis overy algorithm) used in the rule quality measure that ensures the indu tion of high quality rules also in the heuristi subgroup dis overy pro ess. The paper argues that expert's involvement in the indu tion pro ess is ne essary for su essful a tionable knowledge generation.\nThe proposed expert-guided subgroup dis overy pro ess onsists of the following steps: problem understanding, data understanding and preparation, subgroup dis overy, subgroup subset sele tion, statisti al hara terization of subgroups, subgroup visualization, their interpretation and evaluation. The main steps, des ribed in detail in this paper, are subgroup dis overy and the sele tion of a subset of diverse subgroups, followed by the statisti al hara terization of subgroups that adds supporting fa tors to the indu ed subgroup des riptions.\nSupporting fa tors represent redundant information about subgroups, but, in our opinion, their fun tion is extremely important in pattern des ription, be ause they help the experts to obtain a more omplete hara terization and better understanding of subgroups. Moreover, they in rease the expert's on den e that the pattern is appropriate for the problem that he is trying to solve. In addition, subgroup visualization helps in understanding the relationships among patterns and gives visual insights into their sensitivity and false alarm rate.\nThe presented approa h to des riptive indu tion uses expert knowledge at every step. Our intention was not to build a system that will repla e experts but rather to provide a methodology that will help experts in the knowledge dis overy pro ess. In our view, the possibility of guiding the indu tion pro ess is an advantage of this approa h."}, {"heading": "A knowledgments", "text": "This work was supported by the the Croatian Ministry of S ien e and Te hnology, Slovenian\nMinistry of Edu ation, S ien e and Sport, and the EU funded proje t Data Mining and De ision Support for Business Competitiveness: A European Virtual Enterprise (IST-199911495). We are grateful to Goran Krsta i and Tomislav Smu for their ollaboration in the experiments in oronary heart disease risk group dete tion, to Peter Fla h for his\nollaboration in the analysis of di erent quality measures, and to Dietri h Wetts here k for\nproviding pointers to the related work in visualization.\nReferen es\nBrazdil, P., Soares, C., & Pereira, R. (2001). Redu ing rankings of lassi ers by eliminating\nredundant lassi ers. In Progress in Arti ial Intelligen e: Pro eedings of the Tenth Portuguese Conferen e on Arti ial Intelligen e. Springer.\nCard, S. K., Ma kinlay, J. D., & Shneidermann, B. (1999). Readings in information visual-\nization. Morgan Kaufmann.\nClark, P., & Niblett, T. (1989). The CN2 indu tion algorithm. Ma hine Learning, 3,\n261{283.\nCohen, W. W. (1999). A simple, fast, and e e tive rule learner. In Pro eedings of Annual\nConferen e of Ameri an Asso iation for Arti ial Intelligen e.\nDe Raedt, L., & Dehaspe, L. (1997). Clausal dis overy. Ma hine Learning, 26, 99{146.\nD zeroski, S., & Lavra , N. (2001). Relational Data Mining. Springer.\nFayyad, U. M., Grinstein, G. G., & Wierse, A. (2002). Information visualization in data\nmining and knowledge dis overy. Morgan Kaufmann.\nFayyad, U. M., & Irani, K. B. (1992). On the handling of ontinuous-valued attributes in\nde ision tree generation. Ma hine Learning, 8, 87{102.\nFayyad, U. M., Piatetsky-Shapiro, G., & Smyth, P. (1996). From data mining to knowledge\ndis overy: An overview. In Advan es in Knowledge Dis overy and Data Mining. AAAI Press.\nFreund, Y., & Shapire, R. E. (1996). Experiments with a new boosting algorithm. In\nPro eedings of the Thirteenth International Conferen e on Ma hine Learning. Ma hine Learning.\nGamberger, D., & Lavra , N. (2000). Con rmation rule sets. In Pro eedings of the\nFourth European Conferen e on Prin iples of Data Mining and Knowledge Dis overy. Springer.\nGamberger, D., Lavra , N., & Wetts here k, D. (2002). Subgroup visualization: A method\nand appli ation in population s reening. In Pro eedings of the International Workshop on Intelligent Data Analysis in Medi ine and Pharma ology, IDAMAP-2002.\nGamberger, D., & Smu , T. (2001). On-line Data Mining Server. Rudjer Bo skovi Institute,\nhttp://dms.irb.hr.\nGebhardt, F. (1991). Choosing among ompeting generalizations. Knowledge A quisition\nJournal, 3, 361{380.\nGoldman, L., Garber, A. M., Grover, S. A., & Hlatky, M. A. (1996). Cost-e e tiveness of\nassessments and management of risk fa tors. Journal of Ameri an College Cardiology, 27, 1020{1030.\nHsu, D., Soderland, S., & Etzioni, O. (1998). A redundant overing algorithm applied\nto text lassi ation. In Pro eedings of the AAAI Workshop on Learning from Text Categorization.\nJovanoski, V., & Lavra , N. (2001). Classi ation rule learning with APRIORI-C. In\nProgress in Arti ial Intelligen e: Pro eedings of the Tenth Portuguese Conferen e on Arti ial Intelligen e. Springer.\nKeim, D. A., & Kriegel, H. P. (1996). Visualization te hniques for mining large databases:\na omparison. IEEE Transa tions on Knowledge and Data Engineering, 8, 923{938.\nKeller, J., Paterson, I., & Berrer, H. (2000). An integrated on ept for multi riteria ranking\nof data mining algorithms. In ECML-2000 Workshop on Meta-Learning: Building Automati Advi e Strategies for Model Sele tion and Method Combination.\nKl osgen, W. (1996). Explora: A multipattern and multistrategy dis overy assistant. In\nAdvan es in Knowledge Dis overy and Data Mining. MIT Press.\nKononenko, I. (1993). Indu tive and Bayesian learning in medi al diagnosis. Applied Arti-\nial Intelligen e, 7, 317{337.\nLavra , N., & D zeroski, S. (1994). Indu tive Logi Programming: Te hniques and Appli a-\ntions. Ellis Horwood.\nLavra , N., Fla h, P., Kav sek, B., & Todorovski, L. (2002). Adapting lassi ation rule\nindu tion to subgroup dis overy. In Pro eedings of the IEEE International Conferen e on Data Mining.\nLavra , N., Gamberger, D., & Turney, P. (1998). A relevan y lter for onstru tive indu -\ntion. IEEE Intelligent Systems & Their Appli ations, 13, 50{56.\nLavra , N., Zelezn y, F., & Fla h, P. (2002). RSD: Relational subgroup dis overy through\nrst-order feature onstru tion. In Pro eedings of the Twelfth International Confer-\nen es on Indu tive Logi Programming. Springer.\nLee, H. Y., Ong, H. L., & Quek, L. H. (1995). Exploiting visualization in knowledge dis-\novery. In Pro eedings of the First International Conferen e on Knowledge Dis overy\nand Data Mining.\nLee, Y., Bu hanan, B. G., & Aronis, J. M. (1998). Knowledge-based learning in exploratory\ns ien e: Learning rules to predi t rodent ar inogeni ity. Ma hine Learning, 30, 217{ 240.\nMannila, H., & Toivonen, H. (1996). On an algorithm for nding all interesting senten es.\nIn Pro eedings of Cyberneti s and Systems'96.\nMi halski, R. S., Mozeti , I., Hong, J., & Lavra , N. (1986). The multi-purpose in remen-\ntal learning system AQ15 and its testing appli ation on three medi al domains. In Pro eedings of the Fifth National Conferen e on Arti ial Intelligen e. Morgan Kaufmann.\nProvost, F., & Faw ett, T. (2001). Robust lassi ation for impre ise environments.Ma hine\nLearning, 42, 203{231.\nS hapire, R. E., & Singer, Y. (1999). Improved boosting algorithms using on den e-rated\npredi tions. Ma hine Learning, 37, 297{336.\nSilbers hatz, A., & Tuzhilin, A. (1995). On subje tive measures of interestingness in knowl-\nedge dis overy. In Pro eedings of the First International Conferen e on Knowledge Dis overy and Data Mining. AAAI Press.\nSimo , S. J., Noirhomme-Fraiture, M., & Boehlen, M. H. (2001). Pro eedings of the PKDD-\n2001 Workshop on Visual Data Mining.\nTodorovski, L., Fla h, P., & Lavra , N. (2000). Predi tive performan e of weighted relative\na ura y. In Pro eedings of the Fourth European Conferen e on Prin iples of Data Mining and Knowledge Dis overy. Springer.\nTukey, J. W. (1977). Exploratory Data Analysis. Addison Wesley.\nUnwin, A. (2000). Visualization for data mining. http://www1.math.uni-\naugsburg.de/~unwin/AntonyArts/VisLargeKoreaDe 2000.pdf.\nSmu , T., Gamberger, D., & Krsta i , G. (2001). Combining unsupervized and supervized\nma hine learning in analysis of the CHD patient database. In Pro eedings of Eighth Conferen e on Arti ial Intelligen e in Medi ine in Europe. Springer.\nWrobel, S. (1997). An algorithm for multi-relational dis overy of subgroups. In Pro eed-\nings of the First European Symposium on Prin iples of Data Mining and Knowledge Dis overy. Springer.\nWrobel, S. (2001). Indu tive logi programming for knowledge dis overy in databases. In\nRelational Data Mining. Springer.\nWrobel, S., & D zeroski, S. (1995). The ILP des ription learning problem: Towards a gen-\neral model-level de nition of data mining in ILP. In Pro eedings Fa hgruppentre en Mas hinelles Lernen. Univ. Dortmund.\nWrobel S., Wetts here k, D., Verkamo, A. I., Siebes, A., Mannila, H., Kwakkel, F., & Kloes-\ngen, W. (1996). User intera tivity in very large s ale data mining. In Pro eedings of the German Workshop on Ma hine Learning. Univ. Chemnitz."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "This paper presents an approa h to expert-guided subgroup dis overy. The main step of the subgroup dis overy pro ess, the indu tion of subgroup des riptions, is performed by a heuristi beam sear h algorithm, using a novel parametrized de nition of rule quality whi h is analyzed in detail. The other important steps of the proposed subgroup dis overy pro ess are the dete tion of statisti ally signi ant properties of sele ted subgroups and subgroup visualization: statisti ally signi ant properties are used to enri h the des riptions of indu ed subgroups, while the visualization shows subgroup properties in the form of distributions of the numbers of examples in the subgroups. The approa h is illustrated by the results obtained for a medi al problem of early dete tion of patient risk groups.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}