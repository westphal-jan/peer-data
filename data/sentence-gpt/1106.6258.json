{"id": "1106.6258", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "A Note on Improved Loss Bounds for Multiple Kernel Learning", "abstract": "The paper \\cite{hs-11} presented a bound on the generalisation error of classifiers learned through multiple kernel learning. The bound has (an improved) \\emph{additive} dependence on the number of kernels (with the same logarithmic dependence on this number). However, parts of the proof were incorrectly presented in that paper, and the underlying solution was shown by a new algorithm that was a simpler, faster, more reliable method. The problem with this approach is that it allows for simpler representation of different kernels (for example, given that the function is an integer) in one way or another, but that it is quite a generalisation. It is also possible to compare the kernels using one other method, but this is not an exhaustive list of solutions, and one must use it in a particular way.\n\n\nFor a particular problem (the kernel learning problem) the first thing the researchers (and the authors) are able to find is the type of type that their work is performing. The problem with this approach is that they know which kernels can be learned, whereas the other methods can only learn them. If we see a function, we can find the most likely ones. A common solution is the following:\n1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18", "histories": [["v1", "Thu, 30 Jun 2011 15:03:58 GMT  (9kb)", "https://arxiv.org/abs/1106.6258v1", "Extended proof"], ["v2", "Mon, 12 May 2014 19:40:40 GMT  (13kb)", "http://arxiv.org/abs/1106.6258v2", "Extended proof"]], "COMMENTS": "Extended proof", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zakria hussain", "john shawe-taylor", "mario marchand"], "accepted": false, "id": "1106.6258"}, "pdf": {"name": "1106.6258.pdf", "metadata": {"source": "CRF", "title": "A Note on Improved Loss Bounds for Multiple Kernel Learning", "authors": ["Zakria Hussain"], "emails": ["z.hussain@cs.ucl.ac.uk", "jst@cs.ucl.ac.uk", "mario.marchand@ift.ulaval.ca"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 6.\n62 58"}, {"heading": "1 Introduction", "text": "We refer to [4] for the motivation and definitions of multiple kernel learning. It presents a number of results, including a new Rademacher complexity bound on the generalisation error of classifiers learned from a multiple kernel class with a logarithmic dependence on the number of kernels used and with that logarithm entering additively into the bound\u2014that is, independently\nof the complexity of the individual kernels or the margin of the classifier on the training set.\nIn this paper, we follow the approach presented in [4] but correct some of the errors that are present. Unfortunately, the Rademacher complexity risk bound turns out to exhibit a multiplicative dependence on the logarithm of the number of kernels and the margin achieved by the classifier."}, {"heading": "2 Detailed proof", "text": ""}, {"heading": "2.1 Preliminaries", "text": "Let z = {(xi, yi)}mi=1 be an m-sample where xi \u2208 X \u2282 Rn and yi \u2208 Y = {\u22121,+1}, with Z = X \u00d7Y. Let x = {x1, . . . , xm} contain the input vectors. Definition 1 ([1]). A kernel is a function \u03ba that for all x, x\u2032 \u2208 X satisfies\n\u03ba ( x, x\u2032 ) = \u3008\u03c6(x), \u03c6(x\u2032)\u3009,\nwhere \u03c6 is a mapping from X to an (inner product) Hilbert space H\n\u03c6 : X 7\u2192 H.\nKernel learning algorithms [7, 8] make use of the m \u00d7m kernel matrix K = [\u03ba(xi, xi\u2032)] m i,i\u2032=1 defined using the training inputs x. When using the kernel representation it is not always possible to represent the weight vector w explicitly and so we can use the function f directly as the predictor:\nf(x) = m \u2211\ni=1\n\u03b1iyi\u03ba(xi, x) = \u3008w,\u03c6(x)\u3009,\nwhere \u03b1 = (\u03b11, . . . , \u03b1m) is the dual weight vector and the corresponding norm of the weight vector is\n\u2016w\u20162 = m \u2211\ni,j=1\n\u03b1iyi\u03b1jyj\u03ba(xi, xj).\nGiven a kernel \u03ba, we will use \u03c6\u03ba(\u00b7) to denote a feature space mapping satisfying \u03ba(x, x\u2032) = \u3008\u03c6\u03ba(x), \u03c6\u03ba(x\u2032)\u3009. Hence, learning with a kernel \u03ba can be described as finding a function from the class of functions [9]\nF\u03ba = {x 7\u2192 \u3008w,\u03c6\u03ba(x)\u3009 | \u2016w\u20162 \u2264 1, }\nminimising the empirical average of the hinge loss\nh\u03b3(yf(x)) = max\n(\n1\u2212 yf(x) \u03b3 , 0\n)\n.\nwhere we call \u03b3 \u2208 [0, 1] the margin. For multiple kernel learning we consider a family of kernels K and the corresponding function class"}, {"heading": "FK = {x 7\u2192 \u3008w,\u03c6\u03ba(x)\u3009 | \u2016w\u20162 \u2264 1, for some \u03ba \u2208 K} .", "text": "For a distributionD, we use the notation ED[f(x)] to denote the expected value of f(x) when x \u223c D. Given a training set x we denote E\u0302[f ] to denote its empirical average over the sample x.\nFor the generalisation error bounds we assume that the data are generated iid from a fixed but unknown probability distribution D over the joint space X \u00d7 Y. Given the true error of a function f :\nerr(f) = E(x,y)\u223cD(yf(x) \u2264 0) = ED[yf(x)],\nthe empirical margin error of f with margin \u03b3 > 0:\ne\u0302rr\u03b3(f) = 1\nm\nm \u2211\ni=1\nI(yif(xi) < \u03b3) = E\u0302[I(yif(xi) < \u03b3)] ,\nwhere I is the indicator function, and the estimation error est\u03b3(f) is defined as est\u03b3(f) = |err(f)\u2212 e\u0302rr\u03b3(f)|, we would like to find an upper bound for est\u03b3(f). In the sequel we will state the bounds in standard form, where the true error err(f) of a function f is upper bounded by the empirical margin error e\u0302rr\u03b3(f) plus the estimation error est\u03b3(f):\nerr(f) \u2264 e\u0302rr\u03b3(f) + est\u03b3(f). (1)\nWe further consider the clipped hinge function:\nA\u03b3(s) =\n\n\n 0; if s \u2265 \u03b3 1\u2212 s/\u03b3; if 0 \u2264 s \u2264 \u03b3; 1; otherwise,\nand its empirical estimation E\u0302[A\u03b3(yf(x))]. Note that err(f) \u2264 ED[A\u03b3(yf(x))], E\u0302[A\u03b3(yf(x))] \u2264 e\u0302rr\u03b3(f) and E\u0302[A\u03b3(yf(x))] \u2264 E\u0302[h\u03b3(yf(x))).\nLet K = {\u03ba1, . . . , \u03bap} denote a family of kernels, where each kernel \u03baj is called the jth base kernel. The following kernel family is formed using a convex combination of base kernels:\nKcon(\u03ba1, . . . , \u03bap) =\n\n\n\n\u03ba\u03bb =\np \u2211\nj=1\n\u03bbj\u03baj | \u03bbj \u2265 0, p \u2211\nj=1\n\u03bbj = 1\n\n\n\n.\nNote, p is the complexity of the kernel family (i.e., cardinality of the set of base kernels)."}, {"heading": "2.2 Rademacher complexity bound for MKL", "text": "In this section we correct the MKL risk bound of [4]. We begin by the following definition of Rademacher complexity.\nDefinition 2 (Rademacher complexity). For a sample x = {x1, . . . , xm} generated by a distribution DX on a set X and a real-valued function class F with domain X , the empirical Rademacher complexity of F is the random variable\nR\u0302m(F) = E\u03c3 [\nsup f\u2208F\n2\nm\nm \u2211\ni=1\n\u03c3if(xi) | x1, . . . , xm ] .\nwhere \u03c3 = (\u03c31, . . . , \u03c3m) are independent uniform {\u00b11}-valued (Rademacher) random variables. The (true) Rademacher complexity is:\nRm(F) = Ex [ R\u0302m(F) ] = Ex\u03c3\n[\nsup f\u2208F\n2\nm\nm \u2211\ni=1\n\u03c3if(xi)\n]\n.\nThe standard Rademacher bound for function classes is given in the following theorem.\nTheorem 1 ([3]). Fix \u03b4 \u2208 (0, 1), and let F be a class of functions mapping from Z = X \u00d7Y to [0, 1]. Let z = {zi}mi=1 be drawn independently according to a probability distribution D. Then with probability 1 \u2212 \u03b4 over random draws of samples of size m, every f \u2208 F satisfies\nED(f) \u2264 E\u0302(f) + R\u0302m(F) + 3 \u221a ln(2/\u03b4)\n2m .\nWe have attributed this bound to [3], though, strictly speaking, they used the slightly weaker version of Rademacher complexity including an absolute value of the sum. This version is obtained by a slight tightening of the argument. This bound is quite general and applicable to various learning algorithms if a tight upper bound of empirical Rademacher complexity R\u0302m(F) of the function class F can be found. For kernel methods, a wellknown result uses the trace of the kernel matrix to bound the empirical Rademacher complexity.\nTheorem 2 ([3]). If \u03ba : X \u00d7X 7\u2192 R is a kernel, and x = {x1, . . . , xm} is a sample of points from X , then the empirical Rademacher complexity of the class F\u03ba satisfies\nR\u0302m(F\u03ba) \u2264 2\nm\n\u221a \u221a \u221a \u221a m \u2211\ni=1\n\u03ba(xi, xi).\nFurthermore, if R2 \u2265 \u03ba(x, x) for all x \u2208 X and \u03ba is a normalised kernel such that\n\u2211m i=1 \u03ba(xi, xi) = m, then we have\n2\nm\n\u221a \u221a \u221a \u221a m \u2211\ni=1\n\u03ba(xi, xi) \u2264 2R\u221a m .\nThe problem of learning kernels from a convex combination of base kernels is related to using the convex hull of a set of functions. Consider\ncon(F) =\n \n\n\u2211\nj\najfj | fj \u2208 F , aj \u2265 0, \u2211\nj\naj \u2264 1\n \n\n. (2)\nSince adding kernels corresponds to concatenating feature spaces, it is clear that (here wj is the restriction of w to the feature space defined by the mapping \u03c6\u03baj (\u00b7) corresponding to kernel \u03baj)\nFKcon(\u03ba1,...,\u03bap) =\n\n\n\nx 7\u2192 \u3008w,\u03c6\u03ba(x)\u3009 | \u2016w\u20162 \u2264 1, \u03ba = p \u2211\nj=1\n\u03bbj\u03baj ,\np \u2211\nj=1\n\u03bbj = 1\n\n\n\n=\n\n\n\nx 7\u2192 p \u2211\nj=1\n\u221a \u03bbj\u2016wj\u2016 \u2329 wj \u2016wj\u2016 , \u03c6\u03baj (x) \u232a | \u2016w\u20162 \u2264 1, p \u2211\nj=1\n\u03bbj = 1\n\n\n\n= con\n\n\np \u22c3\nj=1\nF\u03baj\n\n , (3)\nsince, by the Cauchy Schwartz inequality, we have\np \u2211\nj=1\n\u221a\n\u03bbj\u2016wj\u2016 \u2264\n\u221a \u221a \u221a \u221a p \u2211\nj=1\n\u03bbj\n\u221a \u221a \u221a \u221a p \u2211\nj=1\n\u2016wj\u20162 \u2264 1.\nHence, we are interested in the empirical Rademacher complexity of a convex hull as given by Equation (2), which is well known to satisfy\nR\u0302m(con(F)) = R\u0302m(F) . (4)\nFurthermore, following [5] and [2], we have the following result.\nTheorem 3 ([5]). The empirical Rademacher complexity of the function class L(F) where L(\u00b7) is Lipschitz function with Lipschitz constant L is bounded by\nR\u0302m(L(F)) \u2264 LR\u0302m(F).\nGiven all these results, we are now in a position to state the following theorem, which proves a high probability upper bound for the empirical Rademacher complexity of a union of function classes\n\u22c3p j=1Fj = F .\nTheorem 4. Let x = {x1, . . . , xm} be an m-sample of points from X , then the empirical Rademacher complexity R\u0302m of the class F = \u222apj=1Fj, where the range of all the functions in F is [0, 1], satisfies:\nR\u0302m(F) \u2264 max 1\u2264j\u2264p\nR\u0302m(Fj) + \u221a 8 ln(p)\nm .\nProof. Since F is the union of p function classes, we have\nR\u0302m(F) = E\u03c3 max 1\u2264j\u2264p sup f\u2208Fj\n2\nm\nm \u2211\ni=1\n\u03c3if(xi) .\nFrom Jensen\u2019s inequality, we have, for any \u03bb \u2265 0, that\nexp ( \u03bbR\u0302m(F) ) \u2264 E\u03c3 exp ( \u03bb [\nmax 1\u2264j\u2264p sup f\u2208Fj\n2\nm\nm \u2211\ni=1\n\u03c3if(xi)\n])\n= E\u03c3 max 1\u2264j\u2264p exp\n(\n\u03bb\n[\nsup f\u2208Fj\n2\nm\nm \u2211\ni=1\n\u03c3if(xi)\n])\n\u2264 p \u2211\nj=1\nE\u03c3 exp\n(\n\u03bb\n[\nsup f\u2208Fj\n2\nm\nm \u2211\ni=1\n\u03c3if(xi)\n])\n. (5)\nNow, for any fixed function class Fj and any fixed training sample, let\n\u03be(\u03c31, . . . , \u03c3m) def = sup\nf\u2208Fj\n2\nm\nm \u2211\ni=1\n\u03c3if(xi) .\nA basic result of McDiarmid [6] states that for any \u03bb \u2265 0, we have\nEe\u03bb\u03be \u2264 e\u03bb 2 8 \u2211m i=1 c 2 i \u00b7 e\u03bbE\u03be ,\nwhere, for all i, we have\nsup \u03c31,...,\u03c3m,\u03c3\u0302i\n|\u03be(\u03c31, . . . , \u03c3m)\u2212 \u03be(\u03c31, . . . , \u03c3i\u22121, \u03c3\u0302i, \u03c3i+1, . . . , \u03c3m)| \u2264 ci .\nIn our case, we have that ci \u2264 4/m \u2200i \u2208 {1, . . . ,m}. Hence, from Equation (5), we have\nexp ( \u03bbR\u0302m(F) ) \u2264 e2\u03bb2/m p \u2211\nj=1\ne\u03bbR\u0302m(Fj) .\nBy taking the logarithm on both sides of this equation, we obtain\n\u03bbR\u0302m(F) \u2264 2\u03bb2\nm + ln\n\n\np \u2211\nj=1\ne\u03bbR\u0302m(Fj )\n\n\n\u2264 2\u03bb 2\nm + ln\n[\np \u00b7 max 1\u2264j\u2264p\ne\u03bbR\u0302m(Fj) ]\n\u2264 2\u03bb 2\nm + ln(p) + max 1\u2264j\u2264p \u03bbR\u0302m(Fj) .\nHence, we have\nR\u0302m(F) \u2264 2\u03bb\nm +\n1 \u03bb ln(p) + max 1\u2264j\u2264p R\u0302m(Fj) .\nThe theorem then follows from this equation by choosing\n\u03bb =\n\u221a\nm\n2 ln p .\nRecall the function A\u03b3(\u00b7) and the properties err(f) \u2264 ED[A\u03b3(yf(x))] and E[A\u03b3(yf(x))] \u2264 err\u03b3(f). Therefore we have the following generalization error bound for MKL in the case of a convex combination of kernels.\nTheorem 5. Fix \u03b3 > 0 and \u03b4 \u2208 (0, 1). Let K = {\u03ba1, . . . , \u03bap} be a family of kernels containing p base kernels and let z = {zi}mi=1 be a randomly generated sample from distribution D. Then with probability 1 \u2212 \u03b4 over the random draws of samples of size m, every f \u2208 FKcon satisfies\nerr(f) \u2264 E\u0302[A\u03b3(yf(x))] + 1 \u03b3\n\nmax 1\u2264j\u2264p\n2\nm\n\u221a \u221a \u221a \u221a m \u2211\ni=1\n\u03baj(xi, xi) +\n\u221a\n8 ln p\nm\n\n\n+ 3\n\u221a\nln(2/\u03b4)\n2m .\nAlso, if each kernel \u03baj is normalised and bounded by R 2 \u2265 \u03baj(x, x) for all x \u2208 X and j \u2208 {1, . . . , p}, we have\nerr(f) \u2264 E\u0302[A\u03b3(yf(x))] + 1 \u03b3\n[\n2R\u221a m +\n\u221a\n8 ln p\nm\n]\n+ 3\n\u221a\nln(2/\u03b4)\n2m .\nProof. Each kernel \u03baj defines the class Fj = {x 7\u2192 \u3008w,\u03c6\u03baj (x)\u3009 : \u2016w\u2016 \u2264 1}. Hence, applying Theorem 1 to the class A\u03b3(FK) = {A\u03b3 \u25e6 f : f \u2208 FK}, we have\nerr(f) \u2264 ED[A\u03b3(yf(x))]\n\u2264 E\u0302[A\u03b3(yf(x))] + R\u0302m(A\u03b3(FKcon)) + 3 \u221a ln(2/\u03b4)\n2m\n\u2264 E\u0302[A\u03b3(yf(x))] + 1 \u03b3 R\u0302m(FKcon) + 3\n\u221a\nln(2/\u03b4)\n2m\n= E\u0302[A\u03b3(yf(x))] + 1 \u03b3 R\u0302m\n\n\np \u22c3\nj=1\nF\u03baj\n\n+ 3\n\u221a\nln(2/\u03b4)\n2m\n\u2264 E\u0302[A\u03b3(yf(x))] + 1 \u03b3\n[\nmax 1\u2264j\u2264p\nR\u0302m(Fj) + \u221a 8 ln p\nm\n]\n+ 3\n\u221a\nln(2/\u03b4)\n2m\n\u2264 E\u0302[A\u03b3(yf(x))] + 1 \u03b3\n\nmax 1\u2264j\u2264p\n2\nm\n\u221a \u221a \u221a \u221a m \u2211\ni=1\n\u03baj(xi, xi) +\n\u221a\n8 ln p\nm\n\n+ 3\n\u221a\nln(2/\u03b4)\n2m\n\u2264 E\u0302[A\u03b3(yf(x))] + 1 \u03b3\n[\nmax 1\u2264j\u2264p\n2\nm\n\u221a mR2 + \u221a 8 ln p\nm\n]\n+ 3\n\u221a\nln(2/\u03b4)\n2m\n= E\u0302[A\u03b3(yf(x))] + 1 \u03b3\n[\n2R\u221a m +\n\u221a\n8 ln p\nm\n]\n+ 3\n\u221a\nln(2/\u03b4)\n2m ,\nwhere the third line comes from applying Theorem 3 with with Lipschitz constant L = 1/\u03b3. The forth line comes by applying Equation (4). The fifth line comes by applying Theorem 4. The 6th line follows from Theorem 2. Finally, the 7th line follows from the hypothesis that \u03baj(x, x) \u2264 R2 \u2200x."}, {"heading": "3 Discussion", "text": "Using the notation from above, the un-normalized version of the bound of Theorem 8 of [4] is\nerr(f) \u2264 e\u0302rr\u03b3(f) + 2 \u03b3m max 1\u2264j\u2264p\n\u221a \u221a \u221a \u221a m \u2211\ni=1\n\u03baj(xi, xi) + 5\n\u221a\nln((p + 3)/\u03b4)\n2m .\nComparing this to Theorem 5 (the corrected version), we can see that the major difference is the fact that 1/\u03b3 is multiplying ln p in Theorem 5, while it is not in the Theorem 8 of [4]. However, the latter was obtained by incorrectly assuming that R\u0302m(A\u03b3(con(F))) is upper bounded by R\u0302m(con(A\u03b3(F))).\nWhile Theorem 2 of [4] shows an additive dependence on the logarithm of the number of kernels it has an additional term that includes the number of kernels d involved in the final solution and this number is also multiplied by the logarithm of the number of kernels. However, these quantities are separate from the main margin complexity term. A similar result could be obtained for the Rademacher bound given here resulting in a partial independence between the complexity and number of kernel terms, but with the final number of active kernels entering as an additional complexity term.\nAcknowldgements\nWe thank an anonymous reviewer from the Journal of Machine Learning Research who pointed out the flaw in the original proof of Theorem 5."}], "references": [{"title": "Theoretical foundations of the potential function method in pattern recognition learning", "author": ["M. Aizerman", "E. Braverman", "L. Rozonoer"], "venue": "Automation and Remote Control, 25:821 \u2013 837", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1964}, {"title": "Complexity of pattern classes and lipschitz property", "author": ["A. Ambroladze", "J. Shawe-Taylor"], "venue": "Algorithmic Learning Theory, volume 3244 of Lecture Notes in Computer Science, pages 181\u2013193. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Rademacher and gaussian complexities: risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Improved loss bounds for multiple kernel learning", "author": ["Z. Hussain", "J. Shawe-Taylor"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Rademacher composition", "author": ["S. Kakade", "A. Tewari"], "venue": "CMSC 35900 Learning Theory, pages 1\u201322", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": ". L. M. S. L. N. Series, editor, Surveys in Combinatorics 1989, pages 148\u2013188. Cambridge University Press, Cambridge", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press, Cambridge, U.K.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning bounds for support vector machines with learned kernels", "author": ["N. Srebro", "S. Ben-David"], "venue": "Computational Learning Theory, volume 4005 of Lecture Notes in Computer Science, pages 169\u2013183. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 3, "context": "Abstract In this paper, we correct an upper bound, presented in [4], on the generalisation error of classifiers learned through multiple kernel learning.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "The bound in [4] uses Rademacher complexity and has anadditive dependence on the logarithm of the number of kernels and the margin achieved by the classifier.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "1 Introduction We refer to [4] for the motivation and definitions of multiple kernel learning.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "In this paper, we follow the approach presented in [4] but correct some of the errors that are present.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Definition 1 ([1]).", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Kernel learning algorithms [7, 8] make use of the m \u00d7m kernel matrix K = [\u03ba(xi, xi\u2032)] m i,i=1 defined using the training inputs x.", "startOffset": 27, "endOffset": 33}, {"referenceID": 7, "context": "Kernel learning algorithms [7, 8] make use of the m \u00d7m kernel matrix K = [\u03ba(xi, xi\u2032)] m i,i=1 defined using the training inputs x.", "startOffset": 27, "endOffset": 33}, {"referenceID": 8, "context": "Hence, learning with a kernel \u03ba can be described as finding a function from the class of functions [9] F\u03ba = {x 7\u2192 \u3008w,\u03c6\u03ba(x)\u3009 | \u2016w\u20162 \u2264 1, } 2", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "where we call \u03b3 \u2208 [0, 1] the margin.", "startOffset": 18, "endOffset": 24}, {"referenceID": 3, "context": "2 Rademacher complexity bound for MKL In this section we correct the MKL risk bound of [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Theorem 1 ([3]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Fix \u03b4 \u2208 (0, 1), and let F be a class of functions mapping from Z = X \u00d7Y to [0, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 2, "context": "We have attributed this bound to [3], though, strictly speaking, they used the slightly weaker version of Rademacher complexity including an absolute value of the sum.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Theorem 2 ([3]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "(4) Furthermore, following [5] and [2], we have the following result.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "(4) Furthermore, following [5] and [2], we have the following result.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Theorem 3 ([5]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": ", xm} be an m-sample of points from X , then the empirical Rademacher complexity R\u0302m of the class F = \u222apj=1Fj, where the range of all the functions in F is [0, 1], satisfies: R\u0302m(F) \u2264 max 1\u2264j\u2264p R\u0302m(Fj) + \u221a 8 ln(p) m .", "startOffset": 156, "endOffset": 162}, {"referenceID": 5, "context": "A basic result of McDiarmid [6] states that for any \u03bb \u2265 0, we have Ee \u2264 e\u03bb 2 8 \u2211m i=1 c 2 i \u00b7 e , where, for all i, we have sup \u03c31,.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "3 Discussion Using the notation from above, the un-normalized version of the bound of Theorem 8 of [4] is err(f) \u2264 \u00earr(f) + 2 \u03b3m max 1\u2264j\u2264p \u221a \u221a", "startOffset": 99, "endOffset": 102}], "year": 2014, "abstractText": "In this paper, we correct an upper bound, presented in [4], on the generalisation error of classifiers learned through multiple kernel learning. The bound in [4] uses Rademacher complexity and has anadditive dependence on the logarithm of the number of kernels and the margin achieved by the classifier. However, there are some errors in parts of the proof which are corrected in this paper. Unfortunately, the final result turns out to be a risk bound which has a multiplicative dependence on the logarithm of the number of kernels and the margin achieved by the classifier.", "creator": "LaTeX with hyperref package"}}}