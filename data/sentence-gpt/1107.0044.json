{"id": "1107.0044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Towards Understanding and Harnessing the Potential of Clause Learning", "abstract": "Efficient implementations of DPLL with the addition of clause learning are the fastest complete Boolean satisfiability solvers and can handle many significant real-world problems, such as verification, planning and design. Despite its importance, little is known of the ultimate strengths and limitations of the technique. This paper presents the first precise characterization of clause learning as a proof system (CL), and begins the task of understanding its power by relating it to the well-studied resolution proof system. The first step, based on the approach proposed by John McNeill in this article, is to describe an intuitive way to generate a simple and elegant proof of concept for generating a concrete proof of concept. The first step is to define what happens if the answer is a given given proof. This can be seen by studying the first example for a function, in which the answer consists of an element of the input element. When the function is satisfied, the problem becomes obvious to the person involved, the person involved, or the person involved, or the person involved. In order to understand a concrete proof of concept, we need to know what the concrete proof of concept is, what the real proof is. To illustrate this, we will use a simple simple and convenient proof of concept.\n\n\nDefinition: Function of a proof-of-concept\nWhat we mean by \"functional proof\" is a simple simple proof of concept. If the function of a function is satisfied, a function function that is a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function of a function", "histories": [["v1", "Thu, 30 Jun 2011 20:39:28 GMT  (264kb)", "http://arxiv.org/abs/1107.0044v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["p beame", "h kautz", "a sabharwal"], "accepted": false, "id": "1107.0044"}, "pdf": {"name": "1107.0044.pdf", "metadata": {"source": "CRF", "title": "Towards Understanding and Harnessing the Potential of Clause Learning", "authors": ["Paul Beame", "Henry Kautz", "Ashish Sabharwal"], "emails": ["beame@cs.washington.edu", "kautz@cs.washington.edu", "ashish@cs.washington.edu"], "sections": [{"heading": "1. Introduction", "text": "In recent years the task of deciding whether or not a given CNF propositional logic formula is satisfiable has gone from a problem of theoretical interest to a practical approach for solving real-world problems. Satisfiability (SAT) procedures are now a standard tool for hardware verification, including verification of super-scalar processors (Velev & Bryant, 2001; Biere et al., 1999a). Open problems in group theory have been encoded and solved using satisfiability solvers (Zhang & Hsiang, 1994). Other applications of SAT include circuit diagnosis and experiment design (Konuk & Larrabee, 1993; Gomes et al., 1998b).\nThe most surprising aspect of such relatively recent practical progress is that the best complete satisfiability testing algorithms remain variants of the Davis-Putnam-LogemannLoveland or DPLL procedure (Davis & Putnam, 1960; Davis et al., 1962) for backtrack search in the space of partial truth assignments. The key idea behind its efficacy is the pruning of search space based on falsified clauses. Since its introduction in the early 1960\u2019s, the main improvements to DPLL have been smart branch selection heuristics (e.g., Li &\nc\u00a92004 AI Access Foundation. All rights reserved.\nAnbulagan, 1997), and extensions such as randomized restarts (Gomes et al., 1998a) and clause learning (see e.g., Marques-Silva & Sakallah, 1996). One can argue that of these, clause learning has been the most significant in scaling DPLL to realistic problems. This paper attempts to understand the potential of clause learning and suggests ways to harness its power.\nClause learning grew out of work in AI on explanation-based learning (EBL), which sought to improve the performance of backtrack search algorithms by generating explanations for failure (backtrack) points, and then adding the explanations as new constraints on the original problem (de Kleer & Williams, 1987; Stallman & Sussman, 1977; Genesereth, 1984; Davis, 1984). For general constraint satisfaction problems the explanations are called \u201cconflicts\u201d or \u201cno goods\u201d; in the case of Boolean CNF satisfiability, the technique becomes clause learning \u2013 the reason for failure is learned in the form of a \u201cconflict clause\u201d which is added to the set of given clauses. A series of researchers (Bayardo Jr. & Schrag, 1997; Marques-Silva & Sakallah, 1996; Zhang, 1997; Moskewicz et al., 2001; Zhang et al., 2001) showed that clause learning can be efficiently implemented and used to solve hard problems that cannot be approached by any other technique.\nDespite its importance there has been little work on formal properties of clause learning, with the goal of understanding its fundamental strengths and limitations. A likely reason for such inattention is that clause learning is a rather complex rule of inference \u2013 in fact, as we describe below, a complex family of rules of inference. A contribution of this paper is a precise mathematical specification of various concepts used in describing clause learning.\nAnother problem in characterizing clause learning is defining a formal notion of the strength or power of a reasoning method. This paper uses the notion of proof complexity (Cook & Reckhow, 1977), which compares inference systems in terms of the sizes of the shortest proofs they sanction. We use CL to denote clause learning viewed as a proof system. A family of formulas C provides an exponential separation between systems S1 and S2 if the shortest proofs of formulas in C in system S1 are exponentially smaller than the corresponding shortest proofs in S2. From this basic propositional proof complexity point of view, only families of unsatisfiable formulas are of interest, because only proofs of unsatisfiability can be large; minimum proofs of satisfiability are linear in the number of variables of the formula. Nevertheless, Achlioptas et al. (2001) have shown how negative proof complexity results for unsatisfiable formulas can be used to derive time lower bounds for specific inference algorithms running on satisfiable formulas as well.\nProof complexity does not capture everything we intuitively mean by the power of a reasoning system, because it says nothing about how difficult it is to find shortest proofs. However, it is a good notion with which to begin our analysis, because the size of proofs provides a lower bound on the running time of any implementation of the system. In the systems we consider, a branching function, which determines which variable to split upon or which pair of clauses to resolve, guides the search. A negative proof complexity result for a system tells us that a family of formulas is intractable even with a perfect branching function; likewise, a positive result gives us hope of finding a branching function.\nA basic result in proof complexity is that general resolution, denoted RES, is exponentially stronger than the DPLL procedure (Bonet et al., 2000; Ben-Sasson et al., 2000). This is because the trace of DPLL running on an unsatisfiable formula can be converted to a treelike resolution proof of the same size, and tree-like proofs must sometimes be exponentially\nlarger than the DAG-like proofs generated by RES. Although RES can yield shorter proofs, in practice DPLL is better because it provides a more efficient way to search for proofs. The weakness of the tree-like proofs that DPLL generates is that they do not reuse derived clauses. The conflict clauses found when DPLL is augmented by clause learning correspond to reuse of derived clauses in the associated resolution proofs and thus to more general forms of resolution proofs. As a theoretical upper bound, all DPLL based approaches, including those involving clause learning, are captured by RES. An intuition behind the results in this paper is that the addition of clause learning moves DPLL closer to RES while retaining its practical efficiency.\nIt has been previously observed that clause learning can be viewed as adding resolvents to a tree-like proof (Marques-Silva, 1998). However, this paper provides the first mathematical proof that clause learning, viewed as a propositional proof system CL, is exponentially stronger than tree-like resolution. This explains, formally, the performance gains observed empirically when clause learning is added to DPLL based solvers. Further, we describe a generic way of extending families of formulas to obtain ones that exponentially separate CL from many refinements of resolution known to be intermediate in strength between RES and tree-like resolution. These include regular and Davis-Putnam resolution, and any other proper refinement of RES that behaves naturally under restrictions of variables, i.e., for any formula F and restriction \u03c1 on its variables, the shortest proof of F |\u03c1 in the system is not any larger than a proof of F itself. The argument used to prove this result involves a new clause learning scheme called FirstNewCut that we introduce specifically for this purpose. Our second technical result shows that combining a slight variant of CL, denoted CL--, with unlimited restarts results in a proof system as strong as RES itself. This intuitively explains the speed-ups obtained empirically when randomized restarts are added to DPLL based solvers, with or without clause learning.\nGiven these results about the strengths and limitations of clause learning, it is natural to ask how the understanding we gain through this kind of analysis may lead to practical improvement in SAT solvers. The theoretical bounds tell us the potential power of clause learning; they don\u2019t give us a way of finding short solutions when they exist. In order to leverage their strength, clause learning algorithms must follow the \u201cright\u201d variable order for their branching decisions for the underlying DPLL procedure. While a good variable order may result in a polynomial time solution, a bad one can make the process as slow as basic DPLL without learning. The latter half of this paper addresses this problem of moving from analytical results to practical improvement. The approach we take is the use of the problem structure for guiding SAT solvers in their branch decisions.\nBoth random CNF formulas and those encoding various real-world problems are quite hard for current SAT solvers. However, while DPLL based algorithms with lookahead but no learning (such as satz by Li & Anbulagan, 1997) and those that try only one carefully chosen assignment without any backtracks (such as SurveyProp by Me\u0301zard & Zecchina, 2002) are our best tools for solving random formula instances, formulas arising from various real applications seem to require clause learning as a critical ingredient. The key thing that makes this second class of formulas different is the inherent structure, such as dependence graphs in scheduling problems, causes and effects in planning, and algebraic structure in group theory.\nMost theoretical and practical problem instances of satisfiability problems originate, not surprisingly, from a higher level description, such as planning domain definition language or PDDL specification for planning, timed automata or logic description for model checking, task dependency graph for scheduling, circuit description for VLSI, algebraic structure for group theory, and processor specification for hardware. Typically, this description contains more structure of the original problem than is visible in the flat CNF representation in DIMACS format (Johnson & Trick, 1996) to which it is converted before being fed into a SAT solver. This structure can potentially be used to gain efficiency in the solution process. While there has been work on extracting structure after conversion into a CNF formula by exploiting variable dependency (Giunchiglia et al., 2002; Ostrowski et al., 2002), constraint redundancy (Ostrowski et al., 2002), symmetry (Aloul et al., 2002), binary clauses (Brafman, 2001), and partitioning (Amir & McIlraith, 2000), using the original higher level description itself to generate structural information is likely to be more effective. The latter approach, despite its intuitive appeal, remains largely unexplored, except for suggested use in bounded model checking (Shtrichman, 2000) and the separate consideration of cause variables and effect variables in planning (Kautz & Selman, 1996).\nIn this paper, we further open this line of research by proposing an effective method for exploiting problem structure to guide the branching decision process of clause learning algorithms. Our approach uses the original high level problem description to generate not only a CNF encoding but also a branching sequence that guides the SAT solver toward an efficient solution. This branching sequence serves as auxiliary structural information that was possibly lost in the process of encoding the problem as a CNF formula. It makes clause learning algorithms learn useful clauses instead of wasting time learning those that may not be reused in future at all. We give an exact sequence generation algorithm for pebbling formulas, using the underlying pebbling graph as the high level description. We also give a much simpler but approximate branching sequence generation algorithm for GTn formulas, utilizing their underlying ordering structure. Our sequence generators work for the 1UIP learning scheme (Zhang et al., 2001), which is one of the best known. They can also be extended to other schemes, including FirstNewCut. Our empirical results are based on our extension of the popular SAT solver zChaff (Moskewicz et al., 2001).\nWe show that the use of branching sequences produced by our generator leads to exponential empirical speedups for the class of grid and randomized pebbling formulas. These formulas, more commonly occurring in theoretical proof complexity literature (Ben-Sasson et al., 2000; Beame et al., 2003a), can be thought of as representing precedence graphs in dependent task systems and scheduling scenarios. They can also be viewed as restricted planning problems. Although admitting a polynomial size solution, both grid and randomized pebbling problems are not so easy to solve deterministically, as is indicated by our experimental results for unmodified zChaff. We also report significant gains obtained for the class of GTn formulas which, again, have appeared frequently in proof complexity results (Krishnamurthy, 1985; Bonet & Galesi, 2001; Alekhnovich et al., 2002). From a broader perspective, our results for pebbling and GTn formulas serve as a proof of concept that analysis of problem structure can be used to achieve dramatic improvements even in the current best clause learning based SAT solvers."}, {"heading": "2. Preliminaries", "text": "A CNF formula F is an and (\u2227) of clauses, where each clause is an or (\u2228) of literals, and a literal is a variable or its negation (\u00ac). It is natural to think of F as a set of clauses and each clause as a set of literals. A clause that is a subset of another is called its subclause. The size of F is the number of clauses in F .\nLet \u03c1 be a partial assignment to the variables of F . The restricted formula F \u03c1 is obtained from F by replacing variables in \u03c1 with their assigned values. F is said to be simplified if all clauses with at least one true literal are deleted and all occurrences of false literals are removed from clauses. F |\u03c1 denotes the result of simplifying the restricted formula F \u03c1.\n2.1 The DPLL Procedure\nThe basic idea of the Davis-Putnam-Logemann-Loveland (DPLL) procedure (Davis & Putnam, 1960; Davis et al., 1962) for testing satisfiability of CNF formulas is to branch on variables, setting them to true or false, until either an initial clause is violated (i.e. has all literals set to false) or no more clauses remain in the simplified residual formula. In the former case, we backtrack to the last branching decision whose other branch has not been tried yet, reverse the decision, and proceed recursively. In the latter, we terminate with a satisfying assignment. If all possible branches have been unsuccessfully tried, the formula is declared unsatisfiable. To increase efficiency, unit clauses (those with only one unset literal) are immediately set to true. Pure literals (those whose negation does not appear) are also set to true as a preprocessing step and, in some implementations, in the simplification process after every branch.\nIn this paper, we will use the term DPLL to denote the basic branching and backtracking procedure described above. It will not include learning conflict clauses when backtracking, but will allow intelligent branching heuristics as well as common extensions such as fast backtracking and restarts. Note that this is in contrast with the occasional use of the term DPLL to encompass practically all branching and backtracking approaches to SAT, including those involving learning."}, {"heading": "2.2 Proof Systems", "text": "A propositional proof system (Cook & Reckhow, 1977) is a polynomial time computable predicate S such that a propositional formula F is unsatisfiable iff there exists a proof p for which S(F, p) holds. In other words, it is an efficient (in the size of the proof) procedure to check the correctness of proofs presented in a certain format. Finding short proofs, however, may still be difficult. In fact, short proofs may not exist in the proof system if it is too weak. In the rest of this paper, we refer to such systems simply as proof systems and omit the word propositional.\nDefinition 1. For a proof system S and an unsatisfiable formula F , the complexity of F in S, denoted CS(F ), is the length of the shortest refutation of F in S. For a family {Fn} of formulas over increasing number of variables n, its asymptotic complexity in S, denoted CS(Fn) with abuse of notation, is measured with respect to the increasing sizes of Fn.\nDefinition 2. For proof systems S and T , and a function f : N \u2192 N,\n\u2022 S is natural if for any formula F and restriction \u03c1 on its variables, CS(F |\u03c1) \u2264 CS(F ).\n\u2022 S is a refinement of T if proofs in S are also (restricted) proofs in T .\n\u2022 A refinement S of T is f(n)-proper if there exists a witnessing family {Fn} of formulas such that CS(Fn) \u2265 f(n) \u00b7 CT (Fn). The refinement is exponentially-proper if f(n) =\n2n \u2126(1) and super-polynomially-proper if f(n) = n\u03c9(1)."}, {"heading": "2.3 Resolution", "text": "Resolution (RES) is a widely studied simple proof system that can be used to prove unsatisfiability of CNF formulas. Our complexity results concerning the power of clause learning are in relation to this system. The resolution rule states that given clauses (A \u2228 x) and (B\u2228\u00acx), we can derive clause (A\u2228B) by resolving on x. A resolution derivation of C from a CNF formula F is a sequence \u03c0 = (C1, C2, . . . , Cs \u2261 C) where each clause Ci is either a clause of F (an initial clause) or derived by applying the resolution rule to Cj and Ck, j, k < i (a derived clause). The size of \u03c0 is s, the number of clauses occurring in it. We will assume that each Cj 6= C in \u03c0 is used to derive at least one other clause Ci, i > j. Any derivation of the empty clause \u039b from F , also called a refutation or proof of F , shows that F is unsatisfiable.\nDespite its simplicity, unrestricted resolution is hard to implement efficiently due to the difficulty of finding good choices of clauses to resolve; natural choices typically yield huge storage requirements. Various restrictions on the structure of resolution proofs lead to less powerful but easier to implement refinements that have been studied well, such as treelike, regular, linear, positive, negative, semantic, and Davis-Putnam resolution. Tree-like resolution uses non-empty derived clauses exactly once in the proof and is equivalent to an optimal DPLL procedure. Regular resolution allows any variable to be resolved upon at most once along any \u201cpath\u201d in the proof from an initial clause to \u039b, allowing (restricted) reuse of derived clauses. Linear resolution requires each clause Ci in a derivation (C1, C2, . . . , Cs) to be either an initial clause or be derived by resolving Ci\u22121 with Cj, j < i \u2212 1. For any assignment \u03b1 to the variables, an \u03b1-derivation requires at least one clause involved in each resolution step to be falsified by \u03b1. When \u03b1 is the all false assignment, the derivation is positive. When it is the all true assignment, the derivation is negative. A derivation is semantic if it is an \u03b1-derivation for some \u03b1. Davis-Putnam resolution, also called ordered resolution, is a refinement of regular resolution where every sequence of variables in a path from an initial clause to \u039b respects the same ordering on the variables.\nWhile all these refinements are sound and complete as proof systems, they differ in efficiency. For instance, regular, linear, positive, negative, semantic, and Davis-Putnam resolution are all known to be exponentially stronger than tree-like resolution (Bonet et al., 2000; Bonet & Galesi, 2001; Buresh-Oppenheim & Pitassi, 2003) whereas tree-like, regular, and Davis-Putnam resolution are known to be exponentially weaker than RES (Bonet et al., 2000; Alekhnovich et al., 2002).\nProposition 1. Tree-like, regular, linear, positive, negative, semantic, and Davis-Putnam resolution are natural refinements of RES.\nProposition 2 (Bonet et al., 2000; Alekhnovich et al., 2002). Tree-like, regular, and Davis-Putnam resolution are exponentially-proper natural refinements of RES."}, {"heading": "2.4 Clause Learning", "text": "Clause learning (see e.g., Marques-Silva & Sakallah, 1996) can be thought of as an extension of the DPLL procedure that caches causes of assignment failures in the form of learned clauses. It proceeds by following the normal branching process of DPLL until there is a \u201cconflict\u201d after unit propagation. If this conflict occurs when no variable is currently branched upon, the formula is declared unsatisfiable. Otherwise, the \u201cconflict graph\u201d is analyzed and the \u201ccause\u201d of the conflict is learned in the form of a \u201cconflict clause.\u201d The procedure now backtracks and continues as in ordinary DPLL, treating the learned clause just like initial ones. A clause is said to be known at a stage if it is either an initial clause or has previously been learned.\nThe learning process is expected to save us from redoing the same computation when we later have an assignment that causes conflict due in part to the same reason. Variations of such conflict-driven learning include different ways of choosing the clause to learn (different learning schemes) and possibly allowing multiple clauses to be learned from a single conflict (Zhang et al., 2001). In the last decade, many algorithms based on this idea have been proposed and demonstrated to be empirically successful on large problems that could not be handled using other methodologies (Bayardo Jr. & Schrag, 1997; Marques-Silva & Sakallah, 1996; Zhang, 1997; Moskewicz et al., 2001). We leave a more detailed discussion of the concepts involved in clause learning as well as its formulation as a proof system CL to Section 3."}, {"heading": "2.5 Pebbling Formulas", "text": "Pebbling formulas are unsatisfiable CNF formulas whose variations have been used repeatedly in proof complexity to obtain theoretical separation results between different proof systems (Ben-Sasson et al., 2000; Beame et al., 2003a). The version we will use in this paper is known to be easy for regular resolution but hard for tree-like resolution, and hence for DPLL without learning (Ben-Sasson et al., 2000). We use these formulas to show how one can utilize problem structure to allow clause learning algorithms to handle much bigger problems than they otherwise can.\nPebbling formulas represent the constraints for sequencing a system of tasks that need to be completed, where each task can be accomplished in a number of alternative ways. The associated pebbling graph has a node for each task, labeled by a disjunction of variables representing the different ways of completing the task. Placing a pebble on a node in the graph represents accomplishing the corresponding task. Directed edges between nodes denote task precedence; a node is pebbled when all of its predecessors in the graph are pebbled. The pebbling process is initialized by placing pebbles on all indegree zero nodes. This corresponds to completing those tasks that do not depend on any other.\nFormally, a Pebbling formula PblG is an unsatisfiable CNF formula associated with a directed, acyclic pebbling graph G (see Figure 1). Nodes of G are labeled with disjunctions of variables, i.e. with clauses. A node labeled with clause C is thought of as pebbled under a (partial) variable assignment \u03c3 if C|\u03c3 = true. PblG contains three kinds of clauses \u2013 precedence clauses, source clauses and target clauses. For instance, a node labeled (x1\u2228x2) with three predecessors labeled (p1\u2228p2\u2228p3), q1 and (r1\u2228r2) generates six precedence clauses (\u00acpi \u2228 \u00acqj \u2228 \u00acrk \u2228 x1 \u2228 x2), where i \u2208 {1, 2, 3}, j \u2208 {1} and k \u2208 {1, 2}. The precedence\nclauses imply that if all predecessors of a node are pebbled, then the node itself must also be pebbled. For every indegree zero source node s of G, PblG contains the clause labeling s as a source clause. Thus, PblG implies that all source nodes are pebbled. For every outdegree zero target node of G labeled, say, (t1 \u2228 t2), PblG has target clauses \u00act1 and \u00act2. These imply that target nodes are not pebbled, and provide a contradiction.\nGrid pebbling formulas are based on simple pyramid-shaped layered pebbling graphs with distinct variable labels, 2 predecessors per node, and disjunctions of size 2 (see Figure 1). Randomized pebbling formulas are more complicated and correspond to random pebbling graphs. In this paper, we only consider pebbling graphs where no variable appears more than once in any node label. In general, random pebbling graphs allow multiple target nodes. However, the more the targets, the easier it is to produce a contradiction because we can focus only on the (relatively smaller) subgraph under the lowest target. Hence, for our experiments, we add a simple grid structure at the top of randomly generated pebbling formulas to make them have exactly one target.\nAll pebbling formulas with a single target are minimally unsatisfiable, i.e. any strict subset of their clauses admits a satisfying assignment. For each formula PblG we use for our experiments, we also use a satisfiable version of it, called PblSATG , obtained by randomly choosing a clause of PblG and deleting it. When G is viewed as a task graph, Pbl SAT G corresponds to a task system with a single fault, and finding a satisfying assignment for it corresponds to locating the fault.\n2.6 The GTn Formulas\nThe GTn formulas are unsatisfiable CNF formulas based on the ordering principle that any partial order on the set {1, 2, . . . , n} must have a maximal element. They were first considered by Krishnamurthy (1985) and later used by Bonet and Galesi (2001) to show the\noptimality of the size-width relationship of resolution proofs. Recently, Alekhnovich et al. (2002) used a variation, called GT \u2032n, to show an exponential separation between RES and regular resolution.\nThe variables of GTn are xi,j for i, j \u2208 [n], i 6= j, which should be thought of as the binary predicate i \u227b j. Clauses (\u00acxi,j \u2228\u00acxj,i) ensure that \u227b is anti-symmetric and (\u00acxi,j \u2228 \u00acxj,k \u2228 xi,k) ensure that \u227b is transitive. This makes \u227b a partial order on [n]. Successor clauses (\u2228k 6=jxk,j) provide the contradiction by saying that every element j has a successor in [n] \\ {j}, which is clearly false for the maximal elements of [n] under the ordering \u227b.\nThese formulas, although capturing a simple mathematical principle, are empirically difficult for many SAT solvers including zChaff. We employ our techniques to improve the performance of zChaff on these formulas. We use for our experiments the unsatisfiable version GTn described above as well as a satisfiable version GT SAT n obtained by deleting a randomly chosen successor clause. The reason we consider these ordering formulas in addition to seemingly harder pebbling formulas is that the latter admit short tree-like proofs in certain extensions of RES whereas the former seem to critically require reuse of derived or learned clauses for short refutations. We elaborate on this in Section 6.2."}, {"heading": "3. A Formal Framework for Studying Clause Learning", "text": "Although many SAT solvers based on clause learning have been proposed and demonstrated to be empirically successful, a theoretical discussion of the underlying concepts and structures needed for our analysis is lacking. This section focuses on this formal framework."}, {"heading": "3.1 Unit Propagation and Decision Levels", "text": "All clause learning algorithms discussed in this paper are based on unit propagation, which is the process of repeatedly applying the unit clause rule followed by formula simplification until no clause with exactly one unassigned literal remains. In this context, it is convenient to work with residual formulas at different stages of DPLL. Let \u03c1 be the partial assignment at some stage of DPLL on formula F . The residual formula at this stage is obtained by applying unit propagation to the simplified formula F |\u03c1.\nWhen using unit propagation, variables assigned values through the actual branching process are called decision variables and those assigned values as a result of unit propagation are called implied variables. Decision and implied literals are analogously defined. Upon backtracking, the last decision variable no longer remains a decision variable and might instead become an implied variable depending on the clauses learned so far. The decision level of a decision variable x is one more than the number of current decision variables at the time of branching on x. The decision level of an implied variable is the maximum of the decision levels of decision variables used to imply it. The decision level at any step of the underlying DPLL procedure is the maximum of the decision levels of all current decision variables. Thus, for instance, if the clause learning algorithm starts off by branching on x, the decision level of x is 1 and the algorithm at this stage is at decision level 1.\nA clause learning algorithm stops and declares the given formula to be unsatisfiable whenever unit propagation leads to a conflict at decision level zero, i.e. when no variable is currently branched upon. This condition will be referred to in this paper as a conflict at decision level zero."}, {"heading": "3.2 Branching Sequence", "text": "We use the notion of branching sequence to prove an exponential separation between DPLL and clause learning. It generalizes the idea of a static variable order by letting the order differ from branch to branch in the underlying DPLL procedure. In addition, it also specifies which branch (true or false) to explore first. This can clearly be useful for satisfiable formulas, and can also help on unsatisfiable ones by making the algorithm learn useful clauses earlier in the process.\nDefinition 3. A branching sequence for a CNF formula F is a sequence \u03c3 = (l1, l2, . . . , lk) of literals of F , possibly with repetitions. A DPLL based algorithm A on F branches according to \u03c3 if it always picks the next variable v to branch on in the literal order given by \u03c3, skips v if v is currently assigned a value, and otherwise branches further by setting the chosen literal to false and deleting it from \u03c3. When \u03c3 becomes empty, A reverts back to its default branching scheme.\nDefinition 4. A branching sequence \u03c3 is complete for a formula F under a DPLL based algorithm A if A branching according to \u03c3 terminates before or as soon as \u03c3 becomes empty. Otherwise it is incomplete or approximate.\nClearly, how well a branching sequence works for a formula depends on the specifics of the clause learning algorithm used, such as its learning scheme and backtracking process. One needs to keep these in mind when generating the sequence. It is also important to note that while the size of a variable order is always the same as the number of variables in the formula, that of an effective branching sequence is typically much more. In fact, the size of a branching sequence complete for an unsatisfiable formula F is equal to the size of an unsatisfiability proof of F , and when F is satisfiable, it is proportional to the time needed to find a satisfying assignment."}, {"heading": "3.3 Clause Learning Proofs", "text": "The notion of clause learning proofs connects clause learning with resolution and provides the basis for our complexity bounds. If a given CNF formula F is unsatisfiable, clause learning terminates with a conflict at decision level zero. Since all clauses used in this final conflict themselves follow directly or indirectly from F , this failure of clause learning in finding a satisfying assignment constitutes a logical proof of unsatisfiability of F . We denote by CL the proof system consisting of all such proofs. Our bounds compare the sizes of proofs in CL with the sizes of (possibly restricted) resolution proofs. Recall that clause learning algorithms can use one of many learning schemes, resulting in different proofs.\nDefinition 5. A clause learning (CL) proof \u03c0 of an unsatisfiable CNF formula F under learning scheme S and induced by branching sequence \u03c3 is the result of applying DPLL with unit propagation on F , branching according to \u03c3, and using scheme S to learn conflict clauses such that at the end of this process, there is a conflict at decision level zero. The size of the proof, size(\u03c0), is |\u03c3|."}, {"heading": "3.4 Implication Graph and Conflicts", "text": "Unit propagation can be naturally associated with an implication graph that captures all possible ways of deriving all implied literals from decision literals.\nDefinition 6. The implication graph G at a given stage of DPLL is a directed acyclic graph with edges labeled with sets of clauses. It is constructed as follows:\n1. Create a node for each decision literal, labeled with that literal. These will be the indegree zero source nodes of G.\n2. While there exists a known clause C = (l1 \u2228 . . . lk \u2228 l) such that \u00acl1, . . . ,\u00aclk label nodes in G,\n(a) Add a node labeled l if not already present in G.\n(b) Add edges (li, l), 1 \u2264 i \u2264 k, if not already present.\n(c) Add C to the label set of these edges. These edges are thought of as grouped together and associated with clause C.\n3. Add to G a special node \u039b. For any variable x which occurs both positively and negatively in G, add directed edges from x and \u00acx to \u039b.\nSince all node labels in G are distinct, we identify nodes with the literals labeling them. Any variable x occurring both positively and negatively in G is a conflict variable, and x as well as \u00acx are conflict literals. G contains a conflict if it has at least one conflict variable. DPLL at a given stage has a conflict if the implication graph at that stage contains a conflict. A conflict can equivalently be thought of as occurring when the residual formula contains the empty clause \u039b.\nBy definition, an implication graph may not contain a conflict at all, or it may contain many conflict variables and several ways of deriving any single literal. To better understand and analyze a conflict when it occurs, we work with a subgraph of an implication graph, called the conflict graph (see Figure 2), that captures only one among possibly many ways of reaching a conflict from the decision variables using unit propagation.\nDefinition 7. A conflict graph H is any subgraph of an implication graph with the following properties:\n1. H contains \u039b and exactly one conflict variable.\n2. All nodes in H have a path to \u039b.\n3. Every node l in H other than \u039b either corresponds to a decision literal or has precisely the nodes \u00acl1,\u00acl2, . . . ,\u00aclk as predecessors where (l1\u2228 l2\u2228 . . .\u2228 lk\u2228 l) is a known clause.\nWhile an implication graph may or may not contain conflicts, a conflict graph always contains exactly one. The choice of the conflict graph is part of the strategy of the solver. A typical strategy will maintain one subgraph of an implication graph that has properties 2 and 3 from Definition 7, but not property 1. This can be thought of as a unique inference subgraph of the implication graph. When a conflict is reached, this unique inference subgraph is extended to satisfy property 1 as well, resulting in a conflict graph, which is then used to analyze the conflict."}, {"heading": "3.4.1 Conflict clauses", "text": "Consider the implication graph at a stage where there is a conflict and fix a conflict graph contained in that implication graph. Pick any cut in the conflict graph that has all decision variables on one side, called the reason side, and \u039b as well as at least one conflict literal on the other side, called the conflict side. All nodes on the reason side that have at least one edge going to the conflict side form a cause of the conflict. The negations of the corresponding literals forms the conflict clause associated with this cut."}, {"heading": "3.5 Trivial Resolution and Learned Clauses", "text": "Definition 8. A resolution derivation (C1, C2, . . . , Ck) is trivial iff all variables resolved upon are distinct and each Ci, i \u2265 3, is either an initial clause or is derived by resolving Ci\u22121 with an initial clause.\nA trivial derivation is tree-like, regular, linear, as well as ordered. As the following Propositions show, trivial derivations correspond to conflicts in clause learning algorithms.\nProposition 3. Let F be a CNF formula. If there is a trivial resolution derivation of a clause C /\u2208 F from F then setting all literals of C to false leads to a conflict by unit propagation.\nProof. Let \u03c0 = (C1, C2, . . . , Ck \u2261 C) be a trivial resolution derivation of C from F . Let Ck = (l1 \u2228 l2 \u2228 . . . \u2228 lq) and \u03c1 be the partial assignment that sets all li, 1 \u2264 i \u2264 q, to false. Assume without loss of generality that clauses in \u03c0 are ordered so that all initial clauses precede any derived clause. We give a proof by induction on the number of derived clauses in \u03c0.\nFor the base case, \u03c0 has only one derived clause, C \u2261 Ck, Assume without loss of generality that Ck = (A\u2228B) and Ck is derived by resolving two initial clauses (A\u2228 x) and (B \u2228 \u00acx) on variable x. Since \u03c1 falsifies Ck, it falsifies all literals of A, implying x = true by unit propagation. Similarly, \u03c1 falsifies B, implying x = false and resulting in a conflict.\nWhen \u03c0 has at least two derived clauses, Ck, by triviality of \u03c0, must be derived by resolving Ck\u22121 /\u2208 F with a clause in F . Assume without loss of generality that Ck\u22121 \u2261\n(A\u2228x) and the clause from F used in this resolution step is (B \u2228\u00acx), where Ck = (A\u2228B). Since \u03c1 falsifies C \u2261 Ck, it falsifies all literals of B, implying x = false by unit propagation. This in turn results in falsifying all literals of Ck\u22121 because all literals of A are also set to false by \u03c1. Now (C1, . . . , Ck\u22121) is a trivial resolution derivation of Ck\u22121 /\u2208 F from F with one less derived clause than \u03c0, and all literals of Ck\u22121 are falsified. By induction, this must lead to a conflict by unit propagation.\nProposition 4. Any conflict clause can be derived from initial and previously derived clauses using a trivial resolution derivation.\nProof. Let \u03c3 be the cut in a fixed conflict graph associated with the given conflict clause. Let Vconflict(\u03c3) denote the set of variables on the conflict side of \u03c3, but including the conflict variable only if it occurs both positively and negatively on the conflict side. We will prove by induction on |Vconflict(\u03c3)| the stronger statement that the conflict clause associated with a cut \u03c3 has a trivial derivation from known (i.e. initial or previously derived) clauses resolving precisely on the variables in Vconflict(\u03c3).\nFor the base case, Vconflict(\u03c3) = \u03c6 and the conflict side contains only \u039b and a conflict literal, say x. The cause associated with this cut consists of node \u00acx that has an edge to \u039b, and nodes \u00acl1,\u00acl2, . . . ,\u00aclk, corresponding to a known clause Cx = (l1 \u2228 l2 \u2228 . . .\u2228 lk \u2228x), that each have an edge to x. The conflict clause for this cut is simply the known clause Cx itself, having a length zero trivial derivation.\nWhen Vconflict(\u03c3) 6= \u03c6, pick a node y on the conflict side all whose predecessors are on the reason side (see Figure. 3). Let the conflict clause be C = (l1 \u2228 l2 \u2228 . . . \u2228 lp) and assume without loss of generality that the predecessors of y are \u00acl1,\u00acl2, . . . ,\u00aclk for some k \u2264 p. By definition of unit propagation, Cy = (l1 \u2228 l2 \u2228 . . . \u2228 lk \u2228 y) must be a known clause. Obtain a new cut \u03c3\u2032 from \u03c3 by moving node y from the conflict side to the reason side. The new associated conflict clause must be of the form C \u2032 = (\u00acy \u2228D), where D is a\nsubclause of C. Now Vconflict(\u03c3 \u2032) \u2282 Vconflict(\u03c3). Consequently, by induction, C \u2032 must have a trivial resolution derivation from known clauses resolving precisely upon the variables in Vconflict(\u03c3\n\u2032). Recall that no variable occurs twice in a conflict graph except the conflict variable. Hence Vconflict(\u03c3\n\u2032) has exactly all variables of Vconflict(\u03c3) other than y. Using this trivial derivation of C \u2032 and finally resolving C \u2032 with the known clause Cy on variable y gives us a trivial derivation of C from known clauses. This completes the inductive step."}, {"heading": "3.6 Different Learning Schemes", "text": "Different cuts separating decision variables from \u039b and a conflict literal correspond to different learning schemes (see Figure 2). One can also create learning schemes based on cuts not involving conflict literals at all (Zhang et al., 2001), but the effectiveness of such schemes is not clear. These will not be considered here.\nIt is insightful to think of the nondeterministic scheme as the most general learning scheme. Here we pick the cut nondeterministically, choosing, whenever possible, one whose associated clause is not already known. Since we can repeatedly branch on the same last variable, nondeterministic learning subsumes learning multiple clauses from a single conflict as long as the sets of nodes on the reason side of the corresponding cuts form a (set-wise) decreasing sequence. For simplicity, we will assume that only one clause is learned from any conflict.\nIn practice, however, we employ deterministic schemes. The decision scheme (Zhang et al., 2001), for example, uses the cut whose reason side comprises all decision variables. rel-sat (Bayardo Jr. & Schrag, 1997) uses the cut whose conflict side consists of all implied variables at the current decision level. This scheme allows the conflict clause to have exactly one variable from the current decision level, causing an automatic flip in its assignment upon backtracking.\nThis nice flipping property holds in general for all unique implication points (UIPs) (Marques-Silva & Sakallah, 1996). A UIP of an implication graph is a node at the current decision level d such that any path from the decision variable at level d to the conflict variable as well as its negation must go through it. Intuitively, it is a single reason at level d that causes the conflict. Whereas rel-sat uses the decision variable as the obvious UIP, GRASP (Marques-Silva & Sakallah, 1996) and zChaff (Moskewicz et al., 2001) use FirstUIP, the one that is \u201cclosest\u201d to the conflict variable. GRASP also learns multiple clauses when faced with a conflict. This makes it typically require fewer branching steps but possibly slower because of the time lost in learning and unit propagation.\nThe concept of UIP can be generalized to decision levels other than the current one. The 1UIP scheme corresponds to learning the FirstUIP clause of the current decision level, the 2UIP scheme to learning the FirstUIP clauses of both the current level and the one before, and so on. Zhang et al. (2001) present a comparison of all these and other learning schemes and conclude that 1UIP is quite robust and outperforms all other schemes they consider on most of the benchmarks."}, {"heading": "3.6.1 The FirstNewCut Scheme", "text": "We propose a new learning scheme called FirstNewCut whose ease of analysis helps us demonstrate the power of clause learning. We would like to point out that we use this scheme\nhere only to prove our theoretical bounds using specific formulas. Its effectiveness on other formulas has not been studied yet. We would also like to point out that the experimental results in this paper are for the 1UIP learning scheme, but can also be extended to certain other schemes, including FirstNewCut.\nThe key idea behind FirstNewCut is to make the conflict clause as relevant to the current conflict as possible by choosing a cut close to the conflict literals. This is what the FirstUIP scheme also tries to achieve in a slightly different manner. For the following definitions, fix a cut in a conflict graph and let S be the set of nodes on the reason side that have an edge to some node on the conflict side. S is the reason side frontier of the cut. Let CS be the conflict clause associated with this cut.\nDefinition 9. Minimization of conflict clause CS is the following process: while there exists a node v \u2208 S all of whose predecessors are also in S, move v to the conflict side, remove it from S, and repeat.\nDefinition 10. FirstNewCut scheme: Start with a cut whose conflict side consists of \u039b and a conflict literal. If necessary, repeat the following until the associated conflict clause, after minimization, is not already known: pick a node on the conflict side, and move all its predecessors that lie on the reason side, other than those that correspond to decision variables, to the conflict side. Finally, learn the resulting new minimized conflict clause.\nThis scheme starts with the cut that is closest to the conflict literals and iteratively moves it back toward the decision variables until a new associated conflict clause is found. This backward search always halts because the cut with all decision variables on the reason side is certainly a new cut. Note that there are potentially several ways of choosing a literal to move the cut back, leading to different conflict clauses. The FirstNewCut scheme, by definition, always learns a clause not already known. This motivates the following:\nDefinition 11. A clause learning scheme is non-redundant if on a conflict, it always learns a clause not already known."}, {"heading": "3.7 Fast Backtracking and Restarts", "text": "Most clause learning algorithms use fast backtracking or conflict directed backjumping where one uses the conflict graph to undo not only the last branching decision but also all other recent decisions that did not contribute to the current conflict (Stallman & Sussman, 1977). In particular, the SAT solver zChaff that we use for our experiments backtracks to decision level zero when it learns a unit clause. This property influences the structure of the sequence generation algorithm presented in Section 6.1.1.\nMore precisely, the level that a clause learning algorithm employing this technique backtracks to is one less than the maximum of the decision levels of all decision variables (i.e. the sources of the conflict) present in the underlying conflict graph. Note that the current conflict might use clauses learned earlier as a result of branching on the apparently redundant variables. This implies that fast backtracking in general cannot be replaced by a \u201cgood\u201d branching sequence that does not produce redundant branches. For the same reason, fast backtracking cannot either be replaced by simply learning the decision scheme clause. However, the results we present in this paper are independent of whether or not fast backtracking is used.\nRestarts allow clause learning algorithms to arbitrarily restart their branching process from decision level zero. All clauses learned so far are however retained and now treated as additional initial clauses (Baptista & Silva, 2000). As we will show, unlimited restarts, performed at the correct step, can make clause learning very powerful. In practice, this requires extending the strategy employed by the solver to include when and how often to restart. Unless otherwise stated, clause learning proofs will be assumed to allow no restarts.\n4. Clause Learning and Proper Natural Refinements of RES\nWe prove that the proof system CL, even without restarts, is stronger than all proper natural refinements of RES. We do this by first introducing a way of extending any CNF formula based on a given RES proof of it. We then show that if a formula F f(n)-separates RES from a natural refinement S, its extension f(n)-separates CL from S. The existence of such an F is guaranteed for all f(n)-proper natural refinements by definition."}, {"heading": "4.1 The Proof Trace Extension", "text": "Definition 12. Let F be a CNF formula and \u03c0 be a RES refutation of it. Let the last step of \u03c0 resolve v with \u00acv. Let S = \u03c0 \\ (F \u222a {\u00acv,\u039b}). The proof trace extension PT (F, \u03c0) of F is a CNF formula over variables of F and new trace variables tC for clauses C \u2208 S. The clauses of PT (F, \u03c0) are all initial clauses of F together with a trace clause (\u00acx \u2228 tC) for each clause C \u2208 S and each literal x \u2208 C.\nWe first show that if a formula has a short RES refutation, then the corresponding proof trace extension has a short CL proof. Intuitively, the new trace variables allow us to simulate every resolution step of the original proof individually, without worrying about extra branches left over after learning a derived clause.\nLemma 1. Suppose a formula F has a RES refutation \u03c0. Let F \u2032 = PT (F, \u03c0). Then CCL(F \u2032) < size(\u03c0) when CL uses the FirstNewCut scheme and no restarts.\nProof. Suppose \u03c0 contains a derived clause Ci whose strict subclause C \u2032 i can be derived by resolving two previously occurring clauses. We can replace Ci with C \u2032 i, do trivial simplifications on further derivations that used Ci and obtain a simpler proof \u03c0 \u2032 of F . Doing this repeatedly will remove all such redundant clauses and leave us with a simplified proof no larger in size. Hence we will assume without loss of generality that \u03c0 has no such clause.\nViewing \u03c0 as a sequence of clauses, its last two elements must be a literal, say v, and \u039b. Let S = \u03c0 \\ (F \u222a {v,\u039b}). Let (C1, C2, . . . , Ck) be the subsequence of \u03c0 that has precisely the clauses in S. Note that Ci \u2261 \u00acv for some i, 1 \u2264 i \u2264 k. We claim that the branching sequence \u03c3 = (tC1 , tC2 , . . . , tCk) induces a CL proof of F of size k using the FirstNewCut scheme. To prove this, we show by induction that after i branching steps, the clause learning procedure branching according to \u03c3 has learned clauses C1, C2, . . . , Ci, has trace variables tC1 , tC2 , . . . , tCi set to true, and is at decision level i.\nThe base case for induction, i = 0, is trivial. The clause learning procedure is at decision level zero and no clauses have been learned. Suppose the inductive claim holds after branching step i \u2212 1. Let Ci = (x1 \u2228 x2 \u2228 . . . \u2228 xl). Ci must have been derived in \u03c0 by resolving two clauses (A \u2228 y) and (B \u2228 \u00acy) coming from F \u222a {C1, C2, . . . , Ci\u22121}, where\nCi = (A \u2228 B). The i th branching step sets tCi = false. Unit propagation using trace clauses (\u00acxj \u2228 tCi), 1 \u2264 j \u2264 l, sets each xj to false, thereby falsifying all literals of A and B. Further unit propagation using (A \u2228 y) and (B \u2228 \u00acy) implies y as well as \u00acy, leading to a conflict. The cut in the conflict graph containing y and \u00acy on the conflict side and everything else on the reason side yields Ci as the FirstNewCut clause, which is learned from this conflict. The process now backtracks and flips the branch on tCi by setting it to true. At this stage, the clause learning procedure has learned clauses C1, C2, . . . , Ci, has trace variables tC1 , tC2 , . . . , tCi set to true, and is at decision level i. This completes the inductive step.\nThe inductive proof above shows that when the clause learning procedure has finished branching on all k literals in \u03c3, it will have learned all clauses in S. Adding to this the initial clauses F that are already known, the procedure will have as known clauses \u00acv as well as the two unit or binary clauses used to derive v in \u03c0. These immediately generate \u039b in the residual formula by unit propagation using variable v, leading to a conflict at decision level k. Since this conflict does not use any decision variable, fast backtracking retracts all k branches. The conflict, however, still exists at decision level zero, thereby concluding the clause learning procedure and finishing the CL proof.\nLemma 2. Let S be an f(n)-proper natural refinement of RES whose weakness is witnessed by a family {Fn} of formulas. Let {\u03c0n} be the family of shortest RES proofs of {Fn}. Let {F \u2032n} = {PT (Fn, \u03c0n)}. For CL using the FirstNewCut scheme and no restarts, CS(F \u2032 n) \u2265 f(n) \u00b7 CCL(F \u2032 n).\nProof. Let \u03c1n the restriction that sets every trace variable of F \u2032 n to true. We claim that CS(F \u2032 n) \u2265 CS(F \u2032 n|\u03c1n) = CS(Fn) \u2265 f(n) \u00b7 CRES(Fn) > f(n) \u00b7 CCL(F \u2032 n). The first inequality holds because S is a natural proof system. The following equality holds because \u03c1n keeps the original clauses of Fn intact and trivially satisfies all trace clauses, thereby reducing the initial clauses of F \u2032n to precisely Fn. The next inequality holds because S is an f(n)-proper refinement of RES. The final inequality follows from Lemma 1.\nThis gives our first main result and its corollary using Proposition 2:\nTheorem 1. For any f(n)-proper natural refinement S of RES and for CL using the FirstNewCut scheme and no restarts, there exist formulas {Fn} such that CS(Fn) \u2265 f(n)\u00b7CCL(Fn).\nCorollary 1. CL can provide exponentially shorter proofs than tree-like, regular, and DavisPutnam resolution.\nRemark. As clause learning yields resolution proofs of unsatisfiable formulas, CL is a refinement of RES. However, it is not necessarily a natural proof system. If it were shown to be natural, Theorem 1, by a contradiction argument, would imply that CL using the FirstNewCut scheme and no restarts is as powerful as RES itself."}, {"heading": "5. Clause Learning and General Resolution", "text": "We begin this section by showing that CL proofs, irrespective of the learning scheme, branching strategy, or restarts used, can be efficiently simulated by RES. In the reverse direction,\nwe show that CL, with a slight variation and with unlimited restarts, can efficiently simulate RES in its full generality. The variation relates to the variables one is allowed to branch upon.\nLemma 3. For any formula F over n variables and CL using any learning scheme and any number of restarts, CRES(F ) \u2264 n \u00b7 CCL(F ).\nProof. Given a CL proof \u03c0 of F , a RES proof can be constructed by sequentially deriving all clauses that \u03c0 learns, which includes the empty clause \u039b. From Proposition 4, all these derivations are trivial and hence require at most n steps each. Consequently, the size of the resulting RES proof is at most n \u00b7size(\u03c0). Note that since we derive clauses of \u03c0 individually, restarts in \u03c0 do not affect the construction.\nDefinition 13. Let CL-- denote the variation of CL where one is allowed to branch on a literal whose value is already set explicitly or because of unit propagation.\nOf course, such a relaxation is useless in ordinary DPLL; there is no benefit in branching on a variable that doesn\u2019t even appear in the residual formula. However, with clause learning, such a branch can lead to an immediate conflict and allow one to learn a key conflict clause that would otherwise have not been learned. We will use this property to show that RES can be efficiently simulated by CL-- with enough restarts.\nWe first state a generalization of Lemma 3. CL-- can, by definition, do all that usual CL can, and is potentially stronger. The simulation of CL by RES can in fact be extended to CL-- as well. The proof goes exactly as the proof of Lemma 3 and uses the easy fact that Proposition 4 doesn\u2019t change even when one is allowed to branch on variables that are already set. This gives us:\nProposition 5. For any formula F over n variables and CL-- using any learning scheme and any number of restarts, CRES(F ) \u2264 n \u00b7 CCL--(F ).\nLemma 4. For any formula F over n variables and CL using any non-redundant scheme and at most CRES(F ) restarts, CCL--(F ) \u2264 n \u00b7 CRES(F ).\nProof. Let \u03c0 be a RES proof of F of size s. Assume without loss of generality as in the proof of Lemma 1 that \u03c0 does not contain a derived clause Ci whose strict subclause C \u2032 i can be derived by resolving two clauses occurring previously in \u03c0. The proof of this Lemma is very similar to that of Lemma 1. However, since we do not have trace variable to allow us to simulate each resolution step individually and independently, we use explicit restarts.\nViewing \u03c0 as a sequence of clauses, its last two elements must be a literal, say v, and \u039b. Let S = \u03c0 \\ (F \u222a {v,\u039b}). Let (C1, C2, . . . , Ck) be the subsequence of \u03c0 that has precisely the clauses in S. Note that Ci \u2261 \u00acv for some i, 1 \u2264 i \u2264 k. For convenience, define an extended branching sequence to be a branching sequence in which certain places, instead of being literals, can be marked as restart points. Let \u03c3 be the extended branching sequence consisting of all literals of C1, followed by a restart point, followed by all literals of C2, followed by a second restart point, and so on up to Ck. We claim that \u03c3 induces a CL-proof of F using any non-redundant learning scheme. To prove this, we show by induction that after the ith restart point in \u03c3, the CL-- procedure has learned clauses C1, C2, . . . , Ci and is at decision level zero.\nThe base case for induction, i = 0, is trivial. No clauses have been learned and the clause learning procedure is at decision level zero. Suppose the inductive claim holds after the (i\u2212 1)st restart point in \u03c3. Let Ci = (x1 \u2228 x2 \u2228 . . . \u2228 xl). Ci must have been derived in \u03c0 by resolving two clauses (A\u2228 y) and (B \u2228\u00acy) coming from F \u222a{C1, C2, . . . , Ci\u22121}, where Ci = (A \u2228 B). Continuing to branch according to \u03c3 till before the i\nth restart point makes the CL-- procedure set all if x1, x2, . . . , xl to false. Note that when all literals appearing in A and B are distinct, the last branch on xl here is on a variable that is already set because of unit propagation. CL--, however, allows this. At this stage, unit propagation using (A\u2228y) and (B \u2228 \u00acy) implies y as well as \u00acy, leading to a conflict. The conflict graph consists of \u00acxj\u2019s, 1 \u2264 j \u2264 l, as the decision literals, y and \u00acy as implied literals, and \u039b. The only new conflict clause that can learned from this very simple conflict graph is Ci. Thus, Ci is learned using any non-redundant learning scheme and the ith restart executed, as dictated by \u03c3. At this stage, the CL-- procedure has learned clauses C1, C2, . . . , Ci, and is at decision level zero. This completes the inductive step.\nThe inductive proof above shows that when the CL-- procedure has finished with the kth restart in \u03c3, it will have learned all clauses in S. Adding to this the initial clauses F that are already known, the procedure will have as known clauses \u00acv as well as the two unit or binary clauses used to derive v in \u03c0. These immediately generate \u039b in the residual formula by unit propagation using variable v, leading to a conflict at decision level zero, thereby concluding the clause learning procedure and finishing the CL-- proof. The bounds on the size of this proof and the number of restarts needed immediately follow from the definition of \u03c3.\nCombining Lemma 4 with Proposition 5, we get\nTheorem 2. CL-- with any non-redundant scheme and unlimited restarts is polynomially equivalent to RES.\nNote that Baptista and Silva (2000) showed that CL together with restarts is complete. Our theorem makes a much stronger claim about a slight variation of CL, namely, with enough restarts, this variation can always find proofs that are as short as those of RES."}, {"heading": "6. From Analysis to Practice", "text": "The complexity bounds established in the previous sections indicate that clause learning is potentially quite powerful, especially when compared to ordinary DPLL. However, natural choices such as which conflict graph to choose, which cut in it to consider, in what order to branch on variables, and when to restart, make the process highly nondeterministic. These choices must be made deterministically (or randomly) when implementing a clause learning algorithm. To harness its full potential on a given problem domain, one must, in particular, implement a learning scheme and a branch decision process suited to that domain."}, {"heading": "6.1 Solving Pebbling Formulas", "text": "As a first step toward our grand goal of translating theoretical understanding into effective implementations, we show, using pebbling problems as a concrete example, how one can utilize high level problem descriptions to generate effective branching strategies for clause\nlearning algorithms. Specifically, we use insights from our theoretical analysis to give an efficient algorithm to generate an effective branching sequence for unsatisfiable as well as satisfiable pebbling formulas (see Section 2.5). This algorithm takes as input the underlying pebbling graph (which is the high level description of the pebbling problem), and not the CNF pebbling formula itself. As we will see in Section 6.3, the generated branching sequence gives exponential empirical speedup over zChaff for both grid and randomized pebbling formulas.\nzChaff, despite being one of the current best clause learners, by default does not perform very well on seemingly simple pebbling formulas, even on the uniform grid version. Although clause learning should ideally need only polynomial time to solve these problem instances (in fact, linear time in the size of the formula), choosing a good branching order is critical for this to happen. Since nodes are intuitively pebbled in a bottom up fashion, we must also learn the right clauses (i.e. clauses labeling the nodes) in a bottom up order. However, branching on variables labeling lower nodes before those labeling higher ones prevents any DPLL based learning algorithm from backtracking the right distance and proceeding further in an effective manner. To make this clear, consider the general pebbling graph of Figure 1. Suppose we branch on and set d1, d2, d3 and a1 to false. This will lead to a contradiction through unit propagation by implying a2 is true and b1 and b2 are both false. We will learn (d1 \u2228 d2 \u2228 d3 \u2228 \u00aca2) as the associated 1UIP conflict clause and backtrack. There will still be a contradiction without any further branches, making us learn (d1 \u2228 d2 \u2228 d3) and backtrack. At this stage, we will have learned the correct clause but will be stuck with two branches on d1 and d2. Unless we had branched on e1 before branching on the variables of node d, we will not be able to learn e1 as the clause corresponding to the next higher pebbling node.\n6.1.1 Automatic Sequence Generation: PebSeq1UIP\nAlgorithm 1, PebSeq1UIP, describes a way of generating a good branching sequence for pebbling formulas. It works on any pebbling graph G with distinct label variables as input and produces a branching sequence linear in the size of the associated pebbling formula. In particular, the sequence size is linear in the number of variables as well when the indegree as well as label size are bounded by a constant.\nPebSeq1UIP starts off by handling the set U of all nodes labeled with unit clauses. Their outgoing edges are deleted and they are treated as pseudo sources. The procedure first generates a branching sequence for non-target nodes in U in increasing order of height. The key here is that when zChaff learns a unit clause, it fast-backtracks to decision level zero, effectively restarting at that point. We make use of this fact to learn these unit clauses in a bottom up fashion, unlike the rest of the process which proceeds top down in a depth-first way.\nPebSeq1UIP now adds branching sequences for the targets. Note that for an unsatisfiability proof, we only need the sequence corresponding to the first (lowest) target. However, we process all targets so that this same sequence can also be used when the formula is made satisfiable by deleting enough clauses. The subroutine PebSubseq1UIP runs on a node v, looking at its ith predecessor u in increasing order by height. No labels are output if u is the lowest predecessor; the negations of these variables will be indirectly implied during\nInput : Pebbling graph G with no repeated labels Output : Branching sequence for PblG for the 1UIP learning scheme begin foreach v in BottomUpTraversal(G) do\nv.height \u2190 1 +maxu\u2208v.preds{u.height} Sort(v.preds, increasing order by height)\n// First handle unit clause labeled nodes and generate their sequence U \u2190 {v \u2208 G.nodes : |v.labels| = 1} G.edges \u2190 G.edges \\ {(u, v) \u2208 G.edges : u \u2208 U} Add to G.sources any new nodes with now 0 preds Sort(U , increasing order by height) foreach u \u2208 U \\G.targets do\nOutput u.label PebSubseq1UIPWrapper(u)\n// Now add branching sequence for targets by increasing height Sort(G.targets, increasing order by height) foreach t \u2208 G.targets do\nPebSubseq1UIPWrapper(t)\nend\nPebSubseq1UIPWrapper(node v) begin if |v.preds| > 0 then\nPebSubseq1UIP(v, |v.preds|)\nend\nPebSubseq1UIP(node v, int i) begin u \u2190 v.preds[i]\n// If this is the lowest predecessor . . . if i = 1 then\nif !u.visited and u /\u2208 G.sources then u.visited \u2190 true PebSubseq1UIPWrapper(u)\nreturn\n// If this is not the lowest one . . . Output u.labels \\ {u.lastLabel} if !u.visitedAsHigh and u /\u2208 G.sources then\nu.visitedAsHigh\u2190 true Output u.lastLabel if !u.visited then\nu.visited \u2190 true PebSubseq1UIPWrapper(u)\nPebSubseq1UIP(v, i\u2212 1)\nfor j \u2190 (|u.labels| \u2212 2) downto 1 do Output u.labels[1], . . . , u.labels[j] PebSubseq1UIP(v, i\u2212 1)\nPebSubseq1UIP(v, i\u2212 1)\nend\nAlgorithm 1: PebSeq1UIP, generating branching sequence for pebbling formulas\nclause learning. However, it is recursed upon if not previously visited. This recursive sequence results in learning something close to the clause labeling this lowest node, but not quite that exact clause. If u is a higher predecessor (it will be marked as visitedAsHigh), PebSubseq1UIP outputs all but one variables labeling u. If u is not a source and has not previously been visited as high, the last label is output as well, and u recursed upon if necessary. This recursive sequence results in learning the clause labeling u. Finally, PebSubseq1UIP generates a recursive pattern, calling the subroutine with the next lower predecessor of v. The precise structure of this pattern is dictated by the 1UIP learning scheme and fast backtracking used in zChaff. Its size is exponential in the degree of v with label size as the base. The Grid Case. It is insightful to look at the simplified version of the sequence generation algorithm that works only for grid pebbling formulas. This is described below as Algorithm 2, GridPebSeq1UIP. Note that both predecessors of any node are at the same level for grid pebbling graphs and need not be sorted by height. There are no nodes labeled with unit clauses and there is exactly one target node t, simplifying the whole algorithm to a single call to PebSubseq1UIP(t,2) in the notation of Algorithm 1. The last for loop of this procedure and the recursive call that follows it are now redundant. We combine the original wrapper method and the calls to PebSubseq1UIP with parameters (v, 2) and (v, 1) into a single method GridPebSubseq1UIP with parameter v.\nInput : Grid pebbling graph G with target node t Output : Branching sequence for PblG for the 1UIP learning scheme begin GridPebSubseq1UIP(t)\nend\nGridPebSubseq1UIP(node v) begin if v \u2208 G.sources then\nreturn\nu \u2190 v.preds.left Output u.firstLabel if !u.visitedAsLeft and u /\u2208 G.sources then\nu.visitedAsLeft\u2190 true Output u.secondLabel if !u.visited then\nu.visited \u2190 true GridPebSubseq1UIP(u)\nu \u2190 v.preds.right if !u.visited and u /\u2208 G.sources then\nu.visited\u2190 true GridPebSubseq1UIP(u)\nend\nAlgorithm 2: GridPebSeq1UIP, generating branching sequence for grid pebbling formulas\nThe resulting branching sequence can actually be generated by a simple depth first traversal (DFS) of the grid pebbling graph, printing no labels for the nodes on the rightmost\npath (including the target node), both labels for internal nodes, and one arbitrarily chosen label for source nodes. However, this resemblance to DFS is a somewhat misleading coincidence. The resulting sequence diverges substantially from DFS order as soon as label size or indegree of some nodes is changed. For the 10 node depth 4 grid pebbling graph shown in Figure 1, the branching sequence generated by the algorithm is h1, h2, e1, e2, a1, b1, f1, f2, c1. Here, for instance, b1 is generated after a1 not because it labels the right (second) predecessor of node e but because it labels the left (first) predecessor of node f . Similarly, f1 and f2 appear after the subtree rooted at h as left predecessors of node i rather than as right predecessors of node h. Example for the General Case. To clarify the general algorithm, we describe its execution on a small example. Let G be the pebbling graph in Figure 4. Denote by t the node labeled (t1 \u2228 t2), and likewise for other nodes. Nodes c, d, f and g are at height 1, nodes a and e at height 2, node b at height 3, and node t at height 4. U = {a, b}. The edges (a, t) and (b, t) originating from these unit clause labeled nodes are removed, and t, with no predecessors anymore, is added to the list of sources. We output the label of the non-target unit nodes in U in increasing order of height, and recurse on each of them in order, i.e. we output a1, setting B = (a1), call PebSubseq1UIPWrapper on a, and then repeat this process for b. This is followed by a recursive call to PebSubseq1UIPWrapper on the target node t.\nThe call PebSubseq1UIPWrapper on a in turn invokes PebSubseq1UIP with parameters (a, 2). This sorts the predecessors of a in increasing order of height to, say, d, c, with d being the lowest predecessor. v is set to a and u is set to the second predecessor c. We output all but the last label of u, i.e. of c, making the current branching sequence B = (a1, c1). Since u is a source, nothing more needs to be done for it and we make a recursive call to PebSubseq1UIP with parameters (a, 1). This sets u to d, which is the lowest predecessor and requires nothing to be done because it is also a source. This finishes the sequence generation for a, ending at B = (a1, c1). After processing this part of the sequence, zChaff will have a as a learned clause.\nWe now output b1, the label of the unit clause b. The call, PebSubseq1UIPWrapper on b, proceeds similarly, setting predecessor order as (d, f, e), with d as the lowest predecessor. Procedure PebSubseq1UIP is called first with parameters (b, 3), setting u to e. This adds all but the last label of e to the branching sequence, making it B = (a1, c1, b1, e1, e2).\nSince this is the first time e is being visited as high, its last label is also added, making B = (a1, c1, b1, e1, e2, e3), and it is recursed upon with PebSubseq1UIPWrapper(e). This recursion extends the sequence to B = (a1, c1, b1, e1, e2, e3, f1). After processing this part of B, zChaff will have both a and (e1 \u2228 e2 \u2228 e3) as learned clauses. Getting to the second highest predecessor f of b, which happens to be a source, we simply add another f1 to B. Finally, we get to the third highest predecessor d of b, which happens to be the lowest as well as a source, thus requiring nothing to be done. Coming out of the recursion, back to u = f , we generate the pattern given by the last for loop, which is empty because the label size of f is only 2. Coming out once more of the recursion to u = e, the for loop pattern generates e1, f1 and is followed by a call to PebSubseq1UIP with the next lower predecessor f as the second parameter, which generates f1. This makes the current sequence B = (a1, c1, b1, e1, e2, e3, f1, f1, e1, f1, f1). After processing this, zChaff will also have b as a learned clause.\nThe final call to PebSubseq1UIPWrapper with parameter t doesn\u2019t do anything because both predecessors of t were removed in the beginning. Since both a and b have been learned, zChaff will have an immediate contradiction at decision level zero. This gives us the complete branching sequence B = (a1, c1, b1, e1, e2, e3, f1, f1, e1, f1, f1) for the pebbling formula PblG."}, {"heading": "6.1.2 Complexity of Sequence Generation", "text": "Let graph G have n nodes, indegree of non-source nodes between dmin and dmax, and label size between lmin and lmax. For simplicity of analysis, we will assume that lmin = lmax = l and dmin = dmax = d (l = d = 2 for a grid graph).\nLet us first compute the size of the pebbling formula associated with G. The running time of PebSeq1UIP and the size of the branching sequence generated will be given in terms of this size. The number of clauses in the pebbling formula PblG is roughly nl\nd. Taking clause sizes into account, the size of the formula, |PblG|, is roughly n(l + d)l\nd. Note that the size of the CNF formula itself grows exponentially with the indegree and gets worse as label size increases. The best case is when G is the grid graph, where |PblG| = \u0398(n). This explains the degradation in performance of zChaff, both original and modified, as we move from grid graphs to random graphs (see section 6.3). Since we construct PblSATG by deleting exactly one randomly chosen clause from PblG (see Section 2.5), the size |Pbl SAT G | of the satisfiable version is also essentially the same. Let us now compute the running time of PebSeq1UIP. Initial computation of heights and predecessor sorting takes time \u0398(nd log d). Assuming nu unit clause labeled nodes and nt target nodes, the remaining node sorting time is \u0398(nu log nu + nt log nt). Since PebSubseq1UIPWrapper is called at most once for each node, the total running time of PebSeq1UIP is \u0398(nd log d + nu log nu + nt log nt + nTwrapper), where Twrapper denotes the running time of PebSubseq1UIP- Wrapper without taking into account recursive calls to itself. When nu and nt are much smaller than n, which we will assume as the typical case, this simplifies to \u0398(nd log d + nTwrapper). If T (v, i) denotes the running time of PebSubseq1UIP(v,i), again without including recursive calls to the wrapper method, then Twrapper = T (v, d). However, T (v, d) = lT (v, d\u22121)+\u0398(l), which gives Twrapper = T (v, d) =\n\u0398(ld+1). Substituting this back, we get that the running time of PebSeq1UIP is \u0398(nld+1), which is about the same as |PblG|.\nFinally, we consider the size of the branching sequence generated. Note that for each node, most of its contribution to the sequence is from the recursive pattern generated near the end of PebSubseq1UIP. Let Q(v, i) denote this contribution. Q(v, i) = (l \u2212 2)(Q(v, i \u2212 1) + \u0398(l)), which gives Q(v, i) = \u0398(ld+2). Hence, the size of the sequence generated is \u0398(nld+2), which again is about the same as |PblG|.\nTheorem 3. Given a pebbling graph G with label size at most l and indegree of non-source nodes at most d, algorithm PebSeq1UIP produces a branching sequence \u03c3 of size at most S in time \u0398(dS), where S = |PblG| \u2248 |Pbl SAT G |. Moreover, the sequence \u03c3 is complete for PblG as well as for Pbl SAT G under any clause learning algorithm using fast backtracking and 1UIP learning scheme (such as zChaff).\nProof. The size and running time bounds follow from the previous discussion in this section. That this sequence is complete can be verified by a simple hand calculation simulating clause learning with fast backtracking and 1UIP learning scheme.\n6.2 Solving GTn Formulas\nWhile pebbling formulas are not so easy to solve by popular SAT solvers, they are inherently not too difficult for clause learning algorithms. In fact, even without any learning, they admit tree-like proofs under a somewhat stronger related proof system, RES(k), for large enough k:\nProposition 6 (Esteban et al., 2002). PblG has a size O(|G|) tree-like RES(k) refutation, where k is the maximum width of a clause labeling a node of G. In particular, when G is a grid graph with n nodes, PblG has an O(n) size tree-like RES(2) refutation.\nHere RES(k) denotes the extension of RES that allows resolving, instead of clauses, disjunctions of conjunctions of up to k literals (Kraj\u0301\u0131c\u030cek, 2001). RES(1) is simply RES. Proposition 6 implies that addition of natural extension variables corresponding to k-conjunctions of variables of PblG leads to O(|G| \u00b7 k) size tree-like resolution proofs of related pebbling formulas PblG(k) (Atserias & Bonet, 2002).\nFor GTn formulas, however, no such short tree-like proofs are known in RES(k) for any k. Reusing derived clauses (equivalently, learning clauses with DPLL) seems to be the key to finding short proofs of GTn. This makes them a good candidate for testing clause learning based SAT solvers. Our experiments indicate that GTn formulas, despite their simplicity, are quite hard for default zChaff. Using a good branching sequence based on the ordering structure underlying these formulas leads to significant performance gains. Recall that PebSeq1UIP was a fairly complex algorithm that generated a perfect branching sequence for randomized pebbling graphs. In contrast, the branching sequence we give below for GTn formulas is generated by a nearly trivial algorithm. It is an incomplete sequence (see Definition 4) but boosts performance in practice.\n6.2.1 Automatic Sequence Generation: GTnSeq1UIP\nSince there is exactly one, well defined, unsatisfiable GT formula for a fixed parameter n, the approximate branching sequence given in Figure 5 below is straightforward. However,\nthe fact that the same branching sequence works well for the satisfiable version of the GTn formulas, obtained by deleting a randomly chosen successor clause, is worth noting."}, {"heading": "6.3 Experimental Results", "text": "We conducted experiments on a Linux machine with a 1600 MHz AMD Athelon processor, 256 KB cache and 1024 MB RAM. Time limit was set to 6 hours and memory limit to 512 MB; the program was set to abort as soon as either of these was exceeded. We took the base code of zChaff (Moskewicz et al., 2001), version 2001.6.15, and modified it to incorporate a branching sequence given as part of the input, along with a CNF formula. When an incomplete branching sequence is specified that gets exhausted before a satisfying assignment is found or the formula is proved to be unsatisfiable, the code reverts to the default variable selection scheme VSIDS of zChaff. For consistency, we analyzed the performance with random restarts turned off. For all other parameters, we used the default values of zChaff. For all formulas, results are reported for DPLL (zChaff with clause learning disabled), for CL (unmodified zChaff), and for CL with a specified branching sequence (modified zChaff).\nTables 1 and 2 show the performance on grid pebbling and randomized pebbling formulas, respectively. In both cases, the branching sequence used was generated by Algorithm 1, PebSeq1UIP, and the size of problems that can be solved increases substantially as we move down the tables. Note that randomized pebbling graphs typically have a more complex structure than grid pebbling graphs. In addition, higher indegree and larger disjunction labels make both the CNF formula size as well as the required branching sequence larger. This explains the difference between the performance of zChaff, both original and modified, on grid and randomized pebbling instances. For all instances considered, the time taken to generate the branching sequence from the input graph was significantly less than that for generating the pebbling formula itself.\nTable 3 shows the performance on the GTn formulas using the branching sequence given in Figure 5. As this sequence is incomplete, the solver had to revert back to zChaff\u2019s VSIDS heuristic to choose variables to branch on after using the given branching sequence as a guide for the first few decisions. Nevertheless, the sizes of problems that could be\nhandled increased significantly. The satisfiable versions proved to be relatively easier, with or without a specified branching sequence."}, {"heading": "7. Conclusion", "text": "This paper has begun the task of formally analyzing the power of clause learning from a proof complexity perspective. Understanding where clause learning stands in relation to well studied proof systems should lead to better insights on why it works well on certain domains and fails on others. For instance, pebbling problems are an example of a domain where our results say that learning is necessary and sufficient, given a good branching order, to obtain sub-exponential solutions using DPLL based methods. On the other hand, the connection with resolution also implies that any problem that contains as a sub-problem a formula that is inherently hard even for RES, such as the pigeonhole principle (Haken, 1985), must be hard for any variant of clause learning. For such domains, theoretical results suggest practical extensions such as symmetry breaking and counting techniques for obtaining efficient solutions.\nThe complexity results in this paper are about proofs of unsatisfiability, and hence apply directly only to the unsatisfiable version of the pebbling formulas. Despite this, the experiments show a clear speed-up on satisfiable versions as well. This, as mentioned in Section 1, can be explained by the idea of Achlioptas et al. (2001): any DPLL based algorithm run on a satisfiable problem instance will take a long time to run precisely when the algorithm encounters an unsatisfiable sub-problem of the original problem on which it must take a long time. This lets one translate hardness results from unsatisfiable formulas to their satisfiable counterparts.\nThis paper inspires but leaves open several interesting questions of proof complexity. We showed that there are formulas on which CL is much more efficient than any proper natural refinement of RES. In general, can every short refutation in any such refinement be converted into a short CL proof? Or are these refinements and CL incomparable? We have shown that with arbitrary restarts, a slight variant of CL is as powerful as RES. However,\njudging when to restart and deciding what branching sequence to use after restarting adds more nondeterminism to the process, making it harder for practical implementations. Can CL with limited restarts also simulate RES efficiently?\nIn practice, a solver must employ good branching heuristics as well as implement a powerful proof system. Our results that pebbling formulas have short CL proofs depends critically upon the solver choosing a branching sequence that solves the formula in a \u201cbottomup\u201d fashion, so that the learned clauses have maximal reuse. Nevertheless, we were able to automatically generate such sequences for grid and randomized pebbling formulas. Although somewhat artificial and capturing the narrow domain of task precedence, pebbling graphs are structurally similar to the layered graphs induced naturally by problems involving unwinding of state space over time, such as STRIPS planning (Kautz & Selman, 1992) and bounded model checking (Biere et al., 1999b). Pebbling problems also provide hard instances for some of the best existing SAT solvers like zChaff. This bolsters our belief that high level structure can be recovered and exploited to make clause learning more efficient.\nThe form in which we extract and use problem structure is a branching sequence. Although capable of capturing more information than a static variable order and avoiding the overhead of dynamic branching schemes, the exactness and detail branching sequences seem to require for pebbling formulas might pose problems when we move to harder domains where a polynomial size sequence is unlikely to exist. We may still be able to obtain substantial (but not exponential) improvements as long as an incomplete or approximate branching sequence made correct decisions most of the time, especially near the top of the underlying DPLL tree. The performance gains reported for GTn formulas indicate that even a very simple and partial branching sequence can make a big difference in practice. Along these lines, variable orders in general have been studied in other scenarios, such as for algorithms based on BDDs (see e.g., Aziz et al., 1994; Harlow & Brglez, 1998). There has been work on using BDD variable orders for DPLL algorithms without learning (Reda et al., 2002). The ideas here can potentially provide new ways of capturing structural information.\nFinally, our approach of exploiting high level problem description to generate auxiliary information for SAT solvers requires the knowledge of this high level description. The standard CNF benchmarks, unfortunately, do not come with such a description. Of course, there is no reason for this information to not be available since CNF formulas for practically all real-world problems are created from a more abstract specification."}, {"heading": "Acknowledgments", "text": "The authors wish to thank the anonymous referees for providing useful comments and for pointing out the existence of short tree-like RES(k) proofs of pebbling formulas. This research was supported by NSF Grant ITR-0219468 and parts of this paper appeared earlier in IJCAI \u201903 and SAT \u201903 conferences (Beame et al., 2003b; Sabharwal et al., 2003)."}], "references": [{"title": "A sharp threshold in proof complexity", "author": ["D. Achlioptas", "P. Beame", "M. Molloy"], "venue": "In Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Achlioptas et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Achlioptas et al\\.", "year": 2001}, {"title": "An exponential separation between regular and general resolution", "author": ["M. Alekhnovich", "J. Johannsen", "T. Pitassi", "A. Urquhart"], "venue": "In Proceedings of the Thirty-Fourth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Alekhnovich et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Alekhnovich et al\\.", "year": 2002}, {"title": "Solving difficult SAT instances in the presence of symmetry", "author": ["F.A. Aloul", "A. Ramani", "I.L. Markov", "K.A. Sakallah"], "venue": "In Proceedings of the 39th Design Automation Conference,", "citeRegEx": "Aloul et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Aloul et al\\.", "year": 2002}, {"title": "Partition-based logical reasoning", "author": ["E. Amir", "S.A. McIlraith"], "venue": "In Proceedings of the 7th International Conference on Principles of Knowledge Representation and Reasoning,", "citeRegEx": "Amir and McIlraith,? \\Q2000\\E", "shortCiteRegEx": "Amir and McIlraith", "year": 2000}, {"title": "On the automatizability of resolution and related propositional proof systems", "author": ["A. Atserias", "M.L. Bonet"], "venue": "In CSL \u201902: 16th Workshop on Computer Science Logic,", "citeRegEx": "Atserias and Bonet,? \\Q2002\\E", "shortCiteRegEx": "Atserias and Bonet", "year": 2002}, {"title": "BDD variable orderings for interacting finite state machines", "author": ["A. Aziz", "S. Tasiran", "R.K. Brayton"], "venue": "In Proceedings of the 31th Design Automation Conference,", "citeRegEx": "Aziz et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Aziz et al\\.", "year": 1994}, {"title": "Using randomization and learning to solve hard real-world instances of satisfiability", "author": ["L. Baptista", "J.P.M. Silva"], "venue": "In 6th Principles and Practice of Constraint Programming,", "citeRegEx": "Baptista and Silva,? \\Q2000\\E", "shortCiteRegEx": "Baptista and Silva", "year": 2000}, {"title": "Using CST look-back techniques to solve realworld SAT instances", "author": ["R.J. Bayardo Jr.", "R.C. Schrag"], "venue": "In Proceedings,", "citeRegEx": "Jr. and Schrag,? \\Q1997\\E", "shortCiteRegEx": "Jr. and Schrag", "year": 1997}, {"title": "Memoization and DPLL: Formula caching proof systems", "author": ["P. Beame", "R. Impagliazzo", "T. Pitassi", "N. Segerlind"], "venue": "In Proceedings 18th Annual IEEE Conference on Computational Complexity,", "citeRegEx": "Beame et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Beame et al\\.", "year": 2003}, {"title": "Understanding the power of clause learning", "author": ["P. Beame", "H. Kautz", "A. Sabharwal"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Beame et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Beame et al\\.", "year": 2003}, {"title": "Near-optimal separation of treelike and general resolution", "author": ["E. Ben-Sasson", "R. Impagliazzo", "A. Wigderson"], "venue": "Tech. rep. TR00-005,", "citeRegEx": "Ben.Sasson et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ben.Sasson et al\\.", "year": 2000}, {"title": "Symbolic model checking using SAT procedures instead of BDDs", "author": ["A. Biere", "A. Cimatti", "E.M. Clarke", "M. Fujita", "Y. Zhu"], "venue": "In Proceedings of the 36th Design Automation Conference,", "citeRegEx": "Biere et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Biere et al\\.", "year": 1999}, {"title": "Symbolic model checking without BDDs", "author": ["A. Biere", "A. Cimatti", "E.M. Clarke", "Y. Zhu"], "venue": "In 5th International Conference on Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Biere et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Biere et al\\.", "year": 1999}, {"title": "On the relative complexity of resolution refinements and cutting planes proof systems", "author": ["M.L. Bonet", "J.L. Esteban", "N. Galesi", "J. Johansen"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Bonet et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bonet et al\\.", "year": 2000}, {"title": "Optimality of size-width tradeoffs for resolution", "author": ["M.L. Bonet", "N. Galesi"], "venue": "Computational Complexity,", "citeRegEx": "Bonet and Galesi,? \\Q2001\\E", "shortCiteRegEx": "Bonet and Galesi", "year": 2001}, {"title": "A simplifier for propositional formulas with many binary clauses", "author": ["R.I. Brafman"], "venue": "In Proceedings of the 17th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Brafman,? \\Q2001\\E", "shortCiteRegEx": "Brafman", "year": 2001}, {"title": "The complexity of resolution refinements", "author": ["J. Buresh-Oppenheim", "T. Pitassi"], "venue": "In 18th Annual IEEE Symposium on Logic in Computer Science,", "citeRegEx": "Buresh.Oppenheim and Pitassi,? \\Q2003\\E", "shortCiteRegEx": "Buresh.Oppenheim and Pitassi", "year": 2003}, {"title": "The relative efficiency of propositional proof systems", "author": ["S.A. Cook", "R.A. Reckhow"], "venue": "Journal of Symbolic Logic,", "citeRegEx": "Cook and Reckhow,? \\Q1977\\E", "shortCiteRegEx": "Cook and Reckhow", "year": 1977}, {"title": "A machine program for theorem proving", "author": ["M. Davis", "G. Logemann", "D. Loveland"], "venue": "Communications of the ACM,", "citeRegEx": "Davis et al\\.,? \\Q1962\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1962}, {"title": "A computing procedure for quantification theory", "author": ["M. Davis", "H. Putnam"], "venue": "Communications of the ACM,", "citeRegEx": "Davis and Putnam,? \\Q1960\\E", "shortCiteRegEx": "Davis and Putnam", "year": 1960}, {"title": "Diagnostic reasoning based on structure and behavior", "author": ["R. Davis"], "venue": "Artificial Intelligence,", "citeRegEx": "Davis,? \\Q1984\\E", "shortCiteRegEx": "Davis", "year": 1984}, {"title": "Diagnosing multiple faults", "author": ["J. de Kleer", "B.C. Williams"], "venue": "Artificial Intelligence,", "citeRegEx": "Kleer and Williams,? \\Q1987\\E", "shortCiteRegEx": "Kleer and Williams", "year": 1987}, {"title": "On the complexity of resolution with bounded conjunctions", "author": ["J.L. Esteban", "N. Galesi", "J. Messner"], "venue": "InAutomata, Languages, and Programming: 29th International Colloquium,", "citeRegEx": "Esteban et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Esteban et al\\.", "year": 2002}, {"title": "The use of design descriptions in automated diagnosis", "author": ["R. Genesereth"], "venue": "Artificial Intelligence,", "citeRegEx": "Genesereth,? \\Q1984\\E", "shortCiteRegEx": "Genesereth", "year": 1984}, {"title": "Dependent and independent variables in propositional satisfiability", "author": ["E. Giunchiglia", "M. Maratea", "A. Tacchella"], "venue": "In Proceedings of the 8th European Conference on Logics in Artificial Intelligence (JELIA),", "citeRegEx": "Giunchiglia et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Giunchiglia et al\\.", "year": 2002}, {"title": "Boosting combinatorial search through randomization", "author": ["C.P. Gomes", "B. Selman", "H. Kautz"], "venue": "In Proceedings,", "citeRegEx": "Gomes et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 1998}, {"title": "Randomization in backtrack search: Exploiting heavy-tailed profiles for solving hard scheduling problems", "author": ["C.P. Gomes", "B. Selman", "K. McAloon", "C. Tretkoff"], "venue": "In Proceedings of the 4th International Conference on Artificial Intelligence Planning Systems,", "citeRegEx": "Gomes et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 1998}, {"title": "The intractability of resolution", "author": ["A. Haken"], "venue": "Theoretical Computer Science,", "citeRegEx": "Haken,? \\Q1985\\E", "shortCiteRegEx": "Haken", "year": 1985}, {"title": "Design of experiments in BDD variable ordering: Lessons learned", "author": ["J.E. Harlow", "F. Brglez"], "venue": "In Proceedings of the International Conference on Computer Aided Design,", "citeRegEx": "Harlow and Brglez,? \\Q1998\\E", "shortCiteRegEx": "Harlow and Brglez", "year": 1998}, {"title": "Cliques, Coloring and Satisfiability: Second DIMACS Implementation Challenge, Vol. 26 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science. AMS", "author": ["D.S. Johnson", "M.A. Trick"], "venue": null, "citeRegEx": "Johnson and Trick,? \\Q1996\\E", "shortCiteRegEx": "Johnson and Trick", "year": 1996}, {"title": "Planning as satisfiability", "author": ["H.A. Kautz", "B. Selman"], "venue": "In Proceedings of the 10th European Conference on Artificial Intelligence,", "citeRegEx": "Kautz and Selman,? \\Q1992\\E", "shortCiteRegEx": "Kautz and Selman", "year": 1992}, {"title": "Pushing the envelope: Planning, propositional logic, and stochastic search", "author": ["H.A. Kautz", "B. Selman"], "venue": "In Proceedings,", "citeRegEx": "Kautz and Selman,? \\Q1996\\E", "shortCiteRegEx": "Kautz and Selman", "year": 1996}, {"title": "Explorations of sequential ATPG using boolean satisfiability", "author": ["H. Konuk", "T. Larrabee"], "venue": "In 11th VLSI Test Symposium,", "citeRegEx": "Konuk and Larrabee,? \\Q1993\\E", "shortCiteRegEx": "Konuk and Larrabee", "year": 1993}, {"title": "On the weak pigeonhole principle", "author": ["J. Kraj\u0301\u0131\u010dek"], "venue": "Fundamenta Mathematicae,", "citeRegEx": "Kraj\u0301\u0131\u010dek,? \\Q2001\\E", "shortCiteRegEx": "Kraj\u0301\u0131\u010dek", "year": 2001}, {"title": "Short proofs for tricky formulas", "author": ["B. Krishnamurthy"], "venue": "Acta Informatica,", "citeRegEx": "Krishnamurthy,? \\Q1985\\E", "shortCiteRegEx": "Krishnamurthy", "year": 1985}, {"title": "Heuristics based on unit propagation for satisfiability problems", "author": ["C.M. Li"], "venue": "In Proceedings of the 15th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Li,? \\Q1997\\E", "shortCiteRegEx": "Li", "year": 1997}, {"title": "GRASP \u2013 a new search algorithm for satisfiability", "author": ["J.P. Marques-Silva", "K.A. Sakallah"], "venue": "In Proceedings of the International Conference on Computer Aided Design,", "citeRegEx": "Marques.Silva and Sakallah,? \\Q1996\\E", "shortCiteRegEx": "Marques.Silva and Sakallah", "year": 1996}, {"title": "An overview of backtrack search satisfiability algorithms", "author": ["J. Marques-Silva"], "venue": "In 5th International Symposium on Artificial Intelligence and Mathematics,", "citeRegEx": "Marques.Silva,? \\Q1998\\E", "shortCiteRegEx": "Marques.Silva", "year": 1998}, {"title": "Random k-satisfiability problem: From an analytic solution to an efficient algorithm", "author": ["M. M\u00e9zard", "R. Zecchina"], "venue": "Physical Review E,", "citeRegEx": "M\u00e9zard and Zecchina,? \\Q2002\\E", "shortCiteRegEx": "M\u00e9zard and Zecchina", "year": 2002}, {"title": "Chaff: Engineering an efficient SAT solver", "author": ["M.W. Moskewicz", "C.F. Madigan", "Y. Zhao", "L. Zhang", "S. Malik"], "venue": "In Proceedings of the 38th Design Automation Conference,", "citeRegEx": "Moskewicz et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Moskewicz et al\\.", "year": 2001}, {"title": "Recovering and exploiting structural knowledge from cnf formulas", "author": ["R. Ostrowski", "E. Gr\u00e9goire", "B. Mazure", "L. Sais"], "venue": "In 8th Principles and Practice of Constraint Programming,", "citeRegEx": "Ostrowski et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ostrowski et al\\.", "year": 2002}, {"title": "On the relation between SAT and BDDs for equivalence checking", "author": ["S. Reda", "R. Drechsler", "A. Orailoglu"], "venue": "In Proceedings of the International Symposium on Quality Electronic Design,", "citeRegEx": "Reda et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Reda et al\\.", "year": 2002}, {"title": "Using problem structure for efficient clause learning", "author": ["A. Sabharwal", "P. Beame", "H. Kautz"], "venue": "In Proceedings of the 6th International Conference on Theory and Applications of Satisfiability Testing,", "citeRegEx": "Sabharwal et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sabharwal et al\\.", "year": 2003}, {"title": "Tuning SAT checkers for bounded model checking", "author": ["O. Shtrichman"], "venue": "In Proceedings of the 12th International Conference on Computer Aided Verification,", "citeRegEx": "Shtrichman,? \\Q2000\\E", "shortCiteRegEx": "Shtrichman", "year": 2000}, {"title": "Forward reasoning and dependency-directed backtracking in a system for computer-aided circuit analysis", "author": ["R. Stallman", "G.J. Sussman"], "venue": "Artificial Intelligence,", "citeRegEx": "Stallman and Sussman,? \\Q1977\\E", "shortCiteRegEx": "Stallman and Sussman", "year": 1977}, {"title": "Effective use of boolean satisfiability procedures in the formal verification of superscalar and vliw microprocessors", "author": ["M. Velev", "R. Bryant"], "venue": "In Proceedings of the 38th Design Automation Conference,", "citeRegEx": "Velev and Bryant,? \\Q2001\\E", "shortCiteRegEx": "Velev and Bryant", "year": 2001}, {"title": "Solving open quasigroup problems by propositional reasoning", "author": ["H. Zhang", "J. Hsiang"], "venue": "In Proceedings of the International Computer Symp.,", "citeRegEx": "Zhang and Hsiang,? \\Q1994\\E", "shortCiteRegEx": "Zhang and Hsiang", "year": 1994}, {"title": "SATO: An efficient propositional prover", "author": ["H. Zhang"], "venue": "In Proceedings of the 14th International Conference on Automated Deduction,", "citeRegEx": "Zhang,? \\Q1997\\E", "shortCiteRegEx": "Zhang", "year": 1997}, {"title": "Efficient conflict driven learning in a boolean satisfiability solver", "author": ["L. Zhang", "C.F. Madigan", "M.H. Moskewicz", "S. Malik"], "venue": "In Proceedings of the International Conference on Computer Aided Design,", "citeRegEx": "Zhang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 18, "context": "The most surprising aspect of such relatively recent practical progress is that the best complete satisfiability testing algorithms remain variants of the Davis-Putnam-LogemannLoveland or DPLL procedure (Davis & Putnam, 1960; Davis et al., 1962) for backtrack search in the space of partial truth assignments.", "startOffset": 203, "endOffset": 245}, {"referenceID": 23, "context": "Clause learning grew out of work in AI on explanation-based learning (EBL), which sought to improve the performance of backtrack search algorithms by generating explanations for failure (backtrack) points, and then adding the explanations as new constraints on the original problem (de Kleer & Williams, 1987; Stallman & Sussman, 1977; Genesereth, 1984; Davis, 1984).", "startOffset": 282, "endOffset": 366}, {"referenceID": 20, "context": "Clause learning grew out of work in AI on explanation-based learning (EBL), which sought to improve the performance of backtrack search algorithms by generating explanations for failure (backtrack) points, and then adding the explanations as new constraints on the original problem (de Kleer & Williams, 1987; Stallman & Sussman, 1977; Genesereth, 1984; Davis, 1984).", "startOffset": 282, "endOffset": 366}, {"referenceID": 47, "context": "A series of researchers (Bayardo Jr. & Schrag, 1997; Marques-Silva & Sakallah, 1996; Zhang, 1997; Moskewicz et al., 2001; Zhang et al., 2001) showed that clause learning can be efficiently implemented and used to solve hard problems that cannot be approached by any other technique.", "startOffset": 24, "endOffset": 141}, {"referenceID": 39, "context": "A series of researchers (Bayardo Jr. & Schrag, 1997; Marques-Silva & Sakallah, 1996; Zhang, 1997; Moskewicz et al., 2001; Zhang et al., 2001) showed that clause learning can be efficiently implemented and used to solve hard problems that cannot be approached by any other technique.", "startOffset": 24, "endOffset": 141}, {"referenceID": 48, "context": "A series of researchers (Bayardo Jr. & Schrag, 1997; Marques-Silva & Sakallah, 1996; Zhang, 1997; Moskewicz et al., 2001; Zhang et al., 2001) showed that clause learning can be efficiently implemented and used to solve hard problems that cannot be approached by any other technique.", "startOffset": 24, "endOffset": 141}, {"referenceID": 13, "context": "A basic result in proof complexity is that general resolution, denoted RES, is exponentially stronger than the DPLL procedure (Bonet et al., 2000; Ben-Sasson et al., 2000).", "startOffset": 126, "endOffset": 171}, {"referenceID": 10, "context": "A basic result in proof complexity is that general resolution, denoted RES, is exponentially stronger than the DPLL procedure (Bonet et al., 2000; Ben-Sasson et al., 2000).", "startOffset": 126, "endOffset": 171}, {"referenceID": 0, "context": "Nevertheless, Achlioptas et al. (2001) have shown how negative proof complexity results for unsatisfiable formulas can be used to derive time lower bounds for specific inference algorithms running on satisfiable formulas as well.", "startOffset": 14, "endOffset": 39}, {"referenceID": 37, "context": "It has been previously observed that clause learning can be viewed as adding resolvents to a tree-like proof (Marques-Silva, 1998).", "startOffset": 109, "endOffset": 130}, {"referenceID": 24, "context": "While there has been work on extracting structure after conversion into a CNF formula by exploiting variable dependency (Giunchiglia et al., 2002; Ostrowski et al., 2002), constraint redundancy (Ostrowski et al.", "startOffset": 120, "endOffset": 170}, {"referenceID": 40, "context": "While there has been work on extracting structure after conversion into a CNF formula by exploiting variable dependency (Giunchiglia et al., 2002; Ostrowski et al., 2002), constraint redundancy (Ostrowski et al.", "startOffset": 120, "endOffset": 170}, {"referenceID": 40, "context": ", 2002), constraint redundancy (Ostrowski et al., 2002), symmetry (Aloul et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 2, "context": ", 2002), symmetry (Aloul et al., 2002), binary clauses (Brafman, 2001), and partitioning (Amir & McIlraith, 2000), using the original higher level description itself to generate structural information is likely to be more effective.", "startOffset": 18, "endOffset": 38}, {"referenceID": 15, "context": ", 2002), binary clauses (Brafman, 2001), and partitioning (Amir & McIlraith, 2000), using the original higher level description itself to generate structural information is likely to be more effective.", "startOffset": 24, "endOffset": 39}, {"referenceID": 43, "context": "The latter approach, despite its intuitive appeal, remains largely unexplored, except for suggested use in bounded model checking (Shtrichman, 2000) and the separate consideration of cause variables and effect variables in planning (Kautz & Selman, 1996).", "startOffset": 130, "endOffset": 148}, {"referenceID": 48, "context": "Our sequence generators work for the 1UIP learning scheme (Zhang et al., 2001), which is one of the best known.", "startOffset": 58, "endOffset": 78}, {"referenceID": 39, "context": "Our empirical results are based on our extension of the popular SAT solver zChaff (Moskewicz et al., 2001).", "startOffset": 82, "endOffset": 106}, {"referenceID": 10, "context": "These formulas, more commonly occurring in theoretical proof complexity literature (Ben-Sasson et al., 2000; Beame et al., 2003a), can be thought of as representing precedence graphs in dependent task systems and scheduling scenarios.", "startOffset": 83, "endOffset": 129}, {"referenceID": 34, "context": "We also report significant gains obtained for the class of GTn formulas which, again, have appeared frequently in proof complexity results (Krishnamurthy, 1985; Bonet & Galesi, 2001; Alekhnovich et al., 2002).", "startOffset": 139, "endOffset": 208}, {"referenceID": 1, "context": "We also report significant gains obtained for the class of GTn formulas which, again, have appeared frequently in proof complexity results (Krishnamurthy, 1985; Bonet & Galesi, 2001; Alekhnovich et al., 2002).", "startOffset": 139, "endOffset": 208}, {"referenceID": 18, "context": "The basic idea of the Davis-Putnam-Logemann-Loveland (DPLL) procedure (Davis & Putnam, 1960; Davis et al., 1962) for testing satisfiability of CNF formulas is to branch on variables, setting them to true or false, until either an initial clause is violated (i.", "startOffset": 70, "endOffset": 112}, {"referenceID": 13, "context": "For instance, regular, linear, positive, negative, semantic, and Davis-Putnam resolution are all known to be exponentially stronger than tree-like resolution (Bonet et al., 2000; Bonet & Galesi, 2001; Buresh-Oppenheim & Pitassi, 2003) whereas tree-like, regular, and Davis-Putnam resolution are known to be exponentially weaker than RES (Bonet et al.", "startOffset": 158, "endOffset": 234}, {"referenceID": 13, "context": ", 2000; Bonet & Galesi, 2001; Buresh-Oppenheim & Pitassi, 2003) whereas tree-like, regular, and Davis-Putnam resolution are known to be exponentially weaker than RES (Bonet et al., 2000; Alekhnovich et al., 2002).", "startOffset": 166, "endOffset": 212}, {"referenceID": 1, "context": ", 2000; Bonet & Galesi, 2001; Buresh-Oppenheim & Pitassi, 2003) whereas tree-like, regular, and Davis-Putnam resolution are known to be exponentially weaker than RES (Bonet et al., 2000; Alekhnovich et al., 2002).", "startOffset": 166, "endOffset": 212}, {"referenceID": 13, "context": "Proposition 2 (Bonet et al., 2000; Alekhnovich et al., 2002).", "startOffset": 14, "endOffset": 60}, {"referenceID": 1, "context": "Proposition 2 (Bonet et al., 2000; Alekhnovich et al., 2002).", "startOffset": 14, "endOffset": 60}, {"referenceID": 48, "context": "Variations of such conflict-driven learning include different ways of choosing the clause to learn (different learning schemes) and possibly allowing multiple clauses to be learned from a single conflict (Zhang et al., 2001).", "startOffset": 204, "endOffset": 224}, {"referenceID": 47, "context": "In the last decade, many algorithms based on this idea have been proposed and demonstrated to be empirically successful on large problems that could not be handled using other methodologies (Bayardo Jr. & Schrag, 1997; Marques-Silva & Sakallah, 1996; Zhang, 1997; Moskewicz et al., 2001).", "startOffset": 190, "endOffset": 287}, {"referenceID": 39, "context": "In the last decade, many algorithms based on this idea have been proposed and demonstrated to be empirically successful on large problems that could not be handled using other methodologies (Bayardo Jr. & Schrag, 1997; Marques-Silva & Sakallah, 1996; Zhang, 1997; Moskewicz et al., 2001).", "startOffset": 190, "endOffset": 287}, {"referenceID": 10, "context": "Pebbling formulas are unsatisfiable CNF formulas whose variations have been used repeatedly in proof complexity to obtain theoretical separation results between different proof systems (Ben-Sasson et al., 2000; Beame et al., 2003a).", "startOffset": 185, "endOffset": 231}, {"referenceID": 10, "context": "The version we will use in this paper is known to be easy for regular resolution but hard for tree-like resolution, and hence for DPLL without learning (Ben-Sasson et al., 2000).", "startOffset": 152, "endOffset": 177}, {"referenceID": 33, "context": "They were first considered by Krishnamurthy (1985) and later used by Bonet and Galesi (2001) to show the", "startOffset": 30, "endOffset": 51}, {"referenceID": 14, "context": "They were first considered by Krishnamurthy (1985) and later used by Bonet and Galesi (2001) to show the", "startOffset": 69, "endOffset": 93}, {"referenceID": 1, "context": "Recently, Alekhnovich et al. (2002) used a variation, called GT \u2032 n, to show an exponential separation between RES and regular resolution.", "startOffset": 10, "endOffset": 36}, {"referenceID": 48, "context": "One can also create learning schemes based on cuts not involving conflict literals at all (Zhang et al., 2001), but the effectiveness of such schemes is not clear.", "startOffset": 90, "endOffset": 110}, {"referenceID": 48, "context": "The decision scheme (Zhang et al., 2001), for example, uses the cut whose reason side comprises all decision variables.", "startOffset": 20, "endOffset": 40}, {"referenceID": 39, "context": "Whereas rel-sat uses the decision variable as the obvious UIP, GRASP (Marques-Silva & Sakallah, 1996) and zChaff (Moskewicz et al., 2001) use FirstUIP, the one that is \u201cclosest\u201d to the conflict variable.", "startOffset": 113, "endOffset": 137}, {"referenceID": 37, "context": "This nice flipping property holds in general for all unique implication points (UIPs) (Marques-Silva & Sakallah, 1996). A UIP of an implication graph is a node at the current decision level d such that any path from the decision variable at level d to the conflict variable as well as its negation must go through it. Intuitively, it is a single reason at level d that causes the conflict. Whereas rel-sat uses the decision variable as the obvious UIP, GRASP (Marques-Silva & Sakallah, 1996) and zChaff (Moskewicz et al., 2001) use FirstUIP, the one that is \u201cclosest\u201d to the conflict variable. GRASP also learns multiple clauses when faced with a conflict. This makes it typically require fewer branching steps but possibly slower because of the time lost in learning and unit propagation. The concept of UIP can be generalized to decision levels other than the current one. The 1UIP scheme corresponds to learning the FirstUIP clause of the current decision level, the 2UIP scheme to learning the FirstUIP clauses of both the current level and the one before, and so on. Zhang et al. (2001) present a comparison of all these and other learning schemes and conclude that 1UIP is quite robust and outperforms all other schemes they consider on most of the benchmarks.", "startOffset": 87, "endOffset": 1092}, {"referenceID": 6, "context": "Note that Baptista and Silva (2000) showed that CL together with restarts is complete.", "startOffset": 10, "endOffset": 36}, {"referenceID": 22, "context": "Proposition 6 (Esteban et al., 2002).", "startOffset": 14, "endOffset": 36}, {"referenceID": 33, "context": "Here RES(k) denotes the extension of RES that allows resolving, instead of clauses, disjunctions of conjunctions of up to k literals (Kraj\u0301\u0131\u010dek, 2001).", "startOffset": 133, "endOffset": 150}, {"referenceID": 39, "context": "We took the base code of zChaff (Moskewicz et al., 2001), version 2001.", "startOffset": 32, "endOffset": 56}, {"referenceID": 27, "context": "On the other hand, the connection with resolution also implies that any problem that contains as a sub-problem a formula that is inherently hard even for RES, such as the pigeonhole principle (Haken, 1985), must be hard for any variant of clause learning.", "startOffset": 192, "endOffset": 205}, {"referenceID": 0, "context": "This, as mentioned in Section 1, can be explained by the idea of Achlioptas et al. (2001): any DPLL based algorithm run on a satisfiable problem instance will take a long time to run precisely when the algorithm encounters an unsatisfiable sub-problem of the original problem on which it must take a long time.", "startOffset": 65, "endOffset": 90}, {"referenceID": 41, "context": "There has been work on using BDD variable orders for DPLL algorithms without learning (Reda et al., 2002).", "startOffset": 86, "endOffset": 105}, {"referenceID": 42, "context": "This research was supported by NSF Grant ITR-0219468 and parts of this paper appeared earlier in IJCAI \u201903 and SAT \u201903 conferences (Beame et al., 2003b; Sabharwal et al., 2003).", "startOffset": 131, "endOffset": 176}], "year": 2011, "abstractText": "Efficient implementations of DPLL with the addition of clause learning are the fastest complete Boolean satisfiability solvers and can handle many significant real-world problems, such as verification, planning and design. Despite its importance, little is known of the ultimate strengths and limitations of the technique. This paper presents the first precise characterization of clause learning as a proof system (CL), and begins the task of understanding its power by relating it to the well-studied resolution proof system. In particular, we show that with a new learning scheme, CL can provide exponentially shorter proofs than many proper refinements of general resolution (RES) satisfying a natural property. These include regular and Davis-Putnam resolution, which are already known to be much stronger than ordinary DPLL. We also show that a slight variant of CL with unlimited restarts is as powerful as RES itself. Translating these analytical results to practice, however, presents a challenge because of the nondeterministic nature of clause learning algorithms. We propose a novel way of exploiting the underlying problem structure, in the form of a high level problem description such as a graph or PDDL specification, to guide clause learning algorithms toward faster solutions. We show that this leads to exponential speed-ups on grid and randomized pebbling problems, as well as substantial improvements on certain ordering formulas.", "creator": "dvips(k) 5.92b Copyright 2002 Radical Eye Software"}}}