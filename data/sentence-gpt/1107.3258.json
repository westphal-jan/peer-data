{"id": "1107.3258", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jul-2011", "title": "On Learning Discrete Graphical Models using Greedy Methods", "abstract": "In this paper, we address the problem of learning the structure of a pairwise graphical model from samples in a high-dimensional setting. Our first main result studies the sparsistency, or consistency in sparsity pattern recovery, properties of a forward-backward greedy algorithm as applied to general statistical models. As a special case, we then apply this algorithm to learn the structure of a discrete graphical model via neighborhood estimation. As a corollary of our general result, we derive sufficient conditions on the number of samples n, the maximum node-degree d and the problem size p, as well as other conditions on the model parameters, so that the algorithm recovers all the edges with high probability. Our result guarantees graph selection for samples scaling as n = Omega(d^2 log(p)), in contrast to existing convex-optimization based algorithms that require a sample complexity of \\Omega(d^3 log(p)). Further, the greedy algorithm only requires a restricted strong convexity condition which is typically milder than irrepresentability assumptions. We corroborate these results using numerical simulations at the end.\n\n\n\nFigure 3\nWe find the best results to date of the model selection and are very promising in providing us with an effective solution. Here, the results from the high-dimensional space analysis are shown in the chart below. The main advantage of this approach, however, is that we have no constraints on the complexity of the data in which the model is generated (the two data points can be compared using a very robust technique). This means that our model selection can only be obtained using the data from the sample dataset which can be used to identify and characterize the structure of the model. This means that in the model selection we only have the option to use the data obtained from the sample dataset which can be used to identify and characterize the structure of the model. This means that the models can only be obtained using the data obtained from the sample dataset which can be used to identify and characterize the structure of the model.\nThe main advantage of this approach, however, is that we have no constraints on the complexity of the data in which the model is generated (the two data points can be compared using a very robust technique). This means that our model selection can only be obtained using the data from the sample dataset which can be used to identify and characterize the structure of the model. This means that in the model selection we only have the option to use the data obtained from the sample dataset which can be used to identify and", "histories": [["v1", "Sat, 16 Jul 2011 22:04:13 GMT  (39kb)", "http://arxiv.org/abs/1107.3258v1", null]], "reviews": [], "SUBJECTS": "cs.LG math.ST stat.ML stat.TH", "authors": ["ali jalali", "christopher c johnson", "pradeep ravikumar"], "accepted": true, "id": "1107.3258"}, "pdf": {"name": "1107.3258.pdf", "metadata": {"source": "CRF", "title": "On Learning Discrete Graphical Models Using Greedy Methods", "authors": ["Ali Jalali"], "emails": ["alij@mail.utexas.edu", "cjohnson@cs.utexas.edu", "pradeepr@cs.utexas.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 7.\n32 58"}, {"heading": "1 Introduction", "text": "Undirected graphical models, also known as Markov random fields, are used in a variety of domains, including statistical physics, natural language processing and image analysis among others. In this paper we are concerned with the task of estimating the graph structure G of a Markov random field (MRF) over a discrete random vector X = (X1, X2, . . . , Xp), given n independent and identically distributed samples\n{x(1), x(2), . . . , x(n)}. This underlying graph structure encodes conditional independence assumptions among subsets of the variables, and thus plays an important role in a broad range of applications of MRFs. Existing approaches: Neighborhood Estimation, Greedy Local Search. Methods for estimating such graph structure include those based on constraint and hypothesis testing [22], and those that estimate restricted classes of graph structures such as trees [8], polytrees [11], and hypertrees [23]. A recent class of successful approaches for graphical model structure learning are based on estimating the local neighborhood of each node. One subclass of these for the special case of bounded degree graphs involve the use of exhaustive search so that their computational complexity grows at least as quickly as O(pd), where d is the maximum neighborhood size in the graphical model [1, 4, 9]. Another subclass use convex programs to learn the neighborhood structure: for instance [20, 17, 16] estimate the neighborhood set for each vertex r \u2208 V by optimizing its \u21131-regularized conditional likelihood; [15, 10] use \u21131/\u21132-regularized conditional likelihood. Even these methods, however need to solve regularized convex programs with typically polynomial computational cost of O(p4) or O(p6), are still expensive for large problems. Another popular class of approaches are based on using a score metric and searching for the best scoring structure from a candidate set of graph structures. Exact search is typically NP-hard [7]; indeed for general discrete MRFs, not only is the search space intractably large, but calculation of typical score metrics itself is computationally intractable since they involve computing the partition function associated with the Markov random field [26]. Such methods thus have to use approximations and search heuristics for tractable computation. Question: Can one use local procedures that are as inexpensive as the heuristic greedy approaches, and yet come with the strong statistical guarantees of the regularized convex program based approaches? High-dimensional Estimation; Greedy Methods. There has been an increasing focus in recent years on high-dimensional statistical models where the number of parameters p is comparable to or even larger than the number of observations n. It is now well understood that consistent estimation is possible even under such high-dimensional scaling if some low-dimensional structure is imposed on the model space. Of relevance to graphical model structure learning is the structure of sparsity, where a sparse set of non-zero parameters entail a sparse set of edges. A surge of recent work [5, 12] has shown that \u21131-regularization for learning such sparse models can lead to practical algorithms with strong theoretical guarantees. A line of recent work (cf. paragraph above) has thus leveraged this sparsity inducing nature of \u21131-regularization, to propose and analyze convex programs based on regularized log-likelihood functions. A related line of recent work on learning sparse models has focused on \u201cstagewise\u201d greedy algorithms. These perform simple forward steps (adding parameters greedily), and possibly also backward steps (removing parameters greedily), and yet provide strong statistical guarantees for the estimate after a finite number of greedy steps. The forward greedy variant which performs just the forward step has appeared in various guises in multiple communities: in machine learning as boosting [13], in function approximation [24], and in signal processing as basis pursuit [6]. In the context of statistical model estimation, Zhang [28] analyzed the forward greedy algorithm for the case of sparse linear regression; and showed that the forward greedy algorithm is sparsistent (consistent for\nmodel selection recovery) under the same \u201cirrepresentable\u201d condition as that required for \u201csparsistency\u201d of the Lasso. Zhang [27] analyzes a more general greedy algorithm for sparse linear regression that performs forward and backward steps, and showed that it is sparsistent under a weaker restricted eigenvalue condition. Here we ask the question: Can we provide an analysis of a general forward backward algorithm for parameter estimation in general statistical models? Specifically, we need to extend the sparsistency analysis of [28] to general non-linear models, which requires a subtler analysis due to the circular requirement of requiring to control the third order terms in the Taylor series expansion of the log-likelihood, that in turn requires the estimate to be well-behaved. Such extensions in the case of \u21131-regularization occur for instance in [20, 25, 3]. Our Contributions. In this paper, we address both questions above. In the first part, we analyze the forward backward greedy algorithm [28] for general statistical models. We note that even though we consider the general statistical model case, our analysis is much simpler and accessible than [28], and would be of use even to a reader interested in just the linear model case of Zhang [28]. In the second part, we use this to show that when combined with neighborhood estimation, the forward backward variant applied to local conditional log-likelihoods provides a simple computationally tractable method that adds and deletes edges, but comes with strong sparsistency guarantees. We reiterate that the our first result on the sparsistency of the forward backward greedy algorithm for general objectives is of independent interest even outside the context of graphical models. As we show, the greedy method is better than the \u21131-regularized counterpart in [20] theoretically, as well as experimentally. The sufficient condition on the parameters imposed by the greedy algorithm is a restricted strong convexity condition [19], which is weaker than the irrepresentable condition required by [20]. Further, the number of samples required for sparsistent graph recovery scales as O(d2 log p), where d is the maximum node degree, in contrast to O(d3 log p) for the \u21131-regularized counterpart. We corroborate this in our simulations, where we find that the greedy algorithm requires fewer observations than [20] for sparsistent graph recovery."}, {"heading": "2 Review, Setup and Notation", "text": ""}, {"heading": "2.1 Markov Random Fields", "text": "Let X = (X1, . . . , Xp) be a random vector, each variable Xi taking values in a discrete set X of cardinality m. Let G = (V,E) denote a graph with p nodes, corresponding to the p variables {X1, . . . , Xp}. A pairwise Markov random field over X = (X1, . . . , Xp) is then specified by nodewise and pairwise functions \u03b8r : X 7\u2192 R for all r \u2208 V , and \u03b8rt : X \u00d7 X 7\u2192 R for all (r, t) \u2208 E:\nP(x) \u221d exp {\u2211\nr\u2208V\n\u03b8r(xr) + \u2211\n(r,t)\u2208E\n\u03b8rt(xr, xt) } . (1)\nIn this paper, we largely focus on the case where the variables are binary with X = {\u22121,+1}, where we can rewrite (1) to the Ising model form [14] for some set of parameters {\u03b8r} and {\u03b8rt} as\nP(x) \u221d exp {\u2211\nr\u2208V\n\u03b8rxr + \u2211\n(r,t)\u2208E\n\u03b8rtxrxt } . (2)"}, {"heading": "2.2 Graphical Model Selection", "text": "Let D := {x(1), . . . , x(n)} denote the set of n samples, where each p-dimensional vector x(i) \u2208 {1, . . . ,m}p is drawn i.i.d. from a distribution P\u03b8\u2217 of the form (1), for parameters \u03b8\u2217 and graph G = (V,E\u2217) over the p variables. Note that the true edge set E\u2217 can also be expressed as a function of the parameters as\nE\u2217 = {(r, t) \u2208 V \u00d7 V : \u03b8\u2217st 6= 0}. (3)\nThe graphical model selection task consists of inferring this edge set E\u2217 from the samples D. The goal is to construct an estimator E\u0302n for which P[E\u0302n = E\u2217] \u2192 1 as n \u2192 \u221e. Denote by N \u2217(r) the set of neighbors of a vertex r \u2208 V , so that N \u2217(r) = {t : (r, t) \u2208 E\u2217}. Then the graphical model selection problem is equivalent to that of estimating the neighborhoods N\u0302n(r) \u2282 V , so that P[N\u0302n(r) = N \u2217(r); \u2200r \u2208 V ] \u2192 1 as n \u2192 \u221e.\nFor any pair of random variables Xr and Xt, the parameter \u03b8rt fully characterizes whether there is an edge between them, and can be estimated via its conditional likelihood. In particular, defining \u0398r := (\u03b8r1, . . . , \u03b8rp), our goal is to use the conditional likelihood of Xr conditioned on XV \\r to estimate \u0398r and hence its neighborhood N (r). This conditional distribution of Xr conditioned on XV \\r generated by (2) is given by the logistic model\nP ( Xr = xr \u2223\u2223\u2223XV \\r = xV \\r ) =\nexp(\u03b8rxr + \u2211\nt\u2208V \\r \u03b8rtxrxt)\n1 + exp(\u03b8r + \u2211 r\u2208V \\r \u03b8rtxr) .\nGiven the n samples D, the corresponding conditional log-likelihood is given by\nL(\u0398r;D) = 1 n\nn\u2211\ni=1\n  log  1+ exp  \u03b8rx(i)+ \u2211\nt\u2208V \\r\n\u03b8rtx (i) r x (i) t\n\n\n \u2212\u03b8rx(i)r \u2212 \u2211\nt\u2208V \\r\n\u03b8rtx (i) r x (i) t\n   .\n(4)\nIn Section 4, we study a greedy algorithm (Algorithm 2) that finds these node neighborhoods N\u0302n(r) = Supp(\u0398\u0302r) of each random variable Xr separately by a greedy stagewise optimization of the conditional log-likelihood of Xr conditioned on XV \\r.\nThe algorithm then combines these neighborhoods to obtain a graph estimate E\u0302 using an \u201cOR\u201d rule: E\u0302n = \u222ar{(r, t) : t \u2208 N\u0302n(r)}. Other rules such as the \u201cAND\u201d rule, that add an edge only if it occurs in each of the respective node neighborhoods, could be used to combine the node-neighborhoods to a graph estimate. We show in Theorem 2 that the neighborhood selection by the greedy algorithm succeeds in recovering the exact node-neighborhoods with high probability, so that by a union bound, the graph estimates using either the AND or OR rules would be exact with high probability as well.\nBefore we describe this greedy algorithm and its analysis in Section 4 however, we first consider the general statistical model case in the next section. We first describe the forward backward greedy algorithm of Zhang [28] as applied to general statistical models, followed by a sparsistency analysis for this general case. We then specialize these general results in Section 4 to the graphical model case. The next section is thus of independent interest even outside the context of graphical models.\nAlgorithm 1 Greedy forward-backward algorithm for finding a sparse optimizer of L(\u00b7) Input: Data D := {x(1), . . . , x(n)}, Stopping Threshold \u01ebS , Backward Step Factor \u03bd \u2208 (0, 1)\nOutput: Sparse optimizer \u03b8\u0302\n\u03b8\u0302(0) \u2190\u2212 0 and S\u0302(0) \u2190\u2212 \u03c6 and k \u2190\u2212 1\nwhile true do {Forward Step} (j\u2217, \u03b1\u2217) \u2190\u2212 arg min\nj\u2208(S\u0302(k\u22121))c ;\u03b1 L(\u03b8\u0302(k\u22121)+\u03b1ej ;D)\nS\u0302(k) \u2190\u2212 S\u0302(k\u22121) \u222a {j\u2217} \u03b4 (k) f \u2190\u2212 L(\u03b8\u0302(k\u22121);D)\u2212 L(\u03b8\u0302(k\u22121) + \u03b1\u2217ej\u2217 ;D) if \u03b4(k)f \u2264 \u01ebS then break end if\n\u03b8\u0302(k) \u2190\u2212 argmin \u03b8\nL ( \u03b8 S\u0302(k) ;D )\nk \u2190\u2212 k + 1\nwhile true do {Backward Step} j\u2217 \u2190\u2212 arg min\nj\u2208S\u0302(k\u22121) L(\u03b8\u0302(k\u22121) \u2212 \u03b8\u0302(k\u22121)j ej ;D)\nif L ( \u03b8\u0302(k\u22121) \u2212 \u03b8\u0302(k\u22121)j\u2217 ej\u2217 ;D ) \u2212 L ( \u03b8\u0302(k\u22121);D ) > \u03bd\u03b4 (k) f then\nbreak end if\nS\u0302(k\u22121) \u2190\u2212 S\u0302(k) \u2212 {j\u2217} \u03b8\u0302(k\u22121) \u2190\u2212 argmin\n\u03b8 L ( \u03b8 S\u0302(k\u22121) ;D )\nk \u2190\u2212 k \u2212 1 end while\nend while"}, {"heading": "3 Greedy Algorithm for General Losses", "text": "Consider a random variable Z with distribution P, and let Zn1 := {Z1, . . . , Zn} denote n observations drawn i.i.d. according to P. Suppose we are interested in estimating some parameter \u03b8\u2217 \u2208 Rp of the distribution P that is sparse; denote its number of nonzeroes by s\u2217 := \u2016\u03b8\u2217\u20160. Let L : Rp \u00d7Zn 7\u2192 R be some loss function that assigns a cost to any parameter \u03b8 \u2208 Rp, for a given set of observations Zn1 . For ease of notation, in the sequel, we adopt the shorthand L(\u03b8) for L(\u03b8;Zn1 ). We assume that \u03b8\u2217 satisfies EZ [\u2207L(\u03b8\u2217)] = 0.\nWe now consider the forward backward greedy algorithm in Algorithm 1 that rewrites the algorithm in [27] to allow for general loss functions. The algorithm starts with an empty set of active variables S\u0302(0) and gradually adds (and removes) vairables\nto the active set until it meets the stopping criterion. This algorithm has two major steps: the forward step and the backward step. In the forward step, the algorithm finds the best next candidate and adds it to the active set as long as it improves the loss function at least by \u01ebS , otherwise the stopping criterion is met and the algorithm terminates. Then, in the backward step, the algorithm checks the influence of all variables in the presence of the new added variable. If one or some of the previously added variables do not contribute at least \u03bd\u01ebS to the loss function, then the algorithm removes them from the active set. This procedure ensures that at each round, the loss function is improved by at least (1\u2212 \u03bd)\u01ebS and hence it terminates within a finite number of steps.\nWe state the assumptions on the loss function so that sparsistency could be guaranteed. Let us first recall the definition of restricted strong convexity from Negahban et al. [18]. Specifically, for a given set S, the loss function is said to satisfy restricted strong convexity (RSC) with parameter \u03bal if\nL(\u03b8 +\u2206;Zn1 )\u2212 L(\u03b8;Zn1 )\u2212 \u3008\u2207L(\u03b8;Zn1 ),\u2206\u3009 \u2265 \u03bal 2 \u2016\u2206\u201622 for all \u2206 \u2208 S. (5)\nWe can now define sparsity restricted strong convexity as follows. Specifically, we say that the loss function L satisfies RSC(k) with parameter \u03bal if it satisfies RSC with parameter \u03bal for all sets S \u2286 {1, . . . , p} such that \u2016S\u20160 \u2264 k.\nIn contrast, we say the loss function satisfies restricted strong smoothness (RSS) with parameter \u03bau, for a given set S if\nL(\u03b8 +\u2206;Zn1 )\u2212 L(\u03b8;Zn1 )\u2212 \u3008\u2207L(\u03b8;Zn1 ),\u2206\u3009 \u2264 \u03bau 2 \u2016\u2206\u201622 for all \u2206 \u2208 S.\nWe can define RSS(k) similarly: the loss function L satisfies RSS(k) with parameter \u03bau if it satisfies RSS with parameter \u03bau for all sets S \u2286 {1, . . . , p} such that \u2016S\u20160 \u2264 k at all points \u03b8 with \u2016\u03b8\u20160 \u2264 k. Given any constants \u03bal and \u03bau, and a sample based loss functionL, we can typically use concentration based arguments to obtain bounds on the sample size required so that the RSS and RSC conditions hold with high probability.\nAnother property of the loss function that we require is an upper bound \u03bbn on the \u2113\u221e norm of the gradient of the loss at the true parameter \u03b8\u2217, i.e., \u03bbn \u2265 \u2016\u2207L(\u03b8\u2217)\u2016\u221e. This captures the \u201cnoise level\u201d of the samples with respect to the loss. Here too, we can typically use concentration arguments to show for instance that \u03bbn \u2264 cn(log(p)/n)1/2, for some constant cn > 0 with high probability.\nTheorem 1 (Sparsistency). Suppose the loss function L(\u00b7) satisfies RSC (\u03b7 s\u2217) and RSS (\u03b7 s\u2217) with parameters \u03bal and \u03bau for some \u03b7 \u2265 2 + 4\u03c12( \u221a (\u03c12 \u2212 \u03c1)/s\u2217 + \u221a 2)2 with \u03c1 = \u03bau/\u03bal. Moreover, suppose that the true parameters \u03b8\u2217 satisfy minj\u2208S\u2217 |\u03b8\u2217j | >\u221a 32\u03c1\u01ebS/\u03bal. Then if we run Algorithm 1 with stopping threshold \u01ebS \u2265 (8\u03c1\u03b7/\u03bal) s\u2217\u03bb2n, the output \u03b8\u0302 with support S\u0302 satisfies:\n(a) Error Bound: \u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 2\u03bal \u221a s\u2217 (\u03bbn \u221a \u03b7 + \u221a \u01ebS \u221a 2\u03bau).\n(b) No False Exclusions: S\u2217 \u2212 S\u0302 = \u2205.\n(c) No False Inclusions: S\u0302 \u2212 S\u2217 = \u2205.\nProof. The proof theorem hinges on three main lemmas: Lemmas 5 and 7 are simple consequences of the forward and backward steps failing when the greedy algorithm stops, and Lemma 6 which uses these two lemmas and extends techniques from [21] and [19] to obtain an \u21132 error bound on the error. Provided these lemmas hold, we then show below that the greedy algorithm is sparsistent. However, these lemmas require apriori that the RSC and RSS conditions hold for sparsity size |S\u2217 \u222a S\u0302|. Thus, we use the result in Lemma 8 that if RSC(\u03b7s\u2217) holds, then the solution when the algorithm terminates satisfies |S\u0302| \u2264 (\u03b7 \u2212 1)s\u2217, and hence |S\u0302 \u222a S\u2217| \u2264 \u03b7s\u2217. Thus, we can then apply Lemmas 5, 7 and Lemma 6 to complete the proof as detailed below.\n(a) The result follows directly from Lemma 6, and noting that |S\u0302\u222aS\u2217| \u2264 \u03b7s\u2217. In that Lemma, we show that the upper bound holds by drawing from fixed point techniques in [21] and [19], and by using a simple consequence of the forward step failing when the greedy algorithm stops.\n(b) Following the argument in [27], we use the chaining argument. For any \u03c4 \u2208 R, we have\n\u03c4 |{j \u2208 S\u2217 \u2212 S\u0302 : |\u03b8\u2217j |2 > \u03c4}| \u2264 \u2016\u03b8\u2217S\u2217\u2212S\u0302\u2016 2 2 \u2264 \u2016\u03b8\u2217 \u2212 \u03b8\u0302\u201622\n\u2264 8\u03b7s \u2217\u03bb2n \u03ba2l + 16\u03bau\u01ebS \u03ba2l |S\u2217 \u2212 S\u0302|,\nwhere the last inequality follows from part (a) and the inequality (a + b)2 \u2264 2a2 + 2b2. Now, setting \u03c4 = 32\u03bau\u01ebS\n\u03ba2 l\n, and dividing both sides by \u03c4/2 we get\n2|{j \u2208 S\u2217 \u2212 S\u0302 : |\u03b8\u2217j |2 > \u03c4}| \u2264 \u03b7s\u2217\u03bb2n 2\u03bau\u01ebS + |S\u2217 \u2212 S\u0302|.\nSubstituting |{j \u2208 S\u2217 \u2212 S\u0302 : |\u03b8\u2217j |2 > \u03c4}| = |S\u2217 \u2212 S\u0302| \u2212 |{j \u2208 S\u2217 \u2212 S\u0302 : |\u03b8\u2217j |2 \u2264 \u03c4}|, we get\n|S\u2217 \u2212 S\u0302| \u2264 |{j \u2208 S\u2217 \u2212 S\u0302 : |\u03b8\u2217j |2 \u2264 \u03c4}|+ \u03b7s\u2217\u03bb2n 2\u03bau\u01ebS \u2264 |{j \u2208 S\u2217 \u2212 S\u0302 : |\u03b8\u2217j |2 \u2264 \u03c4}|+ 1/2,\ndue to the setting of the stopping threshold \u01ebS . This in turn entails that\n|S\u2217 \u2212 S\u0302| \u2264 |{j \u2208 S\u2217 \u2212 S\u0302 : |\u03b8\u2217j |2 \u2264 \u03c4}| = 0,\nby our assumption on the size of the minimum entry of \u03b8\u2217.\n(c) From Lemma 7, which provides a simple consequence of the backward step failing when the greedy algorithm stops, for \u2206\u0302 = \u03b8\u0302 \u2212 \u03b8\u2217, we have \u01ebS/\u03bau|S\u0302 \u2212 S\u2217| \u2264 \u2016\u2206\u0302S\u0302\u2212S\u2217\u201622 \u2264 \u2016\u2206\u0302\u201622, so that using Lemma 6 and that |S\u2217 \u2212 S\u0302| = 0, we obtain that |S\u0302 \u2212 S\u2217| \u2264 4\u03b7s \u2217\u03bb2n\u03bau\n\u01ebS\u03ba 2 l\n\u2264 1/2, due to the setting of the stopping threshold \u01ebS ."}, {"heading": "3.1 Lemmas for Theorem 1", "text": "We list the simple lemmas that characterize the solution obtained when the algorithm terminates, and on which the proof of Theorem 1 hinges.\nAlgorithm 2 Greedy forward-backward algorithm for pairwise discrete graphical model learning\nInput: Data D := {x(1), . . . , x(n)}, Stopping Threshold \u01ebS , Backward Step Factor \u03bd \u2208 (0, 1) Output: Estimated Edges E\u0302\nfor r \u2208 V do Run Algorithm 1 with L(\u00b7) described by (4) to get \u0398r and its support N\u0302r end for\nOutput E\u0302 = \u22c3\nr\n{ (r, t) : t \u2208 N\u0302r }\nLemma 1 (Stopping Forward Step). When the algorithm 1 stops with parameter \u03b8\u0302 supported on S\u0302, we have\n\u2223\u2223\u2223L ( \u03b8\u0302 ) \u2212L (\u03b8\u2217) \u2223\u2223\u2223 < \u221a 2 |S\u2217 \u2212 S\u0302|\u03bau \u01ebS \u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217 \u2225\u2225\u2225 2 .\nLemma 2 (Stopping Backward Step). When the algorithm 1 stops with parameter \u03b8\u0302 supported on S\u0302, we have\n\u2225\u2225\u2225\u2206\u0302S\u0302\u2212S\u2217 \u2225\u2225\u2225 2\n2 \u2265 \u01ebS \u03bau\n\u2223\u2223\u2223S\u0302 \u2212 S\u2217 \u2223\u2223\u2223 .\nLemma 3 (Stopping Error Bound). When the algorithm 1 stops with parameter \u03b8\u0302 supported on S\u0302, we have\n\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217 \u2225\u2225\u2225 2 \u2264 2 \u03bal\n( \u03bbn \u221a\u2223\u2223\u2223S\u2217 \u222a S\u0302 \u2223\u2223\u2223+ \u221a 2 \u2223\u2223\u2223S\u2217 \u2212 S\u0302 \u2223\u2223\u2223\u03bau\u01ebS ) .\nLemma 4 (Stopping Size). If \u01ebS > \u03bb2n \u03bau\n(\u221a 2\n\u03b7\u22121 \u2212 \u221a 2 \u03b7 )\u22122 and RSC (\u03b7s\u2217) holds for\nsome \u03b7 \u2265 2 + 4\u03c12 (\u221a\n\u03c12\u2212\u03c1 s\u2217\n+ \u221a 2 )2 , then the algorithm 1 stops with k \u2264 (\u03b7 \u2212 1)s\u2217.\nNotice that if \u01ebS \u2265 (8\u03c1\u03b7/\u03bal) (\u03b72/(4\u03c12)) \u03bb2n, then, the assumption of this lemma is satisfied. Hence for large value of s\u2217 \u2265 8\u03c12 > \u03b72/(4\u03c12), it suffices to have \u01ebS \u2265 (8\u03c1\u03b7/\u03bal) s \u2217\u03bb2n."}, {"heading": "4 Greedy Algorithm for Pairwise Graphical Models", "text": "Suppose we are given set of n i.i.d. samples D := {x(1), . . . , x(n)}, drawn from a pairwise Ising model as in (2), with parameters \u03b8\u2217, and graph G = (V,E\u2217). It will be useful to denote the maximum node-degree in the graph E\u2217 by d. As we will show, our model selection performance depends critically on this parameter d. We then propose the Algorithm 2 for estimating the underlying graphical model from the n samples D.\nTheorem 2 (Pairwise Sparsistency). Suppose we run Algorithm 2 with stopping threshold \u01ebS \u2265 c1 d log pn , where, d is the maximum node degree in the graphical model, and the true parameters \u03b8\u2217 satisfy c3\u221a\nd > minj\u2208S\u2217 |\u03b8\u2217j | > c2\n\u221a \u01ebS , and further that number\nof samples scales as n > c4 d 2 log p,\nfor some constants c1, c2, c3, c4. Then, with probability at least 1 \u2212 c\u2032 exp(\u2212c\u2032\u2032n), the output \u03b8\u0302 supported on S\u0302 satisfies:\n(a) No False Exclusions: E\u2217 \u2212 E\u0302 = \u2205.\n(b) No False Inclusions: E\u0302 \u2212 E\u2217 = \u2205.\nProof. This theorem is a corollary to our general Theorem 1. We first show that the conditions of Theorem 1 hold under the assumptions in this corollary. RSC, RSS. We first note that the conditional log-likelihood loss function in (4) corresponds to a logistic likelihood. Moreover, the covariates are all binary, and bounded, and hence also sub-Gaussian. [19, 2] analyze the RSC and RSS properties of generalized linear models, of which logistic models are an instance, and show that the following result holds if the covariates are sub-Gaussian. Let \u2202L(\u2206; \u03b8\u2217) = L(\u03b8\u2217 + \u2206)\u2212L(\u03b8\u2217)\u2212\u3008\u2207L(\u03b8\u2217),\u2206\u3009 be the second order Taylor series remainder. Then, Proposition 2 in [19] states that that there exist constants \u03bal1 and \u03ba l 2, independent of n, p such that with probability at least 1\u2212 c1 exp(\u2212c2n), for some constants c1, c2 > 0,\n\u2202L(\u2206; \u03b8\u2217) \u2265 \u03bal1\u2016\u2206\u20162 { \u2016\u2206\u20162 \u2212 \u03bal2 \u221a log(p)\nn \u2016\u2206\u20161\n} for all \u2206 : \u2016\u2206\u20162 \u2264 1.\nThus, if \u2016\u2206\u20160 \u2264 k := \u03b7d, then \u2016\u2206\u20161 \u2264 \u221a k\u2016\u2206\u20162, so that\n\u2202L(\u2206; \u03b8\u2217) \u2265 \u2016\u2206\u201622 ( \u03bal1 \u2212 \u03bal2 \u221a k log p\nn\n) \u2265 \u03ba l 1\n2 \u2016\u2206\u201622,\nif n > 4(\u03bal2/\u03ba l 1) 2 \u03b7d log(p). In other words, with probability at least 1\u2212c1 exp(\u2212c2n), the loss functionL satisfies RSC(k) with parameter \u03bal1 providedn > 4(\u03bal2/\u03bal1)2 \u03b7d log(p). Similarly, it follows from [19, 2] that there exist constants \u03bau1 and \u03ba u 2 such that with probability at least 1\u2212 c\u20321 exp(\u2212c\u20322n),\n\u2202L(\u2206; \u03b8\u2217) \u2264 \u03bau1\u2016\u2206\u20162{\u2016\u2206\u20162 \u2212 \u03bau2\u2016\u2206\u20161} for all \u2206 : \u2016\u2206\u20162 \u2264 1,\nso that by a similar argument, with probability at least 1 \u2212 c\u20321 exp(\u2212c\u20322n), the loss function L satisfies RSS(k) with parameter \u03bau1 provided n > 4(\u03bau2/\u03bau1 )2 \u03b7d log(p). Noise Level. Next, we obtain a bound on the noiselevel \u03bbn \u2265 \u2016\u2207L(\u03b8\u2217)\u2016\u221e following similar arguments to [20]. Let W denote the gradient \u2207L(\u03b8\u2217) of the loss function (4). Any entry of W has the form Wt = 1n \u2211n i=1 Z (i) rt , where Z (i) rt = x (i) t (x (i) r \u2212 P(xr = 1|x(i)\\s )) are zero-mean, i.i.d. and bounded |Z (i) rt | \u2264 1. Thus, an application of Hoeffding\u2019s inequality yields that P[|Wt| > \u03b4] \u2264 2 exp(\u22122n\u03b42). Applying a union bound\nover indices in W , we get P[\u2016W\u2016\u221e > \u03b4] \u2264 2 exp(\u22122n\u03b42 + log(p)). Thus, if \u03bbn = (log(p)/n)1/2, then \u2016W\u2016\u221e \u2264 \u03bbn with probability at least 1\u2212 exp(\u2212n\u03bb2n + log(p)).\nWe can now verify that under the assumptions in the corollary, the conditions on the stopping size \u01ebS and the minimum absolute value of the non-zero parameters minj\u2208S\u2217 |\u03b8\u2217j | are satisfied. Moreover, from the discussion above, under the sample size scaling in the corollary, the required RSC and RSS conditions hold as well. Thus, Theorem 1 yields that each node neighborhood is recovered with no false exclusions or inclusions with probability at least 1\u2212 c\u2032 exp(\u2212c\u2032\u2032n). An application of a union bound over all nodes completes the proof.\nRemarks. The sufficient condition on the parameters imposed by the greedy algorithm is a restricted strong convexity condition [19], which is weaker than the irrepresentable condition required by [20]. Further, the number of samples required for sparsistent graph recovery scales as O(d2 log p), where d is the maximum node degree, in contrast to O(d3 log p) for the \u21131 regularized counterpart. We corroborate this in our simulations, where we find that the greedy algorithm requires fewer observations than [20] for sparsistent graph recovery.\nWe also note that the result can also be extended to the general pairwise graphical model case, where each random variable takes values in the range {1, . . . ,m}. In that case, the conditional likelihood of each node conditioned on the rest of the nodes takes the form of a multiclass logistic model, and the greedy algorithm would take the form of a \u201cgroup\u201d forward-backward greedy algorithm, which would add or remove all the parameters corresponding to an edge as a group. Our analysis however naturally extends to such a group greedy setting as well. The analysis for RSC and RSS remains the same and for bounds on \u03bbn, see equation (12) in [15]. We defer further discussion on this due to the lack of space."}, {"heading": "5 Experimental Results", "text": "We now present experimental results that illustrate the power of Algorithm 2 and support our theoretical guarantees. We simulated structure learning of several different graph structures and compared the learning rates of our method against that of a standard \u21131-logistic regression method as outlined in [20].\nWe performed experiments using 3 different graph structures: (a) chain (line graph), (b) 4-nearest neighbor (grid graph) and (c) star graph. For each experiment, we assumed a pairwise binary Ising model in which each \u03b8\u2217rt = \u00b11 randomly. For each graph type, we generated a set of n samples x(1), ..., x(n) using Gibbs sampling. We then attempted to learn the structure of the model using both Algorithm 2 as well as \u21131-logistic regression. We then compared the actual graph structure with the empirically learned graph structures. If the graph structures matched completely then we declared the result a success otherwise we declared the result a failure. We compared these results over a range of sample sizes (n) and averaged the results for each sample size over a batch of size 10. For all greedy experiments we set the stopping threshold \u01ebS = c log(np) n , where c is a tuning constant, as suggested by Theorem 2, and set the\nbackwards step threshold \u03bd = 0.5. For all \u21131-logistic regression experiments we set the regularization parameter \u03bbn = c\u2032 \u221a log(p)/n, where c\u2032 is set via cross-validation.\nFigure 1 shows the results for the chain (d = 2), grid (d = 4) and star (d = 0.1p) graphs using both Algorithm 2 and \u21131-logistic regression for three different graph sizes p \u2208 {36, 64, 100} with mixed (random sign) couplings. For each sample size, we generated a batch of 10 different graphical models and averaged the probability of success (complete structure learned) over the batch. Each curve then represents the probability of success versus the control parameter \u03b2(n, p, d) = n/[20d log(p)] which increases with the sample size n. These results support our theoretical claims and demonstrate the efficiency of the greedy method in comparison to node-wise logistic regression [20]."}, {"heading": "A Auxiliary Lemmas for Theorem 1", "text": "In this section, we prove the Lemmas used in the proof of Theorem 1. Note that when the algorithm terminates, the forward step fails to go through. This entails that\nL(\u03b8\u0302)\u2212 inf j\u2208S\u0302c,\u03b1\u2208R L(\u03b8\u0302 + \u03b1ej) < \u01ebS . (6)\nThe next lemma shows that this has the consequence of upper bounding the deviation in loss between the estimated parameters \u03b8\u0302 and the true parameters \u03b8\u2217.\nLemma 5 (Stopping Forward Step). When the algorithm stops with parameter \u03b8\u0302 supported on S\u0302, we have\n\u2223\u2223\u2223L ( \u03b8\u0302 ) \u2212 L (\u03b8\u2217) \u2223\u2223\u2223 < \u221a 2 |S\u2217 \u2212 S\u0302|\u03bau \u01ebS \u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217 \u2225\u2225\u2225 2 . (7)\nProof. Let \u2206\u0302 = \u03b8\u2217 \u2212 \u03b8\u0302. For any \u03b7 \u2208 R, we have\nL ( \u03b8\u0302 + \u03b7\u2206\u0302jej ) \u2264 L ( \u03b8\u0302 ) + \u03b7\u2207jL ( \u03b8\u0302 ) \u2206\u0302j + \u03b7\n2\u03bau 2 \u2206\u03022j .\nThus, we can establish\n\u2212|S\u2217 \u2212 S\u0302|\u01ebS < \u2211\nj\u2208S\u2217\u2212S\u0302\n( L ( \u03b8\u0302 + \u03b7\u2206\u0302jej ) \u2212 L ( \u03b8\u0302 ))\n\u2264 \u03b7 ( L (\u03b8\u2217)\u2212 L ( \u03b8\u0302 ))\n+ \u03b72 \u03bau 2\n\u2225\u2225\u2225\u2206\u0302 \u2225\u2225\u2225 2\n2 .\nOptimizing the RHS over \u03b7, we obtain\n\u2212|S\u2217 \u2212 S\u0302| \u01ebS < \u2212\n( L(\u03b8\u2217)\u2212 L ( \u03b8\u0302 ))2\n2 \u03bau \u2016\u2206\u0302\u201622 ,\nwhence the lemma follows.\nLemma 6 (Stopping Error Bound). When the algorithm stops with parameter \u03b8\u0302 supported on S\u0302, we have\n\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162 \u2264 2\n\u03bal\n( \u03bbn \u221a\u2223\u2223\u2223S\u2217 \u222a S\u0302 \u2223\u2223\u2223+ \u221a 2 \u2223\u2223\u2223S\u2217 \u2212 S\u0302 \u2223\u2223\u2223\u03bau\u01ebS ) . (8)\nProof. For \u2206 \u2208 R, let\nG(\u2206) = L (\u03b8\u2217 +\u2206)\u2212 L (\u03b8\u2217)\u2212 \u221a 2 \u2223\u2223\u2223S\u2217 \u2212 S\u0302 \u2223\u2223\u2223 \u03bau \u01ebS \u2016\u2206\u20162 .\nIt can be seen that G(0) = 0, and from the previous lemma, G(\u2206\u0302) \u2264 0. Further, G(\u2206) is sub-homogeneous (over a limited range): G(t\u2206) \u2264 tG(\u2206) for t \u2208 [0, 1]. Thus,\nfor a carefully chosen r > 0, if we show that G(\u2206) > 0 for all \u2206 \u2208 {\u2206 : \u2016\u2206\u20162 \u2264 r, \u2016\u2206\u20160 \u2264 |S|}, where S = |S\u0302 \u222a S\u2217|, then it follows that \u2016\u2206\u0302\u20162 \u2264 r. If not, then there would exist some t \u2208 [0, 1) such that \u2016t\u2206\u0302\u2016 = r, whence we would arrive at the contradiction\n0 < G(t\u2206\u0302) \u2264 tG(\u2206\u0302) \u2264 0.\nThus, it remains to show that G(\u2206) > 0 for all \u2206 \u2208 {\u2206 : \u2016\u2206\u20162 \u2264 r, \u2016\u2206\u20160 \u2264 |S|}. By restricted strong convexity property of L, we have\nL(\u03b8\u2217 +\u2206)\u2212 L(\u03b8\u2217) \u2265 \u3008\u2207L(\u03b8\u2217),\u2206\u3009+ \u03bal 2 \u2016\u2206\u201622 .\nWe can establish\n\u3008\u2207L(\u03b8\u2217),\u2206\u3009 \u2265 \u2212 |\u3008\u2207L(\u03b8\u2217),\u2206\u3009| \u2265 \u2212\u2016\u2207L(\u03b8\u2217)\u2016\u221e \u2016\u2206\u20161 = \u03bbn \u2016\u2206\u20161 ,\nand hence,\nG(\u03b8\u2217 +\u2206) \u2265 \u2212\u03bbn\u2016\u2206\u20161 + \u03bal 2 \u2016\u2206\u201622 \u2212\n\u221a 2 \u2223\u2223\u2223S\u2217 \u2212 S\u0302 \u2223\u2223\u2223 \u03bau\u01ebS\u2016\u2206\u20162\n> \u2016\u2206\u20162 ( \u03bal 2 \u2016\u2206\u20162 \u2212 \u03bbn \u221a\u2223\u2223\u2223S\u2217 \u222a S\u0302 \u2223\u2223\u2223\u2212 \u221a 2 \u2223\u2223\u2223S\u2217 \u2212 S\u0302 \u2223\u2223\u2223 \u03bau\u01ebS )\n> 0,\nif \u2016\u2206\u20162 = r for\nr = 2\n\u03bal\n( \u03bbn \u221a\u2223\u2223\u2223S\u2217 \u222a S\u0302 \u2223\u2223\u2223+ \u221a 2 \u2223\u2223\u2223S\u2217 \u2212 S\u0302 \u2223\u2223\u2223\u03bau\u01ebS ) .\nThis concludes the proof of the lemma.\nNext, we note that when the algorithm terminates, the backward step with the current parameters has failed to go through. This entails that\ninf j\u2208S\u0302\nL(\u03b8\u0302 \u2212 \u03b8\u0302jej)\u2212 L(\u03b8\u0302) > \u01ebS/2. (9)\nThe next lemma shows the consequence of this bound.\nLemma 7 (Stopping Backward Step). When the algorithm stops with parameter \u03b8\u0302 supported on S\u0302, we have\n\u2225\u2225\u2225\u2206\u0302S\u0302\u2212S\u2217 \u2225\u2225\u2225 2 2 \u2265 \u01ebS \u03bau \u2223\u2223\u2223S\u0302 \u2212 S\u2217 \u2223\u2223\u2223 . (10)\nProof. We have\n|S\u0302 \u2212 S\u2217| inf j\u2208S\u0302\nL(\u03b8\u0302 \u2212 \u03b8\u0302jej) \u2264 \u2211\nj\u2208S\u0302\u2212S\u2217 L(\u03b8\u0302 \u2212 \u03b8\u0302jej)\n\u2264 |S\u0302 \u2212 S\u2217|L(\u03b8\u0302) + \u2211\nj\u2208S\u0302\u2212S\u2217\n( \u2207jL(\u03b8\u0302) \u03b8\u0302j +\n\u03bau 2 \u03b8\u03022j\n)\n\u2264 |S\u0302 \u2212 S\u2217|L(\u03b8\u0302) + \u03bau 2 \u2225\u2225\u2225\u2206\u0302S\u0302\u2212S\u2217 \u2225\u2225\u2225 2 2 ,\nwhere the second inequality uses the fact that [\u2207L(\u03b8\u0302)]S\u0302 = 0. Substituting (9) above, the lemma follows."}, {"heading": "B Lemmas on the Stopping Size", "text": "Lemma 8. If \u01ebS > \u03bb2n \u03bau\n( 1 2\u03c1 \u221a \u03b3\u2212 \u221a \u03c12\u2212\u03c1 k\u2217\u221a\n1+\u03b3 \u2212 \u221a 2 2+\u03b3 )\u22122 and RSC ((2 + \u03b3)k\u2217) holds for\nsome \u03b3 \u2265 4\u03c12 (\u221a\n\u03c12\u2212\u03c1 k\u2217 +\n\u221a 2 )2 , then the algorithm stops with k \u2264 (1 + \u03b3)k\u2217.\nProof. Consider the first time the algorithm reaches k = (1 + \u03b3)k\u2217 + 1, then by Lemma 9 and 11, we have\n\u221a k \u2212 1\u2212 k\u2217 k \u2212 1 \u2264 \u221a |S\u0302(k\u22121) \u2212 S\u2217| |S\u0302(k\u22121) \u222a S\u2217| \u2264 2\u03bau \u221a \u03bau(\u03bau \u2212 \u03bal)\n\u03ba2l\n\u221a |S\u0302(k\u22121) \u222a S\u2217|\n+ 2\u03bau \u03bal ( \u03bbn\u221a \u03bau\u01ebS + \u221a 2|S\u2217 \u2212 S\u0302(k\u22121)| |S\u2217 \u222a S\u0302(k\u22121)| )\n\u2264 2\u03bau\u03bal\n\u221a( \u03bau \u03bal )2 \u2212 \u03bau\u03bal\u221a\nk \u2212 1 + 2\u03bau \u03bal ( \u03bbn\u221a \u03bau\u01ebS + \u221a 2k\u2217 k + k\u2217 \u2212 1 ) .\nHence, we get 1 2\u03c1 \u221a \u03b3 \u2212 \u221a \u03c12\u2212\u03c1 k\u2217\u221a\n1 + \u03b3 \u2212 \u221a 2 2 + \u03b3 \u2264 \u03bbn\u221a \u03bau\u01ebS .\nFor \u03b3 \u2265 4\u03c12 (\u221a\n\u03c12\u2212\u03c1 k\u2217 +\n\u221a 2 )2 , the LHS is positive and we arrive to a contradiction\nwith the assumption on \u01ebS .\nWhen the algorithm reaches the support size of k at the beginning of the forward step, i.e., we added the kth variable to the support and the backward step did not remove any variable, let \u03b8\u0302(k) denote the current parameter and S\u0302(k) = Supp(\u03b8\u0302(k)) with k = |S\u0302(k)|. Let \u03b8\u2217 be the target parameter matrix (i.e., E [\u2207L(\u03b8\u2217)] = 0), with S\u2217 = Supp(\u03b8\u2217) and k\u2217 = |S\u2217|. Lemmas 9, 10 and 11 follow along similar lines to\ntheir counterparts in Lemmas 7, 5 and 6 respectively: the latter held when the algorithm terminates, while the lemmas below hold at any iterate \u03b8\u0302(k) where we have first added the kth variable to the support. We provide their detailed proofs for completeness.\nLemma 9 (General Backward Step). The first time the algorithm reaches a support size of k > k\u2217+4 ( \u03bau \u03bal )4 +1 at the beginning of the forward step, assumingRSC ( |S\u0302(k) \u222a S\u2217| ) holds, we have\n\u2225\u2225\u2225\u03b8\u0302(k\u22121) S\u0302(k\u22121)\u2212S\u2217 \u2225\u2225\u2225 2 2 \u2265\n  \u221a\n|S\u0302(k\u22121) \u2212 S\u2217| \u03bau \u2212 2\u03bau \u221a \u03bau \u2212 \u03bal \u03ba2l\n  2\n\u03b4 (k) f . (11)\nProof. Under the assumption of the lemma, the immediate previous backward step has not gone through and hence,\ninf j\u2208S\u0302(k)\u2212S\u2217\nL ( \u03b8\u0302(k) \u2212 \u03b8\u0302(k)j ej ) \u2212 L ( \u03b8\u0302(k) ) \u2265 \u03b4 (k) f\n2 .\nConsequently, we get\n|S\u0302(k\u22121) \u2212 S\u2217| \u03b4 (k) f\n2 \u2264\n\u2211\nj\u2208S\u0302(k\u22121)\u2212S\u2217 L(\u03b8\u0302(k) \u2212 \u03b8\u0302(k)j ej)\u2212 L(\u03b8\u0302(k))\n\u2264 \u03bau 2 \u2225\u2225\u2225\u03b8\u0302(k) S\u0302(k\u22121)\u2212S\u2217 \u2225\u2225\u2225 2 2 \u2264 \u03bau 2 (\u2225\u2225\u2225\u03b8\u0302(k\u22121) S\u0302(k\u22121)\u2212S\u2217 \u2225\u2225\u2225 2 + \u2225\u2225\u2225\u2206(k) \u2225\u2225\u2225 2 )2 ,\nwhere, \u2206(k) = \u03b8\u0302(k) S\u0302(k\u22121) \u2212 \u03b8\u0302(k\u22121). This entails that   \u221a\n|S\u0302(k\u22121) \u2212 S\u2217| \u03bau \u03b4 (k) f \u2212 \u2225\u2225\u2225\u2206(k) \u2225\u2225\u2225 2\n  2 \u2264 \u2225\u2225\u2225\u03b8\u0302(k\u22121)\nS\u0302(k\u22121)\u2212S\u2217\n\u2225\u2225\u2225 2\n2 .\nThus, it suffices to show that \u2225\u2225\u2206(k) \u2225\u2225 2 \u2264 2\u03bau\n\u03ba2 l\n\u221a (\u03bau \u2212 \u03bal)\u03b4(k)f .\nFrom the forward step, we have\nL ( \u03b8\u0302(k\u22121) ) \u2212 inf j /\u2208S\u0302(k\u22121),\u03b1\u2208R L ( \u03b8\u0302(k\u22121) + \u03b1ej ) = \u03b4 (k) f .\nLet (j\u2217, \u03b1\u2217 6= 0) be the optimizer of the equation above. Now, we have \u03bal 2 \u2225\u2225\u2225\u2206(k) \u2225\u2225\u2225 2 2 \u2264 L ( \u03b8\u0302 (k) S\u0302(k\u22121) ) \u2212 L ( \u03b8\u0302(k\u22121) )\n\u2264 L ( \u03b8\u0302 (k)\nS\u0302(k\u22121)\n) \u2212 L ( \u03b8\u0302(k) ) + L ( \u03b8\u0302(k) ) \u2212 L ( \u03b8\u0302(k\u22121) )\n\u2264 \u03bau 2 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 2 \u2212 \u03bal 2 \u2225\u2225\u2225\u2206(k) \u2225\u2225\u2225 2 2 \u2212 \u03bal 2 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 2 .\nHence, \u2225\u2225\u2206(k) \u2225\u22252 2 \u2264 \u03bau\u2212\u03bal2\u03bal \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 2 and we only need to show that \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 \u2264 2\u03bau\u03bal \u221a 2 \u03bal \u03b4 (k) f .\nSince \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2212 \u03b1\u2217 \u2223\u2223\u2223 + |\u03b1\u2217|, we can equivalently control the latter two terms. First, by forward step construction, \u03bal2 |\u03b1\u2217| 2 \u2264 L ( \u03b8\u0302(k\u22121) ) \u2212 L ( \u03b8\u0302(k\u22121) + \u03b1\u2217ej\u2217 ) =\n\u03b4 (k) f and hence |\u03b1\u2217| \u2264 \u221a 2 \u03bal \u03b4 (k) f . Second, we claim that \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2212 \u03b1\u2217 \u2223\u2223\u2223 \u2264 2\u03bau\u2212\u03bal\u03bal |\u03b1\u2217| and we are done.\nIn contrary, suppose \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2212 \u03b1\u2217 \u2223\u2223\u2223 2 > ( 2\u03bau\u2212\u03bal \u03bal )2 |\u03b1\u2217|2 \u2265 \u03bau\u03bal |\u03b1\u2217| 2. We have\n\u03bal 2 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2212 \u03b1\u2217 \u2223\u2223\u2223 2 > \u03bau 2 |\u03b1\u2217|2\n\u2265 L ( \u03b8\u0302(k) \u2212 \u03b1\u2217ej\u2217 ) \u2212 L ( \u03b8\u0302(k) ) \u2265 L ( \u03b8\u0302(k) \u2212 \u03b1\u2217ej\u2217 ) \u2212 L ( \u03b8\u0302(k\u22121) ) + L ( \u03b8\u0302(k\u22121) ) \u2212 L ( \u03b8\u0302(k) )\n\u2265 \u03bal 2\n\u2225\u2225\u2225\u2206(k) \u2225\u2225\u2225 2\n2 + \u03bal 2 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2212 \u03b1\u2217 \u2223\u2223\u2223 2 +\u2207j\u2217L ( \u03b8\u0302(k\u22121) )( \u03b8\u0302 (k) j\u2217 \u2212 \u03b1\u2217 )\n+ \u03bal 2\n\u2225\u2225\u2225\u2206(k) \u2225\u2225\u2225 2\n2 + \u03bal 2 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 2 .\nThis is a contradiction provided that \u03bal2 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 2 + \u2207j\u2217L ( \u03b8\u0302(k\u22121) )( \u03b8\u0302 (k) j\u2217 \u2212 \u03b1\u2217 )\n\u2265 0. Later, we will show that Sign ( \u2207j\u2217L ( \u03b8\u0302(k\u22121) )) = \u2212Sign (\u03b1\u2217) and \u03bal|\u03b1\u2217| \u2264 \u2223\u2223\u2223\u2207j\u2217L ( \u03b8\u0302(k\u22121) )\u2223\u2223\u2223 \u2264 \u03bau|\u03b1\u2217|. With these, if \u03b8\u0302 (k) j\u2217\n\u03b1\u2217 \u2264 1, we have \u2207j\u2217L\n( \u03b8\u0302(k\u22121) )( \u03b8\u0302 (k) j\u2217 \u2212 \u03b1\u2217 )\n\u2265 0 and the claim follows. Otherwise, we have \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 \u2265 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 \u2212 |\u03b1\u2217| =\n\u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2212 \u03b1\u2217 \u2223\u2223\u2223 so that\u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 \u2265 2\u03bau\u03bal |\u03b1\u2217| and hence,\n\u03bal 2 \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2223\u2223\u2223 2 +\u2207j\u2217L ( \u03b8\u0302(k\u22121) )( \u03b8\u0302 (k) j\u2217 \u2212 \u03b1\u2217 ) \u2265 \u03bal 2 2\u03bau \u03bal |\u03b1\u2217| \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2212 \u03b1\u2217 \u2223\u2223\u2223\u2212 \u03bau |\u03b1\u2217| \u2223\u2223\u2223\u03b8\u0302(k)j\u2217 \u2212 \u03b1\u2217 \u2223\u2223\u2223\n= 0.\nTo get the claimed properties of \u2207j\u2217L ( \u03b8\u0302(k\u22121) ) , note that\n\u03bal 2 |\u03b1\u2217|2 \u2264 L\n( \u03b8\u0302(k\u22121) ) \u2212 L ( \u03b8\u0302(k\u22121) + \u03b1\u2217ej\u2217 )\n\u2264 \u2212\u03bal 2 |\u03b1\u2217|2 \u2212\u2207j\u2217L\n( \u03b8\u0302(k\u22121) ) \u03b1\u2217 ,\nand hence Sign ( \u2207j\u2217L ( \u03b8\u0302(k\u22121) )) = \u2212Sign (\u03b1\u2217) and \u03bal|\u03b1\u2217| \u2264 \u2223\u2223\u2223\u2207j\u2217L ( \u03b8\u0302(k\u22121) )\u2223\u2223\u2223. Also, we can establish\n\u03bau 2\n|\u03b1\u2217|2 \u2265 L ( \u03b8\u0302(k\u22121) ) \u2212 L ( \u03b8\u0302(k\u22121) + \u03b1\u2217ej\u2217 )\n\u2265 \u2212\u03bau 2\n|\u03b1\u2217|2 \u2212\u2207j\u2217L ( \u03b8\u0302(k\u22121) ) \u03b1\u2217 .\nSince \u2212\u2207j\u2217L ( \u03b8\u0302(k\u22121) ) \u03b1\u2217 \u2265 0, we can conclude that \u2223\u2223\u2223\u2207j\u2217L ( \u03b8\u0302(k\u22121) )\u2223\u2223\u2223 \u2264 \u03bau|\u03b1\u2217|. This concludes the proof of the lemma.\nLemma 10 (General Forward Step). The first time the algorithm reaches a support size of k at the beginning of the forward step, we have\n\u2223\u2223\u2223L (\u03b8\u2217)\u2212 L ( \u03b8\u0302(k\u22121) )\u2223\u2223\u2223 \u2264 \u221a 2 \u2223\u2223\u2223S\u2217 \u2212 S\u0302(k\u22121) \u2223\u2223\u2223 \u03bau \u03b4(k)f \u2225\u2225\u2225\u03b8\u2217 \u2212 \u03b8\u0302(k\u22121) \u2225\u2225\u2225 2 .\nProof. Under the assumption of the lemma, we have\nL ( \u03b8\u0302(k\u22121) ) \u2212 inf j /\u2208S\u0302(k\u22121),\u03b1\u2208R L ( \u03b8\u0302(k\u22121) + \u03b1ej ) = \u03b4 (k) f .\nFor any \u03b7 \u2208 R, we have\n\u2212 \u2223\u2223\u2223S\u2217 \u2212 S\u0302(k\u22121) \u2223\u2223\u2223 \u03b4(k)f \u2264 \u2211 j\u2208S\u2217\u2212S\u0302(k\u22121) L ( \u03b8\u0302(k\u22121) + \u03b7\u03b8\u2217j ej ) \u2212 L ( \u03b8\u0302(k\u22121) )\n\u2264 \u03b7 \u2211\nj\u2208S\u2217\u2212S\u0302(k\u22121) \u2207jL\n( \u03b8\u0302(k\u22121) ) \u03b8\u2217j + \u03b7\n2\u03bau 2\n\u2225\u2225\u2225\u03b8\u2217 \u2212 \u03b8\u0302(k\u22121) \u2225\u2225\u2225 2\n2\n\u2264 \u03b7 ( L (\u03b8\u2217)\u2212 L ( \u03b8\u0302(k\u22121) )) + \u03b72\n\u03bau 2\n\u2225\u2225\u2225\u03b8\u2217 \u2212 \u03b8\u0302(k\u22121) \u2225\u2225\u2225 2\n2 .\nOptimizing the RHS over \u03b7, we obtain\n|S\u2217 \u2212 S\u0302(k\u22121)|\u03b4(k)f \u2265\n( L (\u03b8\u2217)\u2212 L ( \u03b8\u0302(k\u22121) ))2\n2\u03bau \u2016\u03b8\u2217 \u2212 \u03b8\u0302(k\u22121)\u201622 .\nThis concludes the proof of the lemma.\nLemma 11 (General Error Bound). The first time the algorithm reaches a support size of k at the beginning of the forward step, assuming RSC ( |S\u0302(k) \u222a S\u2217| ) holds, we have\n\u2225\u2225\u2225\u03b8\u0302(k\u22121) S\u0302(k\u22121)\u2212S\u2217 \u2225\u2225\u2225 2 2 \u2264 4\u03bau|S\u2217 \u222a S\u0302(k\u22121)|\u03b4(k)f \u03ba2l ( \u03bbn\u221a \u03bau\u01ebS + \u221a 2|S\u2217 \u2212 S\u0302(k\u22121)| |S\u2217 \u222a S\u0302(k\u22121)| )2 .\nProof. Let\nG (\u2206) := L(\u03b8\u2217 +\u2206)\u2212 L(\u03b8\u2217)\u2212 \u221a 2|S\u2217 \u2212 S\u0302(k\u22121)|\u03bau \u03b4(k)f \u2016\u2206\u20162.\nIt can be seen that G(0) = 0, and from Lemma 10, G(\u03b8\u0302(k\u22121)\u2212\u03b8\u2217) \u2264 0. Further, G(\u2206) is sub-homogeneous (over a limited range): G(t\u2206) \u2264 tG(\u2206) for t \u2208 [0, 1]. Thus, for\na carefully chosen r > 0, if we show that G(\u2206) > 0 for all \u2206 \u2208 {\u2206 : \u2016\u2206\u20162 \u2264 r, \u2016\u2206\u20160 \u2264 |S|}, where S = |S\u0302(k) \u222a S\u2217|, then it follows that \u2016\u03b8\u0302(k) \u2212 \u03b8\u2217\u20162 \u2264 r. If not, then there would exist some t \u2208 [0, 1) such that \u2016t(\u03b8\u0302(k) \u2212 \u03b8\u2217)\u20162 = r, whence we would arrive at the contradiction\n0 < G ( t(\u03b8\u0302(k) \u2212 \u03b8\u2217) ) \u2264 tG ( \u03b8\u0302(k) \u2212 \u03b8\u2217 ) \u2264 0.\nThus, it remains to show that G(\u2206) > 0 for all \u2206 \u2208 {\u2206 : \u2016\u2206\u20162 \u2264 r, \u2016\u2206\u20160 \u2264 |S|}. By RSC, we have\nL(\u03b8\u2217 +\u2206)\u2212 L(\u03b8\u2217) \u2265 \u2207L(\u03b8\u2217) \u00b7\u2206+ \u03bal 2 \u2016\u2206\u201622.\nWe can establish\n\u2207L(\u03b8\u2217) \u00b7\u2206 \u2265 \u2212|\u2207L(\u03b8\u2217) \u00b7\u2206| \u2265 \u2212\u2016\u2207L(\u03b8\u2217)\u2016\u221e\u2016\u2206\u20161 = \u2212\u03bbn\u2016\u2206\u20161,\nand hence,\nG(\u03b8\u2217 +\u2206) \u2265 \u2212\u03bbn\u2016\u2206\u20161 + \u03bal 2 \u2016\u2206\u201622 \u2212\n\u221a 2|S\u2217 \u2212 S\u0302(k\u22121)|\u03bau\u03b4(k)f \u2016\u2206\u20162\n\u2265 \u2016\u2206\u20162 ( \u03bal 2 \u2016\u2206\u20162 \u2212 \u03bbn \u221a |S\u2217 \u222a S\u0302(k)| \u2212 \u221a 2|S\u2217 \u2212 S\u0302(k\u22121)|\u03bau\u03b4(k)f )\n> 0,\nif \u2016\u2206\u20162 = r for\nr = 2\n\u03bal\n( \u03bbn \u221a |S\u2217 \u222a S\u0302(k)|+ \u221a 2|S\u2217 \u2212 S\u0302(k\u22121)|\u03bau\u03b4(k)f ) .\nHence,\n\u2225\u2225\u2225\u03b8\u0302(k\u22121) S\u0302(k\u22121)\u2212S\u2217 \u2225\u2225\u2225 2 2 \u2264 4\u03bau|S\u2217 \u222a S\u0302(k\u22121)|\u03b4(k)f \u03ba2l\n  \u03bbn\u221a\n\u03bau\u03b4 (k) f\n+ \u221a 2|S\u2217 \u2212 S\u0302(k\u22121)| |S\u2217 \u222a S\u0302(k\u22121)|   2 .\nFinally, consider the fact that \u03b4(k)f \u2265 \u01ebS . This concludes the proof of the lemma."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "<lb>In this paper, we address the problem of learning the structure of a pairwise<lb>graphical model from samples in a high-dimensional setting. Our first main re-<lb>sult studies the sparsistency, or consistency in sparsity pattern recovery, properties<lb>of a forward-backward greedy algorithm as applied to general statistical models.<lb>As a special case, we then apply this algorithm to learn the structure of a discrete<lb>graphical model via neighborhood estimation. As a corollary of our general result,<lb>we derive sufficient conditions on the number of samples n, the maximum node-<lb>degree d and the problem size p, as well as other conditions on the model param-<lb>eters, so that the algorithm recovers all the edges with high probability. Our result<lb>guarantees graph selection for samples scaling as n = \u03a9(d log(p)), in contrast<lb>to existing convex-optimization based algorithms that require a sample complexity<lb>of \u03a9(d log(p)). Further, the greedy algorithm only requires a restricted strong<lb>convexity condition which is typically milder than irrepresentability assumptions.<lb>We corroborate these results using numerical simulations at the end.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}