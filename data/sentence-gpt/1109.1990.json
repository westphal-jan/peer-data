{"id": "1109.1990", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2011", "title": "Trace Lasso: a trace norm regularization for correlated designs", "abstract": "Using the $\\ell_1$-norm to regularize the estimation of the parameter vector of a linear model leads to an unstable estimator when covariates are highly correlated. In this paper, we introduce a new penalty function which takes into account the correlation of the design matrix to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm, which is a convex surrogate of the rank, of the selected covariates as the criterion of model complexity and error tolerance.\n\n\n\nThe sample size for our paper is roughly the same as the one given by Lasso. This is especially useful because it is very different in this respect. First, we do not need to take in the form of a linear model: we can assume that, in terms of linear regression, the distribution of the distribution of the distribution of the number of different covariates in each distribution of the distribution of the model must be large (Figure 4).\nTo estimate the distribution of the distribution of the distribution of the distribution of the model, we need to calculate the probability of the distribution of the distribution of the distribution of the model to estimate the chance of the distribution of the distribution of the distribution of the model to estimate the probability of the distribution of the distribution of the model to estimate the probability of the distribution of the distribution of the model to estimate the probability of the distribution of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the model to estimate the probability of the distribution of the", "histories": [["v1", "Fri, 9 Sep 2011 13:01:41 GMT  (129kb,D)", "http://arxiv.org/abs/1109.1990v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["edouard grave", "guillaume obozinski", "francis r bach"], "accepted": true, "id": "1109.1990"}, "pdf": {"name": "1109.1990.pdf", "metadata": {"source": "CRF", "title": "Trace Lasso: a trace norm regularization for correlated designs", "authors": ["Edouard Grave", "Guillaume Obozinski"], "emails": ["edouard.grave@inria.fr", "guillaume.obozinski@inria.fr", "francis.bach@inria.fr"], "sections": [{"heading": "1 Introduction", "text": "The concept of parsimony is central in many scientific domains. In the context of statistics, signal processing or machine learning, it takes the form of variable or feature selection problems, and is commonly used in two situations: first, to make the model or the prediction more interpretable or cheaper to use, i.e., even if the underlying problem does not admit sparse solutions, one looks for the best sparse approximation. Second, sparsity can also be used given prior knowledge that the model should be sparse. Many methods have been designed to learn sparse models, namely methods based on combinatorial optimization [1, 2], Bayesian inference [3] or convex optimization [4, 5].\nIn this paper, we focus on the regularization by sparsity-inducing norms. The simplest example of such norms is the `1-norm, leading to the Lasso, when used within a least-squares framework. In recent years, a large body of work has shown that the Lasso was performing optimally in highdimensional low-correlation settings, both in terms of prediction [6], estimation of parameters or estimation of supports [7, 8]. However, most data exhibit strong correlations, with various correlation structures, such as clusters (i.e., close to block-diagonal covariance matrices) or sparse graphs, such as for example problems involving sequences (in which case, the covariance matrix is close to a Toeplitz matrix [9]). In these situations, the Lasso is known to have stability problems: although its predictive performance is not disastrous, the selected predictor may vary a lot (typically, given two correlated variables, the Lasso will only select one of the two, at random).\nSeveral remedies have been proposed to this instability. First, the elastic net [10] adds a strongly convex penalty term (the squared `2-norm) that will stabilize selection (typically, given two correlated variables, the elastic net will select the two variables). However, it is blind to the exact correlation structure, and while strong convexity is required for some variables, it is not for other variables. Another solution is to consider the group Lasso, which will divide the predictors into groups and penalize the sum of the `2-norm of these groups [11]. This is known to accomodate strong correlations within groups [12]; however it requires to know the group in advance, which is not always possible. A third line of research has focused on sampling-based techniques [13, 14, 15].\nar X\niv :1\n10 9.\n19 90\nv1 [\ncs .L\nG ]\n9 S\nep 2\nAn ideal regularizer should thus be adapted to the design (like the group Lasso), but without requiring human intervention (like the elastic net); it should thus add strong convexity only where needed, and not modifying variables where things behave correctly. In this paper, we propose a new norm towards this end.\nMore precisely we make the following contributions:\n\u2022 We propose in Section 2 a new norm based on the trace norm (a.k.a. nuclear norm) that interpolates between the `1-norm and the `2-norm depending on correlations.\n\u2022 We show that there is a unique minimum when penalizing with this norm in Section 2.2.\n\u2022 We provide optimization algorithms based on reweighted least-squares in Section 3.\n\u2022 We study the second-order expansion around independence and relate to existing work on including correlations in Section 4.\n\u2022 We perform synthetic experiments in Section 5, where we show that the trace Lasso outperforms existing norms in strong-correlation regimes.\nNotations. Let M \u2208 Rn\u00d7p. The columns of M are noted using superscript, i.e., M(i) denotes the i-th column, while the rows are noted using subscript, i.e., Mi denotes the i-th row. For M \u2208 Rp\u00d7p, diag(M) \u2208 Rp is the diagonal of the matrix M, while for u \u2208 Rp, Diag(u) \u2208 Rp\u00d7p is the diagonal matrix whose diagonal elements are the ui. Let S be a subset of {1, ..., p}, then uS is the vector u restricted to the support S, with 0 outside the support S. We denote by Sp the set of symmetric matrices of size p. We will use various matrix norms, here are the notations we use:\n\u2022 \u2016M\u2016\u2217 is the trace norm, i.e., the sum of the singular values of the matrix M,\n\u2022 \u2016M\u2016op is the operator norm, i.e., the maximum singular value of the matrix M, \u2022 \u2016M\u2016F is the Frobenius norm, i.e., the `2-norm of the singular values, which is also equal to\u221a tr(M>M), \u2022 \u2016M\u20162,1 is the sum of the `2-norm of the columns of M: \u2016M\u20162,1 = p\u2211 i=1 \u2016M(i)\u20162."}, {"heading": "2 Definition and properties of the trace Lasso", "text": "We consider the problem of predicting y \u2208 R, given a vector x \u2208 Rp, assuming a linear model\ny = w>x + \u03b5,\nwhere \u03b5 is (Gaussian) noise with mean 0 and variance \u03c32. Given a training set X = (x1, ...,xn) > \u2208 Rn\u00d7p and y = (y1, ..., yn) > \u2208 Rn, a widely used method to estimate the parameter vector w is the penalized empirical risk minimization\nw\u0302 \u2208 argmin w\n1\nn n\u2211 i=1 `(yi,w >xi) + \u03bbf(w), (1)\nwhere ` is a loss function used to measure the error we make by predicting w>xi instead of yi, while f is a regularization term used to penalize complex models. This second term helps avoiding overfitting, especially in the case where we have many more parameters than observation, i.e., n p."}, {"heading": "2.1 Related work", "text": "We will now present some classical penalty functions for linear models which are widely used in the machine learning and statistics community. The first one, known as Tikhonov regularization [16] or ridge regression [17], is the squared `2-norm. When used with the square loss, estimating the parameter vector w is done by solving a linear system. One of the main drawbacks of this penalty function is the fact that it does not perform variable selection and thus does not behave well in sparse high-dimensional settings.\nHence, it is natural to penalize linear models by the number of variables used by the model. Unfortunately, this criterion, sometimes denoted by \u2016 \u00b7 \u20160 (`0-penalty), is not convex and solving the problem in Eq. (1) is generally NP-hard [18]. Thus, a convex relaxation for this problem was introduced, replacing the size of the selected subset by the `1-norm of w. This estimator is known as the Lasso [4] in the statistics community and basis pursuit [5] in signal processing. It was later shown that under some assumptions, the two problems were in fact equivalent (see for example [19] and references therein).\nWhen two predictors are highly correlated, the Lasso has a very unstable behavior: it may only select the variable that is the most correlated with the residual. On the other hand, the Tikhonov regularization tends to shrink coefficients of correlated variables together, leading to a very stable behavior. In order to get the best of both worlds, stability and variable selection, Zou and Hastie introduced the elastic net [10], which is the sum of the `1-norm and squared `2-norm. Unfortunately, this estimator needs two regularization parameters and is not adaptive to the precise correlation structure of the data. Some authors also proposed to use pairwise correlations between predictors to interpolate more adaptively between the `1-norm and squared `2-norm, by introducing the pairwise elastic net [20] (see comparisons with our approach in Section 5).\nFinally, when one has more knowledge about the data, for example clusters of variables that should be selected together, one can use the group Lasso [11]. Given a partition (Si) of the set of variables, it is defined as the sum of the `2-norms of the restricted vectors wSi :\n\u2016w\u2016GL = k\u2211 i=1 \u2016wSi\u20162.\nThe effect of this penalty function is to introduce sparsity at the group level: variables in a group are selected altogether. One of the main drawback of this method, which is also sometimes one of its quality, is the fact that one needs to know the partition of the variables, and so one needs to have a good knowledge of the data."}, {"heading": "2.2 The ridge, the Lasso and the trace Lasso", "text": "In this section, we show that Tikhonov regularization and the Lasso penalty can be viewed as norms of the matrix X Diag(w). We then introduce a new norm involving this matrix.\nThe solution of empirical risk minimization penalized by the `1-norm or `2-norm is not equivariant by rescaling of the predictors X(i), so it is common to normalize the predictors. When normalizing the predictors X(i), and penalizing by Tikhonov regularization or by the Lasso, people are implicitly using a regularization term that depends on the data or design matrix X. In fact, there is an equivalence between normalizing the predictors and not normalizing them, using the two following reweighted `2 and `1-norms instead of the Tikhonov regularization and the Lasso:\n\u2016w\u201622 = p\u2211 i=1 \u2016X(i)\u201622 w2i and \u2016w\u20161 = p\u2211 i=1 \u2016X(i)\u20162 |wi|. (2)\nThese two norms can be expressed using the matrix X Diag(w):\n\u2016w\u20162 = \u2016X Diag(w)\u2016F and \u2016w\u20161 = \u2016X Diag(w)\u20162,1,\nand a natural question arises: are there other relevant choices of functions or matrix norms? A classical measure of the complexity of a model is the number of predictors used by this model,\nwhich is equal to the size of the support of w. This penalty being non-convex, people use its convex relaxation, which is the `1-norm, leading to the Lasso.\nHere, we propose a different measure of complexity which can be shown to be more adapted in model selection settings [21]: the dimension of the subspace spanned by the selected predictors. This is equal to the rank of the selected predictors, or also to the rank of the matrix X Diag(w). As for the size of the support, this function is non-convex, and we propose to replace it by a convex surrogate, the trace norm, leading to the following penalty that we call \u201ctrace Lasso\u201d:\n\u2126(w) = \u2016X Diag(w)\u2016\u2217.\nThe trace Lasso has some interesting properties: if all the predictors are orthogonal, then, it is equal to the `1-norm. Indeed, we have the decomposition:\nX Diag(w) = p\u2211 i=1 ( \u2016X(i)\u20162wi ) X(i) \u2016X(i)\u20162 e>i ,\nwhere ei are the vectors of the canonical basis. Since the predictors are orthogonal and the ei are orthogonal too, this gives the singular value decomposition of X Diag(w) and we get\n\u2016X Diag(w)\u2016\u2217 = p\u2211 i=1 \u2016X(i)\u20162|wi| = \u2016X Diag(w)\u20162,1.\nOn the other hand, if all the predictors are equal to X(1), then\nX Diag(w) = X(1)w>,\nand we get \u2016X Diag(w)\u2016\u2217 = \u2016X(1)\u20162\u2016w\u20162 = \u2016X Diag(w)\u2016F , which is equivalent to the Tikhonov regularization. Thus when two predictors are strongly correlated, our norm will behave like the Tikhonov regularization, while for almost uncorrelated predictors, it will behave like the Lasso.\nAlways having a unique minimum is an important property for a statistical estimator, as it is a first step towards stability. The trace Lasso, by adding strong convexity exactly in the direction of highly correlated covariates, always has a unique minimum, and is much more stable than the Lasso.\nProposition 1. If the loss function ` is strongly convex with respect to its second argument, then the solution of the empirical risk minimization penalized by the trace Lasso, i.e., Eq. (1), is unique.\nThe technical proof of this proposition is given in appendix B, and consists of showing that in the flat directions of the loss function, the trace Lasso is strongly convex."}, {"heading": "2.3 A new family of penalty functions", "text": "In this section, we introduce a new family of penalties, inspired by the trace Lasso, allowing us to write the `1-norm, the `2-norm and the newly introduced trace Lasso as special cases. In fact, we note that \u2016Diag(w)\u2016\u2217 = \u2016w\u20161 and \u2016p\u22121/21>Diag(w)\u2016\u2217 = \u2016w>\u2016\u2217 = \u2016w\u20162. In other words, we can express the `1 and `2-norms of w using the trace norm of a given matrix times the matrix Diag(w). A natural question to ask is: what happens when using a matrix P other than the identity or the line vector p\u22121/21>, and what are good choices of such matrices? Therefore, we introduce the following family of penalty functions:\nDefinition 1. Let P \u2208 Rk\u00d7p, all of its columns having unit norm. We introduce the norm \u2126P as\n\u2126P(w) = \u2016P Diag(w)\u2016\u2217.\nProof. The positive homogeneity and triangle inequality are direct consequences of the linearity of w 7\u2192 P Diag(w) and the fact that \u2016 \u00b7 \u2016\u2217 is a norm. Since all the columns of P are not equal to zero, we have P Diag(w) = 0\u21d4 w = 0, and so, \u2126P separates points and is a norm.\nAs stated before, the `1 and `2-norms are special cases of the family of norms we just introduced. Another important penalty that can be expressed as a special case is the group Lasso, with nonoverlapping groups. Given a partition (Sj) of the set {1, ..., p}, the group Lasso is defined by\n\u2016w\u2016GL = \u2211 Sj \u2016wSj\u20162.\nWe define the matrix PGL by\nPGLij =\n{ 1/ \u221a |Sk| if i and j are in the same group Sk,\n0 otherwise.\nThen,\nPGL Diag(w) = \u2211 Sj 1Sj\u221a |Sj | w>Sj . (3)\nUsing the fact that (Sj) is a partition of {1, ..., p}, the vectors 1Sj are orthogonal and so are the vectors wSj . Hence, after normalizing the vectors, Eq. (3) gives a singular value decomposition of PGL Diag(w) and so the group Lasso penalty can be expressed as a special case of our family of norms:\n\u2016PGL Diag(w)\u2016\u2217 = \u2211 Sj \u2016wSj\u20162 = \u2016w\u2016GL.\nIn the following proposition, we show that our norm only depends on the value of P>P. This is an important property for the trace Lasso, where P = X, since it underlies the fact that this penalty only depends on the correlation matrix X>X of the covariates.\nProposition 2. Let P \u2208 Rk\u00d7p, all of its columns having unit norm. We have\n\u2126P(w) = \u2016(P>P)1/2 Diag(w)\u2016\u2217.\nWe plot the unit ball of our norm for the following value of P>P (see figure (1)): 1 0.9 0.10.9 1 0.1 0.1 0.1 1   1 0.7 0.490.7 1 0.7 0.49 0.7 1   1 1 01 1 0 0 0 1  We can lower bound and upper bound our norms by the `2-norm and `1-norm respectively. This shows that, as for the elastic net, our norms interpolate between the `1-norm and the `2- norm. But the main difference between the elastic net and our norms is the fact that our norms are adaptive, and require a single regularization parameter to tune. In particular for the trace\nLasso, when two covariates are strongly correlated, it will be close to the `2-norm, while when two covariates are almost uncorrelated, it will behave like the `1-norm. This is a behavior close to the one of the pairwise elastic net [20].\nProposition 3. Let P \u2208 Rk\u00d7p, all of its columns having unit norm. We have\n\u2016w\u20162 \u2264 \u2126P(w) \u2264 \u2016w\u20161."}, {"heading": "2.4 Dual norm", "text": "The dual norm is an important quantity for both optimization and theoretical analysis of the estimator. Unfortunately, we are not able in general to obtain a closed form expression of the dual norm for the family of norms we just introduced. However we can obtain a bound, which is exact for some special cases:\nProposition 4. The dual norm, defined by \u2126\u2217P(u) = max \u2126P(v)\u22641 u>v, can be bounded by:\n\u2126\u2217P(u) \u2264 \u2016P Diag(u)\u2016op.\nProof. Using the fact that diag(P>P) = 1, we have\nu>v = tr ( Diag(u)P>P Diag(v) ) \u2264 \u2016P Diag(u)\u2016op\u2016P Diag(v)\u2016\u2217,\nwhere the inequality comes from the fact that the operator norm \u2016 \u00b7 \u2016op is the dual norm of the trace norm. The definition of the dual norm then gives the result.\nAs a corollary, we can bound the dual norm by a constant times the `\u221e-norm:\n\u2126\u2217P(u) \u2264 \u2016P Diag(u)\u2016op \u2264 \u2016P\u2016op\u2016Diag(u)\u2016op = \u2016P\u2016op\u2016u\u2016\u221e.\nUsing proposition (3), we also have the inequality \u2126\u2217P(u) \u2265 \u2016u\u2016\u221e."}, {"heading": "3 Optimization algorithm", "text": "In this section, we introduce an algorithm to estimate the parameter vector w when the loss function is equal to the square loss: `(y,w>x) = 12 (y\u2212w\n>x)2 and the penalty is the trace Lasso. It is straightforward to extend this algorithm to the family of norms indexed by P. The problem we consider is\nmin w\n1 2 \u2016y \u2212Xw\u201622 + \u03bb\u2016X Diag(w)\u2016\u2217.\nWe could optimize this cost function by subgradient descent, but this is quite inefficient: computing the subgradient of the trace Lasso is expensive and the rate of convergence of subgradient descent is quite slow. Instead, we consider an iteratively reweighted least-squares method. First, we need to introduce a well-known variational formulation for the trace norm [22]:\nProposition 5. Let M \u2208 Rn\u00d7p. The trace norm of M is equal to:\n\u2016M\u2016\u2217 = 1\n2 inf S 0\ntr ( M>S\u22121M ) + tr (S) ,\nand the infimum is attained for S = ( MM> )1/2 .\nUsing this proposition, we can reformulate the previous optimization problem as\nmin w inf S 0\n1 2 \u2016y \u2212Xw\u201622 + \u03bb 2 w>Diag\n( diag(X>S\u22121X) ) w + \u03bb\n2 tr(S).\nThis problem is jointly convex in (w, S) [23]. In order to optimize this objective function by alternating the minimization over w and S, we need to add a term \u03bb\u00b5i2 tr(S\n\u22121). Otherwise, the infimum over S could be attained at a non invertible S, leading to a non convergent algorithm.\nThe infimum over S is then attained for S = ( X Diag(w)2X> + \u00b5iI )1/2 .\nOptimizing over w is a least-squares problem penalized by a reweighted `2-norm equal to w>Dw, where D = Diag ( diag(X>S\u22121X) ) . It is equivalent to solving the linear system\n(X>X + \u03bbD)w = X>y.\nThis can be done efficiently by using a conjugate gradient method. Since the cost of multiplying (X>X +\u03bbD) by a vector is O(np), solving the system has a complexity of O(knp), where k \u2264 p is the number of iterations needed to converge. Using warm restarts, k can be much smaller than p, since the linear system we are solving does not change a lot from an iteration to another. Below we summarize the algorithm:\nIterative algorithm for estimating w\nInput: the design matrix X, the initial guess w0, number of iteration N , sequence \u00b5i. For i = 1...N :\n\u2022 Compute the eigenvalue decomposition U Diag(sk)U> of X Diag(wi\u22121)2X>. \u2022 Set D = Diag(diag(X>S\u22121X)), where S\u22121 = U Diag(1/ \u221a sk + \u00b5i)U >.\n\u2022 Set wi by solving the system (X>X + \u03bbD)w = X>y.\nFor the sequence \u00b5i, we use a decreasing sequence converging to ten times the machine precision."}, {"heading": "3.1 Choice of \u03bb", "text": "We now give a method to choose the regularization path. In fact, we know that the vector 0 is solution if and only if \u03bb \u2265 \u2126\u2217(X>y) [24]. Thus, we need to start the path at \u03bb = \u2126\u2217(X>y), corresponding to the empty solution 0, and then decrease \u03bb. Using the inequalities on the dual norm we obtained in the previous section, we get\n\u2016X>y\u2016\u221e \u2264 \u2126\u2217(X>y) \u2264 \u2016X\u2016op\u2016X>y\u2016\u221e.\nTherefore, starting the path at \u03bb = \u2016X\u2016op\u2016X>y\u2016\u221e is a good choice."}, {"heading": "4 Approximation around the Lasso", "text": "In this section, we compute the second order approximation of our norm around the special case corresponding to the Lasso. We recall that when P = I \u2208 Rp\u00d7p, our norm is equal to the `1-norm. We add a small perturbation \u2206 \u2208 Sp to the identity matrix, and using Prop. 6 of the appendix A, we obtain the following second order approximation:\n\u2016(I + \u2206) Diag(w)\u2016\u2217 = \u2016w\u20161 + diag(\u2206)>|w|+\u2211 |wi|>0 \u2211 |wj |>0 (\u2206ji|wi| \u2212\u2206ij |wj |)2 4(|wi|+ |wj |) + \u2211 |wi|=0 \u2211 |wj |>0 (\u2206ij |wj |)2 2|wj | + o(\u2016\u2206\u20162).\nWe can rewrite this approximation as\n\u2016(I + \u2206) Diag(w)\u2016\u2217 = \u2016w\u20161 + diag(\u2206)>|w|+ \u2211 i,j \u22062ij(|wi| \u2212 |wj |)2 4(|wi|+ |wj |) + o(\u2016\u2206\u20162),\nusing a slight abuse of notation, considering that the last term is equal to 0 when wi = wj = 0. The second order term is quite interesting: it shows that when two covariates are correlated, the effect of the trace Lasso is to shrink the corresponding coefficients toward each other. Another interesting remark is the fact that this term is very similar to pairwise elastic net penalties, which are of the form |w|>P|w|, where Pij is a decreasing function of \u2206ij ."}, {"heading": "5 Experiments", "text": "In this section, we perform synthetic experiments to illustrate the behavior of the trace Lasso and other classical penalties when there are highly correlated covariates in the design matrix. For all experiments, we have p = 1024 covariates and n = 256 observations. The support S of w is equal to {1, ..., k}, where k is the size of the support. For i in the support of w, we have wi = 2 (bi \u2212 1/2), where each bi is independently drawn from a uniform distribution on [0, 1]. The observations xi are drawn from a multivariate Gaussian with mean 0 and covariance matrix \u03a3. For the first experiment, \u03a3 is set to the identity, for the second experiment, \u03a3 is block diagonal with blocks equal to 0.2I + 0.811> corresponding to clusters of eight variables, finally for the third experiment, we set \u03a3ij = 0.95\n|i\u2212j|, corresponding to a Toeplitz design. For each method, we choose the best \u03bb for the estimation error, which is reported.\nOverall all methods behave similarly in the noiseless and the noisy settings, hence we only report results for the noisy setting. In all three graphs of Figure 2, we observe behaviors that are typical of Lasso, ridge and elastic net: the Lasso performs very well on sparse models, but its performance is rather poor for denser models, almost as poor as the ridge regression. The elastic net offers the best of both worlds since its two parameters allow it to interpolate adaptively between the Lasso and the ridge. In experiment 1, since the variables are uncorrelated, there is no reason to couple their selection. This suggests that the Lasso should be the most appropriate convex regularization. The trace Lasso approaches the Lasso as n goes to infinity, but the weak coupling induced by empirical correlations is sufficient to slightly decrease its performance compared to that of the Lasso. By contrast, in experiments 2 and 3, the trace Lasso outperforms other methods (including the pairwise elastic net) since variables that should be selected together are indeed correlated. As for the penalized elastic net, since it takes into account the correlations between variables it is not surprising that in experiment 2 and 3 it performs better than methods that do not. We do not have a compelling explanation for its superior performance in experiment 1."}, {"heading": "6 Conclusion", "text": "We introduce a new penalty function, the trace Lasso, which takes advantage of the correlation between covariates to add strong convexity exactly in the directions where needed, unlike the elastic net for example, which blindly adds a squared `2-norm term in every directions. We show on synthetic data that this adaptive behavior leads to better estimation performance. In the future, we want to show that if a dedicated norm using prior knowledge such as the group Lasso can be used, the trace Lasso will behave similarly and its performance will not degrade too much, providing theoretical guarantees to such adaptivity. Finally, we will seek applications of this estimator in inverse problems such as deblurring, where the design matrix exhibits strong correlation structure."}, {"heading": "Acknowledgements", "text": "This paper was partially supported by the European Research Council (SIERRA Project ERC239993)."}, {"heading": "A Perturbation of the trace norm", "text": "We follow the technique used in [25] to obtain an approximation of the trace norm.\nA.1 Jordan-Wielandt matrices\nLet M \u2208 Rn\u00d7p of rank r. We note s1 \u2265 s2 \u2265 ... \u2265 sr > 0, the strictly positive singular values of M and ui, vi the associated left and right singular vectors. We introduce the Jordan-Wielandt matrix\nM\u0303 =\n( 0 M\nM> 0\n) \u2208 R(n+p)\u00d7(n+p).\nThe singular values of M and the eigenvalues of M\u0303 are related: M\u0303 has eigenvalues si and s\u2212i = \u2212si associated to eigenvectors\nwi = 1\u221a 2 ( ui vi ) and w\u2212i = 1\u221a 2 ( ui \u2212vi ) .\nThe remaining eigenvalues of M\u0303 are equal to 0 and are associated to eigenvectors of the form\nw = 1\u221a 2 ( u v ) and w = 1\u221a 2 ( u \u2212v ) ,\nwhere \u2200 i \u2208 {1, ..., r}, u>ui = v>vi = 0.\nA.2 Cauchy residue formula\nLet C be a closed curve that does not go through the eigenvalues of M\u0303. We define\n\u03a0C(M\u0303) = 1\n2i\u03c0 \u222b C \u03bb(\u03bbI\u2212 M\u0303)\u22121d\u03bb.\nWe have\n\u03a0C(M\u0303) = 1\n2i\u03c0 \u222e \u2211 j \u03bb \u03bb\u2212 sj wjw > j d\u03bb\n= 1\n2i\u03c0 \u222e \u2211 j ( 1 + sj \u03bb\u2212 sj ) wjw > j d\u03bb\n= \u2211 sj\u2208C sjwjw > j .\nA.3 Perturbation analysis\nLet \u2206 \u2208 Rn\u00d7p be a perturbation matrix such that \u2016\u2206\u2016op < sr/4, and let C be a closed curve around the r largest eigenvalues of M\u0303 and M\u0303 + \u2206\u0303. We can study the perturbation of the strictly positive singular values of M by computing the trace of \u03a0C(M\u0303 + \u2206\u0303) \u2212 \u03a0C(M\u0303). Using the fact that (\u03bbI\u2212 M\u0303\u2212 \u2206\u0303)\u22121 = (\u03bbI\u2212 M\u0303)\u22121 + (\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303\u2212 \u2206\u0303)\u22121, we have\n\u03a0C(M\u0303 + \u2206\u0303)\u2212\u03a0C(M\u0303) = 1\n2i\u03c0\n\u222e \u03bb(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303)\u22121d\u03bb\n+ 1\n2i\u03c0\n\u222e \u03bb(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303)\u22121d\u03bb\n+ 1\n2i\u03c0\n\u222e \u03bb(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303\u2212 \u2206\u0303)\u22121d\u03bb.\nWe note A and B the first two terms of the right hand side of this equation. We have\ntr(A) = \u2211 j,k tr(wjw > j \u2206\u0303wkw > k ) 1 2i\u03c0 \u222e C\n\u03bbd\u03bb\n(\u03bb\u2212 sj)(\u03bb\u2212 sk)\n= \u2211 j tr(w>j \u2206\u0303wj) 1 2i\u03c0 \u222e C \u03bbd\u03bb (\u03bb\u2212 sj)2\n= \u2211 j tr(w>j \u2206\u0303wj)\n= \u2211 j tr(u>j \u2206vj),\nand\ntr(B) = \u2211 j,k,l tr(wjw > j \u2206\u0303wkw > k \u2206\u0303wlw > l ) 1 2i\u03c0 \u222e C\n\u03bbd\u03bb\n(\u03bb\u2212 sj)(\u03bb\u2212 sk)(\u03bb\u2212 sl)\n= \u2211 j,k tr(wj\u2206\u0303wkwk\u2206\u0303wj) 1 2i\u03c0 \u222e C\n\u03bbd\u03bb\n(\u03bb\u2212 sj)2(\u03bb\u2212 sk) .\nIf sj = sk, the integral is nul. Otherwise, we have\n\u03bb\n(\u03bb\u2212 sj)2(\u03bb\u2212 sk) =\na\n\u03bb\u2212 sj +\nb\n\u03bb\u2212 sk +\nc\n(\u03bb\u2212 sj)2 ,\nwhere\na = \u2212sk\n(sk \u2212 sj)2 ,\nb = sk\n(sk \u2212 sj)2 ,\nc = sj\nsj \u2212 sk .\nTherefore, if sj and sk are both inside or outside the interior of C, the integral is equal to zero. So\ntr(B) = \u2211 sj>0 \u2211 sk\u22640 \u2212sk(w>j \u2206\u0303wk)2 (sj \u2212 sk)2 + \u2211 sj\u22640 \u2211 sk>0 sk(w > j \u2206\u0303wk) 2 (sj \u2212 sk)2\n= \u2211 sj>0 \u2211 sk>0 sk(w > j \u2206\u0303w\u2212k) 2 (sj + sk)2 + \u2211 sj>0 \u2211 sk>0 sk(w > \u2212j\u2206\u0303wk) 2 (sj + sk)2 + \u2211 sj=0 \u2211 sk>0 (w>j \u2206\u0303wk) 2 sk\n= \u2211 sj>0 \u2211 sk>0 (w>\u2212j\u2206\u0303wk) 2 sj + sk + \u2211 sj=0 \u2211 sk>0 (w>j \u2206\u0303wk) 2 sk .\nFor sj > 0 and sk > 0, we have\nw>\u2212j\u2206\u0303wk = 1\n2\n( u>j \u2206vk \u2212 u>k \u2206vj ) ,\nand for sj = 0 and sk > 0, we have\nw>j \u2206\u0303wk = 1\n2\n( \u00b1u>k \u2206vj + u>j \u2206vk ) .\nSo\ntr(B) = \u2211 sj>0 \u2211 sk>0 (u>j \u2206vk \u2212 u>k \u2206vj)2 4(sj + sk) + \u2211 sj=0 \u2211 sk>0 (u>k \u2206vj) 2 + (u>j \u2206vk) 2 2sk .\nNow, let C0 be the circle of center 0 and radius sr/2. We can study the perturbation of the singular values of M equal to zero by computing the trace norm of \u03a0C0(M\u0303 + \u2206\u0303) \u2212 \u03a0C0(M\u0303). We have\n\u03a0C0(M\u0303 + \u2206\u0303)\u2212\u03a0C0(M\u0303) = 1\n2i\u03c0 \u222e C0 \u03bb(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303)\u22121d\u03bb\n+ 1\n2i\u03c0 \u222e C0 \u03bb(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303)\u22121d\u03bb\n+ 1\n2i\u03c0 \u222e C0 \u03bb(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303)\u22121\u2206\u0303(\u03bbI\u2212 M\u0303\u2212 \u2206\u0303)\u22121d\u03bb.\nThen, if we note the first integral C and the second one D, we get C = \u2211 j,k wjw > j \u2206\u0303wkw > k 1 2i\u03c0 \u222e C0 \u03bbd\u03bb (\u03bb\u2212 sj)(\u03bb\u2212 sk) .\nIf both sj and sk are outside int(C0), then the integral is equal to zero. If one of them is inside, say sj , then sj = 0 and the integral is equal to\u222e\nC0\nd\u03bb\n\u03bb\u2212 sk\nThen this integral is non nul if and only if sk is also inside int(C0). Thus C = \u2211 j,k wjw > j \u2206\u0303wkw > k 1sj\u2208int(C0)1sk\u2208int(C0)\n= \u2211 sj=0 \u2211 sk=0 wjw > j \u2206\u0303wkw > k = W0W > 0 \u2206\u0303W0W > 0 ,\nwhere W0 are the eigenvectors associated to the eigenvalue 0. We have D = \u2211 j,k,l wjw > j \u2206\u0303wkw > k \u2206\u0303wlw > l 1 2i\u03c0 \u222e C0 \u03bbd\u03bb (\u03bb\u2212 sj)(\u03bb\u2212 sk)(\u03bb\u2212 sl) .\nThe integral is not equal to zero if and only if exactly one eigenvalue, say si, is outside int(C0). The integral is then equal to \u22121/si. Thus\nD = \u2212W0W>0 \u2206\u0303W0W>0 \u2206\u0303WS\u22121W> \u2212WS\u22121W>\u2206\u0303W0W>0 \u2206\u0303W0W>0 \u2212W0W>0 \u2206\u0303WS\u22121W>\u2206\u0303W0W>0 ,\nwhere S = Diag(\u2212s, s). Finally, putting everything together, we get\nProposition 6. Let M = U Diag(s)V> \u2208 Rn\u00d7p, the singular value decomposition of M, with U \u2208 Rn\u00d7r, V \u2208 Rp\u00d7r. Let \u2206 \u2208 Rn\u00d7p. We have\n\u2016M + \u2206\u2016\u2217 = \u2016M\u2016\u2217 + \u2016Q\u2016\u2217 + tr(VU>\u2206)+\u2211 sj>0 \u2211 sk>0 (u>j \u2206vk \u2212 u>k \u2206vj)2 4(sj + sk) + \u2211 sj=0 \u2211 sk>0 (u>k \u2206v0j) 2 + (u>0j\u2206vk) 2 2sk + o(\u2016\u2206\u20162),\nwhere\nQ = U>0 \u2206V0 \u2212 U>0 \u2206V0V>0 \u2206>U Diag(s)\u22121\n\u2212 Diag(s)\u22121V>\u2206>U0U>0 \u2206V0 \u2212 U>0 \u2206V Diag(s)\u22121U>\u2206V0."}, {"heading": "B Proof of proposition 1", "text": "In this section, we prove that if the loss function is strongly convex with respect to its second argument, then the solution of the penalized empirical risk minimization is unique.\nLet w\u0302 \u2208 argminw \u2211n i=1 `(yi,w\n>xi)+\u03bb\u2016X Diag(w)\u2016\u2217. If w\u0302 is in the nullspace of X, then w\u0302 = 0 and the minimum is unique. From now on, we suppose that the minima are not in the nullspace of X.\nLet u,v \u2208 argminw \u2211n i=1 `(yi,w\n>xi) + \u03bb\u2016X Diag(w)\u2016\u2217 and \u03b4 = v \u2212 u. By convexity of the objective function, all the w = u+t\u03b4, for t \u2208]0, 1[ are also optimal solutions, and so, we can choose an optimal solution w such that wi 6= 0 for all i in the support of \u03b4. Because the loss function is strongly convex outside the nullspace of X, \u03b4 is in the nullspace of X.\nLet X Diag(w) = U Diag(s)V> be the SVD of X Diag(w). We have the following development around w:\n\u2016X Diag(w + t\u03b4)\u2016\u2217 = \u2016X Diag(w)\u2016\u2217 + tr(Diag(t\u03b4)X>UV>)+\u2211 si>0 \u2211 sj>0 tr(Diag(t\u03b4)X>(uiv > j \u2212 ujv>i ))2 4(si + sj) + \u2211 si>0 \u2211 sj=0 tr(Diag(t\u03b4)X>uiv > j ) 2 2si + o(t2).\nWe note S the support of w. Using the fact that the support of \u03b4 is included in S, we have X Diag(t\u03b4) = X Diag(w) Diag(t\u03b3), where \u03b3i =\n\u03b4i wi for i \u2208 S and 0 otherwise. Then:\n\u2016X Diag(w + t\u03b4)\u2016\u2217 = \u2016X Diag(w)\u2016\u2217 + t\u03b3> diag(V Diag(s)V>)+\u2211 si>0 \u2211 sj>0 t2 tr ( (si \u2212 sj) Diag(\u03b3)viv>j )2 4(si + sj) + \u2211 si>0 \u2211 sj=0 t2 tr ( si Diag(\u03b3)viv > j )2 2si + o(t2).\nFor small t, w + t\u03b4 is also a minimum, and therefore, we have: \u2200 si > 0, sj > 0, (si \u2212 sj) tr ( Diag(\u03b3)viv > j ) = 0, (4)\n\u2200 si > 0, sj = 0, tr ( Diag(\u03b3)viv > j ) = 0. (5)\nThis could be summarized as\n\u2200 si 6= sj , v>i (Diag(\u03b3)vj) = 0. (6)\nThis means that the eigenspaces of Diag(w)X>X Diag(w) are stable by the matrix Diag(\u03b3). Therefore, Diag(w)X>X Diag(w) and Diag(\u03b3) are simultaneously diagonalizable and so, they commute. Therefore:\n\u2200 i, j \u2208 S, \u03c3ij\u03b3i = \u03c3ij\u03b3j (7)\nwhere \u03c3ij = [X >X]ij . We define a partition (Sk) of S, such that i and j are in the same set Sk if there exists a path i = a1, ..., am = j such that \u03c3an,an+1 6= 0 for all n \u2208 {1, ...,m\u2212 1}. Then, using equation (7), \u03b3 is constant on each Sk. \u03b4 being in the nullspace of X, we have:\n0 = \u03b4>X>X\u03b4 (8) = \u2211 Sk \u2211 Sl \u03b4>SkX >X\u03b4Sl (9)\n= \u2211 Sk \u03b4>SkX >X\u03b4Sk (10)\n= \u2211 Sk \u2016X\u03b4Sk\u201622. (11)\nSo for all Si, X\u03b4Si = 0. Since a predictor Xi is orthogonal to all the predictors belonging to other groups defined by the partition (Sk), we can decompose the norm \u2126:\n\u2016X Diag(w)\u2016\u2217 = \u2211 Sk \u2016X Diag(wSk)\u2016\u2217. (12)\nWe recall that \u03b3 is constant on each Sk and so \u03b4Sk is colinear to wSi , by definition of \u03b3. If \u03b4Si is not equal to zero, this means that wSi , which is not equal to zero, is in the nullspace of X. Replacing wSi by 0 will not change the value of the data fitting term but it will strictly decreases the value of the norm \u2126. This is a contradiction with the optimality of w. Thus all the \u03b4Si are equal to zero and the minimum is unique."}, {"heading": "C Proof of proposition 3", "text": "For the first inequality, we have\n\u2016w\u20162 = \u2016P Diag(w)\u2016F \u2264 \u2016P Diag(w)\u2016\u2217.\nFor the second inequality, we have\n\u2016P Diag(w)\u2016\u2217 = max \u2016M\u2016op\u22641\ntr ( M>P Diag(w) ) = max \u2016M\u2016op\u22641 diag ( M>P )> w\n\u2264 max \u2016M\u2016op\u22641 p\u2211 i=1 |M(i)>P(i)| |wi| \u2264 \u2016w\u20161.\nThe first equality is the fact that the dual norm of the trace norm is the operator norm and the second inequality uses the fact that all matrices of operator norm smaller than one have columns of `2 norm smaller than one."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "<lb>Using the `1-norm to regularize the estimation of the parameter vector of a linear model<lb>leads to an unstable estimator when covariates are highly correlated. In this paper, we in-<lb>troduce a new penalty function which takes into account the correlation of the design matrix<lb>to stabilize the estimation. This norm, called the trace Lasso, uses the trace norm, which<lb>is a convex surrogate of the rank, of the selected covariates as the criterion of model com-<lb>plexity. We analyze the properties of our norm, describe an optimization algorithm based on<lb>reweighted least-squares, and illustrate the behavior of this norm on synthetic data, showing<lb>that it is more adapted to strong correlations than competing methods such as the elastic net.", "creator": "LaTeX with hyperref package"}}}