{"id": "1110.1394", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2011", "title": "Learning Sentence-internal Temporal Relations", "abstract": "In this paper we propose a data intensive approach for inferring sentence-internal temporal relations. Temporal inference is relevant for practical NLP applications which either extract or synthesize temporal information (e.g., if an item is a category that has an A and a B ). If we can define a temporal pattern in which a sentence is a category and a category and a category and a category, it will be possible to derive a temporal pattern that is the A and B-b respectively. We also demonstrate that this method can be used in other applications. We propose a system for inferring sentence-internal temporal relations which can be applied in other applications.", "histories": [["v1", "Thu, 6 Oct 2011 20:55:54 GMT  (827kb)", "http://arxiv.org/abs/1110.1394v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["m lapata", "a lascarides"], "accepted": true, "id": "1110.1394"}, "pdf": {"name": "1110.1394.pdf", "metadata": {"source": "CRF", "title": "Learning Sentence-internal Temporal Relations", "authors": ["Mirella Lapata", "Alex Lascarides"], "emails": ["MLAP@INF.ED.AC.UK", "ALEX@INF.ED.AC.UK"], "sections": [{"heading": null, "text": "lations. Our approach bypasses the need for manual coding by exploiting the presence of temporal markers like after, which overtly signal a temporal relation. Our experiments concentrate on two tasks relevant for applications which either extract or synthesise temporal information (e.g., summarisation, question answering). Our first task focuses on interpretation: given a subordinate clause and main clause, identify the temporal relation between them. The second is a fusion task: given two clauses and a temporal relation between them, decide which one contained the temporal marker (i.e., identify the subordinate and main clause). We compare and contrast several probabilistic models differing in their feature space, linguistic assumptions and data requirements. We evaluate performance against a gold standard corpus and also against human subjects performing the same tasks. The best model achieves 69.1% F-score in inferring the temporal relation between two clauses and 93.4% F-score in distinguishing the main vs. the subordinate clause, assuming that the temporal relation is known."}, {"heading": "1. Introduction", "text": "The computational treatment of temporal information has recently attracted much attention, in part because of its increasing importance for potential applications. In multidocument summarisation, for example, information that is to be included in the summary must be extracted from various documents and synthesised into a meaningful text. Knowledge about the temporal order of events is important for determining what content should be communicated (interpretation) and for correctly merging and presenting information in the summary (generation). Indeed, ignoring temporal relations in either the information extraction phase or the summary generation phase potentially results in a summary which is misleading with respect to the temporal information in the original documents. In question answering, one often seeks information about the temporal properties of events (e.g., When did X resign? ) or how events relate to each other (e.g., Did X resign before Y? ). An important first step towards the automatic handling of temporal phenomena is the analysis and identification of time expressions. Such expressions include absolute date or time specifications (e.g., October 19th, 2000 ), descriptions of intervals (e.g., thirty years), indexical expressions (e.g., last week ), etc. It is therefore not surprising that much previous work has focused on the recognition, interpretation, and normalisation of time expressions1 (Wilson, Mani, Sundheim, & Ferro,\n1. See also the Time Expression Recognition and Normalisation (TERN) evaluation exercise (http://timex2.mitre. org/tern.html).\nc 2005 AI Access Foundation. All rights reserved.\nLAPATA & LASCARIDES\n2001; Schilder & Habel, 2001; Wiebe, O\u2019Hara, O\u0308hrstro\u0308m Sandgren, & McKeever, 1998). Reasoning with time, however, goes beyond temporal expressions; it involves interpretation of the order of events in discourse, analysis of their temporal relations, and generally the ability to draw inferences over time elements. An additional challenge to this task poses the nature of temporal information itself which is often implicit (i.e., not overtly verbalised) and must be inferred using both linguistic and non-linguistic knowledge. Consider the examples in (1) taken from Katz and Arosio (2001). Native speakers can infer that John first met and then kissed the girl; that he left the party after kissing the girl and then walked home; and that the events of talking to her and asking her for her name temporally overlap (and occurred before he left the party).\n(1) a. John kissed the girl he met at a party. b. Leaving the party, John walked home. c. He remembered talking to her and asking her for her name.\nThe temporal relations just described are part of the interpretation of this text, even though there are no overt markers, such as after or while, signalling them. They are inferable from a variety of cues, including the order of the clauses, their compositional semantics (e.g., information about tense and aspect), the semantic relationships among the words in the clauses, and real world knowledge. In this paper we describe a data intensive approach that automatically captures information pertaining to the temporal relations among events like the ones illustrated in (1). A standard approach to this task would be to acquire a model of temporal relations from a corpus annotated with temporal information. Although efforts are underway to develop treebanks marked with temporal relations (Katz & Arosio, 2001) and devise annotation schemes that are suitable for coding temporal relations (Saur\u0131\u0301, Littman, Gaizauskas, Setzer, & Pustejovsky, 2004; Ferro, Mani, Sundheim, & Wilson, 2000; Setzer & Gaizauskas, 2001), the existing corpora are too small in size to be amenable to supervised machine learning techniques which normally require thousands of training examples. The TimeBank2 corpus, for example, contains a set of 186 news report documents annotated with the TimeML mark-up language for temporal events and expressions (see Section 2 for details). The corpus consists of 68.5K words in total. Contrast this with the Penntreebank, a corpus which is often used in many NLP tasks and contains approximately 1M words (i.e., it is 16 times larger than TimeBank). The annotation of temporal information is not only timeconsuming but also error prone. In particular, if there are n kinds of temporal relations, then the number of possible relations to annotate is a polynomial of factor n on the number of events in the text. Pustejovsky et al. (2003) found evidence that this annotation task is sufficiently complex that human annotators can realistically identify only a small number of the temporal relations that hold in reality; i.e., recall is compromised. In default of large volumes of data labelled with temporal information, we turn to unannotated texts which nevertheless contain expressions that overtly convey the information we want our models to learn. Although temporal relations are often underspecified, sometimes there are temporal markers, such as before, after and while, which make relations among events explicit:\n(2) a. Leonard Shane, 65 years old, held the post of president before William Shane, 37, was elected to it last year.\nb. The results were announced after the market closed. c. Investors in most markets sat out while awaiting the U.S. trade figures.\n2. Available from http://www.cs.brandeis.edu/\u02dcjamesp/arda/time/timebank.html\n2\nLEARNING TEMPORAL RELATIONS\nIt is precisely this type of data that we will exploit for making predictions about the temporal relationships among events in text. We will assess the feasibility of such an approach by initially focusing on sentence-internal temporal relations. We will obtain sentences like the ones shown in (2), where a main clause is connected to a subordinate clause with a temporal marker and we will develop a probabilistic framework where the temporal relations will be learnt by gathering informative features from the two clauses. In this paper we focus on two tasks, both of which are important for any NLP system requiring information extraction and text synthesis. The first task addresses the interpretation of temporal relations: given a main and a subordinate clause, identify the temporal marker which connected them. So for this task, our models view the marker from each sentence in the training corpus as the label to be learnt. In the test corpus the marker is removed and the models\u2019 task is to pick the most likely label\u2014or equivalently marker. Our second task concerns the generation of temporal relations. Non-extractive summarisers that produce sentences by fusing together sentence fragments (e.g., Barzilay, 2003) must be able to determine whether to include an overt temporal marker in the generated text, where the marker should be placed, and what lexical item should be used. Rather than attempting all three tasks at once, we focus on determining the appropriate ordering among a temporal marker and two clauses. We infer probabilistically which of the two clauses is introduced by the marker, and effectively learn to distinguish between main and subordinate clauses. In this case the main vs. subordinate clause are treated as labels. The test corpus consists of sentences with overtly marked temporal markers, however information regarding their position is removed. By the very nature of these tasks, our models focus exclusively on sentence-internal temporal relations. It is hoped that they can be used to infer temporal relations among events in data where overt temporal markers are absent (e.g., as in (1)), although this is beyond the scope of this paper. In attempting to infer temporal relations probabilistically, we consider different classes of models with varying degrees of faithfulness to linguistic theory. Our models differ along two dimensions: the employed feature space and the underlying independence assumptions. We compare and contrast models which utilise word-co-occurrences with models which exploit linguistically motivated features (such as verb classes, argument relations, and so on). Linguistic features typically allow our models to form generalisations over classes of words, thereby requiring less training data than word co-occurrence models. We also compare and contrast two kinds of models: one assumes that the properties of the two clauses are mutually independent; the other makes slightly more realistic assumptions about dependence. (Details of the models and features used are given in Sections 3 and 4.2). We furthermore explore the benefits of ensemble learning methods for the two tasks introduced above and show that improved performance can be achieved when different learners (modelling complementary knowledge sources) are combined. Our machine learning experiments are complemented by a study in which we investigate human performance on our two tasks, thereby assessing their feasibility and providing a ceiling on model performance. The next section gives an overview of previous work in the area of computing temporal information and discusses related work which utilises overt markers as a means for avoiding manual labelling of training data. Section 3 describes our probabilistic models and Section 4 discusses our features and the motivation behind their selection. Our experiments are presented in Sections 5\u20137. Section 8 offers some discussion and concluding remarks.\n3\nLAPATA & LASCARIDES"}, {"heading": "2. Related Work", "text": "Traditionally, methods for inferring temporal relations among events in discourse have utilised a semantics and inference-based approach. This involves complex reasoning over a variety of rich information sources, including representations of domain knowledge and detailed logical forms of the clauses (e.g., Dowty, 1986; Hwang & Schubert, 1992; Hobbs et al., 1993; Lascarides & Asher, 1993; Kamp & Reyle, 1993a; Kehler, 2002). This approach, while theoretically elegant, is impractical except for applications in very narrow domains for a number of reasons. First, grammars that produce detailed semantic representations inevitably lack linguistic coverage and are brittle in the face of natural data; similarly, the representations of domain knowledge can lack coverage. Secondly, the complex reasoning required with these rich information sources typically involves nonmonotonic inferences (e.g., Hobbs et al., 1993; Lascarides & Asher, 1993), which become intractable except for toy examples. Allen (1995), Hitzeman et al. (1995), and Han and Lavie (2004) propose more computationally tractable approaches to inferring temporal information from text, by hand-crafting algorithms which integrate shallow versions of the knowledge sources that are exploited in the above theoretical literature (e.g., Hobbs et al., 1993; Kamp & Reyle, 1993a). While this type of symbolic approach is promising, and overcomes some of the impracticalities of utilising full logical forms and complex reasoning over rich domain knowledge sources, it is not grounded in empirical evidence of the way the various linguistic features contribute to the temporal semantics of a discourse; nor are these algorithms evaluated against real data. Moreover, the approach is typically domain-dependent and robustness is compromised when porting to new domains or applications. Acquiring a model of temporal relations via machine learning over a training corpus promises to provide systems which are precise, robust and grounded in empirical evidence. A number of markup languages have recently emerged that can greatly facilitate annotation efforts in creating suitable corpora. A notable example is TimeML (Pustejovsky, Ingria, Sauri, Castano, Littman, Gaizauskas, & Setzer, 2004; see also the annotation scheme in Katz & Arosio, 2001), a metadata standard for expressing information about the temporal properties of events and temporal relations between them. The scheme can be used to annotate a variety of temporal expressions, including tensed verbs, adjectives and nominals that correspond to times, events or states. The type of temporal information that can be expressed on these various linguistic expressions includes the class of event, its tense, grammatical aspect, polarity (positive or negative), the time denoted (e.g., one can annotate yesterday as denoting the day before the document date), and temporal relations between pairs of eventualities and between events and times. TimeML\u2019s expressive capabilities are illustrated in the TimeBank corpus which contains temporal annotations of news report documents (see Section 1). Mani et al. (2003) and Mani and Schiffman (2005) demonstrate that TimeML-compliant annotations are useful for learning a model of temporal relations in news text. They focus on the problem of ordering pairs of successively described events. A decision tree classifier is trained on a corpus of temporal relations provided by human subjects. Using features such as the position of the sentence within the paragraph (and the position of the paragraph in the text), discourse connectives, temporal prepositions and other temporal modifiers, tense features, aspect shifts and tense shifts, their best model achieves 75.4% accuracy in identifying the temporal order of events. Boguraev and Ando (2005) use semi-supervised learning for recognising events and inferring temporal relations (between two events or between an event and a time expression). Their method exploits TimeML\n4\nLEARNING TEMPORAL RELATIONS\nannotations from the TimeBank corpus and large amounts of unannotated data. They first build a classifier from the TimeML annotations using a variety of features based on syntactic analysis and the identification of temporal expressions. The original feature vectors are next augmented with unlabelled data sharing structural similarities with the training data. Their algorithm yields performances well above the baseline for both tasks. Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier. For example, for text spans connected with RESULT, it is implied by the semantics of this relation, that the events in the first span temporally precede the second; thus, a classifier of rhetorical relations could indirectly contribute to a classifier of temporal relations. Corpus-based methods for computing discourse structure are beginning to emerge (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge & Lascarides, 2005). But there is currently no automatic mapping from these discourse structures to their temporal consequences; so although there is potential for eventually using linguistic resources labelled with discourse structure to acquire a model of temporal relations, that potential cannot be presently realised. Continuing on the topic of discourse relations, it is worth mentioning Marcu and Echihabi (2002) whose approach bypasses altogether the need for manual coding in a supervised learning setting. A key insight in their work is that rhetorical relations (e.g., EXPLANATION and CONTRAST) are sometimes signalled by an unambiguous discourse connective (e.g., because for EXPLANATION and but for CONTRAST). They extract sentences containing such unambiguous markers from a corpus, and then (automatically) identify the text spans connected by the marker, remove the marker and replace it with the rhetorical relation it signals. A Naive Bayes classifier is trained on this automatically labelled data. The model is designed to be maximally simple and employs solely word bigrams as features. Specifically, bigrams are constructed over the cartesian product of words occurring in the two text spans and it is assumed that word pairs are conditionally independent. Marcu and Echihabi demonstrate that such a knowledge-lean approach performs well, achieving an accuracy of 49.70% when distinguishing six relations (over a baseline of 16.67%). However, since the model relies exlusively on word-co-occurrences, an extremely large training corpus (in the order of 40 M sentences) is required to avoid sparse data (see Sporleder and Lascarides (2005) for more detailed discussion). In a sense, when considering the complexity of various models used to infer temporal and discourse relations, Marcu and Echihabi\u2019s (2002) model lies at the simple extreme of the spectrum, whereas the semantics and inference-based approaches to discourse interpretation (e.g., Hobbs et al., 1993; Asher & Lascarides, 2003) lie at the other extreme, for these latter theories assume no independence among the properties of the spans, and they exploit linguistic and non-linguistic features to the full. In this paper, we aim to explore a number of probabilistic models which lie in between these two extremes, thereby giving us the opportunity to study the tradeoff between the complexity of the model on the one hand, and the amount of training data required on the other. We are particularly interested in assessing the performance of models on smaller training sets than those used by Marcu and Echihabi (2002); such models will be useful for classifiers that are trained on data sets where relatively rare discourse connectives are exploited. Our work differs from Mani et al. (2003) and Boguraev and Ando (2005) in that we do not exploit manual annotations in any way. Our aim is however similar, we infer temporal relations between pairs of events. We share with Marcu and Echihabi (2002) the use of data with overt markers as a proxy for hand coded temporal relations. Apart from the fact that our interepretation task is\n5\nLAPATA & LASCARIDES\ndifferent from theirs, our work departs from Marcu and Echihabi (2002) in three further important ways. First, we propose alternative models and explore the contribution of linguistic information to the inference task, investigating how this enables one to train on considerably smaller data sets. Secondly, we apply the proposed models to a generation task, namely information fusion. And finally, we evaluate the models against human subjects performing the same task, as well as against a gold standard corpus. In the following section we present our models and formalise our interpretation and generation tasks."}, {"heading": "3. Problem Formulation", "text": "Interpretation Given a main clause and a subordinate clause attached to it, our task is to infer the temporal marker linking the two clauses. P(SM ; t j;SS) represents the probability that a marker t j relates a main clause SM and a subordinate clause SS. We aim to identify which marker t j in the set of possible markers T maximises P(SM ; t j;SS):\nt = argmax t j2T P(SM; t j;SS) t = argmax t j2T P(SM)P(SSjSM)P(t jjSM ;SS)\n(3)\nWe ignore the terms P(SM) and P(SSjSM) in (3) as they are constant and use Bayes\u2019 Rule to calculate P(t jjSM ;SS):\nt = argmax t j2T P(t jjSM;SS) t = argmax t j2T P(t j)P(SM;SSjt j) t = argmax t j2T P(t j)P(a hM;1i ahS;nijt j)\n(4)\nSM and SS are vectors of features a hM;1i ahM;ni and ahS;1i ahS;ni characteristic of the propositions occurring with the marker t j (our features are described in detail in Section 4.2). Estimating the different P(a\nhM;1i ahS;nijt j) terms will not be feasible unless we have a very large set of training data. We will therefore make the simplifying assumption that a temporal marker t j can be determined by observing feature pairs representative of a main and a subordinate clause. We further assume that these feature pairs are conditionally independent given the temporal marker and are not arbitrary; rather than considering all pairs in the cartesian product of a\nhM;1i ahM;ni, we restrict ourselves to feature pairs that belong to the same class i. Thus, the probability of observing the conjunction a hM;1i ahS;ni given t j is:\nt = argmax t j2T\nP(t j) n\n\u220f i=1 P(a hM;i;i;ahS;iijt j)\n(5)\nFor example, if we were assuming our feature space consisted solely of nouns and verbs, we will estimate P(a\nhM;i;i;ahS;iijt j) by taking into account all noun-noun and verb-verb bigrams that are attested in S and M and co-occur with t j. The model in (4) can be further simplified by assuming that the likelihood of the subordinate clause SS is conditionally independent of the main clause SM (i.e., P(SS;SM jt j) P(SSjt j)P(SMjt j)).\n6\nLEARNING TEMPORAL RELATIONS\nThe assumption is clearly a simplification but makes the estimation of the probabilities P(SM jt j) and P(SSjt j) more reliable in the face of sparse data:\nt argmax t j2T P(t j)P(SMjt j)P(SSjt j) (6)\nSM and SS are again vectors of features a hM;1i ahM;ni and ahS;1i ahS;ni representing the clauses co-occurring with the marker t j. Now individual features (instead of feature pairs) are assumed to be conditionally independent given the temporal marker and therefore:\nt = argmax t j2T\nP(t j) n\n\u220f i=1 P(a hM;iijt j)P(ahS;iijt j)\n(7)\nReturning to our example feature space of nouns and verbs, P(a hM;iijt j) and P(ahS;iijt j) will be estimated by considering how often verbs and nouns co-occur with t j. These co-occurrences will be estimated separately for main and subordinate clauses. Throughout this paper we will use the terms conjunctive for model (5) and disjunctive for model (7). We effectively treat the temporal interpretation problem as a disambiguation task. From a (confusion) set T of temporal markers, e.g., fafter, before, sinceg, we select the one that maximises (5) or (7) (see Section 4 for details on our confusion set and corpus). The conjunctive model explicitly captures dependencies between the main and subordinate clauses, whereas the disjunctive model is somewhat simplistic in that relationships between features across the two clauses are not captured directly. However, if two values of these features for the main and subordinate clauses co-occur frequently with a particular marker, then the conditional probability of these features on that marker will approximate the right biases. The conjunctive model is more closely related to the kinds of symbolic rules for inferring temporal relations that are used in semantics and inference-based accounts (e.g., Hobbs et al., 1993). Many rules typically draw on the relationships between the verbs in both clauses, or the nouns in both clauses, and so on. Both the disjunctive and conjunctive models are different from Marcu and Echihabi\u2019s (2002) model in several respects. They utilise linguistic features rather than word bigrams. The conjunctive model\u2019s features are two-dimensional with each dimension belonging to the same feature class. The disjunctive model has the added difference that it assumes independence in the features attested in the two clauses.\nFusion For the sentence fusion task, the identity of the two clauses is unknown, and our task is to infer which clause contains the marker. Conjunctive and disjunctive models can be expressed as follows:\np = argmax p2fM;Sg\nP(t) n\n\u220f i=1 P(a hp;i;i;ahp;iijt)\n(8)\np = argmax p2fM;Sg\nP(t) n\n\u220f i=1 P(a hp;iijt)P(ahp;iijt)\n(9)\nwhere p is generally speaking a sentence fragment to be realised as a main or subordinate clause (fp = Sjp = Mg or fp = Mjp = Sg), and t is the temporal marker linking the two clauses. Features are generated similarly to the interpretation case by taking the co-occurrences of temporal markers and individual features (disjunctive model) or feature pairs (conjuctive model) into account.\n7\nLAPATA & LASCARIDES\n(S1 (S (NP (DT The) (NN company)) (VP (VBD said) (S (NP (NNS employees))\n(VP (MD will) (VP (VB lose) (NP (PRP their) (NNS jobs)) (SBAR-TMP (IN after) (S (NP (DT the) (NN sale)) (VP (AUX is) (VP (VBN completed)))\n))))))))\nFigure 1: Extraction of main and subordinate clause from parse tree"}, {"heading": "4. Parameter Estimation", "text": "We can estimate the parameters for our models from a large corpus. In their simplest form, the features a\nhM;ii and ahS;ii can be the words making up main and subordinate clauses. In order to extract relevant features, we first identify clauses in a hypotactic relation, i.e., main clauses of which the subordinate clause is a constituent. Next, in the training phase, we estimate the probabilities P(a\nhM;iijt j) and P(ahS;iijt j) for the disjunctive model by simply counting the occurrence of the features a\nhM;ii and ahS;ii with marker t (i.e., f (ahM;ii; t j)) and ( f (ahS;ii; t j)). In essence, we assume for this model that the corpus is representative of the way various temporal markers are used in English. For the conjunctive model we estimate the co-occurrence frequencies f (a\nhM;ii;ahS;ii; t j). Features with zero counts are smoothed in both models; we adopt the m-estimate with uniform priors, with m equal to the size of the feature space (Cestnik, 1990). In the testing phase, all occurrences of the relevant temporal markers are removed for the interpretation task and the model must decide which member of the confusion set to choose. For the sentence fusion task, it is the textual order of the two clauses that is unknown and must be inferred."}, {"heading": "4.1 Data Extraction", "text": "In order to obtain training and testing data for the models described in the previous section, subordinate clauses (and their main clause counterparts) were extracted from the BLLIP corpus (30 M words). The latter is a Treebank-style, machine-parsed version of the Wall Street Journal (WSJ, years 1987\u201389) which was produced using Charniak\u2019s (2000) parser. Our study focused on the following (confusion) set of temporal markers: fafter, before, while, when, as, once, until, sinceg. We initially compiled a list of all temporal markers discussed in Quirk, Greenbaum, Leech, and Svartvik (1985) and eliminated markers with frequency less than 10 per million in our corpus. We identify main and subordinate clauses connected by temporal discourse markers, by first traversing the tree top-down until we identify the tree node bearing the subordinate clause label we are interested in and then extract the subtree it dominates. Assuming we want to extract after subordinate clauses, this would be the subtree dominated by SBAR-TMP in Figure 1 indicated by the arrow pointing down (see after the sale is completed ). Having found the subordinate clause, we proceed to extract the main clause by traversing the tree upwards and identifying the S node immediately dominating the subordinate clause node (see the arrow pointing up in Figure 1, employees will lose their jobs). In cases where the subordinate clause is sentence initial, we first identify the\n8\nLEARNING TEMPORAL RELATIONS\nSBAR-TMP node and extract the subtree dominated by it, and then traverse the tree downwards in order to extract the S-tree immediately dominating it. For the experiments described here we focus solely on subordinate clauses immediately dominated by S, thus ignoring cases where nouns are related to clauses via a temporal marker. Note that there can be more than one main clause that qualify as attachment sites for a subordinate clause. In Figure 1 the subordinate clause after the sale is completed can be attached either to said or will loose. There can be similar structural ambiguities for identifying the subordinate clause; for example see (10), where the conjunction and should lie within the scope of the subordinate before-clause (and indeed, the parser disambiguates the structural ambiguity correctly for this case):\n(10) [ Mr. Grambling made off with $250,000 of the bank\u2019s money [ before Colonial caught on and denied him the remaining $100,000. ] ]\nWe are relying on the parser for providing relatively accurate resolutions of structural ambiguities, but unavoidably this will create some noise in the data. To estimate the extent of this noise, we manually inspected 30 randomly selected examples for each of our temporal discourse markers i.e., 240 examples in total. All the examples that we inspected were true positives of temporal discourse markers save one, where the parser assumed that as took a sentential complement whereas in reality it had an NP complement (i.e., an anti-poverty worker):\n(11) [ He first moved to West Virginia [ as an anti-poverty worker, then decided to stay and start a political career, eventually serving two terms as governor. ] ]\nIn most cases the noise is due to the fact that the parser either overestimates or underestimates the extent of the text span for the two clauses. 98.3% of the main clauses and 99.6% of the subordinate clauses were accurately identified in our data set. Sentence (12) is an example where the parser incorrectly identifies the main clause: it predicts that the after-clause is attached to to denationalise the country\u2019s water industry. Note, however, that the subordinate clause (as some managers resisted the move and workers threatened lawsuits). is correctly identified.\n(12) [ Last July, the government postponed plans [ to denationalise the country\u2019s water industry [ after some managers resisted the move and workers threatened lawsuits. ] ] ]\n9\nLAPATA & LASCARIDES\nThe size of the corpus we obtain with these extraction methods is detailed in Table 1. There are 83,810 instances overall (i.e., just 0.20% of the size of the corpus used by Marcu and Echihabi, 2002). Also note that the distribution of temporal markers ranges from 0.76% (for once) to 42.83% (for when). Some discourse markers from our confusion set underspecify temporal semantic information. For example, when can entail temporal overlap (see (13a), from Kamp & Reyle, 1993a), or temporal progression (see (13c), from Moens & Steedman, 1988). The same is true for once and since:\n(13) a. Mary left when Bill was preparing dinner. b. When they built the bridge, they solved all their traffic problems.\n(14) a. Once John moved to London, he got a job with the council. b. Once John was living was living in London, he got a job with the council.\n(15) a. John has worked for the council since he\u2019s been living in London. b. John moved to London since he got a job with the council there.\nThis means that if the model chooses when, once, or since as the most likely marker between a main and subordinate clause, then the temporal relation between the events described is left underspecified. Of course the semantics of when or once limits the range of possible relations to two, but our model does not identify which specific relation is conveyed by these markers for a given example. Similarly, while is ambiguous between a temporal use in which it signals that the eventualities temporally overlap (see (16a)) and a contrastive use which does not convey any particular temporal relation (although such relations may be conveyed by other features in the sentence, such as tense, aspect and real world knowledge; see (16b)). The maker as can also denote two relations, i.e., overlap (see 17a) or cause (see 17b).\n(16) a. While the stock market was rising steadily, even companies stuffed with cash rushed to issue equity.\nb. While on the point of history he was directly opposed to Liberal Theology, his appeal to a \u2018spirit\u2019 somehow detachable from the Jesus of history run very much along similar lines to the Liberal approach.\n(17) a. Grand melodies poured out of him as he contemplated Caesar\u2019s conquest of Egypt. b. I wen to the bank as I run out of cash.\nWe inspected 30 randomly-selected examples for markers with underspecified readings (i.e., when, once, since, while and as). The marker when entails a temporal overlap interpretation 70% of the time, whereas once and since are more likely to entail temporal progression (74% and 80%, respectively). The markers as and while receive predominantly temporal interpretations in our corpus. Specifically, while has non-temporal uses in 13.3% of the instances in our sample and as in 25%. Once the interence procedure has taken place, we could use these biases to disambiguate, albeit coarsely, markers with underspecified meanings."}, {"heading": "4.2 Model Features", "text": "A number of knowledge sources are involved in inferring temporal ordering including tense, aspect, temporal adverbials, lexical semantic information, and world knowledge (Asher & Lascarides,\n10\nLEARNING TEMPORAL RELATIONS\n2003). By selecting features that represent, albeit indirectly and imperfectly, these knowledge sources, we aim to empirically assess their contribution to the temporal inference task. Below we introduce our features and provide motivation behind their selection.\nTemporal Signature (T) It is well known that verbal tense and aspect impose constraints on the temporal order of events and also on the choice of temporal markers. These constraints are perhaps best illustrated in the system of Dorr and Gaasterland (1995) who examine how inherent (i.e., states and events) and non-inherent (i.e., progressive, perfective) aspectual features interact with the time stamps of the eventualities in order to generate clauses and the markers that relate them. Although we cannot infer inherent aspectual features from verb surface form (for this we would need a dictionary of verbs and their aspectual classes together with a process that assigns aspectual classes in a given context), we can extract non-inherent features from our parse trees. We first identify verb complexes including modals and auxiliaries and then classify tensed and non-tensed expressions along the following dimensions: finiteness, non-finiteness, modality, aspect, voice, and polarity. The values of these features are shown in Table 2. The features finiteness and non-finiteness are mutually exclusive. Verbal complexes were identified from the parse trees heuristically by devising a set of 30 patterns that search for sequences of auxiliaries and verbs. From the parser output verbs were classified as passive or active by building a set of 10 passive identifying patterns requiring both a passive auxiliary (some form of be and get) and a past participle. To illustrate with an example, consider again the parse tree in Figure 1. We identify the verbal groups will lose and is completed from the main and subordinate clause respectively. The former is mapped to the features fpresent, 0, future, imperfective, active, affirmativeg, whereas the latter is mapped to fpresent, 0, /0, imperfective, passive, affirmativeg, where 0 indicates the verb form is finite\n11\nLAPATA & LASCARIDES\nand /0 indicates the absence of a modal. In Table 3 we show the relative frequencies in our corpus for finiteness (FIN), past tense (PAST), active voice (ACT), and negation (NEG) for main and subordinate clauses conjoined with the markers once and since. As can be seen there are differences in the distribution of counts between main and subordinate clauses for the same and different markers. For instance, the past tense is more frequent in since than once subordinate clauses and modal verbs are more often attested in since main clauses when compared with once main clauses. Also, once main clauses are more likely to be active, whereas once subordinate clauses can be either active or passive.\nVerb Identity (V) Investigations into the interpretation of narrative discourse have shown that specific lexical information plays an important role in determining temporal interpretation (e.g., Asher and Lascarides 2003). For example, the fact that verbs like push can cause movement of the patient and verbs like fall describe the movement of their subject can be used to interpret the discourse in (18) as the pushing causing the falling, thus making the linear order of the events mismatch their temporal order.\n(18) Max fell. John pushed him.\nWe operationalise lexical relationships among verbs in our data by counting their occurrence in main and subordinate clauses from a lemmatised version of the BLLIP corpus. Verbs were extracted from the parse trees containing main and subordinate clauses. Consider again the tree in Figure 1. Here, we identify lose and complete, without preserving information about tense or passivisation which is explicitly represented in our temporal signatures. Table 4 lists the most frequent verbs attested in main (VerbM) and subordinate (VerbS) clauses conjoined with the temporal markers after, as, before, once, since, until, when, and while (TMark).\nVerb Class (VW, VL) The verb identity feature does not capture meaning regularities concerning the types of verbs entering in temporal relations. For example, in Table 4 sell and pay are possession verbs, say and announce are communication verbs, and come and rise are motion verbs. Asher and Lascarides (2003) argue that many of the rules for inferring temporal relations should be specified in terms of the semantic class of the verbs, as opposed to the verb forms themselves, so as to maximise the linguistic generalisations captured by a model of temporal relations. For our purposes, there is an\n12\nLEARNING TEMPORAL RELATIONS\nadditional empirical motivation for utilising verb classes as well as the verbs themselves: it reduces the risk of sparse data. Accordingly, we use two well-known semantic classifications for obtaining some degree of generalisation over the extracted verb occurrences, namely WordNet (Fellbaum, 1998) and the verb classification proposed by Levin (1995). Verbs in WordNet are classified in 15 broad semantic domains (e.g., verbs of change, verbs of cognition, etc.) often referred to as supersenses (Ciaramita & Johnson, 2003). We therefore mapped the verbs occurring in main and subordinate clauses to WordNet supersenses. (feature VW). Semantically ambiguous verbs will correspond to more than one semantic class. We resolve ambiguity heuristically by always defaulting to the verb\u2019s prime sense (as indicated in WordNet) and selecting its corresponding supersense. In cases where a verb is not listed in WordNet we default to its lemmatised form. Levin (1995) focuses on the relation between verbs and their arguments and hypothesises that verbs which behave similarly with respect to the expression and interpretation of their arguments share certain meaning components and can therefore be organised into semantically coherent classes (200 in total). Asher and Lascarides (2003) argue that these classes provide important information for identifying semantic relationships between clauses. Verbs in our data were mapped into their corresponding Levin classes (feature VL); polysemous verbs were disambiguated by the method proposed in Lapata and Brew (1999).3 Again, for verbs not included in Levin, the lemmatised verb form is used. Examples of the most frequent Levin classes in main and subordinate clauses as well as WordNet supersenses are given in Table 4.\nNoun Identity (N) It is not only verbs, but also nouns that can provide important information about the semantic relation between two clauses; Asher and Lascarides (2003) discuss an example in which having the noun meal in one sentence and salmon in the other serves to trigger inferences that the events are in a part-whole relation (eating the salmon was part of the meal). An example from our domain concerns the nouns share and market. The former is typically found in main clauses preceding the latter which is often in a subordinate clause. Table 5 shows the most frequently attested nouns (excluding proper names) in main (NounM) and subordinate (NounS) clauses for each temporal marker. Notice that time denoting nouns (e.g., year, month ) are quite frequent in this data set. Nouns were extracted from a lemmatised version of the parser\u2019s output. In Figure 1 the nouns employees, jobs and sales are relevant for the Noun feature. In cases of noun compounds, only the compound head (i.e., rightmost noun) was taken into account. A small set of rules was used to identify organisations (e.g., United Laboratories Inc.), person names (e.g., Jose Y. Campos), and locations (e.g., New England ) which were subsequently substituted by the general categories person, organisation, and location.\nNoun Class (NW) As with verbs, Asher and Lascarides (2003) argue in favour of symbolic rules for inferring temporal relations that utilise the semantic classes of nouns wherever possible, so as to maximise the linguistic generalisations that are captured. For example, they argue that one can infer a causal relation in (19) on the basis that the noun bruise has a cause via some act-on predicate with some underspecified agent (other nouns in this class include injury, sinking, construction):\n3. Lapata and Brew (1999) develop a simple probabilistic model which determines for a given polysemous verb and its frame its most likely meaning overall (i.e., across a corpus), without relying on the availability of a disambiguated corpus. Their model combines linguistic knowledge in the form of Levin (1995) classes and frame frequencies acquired from a parsed corpus.\n13\nLAPATA & LASCARIDES\n(19) John hit Susan. Her bruise is enormous.\nAs in the case of verbs, nouns were also represented by supersenses from the WordNet taxonomy. Nouns in WordNet do not form a single hierarchy; instead they are partitioned according to a set of semantic primitives into 25 supersenses (e.g., nouns of cognition, events, plants, substances, etc.), which are treated as the unique beginners of separate hierarchies. The nouns extracted from the parser were mapped to WordNet classes. Ambiguity was handled in the same way as for verbs. Examples of the most frequent noun classes attested in main and subordinate clauses are illustrated in Table 5.\nAdjective (A) Our motivation for including adjectives in the feature set is twofold. First, we hypothesise that temporal adjectives (e.g., old, new, later) will be frequent in subordinate clauses introduced by temporal markers such as before, after, and until and therefore may provide clues for the marker interpretation task. Secondly, similarly to verbs and nouns, adjectives carry important lexical information that can be used for inferring the semantic relation that holds between two clauses. For example, antonyms can often provide clues about the temporal sequence of two events (see incoming and outgoing in (20)).\n(20) The incoming president delivered his inaugural speech. The outgoing president resigned last week.\nAs with verbs and nouns, adjectives were extracted from the parser\u2019s output. The most frequent adjectives in main (AdjM) and subordinate (AdjS) clauses are given in Table 4.\nSyntactic Signature (S) The syntactic differences in main and subordinate clauses are captured by the syntactic signature feature. The feature can be viewed as a measure of tree complexity, as it encodes for each main and subordinate clause the number of NPs, VPs, PPs, ADJPs, and ADVPs it contains. The feature can be easily read off from the parse tree. The syntactic signature for the main clause in Figure 1 is [NP:2 VP:2 ADJP:0 ADVP:0 PP:0] and for the subordinate clause [NP:1 VP:1 ADJP:0 ADVP:0 PP:0]. The most frequent syntactic signature for main clauses is [NP:2 VP:1 PP:0 ADJP:0 ADVP:0]; subordinate clauses typically contain an adverbial phrase [NP:2 VP:1 ADJP:0 ADVP:1 PP:0]. One motivating case for using this syntactic feature involves verbs describing propositional attitudes (e.g., said, believe, realise). Our set of temporal discourse markers\n14\nLEARNING TEMPORAL RELATIONS\nwill have varying distributions as to their relative semantic scope to these verbs. For example, one would expect until to take narrow semantic scope (i.e., the until-clause would typically attach to the verb in the sentential complement to the propositional attitude verb, rather than to the propositional attitude verb itself), while the situation might be different for once.\nArgument Signature (R) This feature captures the argument structure profile of main and subordinate clauses. It applies only to verbs and encodes whether a verb has a direct or indirect object, and whether it is modified by a preposition or an adverbial. As the rules for inferring temporal relations in Hobbs et al. (1993) and Asher and Lascarides (2003) attest, the predicate argument structure of clauses is crucial to making the correct temporal inferences in many cases. To take a simple example, observe that inferring the causal relation in (18) crucially depends on the fact that the subject of fall denotes the same person as the direct object of push ; without this, a relation other than a causal one would be inferred. As with syntactic signature, this feature was read from the main and subordinate clause parsetrees. The parsed version of the BLLIP corpus contains information about subjects. NPs whose nearest ancestor was a VP were identified as objects. Modification relations were recovered from the parse trees by finding all PPs and ADVPs immediately dominated by a VP. In Figure 1 the argument signature of the main clause is [SUBJ,OBJ] and for the subordinate it is [OBJ].\nPosition (P) This feature simply records the position of the two clauses in the parse tree, i.e., whether the subordinate clause precedes or follows the main clause. The majority of the main clauses in our data are sentence initial (80.8%). However, there are differences among individual markers. For example, once clauses are equally frequent in both positions. 30% of the when clauses are sentence initial whereas 90% of the after clauses are found in the second position. These statistics clearly show that the relative positions of the main vs. subordinate clauses are going to be relatively informative for the the interpretation task. In the following sections we describe our experiments with the models introduced in Section 3. We first investigate their performance on the temporal interpretation and fusion tasks (Experiments 1 and 2) and then describe a study with humans (Experiment 3). The latter enables us to examine in more depth the models\u2019 performance and the difficulty of our inference tasks."}, {"heading": "5. Experiment 1: Sentence Interpretation", "text": "Method Our models were trained on main and subordinate clauses extracted from the BLLIP corpus as detailed in Section 4. Recall that we obtained 83,810 main-subordinate pairs. These were randomly partitioned into training (80%), development (10%) and test data (10%). Eighty randomly selected pairs from the test data were reserved for the human study reported in Experiment 3. We performed parameter tuning on the development set; all our results are reported on the unseen test set, unless otherwise stated. We compare the performance of the conjunctive and disjunctive models, thereby assessing the effect of feature (in)dependence on the temporal interpretation task. Furthermore, we compare the performance of the two proposed models against a baseline disjunctive model that employs a word-based feature space (see (7) where P(a\nhM;ii = whM;iijt j)) and P(ahS;ii = whS;iijt j)). This model resembles Marcu and Echihabi\u2019s (2002)\u2019s model in that it does not make use of the linguistically motivated features presented in the previous section; all that is needed for estimating its parameters\n15\nLAPATA & LASCARIDES\nis a corpus of main-subordinate clause pairs. We also report the performance of a majority baseline (i.e., always select when, the most frequent marker in our data set). In order to assess the impact of our feature classes (see Section 4.2) on the interpretation task, the feature space was exhaustively evaluated on the development set. We have nine classes, which results in 9!\n(9 k)! combinations where k is the arity of the combination (unary, binary, ternary, etc.). We measured the accuracy of all class combinations (1,023 in total) on the development set. From these, we selected the best performing ones for evaluating the models on the test set.\nResults Our results are shown in Table 7. We report both accuracy and F-score. A set of diacritics is used to indicate significance (on accuracy) throughout this paper (see Table 6). The best performing model on the test set (accuracy 62.6%) was observed with the combination of verbs (V) with syntactic signatures (S) for the disjunctive model (see Table 7). The combination of verbs (V), verb classes (VL, VW ), syntactic signatures (S) and clause position (P) yielded the highest accuracy (60.3%) for the conjunctive model (see Table 7). Both conjunctive and disjunctive models performed significantly better than the majority baseline and word-based model which also significantly outperformed the majority baseline. The disjunctive model (SV) significantly outperformed the conjunctive one (VWVLPSV). We attribute the conjunctive model\u2019s worse performance to data sparseness. There is clearly a trade-off between reflecting the true complexity of the task of inferring temporal relations and the amount of training data available. The size of our data set favours a simpler model over a more complex one. The difference in performance between the models relying on linguistically-motivated\n16\nLEARNING TEMPORAL RELATIONS\nfeatures and the word-based model also shows, in line with the findings in Sporleder and Lascarides (2005), that linguistic abstractions are useful in overcoming sparse data. We further analysed the data requirements for our models by varying the amount of instances on which they are trained. Figure 2 shows learning curves for the best conjunctive and disjunctive models (SV and VWVLPSV). For comparison, we also examine how training data size affects the (disjunctive) word-based baseline model. As can be seen, the disjunctive model has an advantage over the conjunctive one; the difference is more pronounced with smaller amounts of training data. Very small performance gains are obtained with increased training data for the word baseline model. A considerably larger training set is required for this model to be competitive against the more linguistically aware models. This result is in agreement with Marcu and Echihabi (2002) who employ a very large corpus (1 billion words) for training their word-based model. Further analysis of our models revealed that some feature combinations performed reasonably well on individual markers for both the disjunctive and conjunctive model, even though their overall accuracy did not match the best feature combinations for either model class. Some accuracies for these combinations are shown in Table 8. For example, NPRSTV was one of the best combinations for generating after under the disjunctive model, whereas SV was better for before (feature abbreviations are as introduced in Section 4.2). Given the complementarity of different models, an obvious question is whether these can be combined. An important finding in machine learning is that a set of classifiers whose individual decisions are combined in some way (an ensemble) can be more accurate than any of its component classifiers if the errors of the individual classifiers are sufficiently uncorrelated (Dietterich, 1997). The next section reports on our ensemble learning experiments.\nEnsemble Learning An ensemble of classifiers is a set of classifiers whose individual decisions are combined in some way to classify new examples. This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis\n17\nLAPATA & LASCARIDES\nand part-of-speech tagging (see Dietterich, 1997 and van Halteren, Zavrel, & Daelemans, 2001 for overviews). Ensemble learners often yield superior results to individual learners provided that the component learners are accurate and diverse (Hansen & Salamon, 1990). An ensemble is typically built in two steps, i.e., first multiple component learners are trained and their predictions are combined. Multiple classifiers can be generated either by using subsamples of the training data (Breiman, 1996a; Freund & Shapire, 1996) or by manipulating the set of input features available to the component learners (Cherkauer, 1996). Weighted or unweighted voting is the method of choice for combining individual classifiers in an ensemble. A more sophisticated combination method is stacking where a learner is trained to predict the correct output class when given as input the outputs of the ensemble classifiers (Wolpert, 1992; Breiman, 1996b; van Halteren et al., 2001). In other words, a second-level learner is trained to select its output on the basis of the patterns of co-occurrence of the output of several component learners. We generated multiple classifiers (for combination in the ensemble) by varying the number and type of features available to the conjunctive and disjunctive models discussed in the previous section. The outputs of these models were next combined using c5.0 (Quinlan, 1993), a decision-tree second level-learner. Decision trees are among the most widely used machine learning algorithms. They perform a general to specific search of a feature space, adding the most informative features to a tree structure as the search proceeds. The objective is to select a minimal set of features that efficiently partitions the feature space into classes of observations and assemble them into a tree (see Quinlan, 1993 for details). A classification for a test case is made by traversing the tree until either a leaf node is found or all further branches do not match the test case, and returning the most frequent class at the last node. Learning in this framework requires a primary training set, for training the component learners; a secondary training set for training the second-level learner and a test set for assessing the stacked classifier. We trained the decision-tree learner on the development set using 10-fold cross-validation. We experimented with 133 different conjunctive models and 65 disjunctive models; the best results on the development set were obtained with the combination of 22 conjunctive models and 12 dis-\n18\nLEARNING TEMPORAL RELATIONS\njunctive models. The component models are presented in Table 9. The ensembles\u2019 performance on the test set is reported in Table 7. As can be seen, both types of ensemble significantly outperform the word-based baseline, and the best performing individual models. Furthermore, the disjunctive ensemble significantly outperforms the conjunctive one. Table 10 details the performance of the two ensembles for each individual marker. Both ensembles have difficulty inferring the markers since, once and while; the difficulty is more pronounced in the conjunctive ensemble. We believe that the worse performance for predicting these relations is due to a combination of sparse data and ambiguity. First, observe that these three classes have have fewest examples in our data set (see Table 1). Secondly, once is temporally ambiguous, conveying temporal progression and temporal overlap (see example (14)). The same ambiguity is observed with since (see example (15)). Finally, although the temporal sense of while always conveys temporal overlap, it has a non-temporal, contrastive sense too which potentially creates some noise in the training data, as discussed in Section 4.1. Another contributing factor to while\u2019s poor performance is the lack of sufficient training data. Note that the extracted instances for this marker constitute only 4.2% of our data. In fact, the model often confuses the marker since with the semantically similar while. This can be explained by the fact that both markers convey similar relations: they both imply temporal overlap but also have contrastive usages (thereby entailing temporal progression). Let us now examine which classes of features have the most impact on the interpretation task by observing the component learners selected for our ensembles. As shown in Table 8, verbs either as lexical forms (V) or classes (VW, VL), the syntactic structure of the main and subordinate clauses (S) and their position (P) are the most important features for interpretation. Verb-based features are present in all component learners making up the conjunctive ensemble and in 10 (out of 12) learners for the disjunctive ensemble. The argument structure feature (R) seems to have some influence (it is present in five of the 12 component (disjunctive) models), however we suspect that there is some overlap with S. Nouns, adjectives and temporal signatures seem to have a small impact on the interpretation task, at least for the WSJ domain. Our results so far point to the importance of the lexicon for the marker interpretation task but also indicate that the syntactic complexity of the two clauses is crucial for inferring their semantic relation. Asher and Lascarides\u2019 (2003) symbolic theory of discourse interpretation also emphasises the importance of lexical information in inferring temporal relations.\n19\nLAPATA & LASCARIDES"}, {"heading": "6. Experiment 2: Sentence Fusion", "text": "Method For the sentence fusion task we built models that used the feature space introduced in Section 4.2, with the exception of the position feature (P). Knowing the linear precedence of the two clauses is highly predictive of their type: 80.8% of the main clauses are sentence initial. However, this type of positional information is typically not known when fragments are synthesised into a meaningful sentence and was therefore not taken into account in our experiments. To find the best performing model, the feature space was exhaustively evaluated on the development set. As in Experiment 1, we compared the performance of conjunctive and disjunctive models. These models were in turn evaluated against a word-based disjunctive model (where P(a\nhp;ii =\nw hp;iijt)) and P(ahp;ii = whp;iijt)) and a simple baseline that decides which clause should be introduced by the temporal marker at random.\nResults The best performing conjunctive and disjunctive models are presented in Table 11. The feature combination NT delivered the highest accuracy for the conjunctive model (68.3%), whereas ARSVVW, was the best disjunctive model reaching an accuracy of 80.1%. Both models significantly outperformed the word-based model and the random guessing baseline. Similarly to the interpre-\n20\nLEARNING TEMPORAL RELATIONS\ntation task, the conjunctive model performs significantly worse than the disjunctive one. We also examined the amount of data required for achieving satisfactory performance. The learning curves are given in Figure 3. The disjunctive model achieves a good performance with approximately 3,000 training instances. Also note that the conjunctive model suffers from data sparseness (similarly to the word-based model). With increased amounts of training data, it manages to outperform the word-based model, without however matching the performance of the disjunctive model. We next report on our experiments with ensemble models. Inspection of the performance of individual models on the development set revealed that they are complementary, i.e., they differ in their ability to perform the fusion task. Feature combinations with the highest accuracy (on the development set) for individual markers are shown in Table 12).\n21\nLAPATA & LASCARIDES\nEnsemble Learning Similarly to the interpretation task, an ensemble of classifiers was built in order to take advantage of the complementarity of different models. The second-level decision tree learner was again trained on the development set using 10-fold cross-validation. We experimented with 77 conjunctive and 44 different disjunctive models; the component models for which we obtained the best results on the development set are shown in Table 13 and formed the ensemble whose performance was evaluated on the test set. The conjunctive ensemble reached an accuracy of 80.8%. The latter was significantly outperformed by the disjunctive ensemble whose accuracy was 97.3% (see Table 11). In comparison, the best performing model\u2019s accuracy on the test set (ARSTV, disjunctive) was 80.1%. Table 14 shows how well the ensembles are performing the fusion task for individual markers. We only report accuracy since the recall is always one. The conjunctive ensemble performs poorly on the fusion task when the temporal marker is once. This is to be expected, since once is the least frequent marker in our data set, and as we have already observed the conjunctive model is particularly prone to sparse data. Not surprisingly, the features V and S are also important for the fusion task (see Table 14). Adjectives (A), nouns (N and NW) and temporal signatures (T), all seem to play more of a role in this task than they did in the interpretation task. This is perhaps to be expected given that the differences between main and subordinate clauses are rather subtle (semantically and structurally) and more information is needed to perform the inference.\n22\nLEARNING TEMPORAL RELATIONS\nAlthough for the interpretation and fusion tasks the ensemble outperformed the single best model, it is worth noting that the best individual models (ARSTV and SV for fusion and interpretation, respectively) rely on features that can be simply extracted from the parse trees without recourse to taxonomic information. Removing from the disjunctive ensemble the feature combinations that rely on corpus external resources (i.e., Levin, WordNet) yields an overall accuracy of 65.0% for interpretation and 95.6% for fusion."}, {"heading": "7. Experiment 3: Human Evaluation", "text": "Method We further compared our model\u2019s performance against human judges by conducting two separate studies, one fore interpretation an one one for fusion. In the first study, participants were asked to perform a multiple choice task. They were given a set of 40 main-subordinate pairs (five for each marker) randomly chosen from our test data. The marker linking the two clauses was removed and participants were asked to select the missing word from a set of eight temporal markers, thus mimicking the models\u2019 task. In the second study, participants were presented with a series of three sentence fragments and were asked to arrange them so that a coherent sentence is formed. The fragments were a main clause, a subordinate clause and a marker. Punctuation was removed so as not to reveal any ordering clues. Participants saw 40 such triples randomly selected from our test set. The set of items was different from those used in the interpretation task; again five items were selected for each marker. Examples of the materials our participants saw are given in Apendix A. Both studies were conducted remotely over the Internet. Subjects first saw a set of instructions that explained the task, and had to fill in a short questionnaire including basic demographic information. For the interpretation task, a random order of main-subordinate pairs and a random order of markers per pair was generated for each subject. For the fusion task, a random order of items and a random order of fragments per item was generated for each subject. The interpretation study was completed by 198 volunteers, all native speakers of English. 100 volunteers participated in the fusion study, again all native speakers of English. Subjects were recruited via postings to local Email lists.\nResults Our results are summarised in Table 15. We measured how well human subjects (H) agree with the gold standard (G)\u2014i.e., the corpus from which the experimental items were selected\u2014and how well they agree with each other. We also show how well the disjunctive ensembles (E) for the fusion and interpretation task respectively agree with the humans (H) and the gold standard (G). We measured agreement using the Kappa coefficient (Siegel & Castellan, 1988) but also report percentage agreement to facilitate comparison with our model. In all cases we compute pairwise agreements and report the mean. As shown in Table 15 there is less agreement among humans for the interpretation task than the sentence fusion task. This is expected given that some of the markers are semantically similar and in some cases more than one marker are compatible with the temporal implicatures that arise from joining the two clauses. Also note that neither the model nor the subjects have access to the context surrounding the sentence whose marker must be inferred (we discuss this further in Section 8). Additional analysis of the interpretation data revealed that the majority of disagreements arose for as and once clauses. Once was also problematic for the ensemble model (see Table 10). Only 33% of the subjects agreed with the gold standard for as clauses; 35% of the subjects agreed with the gold standard for once clauses. For the other markers, the subject agreement with the gold standard was\n23\nLAPATA & LASCARIDES\naround 55%. The highest agreement was observed for since and until (63% and 65% respectively). A confusion matrix summarizing the resulting inter-subject agreement for the interpretation task is shown in Table 16. The ensemble\u2019s agreement with the gold standard approximates human performance on the interpretation task (.413 for E-G vs. .421 for H-G). The agreement of the ensemble with the subjects is also close to the upper bound, i.e., inter-subject agreement (see, E-H and H-H in Table 15). A similar pattern emerges for the fusion task: comparison between the ensemble and the gold standard yields an agreement of .489 (see E-G) when subject and gold standard agreement is .522 (see H-G); agreement of the ensemble with the subjects is .468 when the upper bound is .490 (see E-H and H-H, respectively)."}, {"heading": "8. General Discussion", "text": "In this paper we proposed a data intensive approach for inferring the temporal relations in text. We introduced models that learn temporal relations from sentences where temporal information is made explicit via temporal markers. These models could potentially be used in cases where overt temporal markers are absent. We also evaluated our models against a sentence fusion task. The latter is relevant for applications such as summarisation or question answering where sentence fragments (extracted from potentially multiple documents) must be combined into a fluent sentence. For\n24\nLEARNING TEMPORAL RELATIONS\nthe fusion task our models determine the appropriate ordering among a temporal marker and two clauses. Previous work on temporal inference has focused on the automatic tagging of temporal expressions (e.g., Wilson et al., 2001) or on learning the ordering of events from manually annotated data (e.g., Mani et al., 2003, Boguraev & Ando, 2005). Our models bypass the need for manual annotation by focusing on instances of temporal relations that are made explicit by the presence of temporal markers. We compared and contrasted several models varying in their linguistic assumptions and employed feature space. We also explored the tradeoff between model complexity and data requirements. Our results indicate that less sophisticated models (e.g., the disjunctive model) tend to perform reasonably when utilising expressive features and training data sets that are relatively modest in size. We experimented with a variety of linguistically motivated features ranging from verbs and their semantic classes to temporal signatures and argument structure. Many of these features were inspired by symbolic theories of temporal interpretation, which often exploit semantic representations (e.g., of the two clauses) as well as complex inferences over real world knowledge (e.g., Hobbs et al., 1993; Lascarides & Asher, 1993; Kehler, 2002). Our best model achieved an F-score of 69.1% on the interpretation task and 93.4% on the fusion task. This performance is a significant improvement over the baseline and compares favourably with human performance on the same tasks. Our experiments further revealed that not only lexical but also syntactic information is important for both tasks. This result is in agreement with Soricut and Marcu (2003) who find that syntax trees encode sufficient information to enable accurate derivation of discourse relations. In sum, we have shown that it is possible to infer temporal information from corpora even if they are not semantically annotated in any way. An important future direction lies in modelling the temporal relations of events across sentences. In order to achieve full-scale temporal reasoning, the current model must be extended in a number of ways. These involve the incorporation of extra-sentential information to the modelling task as well as richer temporal information (e.g., tagged time expressions; see Mani et al., 2003). The current models perform the inference task independently of their surrounding context. Experiment 3 revealed, this is a rather difficult task; even humans cannot easily make decisions regarding temporal relations out-of-context. We plan to take into account contextual (lexical and syntactic) as well as discourse-based features (e.g., coreference resolution). Another issue related to the nature of our training data concerns the temporal information entailed by some of our markers which can be ambiguous. This could be remedied either heuristically as discussed in Section 4.1 or by using models trained on unambiguous markers (e.g., before, after) to disambiguate instances with multiple readings. Another possibility is to apply a separate disambiguation procedure on the training data (i.e., prior to the learning of temporal inference models). The approach presented in this paper can be also combined with the annotations present in the TimeML corpus in a semi-supervised setting similar to Boguraev and Ando (2005) to yield improved performance. Another interesting direction is to use the models proposed here in a bootstrapping approach. Initially, a model is learned from unannotated data and its output is manually edited following the \u201cannotate automatically, correct manually\u201d methodology used to provide high volume annotation in the Penntreebank project. At each iteration the model is retrained on progressively more accurate and representative data. Finally, temporal relations and discourse structure are co-dependent (Kamp & Reyle, 1993b; Asher & Lascarides, 2003). It is a matter of future work to devise models that integrate discourse\n25\nLAPATA & LASCARIDES\nand temporal relations, with the ultimate goal of performing full-scale text understanding. In fact, the two types of knowledge may be mutually benefitial, thus improving both temporal and discourse text analysis."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the support of EPSRC (grant numbers GR/R40036/01 and GR/T04540/01). Thanks to Regina Barzilay and Frank Keller for helpful comments and suggestions."}, {"heading": "Appendix A. Experimental Materials for Human Evaluation", "text": "The following is the list of materials used in the human evaluation studies reported in Experiment 3 (Section 7). The sentences were extracted from the BLLIP corpus following the procedure described in Section 4.1.\n30\nLEARNING TEMPORAL RELATIONS\n31\nLAPATA & LASCARIDES\n32\nLEARNING TEMPORAL RELATIONS\n33\nLAPATA & LASCARIDES\n34\n3.3 K 6.7 K 10 K 13 .4K 16 .8K 20 .1K 23 .4K 26 .8K 30 .1K 33 .5K 36 .8K 40 .2K 43 .6K 46 .9K 50 .3K 53 .6K 56 .9K 60 .3K\nNumber of instances in training data\n40\n45\n50\n55\n60\n65\n70\n75\n80\n85\n90\nA cc\nur ac\ny (%\n)\nWord-based Baseline Conjunctive Model Disjunctive Model\n45\n50\n5\n60\n65\n7\nA cc\nur ac\ny (%\n)\nr - ase aseli e j cti e el\nisj cti e el\nar X\niv :1\n11 0.\n13 94\nv1 [\ncs .C\nL ]\n6 O\nct 2\n01 1\nInferring Sentence-internal Temporal Relations\nMirella Lapata Department of Computer Science\nUniversity of Sheffield Regent Court, 211 Portobello Street\nSheffield S1 4DP, UK mlap@dcs.shef.ac.uk\nAlex Lascarides School of Informatics\nUniversity of Edinburgh 2 Buccleuch Place\nEdinburgh EH8 9LW, UK alex@inf.ed.ac.uk\nAbstract\nIn this paper we propose a data intensive approach for inferring sentence internal temporal relations, which relies on a simple probabilistic model and assumes no manual coding. We explore various combinations of features, and evaluate performance against a goldstandard corpus and human subjects performing the same task. The best model achieves 70.7% accuracy in inferring the temporal relation between two clauses and 97.4% accuracy in ordering them, assuming that the temporal relation is known."}, {"heading": "1 Introduction", "text": "The ability to identify and analyse temporal information is crucial for a variety of practical NLP applications such as information extraction, question answering, and summarisation. In multidocument summarisation, information must be extracted, potentially fused, and synthesised into a meaningful text. Knowledge about the temporal order of events is important for determining what content should be communicated (interpretation) but also for correctly merging and presenting information (generation). In question answering one would like to find out when a particular event occurred (e.g., When did X resign? ) but also to obtain information about how events relate to each other (e.g., Did X resign before Y? ).\nAlthough temporal relations and their interaction with discourse relations (e.g., Parallel, Result) have received much attention in linguistics (???), the automatic interpretation of events and their temporal relations is beyond the capabilities of current open-domain NLP systems. While corpus-based methods have accelerated progress in other areas of NLP, they have yet to make a substantial impact on the processing of temporal information. This is partly due to the absence of readily available corpora annotated with temporal information, although\nefforts are underway to develop treebanks marked with temporal relations (?) and devise annotation schemes that are suitable for coding temporal relations (??). Absolute temporal information has received some attention (???) and systems have been developed for identifying and assigning referents to time expressions.\nAlthough the treatment of time expressions is an important first step towards the automatic handling of temporal phenomena, much temporal information is not absolute but relative and not overtly expressed but implicit. Consider the examples in (1) taken from ?. Native speakers can infer that John first met and then kissed the girl and that he first left the party and then walked home, even though there are no overt markers signalling the temporal order of the described events.\n(1) a. John kissed the girl he met at a party. b. Leaving the party, John walked home.\nc. He remembered talking to her and asking her for her name.\nIn this paper we describe a data intensive approach that automatically captures information pertaining to the temporal order and relations of events like the ones illustrated in (1). Of course trying to acquire temporal information from a corpus that is not annotated with temporal relations, tense, or aspect seems rather futile. However, sometimes there are overt markers for temporal relations, the conjunctions before, after, while, and when being the most obvious, that make relational information about events explicit:\n(2) a. Leonard Shane, 65 years old, held the post of president before William Shane, 37, was elected to it last year.\nb. The results were announced after the market closed. c. Investors in most markets sat out while awaiting the U.S. trade figures.\nIt is precisely this type of data that we will exploit for making predictions about the order in which events occurred when there are no obvious markers signalling temporal ordering. We will assess the feasibility of such an\napproach by initially focusing on sentence-internal temporal relations. We will obtain sentences like the ones shown in (2), where a main clause is connected to a subordinate clause with a temporal marker and we will develop a probabilistic framework where the temporal relations will be learned by gathering informative features from the two clauses. This framework can then be used for interpretation in cases where overt temporal markers are absent (see the examples in (1)).\nPractical NLP applications such as text summarisation and question answering place increasing demands not only on the analysis but also on the generation of temporal relations. For instance, non-extractive summarisers that generate sentences by fusing together sentence fragments (e.g., Barzilay 2003) must be able to determine whether or not to include an overt temporal marker in the generated text, where the marker should be placed, and what lexical item should be used. We assess how appropriate our approach is when faced with the information fusion task of determining the appropriate ordering among a temporal marker and two clauses. We infer probabilistically which of the two clauses is introduced by the marker, and effectively learn to distinguish between main and subordinate clauses."}, {"heading": "2 The Model", "text": "Given a main clause and a subordinate clause attached to it, our task is to infer the temporal marker linking the two clauses. Formally, P(SM, t j ,SS) represents the probability that a marker t j relates a main clause SM and a subordinate clause SS. We aim to identify which marker t j in the set of possible markers T maximises P(SM, t j,SS):\n(3) t\u2217 = argmax t j\u2208T P(SM, t j,SS)\n= argmax t j\u2208T P(SM)P(t j|SM)P(SS|SM, t j)\nWe ignore the term P(SM) in (3) as it is a constant and use Bayes\u2019 Rule to derive P(SM|t j) from P(t j|SM):\n(4) t\u2217 = argmax t j\u2208T P(t j|SM)P(SS|SM, t j)\n= argmax t j\u2208T P(t j)P(SM|t j)P(SS|SM, t j)\nWe will further assume that the likelihood of the subordinate clause SS is conditionally independent of the main clause SM (i.e., P(SS|SM, t j) \u2248 P(SS|t j)). The assumption is clearly a simplification but makes the estimation of the probabilities P(SM|t j) and P(SS|t j) more reliable in the face of sparse data.\n(5) t\u2217 \u2248 argmax t j\u2208T P(t j)P(SM|t j)P(SS|t j)\nSM and SS are vectors of features a\u3008M,1\u3009 \u00b7 \u00b7 \u00b7a\u3008M,n\u3009 and a\u3008S,1\u3009 \u00b7 \u00b7 \u00b7a\u3008S,n\u3009 characteristic of the propositions occurring\nwith the marker t j (our features are described in detail in Section 3.2). By making the simplifying assumption that these features are conditionally independent given the temporal marker, the probability of observing the conjunctions a\u3008M,1\u3009 \u00b7 \u00b7 \u00b7a\u3008M,n\u3009 and a\u3008S,1\u3009 \u00b7 \u00b7 \u00b7a\u3008S,n\u3009 is:\n(6) t\u2217 = argmax t j\u2208T P(t j)\u220f i\n(\nP(a\u3008M,i\u3009|t j)P(a\u3008S,i\u3009|t j)\n)\nWe effectively treat the temporal interpretation problem as a disambiguation task. From the (confusion) set T of temporal markers {after, before, while, when, as, once, until, since}, we select the one that maximises (6). We compiled a list of temporal markers from ?. Markers with corpus frequency less than 10 per million were excluded from our confusion set (see Section 3.1 for a description of our corpus).\nThe model in (6) is simplistic in that the relationships between the features across the clauses are not captured directly. However, if two values of these features for the main and subordinate clauses co-occur frequently with a particular marker, then the conditional probability of these features on that marker will approximate the right biases. Also note that some of these markers are ambiguous with respect to their meaning: one sense of while denotes overlap, another contrast; since can indicate a sequence of events in which the main clause occurs after the subordinate clause or cause, as indicates overlap or cause, and when can denote overlap, a sequence of events, or contrast. Our model selects the appropriate markers on the basis of distributional evidence while being agnostic to their specific meaning when they are ambiguous.\nFor the sentence fusion task, the identity of the two clauses is unknown, and our task is to infer which clause contains the marker. This can be expressed as:\n(7) p\u2217 = argmax p\u2208{M,S} P(t)\u220f i\n(\nP(a\u3008p,i\u3009|t)P(a\u3008p,i\u3009|t)\n)\nwhere p is generally speaking a sentence fragment to be realised as a main or subordinate clause ({p = S|p = M} or {p = M|p = S}), and t is the temporal marker linking the two clauses.\nWe can estimate the parameters for the models in (6) and (7) from a parsed corpus. We first identify clauses in a hypotactic relation, i.e., main clauses of which the subordinate clause is a constituent. Next, in the training phase, we estimate the probabilities P(a\u3008M,i\u3009|t j) and P(a\u3008S,i\u3009|t j) by simply counting the occurrence of the features a\u3008M,i\u3009 and a\u3008S,i\u3009 with marker t. For features with zero counts, we use add-k smoothing (?), where k is a small number less than one. In the testing phase, all occurrences of the relevant temporal markers are removed for the interpretation task and the model must decide which member of the confusion set to choose. For the sentence fusion task, it is the temporal order of the two clauses that is unknown\nand must be inferred. A similar approach has been advocated for the interpretation of discourse relations by ?. They train a set of naive Bayes classifiers on a large corpus (in the order of 40 M sentences) representative of four rhetorical relations using word bigrams as features. The discourse relations are read off from explicit discourse markers thus avoiding time consuming hand coding. Apart from the fact that we present an alternative model, our work differs from ? in two important ways. First we explore the contribution of linguistic information to the inference task using considerably smaller data sets and secondly apply the proposed model to a generation"}, {"heading": "3 Parameter Estimation", "text": ""}, {"heading": "3.1 Data Extraction", "text": ""}, {"heading": "3.2 Model Features", "text": "(S1 (S (NP (DT The) (NN company)) (VP (VBD said)\n(S (NP (NNS employees)) (VP (MD will) (VP (VB lose) (NP (PRP their) (NNS jobs)) (SBAR-TMP (IN after) (S (NP (DT the) (NN sale)) (VP (AUX is) (VP (VBN completed)))\n))))))))\nFINITE = {past, present} NON-FINITE = {infinitive, ing-form, en-form} MODALITY = { /0, future, ability, possibility, obligation} ASPECT = {imperfective, perfective, progressive} VOICE = {active, passive} NEGATION = {affimative, negative}\nTable 1: Temporal signatures\nFeature onceM onceS sinceM sinceS FIN 0.69 0.72 0.75 0.79 PAST 0.28 0.34 0.35 0.71 ACT 0.87 0.51 0.85 0.81 MOD 0.22 0.02 0.07 0.05 NEG 0.97 0.98 0.95 0.97\nTable 2: Relative frequency counts for temporal features\nmodal verbs are more often attested in since main clauses when compared with once main clauses. Also, once main clauses are more likely to be active, whereas once subordinate clauses can be either active or passive.\nVerb Identity (V) Investigations into the interpretation of narrative discourse have shown that specific lexical information plays an important role in determing temporal interpretation (e.g., Asher and Lascarides 2003). For example, the fact that verbs like push can cause movement of the patient and verbs like fall describe the movement of their subject can be used to predict that the discourse (8) is interpreted as the pushing causing the falling, making the linear order of the events mismatch their temporal order.\n(8) Max fell. John pushed him.\nWe operationalise lexical relationships among verbs in our data by counting their occurrence in main and subordinate clauses from a lemmatised version of the BLLIP corpus. Verbs were extracted from the parse trees containing main and subordinate clauses. Consider again the tree in Figure 1. Here, we identify lose and complete, without preserving information about tense or passivisation which is explictly represented in our temporal signatures. Table 3 lists the most frequent verbs attested in main (VerbM) and subordinate (VerbS) clauses conjoined with the temporal markers after, as, before, once, since, until, when, and while (TMark in Table 3).\nVerb Class (VW, VL) The verb identity feature does not capture meaning regularities concerning the types of verbs entering in temporal relations. For example, in Table 3 sell and pay are possession verbs, say and announce are communication verbs, and come and rise are motion verbs. We use a semantic classification for obtaining some degree of generalisation over the extracted verb occurrences. We experimented with WordNet (?) and the verb classification proposed by ?.\nTMark VerbM VerbS NounN NounS AdjM AdjS after sell leave year company last new as come acquire market dollar recent previous before say announce time year long new once become complete stock place more new since rise expect company month first last until protect pay president year new next when make sell year year last last while wait complete chairman plan first other\nTable 3: Verb, noun, and adjective occurrences in main and subordinate clauses\nVerbs in WordNet are classified in 15 general semantic domains (e.g., verbs of change, verbs of cognition, etc.). We mapped the verbs occurring in main and subordinate clauses to these very general semantic categories (feature VW). Ambiguous verbs in WordNet will correspond to more than one semantic class. We resolve ambiguity heuristically by always defaulting to the verb\u2019s prime sense and selecting the semantic domain for this sense. In cases where a verb is not listed in WordNet we default to its lemmatised form.\n? focuses on the relation between verbs and their arguments and hypothesizes that verbs which behave similarly with respect to the expression and interpretation of their arguments share certain meaning components and can therefore be organised into semantically coherent classes (200 in total). ? argue that these classes provide important information for identifying semantic relationships between clauses. Verbs in our data were mapped into their corresponding Levin classes (feature VL); polysemous verbs were disambiguated by the method proposed in ?. Again, for verbs not included in Levin, the lemmatised verb form is used.\nNoun Identity (N) It is not only verbs, but also nouns that can provide important information about the semantic relation between two clauses (see ? for detailed motivation). In our domain for example, the noun share is found in main clauses typically preceding the noun market which is often found in subordinate clauses. Table 3 shows the most frequently attested nouns (excluding proper names) in main (NounM) and subordinate (NounS) clauses for each temporal marker. Notice that time denoting nouns (e.g., year, month ) are quite frequent in this data set.\nNouns were extracted from a lemmatised version of the parser\u2019s output. In Figure 1 the nouns employees, jobs and sales are relevant for the Noun feature. In cases of noun compounds, only the compound head (i.e., rightmost noun) was taken into account. A small set of rules was used to identify organisations (e.g., United Laboratories Inc.), person names (e.g., Jose Y. Campos), and locations (e.g., New England ) which were subsequently substituted by the general categories person, organisation, and location.\nNoun Class (NW). As in the case of verbs, nouns were also represented by broad semantic classes from the WordNet taxonomy. Nouns in WordNet do not form a single hierarchy; instead they are partitioned according to a set of semantic primitives into 25 semantic classes (e.g., nouns of cognition, events, plants, substances, etc.), which are treated as the unique beginners of separate hierarchies. The nouns extracted from the parser were mapped to WordNet classes. Ambiguity was handled in the same way as for verbs.\nAdjective (A) Our motivation for including adjectives in our feature set is twofold. First, we hypothesise that temporal adjectives will be frequent in subordinate clauses introduced by strictly temporal markers such as before, after, and until and therefore may provide clues for the marker interpretation task. Secondly, similarly to verbs and nouns, adjectives carry important lexical information that can be used for inferring the semantic relation that holds between two clauses. For example, antonyms can often provide clues about the temporal sequence of two events (see incoming and outgoing in (9)).\n(9) The incoming president delivered his inaugural speech. The outgoing president resigned last week.\nAs with verbs and nouns, adjectives were extracted from the parser\u2019s output. The most frequent adjectives in main (AdjM) and subordinate (AdjS) clauses are given in Table 3.\nSyntactic Signature (S) The syntactic differences in main and subordinate clauses are captured by the syntactic signature feature. The feature can be viewed as a measure of tree complexity, as it encodes for each main and subordinate clause the number of NPs, VPs, PPs, ADJPs, and ADVPs it contains. The feature can be easily read off from the parse tree. The syntactic signature for the main clause in Figure 1 is [NP:2 VP:2 ADJP:0 ADVP:0 PP:0] and for the subordinate clause [NP:1 VP:1 ADJP:0 ADVP:0 PP:0]. The most frequent syntactic signature for main clauses is [NP:2 VP:1 PP:0 ADJP:0 ADVP:0]; subordinate clauses typically contain an adverbial phrase [NP:2 VP:1 ADJP:0 ADVP:1 PP:0].\nArgument Signature (R) This feature captures the argument structure profile of main and subordinate clauses. It applies only to verbs and encodes whether a verb has a direct or indirect object, whether it is modified by a preposition or an adverbial. As with syntactic signature, this feature was read from the main and subordinate clause parse-trees. The parsed version of the BLLIP corpus contains information about subjects. NPs whose nearest ancestor was a VP were identified as objects. Modification relations were recovered from the parse trees by finding all PPs and ADVPs immediately dominated by a VP. In Figure 1 the argument signature of the main clause is [SUBJ,OBJ] and for the subordinate it is [OBJ].\nPosition (P) This feature simply records the position of the two clauses in the parse tree, i.e., whether the subordinate clause precedes or follows the main clause. The majority of the main clauses in our data are sentence intitial (80.8%). However, there are differences among individual markers. For example, once clauses are equally frequent in both positions. 30% of the when clauses are sentence intitial whereas 90% of the after clauses are found in the second position.\nIn the following sections we describe our experiments with the model introduced in Section 2. We first investigate the model\u2019s accuracy on the temporal interpretation and fusion tasks (Experiment 1) and then describe a study with humans (Experiment 2). The latter enables us to examine in more depth the model\u2019s classification accuracy when compared to human judges."}, {"heading": "4 Experiment 1: Interpretation and Fusion", "text": ""}, {"heading": "4.1 Method", "text": "The model was trained on main and subordinate clauses extracted from the BLLIP corpus as detailed in Section 3.1. We obtained 83,810 main-subordinate pairs. These were randomly partitioned into training (80%), development (10%) and test data (10%). Eighty randomly selected pairs from the test data were reserved for the human study reported in Experiment 2. We performed parameter tuning on the development set; all our results are reported on the unseen test set, unless otherwise stated."}, {"heading": "4.2 Results", "text": "In order to assess the impact of our features on the interpretation task, the feature space was exhaustively evaluated on the development set. We have nine features, which results in 9!(9\u2212k)! feature combinations where k is the arity of the combination (unary, binary, ternary, etc.). We measured the accuracy of all feature combinations (1023 in total) on the develoment set. From these, we selected the most informative combinations for evaluating the model on the test set. The best accuracy (61.4%) on the development set was observed with the combination of verbs (V) with syntactic signatures (S). We also observed that some feature combinations performed reasonably well on individual markers, even though their overall accuracy was not better than V and S combined. Some accuracies for these combinations are shown in Table 4. For example, NPRSTV was one of the best combinations for generating after, whereas SV was better for before (feature abbreviations are as introduced in Section 3.2).\nGiven the complementarity of different model parametrisations, an obvious question is whether these can be combined. An important finding in Machine Learning is that a set of classifiers whose individual decisions are combined in some way (an ensemble) can be more accurate than any of its component classifiers if the errors of the individual classifiers are sufficiently uncor-\nInterpretation Fusion TMark Feat Acc Feat Acc after NPRSTV 69.9 AVWV 77.9 as ANNWPSV 57.0 AV 75.8 before SV 42.1 ANSTV 85.4 once PRS 40.7 RT 100 since PRST 25.1 T 85.2 when VLPS 85.5 RST 86.9 while PST 49.0 VWS 79.4 until VLVWRT 69.4 TV 90.5\nTable 4: Best feature combinations for individual markers (development set)\nInterpretation Fusion E SV E ARSTV\nTMark Prec Rec Prec Rec Prec Prec after 61.5 66.5 51.6 55.2 96.7 75.2 as 61.5 62.6 57.0 52.8 93.2 70.5 before 50.0 51.5 32.0 39.1 96.8 84.1 once 60.0 25.0 12.7 15.0 100 88.3 since 69.4 26.3 25.4 12.0 98.2 81.0 when 83.0 91.1 84.7 85.0 99.3 83.8 while 71.5 28.9 38.0 25.8 97.7 82.8 until 57.8 52.4 38.5 47.7 97.8 87.8 Acc 70.7 62.6 97.3 80.1 Baseline 42.6 42.6 42.6 42.6 50.0 50.0\n10-fold cross-validation. Table 5 shows precision (Prec) and recall (Rec). For comparison we also report precision and recall for the best individual feature combination on the test set (SV) and the baseline of always selecting when, the most frequent marker in our data set (42.6%). The ensemble (E) classified correctly 70.7% of the instances in the test set, whereas SV obtained an accuracy of 62.6%. The ensemble performs significantly better than SV (\u03c72 = 102.57, df = 1, p < .005) and both SV and E perform significantly better than the baseline (\u03c72 = 671.73, df = 1, p < .005 and \u03c72 = 1278.61, df = 1, p < .005, respectively). The ensemble has difficulty inferring the markers since, once and while (see the recall figures in Table 5). Since is often confused with the semantically similar while. Until is not ambiguous, however it is relatively infrequent in our corpus (6.3% of our data set). We suspect that there is simply not enough data for the model to accurately infer these markers.\nFor the fusion task we also explored the feature space exhaustively on the development set, after removing the position feature (P). Knowing the linear precedence of the two clauses is highly predictive of their type: 80.8% of the main clauses are sentence initial. However, this type of positional information is typically not known when fragments are synthesised into a meaningful sentence."}, {"heading": "5 Experiment 2: Human Evaluation", "text": ""}, {"heading": "5.1 Method", "text": ""}, {"heading": "5.2 Results", "text": "Interpretation Fusion K % K %\nH-H .410 45.0 .490 70.0 H-G .421 46.9 .522 79.2 E-H .390 44.3 .468 70.0 E-G .413 47.5 .489 75.0\nwe compute pairwise agreements and report the mean. In Table 6, H refers to the subjects, G to the gold-standard, and E to the ensemble.\nAs shown in Table 6 there is less agreement among humans for the interpretation task than the sentence fusion task. This is expected given that some of the markers are semantically similar and in some cases more than one marker are compatible with the meaning of the two clauses. Also note that neither the model nor the subjects have access to the context surrounding the sentence whose marker must be inferred (we discuss this further in Section 6). Additional analysis of the interpretation data revealed that the majority of disagreements arose for as and once clauses. Once was also problematic for our model (see the Recall in Table 5). Only 33% of the subjects agreed with the gold-standard for as clauses; 35% of the subjects agreed with the gold-standard for once clauses. For the other markers, the subject agreement with the gold-standard was around 55%. The highest agreement was observed for since and until (63% and 65% respectively).\nThe ensemble\u2019s agreement with the gold-standard approximates human performance on the interpretation task (.413 for E-G vs. .421 for H-G). The agreement of the ensemble with the subjects is also close to the upper bound, i.e., inter-subject agreement (see, E-H and H-H in Table 6). A similar pattern emerges for the fusion task: comparison between the ensemble and the gold-standard yields an agreement of .489 (see E-G) when subject and gold-standard agreement is .522 (see H-G); agreement of the ensemble with the subjects is .468 when the upper bound is .490 (see E-H and H-H, respectively)."}, {"heading": "6 Discussion", "text": "In this paper we proposed a data intensive approach for inferring the temporal relations of events. We introduced a model that learns temporal relations from sentences where temporal information is made explicit via temporal markers. This model then can be used in cases where overt temporal markers are absent. We also evaluated our model against a sentence fusion task. The latter is relevant for applications such as summarisation or question answering where sentence fragments must be combined into a fluent sentence. For the fusion task our model determines the appropriate ordering among a temporal marker\nand two clauses. We experimented with a variety of linguistically motivated features and have shown that it is possible to extract semantic information from corpora even if they are not semantically annotated in any way. We achieved an accuracy of 70.7% on the interpretation task and 97.4% on the fusion task. This performance is a significant improvement over the baseline and compares favourably with human performance on the same tasks. Previous work on temporal inference has focused on the automatic tagging of temporal expressions (e.g., ?) or on learning the ordering of events from manually annotated data (e.g., ?). Our experiments further revealed that not only lexical but also syntactic information is important for both tasks. This result is in agreement with ? who find that syntax trees encode sufficient information to enable accurate derivation of discourse relations.\nAn important future direction lies in modelling the temporal relations of events across sentences. The approach presented in this paper can be used to support the \u201cannotate automatically, correct manually\u201d methodology used to provide high volume annotation in the Penntreebank project. An important question for further investigation is the contribution of linguistic and extra-sentential information to modelling temporal relations. Our model can be easily extended to include contextual features and also richer temporal information such as tagged time expressions (see ?). Apart from taking more features into account, in the future we plan to experiment with models where main and subordinate clauses are not assumed to be conditionally independent and investigate the influence of larger data sets on prediction accuracy."}, {"heading": "Acknowledgments", "text": "The authors are supported by EPSRC grant number GR/R40036. Thanks to Regina Barzilay and Frank Keller for helpful comments and suggestions.\narXiv:1110.1394v1 [cs.CL] 6 Oct 2011\n...\n1"}], "references": [{"title": "Natural Language Understanding", "author": ["J. Allen"], "venue": "Benjamin Cummins.", "citeRegEx": "Allen,? 1995", "shortCiteRegEx": "Allen", "year": 1995}, {"title": "Logics of Conversation", "author": ["N. Asher", "A. Lascarides"], "venue": null, "citeRegEx": "Asher and Lascarides,? \\Q2003\\E", "shortCiteRegEx": "Asher and Lascarides", "year": 2003}, {"title": "Probabilistic head-driven parsing for discourse structure", "author": ["J. Baldridge", "A. Lascarides"], "venue": "In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL)", "citeRegEx": "Baldridge and Lascarides,? \\Q2005\\E", "shortCiteRegEx": "Baldridge and Lascarides", "year": 2005}, {"title": "Information Fusion for Multi-Document Summarization: Praphrasing and Generation", "author": ["R. Barzilay"], "venue": "Ph.D. thesis, Columbia University.", "citeRegEx": "Barzilay,? 2003", "shortCiteRegEx": "Barzilay", "year": 2003}, {"title": "Timeml-compliant text analysis for temporal reasoning", "author": ["B. Boguraev", "R.K. Ando"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Boguraev and Ando,? \\Q2005\\E", "shortCiteRegEx": "Boguraev and Ando", "year": 2005}, {"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Machine Learning, 2(24), 123\u2013140.", "citeRegEx": "Breiman,? 1996a", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Stacked regressions", "author": ["L. Breiman"], "venue": "Machine Learning, 3(24), 49\u201364.", "citeRegEx": "Breiman,? 1996b", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory", "author": ["L. Carlson", "D. Marcu", "M. Okurowski"], "venue": "In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue,", "citeRegEx": "Carlson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Carlson et al\\.", "year": 2001}, {"title": "Estimating probabilities: a crucial task in machine learning", "author": ["B. Cestnik"], "venue": "Proceedings of the 16th European Conference on Artificial Intelligence, pp. 147\u2013149, Stockholm, Sweden.", "citeRegEx": "Cestnik,? 1990", "shortCiteRegEx": "Cestnik", "year": 1990}, {"title": "A maximum-entropy-inspired parser", "author": ["E. Charniak"], "venue": "Proceedings of the 1st Conference of the North American Chapter of the Assocation for Computational Linguistics, pp. 132\u2013139, Seattle, WA.", "citeRegEx": "Charniak,? 2000", "shortCiteRegEx": "Charniak", "year": 2000}, {"title": "Human expert-level performance on a scientific image analysis task by a system using combined artificial neural networks", "author": ["K.J. Cherkauer"], "venue": "In Working Notes of the AAAI Workshop on Integrating Multiple Learned Models, pp. 15\u201321, Portland, OR.", "citeRegEx": "Cherkauer,? 1996", "shortCiteRegEx": "Cherkauer", "year": 1996}, {"title": "Supersense tagging of unknown words in wordnet", "author": ["M. Ciaramita", "M. Johnson"], "venue": "In Proceedings of the 8th,", "citeRegEx": "Ciaramita and Johnson,? \\Q2003\\E", "shortCiteRegEx": "Ciaramita and Johnson", "year": 2003}, {"title": "Machine learning research: Four current directions", "author": ["T.G. Dietterich"], "venue": "AI Magazine, 18(4), 97\u2013136.", "citeRegEx": "Dietterich,? 1997", "shortCiteRegEx": "Dietterich", "year": 1997}, {"title": "Selecting tnese aspect and connective words in language generation", "author": ["B. Dorr", "T. Gaasterland"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Dorr and Gaasterland,? \\Q1995\\E", "shortCiteRegEx": "Dorr and Gaasterland", "year": 1995}, {"title": "The effects of aspectual class on the temporal sturcture of discourse: Semantics or pragmatics", "author": ["D. Dowty"], "venue": "Linguistics and Philosophy, 9(1), 37\u201361.", "citeRegEx": "Dowty,? 1986", "shortCiteRegEx": "Dowty", "year": 1986}, {"title": "WordNet: An Electronic Database", "author": ["C. Fellbaum"], "venue": null, "citeRegEx": "Fellbaum,? \\Q1998\\E", "shortCiteRegEx": "Fellbaum", "year": 1998}, {"title": "Tides temporal annotation guidelines", "author": ["L. Ferro", "I. Mani", "B. Sundheim", "G. Wilson"], "venue": "Tech. rep., The MITRE Corporation", "citeRegEx": "Ferro et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Ferro et al\\.", "year": 2000}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Shapire"], "venue": "Proceedings of the 13th International Conference on Machine Learning,", "citeRegEx": "Freund and Shapire,? \\Q1996\\E", "shortCiteRegEx": "Freund and Shapire", "year": 1996}, {"title": "A framework for resolution of time in natural language", "author": ["B. Han", "A. Lavie"], "venue": "ACM Transactions on Asian Language Information Processing (TALIP),", "citeRegEx": "Han and Lavie,? \\Q2004\\E", "shortCiteRegEx": "Han and Lavie", "year": 2004}, {"title": "Neural network ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions in Pattern Analysis and Machine Intelligence,", "citeRegEx": "Hansen and Salamon,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon", "year": 1990}, {"title": "Algorithms for analyzing the temporal structure of discourse", "author": ["J. Hitzeman", "M. Moens", "C. Grover"], "venue": "In Proceedings of the 7th Meeting of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Hitzeman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Hitzeman et al\\.", "year": 1995}, {"title": "Interpretation as abduction", "author": ["J.R. Hobbs", "M. Stickel", "D. Appelt", "P. Martin"], "venue": "Artificial Intelligence,", "citeRegEx": "Hobbs et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Hobbs et al\\.", "year": 1993}, {"title": "Tense trees as the finite structure of discourse", "author": ["C. Hwang", "L. Schubert"], "venue": "In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Hwang and Schubert,? \\Q1992\\E", "shortCiteRegEx": "Hwang and Schubert", "year": 1992}, {"title": "From Discourse to the Lexicon: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory", "author": ["H. Kamp", "U. Reyle"], "venue": null, "citeRegEx": "Kamp and Reyle,? \\Q1993\\E", "shortCiteRegEx": "Kamp and Reyle", "year": 1993}, {"title": "From Discourse to Logic: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer, Dordrecht", "author": ["H. Kamp", "U. Reyle"], "venue": null, "citeRegEx": "Kamp and Reyle,? \\Q1993\\E", "shortCiteRegEx": "Kamp and Reyle", "year": 1993}, {"title": "The annotation of temporal information in natural language sentences", "author": ["G. Katz", "F. Arosio"], "venue": "In Proceedings of ACL Workshop on Temporal and Spatial Information Processing,", "citeRegEx": "Katz and Arosio,? \\Q2001\\E", "shortCiteRegEx": "Katz and Arosio", "year": 2001}, {"title": "Coherence, Reference and the Theory of Grammar", "author": ["A. Kehler"], "venue": "CSLI Publications, Cambridge University Press.", "citeRegEx": "Kehler,? 2002", "shortCiteRegEx": "Kehler", "year": 2002}, {"title": "Using subcategorisation to resolve verb class ambiguity", "author": ["M. Lapata", "C. Brew"], "venue": "In Proceedings of Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Lapata and Brew,? \\Q1999\\E", "shortCiteRegEx": "Lapata and Brew", "year": 1999}, {"title": "Temporal interpretation, discourse relations and commonsense entailment", "author": ["A. Lascarides", "N. Asher"], "venue": "Linguistics and Philosophy,", "citeRegEx": "Lascarides and Asher,? \\Q1993\\E", "shortCiteRegEx": "Lascarides and Asher", "year": 1993}, {"title": "English Verb Classes and Alternations", "author": ["B. Levin"], "venue": "Chicago University Press.", "citeRegEx": "Levin,? 1995", "shortCiteRegEx": "Levin", "year": 1995}, {"title": "Temporally anchoring and ordering events in news", "author": ["I. Mani", "B. Schiffman"], "venue": null, "citeRegEx": "Mani and Schiffman,? \\Q2005\\E", "shortCiteRegEx": "Mani and Schiffman", "year": 2005}, {"title": "Inferring temporal ordering of events in news", "author": ["I. Mani", "B. Schiffman", "J. Zhang"], "venue": "In Proceedings of the 1st Human Language Technology Conference and Annual Meeting", "citeRegEx": "Mani et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Mani et al\\.", "year": 2003}, {"title": "A formal and computational synthesis of grosz and sidner\u2019s and mann and thompson\u2019s theories", "author": ["D. Marcu"], "venue": "Workshop on the Levels of Representation of Discourse, pp. 101\u2013 107, Edinburgh.", "citeRegEx": "Marcu,? 1999", "shortCiteRegEx": "Marcu", "year": 1999}, {"title": "An unsupervised approach to recognizing discourse relations", "author": ["D. Marcu", "A. Echihabi"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Marcu and Echihabi,? \\Q2002\\E", "shortCiteRegEx": "Marcu and Echihabi", "year": 2002}, {"title": "Temporal ontology and temporal reference", "author": ["M. Moens", "M.J. Steedman"], "venue": "Computational Linguistics,", "citeRegEx": "Moens and Steedman,? \\Q1988\\E", "shortCiteRegEx": "Moens and Steedman", "year": 1988}, {"title": "The specification of timeml", "author": ["J. Pustejovsky", "B. Ingria", "R. Sauri", "J. Castano", "J. Littman", "R. Gaizauskas", "A. Setzer"], "venue": "The Language of Time: A reader,", "citeRegEx": "Pustejovsky et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2004}, {"title": "Arda summer workshop on graphical annotation toolkit for timeml", "author": ["J. Pustejovsky", "I. Mani", "L. Belanger", "B. Boguraev", "B. Knippen", "J. Litman", "A. Rumshisky", "A. See", "S. Symonen", "J. van Guilder", "L. van Guilder", "M. Verhagen"], "venue": "Tech. rep.", "citeRegEx": "Pustejovsky et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "A Comprehensive Grammar of the English Language", "author": ["R. Quirk", "S. Greenbaum", "G. Leech", "J. Svartvik"], "venue": null, "citeRegEx": "Quirk et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 1985}, {"title": "From temporal expressions to temporal information: Smeantic tagging of news messages", "author": ["F. Schilder", "C. Habel"], "venue": "In Proceedings of ACL Workshop on Temporal and Spatial Information Processing,", "citeRegEx": "Schilder and Habel,? \\Q2001\\E", "shortCiteRegEx": "Schilder and Habel", "year": 2001}, {"title": "A pilot study on annotating temporal relations in text", "author": ["A. Setzer", "R. Gaizauskas"], "venue": "In Proceedings of ACL Workshop on Temporal and Spatial Information Processing,", "citeRegEx": "Setzer and Gaizauskas,? \\Q2001\\E", "shortCiteRegEx": "Setzer and Gaizauskas", "year": 2001}, {"title": "Non Parametric Statistics for the Behavioral Sciences", "author": ["S. Siegel", "N.J. Castellan"], "venue": null, "citeRegEx": "Siegel and Castellan,? \\Q1988\\E", "shortCiteRegEx": "Siegel and Castellan", "year": 1988}, {"title": "Sentence level discourse parsing using syntactic and lexical information", "author": ["R. Soricut", "D. Marcu"], "venue": "In Proceedings of the 1st Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Soricut and Marcu,? \\Q2003\\E", "shortCiteRegEx": "Soricut and Marcu", "year": 2003}, {"title": "Exploiting linguistic cues to classify rhetorical relations", "author": ["C. Sporleder", "A. Lascarides"], "venue": "In Proceedings of the 16th Irish Conference on Artificial Intelligence and Cognitive Science", "citeRegEx": "Sporleder and Lascarides,? \\Q2005\\E", "shortCiteRegEx": "Sporleder and Lascarides", "year": 2005}, {"title": "Improving accuracy in wordclass tagging through combination of machine learning systems", "author": ["H. van Halteren", "J. Zavrel", "W. Daelemans"], "venue": "Computational Linguistics,", "citeRegEx": "Halteren et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Halteren et al\\.", "year": 2001}, {"title": "An empirical approach to temporal reference resolution", "author": ["J.M. Wiebe", "T.P. O\u2019Hara", "T. \u00d6hrstr\u00f6m Sandgren", "K.J. McKeever"], "venue": "Journal of Artifical Intelligence Research,", "citeRegEx": "Wiebe et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Wiebe et al\\.", "year": 1998}, {"title": "A multilingual approach to annotating and extracting temporal information", "author": ["G. Wilson", "I. Mani", "B. Sundheim", "L. Ferro"], "venue": "In Proceedings of ACL Workshop on Temporal and Spatial Information Processing,", "citeRegEx": "Wilson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2001}, {"title": "Stacked generalization", "author": ["D.H. Wolpert"], "venue": "Neural Networks, 5, 241\u2013259.", "citeRegEx": "Wolpert,? 1992", "shortCiteRegEx": "Wolpert", "year": 1992}, {"title": "Line has cut 1,900 jobs since it acquired the core assets of the Mulwaukee Road trail line", "author": ["38 Soo"], "venue": null, "citeRegEx": "Soo,? \\Q1985\\E", "shortCiteRegEx": "Soo", "year": 1985}], "referenceMentions": [{"referenceID": 0, "context": "An additional challenge to this task poses the nature of temporal information itself which is often implicit (i.e., not overtly verbalised) and must be inferred using both linguistic and non-linguistic knowledge. Consider the examples in (1) taken from Katz and Arosio (2001). Native speakers can infer that John first met and then kissed the girl; that he left the party after kissing the girl and then walked home; and that the events of talking to her and asking her for her name temporally overlap (and occurred before he left the party).", "startOffset": 16, "endOffset": 276}, {"referenceID": 35, "context": "Pustejovsky et al. (2003) found evidence that this annotation task is sufficiently complex that human annotators can realistically identify only a small number of the temporal relations that hold in reality; i.", "startOffset": 0, "endOffset": 26}, {"referenceID": 21, "context": "This involves complex reasoning over a variety of rich information sources, including representations of domain knowledge and detailed logical forms of the clauses (e.g., Dowty, 1986; Hwang & Schubert, 1992; Hobbs et al., 1993; Lascarides & Asher, 1993; Kamp & Reyle, 1993a; Kehler, 2002).", "startOffset": 164, "endOffset": 288}, {"referenceID": 26, "context": "This involves complex reasoning over a variety of rich information sources, including representations of domain knowledge and detailed logical forms of the clauses (e.g., Dowty, 1986; Hwang & Schubert, 1992; Hobbs et al., 1993; Lascarides & Asher, 1993; Kamp & Reyle, 1993a; Kehler, 2002).", "startOffset": 164, "endOffset": 288}, {"referenceID": 0, "context": "Allen (1995), Hitzeman et al.", "startOffset": 0, "endOffset": 13}, {"referenceID": 0, "context": "Allen (1995), Hitzeman et al. (1995), and Han and Lavie (2004) propose more computationally tractable approaches to inferring temporal information from text, by hand-crafting algorithms which integrate shallow versions of the knowledge sources that are exploited in the above theoretical literature (e.", "startOffset": 0, "endOffset": 37}, {"referenceID": 0, "context": "Allen (1995), Hitzeman et al. (1995), and Han and Lavie (2004) propose more computationally tractable approaches to inferring temporal information from text, by hand-crafting algorithms which integrate shallow versions of the knowledge sources that are exploited in the above theoretical literature (e.", "startOffset": 0, "endOffset": 63}, {"referenceID": 30, "context": "(2003) and Mani and Schiffman (2005) demonstrate that TimeML-compliant annotations are useful for learning a model of temporal relations in news text.", "startOffset": 11, "endOffset": 37}, {"referenceID": 4, "context": "Boguraev and Ando (2005) use semi-supervised learning for recognising events and inferring temporal relations (between two events or between an event and a time expression).", "startOffset": 0, "endOffset": 25}, {"referenceID": 7, "context": "Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier.", "startOffset": 95, "endOffset": 117}, {"referenceID": 7, "context": "Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier. For example, for text spans connected with RESULT, it is implied by the semantics of this relation, that the events in the first span temporally precede the second; thus, a classifier of rhetorical relations could indirectly contribute to a classifier of temporal relations. Corpus-based methods for computing discourse structure are beginning to emerge (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge & Lascarides, 2005). But there is currently no automatic mapping from these discourse structures to their temporal consequences; so although there is potential for eventually using linguistic resources labelled with discourse structure to acquire a model of temporal relations, that potential cannot be presently realised. Continuing on the topic of discourse relations, it is worth mentioning Marcu and Echihabi (2002) whose approach bypasses altogether the need for manual coding in a supervised learning setting.", "startOffset": 96, "endOffset": 1004}, {"referenceID": 7, "context": "Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier. For example, for text spans connected with RESULT, it is implied by the semantics of this relation, that the events in the first span temporally precede the second; thus, a classifier of rhetorical relations could indirectly contribute to a classifier of temporal relations. Corpus-based methods for computing discourse structure are beginning to emerge (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge & Lascarides, 2005). But there is currently no automatic mapping from these discourse structures to their temporal consequences; so although there is potential for eventually using linguistic resources labelled with discourse structure to acquire a model of temporal relations, that potential cannot be presently realised. Continuing on the topic of discourse relations, it is worth mentioning Marcu and Echihabi (2002) whose approach bypasses altogether the need for manual coding in a supervised learning setting. A key insight in their work is that rhetorical relations (e.g., EXPLANATION and CONTRAST) are sometimes signalled by an unambiguous discourse connective (e.g., because for EXPLANATION and but for CONTRAST). They extract sentences containing such unambiguous markers from a corpus, and then (automatically) identify the text spans connected by the marker, remove the marker and replace it with the rhetorical relation it signals. A Naive Bayes classifier is trained on this automatically labelled data. The model is designed to be maximally simple and employs solely word bigrams as features. Specifically, bigrams are constructed over the cartesian product of words occurring in the two text spans and it is assumed that word pairs are conditionally independent. Marcu and Echihabi demonstrate that such a knowledge-lean approach performs well, achieving an accuracy of 49.70% when distinguishing six relations (over a baseline of 16.67%). However, since the model relies exlusively on word-co-occurrences, an extremely large training corpus (in the order of 40 M sentences) is required to avoid sparse data (see Sporleder and Lascarides (2005) for more detailed discussion).", "startOffset": 96, "endOffset": 2245}, {"referenceID": 7, "context": "Conceivably, existing corpus data annotated with discourse structure, such as the RST treebank (Carlson et al., 2001), might be reused to train a temporal relations classifier. For example, for text spans connected with RESULT, it is implied by the semantics of this relation, that the events in the first span temporally precede the second; thus, a classifier of rhetorical relations could indirectly contribute to a classifier of temporal relations. Corpus-based methods for computing discourse structure are beginning to emerge (e.g., Marcu, 1999; Soricut & Marcu, 2003; Baldridge & Lascarides, 2005). But there is currently no automatic mapping from these discourse structures to their temporal consequences; so although there is potential for eventually using linguistic resources labelled with discourse structure to acquire a model of temporal relations, that potential cannot be presently realised. Continuing on the topic of discourse relations, it is worth mentioning Marcu and Echihabi (2002) whose approach bypasses altogether the need for manual coding in a supervised learning setting. A key insight in their work is that rhetorical relations (e.g., EXPLANATION and CONTRAST) are sometimes signalled by an unambiguous discourse connective (e.g., because for EXPLANATION and but for CONTRAST). They extract sentences containing such unambiguous markers from a corpus, and then (automatically) identify the text spans connected by the marker, remove the marker and replace it with the rhetorical relation it signals. A Naive Bayes classifier is trained on this automatically labelled data. The model is designed to be maximally simple and employs solely word bigrams as features. Specifically, bigrams are constructed over the cartesian product of words occurring in the two text spans and it is assumed that word pairs are conditionally independent. Marcu and Echihabi demonstrate that such a knowledge-lean approach performs well, achieving an accuracy of 49.70% when distinguishing six relations (over a baseline of 16.67%). However, since the model relies exlusively on word-co-occurrences, an extremely large training corpus (in the order of 40 M sentences) is required to avoid sparse data (see Sporleder and Lascarides (2005) for more detailed discussion). In a sense, when considering the complexity of various models used to infer temporal and discourse relations, Marcu and Echihabi\u2019s (2002) model lies at the simple extreme of the spectrum, whereas the semantics and inference-based approaches to discourse interpretation (e.", "startOffset": 96, "endOffset": 2414}, {"referenceID": 32, "context": "ularly interested in assessing the performance of models on smaller training sets than those used by Marcu and Echihabi (2002); such models will be useful for classifiers that are trained on data sets", "startOffset": 101, "endOffset": 127}, {"referenceID": 30, "context": "Our work differs from Mani et al. (2003) and Boguraev and Ando (2005) in that we do not", "startOffset": 22, "endOffset": 41}, {"referenceID": 4, "context": "(2003) and Boguraev and Ando (2005) in that we do not", "startOffset": 11, "endOffset": 36}, {"referenceID": 32, "context": "We share with Marcu and Echihabi (2002) the use of data with overt markers as a proxy for hand coded temporal relations.", "startOffset": 14, "endOffset": 40}, {"referenceID": 32, "context": "different from theirs, our work departs from Marcu and Echihabi (2002) in three further important ways.", "startOffset": 45, "endOffset": 71}, {"referenceID": 21, "context": ", Hobbs et al., 1993). Many rules typically draw on the relationships between the verbs in both clauses, or the nouns in both clauses, and so on. Both the disjunctive and conjunctive models are different from Marcu and Echihabi\u2019s (2002) model in several respects.", "startOffset": 2, "endOffset": 237}, {"referenceID": 8, "context": "Features with zero counts are smoothed in both models; we adopt the m-estimate with uniform priors, with m equal to the size of the feature space (Cestnik, 1990).", "startOffset": 146, "endOffset": 161}, {"referenceID": 9, "context": "The latter is a Treebank-style, machine-parsed version of the Wall Street Journal (WSJ, years 1987\u201389) which was produced using Charniak\u2019s (2000) parser.", "startOffset": 128, "endOffset": 146}, {"referenceID": 9, "context": "The latter is a Treebank-style, machine-parsed version of the Wall Street Journal (WSJ, years 1987\u201389) which was produced using Charniak\u2019s (2000) parser. Our study focused on the following (confusion) set of temporal markers: fafter, before, while, when, as, once, until, sinceg. We initially compiled a list of all temporal markers discussed in Quirk, Greenbaum, Leech, and Svartvik (1985) and eliminated markers with frequency less than 10 per million in our corpus.", "startOffset": 128, "endOffset": 391}, {"referenceID": 13, "context": "These constraints are perhaps best illustrated in the system of Dorr and Gaasterland (1995) who examine how inherent (i.", "startOffset": 64, "endOffset": 92}, {"referenceID": 1, "context": "Asher and Lascarides (2003) argue that many of the rules for inferring temporal relations should be specified in terms of the semantic class of the verbs, as opposed to the verb forms themselves, so as to maximise the linguistic generalisations captured by a model of temporal relations.", "startOffset": 0, "endOffset": 28}, {"referenceID": 15, "context": "Accordingly, we use two well-known semantic classifications for obtaining some degree of generalisation over the extracted verb occurrences, namely WordNet (Fellbaum, 1998) and the verb classification proposed by Levin (1995).", "startOffset": 156, "endOffset": 172}, {"referenceID": 14, "context": "Accordingly, we use two well-known semantic classifications for obtaining some degree of generalisation over the extracted verb occurrences, namely WordNet (Fellbaum, 1998) and the verb classification proposed by Levin (1995). Verbs in WordNet are classified in 15 broad semantic domains (e.", "startOffset": 157, "endOffset": 226}, {"referenceID": 14, "context": "Accordingly, we use two well-known semantic classifications for obtaining some degree of generalisation over the extracted verb occurrences, namely WordNet (Fellbaum, 1998) and the verb classification proposed by Levin (1995). Verbs in WordNet are classified in 15 broad semantic domains (e.g., verbs of change, verbs of cognition, etc.) often referred to as supersenses (Ciaramita & Johnson, 2003). We therefore mapped the verbs occurring in main and subordinate clauses to WordNet supersenses. (feature VW). Semantically ambiguous verbs will correspond to more than one semantic class. We resolve ambiguity heuristically by always defaulting to the verb\u2019s prime sense (as indicated in WordNet) and selecting its corresponding supersense. In cases where a verb is not listed in WordNet we default to its lemmatised form. Levin (1995) focuses on the relation between verbs and their arguments and hypothesises that verbs which behave similarly with respect to the expression and interpretation of their arguments share certain meaning components and can therefore be organised into semantically coherent classes (200 in total).", "startOffset": 157, "endOffset": 835}, {"referenceID": 1, "context": "Asher and Lascarides (2003) argue that these classes provide important information for identifying semantic relationships between clauses.", "startOffset": 0, "endOffset": 28}, {"referenceID": 1, "context": "Asher and Lascarides (2003) argue that these classes provide important information for identifying semantic relationships between clauses. Verbs in our data were mapped into their corresponding Levin classes (feature VL); polysemous verbs were disambiguated by the method proposed in Lapata and Brew (1999).3 Again, for verbs not included in Levin, the lemmatised verb form is used.", "startOffset": 0, "endOffset": 307}, {"referenceID": 1, "context": "Noun Identity (N) It is not only verbs, but also nouns that can provide important information about the semantic relation between two clauses; Asher and Lascarides (2003) discuss an example in which having the noun meal in one sentence and salmon in the other serves to trigger inferences that the events are in a part-whole relation (eating the salmon was part of the meal).", "startOffset": 143, "endOffset": 171}, {"referenceID": 1, "context": "Noun Class (NW) As with verbs, Asher and Lascarides (2003) argue in favour of symbolic rules for inferring temporal relations that utilise the semantic classes of nouns wherever possible, so as to maximise the linguistic generalisations that are captured.", "startOffset": 31, "endOffset": 59}, {"referenceID": 27, "context": "Lapata and Brew (1999) develop a simple probabilistic model which determines for a given polysemous verb and its frame its most likely meaning overall (i.", "startOffset": 0, "endOffset": 23}, {"referenceID": 27, "context": "Lapata and Brew (1999) develop a simple probabilistic model which determines for a given polysemous verb and its frame its most likely meaning overall (i.e., across a corpus), without relying on the availability of a disambiguated corpus. Their model combines linguistic knowledge in the form of Levin (1995) classes and frame frequencies acquired from a parsed corpus.", "startOffset": 0, "endOffset": 309}, {"referenceID": 20, "context": "As the rules for inferring temporal relations in Hobbs et al. (1993) and Asher and Lascarides (2003) attest, the predicate argument structure of clauses is crucial to making the correct temporal inferences in many cases.", "startOffset": 49, "endOffset": 69}, {"referenceID": 1, "context": "(1993) and Asher and Lascarides (2003) attest, the predicate argument structure of clauses is crucial to making the correct temporal inferences in many cases.", "startOffset": 11, "endOffset": 39}, {"referenceID": 32, "context": "This model resembles Marcu and Echihabi\u2019s (2002)\u2019s model in that it does not make use of the linguistically motivated features presented in the previous section; all that is needed for estimating its parameters", "startOffset": 21, "endOffset": 49}, {"referenceID": 40, "context": "features and the word-based model also shows, in line with the findings in Sporleder and Lascarides (2005), that linguistic abstractions are useful in overcoming sparse data.", "startOffset": 75, "endOffset": 107}, {"referenceID": 32, "context": "This result is in agreement with Marcu and Echihabi (2002) who employ a very large corpus (1 billion words) for training their word-based model.", "startOffset": 33, "endOffset": 59}, {"referenceID": 12, "context": "uncorrelated (Dietterich, 1997).", "startOffset": 13, "endOffset": 31}, {"referenceID": 5, "context": "Multiple classifiers can be generated either by using subsamples of the training data (Breiman, 1996a; Freund & Shapire, 1996) or by manipulating the set of input features available to the component learners (Cherkauer, 1996).", "startOffset": 86, "endOffset": 126}, {"referenceID": 10, "context": "Multiple classifiers can be generated either by using subsamples of the training data (Breiman, 1996a; Freund & Shapire, 1996) or by manipulating the set of input features available to the component learners (Cherkauer, 1996).", "startOffset": 208, "endOffset": 225}, {"referenceID": 46, "context": "A more sophisticated combination method is stacking where a learner is trained to predict the correct output class when given as input the outputs of the ensemble classifiers (Wolpert, 1992; Breiman, 1996b; van Halteren et al., 2001).", "startOffset": 175, "endOffset": 233}, {"referenceID": 6, "context": "A more sophisticated combination method is stacking where a learner is trained to predict the correct output class when given as input the outputs of the ensemble classifiers (Wolpert, 1992; Breiman, 1996b; van Halteren et al., 2001).", "startOffset": 175, "endOffset": 233}, {"referenceID": 1, "context": "Asher and Lascarides\u2019 (2003) symbolic theory of discourse interpretation also emphasises the importance of lexical information in inferring", "startOffset": 0, "endOffset": 29}, {"referenceID": 26, "context": ", of the two clauses) as well as complex inferences over real world knowledge (e.g., Hobbs et al., 1993; Lascarides & Asher, 1993; Kehler, 2002).", "startOffset": 78, "endOffset": 144}, {"referenceID": 20, "context": ", Hobbs et al., 1993; Lascarides & Asher, 1993; Kehler, 2002). Our best model achieved an F-score of 69.1% on the interpretation task and 93.4% on the fusion task. This performance is a significant improvement over the baseline and compares favourably with human performance on the same tasks. Our experiments further revealed that not only lexical but also syntactic information is important for both tasks. This result is in agreement with Soricut and Marcu (2003) who find that syntax trees encode sufficient information to enable accurate derivation of discourse relations.", "startOffset": 2, "endOffset": 467}, {"referenceID": 4, "context": "The approach presented in this paper can be also combined with the annotations present in the TimeML corpus in a semi-supervised setting similar to Boguraev and Ando (2005) to yield", "startOffset": 148, "endOffset": 173}], "year": 2011, "abstractText": "In this paper we propose a data intensive approach for inferring sentence-internal temporal relations. Our approach bypasses the need for manual coding by exploiting the presence of temporal markers like after, which overtly signal a temporal relation. Our experiments concentrate on two tasks relevant for applications which either extract or synthesise temporal information (e.g., summarisation, question answering). Our first task focuses on interpretation: given a subordinate clause and main clause, identify the temporal relation between them. The second is a fusion task: given two clauses and a temporal relation between them, decide which one contained the temporal marker (i.e., identify the subordinate and main clause). We compare and contrast several probabilistic models differing in their feature space, linguistic assumptions and data requirements. We evaluate performance against a gold standard corpus and also against human subjects performing the same tasks. The best model achieves 69.1% F-score in inferring the temporal relation between two clauses and 93.4% F-score in distinguishing the main vs. the subordinate clause, assuming that the temporal relation is known.", "creator": "LaTeX with hyperref package"}}}