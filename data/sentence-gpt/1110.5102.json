{"id": "1110.5102", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Oct-2011", "title": "Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models", "abstract": "Scene understanding includes many related sub-tasks, such as scene categorization, depth estimation, object detection, etc. Each of these sub-tasks is often notoriously hard, and state-of-the-art classifiers already exist for many of them. These classifiers operate on the same raw image and provide correlated outputs. It is desirable to have an algorithm that can capture such correlation without requiring any changes to the inner workings of any classifier.\n\n\n\nWith these classifiers and their respective classes, the overall picture should look something like this:\nNow we have our own algorithm, which is in its infancy, and we have the following tools to do it:\nThe most popular tool to solve this problem is the GIST API, which works in parallel with the API itself. I'm currently using this API for a simple GIST project, so let's just use it for our purposes. This example is a bit like the following:\n#!/usr/bin/python/GIST_API.py\nThe next thing we need to do is to create a simple GIST API for the object, and define it as a method on a GIST repository:\n#!/usr/bin/python/GIST_API.py\nFor a complete version of this code, see the documentation for the tool:\n#!/usr/bin/python/GIST_API.py\nWe'll take a look at the GIST API itself to give us a very simple implementation of the GIST API and the GIST API:\n#!/usr/bin/python/GIST_API.py\nThe GIST API is a fairly straightforward interface to the GIST API and we'll start our program using one of the few interfaces in this class:\n#!/usr/bin/python/GIST_API.py\nThe other two methods we'll use for each GIST API are the method on a GIST repository:\n#!/usr/bin/python/GIST_API.py\nThe GIST API is a straightforward interface to the GIST API and we'll start our program using one of the few interfaces in this class:\n#!/usr/bin/python/GIST_API.py\nThe GIST API is a relatively straightforward interface to the GIST API and we'll start our program using one of the few interfaces in this class:\n#!/usr/bin/python/GIST_API.py\nThe GIST API is", "histories": [["v1", "Mon, 24 Oct 2011 00:31:00 GMT  (6019kb,D)", "http://arxiv.org/abs/1110.5102v1", "14 pages, 11 figures"]], "COMMENTS": "14 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["congcong li", "adarsh kowdle", "ashutosh saxena", "tsuhan chen"], "accepted": true, "id": "1110.5102"}, "pdf": {"name": "1110.5102.pdf", "metadata": {"source": "CRF", "title": "Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models", "authors": ["Congcong Li", "Tsuhan Chen"], "emails": ["apk64}@cornell.edu,", "tsuhan@ece.cornell.edu"], "sections": [{"heading": null, "text": "Index Terms\u2014Scene understanding, Classification, Machine learning, Robotics.\nF"}, {"heading": "1 INTRODUCTION", "text": "O NE of the primary goals in computer vision is holisticscene understanding, which involves many sub-tasks, such as depth estimation, scene categorization, saliency detection, object detection, event categorization, etc. (See Figure 1.) Each of these tasks explains some aspect of a particular scene and in order to fully understand a scene, we would need to solve for each of these sub-tasks. Several independent efforts have resulted in good classifiers for each of these sub-tasks. In practice, we see that the subtasks are coupled\u2014for example, if we know that the scene is an indoor scene, it would help us estimate depth from that single image more accurately. In another example in the robotic grasping domain, if we know what kind of object we are trying to grasp, then it is easier for a robot to figure out how to pick it up. In this paper, we propose a unified model that jointly optimizes for all the sub-tasks, allowing them to share information and guide the classifiers towards a joint optimal. We show that this can be seamlessly applied across different applications.\nRecently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.e., a hardcoded rule is used) and often an intimate knowledge of the inner workings of the individual classifiers is required.\n\u2022 Congcong Li, Adarsh Kowdle and Tsuhan Chen are with the School of Electrical and Computer Engineering, Cornell University, Ithaca, NY, 14853. E-mail: {cl758, apk64}@cornell.edu, tsuhan@ece.cornell.edu \u2022 Ashutosh Saxena is with the Department of Computer Science, Cornell University, Ithaca, NY, 14853. E-mail: asaxena@cs.cornell.edu\nEven beyond vision, in many other domains, state-of-the-art classifiers already exist for many sub-tasks. However, these carefully engineered models are often tricky to modify, or even to simply re-implement from the available descriptions. Heitz et. al. [11] recently developed a framework for scene understanding called Cascaded Classification Models (CCM) treating each classifier as a \u2018black-box\u2019. Each classifier is repeatedly instantiated with the next layer using the outputs of the previous classifiers as inputs. While this work proposed a method of combining the classifiers in a way that increased the performance in all of the four tasks they considered, it had a drawback that it optimized for each task independently and there was no way of feeding back information from later classifiers to earlier classifiers during training. This feedback can potentially help the CCM achieve a more optimal solution.\nIn our work, we propose Feedback Enabled Cascaded Classification Models (FE-CCM), which provides feedback from the later classifiers to the earlier ones, during the training phase. This feedback, provides earlier stages information about what error modes should be focused on, or what can be ignored without hurting the performance of the later classifiers. For example, misclassifying a street scene as highway may not hurt as much as misclassifying a street scene as open country. Therefore we prefer the first layer classifier to focus on fixing the latter error instead of optimizing the training accuracy. In another example, allowing the depth estimation to focus on some specific regions can help perform better scene categorization. For instance, the open country scene is characterized by its upper part as a wide sky area. Therefore, estimating the\nar X\niv :1\n11 0.\n51 02\nv1 [\ncs .C\nV ]\n2 4\nO ct\n2 01\n1\n2 Scene:\nOpen-country\nEvent: Polo\nObject detection Geometric layout Saliency detection\nDepth estimation Scene and Event Categorization Test image\nFig. 1. Given a test image, Holistic Scene Understanding corresponds to inferring the labels for all possible scene understanding dimensions. In our work, we infer labels corresponding to, scene categorization, event categorization, depth estimation (Black = close, white = far), object detection, geometric layout (green = vertical, red = horizontal, blue = vertical) and saliency detection (cyan = salient) as shown above and achieve this jointly using one unified model. Note that different tasks help each other, for example, the depth estimate of the scene can help the object detector look for the horse; the object detection can help perform better saliency detection, etc.\ndepth well in that region by sacrificing some regions in the bottom may help to correctly classify an image. In detail, we do so by jointly optimizing all the tasks; the outputs of the first layers are treated as latent variables and training is done using an iterative algorithm. Another benefit of our method is that each of the classifiers can be trained using their own independent training datasets, i.e., our model does not require a datapoint to have labels for all the sub-tasks, and hence it scales well with heterogeneous datasets.\nIn our approach, we treat each classifier as a \u2018black-box\u2019, with no restrictions on its operation other than requiring the ability to train on data and have an input/output interface. (Often each of these individual classifier could be quite complex, e.g., producing labelings over pixels in an entire image.) Therefore, our method is applicable to many other tasks that have different but correlated outputs.\nIn extensive experiments, we show that our method achieves significant improvements in the performance of all the six sub-tasks we consider: depth estimation, object detection, scene categorization, event categorization, geometric labeling and saliency detection. We also successfully apply the same model to two robotics applications: robotic grasping, and robotic object detection.\nThe rest of the paper is organized as follows. We first define holistic scene understanding and discuss the related works in Section 2. We describe our FE-CCM method in Section 3 followed by the discussion about handling heterogeneous datasets in Section 4. We provide the implementation details of the classifiers in Section 5. We present the experiments and results in Section 6 and some robotic applications in Section 7. We finally conclude in Section 8."}, {"heading": "2 OVERVIEW OF SCENE UNDERSTANDING", "text": ""}, {"heading": "2.1 Holistic Scene Understanding", "text": "When we look at an image of a scene, such as in Figure 1, we are often interested in answering several different questions: What objects are there in the image? How far are things? What is going on in the scene? What type of scene it is? And so on. These are only a few examples of questions in the area of scene understanding; and there may even be more.\nIn the past, the focus has been to address each task in isolation, where the goal of each task is to produce a label Yi \u2208 Si for the ith sub-task. If we are considering depth estimation (see Figure 1), then the label would be Y1 \u2208 S1 = R100\u00d7100+ for continuous values of depth in a 100\u00d7 100 output. For scene categorization, we will have Y2 \u2208 S2 = {1, . . . ,K} for K scene classes. If we have n subtasks, then we would have to produce an output as:\nY = {Y1, . . . , Yn} \u2208 S1 \u00d7 S2 . . .\u00d7 Sn. The interesting part here is that often we want to solve different combinations of the sub-tasks depending on the situation. The goal of this work is to design an algorithm that does not depend on the particular sub-tasks in question."}, {"heading": "2.2 Related Work", "text": "Cascaded classifiers. Using information from related tasks to improve the performance of the task in question has been studied in various fields of machine learning. The idea of cascading layers of classifiers to aid a task was first introduced with neural networks as multi-level perceptrons where, the output of the first layer of perceptrons is passed on as input to the next layer [12\u201314]. However, it is often hard to train neural networks and gain an insight into its operation, making it hard to work for complicated tasks.\nThe idea of improving classification performance by combining outputs of many classifiers is used in methods such as Boosting [15], where many weak learners are combined to obtain a more accurate classifier; this has been applied to tasks such as face detection [16, 17]. To incorporate contextual information, Fink and Perona [18] exploited local dependencies between objects in a boosting framework, but did not allow for multiple rounds of communication between objects. Torralba et al. [19] introduced Boosted Random Fields to model object dependency, which used boosting to learn the graph structure and local evidence of a conditional random field. Tu [20] proposed a more general framework which used pixel-level label maps to learn a contextual model through a cascaded classifier approach. All these works mainly consider the interactions between labels of the same type. However, in our CCM framework [21, 22], the focus is on capturing contextual interactions between labels of different types. Furthermore, compared to the feed-forward only cascade method in [20], our model with feedback not only iteratively refines the contextual interactions, but also refines the individual classifiers to provide helpful context. Sensor fusion. There has been a huge body of work in the area of sensor fusion where classifiers output the same labels but work with different modalities, each one giving additional information and thus improving the performance, e.g., in biometrics, data from voice recognition and face recognition is combined [23]. However, in our scenario, we consider multiple tasks where each classifier is tackling a different problem (i.e., predicting different labels), with the same input being provided to all the classifiers. Structured Models for combining tasks. While the methods discussed above combine classifiers to predict the same labels, there is a group of works that designs models\n3 for predicting heterogenous labels. Kumar and Hebert [1] developed a large MRF-based probabilistic model to link multi-class segmentation and object detection. Li et al. [24] modeled mulitple interactions within tasks and across tasks by defining a MRF over parameters. Similar efforts have been made in the field of natural language processing. Sutton and McCallum [6] combined a parsing model with a semantic role labeling model into a unified probabilis-\ntic framework that solved both simultaneously. Ando and Zhang [25] proposed a general framework for learning predictive functional structures from multiple tasks. All these models require knowledge of the inner workings of the individual classifiers, which makes it hard to fit existing state-of-the-art classifiers of certain tasks into the models.\nStructured learning algorithms (e.g., [26\u201328]) can also be a viable option for the setting of combining multiple tasks. There has been recent development in structured learning on handling latent variables (e.g. hidden conditional random field [29], latent structured SVM [30]), which can be potentially applied to multi-task settings with disjoint datasets. With considerable understanding into each of the tasks, the loss function in structured learning provides a nice way to leverage different tasks. However, in this work, we focus on developing a more generic algorithm that can be easily applied even without intimate knowledge of the tasks.\nThere have been many works which show that with a well-designed model, one can improve the performance of a particular task by using cues from other tasks (e.g., [7\u2013 9]). Saxena et al. manually designed the terms in an MRF to combine depth estimation with object detection [2] and stereo cues [10]. Sudderth et al. [5] used object recognition to help 3D structure estimation. Context. There is a large body of work that leverages contextual information to help specific tasks. Various sources of context have been explored, ranging from the global scene layout, interactions between objects and regions to local features. To incorporate scene-level information, Torralba et al. [31, 32] used the statistics of low-level features across the entire scene to prime object detection or help depth estimation. Hoiem et al. [33] used 3D scene information to provide priors on potential object locations. Park et al. [34] used the ground plane estimation as contextual information for pedestrian detection. Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345]. These methods improve the performance of some specific tasks by combining information from different aspects. However, most of these methods can not be applied to cases when we only have \u201cblack-box\u201d classifiers for the individual tasks. Holistic Scene Understanding. Hoiem et. al. [3] proposed an innovative but ad-hoc system that combined boundary detection and surface labeling by sharing some lowlevel information between the classifiers. Li et. al. [4, 46] combined image classification, annotation and segmentation with a hierarchical graphical model. However, these methods required considerable attention to each classifier, and considerable insight into the inner workings of each task and also the connections between them. This limits\n!\"\"#$%&'()'#*\n!\"\"#$+),-*\n\u03b81 \u03b82\n\u03c91\n./)0012\"'3* ./)0012\"'4*\n./)0012\"'3* ./)0012\"'4*\n./)0012\"'5*\n./)0012\"'5*\n6*6*6*6*6*6*6**\nY1 Y2\nZ1 Z2\nYn\nZn\n\u03b8n\n\u03c92 \u03c9n\n7389:* 7489:* 7589:*\n6*6*6*6*6*6*6**\nFig. 2. The proposed Feed-back enabled cascaded classification model (FE-CCM) for combining related classifiers. (\u2200i \u2208 {1, 2, . . . , n}, \u03a8i(X) = Features corresponding to Classifieri extracted from image X, Zi = Output of the Classifieri in the first stage parameterized by \u03b8i, Yi = Output of the Classifieri in the second stage parameterized by \u03c9i). In the proposed FE-CCM model, there is feed-back from the latter stages to help achieve a model which optimizes all the tasks considered, jointly. Here Classifieri\u2019s on the two layers can have different forms though they are for the same classification task. (Note that different colors of lines are used only to make the figure more readable.)\nthe generality of the approaches in introducing new tasks easily or being applied to other domains. Deep Learning. There is also a large body of work in the areas of deep learning, and we refer the reader to Bengio and LeCun [47] for a nice overview of deep learning architectures and Caruana [48] for multitask learning with shared representation. While efficient back-propagation methods like [49] have been commonly used in learning a multilayer network, it is not as easy to apply to our case where each node is a complex classifier. Most works in deep learning (e.g., [50\u201352]) are different from our work in that, those works focus on one particular task (same labels) by building different classifier architectures, as compared to our setting of different tasks with different labels. Hinton et al. [51] used unsupervised learning to obtain an initial configuration of the parameters. This provides a good initialization and hence their multi-layered architecture does not suffer from local minimas during optimization. At a high-level, we can also look at our work as a multi-layered architecture (where each node typically produces complex outputs, e.g., labels over the pixels in the image); and initialization in our case comes from existing state-of-the-art individual classifiers. Given this initialization, our training procedure finds parameters that (consistently) improve performance across all the sub-tasks."}, {"heading": "3 FEEDBACK ENABLED CASCADED CLASSIFICATION MODELS", "text": "In the field of scene understanding, a lot of independent research into each of the vision sub-tasks has led to excellent classifiers. These independent classifiers are typically trained on different or heterogenous datasets due to the lack of ground-truth labels for all the sub-tasks. In addition, each of these classifiers come with their own learning and inference methods. Our goal is to consider each of them as a \u2018black-box\u2019, which makes it easy to combine them. We describe what we mean by \u2018black-box classifiers\u2019 below.\nBlack-box Classifier. A black-box classifier, as the name suggests, is a classifier for which operations (such as learning and inference algorithms) are available for use, but their inner workings are not known. We assume that,\n4 given the training dataset X, features extracted \u03a8(X) and the target outputs of the ith task Yi, the black-box classifier has some internal learning function f ilearn with parameters \u03b8i that optimizes the mapping from the inputs to the outputs for the training data. 1 Once the parameters have been learnt, given a new data point, X with features \u03a8(X) \u2208 RK , where K can be changed as desired,2 the black-box classifier returns the output Y\u0302i according to its internal inference function f iinfer. This is illustrated through the equations below. For the ith task,\nLearning : \u03b8\u2217i = optimize \u03b8i f ilearn(\u03a8(X),Yi; \u03b8i) (1)\nInference : Y\u0302i = optimize Yi\nf iinfer(\u03a8(X), Yi; \u03b8 \u2217 i ). (2)\nThis approach of treating each classifier as a black-box allows us to use different existing classifiers which have been known to perform well at specific sub-tasks. Furthermore, without changing their inner workings, it allows us to compose them into one model which exploits the information from each sub-task to aid holistic scene understanding."}, {"heading": "3.1 Our Model", "text": "Our model is built in the form of a two-layer cascade, as shown in Figure 2. The first layer consists of an instantiation of each of the black-box classifiers with the image features as input. The second layer is a repeated instantiation of each of the classifiers with the first layer classifier outputs as well as the image features as inputs. Note that the repeated classifier on the second layer is not necessary to have the same mathematical form with the one on the first layer. Instead, we consider it as a repeated instantiation only because they are used for the same classification task.\nNotation: We consider n related sub-tasks Classifieri, i \u2208 {1, 2, . . . , n} (Figure 2). We describe the notations used in this paper as follows:\n\u03a8i(X) Features corresponding to Classifieri extracted from image X . Zi, Z Zi indicates output from the first layer Classifieri. Many classifiers output continuous scores instead of labels. In cases where this is not the case, it is trivial to convert a binary classiers output to a log-odds scores. For aK-class (K > 2) classifier, we consider the output to be a K-dimensional vector. Z indicates the set {Z1, Z2, . . . , Zn}.\n\u03b8i, \u0398 \u03b8i indicates parameters corresponding to first layer Classifieri. \u0398 indicates the set {\u03b81, . . . , \u03b8n}. Yj , Y Yj indicates output for the jth task in the second layer, using the original features \u03a8j(X) as well as all the outputs from the first layer as input. Y indicates the set {Y1, Y2, . . . , Yn}. \u03c9j , \u2126 \u03c9j indicates parameters for the second layer Classifierj . \u2126 indicates the set {\u03c91, . . . , \u03c9n}. \u0393j , \u0393 Dataset for the jth task, which consists of labeled pairs {X,Yj} in the training set. \u0393 represents all the labeled data. f iinfer, f i learn the internal inference function and learning function\nfor the ith classifier on the first layer. f \u2032iinfer, f \u2032i learn the internal inference function and learning function for the ith classifier on the second layer. 1. Unless specified, the regular symbols (e.g. X , Yi, etc.) are used for a particular data-point, and the bold-face symbols (e.g. X, Yi, etc.) are used for a dataset.\n2. If the input dimension of the black-box classifier can not be changed, then we will use that black-box in the first layer only.\nWith the notations in place we will now first describe the inference and learning algorithms for the proposed model in the following sections, followed by probabilistic interpretation of our method."}, {"heading": "3.2 Inference Algorithm", "text": "During inference, the inputs \u03a8i(X) are given and our goal is to infer the final outputs Yi. Using the learned parameters \u03b8i for the first level of classifiers and \u03c9i for the second level of classifiers, we first infer the first-layer outputs Zi and then infer the second-layer outputs Yi. More formally, we perform the following.\nZ\u0302i = optimize Zi\nf iinfer(\u03a8i(X), Zi; \u03b8\u0302i) (3)\nY\u0302i = optimize Yi\nf \u2032 i infer([\u03a8i(X) Z\u0302], Yi; \u03c9\u0302i) (4)\nThe inference algorithm is given in Algorithm 1. This method allows us to use the internal inference function (Equation 2) of the black-box classifiers without knowing its inner workings. Note that the complexity here is no more than constant times the complexity of inference in the original classiers.\nAlgorithm 1 Inference 1. Inference for first layer:\nfor i = 1 : n Infer the outputs of the ith classifier using Equation 3;\nend 2. Inference for second layer:\nfor i = 1 : n Infer the outputs of the ith classifier using Equation 4; end"}, {"heading": "3.3 Learning Algorithm", "text": "During the training stage, the inputs \u03a8i(X) as well as the target outputs, Y1, Y2, . . . , Yn of the second level of classifiers, are all observed (because the ground-truth labels are available). In our algorithm, we consider Z (outputs of layer 1 and inputs to layer 2) as hidden variables. In previous work, Heitz et al. [11] assume that each layer is independent and that each layer produces the best output independently (without consideration for other layers), and therefore use the ground-truth labels for Z even for training the classifiers in the first layer.\nOn the other hand, we want to optimize for the final outputs as much as possible. Thus the first layer classifiers need not perform their best (w.r.t. groundtruth), but rather focus on error modes that would result in the second layer\u2019s output (Y1, Y2, . . . , Yn) being more correct. Therefore, we learn the model through an iterative ExpectationMaximization formulation, given the independencies between classifiers represented by the model in Figure 2. In one step (Feed-forward step) we assume the variables Zi\u2019s are known and learn the parameters and in the other step (Feed-back step) we fix the parameters estimated previously and estimate the variables Zi\u2019s. Since the Zi\u2019s are not fixed to the ground truth, as the iterations progress, the first level of classifiers start focusing on the error modes which would give the best improvement in performance at the end of the second level of classifiers. The learning algorithm is summarized in Algorithm 2.\n5 Algorithm 2 Learning 1. Initialize latent variables Z with the ground-truth Y .\n2. Do until convergence or maximum iteration: { Feed-foward step: Fix latent variables Z , estimate the parameters \u0398 and \u2126 using Equation 5 and Equation 6. Feed-back step: Fix the parameters \u0398 and \u2126, compute latent variables Z using Equation 7. }\nInitialization: We initialize the model by setting the latent variables Zi\u2019s to the groundtruth, i.e. Zi = Z gt i . Training with this initialization, our cascade is equivalent to CCM in [11], where the classifiers (and the parameters) in the first layer are similar to the original state-of-the-art classifier and the classifiers in the second layer use the outputs of the first layer in addition to the original features as input. Feed-forward Step: In this step, we estimate the parameters \u0398 and \u2126. We assume that the latent variables Zi\u2019s are known (and Yi\u2019s are known because they are the groundtruth during learning, i.e. Yi = Y gt i ). We then learn the parameters of each classifier independently. Learning \u03b8i is precisely the learning problem of the \u2018black-box classifier\u2019, and learning \u03c9i is also an instantiation of the original learning problem, but with the original input features appended with the outputs of the first level classifiers. Therefore, we can use the learning method provided by the individual black-box classifier (Equation 1).\n\u03b8\u0302i = optimize \u03b8i\nf ilearn(\u03a8i(X),Zi; \u03b8i) (5)\n\u03c9\u0302i = optimize \u03c9i\nf \u2032 i learn([\u03a8i(X) Z],Yi;\u03c9i) (6)\nWe now have the parameters for all the classifiers. Feed-back Step: In the second step, we will estimate the values of the variables Zi\u2019s assuming that the parameters are fixed (and Yi\u2019s are given because the ground-truth is available, i.e. Yi = Y gt i ). This feed-back step is the crux that provides information to the first-layer classifiers what error modes should be focused on and what can be ignored without hurting the final performance. Given \u03b8i\u2019s and \u03c9i\u2019s are fixed, we want the Zi\u2019s to be good predictions from the first-layer classifiers and also help to increase the correction predictions of Yi\u2019s as much as possible. We optimize the following function for the feed-back step:\noptimize Z n\u2211 i=1 ( J i1(\u03a8i(X), Zi; \u03b8\u0302i) + J i 2(\u03a8i(X),Z, Yi; \u03c9\u0302i) ) (7) where J i1\u2019s and J i 2\u2019s are functions respectively related to the first-layer classifiers and the second-layer classifiers. one option is to have J i1(\u03a8i(X), Zi; \u03b8\u0302i) = f i infer(\u03a8i(X), Zi; \u03b8\u0302i) and J i2(\u03a8i(X),Z, Yi; \u03c9\u0302i) = f \u2032 i infer([\u03a8i(X),Z], Yi; \u03c9\u0302i) if the intrinsic inference functions for the classifiers are known. More discussions will be given in Section 3.4 if the intrinsic functions are unknown. The updated Zi\u2019s will be used to re-learn the classifier parameters in the feed-forward step of next iteration. Note that the updated Zi\u2019s have continuous values. If the internal learning function of a classifier accepts only labels, we threshold the values of Zi\u2019s to get labels."}, {"heading": "3.4 Probabilistic Interpretation", "text": "Our algorithm can be explained with a probabilistic interpretation where the goal is to maximize the log-likelihood of the outputs of all tasks given the observed inputs, i.e., logP (Y|X), where X is an image belonging to training set \u0393. Therefore, the goal of the proposed model shown in Figure 2 is to maximize\nlog \u220f X\u2208\u0393 P (Y|X; \u0398,\u2126) (8)\nTo introduce the hidden valuables Zi\u2019s, we expand Equation 8 as follows, using the independencies represented by the directed model in Figure 2.\n= \u2211 X\u2208\u0393 log \u2211 Z P (Y1, . . . , Yn,Z|X; \u0398,\u2126) (9)\n= \u2211 X\u2208\u0393 log \u2211 Z n\u220f i=1 P (Yi|\u03a8i(X),Z;\u03c9i)P (Zi|\u03a8i(X); \u03b8i) (10) However, the summation inside the log makes it difficult to learn the parameters. Motivated by the Expectation Maximization algorithm [53], we iterate between the two steps as described in the following. Again we initialize the classifiers by learning the classifiers with ground-truth as discussed Section 3.3. Feed-forward Step: In this step, we estimate the parameters by assuming that the latent variables Zi\u2019s are known (and Yi\u2019s are known anyway because they are the groundtruth). This results in\nmaximize \u03b81,...,\u03b8n,\u03c91,...,\u03c9n \u2211 X\u2208\u0393 log n\u220f i=1 P (Yi|\u03a8i(X),Z;\u03c9i)P (Zi|\u03a8i(X); \u03b8i)\n(11)\nNow in this feed-forward step, the terms for maximizing the different parameters turn out to be independent. So, for the ith classifier we have:\nmaximize \u03b8i \u2211 X\u2208\u0393 logP (Zi|\u03a8i(X); \u03b8i) (12)\nmaximize \u03c9i \u2211 X\u2208\u0393 logP (Yi|\u03a8i(X),Z;\u03c9i) (13)\nNote that the optimization problem nicely breaks down into the sub-problems of training the individual classifier for the respective sub-tasks. We can solve each sub-problem separately given the probabilistic interpretation of the corresponding classifier. When the classifier is taken as \u2018blackbox\u2019, this can be approximated using the original learning method provided by the individual black-box classifier (Equation 5 and Equation 6) Feed-back Step: In this step, we estimate the values of the latent variables Zi\u2019s assuming that the parameters are fixed. We perform MAP inference on Zi\u2019s (and not marginalization). This can be considered as a special variant of the general EM framework (hard EM, [54]). Using Equation 10, we get the following optimization problem: maximize\nZ logP (Y1, . . . , Yn,Z|X; \u03b8\u03021, . . . , \u03b8\u0302n, \u03c9\u03021, . . . , \u03c9\u0302n)\u21d4\nmaximize Z n\u2211 i=1 ( logP (Yi|\u03a8i(X),Z; \u03c9\u0302i) + logP (Zi|\u03a8i(X); \u03b8\u0302i) ) (14)\nThis maximization problem requires that we have access to the characterization of the individual black-box classifiers\n6 in a probabilistic form. If the probabilistic interpretations of the classifiers are known, we can solve the above function accordingly. Note that Equation 14 is same as Equation 7 with J i1(\u03a8i(X), Zi; \u03b8\u0302i) = logP (Zi|\u03a8i(X); \u03b8\u0302i) and J i2(\u03a8i(X),Z, Yi; \u03c9\u0302i) = logP (Yi|\u03a8i(X),Z; \u03c9\u0302i).\nIn some cases, the classifier log-likelihoods in Equation 14 actually turn out to be convex. For example, if the individual classifiers are linear or logistic classifiers, the minimization problem is convex and can be solved using gradient descent (or any such method).\nHowever, if the probabilistic interpretations of the classifiers are unknown, the feedback step requires extra modeling. Some modeling options are provided as follows:\n\u2022 Case 1: Insight into the vision problem is available. In this case, one could use the domain knowledge of the task into the problem to properly model J i1\u2019s and J i 2\u2019s. \u2022 Case 2: No insight into the vision problem is available and no internal function of the original classifier is known. In this case, we formulate the J i1\u2019s and J i 2\u2019s\nas follows. The J i1 is defined to be a distance function between the target Zi and the estimated Z\u0302i, which serves as a regularization for the first-layer classifiers.\nJ i1(\u03a8i(X), Zi; \u03b8\u0302i) = \u2223\u2223\u2223\u2223\u2223\u2223Zi \u2212 Z\u0302i\u2223\u2223\u2223\u2223\u2223\u22232\ns.t. Z\u0302i = optimize Zi\nf iinfer(\u03a8i(X), Zi; \u03b8\u0302i) (15)\nTo formulate J i2\u2019s, we make a variational approximation on the output of the second-layer classifier for task i (i.e., approximating it as a Gaussian, [55]) to get:\nminimize \u03b1i \u2211 X\u2208\u0393 \u2223\u2223\u2223\u2223\u2223\u2223Y\u0302i \u2212 \u03b1Ti [\u03a8i(X), Z\u0302]\u2223\u2223\u2223\u2223\u2223\u22232 2\n(16)\nwhere \u03b1i are parameters of the approximation model. Y\u0302i is the actual output of the second layer classifier for the task i, i.e. Y\u0302i = optimizeYi f \u2032 infer([\u03a8i(X) Z\u0302], Yi; \u03c9\u0302i). Then we define the J i2\u2019s as follows.\nJ i2(\u03a8i(X),Z, Yi; \u03c9\u0302i) = \u2223\u2223\u2223\u2223\u2223\u2223Yi \u2212 \u03b1\u0302Ti [\u03a8i(X),Z]\u2223\u2223\u2223\u2223\u2223\u22232\n2 (17)\nSparsity: Note that the parameter \u03b1i is typically extremely high-dimensional (and increases with the number of tasks) because the second layer classifiers take as input the original features as well as outputs of all previous layers. The learning for the approximation model may become ill-conditioned. Therefore, we want our model to select only a few non-zero weights, i.e., only a few non-zero entries in \u03b1i. We do this by introducing the l1 sparsity in the parameters [56]. So Equation 16 is extended as follows.\nminimize \u03b1i \u2211 X\u2208\u0393 (\u2223\u2223\u2223\u2223\u2223\u2223Y\u0302i \u2212 \u03b1Ti [\u03a8i(X), Z\u0302]\u2223\u2223\u2223\u2223\u2223\u22232 2 + \u03b2 |\u03b1i| ) (18)\nInference: As introduced in Section 3.2, our inference procedure consists of two steps: first maximize over hidden variable Z and then maximize over Y . 3\nZ\u0302 = argmax Z logP (Z|X, \u0398\u0302) (19)\nY\u0302 = argmax Y logP (Y|Z\u0302, X, \u2126\u0302) (20)\n3. Another alternative would have been to maximize P (Y |X) =\u2211 Z P (Y, Z|X); however, this would require marginalization over the variable Z which is expensive to compute.\nGiven the structure of our directed graph, the outputs for different classifiers on the same layer are independent given their inputs and parameters. Therefore, Equations 19 and 20 are equivalent to the following:\nZ\u0302i = argmax Zi\nlogP (Zi|\u03a8i(X); \u03b8\u0302i), i = 1, . . . , n (21)\nY\u0302i = argmax Yi\nlogP (Yi|\u03a8i(X); Z\u0302; \u03c9\u0302i), i = 1, . . . , n (22)\nAs we see, Equation 21 and Equation 22 are instantiations of Equation 3 and Equation 4 in the probabilistic form."}, {"heading": "4 TRAINING WITH HETEROGENEOUS DATASETS", "text": "Often real datasets are disjoint for different tasks, i.e, each datapoint does not have the labels for all the tasks. Our formulation handles this scenario well. In this section, we show our formulation for this general case, where we use \u0393i as the dataset that has labels only for the ith task.\nIn the following we provide the modifications to the feedforward step and the feed-back step while dealing with disjoint datasets, i.e., data in dataset \u0393i only have labels for the ith task. These modifications also allow us to develop different variants of the model, described in Section 4.1. Feed-forward Step: Using the feedback step, we can have Zi\u2019s for all the data. Therefore, we use all the datasets in order to re-learn each of the first-layer classifiers. If the internal learning function of the black-box classifier is additive over the data points, then we have\n\u03b8\u0302i = optimize \u03b8i \u2211 j \u2211 X\u2208\u0393j \u03c0jf i learn(\u03a8i(X), Zi; \u03b8i), (23)\nwhere \u03c0j\u2019s are the importance factors given to different datasets, and satisfy \u2211 j \u03c0j = 1. (See Section 4.1 on how to choose \u03c0j\u2019s.) If the internal learning function is not additive over the data points, we provide an alternative solution here. We sample a subset of data Xj from each dataset \u0393j , i.e. Xj \u2286 \u0393j and combine them into a new set X = [X1, . . . ,Xn]. In X, the ratio of data belonging to Xj is equal to \u03c0j , i.e. |X\nj | |X| = \u03c0j , where | \u00b7 | indicates the number of datapoints. Then we can learn the parameters of the first-layer classifiers as follows.\n\u03b8\u0302i = optimize \u03b8i\nf ilearn(\u03a8i(X),Zi; \u03b8i), (24)\nTo re-learn the second-layer classifiers, the only change made to Equation 6 is that instead of using all data while optimizing for a particular task, we use only the data-points that have the ground-truth label for the corresponding task. \u03c9\u0302i = optimize\n\u03c9i\nf \u2032 i learn([\u03a8i(X) Z],Yi;\u03c9i), s.t. X = \u0393i (25)\nFeed-back Step: In this step, we change Equation 7 as follows. Since a datapoint in the set \u0393j only has groundtruth label for the jth task (Yj), we only consider J j 2 in the second term. However, since this datapoint has outputs for all the first-layer classifiers using the feed-forward step, we consider all the J i1\u2019s, i = 1, \u00b7 \u00b7 \u00b7 , n. Therefore, in order to obtain the value of Z corresponding to each data-point X \u2208 \u0393j , we have\noptimize Z n\u2211 i=1 ( J i1(\u03a8j(X), Zi; \u03b8\u0302i) ) + Jj2 (\u03a8j(X),Z, Yj ; \u03c9\u0302j).\n(26)\n7"}, {"heading": "4.1 FECCM: Different Instantiations", "text": "The parameters \u03c0j allow us to formulate three different instantiations of our model.\n\u2022 Unified FECCM: In this instantiation, our goal is to achieve improvements in all tasks with one set of parameters {\u0398,\u2126}. We want to balance the data from different datasets (i.e., with different task labels). Towards this goal, \u03c0j is set to be inversely proportional to the amount of data in the dataset of the jth task. Therefore, the unified FECCM balances the amount of data in different datasets, based on Equation 23.\n\u2022 One-goal FECCM: In this instantiation, we set \u03c0j = 1 if j = k, and \u03c0j = 0 otherwise. This is an extreme setting to favor the specific task k. In this case, the retraining of the first-layer classifiers will only use the feedback from the Classifierk on the second layer, i.e., only use the dataset with labels for the kth task. Therefore, FECCM degrades to a model with only one target task (the kth\ntask) on the second layer and all the other tasks are only instantiated on the first layer. Although the goal in this setting is to completely benefit the kth task, in practice it often results in overfitting and does not always achieve the best results even for the specific task (see Table 1 in Section 6). In this case, we train different models, i.e. different \u03b8i\u2019s and \u03c9i\u2019s, for different target tasks.\n\u2022 Target-Specific FECCM: This instantiation is to optimize the performance of a specific task. As compared to one-goal FECCM where we manually remove the other tasks on the second layer, in this instantiation we keep all the tasks on the second layer and conduct data-driven selection of the parameters \u03c0j for different datasets. In detail, \u03c0j is selected through cross validation on a hold-out set in the learning process in order to optimize the second-layer output of a specific task. Since TargetSpecific FECCM still has all the tasks instantiated on the second layer, the re-training of the first-layer classifiers can still use data from different datasets (i.e., with different task labels). Here we train different models, i.e. different \u03b8i\u2019s and \u03c9i\u2019s, for different target tasks."}, {"heading": "5 SCENE UNDERSTANDING: IMPLEMENTATION", "text": "In this section we describe the implementation details of our instantiation of FE-CCM for scene understanding. Each of the classifiers described below for the sub-tasks are our \u201cbase-model\u201d shown in Table 1. In some sub-tasks, our base-model will be simpler than the state-of-the-art models (that are often hand-tuned for the specific sub-tasks respectively). However, even when using base-models in our FE-CCM, our model will still outperform the stateof-the-art models for the respective sub-tasks (on the same standard respective datasets) in Section 6.\nIn order to explain the implementation details for the different tasks, we will use the following notation. Let i be the index of the tasks we consider. We consider 6 tasks for our experiments on scene understanding: scene categorization (i = 1), depth estimation (i = 2), event\ncategorization (i = 3), saliency detection (i = 4), object detection (i = 5) and geometric labeling (i = 6). The inputs for the jth task at the first layer are given by the low-level features \u03a8j . At the second layer, in addition to the original features \u03a8j , the inputs include the outputs from the first layer classifiers. This is given by,\n\u03a6j = [\u03a8j Z1 Z2 Z3 Z4 Z5 Z6] (27) where, \u03a6j is the input feature vector for the jth task on the second layer, and Zi (i = 1, . . . , 6) represents the output from the ith task which is appended to the input to the jth task on the second layer and so on.\nScene Categorization. For scene categorization, we classify an image into one of the 8 categories defined by Torralba et al. [57] tall building, inside city, street, highway, coast, open country, mountain and forest. We evaluate the performance by measuring the rate of incorrectly assigning a scene label to an image on the MIT outdoor scene dataset [57]. The feature inputs for the first-layer scene classifier \u03a81 \u2208 R512 is the GIST feature [57], extracted at 4 \u00d7 4 regions of the image, on 4 scales and 8 orientations.\nWe use an RBF-Kernel SVM classifier [58], as the firstlayer scene classifier, and a multi-class logistic classifier for the second layer. The output of the first-layer scene classifier Z1 \u2208 R8 is an 8-dimensional vector where each element represents the log-odds score of the corresponding image belonging to a scene category. This 8-dimensional output is fed to each of the second-layer classifiers.\nDepth Estimation. For the single image depth estimation task, we estimate the depth of every pixel in an image. We evaluate the estimation performance by computing the root mean square error of the estimated depth with respect to ground truth laser scan depth using the Make3D Range Image dataset [59, 60]. We uniformly divide each image into 55 \u00d7 305 patches as [59]. The feature inputs for the first-layer depth estimation \u03a82 \u2208 R104 are features which capture texture, color and gradient properties of the patch. This is obtained by convolving the image with Laws\u2019 masks and computing the energy and Kurtosis over the patch along with the shape features as described by Saxena et al. [59].\nWe use a linear regression for the first-level and secondlevel instantiation of the depth estimation module. The output of the first-layer depth estimation Z2 \u2208 R+ is the predicted depth of each patch in the image. In order to feed the first-layer depth output to the second-layer classifiers, for the scene categorization and event categorization tasks, we use a vector with the predicted depth of all patches in the image; for the other tasks, we use the 1-dimensional predicted depth for the patch/pixel/bounding-box, etc.\nEvent Categorization: For event categorization, we classify an image into one of the 8 sports events as defined by Li et al. [46]: bocce, badminton, polo, rowing, snowboarding, croquet, sailing and rock-climbing. For evaluation, we compute the rate of incorrectly assigning an event label to an image. The feature inputs for the first-layer event classifier \u03a83 \u2208 R43 is a 43-dimensional feature vector, which includes the top 30 PCA projections of the 512- dimensional GIST features [61], the 12-dimension global\n8 color features (mean and variance of RGB and YCrCb color channels over the entire image), and a bias term.\nWe use a multi-class logistic classifier on each layer for event classification. The output of the first-layer event classifier Z3 \u2208 R8 is an 8-dimensional vector where each element represents the log-odd score of the corresponding image belonging to a event category. This 8-dimensional output is feed to each of the second-layer classifiers.\nSaliency Detection. The goal of the saliency detection task is to classify each pixel in the image as either salient or non-salient. We use the saliency detection dataset used by Achanta et. al. [62] for our experiments. The feature inputs for the first-layer saliency classifier \u03a84 \u2208 R4 includes the 3-dimensional color-offset features based on the Lab color space as described by Achanta et al. [62] and a bias term.\nWe use a logistic model for the saliency estimation classifiers on both layers. The output of the first-layer saliency classifier Z4 \u2208 R is the log-odd score of a pixel being salient. In order to feed the first-layer saliency detection output to the second-layer classifiers, for the scene categorization and event categorization tasks, we form a vector with the predicted saliency of all the pixels in the image; for the other tasks, we use the 1-dimensional average saliency for the corresponding pixel/patch/bounding-box.\nObject Detection. We consider the following object categories: car, person, horse and cow. We use the train-set and test-set of PASCAL 2006 [63] for our experiments. Our object detection module builds on the part-based detector of Felzenszwalb et. al. [64]. We first generate 5 to 100 candidate windows for each image by applying the partbased detector with a low threshold (over-detection). The feature inputs for the first-layer object detection classifier \u03a85 \u2208 RK are the HOG features extracted based on the candidate window as [65] plus the detection score from the part-based detector [64]. K depends on the number of scales to be considered and the size of the object template.\nWe learn an RBF-kernel SVM model as the first layer classifier. The classifier assigns each window a +1 or 0 label indicating whether the window belongs to the object or not. For the second-layer classifier, we learn a logistic model over the feature vector constituted by the outputs of all first-level tasks and the original HOG feature. We use average precision to quantitatively measure the performance. The output of the first-layer object detection classifier Z5 \u2208 R4 are the estimated 0 or 1 labels for a region to belong to the 4 object categories we consider. In order to feed the first-layer object detection output to the second-layer classifiers, we first generate a detection map for each object. Pixels inside the estimated positive boxes are labeled as \u201c+1\u201d, otherwise they are labeled as \u201c0\u201d. For scene categorization and event categorization on the second layer, we feed all the elements on the map; for the other tasks, we use the 1-dimensional average value on the map for the corresponding pixel/patch/bounding-box.\nGeometric labeling. The geometric labeling task refers to assigning each pixel to one of three geometric classes: support, vertical and sky, as defined by Hoiem et al. [33].\nFor evaluation, we compute the accuracy of assigning the correct geometric label to a pixel. The feature inputs for the first-layer geometry labeling classifier \u03a86 \u2208 R52 are the region-based features as described by Hoiem et al. [33].\nWe use the dataset and the algorithm by [33] as the firstlayer geometric labeling module. To reduce the computation time, we avoid the multiple segmentations and instead use a single segmentation with 100 segments per image. We use a logistic model as the second-layer classifier. The output of the first-layer geometry classifier Z6 \u2208 R3 is a 3-dimensional vector with each element representing the log-odd score of the corresponding pixel belonging to a geometric category. In order to feed the first-layer geometry output to the second-layer classifiers, for scene/event categorization we form a vector with the predicted scores of all pixels; for the other tasks we use the 3-dimensional vector with each element representing the average scores for the corresponding pixel/patch/bounding-box."}, {"heading": "6 EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "6.1 Experimental Setting", "text": "The proposed FE-CCM model is a unified model which jointly optimizes for all the sub-tasks. We believe this is a powerful algorithm in that, while independent efforts towards each sub-task have led to state-of-the-art algorithms that require intricate modeling for that specific sub-task, the proposed approach is a unified model which can beat the state-of-the-art performance in each sub-task and, can be seamlessly applied across different applications.\nWe evaluate our proposed method on combining six tasks introduced in Section 5. In our experiment, the training of FE-CCM takes 4-5 iterations. For each of the sub-tasks in each of the domains, we evaluate our performance on the standard dataset for that sub-task (and compare against the specifically designed state-of-the-art algorithm for that dataset). Note that, with such disjoint yet practical datasets, no image would have ground truth available for more than one task. Our model handles this well.\nIn experiment we evaluate the following algorithms as shown in Table 1, \u2022 Base model: Our implementation (Section 5) of each\nsub-task, which serves as a base model for our FECCM. (The base model uses less information than state-of-the-art algorithms for some sub-tasks.)\n\u2022 All-features-direct: A classifier that takes all the features of all sub-tasks, appends them together, and builds a separate classifier for each sub-task.\n\u2022 State-of-the-art model: The state-of-the-art algorithm for each sub-task respectively on that specific dataset.\n\u2022 CCM: The cascaded classifier model by Heitz et al. [11], which we re-implement for six sub-tasks.\n\u2022 FE-CCM (unified): This is our proposed model. Note that this is one single model which maximizes the joint likelihood of all the sub-tasks.\n\u2022 FE-CCM (one goal): In this case, we have only one sub-task instantiated on the second layer, and the goal is to optimize the outputs of that sub-task. We train a specific one-goal FE-CCM for each sub-task.\n9TABLE 1 Summary of results for the SIX vision tasks. Our method improves performance in every single task. (Note: Bold face corresponds to our\n\u2022 FE-CCM (target specific): In this case, we train a specific FE-CCM for each sub-task, by using crossvalidation to estimate \u03c0j\u2019s in Equation 23. Different values for \u03c0j\u2019s result in different parameters learned for each FE-CCM.\nNote that both CCM and All-features-direct use information from all sub-tasks, and state-of-the-art models also use carefully designed models that implicitly capture information from the other sub-tasks."}, {"heading": "6.2 Datasets", "text": "The datasets used are mentioned in Section 5, and the number of test images in each dataset is shown in Table 1. For each dataset we use the same number of training images as the state-of-the-art algorithm (for comparison). We perform 6-fold cross validation on the whole model with 5 of 6 sub-tasks to evaluate the performance on each task. We do not do cross-validation on object detection as it is standard on the PASCAL 2006 [63] dataset (1277 train and 2686 test images respectively)."}, {"heading": "6.3 Results", "text": "To quantitatively evaluate our method for each of the subtasks, we consider the metrics appropriate to each of the six tasks in Section 5. Table 1 and Figure 3 show that FECCM not only beats state of the art in all the tasks but also does it jointly as one single unified model.\nIn detail, we see that all-features-direct improves over the base model because it uses features from all the tasks.\nThe state-of-the-art classifiers improve on the base model by explicitly hand-designing the task specific probabilistic model [46, 59] or by using adhoc methods to implicitly use information from other tasks [33]. Our FE-CCM model, which is a single model that was not given any manually designed task-specific insight, achieves a more significant improvement over the base model.\nWe also compare the three instantiations of FE-CCM in Table 1 (the last three rows). We observe that the target-specific FE-CCM achieves the best performance, by selecting a set of \u03c0j\u2019s to optimize for each task independently. Though the unified FE-CCM achieves slightly worse performance, it jointly optmizes for all the tasks by training only one set of parameters. The performance of one-goal FE-CCM is less stable compared to the other two instantiations. It is mainly because the first-layer classifiers only gain feedback from the specific task on the second layer in one-goal FE-CCM, which easily causes overfitting.\nWe note that our target-specific FE-CCM, which is optimized for each task independently and achieves the best performance, is a more fair comparison to the state-of-theart because each state-of-the-art model is trained specifically for the respective task. Furthermore, Figure 3 shows the results for CCM (which is a cascade without feedback information) and all-features-direct (which uses features\n4. The state-of-the-art method for depth estimation in [59] follows a slightly different testing procedure. In that case, our target-specific FECCM method achieves RMSE = 15.3.\nfrom all the tasks). This indicates that the improvement is strictly due to the proposed feedback and not just because of having more information.\nWe show some visual improvements due to the proposed FE-CCM in Figure 4. In comparison to CCM, FE-CCM leads to better depth estimation of the sky and the ground, and it leads to better coverage and accurate labeling of the salient region in the image, and it also leads to better geometric labeling and object detection. Figure 5 also provides the confusion matrices for the three tasks: scene categorization, event categorization, geometric labeling.\nFigure 6 provides scatter plots of the performance difference for each image between the unified FE-CCM method and the all-features-direct method, respectively for the tasks of geometric labeling, saliency detection, and depth estimation. We note that for all three tasks, the unified FECCM outperforms the all-features-direct method on most images. For geometric labeling and saliency detection, the improvement from the unified FE-CCM method is mainly due to large improvements on some images. For depth estimation, the improvement is scattered over many images.\nThe cause of improvement. We have shown improvements of FE-CCM in Table 1 under the situation of heterogeneous\ndatasets. The improvement can be caused by one or both of the following reasons: (1) the feedback process finds better error modes for the first-layer classifiers; (2) the feedback generates additional \u201clabels\u201d to retrain the firstlayer classifiers. In order to analyze this, we consider the two tasks of scene recognition and object detection on the DS1 dataset in [11], which contains ground-truth labels for both the tasks. We compare the various methods under two settings: (1) train with the fully-labeled data; (2) train with only the scene labels for one half of the training data and only the object labels for the second half. Table 2 compares the performance of training with partially-labeled datasets and the performance of different methods under these two settings. The experiments are performed using 5-fold cross validation. The unified FE-CCM method outperforms the other methods under both partially-labeled and fully-labeled situations. We note that all methods listed perform better when full labels are provided. In fact, FE-CCM achieves close performance in both settings. We also note that the FE-CCM method trained with partially-labeled datasets outperforms the CCM method trained with fully-labeled datasets, which indicates that the improvement achieved by the FE-CCM method is not simply from generating more labels for training the first-layer classifiers, but also due to finding useful modes for the first-layer classifiers.\nFigure 7 illustrates the first-layer outputs of a test image, respectively at initialization and at the 5th iteration. Our\ninitialization is the same as CCM, i.e., using groundtruth labels to train the first-layer classifiers. We note that with feedback, the first-layer output shifts to focus on more meaningful modes, e.g., At initialization, the event classifier has widespread confusion with other categories. With feedback, the event classifier turns to be confused with only the \u2019rock-climbing\u2019 and \u2019croquet\u2019 events which are more similar to \u2019bocce\u2019. Moreover, the first-layer scene, depth, and object classifiers also give more meaningful predictions while trained with feedback. With better firstlayer predictions, our FE-CCM correctly classifies the event as \u2019bocce\u2019, while CCM misclassifies it as \u2019rowing\u2019."}, {"heading": "6.4 Discussion", "text": "FE-CCM allows each classifier in the second layer to learn which information from the other first-layer sub-tasks is useful, and this can be seen in the learned weights \u2126 for the second-layer. We provide a visualization of the weights for the six vision tasks in Figure 8(a). We see that the model agrees with our intuitions that large weights are assigned to the outputs of the same task from the first layer classifier (see the large weights assigned to the diagonals in the categorization tasks), though saliency detection is an exception which depends more on its original features (not shown here) and the geometric labeling output. We also observe that the weights are sparse. This is an advantage of our approach since the algorithm automatically figures out which outputs from the first level classifiers are useful for the second level classifier to achieve the best performance.\nFigure 8(b) provides a closer look to the positive weights given to the various outputs for a second-level geometric classifier. We observe that large positive weights are assigned to \u201cmountain\u201d, \u201cforest\u201d, \u201ctall building\u201d, etc. for supporting the geometric class \u201cvertical\u201d, and similarly \u201ccoast\u201d, \u201csailing\u201d and \u201cdepth\u201d for supporting the \u201csky\u201d\nclass. These illustrate some of the relationships the model learns automatically without any manual intricate modeling.\nFigure 8(c) visualizes the weights given to the depth attributes (first-layer depth outputs) for the task of event categorization. Figure 8(d) shows the same for the task of scene categorization. We see that the depth plays an important role in these tasks. In Figure 8(c), we observe that most event categories rely on the middle part of the image, where the main objects of the event are often located. E.g., most of the \u201cpolo\u201d images have horses and people in the middle of the image while many \u201csnowboarding\u201d images have people jumping in the upper-middle part. For scene categorization, most of the scene categories (e.g., coast, mountain, open country) have sky in the top part, which is not as discriminative as the bottom part. In scene categories of tall buildings and street, the upper part of the street consists of buildings, which discriminates these two categories from the others. Not surprisingly, our method had automatically figured this out (see Figure 8(d)). Stability of the FE-CCM algorithm: In this paper, we have presented results for six sub-tasks. In order to find out how our method scales with different combination and number of sub-tasks, we have tried several combinations, and in each case we get consistent improvement in each sub-task. For example, in our preliminary experiments, we combined depth estimation and scene categorization and our reduction in error are 12.0% and 13.2% respectively. Combining scene categorization and object detection gives us 15.4% and 10.2% respective improvements (Table 2). We then combined four tasks: event categorization, scene categorization, depth estimation, and saliency detection, and got improvements in all these sub-tasks [22]. Finally, we also combined different tasks for robotic applications, and the performance improvement was similar.\n12\nsup por\nt\nver tica l sky\n(a) (b)\n(c)\n(d)\nFig. 8. (a) The absolute values of the weight vectors for second-level classifiers, i.e. \u03c9. Each column shows the contribution of the various tasks towards a certain task. (b) Detailed illustration of the positive values in the weight vector for a second-level geometric classifier. (c)(d) Illustration of the importance of depths in different regions for predicting different events (c) and scenes (d). An example image for each class is also shown above the map of the weights. (Note: Blue is low and Red is high. Best viewed in Color)."}, {"heading": "7 ROBOTIC APPLICATIONS", "text": "In order to show the applicability of our FE-CCM to different scene understanding domains, we also used the proposed method in multiple robotic applications."}, {"heading": "7.1 Robotic Grasping", "text": "Given an image and a depthmap (Figure 9), the goal of the learning algorithm in a grasping robot is to select a point to grasp the object (this location is called the grasp point, [66]). It turns out that different categories of objects demand different strategies for grasping. In prior work, Saxena et al. [66, 67] did not use object category information for grasping. In this work, we use our FE-CCM to combine object classification and grasping point detection. Implementation: We work with the labeled synthetic dataset by Saxena et al. [66] which spans 6 object categories and also includes an aligned pixel level depth map for each image, as shown in Figure 9. The six object categories include spherically symmetric objects such as cerealbowl, rectangular objects such as eraser, martini glass, books, cups and long objects such as pencil.\nFor grasp point detection, we compute image and depthmap features at each point in the image (using codes given by [66]). The features describe the response of the image and the depth map to a bank of filters (similar to Make3D) while also capturing information from the neighboring grid elements. We then use a regression over the features. The output is a confidence score for each point being a good grasping point. In an image, we pick the point with the highest score as the grasping point.\nFor object detection, we use a logistic classifier to perform the classification. The output of the classifier is a 6-\nTABLE 3 Summary of results for the the robotic grasping experiment. Our method improves performance in every single task.\nGraping point Object Model Detection Classification (% accuracy) (% accuracy) Images in testset 6000 1200 Chance 50 16.7 All features direct 87.7 45.8 Our base-model 87.7 45.8 CCM (Heitz et. al.) 90.5 49.5 FE-CCM 92.2 49.7\nFig. 10. Left: the grasping point detected by our algorithm. Right: Our robot grasping an object using our algorithm.\ndimensional vector representing the log-odds score for each category. The final classification is performed by assigning the image to the category with the highest score. Results: We evaluate our algorithm on a dataset published in [66], and perform cross-validation to evaluate the performance on each task. We use 6000 images for grasping point detection (3000 for training and 3000 for testing) and 1200 images for object classification (600 for training and 600 for testing). Table 3 shows the results for our algorithm\u2019s ability to predict the grasping point, given an image and the depths observed by the robot using its sensors. We see that our FE-CCM obtains significantly better performance over allfeatures-direct and CCM (our implementation). Figure 10 shows an example of our robot grasping an object."}, {"heading": "7.2 Object-finding Robot", "text": "Given an image, the goal of an object-finding robot is to find a desired object in a cluttered room. As we have discussed earlier, some types of scenes such as living room are more likely to have objects (e.g., shoes) than other types of scenes such as kitchen. Similarly, office scenes are more likely to contain tv-monitors than kitchen scenes. Furthermore, it is also intuitive that shoes are more likely to appear on the supportive surface such as floor, instead of the vertical surface such as the wall. Therefore, in this work, we use our FE-CCM to combine object detection with indoor scene categorization and geometric labeling. Implementation: For scene categorization, we use the indoor scene subsets in the Cal-Scene Dataset [68] and\n13\nclassify an image into one of the four categories: bedroom, living room, kitchen and office. For geometric labeling, we use the Indoor Layout Data [69] and assign each pixel to one of three geometry classes: ground, wall and ceiling. We use the same features and classifiers for scene categorization as in Section 5.\nFor object detection, we use the PASCAL 2007 Dataset [70] and our own shoe dataset to learn detectors for four object categories: shoe, dining table, tv-monitor, and sofa. We first use the part-based object detection algorithm in [38] to create candidate windows, and then use the same classifiers as described in Section 5. Results: We use this method to build a shoe-finding robot, as shown on Figure 11-left. With a limited number of training images (86 positive images in our case), it is hard to train a robust shoe detector to find a shoe far away from the camera. However, using our FE-CCM model, the robot learns to leverage the other tasks and performs more robust shoe detection. Figure 11-right shows a successful detection. For more details and videos, please see [71]."}, {"heading": "8 CONCLUSIONS", "text": "We propose a method for combining existing classifiers for different but related tasks in scene understanding. We only consider the individual classifiers as a \u2018black-box\u2019 (thus not needing to know the inner workings of the classifier) and propose learning techniques for combining them (thus not needing to know how to combine the tasks). Our method introduces feedback in the training process from the later stage to the earlier one, so that a later classifier can provide the earlier classifiers information about what error modes to focus on, or what can be ignored without hurting the joint performance.\nOur extensive experiments show that our unified model (a single FE-CCM trained for all the sub-tasks) improves performance significantly across all the sub-tasks considered over the respective state-of-the-art classifiers. We show that this was the result of our feedback process. The classifier actually learns meaningful relationships between the tasks automatically. We believe that this is a small step towards holistic scene understanding."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Anish Nahar, Matthew Cong, TP Wong, Norris Xu, and Colin Ponce for help with the robotic experiments. We also thank Daphne Koller for useful discussions."}], "references": [{"title": "A hierarchical field framework for unified context-based classification", "author": ["S. Kumar", "M. Hebert"], "venue": "ICCV, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Make3d: Learning 3d scene structure from a single still image", "author": ["A. Saxena", "M. Sun", "A.Y. Ng"], "venue": "PAMI, vol. 30, no. 5, 2009.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Closing the loop on scene interpretation", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "CVPR, 2008.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework", "author": ["L.-J. Li", "R. Socher", "L. Fei-Fei"], "venue": "CVPR, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Depth from familiar objects: A hierarchical model for 3d scenes", "author": ["E.B. Sudderth", "A. Torralba", "W.T. Freeman", "A.S. Willsky"], "venue": "CVPR, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Joint parsing and semantic role labeling", "author": ["C. Sutton", "A. McCallum"], "venue": "CoNLL, 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "From appearance to contextbased recognition: Dense labeling in small images", "author": ["D. Parikh", "C. Zitnick", "T. Chen"], "venue": "CVPR, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Object detection via boundary structure segmentation", "author": ["A. Toshev", "B. Taskar", "K. Daniilidis"], "venue": "CVPR, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Monocular human motion capture with a mixture of regressors", "author": ["A. Agarwal", "B. Triggs"], "venue": "CVPR Workshop on Vision for HCI, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Depth estimation using monocular and stereo cues", "author": ["A. Saxena", "J. Schulte", "A.Y. Ng"], "venue": "IJCAI, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Cascaded classification models: Combining models for holistic scene understanding.", "author": ["G. Heitz", "S. Gould", "A. Saxena", "D. Koller"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Neural network ensembles", "author": ["L. Hansen", "P. Salamon"], "venue": "PAMI, vol. 12, no. 10, pp. 993\u20131001, 1990.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1990}, {"title": "Cascaded neural networks based image classifier", "author": ["Y. Freund", "R.E. Schapire"], "venue": "ICASSP, 1993.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1993}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML, 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "EuroCOLT, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "On the design of cascades of boosted ensembles for face detection", "author": ["S.C. Brubaker", "J. Wu", "J. Sun", "M.D. Mullin", "J.M. Rehg"], "venue": "IJCV, vol. 77, no. 1-3, pp. 65\u201386, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M.J. Jones"], "venue": "IJCV, vol. 57, no. 2, pp. 137\u2013154, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Mutual boosting for contextual inference", "author": ["M. Fink", "P. Perona"], "venue": "In NIPS, 2004.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Contextual models for object detection using boosted random fields", "author": ["A. Torralba", "K. Murphy", "W. Freeman"], "venue": "In NIPS, 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Auto-context and its application to high-level vision tasks", "author": ["Z. Tu"], "venue": "CVPR, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Feedback enabled cascaded classication models for scene understanding", "author": ["C. Li", "A. Kowdle", "A. Saxena", "T. Chen"], "venue": "NIPS, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "A generic model to compose vision modules for holistic scene understanding", "author": ["A. Kowdle", "C. Li", "A. Saxena", "T. Chen"], "venue": "ECCV Workshop on Parts and Attributes, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "On combining classifiers", "author": ["J. Kittler", "M. Hatef", "R.P. Duin", "J. Matas"], "venue": "PAMI, vol. 20, pp. 226\u2013239, 1998.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "\u03b8-mrf:capturing spatial and semantic structure in the parameters for scene understanding", "author": ["C. Li", "A. Saxena", "T. Chen"], "venue": "NIPS, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "J. Mach. Learn. Res., vol. 6, pp. 1817\u20131853, December 2005.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1817}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims"], "venue": "ICML, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Max-margin markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "NIPS, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "Semantic labeling of 3d point clouds for indoor scenes", "author": ["H. Koppula", "A. Anand", "T. Joachims", "A. Saxena"], "venue": "Neural Information Processing Systems (NIPS), 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Conditional random fields for object recognition", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": "In NIPS, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning structural svms with latent variables", "author": ["C.-N.J. Yu", "T. Joachims"], "venue": "ICML, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Contextual priming for object detection", "author": ["A. Torralba"], "venue": "Int. J. Comput. Vision, vol. 53, no. 2, pp. 169\u2013191, 2003.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2003}, {"title": "Depth estimation from image structure", "author": ["A. Torralba", "A. Oliva"], "venue": "IEEE PAMI, vol. 24, pp. 1226\u20131238, September 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Putting objects in perspective", "author": ["D. Hoiem", "A.A. Efros", "M. Hebert"], "venue": "IJCV, 2008.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiresolution models for object detection", "author": ["D. Park", "D. Ramanan", "C. Fowlkes"], "venue": "ECCV, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning spatial context: Using stuff to find things", "author": ["G. Heitz", "D. Koller"], "venue": "ECCV, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Context by region ancestry", "author": ["J. Lim", "P. Arbel andez", "C. Gu", "J. Malik"], "venue": "ICCV, 2009.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "A hierarchical field framework for unified  14 context-based classification", "author": ["S. Kumar", "M. Hebert"], "venue": "ICCV, 2005.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "PAMI, 2009.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Objects in context", "author": ["A. Rabinovich", "A. Vedaldi", "C. Galleguillos", "E. Wiewiora", "S. Belongie"], "venue": "ICCV, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "From appearance to contextbased recognition: Dense labeling in small images", "author": ["D. Parikh", "C. Zitnick", "T. Chen"], "venue": "CVPR, 2008.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative models for multi-class object layout", "author": ["C. Desai", "D. Ramanan", "C. Fowlkes"], "venue": "ICCV, 2009.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Modeling mutual context of object and human pose in human-object interaction activities", "author": ["B. Yao", "L. Fei-Fei"], "venue": "CVPR, 2010.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiclass object localization by combining local contextual interactions", "author": ["C. Galleguillos", "B. McFee", "S. Belongie", "G. Lanckriet"], "venue": "CVPR, 2010.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "An empirical study of context in object detection", "author": ["S. Divvala", "D. Hoiem", "J. Hays", "A. Efros", "M. Hebert"], "venue": "CVPR, 2009.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2009}, {"title": "Object localization with global and local context kernels", "author": ["M. Blaschko", "C. Lampert"], "venue": "BMVC, 2009.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "What, where and who? classifying event by scene and object recognition", "author": ["L. Li", "L. Fei-Fei"], "venue": "ICCV, 2007.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2007}, {"title": "Scaling learning algorithms towards ai", "author": ["Y. Bengio", "Y. LeCun"], "venue": "Large-Scale Kernel Machines, 2007.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, vol. 28, pp. 41\u201375, 1997.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1997}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade, G. Orr and M. K., Eds. Springer, 1998.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1998}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxena", "H. Lee", "A. Ng"], "venue": "NIPS, 2009.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "N. Comp, 2006.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2006}, {"title": "Deconvolutional networks", "author": ["M. Zeiler", "D. Krishnan", "G. Taylor", "R. Fergus"], "venue": "CVPR, 2010.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J of Royal Stat. Soc., Series B, vol. 39, no. 1, pp. 1\u201338, 1977.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1977}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": "Learning in graphical models, vol. 89, pp. 355\u2013368, 1998.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1998}, {"title": "Variational gaussian process classifiers", "author": ["M. Gibbs", "D. Mackay"], "venue": "Neural Networks, IEEE Trans, 2000.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2000}, {"title": "Discriminative sparse image models for class-specific edge detection and image interpretation", "author": ["J. Mairal", "M. Leordeanu", "F. Bach", "M. Hebert", "J. Ponce"], "venue": "ECCV, 2008.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "IJCV, vol. 42, pp. 145\u2013175, 2001.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2001}, {"title": "MIT outdoor scene dataset", "author": ["A. Torralba", "A. Oliva"], "venue": "http://people.csail.mit.edu/torralba/code/spatialenvelope/index.html.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 0}, {"title": "3-d depth reconstruction from a single still image", "author": ["A. Saxena", "S.H. Chung", "A.Y. Ng"], "venue": "IJCV, vol. 76, 2007.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning depth from single monocular images", "author": ["A. Saxena", "S. Chung", "A. Ng"], "venue": "NIPS, 2005.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2005}, {"title": "Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.", "author": ["A. Torralba", "A. Oliva", "M.S. Castelhano", "J.M. Henderson"], "venue": "Psychol Rev,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2006}, {"title": "Frequencytuned Salient Region Detection", "author": ["R. Achanta", "S. Hemami", "F. Estrada", "S. Susstrunk"], "venue": "CVPR, 2009.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2009}, {"title": "The PASCAL VOC2006 Results.", "author": ["M. Everingham", "A. Zisserman", "C.K.I. Williams", "L. Van Gool"], "venue": null, "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2006}, {"title": "Discriminatively trained deformable part models, release 3", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "http://people.cs.uchicago.edu/\u223cpff/latent-release3/.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 0}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR, 2005.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2005}, {"title": "Robotic grasping of novel objects", "author": ["A. Saxena", "J. Driemeyer", "J. Kearns", "A.Y. Ng"], "venue": "NIPS, 2006.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2006}, {"title": "Robotic grasping of novel objects using vision", "author": ["A. Saxena", "J. Driemeyer", "A.Y. Ng"], "venue": "IJRR, vol. 27, no. 2, pp. 157\u2013173, 2008.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2008}, {"title": "A bayesian hierarchical model for learning natural scene categories", "author": ["L. Fei-Fei", "P. Perona"], "venue": "CVPR, 2005.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2005}, {"title": "Recovering the spatial layout of cluttered rooms", "author": ["V. Hedau", "D. Hoiem", "D. Forsyth"], "venue": "ICCV, 2009.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2009}, {"title": "The PASCAL Visual Object Classes  Challenge 2007 (VOC2007) Results", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "http://www.pascalnetwork.org/challenges/VOC/voc2007/workshop/index.html.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 1, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 2, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 3, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 4, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 5, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 6, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 7, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 8, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 9, "context": "Recently, several approaches have tried to combine these different classifiers for related tasks in vision [1\u201310]; however, most of them tend to be ad-hoc (i.", "startOffset": 107, "endOffset": 113}, {"referenceID": 10, "context": "[11] recently developed a framework for scene understanding called Cascaded Classification Models (CCM) treating each classifier as a \u2018black-box\u2019.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The idea of cascading layers of classifiers to aid a task was first introduced with neural networks as multi-level perceptrons where, the output of the first layer of perceptrons is passed on as input to the next layer [12\u201314].", "startOffset": 219, "endOffset": 226}, {"referenceID": 12, "context": "The idea of cascading layers of classifiers to aid a task was first introduced with neural networks as multi-level perceptrons where, the output of the first layer of perceptrons is passed on as input to the next layer [12\u201314].", "startOffset": 219, "endOffset": 226}, {"referenceID": 13, "context": "The idea of cascading layers of classifiers to aid a task was first introduced with neural networks as multi-level perceptrons where, the output of the first layer of perceptrons is passed on as input to the next layer [12\u201314].", "startOffset": 219, "endOffset": 226}, {"referenceID": 14, "context": "The idea of improving classification performance by combining outputs of many classifiers is used in methods such as Boosting [15], where many weak learners are combined to obtain a more accurate classifier; this has been applied to tasks such as face detection [16, 17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 15, "context": "The idea of improving classification performance by combining outputs of many classifiers is used in methods such as Boosting [15], where many weak learners are combined to obtain a more accurate classifier; this has been applied to tasks such as face detection [16, 17].", "startOffset": 262, "endOffset": 270}, {"referenceID": 16, "context": "The idea of improving classification performance by combining outputs of many classifiers is used in methods such as Boosting [15], where many weak learners are combined to obtain a more accurate classifier; this has been applied to tasks such as face detection [16, 17].", "startOffset": 262, "endOffset": 270}, {"referenceID": 17, "context": "To incorporate contextual information, Fink and Perona [18] exploited local dependencies between objects in a boosting framework, but did not allow for multiple rounds of communication between objects.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "[19] introduced Boosted Random Fields to model object dependency, which used boosting to learn the graph structure and local evidence of a conditional random field.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Tu [20] proposed a more general framework which used pixel-level label maps to learn a contextual model through a cascaded classifier approach.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "However, in our CCM framework [21, 22], the focus is on capturing contextual interactions between labels of different types.", "startOffset": 30, "endOffset": 38}, {"referenceID": 21, "context": "However, in our CCM framework [21, 22], the focus is on capturing contextual interactions between labels of different types.", "startOffset": 30, "endOffset": 38}, {"referenceID": 19, "context": "Furthermore, compared to the feed-forward only cascade method in [20], our model with feedback not only iteratively refines the contextual interactions, but also refines the individual classifiers to provide helpful context.", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": ", in biometrics, data from voice recognition and face recognition is combined [23].", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Kumar and Hebert [1] developed a large MRF-based probabilistic model to link multi-class segmentation and object detection.", "startOffset": 17, "endOffset": 20}, {"referenceID": 23, "context": "[24] modeled mulitple interactions within tasks and across tasks by defining a MRF over parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Sutton and McCallum [6] combined a parsing model with a semantic role labeling model into a unified probabilistic framework that solved both simultaneously.", "startOffset": 20, "endOffset": 23}, {"referenceID": 24, "context": "Ando and Zhang [25] proposed a general framework for learning predictive functional structures from multiple tasks.", "startOffset": 15, "endOffset": 19}, {"referenceID": 25, "context": ", [26\u201328]) can also be a viable option for the setting of combining multiple tasks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 26, "context": ", [26\u201328]) can also be a viable option for the setting of combining multiple tasks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 27, "context": ", [26\u201328]) can also be a viable option for the setting of combining multiple tasks.", "startOffset": 2, "endOffset": 9}, {"referenceID": 28, "context": "hidden conditional random field [29], latent structured SVM [30]), which can be potentially applied to multi-task settings with disjoint datasets.", "startOffset": 32, "endOffset": 36}, {"referenceID": 29, "context": "hidden conditional random field [29], latent structured SVM [30]), which can be potentially applied to multi-task settings with disjoint datasets.", "startOffset": 60, "endOffset": 64}, {"referenceID": 8, "context": ", [7\u2013 9]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "manually designed the terms in an MRF to combine depth estimation with object detection [2] and stereo cues [10].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "manually designed the terms in an MRF to combine depth estimation with object detection [2] and stereo cues [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "[5] used object recognition to help 3D structure estimation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[31, 32] used the statistics of low-level features across the entire scene to prime object detection or help depth estimation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 31, "context": "[31, 32] used the statistics of low-level features across the entire scene to prime object detection or help depth estimation.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[33] used 3D scene information to provide priors on potential object locations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] used the ground plane estimation as contextual information for pedestrian detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 92, "endOffset": 99}, {"referenceID": 35, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 92, "endOffset": 99}, {"referenceID": 36, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 92, "endOffset": 99}, {"referenceID": 37, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 38, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 39, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 40, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 41, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 109, "endOffset": 116}, {"referenceID": 42, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 126, "endOffset": 133}, {"referenceID": 43, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 126, "endOffset": 133}, {"referenceID": 44, "context": "Many works also model context to capture the local interactions between neighboring regions [35\u201337], objects [38\u201342], or both [43\u201345].", "startOffset": 126, "endOffset": 133}, {"referenceID": 2, "context": "[3] proposed an innovative but ad-hoc system that combined boundary detection and surface labeling by sharing some lowlevel information between the classifiers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4, 46] combined image classification, annotation and segmentation with a hierarchical graphical model.", "startOffset": 0, "endOffset": 7}, {"referenceID": 45, "context": "[4, 46] combined image classification, annotation and segmentation with a hierarchical graphical model.", "startOffset": 0, "endOffset": 7}, {"referenceID": 46, "context": "There is also a large body of work in the areas of deep learning, and we refer the reader to Bengio and LeCun [47] for a nice overview of deep learning architectures and Caruana [48] for multitask learning with shared representation.", "startOffset": 110, "endOffset": 114}, {"referenceID": 47, "context": "There is also a large body of work in the areas of deep learning, and we refer the reader to Bengio and LeCun [47] for a nice overview of deep learning architectures and Caruana [48] for multitask learning with shared representation.", "startOffset": 178, "endOffset": 182}, {"referenceID": 48, "context": "While efficient back-propagation methods like [49] have been commonly used in learning a multilayer network, it is not as easy to apply to our case where each node is a complex classifier.", "startOffset": 46, "endOffset": 50}, {"referenceID": 49, "context": ", [50\u201352]) are different from our work in that, those works focus on one particular task (same labels) by building different classifier architectures, as compared to our setting of different tasks with different labels.", "startOffset": 2, "endOffset": 9}, {"referenceID": 50, "context": ", [50\u201352]) are different from our work in that, those works focus on one particular task (same labels) by building different classifier architectures, as compared to our setting of different tasks with different labels.", "startOffset": 2, "endOffset": 9}, {"referenceID": 51, "context": ", [50\u201352]) are different from our work in that, those works focus on one particular task (same labels) by building different classifier architectures, as compared to our setting of different tasks with different labels.", "startOffset": 2, "endOffset": 9}, {"referenceID": 50, "context": "[51] used unsupervised learning to obtain an initial configuration of the parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] assume that each layer is independent and that each layer produces the best output independently (without consideration for other layers), and therefore use the ground-truth labels for Z even for training the classifiers in the first layer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Training with this initialization, our cascade is equivalent to CCM in [11], where the classifiers (and the parameters) in the first layer are similar to the original state-of-the-art classifier and the classifiers in the second layer use the outputs of the first layer in addition to the original features as input.", "startOffset": 71, "endOffset": 75}, {"referenceID": 52, "context": "Motivated by the Expectation Maximization algorithm [53], we iterate between the two steps as described in the following.", "startOffset": 52, "endOffset": 56}, {"referenceID": 53, "context": "This can be considered as a special variant of the general EM framework (hard EM, [54]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 54, "context": ", approximating it as a Gaussian, [55]) to get:", "startOffset": 34, "endOffset": 38}, {"referenceID": 55, "context": "We do this by introducing the l1 sparsity in the parameters [56].", "startOffset": 60, "endOffset": 64}, {"referenceID": 56, "context": "[57] tall building, inside city, street, highway, coast, open country, mountain and forest.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "We evaluate the performance by measuring the rate of incorrectly assigning a scene label to an image on the MIT outdoor scene dataset [57].", "startOffset": 134, "endOffset": 138}, {"referenceID": 56, "context": "The feature inputs for the first-layer scene classifier \u03a81 \u2208 R is the GIST feature [57], extracted at 4 \u00d7 4 regions of the image, on 4 scales and 8 orientations.", "startOffset": 83, "endOffset": 87}, {"referenceID": 57, "context": "We use an RBF-Kernel SVM classifier [58], as the firstlayer scene classifier, and a multi-class logistic classifier for the second layer.", "startOffset": 36, "endOffset": 40}, {"referenceID": 58, "context": "We evaluate the estimation performance by computing the root mean square error of the estimated depth with respect to ground truth laser scan depth using the Make3D Range Image dataset [59, 60].", "startOffset": 185, "endOffset": 193}, {"referenceID": 59, "context": "We evaluate the estimation performance by computing the root mean square error of the estimated depth with respect to ground truth laser scan depth using the Make3D Range Image dataset [59, 60].", "startOffset": 185, "endOffset": 193}, {"referenceID": 58, "context": "We uniformly divide each image into 55 \u00d7 305 patches as [59].", "startOffset": 56, "endOffset": 60}, {"referenceID": 58, "context": "[59].", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[46]: bocce, badminton, polo, rowing, snowboarding, croquet, sailing and rock-climbing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "The feature inputs for the first-layer event classifier \u03a83 \u2208 R is a 43-dimensional feature vector, which includes the top 30 PCA projections of the 512dimensional GIST features [61], the 12-dimension global", "startOffset": 177, "endOffset": 181}, {"referenceID": 61, "context": "[62] for our experiments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 61, "context": "[62] and a bias term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "We use the train-set and test-set of PASCAL 2006 [63] for our experiments.", "startOffset": 49, "endOffset": 53}, {"referenceID": 63, "context": "[64].", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "The feature inputs for the first-layer object detection classifier \u03a85 \u2208 R are the HOG features extracted based on the candidate window as [65] plus the detection score from the part-based detector [64].", "startOffset": 138, "endOffset": 142}, {"referenceID": 63, "context": "The feature inputs for the first-layer object detection classifier \u03a85 \u2208 R are the HOG features extracted based on the candidate window as [65] plus the detection score from the part-based detector [64].", "startOffset": 197, "endOffset": 201}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33].", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "We use the dataset and the algorithm by [33] as the firstlayer geometric labeling module.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "[11], which we re-implement for six sub-tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 22, "endOffset": 26}, {"referenceID": 58, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 34, "endOffset": 38}, {"referenceID": 57, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 48, "endOffset": 52}, {"referenceID": 61, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "4 model (reported) Li [46] Saxena [59] Torralba [58] Achanta [62] Hoiem [33] Felzenswalb et.", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "[38] (base)", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "CCM [11] 73.", "startOffset": 4, "endOffset": 8}, {"referenceID": 62, "context": "We do not do cross-validation on object detection as it is standard on the PASCAL 2006 [63] dataset (1277 train and 2686 test images respectively).", "startOffset": 87, "endOffset": 91}, {"referenceID": 45, "context": "The state-of-the-art classifiers improve on the base model by explicitly hand-designing the task specific probabilistic model [46, 59] or by using adhoc methods to implicitly use information from other tasks [33].", "startOffset": 126, "endOffset": 134}, {"referenceID": 58, "context": "The state-of-the-art classifiers improve on the base model by explicitly hand-designing the task specific probabilistic model [46, 59] or by using adhoc methods to implicitly use information from other tasks [33].", "startOffset": 126, "endOffset": 134}, {"referenceID": 32, "context": "The state-of-the-art classifiers improve on the base model by explicitly hand-designing the task specific probabilistic model [46, 59] or by using adhoc methods to implicitly use information from other tasks [33].", "startOffset": 208, "endOffset": 212}, {"referenceID": 58, "context": "The state-of-the-art method for depth estimation in [59] follows a slightly different testing procedure.", "startOffset": 52, "endOffset": 56}, {"referenceID": 10, "context": "5 CCM [11] 50.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "In order to analyze this, we consider the two tasks of scene recognition and object detection on the DS1 dataset in [11], which contains ground-truth labels for both the tasks.", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "We then combined four tasks: event categorization, scene categorization, depth estimation, and saliency detection, and got improvements in all these sub-tasks [22].", "startOffset": 159, "endOffset": 163}, {"referenceID": 65, "context": "1 Robotic Grasping Given an image and a depthmap (Figure 9), the goal of the learning algorithm in a grasping robot is to select a point to grasp the object (this location is called the grasp point, [66]).", "startOffset": 199, "endOffset": 203}, {"referenceID": 65, "context": "[66, 67] did not use object category information for grasping.", "startOffset": 0, "endOffset": 8}, {"referenceID": 66, "context": "[66, 67] did not use object category information for grasping.", "startOffset": 0, "endOffset": 8}, {"referenceID": 65, "context": "[66] which spans 6 object categories and also includes an aligned pixel level depth map for each image, as shown in Figure 9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 65, "context": "For grasp point detection, we compute image and depthmap features at each point in the image (using codes given by [66]).", "startOffset": 115, "endOffset": 119}, {"referenceID": 65, "context": "Results: We evaluate our algorithm on a dataset published in [66], and perform cross-validation to evaluate the performance on each task.", "startOffset": 61, "endOffset": 65}, {"referenceID": 67, "context": "Implementation: For scene categorization, we use the indoor scene subsets in the Cal-Scene Dataset [68] and", "startOffset": 99, "endOffset": 103}, {"referenceID": 68, "context": "For geometric labeling, we use the Indoor Layout Data [69] and assign each pixel to one of three geometry classes: ground, wall and ceiling.", "startOffset": 54, "endOffset": 58}, {"referenceID": 69, "context": "For object detection, we use the PASCAL 2007 Dataset [70] and our own shoe dataset to learn detectors for four object categories: shoe, dining table, tv-monitor, and sofa.", "startOffset": 53, "endOffset": 57}, {"referenceID": 37, "context": "We first use the part-based object detection algorithm in [38] to create candidate windows, and then use the same classifiers as described in Section 5.", "startOffset": 58, "endOffset": 62}], "year": 2011, "abstractText": "Scene understanding includes many related sub-tasks, such as scene categorization, depth estimation, object detection, etc. Each of these sub-tasks is often notoriously hard, and state-of-the-art classifiers already exist for many of them. These classifiers operate on the same raw image and provide correlated outputs. It is desirable to have an algorithm that can capture such correlation without requiring any changes to the inner workings of any classifier. We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that jointly optimizes all the sub-tasks, while requiring only a \u2018black-box\u2019 interface to the original classifier for each sub-task. We use a two-layer cascade of classifiers, which are repeated instantiations of the original ones, with the output of the first layer fed into the second layer as input. Our training method involves a feedback step that allows later classifiers to provide earlier classifiers information about which error modes to focus on. We show that our method significantly improves performance in all the sub-tasks in the domain of scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection. Our method also improves performance in two robotic applications: an object-grasping robot and an object-finding robot.", "creator": "LaTeX with hyperref package"}}}