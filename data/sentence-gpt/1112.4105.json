{"id": "1112.4105", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2011", "title": "epsilon-Samples of Kernels", "abstract": "We study the worst case error of kernel density estimates via subset approximation. A kernel density estimate of a distribution is the convolution of that distribution with a fixed kernel (e.g. Gaussian kernel). Given a subset (i.e. a point set) of the input distribution, we can compare the kernel density estimates of the input distribution with the subset, and bound the worst case error. If the maximum error is eps, then this subset can be thought of as an eps-sample (aka an eps-approximation) of the range space defined with the input distribution as the ground set and the fixed kernel representing the family of ranges. Note in this case the ranges are not binary, but have a continuous range (for simplicity we mainly discuss kernels with range of [0,1]); these allow for smoother notions of range spaces, the same way that a kernel density estimates have improved upon histograms. It turns out, the use of this smoother family of range spaces has an added benefit of greatly decreasing the size required for eps-samples (in the plane) from O((1/eps^{4/3}) log^{2/3}(1/eps)) for disks to O((1/eps) sqrt{log (1/eps)}) for Gaussian kernels and for kernels with bounded slope that only affect a bounded domain.\n\nThe next section examines the potential impact of this smoother family of range space on the size and distribution of kernel density estimates. To understand these effects, let's look at some very important properties of the kernel density estimate. First, there is an important problem with the distribution of kernel density estimates using the kernel density estimate. In order to reduce the size of the kernel density estimate, we want to reduce the amount of data that we can store in the sample. To reduce the size of the kernel density estimate, we will take a look at the smallest number of lines that are stored in the sample. If we can use a number of lines that have less and less values, we can do so by using a linear approximation (as specified below).\nThe smallest number of lines that we can store in the sample can be defined as two line points that are left to represent the value of the kernel density estimate. A line will contain either the smallest line (2/eps), or the smallest line (1/eps), and a line containing either the smallest line (2/eps), or two lines (1/eps). For example, if the lines 1", "histories": [["v1", "Sun, 18 Dec 2011 01:19:25 GMT  (212kb,D)", "https://arxiv.org/abs/1112.4105v1", "14 pages, 3 figures"], ["v2", "Fri, 27 Jan 2012 05:35:25 GMT  (48kb,D)", "http://arxiv.org/abs/1112.4105v2", "13 pages, 1 figures. Corrected a mistake in previous version (no change to main results) and simplified some proofs"], ["v3", "Tue, 3 Apr 2012 22:46:53 GMT  (93kb,D)", "http://arxiv.org/abs/1112.4105v3", "13 pages, 2 figures. Cleaned up writing"]], "COMMENTS": "14 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CG cs.DS cs.LG", "authors": ["jeff m phillips"], "accepted": false, "id": "1112.4105"}, "pdf": {"name": "1112.4105.pdf", "metadata": {"source": "CRF", "title": "\u03b5-Samples for Kernels", "authors": ["Jeff M. Phillips"], "emails": ["jeffp@cs.utah.edu"], "sections": [{"heading": null, "text": "It turns out, the use of this smoother family of range spaces has an added benefit of greatly decreasing the size required for \u03b5-samples. For instance, in the plane the size is O((1/\u03b54/3) log2/3(1/\u03b5)) for disks (based on VC-dimension arguments) but is only O((1/\u03b5) \u221a log(1/\u03b5)) for Gaussian kernels and for kernels with bounded slope that only affect a bounded domain. These bounds are accomplished by studying the discrepancy of these \u201ckernel\u201d range spaces, and here the improvement in bounds are even more pronounced. In the plane, we show the discrepancy is O( \u221a log n) for these kernels, whereas for balls there is a lower bound of \u2126(n1/4).\nar X\niv :1\n11 2.\n41 05\nv3 [\ncs .C\nG ]\n3 A\npr 2\n01 2"}, {"heading": "1 Introduction", "text": "We study the L\u221e error in kernel density estimates of points sets by a kernel density estimate of their subset. Formally, we start with a size n point set P \u2282 Rd and a kernel K : Rd \u00d7 Rd \u2192 R. Then a kernel density estimate KDEP of a point set P is a convolution of that point set with a kernel, defined at any point x \u2208 Rd:\nKDEP (x) = \u2211 p\u2208P K(x, p) |P | .\nThe goal is to construct a subset S \u2282 P , and bound its size, so that it has \u03b5-bounded L\u221e error, i.e.\nL\u221e (KDEP , KDES) = max x\u2208Rd\n|KDEP (x)\u2212 KDES(x)| \u2264 \u03b5.\nWe call such a subset S an \u03b5-sample of a kernel range space (P,K), where K is the set of all functions K(x, \u00b7) represented by a fixed kernel K and an arbitrary center point x \u2208 Rd. Our main result is the construction in R2 of an \u03b5-sample of size O((1/\u03b5) \u221a log(1/\u03b5)) for a broad variety of kernel range spaces.\nWe will study this result through the perspective of three types of kernels. We use as examples the ball kernels B, the triangle kernels T, and the Gaussian kernels G; we normalize all kernels so K(p, p) = 1.\nGaussian Triangle Ball\nx x x \u2022 For K \u2208 B : K(x, p) = {1 if \u2016x\u2212 p\u2016 \u2264 1 and 0 otherwise}. \u2022 For K \u2208 T : K(x, p) = max{0, 1\u2212 \u2016x\u2212 p\u2016}. \u2022 For K \u2208 G : K(x, p) = exp(\u2212\u2016x\u2212 p\u20162).\nOur main result holds for T and G, but not B. However, in the context of combinatorial geometry, kernels related to B (binary ranges) seem to have been studied the most heavily from an L\u221e error perspective, and require larger \u03b5-samples. In Appendix A we show a lower-bound that such a result cannot hold for B.\nWe re-describe this result next by adapting (binary) range spaces and discrepancy; these same notions will be used to prove our result.\nRange spaces. A kernel range space is an extension of the combinatorial concept of a range space. Let P \u2282 Rd be a set of n points. Let A \u2282 2P be the set of subsets of P , for instance when A = B they are defined by containment in a ball. The pair (P,A) is called a range space.\nThus we can re-imagine a kernel range space (P,K) as the family of fractional subsets of P , that is, each p \u2208 P does not need to be completely in (1) or not in (0) a range, but can be fractionally in a range described by a value in [0, 1]. In the case of the ball kernel K(x, \u00b7) \u2208 B we say the associated range space is a binary range space since all points have a binary value associated with each range, corresponding with in or not in.\nColorings and discrepancy. Let \u03c7 : P \u2192 {\u22121,+1} be a coloring of P . The combinatorial discrepancy of (P,A), given a coloring \u03c7 is defined d\u03c7(P,A) = maxR\u2208A | \u2211 p\u2208R \u03c7(p)|. For a kernel range space\n(P,K), this is generalized as the kernel discrepancy, defined d\u03c7(P,K) = maxx\u2208Rd \u2211\np\u2208P \u03c7(p)K(x, p); we can also write d\u03c7(P,Kx) = \u2211 p\u2208P \u03c7(p)K(x, p) for a specific kernel Kx, often the subscript x is dropped when it is apparent. Then the minimum kernel discrepancy of a kernel range space is defined d(P,K) = min\u03c7 d\u03c7(P,K). See Matous\u0301ek\u2019s [26] and Chazelle\u2019s [10] books for a masterful treatments of this field when restricted to combinatorial discrepancy.\nConstructing \u03b5-samples. Given a (binary) range space (P,A) an \u03b5-sample (a.k.a. an \u03b5-approximation) is a subset S \u2282 P such that the density of P is approximated with respect to A so\nmax R\u2208A \u2223\u2223\u2223\u2223 |R \u2229 P ||P | \u2212 |R \u2229 S||S| \u2223\u2223\u2223\u2223 \u2264 \u03b5.\nClearly, an \u03b5-sample of a kernel range space is a direct generalization of the above defined \u03b5-sample for (binary) range space. In fact, recently Joshi et al. [21] showed that for any kernel range space (P,K) where all super-level sets of kernels are described by elements of a binary range space (P,A), then an \u03b5-sample of (P,A) is also an \u03b5-sample of (P,K). For instance, super-level sets of G,T are balls in B. \u03b5-Samples are a very common and powerful coreset for approximating P ; the set S can be used as proxy for P in many diverse applications (c.f. [2, 30, 14, 31]). For binary range spaces with constant VCdimension [36] a random sample S of size O((1/\u03b52) log(1/\u03b4)) provides an \u03b5-sample with probability at least 1 \u2212 \u03b4 [24]. Better bounds can be achieved through deterministic approaches as outlined by Chazelle and Matousek [12], or see either of their books for more details [10, 26]. This approach is based on the following rough idea. Construct a low discrepancy coloring \u03c7 of P , and remove all points p \u2208 P such that \u03c7(p) = \u22121. Then repeat these color-remove steps until only a small number of points are left (that are always colored +1) and not too much error has accrued. As such, the best bounds for the size of \u03b5samples are tied directly to discrepancy. As spelled out explicitly by Phillips [30] (see also [26, 10] for more classic references), for a range space (P,A) with discrepancy O(log\u03c4 |P |) (resp. O(|P |\u03c8 log\u03c4 |P |)) that can be constructed in time O(|P |w log\u03c6(|P |)), there is an \u03b5-sample of size g(\u03b5) = O((1/\u03b5) log\u03c4 (1/\u03b5)) (resp. O(((1/\u03b5) log\u03c4 (1/\u03b5))1/(1\u2212\u03c8))) that can be constructed in time O(ww\u22121n \u00b7 (g(\u03b5))w\u22121 \u00b7 log\u03c6(g(\u03b5)) + g(\u03b5)). Although, originally intended for binary range spaces, these results hold directly for kernel range spaces."}, {"heading": "1.1 Our Results", "text": "Our main structural result is an algorithm for constructing a low-discrepancy coloring \u03c7 of a kernel range space. The algorithm is relatively simple; we construct a min-cost matching of the points (minimizes sum of distances), and for each pair of points in the matching we color one point +1 and the other\u22121 at random.\nTheorem 1.1. ForP \u2282 Rd of size n, the above coloring \u03c7, has discrepancy d\u03c7(P,T) = O(n1/2\u22121/d \u221a\nlog(n/\u03b4)) and d\u03c7(P,G) = O(n1/2\u22121/d \u221a log(n/\u03b4)) with probability at least 1\u2212 \u03b4.\nThis implies an efficient algorithm for constructing small \u03b5-samples of kernel range spaces.\nTheorem 1.2. For P \u2282 Rd, with probability at least 1\u2212 \u03b4, we can construct in O(n/\u03b52) time an \u03b5-sample of (P,T) or (P,G) of size O((1/\u03b5)2d/(d+2) logd/(d+2)(1/\u03b5\u03b4)).\nNote that in R2, the size is O((1/\u03b5) \u221a\nlog(1/\u03b5\u03b4)), near-linear in 1/\u03b5, and the runtime can be reduced to O((n/ \u221a \u03b5) log5(1/\u03b5)). Furthermore, for B, the best known upper bounds for discrepancy (which are tight up to a log factor, see Appendix A) are noticeably larger at O((1/\u03b5)2d/(d+1) \u00b7 logd/(d+1)(1/\u03b5)), especially in R2 at O((1/\u03b5)4/3 log2/3(1/\u03b5)).\nWe note that many combinatorial discrepancy results also use a matching where for each pair one is colored +1 and the other\u22121. However, these matchings are \u201cwith low crossing number\u201d and are not easy to construct. For a long time these results were existential, relying on the pigeonhole principle. But, recently Bansal [6] provided a randomized constructive algorithm; also see a similar, simpler and more explicit, approach recently on the arXiv [25]. Yet still these are quite more involved than our min-cost matching. We believe that this simpler, and perhaps more natural, min-cost matching algorithm may be of independent practical interest.\nProof overview. The proof of Theorem 1.2 follows from Theorem 1.1, the above stated results in [30], and Edmond\u2019s O(n3) time algorithm for min-cost matching M [18]. So the main difficulty is proving Theorem 1.1. We first outline this proof in R2 on T. In particular, we focus on showing that the coloring \u03c7 derived from M , for any single kernel K, has d\u03c7(P,K) = O( \u221a log(1/\u03b4)) with probability at least 1 \u2212 \u03b4.\nThen in Section 4.1 we extend this to an entire family of kernels with d\u03c7(P,K) = O( \u221a log(n/\u03b4)).\nThe key aspect of kernels required for the proof is a bound on their slope, and this is the critical difference between binary range spaces (e.g. B) and kernel range spaces (e.g. T). On the boundary of a binary range the slope is infinite, and thus small perturbations of P can lead to large changes in the contents of a range; all lower bounds for geometric range spaces seem to be inherently based on creating a lot of action along the boundaries of ranges. For a kernel K(x, \u00b7) with slope bounded by \u03c3, we use a specific variant of a Chernoff bound (in Section 4) that will depend only on \u2211 j \u2206 2 j , where \u2206j = K(x, pj) \u2212 K(x, qj) for each edge\n(pj , qj) \u2208 M . Note that \u2211\nj |\u2206j | \u2265 d\u03c7(P,K) gives a bound on discrepancy, but analyzing this directly gives a poly(n) bound. Also, for a binary kernels \u2211 j \u2206 2 j = poly(n), but if the kernel slope is bounded then\u2211\nj \u2206 2 j \u2264 \u03c32 \u2211 j \u2016pj \u2212 qj\u20162. Then in Section 3 we bound \u2211 j \u2016pj \u2212 qj\u20162 = O(1) within a constant radius ball, specifically the ball Bx for which K(x, \u00b7) > 0. This follows (after some technical details) from a result of Bern and Eppstein [8].\nExtending to G requires multiple invocations (in Section 4) of the \u2211\nj \u2206 2 j bound from Section 3. Extend-\ning to Rd basically requires generalizing the matching result to depend on the sum of distances to the dth power (in Section 3) and applying Jensen\u2019s inequality to relate \u2211 j \u2206 d j to \u2211 j \u2206 2 j (in Section 4)."}, {"heading": "1.2 Motivation", "text": "Near-linear sized \u03b5-samples. Only a limited family of range spaces are known to have \u03b5-samples with size near-linear in 1/\u03b5. Most reasonable range spaces in R1 admit an \u03b5-sample of size 1/\u03b5 by just sorting the points and taking every \u03b5|P |th point in the sorted order. However, near-linear results in higher dimensions are only known for range spaces defined by axis-aligned rectangles (and other variants defined by fixed, but non necessarily orthogonal axes) (c.f. [34, 30]). All results based on VC-dimension admit super-linear polynomials in 1/\u03b5, with powers approaching 2 as d increases. And random sampling bounds, of course, only provide \u03b5-samples of size O(1/\u03b52).\nThis polynomial distinction is quite important since for small \u03b5 (i.e. with \u03b5 = 0.001, which is important for summarizing large datasets) then 1/\u03b52 (in our example the size 1/\u03b52 = 1, 000, 000) is so large it often defeats the purpose of summarization. Furthermore, most techniques other than random sampling (size 1/\u03b52) are quite complicated and rarely implemented (many require Bansal\u2019s recent result [6] or its simplification [25]). One question explored in this paper is: what other families of ranges have \u03b5-samples with size near-linear in 1/\u03b5?\nThis question has gotten far less attention recently than \u03b5-nets, a related and weaker summary. In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases. Super-linear lower bounds are known as well [3, 29]. We believe the questions regarding \u03b5-samples are even more interesting because they can achieve polynomial improvements, rather than at best logarithmic improvements.\nL\u221e kernel density estimates. Much work (mainly in statistics) has studied the approximation properties of kernel density estimates; this grew out of the desire to remove the choice of where to put the breakpoints between bins in histograms. A large focus of this work has been in determining which kernels recover the original functions the best and how accurate the kernel density approximation is. Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gyo\u0308rfi, and/or Lugosi [15, 17, 16] on L1 error. We focus on L\u221e error, which has not generally been studied.\nTypically the error is from two components: the error from use of a subset (between KDEP and KDES), and the error from convoluting the data with a kernel (between P and KDEP ). Here we ignore the second part, since it can be arbitrary large under the L\u221e error. Typically in the first part, only random samples have been considered; we improve over these random sample bounds.\nRecently Chen, Welling, and Smola [13] showed that for any positive definite kernel (including G, but not\nT or B) a greedy process produces a subset of points S \u2282 P such that L2(KDES , KDEP ) \u2264 \u03b5 when S is size |S| = O(1/\u03b5). This improves on standard results from random sampling theory that require |S| = O(1/\u03b52) for such error guarantees. This result helped inspire us to seek a similar result under L\u221e error.\nRelationship with binary range spaces. Range spaces have typically required all ranges to be binary. That is, each p \u2208 P is either completely in or completely not in each rangeR \u2208 A. Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling). That paper showed that an \u03b5-sample for balls is also an \u03b5-sample for a large number of kernel range spaces. An open question from that paper is whether that relationship goes the other direction: is an \u03b5-sample of a kernel range space also necessarily an \u03b5-sample for a binary range space? We answer this question in the negative; in particular, we show near-linear sized \u03b5-samples for kernel range spaces when it is known the corresponding binary range space must have super-linear size.\nThis raises several questions: are these binary range spaces which require size super-linear in 1/\u03b5 really necessary for downstream analysis? Can we simplify many analyses by using kernels in place of binary ranges? One possible target are the quite fascinating, but enormous bounds for bird-flocking [11]."}, {"heading": "2 Preliminaries", "text": "For simplicity, we focus on only rotation-and-shift-invariant kernels so K(pi, pj) = k(\u2016pi \u2212 pj\u2016) can be written as a function of just the distance between its two arguments. The rotation invariant constraint can easily be removed, but would complicate the technical presentation. We also assume the kernels have been scaled so K(p, p) = k(0) = 1. Section 5 discusses removing this assumption.\nWe generalize the family of kernels T (see above) to S\u03c3 which we call \u03c3-bounded; they have slope bounded by a constant \u03c3 > 0 (that is for any x, q, p \u2208 Rd then |K(x, p) \u2212 K(x, q)|/\u2016q \u2212 p\u2016 \u2264 \u03c3), and bounded domain Bx = {p \u2208 Rd | K(x, p) > 0} defined for any x \u2208 Rd. For a rotation-andshift-invariant kernel, Bx is a ball, and for simplicity we assume the space has been scaled so Bx has radius 1 (this is not critical, as we will see in Section 5, since as the radius increases, \u03c3 decreases). In addition to T this class includes, for instance, the Epanechnikov kernels E so for K(x, \u00b7) \u2208 E is defined K(x, p) = max{0, 1\u2212 \u2016x\u2212 p\u20162}.\nWe can also generalize G (see above) to a family of exponential kernels such that |k(z)| \u2264 exp(\u2212| poly(z)|) and has bounded slope \u03c3; this would also include, for instance, the Laplacian kernel. For simplicity, for the remainder of this work we focus technically on the Gaussian kernel.\nLet rad(B) denote the radius of a ball B. The d-dimensional volume of a ball B of radius r is denoted vold(r) = (\u03c0\nd/2/\u0393(d/2+1))rd where \u0393(z) is the gamma function; note \u0393(z) is increasing with z and when z is a positive integer \u0393(z) = (z\u2212 1)!. For balls of radius 1 we set vold(1) = Vd, a constant depending only on d. In general vold(r) = Vdrd. Note that our results hold in Rd where d is assumed constant, and O(\u00b7) notation will absorb terms dependent only on d."}, {"heading": "3 Min-Cost Matchings within Balls", "text": "Let P \u2282 Rd be a set of 2n points. We say M(P ) (or M when the choice of P is clear) is a perfect matching of P if it defines a set of n unordered pairs (q, p) of points from P such that every p \u2208 P is in exactly one pair. We define cost of a perfect matching in terms of the sum of distances: c(M) = \u2211 (q,p)\u2208M ||q \u2212 p||. The min-cost perfect matching is the matching M\u2217 = arg minM c(M). The proof of the main result is based on a lemma that relates the density of matchings inM\u2217 to the volume of a ball containing them. For any ball B \u2282 Rd, define the length \u03c1(B,M) of the matchings from M within\nB as:\n\u03c1(B,M) = \u2211\n(q,p)\u2208M  \u2016q \u2212 p\u2016d if q, p \u2208 B \u2016q \u2212 pB\u2016d if q \u2208 B and p /\u2208 B, where pB is the intersection of qp with \u2202B 0 if q, p /\u2208 B\nLemma 3.1. There exists a constant \u03c6d that depends only on d, such that for any ball B \u2282 Rd the length \u03c1(B,M\u2217) \u2264 (\u03c6d/Vd)vold(rad(B)) = \u03c6drad(B)d.\nProof. A key observation is that we can associate each edge (q, p) \u2208M\u2217 with a shape that has d-dimensional volume proportional to \u2016q\u2212p\u2016d, and if these shapes for each edge do not overlap too much, then the sum of edge lengths to the dth power is at most proportional to the volume of the ball that contains all of the points.\nIn fact, Bern and Eppstein [8] perform just such an analysis considering the minimum cost matching of points within [0, 1]d. They bound the sum of the dth power of the edge lengths to be O(1). For an edge (q, p), the shape they consider is a football, which includes all points y such that qyp makes an angle of at most 170\u25e6; so it is thinner than the disk with diameter from q to p, but still has d-dimensional volume \u2126(\u2016q \u2212 p\u2016d).\nTo apply this result of Bern and Eppstein, we can consider a ball of radius 1/2 that fits inside of [0, 1]d by scaling down all lengths uniformly by 1/2rad(B). If we show the sum of dth power of edge lengths is O(1) for this ball, and by scaling back we can achieve our more general result. From now we will assume rad(B) = 1/2. Now the sum of dth power of edge lengths where both endpoints are within B is at most O(1) since these points are also within [0, 1]d.\nHandling edges of the second type where p /\u2208 B is a bit more difficult. Let B be centered at a point x. First, we can also handle all \u201cshort\u201d edges (q, p) where \u2016x \u2212 p\u2016 < 10 since we can again just scale these points appropriately losing a constant factor, absorbed in O(1).\nNext, we argue that no more than 360d\u22121 \u201clong\u201d edges (q, p) \u2208M\u2217 can exist with q \u2208 B and \u2016p\u2212 x\u2016 > 10. This implies that there are two such edges (q, p) and (q\u2032, p\u2032) where the angle between the vectors from x to p and from x to p\u2032 differ by at most 1 degree. We can now demonstrate that both of these edges cannot be in the minimum cost matching as follows.\nq\nq0 p0\np\nx y\nLet 10 < \u2016x \u2212 p\u2016 < \u2016x \u2212 p\u2032\u2016 and let y be the point a distance 10 from x on the ray towards p\u2032. We can now see the following simple geometric facts: (F1) \u2016q \u2212 q\u2032\u2016 < 1, (F2) \u2016p \u2212 y\u2016 < \u2016p \u2212 x\u2016, (F3) \u2016p\u2032 \u2212 y\u2016+ 10 = \u2016p\u2032 \u2212 x\u2016, (F4) \u2016p\u2212 y\u2016+ \u2016p\u2032 \u2212 y\u2016 \u2264 \u2016p\u2212 p\u2032\u2016, and (F5) \u2016p\u2212 x\u2016 \u2212 1/2 \u2264 \u2016p\u2212 q\u2016 and \u2016p\u2032 \u2212 x\u2016 \u2212 1/2 \u2264 \u2016p\u2032 \u2212 q\u2032\u2016. It follows that\n\u2016q \u2212 q\u2032\u2016+ \u2016p\u2212 p\u2032\u2016 \u2264 1 + \u2016p\u2212 y\u2016+ \u2016p\u2032 \u2212 y\u2016 \u2264 1 + \u2016p\u2212 x\u2016+ \u2016p\u2212 x\u2016 \u2212 10 < \u2016p\u2212 q\u2016+ \u2016p\u2032 \u2212 q\u2032\u2016. via (F1) and (F4) via (F2) and (F3) via (F5)\nThus it would be lower cost to swap some pair of edges if there are more than 360d\u22121 of them. Since each of these at most 360d\u22121 long second-type edges can contribute at most 1 to \u03c1(B,M\u2217) this handles the last remaining class of edges, and proves the theorem."}, {"heading": "4 Small Discrepancy for Kernel Range Spaces", "text": "We construct a coloring \u03c7 : P \u2192 {\u22121,+1} of P by first constructing the minimum cost perfect matching M\u2217 of P , and then for each (pj , qj) \u2208M\u2217 we randomly color one of {pj , qj} as +1 and the other \u22121.\nTo bound the discrepancy of a single kernel d\u03c7(P,K) we consider a random variableXj = \u03c7(pj)K(x, pj)+ \u03c7(qj)K(x, qj) for each pair (pj , qj) \u2208 M\u2217, so d\u03c7(P,K) = | \u2211 j Xj |. We also define a value \u2206j =\n2|K(x, pj)\u2212K(x, qj)| such thatXj \u2208 {\u2212\u2206j/2,\u2206j/2}. The key insight is that we can bound \u2211 j \u2206 2 j using the results from Section 3. Then since each Xj is an independent random variable and has E[Xj ] = 0, we are able to apply a Chernoff bound on d\u03c7(P,K) = | \u2211 j Xj | that says\nPr[d\u03c7(P,K) > \u03b1] \u2264 2 exp ( \u22122\u03b12\u2211 j \u2206 2 j ) . (4.1)\nIn R2, for \u03c3-bounded kernels achieving a probabilistic discrepancy bound is quite straight-forward at this point (using Lemma 3.1); and can be achieved for Gaussian kernels with a bit more work.\nHowever for points P \u2282 Rd for d > 2 applying the above bound is not as efficient since we only have a bound on \u2211 j \u2206\nd j . We can attain a weaker bound using the Jensen\u2019s inequality over at most n terms\u2211\nj\n1 n \u22062j d/2 \u2264\u2211 j 1 n ( \u22062j )d/2 so we can state \u2211 j \u22062j \u2264 n1\u22122/d \u2211 j \u2206dj 2/d . (4.2) \u03c3-bounded kernels. We start with the result for \u03c3-bounded kernels using Lemma 3.1.\nLemma 4.1. In Rd, for any kernel K \u2208 S\u03c3 we can construct a coloring \u03c7 such that Pr[d\u03c7(P,K) > n1/2\u22121/d\u03c3(\u03c6d)\n1/d \u221a\n2 ln(2/\u03b4)] \u2264 \u03b4 for any \u03b4 > 0.\nProof. Consider some K(x, \u00b7) \u2208 S\u03c3 and recall Bx = {y \u2208 R2 | K(x, y) > 0}. Note that\u2211 j \u2206dj = \u2211 j 2d(K(pj , x)\u2212K(qj , x))d \u2264 2d\u03c3d\u03c1(Bx,M\u2217) \u2264 \u03c3d2d\u03c6drad(Bx)d < \u03c3d2d\u03c6d,\nwhere the first inequality follows by the slope of K \u2208 T being at most \u03c3 and K(p, x) = 0 for p /\u2208 B, since we can replace pj with pj,Bx when necessary since both have K(x, \u00b7) = 0, and the second inequality follows by Lemma 3.1. Hence, by Jensen\u2019s inequality (i.e. (4.2)) \u2211 j \u2206 2 j \u2264 n1\u22122/d(\u03c3d2d\u03c6d)2/d = n1\u22122/d\u03c324(\u03c6d) 2/d.\nWe now study the random variable d\u03c7(P,K) = | \u2211\niXi| for a single K \u2208 S\u03c3. Invoking (4.1) we can bound Pr[d\u03c7(P,K) > \u03b1] \u2264 2 exp(\u2212\u03b12/(n1\u22122/d\u03c322(\u03c6d)2/d)). Setting \u03b1 = n1/2\u22121/d\u03c3(\u03c6d)1/d \u221a 2 ln(2/\u03b4)\nreveals Pr[d\u03c7(P,K) > n1/2\u22121/d\u03c3(\u03c6d)1/d \u221a 2 ln(2/\u03b4)] \u2264 \u03b4.\nFor T and E the bound on \u03c3 is 1 and 2, respectively. Also note that in R2 the expected discrepancy for any one kernel is independent of n.\nGaussian kernels. Now we extend the above result for \u03c3-bounded kernels to Gaussian kernels. It requires a nested application of Lemma 3.1. Let zi = 1/2i and Bi = {p \u2208 Rd | K(x, p) \u2265 zi}; let Ai = Bi \\ Bi\u22121 be the annulus of points with K(x, p) \u2208 [zi, zi\u22121). For simplicity define B0 as empty and A1 = B1. We can bound the slopes within each annulus Ai as \u03c31 = 1 = max ddy (\u2212k(y)), and more specifically for i \u2265 2 then \u03c3i = maxy\u2208Ai d dy (\u2212k(y)) = k(zi\u22121) = 1/2 i\u22121. Define yi = \u221a i ln 2 so that k(yi) = zi.\nWe would like to replicate Lemma 3.1 but to bound \u03c1(B,M\u2217) for annuli Ai instead of B. However, this cannot be done directly because an annulus can become skinny, so its d-dimensional volume is not proportional to its width to the dth power. This in turn allows for long edges within an annulus that do not correspond to a large amount of volume in the annulus.\nWe can deal with this situation by noting that each edge either counts towards a large amount of volume towards the annulus directly, or could have already been charged to the ball contained in the annulus. This will be sufficient for the analysis that follows for Gaussian kernels.\nDefine \u03c1(Ai,M) as follows for an annulus centered at point x. For each edge (pj , qj) \u2208M let qj,i be the point on pjqj \u2229Ai furthest from x; if there is a tie, choose one arbitrarily. Let pj,i be the point on pjqj \u2229Ai closest to x, if there is a tie, choose the one closer to qj,i. Then \u03c1(Ai,M) = \u2211 (pj ,qj)\u2208M \u2016pj,i \u2212 qj,i\u2016 d.\nLemma 4.2. \u03c1(Ai,M) \u2264 \u03c1(Bi,M)\u2212 \u03c1(Bi\u22121,M) for i \u2265 1.\nProof. For each edge (pj , qj) \u2208 M such that pjqj intersects Bi, has piqi \u2229 Bi as either entirely, partially, or not at all in Ai. Those that are entirely in or not at all in Ai either contribute exclusively to \u03c1(Ai,M) or \u03c1(Bi\u22121,M), respectively. Those that are partially in Ai contribute to Ai and Bi\u22121 in a superadditive way towards Bi leading to the claimed inequality.\nNow notice that since yi = \u221a i ln 2 then \u03c1(Bi,M\u2217) \u2264 \u03c6d(ln 2)d/2id/2, by Lemma 3.1.\nLemma 4.3. Let \u03a8(n, d, \u03b4) = O(n1/2\u22121/d \u221a\nln(1/\u03b4)). In Rd, for any kernel K \u2208 G we can construct a coloring \u03c7 such that Pr[d\u03c7(P,K) > \u03a8(n, d, \u03b4)] \u2264 \u03b4 for any \u03b4 > 0.\nProof. Now to replicate the result in Lemma 4.1 we need to show that \u2211\n(pj ,p\u2032j)\u2208M\u2217 (K(qj)\u2212K(pj))d < C\nfor some constant C. For a kernel K \u2208 G centered at a point x \u2208 Rd we can decompose its range of influence into a series of annuli A1, A2, . . . , as described above.\nDefine vi,j = K(pj,i) \u2212 K(qj,i) if both pj,i and qj,i exist, otherwise set vi,j = 0. Note that vi,j \u2264 \u2016pj,i \u2212 qj,i\u2016/2i\u22121 since the slope is bounded by 1/2i\u22121 in Ai.\nFor any (pj , qj) \u2208 M\u2217 we claim K(qj) \u2212 K(pj) \u2264 \u2211\ni vi,j \u2264 3 maxi vi,j . The first inequality follows since the slope within annulus Ai is bounded by 1/2i\u22121, so each vi,j corresponds to the change in value of (pj , qj) with Ai. To see the second inequality, let ` be the smallest index such that K(x, pj) \u2265 z`, that is A` is the smallest annulus that intersects qjpj . We can see that v`+1,j \u2265 \u2211\u221e i=`+2 vi,j since (yi \u2212 yi\u22121) is a decreasing function of i and 1/2i\u22121 is geometrically decreasing. Thus the arg maxi vi,j is either v`,j or v`+1,j , since vi,j = 0 for i < `. If v`,j > v`+1,j then 3v`,j > v`,j + v`+1,j + \u2211\u221e i=`+2 vi,j = \u2211\u221e i=1 vi,j . If\nv`,j \u2264 v`+1,j then 3v`+1,j > v`,j + v`+1,j + \u2211\u221e i=`+2 vi,j = \u2211\u221e i=1 vi,j . Hence\n(K(qj)\u2212K(pj))d \u2264 ( \u2211 i vi,j) d < (3 max i vi,j) d \u2264 3d \u2211 i vdi,j .\nWe can now argue that the sum over all Ai, that \u2211\ni \u2211 (pj ,qj)\u2208M\u2217 v d i,j is bounded. By Lemma 4.2, and\nsumming over all (pj , qj) in a fixed Ai\u2211 (pj ,qj)\u2208M\u2217 vdi,j = \u2211 (pj ,qj)\u2208M\u2217 \u2016pj,i \u2212 qj,i\u2016d (2i\u22121)d = \u03c1(Ai,M \u2217) 2d(i\u22121) \u2264 \u03c1(Bi,M \u2217)\u2212 \u03c1(Bi\u22121,M\u2217) 2d(i\u22121) .\nHence (using a reversal in the order of summation)\u2211 (pj ,qj)\u2208M\u2217 (K(qj)\u2212K(pj))d \u2264 3d \u2211 (pj ,qj)\u2208M\u2217 \u221e\u2211 i=1 vdi,j = 3 d \u221e\u2211 i=1 \u2211 (pj ,qj)\u2208M\u2217 vdi,j\n\u2264 3d \u221e\u2211 i=1 \u03c1(Bi,M \u2217)\u2212 \u03c1(Bi\u22121,M\u2217) 2d(i\u22121) = 3d \u221e\u2211 i=1 \u03c1(Bi,M \u2217) ( 1 2d(i\u22121) \u2212 1 2di )\n\u2264 3d \u221e\u2211 i=1 \u03c6d(ln 2) d/2id/2 ( 2d 2di ) \u2264 6d\u03c6d,\nsince for i \u2265 1 and d \u2265 2 the term (i ln 2)d/2/2di < 1/2i is geometrically decreasing as a function of i. Finally we plug \u2206j = 2|K(qj)\u2212K(pj)| and using (4.2)\u2211\nj \u22062j \u2264 n1\u22122/d( \u2211 j \u2206dj ) 2/d \u2264 n1\u22122/d(2d \u00b7 6d\u03c6d)2/d = 144(\u03c6d)2/dn1\u22122/d\ninto a Chernoff bound (4.1) on n pairs in a matching, as in the proof of Lemma 4.1, where Xj represents the contribution to the discrepancy of pair (qj , pj) \u2208M\u2217 and d\u03c7(P,K) = | \u2211 j Xj |. Then\nPr[d\u03c7(P,K) > \u03b1] \u2264 2 exp ( \u22122\u03b12\u2211 j \u2206 2 j ) \u2264 2 exp ( \u2212\u03b12 72(\u03c6d)2/dn1\u22122/d ) .\nSetting \u03b1 = \u03a8(n, d, \u03b4) = 6 \u221a 2(\u03c6d) 1/dn1/2\u22121/d \u221a ln(2/\u03b4) reveals Pr[d\u03c7(P,K) > \u03a8(n, d, \u03b4)] \u2264 \u03b4.\nAgain, in R2 we can show the expected discrepancy for any one Gaussian kernel as independent of n."}, {"heading": "4.1 From a Single Kernel to a Range Space, and to \u03b5-Samples", "text": "The above theorems imply colorings with small discrepancy (O(1) in R2) for an arbitrary choice of K \u2208 T or E or G, and give a randomized algorithm to construct such a coloring that does not use information about the kernel. But this does not yet imply small discrepancy for all choices of K \u2208 S\u03c3 or G simultaneously. To do so, we show we only need to consider a polynomial in n number of kernels, and then show that the discrepancy is bounded for all of them.\nNote for binary range spaces, this result is usually accomplished by deferring to VC-dimension \u03bd, where there are at most n\u03bd distinct subsets of points in the ground set that can be contained in a range. Unlike for binary range spaces, this approach does not work for kernel range spaces since even if the same set Px \u2282 P have non-zero K(x, p) for p \u2208 Px, their discrepancy d\u03c7(P,Kx) may change by recentering the kernel to x\u2032 such that Px = Px\u2032 . Instead, we use the bounded slope (with respect to the size of the domain) of the kernel.\nFor a kernel K \u2208 S\u03c3 or G, let Bx,n = {p \u2208 Rd | K(x, p) > 1/n}.\nLemma 4.4. For any x \u2208 Rd, vold(Bx,n) < Vd for S\u03c3 and vold(Bx,n) = Vd(ln(2n))d/2 for G.\nProof. For S\u03c3, clearly Bx,n \u2282 Bx = {p \u2208 Rd | K(x, p) > 0}, and vold(Bx) = Vd. For G, we have k(z) = 1/n for z = \u221a ln(2n), and a ball of radius \u221a ln(2n) has d-dimensional volume\nVd( \u221a ln(2n))d.\nTheorem 4.1. In Rd, for K as a family in S\u03c3 or G, and a value \u03a8(n, d, \u03b4) = O(n1/2\u22121/d \u221a\nlog(n/\u03b4)), we can choose a coloring \u03c7 such that Pr[d\u03c7(P,K) > \u03a8(n, d, \u03b4)] \u2264 \u03b4 for any \u03b4 > 0.\nProof. Each p \u2208 P corresponds to a ball Bp,n where K(p, \u00b7) > 1/n. Let U = \u22c3 p\u2208P Bp,n. For any q /\u2208 U ,\nthen \u2211\np\u2208P K(p, q) \u2264 1, and thus d\u03c7(P,K) \u2264 1; we can ignore these points. Also, by Lemma 4.4, the d-dimensional volume of U is at most Vdn for S\u03c3 and at most Vdn(ln(2n))d/2 for G. We can then cover U with a net N\u03c4 such that for each x \u2208 U , there exists some q \u2208 N\u03c4 such that \u2016x \u2212 q\u2016 \u2264 \u03c4 . Based on the d-dimensional volume of U , there exists such a net of size |N\u03c4 | \u2264 O(\u03c4dn) for S\u03c3 and |N\u03c4 | \u2264 O(\u03c4dn(ln(2d))d/2) for G.\nThe maximum slope of a kernel K \u2208 G is \u03c3 = 1. Then, if for all q \u2208 N\u03c4 we have d\u03c7(P,Kq) \u2264 D (where D \u2265 1), then any point x \u2208 Rd has discrepancy at most d\u03c7(P,Kx) \u2264 D + \u03c4n\u03c3. Recall for any x /\u2208 U , d\u03c7(P,Kx) < 1. Then the \u03c4n\u03c3 term follows since by properties of the net we can compare discrepancy to a kernel Kq shifted by at most \u03c4 , and thus this affects the kernel values on n points each by at most \u03c3\u03c4 .\nSetting, \u03c4 = 1/n\u03c3 it follows that d\u03c7(P,Kx) \u2264 D + 1 for any x \u2208 Rd. Thus for this condition to hold, it suffices for |Nq| = O(nd+1\u03c3d) for S\u03c3 and |Nq| = O(nd+1(ln(2n))d/2) for G.\nSetting the probability of failure in Lemma 4.1 to \u03b4\u2032 for each such kernel to \u03b4\u2032 = \u2126(\u03b4/|N\u03c4 |) implies that for some value \u03a8(n, d, \u03b4) = n1/2\u22121/d\u03c3(\u03c6d)1/d \u221a 2 ln(2/\u03b4\u2032) + 1 = O(n1/2\u22121/d \u221a log(n/\u03b4)), for K as S\u03c3 or"}, {"heading": "G, the Pr[d\u03c7(P,K) > \u03a8(n, d, \u03b4)] \u2264 \u03b4.", "text": "To transform this discrepancy algorithm into one for \u03b5-samples, we need to repeat it successfullyO(log n) times. Thus to achieve success probability \u03c6we can set \u03b4 = \u03c6/ log n above, and get a discrepancy of at most\u221a\n2 log(n\u03c6 log n) with probability 1\u2212 \u03c6. Now this and [30] implies Theorem 1.2. We can state the corollary below using Varadarajan\u2019s O(n1.5 log5 n) time algorithm [37] for computing the min-cost matching in R2; the log factor in the runtime can be improved using better SSPD constructions [1] toO(n1.5 log2 n) expected time or O(n1.5 log3 n) deterministic time.\nCorollary 4.1. For any point set P \u2282 R2, for any class K of \u03c3-bounded kernels or Gaussian kernels, in O((n/ \u221a \u03b5) log2(1/\u03b5)) expected time (or in O((n/ \u221a \u03b5) log3(1/\u03b5)) deterministic time) we can create an\n\u03b5-sample of size O((1/\u03b5) \u221a log(1/\u03b5\u03c6)) for (P,K), with probability at least 1\u2212 \u03c6."}, {"heading": "5 Extensions", "text": "Bandwidth scaling. A common topic in kernel density estimates is fixing the integral under the kernel (usually at 1) and the \u201cbandwidth\u201d w is scaled. For a shift-and-rotation-invariant kernel, where the default kernel k has bandwidth 1, a kernel with bandwidth w is written and defined kw(z) = (1/wd)k(z/w).\nOur results do not hold for arbitrarily small bandwidths, because then kw(0) = 1/wd becomes arbitrarily large as w shrinks; see Appendix A. However, fix W as some small constant, and consider all kernels KW extended from K to allow any bandwidth w \u2265 W . We can construct an \u03b5-sample of size O((1/\u03b5)1/2\u22121/d \u221a log(1/\u03b5\u03b4)) for (P,KW ). The observation is that increasing w to w\u2032 where 1 < w\u2032/w < \u03b7 affects d\u03c7(P, kw) by at most O(n\u03b7d+1) since \u03c3 and W are assumed constant. Thus we can expand N\u03c4 by a factor of O(nd+1 log n); for each x \u2208 N\u03c4 consider O(nd+1 log n) different bandwidths increasing geometrically by (1 + \u03c3/nd+1) until we reach n1/(d+1). For w > n1/(d+1) the contribution of each of n points to d\u03c7(P,Kx) is at most 1/n, so the discrepancy at any x is at most 1. N\u03c4 is still poly(n), and hence Theorem 4.1 still holds for KW in place of some K \u2208 {S\u03c3,G}.\nAnother perspective in bandwidth scaling is how it affects the constants in the bound directly. The main discrepancy results (Theorem 4.1) would have a discrepancy bound O(r\u03c3n1/2\u22121/d \u221a d log(n/\u03b4)) where r is the radius of a region of the kernel that does not have a sufficiently small K(p, \u00b7) value, and \u03c3 is the maximum slope. Note that k(0) \u2264 r\u03c3. Assume \u03c3 and r represent the appropriate values when w = 1, then as we change w and fix the integral under Kw(x, \u00b7) at 1, we have \u03c3w = \u03c3/wd and rw = rw so rw\u03c3w = r\u03c3/w\nd\u22121. Thus as the bandwidth w decreases, the discrepancy bound increases at a rate inversely proportional to 1/w(d\u22121).\nAlternatively we can fix k(0) = 1 and scale ks(z) = k(z/s). This trades off rs = rs and \u03c3s = \u03c3/s, so rs\u03c3s = r\u03c3 is fixed for any value s. Again, we can increase N\u03c4 by a factor O(n log n) and cover all values s.\nLower bounds. We present several straight-forward lower bounds in Appendix A. In brief we show:\n\u2022 random sampling cannot do better on kernel range spaces than on binary range spaces (sizeO(1/\u03b52)); \u2022 \u03b5-samples for kernels requires \u2126(1/\u03b5) points; \u2022 \u03b5-samples for (P,B) require size \u2126(1/\u03b52d/(d+1)) in Rd; and \u2022 kernels with k(0) arbitrary large can have unbounded discrepancy."}, {"heading": "5.1 Future Directions", "text": "We believe we should be able to make this algorithm deterministic using iterative reweighing of the poly(n) kernels inN\u03c4 . We suspect in R2 this would yield discrepancyO(log n) and an \u03b5-sample of sizeO((1/\u03b5) log(1/\u03b5)).\nWe are not sure that the bounds in Rd for d > 2 require super-linear in 1/\u03b5 size \u03b5-samples. The polynomial increase is purely from Jensen\u2019s inequality. Perhaps a matching that directly minimizes sum of lengths to the dth or (d/2)th power will yield better results, but these are harder to analyze. Furthermore, we have not attempted to optimize for, or solve for any constants. A version of Bern and Eppstein\u2019s result [8] with explicit constants would be of some interest.\nWe provide results for classes of \u03c3-bounded kernels S\u03c3, Gaussian kernels G, and ball kernels B. In fact this covers all classes of kernels described on the Wikipedia page on statistical kernels: http://en. wikipedia.org/wiki/Kernel_(statistics). However, we believe we should in principal be able to extend our results to all shift-invariant kernels with bounded slope. This can include kernels which may be negative (such as the sinc or trapezoidal kernels [15, 17]) and have nice L2 KDE approximation properties. These sometimes-negative kernels cannot use the \u03b5-sample result from [21] because their superlevel sets have unbounded VC-dimension since k(z) = 0 for infinitely many disjoint values of z.\nEdmond\u2019s min-cost matching algorithm [18] runs inO(n3) time in Rd and Varadarajan\u2019s improvement [37] runs in O(n1.5 log5 n) in R2 (and can be improved to O(n1.5 log2 n) [1]). However, \u03b5-approximate algorithms [38] run in O((n/\u03b53) log6 n) in R2. As this result governs the runtime for computing a coloring, and hence an \u03b5-sample, it would be interesting to see if even a constant-factor approximation could attain the same asymptotic discrepancy results; this would imply and \u03b5-sample algorithm that ran in time O(n \u00b7 poly log(1/\u03b5)) for 1/\u03b5 < n."}, {"heading": "Acknowledgements", "text": "I am deeply thankful to Joel Spencer for discussions on this problem, and for leading me to other results that helped resolved some early technical issues. He also provided through personal communication a proof (I believe discussed with Prasad Tetali) that if the matching directly minimizes the sum of squared lengths, then same results hold. Finally, I thank David Gamarnik for communicating the Bern-Eppstein result [8]."}, {"heading": "A Lower Bounds", "text": "We state here a few straight-forward lower bounds. These results are not difficult, and are often left implied, but we provide them here for completeness.\nRandom sampling. We can show that in Rd for a constant d, a random sample of sizeO((1/\u03b52) log(1/\u03b4)) is an \u03b5-sample for all shift-invariant, non-negative, non-increasing kernels (via the improved bound [24] on VC-dimension-bounded range spaces [36] since they describe super-level sets of these kernels using [21]). We can also show that we cannot do any better, even for one kernel K(x, \u00b7). Consider a set P of n points where n/2 points Pa are located at location a, and n/2 points Pb are located at location b. Let \u2016a \u2212 b\u2016 be large enough that K(a, b) < 1/n2 (for instance for K \u2208 T let K(a, b) = 0). Then for an \u03b5-sample S \u2282 P of size k, we require that Sa \u2282 Pa has size ka such that |ka \u2212 k/2| < \u03b5k. The probability (via Proposition 7.3.2 [28] for \u03b5 \u2264 1/8)\nPr[|Pa| \u2265 k/2 + \u03b5k)] \u2265 1 15 exp(\u221216(\u03b5k)2/k) \u2265 1 15 exp(\u221216\u03b52k) \u2265 \u03b4.\nSolving for k reveals k \u2264 (1/16\u03b52) ln(1/15\u03b4). Thus if k = o((1/\u03b52) log(1/\u03b4)) the random sample will have more than \u03b4 probability of having more than \u03b5-error.\nRequirement of 1/\u03b5 points for \u03b5-samples. Consider a set P where there are only t = d1/\u03b5e \u2212 1 distinct locations of points, each containing |P |/t points from P . Then we can consider a range for each of the distinct points that contains only those points, or a kernel K(xi, 0) for i \u2208 [t] that is non-zero (or very small) for only the points in the ith location. Thus if any distinct location in P is not represented in the \u03b5-sample S, then the query on the range/kernel registering just those points would have error greater than \u03b5. Thus an \u03b5-sample must have size at least d1/\u03b5e \u2212 1.\nMinimum \u03b5-sample for balls. We can achieve a lower bound for the size of \u03b5-samples in terms of \u03b5 for any range space (P,A) by leveraging known lower bounds from discrepancy. For instance, for (P,B) it is known (see [26] for many such results) that there exists points sets P \u2282 Rd of size n such that d(P,B) > \u2126(n1/2\u22121/2d). To translate this to a lower bound for the size of an \u03b5-sample S on (P,B) we follow a technique outlined in Lemma 1.6 [26]. An \u03b5-sample of size n/2 implies for an absolute constant C that\nCn1/2\u22121/2d \u2264 d(P,B) \u2264 \u03b5n.\nHence, solving for n/2 (the size of our \u03b5-sample) in terms of \u03b5 reveals that |S| = n/2 \u2265 (1/2)(C/\u03b5)2d/(d+1). Hence for \u03b5 small enough so |S| \u2265 n/2 reveals that the size of the \u03b5-sample for (P,B) is \u2126(1/\u03b52d/(d+1)).\nDelta kernels. We make the assumption that K(x, x) = 1, if instead K(x, x) = \u03b7 and \u03b7 is arbitrarily large, then the discrepancy d(P,K) = \u2126(\u03b7). We can place one point p \u2208 P far enough from all other points p\u2032 \u2208 P so K(p, p\u2032) is arbitrarily small. Now d\u03c7(P,K(p, \u00b7)) approaches \u03b7, no matter whether \u03c7(p) = +1 or \u03c7(p) = \u22121."}], "references": [{"title": "New constructions of sspds and their applications", "author": ["Mohammad A. Abam", "Sariel Har-Peled"], "venue": "Computational Geometry: Theory and Applications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Deterministic algorithms for sampling count data", "author": ["H\u00fcseyin Akcan", "Alex Astashyn", "Herv\u00e9 Br\u00f6nnimann"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A non-linear lower bound for planar epsilon-nets", "author": ["Noga Alon"], "venue": "In Proceedings 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Scale-sensitive dimensions, uniform convergence, and learnability", "author": ["Noga Alon", "Shai Ben-David", "Nocol\u00f2 Cesa-Bianchi", "David Haussler"], "venue": "Journal of ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Small-size epsilon-nets for axis-parallel rectangles and boxes", "author": ["Boris Aronov", "Esther Ezra", "Micha Sharir"], "venue": "SIAM Journal of Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Constructive algorithms for discrepancy minimization", "author": ["Nikhil Bansal"], "venue": "In Proceedings 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Fat-shattering and the learnability of real-valued functions", "author": ["Peter L. Bartlett", "Philip M. Long", "Robert C. Williamson"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Worst-case bounds for subadditive geometric graphs", "author": ["Marshall Bern", "David Eppstein"], "venue": "Proceedings 9th ACM Symposium on Computational Geometry,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["Anselm Blumer", "A. Ehrenfeucht", "David Haussler", "Manfred K. Warmuth"], "venue": "Journal of ACM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "The Discrepancy Method", "author": ["Bernard Chazelle"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "The convergence of bird flocking", "author": ["Bernard Chazelle"], "venue": "In Proceedings 26th Annual Symposium on Computational Geometry,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "On linear-time deterministic algorithms for optimization problems in fixed dimensions", "author": ["Bernard Chazelle", "Jiri Matousek"], "venue": "J. Algorithms,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Super-samples from kernel hearding", "author": ["Yutian Chen", "Max Welling", "Alex Smola"], "venue": "In Conference on Uncertainty in Artificial Intellegence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Structure-aware sampling: Flexible and accurate summarization", "author": ["Edith Cohen", "Graham Cormode", "Nick Duffield"], "venue": "In Proceedings 37th International Conference on Very Large Data Bases,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Nonparametric Density Estimation: The L1 View", "author": ["Luc Devroye", "L\u00e1szl\u00f3 Gy\u00f6rfi"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1984}, {"title": "Combinatorial Methods in Density Estimation", "author": ["Luc Devroye", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Paths, trees, and flowers", "author": ["Jack Edmonds"], "venue": "Canadian Journal of Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1965}, {"title": "A note about weak epsilon-nets for axis-parallel boxes in d-space", "author": ["Esther Ezra"], "venue": "Information Processing Letters,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "epsilon-nets and simplex range queries", "author": ["David Haussler", "Emo Welzl"], "venue": "Disc. & Comp. Geom.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1987}, {"title": "Comparing distributions and shapes using the kernel distance", "author": ["Sarang Joshi", "Raj Varma Kommaraju", "Jeff M. Phillips", "Suresh Venkatasubramanian"], "venue": "In 27th Annual Symposium on Computational Geometry,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Efficient distribution-free learning of probabilistic concepts", "author": ["Michael Kerns", "Robert E. Shapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Almost tight bounds for epsilon nets", "author": ["J. Koml\u00f3s", "J\u00e1nos Pach", "Gerhard Woeginger"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}, {"title": "Improved bounds on the samples complexity of learning", "author": ["Yi Li", "Philip M. Long", "Aravind Srinivasan"], "venue": "J. Comp. and Sys. Sci.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Constructive discrepancy minimization by walking on the edges", "author": ["Shachar Lovett", "Raghu Meka"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Geometric Discrepancy; An Illustrated", "author": ["Jiri Matousek"], "venue": "Guide. Springer,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "How to net a lot with a little: Small \u03b5-nets for disks and halfspaces", "author": ["Jiri Matou\u0161ek", "Raimund Seidel", "Emo Welzl"], "venue": "In Proceedings 6th Annual ACM Symposium on Computational Geometry,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1990}, {"title": "The probabilistic method (lecture notes)", "author": ["Jir\u0159i Matou\u0161ek", "Jan Vondrak"], "venue": "http://kam.mff. cuni.cz/ \u0303matousek/prob-ln.ps.gz,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Tight lower bounds for the size of epsilon-nets", "author": ["J\u00e1nos Pach", "G\u00e1bor Tardos"], "venue": "In Proceedings 27th Annual Symposium on Computational Geometry,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Algorithms for \u03b5-approximations of terrains", "author": ["Jeff M. Phillips"], "venue": "In ICALP,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "The VCdimension of queries and selectivity estimation through sampling", "author": ["Matteo Riondato", "Mert Akdere", "Ugur Cetintemel", "Stanley B. Zdonik", "Eli Upfal"], "venue": "Technical report,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Multivariate Density Estimation: Theory, Practice, and Visualization", "author": ["David W. Scott"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1992}, {"title": "Density Estimation for Statistics and Data Analysis", "author": ["Bernard W. Silverman"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1986}, {"title": "Range counting over multidimensional data streams", "author": ["S. Suri", "C. Toth", "Y. Zhou"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Inductive principles of the search for emperical dependencies", "author": ["Vladimir Vapnik"], "venue": "In Proceedings of the Second Annual Workshop on Computational Learning Theory,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1989}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["Vladimir Vapnik", "Alexey Chervonenkis"], "venue": "The. of Prob. App.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1971}, {"title": "A divide-and-conquer algorithm for min-cost perfect matching in the plane", "author": ["Kasturi R. Varadarajan"], "venue": "In Proceedings 39th IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Interestingly, in this case the ranges are not binary, but have a continuous range (for simplicity we focus on kernels with range of [0, 1]); these allow for smoother notions of range spaces.", "startOffset": 133, "endOffset": 139}, {"referenceID": 0, "context": "Thus we can re-imagine a kernel range space (P,K) as the family of fractional subsets of P , that is, each p \u2208 P does not need to be completely in (1) or not in (0) a range, but can be fractionally in a range described by a value in [0, 1].", "startOffset": 233, "endOffset": 239}, {"referenceID": 24, "context": "See Matou\u015bek\u2019s [26] and Chazelle\u2019s [10] books for a masterful treatments of this field when restricted to combinatorial discrepancy.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "See Matou\u015bek\u2019s [26] and Chazelle\u2019s [10] books for a masterful treatments of this field when restricted to combinatorial discrepancy.", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "[21] showed that for any kernel range space (P,K) where all super-level sets of kernels are described by elements of a binary range space (P,A), then an \u03b5-sample of (P,A) is also an \u03b5-sample of (P,K).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2, 30, 14, 31]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 28, "context": "[2, 30, 14, 31]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 13, "context": "[2, 30, 14, 31]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 29, "context": "[2, 30, 14, 31]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 34, "context": "For binary range spaces with constant VCdimension [36] a random sample S of size O((1/\u03b52) log(1/\u03b4)) provides an \u03b5-sample with probability at least 1 \u2212 \u03b4 [24].", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "For binary range spaces with constant VCdimension [36] a random sample S of size O((1/\u03b52) log(1/\u03b4)) provides an \u03b5-sample with probability at least 1 \u2212 \u03b4 [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "Better bounds can be achieved through deterministic approaches as outlined by Chazelle and Matousek [12], or see either of their books for more details [10, 26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "Better bounds can be achieved through deterministic approaches as outlined by Chazelle and Matousek [12], or see either of their books for more details [10, 26].", "startOffset": 152, "endOffset": 160}, {"referenceID": 24, "context": "Better bounds can be achieved through deterministic approaches as outlined by Chazelle and Matousek [12], or see either of their books for more details [10, 26].", "startOffset": 152, "endOffset": 160}, {"referenceID": 28, "context": "As spelled out explicitly by Phillips [30] (see also [26, 10] for more classic references), for a range space (P,A) with discrepancy O(log |P |) (resp.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "As spelled out explicitly by Phillips [30] (see also [26, 10] for more classic references), for a range space (P,A) with discrepancy O(log |P |) (resp.", "startOffset": 53, "endOffset": 61}, {"referenceID": 9, "context": "As spelled out explicitly by Phillips [30] (see also [26, 10] for more classic references), for a range space (P,A) with discrepancy O(log |P |) (resp.", "startOffset": 53, "endOffset": 61}, {"referenceID": 5, "context": "But, recently Bansal [6] provided a randomized constructive algorithm; also see a similar, simpler and more explicit, approach recently on the arXiv [25].", "startOffset": 21, "endOffset": 24}, {"referenceID": 23, "context": "But, recently Bansal [6] provided a randomized constructive algorithm; also see a similar, simpler and more explicit, approach recently on the arXiv [25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 28, "context": "1, the above stated results in [30], and Edmond\u2019s O(n3) time algorithm for min-cost matching M [18].", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "1, the above stated results in [30], and Edmond\u2019s O(n3) time algorithm for min-cost matching M [18].", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "This follows (after some technical details) from a result of Bern and Eppstein [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 32, "context": "[34, 30]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 28, "context": "[34, 30]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "Furthermore, most techniques other than random sampling (size 1/\u03b52) are quite complicated and rarely implemented (many require Bansal\u2019s recent result [6] or its simplification [25]).", "startOffset": 150, "endOffset": 153}, {"referenceID": 23, "context": "Furthermore, most techniques other than random sampling (size 1/\u03b52) are quite complicated and rarely implemented (many require Bansal\u2019s recent result [6] or its simplification [25]).", "startOffset": 176, "endOffset": 180}, {"referenceID": 18, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 8, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 25, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 21, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 4, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 2, "context": "Super-linear lower bounds are known as well [3, 29].", "startOffset": 44, "endOffset": 51}, {"referenceID": 27, "context": "Super-linear lower bounds are known as well [3, 29].", "startOffset": 44, "endOffset": 51}, {"referenceID": 31, "context": "Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gy\u00f6rfi, and/or Lugosi [15, 17, 16] on L1 error.", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gy\u00f6rfi, and/or Lugosi [15, 17, 16] on L1 error.", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gy\u00f6rfi, and/or Lugosi [15, 17, 16] on L1 error.", "startOffset": 137, "endOffset": 149}, {"referenceID": 15, "context": "Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gy\u00f6rfi, and/or Lugosi [15, 17, 16] on L1 error.", "startOffset": 137, "endOffset": 149}, {"referenceID": 12, "context": "Recently Chen, Welling, and Smola [13] showed that for any positive definite kernel (including G, but not", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 119, "endOffset": 123}, {"referenceID": 6, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 165, "endOffset": 179}, {"referenceID": 3, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 165, "endOffset": 179}, {"referenceID": 33, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 165, "endOffset": 179}, {"referenceID": 19, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 217, "endOffset": 221}, {"referenceID": 10, "context": "This raises several questions: are these binary range spaces which require size super-linear in 1/\u03b5 really necessary for downstream analysis? Can we simplify many analyses by using kernels in place of binary ranges? One possible target are the quite fascinating, but enormous bounds for bird-flocking [11].", "startOffset": 301, "endOffset": 305}, {"referenceID": 7, "context": "In fact, Bern and Eppstein [8] perform just such an analysis considering the minimum cost matching of points within [0, 1]d.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "In fact, Bern and Eppstein [8] perform just such an analysis considering the minimum cost matching of points within [0, 1]d.", "startOffset": 116, "endOffset": 122}, {"referenceID": 0, "context": "To apply this result of Bern and Eppstein, we can consider a ball of radius 1/2 that fits inside of [0, 1]d by scaling down all lengths uniformly by 1/2rad(B).", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "Now the sum of dth power of edge lengths where both endpoints are within B is at most O(1) since these points are also within [0, 1]d.", "startOffset": 126, "endOffset": 132}, {"referenceID": 28, "context": "Now this and [30] implies Theorem 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "5 log n) time algorithm [37] for computing the min-cost matching in R2; the log factor in the runtime can be improved using better SSPD constructions [1] toO(n1.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "5 log n) time algorithm [37] for computing the min-cost matching in R2; the log factor in the runtime can be improved using better SSPD constructions [1] toO(n1.", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "A version of Bern and Eppstein\u2019s result [8] with explicit constants would be of some interest.", "startOffset": 40, "endOffset": 43}, {"referenceID": 14, "context": "This can include kernels which may be negative (such as the sinc or trapezoidal kernels [15, 17]) and have nice L2 KDE approximation properties.", "startOffset": 88, "endOffset": 96}, {"referenceID": 15, "context": "This can include kernels which may be negative (such as the sinc or trapezoidal kernels [15, 17]) and have nice L2 KDE approximation properties.", "startOffset": 88, "endOffset": 96}, {"referenceID": 19, "context": "These sometimes-negative kernels cannot use the \u03b5-sample result from [21] because their superlevel sets have unbounded VC-dimension since k(z) = 0 for infinitely many disjoint values of z.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "Edmond\u2019s min-cost matching algorithm [18] runs inO(n3) time in Rd and Varadarajan\u2019s improvement [37] runs in O(n1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 35, "context": "Edmond\u2019s min-cost matching algorithm [18] runs inO(n3) time in Rd and Varadarajan\u2019s improvement [37] runs in O(n1.", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "5 log n) [1]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "Finally, I thank David Gamarnik for communicating the Bern-Eppstein result [8].", "startOffset": 75, "endOffset": 78}], "year": 2012, "abstractText": "We study the worst case error of kernel density estimates via subset approximation. A kernel density estimate of a distribution is the convolution of that distribution with a fixed kernel (e.g. Gaussian kernel). Given a subset (i.e. a point set) of the input distribution, we can compare the kernel density estimates of the input distribution with that of the subset and bound the worst case error. If the maximum error is \u03b5, then this subset can be thought of as an \u03b5-sample (aka an \u03b5-approximation) of the range space defined with the input distribution as the ground set and the fixed kernel representing the family of ranges. Interestingly, in this case the ranges are not binary, but have a continuous range (for simplicity we focus on kernels with range of [0, 1]); these allow for smoother notions of range spaces. It turns out, the use of this smoother family of range spaces has an added benefit of greatly decreasing the size required for \u03b5-samples. For instance, in the plane the size is O((1/\u03b5) log(1/\u03b5)) for disks (based on VC-dimension arguments) but is only O((1/\u03b5) \u221a log(1/\u03b5)) for Gaussian kernels and for kernels with bounded slope that only affect a bounded domain. These bounds are accomplished by studying the discrepancy of these \u201ckernel\u201d range spaces, and here the improvement in bounds are even more pronounced. In the plane, we show the discrepancy is O( \u221a log n) for these kernels, whereas for balls there is a lower bound of \u03a9(n). ar X iv :1 11 2. 41 05 v3 [ cs .C G ] 3 A pr 2 01 2", "creator": "LaTeX with hyperref package"}}}