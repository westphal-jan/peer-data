{"id": "1112.5309", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2011", "title": "POWERPLAY: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem", "abstract": "Most of computer science focuses on automatically solving given computational problems. I focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans, to train a more and more general problem solver from scratch in an unsupervised fashion. At any given time, the novel algorithmic framework POWERPLAY searches the space of possible pairs of new tasks and modifications of the current problem solver, until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one, while the unmodified predecessor does not solve one of the new ones.\n\n\nPowerPLAY, along with the many others that have helped make the current computer science work, has a wide range of new features, as well as some of the most impressive and exciting new and exciting new features. It is also currently running a suite of programs that will allow you to see and respond to problems within the framework of the past.\nWe have many tools for creating the latest and greatest software, including these:\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPowerPLAY,\nPower", "histories": [["v1", "Thu, 22 Dec 2011 13:50:46 GMT  (28kb)", "http://arxiv.org/abs/1112.5309v1", "19 pages"], ["v2", "Sun, 4 Nov 2012 17:22:46 GMT  (30kb)", "http://arxiv.org/abs/1112.5309v2", "21 pages, additional connections to previous work, references to first experiments with POWERPLAY"]], "COMMENTS": "19 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["j\\\"urgen schmidhuber"], "accepted": false, "id": "1112.5309"}, "pdf": {"name": "1112.5309.pdf", "metadata": {"source": "CRF", "title": "POWERPLAY: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem", "authors": ["J\u00fcrgen Schmidhuber"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n11 2.\n53 09\nv1 [\ncs .A\nI] 2\nContents"}, {"heading": "1 Introduction 3", "text": "1.1 Basic Ideas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Outline of Remainder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2 Notation & Algorithmic Framework POWERPLAY (Variant I) 5\n3 TASK INVENTION, SOLVER MODIFICATION, CORRECTNESS DEMO 6 3.1 Implementing TASK INVENTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n3.1.1 Example: Pattern Recognition Tasks . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.1.2 Example: General Decision Making Tasks . . . . . . . . . . . . . . . . . . . . . . 6\n3.2 Implementing SOLVER MODIFICATION . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.3 Implementing CORRECTNESS DEMONSTRATION . . . . . . . . . . . . . . . . . . . . . . 7\n3.3.1 Most General: Proof Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.3.2 Keeping Track Which Components of the Solver Affect Which Tasks . . . . . . . 8 3.3.3 Advantages of Prefix Code-Based Problem Solvers . . . . . . . . . . . . . . . . . 8\n4 Implementations of POWERPLAY 9 4.1 Implementation Based on Optimal Ordered Problem Solver . . . . . . . . . . . . . . . . . 9\n4.1.1 Building on Existing OOPS Source Code . . . . . . . . . . . . . . . . . . . . . . 10 4.1.2 Alternative Problem Solver Based on Recurrent Neural Networks . . . . . . . . . 10\n4.2 Adapting the Probability Distribution on Programs . . . . . . . . . . . . . . . . . . . . . 10 4.3 Implementation Based on Stochastic or Evolutionary Search . . . . . . . . . . . . . . . . 11"}, {"heading": "5 Outgrowing Trivial Tasks - Compressing Previous Solutions 11", "text": ""}, {"heading": "6 Adding External Tasks 11", "text": "6.1 Self-Reference Through Novel Task Search as an External Task . . . . . . . . . . . . . . 12\n7 Softening Task Acceptance Criteria of POWERPLAY 12 7.1 POWERPLAY Variant II: Explicitly Penalizing Time and Space Complexity . . . . . . . . 12 7.2 Probabilistic POWERPLAY Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"}, {"heading": "8 First Illustrative Experiments 13", "text": ""}, {"heading": "9 Previous Relevant Work 15", "text": "9.1 Existing Theoretically Optimal Universal Problem Solvers . . . . . . . . . . . . . . . . . 15 9.2 Greedy Implementation of the Formal Theory of Creativity . . . . . . . . . . . . . . . . . 15"}, {"heading": "10 Words of Caution 17", "text": "11 Acknowledgments 17"}, {"heading": "1 Introduction", "text": "Given a realistic piece of computational hardware with specific resource limitations, how can one devise software for it that will solve all, or at least many, of the a priori unknown tasks that are in principle easily solvable on this architecture? In other words, how to build a practical general problem solver, given the computational restrictions? It does not need to be universal and asymptotically optimal [13, 11, 32, 35] like the recent (not necessarily practically feasible) general problem solvers discussed in Section 9.1; instead it should take into account all constant architecture-specific slowdowns ignored in the asymptotic optimality notation of theoretical computer science, and be generally useful for real-world applications.\nLet us draw inspiration from biology. How do initially helpless human babies become rather general problem solvers over time? Apparently by playing. For example, even in the absence of external reward or hunger they are curious about what happens if they move their eyes or fingers in particular ways, creating little experiments which lead to initially novel and surprising but eventually predictable sensory inputs, while also learning motor skills to reproduce these outcomes. (See [25, 24, 29, 33, 36, 44] and Section 9.2 for previous artificial systems of this type.) Infants continually seem to invent new tasks that become boring as soon as their solutions are known. Easy-to-learn new tasks are preferred over unsolvable or hard-to-learn tasks. Eventually the numerous skills acquired in this creative, self-supervised way may get re-used to facilitate the search for solutions to external problems, such as finding food when hungry.\nHere I introduce a novel unsupervised algorithmic framework for training a computational problem solver from scratch, continually searching for the simplest (fastest to find) combination of task and corresponding task-solving skill to add to its growing repertoire, without forgetting any previous skills (Section 2), or at least without decreasing average performance on previously solved tasks (Section 7.1). Every new task added to the repertoire is essentially defined by the time required to invent it, to solve it, and to demonstrate that no previously learned skills got lost. The search takes into account that typical problem solvers may learn to solve tasks outside the growing self-made training set due to generalization properties of their architectures. The framework is called POWERPLAY because it continually [21] aims at boosting computational prowess and problem solving capacity, reminiscent of humans or human societies trying to boost their general power/capabilities/knowledge/skills in playful ways, even in the absence of externally defined goals, although the skills learned by this type of pure curiosity may later help to solve externally posed tasks.\nUnlike our first implementations of curious/creative/playful agents from the 1990s [24, 40, 29] (Section 9.2; compare [1, 4, 18, 15]), POWERPLAY provably (by design) does not have any problems with online learning\u2014it cannot forget previously learned skills, automatically segmenting its life into a sequence of clearly identified tasks with explicitly recorded solutions. Unlike the task search of theoretically optimal creative agents [33, 36] (Section 9.2), POWERPLAY\u2019s task search is greedy, but at least practically feasible.\nSome claim that scientists often invent appropriate problems for their methods, rather than inventing methods to solve given problems. The present paper formalizes this in a way that may be more convenient to implement than those of previous work [24, 29, 33, 36], and describes a simple practical framework for building creative artificial scientists or explorers that by design continually come up with the fastest to find, initially novel, but eventually solvable problems."}, {"heading": "1.1 Basic Ideas", "text": "In traditional computer science, given some formally defined task, a search algorithm is used to search a space of solution candidates until a solution to the task is found and verified. If the task is hard the search may take long.\nTo automatically construct an increasingly general problem solver, let us expand the traditional search space in an unusual way, such that it includes all possible pairs of tasks and problem solvers. Given an old problem solver that can already solve a finite known set of previously learned tasks, a search algorithm is used to find a new pair that provably has the following properties: (1) The new task cannot be solved by the old problem solver. (2) The new task can be solved by the new problem solver (some modification of the old one). (3) The new solver can still solve the known set of previously learned tasks.\nOnce such a pair is found, the cycle repeats itself. This will result in a continually growing set of known tasks solvable by an increasingly more powerful problem solver.\nSmart search (Section 4.1, Algorithms 4.1, 8) orders candidate pairs of the type (task, solver) by computational complexity, using concepts of optimal universal search [13, 32], with a bias towards pairs that can be described by few additional bits of information (given the experience so far) and that can be validated quickly.\nAt first glance it might seem harder to search for pairs of tasks and solvers instead of solvers only, due to the apparently larger search space. However, the additional freedom of inventing the tasks to be solved may actually greatly reduce the time intervals between problem solver advances.\nA new task may be about simplifying the old solver such that it can still solve all tasks learned so far, but with less computational resources such as time and storage space (e.g., Section 3.1 and Algorithm 7.1).\nSince the new pair (task, solver) is the first one found and validated, the search automatically trades off the time-varying efforts required to either invent completely new, previously unsolvable problems, or compressing/speeding up previous solutions. Sometimes it is easier to refine or simplify known skills, sometimes to invent new skills.\nOn typical problem solver architectures of personal computers (PCs) or neural networks (NN), while a limited known number of previously learned tasks has become solvable, so too has a large number of unknown, never-tested tasks (in the field of Machine Learning, this is known as generalization). POWERPLAY\u2019s ongoing search is continually testing (and always trying to go beyond) the generalization abilities of the most recent solver instance; some of its search time has to be spent on demonstrating that self-invented new tasks are not already solvable.\nOften, however, much more time will have to be spent on making sure that a newly modified solver did not forget any of the possibly many previously learned skills. Problem solver modularization (Section 3.3, especially 3.3.2) may greatly reduce this time though, making POWERPLAY prefer pairs whose validation does not require the re-testing of too many previously learned skills, thus decomposing at least part of the search space into somewhat independent regions, realizing divide and conquer strategies as by-products of its built-in drive to invent and validate novel tasks/skills as quickly as possible.\nBut what prevents POWERPLAY from inventing trivial tasks forever by extreme modularization, simply allocating a previously unused solver part to each new task, which thus becomes rather quickly verifiable, as its solution does not affect solutions to previous tasks (Section 3.3.3)? At least once the solver\u2019s upper storage size limit is reached, POWERPLAY will have to compress solutions in increasingly non-trivial ways. Even earlier, on typical PC or NN-based solvers, often the most easily found new skills will partially reuse earlier found code (see Sections 5 and 3.3.3).\nA biologically inspired hope is that as the problem solver is becoming more and more general, it will find it easier and easier to solve externally posed tasks (Section 6), just like growing infants often seem to re-use their playfully acquired skills to solve teacher-given problems."}, {"heading": "1.2 Outline of Remainder", "text": "Section 2 will introduce basic notation and Variant 1 of the algorithmic framework POWERPLAY, which invokes the essential procedures TASK INVENTION, SOLVER MODIFICATION, and CORRECTNESS DEMONSTRATION. Section 3 will discuss details of these procedures.\nMore detailed instantiations of POWERPLAY will be described in Section 4.3 (an evolutionary method, Alg. 4.3) and Section 4.1 (an asymptotically optimal program search method, Alg. 4.1).\nAs mentioned above, the skills acquired to solve self-generated tasks may later greatly facilitate solutions to externally posed tasks, just like the numerous motor skills learned by babies during curious exploration of its world often can be re-used later to maximize external reward. Sections 6 and 7.1 will discuss variants of the framework (e.g., Algorithm 7.1) in which some of the Ti can be defined externally.\nSection 7.1 will also describe a natural variant of the framework that explicitly penalizes solution costs (including time and space complexity), and allows for forgetting aspects of previous solutions, provided the average performance on previously solved tasks does not decrease.\nSection 8 will mention preliminary illustrative experiments (but detailed experimental analysis will be left to separate papers). Section 9 will discuss the relationship to previous systems equipped with artificial curiosity and creativity.\n2 Notation & Algorithmic Framework POWERPLAY (Variant I)\nB\u2217 denotes the set of finite sequences or bitstrings over the binary alphabet B = {0, 1}, \u03bb the empty string, x, y, z, p, q, r, u strings in B\u2217, N the natural numbers, R the real numbers, \u01eb \u2208 R a positive constant, m,n, n0, k, i, j, k, l non-negative integers, L(x) the number of bits in x (where L(\u03bb) = 0), f, g functions mapping integers to integers. We write f(n) = O(g(n)) if there exist positive c, n0 such that f(n) \u2264 cg(n) for all n > n0.\nThe computational architecture of the problem solver may be a deterministic universal computer, or a more limited device such as a finite state automaton or a feedforward neural network (NN) [2]. All such problem solvers can be uniquely encoded [7] or implemented on universal computers such as universal Turing Machines (TM) [42]. Therefore, without loss of generality, the remainder of this paper assumes a fixed universal reference computer whose input programs and outputs are elements of B\u2217. A user-defined subset S \u2282 B\u2217 defines the set of possible problem solvers. For example, if the problem solver\u2019s architecture is itself a binary universal TM or a standard computer, then S represents its set of possible programs, or a limited subset thereof\u2014compare Sections 3.2 and 4.1. If it is a feedforward NN, then S could be a highly restricted subset of programs encoding the NN\u2019s possible topologies and weights (floating point numbers)\u2014compare Section 8.\nIn what follows, for convenience I will often identify bitstrings in B\u2217 with things they encode, such as integers, real-valued vectors, weight matrices, or programs\u2014the context will always make clear what is meant.\nThe problem solver\u2019s initial program is called s0. There is a set of possible task descriptions T \u2282 B\u2217, and a particular sequence of task descriptions T1, T2, . . ., where each unique Ti \u2208 T (i = 1, 2, . . .) is chosen or \u201cinvented\u201d by a search method described below such that the solutions of T1, T2, . . . , Ti can be computed by si, the i-th instance of the program, but not by si\u22121 (i = 1, 2, . . .). Each Ti consists of a unique problem identifier that can be read by si through some built-in input processing mechanism (e.g., input neurons of an NN as in Sections 4.1.2 and 8), and a unique description of a deterministic procedure for determining whether the problem has been solved. For example, a simple task may require the solver to answer a particular input pattern with a particular output pattern (more formal details on pattern recognition tasks are given in Section 3.1.1). Or it may require the solver to steer a robot towards a goal through a sequence of actions (more formal details on sequential decision making tasks in unknown environments are given in Section 3.1.2). Denote T\u2264i = {T1, . . . , Ti}; T<i = {T1, . . . , Ti\u22121}.\nA valid task Ti(i > 1) may require solving at least one previously solved task Tk(k < i) more efficiently, by using less resources such as storage space, computation time, energy, etc. See Section 3.1.\nTasks and problem solver modifications are computed and validated by elements of another appropriate set of programs P \u2282 B\u2217. Programs p \u2208 P may contain instructions for reading and executing (parts of) the code of the present problem solver and reading (parts of) a recorded history Trace \u2208 B\u2217 of previous events that led to the present solver. The algorithmic framework (Alg. 2) incrementally trains the problem solver by finding p \u2208 P that increase the set of solvable tasks.\nAlg. 2: Algorithmic Framework POWERPLAY (Variant I) Initialize s0 in some way. for i := 1, 2, . . . do\nrepeat Let a search algorithm (examples in Section 4) create a new candidate program p \u2208 P . Give p limited time to do (not necessarily in this order): * TASK INVENTION: Let p compute a task T \u2208 T . See Section 3.1. * SOLVER MODIFICATION: Let p compute a value of the variable q \u2208 S \u2282 B\u2217 (a candidate for si) by computing a modification of si\u22121. See Section 3.2. * CORRECTNESS DEMONSTRATION: Let p try to show that T cannot be solved by si\u22121, but that T and all Tk(k < i) can be solved by q. See Section 3.3. until CORRECTNESS DEMONSTRATION was successful Set pi := p;Ti := T ; si := q; update Trace.\nend for\n3 TASK INVENTION, SOLVER MODIFICATION, CORRECTNESS DEMO\nA program tested by Alg. 2 has to allocate its runtime to solve three main jobs, namely, TASK INVENTION, SOLVER MODIFICATION, CORRECTNESS DEMONSTRATION. Now examples of each will be listed.\n3.1 Implementing TASK INVENTION\nPart of the job of pi \u2208 P is to compute Ti \u2208 T . This will consume some of the total computation time allocated to pi. Two examples will be given: pattern recognition tasks are treated in Section 3.1.1; sequential decision making tasks in Section 3.1.2."}, {"heading": "3.1.1 Example: Pattern Recognition Tasks", "text": "In the context of learning to recognize or analyze patterns, Ti could be a 4-tuple (Ii, Oi, ti, ni) \u2208 I \u00d7 O \u00d7 N \u00d7 N, where I,O \u2282 B\u2217, and Ti is solved if si satisfies L(si) < ni and needs at most ti discrete time steps to read Ii and compute Oi and halt. Here Ii itself may be a pair (I1i , I 2 i ) \u2208 B\n\u2217 \u00d7 B\u2217, where I1i is constrained to be the address of an image in a given database of patterns, and I 2 i is a pi-generated \u201cquery\u201d that uniquely specifies how the image should be classified through target pattern Oi, such that the same image can be analyzed in different ways during different tasks. For example, depending on the nature of the invented task sequence, the problem solver could eventually learn that O = 1 if I2 = 1001 (suppressing task indices) and the image addressed by I1 contains at least one black pixel, or if I2 = 0111 and the image shows a cow.\nSince the definition of task Ti includes bounds ni, ti on computational resources, Ti may be about solving at least one Tk(k < i) more efficiently. This in turn may also yield more efficient solutions to other tasks Tl(l < i, l 6= k). In practical applications one may insist that such efficiency gains must exceed a certain threshold \u01eb > 0, to avoid task series causing sequences of very minor improvements.\nNote that ni and ti may be unnecessary in special cases such as the problem solver being a fixed topology feedforward NN [2] whose input and target patterns have constant size and whose computational efforts per pattern need constant time and space resources.\nAssuming sufficiently powerful S,P , in the beginning, trivial tasks such as simply copying I2i onto Oi may be interesting in the sense that POWERPLAY can still validate and accept them, but they will become boring (inadmissible) as soon as they are solvable by solutions to previous tasks that generalize to new tasks."}, {"heading": "3.1.2 Example: General Decision Making Tasks", "text": "In the more general context of general problem solving/sequential decision making/reinforcement learning/reward optimization [16, 12, 41] in unknown environments, there may be a set I \u2282 B\u2217 of possible task identification patterns and a set J \u2282 B\u2217 of programs that test properties of bitstrings. Ti could then encode a 4-tuple (Ii, Ji, ti, ni) \u2208 I \u00d7 J \u00d7 N \u00d7 N of finite bitstrings with the following interpretation: si must satisfy L(si) < ni and may spend at most ti discrete time steps on first reading Ii and then interacting with an environment through a sequence of perceptions and actions, to achieve some computable goal defined by Ji.\nMore precisely, while Ti is being solved within ti time steps, at any given time 1 \u2264 t \u2264 ti, the internal state of the problem solver at time t is denoted ui(t) \u2208 B\u2217; its initial default value is ui(0). For example, ui(t) may encode the current contents of the internal tape of a TM, or of certain addresses in the dynamic storage area of a PC, or the present activations of an LSTM recurrent NN [9]. At time t, si can spend a constant number of elementary computational instructions to copy the task dscription Ti or the present environmental input xi(t) \u2208 B\u2217 and a reward signal ri(t) \u2208 B\u2217 (interpreted as a real number) into parts of ui(t), then update other parts of ui(t) (a function of ui(t\u2212 1)) and compute action yi(t) \u2208 B\u2217 encoded as a part of ui(t). yi(t) may affect the environment, and thus future inputs.\nIf P allows for programs that can dynamically acquire additional physical computational resources such as additional CPUs and storage, then the above constant number of elementary computational in-\nstructions should be replaced by a constant amount of real time, to be measured by a reliable physical clock.\nThe sequence of 4-tuples (xi(t), ri(t), ui(t), yi(t)) (t = 1, . . . , ti) gets recorded by the so-called trace Tracei \u2208 B\n\u2217. If at the end of the interaction a desirable computable property Ji(Tracei) (computed by applying program Ji to Tracei) is satisfied, then by definition the task is solved. For practical reasons, the set J of possible Ji may be restricted to bit sequences encoding just a few possible goals. For example, Ji may only encode goals of the form: a robot arm steered by program or \u201cpolicy\u201d si has reached a certain target (a desired final observation xi(ti) recorded in Tracei) without measurably bumping into an obstacle along the way, that is, there were no negative rewards, that is, ri(\u03c4) \u2265 0 for \u03c4 = 1 . . . ti.\nIf the environment is deterministic, e.g., a digital physics simulation of a robot, then its current state can be encoded as part of u(t), and it is straight-forward for CORRECTNESS DEMONSTRATION to test whether some si still can solve a previously solved task Tj(j < i). However, what if the environment is only partially observable, like the real world, and non-stationary, changing in unknown ways? Then CORRECTNESS DEMONSTRATION must check whether si still produces the same action sequence in response to the input sequence recorded in Tracej (often this replay-based test will actually be computationally cheaper than a test involving the environment). Achieving the same goal in a changed environment must be considered a different task, even if the changes are just due to noise on the environmental inputs. (Sure, in the real world sj(j > i) might actually achieve Ji faster than si, given the description of Ti, but CORRECTNESS DEMONSTRATION in general cannot know whether this acceleration was due to plain luck\u2014it must stick to reproducing Tracej to make sure it did not forget anything.)\nSee Section 7.2, however, for a less strict POWERPLAY variant whose CORRECTNESS DEMONSTRATION directly interacts with the real world to collect sufficient problem-solving statistics through repeated trials, making certain assumptions about the probabilistic nature of the environment, and the repeatability of experiments.\n3.2 Implementing SOLVER MODIFICATION\nPart of the job of pi \u2208 P is also to compute si, possibly profiting from having access to si\u22121, because only few changes of si\u22121 may be necessary to come up with an si that goes beyond si\u22121. For example, if the problem solver is a standard PC, then just a few bits of the previous software si\u22121 may need to be changed.\nFor practical reasons, the set S of possible si may be greatly restricted to bit sequences encoding programs that obey the syntax of a standard programming language such as LISP or Java. In turn, the programming language describing P should be greatly restricted such that any pi \u2208 P can only produce syntactically correct si.\nIf the problem solver is a feedforward NN with pre-wired, unmodifiable topology, then S will be restricted to those bit sequences encoding valid weight matrices, si will encode its i-th weight matrix, and P will be restricted to those p \u2208 P that can produce some si \u2208 S. Depending on the user-defined programming language, pi may invoke complex pre-wired subprograms (e.g., well-known learning algorithms) as primitive instructions\u2014see Section 8.\nIn general, p itself determines how much time to spend on SOLVER MODIFICATION\u2014enough time must be left for TASK INVENTION and CORRECTNESS DEMONSTRATION.\n3.3 Implementing CORRECTNESS DEMONSTRATION\nCorrectness demonstration may be the most time-consuming obligation of pi. At first glance it may seem that as the sequence T1, T2, . . . is growing, more and more time will be needed to show that si but not si\u22121 can solve T1, T2, . . . , Ti, because one naive way of ensuring correctness is to re-test si on all previously solved tasks. Theoretically more efficient ways are considered next."}, {"heading": "3.3.1 Most General: Proof Search", "text": "The most general way of demonstrating correctness is to encode (in read-only storage) an axiomatic system A that formally describes computational properties of the problem solver and possible si, and to allow pi to search the space of possible proofs derivable from A, using a proof searcher subroutine that systematically\ngenerates proofs until it finds a theorem stating that si but not si\u22121 solves T1, T2, . . . , Ti (proof search may achieve this efficiently without explicitly re-testing si on T1, T2, . . . , Ti). This could be done like in the Go\u0308del Machine [35] (Section 9.1), which uses an online extension of Universal Search [13] to systematically test proof techniques: proof-generating programs that may invoke special instructions for generating axioms and applying inference rules to prolong an initially empty proof \u2208 B\u2217 by theorems, which are either axioms or inferred from previous theorems through rules such as modus ponens combined with unification, e.g., [6]. P can be easily limited to programs generating only syntactically correct proofs [35]. A has to subsume axioms describing how any instruction invoked by some s \u2208 S will change the state u of the problem solver from one step to the next (such that proof techniques can reason about the effects of any si). Other axioms encode knowledge about arithmetics etc (such that proof techniques can reason about spatial and temporal resources consumed by si).\nIn what follows, CORRECTNESS DEMONSTRATIONS will be discussed that are less general but perhaps more convenient to implement."}, {"heading": "3.3.2 Keeping Track Which Components of the Solver Affect Which Tasks", "text": "Often it is possible to partition s \u2208 S into components, such as individual bits of the software of a PC, or weights of a NN. Here the k-th component of s is denoted sk. For each k (k = 1, 2, . . .) a variable list Lk = (T k1 , T k 2 , . . .) is introduced. Its initial value before the start of POWERPLAY is L k 0 , an empty list. Whenever pi found si and Ti at the end of CORRECTNESS DEMONSTRATION, each Lk is updated as follows: Its new value Lki is obtained by appending to L k i\u22121 those Tj /\u2208 L k i\u22121(j = 1, . . . , i) whose current (possibly revised) solutions now need sk at least once during the solution-computing process, and deleting those Tj whose current solutions do not use sk any more.\nPOWERPLAY\u2019s CORRECTNESS DEMONSTRATION thus has to test only tasks in the union of all Lki . That is, if the most recent task does not require changes of many components of s, and if the changed bits do not affect many previous tasks, then CORRECTNESS DEMONSTRATION may be very efficient.\nSince every new task added to the repertoire is essentially defined by the time required to invent it, to solve it, and to show that no previous tasks became unsolvable in the process, POWERPLAY is generally \u201cmotivated\u201d to invent tasks whose validity check does not require too much computational effort. That is, POWERPLAY will often find pi that generate si\u22121-modifications that don\u2019t affect too many previous tasks, thus decomposing at least part of the spaces of tasks and their solutions into more or less independent regions, realizing divide and conquer strategies as by-products."}, {"heading": "3.3.3 Advantages of Prefix Code-Based Problem Solvers", "text": "Let us restrict P such that tested p \u2208 P cannot change any components of si\u22121 during SOLVER MODIFICATION, but can create a new si only by adding new components to si\u22121. (This means freezing all used components of any sk once Tk is found.) By restricting S to self-delimiting prefix codes like those generated by the Optimal Ordered Problem Solver (OOPS) [32], one can now profit from a sometimes particularly efficient type of CORRECTNESS DEMONSTRATION, ensuring that differences between si and si\u22121 cannot affect solutions to T<i under certain conditions. More precisely, to obtain si, half the search time is spent on trying to process Ti first by si\u22121, extending or prolonging si\u22121 only when the ongoing computation requests to add new components through special instructions [32]\u2014then CORRECTNESS DEMONSTRATION has less to do as the set T<i is guaranteed to remain solvable, by induction. The other half of the time is spent on processing Ti by a new sub-program with new components s\u2032i, a part of si but not of si\u22121, where s\u2032i may read si\u22121 or invoke parts of si\u22121 as sub-programs to solve T\u2264i \u2014 only then CORRECTNESS DEMONSTRATION has to test si not only on Ti but also on T<i (see [32] for details).\nA simple but not very general way of doing something similar is to interleave TASK INVENTION, SOLVER MODIFICATION, CORRECTNESS DEMONSTRATION as follows: restrict all p \u2208 P such that they must define Ii := i as the unique task identifier Ii for Ti (see Section 3.1.2); restrict all s \u2208 S such that the input of Ii = i automatically invokes sub-program s\u2032i, a part of si but not of si\u22121 (although s \u2032 i may read si\u22121 or invoke parts of si\u22121 as sub-programs to solve Ti). Restrict Ji to a subset of acceptable computational outcomes (Section 3.1.2). Run si until it halts and has computed a novel output acceptable by Ji that is different from all outputs computed by the (halting) solutions to T<i; this novel output becomes Ti\u2019s\ngoal. By induction over i, since all previously used components of si\u22121 remain unmodified, the set T<i is guaranteed to remain solvable, no matter s\u2032i. That is, CORRECTNESS DEMONSTRATION on previous tasks becomes trivial. However, in this simple setup there is no immediate generalization across tasks like in OOPS [32] and the previous paragraph: the trivial task identifier i will always first invoke some s\u2032i different from all s\u2032k(k 6= i), instead of allowing for solving a new task solely by previously found code.\n4 Implementations of POWERPLAY"}, {"heading": "4.1 Implementation Based on Optimal Ordered Problem Solver", "text": "The i-th problem is to find a program pi \u2208 P that creates si and Ti and demonstrates that si but not si\u22121 can solve T1, T2, . . . , Ti. This yields a perfectly ordered problem sequence for a variant of the Optimal Ordered Problem Solver OOPS [32] described next.\nWhile a candidate program p \u2208 P is executed, at any given discrete time step t = 1, 2, ..., its internal state or dynamical storage U at time t is denoted U(t) \u2208 B\u2217 (not to be confused with the solver\u2019s internal state u(t) of Section 3.1.2). Its initial default value is U(0). E.g., U(t) could encode the current contents of the internal tape of a TM (to be modified by p), or of certain cells in the dynamic storage area of a PC.\nOnce pi is found, pi, si, Ti, T racei (if applicable; see Section 3.1.2) will be saved in unmodifiable readonly storage, possibly together with other data observed during the search so far. This may greatly facilitate the search for pk, k > i, since pk may contain instructions for addressing and reading pj, sj , Tj , T racej(j = 1, . . . , k\u2212 1) and for copying the read code into modifiable storage U , where pk may further edit the code, and execute the result, which may be a useful subprogram [32].\nDefine a probability distributionP (p) on P to represent the searcher\u2019s initial bias (more likely programs p will be tested earlier [13]). P could be based on program length, e.g.,P (p) = 2\u2212L(p), or on a probabilistic syntax diagram [32, 31]. See Algorithm 4.1.\nAlg. 4.1: Implementing POWERPLAY with Procedure OOPS [32] initialize s0 and u (internal dynamic storage for s \u2208 S) and U (internal dynamic storage for p \u2208 P), where each possible p is a sequence of subprograms p\u2032, p\u2032\u2032, p\u2032\u2032\u2032. for i := 1, 2, . . . do\nset variable time limit tlim := 1; let the variable set H be empty; set Boolean variable DONE := FALSE repeat\nif H is empty then set tlim := 2tlim; H := {p \u2208 P : P (p)tlim \u2265 1} else choose and remove some p from H while not DONE and less than P (p)tlim time was spent on p do\nexecute the next time step of the following computation: 1. Let p\u2032 compute some task T \u2208 T and halt. 2. Let p\u2032\u2032 compute q \u2208 S by modifying a copy of si\u22121, and halt. 3. Let p\u2032\u2032\u2032 try to show that q but not si\u22121 can solve T1, T2, . . . , Ti\u22121, T .\nIf p\u2032\u2032\u2032 was successful set DONE := TRUE. end while Undo all modifications of u and U due to p. This does not cost more time than executing p in the while loop above [32]. end if until DONE set pi := p; Ti := T ; si := q; add a unique encoding of the 5-tuple (i, pi, si, Ti, T racei) to read-only storage readable by programs to be tested in the future.\nend for\nOOPS keeps doubling the time limit until there is sufficient runtime for a sufficiently likely program to compute a novel, previously unsolvable task, plus its solver, which provably does not forget previous solutions. OOPS allocates time to programs according to an asymptotically optimal universal search method [13] for problems with easily verifiable solutions, that is, solutions whose validity can be quickly tested. Given some problem class, if some unknown optimal program p requires f(k) steps to solve a problem instance of size k and demonstrate the correctness of the result, then this search method will need at most O(f(k)/P (p)) = O(f(k)) steps\u2014the constant factor 1/P (p) may be large but does not depend on k. Since OOPS may re-use previously generated solutions and solution-computing programs, however, it may be possible to greatly reduce the constant factor associated with plain universal search [32].\nThe big difference to previous implementations of OOPS is that POWERPLAY has the additional freedom to define its own tasks. As always, every new task added to the repertoire is essentially defined by the time required to invent it, to solve it, and to demonstrate that no previously learned skills got lost."}, {"heading": "4.1.1 Building on Existing OOPS Source Code", "text": "Existing OOPS source code [31] uses a FORTH-like universal programming language to define P . It already contains a framework for testing new code on previously solved tasks, and for efficiently undoing all U -modifications of each tested program. The source code will require a few changes to implement the additional task search described above."}, {"heading": "4.1.2 Alternative Problem Solver Based on Recurrent Neural Networks", "text": "Recurrent NN (RNN) are general computers that allow for both sequential and parallel computations, unlike the strictly sequential FORTH-like language of Section 4.1.1. Here an RNN called RNN1 is used to define S.\nLet us expand the notation of Section 3.1.2. RNN1 has n(u) \u2208 N computational units or neurons; the k-th neuron is denoted uk. At discrete time step t = 1, 2, . . . , tend of a finite interaction sequence with the environment, uk(t) denotes the real-valued activation of uk. The current input vector x(t) (which also includes a unique encoding of the current task identifier) is translated by a fixed procedure into a real-valued vector with n(x) \u2208 N components, where the k-th component is denoted xk(t); we define uk(t) = xk(t) for k = 1, 2, . . . , n(x). The current reward signal r(t) is a special input and interpreted as a real value; we set un(x)+1(t) = r(t). For k = n(x) + 2, . . . , n(x) + n(y) + 1, we set yk(t) = uk(t), thus defining the n(y)-dimensional output vector y(t), which may affect the environment (e.g., by defining a robot action) and thus future x and r. For n(x) + 1 < k \u2264 n(u) we initialize uk(1) = 0 and for 1 \u2264 t < tend compute uk(t + 1) = fk( \u2211 l w lkuk(t)) (if uk is an additive neuron) or uk(t + 1) = fk( \u220f l w lkuk(t)) (if uk is a multiplicative neuron). Here the function fk maps real values to real values, e. g., fk(x) = 1/(1 + e\u2212x), or fk(x) = x, or fk(x) = 1 if x \u2265 0 and 0 otherwise; wlk is the real-valued weight on the directed connection from ul to uk. To program RNN1 means to set the weight matrix \u3008wlk\u3009. Given enough additive and multiplicative neurons and an appropriate weight matrix, RNN1 can compute any function computable by a standard PC [23].\nUsing RNN1 as above, a given problem solver s is defined by some weight matrix \u3008wlk\u3009. One could use Algorithm 4.1 to train it. P may itself be the set of weight matrices of a separate RNN called RNN2, computing tasks for RNN1, and modifications of RNN1, using techniques for network-modifying networks as described in [26, 28, 27]."}, {"heading": "4.2 Adapting the Probability Distribution on Programs", "text": "A straightforward extension of the above works as follows: Whenever a new pi is found, P is updated to make either only pi or all p1, p2, . . . , pi more likely. Simple ways of doing this are described in previous work [37]. This may be justified to the extent that future successful programs turn out to be similar to previous ones."}, {"heading": "4.3 Implementation Based on Stochastic or Evolutionary Search", "text": "A possibly simpler but less general approach is to use an evolutionary algorithm to produce an s-modifying and task-generating program p as requested by POWERPLAY, according to Algorithm 4.3, which refers to the recurrent net problem solver of Section 4.1.2.\nAlg. 4.3: POWERPLAY for RNN Using Stochastic or Evolutionary Search Randomly initialize RNN1\u2019s variable weight matrix \u3008wlk\u3009 and use the result as s0 (see Section 4.1.2) for i := 1, 2, . . . do\nset Boolean variable DONE=FALSE repeat\nuse a black box optimization algorithm BBOA (many are possible [20, 8, 43, 38]) with adaptive parameter vector \u03b8 to create some T \u2208 T (to define the task input to RNN1; see Section 3.1) and a modification of si\u22121, the current \u3008wlk\u3009 of RNN1, thus obtaining a new candidate q \u2208 S if q but not si\u22121 can solve T and all Tk(k < i) (see Sections 3.3, 3.3.2) then\nset DONE=TRUE end if\nuntil DONE set si := q; \u3008wlk\u3009 := q; Ti := T ; (also store Tracei if applicable, see Section 3.1.2). Use the information stored so far to adapt the parameters \u03b8 of the BBOA, e.g., by gradient-based search [43, 38], or according to the principles of evolutionary computation [20, 8, 43].\nend for"}, {"heading": "5 Outgrowing Trivial Tasks - Compressing Previous Solutions", "text": "Could it be that POWERPLAY keeps inventing trivial tasks forever? Not on realistic but general architectures such as PCs and RNNs. At least once the upper storage size limit of s is reached, POWERPLAY will start \u201ccompressing\u201d previous solutions, making s generalize in the sense that the same relatively short piece of code (some part of s) helps to solve different tasks.\nWith many computational architectures, this type of compression will start much earlier though, because new tasks solvable by partial reuse of earlier discovered code will often be easier to find than new tasks solvable by previously unused parts of s. This also holds for growing architectures with potentially unlimited storage space.\nCompare also POWERPLAY Variant II of Section 7.1 whose tasks may explicitly require improving the average time and space complexity of previous solutions by some minimal value.\nIn general, however, over time the system will find it more and more difficult to invent novel tasks without forgetting previous solutions, a bit like humans find it harder and harder to learn truly novel behaviors once they are leaving behind the initial rapid exploration phase typical for babies. Experiments with various problem solver architectures are needed to analyze such effects in detail (compare remarks in Section 8)."}, {"heading": "6 Adding External Tasks", "text": "The growing repertoire of the problem solver may facilitate learning of solutions to externally posed tasks. For example, one may modify POWERPLAY such that for certain i, Ti is defined externally, instead of being invented by the system itself. In general, the resulting si will contain an externally inserted bias in form of code that will make some future self-generated tasks easier to find than others. It should be possible to push the system in a human-understandable or otherwise useful direction by regularly inserting appropriate external goals. See Algorithm 7.1.\nAnother way of exploiting the growing repertoire is to simply copy si for some i and use it as a starting point for a search for a solution to an externally posed task T , without insisting that the modified si also\ncan solve T1, T2, . . . , Ti. This may be much faster than trying to solve T from scratch, to the extent the solutions to self-generated tasks reflect general knowledge (code) re-usable for T .\nIn general, however, it will be possible to design external tasks whose solutions do not profit from those of self-generated tasks\u2014the latter even may turn out to slow down the search.\nOn the other hand, in the real world the benefits of curious exploration seem obvious. One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein."}, {"heading": "6.1 Self-Reference Through Novel Task Search as an External Task", "text": "POWERPLAY\u2019s i-th goal is to find a pi \u2208 P that creates Ti and si (a modification of si\u22121) and shows that si but not si\u22121 can solve T\u2264i. As s itself is becoming a more and more general problem solver, s may help in many ways to achieve such goals in self-referential fashion. For example, the old solver si\u22121 may be able to read a unique formal description (provided by pi) of POWERPLAY\u2019s i-th goal, viewing it as an external task, and produce an output unambiguously describing a candidate for (Ti, si). If s has a theorem prover component (Section 3.3.1), si\u22121 might even output a full proof of (Ti, si)\u2019s validity; alternatively pi could just use the possibly suboptimal suggestions of si\u22121 to narrow down and speed up the search, one of the reasons why Section 2 already mentioned that programs p \u2208 P should contain instructions for reading (and running) the code of the present problem solver.\n7 Softening Task Acceptance Criteria of POWERPLAY\nThe POWERPLAY variants above insist that s may not solve new tasks at the expense of forgetting to solve any previously solved task within its previously established time and space bounds. For example, let us consider the sequential decision-making tasks from Section 3.1.2. Suppose the problem solver can already solve task Tk = (Ik, Jk, tk, nk) \u2208 I \u00d7 J \u00d7 N \u00d7 N. A very similar but admissible new task Ti = (Ik, Jk, ti, nk), (i > k), would be to solve Tk substantially faster: ti < tk \u2212 \u01eb, as long as Ti is not already solvable by si\u22121, and no solution to some Tl(l < i) is forgotten in the process.\nHere I discuss variants of POWERPLAY that soften the acceptance criteria for new tasks in various ways, for example, by allowing some of the computations of solutions to previous non-external (Section 6) tasks to slow down by a certain amount of time, provided the sum of their runtimes does not decrease. This also permits the system to invent new previously unsolved tasks at the expense of slightly increasing time bounds for certain already solved non-external tasks, but without decreasing the average performance on the latter. Of course, POWERPLAY has to be modified accordingly, updating average runtime bounds when necessary.\nAlternatively, one may allow for trading off space and time constraints in reasonable ways, e.g., in the style of asymptotically optimal Universal Search [13], which essentially trades one bit of additional space complexity for a runtime speedup factor of 2.\n7.1 POWERPLAY Variant II: Explicitly Penalizing Time and Space Complexity\nLet us remove time and space bounds from the task definitions of Section 3.1.2, since the modified costbased POWERPLAY framework below (Algorithm 7.1) will handle computational costs (such as time and space complexity of solutions) more directly. In the present section, Ti encodes a tuple (Ii, Ji) \u2208 I \u00d7 J with interpretation: si must first read Ii and then interact with an environment through a sequence of perceptions and actions, to achieve some computable goal defined by Ji within a certain maximal time interval tmax (a positive constant). Let t\u2032s(T ) be tmax if s cannot solve task T , otherwise it is the time needed to solve T by s. Let l\u2032s(T ) be the positive constant lmax if s cannot solve T , otherwise it is the number of components of s needed to solve task T by s. The non-negative real-valued reward r(T ) for solving T is a positive constant rnew for self-defined previously unsolvable T , or user-defined if T is an external task solved by s (Section 6). The real-valued cost Cost(s, TSET ) of solving all tasks in a task set TSET through s is a real-valued function of: all l\u2032s(T ), t \u2032 s(T ) (for all T \u2208 TSET ), L(s),\nand \u2211 T\u2208TSET r(T ). For example, the cost function Cost(s, TSET ) = L(s) + \u03b1 \u2211 T\u2208TSET [t \u2032 s(T ) \u2212 r(T )] encourages compact and fast solvers solving many different tasks with the same components of s, where the real-valued positive parameter \u03b1 weighs space costs against time costs, and rnew should exceed tmax to encourage solutions of novel self-generated tasks, whose cost contributions should be below zero (alternative cost definitions could also take into account energy consumption etc.)\nLet us keep an analogue of the remaining notation of Section 3.1.2, such as ui(t), xi(t), ri(t), yi(t), T racei, Ji(Tracei). As always, if the environment is unknown and possibly changing over time, to test performance of a new solver s on a previous task Tk, only Tracek is necessary\u2014see Section 3.1.2. As always, let T\u2264i denote the set containing all tasks T1, . . . , Ti (note that if Ti=Tk for some k < i then it will appear only once in T\u2264i), and let \u01eb > 0 again define what\u2019s acceptable progress:\nAlg. 7.1: POWERPLAY Framework (Variant II) Explicitly Handling Costs of Solving Tasks\nInitialize s0 in some way for i := 1, 2, . . . do\nCreate new global variables Ti \u2208 T , si \u2208 S, pi \u2208 P , ci, c\u2217i \u2208 R (to be fixed by the end of repeat) repeat\nLet a search algorithm (Section 4.1) set pi, a new candidate program. Give pi limited time to do: * TASK INVENTION: Unless the user specifies Ti (Section 6), let pi set Ti. * SOLVER MODIFICATION: Let pi set si by computing a modification of si\u22121 (Section 3.2). * CORRECTNESS DEMONSTRATION: Let pi compute ci := Cost(si, T\u2264i) and c\u2217i := Cost(si\u22121, T\u2264i)\nuntil c\u2217i \u2212 ci > \u01eb (minimal savings of costs such as time/space/etc on all tasks so far) Freeze/store forever pi, Ti, si, ci, c\u2217i\nend for\nBy Algorithm 7.1, si may forget certain abilities of si\u22121, provided that the overall performance as measured by Cost(si, T\u2264i) has improved, either because a new task became solvable, or previous tasks became solvable more efficiently.\nFollowing Section 3.3, CORRECTNESS DEMONSTRATION can often be facilitated, for example, by tracking which components of si are used for solving which tasks (Section 3.3.2).\nTo further refine this approach, consider that in phase i, the list Lki (defined in Section 3.3.2) contains all previously learned tasks whose solutions depend on sk. This can be used to determine the current value V al(ski ) of some component s k of s: V al(ski ) = \u2212 \u2211\nT\u2208Lk i Cost(si, T\u2264i). It is a simple exercise to invent POWERPLAY variants that do not forget valuable components as easily as less valuable ones.\nThe implementations of Sections 4.1 and 4.3 are easily adapted to the cost-based POWERPLAY framework.\n7.2 Probabilistic POWERPLAY Variants\nSection 3.1.2 pointed out that in partially observable and/or non-stationary unknown environments CORRECTNESS DEMONSTRATION must use Tracek to check whether a new si still knows how to solve an earlier task Tk(k < i). A less strict variant of POWERPLAY, however, will simply make certain assumptions about the probabilistic nature of the environment and the repeatability of trials, assuming that a limited fixed number of interactions with the real world are sufficient to estimate the costs c\u2217i , ci in Algorithm 7.1.\nAnother probabilistic way of softening POWERPLAY is to add new tasks without proof that s won\u2019t forget solutions to previous tasks, provided CORRECTNESS DEMONSTRATION can at least show that the probability of forgetting any previous solution is below some real-valued positive constant threshold."}, {"heading": "8 First Illustrative Experiments", "text": "Simple instances of POWERPLAY were implemented by Rupesh Srivastava (a separate report on this is in preparation). A first experimental setup was limited to pattern classification (Section 3.1.1): s is the weight\nmatrix of a fixed-size feedforward NN [2] which maps 2-dimensional real-valued input vectors from the unit square [0, 1) \u00d7 [0, 1) to binary labels 0 or 1, depending on whether the real-valued activation of the NN\u2019s single output neuron exceeds 0.5 or not (neurons are numbered; wkl denotes the modifiable weight of the connection between neurons k and l).\nBinary programs p \u2208 P compute tasks and modify s as follows. The k-th bit of p is denoted pk. If p1 is 1, no further bits can occur in p, and the current task is to simplify or compress s by weight decay such that it can still correctly label all previously learned input patterns, but with a smaller weight vector length measured by \u2211 k,l(wkl)\n2 (whose old value should exceed the new one by at least some given \u01eb). Penalizing large weights assumes smaller weights are simpler. The simplicity measure derives from a Gaussian prior with zero mean on all weights (the costs of encoding a weight with a certain precision given by some interval size is proportional to the logarithm of its probability [39, 10, 2]).\nIf p1 is 0, however, then the target/label of the current task candidate T is given by the next bit p2, and T \u2019s 2-dimensional input vector is uniquely encoded by the subsequent bit string p3p4 . . . pn according to a prewired coding scheme.\nTo run p for t steps (on a training set of i patterns so far) means to execute \u230at/i\u230b iterations (or epochs) of gradient descent [2] on the training set and to check whether all patterns are correctly classified (CORRECTNESS DEMONSTRATION). Processing a single pattern in the continually growing self-invented training set always costs one step, no matter whether the current task is to simplify the existing solver by decreasing \u2211 k,l(wkl)\n2 on the repertoire so far, or to add a new pattern classification to the repertoire (without forgetting previous ones).\nThere also is a novel, computational resources-based active learning [5] variant of this setup, where the system does not invent labels/targets/classifications by itself, but relies on a teacher-given pre-determined label for each input vector it proposes, essentially querying the teacher through input patterns (that is, the bit p2 above is not needed).\nThe weights of s0 are randomly initialized. Assume POWERPLAY has already learned a version of s called sold able to classify i \u2265 0 previously invented training patterns. Then the next task is defined by the following simple enumerative search (in the style of Universal Search [13]) which combines task simplification in the sense of Algorithm 7.1 and systematic run time growth in the style of Algorithm 4.1 (albeit without learning to re-use code from previous tasks):\nAlgorithm 8: for m := 1, 2, . . . do\nRun all programs p with L(p) \u2264 m for at most 2m\u2212L(p) steps as described above; exit as soon as the first p creates from sold a new version of s called snew correctly classifying all i training patterns so far, where snew either is substantially simpler than sold (that is, \u2211 k,l(wkl)\n2 shrank by at least \u01eb), or can also classify a newly found pattern misclassified by sold.\nend for\nSince NN-compressing programs starting (and ending) with \u20181\u2019 are extremely short, they get a large share of the total search time: roughly half of the time is spent on simplification, the rest on creation of new training pattern associations beyond the NN\u2019s previous generalization ability. After each successful search for a new task the labels of grid points are plotted in a rather dense grid on the unit square, to see how the NN maps [0, 1) \u00d7 [0, 1) to {0, 1}, thus monitoring the evolution of its generalization map. As expected, the experiments showed that in the beginning POWERPLAY prefers to invent and learn simple linearly separable functions. At some point, however, there is a phase transition to more complex non-linear functions, indicating a new developmental stage [19, 30, 17].\nOther illustrative experiments involve an RNN (Section 4.1.2) steering a physically rather realistic simulation of an iCub humanoid robot, where self-invented tasks require mapping self-invented inputs to action sequences leading to more and more complex self-invented goal positions (separate report in preparation). As mentioned in Section 9.1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines."}, {"heading": "9 Previous Relevant Work", "text": "Here I discuss (a) why this work is of interest despite the recent advent of theoretically optimal universal problem solvers (Section 9.1), and (b) how it can be viewed as a greedy but feasible and sound implementation of the formal theory of creativity (Section 9.2)."}, {"heading": "9.1 Existing Theoretically Optimal Universal Problem Solvers", "text": "The new millennium brought universal problem solvers that are theoretically optimal in a certain sense. The fully self-referential [7] Go\u0308del machine [34, 35] may interact with some initially unknown, partially observable environment to maximize future expected utility or reward by solving arbitrary user-defined computational tasks. Its initial algorithm is not hardwired; it can completely rewrite itself without essential limits apart from the limits of computability, but only if a proof searcher embedded within the initial algorithm can first prove that the rewrite is useful, according to the formalized utility function taking into account the limited computational resources. Self-rewrites due to this approach can be shown to be globally optimal, relative to Go\u0308del\u2019s well-known fundamental restrictions of provability [7]. To make sure the Go\u0308del machine is at least asymptotically optimal even before the first self-rewrite, one may initialize it by Hutter\u2019s non-self-referential but asymptotically fastest algorithm for all well-defined problems Hsearch [11], which uses a hardwired brute force proof searcher and ignores the costs of proof search. Assuming discrete input/output domains X/Y \u2282 B\u2217, a formal problem specification f : X \u2192 Y (say, a functional description of how integers are decomposed into their prime factors), and a particular x \u2208 X (say, an integer to be factorized), Hsearch orders all proofs of an appropriate axiomatic system by size to find programs q that for all z \u2208 X provably compute f(z) within time bound tq(z). Simultaneously it spends most of its time on executing the q with the best currently proven time bound tq(x). Hsearch is as fast as the fastest algorithm that provably computes f(z) for all z \u2208 X , save for a constant factor smaller than 1+\u01eb (arbitrarily small real-valued \u01eb > 0) and an f -specific but x-independent additive constant [11]. Given some problem, the Go\u0308del machine may decide to replace Hsearch by a faster method suffering less from large constant overhead, but even if it doesn\u2019t, its performance won\u2019t be less than asymptotically optimal.\nWhy doesn\u2019t everybody use such universal problem solvers for all computational real-world problems? Because most real-world problems are so small that the ominous constant slowdowns (potentially relevant at least before the first self-rewrite) may be large enough to prevent the universal methods from being feasible.\nPOWERPLAY, on the other hand, is designed to incrementally build a practical more and more general problem solver that can solve numerous tasks quickly, not in the asymptotic sense, but by exploiting to the max its given particular search algorithm and computational architecture, with all its space and time limitations, including those reflected by constants ignored by the asymptotic optimality notation.\nAs mentioned in Section 6, however, one must now analyze under which conditions POWERPLAY\u2019s self-generated tasks can accelerate the solution to externally generated tasks (compare previous experimental studies of this type [24, 40, 29, 30])."}, {"heading": "9.2 Greedy Implementation of the Formal Theory of Creativity", "text": "The Formal Theory of Creativity [33, 36] considers agents living in initially unknown environments. At any given time, such an agent uses a reinforcement learning (RL) method [12] to maximize not only expected future external reward for achieving certain goals, but also intrinsic reward for improving an internal model of the environmental responses to its actions, learning to better predict or compress the growing history of observations influenced by its behavior, actively learning skills to influence the input stream such that it contains previously unknown but learnable algorithmic regularities. I have argued that the theory explains essential aspects of intelligence including selective attention, curiosity, creativity, science, art, music, humor, e.g., [33, 36]. Compare recent related work, e.g., [1, 4, 18, 15].\nLike POWERPLAY, such a creative agent produces a sequence of self-generated tasks and their solutions, each task still unsolvable before learning, yet becoming solvable after learning. The costs of learning as well as the learning progress are measured, and enter the reward function. Thus, in the absence of ex-\nternal reward for reaching user-defined goals, at any given time the agent is motivated to invent a series of additional tasks that maximize future expected learning progress.\nFor example, by restricting its input stream to self-generated pairs (I, O) \u2208 I\u00d7O like in Section 3.1.1, and limiting it to predict only O, given I (instead of also trying to predict future (I, O) pairs from previous ones, which the general agent would do), there will be a motivation to actively generate a sequence of (I, O) pairs such that the O are first subjectively unpredictable from their I but then become predictable with little effort, given the limitations of whatever learning algorithm is used. Here cons and pros of POWERPLAY are listed in light of the above. Its drawbacks include:\n1. Instead of maximizing future expected reward, POWERPLAY is greedy, always trying to find the simplest (easiest to find and validate) task to add to the repertoire, or the simplest way of improving the efficiency or compressibility of previous solutions, instead of looking further ahead, as a universal RL method [33, 36] would do. That is, POWERPLAY may potentially sacrifice large long-term gains for small short-term gains: the discovery of many easily solvable tasks may at least temporarily prevent it from learning to solve hard tasks.\nOn general computational architectures such as RNN (Section 4.1.2), however, POWERPLAY is expected to soon run out of easy tasks that are not yet solvable, due to the architecture\u2019s limited capacity and its unavoidable generalization effects (many never-tried tasks will become solvable by solutions to the few explicitly tested Ti). Compare Section 5.\n2. The general creative agent above [33, 36] is motivated to improve performance on the entire history of previous still unsolved tasks, while POWERPLAY will discard much of this history, keeping only a selective list of previously solved tasks. However, as the system is interacting with its environment, one could store the entire continually growing history, and make sure that T always allows for defining the task of better compressing the history so far.\n3. POWERPLAY as in Section 2 has a binary criterion for adding knowledge (was the new task solvable without forgetting old solutions?), while the general agent [33, 36] uses a more informative information-theoretic measure. The cost-based POWERPLAY framework (Alg. 7.1) of Section 7, however, offers similar, more flexible options, rewarding compression or speedup of solutions to previously solved tasks.\nOn the other hand, drawbacks of previous implementations of formal creativity theory include:\n1. Some previous approximative implementations [24, 40] used traditional RL methods [12] with theoretically unlimited look-ahead, but those are not guaranteed to work well in partially observable and/or non-stationary environments where the reward function changes over time, and won\u2019t necessarily generate an optimal sequence of future tasks or experiments. Previous approximative implementations based on traditional RL [24, 40] and on co-evolution of program-like task generators and task solvers [29, 30] also did not have a built-in guarantee that they cannot forget solutions to previously solved tasks, while POWERPLAY as in Section 2 does (and the time and space complexitybased variant Alg. 7.1 of Section 7 can forget only if this improves the average efficiency of previous solutions).\n2. Theoretically optimal implementations [33, 36] are currently still impractical, for reasons similar to those discussed in Section 9.1.\nHence POWERPLAY may be viewed as a greedy but feasible implementation of basic principles of creativity [33, 36]. POWERPLAY-based systems are continually motivated to invent new tasks solvable by formerly unknown procedures, or to compress or speed up problem solving procedures discovered earlier. Unlike previous implementations, POWERPLAY extracts from the lifelong experience history a sequence of clearly identified and separated tasks with explicitly recorded solutions. By design it cannot suffer from online learning problems affecting its solver\u2019s performance on previously solved problems.\nI should state again though that he present paper is purely conceptual. To analyze the novel framework\u2019s consequences in practical settings, experiments are currently being conducted with various problem solver architectures with different generalization properties (compare Section 8)."}, {"heading": "10 Words of Caution", "text": "The behavior of POWERPLAY is determined by the nature and the limitations of S, P , and its algorithm for searching P . If both S and P allow for implementing arbitrary programs, and the search algorithm is a general method for search in program space (Section 4), then there are few limits to what it may do (besides the limits of computability [7]).\nIt may not be advisable to let a general variant of POWERPLAY loose in an uncontrolled situation, e.g., on a multi-computer network on the internet, possibly with access to control of physical devices, and the potential to acquire additional computational and physical resources (Section 3.1.2) through programs executed during POWERPLAY. Unlike, say, traditional virus programs, POWERPLAY-based systems will continually change in a way hard to predict, incessantly inventing and solving novel, self-generated tasks, only driven by a desire to increase their general problem-solving capacity, perhaps a bit like many humans seek to increase their power once their basic needs are satisfied. This type of artificial curiosity/creativity, however, may conflict with human intentions on occasion. On the other hand, unchecked curiosity may sometimes also be harmful or fatal to the learning system itself (Section 6)\u2014curiosity can kill the cat."}, {"heading": "11 Acknowledgments", "text": "Thanks to Mark Ring, Bas Steunebrink, Faustino Gomez, Sohrob Kazerounian, Hung Ngo, Leo Pape, Giuseppe Cuccu, for useful comments. Thanks to Rupesh Srivastava for first implementations and illustrative experiments."}], "references": [{"title": "Intrinsic motivation and reinforcement learning", "author": ["A. Barto"], "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems. Springer,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Intrinsically motivated evolutionary search for vision-based reinforcement learning", "author": ["G. Cuccu", "M. Luciw", "J. Schmidhuber", "F. Gomez"], "venue": "In Proceedings of the 2011 IEEE Conference on Development and Learning and Epigenetic Robotics IEEE-ICDL-EPIROB. IEEE,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Exploration from generalization mediated by multiple controllers", "author": ["P. Dayan"], "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems. Springer,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Theory of optimal experiments", "author": ["V.V. Fedorov"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1972}, {"title": "First-Order Logic and Automated Theorem Proving", "author": ["M.C. Fitting"], "venue": "Graduate Texts in Computer Science. Springer-Verlag, Berlin,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I", "author": ["K. G\u00f6del"], "venue": "Monatshefte fu\u0308r Mathematik und Physik,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1931}, {"title": "Efficient non-linear control through neuroevolution", "author": ["F.J. Gomez", "J. Schmidhuber", "R. Miikkulainen"], "venue": "Journal of Machine Learning Research JMLR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "A method for construction of minimum-redundancy codes", "author": ["D.A. Huffman"], "venue": "Proceedings IRE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1952}, {"title": "The fastest and shortest algorithm for all well-defined problems", "author": ["M. Hutter"], "venue": "International Journal of Foundations of Computer Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Reinforcement learning: a survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of AI research,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Universal sequential search problems", "author": ["L.A. Levin"], "venue": "Problems of Information Transmission,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1973}, {"title": "Artificial curiosity with planning for autonomous perceptual and cognitive development", "author": ["M. Luciw", "V. Graziano", "M. Ring", "J. Schmidhuber"], "venue": "In Proceedings of the First Joint Conference on Development Learning and on Epigenetic Robotics ICDL-EPIROB,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Novelty detection as an intrinsic motivation for cumulative learning robots", "author": ["U. Nehmzow", "Y. Gatsoulis", "E. Kerr", "J. Condell", "N.H. Siddique", "T.M. McGinnity"], "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems. Springer,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "GPS, a program that simulates human thought", "author": ["A. Newell", "H. Simon"], "venue": "Computers and Thought,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1963}, {"title": "Compression progress-based curiosity drive for developmental learning", "author": ["H. Ngo", "M. Ring", "J. Schmidhuber"], "venue": "In Proceedings of the 2011 IEEE Conference on Development and Learning and Epigenetic Robotics IEEE-ICDL-EPIROB. IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Intrinsically motivated learning of real world sensorimotor skills with developmental constraints", "author": ["P.-Y. Oudeyer", "A. Baranes", "F. Kaplan"], "venue": "Intrinsically Motivated Learning in Natural and Artificial Systems. Springer,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "The Child\u2019s Construction of Reality", "author": ["J. Piaget"], "venue": "London: Routledge and Kegan Paul,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1955}, {"title": "Evolutionsstrategie - Optimierung technischer Systeme nach Prinzipien der biologischen Evolution", "author": ["I. Rechenberg"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1971}, {"title": "Continual Learning in Reinforcement Environments", "author": ["M.B. Ring"], "venue": "PhD thesis, University of Texas at Austin, Austin, Texas", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1994}, {"title": "Curiosity-Driven Optimization", "author": ["T. Schaul", "Yi Sun", "D. Wierstra", "F. Gomez", "J. Schmidhuber"], "venue": "In IEEE Congress on Evolutionary Computation (CEC),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Dynamische neuronale Netze und das fundamentale raumzeitliche Lernproblem", "author": ["J. Schmidhuber"], "venue": "Dissertation, Institut fu\u0308r Informatik, Technische Universita\u0308t Mu\u0308nchen,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "In Proceedings of the International Joint Conference on Neural Networks, Singapore,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1991}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1991}, {"title": "Learning to control fast-weight memories: An alternative to recurrent nets", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1992}, {"title": "On decreasing the ratio between learning complexity and number of time-varying variables in fully recurrent nets", "author": ["J. Schmidhuber"], "venue": "In Proceedings of the International Conference on Artificial Neural Networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1993}, {"title": "A self-referential weight matrix", "author": ["J. Schmidhuber"], "venue": "In Proceedings of the International Conference on Artificial Neural Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1993}, {"title": "Artificial curiosity based on discovering novel algorithmic predictability through coevolution", "author": ["J. Schmidhuber"], "venue": "Congress on Evolutionary Computation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Exploring the predictable", "author": ["J. Schmidhuber"], "venue": "Advances in Evolutionary Computing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "OOPS source code in crystalline format: http://www.idsia.ch/ \u0303juergen/oopscode.c", "author": ["J. Schmidhuber"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Optimal ordered problem solver", "author": ["J. Schmidhuber"], "venue": "Machine Learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2004}, {"title": "Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts", "author": ["J. Schmidhuber"], "venue": "Connection Science,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "G\u00f6del machines: Fully self-referential optimal universal self-improvers", "author": ["J. Schmidhuber"], "venue": "Artificial General Intelligence,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Ultimate cognition \u00e0 la G\u00f6del", "author": ["J. Schmidhuber"], "venue": "Cognitive Computation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990-2010)", "author": ["J. Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement", "author": ["J. Schmidhuber", "J. Zhao", "M. Wiering"], "venue": "Machine Learning,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1997}, {"title": "Parameter-exploring policy gradients", "author": ["F. Sehnke", "C. Osendorfer", "T. R\u00fcckstie\u00df", "A. Graves", "J. Peters", "J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "A mathematical theory of communication (parts I and II)", "author": ["C.E. Shannon"], "venue": "Bell System Technical Journal,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1948}, {"title": "Reinforcement driven information acquisition in nondeterministic environments", "author": ["J. Storck", "S. Hochreiter", "J. Schmidhuber"], "venue": "In Proceedings of the International Conference on Artificial Neural Networks, Paris,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1995}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1998}, {"title": "On computable numbers, with an application to the Entscheidungsproblem", "author": ["A.M. Turing"], "venue": "Proceedings of the London Mathematical Society, Series", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1936}, {"title": "Natural evolution strategies", "author": ["D. Wierstra", "T. Schaul", "J. Peters", "J. Schmidhuber"], "venue": "In Congress of Evolutionary Computation", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Planning to be surprised: Optimal Bayesian exploration in dynamic environments", "author": ["S. Yi", "F. Gomez", "J. Schmidhuber"], "venue": "In Proc. Fourth Conference on Artificial General Intelligence (AGI),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}], "referenceMentions": [{"referenceID": 32, "context": "greedy but practical implementation of basic principles of creativity [33, 36].", "startOffset": 70, "endOffset": 78}, {"referenceID": 35, "context": "greedy but practical implementation of basic principles of creativity [33, 36].", "startOffset": 70, "endOffset": 78}, {"referenceID": 12, "context": "Given a realistic piece of computational hardware with specific resource limitations, how can one devise software for it that will solve all, or at least many, of the a priori unknown tasks that are in principle easily solvable on this architecture? In other words, how to build a practical general problem solver, given the computational restrictions? It does not need to be universal and asymptotically optimal [13, 11, 32, 35] like the recent (not necessarily practically feasible) general problem solvers discussed in Section 9.", "startOffset": 413, "endOffset": 429}, {"referenceID": 10, "context": "Given a realistic piece of computational hardware with specific resource limitations, how can one devise software for it that will solve all, or at least many, of the a priori unknown tasks that are in principle easily solvable on this architecture? In other words, how to build a practical general problem solver, given the computational restrictions? It does not need to be universal and asymptotically optimal [13, 11, 32, 35] like the recent (not necessarily practically feasible) general problem solvers discussed in Section 9.", "startOffset": 413, "endOffset": 429}, {"referenceID": 31, "context": "Given a realistic piece of computational hardware with specific resource limitations, how can one devise software for it that will solve all, or at least many, of the a priori unknown tasks that are in principle easily solvable on this architecture? In other words, how to build a practical general problem solver, given the computational restrictions? It does not need to be universal and asymptotically optimal [13, 11, 32, 35] like the recent (not necessarily practically feasible) general problem solvers discussed in Section 9.", "startOffset": 413, "endOffset": 429}, {"referenceID": 34, "context": "Given a realistic piece of computational hardware with specific resource limitations, how can one devise software for it that will solve all, or at least many, of the a priori unknown tasks that are in principle easily solvable on this architecture? In other words, how to build a practical general problem solver, given the computational restrictions? It does not need to be universal and asymptotically optimal [13, 11, 32, 35] like the recent (not necessarily practically feasible) general problem solvers discussed in Section 9.", "startOffset": 413, "endOffset": 429}, {"referenceID": 24, "context": "(See [25, 24, 29, 33, 36, 44] and Section 9.", "startOffset": 5, "endOffset": 29}, {"referenceID": 23, "context": "(See [25, 24, 29, 33, 36, 44] and Section 9.", "startOffset": 5, "endOffset": 29}, {"referenceID": 28, "context": "(See [25, 24, 29, 33, 36, 44] and Section 9.", "startOffset": 5, "endOffset": 29}, {"referenceID": 32, "context": "(See [25, 24, 29, 33, 36, 44] and Section 9.", "startOffset": 5, "endOffset": 29}, {"referenceID": 35, "context": "(See [25, 24, 29, 33, 36, 44] and Section 9.", "startOffset": 5, "endOffset": 29}, {"referenceID": 43, "context": "(See [25, 24, 29, 33, 36, 44] and Section 9.", "startOffset": 5, "endOffset": 29}, {"referenceID": 20, "context": "The framework is called POWERPLAY because it continually [21] aims at boosting computational prowess and problem solving capacity, reminiscent of humans or human societies trying to boost their general power/capabilities/knowledge/skills in playful ways, even in the absence of externally defined goals, although the skills learned by this type of pure curiosity may later help to solve externally posed tasks.", "startOffset": 57, "endOffset": 61}, {"referenceID": 23, "context": "Unlike our first implementations of curious/creative/playful agents from the 1990s [24, 40, 29] (Section 9.", "startOffset": 83, "endOffset": 95}, {"referenceID": 39, "context": "Unlike our first implementations of curious/creative/playful agents from the 1990s [24, 40, 29] (Section 9.", "startOffset": 83, "endOffset": 95}, {"referenceID": 28, "context": "Unlike our first implementations of curious/creative/playful agents from the 1990s [24, 40, 29] (Section 9.", "startOffset": 83, "endOffset": 95}, {"referenceID": 0, "context": "2; compare [1, 4, 18, 15]), POWERPLAY provably (by design) does not have any problems with online learning\u2014it cannot forget previously learned skills, automatically segmenting its life into a sequence of clearly identified tasks with explicitly recorded solutions.", "startOffset": 11, "endOffset": 25}, {"referenceID": 3, "context": "2; compare [1, 4, 18, 15]), POWERPLAY provably (by design) does not have any problems with online learning\u2014it cannot forget previously learned skills, automatically segmenting its life into a sequence of clearly identified tasks with explicitly recorded solutions.", "startOffset": 11, "endOffset": 25}, {"referenceID": 17, "context": "2; compare [1, 4, 18, 15]), POWERPLAY provably (by design) does not have any problems with online learning\u2014it cannot forget previously learned skills, automatically segmenting its life into a sequence of clearly identified tasks with explicitly recorded solutions.", "startOffset": 11, "endOffset": 25}, {"referenceID": 14, "context": "2; compare [1, 4, 18, 15]), POWERPLAY provably (by design) does not have any problems with online learning\u2014it cannot forget previously learned skills, automatically segmenting its life into a sequence of clearly identified tasks with explicitly recorded solutions.", "startOffset": 11, "endOffset": 25}, {"referenceID": 32, "context": "Unlike the task search of theoretically optimal creative agents [33, 36] (Section 9.", "startOffset": 64, "endOffset": 72}, {"referenceID": 35, "context": "Unlike the task search of theoretically optimal creative agents [33, 36] (Section 9.", "startOffset": 64, "endOffset": 72}, {"referenceID": 23, "context": "The present paper formalizes this in a way that may be more convenient to implement than those of previous work [24, 29, 33, 36], and describes a simple practical framework for building creative artificial scientists or explorers that by design continually come up with the fastest to find, initially novel, but eventually solvable problems.", "startOffset": 112, "endOffset": 128}, {"referenceID": 28, "context": "The present paper formalizes this in a way that may be more convenient to implement than those of previous work [24, 29, 33, 36], and describes a simple practical framework for building creative artificial scientists or explorers that by design continually come up with the fastest to find, initially novel, but eventually solvable problems.", "startOffset": 112, "endOffset": 128}, {"referenceID": 32, "context": "The present paper formalizes this in a way that may be more convenient to implement than those of previous work [24, 29, 33, 36], and describes a simple practical framework for building creative artificial scientists or explorers that by design continually come up with the fastest to find, initially novel, but eventually solvable problems.", "startOffset": 112, "endOffset": 128}, {"referenceID": 35, "context": "The present paper formalizes this in a way that may be more convenient to implement than those of previous work [24, 29, 33, 36], and describes a simple practical framework for building creative artificial scientists or explorers that by design continually come up with the fastest to find, initially novel, but eventually solvable problems.", "startOffset": 112, "endOffset": 128}, {"referenceID": 12, "context": "1, 8) orders candidate pairs of the type (task, solver) by computational complexity, using concepts of optimal universal search [13, 32], with a bias towards pairs that can be described by few additional bits of information (given the experience so far) and that can be validated quickly.", "startOffset": 128, "endOffset": 136}, {"referenceID": 31, "context": "1, 8) orders candidate pairs of the type (task, solver) by computational complexity, using concepts of optimal universal search [13, 32], with a bias towards pairs that can be described by few additional bits of information (given the experience so far) and that can be validated quickly.", "startOffset": 128, "endOffset": 136}, {"referenceID": 1, "context": "The computational architecture of the problem solver may be a deterministic universal computer, or a more limited device such as a finite state automaton or a feedforward neural network (NN) [2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 6, "context": "All such problem solvers can be uniquely encoded [7] or implemented on universal computers such as universal Turing Machines (TM) [42].", "startOffset": 49, "endOffset": 52}, {"referenceID": 41, "context": "All such problem solvers can be uniquely encoded [7] or implemented on universal computers such as universal Turing Machines (TM) [42].", "startOffset": 130, "endOffset": 134}, {"referenceID": 1, "context": "Note that ni and ti may be unnecessary in special cases such as the problem solver being a fixed topology feedforward NN [2] whose input and target patterns have constant size and whose computational efforts per pattern need constant time and space resources.", "startOffset": 121, "endOffset": 124}, {"referenceID": 15, "context": "In the more general context of general problem solving/sequential decision making/reinforcement learning/reward optimization [16, 12, 41] in unknown environments, there may be a set I \u2282 B of possible task identification patterns and a set J \u2282 B of programs that test properties of bitstrings.", "startOffset": 125, "endOffset": 137}, {"referenceID": 11, "context": "In the more general context of general problem solving/sequential decision making/reinforcement learning/reward optimization [16, 12, 41] in unknown environments, there may be a set I \u2282 B of possible task identification patterns and a set J \u2282 B of programs that test properties of bitstrings.", "startOffset": 125, "endOffset": 137}, {"referenceID": 40, "context": "In the more general context of general problem solving/sequential decision making/reinforcement learning/reward optimization [16, 12, 41] in unknown environments, there may be a set I \u2282 B of possible task identification patterns and a set J \u2282 B of programs that test properties of bitstrings.", "startOffset": 125, "endOffset": 137}, {"referenceID": 8, "context": "For example, ui(t) may encode the current contents of the internal tape of a TM, or of certain addresses in the dynamic storage area of a PC, or the present activations of an LSTM recurrent NN [9].", "startOffset": 193, "endOffset": 196}, {"referenceID": 34, "context": "This could be done like in the G\u00f6del Machine [35] (Section 9.", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "1), which uses an online extension of Universal Search [13] to systematically test proof techniques: proof-generating programs that may invoke special instructions for generating axioms and applying inference rules to prolong an initially empty proof \u2208 B by theorems, which are either axioms or inferred from previous theorems through rules such as modus ponens combined with unification, e.", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": ", [6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 34, "context": "P can be easily limited to programs generating only syntactically correct proofs [35].", "startOffset": 81, "endOffset": 85}, {"referenceID": 31, "context": ") By restricting S to self-delimiting prefix codes like those generated by the Optimal Ordered Problem Solver (OOPS) [32], one can now profit from a sometimes particularly efficient type of CORRECTNESS DEMONSTRATION, ensuring that differences between si and si\u22121 cannot affect solutions to T<i under certain conditions.", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "More precisely, to obtain si, half the search time is spent on trying to process Ti first by si\u22121, extending or prolonging si\u22121 only when the ongoing computation requests to add new components through special instructions [32]\u2014then CORRECTNESS DEMONSTRATION has less to do as the set T<i is guaranteed to remain solvable, by induction.", "startOffset": 222, "endOffset": 226}, {"referenceID": 31, "context": "The other half of the time is spent on processing Ti by a new sub-program with new components s\u2032i, a part of si but not of si\u22121, where s\u2032i may read si\u22121 or invoke parts of si\u22121 as sub-programs to solve T\u2264i \u2014 only then CORRECTNESS DEMONSTRATION has to test si not only on Ti but also on T<i (see [32] for details).", "startOffset": 295, "endOffset": 299}, {"referenceID": 31, "context": "However, in this simple setup there is no immediate generalization across tasks like in OOPS [32] and the previous paragraph: the trivial task identifier i will always first invoke some s\u2032i different from all s\u2032k(k 6= i), instead of allowing for solving a new task solely by previously found code.", "startOffset": 93, "endOffset": 97}, {"referenceID": 31, "context": "This yields a perfectly ordered problem sequence for a variant of the Optimal Ordered Problem Solver OOPS [32] described next.", "startOffset": 106, "endOffset": 110}, {"referenceID": 31, "context": ", k\u2212 1) and for copying the read code into modifiable storage U , where pk may further edit the code, and execute the result, which may be a useful subprogram [32].", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "Define a probability distributionP (p) on P to represent the searcher\u2019s initial bias (more likely programs p will be tested earlier [13]).", "startOffset": 132, "endOffset": 136}, {"referenceID": 31, "context": ",P (p) = 2, or on a probabilistic syntax diagram [32, 31].", "startOffset": 49, "endOffset": 57}, {"referenceID": 30, "context": ",P (p) = 2, or on a probabilistic syntax diagram [32, 31].", "startOffset": 49, "endOffset": 57}, {"referenceID": 31, "context": "1: Implementing POWERPLAY with Procedure OOPS [32]", "startOffset": 46, "endOffset": 50}, {"referenceID": 31, "context": "This does not cost more time than executing p in the while loop above [32].", "startOffset": 70, "endOffset": 74}, {"referenceID": 12, "context": "OOPS allocates time to programs according to an asymptotically optimal universal search method [13] for problems with easily verifiable solutions, that is, solutions whose validity can be quickly tested.", "startOffset": 95, "endOffset": 99}, {"referenceID": 31, "context": "Since OOPS may re-use previously generated solutions and solution-computing programs, however, it may be possible to greatly reduce the constant factor associated with plain universal search [32].", "startOffset": 191, "endOffset": 195}, {"referenceID": 30, "context": "Existing OOPS source code [31] uses a FORTH-like universal programming language to define P .", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "Given enough additive and multiplicative neurons and an appropriate weight matrix, RNN1 can compute any function computable by a standard PC [23].", "startOffset": 141, "endOffset": 145}, {"referenceID": 25, "context": "P may itself be the set of weight matrices of a separate RNN called RNN2, computing tasks for RNN1, and modifications of RNN1, using techniques for network-modifying networks as described in [26, 28, 27].", "startOffset": 191, "endOffset": 203}, {"referenceID": 27, "context": "P may itself be the set of weight matrices of a separate RNN called RNN2, computing tasks for RNN1, and modifications of RNN1, using techniques for network-modifying networks as described in [26, 28, 27].", "startOffset": 191, "endOffset": 203}, {"referenceID": 26, "context": "P may itself be the set of weight matrices of a separate RNN called RNN2, computing tasks for RNN1, and modifications of RNN1, using techniques for network-modifying networks as described in [26, 28, 27].", "startOffset": 191, "endOffset": 203}, {"referenceID": 36, "context": "Simple ways of doing this are described in previous work [37].", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "do set Boolean variable DONE=FALSE repeat use a black box optimization algorithm BBOA (many are possible [20, 8, 43, 38]) with adaptive parameter vector \u03b8 to create some T \u2208 T (to define the task input to RNN1; see Section 3.", "startOffset": 105, "endOffset": 120}, {"referenceID": 7, "context": "do set Boolean variable DONE=FALSE repeat use a black box optimization algorithm BBOA (many are possible [20, 8, 43, 38]) with adaptive parameter vector \u03b8 to create some T \u2208 T (to define the task input to RNN1; see Section 3.", "startOffset": 105, "endOffset": 120}, {"referenceID": 42, "context": "do set Boolean variable DONE=FALSE repeat use a black box optimization algorithm BBOA (many are possible [20, 8, 43, 38]) with adaptive parameter vector \u03b8 to create some T \u2208 T (to define the task input to RNN1; see Section 3.", "startOffset": 105, "endOffset": 120}, {"referenceID": 37, "context": "do set Boolean variable DONE=FALSE repeat use a black box optimization algorithm BBOA (many are possible [20, 8, 43, 38]) with adaptive parameter vector \u03b8 to create some T \u2208 T (to define the task input to RNN1; see Section 3.", "startOffset": 105, "endOffset": 120}, {"referenceID": 42, "context": ", by gradient-based search [43, 38], or according to the principles of evolutionary computation [20, 8, 43].", "startOffset": 27, "endOffset": 35}, {"referenceID": 37, "context": ", by gradient-based search [43, 38], or according to the principles of evolutionary computation [20, 8, 43].", "startOffset": 27, "endOffset": 35}, {"referenceID": 19, "context": ", by gradient-based search [43, 38], or according to the principles of evolutionary computation [20, 8, 43].", "startOffset": 96, "endOffset": 107}, {"referenceID": 7, "context": ", by gradient-based search [43, 38], or according to the principles of evolutionary computation [20, 8, 43].", "startOffset": 96, "endOffset": 107}, {"referenceID": 42, "context": ", by gradient-based search [43, 38], or according to the principles of evolutionary computation [20, 8, 43].", "startOffset": 96, "endOffset": 107}, {"referenceID": 23, "context": "One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein.", "startOffset": 174, "endOffset": 205}, {"referenceID": 39, "context": "One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein.", "startOffset": 174, "endOffset": 205}, {"referenceID": 28, "context": "One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein.", "startOffset": 174, "endOffset": 205}, {"referenceID": 29, "context": "One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein.", "startOffset": 174, "endOffset": 205}, {"referenceID": 13, "context": "One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein.", "startOffset": 174, "endOffset": 205}, {"referenceID": 2, "context": "One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein.", "startOffset": 174, "endOffset": 205}, {"referenceID": 21, "context": "One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein.", "startOffset": 174, "endOffset": 205}, {"referenceID": 43, "context": "One should analyze theoretically and experimentally under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies in this vein.", "startOffset": 174, "endOffset": 205}, {"referenceID": 12, "context": ", in the style of asymptotically optimal Universal Search [13], which essentially trades one bit of additional space complexity for a runtime speedup factor of 2.", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "matrix of a fixed-size feedforward NN [2] which maps 2-dimensional real-valued input vectors from the unit square [0, 1) \u00d7 [0, 1) to binary labels 0 or 1, depending on whether the real-valued activation of the NN\u2019s single output neuron exceeds 0.", "startOffset": 38, "endOffset": 41}, {"referenceID": 38, "context": "The simplicity measure derives from a Gaussian prior with zero mean on all weights (the costs of encoding a weight with a certain precision given by some interval size is proportional to the logarithm of its probability [39, 10, 2]).", "startOffset": 220, "endOffset": 231}, {"referenceID": 9, "context": "The simplicity measure derives from a Gaussian prior with zero mean on all weights (the costs of encoding a weight with a certain precision given by some interval size is proportional to the logarithm of its probability [39, 10, 2]).", "startOffset": 220, "endOffset": 231}, {"referenceID": 1, "context": "The simplicity measure derives from a Gaussian prior with zero mean on all weights (the costs of encoding a weight with a certain precision given by some interval size is proportional to the logarithm of its probability [39, 10, 2]).", "startOffset": 220, "endOffset": 231}, {"referenceID": 1, "context": "To run p for t steps (on a training set of i patterns so far) means to execute \u230at/i\u230b iterations (or epochs) of gradient descent [2] on the training set and to check whether all patterns are correctly classified (CORRECTNESS DEMONSTRATION).", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "There also is a novel, computational resources-based active learning [5] variant of this setup, where the system does not invent labels/targets/classifications by itself, but relies on a teacher-given pre-determined label for each input vector it proposes, essentially querying the teacher through input patterns (that is, the bit p above is not needed).", "startOffset": 69, "endOffset": 72}, {"referenceID": 12, "context": "Then the next task is defined by the following simple enumerative search (in the style of Universal Search [13]) which combines task simplification in the sense of Algorithm 7.", "startOffset": 107, "endOffset": 111}, {"referenceID": 18, "context": "At some point, however, there is a phase transition to more complex non-linear functions, indicating a new developmental stage [19, 30, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 29, "context": "At some point, however, there is a phase transition to more complex non-linear functions, indicating a new developmental stage [19, 30, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 16, "context": "At some point, however, there is a phase transition to more complex non-linear functions, indicating a new developmental stage [19, 30, 17].", "startOffset": 127, "endOffset": 139}, {"referenceID": 23, "context": "1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines.", "startOffset": 154, "endOffset": 185}, {"referenceID": 39, "context": "1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines.", "startOffset": 154, "endOffset": 185}, {"referenceID": 28, "context": "1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines.", "startOffset": 154, "endOffset": 185}, {"referenceID": 29, "context": "1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines.", "startOffset": 154, "endOffset": 185}, {"referenceID": 13, "context": "1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines.", "startOffset": 154, "endOffset": 185}, {"referenceID": 2, "context": "1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines.", "startOffset": 154, "endOffset": 185}, {"referenceID": 21, "context": "1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines.", "startOffset": 154, "endOffset": 185}, {"referenceID": 43, "context": "1, it is also intended to study under which conditions the creation of self-generated tasks can accelerate the solution to externally generated tasks\u2014see [24, 40, 29, 30, 14, 3, 22, 44] for previous simple experimental studies along these lines.", "startOffset": 154, "endOffset": 185}, {"referenceID": 6, "context": "The fully self-referential [7] G\u00f6del machine [34, 35] may interact with some initially unknown, partially observable environment to maximize future expected utility or reward by solving arbitrary user-defined computational tasks.", "startOffset": 27, "endOffset": 30}, {"referenceID": 33, "context": "The fully self-referential [7] G\u00f6del machine [34, 35] may interact with some initially unknown, partially observable environment to maximize future expected utility or reward by solving arbitrary user-defined computational tasks.", "startOffset": 45, "endOffset": 53}, {"referenceID": 34, "context": "The fully self-referential [7] G\u00f6del machine [34, 35] may interact with some initially unknown, partially observable environment to maximize future expected utility or reward by solving arbitrary user-defined computational tasks.", "startOffset": 45, "endOffset": 53}, {"referenceID": 6, "context": "Self-rewrites due to this approach can be shown to be globally optimal, relative to G\u00f6del\u2019s well-known fundamental restrictions of provability [7].", "startOffset": 143, "endOffset": 146}, {"referenceID": 10, "context": "To make sure the G\u00f6del machine is at least asymptotically optimal even before the first self-rewrite, one may initialize it by Hutter\u2019s non-self-referential but asymptotically fastest algorithm for all well-defined problems Hsearch [11], which uses a hardwired brute force proof searcher and ignores the costs of proof search.", "startOffset": 232, "endOffset": 236}, {"referenceID": 10, "context": "Hsearch is as fast as the fastest algorithm that provably computes f(z) for all z \u2208 X , save for a constant factor smaller than 1+\u01eb (arbitrarily small real-valued \u01eb > 0) and an f -specific but x-independent additive constant [11].", "startOffset": 225, "endOffset": 229}, {"referenceID": 23, "context": "As mentioned in Section 6, however, one must now analyze under which conditions POWERPLAY\u2019s self-generated tasks can accelerate the solution to externally generated tasks (compare previous experimental studies of this type [24, 40, 29, 30]).", "startOffset": 223, "endOffset": 239}, {"referenceID": 39, "context": "As mentioned in Section 6, however, one must now analyze under which conditions POWERPLAY\u2019s self-generated tasks can accelerate the solution to externally generated tasks (compare previous experimental studies of this type [24, 40, 29, 30]).", "startOffset": 223, "endOffset": 239}, {"referenceID": 28, "context": "As mentioned in Section 6, however, one must now analyze under which conditions POWERPLAY\u2019s self-generated tasks can accelerate the solution to externally generated tasks (compare previous experimental studies of this type [24, 40, 29, 30]).", "startOffset": 223, "endOffset": 239}, {"referenceID": 29, "context": "As mentioned in Section 6, however, one must now analyze under which conditions POWERPLAY\u2019s self-generated tasks can accelerate the solution to externally generated tasks (compare previous experimental studies of this type [24, 40, 29, 30]).", "startOffset": 223, "endOffset": 239}, {"referenceID": 32, "context": "The Formal Theory of Creativity [33, 36] considers agents living in initially unknown environments.", "startOffset": 32, "endOffset": 40}, {"referenceID": 35, "context": "The Formal Theory of Creativity [33, 36] considers agents living in initially unknown environments.", "startOffset": 32, "endOffset": 40}, {"referenceID": 11, "context": "At any given time, such an agent uses a reinforcement learning (RL) method [12] to maximize not only expected future external reward for achieving certain goals, but also intrinsic reward for improving an internal model of the environmental responses to its actions, learning to better predict or compress the growing history of observations influenced by its behavior, actively learning skills to influence the input stream such that it contains previously unknown but learnable algorithmic regularities.", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": ", [33, 36].", "startOffset": 2, "endOffset": 10}, {"referenceID": 35, "context": ", [33, 36].", "startOffset": 2, "endOffset": 10}, {"referenceID": 0, "context": ", [1, 4, 18, 15].", "startOffset": 2, "endOffset": 16}, {"referenceID": 3, "context": ", [1, 4, 18, 15].", "startOffset": 2, "endOffset": 16}, {"referenceID": 17, "context": ", [1, 4, 18, 15].", "startOffset": 2, "endOffset": 16}, {"referenceID": 14, "context": ", [1, 4, 18, 15].", "startOffset": 2, "endOffset": 16}, {"referenceID": 32, "context": "Instead of maximizing future expected reward, POWERPLAY is greedy, always trying to find the simplest (easiest to find and validate) task to add to the repertoire, or the simplest way of improving the efficiency or compressibility of previous solutions, instead of looking further ahead, as a universal RL method [33, 36] would do.", "startOffset": 313, "endOffset": 321}, {"referenceID": 35, "context": "Instead of maximizing future expected reward, POWERPLAY is greedy, always trying to find the simplest (easiest to find and validate) task to add to the repertoire, or the simplest way of improving the efficiency or compressibility of previous solutions, instead of looking further ahead, as a universal RL method [33, 36] would do.", "startOffset": 313, "endOffset": 321}, {"referenceID": 32, "context": "The general creative agent above [33, 36] is motivated to improve performance on the entire history of previous still unsolved tasks, while POWERPLAY will discard much of this history, keeping only a selective list of previously solved tasks.", "startOffset": 33, "endOffset": 41}, {"referenceID": 35, "context": "The general creative agent above [33, 36] is motivated to improve performance on the entire history of previous still unsolved tasks, while POWERPLAY will discard much of this history, keeping only a selective list of previously solved tasks.", "startOffset": 33, "endOffset": 41}, {"referenceID": 32, "context": "POWERPLAY as in Section 2 has a binary criterion for adding knowledge (was the new task solvable without forgetting old solutions?), while the general agent [33, 36] uses a more informative information-theoretic measure.", "startOffset": 157, "endOffset": 165}, {"referenceID": 35, "context": "POWERPLAY as in Section 2 has a binary criterion for adding knowledge (was the new task solvable without forgetting old solutions?), while the general agent [33, 36] uses a more informative information-theoretic measure.", "startOffset": 157, "endOffset": 165}, {"referenceID": 23, "context": "Some previous approximative implementations [24, 40] used traditional RL methods [12] with theoretically unlimited look-ahead, but those are not guaranteed to work well in partially observable and/or non-stationary environments where the reward function changes over time, and won\u2019t necessarily generate an optimal sequence of future tasks or experiments.", "startOffset": 44, "endOffset": 52}, {"referenceID": 39, "context": "Some previous approximative implementations [24, 40] used traditional RL methods [12] with theoretically unlimited look-ahead, but those are not guaranteed to work well in partially observable and/or non-stationary environments where the reward function changes over time, and won\u2019t necessarily generate an optimal sequence of future tasks or experiments.", "startOffset": 44, "endOffset": 52}, {"referenceID": 11, "context": "Some previous approximative implementations [24, 40] used traditional RL methods [12] with theoretically unlimited look-ahead, but those are not guaranteed to work well in partially observable and/or non-stationary environments where the reward function changes over time, and won\u2019t necessarily generate an optimal sequence of future tasks or experiments.", "startOffset": 81, "endOffset": 85}, {"referenceID": 23, "context": "Previous approximative implementations based on traditional RL [24, 40] and on co-evolution of program-like task generators and task solvers [29, 30] also did not have a built-in guarantee that they cannot forget solutions to previously solved tasks, while POWERPLAY as in Section 2 does (and the time and space complexitybased variant Alg.", "startOffset": 63, "endOffset": 71}, {"referenceID": 39, "context": "Previous approximative implementations based on traditional RL [24, 40] and on co-evolution of program-like task generators and task solvers [29, 30] also did not have a built-in guarantee that they cannot forget solutions to previously solved tasks, while POWERPLAY as in Section 2 does (and the time and space complexitybased variant Alg.", "startOffset": 63, "endOffset": 71}, {"referenceID": 28, "context": "Previous approximative implementations based on traditional RL [24, 40] and on co-evolution of program-like task generators and task solvers [29, 30] also did not have a built-in guarantee that they cannot forget solutions to previously solved tasks, while POWERPLAY as in Section 2 does (and the time and space complexitybased variant Alg.", "startOffset": 141, "endOffset": 149}, {"referenceID": 29, "context": "Previous approximative implementations based on traditional RL [24, 40] and on co-evolution of program-like task generators and task solvers [29, 30] also did not have a built-in guarantee that they cannot forget solutions to previously solved tasks, while POWERPLAY as in Section 2 does (and the time and space complexitybased variant Alg.", "startOffset": 141, "endOffset": 149}, {"referenceID": 32, "context": "Theoretically optimal implementations [33, 36] are currently still impractical, for reasons similar to those discussed in Section 9.", "startOffset": 38, "endOffset": 46}, {"referenceID": 35, "context": "Theoretically optimal implementations [33, 36] are currently still impractical, for reasons similar to those discussed in Section 9.", "startOffset": 38, "endOffset": 46}, {"referenceID": 32, "context": "Hence POWERPLAY may be viewed as a greedy but feasible implementation of basic principles of creativity [33, 36].", "startOffset": 104, "endOffset": 112}, {"referenceID": 35, "context": "Hence POWERPLAY may be viewed as a greedy but feasible implementation of basic principles of creativity [33, 36].", "startOffset": 104, "endOffset": 112}, {"referenceID": 6, "context": "If both S and P allow for implementing arbitrary programs, and the search algorithm is a general method for search in program space (Section 4), then there are few limits to what it may do (besides the limits of computability [7]).", "startOffset": 226, "endOffset": 229}], "year": 2017, "abstractText": "Most of computer science focuses on automatically solving given computational problems. I focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans, to train a more and more general problem solver from scratch in an unsupervised fashion. At any given time, the novel algorithmic framework POWERPLAY searches the space of possible pairs of new tasks and modifications of the current problem solver, until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one, while the unmodified predecessor does not. The new task and its corresponding task-solving skill are those first found and validated. Newly invented tasks may require making previously learned skills more efficient. The greedy search of typical POWERPLAY variants orders candidate pairs of tasks and solver modifications by their conditional computational complexity, given the stored experience so far. This biases the search towards pairs that can be described compactly and validated quickly. Standard problem solver architectures of personal computers or neural networks tend to generalize by solving numerous tasks outside the self-invented training set; POWERPLAY\u2019s ongoing search for novelty keeps fighting to extend beyond the generalization abilities of its present solver. The continually increasing repertoire of problem solving procedures can be exploited by a parallel search for solutions to additional externally posed tasks. POWERPLAY may be viewed as a greedy but practical implementation of basic principles of creativity [33, 36]. The present paper is purely conceptual though; detailed experimental analysis of various problem solver architectures with different generalization properties is left to separate papers.", "creator": "LaTeX with hyperref package"}}}