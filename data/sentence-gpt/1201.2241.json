{"id": "1201.2241", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2012", "title": "Distance-Based Bias in Model-Directed Optimization of Additively Decomposable Problems", "abstract": "For many optimization problems it is possible to define a distance metric between problem variables that correlates with the likelihood and strength of interactions between the variables. For example, one may define a metric so that the dependencies between variables that are closer to each other with respect to the metric are expected to be stronger than the dependencies between variables that are further apart. The purpose of this paper is to describe a method that combines such a problem-specific distance metric with information mined from probabilistic models obtained in previous runs of estimation of distribution algorithms with the goal of solving future problem instances of similar type with increased speed, accuracy and reliability. While the focus of the paper is on additively decomposable problems and the hierarchical Bayesian optimization algorithm, it should be straightforward to generalize the approach to other model-directed optimization techniques and other problem classes. Compared to other techniques for learning from experience put forward in the past, the proposed technique is both more practical and more broadly applicable.", "histories": [["v1", "Wed, 11 Jan 2012 04:59:57 GMT  (110kb)", "http://arxiv.org/abs/1201.2241v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["martin pelikan", "mark w hauschild"], "accepted": false, "id": "1201.2241"}, "pdf": {"name": "1201.2241.pdf", "metadata": {"source": "CRF", "title": "Distance-Based Bias in Model-Directed Optimization of Additively Decomposable Problems", "authors": ["Martin Pelikan", "Mark W. Hauschild"], "emails": ["medal@cs.umsl.edu", "pelikan@cs.umsl.edu", "mwh308@umsl.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 1.\n22 41\nv1 [\ncs .N\nFor many optimization problems it is possible to define a distance metric between problem variables that correlates with the likelihood and strength of interactions between the variables. For example, one may define a metric so that the dependencies between variables that are closer to each other with respect to the metric are expected to be stronger than the dependencies between variables that are further apart. The purpose of this paper is to describe a method that combines such a problem-specific distance metric with information mined from probabilistic models obtained in previous runs of estimation of distribution algorithms with the goal of solving future problem instances of similar type with increased speed, accuracy and reliability. While the focus of the paper is on additively decomposable problems and the hierarchical Bayesian optimization algorithm, it should be straightforward to generalize the approach to other model-directed optimization techniques and other problem classes. Compared to other techniques for learning from experience put forward in the past, the proposed technique is both more practical and more broadly applicable.\nKeywords\nEstimation of distribution algorithms, hierarchical Bayesian optimization algorithm, decomposable problems, inductive transfer, learning from experience, efficiency enhancement.\nMissouri Estimation of Distribution Algorithms Laboratory (MEDAL) Department of Mathematics and Computer Science University of Missouri\u2013St. Louis One University Blvd., St. Louis, MO 63121 E-mail: medal@cs.umsl.edu WWW: http://medal.cs.umsl.edu/\nDistance-Based Bias in Model-Directed Optimization of Additively\nDecomposable Problems\nMartin Pelikan\nMissouri Estimation of Distribution Algorithms Laboratory (MEDAL) Dept. of Mathematics and Computer Science, 320 ESH\nUniversity of Missouri in St. Louis One University Blvd., St. Louis, MO 63121\npelikan@cs.umsl.edu\nMark W. Hauschild\nMissouri Estimation of Distribution Algorithms Laboratory (MEDAL) Dept. of Mathematics and Computer Science, 321 ESH\nUniversity of Missouri in St. Louis One University Blvd., St. Louis, MO 63121\nmwh308@umsl.edu\nJanuary 12, 2012\nAbstract\nFor many optimization problems it is possible to define a distance metric between problem variables that correlates with the likelihood and strength of interactions between the variables. For example, one may define a metric so that the dependencies between variables that are closer to each other with respect to the metric are expected to be stronger than the dependencies between variables that are further apart. The purpose of this paper is to describe a method that combines such a problem-specific distance metric with information mined from probabilistic models obtained in previous runs of estimation of distribution algorithms with the goal of solving future problem instances of similar type with increased speed, accuracy and reliability. While the focus of the paper is on additively decomposable problems and the hierarchical Bayesian optimization algorithm, it should be straightforward to generalize the approach to other modeldirected optimization techniques and other problem classes. Compared to other techniques for learning from experience put forward in the past, the proposed technique is both more practical and more broadly applicable."}, {"heading": "1 Introduction", "text": "Even for optimization problems that are extremely difficult to solve, it may be straightforward to extract information about important dependencies between variables and other problem regularities directly from the problem definition (Baluja, 2006; Drezner & Salhi, 2002; Hauschild & Pelikan, 2010; Stonedahl, Rand, & Wilensky, 2008; Schwarz & Ocenasek, 2000). Furthermore, when solving many problem instances of similar type, it may be possible to gather information about variable interactions and other problem features by examining previous runs of the optimization algorithm, and to use this information to bias optimization of future problem instances to increase its speed, accuracy and reliability (Hauschild & Pelikan, 2008;\nHauschild, Pelikan, Sastry, & Goldberg, 2011). The use of information from previous runs to introduce bias into future runs of an evolutionary algorithm is often referred to as learning from experience (Hauschild & Pelikan, 2008; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Pelikan, 2002). The use of bias based on the results of other learning tasks in the same problem domain is also commonplace in machine learning where it is referred to as inductive transfer or transfer learning (Pratt, Mostow, Kamm, & Kamm, 1991; Caruana, 1997). Numerous studies have shown that using prior knowledge and learning from experience promise improved efficiency and problem solving capabilities (Baluja, 2006; Drezner & Salhi, 2002; Hauschild & Pelikan, 2010; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Rothlauf, 2006; Stonedahl, Rand, & Wilensky, 2008; Schwarz & Ocenasek, 2000). However, most prior work in this area was based on hand-crafted search operators, model restrictions, or representations.\nThis paper describes an approach that combines prior problem-specific knowledge with learning from experience. The basic idea of the proposed approach comprises of (1) defining a problemspecific distance metric, (2) analyzing previous EDA models to quantify the likelihood and nature of dependencies at various distances, and (3) introducing bias into EDA model building based on the results of the analysis using Bayesian statistics. One of the key goals of this paper is to develop an automated procedure capable of introducing bias based on a distance metric and prior EDA runs, without requiring much expert knowledge or hand-crafted model restrictions from the practitioner. Furthermore, the proposed approach is intended to be applicable in a more practical manner than other approaches to learning from experience. For example, the proposed approach makes it feasible to use prior runs on problems of a smaller size to introduce bias when solving problem instances of a larger size, and the bias can be introduced even when the importance of dependencies between specific pairs of variables varies significantly across the problem domain. Although this paper focuses on the hierarchical Bayesian optimization algorithm (hBOA) and additively decomposable functions (ADFs), the proposed approach can also be applied to other model-directed optimization techniques and other problem types. The paper outlines a framework that can be used to adapt the proposed approach to a different context.\nThe paper is organized as follows. Section 2 describes hBOA. Section 3 outlines the framework for introducing bias based on prior runs on similar problems in model-directed optimization. Section 4 presents the proposed approach to introducing bias into hBOA model building for additively decomposable problems. Section 5 presents experimental results. Section 6 summarizes and concludes the paper."}, {"heading": "2 Hierarchical BOA", "text": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larran\u0303aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011). hBOA works with a population of candidate solutions represented by fixed-length strings over a finite alphabet. In this paper, candidate solutions are represented by n-bit binary strings. The initial population of binary strings is generated at random according to the uniform distribution over candidate solutions. Each iteration starts by selecting promising solutions from the current population; here binary tournament selection without replacement is used. Next, hBOA (1) learns a Bayesian network with local structures for the selected solutions and (2) generates new candidate solutions by sampling the distribution encoded by the built network (Chickering, Heckerman, & Meek, 1997; Friedman & Goldszmidt, 1999). To\nmaintain useful diversity in the population, the new candidate solutions are incorporated into the original population using restricted tournament selection (RTS) (Harik, 1995). The run is terminated when termination criteria are met. In this paper, each run is terminated either when the global optimum is found or when a maximum number of iterations is reached. Since the basic understanding of probabilistic models used in hBOA is necessary for the remainder of the paper, the rest of this section discusses the class of probabilistic models used in hBOA.\nhBOA represents probabilistic models of candidate solutions by Bayesian networks with local structures (Chickering, Heckerman, & Meek, 1997; Friedman & Goldszmidt, 1999). A Bayesian network is defined by two components: (1) an acyclic directed graph over problem variables specifying direct dependencies between variables and (2) conditional probabilities specifying the probability distribution of each variable given the values of the variable\u2019s parents. A Bayesian network encodes a joint probability distribution as p(X1, . . . ,Xn) = \u220fn i=1 p(Xi|\u03a0i) where Xi is the ith variable and \u03a0i are the parents of Xi in the underlying graph.\nTo represent conditional probabilities of each variable given the variable\u2019s parents, hBOA uses decision trees. Each internal node of a decision tree specifies a variable, and the subtrees of the node correspond to the different values of the variable. Each leaf of the decision tree for a particular variable defines the probability distribution of the variable given a condition specified by the constraints given by the path from the root of the tree to this leaf (constraints are given by the assignments of the variables along this path).\nTo build probabilistic models, hBOA typically uses a greedy algorithm that initializes the decision tree for each problem variable Xi to a single-node tree that encodes the unconditional probability distribution of Xi. In each iteration, the model building algorithm tests how much a model would improve after splitting each leaf of each decision tree on each variable that is not already located on the path to the leaf. The algorithm executes the split that provides the most improvement, and the process is repeated until no more improvement is possible.\nImprovement of the model after a split is often evaluated using the Bayesian-Dirichlet (BDe) metric with penalty for model complexity. Bayesian measures evaluate the goodness of a Bayesian network structure given data D and background knowledge \u03be as (Cooper & Herskovits, 1992; Heckerman, Geiger, & Chickering, 1994)\np(B|D, \u03be) = cp(B|\u03be)p(D|B, \u03be), (1)\nwhere c is a normalization constant. For the Bayesian-Dirichlet metric, the term p(D|B, \u03be) is estimated as (Chickering, Heckerman, & Meek, 1997)\np(D|B, \u03be) =\nn \u220f\ni=1\n\u220f\nl\u2208Li\n\u0393(m\u2032i(l))\n\u0393(mi(l) +m\u2032i(l))\n\u220f\nxi\n\u0393(mi(xi, l) +m \u2032 i(xi, l))\n\u0393(m\u2032i(xi, l)) ,\n(2)\nwhere Li is the set of leaves in the decision tree Ti for Xi; mi(l) is the number of instances in the selected population which end up the traversal through the tree Ti in the leaf l; mi(xi, l) is the number of instances that have Xi = xi and end up the traversal of the tree Ti in the leaf l; m\u2032i(l) represents the prior knowledge about the value of mi(i, l); and m \u2032 i(xi, l) represents the prior knowledge about the value of mi(xi, l). Without any prior knowledge, an uninformative prior m\u2032i(xi, l) = 1 is typically used. To favor simpler networks to the more complex ones, the prior probability of each network decreases exponentially fast with respect to the description length of\nthis network\u2019s parameters (Friedman & Goldszmidt, 1999; Pelikan, 2005):\np(B) = c2\u22120.5( \u2211 i |Li|) log2 N , (3)\nwhere c is a normalization constant required for the prior probabilities of all network structures to sum to one."}, {"heading": "3 Bias Based on Previous EDA Runs", "text": "Building an accurate probabilistic model in hBOA and other EDAs based on complex probabilistic models can be time consuming and it may require rather large populations of solutions. That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations of candidate solutions (Baluja, 2006; Hauschild & Pelikan, 2008; Hauschild & Pelikan, 2009; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Mu\u0308hlenbein & Mahnig, 2002). Learning from experience (Hauschild & Pelikan, 2008; Hauschild & Pelikan, 2009; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Pelikan, 2005) represents one approach to dealing with this issue. In learning from experience, models discovered by EDAs in previous runs are mined to identify regularities and the discovered regularities are used to bias model building in future runs on problems of similar type. Since learning model structure is often the most challenging task in model building, learning from experience often focuses on identifying regularities in model structure and using these regularities to bias structural learning in future runs.\nIt is straightforward to collect statistics on the most frequent dependencies in EDA models. Nonetheless, for the collected statistics to be useful, it is important to ensure that the statistics are meaningful with respect to the problem being solved. For example, consider optimization of NK landscapes (Kauffman, 1989), in which the fitness function is defined as the sum of n subfunctions {fi} n i=1, and the subfunction fi is applied to the ith bit and its k neighbors. The neighbors of each bit are typically chosen at random for each problem instance. Therefore, if we consider 1,000 problem instances of NK landscapes, looking at the percentage of models that included a dependency between the first two bits for the first 999 instances will not say much about the likelihood of the same dependency for the last instance. A similar observation can be made for many other important problem classes, such as MAXSAT or the quadratic assignment problem. That is why it is important to develop a more general framework that allows one to learn and use statistics on dependencies in EDA models across a range of problem domains of different structure and properties. In the remainder of this section we describe one such framework.\nTo formalize the proposed framework to identifying structural regularities in EDA models, let us define a set of m dependency categories D = {D1, . . . ,Dm} and denote the background knowledge about the problem by \u03be. Then, we can define a function \u03b3(i, j, \u03be) that, given \u03be, maps any dependency (i, j) covered by the probabilistic model into one of the m categories so that \u03b3(i, j, \u03be) = k if and only if (i, j) \u2208 Dk. Two straightforward possibilities for defining \u03b3 function were proposed by Hauschild et al. (2008, 2009): (1) Each pair of problem variables Xi and Xj defines a special category, and (2) categories are defined using a discretization of a problem-specific distance metric between variables. The first approach is useful especially when solving a number of instances of a problem where each variable has a fixed meaning across the entire set of instances; this is the case for example in spin glasses defined on a regular lattice, where every pair of variables in the lattice can be assigned a special category because the structure of the problem does not change from one instance to another (Hauschild, Pelikan, Sastry, & Lima, 2009). The latter approach is useful especially when one can define a distance metric on variables so that the distance between\ntwo variables correlates strongly with the likelihood or strength of their interaction; for example, one may define a distance metric such that variables that interact more strongly are closer to each other according to the metric. Such a distance metric can be defined for example in the quadratic assignment problem, traveling salesman problem or, more generally, classes of additively decomposable functions. While these two approaches are applicable to many important classes of problems, one may envision many other approaches based on this framework. The key issue in defining \u03b3 is that the categories should be related to the problem, so that each category contains pairs of variables that have a lot in common and that can be expected to be either correlated or independent most of the time.\nThe statistics obtained from previous EDA models can be used to bias the search operators of model-directed optimization methods using either a soft bias or a hard bias. A soft bias allows one to define preference to some models using a prior distribution over network structures or partial variable assignments (Schwarz & Ocenasek, 2000; Hauschild & Pelikan, 2009). A hard bias encodes hard restrictions on model structure or variable assignments, restricting the class of allowable models (Mu\u0308hlenbein & Mahnig, 2002; Baluja, 2006; Hauschild & Pelikan, 2008). While in most prior work on bias in EDAs the bias was based on expert knowledge, in learning from experience the focus is on automated learning of a bias from past EDA runs.\nIn this paper we describe one way of using the above framework to facilitate learning from experience in hBOA for additively decomposable problems based on a problem-specific distance metric. However, note that the framework can be applied to other EDAs based on graphical models."}, {"heading": "4 Distance-Based Bias", "text": ""}, {"heading": "4.1 Additively Decomposable Functions", "text": "For many optimization problems, the objective function (fitness function) can be expressed in the form of an additively decomposable function (ADF) of m subproblems:\nf(X1, . . . ,Xn) = m \u2211\ni=1\nfi(Si), (4)\nwhere (X1, . . . ,Xn) are problem\u2019s decision variables, fi is the ith subfunction, and Si \u2282 {X1,X2, . . . ,Xn} is the subset of variables contributing to fi. While they may often exist multiple ways of decomposing the problem using additive decomposition, one would typically prefer decompositions that minimize the sizes of subsets {Si}. It is of note that the difficulty of ADFs is not fully determined by the order of subproblems, but also by the definition of the subproblems and their interaction. In fact, there exist a number of NP-complete problems that can be formulated as ADFs with subproblems of order 2 or 3, such as MAXSAT for 3CNF formulas. On the other hand, one may easily define ADFs with subproblems of order n that can be solved by a simple bit-flip hill climbing in low-order polynomial time."}, {"heading": "4.2 Measuring Variable Distances for ADFs", "text": "The definition of a distance between two variables of an ADF used in this paper follows Hauschild and Pelikan (2008) and Hauschild et al. (2011). Given an additively decomposable problem, we define the distance between two variables using a graph G of n nodes, one node per variable. For any two variables Xi and Xj in the same subset Sk, that is, Xi,Xj \u2208 Sk, we create an\nedge in G between the nodes Xi and Xj. Denoting by li,j the number of edges along the shortest path between Xi and Xj in G (in terms of the number of edges), we define the distance between two variables as\nD(Xi,Xj) =\n{\nli,j if a path between Xi and Xj exists n otherwise\nThe above distance measure makes variables in the same subproblem close to each other, whereas for the remaining variables, the distances correspond to the length of the chain of subproblems that relate the two variables. The distance is maximal for variables that are completely independent (the value of a variable does not influence the contribution of the other variable in any way).\nSince interactions between problem variables are encoded mainly in the subproblems of the additive problem decomposition, the above distance metric should typically correspond closely to the likelihood of dependencies between problem variables in probabilistic models discovered by EDAs. Specifically, the variables located closer with respect to the metric should more likely interact with each other. Figure 1 illustrates this on two ADFs discussed later in this paper\u2014the NK landscape with nearest neighbor interactions and the two-dimensional Ising spin glass (for a description of these problems, see section 5.1). The figure analyzes probabilistic models discovered by hBOA in 10 independent runs on each of the 1,000 random instances for each problem and problem size. For a range of distances d between problem variables, the figure shows the proportion of splits on a variable located at distance d. The results clearly support the fact that hBOA models indicate strongest dependencies between variables located close to each other according to the aforementioned metric and that there is a clear correlation between the distance metric and the likelihood of dependencies. Furthermore, the figure indicates that the likelihood of dependencies at a specific distance does not change much from one problem size to another, indicating that the bias based on these statistics should be applicable across a range of problem sizes.\nIt is important to note that other approaches may be envisioned to defining a distance metric for ADFs. For example, a weight may be added on each edge that would decrease with the number of subsets that contain the two connected variables. Another interesting possibility would be to consider the subfunctions themselves in measuring the distances, so that only correlations that lead to nonlinearities are considered or that some correlations are given a priority over others. Finally, the distance of variables may depend on the problem definition itself, not on the decomposition\nonly. For example, in the quadratic assignment problem, a distance between two facility locations is specified directly by the problem instance. The key is to use problem-specific information to specify a distance metric so that the distance between a pair of variables correlates with the likelihood or strength of their interaction."}, {"heading": "4.3 Using Distance-Based Bias in hBOA", "text": "The basic idea of incorporating the distance-based bias based on prior runs into hBOA is inspired mainly by the work of Hauschild et al. (Hauschild & Pelikan, 2009). Hauschild et al. proposed to incorporate learning from experience into hBOA by modifying prior probabilities of network structures using the statistics that capture the number of splits on each variable in the decision tree for each other variable in past hBOA runs on similar problems. Nonetheless, the approach of Hauschild et al. (Hauschild & Pelikan, 2009) is only applicable to problems where the strength of interactions between any two variables is not expected to change much from instance to instance. That is why this approach can only be applied in a limited set of problem domains and it is difficult to use this approach when problem size is not fixed in all runs. In this paper, we propose to capture the nature of dependencies between variables with respect to their distance using the distance metric defined for ADFs or another distance metric. This allows one to apply the technique in more problem domains and also allows models from problem instances of one size to be useful in solving problem instances of different sizes (see figure 1).\nRecall that the BDe metric used to evaluate probabilistic models in hBOA contains two parts: (1) prior probability p(B|\u03be) of the network structure B, and (2) posterior probability p(D|B, \u03be) of the data (population of selected solutions) given B. Prior probabilities of network structures are typically set to represent the uniform distribution over admissible network structures or to provide a bias toward simple models regardless of the problem. However, the prior probability distribution of network structures can also be used to specify preferable structures. In this paper, we will use the prior probability distribution of network structures to introduce bias toward models that resemble models obtained in previous runs on problems of similar type with the focus on distancebased statistics. An analogous approach can be used to incorporate bias into hBOA for a different mapping \u03b3 of pairs of variables into dependency categories {Di}.\nLet us assume a set M of hBOA models from prior hBOA runs on similar ADFs. Before applying the bias by modifying the prior probability distribution of models in hBOA, the models in M are first processed to generate data that will serve as the basis for introducing the bias. The processing starts by analyzing the models in M to determine the number s(m,d, j) of splits on any variable Xi such that D(Xi,Xj) = d in a decision tree Tj for variable Xj for a model m \u2208 M . Then, the values s(m,d, j) are used to compute the probability Pk(d, j) of a kth split on a variable at distance d from Xj in a dependency tree Tj given that k\u2212 1 such splits were already performed in Tj :\nPk(d, j) = |{m \u2208 M : s(m,d, j) \u2265 k}|\n|{m \u2208 M : s(m,d, j) \u2265 k \u2212 1}| \u00b7 (5)\nGiven the terms Pk(d, j), we can now define the prior probability of a network B as\np(B, \u03be) = c\nn \u220f\nd=1\nn \u220f\nj=1\nns(d,j) \u220f\nk=1\nP \u03bak (d, j), (6)\nwhere ns(d, j) denotes the number of splits on any variable Xi such that D(Xi,Xj) = d in Tj , \u03ba > 0 is used to tune the strength of bias (the strength of bias increases with \u03ba), and c is a normalization\nconstant. Since log-likelihood is typically used to evaluate model quality, to evaluate the contribution of any particular split, the main difference from the standard version of the BDe metric with the complexity penalty is that instead of reducing the metric according to the additional complexity of log2(N)/2 for each new split, we reduce the metric by the corresponding \u03ba log2 Pk(d, j). Therefore, the computation of the change of the prior probability of the network structure can still be done in constant time. Of course, the change in p(D|B, \u03be) requires computation of marginal frequencies, so it cannot be done in constant time.\nIt is important to note that the prior probability of hBOA models defined in eq. (6) is certainly not the only possible approach to incorporating learning from experience using distance-based statistics into hBOA. The main source of inspiration for the proposed approach is the work on incorporating bias in learning Bayesian networks using Bayesian metrics (Heckerman, Geiger, & Chickering, 1994) and the prior work on learning from experience in hBOA by Hauschild et al. (Hauschild & Pelikan, 2009). The experimental results presented in the next section confirm that this approach leads to substantial speedups in both problem classes considered in this paper and preliminary experiments in other problem domains including MAXSAT and minimum vertex cover indicate that substantial speedups can be obtained also in other problem classes defined as ADFs."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Test Problems", "text": "To test the proposed approach to biasing hBOA model building, we consider two problem classes: shuffled nearest-neighbor NK landscapes and two-dimensional spin glasses. Both these problem classes were shown to be challenging for conventional genetic algorithms and many other optimization techniques due to the rugged landscape, strong epistasis, and complex structure of interactions between problem variables (Kauffman, 1989; Young, 1998; Pelikan, 2010; Pelikan & Hartmann, 2006). However, for both problem classes, it is straightforward to generate a large number of problem instances with known optima. For each problem class and problem size, we use 1,000 unique problem instances; the reason for using such a large number of instances is that for these problem classes, algorithm performance often varies substantially from one instance to another and the results would thus be unreliable if only a few instances were used.\nAn NK fitness landscape (Kauffman, 1989) is fully defined by the following components: (1) The number of bits, n, (2) the number of neighbors per bit, k, (3) a set of k neighbors \u03a0(Xi) of the ith bit for every i \u2208 {1, . . . , n}, and (4) a subfunction fi defining a real value for each combination of values of Xi and \u03a0(Xi) for every i \u2208 {1, . . . , n}. Typically, each subfunction is defined as a lookup table. The objective function fnk to maximize is defined as fnk(X1,X2, . . . ,Xn) = \u2211n i=1 fi(Xi,\u03a0(Xi)). The difficulty of optimizing NK landscapes depends on all components defining an NK problem instance. In this paper, we consider nearest-neighbor NK landscapes, in which neighbors of each bit are restricted to the k bits that immediately follow this bit. The neighborhoods wrap around; thus, for bits which do not have k bits to the right, the neighborhood is completed with the first few bits of solution strings. The reason for restricting neighborhoods to nearest neighbors was to ensure that the problem instances can be solved in polynomial time even for k > 1 using dynamic programming (Pelikan, 2010). The subfunctions are represented by look-up tables (a unique value is used for each instance of a bit and its neighbors), and each entry in the look-up table is generated with the uniform distribution from [0, 1). The used class of NK landscapes with nearest neighbors is thus the same as that in Pelikan (2010). In all experiments, we use k = 5 and n \u2208 {100, 150, 200}.\nFor each n, we use 1,000 unique, independently generated instances; overall, 3,000 unique instances of NK landscapes were tested.\nIsing spin glasses are prototypical models for disordered systems (Young, 1998). A simple model to describe a finite-dimensional Ising spin glass is typically arranged on a regular 2D or 3D grid where each node i corresponds to a spin si and each edge \u3008i, j\u3009 corresponds to a coupling between two spins si and sj . Each edge has a real value Ji,j associated with it that defines the relationship between the two connected spins. To approximate the behavior of the large-scale system, periodic boundary conditions are often used that introduce a coupling between the first and the last elements in each dimension. For the classical Ising model, each spin si can be in one of two states: si = +1 or si = \u22121. Given a set of coupling constants Ji,j , and a configuration of spins C, the energy can be computed as E(C) = \u2212 \u2211\n\u3008i,j\u3009 siJi,jsj, where the sum runs over all couplings \u3008i, j\u3009. Here the task is to find a spin configuration for a given set of coupling constants that minimizes the energy of the spin glass. The states with minimum energy are called ground states. The spin configurations are encoded with binary strings where each bit specifies the value of one spin (0 for a spin +1, 1 for a spin -1). One generally analyzes a large set of random spin glass instances for a given distribution of the spin-spin couplings. In this paper we consider the \u00b1J spin glass, where each spin-spin coupling is set randomly to either +1 or \u22121 with equal probability. We use instances arranged on square grids of sizes 10\u00d7 10, 12\u00d7 12, 14\u00d7 14, 16\u00d7 16, 18\u00d7 18 and 20\u00d7 20 spins; that is, the problem sizes range from 100 to 400 spins. We consider periodic boundary conditions. For each problem size, we use 1,000 unique, independently generated problem instances; overall, 6,000 unique instances of the 2D spin glass were tested. All instances were obtained from the Spin Glass Ground State Server (Spin Glass Ground State Server, 2004)."}, {"heading": "5.2 10-Fold Crossvalidation", "text": "To ensure that the same problem instances were not used for defining the bias as well as for testing it, 10-fold crossvalidation was used. For each problem size and each problem, 1,000 random problem instances were used in the experiments. The 1,000 instances in each set were randomly split into 10 equally sized subsets of 100 instances each. In each round of crossvalidation, 1 subset of 100 instances was left out and hBOA was run on the remaining 9 subsets of 900 instances total. The runs on the 9 subsets produced a number of models that were analyzed in order to obtain the probabilities Pk(d, j) for all d, j, and k. The bias based on the obtained values of Pk(d, j) was then used in hBOA runs on the remaining subset that was left out. The same procedure was repeated for each subset; overall, 10 rounds of crossvalidation were performed for each set of 1,000 instances. Each problem instance was used exactly once in the test of the proposed approach to biasing hBOA models and in every test, models used to generate statistics for hBOA bias were obtained from hBOA runs on different problem instances.\nWhile the experiments were performed across a variety of computer architectures and configurations, it was always ensured that the base case with no bias and the case with bias were both run on the same computational node and the results of the two runs can therefore be compared against each other with respect to the actual CPU time."}, {"heading": "5.3 Experimental Setup", "text": "The maximum number of iterations for each problem instance was set to the overall number of bits in the problem; according to preliminary experiments, this upper bound substantially exceeded the actual number of iterations required to solve each problem. Each run was terminated either when the global optimum was found, when the population consisted of copies of a single candidate\nsolution, or when the maximum number of iterations was reached. For each problem instance, we used bisection (Sastry, 2001; Pelikan, 2005) to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.\nBit-flip hill climbing (HC) is incorporated into hBOA to improve its performance. HC takes a candidate solution represented by an n-bit binary string on input. Then, it performs one-bit changes on the solution that lead to the maximum improvement of solution quality. HC is terminated when no single-bit flip improves solution quality and the solution is thus locally optimal. Here, HC is used to improve every solution in the population before the evaluation is performed. Without HC, the number and size of problem instances would have to substantially reduce due to the increased computational requirements. However, preliminary results indicate that even without HC the benefits of the proposed approach would be substantial.\nThe proposed approach to distance-based bias in hBOA is tested for \u03ba = {1, 2, 3, 4, 5, 6, 7} to assess how the strength of the bias represented by \u03ba affects hBOA performance.\nTo evaluate hBOA performance, we focus on (1) the execution time per run, (2) the number of steps of HC, (3) the number of evaluations, and (4) the required population size. The steps of HC are not counted as evaluations in order to distinguish between evaluations and HC steps, because for many additively decomposable problems, performing a HC step is much less computationally expensive than evaluating a solution. To evaluate the benefits of distance-based bias, the paper uses multiplicative speedups, where the speedup is defined as a multiplicative factor by which a particular complexity measure improves by using the distance-based bias compared to the base case with no distance-based bias. For example, an execution-time speedup of 2 indicates that the bias allowed hBOA to find the optimum twice as fast as without the bias. Although the code could be further optimized for efficiency, the primary focus of our experiments concerning the execution times was on the speedups of the CPU times rather than their absolute values. We have used the most efficient implementation of hBOA available for the base case with no bias and we only modified it for the remaining cases to incorporate the bias."}, {"heading": "5.4 Results", "text": "Fig. 2(a) shows the effects of \u03ba on the multiplicative speedups with respect to the execution time, the number of evaluations, the number of HC steps, and the population size. The results confirm that, for adequate values of \u03ba, the speedups in terms of execution time are substantial for both NK landscapes as well as 2D spin glasses; the maximum speedup for NK landscapes was over 2.26 whereas for spin glasses it was over 1.66. For NK landscapes, the speedups in terms of execution time grow both with problem size n and with \u03ba, and they can be expected to increase further for even larger values of n or \u03ba. For spin glasses, the speedups seem nearly independent of problem size and best speedups are obtained for \u03ba = 3. The multiplicative speedups in terms of the number of evaluations, the number of HC steps and the population size indicate that the reduction in the population sizes appears to be one of the most important factors reducing the overall computational cost, although improvements can be observed also in most other statistics.\nFig. 2(b) shows the speedups obtained with respect to the problem size for a range of values of \u03ba; these results are useful for visualizing the relationship between the problem size and the speedups obtained with the distance-based bias. The results confirm that for NK landscapes, the speedups appear to grow at least linearly with the problem size, regardless of the value of \u03ba. However, for the 2D spin glass, the speedups fluctuate around the same value for all problem sizes. The speedups obtained on NK landscapes are thus not only better, but they further improve with problem size, unlike for the 2D spin glass. On one hand, one may argue that this is due to the fact that NK\nlandscapes with nearest neighbor interactions have a simpler structure than the 2D spin glass due to the short-range interactions. On the other hand, the interactions in 2D spin glasses are of much smaller order (subproblems in problem decomposition have 2 bits each instead of 6). We are currently evaluating the distance-based bias on other classes of nearly decomposable problems such as MAXSAT and the minimum vertex cover in order to provide more empirical evidence that would help explain when the distance-based bias works better and when it has limitations.\nIn summary, fig. 2 provides solid empirical evidence that the speedups obtained are substantial and that the proposed approach to learning from experience is useful in practice."}, {"heading": "6 Summary and Conclusions", "text": "This paper introduced a practical approach to incorporating bias in estimation of distribution algorithms (EDAs) based on models built in previous EDA runs on problems of similar type. The approach was demonstrated on the hierarchical Bayesian optimization algorithm (hBOA) and additively decomposable functions, although the framework can be applied also to other EDAs based on graphical models and other problem types. For example, it should be straightforward to adapt this framework to the extended compact genetic algorithm (Harik, 1999) or classes of facility location problems. The key idea of the proposed approach was to define a distance metric that corresponds to the likelihood of dependencies between variables, and to use the statistics on dependencies at various distances in previous hBOA runs as the basis for introducing bias in future hBOA runs. The bias was introduced using prior probabilities of Bayesian network structures. The models were thus learned using a combination of the selected population of candidate solutions and the prior knowledge extracted from previous hBOA models. The strength of the bias can be tuned with a user-defined parameter \u03ba > 0. The proposed approach was tested on two challenging additively decomposable functions, the NK landscapes with nearest-neighbor interactions and the two-dimensional Ising spin glass. The results on 9,000 unique problem instances from the two problem classes provided empirical evidence that the proposed approach provides substantial speedups across a variety of settings. Specifically, speedups of over 2.26 were achieved for NK landscapes, and speedups of over 1.66 were achieved for spin glasses. Furthermore, the speedups for NK landscapes grew with problem size and the parameter \u03ba, and it can thus be expected that higher speedups can be achieved in practice. Preliminary experiments on other problem classes, including MAXSAT and minimum vertex cover, indicate that the approach provides substantial benefits also in other important classes of additively decomposable functions.\nThe results thus reaffirm that one of the key advantages of EDAs is that EDAs provide practitioners with a rigorous framework for incorporating prior knowledge and for automated learning from solving instances of similar type so that future problem instances can be solved with increased speed, accuracy, and reliability. EDAs thus not only allow practitioners to scalably solve problems with high levels of epistasis (variable interactions), but they also allow effective inductive transfer (transfer learning) in optimization.\nIn future work, the approach should be tested on other additively decomposable problems. Experiments should also be done to confirm the hypothesis that models obtained on problems of one size can be used to bias model building on problems of another size. Furthermore, the approach should be adapted to other model-directed optimization techniques, including other EDAs and genetic algorithms with linkage learning. The approach should also be modified to introduce bias on problems that cannot be formulated using an additive decomposition in a straightforward manner. Finally, it is important to study the limitations of the proposed approach, and create theoretical models to automatically tune the strength of the bias and predict expected speedups."}, {"heading": "Acknowledgments", "text": "This project was sponsored by the National Science Foundation under grants ECS-0547013 and IIS1115352, and by the University of Missouri in St. Louis through the High Performance Computing Collaboratory sponsored by Information Technology Services. Most experiments were performed on the Beowulf cluster maintained by ITS at the University of Missouri in St. Louis and the HPC resources at the University of Missouri Bioinformatics Consortium. hBOA was developed at the Illinois Genetics Algorithm Laboratory at the University of Illinois at Urbana-Champaign. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."}], "references": [{"title": "Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning (Tech", "author": ["S. Baluja"], "venue": "Rep. No. CMU-CS-94-163). Pittsburgh, PA: Carnegie Mellon University.", "citeRegEx": "Baluja,? 1994", "shortCiteRegEx": "Baluja", "year": 1994}, {"title": "Incorporating a priori knowledge in probabilistic-model based optimization", "author": ["S. Baluja"], "venue": "Cant\u00fa-Paz, E., Pelikan, M., & Sastry, K. (Eds.), Scalable optimization via probabilistic modeling: From algorithms to applications (pp. 205\u2013219). Springer.", "citeRegEx": "Baluja,? 2006", "shortCiteRegEx": "Baluja", "year": 2006}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning , 28 , 41\u201375.", "citeRegEx": "Caruana,? 1997", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "A Bayesian approach to learning Bayesian networks with local structure (Technical Report MSR-TR-97-07)", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": null, "citeRegEx": "Chickering et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chickering et al\\.", "year": 1997}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E.H. Herskovits"], "venue": "Machine Learning ,", "citeRegEx": "Cooper and Herskovits,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Herskovits", "year": 1992}, {"title": "Using hybrid metaheuristics for the one-way and two-way network design problem", "author": ["Z. Drezner", "S. Salhi"], "venue": "Naval Research Logistics,", "citeRegEx": "Drezner and Salhi,? \\Q2002\\E", "shortCiteRegEx": "Drezner and Salhi", "year": 2002}, {"title": "Learning Bayesian networks with local structure", "author": ["N. Friedman", "M. Goldszmidt"], "venue": null, "citeRegEx": "Friedman and Goldszmidt,? \\Q1999\\E", "shortCiteRegEx": "Friedman and Goldszmidt", "year": 1999}, {"title": "Linkage learning via probabilistic modeling in the ECGA (IlliGAL Report No", "author": ["G. Harik"], "venue": "99010). Urbana, IL: University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory.", "citeRegEx": "Harik,? 1999", "shortCiteRegEx": "Harik", "year": 1999}, {"title": "Finding multimodal solutions using restricted tournament selection", "author": ["G.R. Harik"], "venue": "Proc. of the Int. Conf. on Genetic Algorithms (ICGA-95), 24\u201331.", "citeRegEx": "Harik,? 1995", "shortCiteRegEx": "Harik", "year": 1995}, {"title": "Enhancing efficiency of hierarchical BOA via distancebased model restrictions. Parallel Problem Solving from Nature, 417\u2013427", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": null, "citeRegEx": "Hauschild and Pelikan,? \\Q2008\\E", "shortCiteRegEx": "Hauschild and Pelikan", "year": 2008}, {"title": "Intelligent bias of network structures in the hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "Hauschild and Pelikan,? \\Q2009\\E", "shortCiteRegEx": "Hauschild and Pelikan", "year": 2009}, {"title": "Network crossover performance on NK landscapes and deceptive problems", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "Hauschild and Pelikan,? \\Q2010\\E", "shortCiteRegEx": "Hauschild and Pelikan", "year": 2010}, {"title": "An introduction and survey of estimation of distribution algorithms", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Swarm and Evolutionary Computation,", "citeRegEx": "Hauschild and Pelikan,? \\Q2011\\E", "shortCiteRegEx": "Hauschild and Pelikan", "year": 2011}, {"title": "Using previous models to bias structural learning in the hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan", "K. Sastry", "D.E. Goldberg"], "venue": null, "citeRegEx": "Hauschild et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hauschild et al\\.", "year": 2011}, {"title": "Analyzing probabilistic models in hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan", "K. Sastry", "C.F. Lima"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "Hauschild et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hauschild et al\\.", "year": 2009}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data (Technical Report MSR-TR-94-09)", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": null, "citeRegEx": "Heckerman et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1994}, {"title": "Adaptation on rugged fitness landscapes", "author": ["S. Kauffman"], "venue": "Stein, D. L. (Ed.), Lecture Notes in the Sciences of Complexity (pp. 527\u2013618). Addison Wesley.", "citeRegEx": "Kauffman,? 1989", "shortCiteRegEx": "Kauffman", "year": 1989}, {"title": "Estimation of distribution algorithms: A new tool for evolutionary computation", "author": ["P. Larra\u00f1aga", "J.A. Lozano"], "venue": null, "citeRegEx": "Larra\u00f1aga and Lozano,? \\Q2002\\E", "shortCiteRegEx": "Larra\u00f1aga and Lozano", "year": 2002}, {"title": "Towards a new evolutionary computation: Advances on estimation of distribution", "author": ["J.A. Lozano", "P. Larra\u00f1aga", "I. Inza", "E. Bengoetxea"], "venue": null, "citeRegEx": "Lozano et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lozano et al\\.", "year": 2006}, {"title": "Evolutionary optimization and the estimation of search distributions with applications to graph bipartitioning", "author": ["H. M\u00fchlenbein", "T. Mahnig"], "venue": "International Journal of Approximate Reasoning ,", "citeRegEx": "M\u00fchlenbein and Mahnig,? \\Q2002\\E", "shortCiteRegEx": "M\u00fchlenbein and Mahnig", "year": 2002}, {"title": "Bayesian optimization algorithm: From single level to hierarchy", "author": ["M. Pelikan"], "venue": "Doctoral dissertation, University of Illinois at Urbana-Champaign, Urbana, IL.", "citeRegEx": "Pelikan,? 2002", "shortCiteRegEx": "Pelikan", "year": 2002}, {"title": "Hierarchical Bayesian optimization algorithm: Toward a new generation of evolutionary algorithms", "author": ["M. Pelikan"], "venue": "Springer.", "citeRegEx": "Pelikan,? 2005", "shortCiteRegEx": "Pelikan", "year": 2005}, {"title": "NK landscapes, problem difficulty, and hybrid evolutionary algorithms", "author": ["M. Pelikan"], "venue": "Genetic and Evol. Comp. Conf. (GECCO-2010), 665\u2013672.", "citeRegEx": "Pelikan,? 2010", "shortCiteRegEx": "Pelikan", "year": 2010}, {"title": "Escaping hierarchical traps with competent genetic algorithms", "author": ["M. Pelikan", "D.E. Goldberg"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "Pelikan and Goldberg,? \\Q2001\\E", "shortCiteRegEx": "Pelikan and Goldberg", "year": 2001}, {"title": "A hierarchy machine: Learning to optimize from nature and humans", "author": ["M. Pelikan", "D.E. Goldberg"], "venue": "Complexity ,", "citeRegEx": "Pelikan and Goldberg,? \\Q2003\\E", "shortCiteRegEx": "Pelikan and Goldberg", "year": 2003}, {"title": "A survey of optimization by building and using probabilistic models", "author": ["M. Pelikan", "D.E. Goldberg", "F. Lobo"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Pelikan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pelikan et al\\.", "year": 2002}, {"title": "Searching for ground states of Ising spin glasses with hierarchical BOA and cluster exact approximation", "author": ["M. Pelikan", "A.K. Hartmann"], "venue": null, "citeRegEx": "Pelikan and Hartmann,? \\Q2006\\E", "shortCiteRegEx": "Pelikan and Hartmann", "year": 2006}, {"title": "Scalable optimization via probabilistic modeling: From algorithms to applications", "author": ["M. Pelikan", "K. Sastry", "E. Cant\u00fa-Paz"], "venue": null, "citeRegEx": "Pelikan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pelikan et al\\.", "year": 2006}, {"title": "Direct transfer of learned information among neural networks", "author": ["L.Y. Pratt", "J. Mostow", "C.A. Kamm", "A.A. Kamm"], "venue": "In Proceedings of the Ninth National Conference on Artificial Intelligence (pp. 584589)", "citeRegEx": "Pratt et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Pratt et al\\.", "year": 1991}, {"title": "Representations for genetic and evolutionary algorithms", "author": ["F. Rothlauf"], "venue": "Springer.", "citeRegEx": "Rothlauf,? 2006", "shortCiteRegEx": "Rothlauf", "year": 2006}, {"title": "Evaluation-relaxation schemes for genetic and evolutionary algorithms", "author": ["K. Sastry"], "venue": "Master\u2019s thesis, University of Illinois at Urbana-Champaign, Department of General Engineering, Urbana, IL.", "citeRegEx": "Sastry,? 2001", "shortCiteRegEx": "Sastry", "year": 2001}, {"title": "A problem-knowledge based evolutionary algorithm KBOA for hypergraph partitioning", "author": ["J. Schwarz", "J. Ocenasek"], "venue": "In Proc. of the Fourth Joint Conf. on Knowledge-Based Software Engineering (pp. 51\u201358)", "citeRegEx": "Schwarz and Ocenasek,? \\Q2000\\E", "shortCiteRegEx": "Schwarz and Ocenasek", "year": 2000}, {"title": "Crossnet: a framework for crossover with", "author": ["F. Stonedahl", "W. Rand", "U. Wilensky"], "venue": null, "citeRegEx": "Stonedahl et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Stonedahl et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Even for optimization problems that are extremely difficult to solve, it may be straightforward to extract information about important dependencies between variables and other problem regularities directly from the problem definition (Baluja, 2006; Drezner & Salhi, 2002; Hauschild & Pelikan, 2010; Stonedahl, Rand, & Wilensky, 2008; Schwarz & Ocenasek, 2000).", "startOffset": 234, "endOffset": 359}, {"referenceID": 20, "context": "The use of information from previous runs to introduce bias into future runs of an evolutionary algorithm is often referred to as learning from experience (Hauschild & Pelikan, 2008; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Pelikan, 2002).", "startOffset": 155, "endOffset": 243}, {"referenceID": 2, "context": "The use of bias based on the results of other learning tasks in the same problem domain is also commonplace in machine learning where it is referred to as inductive transfer or transfer learning (Pratt, Mostow, Kamm, & Kamm, 1991; Caruana, 1997).", "startOffset": 195, "endOffset": 245}, {"referenceID": 1, "context": "Numerous studies have shown that using prior knowledge and learning from experience promise improved efficiency and problem solving capabilities (Baluja, 2006; Drezner & Salhi, 2002; Hauschild & Pelikan, 2010; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Rothlauf, 2006; Stonedahl, Rand, & Wilensky, 2008; Schwarz & Ocenasek, 2000).", "startOffset": 145, "endOffset": 332}, {"referenceID": 29, "context": "Numerous studies have shown that using prior knowledge and learning from experience promise improved efficiency and problem solving capabilities (Baluja, 2006; Drezner & Salhi, 2002; Hauschild & Pelikan, 2010; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Rothlauf, 2006; Stonedahl, Rand, & Wilensky, 2008; Schwarz & Ocenasek, 2000).", "startOffset": 145, "endOffset": 332}, {"referenceID": 21, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al.", "startOffset": 56, "endOffset": 123}, {"referenceID": 0, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011).", "startOffset": 173, "endOffset": 305}, {"referenceID": 25, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011).", "startOffset": 173, "endOffset": 305}, {"referenceID": 18, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011).", "startOffset": 173, "endOffset": 305}, {"referenceID": 27, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011).", "startOffset": 173, "endOffset": 305}, {"referenceID": 8, "context": "maintain useful diversity in the population, the new candidate solutions are incorporated into the original population using restricted tournament selection (RTS) (Harik, 1995).", "startOffset": 163, "endOffset": 176}, {"referenceID": 21, "context": "this network\u2019s parameters (Friedman & Goldszmidt, 1999; Pelikan, 2005):", "startOffset": 26, "endOffset": 70}, {"referenceID": 1, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations of candidate solutions (Baluja, 2006; Hauschild & Pelikan, 2008; Hauschild & Pelikan, 2009; Hauschild, Pelikan, Sastry, & Goldberg, 2011; M\u00fchlenbein & Mahnig, 2002).", "startOffset": 178, "endOffset": 319}, {"referenceID": 21, "context": "Learning from experience (Hauschild & Pelikan, 2008; Hauschild & Pelikan, 2009; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Pelikan, 2005) represents one approach to dealing with this issue.", "startOffset": 25, "endOffset": 140}, {"referenceID": 16, "context": "For example, consider optimization of NK landscapes (Kauffman, 1989), in which the fitness function is defined as the sum of n subfunctions {fi} n i=1, and the subfunction fi is applied to the ith bit and its k neighbors.", "startOffset": 52, "endOffset": 68}, {"referenceID": 1, "context": "A hard bias encodes hard restrictions on model structure or variable assignments, restricting the class of allowable models (M\u00fchlenbein & Mahnig, 2002; Baluja, 2006; Hauschild & Pelikan, 2008).", "startOffset": 124, "endOffset": 192}, {"referenceID": 9, "context": "The definition of a distance between two variables of an ADF used in this paper follows Hauschild and Pelikan (2008) and Hauschild et al.", "startOffset": 88, "endOffset": 117}, {"referenceID": 9, "context": "The definition of a distance between two variables of an ADF used in this paper follows Hauschild and Pelikan (2008) and Hauschild et al. (2011). Given an additively decomposable problem, we define the distance between two variables using a graph G of n nodes, one node per variable.", "startOffset": 88, "endOffset": 145}, {"referenceID": 16, "context": "Both these problem classes were shown to be challenging for conventional genetic algorithms and many other optimization techniques due to the rugged landscape, strong epistasis, and complex structure of interactions between problem variables (Kauffman, 1989; Young, 1998; Pelikan, 2010; Pelikan & Hartmann, 2006).", "startOffset": 242, "endOffset": 312}, {"referenceID": 22, "context": "Both these problem classes were shown to be challenging for conventional genetic algorithms and many other optimization techniques due to the rugged landscape, strong epistasis, and complex structure of interactions between problem variables (Kauffman, 1989; Young, 1998; Pelikan, 2010; Pelikan & Hartmann, 2006).", "startOffset": 242, "endOffset": 312}, {"referenceID": 16, "context": "An NK fitness landscape (Kauffman, 1989) is fully defined by the following components: (1) The number of bits, n, (2) the number of neighbors per bit, k, (3) a set of k neighbors \u03a0(Xi) of the ith bit for every i \u2208 {1, .", "startOffset": 24, "endOffset": 40}, {"referenceID": 22, "context": "The reason for restricting neighborhoods to nearest neighbors was to ensure that the problem instances can be solved in polynomial time even for k > 1 using dynamic programming (Pelikan, 2010).", "startOffset": 177, "endOffset": 192}, {"referenceID": 16, "context": "Both these problem classes were shown to be challenging for conventional genetic algorithms and many other optimization techniques due to the rugged landscape, strong epistasis, and complex structure of interactions between problem variables (Kauffman, 1989; Young, 1998; Pelikan, 2010; Pelikan & Hartmann, 2006). However, for both problem classes, it is straightforward to generate a large number of problem instances with known optima. For each problem class and problem size, we use 1,000 unique problem instances; the reason for using such a large number of instances is that for these problem classes, algorithm performance often varies substantially from one instance to another and the results would thus be unreliable if only a few instances were used. An NK fitness landscape (Kauffman, 1989) is fully defined by the following components: (1) The number of bits, n, (2) the number of neighbors per bit, k, (3) a set of k neighbors \u03a0(Xi) of the ith bit for every i \u2208 {1, . . . , n}, and (4) a subfunction fi defining a real value for each combination of values of Xi and \u03a0(Xi) for every i \u2208 {1, . . . , n}. Typically, each subfunction is defined as a lookup table. The objective function fnk to maximize is defined as fnk(X1,X2, . . . ,Xn) = \u2211n i=1 fi(Xi,\u03a0(Xi)). The difficulty of optimizing NK landscapes depends on all components defining an NK problem instance. In this paper, we consider nearest-neighbor NK landscapes, in which neighbors of each bit are restricted to the k bits that immediately follow this bit. The neighborhoods wrap around; thus, for bits which do not have k bits to the right, the neighborhood is completed with the first few bits of solution strings. The reason for restricting neighborhoods to nearest neighbors was to ensure that the problem instances can be solved in polynomial time even for k > 1 using dynamic programming (Pelikan, 2010). The subfunctions are represented by look-up tables (a unique value is used for each instance of a bit and its neighbors), and each entry in the look-up table is generated with the uniform distribution from [0, 1). The used class of NK landscapes with nearest neighbors is thus the same as that in Pelikan (2010). In all experiments, we use k = 5 and n \u2208 {100, 150, 200}.", "startOffset": 243, "endOffset": 2192}, {"referenceID": 30, "context": "For each problem instance, we used bisection (Sastry, 2001; Pelikan, 2005) to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.", "startOffset": 45, "endOffset": 74}, {"referenceID": 21, "context": "For each problem instance, we used bisection (Sastry, 2001; Pelikan, 2005) to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.", "startOffset": 45, "endOffset": 74}, {"referenceID": 7, "context": "For example, it should be straightforward to adapt this framework to the extended compact genetic algorithm (Harik, 1999) or classes of facility location problems.", "startOffset": 108, "endOffset": 121}], "year": 2012, "abstractText": "For many optimization problems it is possible to define a distance metric between problem variables that correlates with the likelihood and strength of interactions between the variables. For example, one may define a metric so that the dependencies between variables that are closer to each other with respect to the metric are expected to be stronger than the dependencies between variables that are further apart. The purpose of this paper is to describe a method that combines such a problem-specific distance metric with information mined from probabilistic models obtained in previous runs of estimation of distribution algorithms with the goal of solving future problem instances of similar type with increased speed, accuracy and reliability. While the focus of the paper is on additively decomposable problems and the hierarchical Bayesian optimization algorithm, it should be straightforward to generalize the approach to other model-directed optimization techniques and other problem classes. Compared to other techniques for learning from experience put forward in the past, the proposed technique is both more practical and more broadly applicable.", "creator": "gnuplot 4.4 patchlevel 4"}}}