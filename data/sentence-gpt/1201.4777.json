{"id": "1201.4777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2012", "title": "A probabilistic methodology for multilabel classification", "abstract": "Multilabel classification is a relatively recent subfield of machine learning. Unlike to the classical approach, where patterns are labeled with only one category, in multilabel classification, an arbitrary number of categories is chosen to label a pattern. Due to the problem complexity (the solution is one among an exponential number of alternatives), a very common solution (the binary method) is frequently used, learning a binary classifier for every category, and combining them all afterwards. The assumption taken in this solution is not realistic, and in this work we give examples where the decisions for all the labels are not taken independently, and thus, a supervised approach should learn those existing relationships among categories to make a better classification. Therefore, we show here a generic methodology that can improve the results obtained by a set of independent probabilistic binary classifiers, by using a combination procedure with a classifier trained on the co-occurrences of the labels. We show an exhaustive experimentation in three different standard corpora of labeled documents (Reuters-21578, Ohsumed-23 and RCV1), which present noticeable improvements in all of them, when using our methodology, in three probabilistic base classifiers. The results from the study are presented in a standard library, the standard library, and a formal library. The final results of the study are presented in a standard library, the standard library, and a formal library.\n\n\nFigure 1: The results obtained by a set of independent probabilistic binary classifiers. Credit: B.G. C. N. J. Kontzkou (2010). \"A Bayesian Bounded Linear Model (BCM)\" In the field of Bayesian classification, we have a variety of parameters that can be used to calculate the rank of each category in order to describe the predicted rank of each group. We use these parameters to plot the rank of each group by the distribution of the top (b = 1 \u00d7 0.01), as well as the top (c = 0.01), as well as the top (d = 1 \u00d7 1.00), in a supervised model, such as PDE and M.R.G. I. M. D. E. F. L. D. F. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D. D.", "histories": [["v1", "Mon, 23 Jan 2012 17:25:34 GMT  (16kb)", "https://arxiv.org/abs/1201.4777v1", "14 pages, 1 figure, under review"], ["v2", "Thu, 28 Feb 2013 20:22:47 GMT  (23kb)", "http://arxiv.org/abs/1201.4777v2", "14 pages, 1 figure, under review"]], "COMMENTS": "14 pages, 1 figure, under review", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["alfonso e romero", "luis m de campos"], "accepted": false, "id": "1201.4777"}, "pdf": {"name": "1201.4777.pdf", "metadata": {"source": "CRF", "title": "A probabilistic methodology for multilabel classification", "authors": ["Alfonso E. Romero", "Luis M. de Campos"], "emails": ["aeromero@cs.rhul.ac.uk", "lci@decsai.ugr.es"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 1.\n47 77"}, {"heading": "1 Introduction", "text": "In this work we present a novel solution for the multilabel categorization problem. In this kind of problems, a subset of categories (instead of just one) is assigned to each instance. Multilabel classification problems arise, in a natural way, in information processing, concretely on the subfield of automatic document categorization [23]. Due to their nature, an important number of text corpora are of a multilabel kind. For example, news articles can often belong to more than one category (this is the case of the Reuters-21578 [12] and the RCV1 [13] collections, which are composed of articles from the Reuters agency). In other domains, multiple labels are assigned as metadata which give a better description of the documents (this occurs, for example, in scientific papers and legal documents, which have associated keywords from a controlled vocabulary, like the Mathematics Subject Classification or the MeSH Thesaurus in one case, and the Eurovoc thesaurus in the other case [4]). On the other hand, multilabel instances occur very commonly in the Internet: in many blog applications, blog posts for example, can be categorized with an arbitrary number of labels. Furthermore, in collaborative environments (like folksonomies, [31]) where users can add tags, multilabel is an ordinary process. Due to this fact, sometimes the word \u201ctag\u201d or \u201clabel\u201d is used instead of \u201ccategory\u201d, they are all synonyms.\nMore recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions [26, 33], scene or image categorization [7, 24], protein and gene function prediction [18, 30] or medical diagnosis [25].\nAlthough each instance has a given set of associated labels, and they are assigned as a whole, the normal approach to solve this problem is just ignore this fact and concentrate in obtaining good solutions to the individual binary problems (i.e., deciding for each label if it should be assigned to the instance or not). Here, we propose a solution which takes into account the inter-category dependence, trying to find natural associations in order to improve the final categorization results.\nThe content of this paper is organized as follows: first of all (in Section 1.1) we recall the well-known problem of multilabel supervised categorization, reviewing some previous works in this area (Section 1.2) together with a brief explanation (Section 1.3) of what is the semantic of adding multiple labels to an instance instead of one. Based on probabilistic foundations, our approach is presented in Section 2, which results in two different models. An extensive experimentation with test collections coming from the text categorization field will be carried out in Section 3 to prove the validity of our proposal. Finally, in the light of the results previously obtained, several conclusions and future works will be pointed out in Section 4."}, {"heading": "1.1 Multilabel supervised categorization", "text": "The problem of supervised multilabel classification (see, for instance [27]) deals with supervised learning, where the associated labels can be a set of unde-\ntermined size. Formally, it can be stated as follows: given a set of categories C = {c1, . . . , cp}, an input instance space X , and a set of labeled data composed of instances and the set of assigned labels, {xi, yi}i=1,...,n where yi \u2282 2C \\ \u2205, xi \u2208 X , learning a multilabel classifier means inferring a function f : X \u2212\u2192 2C \\ \u2205, in other words, a function able to assign non-empty subsets of labels to any unlabeled instance.\nIn order to cope with this exponential output space, the classical approach consists in dividing the problem into |C| binary independent problems, and therefore learning |C| binary classifiers fi : X \u2212\u2192 {ci, ci} (which decide whether the category is assigned or not to the instance). Although this is a naive solution, it works reasonably well, and in multilabel classification literature this can be used as a baseline. This approach is called the binary relevance method [9, 27, 36], and it is often criticized for ignoring the existing correlations among labels.\nGiven the fact that many of the multilabel problems come from multi-tagged collections (i.e., collections of objects which are manually assigned a subset of categories of the whole set), it is very likely that some of these tags are associated not only because of the content, but also due to the presence/absence of another tag. We explain examples of this phenomenon in section 1.3. Roughly speaking, the main motivation of this paper is to look carefully at the results of the individual binary classifiers and modify those results given a model previously trained on the label assignment vectors, which explicitly captures relationships among categories and therefore improves classification results. As it is shown, our approach utilizes a simple but powerful independence assumption which results in a model that is more complex than the binary relevance method, but still is cheap enough."}, {"heading": "1.2 Related work", "text": "There is more than one hundred references partially related with multilabel categorization, most of them in the last years1. At first sight, the two possible solutions to this problem basically consist of transforming it to a single-label one, or adapting a learning procedure to work with these multiple labels at the same time. This taxonomy is given in [27], naming the former solutions problem transformation methods and algorithm adaptation the latter ones. Because our contribution adapts a learning procedure to multilabel learning, we review here only approaches of the second kind.\nAlmost all the typical classification algorithms have a multilabel version. For example, an adaptation of the entropy formula of the C4.5 tree learning algorithm has been proposed in [3] for multilabel classification. In the lazy algorithms field, variations of the k-NN algorithm have been presented for this kind of problems [14, 36]. Also, in [2] a k-NN is combined with a logistic regression classifier (in a different way that we do) to cope with multiple labels. Of course, variations on the SVM algorithm are shown in [9], where both intraclass dependencies and an improvement of the definition of margin for multilabel\n1See http://www.citeulike.org/group/7105/tag/multilabel, and references therein.\nclassification are used to build a new model. On the other hand, there are methods dealing with multilabel classification within the probabilistic framework and therefore closer to our approach. In [17], a generative model is trained using training data, and completed with the EM algorithm, and computed the most probable vector of categories that should be assigned to the document. A subset of Reuters-21578 is used for experimentation, and noticeable improvements are shown.\nA generative model is also presented in [29]. Here, the main assumption is that words in documents belonging to several categories can be characterized as a mixture of characteristic words related to each of the categories, being this assumption confirmed with experimentation. Both first (PMM1) and second (PMM2) order models are built, and learning algorithms (using a MAP estimation) are proposed for both alternatives. Experiments are carried out with webpages gathered from the yahoo.com server. Presented results are good, and improve other methods as SVM, naive Bayes and k-NN.\nMore recently [5] proposed a novel method, called probabilistic classifier chains (generalizing the classifier chains) which exploits label dependence, showing that the method outperforms others in terms of loss functions. This is claimed via an extensive experimentation with artificial and real datasets."}, {"heading": "1.3 The semantic of assigning multiple labels", "text": "In this section we try to enumerate three clear examples showing possible reasons for a manual indexer to assign multiple labels. Although we do not pretend to be exhaustive, we think that the three presented examples are common and may occur easily in multilabel problems:\n1. Mixture of topics. An instance matches all the abstract description of several categories. This is the case, for example of a medical paper which deals with several topics represented as MeSH keywords.\n2. Contextualization. Some tags are added in order to fix the context in which other label is used. For example, in scene classification, a picture of fishermen working in the coast of Motril tagged with \u201csea\u201d and \u201cpeople\u201d can be contextualized with \u201ctown\u201d in order to distinguish it from submarine photos.\n3. Non overlapping labels. Some subsets of labels do not admit instances belonging to all of them. For example, in music classification, it is inconceivable to have songs tagged with both \u201cbaroque\u201d and \u201creggae\u201d, although other combinations as \u201cflamenco\u201d and \u201cjazz\u201d are possible.\nThe only \u201cpure\u201d multilabel phenomenon is the first one. The second and the third denote that the occurrence of labels is not only based on the content of the instance, but also in the occurrence (or not) of other labels. For the second case, a label is added to contextualize two previously given labels. That is to say, the occurrence of a certain subset of labels increases the likelihood of other being\nadded. On the other hand, in the third case, a song labeled with \u201cbaroque\u201d (at a certain degree) and \u201creggae\u201d (in a lower degree) may be detected by a classifier as an \u201canomaly\u201d, and be labeled only with \u201cbaroque\u201d (given that the system has previously learned that the label \u201cbaroque\u201d gives information about the low likelihood of also using the label \u201creggae\u201d given that \u201cbaroque\u201d is being used).\nAlthough the first phenomenon can be initially captured by different binary classifiers, the second and the third can be tackled by looking at the labels of a training set2, and not only to the content of the instances. Therefore, our contribution will be to state that the final labeling of an instance, in a certain category, will be a combination of the result of the binary classifier with the evidence given in the other categories by the other binary classifiers, taking into account the existing relationships among categories captured in the training set. In fact, this issue of label dependence has been recently shown to be crucial in multilabel learning [6]. All of this will be modeled in a probabilistic framework, which will give us a rich language to describe this procedure."}, {"heading": "2 A probabilistic model for multilabel classifica-", "text": "tion\nLet xi \u2208 X be an instance. Suppose we have a set of p categories C = {c1, . . . , cp}. For every category cj, we define a binary random variable Cj = {cj, cj}. Let us assume that we have p probabilistic binary classifiers, based only on the content of the instances (the features describing them). Thus, the conditional probability pj(cj |xi) represents the probability that the instance xi is labeled with cj (the subindex j indicates that this distribution is different and independent for every category). Assuming we have a perfect knowledge of the underlying probability distribution, these classifiers define p labeling rules as follows (Bayes optimal classifier): \u201cclassify xi as cj if pj(cj |xi) > pj(cj |xi)\u201d or, alternatively \u201cclassify xi as cj if pj(cj |xi) > 0.5\u201d.\nFor every category cj , we shall also define a random vector of binary variables Lj = (lj1, . . . , ljp\u22121), which represents the labeling of an instance with the other p\u2212 1 classifiers. We shall note for lj a particular value of the vector Lj (that is to say, a label for the instance in all the categories except the j-th one). Thus, every component ljk of the vector lj is binary, and corresponds to the variable Ck if k < j and to Ck+1 otherwise.\nOur aim is to give a model which describes the probability of a class given knowledge of both the content of the instance, xi, and the labeling of the other categories (lj). In other words, an expression for the probability pj(cj |xi, lj).\n2Or adding expert knowledge with explicit relations among the labels, like a hierarchy."}, {"heading": "2.1 The proposed model", "text": "Our model will start making a reasonable simplifying assumption (general naive Bayes assumption3): given the true value of a category j, the events of finding a certain instance xi and a certain labeling on the other categories for this instance, lj, are independent, that is\npj(xi, lj|cj) = pj(xi|cj) pj(lj|cj), \u2200j \u2208 {1, . . . , p} . (1)\n\u00bfFrom the point of view of a certain category cj , if its value is known, this equation assumes that the values of the other associated categories are probabilistically independent of the \u201ccontent\u201d of the instance xi. While this assumption might seem a bit unrealistic, it is basically the same which is performed in the naive Bayes classifier (this is why is called general naive Bayes). We shall show later that, as in the case of the naive Bayes classifier, the assumption is neither intuitive nor much realistic but can result in very good classification performance. Nevertheless, it should be stressed that our assumption does not mean independence between labels at all, but independence between labels and content given other label. As clearly expressed by the term pj(lj|cj) in eq. (1), our aim is to explicitly model a clear dependence between each label cj and the other labels represented in lj.\nThen, using Bayes\u2019 theorem with this assumption, we compute the desired probability:\npj(cj |xi, lj) = pj(xi, lj|cj) pj(cj)\npj(xi, lj)\n= pj(xi|cj) pj(lj|cj) pj(cj)\npj(xi, lj)\n=\n( pj(xi) pj(lj)\npj(xi, lj)\n)( pj(cj |xi) pj(cj |lj)\npj(cj)\n) .\nThe first term is a proportionality factor which does not depend on the category. Therefore we get the expression,\npj(cj |xi, lj) \u221d pj(cj |xi) pj(cj |lj)\npj(cj) ,\nwhich leads us to the final formula:\npj(cj |xi, lj) = pj(cj |xi) pj(cj |lj)/pj(cj)\npj(cj |xi) pj(cj |lj)/pj(cj) + pj(cj |xi) pj(cj |lj)/pj(cj) (2)\n3The term general is used here in a sense of analogy with the \u201cclassic\u201d naive Bayes assumption (given the true category of an instance, the joint probability of its features factorizes as a product of independent probability distributions). The previous labelings of the document along with its content can be considered as two \u201cgeneral\u201d features, and this assumption means that we apply it to that set of two features.\nTaking into account that we are in binary classification, it holds that pj(cj) = 1 \u2212 pj(cj), pj(cj |xi) = 1 \u2212 pj(cj |xi) and pj(cj |lj) = 1 \u2212 pj(cj |lj). The values pj(cj |xi) can be simply obtained with any probabilistic binary classifier for the category cj . Prior probabilities pj(cj) are estimated as the number of instances which belong to class cj over the total number of instances. Probabilities pj(cj |lj) can be estimated, through a learning process, from the labels of the training data, where every instance has as features some binary values telling if the instance belongs or not to any of the p\u2212 1 categories (all categories except j). So, in our model, we need to train p binary classifiers from the content of the instances, and p binary classifiers from the labels assigned to these instances.\nA last point should be clarified in the model. Given an instance xi to classify, we easily compute, for a category cj both the values pj(cj) and pj(cj |xi). However, to obtain the probability pj(cj |lj) we would need to know the true assignments of the labels which are not cj . In our model, we shall make a second assumption, approximating lj for l\u0302j, which is computed as follows:\nl\u0302j = ( \u03c4k ( pk(ck|xi) ) ) k\u2208{1,...,p}\\j . (3)\nWhere \u03c4k is a threshold function (\u03c4k(z) = ck if z < 0.5, \u03c4k(z) = ck otherwise). In other words,\nl\u0302jk = arg max {ck,ck} pk(c|xi).\nTherefore pj(cj |lj) will be approximated by pj(cj |l\u0302j)."}, {"heading": "2.2 An improved version of the model", "text": "It should be noticed that the model summed up in eq. (2) does not really need the assumption that the random vectors Lj are made of binary variables. In fact, the only required binary variables are the Cj ones. Thus, the same equation can be applied equally if the variables in Lj are continuous.\nTherefore, we can rewrite an approximation of lj different than the one given in eq. (3) by removing the threshold function:\nl\u0302j = ( pk(ck|xi) ) k\u2208{1,...,p}\\j . (4)\nThis means that the components of the vector l\u0302j are values in [0, 1] which represent our degree of belief in these labels being assigned to the instance. Note that, in order to use this extended version of the model, we need a classifier to compute pj(cj |lj) which is capable of dealing with continuous inputs (in the previous version we could assume the inputs were binary, and so the classifier)."}, {"heading": "2.3 The algorithm", "text": "We present finally the proposed algorithm for multilabel classification. We need, as input data, the prior probabilities of each category (pj(cj)), p content-only\nbinary classifiers (capable of providing an output pj(cj |xi)), and p classifiers which predict each category cj based on the values of the labels different from cj : pj(cj |lj).\n1. Given an instance xi, we obtain the values pj(cj |xi) from p binary probabilistic classifiers (j = 1, . . . , p).\n2. For every category cj , j \u2208 {1, . . . , p},\n(a) Using pk(ck|xi), we compute l\u0302j either by eq. (3) or eq. (4). This will be used as an approximation of lj.\n(b) We compute the likelihood of the category given the probabilities of the other categories as pj(cj |lj).\n(c) We compute the probability value pj(cj |xi, lj), following eq. (2).\n3. The scores of the instance xi in the set of categories C are the values pj(cj |xi, lj). They can be thresholded, optionally, if we are doing hard categorization.\nThe whole process is summed up in Figure 1. Note that the computational load with respect to the binary relevance method is very low. Initially, apart from the binary classifiers based on content (which work on an input space containing, say, m features), one label classifier needs to be trained for each class. However, the label classifiers usually will be very cheap to train, as they work on an input space possessing p \u2212 1 features (and usually p \u226a m). For the classification part, the binary relevance method needs the example to be classified on each of the p binary classifiers. Once the scores of all the classifiers have been obtained, p extra classifications are needed in the label classifiers (which should be much faster as the number of features is low), and some additional computations needs to be performed, summarized in eq. (2). As it can be seen, neither much additional memory space, nor computational power is needed to perform this approach. Therefore, its scalability should not be an issue.\nWe have shown a new method to deal with relationships among labels, using probabilistic classifiers. This is why it is presented as a \u201cmethodology\u201d. For a concrete problem, two decisions should be made about which underlying models should be used: first for the content classifiers, and second for the label classifiers. This choice relies heavily on the kind of problem selected, and is suitable of previous experimentation to find a working set of methods. Also one may note that both choices are independent (so, with a set of nc probabilistic content classifiers and nl label classifiers, ncnl combinations are possible, each one with two possibilities for computing the vectors lj)."}, {"heading": "3 Experimentation", "text": "We shall expose in this section an experimentation to test the validity of our approach, where some combinations of classifiers will be selected following a\ncertain criterion. Of course, we do not aim being exhaustive and giving a long list of experiments. We are aware that many different collections and combinations of classifiers could be selected, so we tried to make a good experimentation by selecting a restricted but representative set of classifiers."}, {"heading": "3.1 Corpora", "text": "Three different document categorization corpora have been used for experimentation. We describe them now, together with the preprocessing procedure used to obtain the term vectors.\nFirst, Reuters-21578 (see [12], for instance) is a collection of 21578 news articles. We have used the most famous split (the one named ModApte), which divides the set of documents into a training and a test set, and categories only assigned to documents in the test set are removed (then resulting only 90 of them).\nThe Ohsumed collection [10] is a set of 348566 references from MEDLINE, an online medical information database. For every record the assigned MeSH\nterms (categories) are given. Because the number of categories of the MeSH thesaurus is huge, it is often chosen a subset of 23 categories (heart diseases), which are the root of some categories in a hierarchy. Documents which do not belong to that subtree of categories, are discarded, and the resulting corpus is called Ohsumed-23. This is the methodology followed in [11].\nFinally, the RCV1 corpus is a relatively more recent corpus (see [13]), also based on Reuters news stories. It contains many documents (806791 for the final version, RCV1-v2), where the documents are preprocessed, with stopwords removed, and terms already stemmed. The number of categories named \u201ctopic codes\u201d4 is 103 (after the split). An standard split is also provided, called LYRL2004, which gives a training set with over 23000 documents, and a test set with 781000. We have removed two categories which appear only in the training set (reducing the number to 101).\nIn the first two cases, the stopwords list used consists of 571 stopwords of the SMART retrieval system [22]. Also, the English stemming algorithm of the Snowball package [20] was applied to resulting words. In the Reuters-21578 also XML marks were removed.\nIn order to reduce the size of the lexicon, terms occurring in less than three documents were removed in Reuters-21578 and Ohsumed-23 (following the guidelines of [11]), and in less than five documents in RCV1, as done in [13]. All document preprocessing stage was made with DauroLab [21]."}, {"heading": "3.2 Evaluation measures", "text": "In this subsection we discuss the different evaluation measures we have used for our experimentation. Following the taxonomy considered in [15, 28], we have selected label-based measures, example-based and ranking measures. We shall present the chosen measures along with a brief explanation of them, and a discussion of which performance aspects are considered for each one."}, {"heading": "3.2.1 Example-based measures", "text": "They are measures specifically designed for multilabel problems. That is, they try to account for the performance of the multilabel task itself (the assignment or not of a certain set of labels to one example). Of those available, we have selected two: Hamming loss and subset 0/1 loss. Note that, as both measures are losses, the lower the value they have, the better the classification is.\nHamming loss computes the normalized Hamming distance between the set of assigned labels and the set of predicted ones. That is, for a certain instance xi, being f(xi) the set of predicted labels, and yi the set of true labels, the Hamming loss will be equal to:\nHammingLoss = yi \u2206 f(xi)\np ,\n4Other two disjoint set of categories, \u201cindustry codes\u201d and \u201cregion codes\u201d are given for this corpus, but they are not normally used for categorization.\nwhere \u2206 stands for the symmetric difference of two sets and p is the number of labels. The measure ranges between 0 and 1 and will be averaged through all the elements of the test set to obtain its final value.\nSubset 0/1 loss, in contrast, is a generalization of the 0/1 loss for multilabel problems where the set of labels is considered as a whole. For the previous case, the loss function is equal to:\nSubsetLoss = I (yi 6= f(xi)) .\nIn the previous equation, I (yi 6= f(xi)) = 0 if and only if yi = f(xi), and 1 otherwise. That is, there is no loss if the set of predicted labels equals the set of true labels. Again, this measure will be averaged through all the test set."}, {"heading": "3.2.2 Label-based measures", "text": "These are classical measures for binary classification problems. We have made in all cases hard categorization (assigning or not every label) and then, we have used suitable measures for this task. Being defined for every category cj , the precision and recall (\u03c0j and \u03c1j , respectively) are:\n\u03c0j = TPj\nTPj + FPj , \u03c1j =\nTPj\nTPj + FNj ,\nwhere TPj, FPj and FNj stand for \u201ctrue positives\u201d, \u201cfalse positives\u201d and \u201cfalse negatives\u201d of the j-th category. We have then selected the F1-measure adapted for categorization (the harmonic mean between precision and recall), in its macro and micro averaged versions (denoted by MF1 and \u00b5F1, respectively). See [15, 23] for more details.\nAll the probabilistic classifiers have a natural threshold in 0.5. We have not made any threshold tuning because it was not the aim of this paper. Anyway, it could be made independently, likely improving the results (see the discussion in Section 4)."}, {"heading": "3.2.3 Ranking measures", "text": "We have implemented one ranking measure, the one error, which evaluates how many times the label ranked at the top is not in the set of relevant labels of the instance. This measure is important for us, at it takes partially into account the ranking to perform the evaluation. For real-world use cases, in multilabel classification, where predicted labels are suggested to a human indexer using a ranking procedure, it would be very important that the labels at the top of the list were relevant."}, {"heading": "3.3 Basic probabilistic classifiers", "text": "In order to have, first a baseline, and secondly a basic content classifier to be used afterwards for our model, we have considered three different classifiers, widely used in the literature. One which usually obtains discrete results, and\ntwo with better results, all of them of a different nature. For the first case, we selected the multinomial naive Bayes, in its binary version [16]. For the second case, a k-NN classifier has been used, normalizing the output in order to obtain a value in [0, 1] which can be interpreted as a probability. Finally, a linear SVM, with probabilistic output5, as performed by Platt\u2019s algorithm [19] was chosen. We recall that the flexibility of this procedure is very high because any probabilistic classifier could be used instead of these three.\nFor the k-NN classifier, the best performing value of k was selected, based on previous experimentation existing in other works. Thus, we chose k = 30 for Reuters and k = 45 for Ohsumed-23 (as set in [11]). For RCV1, a value of k = 100 was used [13]. Also, following those references, we have performed feature selection in Reuters (1000 features selected by information gain, using the \u201csum\u201d combination [23]), and in RCV1 (8000 features selected by \u03c72, using the \u201cmax\u201d combination). No feature selection was performed in Ohsumed-23 as noted in [11].\nThe implementation used for the algorithms was that contained in the DauroLab [21] package, except for the SVM where libsvm [1] was chosen."}, {"heading": "3.4 Label classifiers", "text": "Here we show the two alternatives that we have selected as label classifiers."}, {"heading": "3.4.1 Logistic regression", "text": "On the first hand, and based on previous experimentations (not shown here), we have chosen a logistic regression-based classifier to model the posterior probability distribution of the category given the correct labels of all the other categories, namely:\npj(cj |lj) = 1\n1 + e\u2212(w0+w\u00b7lj) ,\nwhere w0 is a real scalar and w a p\u2212 1 dimensional real vector, and \u201c\u00b7\u201d is the usual dot product. The parameters of this model are learned to maximize a certain criterion (generally a maximum likelihood approach).\nWe have selected this classifier instead of other proposals for three reasons: first of all, it can deal with real inputs. Second, it is a discriminative method, which does not try to find the joint distribution of Cj and Lj (in which we are not interested). Finally, it is very simple, fast to learn, and works reasonably well in almost all the environments.\nIn order to have accurate estimates, and because the dimensionality of the problem p is high, the method selected to learn the weights is a Bayesian logistic regression, concretely the one proposed in [8], with Gaussian priors. The implementation used here is the one included with the Weka package, with the default parameters (see [32] for details).\n5The one implemented in LibSVM.\nOne of the flexibilities of any logistic regression classifier is that it can model distributions with real inputs. As we have two approximations for the input vector in this classifier, we can propose different models, which will be called M1 (corresponding to eq. (3)) and M2 (for eq. (4)). It seems reasonable that, if the content classifiers perform well, the model M2 will be more accurate than the model M1. We shall discuss this point afterwards."}, {"heading": "3.4.2 Linear Support Vector Machine", "text": "As a complement to the previous one, we have added the results obtained by using a linear support vector machine with probabilistic outputs (fitted using Platt\u2019s algorithm [19]). Thus, the input to the SVM will be the vector with the output values of the p \u2212 1 binary classifiers pk(ck|x), k \u2208 {1, . . . , p} \\ j, and the real-valued outputs of the linear classifier will be therefore transformed to probabilities with the model learned in the training data by the mentioned algorithm.\nIn order to show the validity of our approach we have used a linear kernel (i.e., the simplest SVM) without any further modification to the default parameters to those included in the canonical implementation of the Weka [32] SMO (Sequential Minimal Optimization) class. It is well known that linear SVMs tend to perform reasonably well in classification tasks, although they are generally outperformed by their kernelized counterparts. We have selected the simplest model to illustrate that we can improve in some measures the baseline of the binary classifiers without too much effort."}, {"heading": "3.5 Results", "text": ""}, {"heading": "3.5.1 Experiments with label-based measures", "text": "We present the results for Reuters-21578 (in Table 1), Ohsumed-23 (in Table 2) and RCV1 (in Table 3). NB denotes the multinomial naive Bayes, k-NN the k nearest neighbors classifier, and SVM the linear support vector machine with probabilistic outputs. The terms \u2018BLR\u2019 or \u2018SMO\u2019 refers to the label classifiers used, if any (Bayesian Logistic Regression, and linear Support Vector Machine, respectively). We have used the term SMO to distinguish this label classifier from the SVM in the content classifier.\nA classifier + M1 or M2 denotes our proposal with the binary or real inputs presented to the corresponding label classifier, as explained before. Also, the results of the base classifier have been shown for comparison purposes.\nWe give the micro and macro F1 values (\u00b5F1 and MF1, respectively), and the percentage of improvement (positive or negative) over the baseline, denoted by \u2206.\nIn all the cases our proposal with the M2 version of the algorithm improved at least one of both measures of the baseline (i.e. \u2206 > 0), of which, most of them had the two measures improved. The results were noticeable, even reaching a gain of 72% in the case of the macro F1 measure with respect to\nthe baseline (which is a remarkable achievement). On the other hand, the M1 version presented not so systematic improvements.\nMoreover, we have run statistical significance tests for every couple of classifiers (a basic probabilistic classifier, and our proposal, either with the M1 or the M2 configuration). For the micro measure, a micro sign s-test was performed, and for the macro measure, we chose the macro S-test. Both are presented in [35] and constitute the nowadays standard for comparing this kind of experiments. Nevertheless it should be noticed that the s-test is not specifically designed for the F1 measure, taking into account both true positives and true negatives (as shown in Section 3.2, F1 only considers true positives). Also, note that the macro S-test does not take into account the amount of improvement, but the number of categories where the measure is improved, leading sometimes to counter-intuitive results (with a non-significant high improvement in macro due to the improvement in only very few categories).\nIn the test the first system, A, will be the best performing one, in terms of micro or macro F1 measure, being B the second. The null hypothesis is that A is similar to B. On the other hand, the alternative hypothesis says that A is better than B.\nIf the p-value is less than 0.01, we show in the tables the sign \u226a (resp. \u226b) to denote that our baseline plus M1 or M2 is significantly better (resp. worse) than the baseline alone. The signs < and > are used to indicate the same fact if the p-value is between 0.01 and 0.05. With the sign \u223c we point out no significant difference between the two systems. Table 4 displays a summary of the results, showing the number of times that each one of our models is better, significantly better, worse and significantly worse than the baseline.\nOnce shown the results, we may state that the methodology presented is useful for improving classification results in a multilabel environment. Concretely,\nin the presented tables, the following facts can be found:\n\u2022 The use of this technique clearly improves the classification results of the baseline. For the macro experiments, all the measures are improved from the baselines. In the micro ones, also good improvements are found, especially for the M2 version of the classifier and BLR, which improves the baseline in all but one cases (Reuters with k-NN, where the loss is around 1%). The SMO classifier went a bit worse than the BLR, presenting in general smaller deltas, and worsening all the micro results in RCV1.\n\u2022 Comparing both approaches, the model M1 (binarized) performs, in general, worse than the continuous (M2) version, regardless of the label classifier used. This is due to the binarization procedure, which removes some information represented in the granularity of the assignment that is well captured by the classifier (if for a class cj, the base classifier assigns to two instances the probabilities 0.99 and 0.55, the binarized version will treat these two cases in the same way, assuming that both examples are of class cj ; however the continuous version will clearly distinguish them).\n\u2022 In general, we can say that this methodology seems to benefit classification of less populated categories (those with a very low prior) a lot, without harming more frequent categories. This produces the fact that, in general, the macro measures (where all categories weight the same) are heavily lifted up, whereas micro measures are improved at a lesser degree.\n\u2022 The improvement for the micro measure in the RCV1 corpus were quite small, but it should be noted that the baseline value (0.80570) was very close to the best result obtained by Lewis [13] with SVM (0.816) and a sophisticated threshold tuning algorithm (ScutFBR.1). Perhaps results better than this high value are very difficult to get."}, {"heading": "3.5.2 Experiments with example-based and ranking measures", "text": "We present the results for Reuters-21578 (in Table 5), Ohsumed-23 (in Table 6) and RCV1 (in Table 7). The rest of the notation is similar to the one used in the previous tables. We have added the symbol \u2022 to the values which are better or equal than the corresponding baseline, and \u25e6 to those which are worse.\nThe results show different patterns for each one of the measures. One error behaves really good, showing a very good performance in all three collections, improving the baseline both in the M1 and M2 versions for almost all the cases. For the rest of measures, they tend to work well with naive Bayes (perhaps the weakest of the three classifiers). Subset 0/1 loss is improved in all cases in Ohsumed-23 but only in 5 and in 3 cases in Reuters and RCV1, respectively. It seems that, for most of the cases, M2 is better than M1 for this measure, and BLR better than SMO.\nHamming loss shows a good performance for Ohsumed-23, where all the M2 versions of our classifier outperform the baselines. In Reuters, however, only 5\nout of 12 cases a performance improvement was shown. This fact is also present in RCV1, where only for naive Bayes and BLR an improvement is obtained. Again, M2 presents a better performance than M1 in general. The fact that the two example-based measures do not show great improvements (and even decrease their performance) can be explained with the fact that our method tends to be designed to produce great increments in macro- measures (i.e. per category) and these measures are averaged for each instance (that is, they are close to the micro- ones), except of the fact that Hamming loss accounts for the number of false positives and negatives, regardless of how many other labels have been predicted well. A summary of these results is shown in Figure 8, where the number of times that each one of our models is better for each measure than the corresponding baseline is shown."}, {"heading": "4 Conclusions and future works", "text": "In this paper we have proposed a quite general methodology to better manage multilabel classification problems. It is based on explicitly taking into account the dependences among labels. The proposed method can use as the starting point any set of binary classifiers (one for each label, which are trained from the content of the instances in the training set) able to produce a probabilistic output. This information is merged in a principled way with the information generated by another set of binary classifiers, which in this case are trained using only the information about the labels assigned to the instances in the training set (capturing the dependences among labels). These label-based classifiers, like the content-based ones, can be built using a variety of learning methods, provided that their output can be interpreted probabilistically.\nWe have carried out experiments with three well-known multilabel document collections, using three very different content-based baseline classifiers (naive Bayes, k-NN and SVM) and two label-based classifiers (logistic regression and SVM). The experiments confirm that our methodology often tends to improve the results obtained by the baseline classifiers.\nThe combination of our methods (which so far use a natural threshold of 0.5) with any of the well-known thresholding techniques [34] is an open question. The first and obvious question is: what happens if a thresholding algorithm is applied after our method (instead of just binarizing the final probability output)? At a first glance it should improve the results, but how? Is this combination worthwhile? Also, a more sophisticated variant of the M1 method, with a better thresholding function \u03c4 (using learned values for the thresholds, not just 0.5) can be studied. Or even, a combination of both proposals. As future work we would like to study the synergies between our proposal for improving multilabel classification and some thresholding techniques.\nAnother open question is the following: using the same approach, a different independence assumption than the one given in eq. (1) could be given, leading to other models. In particular, we would like to explore methods where the independence assumption is relaxed, because we think that a complete independence\nmay be unrealistic in some categories. Finally, we would like to use this methodology in a different environment. We have selected document categorization for validating this model because it is a natural form of multilabeled instances. In the future, we would like to work with different datasets as, for example, musical patterns, protein data or social network data, which we think are also suitable for this method.\nAcknowledgements This study has been jointly supported by the Spanish research programme Consolider Ingenio 2010 and the Consejer\u0301\u0131a de Innovacio\u0301n, Ciencia y Empresa de la Junta de Andaluc\u0301\u0131a, under the projects MIPRCV (CSD2007-00018) and P09-TIC-4526, respectively."}], "references": [{"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM T. Intel. Syst. Tech.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Combining instance-based learning and logistic regression for multilabel classification", "author": ["Weiwei Cheng", "Eyke H\u00fcllermeier"], "venue": "Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Knowledge discovery in Multi-Label phenotype data", "author": ["A. Clare", "R.D. King"], "venue": "In Proceedings of the 5th European Conference on Principles of Data Mining and Knowledge Discovery (PKDD", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Bayesian network models for hierarchical text classification from a thesaurus", "author": ["Luis M. de Campos", "Alfonso E. Romero"], "venue": "Int. J. Approx. Reason.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Bayes optimal multilabel classification via probabilistic classifier chains", "author": ["Krzysztof Dembczynski", "Weiwei Cheng", "Eyke H\u00fcllermeier"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "On label dependence and loss minimization in multi-label classification", "author": ["Krzysztof Dembczy\u0144ski", "Willem Waegeman", "Weiwei Cheng", "Eyke H\u00fcllermeier"], "venue": "Mach. Learn.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "An empirical study of multi-label learning methods for video annotation", "author": ["Anastasios Dimou", "Grigorios Tsoumakas", "Vasileios Mezaris", "Ioannis Kompatsiaris", "Ioannis Vlahavas"], "venue": "In Proceedings of the 2009 Seventh International Workshop on Content-Based Multimedia Indexing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Large-scale bayesian logistic regression for text", "author": ["Alexander Genkin", "David D. Lewis", "David Madigan"], "venue": "categorization. Technometrics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Discriminative methods for multilabeled classification", "author": ["Shantanu Godbole", "Sunita Sarawagi"], "venue": "Adv. Knowl. Disc. Data Min.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Ohsumed: an interactive retrieval evaluation and new large test collection for research", "author": ["William Hersh", "Chris Buckley", "T.J. Leone", "David Hickam"], "venue": "In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Text categorization with suport vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the 10th European Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "A comparison of two learning algorithms for text categorization", "author": ["D.D. Lewis", "M. Ringuette"], "venue": "In Symposium on Document Analysis and Information Retrieval.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Rcv1: A new benchmark collection for text categorization research", "author": ["David D. Lewis", "Yiming Yang", "Tony G. Rose", "Fan Li"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Evaluation of two systems on multiclass multi-label document classification", "author": ["Xiao Luo", "A. Nur Zincir-Heywood"], "venue": "In Proceedings of the 15th international conference on Foundations of Intelligent Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["Gjorgji Madjarov", "Dragi Kocev", "Dejan Gjorgjevikj", "Sa\u0161o D\u017eeroski"], "venue": "Pattern Recogn.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "A comparison of event models for naive bayes text classification", "author": ["Andrew McCallum", "Kamal Nigam"], "venue": "Workshope on Learning for Text Categorization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Multi-label text classification with a mixture model trained by em", "author": ["Andrew Kachites McCallum"], "venue": "In AAAI 99 Workshop on Text Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Computational approaches for protein function prediction: A survey", "author": ["Gaurav Pandey", "Vipin Kumar", "Michael Steinbach"], "venue": "Technical Report 06-028,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods", "author": ["J. Platt"], "venue": "In Advances in Large Margin Classifiers,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Snowball: A language for stemming algorithms", "author": ["Martin F. Porter"], "venue": "Published online,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Software available at http://sourceforge.net/projects/daurolab", "author": ["Alfonso E. Romero"], "venue": "Daurolab. Published online,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "The smart automatic document retrieval systems-an illustration", "author": ["G. Salton", "M.E. Lesk"], "venue": "Commun. ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1965}, {"title": "Machine learning in automated text categorization", "author": ["Fabrizio Sebastiani"], "venue": "ACM Comput. Surv.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Effective semantic annotation by image-to-concept distribution model", "author": ["Ja-Hwung Su", "Chien-Li Chou", "Ching-Yung Lin", "V.S. Tseng"], "venue": "IEEE T. Multimedia,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Classifying human motion quality for knee osteoarthritis using accelerometers", "author": ["Portia E. Taylor", "Gustavo J.M. Almeida", "Takeo Kanade", "Jessica K. Hodgins"], "venue": "Conf. Proc. IEEE Eng. Med. Biol. Soc,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Multilabel Classification of Music into Emotions", "author": ["K. Trohidis", "G. Tsoumakas", "G. Kalliris", "I. Vlahavas"], "venue": "In Proc. 9th International Conference on Music Information Retrieval (ISMIR", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Multi-label classification: An overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "IJDWM,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Mining multi-label data", "author": ["Grigorios Tsoumakas", "Ioannis Katakis", "Ioannis Vlahavas"], "venue": "In Data Mining and Knowledge Discovery Handbook,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Parametric mixture models for multilabeled text", "author": ["Naonori Ueda", "Kazumi Saito"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2003}, {"title": "True path rule hierarchical ensembles for genome-wide gene function prediction", "author": ["Giorgio Valentini"], "venue": "IEEE/ACM Trans. Comput. Biol. Bioinformatics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Folksonomy coinage and definition", "author": ["Thomas Vander Wal"], "venue": "Website, Februar", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques, Second Edition (Morgan Kaufmann Series in Data Management Systems)", "author": ["Ian H. Witten", "Eibe Frank"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Ranking-based emotion recognition for music organization and retrieval", "author": ["Yi-Hsuan Yang", "H.H. Chen"], "venue": "IEEE T. Audio Speech Lang. Proc.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "A study of thresholding strategies for text categorization", "author": ["Yiming Yang"], "venue": "In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2001}, {"title": "A re-examination of text categorization methods", "author": ["Yiming Yang", "Xin Liu"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}, {"title": "A k-nearest neighbor based algorithm for multi-label classification", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "In Granular Computing,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}], "referenceMentions": [{"referenceID": 22, "context": "Multilabel classification problems arise, in a natural way, in information processing, concretely on the subfield of automatic document categorization [23].", "startOffset": 151, "endOffset": 155}, {"referenceID": 11, "context": "For example, news articles can often belong to more than one category (this is the case of the Reuters-21578 [12] and the RCV1 [13] collections, which are composed of articles from the Reuters agency).", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "For example, news articles can often belong to more than one category (this is the case of the Reuters-21578 [12] and the RCV1 [13] collections, which are composed of articles from the Reuters agency).", "startOffset": 127, "endOffset": 131}, {"referenceID": 3, "context": "In other domains, multiple labels are assigned as metadata which give a better description of the documents (this occurs, for example, in scientific papers and legal documents, which have associated keywords from a controlled vocabulary, like the Mathematics Subject Classification or the MeSH Thesaurus in one case, and the Eurovoc thesaurus in the other case [4]).", "startOffset": 361, "endOffset": 364}, {"referenceID": 30, "context": "Furthermore, in collaborative environments (like folksonomies, [31]) where users can add tags, multilabel is an ordinary process.", "startOffset": 63, "endOffset": 67}, {"referenceID": 25, "context": "More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions [26, 33], scene or image categorization [7, 24], protein and gene function prediction [18, 30] or medical diagnosis [25].", "startOffset": 128, "endOffset": 136}, {"referenceID": 32, "context": "More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions [26, 33], scene or image categorization [7, 24], protein and gene function prediction [18, 30] or medical diagnosis [25].", "startOffset": 128, "endOffset": 136}, {"referenceID": 6, "context": "More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions [26, 33], scene or image categorization [7, 24], protein and gene function prediction [18, 30] or medical diagnosis [25].", "startOffset": 168, "endOffset": 175}, {"referenceID": 23, "context": "More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions [26, 33], scene or image categorization [7, 24], protein and gene function prediction [18, 30] or medical diagnosis [25].", "startOffset": 168, "endOffset": 175}, {"referenceID": 17, "context": "More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions [26, 33], scene or image categorization [7, 24], protein and gene function prediction [18, 30] or medical diagnosis [25].", "startOffset": 214, "endOffset": 222}, {"referenceID": 29, "context": "More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions [26, 33], scene or image categorization [7, 24], protein and gene function prediction [18, 30] or medical diagnosis [25].", "startOffset": 214, "endOffset": 222}, {"referenceID": 24, "context": "More recently, multilabel classification has been useful for different domains like, for instance, analysis of musical emotions [26, 33], scene or image categorization [7, 24], protein and gene function prediction [18, 30] or medical diagnosis [25].", "startOffset": 244, "endOffset": 248}, {"referenceID": 26, "context": "The problem of supervised multilabel classification (see, for instance [27]) deals with supervised learning, where the associated labels can be a set of unde-", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "This approach is called the binary relevance method [9, 27, 36], and it is often criticized for ignoring the existing correlations among labels.", "startOffset": 52, "endOffset": 63}, {"referenceID": 26, "context": "This approach is called the binary relevance method [9, 27, 36], and it is often criticized for ignoring the existing correlations among labels.", "startOffset": 52, "endOffset": 63}, {"referenceID": 35, "context": "This approach is called the binary relevance method [9, 27, 36], and it is often criticized for ignoring the existing correlations among labels.", "startOffset": 52, "endOffset": 63}, {"referenceID": 26, "context": "This taxonomy is given in [27], naming the former solutions problem transformation methods and algorithm adaptation the latter ones.", "startOffset": 26, "endOffset": 30}, {"referenceID": 2, "context": "5 tree learning algorithm has been proposed in [3] for multilabel classification.", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "In the lazy algorithms field, variations of the k-NN algorithm have been presented for this kind of problems [14, 36].", "startOffset": 109, "endOffset": 117}, {"referenceID": 35, "context": "In the lazy algorithms field, variations of the k-NN algorithm have been presented for this kind of problems [14, 36].", "startOffset": 109, "endOffset": 117}, {"referenceID": 1, "context": "Also, in [2] a k-NN is combined with a logistic regression classifier (in a different way that we do) to cope with multiple labels.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Of course, variations on the SVM algorithm are shown in [9], where both intraclass dependencies and an improvement of the definition of margin for multilabel", "startOffset": 56, "endOffset": 59}, {"referenceID": 16, "context": "In [17], a generative model is trained using training data, and completed with the EM algorithm, and computed the most probable vector of categories that should be assigned to the document.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "A generative model is also presented in [29].", "startOffset": 40, "endOffset": 44}, {"referenceID": 4, "context": "More recently [5] proposed a novel method, called probabilistic classifier chains (generalizing the classifier chains) which exploits label dependence, showing that the method outperforms others in terms of loss functions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "In fact, this issue of label dependence has been recently shown to be crucial in multilabel learning [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "This means that the components of the vector l\u0302j are values in [0, 1] which represent our degree of belief in these labels being assigned to the instance.", "startOffset": 63, "endOffset": 69}, {"referenceID": 11, "context": "First, Reuters-21578 (see [12], for instance) is a collection of 21578 news articles.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "The Ohsumed collection [10] is a set of 348566 references from MEDLINE, an online medical information database.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "This is the methodology followed in [11].", "startOffset": 36, "endOffset": 40}, {"referenceID": 12, "context": "Finally, the RCV1 corpus is a relatively more recent corpus (see [13]), also based on Reuters news stories.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "In the first two cases, the stopwords list used consists of 571 stopwords of the SMART retrieval system [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "Also, the English stemming algorithm of the Snowball package [20] was applied to resulting words.", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "In order to reduce the size of the lexicon, terms occurring in less than three documents were removed in Reuters-21578 and Ohsumed-23 (following the guidelines of [11]), and in less than five documents in RCV1, as done in [13].", "startOffset": 163, "endOffset": 167}, {"referenceID": 12, "context": "In order to reduce the size of the lexicon, terms occurring in less than three documents were removed in Reuters-21578 and Ohsumed-23 (following the guidelines of [11]), and in less than five documents in RCV1, as done in [13].", "startOffset": 222, "endOffset": 226}, {"referenceID": 20, "context": "All document preprocessing stage was made with DauroLab [21].", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Following the taxonomy considered in [15, 28], we have selected label-based measures, example-based and ranking measures.", "startOffset": 37, "endOffset": 45}, {"referenceID": 27, "context": "Following the taxonomy considered in [15, 28], we have selected label-based measures, example-based and ranking measures.", "startOffset": 37, "endOffset": 45}, {"referenceID": 14, "context": "See [15, 23] for more details.", "startOffset": 4, "endOffset": 12}, {"referenceID": 22, "context": "See [15, 23] for more details.", "startOffset": 4, "endOffset": 12}, {"referenceID": 15, "context": "For the first case, we selected the multinomial naive Bayes, in its binary version [16].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "For the second case, a k-NN classifier has been used, normalizing the output in order to obtain a value in [0, 1] which can be interpreted as a probability.", "startOffset": 107, "endOffset": 113}, {"referenceID": 18, "context": "Finally, a linear SVM, with probabilistic output, as performed by Platt\u2019s algorithm [19] was chosen.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "Thus, we chose k = 30 for Reuters and k = 45 for Ohsumed-23 (as set in [11]).", "startOffset": 71, "endOffset": 75}, {"referenceID": 12, "context": "For RCV1, a value of k = 100 was used [13].", "startOffset": 38, "endOffset": 42}, {"referenceID": 22, "context": "Also, following those references, we have performed feature selection in Reuters (1000 features selected by information gain, using the \u201csum\u201d combination [23]), and in RCV1 (8000 features selected by \u03c7, using the \u201cmax\u201d combination).", "startOffset": 154, "endOffset": 158}, {"referenceID": 10, "context": "No feature selection was performed in Ohsumed-23 as noted in [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "The implementation used for the algorithms was that contained in the DauroLab [21] package, except for the SVM where libsvm [1] was chosen.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "The implementation used for the algorithms was that contained in the DauroLab [21] package, except for the SVM where libsvm [1] was chosen.", "startOffset": 124, "endOffset": 127}, {"referenceID": 7, "context": "In order to have accurate estimates, and because the dimensionality of the problem p is high, the method selected to learn the weights is a Bayesian logistic regression, concretely the one proposed in [8], with Gaussian priors.", "startOffset": 201, "endOffset": 204}, {"referenceID": 31, "context": "The implementation used here is the one included with the Weka package, with the default parameters (see [32] for details).", "startOffset": 105, "endOffset": 109}, {"referenceID": 18, "context": "As a complement to the previous one, we have added the results obtained by using a linear support vector machine with probabilistic outputs (fitted using Platt\u2019s algorithm [19]).", "startOffset": 172, "endOffset": 176}, {"referenceID": 31, "context": ", the simplest SVM) without any further modification to the default parameters to those included in the canonical implementation of the Weka [32] SMO (Sequential Minimal Optimization) class.", "startOffset": 141, "endOffset": 145}, {"referenceID": 34, "context": "Both are presented in [35] and constitute the nowadays standard for comparing this kind of experiments.", "startOffset": 22, "endOffset": 26}, {"referenceID": 12, "context": "80570) was very close to the best result obtained by Lewis [13] with SVM (0.", "startOffset": 59, "endOffset": 63}, {"referenceID": 33, "context": "5) with any of the well-known thresholding techniques [34] is an open question.", "startOffset": 54, "endOffset": 58}], "year": 2013, "abstractText": "Multilabel classification is a relatively recent subfield of machine learning. Unlike to the classical approach, where instances are labeled with only one category, in multilabel classification, an arbitrary number of categories is chosen to label an instance. Due to the problem complexity (the solution is one among an exponential number of alternatives), a very common solution (the binary method) is frequently used, learning a binary classifier for every category, and combining them all afterwards. The assumption taken in this solution is not realistic, and in this work we give examples where the decisions for all the labels are not taken independently, and thus, a supervised approach should learn those existing relationships among categories to make a better classification. Therefore, we show here a generic methodology that can improve the results obtained by a set of independent probabilistic binary classifiers, by using a combination procedure with a classifier trained on the co-occurrences of the labels. We show an exhaustive experimentation in three different standard corpora of labeled documents (Reuters-21578, Ohsumed-23 and RCV1), which present noticeable improvements in all of them, when using our methodology, in three probabilistic base classifiers.", "creator": "LaTeX with hyperref package"}}}