{"id": "1202.3703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Factored Filtering of Continuous-Time Systems", "abstract": "We consider filtering for a continuous-time, or asynchronous, stochastic system where the full distribution over states is too large to be stored or calculated. We assume that the rate matrix of the system can be compactly represented and that the belief distribution is to be approximated as a product of marginals. The essential computation is the matrix exponential function of the distribution.\n\n\nThe following steps illustrate the general principle of stochastic stochastic system.\nLet\u2019s start from the bottom and find a formula, then assume it is a product of an exponential distribution of the distribution.\nThere is no real-time model to make this approximation.\nLet\u2019s look at a real-time model. Let\u2019s look at the distribution.\nNow it is not possible for us to model an exponential distribution of the distribution at the moment we are moving from the state to the state. We cannot say that if we choose to take the state as an exponential function, we would not have to look at the distribution. The linear curve is a log-like function, but we can also see that the distribution is only the product of the state that is determined by the state. The linear curve has its roots to the left side and is also the state that is determined by the state at the moment we are moving from state to state.\nWe must say that the linear curve is the distribution of the distribution at the moment we are moving from state to state, with the roots for the distribution at the moment we are moving from state to state, with the roots for the distribution at the moment we are moving from state to state.\nWe can use this function for the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle of the general principle", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (432kb)", "http://arxiv.org/abs/1202.3703v1", null]], "reviews": [], "SUBJECTS": "cs.SY cs.AI", "authors": ["e busra celikkaya", "christian r shelton", "william lam"], "accepted": false, "id": "1202.3703"}, "pdf": {"name": "1202.3703.pdf", "metadata": {"source": "CRF", "title": "Factored Filtering of Continuous-Time Systems", "authors": ["E. Busra Celikkaya", "Christian R. Shelton"], "emails": ["celikkae@cs.ucr.edu", "cshelton@cs.ucr.edu", "willmlam@ics.uci.edu"], "sections": [{"heading": null, "text": "We consider filtering for a continuous-time, or asynchronous, stochastic system where the full distribution over states is too large to be stored or calculated. We assume that the rate matrix of the system can be compactly represented and that the belief distribution is to be approximated as a product of marginals. The essential computation is the matrix exponential. We look at two different methods for its computation: ODE integration and uniformization of the Taylor expansion. For both we consider approximations in which only a factored belief state is maintained. For factored uniformization we demonstrate that the KL-divergence of the filtering is bounded. Our experimental results confirm our factored uniformization performs better than previously suggested uniformization methods and the mean field algorithm."}, {"heading": "1 Continuous-Time Markov Systems", "text": "We are interested in monitoring (alternatively tracking or filtering) a continuous-time finite-state homogeneous Markovian stochastic system. This implies that evidence and events can arrive at any real-valued time (asynchronously) and be either instantaneous or have real-valued time durations. The system is stationary and has discrete states which means that a sample (trajectory) for the system consists of a series of times at which the system jumped from one state to another.\nSuch models are common in the queueing theory and verification literatures. The algorithms developed there almost exclusively focus on steady-state properties of the system. A singular exception is the work of Sutton and Jordan (2008) which applied Gibbs sampling to queueing models. By contrast, the continuous\ntime Bayesian network (Nodelman et al., 2002) literature has focused on finite-time properties, so we compare against those algorithms.\nIn verification, continuous-time models can be used to compute the probability a system will function by a given time, such as in continuous stochastic logic (Baier et al., 2003). Other problems of interest include state estimation in asynchronous systems, such as distributed computer systems (Xu and Shelton, 2010), robotics (Ng et al., 2005), social networks (Fan and Shelton, 2009), or phylogenetic trees (Cohn et al., 2009)."}, {"heading": "1.1 Parameterization", "text": "Such a Markovian system is described by an initial distribution over the state space and an intensity (or rate) matrix, often denoted Q. The diagonal element qii = \u2212qi where qi \u2265 0 is the rate of leaving state i. This means that the density of the duration of the process remaining in state i for exactly a duration \u2206t is an exponential distribution: qieqi\u2206t. The non-diagonal elements, qij \u2265 0 are the rates of transitioning from state i to state j. qi = \u2211 j qij in a closed system. (The rate of leaving a state is equal to the sum of the rates of moving to any other state.) The probability of transitioning to state j immediately upon leaving state i is qij/qi. Note that the diagonal elements of Q are non-positive and the non-diagonal elements are non-negative. The sum of each row is 0.\nMonitoring such a system consists of keeping track of the probability distribution over the state at the current time t, given all evidence prior to t. We would like a recursive solution in which evidence before t can be discarded once the distribution (or its estimate) at t has been computed. As an initial problem, we will be interested in tracking in the absence of any evidence. We will cover how to incorporate evidence in Section 5.\nTo simplify notation, we will assume that we have the distribution at time 0 and wish to propagate this dis-\ntribution to time t. If p is the distribution at time 0, represented as a row vector, then p\u2032, the distribution at t also represented as a row vector, is\np\u2032 = peQt\nwhere eQt is the matrix exponential of Qt. For a few hundred states, this computation is tractable. Yet, the number of states of a system grows exponentially with the number of properties required to describe the system\u2019s state. Very quickly it becomes impossible to represent the matrix Q or even the result p\u2032 exactly. It should be noted that almost any useful structure that might be imposed on Q is destroyed by the matrix exponential. Kronecker-sum constructions of Q are the sole exception, but they represent processes with completely independent sub-processes and are therefore not interesting on their own.\nPrevious results have looked at sparse representations of p and p\u2032 (Sidje et al., 2007). These work well for systems with very tightly coupled components in which there are only a few joint assignments to the system properties that have large probability at any one time. By contrast, in this work we are interested in systems with many (relatively) weakly interacting components. Therefore, instead of approximating p and p\u2032 with sparse vectors, we will approximate them with factored representations.\nIn order to have a factored representation, any state must be able to be described as a set of properties or variables. We let Xi represent variable i (out of n); an assignment to Xi is denoted xi; and therefore a joint assignment (a complete state) x = (x1, . . . , xn). We will, therefore, be considering the case of calculating p\u2032 = peQt when p is approximated as p\u0302(x) = \u220f i p\u0302i(xi)\nand p\u2032 is similarly approximated as p\u0302\u2032(x) = \u220f\ni p\u0302 \u2032 i(xi).\nExample. Our running example will use a 4-state system described by the rate matrix\nQ = 264\u22124 1 3 02 \u22127 0 54 0 \u22125 1 0 6 2 \u22128 375 . If the distribution at t = 0 is p = [ 0.4 0.1 0.2 0.3 ] , the marginal distribution at t = 0.5 is\np\u2032 = pe0.5Q = p 2640.4313 0.1443 0.3098 0.11470.2958 0.2648 0.2221 0.21730.4106 0.1406 0.3305 0.1183 0.2886 0.2622 0.2293 0.2199 375 = [0.3708 0.1910 0.2810 0.1572] ."}, {"heading": "2 Matrix Exponential Calculations", "text": "Moler and Loan (2003) give an excellent description of the numeric difficulties in calculating the matrix expo-\nnential. We concentrate here on two of the principal methods for calculating peQt."}, {"heading": "2.1 ODE", "text": "One method is to rewrite f(t) = peQt as the ordinary differential equation\nf\u0307(t) = f(t)Q f(0) = p\nand solve using ODE integration methods. The most common ODE solver is the Runge-Kutta-Fehlberg (RKF) method which which adapts the step size to the current error, thereby allowing for quick progress at times of slow system change. The fundamental calculation is the multiplication of a current estimate of the distribution by Q to calculate the time derivative."}, {"heading": "2.2 Uniformization", "text": "Uniformization is a transformation of a continuoustime Markovian system into a discrete-time one. However, it does not correspond to constructing either the embedded Markov chain of the continuous-time process, nor to time-slicing the system at regular intervals. It is equivalent to sampling the intervals between potential state changes from an exponential with rate \u03b1 and then sampling a suitable Markov chain just at these time points (with stochastic matrix M), so that the resulting distribution over trajectories matches the original continuous-time Markov system.\nMathematically we can construct \u03b1 and M as follows. We express the rate matrix Q as\nQ = \u03b1(M \u2212 I)\nso that\u2014provided \u03b1 \u2265 maxi qi\u2014M is a stochastic matrix: all elements are on [0, 1] and the rows sum to 1. Ideally \u03b1 should be as small as possible as it represents the rate of the process sampling the intervals. Note that while the continuous-time process never has a \u201cself-transition\u201d explicitly, M does have non-zero diagonal elements (corresponding to those states whose rates of leaving are not maximal).\nThis can be applied to the matrix exponential calculation as\neQt = e\u03b1(M\u2212I)t = e\u03b1tMe\u2212\u03b1t .\nIn the context of the matrix exponential, this transformation is usually performed to remove the negative elements in the resulting Taylor expansion and thereby stabilize the calculation. We wish to calculate\npeQt = pe\u2212\u03b1te\u03b1tM = e\u2212\u03b1t \u221e\u2211 k=0 (\u03b1t)k k! pMk . (1)\nGiven that M is a stochastic matrix, p\u2032 is a infinite mixture of distributions p, pM, pM2, . . . with weights e\u2212\u03b1t, e\u2212\u03b1t\u03b1t, e\u2212\u03b1t \u03b1t2 , . . . . Each distribution corresponds to k steps of the Markov chain of M and the weights are the probabilities that exactly k steps happen in a time period of t. These distributions are calculated recursively, so the important subcalculation is the multiplication of a state distribution by M .\nExample. Uniformization on the Q matrix results in \u03b1 = 8 and\nM = 2640.5 0.125 0.375 00.25 0.125 0 0.6250.5 0 0.375 0.125 0 0.75 0.25 0 375 ."}, {"heading": "2.3 Approximate Versions", "text": "Both of these methods involve the calculation of state distributions and their multiplication by either Q or M . We will use v to denote any state distribution vector generated during the course of such a calculation. For the ODE method, v is a particular point and vQ is its time derivative. For uniformization, v is an element of the sum and vM is the next element.\nWhile we might assume that the problem specification is compact and thereby assume that p and Q have compact representations, for a system with many variables, it is usually not possible to express intermediate v values exactly, as structure that may exist in p and Q does not exist in v. Therefore, the most direct method for constructing an approximate filtering algorithm is to keep v restricted to a smaller representation.\nIn the case for uniformization, Sidje et al. (2007) proposed keeping a sparse representation for v (by dropping elements less than a given threshold). If Q has a sparse representation (only a small fraction of any row are non-zero) and any given row can be generated as needed, this results in an algorithm with nonexponential running time (in n, the number of variables). We call this method sparse uniformization.\nSparse uniformization works well for problems in which the distribution to be tracked is highly peaked. However, in problems in which the variables may be more loosely coupled (and therefore an assignment to one does not necessarily dictate a joint assignment to all), this approximation will either be poor, or it will require a large number of states to be tracked, thereby defeating the quick runtime.\nFor such systems, a factored representation of v (as suggested above) is more suitable. This naturally suggests two approximate algorithms. First, we might force the RKF integrator to project v to the space of factored distributions. Second, we might force the\nuniformization method to project v to the space of factored distributions. We call these factored RKF and factored uniformization."}, {"heading": "2.4 Notation", "text": "We will concentrate on approximate calculations of vM (vQ is similar). We consider one subcalculation in the form v\u2032 = vM . Let v\u0302 be the current approximation of v in factored form: v\u0302(x) = \u220f i v\u0302i(xi). Similarly, let v\u0302\u2032 = v\u0302M , the result of multiplying v\u0302 by M , which is not necessarily completely factored. Finally let v\u0303 be the projection of v\u0302\u2032 to the set of factored distributions.\nLetM\u22a5 be the operator that both multiplies byM and projects to the space of factored distributions. Thus v\u0303 = v\u0302M\u22a5 and is a factored distribution (v\u0302\u2032 is not).\nWe abuse notation and let any vector also stand for the distribution it embodies. Further, for any vector v, we let vi be the marginal distribution of v over the variable xi (even if v is not factored), v\u2212i be the marginal distribution over all variables except xi, vui be the marginal distribution over all of xi\u2019s parents, and vi|\u2212i be the distribution over xi conditioned on all the other variables."}, {"heading": "3 Factored Rate Matrix", "text": "For either factored approximate method, we need to be able to calculate the projection of vM (or vQ) onto the space of factored distributions without explicitly generating vM as an exponentially large vector.\nWhile our methods are applicable to Petri nets (Petri, 1962), edge-valued decision diagrams (Wan et al., 2011), and other compact representation of the rate matrix, we focus on using a continuous time Bayesian network (CTBN) to representQ. We note that a correspondence between CTBNs and Petri nets has already been established (Raiteri and Portinale, 2009)."}, {"heading": "3.1 Continuous Time Bayesian Network", "text": "A continuous time Bayesian network (CTBN) (Nodelman et al., 2002) has an initial distribution described by a Bayesian network that is not of direct importance to this work. It has a factored representation of the matrix Q. Each variable\u2019s dynamics depend only on a subset of the other variables (which we denote as parents). Let U i be the parents of Xi. Then for every assignment ui to U i there exists an intensity matrix QXi|ui of dimension equal to the number of states of Xi. It describes the rates of change for Xi when its parents equal ui. If we let \u03b4(x,x\u2032) be the set of variable indexes for which the assignments in x and x\u2032\ndiffer, the complete Q matrix is\nQ(x,x\u2032)=  \u2211 kQXk|uk(xk, x \u2032 k) if x=x \u2032 QXj |uj (xj , x \u2032 j) if \u03b4(x,x\n\u2032)={j} 0 otherwise.\nSo Q is sparse (mostly zeros) with non-zero elements on the diagonal and where only one variable changes. It has the form of a sum of factors (compared with a Bayesian network\u2019s product of factors).\nExample. The Q matrix of our example can be represented by a CTBN of two variables: A (with values a0 and a1) and B (with values b0 and b1). A has no parents and is the parent of B. If we let\nQA = \u00bb \u22121 1 2 \u22122 \u2013 , QB|a0 = \u00bb \u22123 3 4 \u22124 \u2013 , QB|a1 = \u00bb \u22125 5 6 \u22126 \u2013 then the full system has the previous Q matrix if the global states are ordered a0b0, a1b0, a0b1, a1b1.\nTo construct the uniformized matrix M , we would like to select \u03b1 to be the maximally negative diagonal element. In practice this could be a difficult optimization problem. So, instead we select \u03b1 = \u2211n i=1 \u03b1i where \u03b1i = \u2212minui minxi QXi|ui(xi, xi), the maximally negative diagonal element for Xi for any ui. We let MXi|ui = QXi|ui/\u03b1i + I be the stochastic matrix we obtain for Xi with parent assignment ui using uniformization constant \u03b1i. The stochastic matrixM can then be described as\nM(x,x\u2032)=  \u2211 k M\u0303Xk|uk(xk, x \u2032 k) if x=x \u2032 M\u0303Xj |uj (xj , x \u2032 j) if \u03b4(x,x\n\u2032)={j} 0 otherwise.\nwhere M\u0303Xi|ui = \u03b1i \u03b1 MXiui + \u03b1\u2212\u03b1i \u03b1 I. The stochastic matrix M cannot be described as a dynamic Bayesian network (DBN): it has an additive structure, not a multiplicative one, and does not allow the transition of multiple variables. However, M can be described as a mixture of particularly simple DBN transition models: M = \u2211 i \u03b1i \u03b1 Mi where\nMi(x,x\u2032)=  MXi|ui(xi, x \u2032 i) if x=x \u2032 MXi|ui(xi, x \u2032 i) if \u03b4(x,x\n\u2032)={i} 0 otherwise.\nIn particular, it describes a mixture in which with probability \u03b1i\u03b1 variable i transitions according to MXi|Ui and all other variables remain the same. The ith mixture DBN has a structure in which all variables x\u2032j , where j 6= i, have only xj (the same node in the previous slice) as a parent. Variable x\u2032i has the same parents as in the CTBN, with the addition of xi. No intra-slice arcs exist.\nExample. \u03b1A = 2, \u03b1B = 6 and so \u03b1 = 8 (as before). We can decompose the same M as before into\nMA= \u00bb 0.5 0.5 1 0 \u2013 ,MB|a0 = \u00bb 0.5 0.5 0.67 0.33 \u2013 ,MB|a1 = \u00bb 0.17 0.83 1 0 \u2013 which can be viewed as a Markov chain in which with probability 28 A transitions according to MA, otherwise B transitions according to MB|a0 or MB|a1 ."}, {"heading": "3.2 Calculating Factored vM", "text": "Given the mixture-of-DBN interpretation above of M , it is not surprising that v\u0303 = v\u0302M\u22a5 can be calculated in one step without constructing v\u0302M . As Q has the same structure, the same method works for it too.\nAs projection onto a particular variable, xj , is linear, if M\u22a5j is the composition of M and the projection onto xj , then M\u22a5j = \u2211 i \u03b1i \u03b1 Mi\u22a5j where Mi\u22a5j is the composition of Mi and projection operation onto xj . Furthermore, in Mi only variable i can change:\nv\u0303j = v\u0302M\u22a5j = \u2211 i \u03b1i \u03b1 v\u0302Mi\u22a5j = (1\u2212 \u03b1j \u03b1 )v\u0302j + \u03b1j \u03b1 v\u0302Mj\u22a5j .\nMj\u22a5j is the marginal over xj after a transition according to Mj . No other variable changes, so this is the expectation of MXj |Uj over v\u0302\u2019s distribution over U j :\nv\u0303j = v\u0302j [ (1\u2212 \u03b1j\n\u03b1 )I + \u03b1j \u03b1 \u2211 uj v\u0302uj (uj)MXj |uj ] = v\u0302j [\u2211 uj v\u0302uj (uj)M\u0303Xj |uj ] (2)\nTo calculate v\u0302Q and project onto xj , Equation 2 holds if we change M\u0303Xj |uj to QXj |uj . Example. Starting distribution p marginals v\u0302A =[ 0.6 0.4 ] and v\u0302B = [ 0.5 0.5 ] . Multiplying this factored approximation by M and then projecting can be done by multiplying vA by M\u0303A and vB by 0.6M\u0303B|a0 + 0.4M\u0303B|a1 :\nM\u0303A= \u00bb .875 .125 .25 .75 \u2013 , M\u0303B|a0= \u00bb .625 .375 .5 .5 \u2013 , M\u0303B|a1= \u00bb .375 .625 .75 .25 \u2013 ."}, {"heading": "4 Bounds for Factored Uniformization", "text": "While we have not found a suitable way of bounding the approximation error for factored RKF, we can derive bounds similar to those of the BK algorithm (Boyen and Koller, 1998) for discrete-time stochastic processes to bound the error in propagation and projection through a single M matrix. However, because the process is a mixture of processes in which only a single component changes, the BK result for compound processes does not carry over.\nWe then use this bound to bound the error of the entire Taylor expansion and thereby the factored uniformization method."}, {"heading": "4.1 Divergence Bound for Single Step", "text": "We first concentrate on bounding the error of a single multiplication by M . We wish to show that the KL-divergence between v\u2032 = vM and v\u0302\u2032 = v\u0302M is no greater than that between v and v\u0302.\nWe begin with a simple property of the KL-divergence. The proof is omitted, but is a consequence of the fact that entropy does not increase upon conditioning.\nLemma 1. If q(x) = \u220f\ni q(xi) is a factored distribution and p(x) is a (non-factored) distribution over the same sample space, and x\u2212i is the set of all variables except xi,\u2211\niDKL(p(xi | x\u2212i)\u2016q(xi)) \u2265 DKL(p(x)\u2016q(x)) .\nWe require a mixing rate definition from Boyen and Koller (1998):\nDefinition 2. The mixing rate of a stochastic matrix M is defined as \u03b3 , mini1,i2 \u2211 j min(Mi1,j ,Mi2,j).\nWe can then state that M is a contraction mapping with respect to the KL-divergence:\nTheorem 3. Let \u03b3i be the minimum (over ui) mixing rate of the stochastic matrix MXi|uiand \u03b3 = mini \u03b1i\u03b3i \u03b1 . Then, DKL(v\u2032\u2016v\u0302\u2032) \u2264 (1\u2212 \u03b3)DKL(v\u2016v\u0302) .\nProof. If we let v\u2032(i) = vMi and v\u0302\u2032(i) = v\u0302Mi, then\nDKL(vM\u2016v\u0302M) = DKL( \u2211 i \u03b1i \u03b1 v \u2032(i)\u2016 \u2211 i \u03b1i \u03b1 v\u0302 \u2032(i))\n\u2264 \u2211\ni \u03b1i \u03b1 DKL(v \u2032(i)\u2016v\u0302\u2032(i)) = \u2211\ni \u03b1i \u03b1 ( DKL(v \u2032(i) \u2212i \u2016v\u0302 \u2032(i) \u2212i ) +DKL(v \u2032(i) i|\u2212i\u2016v\u0302 \u2032(i) i|\u2212i) ) \u2264 \u2211\ni \u03b1i \u03b1 ( DKL(v\u2212i\u2016v\u0302\u2212i) + (1\u2212\u03b3i)DKL(vi|\u2212i\u2016v\u0302i|\u2212i) ) = \u2211\ni \u03b1i \u03b1 ( DKL(v\u2016v\u0302)\u2212 \u03b3iDKL(vi|\u2212i\u2016v\u0302i|\u2212i) ) \u2264 DKL(v\u2016v\u0302)\u2212 \u03b3 \u2211 iDKL(vi|\u2212i\u2016v\u0302i) \u2264 DKL(v\u2016v\u0302)\u2212 \u03b3DKL(v\u2016v\u0302)\nThe first inequality is from the convexity of the KL-divergence (Cover and Thomas, 1991, Theorem 2.7.2). The next inequality holds because Mi does not change any variables except xi and the conditional KL-divergence of xi contracts by \u03b3i (Boyen and Koller, 1998, Theorem 3). The final inequality is due to the lemma above.\nNote that unlike in BK, the global contraction rate (\u03b3) does not depend on the in- or out-degree of model, but it is inversely proportional to n. This appears\nunfortunate, but the next section demonstrates that it is not a problem for the contraction rate of the entire Taylor expansion.\nWe upper-bound the increase from projection:\nDKL(vM\u2016v\u0302M\u22a5)\u2212DKL(vM\u2016v\u0302M) \u2264 .\nAs a crude upper bound, \u2264 \u2212(n\u22121) ln \u03b7 where \u03b7 is the smallest marginal probability. As shown by Boyen and Koller (1999), better bounds can be placed by more careful analysis or considering the average case.\nTaken together this means that after a single multiplication and projection, the KL-divergence error of our estimate can be bounded as\nDKL(vM\u2016v\u0302M\u22a5) \u2264 (1\u2212 \u03b3)DKL(v\u2016v\u0302) + . (3)\nExample. We have \u03b3A = 0.5, \u03b3B|a0 = 0.5, \u03b3B|a1 = 0.17. Therefore \u03b3 = min(0.5\u00d7 28 , 0.17\u00d7 6 8 ) = 0.125. Therefore the KL-divergence between the true answer and the factored approximation contracts by 1 \u2212 0.125 = 0.825 for each multiplication by M ."}, {"heading": "4.2 Bound on Approximate Taylor Expansion", "text": "Our goal is not to bound the error on a single step, but rather the error of our entire approximation to the matrix exponential. Equation 3 implies that\nDKL(vMk\u2016v\u0302Mk\u22a5) \u2264 (1\u2212\u03b3)kDKL(v\u2016v\u0302) + k\u22121\u2211 i=0 (1\u2212\u03b3)i\n= (1\u2212\u03b3)kDKL(v\u2016v\u0302) + 1\u2212(1\u2212\u03b3) k\n\u03b3 (4)\nIf we combine the bound from Equation 4 with the Taylor expansion of Equation 1, we can obtain a bound on the KL-divergence between the true matrix exponential, and an approximation of the Taylor expansion in which each vector of probabilities is approximated by a factored form and only the first l terms are evaluated (and then renormalized): Theorem 4. Let \u03b1, , and \u03b3 be as defined above. Let p be an arbitrary distribution over the state space of the process. Let p\u0302 be an arbitrary factored distribution over the same. Further, let p\u2032 = peQt be the distribution at time t in the future, and let p\u0302\u2032 =\n1 1\u2212Rl \u2211l k=0 e \u2212\u03b1t (\u03b1t)k k! p\u0302M k \u22a5 be the approximation of p \u2032 constructed by uniformization of a Taylor expansion truncated to l terms in which each matrix multiplication is projected back to the space of factored distributions. Rl = \u2211\u221e k=l+1 e \u2212\u03b1t (\u03b1t)k k! . Then\nDKL(p\u2032\u2016p\u0302\u2032) \u2264 e\u2212\u03b3\u03b1tDKL(p\u2016p\u0302)+ \u03b3 (1\u2212e\u2212\u03b3\u03b1t)+Rl(\u03b4+ \u03b3 )\nwhere \u03b4 = maxx \u2212 ln p\u0302\u2032x, the maximum negative log probability over any joint assignment in the final approximate calculation.\nProof. For compactness of presentation, let \u03b2k = e\u2212\u03b1t (\u03b1t) k\nk! , \u03b3\u0304 = (1\u2212 \u03b3), and R\u0304l = (1\u2212Rl). Then\nDKL(p\u2032\u2016p\u0302\u2032) = DKL( \u2211\u221e k=0\u03b2kpM k\u2016 1\nR\u0304l\n\u2211l k=0\u03b2kp\u0302M k \u22a5)\n\u2264 R\u0304lDKL( 1R\u0304l \u2211l k=0\u03b2kpM k\u2016 1 R\u0304l \u2211l k=0\u03b2kp\u0302M k \u22a5)\n+RlDKL( 1Rl \u2211\u221e k=l+1e \u2212\u03b1t\u03b2kpM k\u2016 1 R\u0304l \u2211l k=0\u03b2kp\u0302M k \u22a5)\n\u2264 \u2211l\nk=0\u03b2kDKL(pM k\u2016p\u0302Mk\u22a5) +Rl\u03b4 \u2264 \u2211l\nk=0\u03b2k[\u03b3\u0304 kDKL(p\u2016p\u0302) + (1\u2212 \u03b3\u0304k)/\u03b3] +Rl\u03b4\n\u2264 e\u2212\u03b3\u03b1tDKL(p\u2016p\u0302) + \u03b3 (1\u2212 e \u2212\u03b3\u03b1t) +Rl(\u03b4 + \u03b3\u0304\nl\n\u03b3 ) .\nThe first inequality is due to the convexity of the KLdivergence in only the first argument. The second is due to the convexity of the KL-divergence in both arguments, and that the KL-divergence is bounded by the negative log of smallest probability of the second argument. The third is due to Theorem 3. The final is due to bounding a finite Taylor expansion of an exponential by the exponential.\nNote that Rl goes to zero as l grows. In the remaining terms, \u03b3 almost always appears multiplied by \u03b1. In the previous section, we commented on how \u03b3 is inversely proportional to n. However, \u03b1 is proportional to n and they exactly cancel out. Thus if l is large enough, we can let \u03b3\u2032 = mini \u03b1i\u03b3i and conclude\nDKL(p\u2032\u2016p\u0302\u2032) \u2264 e\u2212\u03b3 \u2032tDKL(p\u2016p\u0302) +\n\u03b1 \u03b3\u2032 (1\u2212 e\u2212\u03b3 \u2032t) . (5)\nThus the contraction rate for the full approximation is not a function of n. It is unclear how \u03b1 scales with n.\nWe are then left with two error terms. The first decays with t and the second grows with t (but is bounded). The resulting distribution p\u0302\u2032 is a mixture of factored distributions. So, if filtering is to continue past time t, p\u0302\u2032 must be projected back to the space of factored distributions, introducing another similar additive error. This may happen either because evidence arrives at t and requires conditioning, or because we may wish to subdivide a propagation from 0 to t into m propagations of length t/m. Equation 5 implies this is not helpful for accuracy (if one considers applying the bound recursively m times over intervals of length t/m). However, our experimental results show that some interval subdivision is helpful.\nExample. \u03b3\u2032 = \u03b3\u03b1 = 1. For large l, the KLDivergence between the true distribution and the factored approximation before (DKL) and after propagation to t = 0.5 (D\u2032KL) is related by\nD\u2032KL \u2264 0.61DKL + (8 /1)(1\u2212 0.61)\nwhere we note that e\u22121\u00d70.5 \u2248 0.61. If the smallest possible marginal probability is 0.01, then a crude upperbound on is \u2212 ln 0.01 = 4.6."}, {"heading": "5 Adding Evidence", "text": "So far, we have discussed only how to filter without evidence. In a continuous-time process, evidence can take on two forms. First, it can be point evidence. That is, at time t we observe the values of certain variables, but for no duration. We propagate to the time of the evidence, condition the distribution on the evidence, and then continue. In this case, conditioning on the evidence in expectation reduces the error (Boyen and Koller, 1998, Fact 1).\nThe second form of evidence is interval evidence: a variable remains in a particular state from t1 to t2. In this case, at t1 we condition on the evidence (same as for point evidence, above). From t1 until t2, we monitor using a modified Q matrix in which all transitions where the evidence variable changes are set to 0 (but the diagonal elements remain unchanged). The resulting p\u0302 sums to the probability of the interval evidence, but it can be normalized to yield the conditional distribution of the state. The normalization makes the analysis difficult. However, we conjecture that interval evidence will also not increase the error in expectation.\nFinally we note that the sparse uniformization method can have serious difficulties with evidence if none of the maintained states are consistent with the evidence.\nExample. Given the previous distribution p for t = 0, and B = b0 on t = [0.5, 1), we propagate p to t = 0.5 using Equation 1 (bounded), with Equation 2 to calculate the projected multiplications. We get the factored distribution p\u0302A = [ 0.65 0.35 ] , p\u0302B = [ 0.56 0.44 ] .\nWe condition on B = b0 by setting p\u0302B = [ 1 0 ] . Then we similarly propagate to t = 1, but using\nQA = \u00bb \u22121 1 2 \u22122 \u2013 , QB|a0 = \u00bb \u22123 0 0 0 \u2013 , QB|a1 = \u00bb \u22125 0 0 0 \u2013 for which \u03b1A = 2, \u03b1B = 5, and \u03b1 = 7:\nMA= \u00bb 0.5 0.5 1 0 \u2013 ,MB|a0 = \u00bb 0.4 0 0 1 \u2013 ,MB|a1 = \u00bb 0 0 0 1 \u2013 ."}, {"heading": "6 Experimental Results", "text": "We employed two synthetic networks and a network built from a real data set in our evaluations. Since exact filtering is intractable for large models, we limited the number of variables to allow for calculations of the true approximation errors.\nThe synthetic networks we use are the ring and toroid dynamic Ising networks of 20 binary variables from ElHay et al. (2010). The ring network is bidirectional, while the toroid is directed. Nodes try to track their parents with a coupling parameter, \u03b2, indicating the\nstrength of the influence, and a rate parameter \u03c4 that is inversely proportional to the expected time between switching. We set \u03c4 = 4 and \u03b2 = 1. We set both networks to have a deterministic starting distribution. For the toroid the first 5 variables are in state 0 and the remaining variables are in state 1. The ring network\u2019s initial distribution is the reverse.\nThe real network we used was constructed from the British Household Panel Survey (BHPS) data set (Economic & Social Research Council, 2003). The data set records major life changes from a set of roughly 8,000 British citizens in areas including household organization, employment, income, wealth and health. We use the same network model as in Fan et al. (2010) and Nodelman et al. (2005) which chooses 4 variables: employment (student, employed, unemployed), children (0, 1,\u22652), married (not married, married) and smoking (non-smoker, smoker) and adds a hidden binary variable for each (Figure 1a). The structure and parameters of both the initial distribution and the dynamics were learned by the structural EM algorithm (Nodelman et al., 2005) and we used the learned network model for our experiments."}, {"heading": "6.1 KL-Divergence Bound", "text": "Our first experiment tested the theoretic error bound. Figure 1 shows the KL-divergence between the true marginals and the marginals computed by factored uniformization for the BHPS network and the toroid network. The bound on the KL-divergence of the full distribution is also a bound on any marginal, and experimentally the errors on the marginals grow initially and then asymptote, as per Theorem 4."}, {"heading": "6.2 Approximation Comparison", "text": "We then compared our factored uniformization method (UF) to other approximations. In particular, we compared to factored RKF (RKFF), as explained in Section 2.1, sparse uniformization (US), and the mean field (MF) approach of Cohn et al. (2009) for CTBNs. For the purpose of comparison, the MF method was\nextended to accept evidence on subsets of variables.\nWe varied an error tolerance parameter for each method to map the trade-off between error and runtime. For UF, we varied the uniformization-specific parameter \u03b8 that determines the number of intervals of propagation (see Sidje et al., 2007). We fixed the number of terms of the Taylor series expansion (l) to a value that performed reasonably. Similarly, for sparse uniformization we varied \u03b8 and fixed l to a wellperforming value. MF and RKFF both use RKF for integration, so we varied the error tolerance of RKF.\nThe evidence for the ring and toroid networks was set to be relatively unexpected: for t \u2208 [0.5, 1.0) x0 = 1 and x1 = 0. For the ring, we queried the distribution of x10 (far from the evidence) and x19 (adjacent to the evidence) at time t = 1. For the toroid, we queried the distribution of x6 (adjacent to the evidence) and x13 (a node more the in middle) at t = 1. For the BHPS network, we chose evidence where employment is observed to be in the student state continuously from year 1 to year 5. We queried the marginal distribution of the variables smoking and children at t = 5 years.\nFigure 2 shows the KL-divergence between the true and approximated marginals of the query variable for each of the three networks for each query. These results are typical of other query marginals. In general, factored uniformization (UF) performs the best, especially for smaller running times (with US occasionally being the method of choice for longer running times). However, when the query node is close to the evidence, RKFF performs better (x10 for the ring and x13 for the toroid network are examples). The advantage of US for longer running times is predictable as it approaches the true value as more states are retained. Finally, note that for the BHPS query over the children variable, US has infinite KL-divergence (and hence does not appear on the plot). This is because all of the retained states have this variable equal to 0. While this is certainly the most common value at t = 5, it does not have probability 1 under the true distribution and thus the KL-divergence between the true distribution and the approximation of US is infinite."}, {"heading": "7 Conclusion", "text": "We have demonstrated approximate continuous-time filtering based on uniformization. It is simple to implement, and we have proven bounds on the KLdivergence of its error. The approximation can be made more accurate by lumping together variables into joint marginals. The bounds are similar in style to those of BK and also depend on the mixing time of the individual components (adjusted for the continuoustime nature of the system). Our experimental results demonstrate that the theoretic bounded error holds in practice. Furthermore, our method gives superior time-accuracy trade-offs for most of the examples tested in this paper."}], "references": [{"title": "Model checking algorithms for continuous-time Markov chains", "author": ["C. Baier", "B. Haverkort", "H. Hermanns", "Katoen", "J.-P."], "venue": "IEEE Trans. on Soft. Eng., 29(6):524\u2013 541.", "citeRegEx": "Baier et al\\.,? 2003", "shortCiteRegEx": "Baier et al\\.", "year": 2003}, {"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "UAI, pages 33\u201342.", "citeRegEx": "Boyen and Koller,? 1998", "shortCiteRegEx": "Boyen and Koller", "year": 1998}, {"title": "Exploiting the architecture of dynamic systems", "author": ["X. Boyen", "D. Koller"], "venue": "AAAI, pages 313\u2013320.", "citeRegEx": "Boyen and Koller,? 1999", "shortCiteRegEx": "Boyen and Koller", "year": 1999}, {"title": "Mean field variational approximation for continuous-time Bayesian networks", "author": ["I. Cohn", "T. El-Hay", "R. Kupferman", "N. Friedman"], "venue": "UAI.", "citeRegEx": "Cohn et al\\.,? 2009", "shortCiteRegEx": "Cohn et al\\.", "year": 2009}, {"title": "Elements of Information Theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "John Wiley & Sons, Inc.", "citeRegEx": "Cover and Thomas,? 1991", "shortCiteRegEx": "Cover and Thomas", "year": 1991}, {"title": "Continuous-time belief propagation", "author": ["T. El-Hay", "I. Cohn", "N. Friedman", "R. Kupferman"], "venue": "ICML, pages 343\u2013350.", "citeRegEx": "El.Hay et al\\.,? 2010", "shortCiteRegEx": "El.Hay et al\\.", "year": 2010}, {"title": "Learning continuoustime social network dynamics", "author": ["Y. Fan", "C.R. Shelton"], "venue": "UAI.", "citeRegEx": "Fan and Shelton,? 2009", "shortCiteRegEx": "Fan and Shelton", "year": 2009}, {"title": "Importance sampling for continuous time Bayesian networks", "author": ["Y. Fan", "J. Xu", "C.R. Shelton"], "venue": "JMLR, 11(Aug):2115\u20132140.", "citeRegEx": "Fan et al\\.,? 2010", "shortCiteRegEx": "Fan et al\\.", "year": 2010}, {"title": "Nineteen dubious ways to compute the exponential of a matrix, twentyfive years later", "author": ["C. Moler", "C.V. Loan"], "venue": "SIAM Review, 45(1):3\u201349.", "citeRegEx": "Moler and Loan,? 2003", "shortCiteRegEx": "Moler and Loan", "year": 2003}, {"title": "Continuous time particle filtering", "author": ["B. Ng", "A. Pfeffer", "R. Dearden"], "venue": "IJCAI, pages 1360\u20131365.", "citeRegEx": "Ng et al\\.,? 2005", "shortCiteRegEx": "Ng et al\\.", "year": 2005}, {"title": "Continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "UAI, pages 378\u2013387.", "citeRegEx": "Nodelman et al\\.,? 2002", "shortCiteRegEx": "Nodelman et al\\.", "year": 2002}, {"title": "Expectation maximization and complex duration distributions for continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "UAI, pages 421\u2013430.", "citeRegEx": "Nodelman et al\\.,? 2005", "shortCiteRegEx": "Nodelman et al\\.", "year": 2005}, {"title": "Kommunikation mit Automaten", "author": ["C.A. Petri"], "venue": "PhD thesis, University of Bonn.", "citeRegEx": "Petri,? 1962", "shortCiteRegEx": "Petri", "year": 1962}, {"title": "A GSPN semantics for continuous time Bayesian networks with immediate nodes", "author": ["D.C. Raiteri", "L. Portinale"], "venue": "Technical Report TR-INF-2009-03-03-UNIPMN, U. of Piemonte Orientale CS Dept.", "citeRegEx": "Raiteri and Portinale,? 2009", "shortCiteRegEx": "Raiteri and Portinale", "year": 2009}, {"title": "Inexact uniformization method for computing transient distributions of Markov chains", "author": ["R.B. Sidje", "K. Burrage", "S. MacNamara"], "venue": "SIAM Journal of Scientific Computation, 29(6):2562\u20132580.", "citeRegEx": "Sidje et al\\.,? 2007", "shortCiteRegEx": "Sidje et al\\.", "year": 2007}, {"title": "Probabilistic inference in queueing networks", "author": ["C.A. Sutton", "M.I. Jordan"], "venue": "SysML.", "citeRegEx": "Sutton and Jordan,? 2008", "shortCiteRegEx": "Sutton and Jordan", "year": 2008}, {"title": "Approximate steady-state analysis of large Markov models based on the structure of their decision diagram encoding", "author": ["M. Wan", "G. Ciardo", "A.S. Miner"], "venue": "Performance Evaluation, 68(5):463\u2013486.", "citeRegEx": "Wan et al\\.,? 2011", "shortCiteRegEx": "Wan et al\\.", "year": 2011}, {"title": "Intrusion detection using continuous time Bayesian networks", "author": ["J. Xu", "C.R. Shelton"], "venue": "JAIR, 39:745\u2013774.", "citeRegEx": "Xu and Shelton,? 2010", "shortCiteRegEx": "Xu and Shelton", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "By contrast, the continuous time Bayesian network (Nodelman et al., 2002) literature has focused on finite-time properties, so we compare against those algorithms.", "startOffset": 50, "endOffset": 73}, {"referenceID": 13, "context": "A singular exception is the work of Sutton and Jordan (2008) which applied Gibbs sampling to queueing models.", "startOffset": 36, "endOffset": 61}, {"referenceID": 0, "context": "In verification, continuous-time models can be used to compute the probability a system will function by a given time, such as in continuous stochastic logic (Baier et al., 2003).", "startOffset": 158, "endOffset": 178}, {"referenceID": 17, "context": "Other problems of interest include state estimation in asynchronous systems, such as distributed computer systems (Xu and Shelton, 2010), robotics (Ng et al.", "startOffset": 114, "endOffset": 136}, {"referenceID": 9, "context": "Other problems of interest include state estimation in asynchronous systems, such as distributed computer systems (Xu and Shelton, 2010), robotics (Ng et al., 2005), social networks (Fan and Shelton, 2009), or phylogenetic trees (Cohn et al.", "startOffset": 147, "endOffset": 164}, {"referenceID": 6, "context": ", 2005), social networks (Fan and Shelton, 2009), or phylogenetic trees (Cohn et al.", "startOffset": 25, "endOffset": 48}, {"referenceID": 3, "context": ", 2005), social networks (Fan and Shelton, 2009), or phylogenetic trees (Cohn et al., 2009).", "startOffset": 72, "endOffset": 91}, {"referenceID": 14, "context": "Previous results have looked at sparse representations of p and p\u2032 (Sidje et al., 2007).", "startOffset": 67, "endOffset": 87}, {"referenceID": 14, "context": "In the case for uniformization, Sidje et al. (2007) proposed keeping a sparse representation for v (by dropping elements less than a given threshold).", "startOffset": 32, "endOffset": 52}, {"referenceID": 12, "context": "While our methods are applicable to Petri nets (Petri, 1962), edge-valued decision diagrams (Wan et al.", "startOffset": 47, "endOffset": 60}, {"referenceID": 16, "context": "While our methods are applicable to Petri nets (Petri, 1962), edge-valued decision diagrams (Wan et al., 2011), and other compact representation of the rate matrix, we focus on using a continuous time Bayesian network (CTBN) to representQ.", "startOffset": 92, "endOffset": 110}, {"referenceID": 13, "context": "We note that a correspondence between CTBNs and Petri nets has already been established (Raiteri and Portinale, 2009).", "startOffset": 88, "endOffset": 117}, {"referenceID": 10, "context": "A continuous time Bayesian network (CTBN) (Nodelman et al., 2002) has an initial distribution described by a Bayesian network that is not of direct importance to this work.", "startOffset": 42, "endOffset": 65}, {"referenceID": 1, "context": "While we have not found a suitable way of bounding the approximation error for factored RKF, we can derive bounds similar to those of the BK algorithm (Boyen and Koller, 1998) for discrete-time stochastic processes to bound the error in propagation and projection through a single M matrix.", "startOffset": 151, "endOffset": 175}, {"referenceID": 1, "context": "We require a mixing rate definition from Boyen and Koller (1998):", "startOffset": 41, "endOffset": 65}, {"referenceID": 1, "context": "As shown by Boyen and Koller (1999), better bounds can be placed by more careful analysis or considering the average case.", "startOffset": 12, "endOffset": 36}, {"referenceID": 11, "context": "The structure and parameters of both the initial distribution and the dynamics were learned by the structural EM algorithm (Nodelman et al., 2005) and we used the learned network model for our experiments.", "startOffset": 123, "endOffset": 146}, {"referenceID": 7, "context": "We use the same network model as in Fan et al. (2010) and Nodelman et al.", "startOffset": 36, "endOffset": 54}, {"referenceID": 7, "context": "We use the same network model as in Fan et al. (2010) and Nodelman et al. (2005) which chooses 4 variables: employment (student, employed, unemployed), children (0, 1,\u22652), married (not married, married) and smoking (non-smoker, smoker) and adds a hidden binary variable for each (Figure 1a).", "startOffset": 36, "endOffset": 81}, {"referenceID": 3, "context": "1, sparse uniformization (US), and the mean field (MF) approach of Cohn et al. (2009) for CTBNs.", "startOffset": 67, "endOffset": 86}], "year": 2011, "abstractText": "We consider filtering for a continuous-time, or asynchronous, stochastic system where the full distribution over states is too large to be stored or calculated. We assume that the rate matrix of the system can be compactly represented and that the belief distribution is to be approximated as a product of marginals. The essential computation is the matrix exponential. We look at two different methods for its computation: ODE integration and uniformization of the Taylor expansion. For both we consider approximations in which only a factored belief state is maintained. For factored uniformization we demonstrate that the KL-divergence of the filtering is bounded. Our experimental results confirm our factored uniformization performs better than previously suggested uniformization methods and the mean field algorithm. 1 Continuous-Time Markov Systems We are interested in monitoring (alternatively tracking or filtering) a continuous-time finite-state homogeneous Markovian stochastic system. This implies that evidence and events can arrive at any real-valued time (asynchronously) and be either instantaneous or have real-valued time durations. The system is stationary and has discrete states which means that a sample (trajectory) for the system consists of a series of times at which the system jumped from one state to another. Such models are common in the queueing theory and verification literatures. The algorithms developed there almost exclusively focus on steady-state properties of the system. A singular exception is the work of Sutton and Jordan (2008) which applied Gibbs sampling to queueing models. By contrast, the continuous time Bayesian network (Nodelman et al., 2002) literature has focused on finite-time properties, so we compare against those algorithms. In verification, continuous-time models can be used to compute the probability a system will function by a given time, such as in continuous stochastic logic (Baier et al., 2003). Other problems of interest include state estimation in asynchronous systems, such as distributed computer systems (Xu and Shelton, 2010), robotics (Ng et al., 2005), social networks (Fan and Shelton, 2009), or phylogenetic trees (Cohn et al., 2009). 1.1 Parameterization Such a Markovian system is described by an initial distribution over the state space and an intensity (or rate) matrix, often denoted Q. The diagonal element qii = \u2212qi where qi \u2265 0 is the rate of leaving state i. This means that the density of the duration of the process remaining in state i for exactly a duration \u2206t is an exponential distribution: qiei. The non-diagonal elements, qij \u2265 0 are the rates of transitioning from state i to state j. qi = \u2211 j qij in a closed system. (The rate of leaving a state is equal to the sum of the rates of moving to any other state.) The probability of transitioning to state j immediately upon leaving state i is qij/qi. Note that the diagonal elements of Q are non-positive and the non-diagonal elements are non-negative. The sum of each row is 0. Monitoring such a system consists of keeping track of the probability distribution over the state at the current time t, given all evidence prior to t. We would like a recursive solution in which evidence before t can be discarded once the distribution (or its estimate) at t has been computed. As an initial problem, we will be interested in tracking in the absence of any evidence. We will cover how to incorporate evidence in Section 5. To simplify notation, we will assume that we have the distribution at time 0 and wish to propagate this distribution to time t. If p is the distribution at time 0, represented as a row vector, then p\u2032, the distribution at t also represented as a row vector, is", "creator": "TeX"}}}