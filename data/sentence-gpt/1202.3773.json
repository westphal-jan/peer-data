{"id": "1202.3773", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Measuring the Hardness of Stochastic Sampling on Bayesian Networks with Deterministic Causalities: the k-Test", "abstract": "Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local Variance Bound (LVB) to measure the approximation hardness of Bayesian inference on Bayesian networks, assuming the networks model strictly positive joint probability distributions, i.e., each network model is bound with a mean (0.7 s for each network, and 1.4 s for each network), so that a Bayesian prediction predicts a Bayesian prediction that is not a Bayesian prediction.\n\n\n\n\nThe results of the LVM-Bayesian search are presented in Fig. 1. We also obtained the Bayesian LVM-Bayesian posterior distribution in Figure 1. We also obtained the Bayesian posterior distribution in Fig. 2. The Bayesian posterior distribution and the Bayesian posterior distribution in Fig. 2 are also in Fig. 3.\n\nThe Bayesian posterior distribution is described as the posterior distribution (Fig. 4) in Fig. 5. For our LVM-Bayesian posterior distribution, the Bayesian posterior distribution is expressed as a Bayesian posterior distribution (where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution. For our LVM-Bayesian posterior distribution, the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution. The Bayesian posterior distribution is a Bayesian posterior distribution and a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution, where the Bayesian posterior distribution is a Bayesian posterior distribution.\nThe results of this exercise provide a new framework for Bayesian inference, where the Bayesian posterior distribution is a Bayesian posterior distribution. In other words, we are looking for a Bayesian posterior distribution that is the posterior distribution. This is one of the most widely accepted models of Bayesian inference. Our results indicate that a Bayesian posterior distribution", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (224kb)", "http://arxiv.org/abs/1202.3773v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["haohai yu", "robert a van engelen"], "accepted": false, "id": "1202.3773"}, "pdf": {"name": "1202.3773.pdf", "metadata": {"source": "CRF", "title": "Measuring the Hardness of Stochastic Sampling on Bayesian Networks with Deterministic Causalities: the k-Test", "authors": ["Haohai Yu"], "emails": [], "sections": [{"heading": null, "text": "Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local Variance Bound (LVB) to measure the approximation hardness of Bayesian inference on Bayesian networks, assuming the networks model strictly positive joint probability distributions, i.e. zero probabilities are not permitted. This paper introduces the k-test to measure the approximation hardness of inference on Bayesian networks with deterministic causalities in the probability distribution, i.e. when zero conditional probabilities are permitted. Approximation by stochastic sampling is a widely-used inference method that is known to suffer from inefficiencies due to sample rejection. The k-test predicts when rejection rates of stochastic sampling a Bayesian network will be low, modest, high, or when sampling is intractable."}, {"heading": "1 Introduction", "text": "A Bayesian network, belief network, or directed acyclic graphical model, is a probabilistic graph model [28] that has gained wide acceptance in several areas of artificial intelligence [31]. A Bayesian network represents a joint probability distribution (JPD) over a set of statistical variables and structurally models the (conditional) independence relationships between the variables as a directed acyclic graph (DAG). Efficient algorithms exist that perform probabilistic inference on Bayesian networks for reasoning with uncertainty and to solve decision problems with uncertain data.\nExact and approximate probabilistic Bayesian inference is known to be NP-hard [11, 12]. No single Bayesian inference algorithm or a class of algorithms is known to generally outperform others. Approximate inference algorithms are popular due to their anytime\nproperty [17] to produce an approximate result, possibly in real time [23]. For this reason, stochastic sampling algorithms, especially importance sampling, are among the most widely-used approximate inference methods [31]. Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35]. Importance sampling algorithms mainly differ in the choice of importance function to sample the JPD. All sampling algorithms perform poorly on Bayesian networks that are known to be hard for approximate inference. When zero probabilities are permitted in the JPDs, sampling algorithms become inefficient due to sample rejection; samples with zero probability do not contribute to the sum estimate and are therefore effectively rejected. Consequently, the sampling algorithm\u2019s performance is poor on Bayesian networks with deterministic causalities, i.e. networks that model JPDs with zero probabilities.\nThe local variance bound (LVB) [13] metric demarcates the boundary between the class of Bayesian networks with tractable approximations and those with intractable approximations. In certain cases the LVB of a Bayesian network can be used as a quantitative estimation of the expected rate of convergence of the approximate solution to the exact solution under stochastic sampling of the network.\nHowever, the LVB metric is not applicable to measure the approximation hardness of Bayesian networks with deterministic causalities. The LVB metric is the ratio of the maximum to the minimum probability value of the conditional probability table (CPT) entries of the variables in the Bayesian network. Any zero entry in the CPT invalidates the LVB, i.e. when zero probabilities are permitted in the JPD. This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks. Hence, their inference tractability classification fails and it is not possible to give an estimation of the hardness of approximate inference.\nThis paper presents the k-test, a metric to determine the approximation hardness of Bayesian inference. By contrast to the LVB metric, the k-test metric can be applied to Bayesian networks that model JPDs that include zero probabilities. This extends the approximation hardness estimation to an important class of realworld Bayesian networks. The k-test on a Bayesian network can be used to indicate when sample rejection rates are expected to be low, modest, or prohibitively high and sampling is intractible.\nNo polynomial-time algorithm exists that can filter samples x with Pr(x) = 0 from important sampling to prevent rejection. Exact determination of Pr(x) = 0 is known to be NP-complete. It turns out that approximate filtering of samples with Pr(x) = 0 is also NP-hard.\nThe remainder of this paper is organized as follows. Section 2 briefly introduces the Bayesian network formalism, inference by importance sampling, and the sample rejection problem. Section 3 introduces the k-test. Results of the k-test on real-world and benchmark Bayesian networks is presented in Section 4. Finally, Section 5 compares related work and summarizes our contribution."}, {"heading": "2 Background", "text": "This section briefly introduces the Bayesian network formalism, approximate Bayesian inference by importance sampling, and the sample rejection problem."}, {"heading": "2.1 Bayesian Networks", "text": "A Bayesian network is defined as follows.\nDefinition 1 A Bayesian Network BN = (G,Pr) consists of a directed acyclic graph (DAG) G = (V,A) with vertices V, arcs A \u2286 V\u00d7V, and a JPD Pr over the discrete random variables V (represented by the vertices of G). Pr is defined by\nPr(V) = \u220f V \u2208V Pr(V |\u03c0(V )) ,\nwhere \u03c0(V ) denotes the set of parents of a vertex V and the conditional probability tables (CPT) of the BN assign domain-specific probabilities to Pr(V |\u03c0(V )) for all V \u2208 V.\nIn this paper, variables are indicated by uppercase letters and their states by lowercase letters. Sets (vertices, arcs, and states) are represented in boldface, e.g. x \u2208 Config(X) denotes a state (configuration of values) x of a set of variables X \u2286 V. The set of evidence variables (or vertices) is denoted as E, E \u2282 V and the set X = V \\E.\nExact probabilistic inference methods compute Pr(X|e) given evidence e for a set of evidence variables E directly from the network. Approximate inference methods estimate Pr(X|e).\nNote that Pr(v) = 0 for configuration of variables V whenever the CPT entry Pr(vi|\u03c0(vi)) = 0 for Vi \u2208 V. These zero entries are computationally problematic for Bayesian inference with importance sampling."}, {"heading": "2.2 Approximate Bayesian Inference by Importance Sampling", "text": "Let g(X) be a function over m variables X = {X1, . . . , Xm} over some domain \u2126 \u2286 IRm, such that computing g(X) for any X is feasible. Let p be a probability density over X. Consider the problem of estimating the integral\nE[g(X)|p] = \u222b\n\u2126\ng(x)p(x) dx . (1)\nAssuming that p is a density that is easy to sample from, the integral can be approximated by drawing a set of independent and identically distributed (i.i.d.) samples {x1, . . . ,xN} from p(X) and use these to compute the sample mean\ng\u0303N = 1 N N\u2211 i=1 g(xi) . (2)\nAccording to the strong law of large numbers, g\u0303N almost surely converges to E[g(X)|p]. The basic idea of importance sampling is to draw from a distribution other than p, say I, that is easier to sample from than p. We can rewrite (1) into\nE[g(X)|p] = \u222b\n\u2126\ng(x) p(x) I(x) I(x) dx , (3)\nwhere I is often referred to as the importance function. The revised sample mean formula\ng\u0302N = N\u2211 i=1 [g(xi)w(xi)] , (4)\nuses weights w(xi) = p(xi) I(xi) . Again, g\u0302N almost surely converges to E[g(X)|p].\nAssumption 1 The distribution I(X) is assumed to support p(X) on \u2126. That is, \u2200x \u2208 \u2126 : p(x) > 0 \u21d2 I(x) > 0. Furthermore, it is assumed that 00 = 0 in w(xi) =\np(xi) I(xi) .\nFor more details refer to [30, 36].\nThe goal of the importance function I is to approximate the posterior probability distribution Pr(X|e),\nmodeled by a network given some evidence e for E, without actually updating the network to the posterior, which is prohibitively expensive.\nThe importance function I should be tractable. Here, \u201ctractable\u201d means that there exists a sampling order \u03b4, such that for any valid instantiation x\u03b4(1), \u00b7 \u00b7 \u00b7 , x\u03b4(m) of X\u03b4(1), \u00b7 \u00b7 \u00b7 , X\u03b4(m) m \u2265 1, the complexity of computing PrI(x\u03b4(m+1) | x\u03b4(1), \u00b7 \u00b7 \u00b7 , x\u03b4(m)) is polynomial. Here, PrI(\u00b7) is the probability distribution induced by I. A sampling order \u03b4 that meets these requirements is called a tractable sampling order for I, or simply a tractable order.\nImportance sampling methods generally require tractable sampling that is consistent with a Bayesian network\u2019s topological order of the vertices V. This is true for AIS-BN [7], EPIS-BN [35], and SIS [32]. By contrast, the tractable sampling order of DIS [27] is a reversed elimination order, which may or may not be consistent with a Bayesian network\u2019s topological order of the vertices.\nA well-known problem of importance sampling on Bayesian networks with deterministic causalities is that the performance of sampling can be poor. When the JPD has zero probabilities, many samples may end up having zero weights w(x) = 0 in the sampling process. These samples do not contribute to the sum estimate (4) and are effectively rejected. Such a sample is inconsistent, since x is an impossible event w(xi) = 0\u21d2 p(xi) = 0 by Assumption 1.\nThe sample rejection problem under evidential reasoning in a Bayesian network with evidence e is a judgement whether Pr(x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m), e) = 0 for sample x\u03b4(1), \u00b7 \u00b7 \u00b7 , x\u03b4(m) and sampling order \u03b4.\n3 The k-Test\nIn this section the k-test is presented to measure the approximation hardness of sampling Bayesian networks with deterministic causalities. The random kSAT problem and the Satisfiability Threshold Conjecture are explained that form the basis for the k-test. An algorithm to efficiently compute the k-test ratio of a Bayesian network is given."}, {"heading": "3.1 Hardness of Sample Rejection", "text": "Cooper [11] proved that computing Pr(x) for any x is NP-hard in general. Computing Pr(x) to classify inconsistent samples x with Pr(x) = 0 from consistent ones with Pr(x) > 0 is prohibitively expensive as a measure to determine the hardness of sampling.\nFurthermore, the approximate sample rejection problem is too hard to be polynomial, as found by our proof\nin Appendix A. Dagum and Luby [12] proved that approximate Bayesian inference is NP-hard. However, their proof is not applicable to the sample rejection problem of Bayesian networks, because the JPDs of these networks are not strictly positive (an assumption used in their proof).\nTo empirically estimate the rejection rate requires a significant number of samples to be produced to cover the exponential state space of a network. Furthermore, the choice of a sampling algorithm may also influence the estimation of the ratio, such as by CPT learning adopted by state-of-the-art sampling algorithms.\nTherefore, exact, approximate, and empirical determination of the hardness of sampling Bayesian networks exhibiting zero probabilities poses significant computational difficulties.\nThe sample rejection problem can be transformed into an equivalent random k-SAT problem, which forms the basis of our k-test.\n3.2 Random k-SAT\nFranco and Paull [15] first observed, among other important results, that random instances of the k-SAT problem undergo a \u201cphase transition\u201d as the ratio of clauses to variables passes through a threshold. Let Fn,mk denote a k-CNF with n variables andm k-clauses created by uniformly and randomly choosing m clauses from the Ck = 2k ( n k ) possible clauses. Franco and Paull [15] claim that Fn,m=rnk is with high probability (w.h.p. limn\u2192\u221e Pr( n) = 1) unsatisfiable if r \u2265 2k ln 2. The reasons are given as follows. Let a be a truth assignment and let Sk = (2k \u2212 1) ( n k ) be the number of k-clauses consistent with the given assignment a. Then for Fn,mk , Pr(F n,m k (a) = true) = ( Sk m ) / ( Ck m ) \u2264 (1 \u2212 2\u2212k)m the expected number of satisfying truth assignments of Fn,mk is at most 2\nn(1 \u2212 2\u2212k)m = o(1) for m/n \u2265 2k ln 2.\nThis result has led to the following popular conjecture.\nSatisfiability Threshold Conjecture. For k \u2265 3, there exists a constant rk such that\nlim n\u2192\u221e\nPr(Fn,rnk is satisfiable) = {\n1, if r < rk, 0, if r > rk (5)\nSince 1990 much work has been done on this conjecture to narrow the threshold rk. One class of methods is based on mathematical analysis. In the milestone paper [16], Friedgut used the second moment method to prove the existence of a nonuniform satisfiability threshold, i.e. a sequence rk(n), around which the probability of satisfiability goes from 1 to 0. Inspired by [16], Dimitris and Cristopher [1] further narrowed the threshold around O(2k\u22121 ln 2).\nAnother method is to design and analyze a polynomial algorithm that can find a truth assignment with uniformly positive probability (w.u.p.p. limn\u2192\u221e inf Pr( n) > 0) or w.h.p., if, for a satisfiable Fn,rnk , r is smaller than the lower bound of rk. In [6] and [8] this method is used to narrow the lower bound of rk to O(2k/k). The current best result is from [9], which not only presents a polynomial algorithm that finds a satisfying truth assignment w.h.p., if r < (1 \u2212 k)2k ln(k)/k, where k \u2192 0 and k > 10, but also points out that if r is above O(2k ln(k)/k) no polynomial algorithm is known to find a satisfying truth assignment with probability \u2126(1) \u2013 neither on the basis of rigorous or empirical analysis, or any other evidence.\n3.3 The k-Test Algorithm\nIf a one-to-one mapping from the set of consistent samples from a Bayesian network to a set of satisfying truth assignments of a satisfiable k-CNF Fn,mk can be constructed, then the clause density r = m/n can be compared to the threshold value 2k/k to estimate the hardness of the rejection problem to sample the network. The k-test ratio of a Bayesian network is the ratio of the clause density r to the threshold value 2k/k. What remains is to devise an efficient construction method for the k-CNF of a Bayesian network to determine the k-test ratio r : 2k/k. The construction of a k-CNF Fn,mk should satisfy the following requirements:\n1. The satisfying truth assignments of the k-CNF Fn,mk model all and only the consistent configurations of the Bayesian network.\n2. The k-CNF Fn,mk should be minimal. That is, there should not be too many unnecessary binary variables introduced in the k-CNF or too many unnecessary clauses added to the k-CNF.\n3. The construction of the k-CNF and calculation of the clause density r should be performed in polynomial time.\nIn the following, we define an efficient conversion from a Bayesian network to a k-CNF Fn,mk that satisfies these three requirements.\nLet BN = (G,Pr), G = (V,A) be a Bayesian network and E the evidence, where e is the evidence configuration. Let \u2016Vi\u2016 denote the number of states of Vi \u2208 V. The conversion of the BN to a Boolean formula proceeds in two steps.\nStep 1. Convert all variables V to Boolean variables by log encoding [18]. For each Vi \u2208 V, we create a set\nof Boolean variables:\n{Xi1 , Xi2 , \u00b7 \u00b7 \u00b7 , Xidlog2 \u2016Vi\u2016e} (6)\nMap each Xik to the k th bit of the binary representation of Vi\u2019s discrete value. For example, if \u2016Vi\u2016 = 5, then Vi = 3 is mapped to (Xi3 = 0, Xi2 = 1, Xi1 = 1).\nStep 2. Construct the k-CNF formula Fn,mk . There are three different types of clauses in the resulting kCNF: the CPT Clauses, the Variable Clauses, and the Evidence Clauses. The aim is to construct a k-CNF that is minimal, which is accomplished as follows.\nCPT Clauses: each zero entry in a CPT of the Bayesian network represents a constraint that is translated into a disjunctive clause. For example, assume \u2016Vi\u2016 = 5, \u2016Vj\u2016 = 3, then we can translate Pr(Vi = 3 | Vj = 2) = 0 to \u00ac(\u00acXi3 \u2227Xi2 \u2227Xi1 \u2227 Xj2 \u2227 \u00acXj1) = Xi3 \u2228 \u00acXi2 \u2228 \u00acXi1 \u2228 \u00acXj2 \u2228Xj1 .\nVariable Clauses: for each Vi \u2208 V, if \u2016Vi\u2016 is not a power of 2, then certain assignments of Vi\u2019s translated Boolean variables do not satisfy the formula. For example, if \u2016Vi\u2016 = 5, then {Xi1 = 1, Xi2 = 1, Xi3 = 1} is not a valid assignment, since 7 is not a valid value of Vi. We add a clause for each invalid assignment. For the previous example, this means that \u00ac(Xi1 \u2227 Xi2 \u2227 Xi3) is added. However, if this mapping is done na\u0308\u0131vely, the result may yield an exponentially large set of clauses, which clearly does not lead to a minimal k-CNF. Consider \u2016Vj\u2016 = 2m + 1, then for Vj as many as 2m\u22121 clauses will be added to the k-CNF formula. In the worst case, this leads to a (2m\u22121)/(m+1) clause/variable ratio. Fortunately, we can take advantage of the fact that all valid discrete values of Vj are smaller than \u2016Vj\u2016 to minimize the number of clauses. Let 1bm \u00b7 \u00b7 \u00b7 b1 be the binary representation of \u2016Vj\u2016\u22121. Then, for k = m, . . . , 1 such that bk = 0, we have that\nXjm+1 = 1 \u2227 \u00b7 \u00b7 \u00b7 \u2227Xjk+1 = bk+1 \u2192 Xjk = 0. (7)\nFormula (7) can be converted to a disjunctive clause by A \u2192 B \u21d4 \u00acA \u2228 B, since Xl = 1(0) is simply equivalent to Xl = true(false). In the worst case, the variable clauses only contribute m/(m+ 1) clauses to the clause density.\nEvidence Clauses: for each Vi \u2208 E with state Vi = ei, let bmbm\u22121 \u00b7 \u00b7 \u00b7 b1, m = \u2016Vi\u2016, be the binary representation of ei. Add the clause Xim = bm \u2227 Xim\u22121 = bm\u22121 \u2227 \u00b7 \u00b7 \u00b7 \u2227Xi1 = b1 to the k-CNF.\nTo compute the k-test ratio r : 2k/k of a Bayesian network, only the density r of the k-CNF clauses is\nProcedure k-Test (BN ,E) Input: BN = (G,CPT ), evidence set E \u2286 V(G) Output: k, clause density r begin\nn\u2190 \u2211 Vi\u2208V(G) dlog2 \u2016Vi\u2016e m\u2190 |E| k \u2190 maxE\u2208E dlog2 \u2016E\u2016e foreach Vi \u2208 V(G) do\nki \u2190 dlog2 \u2016Vi\u2016e b\u2190 \u2016Vi\u2016 \u2212 1 while b > 0 do\nif b mod 2 = 0 then m\u2190 m+ 1 if ki > k then k \u2190 ki end b\u2190 bb/2c ki \u2190 ki \u2212 1\nend kmax \u2190 \u2211 Vj\u2208\u03c0[Vi] dlog2 \u2016Vj\u2016e + dlog2 \u2016Vi\u2016e foreach Prj \u2208 CPT [Vi] do if Prj = 0 then\nm\u2190 m+ 1 if kmax > k then k \u2190 kmax\nend end\nend r \u2190 m/n\nend\nAlgorithm 1: k-Test\nrequired. Hence, the k-test algorithm is a simplified version of the k-CNF construction algorithm, in which the number of variables n, number of k-clauses m, and k are computed directly from a Bayesian network as shown in Algorithm 1. The algorithm determines k and the clause density r = m/n given a Bayesian network BN and set of evidence variables E. The time complexity of the algorithm is linear to the sum of the sizes of the CPTs in the network. From our empirical results reported in the next section, we found that the k-test only takes seconds to compute for large networks with hundreds of variables."}, {"heading": "4 Results", "text": "In this section the k-test is experimentally verified. Two classes of Bayesian networks were used in the experiments: \u201creal-world\u201d networks and benchmark networks. The real-world networks are shown in Table 1. Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them. A selection1 of synthetic bench-\n1We selected networks that are easy to hard to sample.\nmark networks from the UAI contest [5] is shown in Table 2. Both tables show the number of variables and arcs of the network, the directly computed k-test ratio r : 2k/k, and the average rejection rate of na\u0308\u0131ve sampling. To eliminate any bias of advanced sampling techniques toward any of these networks, na\u0308\u0131ve importance sampling (likelihood sampling) is used in this study. In this way, rejection rates purely depend on the properties of the JPD of the Bayesian network, not on adaptive CPT learning-based optimizations to sample the network as is performed by SIS, AIS-BN, and other advanced algorithms. Furthermore, for each Bayesian network, we randomly generated 50 test cases, and for each test case a random set of 10 to 20 evidence variables and instantiations are randomly selected. The number of samples is 60,000 for each test case.\nFigure 1 compares the directly computed ratio r : 2k/k to the empirically-established rejection rate of likelihood importance sampling for all Bayesian Networks used in this study. In the figure, a rejection rate of 1.0 means that all samples are rejected in the sampling process (100%).\nFrom these results it can be concluded that the k-test\nratio r : 2k/k accurately predicts when the sampling rejection rate will be modest, high, or reaches 100% and sampling becomes intractable. When the k-test ratio < 0.1, sample rejection rates are modest or zero, see Tables 1 and 2. The sampling efficiency is poor when the k-test ratio reaches 0.1. When the k-test ratio > 0.2, sampling is intractable. No importance sampling algorithm can generate consistent samples for the synthetic BNs from BN 69uai to BN 76uai in reasonable time. Likelihood sampling does not even yield a single valid sample in thousands of samples, see Table 2.\nWhen the k-test ratio r : 2k/k of a network is between 0.005 and 0.200, improved importance sampling methods can be used to attempt to lower the sampling rejection rate and thus improve efficiency of sampling. Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].\nState-of-art importance sampling algorithms are known to perform well on Andes (k-test ratio 0.018, the average rejection ratio of AIS-BN is 13.9%) and Pathfinder (k-test ratio 0.173, the average rejection ratio of AIS-BN is 49.5%) as these algorithms mitigate the rejection problem. However, their performance is mixed on Munin (k-test ratio 0.112, the average rejection ratio of AIS-BN is 96.1% and the average rejection ratio of EPIS-BN is 28.5%).\nEasy-to-sample networks do not require sophisticated sampling techniques. For those networks the k-test ratio < 0.005. Indeed, sampling the CPCS networks (Table 1) incurs no rejection overhead. Simple sampling methods suffice for these networks."}, {"heading": "5 Conclusions", "text": "This paper introduced the k-test to measure the hardness of stochastically sampling Bayesian networks that exhibit zero probabilities. Such networks have deterministic causalities defined by the zeros in the conditional probability tables (CPT), which results in samples being rejected. To empirically estimate the rejection rate requires a significant number of samples to be produced to cover the exponential state space of a network. It also requires the use of a set of sampling algorithms to eliminate algorithm bias (such as CPT learning effects). By contrast, the k-test is a linear-time algorithm to determine the hardness of stochastically sampling a Bayesian network and is a good estimator of the rejection rate for any sampling algorithm. The metric identifies networks for which rejection rates will be low, modest, high, or when sampling is intractable. The k-test is based on recent advances in random kSAT analysis. Experimental results for real-world and benchmark networks show the experimental validity of the k-test.\nSampling algorithms have been modified and improved by many authors to mitigate the generation of inconsistent samples and limit the overhead of sample rejection. The rejection problem in importance sampling has been extensively studied in the work on adaptive sampling schemes [7], in the context of constraint propagation [20], and Boolean satisfiability problems [21]. A restricted form of constraint propagation can be used to reduce the amount of rejection [19]. An approach to circumvent the rejection problem by systematically searching for a nonzero weight sample for constraintbased systems was introduced in [20]. The proposed backtracking algorithm, SampleSearch was further improved in [21] and shown to generate a backtrack-free distribution. In [22], the SampleSearch method is fur-\nther generalized as a scheme in the framework of mixed networks [14, 26]. However, the exact rejection problem is NP-complete and the approximate rejection problem is too hard to be polynomial as we proved in this paper. Because Bayesian networks are special cases of mixed networks, we believe that Corollary 1, Theorem 2 and Theorem 1 can be generalized to mixed networks.\nAlthough most state-of-art importance sampling algorithms have a capability to reduce the generation of inconsistent samples, in worst case they still fail to generate sufficient useful samples in reasonable time. It is therefore critical to identify Bayesian networks that are hard to sample, e.g. by using the k-test.\nThe LVB [13] metric demarcates the boundary between the class of Bayesian networks with tractable approximations and those with intractable approximations. LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks. The k-test compliments the LVB by measuring the approximation hardness of these and many other Bayesian networks with deterministic causalities, i.e. networks that model JPDs with zero probabilities. LVB measures the hardness of sampling caused by strictly-positive extreme probability distribution, whereas the k-test measures the difficulties of sampling induced by the rejection problem. Currently there is no satisfactory combination of these two measurements that provides a general metric to measure the hardness of sampling a Bayesian network. This will be an interesting and challenging forthcoming work, because a combined metric enables the measurement of the sampling hardness of networks that exhibit both zero and close-to-zero probabilities."}, {"heading": "A Appendix", "text": "A.1 Probabilistic Turing Machines\nThe Probabilistic Turing Machine (PTM) formulation is used in the complexity analysis of approximate algorithms and probabilistic algorithms. We briefly introduce PTM and class RP, see [4] for more details.\nDefinition 2 The Probabilistic Turing Machine (PTM) is a Turing machine with two transition function sets \u03bb0, \u03bb1. To execute a PTM M on an input x, we choose in each step with probability 12 to apply the transition function in \u03bb0 and with probability 1 2 to apply \u03bb1. This choice is made independently.\nThe machine M only outputs 1 (\u201caccepted\u201d) or 0 (\u201crejected\u201d). We denote by M(x) the random variable corresponding to the value M outputs at the end of execution. For a function T : N \u2192 N, we say that M runs in T (n)-time if for any input x, M halts on x within T (|x|) steps regardless of the random choices it makes.\nDefinition 3 RTIME(T (n)) contains every language L for which there is a PTM M running in T (n) time such that\nx \u2208 L\u21d2 Pr(M(x) = 1) \u2265 23 x /\u2208 L\u21d2 Pr(M(x) = 0) = 1 (8)\nWe define RP = \u22c3 c>0 RTIME(n c)\nObviously RP \u2286 NP.\nA.2 Hardness of the Sample Rejection Problem\nFirst, we give a formal definition of sample rejection problem:\nDefinition 4 For any tractable2 importance function I with tractable sampling order \u03b4, For a BN, e are evidences and e 6= \u2205. I is an importance function and \u03b4 is a sampling order. Pre(\u00b7) is the BN\u2019s posterior probability distribution (Pre(\u00b7) \u2261 0, if Pr(e) = 0). the Rejection Problem of I with \u03b4 is defined as: let e 6= \u2205, Pr(e) > 0, be the observed evidence3, if PrI(x\u03b4(1), \u00b7 \u00b7 \u00b7 , x\u03b4(m\u22121)) > 0 (m \u2265 1), then \u2200x \u2208 Config(X\u03b4(m)) determine whether Pre(x\u03b4(1), \u00b7 \u00b7 \u00b7 , x\u03b4(m\u22121), x) > 0.\nDef. 4 is reasonable, because if during sampling of the mth variable X\u03b4(m) the rejection problem is solved, then we can pick up a x\u03b4(m) from Config(X\u03b4(m)) such that Pre(x\u03b4(1), \u00b7 \u00b7 \u00b7 , x\u03b4(m)) > 0. This process repeats until we find a consistent sample.\nNote that Def. 4 requires nothing when Pr(e) = 0. Thus, the exact rejection algorithm is a partial function (Rejection Function) \u0393\u03b4 : \u2126\u03b4 \u2192 {0, 1}, \u2126\u03b4 = Config(E)\u00d7 \u22c3|X| i=1 Config(X\u03b4(1) \u00b7 \u00b7 \u00b7X\u03b4(i)). If Pr(e) = 0,\n2Only tractable importance functions with tractable sampling orders are considered, because the sampling process should be polynomial.\n3e 6= \u2205 because generating a consistent sample on a BN without evidence is a trivial problem.\n\u0393\u03b4(e, \u00b7) is undefined (could be any); if Pr(e) > 0,\n\u0393\u03b4(e, x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m)) = {\n1, Pre(x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m)) > 0 0, Pre(x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m)) = 0\nHere, the sampling probability distribution Pr\u0393I , induced by importance function I and rejection function \u0393\u03b4, is obtained by:\nPr\u0393I (x\u03b4(m) | x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m\u22121)) = PrI(x\u03b4(m)|x\u03b4(1)\u00b7\u00b7\u00b7x\u03b4(m\u22121))\u00d7\u0393\u03b4(x\u03b4(m)|x\u03b4(1)\u00b7\u00b7\u00b7x\u03b4(m\u22121))P\nx\u2208Config(X\u03b4(m)) PrI(x|x\u03b4(1)\u00b7\u00b7\u00b7x\u03b4(m\u22121))\u00d7\u0393\u03b4(x\u03b4(1)\u00b7\u00b7\u00b7x\u03b4(m\u22121),x)\n.\n(9)\nAssuming Pr(e) > 0, a rejection algorithm solves the rejection problem in computable T (n) time, for input with length n. The ill-defined case Pr(e) = 0 can be bounded by counting down from T (n) and returning a random decision when the counter reaches 0. Hence, it can be assumed that rejection algorithms are time bounded.\nThe sample rejection problem is NP-complete. That is, there is no polynomial time algorithm that generally classifies consistent and inconsistent samples from Bayesian networks.\nLemma 1 For any tractable importance function I with tractable sampling order \u03b4, the rejection problem is in NP.\nLemma 1 is straightforward because verifying whether a sample is consistent is O(n). To prove that the rejection problem is NP-complete, we reduce the 3SAT problem into the rejection problem and the reduction is polynomial.\nCorollary 1 For any tractable importance function I with tractable sampling order \u03b4, the sample rejection problem is NP-complete.\nProof. This follows from [11]. For any 3CNF F , we convert F to PIBNET by [11]. Assume that Y (the descendent vertex of all the other vertices in PIBNET, which represents the value of F) is the only evidence and that Y = True. For any tractable importance function I with sampling order \u03b4, if the rejection problem can be resolved in polynomial time, we can obtain a full sample by Eq. (9) in polynomial time. In case the denominator of Eq. (9) is zero, we can randomly pick a value of X\u03b4(m). If Pr(Y = True) > 0, then \u0393 is well defined and ensures one consistent sample (its weight > 0). If Pr(Y = True) = 0 the generated sample must be inconsistent. Hence, we can differentiate Pr(Y = True) = 0 from Pr(Y = True) > 0 by solving the rejection problem. From Lemma 1 the rejection problem is in NP, and therefore the rejection problem is NP-complete. 2\nApproximate sample rejection problem is also NPhard. A rejection algorithm is called approximate, if it may accept x\u03b4(m), even when Pr(e, x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m)) = 0, for Pr(e) > 0 \u2227 PrI(x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m\u22121)) > 0. Still, if Pr(e, x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m)) > 0 then an approximate algorithm must accept it to avoid biased sampling. In other words, an approximate rejection algorithm is an one-side error approximation. Furthermore, an approximate rejection function \u0393\u0302A\u03b4 of an approximate rejection algorithm A is defined like \u0393\u03b4: if Pr(e) > 0,\n\u0393\u0302A\u03b4 (e, x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m)) = { 1, if A accepts x\u03b4(1) \u00b7 \u00b7 \u00b7x\u03b4(m) 0, else\nWhen there is no confusion, \u0393\u0302A\u03b4 is simplified in this text as \u0393\u0302\u03b4 or simply \u0393\u0302.\nLet Pr\u0393\u0302I denote the sampling probability distribution induced by importance function I and \u0393\u0302 (Pr\u0393\u0302I can be computed from Eq. 9 where \u0393\u03b4 is replaced by \u0393\u0302). Then, for inconsistent sample set \u2126e = {x | x \u2208 Config(X)\u2227Pre(x) = 0}, Pr\u0393\u0302I (\u2126e) > 0. Pr \u0393\u0302 I (\u2126e) gives the probability that inconsistent samples are generated or the average percentage of inconsistent samples over all the samples. If Pr\u0393\u0302I (\u2126e) = 0, we get an exact algorithm. Clearly for an approximate rejection algorithm, the smaller Pr\u0393\u0302I (\u2126e) is, the better is the approximation. We hope that there exists a polynomial \u03be up-bounded (\u03be up-bounded means \u2203\u03be, 0 < \u03be < 1, for any BN and any possible evidences e (Pr(e) > 0), Pr\u0393\u0302I (\u2126e) < \u03be) approximate rejection algorithm, so that n consistent samples can be retrieved from n\u03be samples with high probability.\nFurthermore, since both the algorithm for computing the importance function and the rejection algorithm may be stochastic, for a BN and evidence e, the Pr\u0393\u0302I (\u2126e) may be a random variable. Thus, it is reasonable to define randomly up-bounded or (\u03be, \u03c3) up-bounded as \u2203\u03be \u03c3, 0 < \u03be < 1 \u2227 0 \u2264 \u03c3 < 1, for any BN and BN\u2019s evidence e (Pr(e) > 0), such that Pr[Pr\u0393\u0302I (\u2126e) < \u03be] \u2265 1 \u2212 \u03c3. In other words, we relax the requirement of \u03be up-bounded to the case where \u03be up-bounded is satisfied with high probability. However Theorem 1 gives a pessimistic answer.\nTheorem 1 If there exists a polynomial (\u03be, \u03c3) upbounded approximate rejection algorithm for some tractable importance function I with tractable sampling order \u03b4, then NP \u2286 RP.\nProof. Assume that \u0393\u0302A\u03b4 is the approximate rejection function of approximate rejection algorithm A and Pr\u0393\u0302I is the sampling probability distribution induced by importance function I and \u0393\u0302A\u03b4 . Let \u03be, 0 < \u03be < 1 and \u03c3,\n0 \u2264 \u03c3 < 1. Then for any 3CNF F , we convert F to PIBNET by the method of [11]. In PIBNET, value of vertex Y is corresponding to value of F . For Y = True as the only evidence. Then we independently execute importance sampling process m (m > \u2212 ln 3ln(\u03c3+(1\u2212\u03c3)\u03be) ) times with tractable importance function I and approximate rejection algorithm A. Since both generating importance function I and rejection algorithm A maybe stochastic, we may obtain m different Pr\u0393\u0302I (\u00b7). Then we generate a sample from each Pr\u0393\u0302I (\u00b7). If any one of the m samples is consistent, we accept F . If none of them is consistent, we reject.\n\u2022 If F is unsatisfiable, no consistent sample can be generated.\n\u2022 If F is satisfiable, Pr[Pr\u0393\u0302I (\u2126e) < \u03be] \u2265 1 \u2212 \u03c3 \u21d2 Pr(F rejected) < \u2211m i=0 ( m i ) \u03c3m\u2212i(1\u2212\u03c3)i\u03bei. Since\u2211m\ni=0 ( m i ) \u03c3m\u2212i(1 \u2212 \u03c3)i\u03bei = (\u03c3 + (1 \u2212 \u03c3)\u03be)m, \u03c3 + (1 \u2212 \u03c3)\u03be < 1 and m > \u2212 ln 3ln(\u03c3+(1\u2212\u03c3)\u03be) , so Pr(F rejected) < 13 .\nHence, NP \u2286 RP, if a polynomial (\u03be, \u03c3) up-bounded approximate rejection algorithm exists. 2\nSince RP \u2286 P/poly [2] and NP \u2286 P/poly \u21d2 PH = \u03a32 [33, 25], if a polynomial (\u03be, \u03c3) up-bounded approximate rejection algorithm exists, then PH will collapse to \u03a32. It is widely believed that PH does not collapse to \u03a32, thus a polynomial (\u03be, \u03c3) up-bounded approximate algorithm is unlikely to exist. Furthermore Theorem 1 tells that all tractable importance sampling algorithms may fail to generate sufficient samples in polynomial time for certain cases. Another implication of Theorem 2 is that for any tractable importance function I sampling order \u03b4, no polynomial approximate rejection algorithm satisfies \u2203\u03be \u03c3 : 0 < \u03be < 1 \u2227 0 \u2264 \u03c3 < 1, for any BN and valid e such that Pr\u0393\u0302I (\u2126e) < \u03be PrI(\u2126e) with probability larger than 1\u2212\u03c3 unless PH collapses to \u03a32. Because PrI(\u2126e) < 1, so Pr\u0393\u0302I (\u2126e) < \u03be PrI(\u2126e) \u21d2 Pr \u0393\u0302 I (\u2126e) < \u03be. In other words, no polynomial approximate rejection algorithm can help importance sampling to reduce inconsistent samples with high probability.\nBecause (\u03be, 0) up-bounded is equivalent to \u03be upbounded, It is straightforward to get the corollary 2.\nCorollary 2 If there exists a polynomial \u03be up-bounded approximate rejection algorithm for some tractable importance function I with tractable sampling order \u03b4, then NP \u2286 RP."}], "references": [{"title": "Random k-SAT: Two moments suffice to cross a sharp threshold", "author": ["D. Achlioptas", "C. Moore"], "venue": "SIAM J. Comput.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Two theorems on random polynomial time", "author": ["L. Adleman"], "venue": "SFCS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1978}, {"title": "MUNIN \u2014 an expert EMG assistant", "author": ["S. Andreassen", "F. Jensen", "S. Andersen", "B. Falck", "U. Kj\u00e6rulff", "M. Woldbye", "A.R. Sorensen", "A. Rosenfalck"], "venue": "Computer-Aided Electromyography and Expert Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1989}, {"title": "Computational Complexity: A Modern Approach", "author": ["S. Arora", "B. Barak"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Evaluation of Probabilistic Inference System, 2006. http://ssli.ee.washington.edu/~bilmes/ uai06InferenceEvaluation", "author": ["J. Bilmes", "R. Dechter"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Probabilistic analysis of a generalization of the unit clause literal selection heuristic for the k-satisfiability problem", "author": ["M. Chao", "J. Franco"], "venue": "Information Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks", "author": ["J. Cheng", "M.J. Druzdzel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Mick gets some (the odds are on his side) (satisfiability)", "author": ["V. Chvatal", "B. Reed"], "venue": "SFCS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "A better algorithm for random k-SAT", "author": ["A. Coja-Oghlan"], "venue": "SIAM Journal on Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "On-line student modeling for coached problem solving using Bayesian networks", "author": ["C. Conati", "A.S. Gertner", "K. VanLehn", "M.J. Druzdzel"], "venue": "In Proceedings of the Sixth International Conference on User Modeling", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "The computational complexity of probabilistic inference using Bayesian belief networks", "author": ["G.F. Cooper"], "venue": "Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Approximating probabilistic inference in Bayesian belief networks is NP-hard", "author": ["P. Dagum", "M. Luby"], "venue": "Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "An optimal approximation algorithm for Bayesian inference", "author": ["P. Dagum", "M. Luby"], "venue": "Artif. Intell.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Mixtures of deterministic-probabilistic networks and their and/or search space", "author": ["R. Dechter", "R. Mateescu"], "venue": "Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Probabilistic analysis of the Davis Putnam procedure for solving the satisfiability problem", "author": ["J. Franco", "M. Paull"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1983}, {"title": "Necessary and sufficient conditions for sharp thresholds of graph properties and k- SAT problem", "author": ["E. Friedgut"], "venue": "J. Amer. Math. Soc.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "A survey of research in deliberative real-time artificial intelligence", "author": ["A.J. Garvey", "V.R. Lesser"], "venue": "Real-Time Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "The log-support encoding of CSP into SAT", "author": ["M. Gavanelli"], "venue": "In Proceedings of the 13th international conference on principles and practice of constraint programming,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Approximate inference algorithms for hybrid Bayesian networks with discrete constraints", "author": ["V. Gogate", "R. Dechter"], "venue": "In Proceedings of Uncertainty in Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "A new algorithm for sampling CSP solutions uniformly at random", "author": ["V. Gogate", "R. Dechter"], "venue": "In International Conference on Principles and Practice of Constraint Programming (CP),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Approximate counting by sampling the backtrack-free search space", "author": ["V. Gogate", "R. Dechter"], "venue": "In Conference on Artificial Intelligence (AAAI),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "SampleSearch: Importance sampling in presence of determinism", "author": ["V. Gogate", "R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "A survey on algorithms for real-time Bayesian network inference. In In the joint AAAI-02/KDD-02/UAI-02 workshop on Real-Time Decision Support and Diagnosis", "author": ["H. Guo", "W. Hsu"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Toward normative expert systems: Part I the pathfinder project", "author": ["D.E. Heckerman", "E.J. Horvitz", "B.N. Nathwani"], "venue": "Methods of Information in Medicine,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1992}, {"title": "Turing machines that take advice", "author": ["R. Karp", "R. Lipton"], "venue": "L\u2019Enseignement Mathe\u0301matiques,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1982}, {"title": "Mixed deterministic and probabilistic networks", "author": ["R. Mateescu", "R. Dechter"], "venue": "Annals of Mathematics and Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Dynamic importance sampling in Bayesian networks based on probability trees", "author": ["S. Moral", "A. Salmer\u00f3n"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "Knowledge engineering for large belief networks", "author": ["M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion"], "venue": "In Proceedings of the 10th conference on Uncertainty in artificial intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1994}, {"title": "Simulation and Monte Carlo Method", "author": ["R.Y. Rubinstein"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1981}, {"title": "Artificial intelligence: A modern approach", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1995}, {"title": "Simulation approaches to general probabilistic inference on belief networks", "author": ["R.D. Shachter", "M.A. Peot"], "venue": "In Proceedings of the 5th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1990}, {"title": "Halting space-bounded computations", "author": ["M. Sipser"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1980}, {"title": "Refractor importance sampling", "author": ["H. Yu", "R. van Engelen"], "venue": "In Proceedings of the 24th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "An importance sampling algorithm based on evidence prepropagation", "author": ["C. Yuan", "M.J. Druzdzel"], "venue": "In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2003}, {"title": "Theoretical analysis and practical insights on importance sampling in Bayesian networks", "author": ["C. Yuan", "M.J. Druzdzel"], "venue": "Int. J. Approx. Reasoning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2007}], "referenceMentions": [{"referenceID": 27, "context": "A Bayesian network, belief network, or directed acyclic graphical model, is a probabilistic graph model [28] that has gained wide acceptance in several areas of", "startOffset": 104, "endOffset": 108}, {"referenceID": 30, "context": "artificial intelligence [31].", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "Exact and approximate probabilistic Bayesian inference is known to be NP-hard [11, 12].", "startOffset": 78, "endOffset": 86}, {"referenceID": 11, "context": "Exact and approximate probabilistic Bayesian inference is known to be NP-hard [11, 12].", "startOffset": 78, "endOffset": 86}, {"referenceID": 16, "context": "Approximate inference algorithms are popular due to their anytime property [17] to produce an approximate result, possibly in real time [23].", "startOffset": 75, "endOffset": 79}, {"referenceID": 22, "context": "Approximate inference algorithms are popular due to their anytime property [17] to produce an approximate result, possibly in real time [23].", "startOffset": 136, "endOffset": 140}, {"referenceID": 30, "context": "For this reason, stochastic sampling algorithms, especially importance sampling, are among the most widely-used approximate inference methods [31].", "startOffset": 142, "endOffset": 146}, {"referenceID": 31, "context": "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 17, "endOffset": 21}, {"referenceID": 6, "context": "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 30, "endOffset": 33}, {"referenceID": 26, "context": "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 39, "endOffset": 43}, {"referenceID": 33, "context": "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 49, "endOffset": 53}, {"referenceID": 34, "context": "Examples are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "The local variance bound (LVB) [13] metric demarcates the boundary between the class of Bayesian networks with tractable approximations and those with intractable approximations.", "startOffset": 31, "endOffset": 35}, {"referenceID": 2, "context": "This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.", "startOffset": 121, "endOffset": 124}, {"referenceID": 23, "context": "This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.", "startOffset": 137, "endOffset": 141}, {"referenceID": 9, "context": "This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.", "startOffset": 149, "endOffset": 153}, {"referenceID": 28, "context": "This means that the LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.", "startOffset": 167, "endOffset": 171}, {"referenceID": 29, "context": "For more details refer to [30, 36].", "startOffset": 26, "endOffset": 34}, {"referenceID": 35, "context": "For more details refer to [30, 36].", "startOffset": 26, "endOffset": 34}, {"referenceID": 6, "context": "This is true for AIS-BN [7], EPIS-BN [35], and SIS [32].", "startOffset": 24, "endOffset": 27}, {"referenceID": 34, "context": "This is true for AIS-BN [7], EPIS-BN [35], and SIS [32].", "startOffset": 37, "endOffset": 41}, {"referenceID": 31, "context": "This is true for AIS-BN [7], EPIS-BN [35], and SIS [32].", "startOffset": 51, "endOffset": 55}, {"referenceID": 26, "context": "By contrast, the tractable sampling order of DIS [27] is a reversed elimination order, which may or may not be consistent with a Bayesian network\u2019s topological order of the vertices.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "Cooper [11] proved that computing Pr(x) for any x is NP-hard in general.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": "Dagum and Luby [12] proved that approximate Bayesian inference is NP-hard.", "startOffset": 15, "endOffset": 19}, {"referenceID": 14, "context": "Franco and Paull [15] first observed, among other important results, that random instances of the k-SAT problem undergo a \u201cphase transition\u201d as the ratio of clauses to variables passes through a threshold.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "Franco and Paull [15] claim that F k is with high probability (w.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "stone paper [16], Friedgut used the second moment method to prove the existence of a nonuniform satisfiability threshold, i.", "startOffset": 12, "endOffset": 16}, {"referenceID": 15, "context": "Inspired by [16], Dimitris and Cristopher [1] further narrowed the threshold around O(2k\u22121 ln 2).", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "Inspired by [16], Dimitris and Cristopher [1] further narrowed the threshold around O(2k\u22121 ln 2).", "startOffset": 42, "endOffset": 45}, {"referenceID": 5, "context": "In [6] and [8] this method is used to narrow the lower bound of rk to O(2/k).", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In [6] and [8] this method is used to narrow the lower bound of rk to O(2/k).", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "The current best result is from [9], which not only presents a polynomial algorithm that finds a satisfying truth assignment w.", "startOffset": 32, "endOffset": 35}, {"referenceID": 17, "context": "Convert all variables V to Boolean variables by log encoding [18].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them.", "startOffset": 6, "endOffset": 9}, {"referenceID": 23, "context": "Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them.", "startOffset": 22, "endOffset": 26}, {"referenceID": 9, "context": "Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them.", "startOffset": 34, "endOffset": 38}, {"referenceID": 28, "context": "Munin [3], Pathfinder [24], Andes [10] and CPCS [29] are networks with significant levels of determinism, suggesting difficulties with high rejection rates to sample them.", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "mark networks from the UAI contest [5] is shown in Table 2.", "startOffset": 35, "endOffset": 38}, {"referenceID": 31, "context": "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 38, "endOffset": 42}, {"referenceID": 6, "context": "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 51, "endOffset": 54}, {"referenceID": 26, "context": "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "Example sampling improvements are SIS [32], AIS-BN [7], DIS [27], RIS [34] and EPIS-BN [35].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "The rejection problem in importance sampling has been extensively studied in the work on adaptive sampling schemes [7], in the context of constraint propagation [20], and Boolean satisfiability problems [21].", "startOffset": 115, "endOffset": 118}, {"referenceID": 19, "context": "The rejection problem in importance sampling has been extensively studied in the work on adaptive sampling schemes [7], in the context of constraint propagation [20], and Boolean satisfiability problems [21].", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "The rejection problem in importance sampling has been extensively studied in the work on adaptive sampling schemes [7], in the context of constraint propagation [20], and Boolean satisfiability problems [21].", "startOffset": 203, "endOffset": 207}, {"referenceID": 18, "context": "A restricted form of constraint propagation can be used to reduce the amount of rejection [19].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "An approach to circumvent the rejection problem by systematically searching for a nonzero weight sample for constraintbased systems was introduced in [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 20, "context": "The proposed backtracking algorithm, SampleSearch was further improved in [21] and shown to generate a backtrack-free distribution.", "startOffset": 74, "endOffset": 78}, {"referenceID": 21, "context": "In [22], the SampleSearch method is fur-", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "ther generalized as a scheme in the framework of mixed networks [14, 26].", "startOffset": 64, "endOffset": 72}, {"referenceID": 25, "context": "ther generalized as a scheme in the framework of mixed networks [14, 26].", "startOffset": 64, "endOffset": 72}, {"referenceID": 12, "context": "The LVB [13] metric demarcates the boundary between the class of Bayesian networks with tractable approximations and those with intractable approximations.", "startOffset": 8, "endOffset": 12}, {"referenceID": 2, "context": "LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.", "startOffset": 101, "endOffset": 104}, {"referenceID": 23, "context": "LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "LVB is undefined for many real-world networks that model JPDs with zero probabilities, such as Munin [3], Pathfinder [24], Andes [10] and the CPCS [29] networks.", "startOffset": 147, "endOffset": 151}], "year": 2011, "abstractText": "Approximate Bayesian inference is NP-hard. Dagum and Luby defined the Local Variance Bound (LVB) to measure the approximation hardness of Bayesian inference on Bayesian networks, assuming the networks model strictly positive joint probability distributions, i.e. zero probabilities are not permitted. This paper introduces the k-test to measure the approximation hardness of inference on Bayesian networks with deterministic causalities in the probability distribution, i.e. when zero conditional probabilities are permitted. Approximation by stochastic sampling is a widely-used inference method that is known to suffer from inefficiencies due to sample rejection. The k-test predicts when rejection rates of stochastic sampling a Bayesian network will be low, modest, high, or when sampling is intractable.", "creator": "TeX"}}}