{"id": "1202.3890", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2012", "title": "PAC Bounds for Discounted MDPs", "abstract": "We study upper and lower bounds on the sample-complexity of learning near-optimal behaviour in finite-state discounted Markov Decision Processes (MDPs). For the upper bound we make the assumption that each action leads to at most two possible next-states and prove a new bound for a UCRL-style algorithm on the number of time-steps when it is not Probably Approximately Correct (PAC). The new lower bound strengthens previous work by being both more general (it applies to all policies) and tighter (it extends the idea of the Bayesian model). This is particularly strong when the sample-complexity is the most significant, because each step in the model makes the model slightly better than the minimum (it's not exactly always correct for an important variable that may be wrong). The approach has been criticised for underestimating the accuracy of the Bayesian model. In order to test whether we can use it as a measure of how many times the average error is avoided we need to include a different model to test whether we can use it as a measure of how long the error is avoided.\n\n\n\nThe Bayesian model was developed with the use of two different methods to find and compare different responses to different hypotheses. One way to do this was to compare two different hypotheses about why it is likely to change or change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than not to change more likely than", "histories": [["v1", "Fri, 17 Feb 2012 11:59:55 GMT  (23kb)", "http://arxiv.org/abs/1202.3890v1", "25 LaTeX pages"]], "COMMENTS": "25 LaTeX pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "marcus hutter"], "accepted": false, "id": "1202.3890"}, "pdf": {"name": "1202.3890.pdf", "metadata": {"source": "CRF", "title": "PAC Bounds for Discounted MDPs", "authors": ["Tor Lattimore", "Marcus Hutter"], "emails": ["tor.lattimore@anu.edu.au", "marcus.hutter@anu.edu.au"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 2.\n38 90\nv1 [\ncs .L\nG ]\n1 7\nContents 1 Introduction 2 2 Notation 2 3 Estimation 3 4 Upper Confidence Reinforcement Learning Algorithm 3 5 Upper PAC Bounds 5 6 Eliminating the Assumption 11 7 Lower PAC Bound 11 8 Conclusion 13 References 13 A Proof of Lower PAC Bound 14 B Technical Results 18 C Proof of Lemma 8 20 D Constants 23 E Table of Notation 24\nKeywords\nReinforcement learning; sample-complexity; exploration exploitation; PAC-MDP; Markov decision processes."}, {"heading": "1 Introduction", "text": "The goal of reinforcement learning is to construct algorithms that learn to act optimally, or nearly so, in unknown environments. In this paper we restrict our attention to finite state discounted MDPs with unknown transitions. The performance of reinforcement learning algorithms in this setting can be measured in a number of ways, for instance by using regret or PAC bounds (Kakade, 2003). We focus on the latter, which is a measure of the number of time-steps where an algorithm is not near-optimal with high probability. Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesva\u0301ri, 2010; Auer, 2011).\nWe modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of\nO\u0303\n( |S \u00d7A|\n\u01eb2(1\u2212 \u03b3)3 log\n1\n\u03b4\n) .\nThis bound is an improvement1 on the previous best (Auer, 2011) and published best (Szita and Szepesva\u0301ri, 2010), which are\nO\u0303\n( |S \u00d7A|\n\u01eb2(1\u2212 \u03b3)4 log\n1\n\u03b4\n) and O\u0303 ( |S \u00d7A|\n\u01eb2(1\u2212 \u03b3)6 log\n1\n\u03b4\n)\nrespectively. The additional assumption is unfortunate and is probably unnecessary as discussed in Section 6.\nWe also present a matching (up to logarithmic factors) lower bound that is both larger and more general than the previous best given by Strehl et al. (2009). The class of MDPs used in the counter-example satisfy the assumption used in the upper bound."}, {"heading": "2 Notation", "text": "Unfortunately, we found it impossible to reduce the amount of notation and number of constants. While we have endeavoured to define everything before we use it, readers are encouraged to consult the tables of notation and constants found in the appendix.\nGeneral. N = {0, 1, 2, \u00b7 \u00b7 \u00b7 } is the natural numbers. For the indicator function we write [[x = y]] = 1 if x = y and 0 if x 6= y. We use \u2227 and \u2228 for logical and/or respectively. If A is a set then |A| is its size and A\u2217 is the set of all finite ordered subsets. Unless otherwise mentioned, log represents the natural logarithm. For random variable X we write EX and VarX for its expectation and variance respectively. We make frequent use of the progression zi = 2 i \u2212 2 for i \u2265 1. Define a set Z(a) := {zi : 1 \u2264 i \u2264 argmini {zi \u2265 a}}.\nMarkov Decision Process. An MDP is a tuple M = (S,A, p, r, \u03b3) where S and A are finite sets of states and actions respectively. r : S \u2192 [0, 1] is the reward function. p : S \u00d7A \u00d7 S \u2192 [0, 1] is the transition function and \u03b3 \u2208 (0, 1) the discount rate. A\n1In this slightly restricted setting.\nstationary policy \u03c0 is a function \u03c0 : S \u2192 A mapping a state to an action. We write ps \u2032\ns,a as\nthe probability of moving from state s to s\u2032 when taking action a and ps \u2032 s,\u03c0 := p s\u2032 s,\u03c0(s). The\nvalue of policy \u03c0 in M and state s is V \u03c0M (s) := r(s) + \u03b3 \u2211 s\u2032\u2208S p s\u2032 s,\u03c0V \u03c0 M (s\n\u2032). We view V \u03c0M either as a function V \u03c0M : S \u2192 R or a vector V \u03c0 M \u2208 R |S| and similarly ps,a \u2208 [0, 1] |S| is a vector. The optimal policy of M is defined \u03c0\u2217M := argmax\u03c0 V \u03c0 M . Common MDPs are M , M\u0302 and M\u0303 , which represent the true MDP, the estimated MDP using empirical transition probabilities and a model. We write V := VM , V\u0302 := VM\u0302 and V\u0303 := VM\u0303 for their values respectively. Similarly, \u03c0\u0302\u2217 := \u03c0\u2217\nM\u0302 and in general, variables with an MDP as a subscript\nwill be written with a hat, tilde or nothing as appropriate and the subscript omitted."}, {"heading": "3 Estimation", "text": "In the next section we will introduce the new algorithm, but first we give an intuitive introduction to the type of parameter estimation required to prove sample-complexity bounds for MDPs. The general idea is to use concentration inequalities to show the empiric estimate of a transition probability approaches the true probability exponentially fast in the number of samples gathered. There are a wide variety of concentration inequalities, each catering to a slightly different purpose. We improve on previous work by using Bernstein\u2019s inequality, which takes variance into account (unlike Hoeffding). The following example demonstrates the need for Bernstein\u2019s inequality when estimating the value functions of MDPs. It also gives insight into the workings of the proof in the next two sections.\ns0 r = 1\ns1 r = 0\n1\u2212 p\np\n1\u2212 q\nq Consider the Markov reward process on the right with two states where rewards are shown inside the states and transition probabilities on the edges. Note this is not an MDP because there are no actions. We are only concerned with how well the value can be approximated. Assume p > \u03b3, q arbitrarily large (but not 1) and let p\u0302 be the empiric estimate of p and consider the error in our estimated value and the true value while in state s0. One can show that\n\u2223\u2223\u2223V (s0)\u2212 V\u0302 (s0) \u2223\u2223\u2223 \u2248 |p\u0302\u2212 p|\n(1\u2212 \u03b3)2 . (1)\nTherefore if V \u2212 V\u0302 is to be estimated to within \u01eb accuracy, we need |p\u0302 \u2212 p| < \u01eb(1 \u2212 \u03b3)2. Now suppose we bound |p\u0302\u2212p| via a standard Hoeffding bound, then with high probability |p\u0302\u2212p| . \u221a L/n where n is the number of visits to state s0 and L = log(1/\u03b4). Therefore to obtain an error less than \u01eb(1\u2212 \u03b3)2 we need n > L \u01eb2(1\u2212\u03b3)4 visits to state s0, which is already too many for a bound in terms of 1/(1 \u2212 \u03b3)3. If Bernstein\u2019s inequality is used instead, then |p\u0302 \u2212 p| . \u221a\nLp(1\u2212 p)/n and so n > Lp(1\u2212p)\u01eb2(1\u2212\u03b3)4 is required, but Equation (1) depends\non p > \u03b3. Therefore n > L \u01eb2(1\u2212\u03b3)3 visits are sufficient. If p < \u03b3 then Equation (1) can be improved."}, {"heading": "4 Upper Confidence Reinforcement Learning Algorithm", "text": "UCRL is based on the optimism principle for solving the exploration/exploitation dilemma. It is model-based in the sense that at each time-step the algorithm acts according to a\nmodel (in this case an MDP, M\u0303) chosen from a model class. The idea is to choose the smallest model class guaranteed to contain the true model with high probability and act according to the most optimistic model within this class. With a good choice of model class this guarantees a policy that biases its exploration towards unknown states that may yield good rewards while avoiding states that are known to be bad. The approach has been successful in obtaining uniform sample complexity (or regret) bounds in various domains where the exploration/exploitation problem is an issue (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al., 2010; Auer, 2011).\nUnfortunately, to prove our new bound we needed to make an assumption about the transition probabilities of the true MDP. We do not believe this assumption is crucial, but it substantially eases the analysis by removing some dependencies in the more general problem. In Section 6 we present an approach to remove the assumption as well as some intuition into why this ought to be possible, but non-trivial.\nAssumption 1. The true unknown MDP, M , satisfies ps \u2032 s,a = 0 for all but two s \u2032 \u2208 S denoted sa+, sa\u2212 \u2208 S.2\nThe pseudo-code of UCRL can be found below, but first we define a knownness index, \u03ba. If n is the number of times a state/action pair has been visited then \u03ba(\u03b9, n) is the knownness of that state/action pair at level \u03b9. The knownness of a state increases with the number of visits, is bounded by |S| and is always a natural number. The reason for defining these now is that UCRL will only perform an update when the knownness index of some states would be changed by an update. Unfortunately, the definition below is unlikely to be very intuitive. A more thorough explanation of knownness is given in Section 5.\nDefinition 2 (Knownness). Define constants\nwmin := \u01eb(1\u2212 \u03b3)\n4|S| w\u03b9 := 2\n\u03b9wmin \u03b9max :=\n\u2308 1\nlog 2 log\n8|S|\n\u01eb(1\u2212 \u03b3)2\n\u2309\nI := {0, 1, \u00b7 \u00b7 \u00b7 , \u03b9max} K := Z(|S|).\nWe define the knownness index, \u03ba : I \u00d7 N \u2192 K by\n\u03ba(\u03b9, n) := max { z \u2208 K : z \u2264 n\nw\u03b9m\n} ,\nwhere m \u2208 O\u0303 (\n1 \u01eb2(1\u2212\u03b3)2 log |S\u00d7A| \u03b4\n) is defined in Appendix D.\nNote that the existence of the function ExtendedValueIteration is proven and an algorithm given by Strehl and Littman (2008).\n2Note that sa+ and sa\u2212 are dependent on (s, a) and are known to the algorithm.\nAlgorithm 1 UCRL\n1: t = 1, k = 1, n(s, a) = n(s, a, s\u2032) = 0 for all s, a, s\u2032 and s1 is the start state. 2: H := 11\u2212\u03b3 log 8|S| \u01eb(1\u2212\u03b3) , L1 := log 2 \u03b41 and \u03b41 := \u03b4 2|S\u00d7A|2|K\u00d7I| 3: loop 4: p\u0302sa +\ns,a := n(s, a)/max {1, n(s, a, sa +)} and p\u0302sa\n\u2212 s,a := 1\u2212 p\u0302 sa+ s,a\n5: Mk := { M\u0303 : |p\u0303sa + s,a \u2212 p\u0302 sa+ s,a | \u2264 ConfidenceInterval(p\u0303 sa+ s,a , n(s, a)), \u2200(s, a) }\n6: M\u0303 = ExtendedValueIteration(Mk) 7: \u03c0k = \u03c0\u0303 \u2217 8: v(s, a) = v(s, a, s\u2032) = 0 for all s, a, s\u2032\n9: while \u03ba(\u03b9, n(s, a) + v(s, a)) = \u03ba(\u03b9, n(s, a)), \u2200(s, a), \u03b9 \u2208 I do 10: Act 11: Delay and Update 12: function Delay 13: for j = 1 \u2192 H do 14: Act 15: function Update 16: n(s, a) = n(s, a) + v(s, a) and n(s, a, s\u2032) = n(s, a, s\u2032) + v(s, a, s\u2032) \u2200s, a, s\u2032 and k = k + 1\n17: function Act 18: at = \u03c0k(st) 19: st+1 \u223c pst,at \u22b2 Sample from MDP 20: v(st, at) = v(st, at) + 1 and v(st, at, st+1) = v(st, at, st+1) + 1 and t = t+ 1 21: function ExtendedValueIteration(M) 22: return optimistic M\u0303 \u2208 M such that V \u2217 M\u0303 (s) \u2265 V \u2217 M\u0303 \u2032 (s) for all s \u2208 S and M\u0303 \u2032 \u2208 M. 23: function ConfidenceInterval(p, n)\n24: return min\n{\u221a 2L1p(1\u2212p)\nn + 2L13n , \u221a L1 2n\n}"}, {"heading": "5 Upper PAC Bounds", "text": "We present two new PAC bounds. The first improves on all previous analysis, but relies on Assumption 1. The second is completely general, but gains an additional dependence on |S| leading to a PAC bound in terms of |S|2 and 1/(1\u2212 \u03b3)3. This bound is worse than the previous best in terms of |S|, but better in terms 1/(1 \u2212 \u03b3).\nTheorem 3. Let M be the true MDP satisfying Assumption 1. Let \u03c0 be the actual (nonstationary) policy of UCRL (Algorithm 1), then V \u2217(st)\u2212 V \u03c0(st) > \u01eb for at most\nHUmax +HEmax \u2208 \u00d8 |S \u00d7A|\n\u01eb2(1\u2212 \u03b3)3 log\n|S \u00d7A|\n\u03b4\u01eb(1 \u2212 \u03b3) log2 |S| log2\n|S|\n\u01eb(1\u2212 \u03b3) log2 log\n1\n1\u2212 \u03b3\ntime-steps with probability at least 1\u2212 \u03b4. (Umax and Emax are defined in Appendix D.)\nNote that although \u03c0k is stationary, the global policy of UCRL is non-stationary. Despite this, we will abuse notation by allowing ourselves to write V \u03c0(st), whereas really V \u03c0 should depend on the entire history. Fortunately, when UCRL is not delaying, the policy \u03c0 is nearly stationary in the sense that it will be so for the next H time-steps. This allows us to work almost entirely with stationary policies and so discard the cumbersome notation required for non-stationary policies.\nTheorem 4. Let M be the true MDP (possibly not satisfying Assumption 1) then there exists a policy \u03c0 such that V \u2217(st) \u2212 V \u03c0(st) > \u01eb for at most |S| log 3 |S|(EmaxH + UmaxH) time-steps with probability at least 1\u2212 \u03b4.\nThe proof of Theorem 4 is omitted, but follows easily by converting an arbitrary MDP with |S| states into a functionally equivalent MDP with O(|S|2) states that satisfies Assumption 1. This is done by adding a tree of 2|S| states for each state/action pair and rescaling \u03b3.\nProof Overview. The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesva\u0301ri (2010).\n1. Bound the number of updates by |S \u00d7A| log |S\u00d7A||K\u00d7I| , which follows from the algorithm and the definition of knownness. This bounds the number of delaying time-steps to O\u0303( 11\u2212\u03b3 |S \u00d7A| log |S\u00d7A| |K\u00d7I| ) time-steps, which is insignificant from the point of view of\nTheorem 3.\n2. Show that the true MDP remains in the model class Mk for all k.\n3. Use the optimism principle to show that if M \u2208 Mk and V \u2217 \u2212 V \u03c0 > \u01eb then |V\u0303 \u03c0k \u2212\nV \u03c0k | > \u01eb/2. This key fact shows that if \u03c0 is not nearly-optimal at some timestep t then the true value and model value of \u03c0k differ and so some information is (probably) gained by following this policy.\n4. The final component is to bound the number of time-steps when \u03c0 is not nearlyoptimal.\nEpisodes and phases. UCRL operates in episodes, which are blocks of time-steps ending when update is called. The length of each episode is not fixed, instead, an episode ends when the knownness of a state changes. We often refer to time-step t and episode k and unless there is ambiguity we will not define k and just assume it is the episode in which t resides. A delay phase is the period of H contiguous time-steps where UCRL is in the function delay, which happens immediately before an update. An exploration phase is a period of H time-steps starting at t where t is not in a delay phase and where V\u0303 \u03c0k(st) \u2212 V\n\u03c0k(st) \u2265 \u01eb/2. Exploration phases do note overlap. More formally, the starts of exploration phases, t1, t2, \u00b7 \u00b7 \u00b7 , are defined inductively\nt1 := min { t : V\u0303 \u03c0k(st)\u2212 V \u03c0k(st) \u2265 \u01eb/2 \u2227 t is not in a delay phase } ti := min { t : t \u2265 ti\u22121 +H \u2227 V\u0303 \u03c0k(st)\u2212 V \u03c0k(st) \u2265 \u01eb/2 \u2227 t is not in a delay phase } .\nNote there need not, and with high probability will not, be infinitely many such ti. The exploration phases are only used in the analysis, they are not known to UCRL.\nWeights and variances. We define the weight3 of state/action pair (s, a) as follows.\nw\u03c0(s, a|s\u2032) := [[(s\u2032, \u03c0(s\u2032)) = (s, a)]] + \u03b3 \u2211\ns\u2032\u2032\nps \u2032\u2032 s\u2032,\u03c0(s\u2032)w \u03c0(s, a|s\u2032\u2032) wt(s) := w \u03c0k(s, \u03c0k(s)|st).\n3Also called the discounted future state distribution in Kakade (2003).\nAs usual, w\u0303 and w\u0302 are defined as above but with p replaced by p\u0303 and p\u0302 respectively. Think of wt(s) as the expected number of discounted visits to state/action pair (s, \u03c0k(s)) while following policy \u03c0k starting in state st. The important point is that this value is approximately equal to the expected number of visits to state/action pair (s, \u03c0k(s)) within the next H time-steps. We also define the local variance of the value function. These measure the variability of values while following policy \u03c0.\n\u03c3\u03c0(s)2 := ps,\u03c0 \u00b7 V \u03c02 \u2212 [ps,\u03c0 \u00b7 V \u03c0]2 \u03c3\u0303\u03c0(s)2 := p\u0303s,\u03c0 \u00b7 V\u0303 \u03c02 \u2212 [p\u0303s,\u03c0 \u00b7 V\u0303 \u03c0]2.\nThe active set. We will shortly see that states with small wt(s) cannot influence the differences in value functions. Thus we define an active set of states where wt(s) is not tiny. At each time-step t define the active set Xt by\nXt := { s : wt(s) > \u01eb(1\u2212 \u03b3)\n4|S| =: wmin\n} .\nKnownness. We now expand on the concept of knownness and explain its purpose. We write nt(s, a) for the value of n(s, a) at time-step t and nt(s) := nt(s, \u03c0k(s)) where k is the episode associated with time-step t. Let t be some non-delaying time-step and suppose s is active (s \u2208 Xt). Now let \u03b9t(s) := argmin\u03b9wt(s) > w\u03b9 and note that \u03b9t(s) \u2208 I. We define a partition of the active set Xt by\nKt(\u03ba, \u03b9) := {s \u2208 Xt : \u03b9t(s) = \u03b9 \u2227 \u03bat(\u03b9t(s), nt(s)) = \u03ba} .\nThe set Kt(\u03ba, \u03b9) represents a set of states that have comparable weights and visit counts. We will show that if |Kt(\u03ba, \u03b9)| \u2264 \u03ba for all \u03ba, \u03b9 then the values V\u0303 and V are reasonably close. This result forms a key stage in the proof of Theorem 3 because it shows that if \u03c0 is not nearly-optimal at time-step t then there exists a Kt(\u03ba, \u03b9) that is quite large and where states have not been visited sufficiently. Furthermore, the weights wt(s) where s \u2208 Kt(\u03ba, \u03b9) are large enough that some learning is expected to occur.\nAnalysis. The proof of Theorem 3 follows easily from three key lemmas.\nLemma 5. The following hold:\n1. The total number of updates is bounded by Umax := |S \u00d7A| log |S\u00d7A| |K\u00d7I| . 2. If M \u2208 Mk and t is not in a delay phase and V \u2217(st)\u2212 V \u03c0(st) > \u01eb then\nV\u0303 \u03c0k(st)\u2212 V \u03c0k(s) > \u01eb/2.\nLemma 6. M \u2208 Mk for all k with probability at least 1\u2212 \u03b4/2.\nLemma 7. The number of exploration phases is bounded by Emax with probability at least 1\u2212 \u03b4/2.\nThe proofs of the lemmas are delayed while we apply them to prove Theorem 3.\nProof of Theorem 3. By Lemma 6, M \u2208 Mk for all k with probability 1 \u2212 \u03b4/2. By Lemma 7 we have that the number of exploration phases is bounded by Emax with probability 1\u2212 \u03b4/2. Now if t is not in a delaying or exploration phase and M \u2208 Mk then by Lemma 5, \u03c0 is nearly-optimal. Finally note that the number of updates is bounded by\nUmax and so the number of time-steps in delaying phases is at most HUmax. Therefore UCRL is nearly-optimal for all but HUmax +HEmax time-steps with probability 1\u2212 \u03b4.\nWe now turn our attention to proving Lemmas 5, 6 and 7. Of these, only Lemma 7 presents a substantial challenge.\nProof of Lemma 5. For part 1 we note that for \u03b9 \u2208 I the knownness of a state/action pair at level \u03b9 satisfies \u03ba \u2208 K. Since the knownness index for each \u03b9 is non-decreasing and an update only occurs when an index is increased, the total number of updates is bounded by Umax := |S \u00d7A||K \u00d7 I|.\nThe proof of part 2 is closely related to the approach taken by Strehl and Littman (2008). Recall that M\u0303 is chosen optimistically by extended value iteration. This generates an MDP, M\u0303 , such that V \u2217 M\u0303 (s) \u2265 V \u2217 M\u0303 \u2032 (s) for all M\u0303 \u2032 \u2208 Mk. Since we have assumed M \u2208 Mk we have that V\u0303 \u03c0k(s) \u2261 V \u2217\nM\u0303 (s) \u2265 V \u2217M (s). Therefore V\u0303 \u03c0k(st)\u2212V \u03c0(st) > \u01eb. Finally\nnote that t is a non-delaying time-step and so policy \u03c0 will remain stationary and equal to \u03c0k for at least H time-steps. Using the definition of the horizon, H, we have that |V \u03c0(st)\u2212 V \u03c0k(st)| < \u01eb/2. Therefore V\u0303 \u03c0k(st)\u2212 V \u03c0k(st) > \u01eb/2 as required.\nProof of Lemma 6. In the previous lemma we showed that there are at most Umax updates. Therefore we only need to check M \u2208 Mk for each k up to Umax. Fix an (s, a) pair and apply the best of either Bernstein or Hoeffding inequalities to show that |p\u0302sa +\ns,a \u2212p sa+ s,a | \u2264 ConfidenceInterval(p\u0302 sa+ s,a \u2212p sa+ s,a , n(s, a))) with probability 1\u2212 \u03b41. Setting\n\u03b41 := \u03b4 2|S\u00d7A|Umax \u2261 \u03b42|S\u00d7A|2|K\u00d7I| and applying the union bound completes the proof.\nWe are now ready to work on Lemma 7. The proof follows from two lemmas:\n1. If t is the start of an exploration phase then there exists a (\u03ba, \u03b9) such that |Kt(\u03ba, \u03b9)| > \u03ba.\n2. If |Kt(\u03ba, \u03b9)| > \u03ba for sufficiently many t then sufficient information is gained that some state/action pair must have an increase in knownness.\nLemma 8. Let t be a non-delaying time-step and assume M \u2208 Mk. If |Kt(\u03ba, \u03b9)| \u2264 \u03ba for all \u03ba, \u03b9 \u2208 K then |V\u0303 \u03c0k(st)\u2212 V \u03c0k(st)| \u2264 \u01eb/2.\nThe full proof is long, technical and has been relegated to Appendix B. We provide a sketch, but first we need some useful results about MDPs and the differences in value functions.\nLemma 9. Let M and M\u0303 be two Markov decision processes differing only in transition probabilities and \u03c0 be a stationary policy then\nV \u03c0(st)\u2212 V\u0303 \u03c0(st) = \u03b3\n\u2211\ns\nwt(s)(ps,\u03c0 \u2212 p\u0303s,\u03c0) \u00b7 V\u0303 \u03c0.\nProof sketch. Drop the \u03c0 superscript and write V (st) = r(st) + \u03b3 \u2211 st+1 p st+1 st,\u03c0 V (st+1).\nThen V (st)\u2212 V\u0303 (st) = \u03b3[pst,\u03c0 \u2212 p\u0303st,\u03c0] \u00b7 V\u0303 + \u03b3 \u2211 st+1 p st+1 st,\u03c0 [V (st+1)\u2212 V\u0303 (st+1)]. The result is obtained by continuing to expand the second term of the right hand side.\nLemma 10. If M \u2208 Mk at time-step t and V\u0303 := V\u0303 \u03c0k then\n|(ps,\u03c0k \u2212 p\u0303s,\u03c0k) \u00b7 V\u0303 | \u2264\n\u221a 8L1\u03c3\u0303\u03c0k(s)2\nnt(s) +\n2\n1\u2212 \u03b3\n( L1\nnt(s)\n)3/4 +\n4L1 3nt(s)(1 \u2212 \u03b3) ,\nwhere \u03c3\u0303\u03c0k(s)2 := p\u0303s,a \u00b7 V\u0303 2 \u2212 [ p\u0303s,a \u00b7 V\u0303 ]2 .\nThe idea is to note that M,M\u0303 are in Mk and apply the definition of the confidence intervals. The full proof is subsumed in the proof of the more general Lemma 33 in Appendix C. The following lemma bounds the expected total discounted local variance. Lemma 11. For any stationary \u03c0 and M\u0303 , \u2211\ns\u2208S w\u0303t(s)\u03c3\u0303 \u03c0(s)2 \u2264 1 \u03b32(1\u2212\u03b3)2 .\nSee the paper of Sobel (1982) for a proof.\nProof sketch of Lemma 8. For ease of notation we drop references to \u03c0k. We approximate w(s) \u2248 w\u0303(s) and |(ps,\u03c0k \u2212 p\u0303s,\u03c0k) \u00b7 V\u0303 | . \u221a L1\u03c3\u0303(s)2 n(s) . Using Lemma 9\n|V\u0303 (st)\u2212 V (st)| \u2261 \u2223\u2223\u2223\u2223\u2223\u03b3 \u2211\ns\u2208S\nwt(s)(ps,\u03c0k \u2212 p\u0303s,\u03c0k) \u00b7 V\u0303 \u2223\u2223\u2223\u2223\u2223 . \u2223\u2223\u2223\u2223\u2223 \u2211\ns\u2208X\nwt(s)(ps,\u03c0k \u2212 p\u0303s,\u03c0k) \u00b7 V\u0303 \u2223\u2223\u2223\u2223\u2223 (2)\n. \u2211\ns\u2208X\nwt(s)\n\u221a L1\u03c3\u0303(s)2\nn(s) .\n\u2211\n\u03ba,\u03b9\u2208K\u00d7I\n\u2211\ns\u2208K(\u03ba,\u03b9)\n\u221a L1w\u0303t(s)\u03c3\u0303(s)2\n\u03bam (3)\n\u2264 \u2211\n\u03ba,\u03b9\u2208K\u00d7I\n\u221a\u221a\u221a\u221aL1|K(\u03ba, \u03b9)| \u03bam \u2211\ns\u2208K(\u03ba,\u03b9)\nw\u0303t(s)\u03c3\u0303(s)2 \u2264\n\u221a L1|K \u00d7 I|\nm\u03b32(1\u2212 \u03b3)2 , (4)\nwhere in Equation (2) we used Lemma 9 and the fact that states not in X are visited very infrequently. In Equation (3) we used the approximations for (p \u2212 p\u0303) \u00b7 V\u0303 , the definition of K(\u03ba, \u03b9) and the approximation w \u2248 w\u0303. In Equation (4) we used the Cauchy-Schwartz inequality,4 the fact that \u03ba \u2265 |K(\u03ba, \u03b9)| and Lemma 11. Substituting m := 20L1|K\u00d7I||D| 2\n\u01eb2(1\u2212\u03b3)2+2/\u03b2\ncompletes the proof. The extra terms in m are needed to cover the errors in the approximations made here.\nThe full proof requires formalising the approximations made at the start of the sketch above. The second approximation is comparatively easy while the showing that w(s) \u2248 w\u0303(s) requires substantial work.\nThe following lemmas are used to show that |Kt(\u03ba, \u03b9)| cannot be larger than \u03ba for too many time-steps with high probability. Combined with Lemma 8 above this will be sufficient to bound the number of exploration phases. Let t be the start of an exploration phase and define \u03bdt(s) to be the number of visits to state s within the next H time-steps. Formally, \u03bdt(s) := \u2211t+H\u22121 i=t [[st = s]].\nLemma 12. Let t be the start of an exploration phase and wt(s) \u2265 wmin then E\u03bdt(s) \u2265 wt(s)/2.\n4|\u30081, v\u3009| \u2264 \u20161\u20162 \u2016v\u20162.\nProof sketch. Use the definition of the horizon to show that wt(s) is not much larger than a bounded-horizon version. Compare E\u03bdt(s, \u03c0t(s)) and the definition of wt(s).\nLemma 13. Let N be as in Appendix D. If |Kti(\u03ba, \u03b9)| > \u03ba for 4N exploration phases t1, t2, \u00b7 \u00b7 \u00b7 , t4N then \u22114N i=1 \u2211 s\u2208Kti(\u03ba,\u03b9) \u03bdti(s, \u03c0(s)) \u2265 N\u03baw\u03b9 with probability at least 1\u2212 \u03b41.\nProof. As in the previous proof we drop \u03c0 superscripts and denote Ki := Kti(\u03ba, \u03b9). Define\n\u03bdi := \u2211\ns,a\u2208Ki\n\u03bdti(s) E\u03bdi = \u2211\ns\u2208Ki\nE\u03bdti(s).\nNow |Ki| > \u03ba and so by Lemma 12 we have E\u03bdi \u2265 \u03baw\u03b9/2. We now prepare to use Bernstein\u2019s inequality. Let Xi = \u03bdi \u2212 E\u03bdi, \u00b5 := 1 4N \u22114N i=1E\u03bdi and \u03c3 2 := 14N \u22114N\ni=1VarXi then\nP\n{ 4N\u2211\ni=1\n\u03bdi \u2264 Nw\u03b9\u03ba\n} \u2264 P { 4N\u2211\ni=1\n\u03bdi \u2264 4N\u2211\ni=1\nE\u03bdi/2\n}\n= P\n{ 4N\u2211\ni=1\n[\u03bdi \u2212E\u03bdi] \u2264 \u2212 4N\u2211\ni=1\nE\u03bdi/2\n} \u2264 2 exp ( \u2212\n4N\u00b52\n8\u03c32 + 16\u00b53(1\u2212\u03b3)\n) .\nSetting this equal to \u03b41 and solving for 4N gives\n4N \u2265 8\u03c32 + 16\u00b53(1\u2212\u03b3)\n\u00b52 log\n2\n\u03b41 =\n[ 8\u03c32\n\u00b52 +\n16\n3\u00b5(1\u2212 \u03b3)\n] log 2\n\u03b41 .\nNaively bounding \u03c32/\u00b52 \u2264 1/((1 \u2212 \u03b3)\u00b5) and noting that \u00b5 \u2265 wmin/2 leads to\n4N \u2265 14|S \u00d7A|\n\u01eb(1\u2212 \u03b3)2 log\n2\n\u03b41 .\nSince 4N satisfies this, the result is complete.\nProof of Lemma 7. We proceed in two stages. First we bound the total number of useful visits before |K(\u03ba, \u03b9)| \u2264 \u03ba. We then show this number of visits occurs after O\u0303(m) exploration phases with high probability.\nBounding the number of useful visits. A visit to state/action pair (s, a) in timestep t is (\u03ba, \u03b9)-useful if \u03ba(\u03b9, nt(s, a)) = \u03ba. Fixing a (\u03ba, \u03b9) we bound the number of (\u03ba, \u03b9)useful visits to state/action pair (s, a). Suppose t1 < t2 and \u03ba(\u03b9, nt1(s, a)) = \u03ba and nt2(s, a) \u2212 nt1(s, a) \u2265 mw\u03b9(2\u03ba + 2) then \u03ba(\u03b9, nt3(s, a)) > \u03ba for all t3 \u2265 t2. Therefore for each (\u03ba, \u03b9) pair there at most 6|S \u00d7A|mw\u03b9\u03ba visits that are (\u03ba, \u03b9)-useful.\nBounding the number of exploration phases. Let N := 6|S \u00d7A|m and t be the start of an exploration phase. Therefore V\u0303 \u03c0k(st) \u2212 V\n\u03c0k(st) > \u01eb/2 and so by Lemma 8 there exists a (\u03ba, \u03b9) \u2208 K such that |S| \u2265 |K(\u03ba, \u03b9)| > \u03ba. If |Kti(\u03ba, \u03b9)| > \u03ba at the start of 4N exploration phases, t1, t2, \u00b7 \u00b7 \u00b7 , t4N then by Lemma 13\nP    4N\u2211\ni=1\n\u2211\ns,a\u2208Kti(\u03ba,\u03b9)\nvti(s, a) \u2264 Nw\u03b9\u03ba    \u2264 \u03b41.\nTherefore by the union bound there are at most Emax := 4N |K \u00d7 I| exploration phases with probability 1\u2212 \u03b41|K \u00d7 I| \u2261 1\u2212 |K \u00d7 I|\n\u03b4 2|S\u00d7A|Umax > 1\u2212 \u03b4/2."}, {"heading": "6 Eliminating the Assumption", "text": "The upper bound in the previous section could only be proven using Assumption 1. In this section we describe a possible approach to generalising the proof and why this may be non-trivial. In the work above we used the assumption to bound (ps,\u03c0 \u2212 p\u0303s,\u03c0) \u00b7 V\u0303\n\u2217 .\u221a L1\u03c3\u0303\u03c0(s)2/n. A natural approach to generalising this comes from Bernstein\u2019s inequality (Theorem 30). If V \u03c0 \u2208 R|S| is a value function independent of p\u0302 then Bernstein\u2019s inequality can be used to show that (ps,\u03c0 \u2212 p\u0302s,\u03c0) \u00b7 V \u03c0 . \u221a L1\u03c3\u03c0(s)2/n. This suggests we adjust our model class by letting \u03c0 := \u03c0\u0303\u2217 and changing the condition to (p\u0303s,\u03c0 \u2212 p\u0302s,\u03c0) \u00b7 V\u0303 \u2217 .\u221a\nL1\u03c3\u0303\u03c0(s)2/n. We might then bound (ps,\u03c0\u2212 p\u0303s,\u03c0) \u00b7 V\u0303 \u2217 \u2261 (ps,\u03c0\u2212 p\u0302s,\u03c0) \u00b7 V\u0303 \u2217+(p\u0302s,\u03c0\u2212 p\u0303s,\u03c0) \u00b7 V\u0303 \u2217. The right term is then bounded by the conditions on the model class and the left term can perhaps be bounded by noting that ps,\u03c0 is the true probability distribution. Unfortunately, there are a few problems with this approach:\n1. Bounding (ps,\u03c0 \u2212 p\u0302s,\u03c0) \u00b7 V\u0303 \u2217 does not result in a bound in terms of \u03c3\u0303\u03c0(s)2. This issue\ncan be solved by again applying Bernstein\u2019s inequality to bound (ps,a \u2212 p\u0302s,a) \u00b7 V\u0303 \u22172 .\n2. The value V\u0303 \u2217 is not in general independent of p\u0302. This is because M\u0303 must be chosen to satisfy the conditions on (p\u0302s,\u03c0 \u2212 p\u0303s,\u03c0) \u00b7 V\u0303 \u2217, which depends on p\u0302. This dependence\nviolates the conditions of Bernstein\u2019s inequality when trying to bound (ps,\u03c0\u2212p\u0302s,\u03c0)\u00b7V\u0303 \u2217. The dependence is intuitively quite weak, but nevertheless presents problems for rigorous proof.\n3. The last problem is that extended value iteration is no longer a trivial operation (even granting infinite computation). The problem is that the condition (ps,a \u2212 p\u0302s,a) \u00b7 V \u2217\nis not local to (s, a), it also depends on the choice of ps\u2032,a\u2032 for (s \u2032, a\u2032) \u2208 S \u00d7A. This complication is probably resolvable, but the formal demonstration of extended value iteration is no longer so easy.\nProgress. The first issue above can be solved, as remarked, by bounding (ps,a\u2212 p\u0302s,a) \u00b7 V\u0303 \u22172 using another Bernstein inequality. The problem here is that this condition must now be added to the definition of the model class. The second issue is non-trivial and we cannot claim to have made progress there. We did manage to show that extended value iteration can be extended to the case where the only constraints take the form (p\u0303s,\u03c0 \u2212 p\u0302s,\u03c0) \u00b7 V\u0303\n\u2217 .\u221a L1\u03c3\u0303\u03c0(s)2/n. In this case it can be shown the existence of a globally optimistic MDP. Unfortunately if you add constraints on higher moments, (p\u0303s,\u03c0 \u2212 p\u0302s,\u03c0) \u00b7 V\u0303 2 then results become substantially more complex. Note that in the complete proof of Lemma 8 we used higher moments still, but this is not required. Lemma 8 can be proven using only bounds on (p\u0303 \u2212 p\u0302) \u00b7 V\u0303 \u2217 and (p\u0303\u2212 p\u0302) \u00b7 V\u0303 \u2217 2 ."}, {"heading": "7 Lower PAC Bound", "text": "We now turn our attention to proving a matching lower bound. The approach is similar to that of Strehl et al. (2009), but we make two refinements to improve the bound to depend on 1/(1\u2212\u03b3)3 and remove the policy restrictions. The first is to add a delaying state where no information can be gained, but where an algorithm may still fail to be PAC. The second is more subtle and will be described in the proof.\nDefinition 14. A non-stationary policy is a function \u03c0 : S\u2217 \u2192 A.\nTheorem 15. Let \u03c0 be a (possibly non-stationary) policy depending on S,A, r, \u03b3, \u01eb and \u03b4, then there exists a Markov decision process Mhard such that V \u2217(st)\u2212 V \u03c0(st) > \u01eb for at least N time-steps with probability at least \u03b4 where\nN := c1|S \u00d7A|\n\u01eb2(1\u2212 \u03b3)3 log c2 \u03b4\nand c1, c2 > 0 are independent of the policy \u03c0 as well as all inputs S,A, \u01eb, \u03b4, \u03b3.\nThe proof can found in Appendix A, but we give the counter-example MDP and intuition.\nCounter Example. We prove Theorem 15 for a class of MDPs where S = {0, 1,\u2295,\u2296} and A = {1, 2, \u00b7 \u00b7 \u00b7 , |A|}. The rewards and transitions for a single action are depicted in the diagram on the right where \u01eb(a\u2217) = 16\u01eb(1 \u2212 \u03b3) for some a\u2217 \u2208 A and \u01eb(a) = 0 for all other actions. Some remarks:\n1. States \u2295 and \u2296 are almost completely absorbing and confer maximum/minimum rewards respectively. 2. The transitions are independent of actions for all states except state 1. From this state, actions lead uniformly to \u2295/\u2296 except for one action, a\u2217, which has a slightly higher probability of transitioning to state \u2295. Thus a\u2217 is the optimal action in state 1. 3. State 0 has an absorption rate such that, on average, a policy will stay there for 1/(1\u2212\u03b3) time-steps.\nIntuition. The MDP above is very bandit-like in the sense that once a policy reaches state 1 it should choose the action most likely to lead to state \u2295 whereupon it will either be rewarded or punished (visit state \u2295 or \u2296). Eventually it will return to state 1 when the whole process repeats. This suggests a PAC-MDP algorithm can be used to learn the bandit with p(a) := p\u22951,a. We can then make use of a theorem of Mannor and Tsitsiklis (2004) on bandit sample-complexity to show that the number of times a\u2217 is not selected is at least\nO\u0303\n( 1\n\u01eb2(1\u2212 \u03b3)2 log\n1\n\u03b4\n) . (5)\nImproving the bound to depend on 1/(1\u2212\u03b3)3 is intuitively easy, but technically somewhat annoying. The idea is to consider the value differences in state 0 as well as state 1. State 0 has the following properties:\n1. The absorption rate is sufficiently large that any policy remains in state 0 for around 1/(1 \u2212 \u03b3) time-steps.\n2. The absorption rate is sufficiently small that the difference in values due to bad actions planned in state 1 still matter while in state 0.\nWhile in state 0 an agent cannot make an error in the sense that V \u2217(0) \u2212 Q\u2217(0, a) = 0 for all a. But we are measuring V \u2217(0) \u2212 V \u03c0(0) and so an agent can be penalised if its policy upon reaching state 1 is to make an error. Suppose the agent is in state 0 at some time-step before moving to state 1 and making a mistake. On average it will stay in state 0 for roughly 1/(1\u2212 \u03b3) time-steps during which time it will plan a mistake upon reaching state 1. Thus the bound in Equation (5) can be multiplied by 1/(1 \u2212 \u03b3). The proof is harder because an agent need not plan to make a mistake in all future time-steps when reaching state 1 before eventually doing so in one time-step. Note that Strehl et al. (2009) proved their theorem for a specific class of policies while Theorem 15 holds for all policies."}, {"heading": "8 Conclusion", "text": "Summary. We presented matching upper and lower bounds on the number of time-steps when a reinforcement learning algorithm can be nearly-optimal with high probability. While the lower bound is completely general, the upper bound depends on the assumption that there are at most two next-states for each state/action pair. This assumption aside, the new upper bound improves on the previously best known bound of Auer (2011). If the assumption is dropped then the new proof can be used to construct an algorithm that is better than the bound of Auer (2011) in terms of 1/(1\u2212 \u03b3), but worse in |S|. The lower bound, which comes without assumptions, improves on the work of Strehl et al. (2009) by being both larger and more general. The class of MDPs used for the counter-example do satisfy Assumption 1 and so the upper and lower bounds now match in this restricted case.\nRunning Time. We did not analyze the running time of our version of UCRL, but expect analysis similar to that of Strehl and Littman (2008) can be used to show that UCRL can be approximated to run in polynomial time with no cost to sample-complexity.\nAcknowledgements. Thanks to Peter Sunehag for his careful reading and useful suggestions."}, {"heading": "A Proof of Lower PAC Bound", "text": "The proof makes use of a simple form of bandit and Theorem 16, which lower bounds the sample-complexity of bandit algorithms. We need some new notation required for non-stationary policies and bandits.\nHistory Sequences. We write s1:t = s1, s2, \u00b7 \u00b7 \u00b7 , st for the history sequence of length t. Histories can be concatenated, so s1:t\u2295 = s1, s2, \u00b7 \u00b7 \u00b7 , st,\u2295 where \u2295 \u2208 S.\nBandits. An A-armed bandit is a vector p : A \u2192 [0, 1]. A policy interacts with a bandit sequentially. In time-step t some arm at is played whereupon the policy receives reward 1 with probability p(a) and reward 0 otherwise. This is repeated over all timesteps. More formally, a bandit policy is a function \u03c0 : {0, 1}\u2217 \u2192 A. The optimal arm is defined a\u2217 := argmaxa p(a). A policy dependent on \u01eb, \u03b4 and A has sample-complexity T := T (A, \u01eb, \u03b4) if for all bandits the arm chosen on time-step T satisfies p(a\u2217)\u2212 p(aT ) \u2264 \u01eb with probability at least 1\u2212 \u03b4.\nTheorem 16 (Mannor and Tsitsiklis, 2004). There exist positive constants c1, c2, \u01eb0, and \u03b40, such that for every A \u2265 2, \u01eb \u2208 (0, \u01eb0) and \u03b4 \u2208 (0, \u03b40) there exists a bandit p \u2208 [0, 1] A such that\nT (A, \u01eb, \u03b4) \u2265 c1 |A|\n\u01eb2 log c2 \u03b4\nwith probability at least \u03b4.\nRemark 17. The bandit used in the proof of Theorem 16 satisfies p(a) = 12 for all a except a\u2217 which has p(a\u2217) := 12 + \u01eb.\nWe now prepare to prove Theorem 15. For the remainder of this section let \u03c0 be an arbitrary policy and Mhard be the MDP of Figure 2. As in previous work we write V \u03c0 := V \u03c0Mhard. The idea of the proof will be to use Theorem 16 to show that \u03c0 cannot be approximately correct in state 1 too often. Then use this to show that while in state 0 before-hand it is also not approximately correct.\nDefinition 18. Let s1:\u221e \u2208 S \u221e be the sequence of states seen by policy \u03c0 and for arbitrary history s1:t let\n\u2206(s1:t) := V \u2217(s1:t)\u2212 V \u03c0(s1:t).\nLemma 19. If \u03b3 \u2208 (0, 1), p := 1/(2 \u2212 \u03b3) and q := 2\u2212 1/\u03b3 then\np 1 4(1\u2212\u03b3) > 3/4 and \u221e\u2211\nt=0\npt(1\u2212 p)\u03b3t = 1\n2 .\nProof sketch. Both results follow from the geometric series and easy calculus.\nThe following lemma lower-bounds \u2206(s1:t) if sub-optimal action a 6= a \u2217 is taken in state 1.\nLemma 20. Let s1:t be a history such that st = 1 and a := \u03c0(s1:t) 6= a \u2217 then\n\u2206(s1:t) \u2265 8\u01eb.\nProof. The result essentially follows from the definition of the value function.\n\u2206(s1:t) \u2261 V \u2217(s1:t)\u2212 V \u03c0(s1:t)\n= \u03b3 [ p\u22951,a\u2217V \u2217(s1:t\u2295) + p \u2296 1,a\u2217V \u2217(s1:t\u2296) ] \u2212 \u03b3 [ p\u22951,aV \u03c0(s1:t\u2295) + p \u2296 1,aV \u03c0(s1:t\u2296) ] = \u03b3\n2 [V \u2217(s1:t\u2295)\u2212 V \u03c0(s1:t\u2295) + V \u2217(s1:t\u2296)\u2212 V \u03c0(s1:t\u2296)] + \u03b3\u01eb(a \u2217)V \u2217(s1:t\u2295)\n\u2265 8\u01eb,\nwhere we used the definition of the value function and MDP, Mhard.\nWe now define time-intervals where the policy is in state 0. Recall we chose the absorption in this state such that the expected number of time-steps a policy remains there is approximately 1/(1\u2212 \u03b3). We define the intervals starting when a policy arrives in state 0 and ending when it leaves to state 1.\nDefinition 21. Define t01 := 1 and\nt0i := min {t : t > ti\u22121 \u2227 st = 0 \u2227 st\u22121 6= 0} t 1 i := min { t\u2212 1 : st = 1 \u2227 t > t 0 i } .\nDefine the intervals Ii := [t 0 i , t 1 i ] \u2286 N. We call interval Ii the ith phase.\nNote the following facts:\n1. Since all transition probabilities are non-zero, t0i and t 1 i exist for all i \u2208 N with\nprobability 1. 2. |Ii| is the number of time-steps spent in state 0 before moving to state 1. 3. The values |Ii| are independent of \u03c0 and each other.\nDefinition 22. Suppose t \u2208 N and st = 0 and define the weight of action a, wt(a) by\nwt(a) :=\n\u221e\u2211\nk=0\npk(1\u2212 p)\u03b3k[[\u03c0(s1:t0 k1) = a]].\nLemma 23. \u2211\na\u2208A wt(a) = 1 2 for all t where st = 0.\nProof. We use Lemma 19.\n\u2211\na\u2208A\nwi(a) \u2261 \u2211\na\u2208A\n\u221e\u2211\nk=0\npk(1\u2212 p)\u03b3k[[\u03c0(s1:t0 k1) = a]]\n= \u221e\u2211\nk=0\npk(1\u2212 p)\u03b3k = 1\n2\nas required.\nDefinition 24. Define random variables Ai and Xi by\nAi := [[|Ii| \u2265 1/[16(1 \u2212 \u03b3)] \u2227 \u2211\na6=a\u2217\nwt0i (a) \u2265 1/4]] Xi := [[|Ii| \u2265 1/[4(1 \u2212 \u03b3)]]]\nIntuitively, Xi is the event that the ith phase lasts at least 1/[4(1\u2212 \u03b3)] time-steps. Ai is the event that the ith phase lasts at least 1/[16(1 \u2212 \u03b3)] time-steps and the combined weight of sub-optimal actions at the start of a phase is at least 1/4. The following lemma shows that at least two thirds of all phases have Xi = 1 with high probability. Lemma 25. For all n \u2208 N, P {\u2211n\ni=1Xi \u2264 2 3n\n} \u2264 2e\u2212n/72.\nProof. Preparing to use Hoeffding\u2019s bound,\nP {Xi = 1} := P {|Ii| \u2265 1/[4(1 \u2212 \u03b3)]} = p 1/[4(1\u2212\u03b3)] > 3/4,\nwhere we used the definitions of Xi, Ii and Lemma 19. Therefore EXi > 3/4.\nP\n{ n\u2211\ni=1\nXi \u2264 2\n3 n\n} \u2264 P { n\u2211\ni=1\nXi \u2264 1\n12 n+ nEXi\n} = P { n\u2211\ni=1\nXi \u2212EXi \u2264 1\n12 n\n} \u2264 2e\u2212n/72\nwhere we applied basic inequalities followed by Hoeffding\u2019s bound.\nLemma 26. If \u03b3 > 34 and \u2211 a6=a\u2217 wt(a) \u2265 1 4 then \u2211 a6=a\u2217 wt+k(a) \u2265 1 8 for all t \u2208 N and k satisfying 0 \u2264 k \u2264 1/[16(1 \u2212 \u03b3)].\nProof. Working from the definitions.\n1 4 \u2264\n\u2211\na6=a\u2217\nwt0i (a) \u2261\n\u221e\u2211\nj=0\npj(1\u2212 p)\u03b3j [[\u03c0(s1:t0i 0 j) 6= a\u2217]]\n=\nk\u22121\u2211\nj=0\npj(1\u2212 p)\u03b3j [[\u03c0(s1:t0i 0 j) 6= a\u2217]] + pk\u03b3k\n\u2211\na6=a\u2217\nwa(s1:t0i 0 k)\n\u2264 (1\u2212 p)\nk\u22121\u2211\nj=0\npj\u03b3j + pk\u03b3k \u2211\na6=a\u2217\nwa(s1:t0i 0 k)\nRearranging, setting 0 \u2264 k \u2264 1/[16(1 \u2212 \u03b3)] and using the geometric series completes the proof.\nSo far, none of our results have been especially surprising. Lemma 25 shows that at least two thirds of all phases have length exceeding 1/[4(1 \u2212 \u03b3)] with high probability. Lemma 26 shows that if at the start of a phase \u03c0 assigns a high weight to the sub-optimal actions, then it does so throughout the entire phase. The following lemma is more fundamental. It shows that the number of phases where \u03c0 assigns a high weight to the sub-optimal actions is of order 1\n\u01eb2(1\u2212\u03b3)2 log 1\u03b4 with high probability.\nLemma 27. Let N := c1A \u01eb2(1\u2212\u03b3)2 log c2\u03b4 with constants as in Theorem 16 then\n\u2223\u2223\u2223\u2223\u2223\u2223   i : \u2211\na6=a\u2217\nwt0i (a) >\n1 4 \u2227 i < 2N + 1    \u2223\u2223\u2223\u2223\u2223\u2223 > N\nwith probability at least \u03b4.\nThe idea is similar to that in (Strehl et al., 2009). Assume a policy exists that doesn\u2019t satisfy the condition above and then use it to learn the bandit defined by p(a) := p\u22951,a. Proof. Let p(a) := p\u22951,a be a bandit and use \u03c0 to learn bandit p using Algorithm 2 below, which returns an action abest defined as\nabest := argmax a\n2N\u2211\ni=1\na\u0304i, a\u0304i := argmax a\u2032\nwt0i (a\u2032)\nBy Theorem 16, the strategy in Algorithm 2 must fail with probability at least \u03b4. Therefore with probability at least \u03b4, abest 6= a\n\u2217. However abest is defined as the majority action of all the a\u0304i and so for at least N time-steps a\u0304i 6= a\n\u2217. Suppose wt0i (a) > 14 , then by Lemma\n23, \u2211\na6=a\u2217 wt0i (a) < 1 4 and a\u0304i \u2261 argmaxawt0i (a) = a \u2217. This implies that with probability\n\u03b4, for at least N time-steps \u2211\na6=a\u2217 wt0i (a) > 14 as required.\nAlgorithm 2 Learn Bandit\nt = 1, st = 0, k = 0 loop\nat = \u03c0(s1:t) if st = 1 then\nr \u223c p(at) \u22b2 sample from bandit if r = 1 then\nst+1 = \u2295 else\nst+1 = \u2296\nk = k + 1 if k = 2N then\nabest = argmaxa \u22112N\ni=1[[a = argmaxa\u2032 wt0i (a\u2032)]]\nexit else\nst+1 \u223c pst,at \u22b2 sample from MDP\nProof of Theorem 15. Suppose Ai = 1 and 0 \u2264 k \u2264 1/[16(1\u2212 \u03b3)] then s1:t0i+k = s1:t0i\n0k\nand\n\u2206(s1:ti+k) =\n\u221e\u2211\nt=0\npt(1\u2212 p)\u03b3t\u2206(s1:ti+k0 t1) (6)\n\u2265\n\u221e\u2211\nt=0\npt(1\u2212 p)\u03b3t \u2211\na6=a\u2217\n[[\u03c0(s1:t0i+k 0t1) = a]]8\u01eb (7)\n\u2265 \u2211\na6=a\u2217\nwt0i+k (a)8\u01eb (8)\n\u2265 \u01eb, (9)\nwhere Equation (6) follows from the definition of Mhard and the value function. Equation (7) by Lemma 20. Equation (8) by the definition of wti+k(a) and Equation (8) by Lemma 26. Thus for each i where Ai = 1, policy \u03c0 makes at least 1/[16(1\u2212\u03b3)] \u01eb-errors. The proof is completed by showing that Ai = 1 for at least N/6 time-steps with probability at least \u03b4, which follows easily from Lemma 27 and Lemma 25.\nDependence on S is added trivially by chaining arbitrarily many such Markov decision processes together.\nRemark 28. Dependence on S log S can possibly be added by a similar technique used by Strehl et al. (2009), but details could be messy."}, {"heading": "B Technical Results", "text": "Theorem 29 (Hoeffding Inequality). Let X1, \u00b7 \u00b7 \u00b7 ,Xn be independent [0, 1]-valued random variables with probability 1. Then\nP {\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nXi \u2212EXi \u2223\u2223\u2223\u2223\u2223 \u2265 \u01eb } \u2264 2e\u22122\u01eb 2n.\nTheorem 30 (Bernstein\u2019s Inequality (Bernstein, 1924)). Let X1, \u00b7 \u00b7 \u00b7 ,Xn be independent real-valued random variables with zero mean and variance VarXi = \u03c3 2 i . If |Xk| < c with probability one then\nP {\u2223\u2223\u2223\u2223\u2223 1 n n\u2211\ni=1\nXi \u2223\u2223\u2223\u2223\u2223 \u2265 \u01eb } \u2264 2e \u2212 \u01eb 2n 2\u03c32+2c\u01eb/3 ,\nwhere \u03c32 := 1n \u2211n i=1 \u03c3 2 i .\nWe can use Hoeffding and Bernstein to bound the gaps |p\u2212 p\u0302| and |p\u0302\u2212 p\u0303| we now want to combine these together in a nice way to bound |p \u2212 p\u0303|.\nLemma 31. Let p, p\u0302, p\u0303 \u2208 [0, 1] satisfy\n|p\u2212 p\u0302| \u2264 min {CI1, CI2} ,\nwhere\nCI1 :=\n\u221a 2p(1\u2212 p)\nn log\n2 \u03b4 + 2 3n log 2 \u03b4 CI2 :=\n\u221a 1\n2n log\n2 \u03b4 .\nThen\n|p \u2212 p\u0303| \u2264\n\u221a 8p\u0303(1\u2212 p\u0303)\nn log\n2 \u03b4 + 2\n( 1\nn log\n2\n\u03b4\n)3 4\n+ 4\n3n log\n2\n\u03b4\nProof. Using the first confidence interval\n|p \u2212 p\u0302| \u2264\n\u221a 2p(1\u2212 p)\nn log\n2 \u03b4 + 2 3n log 2 \u03b4\nAssume without loss of generality that 1 \u2212 p \u2265 1 \u2212 p\u0303 (the case where p \u2265 p\u0303 is identical. Therefore\n|p\u2212 p\u0302| \u2264\n\u221a 2p\u0303(1\u2212 p\u0303)\nn log\n2 \u03b4 +\n\u221a 2(p\u2212 p\u0303)(1\u2212 p\u0303)\nn log\n2 \u03b4 + 2 3n log 2 \u03b4\n\u2264\n\u221a 2p\u0303(1\u2212 p\u0303)\nn log\n2 \u03b4 +\n\u221a\u221a\u221a\u221a4 \u221a\n1 2n log 2 \u03b4\nn log\n2 \u03b4 + 2 3n log 2 \u03b4\n=\n\u221a 2p\u0303(1\u2212 p\u0303)\nn log\n2 \u03b4 + 8 1 4\n( 1\nn log\n2\n\u03b4\n) 3 4\n+ 2\n3n log\n2 \u03b4 ,\nwhere we used the second confidence interval and algebra. Bounding |p\u0302 \u2212 p\u0303| by the first confidence interval leads to\n|p \u2212 p\u0303| \u2264\n\u221a 8p\u0303(1\u2212 p\u0303)\nn log\n2 \u03b4 + 2\n( 1\nn log\n2\n\u03b4\n)3 4\n+ 4\n3n log\n2\n\u03b4\nas required."}, {"heading": "C Proof of Lemma 8", "text": "We need to define some higher \u201cmoments\u201d of the value function. This is somewhat unfortunate as it complicates the proof, but may be unavoidable.\nDefinition 32. We define the space of bounded value/reward functions R by\nR(i) :=   v \u2208 [ 0, ( 1 1\u2212 \u03b3 )i]|S|    \u2282 R |S|.\nLet \u03c0 be some stationary policy. For rd \u2208 R(d) define values V \u03c0 d by the Bellman equations\nV \u03c0d (s) = rd(s) + \u03b3 \u2211\ns\u2032\nps \u2032 s,\u03c0V \u03c0 d (s \u2032).\nAdditionally,\n\u03c3\u03c0d (s) 2 := ps,\u03c0 \u00b7 V \u03c0 d 2 \u2212 [ps,\u03c0 \u00b7 V \u03c0 d ] 2 .\nNote that Vd \u2208 R(d+ 1) and \u03c3 2 d \u2208 R(2d + 2). Let r0 \u2208 R(0) be the true reward function r0(s) := r(s) and define a recurrence by r2d+2(s) := \u03c3 \u03c0 d (s) 2. We define r\u0303d, r\u0302d, V\u0303 \u03c0 d , V\u0302 \u03c0 d and \u03c3\u0303\u03c0d , \u03c3\u0302 \u03c0 d similarly but where all parameters have hat/tilde.\nThe following lemma generalises Lemma 10.\nLemma 33. Let M \u2208 Mk at time-step t then\n|(ps,\u03c0 \u2212 p\u0303s,\u03c0) \u00b7 V\u0303 \u03c0 d | \u2264\n\u221a 8L1\u03c3\u0303\u03c0d (s) 2\nnt(s) + 2\n( L1\nnt(s)\n)3 4 1\n(1\u2212 \u03b3)d+1 + 4L1 3nt(s)(1\u2212 \u03b3)d+1\nProof. Drop references to \u03c0 and let p := psa + s,\u03c0 , p\u0303 := p\u0303 sa+ s,\u03c0 and n := nt(s). SinceM,M\u0303 \u2208 Mk then apply Lemma 31 to obtain\n|p\u2212 p\u0303| \u2264\n\u221a 8L1p\u0303(1\u2212 p\u0303)\nn + 2 ( L1 n ) 3 4 + 4L1 3n\nAssume without loss of generality that V\u0303d(sa +) \u2265 V\u0303d(sa \u2212). Therefore we have\n|(ps,\u03c0 \u2212 p\u0303s,\u03c0) \u00b7 V\u0303d| \u2264\n\u221a 8L1p\u0303(1\u2212 p\u0303)\nn\n( V\u0303d(sa +)\u2212 V\u0303d(sa \u2212) ) + 2 ( L1 n ) 3 4 1 (1\u2212 \u03b3)d+1\n+ 4L1\n3n(1\u2212 \u03b3)d+1 , (10)\nwhere we used Assumption 1 and the fact that Vd \u2208 Rd+1. p\u0303(1\u2212 p\u0303) ( V\u0303d(sa +)\u2212 V\u0303d(sa \u2212) )2 = p\u0303(1\u2212 p\u0303) ( V\u0303d(sa +)2 + V\u0303d(sa \u2212)2 \u2212 2V\u0303d(sa +)V\u0303d(sa \u2212) )\n= p\u0303V\u0303d(sa +)2 + (1\u2212 p\u0303)V\u0303d(sa \u2212)2 \u2212 ( p\u0303V\u0303d(sa +) + (1\u2212 p\u0303)V\u0303d(sa \u2212) )2\n= \u03c3\u0303d(s) 2.\nSubstituting into Equation (10) completes the proof.\nProof of Lemma 8. For ease of notation we drop \u03c0 and t super/subscripts. Let\n\u2206d := \u2223\u2223\u2223\u2223\u2223 \u2211\ns\u2208S\n[w(s) \u2212 w\u0303(s)]rd(s) \u2223\u2223\u2223\u2223\u2223 \u2261 |V\u0303d(st)\u2212 Vd(st)|.\nUsing Lemma 9\n\u2206d = \u03b3 \u2223\u2223\u2223\u2223\u2223 \u2211\ns\u2208S\nw(s)(ps \u2212 p\u0303s) \u00b7 V\u0303d \u2223\u2223\u2223\u2223\u2223\n\u2264 \u01eb\n4(1\u2212 \u03b3)d + \u2223\u2223\u2223\u2223\u2223 \u2211\ns\u2208X\nw(s)(p \u2212 p\u0303) \u00b7 V\u0303d \u2223\u2223\u2223\u2223\u2223\n\u2264 \u01eb\n4(1\u2212 \u03b3)d +Ad +Bd + Cd,\nwhere\nAd := \u2211\ns\u2208X\nw(s) \u221a 8L1\u03c3\u03032d n(s) Bd := \u2211\ns\u2208X\nw(s) 4L1\n3n(s)(1 \u2212 \u03b3)d+1 Cd :=\n\u2211\ns\u2208X\nw(s)2 ( L1 n(s) )3/4 .\nThe expressions Bd and Cd are substantially easier to bound than Ad. First we give a naive bound on Ad, which we use later.\nAd \u2264 \u2211\ns\u2208X\n\u221a 8w(s)\u03c3\u03032d(s)L1\nn(s) \u2261\n\u2211\n\u03ba,\u03b9\u2208K\u00d7I\n\u2211\ns\u2208K(\u03ba,\u03b9)\n\u221a 8w(s)\u03c3\u03032d(s)L1\nn(s) (11)\n\u2264 \u2211\n\u03ba,\u03b9\u2208K\u00d7I\n\u221a\u221a\u221a\u221a8L1|K(\u03ba, \u03b9)| m\u03ba \u2211\ns\u2208K(\u03ba,\u03b9)\nw(s)\u03c3\u03032d(s) \u2264 \u2211\n\u03ba,\u03b9\u2208K\u00d7I\n\u221a\u221a\u221a\u221a8L1 m \u2211\ns\u2208K(\u03ba,\u03b9)\nw(s)\u03c3\u03032d(s)\n(12)\n\u2264 \u221a\u221a\u221a\u221a8|K \u00d7 I|L1 m \u2211\n\u03ba,\u03b9\u2208K\n\u2211\ns\u2208K(\u03ba,\u03b9)\nw(s)\u03c3\u03032d(s) \u2264\n\u221a 8|K \u00d7 I|L1\nm\n\u2211\ns\u2208X\nw(s)\u03c3\u03032d(s) (13)\n\u2264 \u221a 8|K \u00d7 I|L1 m(1\u2212 \u03b3)2d+3 , (14)\nwhere in Equation (11) we used the definitions of Ad and K. In Equation (12) we applied Cauchy-Schwartz and the assumption that |K(\u03ba)| \u2264 \u03ba. In Equation (13) we used Cauchy-Schwartz again and the definition of K. Finally we apply the trivial bound of\u2211\nw(s)\u03c3\u03032d(s) \u2264 1/(1 \u2212 \u03b3) 2d+3. Unfortunately this bound is not sufficient for our needs. The solution is approximate w(s) by w\u0303(s) and use Lemma 11 to improve the last step\nabove.\nAd \u2264\n\u221a 8|K \u00d7 I|L1\nm\n\u2211\ns\u2208S\nw(s)\u03c3\u03032d(s) (15)\n\u2261\n\u221a 8|K \u00d7 I|L1\nm\n\u2211\ns\u2208S\nw\u0303(s)\u03c3\u03032d(s) + 8|K \u00d7 I|L1\nm\n\u2211\ns\u2208S\n(w(s)\u2212 w\u0303(s))\u03c3\u03032d(s) (16)\n\u2264 \u221a 8|K \u00d7 I|L1 m(1\u2212 \u03b3)2d+2 + 8|K \u00d7 I|L1 m \u22062d+2, (17)\nwhere Equation (15) is as in the naive bound. Equation (16) is substituting w(s) for w\u0303(s) and Equation (16) uses the definition of \u2206. Therefore\n\u2206d \u2264 \u01eb\n4(1\u2212 \u03b3)d +Bd + Cd +\n\u221a 8L1|K \u00d7 I|\nm\n[ 1\n(1\u2212 \u03b3)2d+2\n] + \u221a 8|K \u00d7 I|L1\nm \u22062d+2.\nExpanding the recurrence up to \u03b2 leads to\n\u22060 \u2264 8 \u2211\nd\u2208D\u2212{\u03b2}\n( L1|K \u00d7 I|\nm\n)d/(d+2) [ \u01eb 4(1\u2212 \u03b3)d +Bd + Cd + \u221a L1|K \u00d7 I| m [ 1 (1\u2212 \u03b3)2d+2 ]]2/(d+2)\n+ 8\n( L1|K \u00d7 I|\nm\n)\u03b2/(\u03b2+2) [ 2 \u221a L1|K \u00d7 I|\nm(1\u2212 \u03b3)2\u03b2+3 +B\u03b2 + C\u03b2\n]2/(\u03b2+2) , (18)\nwhere we used the naive bound to control A\u03b2. The bounds on Bd and Cd are somewhat easier, and follow similar lines to the naive bound on Ad.\nBd \u2261 \u2211\ns\u2208X\nw(s) 4L1\n3n(s)(1 \u2212 \u03b3)d+1 = 4L1 3(1 \u2212 \u03b3)d+1\n\u2211\n\u03ba,\u03b9\u2208K\n|K(\u03ba, \u03b9)\nm\u03ba \u2264 4|K \u00d7 I|L1 3m(1\u2212 \u03b3)d+1\nCd \u2261 2 \u2211\ns\u2208X\nw(s) ( L1 n(s) ) 3 4 1 (1\u2212 \u03b3)d+1 \u2264\n2\n(1\u2212 \u03b3)d+1+1/4\n( |K \u00d7 I|L1\nm\n) 3 4\n.\nLetting m := 20L1|K\u00d7I||D| 2\n\u01eb2(1\u2212\u03b3)2+2/\u03b2 completes the proof."}, {"heading": "D Constants", "text": "The proof of Theorem 3 uses many constants, which can be hard to keep track of. For convenience we list them below, including approximate upper/lower bounds as appropriate.\nConstant O/\u2126\n\u03b9max := \u2308 1 log 2 log 8|S| \u01eb(1\u2212\u03b3)2 \u2309 \u00d8log |S|\u01eb(1\u2212\u03b3) \u03b2 := \u2308\n1 2 log 2 log 1 1\u2212\u03b3 \u2309 \u2126 ( log 11\u2212\u03b3 )\n|D| := |Z(\u03b2)| \u00d8log log 11\u2212\u03b3\n|K| := |Z(|S|)| \u00d8log |S|\n|I| := \u03b9max + 1 \u00d8log |S|\n\u01eb(1\u2212\u03b3)\n|K \u00d7 I| := |K||I| \u00d8log |S| log |S|\u01eb(1\u2212\u03b3)\nH := 11\u2212\u03b3 log 8|S| \u01eb(1\u2212\u03b3) \u00d8 1 1\u2212\u03b3 log |S| \u01eb(1\u2212\u03b3)\nwmin := \u01eb(1\u2212\u03b3) 4|S| \u2126 ( \u01eb(1\u2212\u03b3) |S| )\n\u03b41 := \u03b4 2|S\u00d7A|Umax \u00d8 \u03b4\n|S\u00d7A|2 log |S| log |S|\n\u01eb(1\u2212\u03b3)\nL1 := log 2 \u03b41\n\u00d8log |S\u00d7A|\u03b4\u01eb(1\u2212\u03b3)\nm := 20L1|K\u00d7I||D| 2\n\u01eb2(1\u2212\u03b3)2+2/\u03b2 \u00d8 1 \u01eb2(1\u2212\u03b3)2 log |S\u00d7A|\u03b4\u01eb(1\u2212\u03b3) log |S| log |S| \u01eb(1\u2212\u03b3) log 2 log 11\u2212\u03b3\nN := 6|S \u00d7A|m \u00d8 |S\u00d7A| \u01eb2(1\u2212\u03b3)2 log |S\u00d7A|\u03b4\u01eb(1\u2212\u03b3) log |S| log |S| \u01eb(1\u2212\u03b3) log 2 log 11\u2212\u03b3\nEmax := 4N |K \u00d7 I| \u00d8 |S\u00d7A| \u01eb2(1\u2212\u03b3)2 log |S\u00d7A|\u03b4\u01eb(1\u2212\u03b3) log 2 |S| log2 |S|\u01eb(1\u2212\u03b3) log 2 log 11\u2212\u03b3\nUmax := |S \u00d7A||K \u00d7 I| \u00d8|S \u00d7A| log |S| log |S|\n\u01eb(1\u2212\u03b3)"}, {"heading": "E Table of Notation", "text": "S,A Finite sets of states and actions respectively.\n\u03b3 The discount fact. Satisfies \u03b3 \u2208 (0, 1).\n\u01eb The required accuracy.\n\u03b4 The probability that an algorithm makes more mistakes than its sample-complexity.\nN The natural numbers, starting at 0.\nlog The natural logarithm.\n\u2227,\u2228 Logical and/or respectively.\nEX,VarX The expectation and variance of random variable X respectively.\nzi zi := 2 i \u2212 2.\nZ(a) Defined as a set of all zi up to and including a. Formally Z(a) := {zi : i \u2264 argmini {zi \u2265 a}}. Contains approximately log a elements.\n\u03c0 A policy.\np The transition function, p : S \u00d7 A \u00d7 S \u2192 [0, 1]. We also write ps \u2032\ns,a := p(s, a, s \u2032) for the probability of transitioning to state s\u2032\nfrom state s when taking action a. ps \u2032 s,\u03c0 := p s\u2032 s,\u03c0(s). ps,a \u2208 [0, 1] |S| is the vector of transition probabilities.\np\u0302, p\u0303 Other transition probabilities, as above.\nr The reward function r : S \u2192 A.\nM The true MDP. M := (S,A, p, r, \u03b3).\nM\u0302 The MDP with empirically estimated transition probabilities. M\u0302 := (S,A, p\u0302, r, \u03b3).\nM\u0303 An MDP in the model class, M. M\u0303 := (S,A, p\u0303, r, \u03b3).\nV \u03c0M The value function for policy \u03c0 in MDP M . Can either be viewed as a function V \u03c0M : S \u2192 R or vector V \u03c0 M \u2208 R |S|.\nV\u0303 \u03c0, V\u0302 \u03c0 The values of policy \u03c0 in MDPs M\u0303 and M\u0302 respectively.\n\u03c0\u2217 \u2261 \u03c0\u2217M The optimal policy in MDP M .\n\u03c0\u0303\u2217 \u2261 \u03c0\u2217 M\u0303\nThe optimal policy in M\u0303 .\n\u03c0\u0302\u2217 \u2261 \u03c0\u2217 M\u0302\nThe optimal policy in M\u0302 .\n\u03c0k The (stationary) policy at used in episode k.\nnt(s, a) The number of visits to state/action pair (s, a) at time-step t.\nnt(s, a, s \u2032) The number of visits to state s\u2032 from state s when taking action\na at time-step.\nnt(s) The number of visits to state/action pair (s, \u03c0t(s)) at time-step t.\nvtk(s, a) If tk is the start of an exploration phase then this is the total number of visits to state (s, a) in that exploration phase.\nst, at The state and action in time-step t respectively.\nV \u03c0d A higher \u201cmoment\u201d value function. See Definition 32. \u03c3\u03c0d (s) 2 The variance of Vd(s\n\u2032) when taking action \u03c0(s) in state s\u2032. Defined in Definition 32.\nL1 Defined as log(2/\u03b41).\nD Defined as Z(\u03b2).\nwt(s) The expected discounted number of visits to state s, \u03c0k(s) while following policy \u03c0k.\nXt The active set containing states s where w(s) \u2265 wmin.\nK A set if indices, K := Z(|S|).\nI A set of indices, I := {0, 1, 2, \u00b7 \u00b7 \u00b7 , \u03b9max}.\nKt(\u03ba, \u03b9) A set of states that have\nwt(s) \u2208 [w\u03b9, 2w\u03b9) \u2227 nt(s) \u2208 m[\u03baw\u03b9, (2\u03ba + 2)w\u03b9).\nNote that \u22c3\n\u03ba,\u03b9Kt(\u03ba, \u03b9) contains all states with w(s) \u2265 wmin."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Neural Information Processing Systems", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Near-optimal regret bounds for reinforcement", "author": ["Mach. Learn"], "venue": null, "citeRegEx": "Learn.,? \\Q2002\\E", "shortCiteRegEx": "Learn.", "year": 2002}, {"title": "On a modification of Chebyshev\u2019s inequality and of the error formula of Laplace", "author": ["S. Bernstein"], "venue": "Mathe\u0301matique des Annales Scientifiques des Institutions Savantes de l\u2019Ukraine,", "citeRegEx": "Bernstein.,? \\Q1924\\E", "shortCiteRegEx": "Bernstein.", "year": 1924}, {"title": "On The Sample Complexity Of Reinforcement Learning", "author": ["S. Kakade"], "venue": "PhD thesis,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J. Tsitsiklis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "The variance of discounted Markov decision processes", "author": ["M. Sobel"], "venue": "Journal of Applied Probability,", "citeRegEx": "Sobel.,? \\Q1982\\E", "shortCiteRegEx": "Sobel.", "year": 1982}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["A. Strehl", "M. Littman"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["A. Strehl", "M. Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl and Littman.,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2008}, {"title": "PAC model-free reinforcement learning", "author": ["A. Strehl", "L. Li", "E. Wiewiorac", "J. Langford", "M. Littman"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A. Strehl", "L. Li", "M. Littman"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "Szita and Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri.", "year": 2010}, {"title": "T (A, \u01eb, \u03b4) if for all bandits the arm chosen on time-step T satisfies p(a\u2217)\u2212 p(aT ) \u2264 \u01eb with probability at least 1\u2212 \u03b4", "author": [], "venue": "(Mannor and Tsitsiklis,", "citeRegEx": "T,? \\Q2004\\E", "shortCiteRegEx": "T", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "The performance of reinforcement learning algorithms in this setting can be measured in a number of ways, for instance by using regret or PAC bounds (Kakade, 2003).", "startOffset": 149, "endOffset": 163}, {"referenceID": 3, "context": "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesv\u00e1ri, 2010; Auer, 2011).", "startOffset": 71, "endOffset": 178}, {"referenceID": 7, "context": "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesv\u00e1ri, 2010; Auer, 2011).", "startOffset": 71, "endOffset": 178}, {"referenceID": 11, "context": "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesv\u00e1ri, 2010; Auer, 2011).", "startOffset": 71, "endOffset": 178}, {"referenceID": 0, "context": "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of", "startOffset": 74, "endOffset": 93}, {"referenceID": 0, "context": "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of", "startOffset": 74, "endOffset": 106}, {"referenceID": 0, "context": "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of", "startOffset": 74, "endOffset": 133}, {"referenceID": 11, "context": "This bound is an improvement1 on the previous best (Auer, 2011) and published best (Szita and Szepesv\u00e1ri, 2010), which are", "startOffset": 83, "endOffset": 111}, {"referenceID": 9, "context": "We also present a matching (up to logarithmic factors) lower bound that is both larger and more general than the previous best given by Strehl et al. (2009). The class of MDPs used in the counter-example satisfy the assumption used in the upper bound.", "startOffset": 136, "endOffset": 157}, {"referenceID": 4, "context": "The approach has been successful in obtaining uniform sample complexity (or regret) bounds in various domains where the exploration/exploitation problem is an issue (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al., 2010; Auer, 2011).", "startOffset": 165, "endOffset": 302}, {"referenceID": 7, "context": "The approach has been successful in obtaining uniform sample complexity (or regret) bounds in various domains where the exploration/exploitation problem is an issue (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al., 2010; Auer, 2011).", "startOffset": 165, "endOffset": 302}, {"referenceID": 7, "context": "Note that the existence of the function ExtendedValueIteration is proven and an algorithm given by Strehl and Littman (2008). Note that sa and sa are dependent on (s, a) and are known to the algorithm.", "startOffset": 99, "endOffset": 125}, {"referenceID": 0, "context": "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesv\u00e1ri (2010).", "startOffset": 59, "endOffset": 78}, {"referenceID": 0, "context": "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesv\u00e1ri (2010).", "startOffset": 59, "endOffset": 105}, {"referenceID": 0, "context": "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesv\u00e1ri (2010). 1.", "startOffset": 59, "endOffset": 137}, {"referenceID": 3, "context": "Also called the discounted future state distribution in Kakade (2003).", "startOffset": 56, "endOffset": 70}, {"referenceID": 6, "context": "The proof of part 2 is closely related to the approach taken by Strehl and Littman (2008). Recall that M\u0303 is chosen optimistically by extended value iteration.", "startOffset": 64, "endOffset": 90}, {"referenceID": 6, "context": "See the paper of Sobel (1982) for a proof.", "startOffset": 17, "endOffset": 30}, {"referenceID": 9, "context": "The approach is similar to that of Strehl et al. (2009), but we make two refinements to improve the bound to depend on 1/(1\u2212\u03b3)3 and remove the policy restrictions.", "startOffset": 35, "endOffset": 56}, {"referenceID": 5, "context": "We can then make use of a theorem of Mannor and Tsitsiklis (2004) on bandit sample-complexity to show that the number of times a\u2217 is not selected is at least", "startOffset": 37, "endOffset": 66}, {"referenceID": 9, "context": "Note that Strehl et al. (2009) proved their theorem for a specific class of policies while Theorem 15 holds for all policies.", "startOffset": 10, "endOffset": 31}, {"referenceID": 8, "context": "This assumption aside, the new upper bound improves on the previously best known bound of Auer (2011). If the assumption is dropped then the new proof can be used to construct an algorithm that is better than the bound of Auer (2011) in terms of 1/(1\u2212 \u03b3), but worse in |S|.", "startOffset": 0, "endOffset": 102}, {"referenceID": 8, "context": "This assumption aside, the new upper bound improves on the previously best known bound of Auer (2011). If the assumption is dropped then the new proof can be used to construct an algorithm that is better than the bound of Auer (2011) in terms of 1/(1\u2212 \u03b3), but worse in |S|.", "startOffset": 0, "endOffset": 234}, {"referenceID": 7, "context": "The lower bound, which comes without assumptions, improves on the work of Strehl et al. (2009) by being both larger and more general.", "startOffset": 74, "endOffset": 95}, {"referenceID": 7, "context": "We did not analyze the running time of our version of UCRL, but expect analysis similar to that of Strehl and Littman (2008) can be used to show that UCRL can be approximated to run in polynomial time with no cost to sample-complexity.", "startOffset": 99, "endOffset": 125}], "year": 2012, "abstractText": "We study upper and lower bounds on the sample-complexity of learning nearoptimal behaviour in finite-state discounted Markov Decision Processes (MDPs). For the upper bound we make the assumption that each action leads to at most two possible next-states and prove a new bound for a UCRL-style algorithm on the number of time-steps when it is not Probably Approximately Correct (PAC). The new lower bound strengthens previous work by being both more general (it applies to all policies) and tighter. The upper and lower bounds match up to logarithmic factors.", "creator": "LaTeX with hyperref package"}}}