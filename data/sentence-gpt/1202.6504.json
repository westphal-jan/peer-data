{"id": "1202.6504", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Feb-2012", "title": "Learning from Distributions via Support Measure Machines", "abstract": "This paper presents a kernel-based discriminative learning framework on probability measures. Rather than relying on large collections of vectorial training examples, our framework learns using a collection of probability distributions that have been constructed to meaningfully represent training data. By representing these probability distributions as mean embeddings in the reproducing kernel Hilbert space (RKHS), we are able to apply many standard kernel-based learning techniques in straightforward fashion.\n\n\n\n\nThe dataset, consisting of an 8-dimensional vector and one of two vectors, is a complex and complex representation of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution of the distribution", "histories": [["v1", "Wed, 29 Feb 2012 10:09:26 GMT  (101kb)", "https://arxiv.org/abs/1202.6504v1", "Initial submission"], ["v2", "Sat, 12 Jan 2013 12:43:09 GMT  (104kb)", "http://arxiv.org/abs/1202.6504v2", "Advances in Neural Information Processing Systems 25"]], "COMMENTS": "Initial submission", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["krikamol muandet", "kenji fukumizu", "francesco dinuzzo", "bernhard sch\u00f6lkopf"], "accepted": true, "id": "1202.6504"}, "pdf": {"name": "1202.6504.pdf", "metadata": {"source": "CRF", "title": "Learning from Distributions via Support Measure Machines", "authors": ["Krikamol Muandet"], "emails": ["krikamol@tuebingen.mpg.de", "fukumizu@ism.ac.jp", "fdinuzzo@tuebingen.mpg.de", "bs@tuebingen.mpg.de"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 2.\n65 04\nv2 [\nst at\n.M L\n] 1"}, {"heading": "1 Introduction", "text": "Discriminative learning algorithms are typically trained from large collections of vectorial training examples. In many classical learning problems, however, it is arguably more appropriate to represent training data not as individual data points, but as probability distributions. There are, in fact, multiple reasons why probability distributions may be preferable.\nFirstly, uncertain or missing data naturally arises in many applications. For example, gene expression data obtained from the microarray experiments are known to be very noisy due to various sources of variabilities [1]. In order to reduce uncertainty, and to allow for estimates of confidence levels, experiments are often replicated. Unfortunately, the feasibility of replicating the microarray experiments is often inhibited by cost constraints, as well as the amount of available mRNA. To cope with experimental uncertainty given a limited amount of data, it is natural to represent each array as a probability distribution that has been designed to approximate the variability of gene expressions across slides.\nProbability distributions may be equally appropriate given an abundance of training data. In datarich disciplines such as neuroinformatics, climate informatics, and astronomy, a high throughput experiment can easily generate a huge amount of data, leading to significant computational challenges in both time and space. Instead of scaling up one\u2019s learning algorithms, one can scale down one\u2019s dataset by constructing a smaller collection of distributions which represents groups of similar samples. Besides computational efficiency, aggregate statistics can potentially incorporate higherlevel information that represents the collective behavior of multiple data points.\nPrevious attempts have been made to learn from distributions by creating positive definite (p.d.) kernels on probability measures. In [2], the probability product kernel (PPK) was proposed as a generalized inner product between two input objects, which is in fact closely related to well-known kernels such as the Bhattacharyya kernel [3] and the exponential symmetrized Kullback-Leibler (KL) divergence [4]. In [5], an extension of a two-parameter family of Hilbertian metrics of Tops\u00f8e was used to define Hilbertian kernels on probability measures. In [6], the semi-group kernels were designed for objects with additive semi-group structure such as positive measures. Recently, [7] introduced nonextensive information theoretic kernels on probability measures based on new JensenShannon-type divergences. Although these kernels have proven successful in many applications, they are designed specifically for certain properties of distributions and application domains. Moreover, there has been no attempt in making a connection to the kernels on corresponding input spaces.\nThe contributions of this paper can be summarized as follows. First, we prove the representer theorem for a regularization framework over the space of probability distributions, which is a generalization of regularization over the input space on which the distributions are defined (Section 2). Second, a family of positive definite kernels on distributions is introduced (Section 3). Based on such kernels, a learning algorithm on probability measures called support measure machine (SMM) is proposed. An SVM on the input space is provably a special case of the SMM. Third, the paper presents the relations between sample-based and distribution-based methods (Section 4). If the distributions depend only on the locations in the input space, the SMM particularly reduces to a more flexible SVM that places different kernels on each data point."}, {"heading": "2 Regularization on probability distributions", "text": "Given a non-empty set X , let P denote the set of all probability measures P on a measurable space (X ,A), where A is a \u03c3-algebra of subsets of X . The goal of this work is to learn a function h : P \u2192 Y given a set of example pairs {(Pi, yi)}mi=1, where Pi \u2208 P and yi \u2208 Y . In other words, we consider a supervised setting in which input training examples are probability distributions. In this paper, we focus on the binary classification problem, i.e., Y = {+1,\u22121}.\nIn order to learn from distributions, we employ a compact representation that not only preserves necessary information of individual distributions, but also permits efficient computations. That is, we adopt a Hilbert space embedding to represent the distribution as a mean function in an RKHS [8, 9]. Formally, let H denote an RKHS of functions f : X \u2192 R, endowed with a reproducing kernel k : X \u00d7 X \u2192 R. The mean map from P into H is defined as\n\u00b5 : P \u2192 H, P 7\u2212\u2192\n\u222b\nX\nk(x, \u00b7) dP(x) . (1)\nWe assume that k(x, \u00b7) is bounded for any x \u2208 X . It can be shown that, if k is characteristic, the map (1) is injective, i.e., all the information about the distribution is preserved [10]. For any P, letting \u00b5P = \u00b5(P), we have the reproducing property\nEP[f ] = \u3008\u00b5P, f\u3009H, \u2200f \u2208 H . (2)\nThat is, we can see the mean embedding \u00b5P as a feature map associated with the kernel K : P \u00d7 P \u2192 R, defined as K(P,Q) = \u3008\u00b5P, \u00b5Q\u3009H. Since supx \u2016k(x, \u00b7)\u2016H < \u221e, it also follows that K(P,Q) = \u222b\u222b \u3008k(x, \u00b7), k(z, \u00b7)\u3009H dP(x) dQ(z) = \u222b\u222b k(x, z) dP(x) dQ(z), where the second equality follows from the reproducing property of H. It is immediate that K is a p.d. kernel on P .\nThe following theorem shows that optimal solutions of a suitable class of regularization problems involving distributions can be expressed as a finite linear combination of mean embeddings.\nTheorem 1. Given training examples (Pi, yi) \u2208 P \u00d7 R, i = 1, . . . ,m, a strictly monotonically increasing function \u2126 : [0,+\u221e) \u2192 R, and a loss function \u2113 : (P \u00d7 R2)m \u2192 R \u222a {+\u221e}, any f \u2208 H minimizing the regularized risk functional\n\u2113 (P1, y1,EP1 [f ], . . . ,Pm, ym,EPm [f ]) + \u2126 (\u2016f\u2016H) (3)\nadmits a representation of the form f = \u2211m\ni=1 \u03b1i\u00b5Pi for some \u03b1i \u2208 R, i = 1, . . . ,m.\nTheorem 1 clearly indicates how each distribution contributes to the minimizer of (5). Roughly speaking, the coefficients \u03b1i controls the contribution of the distributions through the mean embeddings \u00b5Pi . Furthermore, if we restrict P to a class of Dirac measures \u03b4x on X and consider\nthe training set {(\u03b4xi, yi)} m i=1, the functional (5) reduces to the usual regularization functional [11]\nand the solution reduces to f = \u2211m\ni=1 \u03b1ik(xi, \u00b7). Therefore, the standard representer theorem is recovered as a particular case (see also [12] for more general results on representer theorem).\nNote that, on the one hand, the minimization problem (5) is different from minimizing the functional EP1 . . .EPm\u2113(x1, y1, f(x1), . . . , xm, ym, f(xm))+\u2126(\u2016f\u2016H) for the special case of the additive loss \u2113. Therefore, the solution of our regularization problem is different from what one would get in the limit by training on an infinitely many points sampled from P1, . . . ,Pm. On the other hand, it is also different from minimizing the functional \u2113(M1, y1, f(M1), . . . ,Mm, ym, f(Mm)) + \u2126(\u2016f\u2016H) where Mi = Ex\u223cPi [x]. In a sense, our framework is something in between."}, {"heading": "3 Kernels on probability distributions", "text": "As the map (1) is linear in P , optimizing the functional (5) amounts to finding a function in H that approximate well functions from P to R in the function class F , {P \u2192 \u222b X g dP |P \u2208 P, g \u2208 C(X )} where C(X ) is a class of bounded continuous functions on X . Since \u03b4x \u2208 P for any x \u2208 X , it follows that C(X ) \u2282 F \u2282 C(P) where C(P) is a class of bounded continuous functions on P endowed with the topology of weak convergence and the associated Borel \u03c3-algebra. The following lemma states the relation between the RKHS H induced by the kernel k and the function class F .\nLemma 2. Assuming that X is compact, the RKHS H induced by a kernel k is dense in F if k is universal, i.e., for every function F \u2208 F and every \u03b5 > 0 there exists a function g \u2208 H with supP\u2208P|F (P)\u2212 \u222b g dP| \u2264 \u03b5.\nProof. Assume that k is universal. Then, for every function f \u2208 C(X ) and every \u03b5 > 0 there exists a function g \u2208 H induced by k with supx\u2208X |f(x)\u2212g(x)| \u2264 \u03b5 [13]. Hence, by linearity ofF , for every F \u2208 F and every \u03b5 > 0 there exists a function h \u2208 H such that supP\u2208P|F (P)\u2212 \u222b h dP| \u2264 \u03b5.\nNonlinear kernels on P can be defined in an analogous way to nonlinear kernels on X , by treating mean embeddings \u00b5P of P \u2208 P as its feature representation. First, assume that the map (1) is injective and let \u3008\u00b7, \u00b7\u3009P be an inner product on P . By linearity, we have \u3008P,Q\u3009P = \u3008\u00b5P, \u00b5Q\u3009H (cf. [8] for more details). Then, the nonlinear kernels on P can be defined as K(P,Q) = \u03ba(\u00b5P, \u00b5Q) = \u3008\u03c8(\u00b5P), \u03c8(\u00b5Q)\u3009H\u03ba where \u03ba is a p.d. kernel. As a result, many standard nonlinear kernels on X can be used to define nonlinear kernels on P as long as the kernel evaluation depends entirely on the inner product \u3008\u00b5P, \u00b5Q\u3009H, e.g., K(P,Q) = (\u3008\u00b5P, \u00b5Q\u3009H+ c)d. Although requiring more computational effort, their practical use is simple and flexible. Specifically, the notion of p.d. kernels on distributions proposed in this work is so generic that standard kernel functions can be reused to derive kernels on distributions that are different from many other kernel functions proposed specifically for certain distributions.\nIt has been recently proved that the Gaussian RBF kernel given by K(P,Q) = exp(\u2212 \u03b32 \u2016\u00b5P \u2212 \u00b5Q\u20162H), \u2200P,Q \u2208 P is universal w.r.t C(P) given that X is compact and the map \u00b5 is injective [14]. Despite its success in real-world applications, the theory of kernel-based classifiers beyond the input space X \u2282 Rd, as also mentioned by [14], is still incomplete. It is therefore of theoretical interest to consider more general classes of universal kernels on probability distributions."}, {"heading": "3.1 Support measure machines", "text": "This subsection extends SVMs to deal with probability distributions, leading to support measure machines (SMMs). In its general form, an SMM amounts to solving an SVM problem with the expected kernel K(P,Q) = Ex\u223cP,z\u223cQ[k(x, z)]. This kernel can be computed in closed-form for certain classes of distributions and kernels k. Examples are given in Table 1.\nAlternatively, one can approximate the kernel K(P,Q) by the empirical estimate:\nKemp(P\u0302n, Q\u0302m) = 1\nn \u00b7m\nn\u2211\ni=1\nm\u2211\nj=1\nk(xi, zj) (4)\nwhere P\u0302n and Q\u0302m are empirical distributions of P and Q given random samples {xi}ni=1 and {zj}mj=1, respectively. A finite sample of size m from a distribution P suffices (with high probability)\nto compute an approximation within an error of O(m\u2212 1\n2 ). Instead, if the sample set is sufficiently large, one may choose to approximate the true distribution by simpler probabilistic models, e.g., a mixture of Gaussians model, and choose a kernel k whose expected value admits an analytic form. Storing only the parameters of probabilistic models may save some space compared to storing all data points.\nNote that the standard SVM feature map \u03c6(x) is usually nonlinear in x, whereas \u00b5P is linear in P. Thus, for an SMM, the first level kernel k is used to obtain a vectorial representation of the measures, and the second level kernel K allows for a nonlinear algorithm on distributions. For clarity, we will refer to k and K as the embedding kernel and the level-2 kernel, respectively"}, {"heading": "4 Theoretical analyses", "text": "This section presents key theoretical aspects of the proposed framework, which reveal important connection between kernel-based learning algorithms on the space of distributions and on the input space on which they are defined."}, {"heading": "4.1 Risk deviation bound", "text": "Given a training sample {(Pi, yi)}mi=1 drawn i.i.d. from some unknown probability distribution P on P \u00d7 Y , a loss function \u2113 : R \u00d7 R \u2192 R, and a function class \u039b, the goal of statistical learning is to find the function f \u2208 \u039b that minimizes the expected risk functional R(f) = \u222b P \u222b X \u2113(y, f(x)) dP(x) dP(P, y). Since P is unknown, the empirical risk Remp(f) = 1 m \u2211m i=1 \u222b X \u2113(yi, f(x)) dPi(x) based on the training sample is considered instead. Furthermore, the risk functional can be simplified further by considering 1 m\u00b7n \u2211m i=1 \u2211 xij\u223cPi\n\u2113(yi, f(xij)) based on n samples xij drawn from each Pi.\nOur framework, on the other hand, alleviates the problem by minimizing the risk functional R\u00b5(f) = \u222b P\n\u2113(y,EP[f(x)]) dP(P, y) for f \u2208 H with corresponding empirical risk functional R\u00b5emp(f) = 1 m \u2211m i=1 \u2113(yi,EPi [f(x)]) (cf. the discussion at the end of Section 2). It is often easier to optimize R\u00b5emp(f) as the expectation can be computed exactly for certain choices of Pi and H. Moreover, for universal H, this simplification preserves all information of the distributions. Nevertheless, there is still a loss of information due to the loss function \u2113.\nDue to the i.i.d. assumption, the analysis of the difference between R and R\u00b5 can be simplified w.l.o.g. to the analysis of the difference between EP[\u2113(y, f(x))] and \u2113(y,EP[f(x)]) for a particular distribution P \u2208 P . The theorem below provides a bound on the difference between EP[\u2113(y, f(x))] and \u2113(y,EP[f(x)]).\nTheorem 3. Given an arbitrary probability distribution P with variance \u03c32, a Lipschitz continuous function f : R \u2192 R with constant Cf , an arbitrary loss function \u2113 : R \u00d7 R \u2192 R that is Lipschitz continuous in the second argument with constant C\u2113, it follows that |Ex\u223cP[\u2113(y, f(x))] \u2212 \u2113(y,Ex\u223cP[f(x)])| \u2264 2C\u2113Cf\u03c3 for any y \u2208 R.\nTheorem 3 indicates that if the random variable x is concentrated around its mean and the function f and \u2113 are well-behaved, i.e., Lipschitz continuous, then the loss deviation |EP[\u2113(y, f(x))] \u2212 \u2113(y,EP[f(x)])| will be small. As a result, if this holds for any distribution Pi in the training set {(Pi, yi)}mi=1, the true risk deviation |R \u2212R \u00b5| is also expected to be small."}, {"heading": "4.2 Flexible support vector machines", "text": "It turns out that, for certain choices of distributions P, the linear SMM trained using {(Pi, yi)}mi=1 is equivalent to an SVM trained using some samples {(xi, yi)}mi=1 with an appropriate choice of kernel function. Lemma 4. Let k(x, z) be a bounded p.d. kernel on a measure space such that \u222b\u222b\nk(x, z)2 dxdz < \u221e, and g(x, x\u0303) be a square integrable function such that \u222b g(x, x\u0303) dx\u0303 < \u221e for all x. Given a sample {(Pi, yi)}mi=1 where each Pi is assumed to have a density given by g(xi, x), the linear SMM is equivalent to the SVM on the training sample {(xi, yi)}mi=1 with kernel Kg(x, z) =\u222b\u222b\nk(x\u0303, z\u0303)g(x, x\u0303)g(z, z\u0303) dx\u0303dz\u0303.\nNote that the important assumption for this equivalence is that the distributions Pi differ only in their location in the parameter space. This need not be the case in all possible applications of SMMs. Furthermore, we have Kg(x, z) = \u2329\u222b k(x\u0303, \u00b7)g(x, x\u0303) dx\u0303, \u222b k(z\u0303, \u00b7)g(z, z\u0303) dz\u0303 \u232a H\n. Thus, it is clear that the feature map of x depends not only on the kernel k, but also on the density g(x, x\u0303). Consequently, by virtue of Lemma 4, the kernel Kg allows the SVM to place different kernels at each data point. We call this algorithm a flexible SVM (Flex-SVM).\nConsider for example the linear SMM with Gaussian distributions N (x1;\u03c321 \u00b7 I), . . . ,N (xm;\u03c3 2 m \u00b7 I) and Gaussian RBF kernel k\u03c32 with bandwidth parameter \u03c3. The convolution theorem of Gaussian distributions implies that this SMM is equivalent to a flexible SVM that places a data-dependent kernel k\u03c32+2\u03c32\ni (xi, \u00b7) on training example xi, i.e., a Gaussian RBF kernel with larger bandwidth."}, {"heading": "5 Related works", "text": "The kernel K(P,Q) = \u3008\u00b5P, \u00b5Q\u3009H is in fact a special case of the Hilbertian metric [5], with the associated kernel K(P,Q) = EP,Q[k(x, x\u0303)], and a generative mean map kernel (GMMK) proposed by [15]. In the GMMK, the kernel between two objects x and y is defined via p\u0302x and p\u0302y , which are estimated probabilistic models of x and y, respectively. That is, a probabilistic model p\u0302x is learned for each example and used as a surrogate to construct the kernel between those examples. The idea of surrogate kernels has also been adopted by the Probability Product Kernel (PPK) [2]. In this case, we have K\u03c1(p, p\u2032) = \u222b X p(x)\u03c1p\u2032(x)\u03c1 dx, which has been shown to be a special case of GMMK when \u03c1 = 1 [15]. Consequently, GMMK, PPK with \u03c1 = 1, and our linear kernels are equivalent when the embedding kernel is k(x, x\u2032) = \u03b4(x \u2212 x\u2032). More recently, the empirical kernel (4) was employed in an unsupervised way for multi-task learning to generalize to a previously unseen task [16]. In contrast, we treat the probability distributions in a supervised way (cf. the regularized functional (5)) and the kernel is not restricted to only the empirical kernel.\nThe use of expected kernels in dealing with the uncertainty in the input data has a connection to robust SVMs. For instance, a generalized form of the SVM in [17] incorporates the probabilistic uncertainty into the maximization of the margin. This results in a second-order cone programming (SOCP) that generalizes the standard SVM. In SOCP, one needs to specify the parameter \u03c4i that reflects the probability of correctly classifying the ith training example. The parameter \u03c4i is therefore closely related to the parameter \u03c3i, which specifies the variance of the distribution centered at the ith example. [18] showed the equivalence between SVMs using expected kernels and SOCP when \u03c4i = 0. When \u03c4i > 0, the mean and covariance of missing kernel entries have to be estimated explicitly, making the SOCP more involved for nonlinear kernels. Although achieving comparable performance to the standard SVM with expected kernels, the SOCP requires a more computationally extensive SOCP solver, as opposed to simple quadratic programming (QP)."}, {"heading": "6 Experimental results", "text": "In the experiments, we primarily consider three different learning algorithms: i) SVM is considered as a baseline algorithm. ii) Augmented SVM (ASVM) is an SVM trained on augmented samples drawn according to the distributions {Pi}mi=1. The same number of examples are drawn from each distribution. iii) SMM is distribution-based method that can be applied directly on the distributions1.\n1We used the LIBSVM implementation."}, {"heading": "6.1 Synthetic data", "text": "Firstly, we conducted a basic experiment that illustrates a fundamental difference between SVM, ASVM, and SMM. A binary classification problem of 7 Gaussian distributions with different means and covariances was considered. We trained the SVM using only the means of the distributions, ASVM with 30 virtual examples generated from each distribution, and SMM using distributions as training examples. A Gaussian RBF kernel with \u03b3 = 0.25 was used for all algorithms.\nFigure 1a shows the resulting decision boundaries. Having been trained only on means of the distributions, the SVM classifier tends to overemphasize the regions with high densities and underrepresent the lower density regions. In contrast, the ASVM is more expensive and sensitive to outliers, especially when learning on heavy-tailed distributions. The SMM treats each distribution as a training example and implicitly incorporates properties of the distributions, i.e., means and covariances, into the classifier. Note that the SVM can be trained to achieve a similar result to the SMM by choosing an appropriate value for \u03b3 (cf. Lemma 4). Nevertheless, this becomes more difficult if the training distributions are, for example, nonisotropic and have different covariance matrices.\nSecondly, we evaluate the performance of the SMM for different combinations of embedding and level-2 kernels. Two classes of synthetic Gaussian distributions on R10 were generated. The mean parameters of the positive and negative distributions are normally distributed with means m+ = (1, . . . , 1) and m\u2212 = (2, . . . , 2) and identical covariance matrix \u03a3 = 0.5 \u00b7 I10, respectively. The covariance matrix for each distribution is generated according to two Wishart distributions with covariance matrices given by \u03a3+ = 0.6 \u00b7 I10 and \u03a3\u2212 = 1.2 \u00b7 I10 with 10 degrees of freedom. The training set consists of 500 distributions from the positive class and 500 distributions from the negative class. The test set consists of 200 distributions with the same class proportion.\nThe kernels used in the experiment include linear kernel (LIN), polynomial kernel of degree 2 (POLY2), polynomial kernel of degree 3 (POLY3), unnormalized Gaussian RBF kernel (RBF), and normalized Gaussian RBF kernel (NRBF). To fix parameter values of both kernel functions and SMM, 10-fold cross-validation (10-CV) is performed on a parameter grid, C \u2208 {2\u22123, 2\u22122, . . . , 27} for SMM, bandwidth parameter \u03b3 \u2208 {10\u22123, 10\u22122, . . . , 102} for Gaussian RBF kernels, and degree parameter d \u2208 {2, 3, 4, 5, 6} for polynomial kernels. The average accuracy and \u00b11 standard deviation for all kernel combinations over 30 repetitions are reported in Table 2. Moreover, we also investigate the sensitivity of kernel parameters for two kernel combinations: RBF-RBF and POLYRBF. In this case, we consider the bandwidth parameter \u03b3 = {10\u22123, 10\u22122, . . . , 103} for Gaussian\n90\n95\n100\n1 vs 8\n95\n100\n3 vs 4\n85 90 95\n100 3 vs 8\n95\n100\nS ca\nlin g\n6 vs 9\n95\n100\nA cc\nur ac\ny (%\n)\n90\n95\n100\n85 90 95\n100\n90\n95\n100\nT ra\nns la\ntio n\n95\n100\n10 20 30\n95\n100\n10 20 30 Number of virtual examples\n85\n90\n95\n100\n10 20 30\n70 80 90\n100\n10 20 30\nR ot\nat io\nn\nFigure 2: the performance of SVM, ASVM, and SMM algorithms on handwritten digits constructed using three basic transformations.\n10-1\n100\n101\n102\n103\n2000 4000 6000R el\nat iv\ne co\nm p.\nc os\nt\nNumber of virtual examples\nSMM ASVM\nFigure 3: relative computational cost of ASVM and SMM (baseline: SMM with 2000 virtual examples).\n50\n55\n60\n65\n70\npLSA SVM LSMM NLSMM\nA cc\nur ac\ny (%\n)\nFigure 4: accuracies of four different techniques for natural scene categorization.\nRBF kernels and degree parameter d = {2, 3, . . . , 8} for polynomial kernels. Figure 1b depicts the accuracy values and average accuracies for considered kernel functions.\nTable 2 indicates that both embedding and level-2 kernels are important for the performance of the classifier. The embedding kernels tend to have more impact on the predictive performance compared to the level-2 kernels. This conclusion also coincides with the results depicted in Figure 1b."}, {"heading": "6.2 Handwritten digit recognition", "text": "In this section, the proposed framework is applied to distributions over equivalence classes of images that are invariant to basic transformations, namely, scaling, translation, and rotation. We consider the handwritten digits obtained from the USPS dataset. For each 16 \u00d7 16 image, the distribution over the equivalence class of the transformations is determined by a prior on parameters associated with such transformations. Scaling and translation are parametrized by the scale factors (sx, sy) and displacements (tx, ty) along the x and y axes, respectively. The rotation is parametrized by an angle \u03b8. We adopt Gaussian distributions as prior distributions, includingN ([1, 1], 0.1\u00b7I2), N ([0, 0], 5\u00b7I2), and N (0;\u03c0). For each image, the virtual examples are obtained by sampling parameter values from the distribution and applying the transformation accordingly.\nExperiments are categorized into simple and difficult binary classification tasks. The former consists of classifying digit 1 against digit 8 and digit 3 against digit 4. The latter considers classifying digit 3 against digit 8 and digit 6 against digit 9. The initial dataset for each task is constructed by randomly selecting 100 examples from each class. Then, for each example in the initial dataset, we generate 10, 20, and 30 virtual examples using the aforementioned transformations to construct virtual data sets consisting of 2,000, 4,000, and 6,000 examples, respectively. One third of examples in the initial dataset are used as a test set. The original examples are excluded from the virtual datasets. The virtual examples are normalized such that their feature values are in [0, 1]. Then, to reduce computational cost, principle component analysis (PCA) is performed to reduce the dimensionality to 16. We compare the SVM on the initial dataset, the ASVM on the virtual datasets, and the SMM. For SVM and ASVM, the Gaussian RBF kernel is used. For SMM, we employ the empirical kernel (4) with Gaussian RBF kernel as a base kernel. The parameters of the algorithms are fixed by 10-CV over parameters C \u2208 {2\u22123, 2\u22122, . . . , 27} and \u03b3 \u2208 {0.01, 0.1, 1}.\nThe results depicted in Figure 2 clearly demonstrate the benefits of learning directly from the equivalence classes of digits under basic transformations2. In most cases, the SMM outperforms both the SVM and the ASVM as the number of virtual examples increases. Moreover, Figure 3 shows the benefit of the SMM over the ASVM in term of computational cost3.\n2While the reported results were obtained using virtual examples with Gaussian parameter distributions (Sec. 6.2), we got similar results using uniform distributions.\n3The evaluation was made on a 64-bit desktop computer with Intel R\u00a9 Core TM\n2 Duo CPU E8400 at 3.00GHz\u00d72 and 4GB of memory."}, {"heading": "6.3 Natural scene categorization", "text": "This section illustrates benefits of the nonlinear kernels between distributions for learning natural scene categories in which the bag-of-word (BoW) representation is used to represent images in the dataset. Each image is represented as a collection of local patches, each being a codeword from a large vocabulary of codewords called codebook. Standard BoW representations encode each image as a histogram that enumerates the occurrence probability of local patches detected in the image w.r.t. those in the codebook. On the other hand, our setting represents each image as a distribution over these codewords. Thus, images of different scenes tends to generate distinct set of patches. Based on this representation, both the histogram and the local patches can be used in our framework.\nWe use the dataset presented in [19]. According to their results, most errors occurs among the four indoor categories (830 images), namely, bedroom (174 images), living room (289 images), kitchen (151 images), and office (216 images). Therefore, we will focus on these four categories. For each category, we split the dataset randomly into two separate sets of images, 100 for training and the rest for testing.\nA codebook is formed from the training images of all categories. Firstly, interesting keypoints in the image are randomly detected. Local patches are then generated accordingly. After patch detection, each patch is transformed into a 128-dim SIFT vector [20]. Given the collection of detected patches, K-means clustering is performed over all local patches. Codewords are then defined as the centers of the learned clusters. Then, each patch in an image is mapped to a codeword and the image can be represented by the histogram of the codewords. In addition, we also have an M \u00d7 128 matrix of SIFT vectors where M is the number of codewords.\nWe compare the performance of a Probabilistic Latent Semantic Analysis (pLSA) with the standard BoW representation, SVM, linear SMM (LSMM), and nonlinear SMM (NLSMM). For SMM, we use the empirical embedding kernel with Gaussian RBF base kernel k: K(hi,hj) =\u2211M\nr=1 \u2211M s=1 hi(cr)hj(cs)k(cr, cs) where hi is the histogram of the ith image and cr is the rth SIFT vector. A Gaussian RBF kernel is also used as the level-2 kernel for nonlinear SMM. For the SVM, we adopt a Gaussian RBF kernel with \u03c72-distance between the histograms [21], i.e., K(hi,hj) = exp ( \u2212\u03b3\u03c72(hi,hj) ) where \u03c72(hi,hj) = \u2211M r=1 (hi(cr)\u2212hj(cr)) 2 hi(cr)+hj(cr) . The parameters of the algorithms are fixed by 10-CV over parameters C \u2208 {2\u22123, 2\u22122, . . . , 27} and \u03b3 \u2208 {0.01, 0.1, 1}. For NLSMM, we use the best \u03b3 of LSMM in the base kernel and perform 10-CV to choose \u03b3 parameter only for the level-2 kernel. To deal with multiple categories, we adopt the pairwise approach and voting scheme to categorize test images. The results in Figure 4 illustrate the benefit of the distribution-based framework. Understanding the context of a complex scene is challenging. Employing distribution-based methods provides an elegant way of utilizing higher-order statistics in natural images that could not be captured by traditional sample-based methods."}, {"heading": "7 Conclusions", "text": "This paper proposes a method for kernel-based discriminative learning on probability distributions. The trick is to embed distributions into an RKHS, resulting in a simple and efficient learning algorithm on distributions. A family of linear and nonlinear kernels on distributions allows one to flexibly choose the kernel function that is suitable for the problems at hand. Our analyses provide insights into the relations between distribution-based methods and traditional sample-based methods, particularly the flexible SVM that allows the SVM to place different kernels on each training example. The experimental results illustrate the benefits of learning from a pool of distributions, compared to a pool of examples, both on synthetic and real-world data."}, {"heading": "Acknowledgments", "text": "KM would like to thank Zoubin Gharamani, Arthur Gretton, Christian Walder, and Philipp Hennig for a fruitful discussion. We also thank all three insightful reviewers for their invaluable comments."}, {"heading": "A Proof of Theorem 1", "text": "Theorem 1. Given training examples (Pi, yi) \u2208 P \u00d7 R, i = 1, . . . ,m, a strictly monotonically increasing function \u2126 : [0,+\u221e) \u2192 R, and a loss function \u2113 : (P \u00d7 R2)m \u2192 R \u222a {+\u221e}, any f \u2208 H minimizing the regularized risk functional\n\u2113 (P1, y1,EP1 [f ], . . . ,Pm, ym,EPm [f ]) + \u2126 (\u2016f\u2016H) (5)\nadmits a representation of the form f = \u2211m\ni=1 \u03b1i\u00b5Pi for some \u03b1i \u2208 R, i = 1, . . . ,m.\nProof. By virtue of Proposition 2 in [10], the linear functional EP[\u00b7] are bounded for all P \u2208 P . Then, given P1,P2, ..., Pm, any f \u2208 H can be decomposed as\nf = f\u00b5 + f \u22a5\nwhere f\u00b5 \u2208 H lives in the span of \u00b5Pi , i.e., f\u00b5 = \u2211m i=1 \u03b1i\u00b5Pi and f \u22a5 \u2208 H satisfying, for all j, \u3008f\u22a5, \u00b5Pj \u3009 = 0. Hence, for all j, we have\nEPj [f ] = EPj [f\u00b5 + f \u22a5] = \u3008f\u00b5 + f \u22a5 , \u00b5Pj \u3009 = \u3008f\u00b5, \u00b5Pj \u3009+ \u3008f \u22a5 , \u00b5Pj \u3009 = \u3008f\u00b5, \u00b5Pj \u3009\nwhich is independent of f\u22a5. As a result, the loss functional \u2113 in (5) does not depend on f\u22a5. For the regularization functional \u2126, since f\u22a5 is orthogonal to\n\u2211m i=1 \u03b1i\u00b5Pi and \u2126 is strictly monotonically increasing, we\nhave \u2126(\u2016f\u2016) = \u2126(\u2016f\u00b5 + f \u22a5\u2016) = \u2126( \u221a \u2016f\u00b5\u20162 + \u2016f\u22a5\u20162) \u2265 \u2126(\u2016f\u00b5\u2016)\nwith equality if and only if f\u22a5 = 0 and thus f = f\u00b5. Consequently, any minimizer must take the form f =\n\u2211m i=1 \u03b1i\u00b5Pi = \u2211m i=1 \u03b1iEPi [k(x, \u00b7)]."}, {"heading": "B Proof of Theorem 3", "text": "Theorem 3. Given an arbitrary probability distribution P with variance \u03c32, a Lipschitz continuous function f : R \u2192 R with constant Cf , an arbitrary loss function \u2113 : R \u00d7 R \u2192 R that is Lipschitz continuous in the second argument with constant C\u2113, it follows that\n|Ex\u223cP[\u2113(y, f(x))]\u2212 \u2113(y,Ex\u223cP[f(x)])| \u2264 2C\u2113Cf\u03c3\nfor any y \u2208 R.\nProof. Assume that x is distributed according to P. Let mX be the mean of X in Rd. Thus, we have\n|EP[\u2113(y, f(x))]\u2212 \u2113(y,EP[f(x)])| \u2264\n\u222b\n|\u2113(y, f(x\u0303))\u2212 \u2113(y,EP[f(x)])|dP(x\u0303)\n\u2264 C\u2113\n\u222b\n|f(x\u0303)\u2212 EP[f(x)]|dP(x\u0303)\n\u2264 C\u2113\n\u222b\n|f(x\u0303)\u2212 f(mX)|dP(x\u0303)\n\ufe38 \ufe37\ufe37 \ufe38\nA\n+C\u2113|f(mX)\u2212 EP[f(x)]| \ufe38 \ufe37\ufe37 \ufe38\nB\n.\nControl of (A) The first term is upper bounded by\nC\u2113\n\u222b\nCf\u2016x\u0303\u2212mX\u2016dP(x\u0303) \u2264 C\u2113Cf\u03c3 , (6)\nwhere the last inequality is given by EP[\u2016x\u0303\u2212mX\u2016] \u2264 \u221a EP[\u2016x\u0303\u2212mX\u20162] = \u03c3.\nControl of (B) Similarly, the second term is upper bounded by\nC\u2113 \u2223 \u2223 \u2223 \u2223 \u222b f(mX)\u2212 f(x\u0303) \u2223 \u2223 \u2223 \u2223 dP(x\u0303) \u2264 C\u2113 \u222b Cf\u2016mX \u2212 x\u0303\u2016dP(x\u0303) \u2264 C\u2113Cf\u03c3 . (7)\nCombining (B) and (7) yields\n|EP[\u2113(y, f(x))]\u2212 \u2113(y,EP[f(x)])| \u2264 2C\u2113Cf\u03c3 ,\nthus completing the proof."}, {"heading": "C Proof of Lemma 4", "text": "Lemma 4. Let k(x, z) be a bounded p.d. kernel on a measure space such that \u222b\u222b\nk(x, z)2 dxdz < \u221e, and g(x, x\u0303) be a square integrable function such that \u222b g(x, x\u0303) dx\u0303 < \u221e for all x. Given a sample {(Pi, yi)}mi=1 where each Pi is assumed to have a density given by g(xi, x), the linear SMM is equivalent to the SVM on the training sample {(xi, yi)}mi=1 with kernel Kg(x, z) = \u222b\u222b k(x\u0303, z\u0303)g(x, x\u0303)g(z, z\u0303) dx\u0303dz\u0303.\nProof. For a training sample {(xi, yi)}mi=1, the SVM with kernel Kg minimizes\n\u2113({xi, yi, f(xi) + b} m i=1) + \u03bb\u2016f\u2016 2\nHKg .\nBy the representer theorem, f(x) = \u2211m\ni=1 \u03b1iKg(x, xj) with some \u03b1i \u2208 R, hence this is equivalent to\n\u2113({xi, yi, m\u2211\nj=1\n\u03b1jKg(xi, xj) + b} m i=1) + \u03bb\nm\u2211\ni,j=1\n\u03b1i\u03b1jKg(xi, xj) .\nNext, consider the kernel mean of the probability measure g(xi, x)dx given by \u00b5i = \u222b k(\u00b7, x\u0303)g(xi, x\u0303) dx\u0303 and note that \u3008\u00b5i, f\u3009Hk = \u222b f(x\u0303)g(xi, x\u0303) dx\u0303 for any f \u2208 Hk. The linear SMM with loss \u2113 and kernel k minimizes\n\u2113({Pi, yi, \u3008\u00b5i, f\u3009Hk + b} m i=1) + \u03bb\u2016f\u2016 2 Hk .\nBy Theorem 1, each minimizer f admits a representation of the form\nf = m\u2211\nj=1\n\u03b1j\u00b5j = m\u2211\nj=1\n\u03b1j\n\u222b\nk(\u00b7, x\u0303)g(xj, x\u0303) dx\u0303 .\nThus, for this f we have\n\u3008\u00b5i, f\u3009Hk = m\u2211\nj=1\n\u03b1j\n\u222b\u222b\nk(z\u0303, x\u0303)g(xi, x\u0303)g(xj, z\u0303) dx\u0303dz\u0303 = m\u2211\nj=1\n\u03b1jKg(xi, xj)\nand\n\u2016f\u20162Hk = m\u2211\ni,j=1\n\u03b1i\u03b1j\u3008\u00b5i, \u00b5j\u3009 = m\u2211\ni,j=1\n\u03b1i\u03b1jKg(xi, xj)\n, as above. This completes the proof."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "This paper presents a kernel-based discriminative learning framework on prob-<lb>ability measures. Rather than relying on large collections of vectorial training<lb>examples, our framework learns using a collection of probability distributions<lb>that have been constructed to meaningfully represent training data. By represent-<lb>ing these probability distributions as mean embeddings in the reproducing kernel<lb>Hilbert space (RKHS), we are able to apply many standard kernel-based learning<lb>techniques in straightforward fashion. To accomplish this, we construct a gener-<lb>alization of the support vector machine (SVM) called a support measure machine<lb>(SMM). Our analyses of SMMs provides several insights into their relationship<lb>to traditional SVMs. Based on such insights, we propose a flexible SVM (Flex-<lb>SVM) that places different kernel functions on each training example. Experi-<lb>mental results on both synthetic and real-world data demonstrate the effectiveness<lb>of our proposed framework.", "creator": "gnuplot 4.4 patchlevel 3"}}}