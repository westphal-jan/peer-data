{"id": "1203.3482", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Formula-Based Probabilistic Inference", "abstract": "Computing the probability of a formula given the probabilities or weights associated with other formulas is a natural extension of logical inference to the probabilistic setting. Surprisingly, this problem has received little attention in the literature to date, particularly considering that it includes many standard inference problems as special cases. In this paper, we propose two algorithms for this problem: formula decomposition and conditioning, which is an exact method, and formula importance sampling, which is an approximate method of decomposition and conditioning.\n\n\n\n\nThe results from this paper are, among other things, simple. The principle of the algorithm is to find out if there are a certain number of probabilities (e.g., the number of the odds) for the probability of a formula given the probabilities or weights associated with other formulas.\nFor this paper, we explore the concept of a condition where the probability of a formula being given by a standard definition is the probability of a formula given by a standard definition (see section below). The results are also not clear-cut. The following methods are available:\nThe algorithm is designed to calculate the probability of a formula given by a standard definition (see section below).\nWe are able to measure the probability of a formula given by a standard definition. For instance, it is possible to obtain a formula given by a standard definition that was chosen in the last part of the book. However, the problem is not that we cannot compute the probability of a formula given by a standard definition (see section below). In fact, it is also possible to derive a theorem that describes the probability of the formula given by a standard definition and an algorithm for that formulation, and to produce that theorem by an algorithm for that definition.\nThe basic theorem is a theorem that can be generalized to generalizable (e.g., a certain number of probabilities is used, and a certain number of probabilities is used, and a certain number of probabilities is used), and a certain number of probabilities is used.\nTo produce a formula given by a standard definition, we need to compute the probability of a formula given by a standard definition. For example, we can say that the probability of a formula given by a standard definition is 1,000 . It can be determined using probability that the probability of a formula given by a standard definition is 1,000 .\nThis theorem is used to solve the following question:\nThe probability of a formula given by a standard definition (see section below).\nIn our approach, the probability of a formula given by a standard definition", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (216kb)", "http://arxiv.org/abs/1203.3482v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vibhav gogate", "pedro domingos"], "accepted": false, "id": "1203.3482"}, "pdf": {"name": "1203.3482.pdf", "metadata": {"source": "CRF", "title": "Formula-Based Probabilistic Inference", "authors": ["Vibhav Gogate"], "emails": ["vgogate@cs.washington.edu", "pedrod@cs.washington.edu"], "sections": [{"heading": null, "text": "Computing the probability of a formula given the probabilities or weights associated with other formulas is a natural extension of logical inference to the probabilistic setting. Surprisingly, this problem has received little attention in the literature to date, particularly considering that it includes many standard inference problems as special cases. In this paper, we propose two algorithms for this problem: formula decomposition and conditioning, which is an exact method, and formula importance sampling, which is an approximate method. The latter is, to our knowledge, the first application of model counting to approximate probabilistic inference. Unlike conventional variable-based algorithms, our algorithms work in the dual realm of logical formulas. Theoretically, we show that our algorithms can greatly improve efficiency by exploiting the structural information in the formulas. Empirically, we show that they are indeed quite powerful, often achieving substantial performance gains over state-of-the-art schemes."}, {"heading": "1 Introduction", "text": "The standard task in the field of automated reasoning is to determine whether a set of logical formulas (the knowledge base KB) entails a query formula Q. (The formulas could be propositional or first-order; in this paper we focus on the propositional case.) Logic\u2019s lack of a representation for uncertainty severely hinders its ability to model real applications, and thus many methods for adding probability to it have been proposed. One of the earliest is Nilsson\u2019s probabilistic logic (Nilsson, 1986), which attaches probabilities to the formulas in the KB and uses these to compute the probability of the query formula. One problem with this approach is that the formula probabilities may be inconsistent, yielding no solution, but consistency can be verified\nand enforced (Nilsson, 1986). Another problem is that in general a set of formula probabilities does not completely specify a distribution, but this is naturally solved by assuming the maximum entropy distribution consistent with the specified probabilities (Nilsson, 1986; Pietra et al., 1997).\nA more serious problem is the lack of efficient inference procedures for probabilistic logic. This contrasts with the large literature on inference for graphical models, which always specify unique and consistent distributions (Pearl, 1988). However, the representational flexibility and compactness of logic is highly desirable, particularly for modeling complex domains. This issue has gained prominence in the field of statistical relational learning (SRL) (Getoor and Taskar, 2007), which seeks to learn models with both logical and probabilistic aspects. For example, Markov logic represents knowledge as a set of weighted formulas, which define a log-linear model (Domingos and Lowd, 2009). Formulas with probabilities with the maximum entropy assumption and weighted formulas are equivalent; the problem of converting the former to the latter is equivalent to the problem of learning the maximum likelihood weights (Pietra et al., 1997). In this paper we assume weighted formulas, but our algorithms are applicable to formulas with probabilities by first performing this conversion.\nAnother reason to seek efficient inference procedures for probabilistic logic is that inference in graphical models can be reduced to it (Park, 2002). Standard inference schemes for graphical models such as junction trees (Lauritzen and Spiegelhalter, 1988) and bucket elimination (Dechter, 1999) have complexity exponential in the treewidth of the model, making them impractical for complex domains. However, treewidth can be overcome by exploiting structural properties like determinism (Chavira and Darwiche, 2008) and context-specific independence (Boutilier, 1996). Several highly efficient algorithms accomplish this by encoding a graphical models as sets of weighted formulas and applying logical inference techniques to them (Sang et al., 2005; Chavira and Darwiche, 2008).\nAll of these algorithms are variable-based, in that they explore the search space defined by truth assignments to the\nvariables. In this paper, we propose a new class of algorithms that explore the search space defined by truth assignments to arbitrary formulas, including but not necessarily those contained in the original specification. Our formula-based schemes generalize variable-based schemes because a variable is a special case of a formula, namely a unit clause. For deriving exact answers, we propose to exhaustively search the space of truth assignments to formulas, yielding the formula decomposition and conditioning (FDC) scheme. FDC performs AND/OR search (Dechter and Mateescu, 2007) or recursive conditioning (Darwiche, 2001), with and without caching, over the space of formulas, utilizing several Boolean constraint propagation and pruning techniques.\nEven with these techniques, large complex domains will still generally require approximate inference. For this, we propose to compute an importance distribution over the formulas, yielding formula importance sampling (FIS). Each sample in FIS is a truth assignment to a set of formulas. To compute the importance weight of each such sampled assignment, we need to know its model count (or number of solutions). These model counts can either be computed exactly, if it is feasible, or approximately using the recently introduced approximate model counters such as SampleCount (Gomes et al., 2007) and SampleSearch (Gogate and Dechter, 2007b). To the best of our knowledge, this is the first work that harnesses the power of model counting for approximate probabilistic inference. We prove that if the model counts can be computed accurately, formula importance sampling will have smaller variance than variablebased importance sampling and thus should be preferred.\nWe present experimental results on three classes of benchmark problems: random Markov networks, QMR-DT networks from the medical diagnosis domain and Markov logic networks. Our experiments show that as the number of variables in the formulas increases, formula-based schemes not only dominate their variable based counterparts but also state-of-the-art exact algorithms such as ACE (Chavira and Darwiche, 2008) and approximate schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling (Geman and Geman, 1984).\nThe rest of the paper is organized as follows. Section 2 describes background. Section 3 presents formula decomposition and conditioning. Section 4 presents formula importance sampling. Experimental results are presented in Section 5 and we conclude in Section 6."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Notation", "text": "Let X = {X1, . . . , Xn} be a set of propositional variables that can be assigned values from the set {0, 1} or {False,True}. Let F be a propositional formula over X.\nA model or a solution of F is a 0/1 truth assignment to all variables in X such that F evaluates to True. We will assume throughout that F is in CNF, namely it is a conjunction of clauses, a clause being a disjunction of literals. A literal is a variableXi or its negation \u00acXi. A unit clause is a clause with one literal. Propositional Satisfiability or SAT is the decision problem of determining whether F has any models. This is the canonical NP-complete problem. Model Counting is the problem of determining the number of models of F , it is a #P-complete problem.\nWe will denote formulas by letters F , G, and H , the set of solutions of F by Sol(F ) and its number of solutions by #(F ). Variables are denoted by letters X and Y . We denote sets by bold capital letters e.g., X, Y etc. Given a set X = {X1, . . . , Xn} of variables, x denotes a truth assignment (x1, . . . , xn), where Xi is assigned the value xi. Clauses are denoted by the letters C, R, S and T . Discrete functions are denoted by small Greek letters, e.g. \u03c6, \u03c8, etc. The variables involved in a function \u03c6, namely the scope of \u03c6 is denoted by V (\u03c6). Similarly, the variables of a clauseC are denoted by V (C). Given an assignment x to a superset X of Y, xY denotes the restriction of x to Y.\nThe expected value of a random variable X with respect to a distribution Q is EQ[X ] = \u2211 x\u2208X xQ(x). The variance\nof x is V arQ[X ] = \u2211 x\u2208X(x\u2212 EQ[X ]) 2Q(x).\nIn this paper, we advocate using a collection of weighted propositional formulas instead of the conventional tabular representations to encode the potentials in Markov random fields (MRFs) or conditional probability tables in Bayesian networks. Specifically, we will use the following representation, which we call as propositional MRF or PropMRF in short. A PropMRF is a Markov logic network (Richardson and Domingos, 2006) in which all formulas are propositional. It is known that any discrete Markov random field or a Bayesian network can be encoded as a PropMRF (Park, 2002; Sang et al., 2005; Chavira and Darwiche, 2008).\nDEFINITION 1 (Propositional MRFs). A propositional MRF (PropMRF), denoted by M is a triple (X,C,R) where X is a set of n Boolean variables, C = {(C1, w1), . . . , (Cm, wm)} is a set of m soft (weighted) clauses and R = {R1, . . . , Rp} is a set of p hard clauses. Each soft clause is a pair (Ci, wi) where Ci is a clause and wi is a real number. We will denote byFM = R1\u2227. . .\u2227Rp, the CNF formula defined by the hard clauses of M. The primal graph of a PropMRF has variables as its vertices and an edge between any two nodes that are involved in the same hard or soft clause.\nWe can associate a discrete function \u03c6i with each soft clause (Ci, wi), defined as follows:\n\u03c6i(xV (Ci)) = { exp(wi) If x evaluates Ci to True 1 Otherwise\nThe probability distribution associated with M is given by:\nPM(x) = { 1 ZM \u220fm i=1 \u03c6i(xV (\u03c6i)) If x \u2208 Sol(FM)\n0 Otherwise (1)\nwhere ZM is the normalization constant; often referred to as the partition function. ZM is given by:\nZM = \u2211\nx\u2208Sol(FM)\nm\u220f\ni=1\n\u03c6i(xV (\u03c6i)) (2)\nNote that if M has no soft clauses, then ZM equals the number of models of the formula FM. Thus, model counting is a special case of computing ZM.\nWe will focus on the query of finding the probability of a CNF formula G, denoted by P (G). By definition:\nP (G) = \u2211\nx\u2208Sol(FM\u2227G)\nPM(x)\n= 1\nZ\n\u2211\nx\u2208Sol(FM\u2227G)\nm\u220f\ni=1\n\u03c6i(xV (\u03c6i)) (3)\nIf we add all the clauses of G to the hard clauses of M yielding another PropMRFM\u2032, then the partition function ZM\u2032 of M\u2032 is given by:\nZM\u2032 = \u2211\nx\u2208Sol(FM\u2227G)\nm\u220f\ni=1\n\u03c6i(xV (\u03c6i)) (4)\nFrom Equations 3 and 4, we get P (G) = ZM\u2032 ZM\n. Because computing P (G) is equivalent to computing a ratio of two partition functions, in the sequel, we will present formulabased algorithms for computing ZM only."}, {"heading": "3 Exact Formula-based Inference", "text": "We first explain how to perform inference by variablebased conditioning and then show how it can be generalized via formula-based conditioning. Consider the expression for ZM (See Equation 2). Given assignments Xj and \u00acXj , we can express ZM as:\nZM = \u2211\nx\u2208Sol(FM\u2227Xj)\nm\u220f\ni=1\n\u03c6i(xV (\u03c6i))\n+ \u2211\nx\u2208Sol(FM\u2227\u00acXj)\nm\u220f\ni=1\n\u03c6i(xV (\u03c6i)) (5)\n= ZMXj + ZM\u00acXj (6)\nwhere MX and M\u00acX are PropMRFs obtained by adding X and \u00acX to the set of hard clauses of M respectively.\nThen, one can perform conditioning to computeZMXj and ZM\u00acXj , recursively for each PropMRF until all variables\nin X have been instantiated. Conditioning by itself is not that useful. For example, if the PropMRF has no hard clauses, then conditioning would perform 2n summations. However, one can augment it with various simplification schemes such as Boolean constraint propagation, and utilize problem decomposition, yielding powerful schemes in practice. These and other ideas form the backbone of many state-of-the-art schemes such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al., 2005). To simplify a PropMRF, we can apply any parsimonious operators - operators which do not change its partition function. In particular, we can remove all clauses which evaluate to True from the set of hard clauses. These clauses are redundant. Examples of operations that aid in identifying such hard clauses are unit propagation, resolution and subsumption elimination. For example, given a hard clause A, the hard clause A \u2228 B is redundant and can be removed because it is subsumed within A. Similarly, A could be removed after unit propagation, because it always evaluates to True. We can simplify the soft clauses based on the hard clauses by removing all soft clauses which evaluate to either True or False, multiplying the partition function with an appropriate constant to account for their removal. For example, given a hard clause A, the soft clause A\u2228B having weight w is always satisfied and can be removed, by multiplying the partition function by exp(w)1.\nAnother advancement that we can use is problem decomposition (Darwiche, 2001; Dechter and Mateescu, 2007). The idea here is that if the soft and hard clauses of a PropMRF can be partitioned into k > 1 sets such that any two clauses in any of the k sets have no variables in com-\n1Note that if we remove a variable that is not a unit clause from all the hard and soft clauses, then we have to multiply the partition function by 2.\nmon, then the partition function equals the product of the partition functions of the k PropMRFs induced by each set.\nThe following example demonstrates simplification and decomposition on an example PropMRF.\nEXAMPLE 1. Consider the PropMRF shown in Figure 1. After conditioning on A and simplifying using Boolean constraint propagation, we get two PropMRFs shown under the True (left) and false (right) branches of A in Figure 2. The PropMRF at the True branch contains only two soft clauses which have no variables in common. Thus, they could be decomposed into two PropMRFs as shown. The contribution to the partition function due to the True branch of A is then simply a product of the partition functions of the two PropMRFs and exp(w1 + w2)\u00d7 22.\nOur main observation is that we can condition on arbitrary formulas instead of variables. Formally, given an arbitrary formula Hj , we can express ZM as:\nZM = \u2211\nx\u2208Sol(FM\u2227Hj)\nm\u220f\ni=1\n\u03c6i(xV (\u03c6i))\n+ \u2211\nx\u2208Sol(FM\u2227\u00acHj)\nm\u220f\ni=1\n\u03c6i(xV (\u03c6i)) (7)\n= ZMHj + ZM\u00acHj (8)\nWhen combined with Boolean constraint propagation and problem decomposition, this seemingly simple idea is quite powerful because it can yield a smaller search space, as we demonstrate in the following example. In some cases, these reductions could be significant.\nEXAMPLE 2. Consider again the PropMRF shown in Figure 1. The first two clauses share a sub-clause A \u2228B \u2228 C. If we condition first onA\u2228B \u2228C, we get the search space shown in Figure 3, which has only 7 leaf nodes. One can\nAlgorithm 1: Formula Decomposition and Conditioning (FDC) Input: A PropMRFM Output: ZM begin\nw = 0; 1. Simplify begin\nSimplify the hard and soft clauses; Add the weights of all soft clauses which evaluate to True to w; Remove all soft clauses which evaluate to either True or False from M. Update w to account for variables completely removed from all formulas; if FM has an empty clause then\nreturn 0 if FM has only unit clauses then\nreturn exp(w)\nend 2. Decompose begin\nif the primal graph of M is decomposable into k components then\nLet M1,M2, ...,Mk be the PropMRF\u2019s corresponding to the k components; return exp(w)\u00d7FDC(M1)\u00d7 . . .\u00d7FDC(Mk)\nend 3. Condition begin\nHeuristically choose a formula R to condition on; Add hard clauses logically equivalent to R and \u00acR to M yielding MR and M\u00acR respectively; return exp(w)\u00d7 (FDC(MR) + FDC(M\u00acR))\nend end\nverify that if we condition only on the variables instead of arbitrary formulas, the best ordering scheme will explore 12 leaf nodes. (This search space is not shown because of lack of space. It can be worked out using Figure 2.)\nAlgorithm Formula Decomposition and Conditioning (FDC) is presented as Algorithm 1. It takes as input a PropMRF M. The first step is the simplification step in which we reduce the size of the hard and the soft clauses using techniques such as unit propagation, resolution and subsumption elimination. In the decomposition step (Step 2), we decompose M into independent PropMRFs if its primal graph is decomposable. Each of them are then solved independently. Note that this is a very important step and is the primary reason for efficiency of techniques such as recursive conditioning (Darwiche, 2001) and AND/OR search (Dechter and Mateescu, 2007). In fact, the whole idea in performing simplification and heuristic conditioning is to split the PropMRF into several PropMRF\u2019s that can be solved independently. Finally, in the conditioning step, we heuristically select a formula R to condition on and then recurse on the true and the false assignments to R.\nWe summarize the dominance of FDC over VDC (where VDC is same as FDC except that we condition only on unit clauses in Step 3) in the following proposition.\nPROPOSITION 1. Given a PropMRF M, let SM,F and SM,V be the number of nodes in the smallest search space explored by FDC and VDC respectively. Then SM,F \u2264 SM,V . Sometimes, this inequality can be strict.\nImprovements\nWe consider two important improvements. First, note that if we are not careful, the algorithm as presented may yield a super-exponential search space. For example, if we condition on a set of arbitrary formulas, none of which simplify the PropMRF, we may end up conditioning on a super-exponential number of formulas. Trivially, to guarantee at least an exponential search space in the size of the clausal specification, the formula selected for conditioning must reduce/simplify at least one soft clause or at least one hard clause. Second, we can augment FDC with component caching and clause learning as in Cachet (Sang et al., 2005) and use w-cutset conditioning (Dechter, 1999) in a straight forward manner. We omit the details."}, {"heading": "3.1 Related work", "text": "FDC generalizes variable-based conditioning schemes such as recursive conditioning (Darwiche, 2001), AND/OR search (Dechter and Mateescu, 2007) and value elimination (Bacchus et al., 2003) because all we have to do is restrict our conditioning to unit clauses. FDC also generalizes weighted model counting (WMC) approaches such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al., 2005). These weighted model counting approaches introduce additional Boolean variables to model each soft clause. Conditioning on these Boolean variables is equivalent to conditioning on the soft clauses present in the PropMRF. Thus, FDC can simulate WMC by restricting its conditioning to not only the unit clauses but also the soft clauses already present in the PropMRF. Finally, FDC is related to streamlined constraint reasoning (SCR) approach of (Gomes and Sellmann, 2004). The idea in SCR is to add a set of streamlining formulas to the input formula in order to cut down the size of its solution space in a controlled manner. The goal of streamlining is solving a Boolean Satisfiability (or a Constraint Satisfaction) problem while FDC uses (streamlined) formulas for weighted model counting."}, {"heading": "4 Formula Importance Sampling", "text": "In this section, we generalize conventional variable-based importance sampling to formula importance sampling and show that our generalization yields new sampling schemes having smaller variance. We first present background on variable-based importance sampling."}, {"heading": "4.1 Variable-Based Importance Sampling", "text": "Importance sampling (Rubinstein, 1981) is a general scheme which can be used to approximate any quantity such as ZM which can be expressed as a sum of a function over a domain. The main idea is to use an importance distribution Q, which satisfies PM(x) > 0 \u21d2 Q(x) > 0 and express ZM as follows:\nZM = \u2211\nx\u2208Sol(F )\nm\u220f\ni=1\n\u03c6i(xV (\u03c6i))\u00d7 Q(x) Q(x)\n= EQ\n[ I(x) \u220fm i=1 \u03c6i(xV (\u03c6i)) Q(x) ] (9)\nwhere I(x) is an indicator function which is 1 if x is a solution of FM and 0 otherwise.\nGiven N independent and identical (i.i.d.) samples (x(1), . . . , x(N)) drawn from Q, we can estimate ZM using Z\u0302N , defined below:\nZ\u0302N = 1\nN\nN\u2211\ni=1\nI(x(i)) \u220fm\nj=1 \u03c6j(x (i) V (\u03c6j) )\nQ(x(i)) (10)\nIt is known (Rubinstein, 1981) that EQ[Z\u0302N ] = ZM, namely it is unbiased. The mean squared error (MSE) of Z\u0302N is given by:\nMSE(Z\u0302N) = V arQ\n[ I(x)\n\u220f m i=1 \u03c6i(xV (\u03c6i)) Q(x)\n]\nN (11)\nThus, we can reduce the mean squared error by either reducing the variance (given in the numerator) or by increasing the number of samples N (or both)."}, {"heading": "4.2 Formula-based Importance Sampling", "text": "Importance sampling can be extended to the space of clauses (or formulas) in a straight forward manner. Let H = {H1, . . . , Hr} be a set of arbitrary formulas over the variables X of M, and let h = (h1, . . . , hr) be a truth assignment to all the clauses in H. Let H be such that every consistent truth assignment h evaluates all soft clauses to either True or False. Note that this condition is critical. Trivially, if H equals the set of soft clauses, then the condition is satisfied. Let Fh be the formula corresponding to conjunction (H1 = h1 \u2227 . . . \u2227 Hr = hr) and let xh \u2208 Sol(Fh). Given a function \u03c6, let xh,V (\u03c6) be the restriction of xh to the scope of \u03c6. Then, given an importance distribution U(H), we can rewrite ZM as:\nZM = \u2211\nh\u2208H\n#(Fh \u2227 FM)\u00d7 \u220fm\ni=1 \u03c6i(xh,V (\u03c6i)) U(h) U(h)\n= EU\n[ #(Fh \u2227 FM)\u00d7 \u220fm i=1 \u03c6i(xh,V (\u03c6i))\nU(h)\n] (12)\nAlgorithm 2: Formula Importance Sampling (FIS) Input: A PropMRFM and an importance distribution U(H) over a set of clauses H = {H1, . . . ,Hr} Output: An unbiased estimate of ZM begin\nZ\u0303 = 0, N = 0 repeat\nqb = 1 (Backtrack-free probability is stored here), G = FM and h = \u03c6 for i = 1 to |H| do\nLet G1 = G \u2227Hi and G0 = G \u2227 \u00acHi if G0 and G1 have a solution (Checked using a SAT solver) then\nSample hi from U(Hi|h) h = h \u222a hi qb = qb\u00d7 U(Hi = hi|h) G = G \u2227 (Hi = hi)\nelse if G0 is Satisfiable then\nh = h \u222a (Hi = 0) G = G \u2227 (Hi = 0) else h = h \u222a (Hi = 1) G = G \u2227 (Hi = 1)\nw = sum of weights of soft clauses satisfied by h s = Estimate of model counts of G Z\u0303 = Z\u0303 + s\u00d7 exp(w)/qb N =N + 1\nuntil timeout Z\u0303 = Z\u0303/N\nreturn Z\u0303 end\nGivenN samples h(1), . . . , h(N) generated from U(H), we can estimate ZM as Z\u0303N , where:\nZ\u0303N = 1\nN\nN\u2211\ni=1\n#(Fh(i) \u2227 FM)\u00d7 \u220fm j=1 \u03c6j(xh(i),V (\u03c6j))\nU(h(i)) (13)\nThere are two issues that need to be addressed in order to use Equation 13 for any practical purposes. First, the importance distribution U(h) may suffer from the rejection problem (Gogate and Dechter, 2007a) in that we may generate truth assignments (to clauses) which are inconsistent, namely their model count is zero. Note that this could happen even if there are no hard clauses in M because the formula combinations considered may be inconsistent. Fortunately, if we ensure that U(h) = 0 whenever h is inconsistent, namely make U(h) backtrack-free (Gogate and Dechter, 2007a), we can avoid this problem altogether. Algorithm 2 outlines a procedure for constructing such a distribution using a complete SAT solver (for example Minisat (Sorensson and Een, 2005)). Second, computing #(Fh(j) \u2227 FM) exactly may be too time consuming. In such cases, we can use state-of-the-art approximate counting techniques such as ApproxCount (Wei and Selman, 2005), SampleCount (Gomes et al., 2007) and SampleSearch (Gogate and Dechter, 2007b)."}, {"heading": "4.3 Variance Reduction", "text": "The estimate Z\u0303N output by Algorithm 2 is likely to have smaller mean squared error than Z\u0302N given in Equation 10. In particular, given a variable-based importance distributionQ(X), we can always construct a formula based importance distribution U(H) from Q(X), such that the variance of Z\u0303N is smaller than that of Z\u0302N . Define:\nU(h) = \u2211\nxh\u2208Sol(Fh\u2227FM)\nQ(xh) (14)\nIntuitively, each sample from U(H) given by Equation 14 is heavy in the sense that it corresponds to #(FM \u2227 Fh) samples from Q(xh). Because of this larger sample size, the variance of Z\u0303N is smaller than that of Z\u0302N (assuming that #(FM \u2227 Fh) can be computed efficiently). The only caveat is that generating samples from U(H) is more expensive. Formally (the proof is provided in the extended version of the paper available online),\nTHEOREM 1. Given a PropMRFM, a proposal distribution Q(X) defined over the variables of M, a set of formulas H = {H1, . . . , Hr} and a distribution U(H) defined as in Equation 14, the variance of Z\u0303N is less than or equal to that of Z\u0302N .\nWe can easily integrate FIS with other variance reduction schemes such as Rao-Blackwellisation (Casella and Robert, 1996) and AND/OR sampling (Gogate and Dechter, 2008). These combinations can lead to interesting time versus variance tradeoffs. We leave these improvements for future work. We describe how U(H) can be constructed in practice in the next section."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Exact Inference", "text": "We compared \u201cFormula Decomposition and Conditioning (FDC)\u201d against \u201cVariable Decomposition and Conditioning (VDC)\u201d, variable elimination (VE) (Dechter, 1999) and ACE (Chavira and Darwiche, 2008) (which internally uses the C2D compiler (Darwiche, 2004)) for computing the partition function on benchmark problems from three domains: (a) Random networks, (b) medical diagnosis networks and (c) Relational networks. ACE, FDC and VDC use the same clausal representation while VE uses tabular representation. Note that the domains are deliberately chosen to elucidate the properties of FDC, in particular, to verify our intuition that as size of the clauses increases, FDC is likely to dominate VDC.\nWe implemented FDC and VDC on top of RELSAT (Roberto J. Bayardo Jr. and Pehoushek, 2000), which is a SAT model counting algorithm. As mentioned earlier, after conditioning on a formula, we use various Boolean\npropagation, pruning techniques such as unit propagation, clause learning, subsumption elimination and resolution. Also, similar to Cachet (Sang et al., 2005), we use component caching and similar to w-cutset conditioning (Dechter, 1999), we invoke bucket elimination at a node if the treewidth of the (remaining) PropMRF at the node is less than 16.\nSince FDC is a DPLL-style backtracking search scheme, its performance is highly dependent upon a good branching heuristic (that selects the next clause to condition on). In our implementation, we used a simple dynamic heuristic of conditioning on the largest sub-clause (unit clause in case of VDC) that is common to most hard and soft clauses, ties broken arbitrarily. The main intuition for this heuristic is that branching on the largest common sub-clause would cause the most propagation, yielding the most reduction in the search space size. We also tried a few other heuristics, both static and dynamic, such as (i) conditioning on a subclause C (and its negation) that causes the most unit propa-\ngations (but one has to perform unit propagations for each candidate clause, which can be quite expensive in practice) (ii) graph partitioning heuristics based on the min-fill, mindegree and hmetis orderings; these heuristics are used by solvers such as ACE (Chavira and Darwiche, 2008) and AND/OR search (Dechter and Mateescu, 2007) and (iii) Entropy-based heuristics. The results for these heuristics show a similar trend as the results for the heuristic used in our experiments, with the latter performing better on an average. We leave the development of sophisticated formulaordering heuristics for future work.\nTable 1 shows the results. For each problem, we generated 10 random instances. For each instance, we set 5% of randomly chosen variables as evidence. Each row shows the average time in seconds for each problem."}, {"heading": "5.1.1 Random networks", "text": "Our first domain is that of random networks. The networks are generated using the model (n,m, s), where n is the number of (Boolean) variables, m is the number of weighted clauses and s is the size of each weighted clause. Given n variables X = {X1, . . . , Xn}, each clause Ci (for i = 1 to m) is generated by randomly selecting s (distinct) random variables from X and negating each with probability 0.5. For our experiments, we set n = m and experimented with three values for n and m: n,m \u2208 {40, 50, 60}. s was varied from 3 to 9 in increments of 2.\nA random problem (n,m, s) is designated as n\u2212m\u2212 s in Table 1. We see that FDC dominates VDC as s increases. ACE is often inferior to FDC and often inferior to VDC. As expected, variable elimination which does not take advantage of the structure of the formulas is the fastest scheme when the treewidth is small but is unable to solve any problems having treewidth greater than 24."}, {"heading": "5.1.2 Medical Diagnosis", "text": "Our second domain is a version of QMR-DT medical diagnosis networks (Shwe et al., 1991) as used in Cachet (Sang et al., 2005). Each problem can be specified using a two layer bipartite graph in which the top layer consists of diseases and the bottom layer consists of symptoms. If a disease causes a symptom, there is an edge from the disease to the symptom. We have a weighted unit clause for each disease and a weighted clause for each symptom, which is simply a logical OR of the diseases that cause it (in (Sang et al., 2005), this clause was hard. We attach an arbitrary weight to it to make the problem harder). For our experiments, we varied the numbers of diseases and symptoms from 40 to 60. For each symptom, we varied the number of diseases that can cause it from 5 to 11 in increments of 2. The diseases for each symptom are chosen randomly.\nA QMR-DT problem (d, f, s) is designated as d\u2212 f \u2212 s in Table 1. We can see that as the size of the clauses increases,\nFDC performs better than VDC. FDC also dominates ACE as the problem size increases."}, {"heading": "5.1.3 Relational networks", "text": "Our final domain is that of relational networks. We experimented with the Friends and Smokers networks and the Entity resolution networks.\nIn the friends and smokers networks (FS), we have three first order predicates smokes(x), which indicates whether a person smokes, cancer(x), which indicates whether a person has cancer, and friends(x, y), which indicates who are friends of whom. The probabilistic model is defined by assigning weights to two logical constraints, friends(x, y) \u2227 smokes(x) \u21d2 smokes(y) and smokes(x) \u21d2 cancer(x). Given a domain for x and y, a PropMRF can be generated from these two logical constraints by considering all possible groundings of each predicate. We experimented with different domain sizes for x and y ranging from 25 to 34. From Table 1, we can see that the time required by FDC is almost the same as VDC. This is because the size of the clauses is small (\u2264 3). ACE dominates both FDC and VDC.\nEntity resolution is the problem of determining which observations correspond to the same entity. In our experiments, we consider the problem of matching citations of scientific papers. We used the CORA Markov logic network given in the Alchemy tutorial (Kok et al., 2004). This MLN has ten predicates such as Author(bib, author), T itle(bib, title), SameAuthor(author, author), SameT itle(title, title) etc. and clauses ranging from size 2 to 6. The clauses express relationship such as: if two fields have high similarity, then they are (probably) the same; if two records are the same, their fields are the same, and vice-versa; etc. We experimented with domain sizes of 2 and 3 for each of the 5 first-order variables present in the domain. The problems are denoted as cora2 and cora3 respectively. From Table 1, we can see that FDC is the only algorithm capable of solving the largest instance."}, {"heading": "5.2 Approximate Inference", "text": "We compared \u201cFormula importance sampling (FIS)\u201d against \u201cVariable importance sampling (VIS)\u201d and stateof-the-art schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling available in Alchemy (Kok et al., 2004) on the three domains described above. For both VIS and FIS, we chose to construct the importance distribution Q from the output of a Belief propagation scheme (BP), because BP was shown to yield a better importance function than other approaches in previous studies (Yuan and Druzdzel, 2006; Gogate and Dechter, 2005).\nWe describe next, how the method described in (Gogate\nand Dechter, 2005) can be adapted to construct an importance distribution over formulas. Here, we first run BP (or Generalized Belief Propagation (Yedidia et al., 2004)) over a factor (or region) graph in which the nodes are the variables and the factors are the hard and the soft clauses. Let (C1, . . . , Cm) be an ordering over the soft clauses. Given a truth assignment to the first i \u2212 1 soft clauses ci\u22121 = (c1, . . . , ci\u22121), we computeU(Ci|ci\u22121) as follows. We first simplify the formula F = FM \u2227 Fci\u22121 , possibly deriving new unit clauses. Let \u03c6Ci be the marginal distribution at the factor corresponding to the clause Ci in the output of BP. Then, U(Ci|ci\u22121) is given by:\nU(Ci = True|ci\u22121) \u221d \u2211\ny\u2208\u03c6Ci\nIF,Ci(y)\u03c6Ci(y) (15)\nwhere IF,Ci(y) = 1 if y evaluates Ci to True but does not violate any unit clause in F , and 0 otherwise. Note that the importance distribution Q over the variables is a special case of the scheme described above in which we construct a distribution over all the unit clauses.\nWe implemented Algorithm 2 as follows. Notice that the algorithm requires a SAT solver and a model counter. We used Minisat (Sorensson and Een, 2005) as our SAT solver. For model counting, we use the RELSAT model counter whenever exact counting was feasible2 and the approximate solver SampleSearch (Gogate and Dechter, 2007b) whenever it wasn\u2019t.\nWe measure the performance of the sampling schemes using the sum Kullback-Leibler divergence (KLD) between the exact and the approximate posterior marginals for each variable given evidence. Time versus sum KLD plots for two representative problems from each domain are shown in Figures 4, 5 and 6. We can clearly see that as the size of the clauses increases, FIS outperforms VIS, MC-SAT and Gibbs sampling."}, {"heading": "6 Summary and Conclusion", "text": "In this paper, we introduced a new formula-based approach for performing exact and approximate inference in graphical models. Formula-based inference is attractive because: (a) it generalizes standard variable-based inference, (b) it yields several new efficient algorithms that are not possible by reasoning just over the variables and (c) it fits naturally within the recent research efforts in combining logical and probabilistic Artificial Intelligence.\nOur empirical evaluation shows that formula-based approach is especially suitable for domains having large clauses. Such clauses are one of the main reasons for using logic instead of tables for representing potentials or\n2Exact counting was invoked if the number of variables was less than 100, which was the case for most networks that we experimented with, except the relational benchmarks.\nCPTs in graphical models. In particular, conventional tabular representations require space exponential in the number of variables in the scope of the potential, while if the potential can be summarized using a constant number of clauses, we only require linear space. Since an efficient inference scheme is one of the main bottleneck in learning PropMRFs having large clauses, we believe that our formula-based approach to inference can lead to new structure and weight learning schemes that learn large weighted clauses from data.\nOur work can be extended in several ways. In particular, we envision formula-based versions of various inference schemes such as variable elimination, belief propagation and Markov Chain Monte Carlo (MCMC) sampling. One of these schemes, namely formula elimination trivially follows from this work, as it is known that conditioning works along the reverse direction of elimination (Dechter, 1999). Also, we envision the development of lifted versions of all the formula-based schemes proposed in this paper."}, {"heading": "Acknowledgements", "text": "This research was partly funded by ARO grant W911NF08-1-0242, AFRL contract FA8750-09-C-0181, DARPA contracts FA8750-05-2-0283, FA8750-07-D-0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-\nD030010, NSF grants IIS-0534881 and IIS-0803481, and ONR grant N00014-08-1-0670. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, DARPA, NSF, ONR, or the United States Government."}], "references": [{"title": "Value Elimination: Bayesian Inference via Backtracking Search", "author": ["Fahiem Bacchus", "Shannon Dalmao", "Toniann Pitassi"], "venue": "In UAI,", "citeRegEx": "Bacchus et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 2003}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Boutilier.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier.", "year": 1996}, {"title": "Rao-Blackwellisation of sampling schemes", "author": ["George Casella", "Christian P. Robert"], "venue": null, "citeRegEx": "Casella and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Casella and Robert.", "year": 1996}, {"title": "On probabilistic inference by weighted model counting", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Chavira and Darwiche.,? \\Q2008\\E", "shortCiteRegEx": "Chavira and Darwiche.", "year": 2008}, {"title": "Recursive conditioning", "author": ["Adnan Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Darwiche.,? \\Q2001\\E", "shortCiteRegEx": "Darwiche.", "year": 2001}, {"title": "New Advances in Compiling CNF into Decomposable Negation Normal Form", "author": ["Adnan Darwiche"], "venue": "In ECAI,", "citeRegEx": "Darwiche.,? \\Q2004\\E", "shortCiteRegEx": "Darwiche.", "year": 2004}, {"title": "AND/OR search spaces for graphical models", "author": ["R. Dechter", "R. Mateescu"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter and Mateescu.,? \\Q2007\\E", "shortCiteRegEx": "Dechter and Mateescu.", "year": 2007}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence", "author": ["Pedro Domingos", "Daniel Lowd"], "venue": null, "citeRegEx": "Domingos and Lowd.,? \\Q2009\\E", "shortCiteRegEx": "Domingos and Lowd.", "year": 2009}, {"title": "Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images", "author": ["Stuart Geman", "Donald Geman"], "venue": "IEEE Transaction on Pattern analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman.,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman.", "year": 1984}, {"title": "Introduction to Statistical Relational Learning", "author": ["Lise Getoor", "Ben Taskar"], "venue": null, "citeRegEx": "Getoor and Taskar.,? \\Q2007\\E", "shortCiteRegEx": "Getoor and Taskar.", "year": 2007}, {"title": "Approximate inference algorithms for hybrid Bayesian networks with discrete constraints", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "In UAI,", "citeRegEx": "Gogate and Dechter.,? \\Q2005\\E", "shortCiteRegEx": "Gogate and Dechter.", "year": 2005}, {"title": "SampleSearch: A scheme that Searches for Consistent Samples", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "Proceedings of the 11th Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Gogate and Dechter.,? \\Q2007\\E", "shortCiteRegEx": "Gogate and Dechter.", "year": 2007}, {"title": "Approximate counting by sampling the backtrack-free search space", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "In Proceedings of 22nd Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Gogate and Dechter.,? \\Q2007\\E", "shortCiteRegEx": "Gogate and Dechter.", "year": 2007}, {"title": "AND/OR Importance Sampling", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Gogate and Dechter.,? \\Q2008\\E", "shortCiteRegEx": "Gogate and Dechter.", "year": 2008}, {"title": "Streamlined constraint reasoning", "author": ["Carla P. Gomes", "Meinolf Sellmann"], "venue": "In CP,", "citeRegEx": "Gomes and Sellmann.,? \\Q2004\\E", "shortCiteRegEx": "Gomes and Sellmann.", "year": 2004}, {"title": "From sampling to model counting", "author": ["Carla P. Gomes", "J\u00f6rg Hoffmann", "Ashish Sabharwal", "Bart Selman"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Gomes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2007}, {"title": "Local computation with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Lauritzen and Spiegelhalter.,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter.", "year": 1988}, {"title": "Using weighted max-sat engines to solve mpe", "author": ["James D. Park"], "venue": "In AAAI,", "citeRegEx": "Park.,? \\Q2002\\E", "shortCiteRegEx": "Park.", "year": 2002}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Inducing features of random fields", "author": ["Stephen Della Pietra", "Vincent J. Della Pietra", "John D. Lafferty"], "venue": "IEEE Transanctions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In AAAI,", "citeRegEx": "Poon and Domingos.,? \\Q2006\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2006}, {"title": "Counting models using connected components", "author": ["Roberto J. Bayardo Jr.", "Joseph Daniel Pehoushek"], "venue": "In Proceedings of 17th National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Jr. and Pehoushek.,? \\Q2000\\E", "shortCiteRegEx": "Jr. and Pehoushek.", "year": 2000}, {"title": "Simulation and the Monte Carlo Method", "author": ["Reuven Y. Rubinstein"], "venue": null, "citeRegEx": "Rubinstein.,? \\Q1981\\E", "shortCiteRegEx": "Rubinstein.", "year": 1981}, {"title": "Heuristics for fast exact model counting", "author": ["Tian Sang", "Paul Beame", "Henry Kautz"], "venue": "In Eighth International Conference on Theory and Applications of Satisfiability Testing (SAT),", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Probabilistic diagnosis using a reformulation of the internist- 1/qmr knowledge base i. the probabilistic model and inference algorithms", "author": ["M. Shwe", "B. Middleton", "D. Heckerman", "M. Henrion", "E. Horvitz", "H. Lehmann", "G. Cooper"], "venue": "Methods of Information in Medicine,", "citeRegEx": "Shwe et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Shwe et al\\.", "year": 1991}, {"title": "Minisat v1.13-a SAT solver with conflict-clause minimization", "author": ["Niklas Sorensson", "Niklas Een"], "venue": "In SAT 2005 competition,", "citeRegEx": "Sorensson and Een.,? \\Q2005\\E", "shortCiteRegEx": "Sorensson and Een.", "year": 2005}, {"title": "A new approach to model counting", "author": ["Wei Wei", "Bart Selman"], "venue": "In SAT,", "citeRegEx": "Wei and Selman.,? \\Q2005\\E", "shortCiteRegEx": "Wei and Selman.", "year": 2005}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["Jonathan S. Yedidia", "William T. Freeman", "Yair Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2004}, {"title": "Importance sampling algorithms for Bayesian networks: Principles and performance", "author": ["Changhe Yuan", "Marek J. Druzdzel"], "venue": "Mathematical and Computer Modelling,", "citeRegEx": "Yuan and Druzdzel.,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Druzdzel.", "year": 2006}], "referenceMentions": [{"referenceID": 19, "context": "Another problem is that in general a set of formula probabilities does not completely specify a distribution, but this is naturally solved by assuming the maximum entropy distribution consistent with the specified probabilities (Nilsson, 1986; Pietra et al., 1997).", "startOffset": 228, "endOffset": 264}, {"referenceID": 18, "context": "This contrasts with the large literature on inference for graphical models, which always specify unique and consistent distributions (Pearl, 1988).", "startOffset": 133, "endOffset": 146}, {"referenceID": 9, "context": "This issue has gained prominence in the field of statistical relational learning (SRL) (Getoor and Taskar, 2007), which seeks to learn models with both logical and probabilistic aspects.", "startOffset": 87, "endOffset": 112}, {"referenceID": 7, "context": "For example, Markov logic represents knowledge as a set of weighted formulas, which define a log-linear model (Domingos and Lowd, 2009).", "startOffset": 110, "endOffset": 135}, {"referenceID": 19, "context": "Formulas with probabilities with the maximum entropy assumption and weighted formulas are equivalent; the problem of converting the former to the latter is equivalent to the problem of learning the maximum likelihood weights (Pietra et al., 1997).", "startOffset": 225, "endOffset": 246}, {"referenceID": 17, "context": "Another reason to seek efficient inference procedures for probabilistic logic is that inference in graphical models can be reduced to it (Park, 2002).", "startOffset": 137, "endOffset": 149}, {"referenceID": 16, "context": "Standard inference schemes for graphical models such as junction trees (Lauritzen and Spiegelhalter, 1988) and bucket elimination (Dechter, 1999) have complexity exponential in the treewidth of the model, making them impractical for complex domains.", "startOffset": 71, "endOffset": 106}, {"referenceID": 3, "context": "However, treewidth can be overcome by exploiting structural properties like determinism (Chavira and Darwiche, 2008) and context-specific independence (Boutilier, 1996).", "startOffset": 88, "endOffset": 116}, {"referenceID": 1, "context": "However, treewidth can be overcome by exploiting structural properties like determinism (Chavira and Darwiche, 2008) and context-specific independence (Boutilier, 1996).", "startOffset": 151, "endOffset": 168}, {"referenceID": 23, "context": "Several highly efficient algorithms accomplish this by encoding a graphical models as sets of weighted formulas and applying logical inference techniques to them (Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 162, "endOffset": 209}, {"referenceID": 3, "context": "Several highly efficient algorithms accomplish this by encoding a graphical models as sets of weighted formulas and applying logical inference techniques to them (Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 162, "endOffset": 209}, {"referenceID": 6, "context": "FDC performs AND/OR search (Dechter and Mateescu, 2007) or recursive conditioning (Darwiche, 2001), with and without caching, over the space of formulas, utilizing several Boolean constraint propagation and pruning techniques.", "startOffset": 27, "endOffset": 55}, {"referenceID": 4, "context": "FDC performs AND/OR search (Dechter and Mateescu, 2007) or recursive conditioning (Darwiche, 2001), with and without caching, over the space of formulas, utilizing several Boolean constraint propagation and pruning techniques.", "startOffset": 82, "endOffset": 98}, {"referenceID": 15, "context": "These model counts can either be computed exactly, if it is feasible, or approximately using the recently introduced approximate model counters such as SampleCount (Gomes et al., 2007) and SampleSearch (Gogate and Dechter, 2007b).", "startOffset": 164, "endOffset": 184}, {"referenceID": 3, "context": "Our experiments show that as the number of variables in the formulas increases, formula-based schemes not only dominate their variable based counterparts but also state-of-the-art exact algorithms such as ACE (Chavira and Darwiche, 2008) and approximate schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling (Geman and Geman, 1984).", "startOffset": 209, "endOffset": 237}, {"referenceID": 20, "context": "Our experiments show that as the number of variables in the formulas increases, formula-based schemes not only dominate their variable based counterparts but also state-of-the-art exact algorithms such as ACE (Chavira and Darwiche, 2008) and approximate schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling (Geman and Geman, 1984).", "startOffset": 277, "endOffset": 302}, {"referenceID": 8, "context": "Our experiments show that as the number of variables in the formulas increases, formula-based schemes not only dominate their variable based counterparts but also state-of-the-art exact algorithms such as ACE (Chavira and Darwiche, 2008) and approximate schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling (Geman and Geman, 1984).", "startOffset": 322, "endOffset": 345}, {"referenceID": 17, "context": "It is known that any discrete Markov random field or a Bayesian network can be encoded as a PropMRF (Park, 2002; Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 100, "endOffset": 159}, {"referenceID": 23, "context": "It is known that any discrete Markov random field or a Bayesian network can be encoded as a PropMRF (Park, 2002; Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 100, "endOffset": 159}, {"referenceID": 3, "context": "It is known that any discrete Markov random field or a Bayesian network can be encoded as a PropMRF (Park, 2002; Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 100, "endOffset": 159}, {"referenceID": 3, "context": "These and other ideas form the backbone of many state-of-the-art schemes such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al.", "startOffset": 85, "endOffset": 113}, {"referenceID": 23, "context": "These and other ideas form the backbone of many state-of-the-art schemes such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al., 2005).", "startOffset": 125, "endOffset": 144}, {"referenceID": 4, "context": "Another advancement that we can use is problem decomposition (Darwiche, 2001; Dechter and Mateescu, 2007).", "startOffset": 61, "endOffset": 105}, {"referenceID": 6, "context": "Another advancement that we can use is problem decomposition (Darwiche, 2001; Dechter and Mateescu, 2007).", "startOffset": 61, "endOffset": 105}, {"referenceID": 4, "context": "Note that this is a very important step and is the primary reason for efficiency of techniques such as recursive conditioning (Darwiche, 2001) and AND/OR search (Dechter and Mateescu, 2007).", "startOffset": 126, "endOffset": 142}, {"referenceID": 6, "context": "Note that this is a very important step and is the primary reason for efficiency of techniques such as recursive conditioning (Darwiche, 2001) and AND/OR search (Dechter and Mateescu, 2007).", "startOffset": 161, "endOffset": 189}, {"referenceID": 23, "context": "Second, we can augment FDC with component caching and clause learning as in Cachet (Sang et al., 2005) and use w-cutset conditioning (Dechter, 1999) in a straight forward manner.", "startOffset": 83, "endOffset": 102}, {"referenceID": 4, "context": "FDC generalizes variable-based conditioning schemes such as recursive conditioning (Darwiche, 2001), AND/OR search (Dechter and Mateescu, 2007) and value elimination (Bacchus et al.", "startOffset": 83, "endOffset": 99}, {"referenceID": 6, "context": "FDC generalizes variable-based conditioning schemes such as recursive conditioning (Darwiche, 2001), AND/OR search (Dechter and Mateescu, 2007) and value elimination (Bacchus et al.", "startOffset": 115, "endOffset": 143}, {"referenceID": 0, "context": "FDC generalizes variable-based conditioning schemes such as recursive conditioning (Darwiche, 2001), AND/OR search (Dechter and Mateescu, 2007) and value elimination (Bacchus et al., 2003) because all we have to do is restrict our conditioning to unit clauses.", "startOffset": 166, "endOffset": 188}, {"referenceID": 3, "context": "FDC also generalizes weighted model counting (WMC) approaches such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al.", "startOffset": 74, "endOffset": 102}, {"referenceID": 23, "context": "FDC also generalizes weighted model counting (WMC) approaches such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al., 2005).", "startOffset": 114, "endOffset": 133}, {"referenceID": 14, "context": "Finally, FDC is related to streamlined constraint reasoning (SCR) approach of (Gomes and Sellmann, 2004).", "startOffset": 78, "endOffset": 104}, {"referenceID": 22, "context": "Importance sampling (Rubinstein, 1981) is a general scheme which can be used to approximate any quantity such as ZM which can be expressed as a sum of a function over a domain.", "startOffset": 20, "endOffset": 38}, {"referenceID": 22, "context": "It is known (Rubinstein, 1981) that EQ[\u1e90N ] = ZM, namely it is unbiased.", "startOffset": 12, "endOffset": 30}, {"referenceID": 25, "context": "Algorithm 2 outlines a procedure for constructing such a distribution using a complete SAT solver (for example Minisat (Sorensson and Een, 2005)).", "startOffset": 119, "endOffset": 144}, {"referenceID": 26, "context": "In such cases, we can use state-of-the-art approximate counting techniques such as ApproxCount (Wei and Selman, 2005), SampleCount (Gomes et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 15, "context": "In such cases, we can use state-of-the-art approximate counting techniques such as ApproxCount (Wei and Selman, 2005), SampleCount (Gomes et al., 2007) and SampleSearch (Gogate and Dechter, 2007b).", "startOffset": 131, "endOffset": 151}, {"referenceID": 2, "context": "We can easily integrate FIS with other variance reduction schemes such as Rao-Blackwellisation (Casella and Robert, 1996) and AND/OR sampling (Gogate and Dechter, 2008).", "startOffset": 95, "endOffset": 121}, {"referenceID": 13, "context": "We can easily integrate FIS with other variance reduction schemes such as Rao-Blackwellisation (Casella and Robert, 1996) and AND/OR sampling (Gogate and Dechter, 2008).", "startOffset": 142, "endOffset": 168}, {"referenceID": 3, "context": "We compared \u201cFormula Decomposition and Conditioning (FDC)\u201d against \u201cVariable Decomposition and Conditioning (VDC)\u201d, variable elimination (VE) (Dechter, 1999) and ACE (Chavira and Darwiche, 2008) (which internally uses the C2D compiler (Darwiche, 2004)) for computing the partition function on benchmark problems from three domains: (a) Random networks, (b) medical diagnosis networks and (c) Relational networks.", "startOffset": 166, "endOffset": 194}, {"referenceID": 5, "context": "We compared \u201cFormula Decomposition and Conditioning (FDC)\u201d against \u201cVariable Decomposition and Conditioning (VDC)\u201d, variable elimination (VE) (Dechter, 1999) and ACE (Chavira and Darwiche, 2008) (which internally uses the C2D compiler (Darwiche, 2004)) for computing the partition function on benchmark problems from three domains: (a) Random networks, (b) medical diagnosis networks and (c) Relational networks.", "startOffset": 235, "endOffset": 251}, {"referenceID": 23, "context": "Also, similar to Cachet (Sang et al., 2005), we use component caching and similar to w-cutset conditioning (Dechter, 1999), we invoke bucket elimination at a node if the treewidth of the (remaining) PropMRF at the node is less than 16.", "startOffset": 24, "endOffset": 43}, {"referenceID": 3, "context": "We also tried a few other heuristics, both static and dynamic, such as (i) conditioning on a subclause C (and its negation) that causes the most unit propagations (but one has to perform unit propagations for each candidate clause, which can be quite expensive in practice) (ii) graph partitioning heuristics based on the min-fill, mindegree and hmetis orderings; these heuristics are used by solvers such as ACE (Chavira and Darwiche, 2008) and AND/OR search (Dechter and Mateescu, 2007) and (iii) Entropy-based heuristics.", "startOffset": 413, "endOffset": 441}, {"referenceID": 6, "context": "We also tried a few other heuristics, both static and dynamic, such as (i) conditioning on a subclause C (and its negation) that causes the most unit propagations (but one has to perform unit propagations for each candidate clause, which can be quite expensive in practice) (ii) graph partitioning heuristics based on the min-fill, mindegree and hmetis orderings; these heuristics are used by solvers such as ACE (Chavira and Darwiche, 2008) and AND/OR search (Dechter and Mateescu, 2007) and (iii) Entropy-based heuristics.", "startOffset": 460, "endOffset": 488}, {"referenceID": 24, "context": "Our second domain is a version of QMR-DT medical diagnosis networks (Shwe et al., 1991) as used in Cachet (Sang et al.", "startOffset": 68, "endOffset": 87}, {"referenceID": 23, "context": ", 1991) as used in Cachet (Sang et al., 2005).", "startOffset": 26, "endOffset": 45}, {"referenceID": 23, "context": "We have a weighted unit clause for each disease and a weighted clause for each symptom, which is simply a logical OR of the diseases that cause it (in (Sang et al., 2005), this clause was hard.", "startOffset": 151, "endOffset": 170}, {"referenceID": 20, "context": "We compared \u201cFormula importance sampling (FIS)\u201d against \u201cVariable importance sampling (VIS)\u201d and stateof-the-art schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling available in Alchemy (Kok et al.", "startOffset": 136, "endOffset": 161}, {"referenceID": 28, "context": "For both VIS and FIS, we chose to construct the importance distribution Q from the output of a Belief propagation scheme (BP), because BP was shown to yield a better importance function than other approaches in previous studies (Yuan and Druzdzel, 2006; Gogate and Dechter, 2005).", "startOffset": 228, "endOffset": 279}, {"referenceID": 10, "context": "For both VIS and FIS, we chose to construct the importance distribution Q from the output of a Belief propagation scheme (BP), because BP was shown to yield a better importance function than other approaches in previous studies (Yuan and Druzdzel, 2006; Gogate and Dechter, 2005).", "startOffset": 228, "endOffset": 279}, {"referenceID": 10, "context": "We describe next, how the method described in (Gogate and Dechter, 2005) can be adapted to construct an importance distribution over formulas.", "startOffset": 46, "endOffset": 72}, {"referenceID": 27, "context": "Here, we first run BP (or Generalized Belief Propagation (Yedidia et al., 2004)) over a factor (or region) graph in which the nodes are the variables and the factors are the hard and the soft clauses.", "startOffset": 57, "endOffset": 79}, {"referenceID": 25, "context": "We used Minisat (Sorensson and Een, 2005) as our SAT solver.", "startOffset": 16, "endOffset": 41}], "year": 2010, "abstractText": "Computing the probability of a formula given the probabilities or weights associated with other formulas is a natural extension of logical inference to the probabilistic setting. Surprisingly, this problem has received little attention in the literature to date, particularly considering that it includes many standard inference problems as special cases. In this paper, we propose two algorithms for this problem: formula decomposition and conditioning, which is an exact method, and formula importance sampling, which is an approximate method. The latter is, to our knowledge, the first application of model counting to approximate probabilistic inference. Unlike conventional variable-based algorithms, our algorithms work in the dual realm of logical formulas. Theoretically, we show that our algorithms can greatly improve efficiency by exploiting the structural information in the formulas. Empirically, we show that they are indeed quite powerful, often achieving substantial performance gains over state-of-the-art schemes.", "creator": "gnuplot 4.2 patchlevel 4 "}}}