{"id": "1203.5073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2012", "title": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding", "abstract": "This paper describes the University of Sheffield's entry in the 2011 TAC KBP entity linking and slot filling tasks. We chose to participate in the monolingual entity linking task, the monolingual slot filling task and the temporal slot filling tasks. We set out to build a framework for experimentation with knowledge base population data, including the duration of each slot filling task, the total number of slots, and the time that it takes to fill slots. This model is described in the section below.\n\n\n\n\n\n\n\nThe following table describes the study of the duration of each slot filling task and its results.\n\n\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 2 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 53 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 103 104 105 106 107 108 109 111 112 113 114 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53", "histories": [["v1", "Thu, 22 Mar 2012 18:34:19 GMT  (116kb,D)", "http://arxiv.org/abs/1203.5073v1", "Proc. Text Analysis Conference (2011)"]], "COMMENTS": "Proc. Text Analysis Conference (2011)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amev burman", "arun jayapal", "sathish kannan", "madhu kavilikatta", "ayman alhelbawy", "leon derczynski", "robert gaizauskas"], "accepted": false, "id": "1203.5073"}, "pdf": {"name": "1203.5073.pdf", "metadata": {"source": "CRF", "title": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding", "authors": ["Amev Burman", "Arun Jayapal", "Sathish Kannan", "Madhu Kavilikatta", "Ayman Alhelbawy", "Leon Derczynski", "Robert Gaizauskas"], "emails": [], "sections": [{"heading": null, "text": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding\nAmev Burman, Arun Jayapal, Sathish Kannan, Madhu Kavilikatta Ayman Alhelbawy, Leon Derczynski, Robert Gaizauskas\nNatural Language Processing Group Department of Computer Science\nUniversity of Sheffield Sheffield, S1 4DP, UK"}, {"heading": "1 Introduction", "text": "This paper describes the University of Sheffield\u2019s entry in the 2011 TAC KBP entity linking and slot filling tasks (Ji et al., 2011). We chose to participate in the monolingual entity linking task, the monolingual slot filling task and the temporal slot filling tasks. Our team consisted of five MSc students, two PhD students and one more senior academic. For the MSc students, their participation in the track formed the core of their MSc dissertation project, which they began in February 2011 and finished at the end of August 2011. None of them had any prior experience in human language technologies or machine learning before their programme started in October 2010. For the two PhD students participation was relevant to their ongoing PhD research. This team organization allowed us to muster considerable manpower without dedicated external funding and within a limited period time; but of course there were inevitable issues with co-ordination of effort and of getting up to speed. The students found participation to be an excellent and very enjoyable learning experience.\nInsofar as any common theme emerges from our approaches to the three tasks it is an effort to learn from and exploit data wherever possible: in entity linking we learn thresholds for nil prediction and acquire lists of name variants from data; in slot filling we learn entity recognizers and relation extractors; in temporal slot filling we use time and event annotators that are learned from data.\nThe rest of this paper describes our approach and related investigations in more detail. Sections 2 and 3 describe in detail our approaches to the EL and SF tasks respectively, and Section 4 summarises\nour temporal slot filling approach."}, {"heading": "2 Entity Linking Task", "text": "The entity linking task is to associate a queried named entity mention, as contextualized within a given document, with a knowledge base (KB) node in a provided knowledge base which describes the same real world entity. If there is no such node the entity should be linked to Nil. There are three main challenges in this task. The first challenge is the ambiguity and multiplicity of names: the same named entity string can occur in different contexts with different meaning (e.g. Norfolk can refer to a city in the United States or the United Kingdom); furthermore, the same named entity may be denoted using various strings, including, e.g. acronyms (USA) and nick names (Uncle Sam). The second challenge is that the queried named entity may not be found in the knowledge base at all. The final challenge is to cluster all Nil linked mentions."}, {"heading": "2.1 System Processing", "text": "Our system consists of four stage model, as shown in Figure 1:\n1. Candidate Generation: In this stage, all KB nodes which might possibly be linked to the query entity are retrieved. 2. Nil Predictor: In this stage, a binary classifier is applied to decide whether the query mention should be linked to a KB node or not. 3. Candidate Selection: In this stage, for each query mention that is to be linked to the KB, one candidate from the candidate set is selected as the link for the query mention.\nar X\niv :1\n20 3.\n50 73\nv1 [\ncs .C\nL ]\n2 2\nM ar\n2 01\n2\n4. Nil Mention Clustering: In this stage, all Nil linked query mentions are clustered so that each cluster contains all mentions that should be linked to a single KB node, i.e. pertain to the same entity."}, {"heading": "2.1.1 Candidate Generation", "text": "The main objective of the candidate generation process is to reduce the search space of potential link targets from the full KB to a small subset of plausible candidate nodes within it. The query mention is used, both as a single phrase and as the set of its constituent tokens, to search for the query string in the titles and body text of the KB node.\nVariant name extraction We extracted different name forms for the same named entity mention from a Wikipedia dump. Hyper-links, redirect pages and disambiguation pages are used to associate different named entity mentions with the same entity (Reddy et al., 2010; Varma et al., 2009). This repository of suggested name variants is then used in query expansion to extend the queries regarding a given entity to all of its possible names. Since the mention of the entity is not yet disambiguated, it is not necessary for all suggested name variants to be accurate.\nQuery Generation We generated sets of queries according to two different strategies. The first strat-\negy is based on name variants, using the previously built repository of Wikipedia name variants. The second strategy uses additional named entity (NE) mentions for query expansion: the Stanford NE recognizer (Finkel et al., 2005) is used to find NE mentions in the query document, and generates a query containing the query entity mention plus all the NE mentions found in the query document,\nRetrieval After query generation, we performed document retrieval using Lucene. All knowledge base nodes, titles, and wiki-text were included in the Lucene index. Documents are represented as in the Vector Space Model (VSM). For ranking results, we use the default Lucene similarity function which is closely related to cosine similarity ."}, {"heading": "2.1.2 Nil Prediction", "text": "In many cases, a named entity mention is not expected to appear in the knowledge base. We need to detect these cases and mark them with a NIL link. The NIL link is assigned after generating a candidate list (see Varma et al. (2009), Radford et al. (2010)).\nIf the generated candidate list is empty, then the query mention is linked to NIL. If the candidate list is not empty, we use two techniques to find a candidate. The first just chooses the highest ranked can-\ndidate, i.e. the highest scoring candidate using the Lucene similarity score. If the the highest scoring candidate score is above some threshold \u03b1 then the candidate is selected and if it is under the threshold, the predictor links the mention to NIL. The second technique calculates the difference between the scores of two highest scoring candidates, then compares this difference with some threshold \u03b2; if the difference exceeds the threshold the highest scoring candidate is selected, otherwise the query mention is linked to NIL.. Results of these two techniques are shown in Tables 1 and 2.\nParameter Setting for Nil Matching To find the best thresholds, a Na\u0131\u0308ve Bayes classifier is trained using the TAC 2010 training data. We created training data as follows. For each query in the training set, we generate a candidate list and the highest scoring document is used as a feature vector. If it is the correct candidate then the output is set to true else the output set to false. From this set of instances a classifier is learned to get the best threshold."}, {"heading": "2.1.3 Candidate Selection", "text": "The candidate selection stage will run only on a non-empty candidate list, since an empty candidate list means linking the query mention to NIL. For each query, the highest-scoring candidate is selected as the correct candidate."}, {"heading": "2.1.4 Nil Clustering", "text": "A simple clustering technique is applied. The Levenshtein distance is measured between the different mentions and if the distance is under a threshold \u03b1, the mentions are grouped into the same clus-\nter. Two experiments are carried out and results are presented in Table 3. As shown clustering according to the string equality achieves better results than allowing a distance of one.\nData Set: The TAC2011 data set contains 2250 instances of which 1126 must be linked to \u201cNil\u201d. In the gold standard, the 1126 Nil instances are clustered into 814 clusters. Only those 1126 instances are sent to the clustering module to check its performance separately, regardless of Nil predictor performance.\nEvaluation Metric: \u201cAll Pair Counting Measures\u201d are used to evaluate the similarity between two clustering algorithm\u2019s results. This metric examines how likely the algorithms are to group or separate a pair of data points together in different clusters. These measures are able to compare clusterings with different numbers of clusters.\nThe Rand index (Rand, 1971) computes similarity between the system output clusters (output of the clustering algorithm) and the clusters found in a gold standard. So, the Rand index measures the percentage of correct decisions \u2013 pairs of data points that are clustered together in both system output and gold standard, or, clustered in different clusters in both system output and gold standard \u2013 made by the algorithm. It can be computed using the following formula:\nRI = Tp+ Tn\nTp+ Tn+ Fp+ Fn"}, {"heading": "2.2 Evaluation", "text": "In this section we provide a short description of different runs and their results. All experiments are evaluated using the B-Cubed+ and micro average scoring metrics. In our experimental setup, a threshold \u03b1 = 5.9 is used in Nil-Predictor and Levenshtein distance = 0 is used for Nil clustering. The standard scorer released by the TAC organizers is used to evaluate each run, with results in Table 4. Different query schemes are used in different runs as follows.\n1. Wiki-text is not used, with search limited to nodes titles only. The search scheme used in this run uses query mention only. 2. Wiki-text is used. The search scheme used in this run uses the query mention and the different name variants for the query mention. 3. Wiki-text is used, The search scheme used in this run uses the query mention and the different name variants for the query mention. Also, it uses the query document named entities recognized by the NER system to search within the wiki-text of the node."}, {"heading": "3 Slot Filling Task", "text": "There are a number of different features that can describe an entity. For an organisation, one might talk about its leaders, its size, and place of origin. For a person, one might talk about their gender, their age, or their religious alignment. These feature types can be seen as \u2018slots\u2019, the values of which can be used to describe an entity.\nThe slot-filling task is to find values for a set of slots for each of a given list of entities, based on a knowledge base of structured data and a source collection of millions of documents of unstructured text. In this section, we discuss our approach to slot filling."}, {"heading": "3.1 System Processing", "text": "Our system is structured as a pipeline. For each entity/slot pair, we begin by selecting documents that are likely to bear slot values, using query formulation (Section 3.1.2) and then information retrieval (Section 3.1.1) steps. After this, we examine the top ranking returned texts and, using learned classifiers, attempt to extract all standard named entity mentions plus mentions of other entity types that can occur as slot values (Section 3.1.3). Then we run a learned slot-specific relation extractor over the sentences containing an occurrence of the target entity and an entity of the type required as a value for the queried slot, yielding a list of candidate slot values (Section 3.1.4). We then rank these candidate slot values and return a slot value, or list of slot values in the case of list-valued slots, from the best candidates (Section 3.1.5)."}, {"heading": "3.1.1 Preprocessing and Indexing", "text": "Information Retrieval (IR) was used to address the tasks of Slot Filling (SF) and Entity Linking (EL) primarily because it helps in choosing the right set of documents and hence reduces the number of documents that need to be processed further down the pipeline. Two variations of IR were used in the SF task: document retrieval (DR) and passage retrieval (PR).\nThe documents were parsed to extract text and their parent elements using JDOM and then indexed using Lucene. We used Lucene\u2019s standard analyzer for indexing and stopword removal. The parent element of the text is used as field name. This gives the flexibility of searching the document using fields and document structure as well as just body (BaezaYates et al., 1999). Instead of returning the text of the document, the pointers or paths of the document were returned when a search is performed. For searching and ranking, Lucene\u2019s default settings were used.\nFor passage retrieval, various design choices were\nconsidered (Roberts and Gaizauskas, 2004) and a two stage process was selected. In the two stage process, the original index built for DR is used to retrieve the top n documents and the plain text (any text between two SGML elements) is extracted as a separate passage. A temporary mini-index is then built on the fly from these passages. From the temporary index, the top n passages are retrieved for a given query. Instead of returning the text of the passages, the location of the passage (element retrieval) in the document is returned as a passage offset within a document referenced by a file system pointer. Two versions of passage systems were created, one that removes stop-words while indexing and searching and other that keeps the stop words. For ranking, Lucene\u2019s default settings were used.\nFinally the IR system and the query formulation strategies were evaluated on the DR task to determine the optimal number of top ranked documents to retrieve for further processing down the pipeline and for PR. This evaluation is further discussed in Section 3.2 below."}, {"heading": "3.1.2 Query Formulation", "text": "This step generates a query for the IR system that attempts to retrieve the best documents for a given entity and slot type.\nVariant name extraction Variant names are the alternate names of an entity (persons or organizations only for the slot filling task in 2011) which are different from their formal name. These include various name forms such as stage names, nick names and abbreviations. Many people have an alias; some people even have more than one alias. In several cases people are better known to the world by their alias names rather than their original name. For example, Tom Cruise is well known to the world as an actor, but his original name is Thomas Cruise Mapother IV. Alias names are very helpful to disambiguate the named entity, but in some cases the\nalias names are also shared among multiple people. For example, MJ is the alias name for both Michael Jackson (Pop Singer) and Michael Jordan (Basketball player).\nVariant name forms are used for query formulation. The methods used in the slot filling task for extracting variant name forms from a Wikipedia page are:\n\u2022 Extract all the name attributes from the infobox, such as nickname, birth name, stage name and alias name. \u2022 Extract the title and all bold text from the first paragraph of the article page. \u2022 Extract the abbreviations of the entity name by finding patterns like \u201c(ABC)\u201d consisting of all capital letters appearing after the given entity name. For example, TCS is an abbreviation of the entity Tata Consultancy Service in case of the following pattern Tata Consultancy Service, (TCS). \u2022 Extract all redirect names that refer to the given entity. For example, the name \u2018King of Pop\u2019 automatically redirects to the entity named \u2018Michael Jackson\u2019. \u2022 In the case of ambiguous names extract all the possible entity names that share the same given name from the disambiguation page.\nA variant name dictionary was created by applying all the above methods to every entity in the Wikipedia dump. Each line of the dictionary contains the entity article title name as in Wikipedia followed by one of the variant name forms. This dictionary is then used at query time to find the variant name forms of the given entity.\nSlot keyword collection The query formulation stage deals with developing a query to retrieve the relevant documents or passages for each slot of each entity. Our approach is as follows:\n1. Collect manually (by referring to public sources such as Wikipedia) a list of keywords for each slot query. Some example keywords for the per:countries of residence slot query are \u2018house in\u2019, \u2018occupies\u2019, \u2018lodges in\u2019, \u2018resides in\u2019, \u2018home in\u2019, \u2018grew up in\u2019 and \u2018brought up in\u2019. 2. Extract all the alternate names of the given entity name the variant name dictionary (Section 3.1.2). 3. Formulate a query for each slot of an entity by including terms for entity mention, variant names and keywords collected for the slot query in the first step. These terms are interconnected by using Boolean operators. 4. The formulated query is then fed into the IR component and the top n documents retrieved."}, {"heading": "3.1.3 Entity Identification", "text": "Given the top n documents returned by the previous phase of the system, the next task is to identify potential slot values. To do this we used entity recognizers trained over existing annotated datasets plus some additional datasets we developed. For a few awkward slot value types we developed regular expression based matchers to identify candidate slot fills. We have also developed a restricted coreference algorithm for identifying coreferring entity mentions, particularly mentions coreferring with the query (target) entity,\nNamed Entity Recognition The Stanford Named Entity Recognition (NER) tool (Finkel et al., 2005) was used to find named entities. It is a supervised learning conditional random field based approach which comes with a pre-trained model for three entity classes. Because we needed a broader range of entity classes we re-trained the classifier using the MUC6 and MUC7 datasets 1 and NLTK (Bird et al., 2009) gazetteers. Training the classifier was not straightforward as the source data had to be reformatted into the format recognized by Stanford NER. The MUC datasets provided training data for the entities Location, Person, Organization, Time, Person, Money, Percent, Date, Number and Ordinal. More classes were added to the MUC training dataset since the slot-filling task required nationality, religion, country, state, city and cause-of-death slot\n1LDC refs. LDC2001T02, LDC2003T13\nfill types to be tagged as well. For country, state and city, which can be viewed as sub-types of type location we semi-automatically adapted the MUC training data by finding all location entities in the data, looking them up in a gazetteer and then manually adding their sub-type. For nationalities, causes of death and religion, we extracted lists of nationalities, causes of death and religions from Wikipedia. In the case of nationality and causes of death we searched for instances of these in the MUC data and then labelled them to provide training data. For religion, however, because there were so few instances in the MUC corpus and because of issues in training directly on Wikipedia text, we used a post-classifier list matching technique to identify religions.\nThe trained classifier was used to identify and tag all mentions of the entity types it knew about in the documents and/or passages returned by the search engine. These tagged documents were then passed on to the co-reference resolution system. After some analysis we discovered that in some cases the target entity supplied in the quey was not being correctly tagged by the entity tagger. Therefore we added a final phase to our entity identifier in which all occurrences of the target entity were identified and tagged with the correct type, regardless of whether they had or had not been tagged correctly by the CRF entity tagger. s\nRestricted Co-reference Resolution To identify the correct slot fill for an entity requires not just identifying mentions which are of the correct slot fill type but of ensuring that the mention stands in the appropriate relation to the target entity \u2013 so, to find Whistler\u2019s mother requires not only finding entities of type PERSON, but also determining that the person found stands the relation \u201cmother-of\u201d to Whistler. Our approach to relation identification, described in the next section, relies on the relation being expressed in a sentence in which both the candidate slot fill and the target entity occur. However, since references to the target entity or to the slot fill may be anaphoric, ability to perform coreference resolution is required.\nOff-the-shelf co-reference resolvers, such as the Stanford CRF-based coreference tool, proved too slow to complete slot-filling runs in a reasonable timeframe. Therefore, we designed a custom al-\ngorithm to do limited heuristic coreference to suit the slot-filling task. Our algorithm is limited in two ways. First, it only considers coreferring references to the target entity and ignores any coreference to candidate slot fills or between any other entities in the text. Second, only a limited set of anaphors is considered. In the case of target entities of type PERSON the only anaphors considered are personal and possessive pronouns such as he, she, his and her. In these cases it also helps to identify whether the target entity is male or female. We trained the maximum entropy classifier provided with NLTK with a list of male names and female names also from NLTK. The last and second to last characters for each name were taken as features for training the classifier. Based on the output produced by the classifier, the system decides whether certain pronouns are candidate anaphors for resolving with the target entity. For example, when the output produced by the classifier for the PERSON entity Michael Jackson is male, only mentions of he and his will be considered as candidate anaphors.\nWhen the target entity is of type ORGANIZATION, only the pronoun it or common nouns referring to types of organization, such as company, club, society, guild, association, etc. are considered as potential anaphors. A list of such organization nouns is extracted from GATE.\nFor both PERSONs and ORGANIZATIONs, when candidate anaphors are identified the algorithm resolves them to the target entity if a tagged mention of the target entity is the textually closest preceding tagged mention of an entity of the target entity type. For example, he will be coreferred with Michael Jackson if a tagged instance of Michael Jackson, or something determined to corefer to it, is the closest preceding mention of a male entity of type PERSON. If an intervening male person is found, then no coreference link is made. When coreference is established, the anaphor \u2013 either pronoun or common noun \u2013 is labelled as \u2018target entity\u201d.\nThis approach to coreference massively reduces the complexity of the generalized coreference task, making it computationally tractable within the inner loop of processing multiple documents per slot per target entity. Informal evaluation across a small number of manually examined documents showed the algorithm performed quite well."}, {"heading": "3.1.4 Candidate Slot Value Extraction", "text": "The next sub-task is to extract candidate slot fills by determining if the appropriate relation holds between a mention of the target entity and a mention of an entity of the appropriate type for the slot. For example if the slot is date of birth and the target entity is Michael Jackson then does the date of birth relation hold between some textual mention of the target entity Michael Jackson (potentially an anaphor labelled as target entity) and some textual mention of an entity tagged as type DATE.\nThe general approach we took was to select all sentences that contained both a target entity mention as well as a mention of the slot value type and run a binary relation detection classifier to detect relations between every potentially related target entity mention-slot value type mention in the sentence. If the given relation is detected in the sentence, the slot value for the relation (e.g. the entity string) is identified as a candidate value for the slot of the target entity.\nTraining the Classifiers A binary relation detection classifier needed to be trained for each type of slot. Since there is no data explicitly labelled with these relations we used a distant supervision approach (see, e.g., Mintz et al. (2009)). This relied on an external knowledge base \u2013 the infoboxes from Wikipedia \u2013 to help train the classifiers. In this approach, the fact names from the Wikipedia infoboxes were mapped to the KBP. These known slot value pairs from the external knowledge base were used to extract sentences that contain the target entity and the known slot value. These formed positive instances. Negative instances were formed from sentences containing the target entity and an entity mention of the appropriate type for the slot fill, but whose value did not match the value taken from the infobox (e.g. a DATE, but not the date of birth as specified in the infobox for the target entity). The classifiers learned from this data were then used on unknown data to extract slot value pairs.\nFeature Set Once the positive and negative training sentences were extracted, the next step was to extract feature sets from these sentences which would then be used by machine learning algorithms\nto train the classifiers. Simple lexical features and surface features were included in the feature set. Some of the features used include:\n\u2022 Bag of Words: all words in the training data not tagged as entities were used as binary features whose value is 1 or 0 for the instance depending on whether they occur in sentence from which the training instance is drawn. \u2022 Words in Window: like Bag of Words but only words between the target entity and candidate slot value mentions plus two words before and after are taken as features. \u2022 N-grams: like bag of words, but using bi-grams instead of unigrams \u2022 Token distance: one of three values \u2013 short (<= 3), medium (> 3 and <= 6) or long (> 6) \u2013 depending on the distance in tokens between the the target entity and candidate slot value mentions. \u2022 Entity in between: binary feature indicating whether there is another entity of the same type between the candidate slot value mention and the target entity. \u2022 Target first: binary feature indicating whether the target entity comes before the candidate slot value in the sentence?\nWe experimented with both the Naive Bayes and Maximum Entropy classifiers in the NLTK. For technical reasons could not get the maximum entropy classifier working in time for the official test runs, so our submitted runs used the Naive Bayes classifiers, which is almost certainly non-optimal given the non-independence of the features."}, {"heading": "3.1.5 Slot Value Selection", "text": "The final stage in our system is to select which candidate slot value (or slot values in the case of listvalued slots) to return as the correct answer from the candidate slot values extracted by the relation extractor in the previous stage. To do this we rank the\ncandidates identified in the candidate slot value extraction stage. Two factors are considered in ranking the candidates: (1) the number of times a value has been extracted, and (2) the confidence score provided for each candidate by the relation extractor classifier. If any value in the list of possible slot values occurs more than three times, then the system uses the number of occurrences as a ranking factor. Otherwise, the system uses the confidence score as a ranking factor. In the first case candidate slot values are sorted on the basis of number of occurrences. In the second case values are sorted on the basis of confidence score. In both cases the top n value(s) are taken as the correct slot value(s) for the given slot query. We use n = 1 for single-valued slots n = 3 for list-valued slots.\nOnce the system selects the final slot value(s), the final results are written to a file in the format required by the TAC guidelines."}, {"heading": "3.2 Evaluation", "text": "We evaluated both overall slot-filling performance, and also the performance of our query formulation / IR components in providing suitable data for slotfilling."}, {"heading": "3.2.1 Overall", "text": "We submitted three runs: one with documentlevel retrieval, no coreference resolution, and bag-of-words extractor features; a second with document-level retrieval, coreference resolution, and n-gram features; a third with passage-level retrieval, coreference resolution, and n-gram features. Our results are in Table 5."}, {"heading": "3.2.2 Query Formulation/Document Retrieval Evaluation", "text": "We evaluated query formulation and document retrieval using the coverage and redundancy measures introduced by Roberts and Gaizauskas (2004), originally developed for question answering. Coverage\nis the proportion of questions for which answers can be found from the documents or passages retrieved, while redundancy is the average number of documents or passages retrieved which contain answers for a given question or query. These measures may be directly carried over to the slot filling task, where we treat each slot as a question.\nThe evaluation used the 2010 TAC-KBP data for all entities and slots; results are shown in Table 6. Strict and lenient versions of each measure were used, where for the strict measure both document ID and response string must match those in the gold standard, while for the lenient only the response string must match, i.e. the slot fill must be correct but the document in which it is found need not be one which has been judged to contain a correct slot fill. This follows the original strict and lenient measures implemented in the tool we used to assist evaluation, IR4QA (Sanka, 2005).\nThe results table shows a clear increase in all measures as the number of top ranked documents is increased. With the exception of lenient redundancy, the improvement in the scores from the top 20 to the 50 documents is not very big. Furthermore if 50 documents are processing through the entire system as opposed to 20, the additional 30 documents will both more than double processing times per slot and introduce many more potential distractors for the correct slot fill (See Section 3.1.5). For these reasons we chose to limit the number of documents passed on from this stage in the processing to 20 per slot. Note that this bounds our slot fill performance to just under 60%."}, {"heading": "3.2.3 Entity Extraction and Coreference Evaluation", "text": "We evaluated our entity extractor as follows. We selected one entity and one slot for entities of type\nORGANIZATION and one for entities of type PERSON and gathered the top 20 documents returned by our query formulation and document retrieval system for each of these entity-slot pairs. We manually annotated all candidate slot value across these two twenty document sets to provide a small gold standard test set. For candidate slot fills in documents matching the ORGANIZATION query, overall F-measure for the entity identifier was 78.3% while for candidate slot fills in documents matching the PERSON query, overall F-measure for the entity identifier was 91.07%. We also manually evaluated our coreference approach over the same two document sets and arrived at an F-measure of 73.07% for coreference relating to the ORGANIZATION target entity and 90.71% for coreference relating to the PERSON target entity. We are still analyzing the wide difference in performance of both entity tagger and coreference resolver when processing documents returned in response to an ORGANIZATION query as compared to documents returned in response to a PERSON query."}, {"heading": "3.2.4 Candidate Slot Value Extraction Evaluation", "text": "To evaluate our candidate slot value extraction process we did two separate things. First we assessed the quality of training data provided by our distant supervision approach. Since it was impossible to check all the training data produced manually we randomly sampled 40 positive examples for each of four slot types (slots expecting DATEs, PERSONs, LOCATIONs and ORGANIZATIONs) and 40 negative examples for each of four slot types. Results of this evaluation are in Table 7.\nIn addition to evaluating the quality of the train-\ning data we generated, we did some evaluation to determine the optimal feature set combination. Ten fold cross validation figures for the optimal feature set over the training data are shown in the first row in Table 8, again for a selection of one slots from each of four slot types . Finally we evaluated the slot value extraction capabilities on a small test set of example sentences selected from the source collection to ensure they contained the target entity and the correct answer, as well as some negative instances, and manually processed to correctly annotate the entities within them (simulating perfect upstream performance). Results are shown in rows2 and 3 of Table 8. The large difference in performance between the ten fold cross validation figures over the training and the evaluation against the small handpicked and annotated gold standard from the source collection may be due to the fact that the training data was Wikipedia texts while the test set is news texts and potentially other text types such as blogs; however, the handpicked test set is very small (70 sentences total) so generalizations may not be warranted."}, {"heading": "4 Temporal Filling Task", "text": "The task is to detect upper and lower bounds on the start and end times of a state denoted by an entityrelation-filler triple. This results in four dates for each unique filler value. There are two temporal tasks, a full temporal bounding task and a diagnostic temporal bounding task. We provide the filler values for the full task, and TAC provides the filler values and source document for the diagnostic task. Our temporal component did not differentiate between the two tasks; for the full task, we used output values generated by our slot-filling component.\nWe approached this task by annotating source documents in TimeML (Pustejovsky et al., 2003), a modern standard for temporal semantic annotation. This involved a mixture of off-the-shelf components and custom code. After annotating the document, we attempted to identify the TimeML event that best corresponded to the entity-relation-filler triple, and then proceeded to detect absolute temporal bounds for this event using TimeML temporal relations and temporal expressions. We reasoned about the responses gathered by this exercise to generate a date quadruple as required by the task.\nIn this section, we describe our approach to temporal filling and evaluate its performance, with subsequent failure analysis."}, {"heading": "4.1 System Processing", "text": "We divide our processing into three parts: initial annotation, selection of an event corresponding to the persistence of the filler\u2019s value, and temporal reasoning to detect start and finish bounds of that state."}, {"heading": "4.1.1 TimeML Annotation", "text": "Our system must output absolute times, and so we are interested in temporal expressions in text, or TIMEX3 as they are in TimeML. We are also interested in events, as these may signify the start, end or whole persistence of a triple. Finally we need to be able to determine the nature of the relation between these times and events; TimeML uses TLINKs to annotate these relationships.\nWe used a recent version of HeidelTime (Stro\u0308tgen and Gertz, 2010) to create TimeML-compliant temporal expression (or timex) annotations on the selected document. This required a document creation time (DCT) reference to function best. For this, we built a regular-expression based DCT extractor2 and used it to create a DCT database of every document in the source collection (this failed for one of the 1 777 888 documents; upon manual examination, the culprit contained no hints of its creation time).\nThe only off-the-shelf TimeML event annotation tool found was Evita (Saur\u0131\u0301 et al., 2005), which requires some preprocessing. Specifically, explicit sentence tokenisation, verb group and noun group annotations need to be added. For our system we used the version of Evita bundled with TARSQI3. Documents were preprocessed with the ANNIE VP Chunker in GATE4. We annotated the resulting documents with Evita, and then stripped the data out, leaving only TimeML events and the timexes from the previous step.\nAt this point, we loaded our documents into a temporal annotation analysis tool, CAVaT (Derczynski and Gaizauskas, 2010), to simplify access to annotations. Our remaining task is temporal relation annotation. We divided the classes of entity that may\n2https://bitbucket.org/leondz/add-dct 3http://timeml.org/site/tarsqi/toolkit/ 4http://gate.ac.uk/\nbe linked into two sets, as per TempEval (Verhagen et al., 2010): intra-sentence event-time links, and inter-sentence event-event links with a 3-sentence window. Then, two classifiers were learned for these types of relation using the TimeBank corpus5 as training data and the linguistic tools and classifiers in NLTK6. Our feature set was the same used as Mani et al. (2007) which relied on surface data available from any TimeML annotation."}, {"heading": "4.1.2 Event Selection", "text": "To find the timexes that temporally bound a triple, we should first find events that occur during that triple\u2019s persistence. We call this task \u201cevent selection\u201d. Our approach was simple. In the first instance we looked for a TimeML event whose text matched the filler. Failing that, we looked for sentences containing the filler, and chose an event in the same sentence. If none were found, we took the entire document text and tried to match a simplified version of the filler text anywhere in the document; we then returned the closest event to any mention. Finally, we tried to find the closest timex to the filler text. If there was still nothing, we gave up on the slot."}, {"heading": "4.1.3 Temporal Reasoning", "text": "Given a TimeML annotation and an event, our task is now to find which timexs exist immediately before and after the event. We can detect these\n5LDC catalogue entry LDC2006T08. 6http://www.nltk.org/\nby exploiting the commutative and transitive nature of some types of temporal relation. To ensure that as many relations as possible are created between events and times, we perform pointwise temporal closure over the initial automatic annotation with CAVaT\u2019s consistency tool, ignoring inconsistent configurations. Generating temporal closures is computationally intensive. We reduced the size of the dataset to be processed by generating isolated groups of related events and times with CAVaT\u2019s subgraph modules, and then computed the closure over just these \u201cnodes\u201d.\nWe now have an event representing the slot filler value, and a directed graph of temporal relations connecting it to times and events, which have been decomposed into start and end points. We populate the times as follows:\n\u2022 T1: Latest timex before event start \u2022 T2: Earliest timex after event start \u2022 T3: Latest timex before event termination \u2022 T4: Earliest timex after event termination\nTimex bounds are simply the start and end points of an annotated TIMEX3 interval. We resolve these to calendar references that specify dates in cases where their granularity is greater than one day; for example, using 2006-06-01 and 2006-06-30 for the start and end of a 2006-06 timex. Arbitrary points are used for season bounds, which assume four seasons of three months each, all in the northern hemisphere. If no bound is found in the direction that we are looking, we leave that value blank."}, {"heading": "4.2 Evaluation", "text": "Testing and sample data were available for the temporal tasks7. These include query sets, temporal slot annotations, and a linking file describing which timexes were deemed related to fillers. The distribution of slots in this data is given in Table 9. To test system efficacy we evaluated output performance with the provided entity query sets against these temporal slot annotations. Results are in Table 10, including per-slot performance.\nResults for the full slot-filling task are given in Table 11. This relies on accurate slot values as well as temporal bounding. An analysis of our approach to the diagnostic temporal task, perhaps using a corpus such as TimeBank, remains for future work."}, {"heading": "5 Conclusion", "text": "We set out to build a framework for experimentation with knowledge base population. This framework was created, and applied to multiple KBP tasks. We demonstrated that our proposed framework is effective and suitable for collaborative development efforts, as well as useful in a teaching environment. Finally we present results that, while very modest, provide improvements an order of magnitude greater than our 2010 attempt (Yu et al., 2010)."}], "references": [{"title": "Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit. O\u2019Reilly Media", "author": ["Bird et al.2009] S. Bird", "E. Klein", "E. Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Analysing Temporally Annotated Corpora with CAVaT", "author": ["Derczynski", "Gaizauskas2010] L. Derczynski", "R. Gaizauskas"], "venue": "In Proceedings of the 7th LREC,", "citeRegEx": "Derczynski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Derczynski et al\\.", "year": 2010}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Finkel et al.2005] J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Overview of the TAC2011 Knowledge Base Population Track", "author": ["Ji et al.2011] H. Ji", "R. Grishman", "H.T. Dang", "X.S. Li", "K. Griffit", "J. Ellis"], "venue": "In Proc. Text Analytics Conference", "citeRegEx": "Ji et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2011}, {"title": "Three approaches to learning TLINKS in TimeML", "author": ["Mani et al.2007] I. Mani", "B. Wellner", "M. Verhagen", "J. Pustejovsky"], "venue": "Technical report,", "citeRegEx": "Mani et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mani et al\\.", "year": 2007}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "In Proceedings of the Joint ACL-IJCNLP Conference,", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "TimeML: Robust specification of event and temporal expressions in text", "author": ["J. Castano", "R. Ingria", "R. Saur\u0131", "R. Gaizauskas", "A. Setzer", "G. Katz", "D. Radev"], "venue": "In IWCS-5 Fifth International Workshop on Computational Se-", "citeRegEx": "Pustejovsky et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "Document-level Entity Linking: CMCRC at TAC", "author": ["Radford et al.2010] W. Radford", "B. Hachey", "J. Nothman", "M. Honnibal", "J.R. Curran"], "venue": "In Proc. Text Analytics Conference", "citeRegEx": "Radford et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2010}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W.M. Rand"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rand.,? \\Q1971\\E", "shortCiteRegEx": "Rand.", "year": 1971}, {"title": "Linking Named Entities to a Structured Knowledge Base", "author": ["K. Reddy", "K. Kumar", "S. Krishna", "P. Pingali", "V. Varma"], "venue": "In Proceedings of 11th International Conference on Intelligent Text Processing and Computational Linguistics", "citeRegEx": "Reddy et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2010}, {"title": "Evaluating Passage Retrieval Approaches for Question Answering", "author": ["Roberts", "Gaizauskas2004] I. Roberts", "R. Gaizauskas"], "venue": "Advances in Information Retrieval: Proceedings of the 26th European", "citeRegEx": "Roberts et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roberts et al\\.", "year": 2004}, {"title": "Passage retrieval for question answering", "author": ["A. Sanka"], "venue": null, "citeRegEx": "Sanka.,? \\Q2005\\E", "shortCiteRegEx": "Sanka.", "year": 2005}, {"title": "Evita: a robust event recognizer for QA systems", "author": ["Saur\u0131\u0301 et al.2005] R. Saur\u0131", "R. Knippen", "M. Verhagen", "J. Pustejovsky"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Saur\u0131\u0301 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Saur\u0131\u0301 et al\\.", "year": 2005}, {"title": "HeidelTime: High quality rule-based extraction and normalization of temporal expressions", "author": ["Str\u00f6tgen", "Gertz2010] J. Str\u00f6tgen", "M. Gertz"], "venue": "In Proceedings of SemEval-2010,", "citeRegEx": "Str\u00f6tgen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Str\u00f6tgen et al\\.", "year": 2010}, {"title": "SemEval-2010 task 13: TempEval-2", "author": ["Verhagen et al.2010] M. Verhagen", "R. Sauri", "T. Caselli", "J. Pustejovsky"], "venue": "In Proceedings of SemEval-2010,", "citeRegEx": "Verhagen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Verhagen et al\\.", "year": 2010}, {"title": "The University of Sheffield System at TAC KBP", "author": ["J. Yu", "O. Mujgond", "R. Gaizauskas"], "venue": "In Proc. Text Analytics Conference", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "This paper describes the University of Sheffield\u2019s entry in the 2011 TAC KBP entity linking and slot filling tasks (Ji et al., 2011).", "startOffset": 115, "endOffset": 132}, {"referenceID": 9, "context": "Hyper-links, redirect pages and disambiguation pages are used to associate different named entity mentions with the same entity (Reddy et al., 2010; Varma et al., 2009).", "startOffset": 128, "endOffset": 168}, {"referenceID": 2, "context": "The second strategy uses additional named entity (NE) mentions for query expansion: the Stanford NE recognizer (Finkel et al., 2005) is used to find NE mentions in the query document, and generates a query containing the query entity mention plus all the NE mentions found in the query document,", "startOffset": 111, "endOffset": 132}, {"referenceID": 7, "context": "(2009), Radford et al. (2010)).", "startOffset": 8, "endOffset": 30}, {"referenceID": 8, "context": "The Rand index (Rand, 1971) computes similarity between the system output clusters (output of the clustering algorithm) and the clusters found in a gold standard.", "startOffset": 15, "endOffset": 27}, {"referenceID": 2, "context": "Named Entity Recognition The Stanford Named Entity Recognition (NER) tool (Finkel et al., 2005) was used to find named entities.", "startOffset": 74, "endOffset": 95}, {"referenceID": 0, "context": "Because we needed a broader range of entity classes we re-trained the classifier using the MUC6 and MUC7 datasets 1 and NLTK (Bird et al., 2009) gazetteers.", "startOffset": 125, "endOffset": 144}, {"referenceID": 5, "context": ", Mintz et al. (2009)).", "startOffset": 2, "endOffset": 22}, {"referenceID": 11, "context": "This follows the original strict and lenient measures implemented in the tool we used to assist evaluation, IR4QA (Sanka, 2005).", "startOffset": 114, "endOffset": 127}, {"referenceID": 6, "context": "We approached this task by annotating source documents in TimeML (Pustejovsky et al., 2003), a modern standard for temporal semantic annotation.", "startOffset": 65, "endOffset": 91}, {"referenceID": 12, "context": "The only off-the-shelf TimeML event annotation tool found was Evita (Saur\u0131\u0301 et al., 2005), which requires some preprocessing.", "startOffset": 68, "endOffset": 89}, {"referenceID": 14, "context": "be linked into two sets, as per TempEval (Verhagen et al., 2010): intra-sentence event-time links, and inter-sentence event-event links with a 3-sentence window.", "startOffset": 41, "endOffset": 64}, {"referenceID": 4, "context": "Our feature set was the same used as Mani et al. (2007) which relied on surface data available from any TimeML annotation.", "startOffset": 37, "endOffset": 56}, {"referenceID": 15, "context": "Finally we present results that, while very modest, provide improvements an order of magnitude greater than our 2010 attempt (Yu et al., 2010).", "startOffset": 125, "endOffset": 142}], "year": 2012, "abstractText": "This paper describes the University of Sheffield\u2019s entry in the 2011 TAC KBP entity linking and slot filling tasks (Ji et al., 2011). We chose to participate in the monolingual entity linking task, the monolingual slot filling task and the temporal slot filling tasks. Our team consisted of five MSc students, two PhD students and one more senior academic. For the MSc students, their participation in the track formed the core of their MSc dissertation project, which they began in February 2011 and finished at the end of August 2011. None of them had any prior experience in human language technologies or machine learning before their programme started in October 2010. For the two PhD students participation was relevant to their ongoing PhD research. This team organization allowed us to muster considerable manpower without dedicated external funding and within a limited period time; but of course there were inevitable issues with co-ordination of effort and of getting up to speed. The students found participation to be an excellent and very enjoyable learning experience. Insofar as any common theme emerges from our approaches to the three tasks it is an effort to learn from and exploit data wherever possible: in entity linking we learn thresholds for nil prediction and acquire lists of name variants from data; in slot filling we learn entity recognizers and relation extractors; in temporal slot filling we use time and event annotators that are learned from data. The rest of this paper describes our approach and related investigations in more detail. Sections 2 and 3 describe in detail our approaches to the EL and SF tasks respectively, and Section 4 summarises our temporal slot filling approach.", "creator": "LaTeX with hyperref package"}}}