{"id": "1203.5443", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2012", "title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA", "abstract": "An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often provide nearly multiplicative speedups. This article is coauthored by David Kohn, and Peter Osterberg from CERN, in collaboration with PASP and CERN.\n\n\n\n\nCERN's recent announcement that a \"high-performance computing computing\" (HFC) solution will be announced by J.-D. Dusher, R.-R.-R.-E.-B. Dausher, R.-R.-R.-R.-E.-B. Dausher and D.-R.-R.-R.-R.-E.-B. Dausher, R.-R.-R.-R.-R.-E.-B. Dausher, R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-R.-", "histories": [["v1", "Sat, 24 Mar 2012 20:11:21 GMT  (79kb)", "https://arxiv.org/abs/1203.5443v1", "Submitted to Parallel Problem Solving from Nature (PPSN XII), 10 pages. arXiv admin note: substantial text overlap witharXiv:1201.2241"], ["v2", "Thu, 21 Jun 2012 12:47:30 GMT  (79kb)", "http://arxiv.org/abs/1203.5443v2", "Accepted at Parallel Problem Solving from Nature (PPSN XII), 10 pages. arXiv admin note: substantial text overlap witharXiv:1201.2241"]], "COMMENTS": "Submitted to Parallel Problem Solving from Nature (PPSN XII), 10 pages. arXiv admin note: substantial text overlap witharXiv:1201.2241", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["martin pelikan", "mark w hauschild", "pier luca lanzi"], "accepted": false, "id": "1203.5443"}, "pdf": {"name": "1203.5443.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA", "authors": ["Martin Pelikan", "Mark W. Hauschild"], "emails": ["medal@medal-lab.org", "martin@martinpelikan.net", "mwh308@umsl.edu", "pierluca.lanzi@polimi.it"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 3.\n54 43\nv2 [\ncs .N\nE ]\nAn automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups."}, {"heading": "Keywords", "text": "Transfer learning, inductive transfer, learning from experience, estimation of distribution algorithms, hierarchical Bayesian optimization algorithm, decomposable problems, efficiency enhancement.\nMissouri Estimation of Distribution Algorithms Laboratory (MEDAL) Department of Mathematics and Computer Science, 321 ESH University of Missouri\u2013St. Louis One University Blvd., St. Louis, MO 63121 E-mail: medal@medal-lab.org WWW: http://medal-lab.org/"}, {"heading": "Transfer Learning, Soft Distance-Based Bias, and", "text": "the Hierarchical BOA"}, {"heading": "Martin Pelikan", "text": "Missouri Estimation of Distribution Algorithms Laboratory (MEDAL) Dept. of Mathematics and Computer Science, 320 ESH\nUniversity of Missouri in St. Louis One University Blvd., St. Louis, MO 63121\nmartin@martinpelikan.net\nhttp://martinpelikan.net/"}, {"heading": "Mark W. Hauschild", "text": "Missouri Estimation of Distribution Algorithms Laboratory (MEDAL) Dept. of Mathematics and Computer Science, 321 ESH\nUniversity of Missouri in St. Louis One University Blvd., St. Louis, MO 63121\nmwh308@umsl.edu"}, {"heading": "Pier Luca Lanzi", "text": "Dipartimento di Elettronica e Informazione Politecnico di Milano\nPiazza Leonardo da Vinci, 32 I-20133 Milano, Italy pierluca.lanzi@polimi.it\nMarch 27, 2012"}, {"heading": "Abstract", "text": "An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups.\nKeywords: Transfer learning, inductive transfer, learning from experience, estimation of distribution algorithms, hierarchical Bayesian optimization algorithm, decomposable problems, efficiency enhancement."}, {"heading": "1 Introduction", "text": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions. The use of probabilistic models in EDAs provides a basis for incorporating prior knowledge about the problem and learning from previous runs in order to solve new problem instances of similar type with increased speed, accuracy and reliability [5, 6]. However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11]. Recently, Pelikan and Hauschild [12] proposed an automated technique capable of learning from previous runs of the hierarchical Bayesian optimization algorithm (hBOA) in order to improve efficiency of future hBOA runs on problems of similar type. The basic idea of the approach was to (1) design a distance metric on problem variables that correlates with the expected strength of dependencies between the variables, (2) collect statistics on hBOA models with respect to the values of the distance metric, and (3) use the collected statistics to bias model building in hBOA when solving future problem instances of similar type. While the distance metric is strongly related to the problem being solved, the aforementioned study [12] described a rather general metric that can be applied to practically any problem with the objective function represented by an additively decomposable function. However, the prior study [12] evaluated the proposed technique on only two classes of problems and it did not demonstrate several key features of this technique.\nThe purpose of this paper is threefold: (1) Demonstrate the technique from ref. [12] on other classes of challenging optimization problems, (2) demonstrate the ability of this technique to learn from problem instances of one size in order to introduce bias for instances of another size, and (3) demonstrate the potential benefits of combining this technique with other efficiency enhancement techniques, such as sporadic model building [13]. As test problems the paper considers several classes of NP-complete additively decomposable problems, including MAXSAT, three-dimensional Ising spin glass, and minimum vertex cover. The new results together with the results published in prior work [12] provide strong evidence of the broad applicability and great potential of this technique for learning from experience (transfer learning) in EDAs.\nThe paper is organized as follows. Section 2 outlines hBOA. Section 3 discusses efficiency enhancement of estimation of distribution algorithms using inductive transfer with main focus on hBOA and the distance-based bias [12]. Section 4 presents and discusses experimental results. Section 5 summarizes and concludes the paper."}, {"heading": "2 Hierarchical BOA", "text": "The hierarchical Bayesian optimization algorithm (hBOA) [5, 14] works with a population of candidate solutions represented by fixed-length strings over a finite alphabet. In this paper, candidate solutions are represented by n-bit binary strings. The initial population of binary strings is generated at random according to the uniform distribution over candidate solutions. Each iteration starts by selecting promising solutions from the current population; here binary tournament selection without replacement is used. Next, hBOA (1) learns a Bayesian network with local structures [15] for the selected solutions and (2) generates new candidate solutions by sampling the distribution encoded by the built network. To maintain useful diversity in the population, the new candidate solutions are incorporated into the original population using restricted tournament selection (RTS) [16]. The run is terminated when termination criteria are met. In this paper, each run is terminated either when the global optimum is found or when a maximum number of iterations is reached.\nhBOA represents probabilistic models of candidate solutions by Bayesian networks with local structures [15, 17]. A Bayesian network is defined by two components: (1) an acyclic directed graph over problem variables specifying direct dependencies between variables and (2) conditional probabilities specifying the probability distribution of each variable given the values of the variable\u2019s parents. A Bayesian network encodes a joint probability distribution as p(X1, . . . ,Xn) = \u220fn i=1 p(Xi|\u03a0i) where Xi is the ith variable (string position) and \u03a0i are the parents of Xi in the underlying graph.\nTo represent conditional probabilities of each variable given the variable\u2019s parents, hBOA uses decision trees [14, 15]. Each internal node of a decision tree specifies a variable, and the subtrees of the node correspond to the different values of the variable. Each leaf of the decision tree for a particular variable defines the probability distribution of the variable given a condition specified by the constraints given by the path from the root of the tree to this leaf (constraints are given by the assignments of the variables along this path).\nTo build probabilistic models, hBOA typically uses a greedy algorithm that initializes the decision tree for each problem variable Xi to a single-node tree that encodes the unconditional probability distribution of Xi. In each iteration, the model building algorithm tests how much a model would improve after splitting each leaf of each decision tree on each variable that is not already located on the path to the leaf. The algorithm executes the split that provides the most improvement, and the process is repeated until no more improvement is possible. Models are evaluated using the Bayesian-Dirichlet (BDe) metric with penalty for model complexity, which estimates the goodness of a Bayesian network structure given data D and background knowledge \u03be as p(B|D, \u03be) = cp(B|\u03be)p(D|B, \u03be), where c is a normalization constant [15, 18]. The BayesianDirichlet metric estimates the term p(D|B, \u03be) by combining the observed and prior statistics for relevant combinations of variables [15]. To favor simpler networks to the more complex ones, the prior probability p(B|\u03be) is often set to decrease exponentially fast with respect to the description length of the network\u2019s parameters [5, 17]."}, {"heading": "3 Learning from Experience using Distance-Based Bias", "text": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19]. However, building complex probabilistic models can be time consuming and it may require rather large populations of solutions [2, 3]. That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21]. Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.\nThe basic idea of learning from experience is to gather information about the problem by examining previous runs of the optimization algorithm and to use the obtained information to bias the search on new problem instances. The use of bias based on the results of other learning tasks is also commonplace in machine learning where it is referred to as inductive transfer or transfer learning [22, 23]. Since learning model structure is often the most computationally expensive task in model building, learning from experience often focuses on identifying regularities in model structure and using these regularities to bias structural learning in future runs.\nAnalyzing probabilistic models built by hBOA and other EDAs is straightforward. The more challenging facet of implementing learning from experience in practice is that one must make sure that the collected statistics are meaningful with respect to the problem being solved. The key to make the learning from experience work is to ensure that the pairs of variables are classified into a set of categories so that the pairs in each category have a lot in common and can be expected to be either correlated or independent simultaneously [12]. This section describes one approach to doing\nthat [12], in which pairs of variables are classified into categories based on a predefined distance metric on variables."}, {"heading": "3.1 Distance Metric for Additively Decomposable Functions", "text": "For many optimization problems, the objective function (fitness function) can be expressed as an additively decomposable function (ADF):\nf(X1, . . . ,Xn) =\nm \u2211\ni=1\nfi(Si), (1)\nwhere (X1, . . . ,Xn) are problem\u2019s decision variables (string positions), fi is the ith subfunction, and Si \u2282 {X1,X2, . . . ,Xn} is the subset of variables contributing to fi. While there may often exist multiple ways of decomposing the problem using additive decomposition, one would typically prefer decompositions that minimize the sizes of subsets {Si}. Note that the difficulty of ADFs is not fully determined by the order of subproblems, but also by the definition of the subproblems and their interaction; even with subproblems of order only 2 or 3, the problem can be NP-complete.\nThe definition of a distance between two variables of an ADF used in this paper as well as ref. [12] follows the work of Hauschild et al. [6, 11, 20]. Given an ADF, we define the distance between two variables using a graph G of n nodes, one node per variable. For any two variables Xi and Xj in the same subset Sk, we create an edge in G between the nodes Xi and Xj. Denoting by li,j the number of edges along the shortest path between Xi and Xj in G (in terms of the number of edges), we define the distance between two variables as\nD(Xi,Xj) =\n{\nli,j if a path between Xi and Xj exists, n otherwise.\nThe above distance measure makes variables in the same subproblem close to each other, whereas for the remaining variables, the distances correspond to the length of the chain of subproblems that relate the two variables. The distance is maximal for variables that are completely independent (the value of a variable does not influence the contribution of the other variable in any way).\nSince interactions between problem variables are encoded mainly in the subproblems of the additive problem decomposition, the above distance metric should typically correspond closely to the likelihood of dependencies between problem variables in probabilistic models discovered by EDAs. Specifically, the variables located closer with respect to the metric should more likely interact with each other. This observation has been confirmed with numerous experimental studies across a number of important problem domains from spin glasses distributed on a finite-dimensional lattice [11, 12] to NK landscapes [12]."}, {"heading": "3.2 Distance-Based Bias Based on Previous Runs of hBOA", "text": "This section describes the approach to learning from experience developed by Pelikan and Hauschild [12] inspired mainly by the work of Hauschild et al. [6, 20, 21]. Let us assume a set M of hBOA models from prior hBOA runs on similar problems. Before applying the bias based on prior runs in hBOA, the models in M are first processed to generate data that will serve as the basis for introducing the bias. The processing starts by analyzing the models in M to determine the number s(m,d, j) of splits on any variable Xi such that D(Xi,Xj) = d in a decision tree Tj for variable Xj in a model m \u2208 M . Then, the values s(m,d, j) are used to compute the probability Pk(d, j) of a kth split on a variable at distance d from Xj in a dependency tree Tj given that k\u2212 1\nsuch splits were already performed in Tj :\nPk(d, j) = |{m \u2208 M : s(m,d, j) \u2265 k}|\n|{m \u2208 M : s(m,d, j) \u2265 k \u2212 1}| \u00b7 (2)\nRecall that the BDe metric for evaluating the quality of probabilistic models in hBOA contains two parts: (1) the prior probability p(B|\u03be) of the network structure B, and (2) the posterior probability p(D|B, \u03be) of the data (population of selected solutions) given B. Pelikan and Hauschild [12] proposed to use the prior probability distribution p(B|\u03be) to introduce a bias based on distance-based statistics from previous hBOA runs represented by Pk(d, j) by setting\np(B|\u03be) = c n \u220f\nd=1\nn \u220f\nj=1\nns(d,j) \u220f\nk=1\nP \u03bak (d, j), (3)\nwhere ns(d, j) denotes the number of splits on any variable Xi in Tj such that D(Xi,Xj) = d, \u03ba > 0 is used to tune the strength of bias (the strength of bias increases with \u03ba), and c is a normalization constant. Since log-likelihood is typically used to evaluate model quality, when evaluating the contribution of any particular split, the change of the prior probability of the network structure can still be done in constant time."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Test Problems and Experimental Setup", "text": "The experiments were done for three problem classes known to be difficult for most genetic and evolutionary algorithms: (1) Three-dimensional Ising spin glasses were considered with \u00b1J couplings and periodic boundary conditions [24, 25]; two problem sizes were used, n = 6\u00d7 6\u00d7 6 = 216 spins and n = 7\u00d7 7\u00d7 7 = 343 spins with 1,000 unique problem instances for each n. (2) Minimum vertex cover was considered for random graphs of fixed ratio c of the number of edges and number of nodes [26, 27]; two ratios (c = 2 and c = 4) and two problem sizes (n = 150 and n = 200) were used with 1,000 unique problem instances for each combination of c and n. (3) MAXSAT was considered for mapped instances of graph coloring with graphs created by combining regular ring lattices (with probability 1\u2212 p) and random graphs (with probability p) [28, 29]; 100 unique problem instances of n = 500 bits (propositions) were used for each considered value of p, from p = 2\u22128 (graphs nearly identical to a regular ring lattice) to p = 2\u22121 (graphs with half of the edges random). For more information about the test problems, we refer the reader to refs. [24, 26, 28].\nThe maximum number of iterations for each problem instance was set to the number of bits in the problem; according to preliminary experiments, this upper bound was sufficient. Each run was terminated either when the global optimum was found, when the population consisted of copies of a single candidate solution, or when the maximum number of iterations was reached. For each problem instance, we used bisection [5, 30] to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs. Bit-flip hill climbing (HC) [5] was incorporated into hBOA to improve its performance on all test problems except for the minimum vertex cover; HC was used to improve every solution in the population. For minimum vertex cover, a repair operator based on ref. [26] was incorporated instead. The strength of the distance-based bias was tweaked using \u03ba \u2208 {1, 3, 5, 7, 9}.\nTo ensure that the same problem instances were not used for defining the bias as well as for testing it, 10-fold crossvalidation was used when evaluating the effects of distance-based bias derived from problem instances of the same size. For each set of problems (by a set of problems we mean a\nset of random problem instances generated with one specific set of parameters), problem instances were randomly split into 10 equally sized subsets. In each round of crossvalidation, 1 subset of instances was left out and hBOA was run on the remaining 9 subsets of instances. The runs on the 9 subsets produced models that were analyzed in order to obtain the probabilities Pk(d, j) for all d, j, and k. The bias based on the obtained values of Pk(d, j) was then used in hBOA runs on the remaining subset of instances. The same procedure was repeated for each subset; overall, 10 rounds of crossvalidation were performed for each set of instances. When evaluating the effects of distance-based bias derived from problem instances of smaller size, we did not use crossvalidation because in this case all runs had to be done on different problem instances (of different size). Most importantly, in every experiment, models used to generate statistics for hBOA bias were obtained from hBOA runs on different problem instances. While the experiments were performed across a variety of computer architectures and configurations, the base case with no bias and the case with bias were always both run on the same computational node; the results of the two runs could therefore be compared against each other with respect to the actual CPU (execution) time.\nTo evaluate hBOA performance, we focus on the multiplicative speedup with respect to the execution time per run; the speedup is defined as a multiplicative factor by which the execution time improves with the distance-based bias compared to the base case. For example, an executiontime speedup of 2 indicates that the bias allowed hBOA to find the optimum using only half the execution time compared to the base case without the bias. We also report the percentage of runs for which the execution time was strictly improved (shown in parentheses after the corresponding average multiplicative speedup).\nIn addition to the speedups achieved for various values of \u03ba, we examine the ability of the distance-based bias based on prior runs to apply across a range of problem sizes; this is done by using previous runs on instances of one size to bias runs on instances of another size. Since for MAXSAT, we only used instances of one size, this facet was only examined for the other two problem classes.\nFinally, we examine the combination of the distance-based bias based on prior runs and the sporadic model building [13]. Specifically, we apply sporadic model building on its own using the model-building delay of \u221a n/2 as suggested by ref. [13], and then we carry out a similar experiment using both the distance-based bias as well as the sporadic model building, recording the speedups with respect to the base case. Ideally, we would expect the speedups from the two sources to multiply. Due to the time requirements of solving MAXSAT, the combined effects were studied only for the remaining two problem classes."}, {"heading": "4.2 Results", "text": "The results presented in tables 1, 2 and 3 confirm the observation from ref. [12] that the stronger the bias the greater the benefits, at least for the examined range of \u03ba \u2208 {1, 3, 5, 7, 9} and most problem settings; that is why in the remainder of this discussion we focus on \u03ba = 9. In all cases, the distancebased bias yielded substantial speedups of about 1.2 to 3.1. Best speedups were obtained for the minimum vertex cover. In all cases, performance on at least about 70% problem instances was strictly improved in terms of execution time; in most cases, the improvements were observed in a much greater majority of instances. The speedups were substantial even when the bias was based on prior runs on problem instances of different, smaller size; in fact, the speedups obtained with such a bias were nearly identical to the speedups with the bias based on the instances of the same size. The results thus provide clear empirical evidence that the distance-based bias is applicable even when the problem instances vary in size, which was argued [12] to be one of the main advantages of the distance-based bias over prior work in the area but was not demonstrated. Finally, the results show\nthe nearly multiplicative effect of the distance-based bias and sporadic model building, providing further support for the importance of the distance-based bias; the combined speedups ranged from about 4 to more than 11."}, {"heading": "5 Summary and Conclusions", "text": "This paper extended the prior work on efficiency enhancement of the hierarchical Bayesian optimization algorithm (hBOA) using a distance-based bias derived from prior hBOA runs [12]. The paper demonstrated that (1) the distance-based bias yields substantial speedups on several previously untested classes of challenging, NP-complete problems, (2) the approach is applicable even when prior runs were executed on problem instances of different size, and (3) the approach can yield nearly multiplicative speedups when combined with other efficiency enhancement techniques. In summary, the results presented in this paper together with the prior work [12] provide clear evidence that learning from experience using a distance-based bias has a great potential to improve efficiency of hBOA in particular and estimation of distribution algorithms (EDAs) in general.\nSeveral topics are of central importance for future work. The approach should be adapted to other model-directed optimization techniques, including other EDAs and genetic algorithms with linkage learning. The approach should also be modified to introduce bias on problems that cannot be formulated using an additive decomposition in a straightforward manner or such a decomposition is not practical. Finally, it is important to study the limitations of the proposed approach, and create theoretical models to automatically tune the strength of the bias and predict expected speedups."}, {"heading": "Acknowledgments", "text": "This project was sponsored by the National Science Foundation under grants ECS-0547013 and IIS-1115352, and by the Univ. of Missouri\u2013St. Louis through the High Performance Computing Collaboratory sponsored by Information Technology Services. Most experiments were performed on the Beowulf cluster maintained by ITS at the Univ. of Missouri in St. Louis and the HPC resources at the University of Missouri Bioinformatics Consortium. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."}], "references": [{"title": "An introduction and survey of estimation of distribution algorithms. Swarm and Evolutionary Computation", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation", "author": ["P. Larra\u00f1aga", "J.A. Lozano", "eds"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "A survey of optimization by building and using probabilistic models. Computational Optimization and Applications", "author": ["M. Pelikan", "D.E. Goldberg", "F. Lobo"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Introduction to estimation of distribution algorithms", "author": ["M. Pelikan", "M.W. Hauschild", "F.G. Lobo"], "venue": "MEDAL Report No", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Hierarchical Bayesian optimization algorithm: Toward a new generation of evolutionary algorithms", "author": ["M. Pelikan"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Using previous models to bias structural learning in the hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan", "K. Sastry", "D.E. Goldberg"], "venue": "Evolutionary Computation", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Schemata, distributions and graphical models in evolutionary optimization", "author": ["H. M\u00fchlenbein", "T. Mahnig", "A.O. Rodriguez"], "venue": "Journal of Heuristics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Evolutionary optimization and the estimation of search distributions with applications to graph bipartitioning", "author": ["H. M\u00fchlenbein", "T. Mahnig"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Incorporating a priori knowledge in probabilistic-model based optimization", "author": ["S. Baluja"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A problem-knowledge based evolutionary algorithm KBOA for hypergraph partitioning", "author": ["J. Schwarz", "J. Ocenasek"], "venue": "Proc. of the Fourth Joint Conf. on Knowledge-Based Software Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Analyzing probabilistic models in hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan", "K. Sastry", "C.F. Lima"], "venue": "IEEE Transactions on Evolutionary Computation", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Distance-based bias in model-directed optimization of additively decomposable problems", "author": ["M. Pelikan", "M. Hauschild"], "venue": "MEDAL Report No", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Sporadic model building for efficiency enhancement of the hierarchical BOA", "author": ["M. Pelikan", "K. Sastry", "D.E. Goldberg"], "venue": "Genetic Programming and Evolvable Machines", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Escaping hierarchical traps with competent genetic algorithms", "author": ["M. Pelikan", "D.E. Goldberg"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "A Bayesian approach to learning Bayesian networks with local structure", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": "Technical Report MSR-TR-97-07,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Finding multimodal solutions using restricted tournament selection", "author": ["G.R. Harik"], "venue": "Proc. of the Int. Conf. on Genetic Algorithms", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Learning Bayesian networks with local structure", "author": ["N. Friedman", "M. Goldszmidt"], "venue": "Graphical models. MIT Press", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E.H. Herskovits"], "venue": "Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Model accuracy in the Bayesian optimization algorithm", "author": ["C.F. Lima", "F.G. Lobo", "M. Pelikan", "D.E. Goldberg"], "venue": "Soft Computing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Enhancing efficiency of hierarchical BOA via distance-based model restrictions", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Parallel Problem Solving from Nature (PPSN X)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Intelligent bias of network structures in the hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Direct transfer of learned information among neural networks", "author": ["L.Y. Pratt", "J. Mostow", "C.A. Kamm", "A.A. Kamm"], "venue": "Proceedings of the Ninth National Conference on Artificial Intelligence", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1991}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Searching for ground states of Ising spin glasses with hierarchical BOA and cluster exact approximation", "author": ["M. Pelikan", "A.K. Hartmann"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "ed.: Spin glasses and random fields", "author": ["A. Young"], "venue": "World Scientific,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Hybrid evolutionary algorithms on minimum vertex cover for random graphs", "author": ["M. Pelikan", "R. Kalapala", "A.K. Hartmann"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Minimal vertex covers on finite-connectivity random graphs: A hardsphere lattice-gas picture", "author": ["M. Weigt", "A.K. Hartmann"], "venue": "Physical Review E", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Hierarchical BOA solves Ising spin glasses and maxsat", "author": ["M. Pelikan", "D.E. Goldberg"], "venue": "Genetic and Evol. Comp. Conf. (GECCO-2003) II", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Morphing: Combining structure and randomness", "author": ["I. Gent", "H.H. Hoos", "P. Prosser", "T. Walsh"], "venue": "Proc. of the American Association of Artificial Intelligence", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Evaluation-relaxation schemes for genetic and evolutionary algorithms. Master\u2019s thesis, University of Illinois at Urbana-Champaign", "author": ["K. Sastry"], "venue": "Department of General Engineering,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 1, "context": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 2, "context": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 3, "context": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 4, "context": "The use of probabilistic models in EDAs provides a basis for incorporating prior knowledge about the problem and learning from previous runs in order to solve new problem instances of similar type with increased speed, accuracy and reliability [5, 6].", "startOffset": 244, "endOffset": 250}, {"referenceID": 5, "context": "The use of probabilistic models in EDAs provides a basis for incorporating prior knowledge about the problem and learning from previous runs in order to solve new problem instances of similar type with increased speed, accuracy and reliability [5, 6].", "startOffset": 244, "endOffset": 250}, {"referenceID": 6, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 100, "endOffset": 113}, {"referenceID": 7, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 100, "endOffset": 113}, {"referenceID": 8, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 100, "endOffset": 113}, {"referenceID": 9, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 100, "endOffset": 113}, {"referenceID": 10, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 11, "context": "Recently, Pelikan and Hauschild [12] proposed an automated technique capable of learning from previous runs of the hierarchical Bayesian optimization algorithm (hBOA) in order to improve efficiency of future hBOA runs on problems of similar type.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "While the distance metric is strongly related to the problem being solved, the aforementioned study [12] described a rather general metric that can be applied to practically any problem with the objective function represented by an additively decomposable function.", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "However, the prior study [12] evaluated the proposed technique on only two classes of problems and it did not demonstrate several key features of this technique.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "[12] on other classes of challenging optimization problems, (2) demonstrate the ability of this technique to learn from problem instances of one size in order to introduce bias for instances of another size, and (3) demonstrate the potential benefits of combining this technique with other efficiency enhancement techniques, such as sporadic model building [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] on other classes of challenging optimization problems, (2) demonstrate the ability of this technique to learn from problem instances of one size in order to introduce bias for instances of another size, and (3) demonstrate the potential benefits of combining this technique with other efficiency enhancement techniques, such as sporadic model building [13].", "startOffset": 357, "endOffset": 361}, {"referenceID": 11, "context": "The new results together with the results published in prior work [12] provide strong evidence of the broad applicability and great potential of this technique for learning from experience (transfer learning) in EDAs.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "Section 3 discusses efficiency enhancement of estimation of distribution algorithms using inductive transfer with main focus on hBOA and the distance-based bias [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 4, "context": "The hierarchical Bayesian optimization algorithm (hBOA) [5, 14] works with a population of candidate solutions represented by fixed-length strings over a finite alphabet.", "startOffset": 56, "endOffset": 63}, {"referenceID": 13, "context": "The hierarchical Bayesian optimization algorithm (hBOA) [5, 14] works with a population of candidate solutions represented by fixed-length strings over a finite alphabet.", "startOffset": 56, "endOffset": 63}, {"referenceID": 14, "context": "Next, hBOA (1) learns a Bayesian network with local structures [15] for the selected solutions and (2) generates new candidate solutions by sampling the distribution encoded by the built network.", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "To maintain useful diversity in the population, the new candidate solutions are incorporated into the original population using restricted tournament selection (RTS) [16].", "startOffset": 166, "endOffset": 170}, {"referenceID": 14, "context": "hBOA represents probabilistic models of candidate solutions by Bayesian networks with local structures [15, 17].", "startOffset": 103, "endOffset": 111}, {"referenceID": 16, "context": "hBOA represents probabilistic models of candidate solutions by Bayesian networks with local structures [15, 17].", "startOffset": 103, "endOffset": 111}, {"referenceID": 13, "context": "To represent conditional probabilities of each variable given the variable\u2019s parents, hBOA uses decision trees [14, 15].", "startOffset": 111, "endOffset": 119}, {"referenceID": 14, "context": "To represent conditional probabilities of each variable given the variable\u2019s parents, hBOA uses decision trees [14, 15].", "startOffset": 111, "endOffset": 119}, {"referenceID": 14, "context": "Models are evaluated using the Bayesian-Dirichlet (BDe) metric with penalty for model complexity, which estimates the goodness of a Bayesian network structure given data D and background knowledge \u03be as p(B|D, \u03be) = cp(B|\u03be)p(D|B, \u03be), where c is a normalization constant [15, 18].", "startOffset": 268, "endOffset": 276}, {"referenceID": 17, "context": "Models are evaluated using the Bayesian-Dirichlet (BDe) metric with penalty for model complexity, which estimates the goodness of a Bayesian network structure given data D and background knowledge \u03be as p(B|D, \u03be) = cp(B|\u03be)p(D|B, \u03be), where c is a normalization constant [15, 18].", "startOffset": 268, "endOffset": 276}, {"referenceID": 14, "context": "The BayesianDirichlet metric estimates the term p(D|B, \u03be) by combining the observed and prior statistics for relevant combinations of variables [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "To favor simpler networks to the more complex ones, the prior probability p(B|\u03be) is often set to decrease exponentially fast with respect to the description length of the network\u2019s parameters [5, 17].", "startOffset": 192, "endOffset": 199}, {"referenceID": 16, "context": "To favor simpler networks to the more complex ones, the prior probability p(B|\u03be) is often set to decrease exponentially fast with respect to the description length of the network\u2019s parameters [5, 17].", "startOffset": 192, "endOffset": 199}, {"referenceID": 1, "context": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19].", "startOffset": 129, "endOffset": 143}, {"referenceID": 2, "context": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19].", "startOffset": 129, "endOffset": 143}, {"referenceID": 10, "context": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19].", "startOffset": 129, "endOffset": 143}, {"referenceID": 18, "context": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19].", "startOffset": 129, "endOffset": 143}, {"referenceID": 1, "context": "However, building complex probabilistic models can be time consuming and it may require rather large populations of solutions [2, 3].", "startOffset": 126, "endOffset": 132}, {"referenceID": 2, "context": "However, building complex probabilistic models can be time consuming and it may require rather large populations of solutions [2, 3].", "startOffset": 126, "endOffset": 132}, {"referenceID": 5, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 7, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 8, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 19, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 20, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 4, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 5, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 11, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 19, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 20, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 21, "context": "The use of bias based on the results of other learning tasks is also commonplace in machine learning where it is referred to as inductive transfer or transfer learning [22, 23].", "startOffset": 168, "endOffset": 176}, {"referenceID": 22, "context": "The use of bias based on the results of other learning tasks is also commonplace in machine learning where it is referred to as inductive transfer or transfer learning [22, 23].", "startOffset": 168, "endOffset": 176}, {"referenceID": 11, "context": "The key to make the learning from experience work is to ensure that the pairs of variables are classified into a set of categories so that the pairs in each category have a lot in common and can be expected to be either correlated or independent simultaneously [12].", "startOffset": 261, "endOffset": 265}, {"referenceID": 11, "context": "that [12], in which pairs of variables are classified into categories based on a predefined distance metric on variables.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "[12] follows the work of Hauschild et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6, 11, 20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "[6, 11, 20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "[6, 11, 20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "This observation has been confirmed with numerous experimental studies across a number of important problem domains from spin glasses distributed on a finite-dimensional lattice [11, 12] to NK landscapes [12].", "startOffset": 178, "endOffset": 186}, {"referenceID": 11, "context": "This observation has been confirmed with numerous experimental studies across a number of important problem domains from spin glasses distributed on a finite-dimensional lattice [11, 12] to NK landscapes [12].", "startOffset": 178, "endOffset": 186}, {"referenceID": 11, "context": "This observation has been confirmed with numerous experimental studies across a number of important problem domains from spin glasses distributed on a finite-dimensional lattice [11, 12] to NK landscapes [12].", "startOffset": 204, "endOffset": 208}, {"referenceID": 11, "context": "This section describes the approach to learning from experience developed by Pelikan and Hauschild [12] inspired mainly by the work of Hauschild et al.", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "[6, 20, 21].", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "[6, 20, 21].", "startOffset": 0, "endOffset": 11}, {"referenceID": 20, "context": "[6, 20, 21].", "startOffset": 0, "endOffset": 11}, {"referenceID": 11, "context": "Pelikan and Hauschild [12] proposed to use the prior probability distribution p(B|\u03be) to introduce a bias based on distance-based statistics from previous hBOA runs represented by Pk(d, j) by setting", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "The experiments were done for three problem classes known to be difficult for most genetic and evolutionary algorithms: (1) Three-dimensional Ising spin glasses were considered with \u00b1J couplings and periodic boundary conditions [24, 25]; two problem sizes were used, n = 6\u00d7 6\u00d7 6 = 216 spins and n = 7\u00d7 7\u00d7 7 = 343 spins with 1,000 unique problem instances for each n.", "startOffset": 228, "endOffset": 236}, {"referenceID": 24, "context": "The experiments were done for three problem classes known to be difficult for most genetic and evolutionary algorithms: (1) Three-dimensional Ising spin glasses were considered with \u00b1J couplings and periodic boundary conditions [24, 25]; two problem sizes were used, n = 6\u00d7 6\u00d7 6 = 216 spins and n = 7\u00d7 7\u00d7 7 = 343 spins with 1,000 unique problem instances for each n.", "startOffset": 228, "endOffset": 236}, {"referenceID": 25, "context": "(2) Minimum vertex cover was considered for random graphs of fixed ratio c of the number of edges and number of nodes [26, 27]; two ratios (c = 2 and c = 4) and two problem sizes (n = 150 and n = 200) were used with 1,000 unique problem instances for each combination of c and n.", "startOffset": 118, "endOffset": 126}, {"referenceID": 26, "context": "(2) Minimum vertex cover was considered for random graphs of fixed ratio c of the number of edges and number of nodes [26, 27]; two ratios (c = 2 and c = 4) and two problem sizes (n = 150 and n = 200) were used with 1,000 unique problem instances for each combination of c and n.", "startOffset": 118, "endOffset": 126}, {"referenceID": 27, "context": "(3) MAXSAT was considered for mapped instances of graph coloring with graphs created by combining regular ring lattices (with probability 1\u2212 p) and random graphs (with probability p) [28, 29]; 100 unique problem instances of n = 500 bits (propositions) were used for each considered value of p, from p = 2\u22128 (graphs nearly identical to a regular ring lattice) to p = 2\u22121 (graphs with half of the edges random).", "startOffset": 183, "endOffset": 191}, {"referenceID": 28, "context": "(3) MAXSAT was considered for mapped instances of graph coloring with graphs created by combining regular ring lattices (with probability 1\u2212 p) and random graphs (with probability p) [28, 29]; 100 unique problem instances of n = 500 bits (propositions) were used for each considered value of p, from p = 2\u22128 (graphs nearly identical to a regular ring lattice) to p = 2\u22121 (graphs with half of the edges random).", "startOffset": 183, "endOffset": 191}, {"referenceID": 23, "context": "[24, 26, 28].", "startOffset": 0, "endOffset": 12}, {"referenceID": 25, "context": "[24, 26, 28].", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "[24, 26, 28].", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "For each problem instance, we used bisection [5, 30] to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.", "startOffset": 45, "endOffset": 52}, {"referenceID": 29, "context": "For each problem instance, we used bisection [5, 30] to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.", "startOffset": 45, "endOffset": 52}, {"referenceID": 4, "context": "Bit-flip hill climbing (HC) [5] was incorporated into hBOA to improve its performance on all test problems except for the minimum vertex cover; HC was used to improve every solution in the population.", "startOffset": 28, "endOffset": 31}, {"referenceID": 25, "context": "[26] was incorporated instead.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Finally, we examine the combination of the distance-based bias based on prior runs and the sporadic model building [13].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "[13], and then we carry out a similar experiment using both the distance-based bias as well as the sporadic model building, recording the speedups with respect to the base case.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] that the stronger the bias the greater the benefits, at least for the examined range of \u03ba \u2208 {1, 3, 5, 7, 9} and most problem settings; that is why in the remainder of this discussion we focus on \u03ba = 9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The results thus provide clear empirical evidence that the distance-based bias is applicable even when the problem instances vary in size, which was argued [12] to be one of the main advantages of the distance-based bias over prior work in the area but was not demonstrated.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "This paper extended the prior work on efficiency enhancement of the hierarchical Bayesian optimization algorithm (hBOA) using a distance-based bias derived from prior hBOA runs [12].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "In summary, the results presented in this paper together with the prior work [12] provide clear evidence that learning from experience using a distance-based bias has a great potential to improve efficiency of hBOA in particular and estimation of distribution algorithms (EDAs) in general.", "startOffset": 77, "endOffset": 81}], "year": 2013, "abstractText": "An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups.", "creator": "LaTeX with hyperref package"}}}