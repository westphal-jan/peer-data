{"id": "1203.6136", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Mar-2012", "title": "Tree Transducers, Machine Translation, and Cross-Language Divergences", "abstract": "Tree transducers are formal automata that transform trees into other trees. Many varieties of tree transducers have been explored in the automata theory literature, and more recently, in the machine translation literature. In this paper I review T and xT transducers, situate them among related formalisms, and show how they can be used to implement rules for machine translation systems that cover all of the cross-language structural divergences described in Bonnie Dorr's influential article on the topic. I also present an implementation of xT transduction, suitable and convenient for experimenting with translation rules.\n\n\nThe following examples describe a transduction system in a manner that can easily be understood, and describe how it can be applied to transform trees. In this paper I highlight the features of xT transduction in relation to the interconnection of natural selection and environmental determinants. This paper introduces the concept of the transduction model (e.g., for example, in which t is a natural selection constraint, the xT transduction model provides an initial set of interlocutors that control the number of interlocutors in all the species and the interactions between those constraints). In the paper the transduction model provides a way to visualize the interconnection of natural selection and environmental determinants through the expression of the transduction model in the natural selection constraint model. The expression of the transduction model in the natural selection constraint model is as follows: The relation of natural selection and environmental determinants is, in this case, the xT transduction model, defined by the constraint: The relationship of natural selection and environmental determinants is, in this case, the xT transduction model. This is a simplified example, as in this example, in which t is a natural selection constraint, the xT transduction model can be represented by the following expression: The relationship of natural selection and environmental determinants is, in this case, the xT transduction model. This expression is a simplified example, as in this example, in which t is a natural selection constraint, the xT transduction model can be represented by the expression: The relationship of natural selection and environmental determinants is, in this case, the xT transduction model. This expression is a simplified example, as in this example, in which t is a natural selection constraint, the xT transduction model can be represented by the expression: The relationship of natural selection and environmental determinants is, in this case, the xT transduction model. This expression is a", "histories": [["v1", "Wed, 28 Mar 2012 02:13:39 GMT  (28kb)", "http://arxiv.org/abs/1203.6136v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alex rudnick"], "accepted": false, "id": "1203.6136"}, "pdf": {"name": "1203.6136.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["alexr@cs.indiana.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 3.\n61 36\nv1 [\ncs .C\nL ]\n2 8\nM ar\n2 01"}, {"heading": "1 Introduction", "text": "Word-based approaches to statistical machine translation, starting with the work from IBM in the early 1990s (Brown et al., 1993) have been successful both in use in production translation systems and in invigorating MT research. Since then, newer phrase-based MT techniques such as the alignment template model (Och and Ney, 2004), and hierarchical phrase-based models (Chiang, 2005) have made significant improvements in SMT translation quality.\nDespite their sophistication and apparent complexity, many word-based and phrase-based SMT models can be implemented entirely in terms of finite-state transducers. This allows researchers to make use of the rich automata literature for finding clean and efficient algorithms; it is also useful from a software engineering perspective, making it possible to do experiments quickly, using generic toolkits for programmatically manipulating finite-state transducers. Several such packages are freely available, such as OpenFST (Allauzen\net al., 2007) and the WRTH FSA Toolkit (Kanthak and Ney, 2004).\nHowever, since they make no attempt to explicitly model the syntax of either involved language, and typically use simple n-gram models to guide generation, the output of word-based SMT systems can be syntactically incoherent, especially in light of long-distance dependencies. Additionally, word-based SMT models have difficulties encoding word order differences across languages.\nSo we have seen new methods in MT that explicitly model syntax, where typically the grammar of a language, and the relationships between the grammars of two languages, can be learned from treebanks. There are many different available theoretical frameworks for describing syntax and transformations over syntactic representations. Both from a theoretical standpoint, and as MT implementors, we would like a framework that is clean and general, and is suitably expressive for explicitly capturing syntactic structures and the divergences across languages. We would also like one for which there are efficient algorithms for training rules and performing transduction (i.e., decoding at translation time), and ideally one for which a good software toolkit is freely available.\nNot all syntactic relationships can be cleanly represented with every syntactic formalism; each formalism has its own expressive power. Bonnie Dorr provides us with an excellent test bed of seven cross-language divergences that may occur when we want to perform translation, even between languages as closely related as English, Spanish and German (Dorr, 1994). While these divergences do not totally describe the ways in which languages can differ in their typical descriptions of an event, they provide a concrete starting point, and are easily accessible.\nIn this paper, I specifically investigate T and xT transducers, situate them in the space of formalisms for describing syntax-based translation,\nand demonstrate that xT transducers are sufficient for modeling all of the syntactic divergences identified by Dorr. I also present kurt, a small software toolkit for experimenting with these transducers, which comes with sample translation rules that handle each of Dorr\u2019s divergences.\nIn the rest of this paper, we will discuss some relevant grammar and transducer formalisms, including a more in-depth look at T and xT transducers; go through the linguistic divergences discussed by Dorr and explain why they might cause difficulties for MT systems; show how xT transducers can be used to address each of these divergences; present the software that I have built; review some of the related work that has informed this paper; and finally, suggest future possible directions for work with tree transducer-based MT."}, {"heading": "2 Grammars and Transducers", "text": "Here we contrast several kinds of formalisms over strings, trees, and pairs of strings and trees; please see Figure 2 for a glossary of different kinds of automata and grammars that will be referenced in this paper. A grammar describes a single set of strings or trees, and consists of a finite set of rules that describes those strings or trees. Familiar formalisms for grammars that describe sets of strings include context-free grammars and the other members of the Chomsky Hierarchy. Some grammars describe sets of trees, and these will be the main focus of the rest of this paper; when discussing grammars over strings, I will specifically mention it. For example, regular tree grammars (RTG) is the class of grammars corresponding to contextfree grammars but describing trees; they describe the trees whose yield (string concatenation of the symbols at the leaves) is a context-free grammar (Knight and Graehl, 2005).\nContrastingly, synchronous grammars describe sets of pairs of objects; here again, we are mostly concerned with synchronous grammars that describe trees. Formally, a synchronous grammar over trees establishes a mathematical relation over two sets of trees, and allows us to answer the question of whether, for a given pair of trees, that pair is in the relation. The production rules of a synchronous grammar do not just describe one language, but have pairs of production rules < r1, r2 >, such that when r1 is used to derive a string in language L1, r2 must be used in the derivation of a string in L2.\nThus synchronous grammars can be used for several kinds of tasks, such as parsing parallel texts, generating parallel text, or most intuitively useful for a machine translation setting, parsing text in one language while jointly generating parse trees that yield text the other. All of these operations are described for synchronous contextfree grammars in David Chiang\u2019s tutorial (Chiang, 2006). In his tutorial, Chiang describes some of the limitations of using synchronous CFGs; notably, they cannot rearrange parts of parse trees that are not sisters. Of particular interest in this work is raising and lowering elements; Chiang gives the example of swapping subjects and objects, as in the example of translation between English and French in Figure 1. Chiang points out that, for syntax-aware MT, we would like to be able to use some more powerful formalism that can perform transformations like this. Synchronous tree substitution grammars, for example, are able to describe transformations of this form, but not the transformation from cross-serial dependencies in subordinate clauses in Dutch to the nested clause structure of English. This latter transformation would require more formal power, which is offered by tree-adjoining grammars."}, {"heading": "2.1 TAG and Related Formalisms", "text": "Tree adjoining grammar, introduced by Joshi (Joshi et al., 1975), has been a popular formalism for describing grammars over trees. It provides additional expressive power not available in regular tree grammars, handling some, but not all contextsensitive languages. TAG can cleanly describe many of the non-context-free features observed in human languages, such as the cross-serial dependencies in Dutch. TAG is thus called \u201cweakly context-sensitive\u201d, and has been shown formally equivalent to several other syntactic formalisms, such as Combinatory Categorial Grammar (CCG) and Linear Indexed Grammars (Vijay-Shanker and Weir, 1994).\nThe operations of TAG are substitution and adjunction, which combine the two different kinds of elementary trees present in a given TAG grammar, initial trees and auxiliary trees. The substitution operation takes two trees, one with a leaf that is an unresolved nonterminal \u03b1, and produces a new tree in which that node has been replaced with an entire subtree (copied from another initial tree in the grammar, or one that has already been\nderived) whose root node is also \u03b1. For example, an initial tree may have an unresolved nonterminal that wants to have an NP attached to it (it has a leaf labelled NP); the substitution operation attaches an existing subtree whose root is NP, producing a new tree where that nonterminal is now resolved. The adjunction operation takes an existing tree and an auxiliary tree, which has a special node marked as the \u201cfoot\u201d, and grafts the auxiliary tree in place in the middle of the existing tree, attaching the tree material at the target location to the foot node of the auxiliary tree. For a very clear tutorial on TAG with good examples, please see (van Noord, 1993), Section 4.2.4. Synchronous TAG has also been investigated, and its use in machine translation has been advocated by Shieber, who argues that its expressive power may make up for its computational complexity (Shieber, 2007).\nRestricted versions of TAG and their synchronous analogues have also been investigated. These do not provide the full expressive power of TAG, but can be parsed and trained more efficiently. The two limited versions of TAG that are most prominently discussed in the literature are tree substitution grammars (TSG) and tree insertion grammars (TIG). TSG only provides the substitution operation, and does not have auxiliary trees or adjunction (Eisner, 2003). TIG, on the other hand, includes both the substitution and adjunction operations, but places constraints on the permissible shapes of auxiliary trees: their foot nodes must be at the leftmost or rightmost edge of the frontier, and a given derivation may not adjoin \u201cleft\u201d auxiliary trees into \u201cright\u201d ones, or viceversa. These restrictions are sufficient to limit the weak generative capacity of TIGs to that of CFGs, but they also ensure that algorithms on TIGs can run more efficiently. While parsing with a TAG takes in the general case O(n6) complexity, TIG\n(like the general case for CFGs) can be parsed in O(n3) (Nesson et al., 2006). Both STIG and STSG have seen use in machine translation; for example, probabilistic STIG is used in (Nesson et al., 2006), and STSG has been notably used in (Eisner, 2003)."}, {"heading": "2.2 Tree Transducers", "text": "While synchronous grammars provide a declarative description of a relation that holds between two sets of trees, tree transducers more explicitly describe the process by which a tree may be converted into other trees. Like finite-state transducers, which operate over strings, tree transducers typically describe nondeterministic processes, so for a given input tree, there is a set of possible output trees; that set may (for example) be described by a regular tree grammar.\nTree transducers and synchronous grammars both describe mathematical relations over trees, so we can sensibly ask about their comparative formal expressive power, and use them to compute similar queries. For example, with either a synchronous grammar or a transducer, we may ask, for a given tree, what are the other trees that are in the mathematical relation with it (Chiang, 2006). There are transducer varieties with the same formal expressive power as certain synchronous grammars. For example, synchronous tree substitution grammars (STSG) have the same formal power as xLNT transducers (Maletti, 2010b), which will be described in more detail in the next section.\nWhile there are very many kinds of possible tree transducers, the ones used in NLP applications typically fall into one of two classes, T transducers, which operate \u201ctop-down\u201d, and \u201cB\u201d transducers, which operate \u201cbottom-up\u201d."}, {"heading": "3 T Transducers", "text": "Let us now describe T transducers in more detail. T transducers transform trees into other trees via sets of production rules. Many production rules may apply at a given step in a derivation, so the transductions are usually nondeterministic, relating a given input tree to many possible output trees. Thus a T transducer, like a synchronous tree grammar, defines a relation over sets of trees.\nIntuitively, transduction begins with an input tree, where its root node is in the initial state q0. Each node in a tree may be in one of the states in Q (the set of possible states), or in no state at all. Transduction proceeds by finding all the transduction rules that can apply to an existing tree, or subtrees of an existing tree. A rule applies when the root of its left-hand side matches a node in the tree, and the state of the node matches the state of the rule. When a rule matches a subtree (call it t), then a new tree is produced and added to the set of current trees by replacing the subtree that matched the rule with the right-hand side of the rule, save that the variables in the right-hand side of the rule have been replaced by the corresponding subtrees of t. Additionally, a rule may specify that subtrees of the new tree being produced should be in states as well, indicating that more transduction work must be done on them before the derivation is finished. A complete, successful transduction in a T transducer begins with the root node being in the initial state, then states propagating down the tree to the leaves, until the entire tree has been transduced. See Figure 3 for an illustrative example, adapted from (Graehl et al., 2008).\nA new tree is produced by replacing the subtree that matched the rule with the right-hand side of the rule, with its variables filled in with the appropriate subtrees. The new tree is then added to the inventory of current trees in the usual way for production systems. A transduction is complete for a tree in the inventory when all of its nodes are no longer in states; at this point, the states will have propagated all the way from the top of the tree to the leaves, and then be resolved; in the case of translation, the symbols in the tree will be words in the output language. The transduction process is nondeterministic; many rules may apply to a given tree in the inventory, and even the same rule may apply to different subtrees. To do a complete search for all possible transductions, we apply each rule to every subtree where it is ap-\nplicable, and produce every possible resulting tree; beam search may also be done, where search paths with low probabilities are pruned.\nFormally, a T transducer has the following elements.\n\u2022 an input alphabet \u03a3\n\u2022 an output alphabet \u2206\n\u2022 a set of states Q\n\u2022 an initial state, typically denoted q0\n\u2022 transition rules, which are tuples of the form (q \u2282 Q,\u03c3 \u2282 \u03a3, tpat, p)\nThe transition rule tuples specify the state that a given node must be in, and the symbol from the input language that the subtree must have, (state q and symbol \u03c3, respectively), in order for this rule to match. They also specify a tree pattern that forms the right-hand side of the rule, and a weight p for this rule. The tree pattern is a tree where some of the elements in the tree may be variables, which refer to subtrees of the left-hand side under consideration."}, {"heading": "3.1 xT Transducers", "text": "The \u201cextended\u201d variation of T transducers, indicated with an \u201cx\u201d prefix, adds the capability for rules to check whether a potentially matching subtree matches a certain pattern of finite size, in addition to the given state and value of the node. The tree pattern in the left-hand side of an xT transduction rule may contain literal symbols as well as variables, which allows for lexicalized rules that only apply when certain words are in a subtree. The tree patterns also make it possible for the rules to reference material finitely far into a subtree, which makes local rotations straightforward; see Figure 4 for example xT rules that perform a local rotation and also use finite lookahead to produce Francophone names. In the notation common in the literature, a state for a node is written next to that node in the tree structure.\nWhile T transducers are not as expressive as synchronous TSG (Shieber, 2004), xT transducers are as expressive, and can even be used to simulate synchronous TAG in some cases (Maletti, 2010a). In addition to their formal expressive power, xT transducers are much more convenient for rule authors; some finite lookahead can be simulated with the standard T transducers, as shown in (Graehl\net al., 2008), but it is somewhat tedious. The use of xT transducers makes writing rules to rearrange material in a tree much more convenient."}, {"heading": "3.2 Restricted Versions of T and xT Transducers", "text": "For computational efficiency purposes, we may also consider placing certain restrictions on the rules in a T or xT transducer. Options that have been explored include requiring that a transducer be linear, which means that any variable occurring in the left-hand side of a rule should appear no more than once in the right-hand side, and nondeleting, which means that a variable in the lefthand side must appear at least once in the righthand side. Linear transducers are given the prefix \u201cL\u201d, and nondeleting transducers the prefix \u201cN\u201d, so for example, extended linear non-deleting topdown transducers are described as \u201cxLNT\u201d. This particular combination of options has been used several times in the literature, including (Galley et al., 2004). Also note that the transducer in Figure 3 is not nondeleting, since Rule 2 does not reference its variables in its right-hand side.\nAmong the benefits of adding these constraints on rules are that, LT and LNT transducers are compositional, meaning that a relation that can be expressed by a cascade of two LT transducers can also be expressed by a single LT transducer, and that the composition of those two transducers can be computed. However this is not possible\nwith any other members of the T-transducer family; even xLNT transducers are non-compositional (Knight, 2007)."}, {"heading": "4 Linguistic Divergences", "text": "Bonnie Dorr, in (Dorr, 1994), enumerates several different kinds of structural divergences that we might see in translation between languages. These divergences occur when translating from English to closely related languages, Spanish and German, all of which have fairly similar word orders. These are not the only kinds of syntactic differences that there can be in a translation. They do not, for example, cover the more large-scale reorderings that we see when translating between SVO and SOV or VSO languages. However, each of these divergences require something more than simple word substitution or reordering the children of a given node: many of these require raising and lowering tree material (performing \u201crotations\u201d, in the terminology of (Shieber, 2004)), and nested phrases that are present in one language are often not present in the other. Many of these divergences may appear in a given pair of translated sentences. The following subsections describe Dorr\u2019s seven kinds of divergence."}, {"heading": "4.1 Thematic Divergence", "text": "Different languages may express a situation by assigning different thematic roles to the participants of an action, swapping (for example) the subject\nand object. For example, translating from English to Spanish, we see:\n\u2022 I like Mary\n\u2022 Mar\u0131\u0301a me gusta a m\u0131\u0301\nIn Spanish it is more common to say that \u201cX pleases Y\u201d than that \u201dY wants/likes X\u201d. The Spanish verb querer has the same structure as the English \u201clike\u201d, but the meaning of \u201cgustar\u201d is closer to the English \u201cto like\u201d."}, {"heading": "4.2 Promotional Divergence", "text": "A modifier in one language may be the head in another language.\n\u2022 John usually goes home\n\u2022 Juan suele ir a casa\nHere in English, an adverb modifies the verb to indicate that it is habitual, but in Spanish we use the verb \u201csoler\u201d (which inflects as \u201csuele\u201d for third-person singular), to express this. It has an infinitive as a dependent."}, {"heading": "4.3 Demotional Divergence", "text": "The demotional divergence is similar to a demotional divergence, viewed in the other direction; in cases of demotional divergence, a head in one language is a modifier in the other. In (Dorr, 1994), a formal distinction is made between the two because in Dorr\u2019s MT system, they would be triggered in different circumstances, but for our purposes they are effectively analogous.\n\u2022 I like eating\n\u2022 Ich esse gern\nIn this example, while English uses the verb \u201cto like\u201d, German has an adverb. The sentence has a literal translation of \u201cI eat likingly\u201d."}, {"heading": "4.4 Categorial Divergence", "text": "In cases of categorial divergence, the meaning of a word with a certain part of speech in one language is expressed with a different part of speech in the other.\n\u2022 I am hungry\n\u2022 Ich habe Hunger.\nThe German sentence here translates literally as \u201cI have hunger.\u201d"}, {"heading": "4.5 Structural Divergence", "text": "In cases of structural divergence, there are phrases in one language not present in the other.\n\u2022 John entered the house\n\u2022 Juan entro\u0301 en la casa\nWhile the English sentence has the destination of the motion verb as an object, in the Spanish we see the prepositional phrase \u201cen la casa\u201d (\u201cin the house\u201d)."}, {"heading": "4.6 Lexical Divergence", "text": "In cases of lexical divergence, the two languages involved have different idiomatic phrases for describing a situation.\n\u2022 John broke into the room\n\u2022 Juan forzo\u0301 la entrada al cuarto\nWhile \u201cbreak into\u201d is a phrasal verb in English, in Spanish it is more idiomatic to \u201cforce entry to\u201d. This example also includes a structural divergence, as \u201cal cuarto\u201d is a prepositional phrase not present in the English."}, {"heading": "4.7 Conflational Divergence", "text": "The meaning of the sentence may be distributed to different words in a different language; the meaning of a verb, for example, may be carried by a verb and its object after translation.\n\u2022 I stabbed John\n\u2022 Yo le di pun\u0303aladas a Juan\nHere the Spanish sentence means literally \u201cI gave John knife wounds\u201d. The words \u201cle\u201d and \u201ca\u201d are both required, but for different reasons: the verb \u201cdar\u201d (to give) requires the personal pronoun beforehand, and whenever a human being is the object of a verb in Spanish, we add the \u201cpersonal a\u201d beforehand."}, {"heading": "5 Implementation", "text": "In the course of this project, I have produced a small, easily-understandable toolkit named kurt (the Keen Utility for Rewriting Trees), for experimenting with weighted xT tree transducers. It is implemented in Python 3 and makes use of the NLTK tree libraries (Bird et al., 2009). kurt has\nbeen released as free software, and is available online 1.\nThe software can perform tree transduction in general for weighted xT transducers: given a tree, it applies xT transduction rules and produces a list of output trees. The implementation is fairly na\u0131\u0308ve, and proceeds as a simple production system. Partial solutions are matched against every rule in the transducer, then each matching rule is applied to the partial solution, producing a new generation of partial solutions. Eventually, the derivation either succeeds by producing at least one tree with no nodes in a state, or it fails if the input tree cannot be completely transduced by the given rules. The system returns all possible output trees, and the complete solutions are printed out at the conclusion of the program.\nThe xT rules are straightforward to write, and are stored in YAML files. I have also provided example xT rules that translate the examples of divergences given by Dorr; these are described in more detail in Section 6.\nA complete and useful MT system based on this software \u2013 such that the rules and their weights were not completely the product of human knowledge engineering \u2013 would require the implementation of a few more algorithms described in (Graehl et al., 2008), particularly their EM training algorithm to calculate weights for a given set of transduction rules, which depends on their transduction algorithm that produces the more compact representation of a transduction, a RTG. Decoding would require beam search over tree transduction, or perhaps over generation using this compact RTG representation. Additionally, some clever algorithm for extracting tree transducer rules from parallel treebanks would be useful for the case where parallel treebanks are available; some candidate techniques for this last problem are discussed in Section 8.2."}, {"heading": "5.1 Using the Software", "text": "Transducers are stored in YAML files, with one xT transducer per file; each rule is specified as an entry in that YAML file, and contains the following entries.\n\u2022 state: (required) The name of the state that a node at the root of a subtree must be in to match this rule\n1http://github.com/alexrudnick/kurt\n\u2022 lhs: (required) The left-hand side of the rule: a tree pattern, typically with variables (tokens starting with ?) that must unify with a subtree in order for that subtree to match this rule\n\u2022 rhs: (required) The right-hand side of the rule: another tree pattern, which is filled in when this rule is applied. It may contain variables, in which case all of the variables must also be present in the left-hand side of the rule.\n\u2022 newstates: (optional) Specifies the locations of transduction states in the subtree produced by this rule. There may be many states specified in the new subtree. They are given in the form [location, statename], where location is a bracketed list that describes the path down the tree from the root of the subtree, with 0-indexed children. For example, to put the second child of the leftmost child of the root in state foo, a rule would have a newstates member [[0,1], foo].\n\u2022 weight: (optional) The weight for this rule. If unspecified, it defaults to 1.0.\nGiven a file with these entries for each rule of a transducer, say called translation.yaml, a Python 3 program can use kurt to do tree transductions in the following way, assuming the libraries are all in the $PYTHONPATHor the current working directory.\nfrom loadrules import loadrules from translate import translate\nrules = loadrules(\"translation.yaml\") tr = Tree(\"\"\"(S (NP (PRP I))\n(VP (VB am) (JJ hungry)))\"\"\")\n## print all valid transductions translate(tr, rules)"}, {"heading": "5.2 Simple Topicalization Example", "text": "In Figure 5, we see a toy example of xT rules realized with the system. This is a complete running example that exercises many features of the software; it translates an English sentence into \u201cLOLcat\u201d Internet slang, which features more promi-\nnent topicalization 2. For simplicity, the syntactic structure of the parse tree is elided. The initial rule matches a sentence in the initial state q, containing \u201clet me show you my x0\u201d and produces a new sentence where \u201cmy x0\u201d has been moved to the front . The rule also specifies that the (0-indexed) child of the S node at index 1 is in the state respell. The second rule matches the word \u201cPoke\u0301mon\u201d when it is in the state respell, replacing it with the slang spelling of \u201cPokemans\u201d. The third rule is for generalization, allowing words other than \u201cPoke\u0301mon\u201d to be translated in this position. Due to both the second and third rules applying to the subtree, both spellings are produced in the output, but the translation with the slang spelling is given a higher weight."}, {"heading": "6 xT Transducers for Linguistic Divergences", "text": "I wrote xT transduction rules for the software toolkit that handle each of Dorr\u2019s divergence examples. Most of the work involved was constructing parse trees for the source- and targetlanguage sentences; I then converted the trees into templates for the desired trees, at which point they were effectively xT transduction rules. Some examples are included in Figures 6 and 7, but the complete set of rules are in german.yaml and spanish.yaml, included with the software. Most of the transformations required to implement these rules are instances of local rotations, as described by (Shieber, 2004)."}, {"heading": "7 Related Work", "text": "In addition to the work on tree-based MT, some very sophisticated string-based MT algorithms have been framed in terms of finite-state transducers. Not long after the introduction of modern word-based SMT, Knight and Al-Onaizan showed that IBM Model 3 could be expressed with a cascade of FSTs (Knight and Al-Onaizan, 1998). Since string transducers can be composed, decoding in this case becomes one enormous beam search over a single state machine. Similarly, Shankar Kumar and William Byrne expressed the phrase-based alignment template model as FSTs (Kumar and Byrne, 2003). The last part of the decoding process in Chiang\u2019s hierarchical phrasebased model can also be described in terms of\n2Readers may or may not be familiar with the moderately popular catchphrase \u201cMy Pokemans, let me show you them\u201d.\nFSTs (Iglesias et al., 2009); Iglesias et al. use finite-state techniques to traverse a lattice of possible translations once chart parsing with an SCFG has completed.\nFor tutorials and related algorithms, Chiang provides an excellent introduction to synchronous grammars in (Chiang, 2006). My understanding of TAG was greatly aided by the TAG section in (van Noord, 1993); it is referenced in the TAG Wikipedia page. For overviews of different applications of T-family tree transducers and their various properties, in a very approachable style, (Knight, 2007) and (Knight and Graehl, 2005) are very helpful. Additionally (Graehl et al., 2008) contains excellent examples for understanding xT transduction (one of which is in this paper in simplified form, though the original example is worth working through and understanding fully), along with a set of algorithms that can be computed over xT transducers, including an EM procedure that can be used to estimate the weights for an xT grammar given a parallel treebank."}, {"heading": "8 Conclusions and Future Work", "text": "Here I have described the \u201cT\u201d family of tree transducers and situated them among the various formalisms for describing relations over strings and trees; I have also demonstrated that xT transducers are sufficient for handling translation across the linguistic divergences described by Dorr. I have presented a software package suitable for experimentation with xT transducers, which comes with example translation rules that perform translations over each of the divergences.\nThere remains significant work to be done on the topic; for example, to my knowledge, there is no easily available end-to-end MT system based on tree transducers, either commercial or Open Source. There are many more questions that I would like to answer; as far as I know, these are open problems in the field."}, {"heading": "8.1 Transducers, Disambiguation, and Language Models", "text": "While weighted synchronous grammars and xT transducers provide generative models of translation, the probabilities that they assign to a given rule are set ahead of time, and are not conditioned on features of the surrounding context. It may be fruitful to try using discriminative approaches (i.e., classifiers) to help a transducer-based MT\nsystem make decisions about which rules are the most likely to apply in a given context, either based on the surrounding tree material, or on the surface words in the source-language sentence. It may turn out that there is a more principled way to achieve the same benefits, perhaps by adding more conditions on the probabilities in a generative model. However, cross-language phrase-sense disambiguation with classifiers, like in the work of Carpuat and Wu (Carpuat and Wu, 2007), has proved useful for phrase-based SMT. For phrasebased SMT in general, discriminative approaches such as Minimum Error-Rate Training (MERT) (Och, 2003) have become quite typical.\nAnother guide for the tree transduction process could be language models, either flat n-gram models or structured ones, which would have the added benefit that they could be trained on larger corpora than those used to produce the tree transduction rules in the first place."}, {"heading": "8.2 Extraction and Training Transducers", "text": "Thus far, it seems as though there is no agreedupon best approach for extracting a set of tree transduction rules from a parallel treebank, such that a tree-to-tree MT system could be constructed. While parallel treebanks are not abundant, with sufficiently good monolingual parsers, parallel trees can be created from bitext, and hopefully these could be used to induce transduction rules for tree-to-tree MT systems. Other work has presented methods for learning tree-to-string transduction rules, for example (Galley et al., 2004) and (DeNeefe and Knight, 2009). These approaches for learning tree-to-string transducers, if I understood them more completely, might turn out to generalize easily to the tree-to-tree case, but if so, it is not yet obvious to me how to do this.\nOne proposed approach for learning relations over trees is given in (Eisner, 2003), in which Eisner presents algorithms for both extracting an STSG grammar and training its weights; STSGs can then be expressed as xT transducers as described by Maletti in (Maletti, 2010a). Additionally, approaches for leaning tree transduction rules have been suggested for tasks other than machine translation, particularly in the summarization work of Cohn and Lapata (Cohn and Lapata, 2007), (Cohn and Lapata, 2008), who work with a corpus that not only has parse trees for both source and target languages (in their case, pairs\nof longer and paraphrased sentences, both in English), but has also been word-aligned. The word alignments inform their grammar extraction. Cohn and Lapata use a very small training paraphrase corpus (480 sentences), which suggests that perhaps their methods would be useful for MT with low-resourced languages. They also use of discriminative methods for training and decoding. Both their algorithm for rule extraction and the tree transducers with discriminative methods may have been used in tree-to-tree MT system, but I have not yet found work that describes this; if it has not yet been tried, someone should explore it."}, {"heading": "8.3 XDG as Transducers", "text": "Given that many grammar formalisms are expressible in terms of tree transducers, one wonders if constraint-based dependency frameworks, such as Extensible Dependency Grammar (Debusmann, 2006), which has been used by Michael Gasser for machine translation (Gasser, 2011), could be expressed in terms of tree transducers. Transducers over dependency trees have already been used for machine translation, for example by Ding and Palmer (Ding and Palmer, 2005). However, XDG defines not just one layer of dependency analysis for a language, but several. Its analysis of a sentence in a given language is a multigraph with multiple dimensions of analysis, with constraints describing permissible structures on each dimension, as well as the relationships between dimensions. This suggests that perhaps XDG could be expressed as a cascade of transducers, with each layer in the cascade describing the relation between one XDG dimension and the next.\nA problem with this interpretation is that not all layers of an XDG multigraph are tree structures. This might mean that XDG cannot be cleanly expressed in this way at all, or perhaps that another kind of transducer that operates on graphs more generally could be used. Alternatively, perhaps XDG could be tweaked such that every layer has a tree structure.\nIf it is in fact possible to express XDG translation rules as a cascade of transducers, then this would present a clear path for integrating machine learning into the largely rule-based system, making use of the training algorithms already present in the literature. As a fairly modest step, given small numbers of parallel training sentences, one could use EM to train the weights\nof the transduction rules that implement the XDG grammar. More ambitiously, one could perhaps extract grammar rules from example translation pairs, although the XDG parse graphs would have to be provided by an expert, for each layer in the analysis. This could be done either simply on demand, when the existing grammar fails to parse and translate a sentence, or using active learning to select sentences for human annotation.\nOne problem not addressed at all in the literature that I have seen is how to translate, either into or out of, morphologically rich languages using tree transducers. It seems as though morphological analysis and lemmatization would be an important first step in a transducer-based MT system, to limit the number of rules that the system needs to consider, but then the morphological information should be used to help the system make choices during transduction (decoding). Perhaps morphological features would be useful to classifiers trained to help make syntactic disambiguation decisions."}], "references": [{"title": "OpenFst: A general and efficient weighted finite-state transducer library", "author": ["Cyril Allauzen", "Michael Riley", "Johan Schalkwyk", "Wojciech Skut", "Mehryar Mohri."], "venue": "Proceedings of the Ninth International Conference on Implementation and Application of", "citeRegEx": "Allauzen et al\\.,? 2007", "shortCiteRegEx": "Allauzen et al\\.", "year": 2007}, {"title": "Natural Language Processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Stephen Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer."], "venue": "Computational Linguistics, 19(2):263\u2013 311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "How phrase sense disambiguation outperforms word sense disambiguation for statistical machine translation", "author": ["Marine Carpuat", "Dekai Wu."], "venue": "11th Conference on Theoretical and Methodological Issues in Machine Translation.", "citeRegEx": "Carpuat and Wu.,? 2007", "shortCiteRegEx": "Carpuat and Wu.", "year": 2007}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 263\u2013270, Ann Arbor, Michigan, June. Association", "citeRegEx": "Chiang.,? 2005", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "An introduction to synchronous grammars", "author": ["David Chiang"], "venue": null, "citeRegEx": "Chiang.,? \\Q2006\\E", "shortCiteRegEx": "Chiang.", "year": 2006}, {"title": "Large margin synchronous generation and its application to sentence compression", "author": ["Trevor Cohn", "Mirella Lapata."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Nat-", "citeRegEx": "Cohn and Lapata.,? 2007", "shortCiteRegEx": "Cohn and Lapata.", "year": 2007}, {"title": "Sentence compression beyond word deletion", "author": ["Trevor Cohn", "Mirella Lapata."], "venue": "Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 137\u2013144, Manchester, UK, August. Coling 2008 Organizing", "citeRegEx": "Cohn and Lapata.,? 2008", "shortCiteRegEx": "Cohn and Lapata.", "year": 2008}, {"title": "Extensible Dependency Grammar: A Modular Grammar Formalism Based On Multigraph Description", "author": ["Ralph Debusmann."], "venue": "Ph.D. thesis, Saarland University, 4.", "citeRegEx": "Debusmann.,? 2006", "shortCiteRegEx": "Debusmann.", "year": 2006}, {"title": "Synchronous tree adjoining machine translation", "author": ["Steve DeNeefe", "Kevin Knight."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 727\u2013736, Singapore, August. Association for Computational Lin-", "citeRegEx": "DeNeefe and Knight.,? 2009", "shortCiteRegEx": "DeNeefe and Knight.", "year": 2009}, {"title": "Machine translation using probabilistic synchronous dependency insertion grammars", "author": ["Yuan Ding", "Martha Palmer."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL\u201905), pages 541\u2013548, Ann Arbor,", "citeRegEx": "Ding and Palmer.,? 2005", "shortCiteRegEx": "Ding and Palmer.", "year": 2005}, {"title": "Machine translation divergences: A formal description and proposed solution", "author": ["Bonnie J. Dorr."], "venue": "Computational Linguistics, 20(4):597\u2013633.", "citeRegEx": "Dorr.,? 1994", "shortCiteRegEx": "Dorr.", "year": 1994}, {"title": "Learning non-isomorphic tree mappings for machine translation", "author": ["Jason Eisner."], "venue": "The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 205\u2013208, Sapporo, Japan, July. Associa-", "citeRegEx": "Eisner.,? 2003", "shortCiteRegEx": "Eisner.", "year": 2003}, {"title": "What\u2019s in a translation rule", "author": ["Michel Galley", "Mark Hopkins", "Kevin Knight", "Daniel Marcu"], "venue": "HLT-NAACL 2004: Main Proceedings,", "citeRegEx": "Galley et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2004}, {"title": "Toward synchronous extensible dependency grammar", "author": ["Michael Gasser."], "venue": "Proceedings of the International Workshop on Free/Open-Source Rule-Based Machine Translation (2nd : 2011 : Barcelona), Barcelona,Spain, January.", "citeRegEx": "Gasser.,? 2011", "shortCiteRegEx": "Gasser.", "year": 2011}, {"title": "Training tree transducers", "author": ["Jonathan Graehl", "Kevin Knight", "Jonathan May."], "venue": "Computational Linguistics, 34(3):391\u2013427.", "citeRegEx": "Graehl et al\\.,? 2008", "shortCiteRegEx": "Graehl et al\\.", "year": 2008}, {"title": "Hierarchical phrasebased translation with weighted finite state transducers", "author": ["Gonzalo Iglesias", "Adri\u00e0 de Gispert", "Eduardo R. Banga", "William Byrne."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North", "citeRegEx": "Iglesias et al\\.,? 2009", "shortCiteRegEx": "Iglesias et al\\.", "year": 2009}, {"title": "Tree adjunct grammars", "author": ["Aravind K. Joshi", "Leon S. Levy", "Masako Takahashi."], "venue": "J. Comput. Syst. Sci., 10(1):136\u2013163, February.", "citeRegEx": "Joshi et al\\.,? 1975", "shortCiteRegEx": "Joshi et al\\.", "year": 1975}, {"title": "Fsa: An efficient and flexible c++ toolkit for finite state automata using on-demand computation", "author": ["Stephan Kanthak", "Hermann Ney."], "venue": "Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL\u201904), Main Volume,", "citeRegEx": "Kanthak and Ney.,? 2004", "shortCiteRegEx": "Kanthak and Ney.", "year": 2004}, {"title": "Translation with finite-state devices", "author": ["Kevin Knight", "Yaser Al-Onaizan."], "venue": "David Farwell, Laurie Gerber, and Eduard H. Hovy, editors, AMTA, volume 1529 of Lecture Notes in Computer Science, pages 421\u2013437. Springer.", "citeRegEx": "Knight and Al.Onaizan.,? 1998", "shortCiteRegEx": "Knight and Al.Onaizan.", "year": 1998}, {"title": "An overview of probabilistic tree transducers for natural language processing", "author": ["Kevin Knight", "Jonathan Graehl."], "venue": "Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 3406 of Lecture Notes in Computer", "citeRegEx": "Knight and Graehl.,? 2005", "shortCiteRegEx": "Knight and Graehl.", "year": 2005}, {"title": "Capturing practical natural language transformations", "author": ["Kevin Knight."], "venue": "Machine Translation, 21(2):121\u2013133.", "citeRegEx": "Knight.,? 2007", "shortCiteRegEx": "Knight.", "year": 2007}, {"title": "A weighted finite state transducer implementation of the alignment template model for statistical machine translation", "author": ["Shankar Kumar", "William J. Byrne."], "venue": "HLT-NAACL.", "citeRegEx": "Kumar and Byrne.,? 2003", "shortCiteRegEx": "Kumar and Byrne.", "year": 2003}, {"title": "A tree transducer model for synchronous tree-adjoining grammars", "author": ["Andreas Maletti."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1067\u2013 1076, Uppsala, Sweden, July. Association for Com-", "citeRegEx": "Maletti.,? 2010a", "shortCiteRegEx": "Maletti.", "year": 2010}, {"title": "Why synchronous tree substitution grammars? In Human Language Technologies: The 2010", "author": ["Andreas Maletti"], "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Maletti.,? \\Q2010\\E", "shortCiteRegEx": "Maletti.", "year": 2010}, {"title": "Induction of probabilistic synchronous tree-insertion grammars for machine translation", "author": ["Rebecca Nesson", "Stuart M. Shieber", "Alexander Rush."], "venue": "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA", "citeRegEx": "Nesson et al\\.,? 2006", "shortCiteRegEx": "Nesson et al\\.", "year": 2006}, {"title": "The alignment template approach to statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational Linguistics, 30(4):417\u2013449.", "citeRegEx": "Och and Ney.,? 2004", "shortCiteRegEx": "Och and Ney.", "year": 2004}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Erhard W. Hinrichs and Dan Roth, editors, ACL, pages 160\u2013167. ACL.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Synchronous grammars as tree transducers", "author": ["Stuart M. Shieber."], "venue": "Proceedings of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+ 7), Vancouver, Canada, May 20-22.", "citeRegEx": "Shieber.,? 2004", "shortCiteRegEx": "Shieber.", "year": 2004}, {"title": "Probabilistic synchronous tree-adjoining grammars for machine translation: The argument from bilingual dictionaries", "author": ["Stuart M. Shieber."], "venue": "Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Transla-", "citeRegEx": "Shieber.,? 2007", "shortCiteRegEx": "Shieber.", "year": 2007}, {"title": "Reversibility in Natural Language Processing", "author": ["Gertjan van Noord."], "venue": "Ph.D. thesis, University of Utrecht.", "citeRegEx": "Noord.,? 1993", "shortCiteRegEx": "Noord.", "year": 1993}, {"title": "The equivalence of four extensions of context-free grammars", "author": ["K. Vijay-Shanker", "David Weir."], "venue": "Mathematical Systems Theory, 27:511\u2013546.", "citeRegEx": "Vijay.Shanker and Weir.,? 1994", "shortCiteRegEx": "Vijay.Shanker and Weir.", "year": 1994}], "referenceMentions": [{"referenceID": 2, "context": "Word-based approaches to statistical machine translation, starting with the work from IBM in the early 1990s (Brown et al., 1993) have been successful both in use in production translation systems and in invigorating MT research.", "startOffset": 109, "endOffset": 129}, {"referenceID": 26, "context": "Since then, newer phrase-based MT techniques such as the alignment template model (Och and Ney, 2004), and hierarchical phrase-based models (Chiang, 2005) have made significant improvements in SMT translation quality.", "startOffset": 82, "endOffset": 101}, {"referenceID": 4, "context": "Since then, newer phrase-based MT techniques such as the alignment template model (Och and Ney, 2004), and hierarchical phrase-based models (Chiang, 2005) have made significant improvements in SMT translation quality.", "startOffset": 140, "endOffset": 154}, {"referenceID": 0, "context": "Several such packages are freely available, such as OpenFST (Allauzen et al., 2007) and the WRTH FSA Toolkit (Kanthak and Ney, 2004).", "startOffset": 60, "endOffset": 83}, {"referenceID": 18, "context": ", 2007) and the WRTH FSA Toolkit (Kanthak and Ney, 2004).", "startOffset": 33, "endOffset": 56}, {"referenceID": 11, "context": "Bonnie Dorr provides us with an excellent test bed of seven cross-language divergences that may occur when we want to perform translation, even between languages as closely related as English, Spanish and German (Dorr, 1994).", "startOffset": 212, "endOffset": 224}, {"referenceID": 20, "context": "For example, regular tree grammars (RTG) is the class of grammars corresponding to contextfree grammars but describing trees; they describe the trees whose yield (string concatenation of the symbols at the leaves) is a context-free grammar (Knight and Graehl, 2005).", "startOffset": 240, "endOffset": 265}, {"referenceID": 5, "context": "All of these operations are described for synchronous contextfree grammars in David Chiang\u2019s tutorial (Chiang, 2006).", "startOffset": 102, "endOffset": 116}, {"referenceID": 17, "context": "Tree adjoining grammar, introduced by Joshi (Joshi et al., 1975), has been a popular formalism for describing grammars over trees.", "startOffset": 44, "endOffset": 64}, {"referenceID": 31, "context": "TAG is thus called \u201cweakly context-sensitive\u201d, and has been shown formally equivalent to several other syntactic formalisms, such as Combinatory Categorial Grammar (CCG) and Linear Indexed Grammars (Vijay-Shanker and Weir, 1994).", "startOffset": 198, "endOffset": 228}, {"referenceID": 5, "context": "Example from (Chiang, 2006).", "startOffset": 13, "endOffset": 27}, {"referenceID": 29, "context": "Synchronous TAG has also been investigated, and its use in machine translation has been advocated by Shieber, who argues that its expressive power may make up for its computational complexity (Shieber, 2007).", "startOffset": 192, "endOffset": 207}, {"referenceID": 12, "context": "TSG only provides the substitution operation, and does not have auxiliary trees or adjunction (Eisner, 2003).", "startOffset": 94, "endOffset": 108}, {"referenceID": 25, "context": "While parsing with a TAG takes in the general case O(n) complexity, TIG (like the general case for CFGs) can be parsed in O(n) (Nesson et al., 2006).", "startOffset": 127, "endOffset": 148}, {"referenceID": 25, "context": "Both STIG and STSG have seen use in machine translation; for example, probabilistic STIG is used in (Nesson et al., 2006), and STSG has been notably used in (Eisner, 2003).", "startOffset": 100, "endOffset": 121}, {"referenceID": 12, "context": ", 2006), and STSG has been notably used in (Eisner, 2003).", "startOffset": 43, "endOffset": 57}, {"referenceID": 5, "context": "For example, with either a synchronous grammar or a transducer, we may ask, for a given tree, what are the other trees that are in the mathematical relation with it (Chiang, 2006).", "startOffset": 165, "endOffset": 179}, {"referenceID": 15, "context": "See Figure 3 for an illustrative example, adapted from (Graehl et al., 2008).", "startOffset": 55, "endOffset": 76}, {"referenceID": 28, "context": "While T transducers are not as expressive as synchronous TSG (Shieber, 2004), xT transducers are as expressive, and can even be used to simulate synchronous TAG in some cases (Maletti, 2010a).", "startOffset": 61, "endOffset": 76}, {"referenceID": 23, "context": "While T transducers are not as expressive as synchronous TSG (Shieber, 2004), xT transducers are as expressive, and can even be used to simulate synchronous TAG in some cases (Maletti, 2010a).", "startOffset": 175, "endOffset": 191}, {"referenceID": 15, "context": "Figure 3: Example transduction steps, simplified from (Graehl et al., 2008).", "startOffset": 54, "endOffset": 75}, {"referenceID": 13, "context": "This particular combination of options has been used several times in the literature, including (Galley et al., 2004).", "startOffset": 96, "endOffset": 117}, {"referenceID": 21, "context": "However this is not possible with any other members of the T-transducer family; even xLNT transducers are non-compositional (Knight, 2007).", "startOffset": 124, "endOffset": 138}, {"referenceID": 11, "context": "Bonnie Dorr, in (Dorr, 1994), enumerates several different kinds of structural divergences that we might see in translation between languages.", "startOffset": 16, "endOffset": 28}, {"referenceID": 28, "context": "However, each of these divergences require something more than simple word substitution or reordering the children of a given node: many of these require raising and lowering tree material (performing \u201crotations\u201d, in the terminology of (Shieber, 2004)), and nested phrases that are present in one language are often not present in the other.", "startOffset": 236, "endOffset": 251}, {"referenceID": 11, "context": "In (Dorr, 1994), a formal distinction is made between the two because in Dorr\u2019s MT system, they would be triggered in different circumstances, but for our purposes they are effectively analogous.", "startOffset": 3, "endOffset": 15}, {"referenceID": 1, "context": "It is implemented in Python 3 and makes use of the NLTK tree libraries (Bird et al., 2009).", "startOffset": 71, "endOffset": 90}, {"referenceID": 15, "context": "A complete and useful MT system based on this software \u2013 such that the rules and their weights were not completely the product of human knowledge engineering \u2013 would require the implementation of a few more algorithms described in (Graehl et al., 2008), particularly their EM training algorithm to calculate weights for a given set of transduction rules, which depends on their transduction algorithm that produces the more compact representation of a transduction, a RTG.", "startOffset": 231, "endOffset": 252}, {"referenceID": 28, "context": "Most of the transformations required to implement these rules are instances of local rotations, as described by (Shieber, 2004).", "startOffset": 112, "endOffset": 127}, {"referenceID": 19, "context": "Not long after the introduction of modern word-based SMT, Knight and Al-Onaizan showed that IBM Model 3 could be expressed with a cascade of FSTs (Knight and Al-Onaizan, 1998).", "startOffset": 146, "endOffset": 175}, {"referenceID": 22, "context": "Similarly, Shankar Kumar and William Byrne expressed the phrase-based alignment template model as FSTs (Kumar and Byrne, 2003).", "startOffset": 103, "endOffset": 126}, {"referenceID": 16, "context": "FSTs (Iglesias et al., 2009); Iglesias et al.", "startOffset": 5, "endOffset": 28}, {"referenceID": 5, "context": "For tutorials and related algorithms, Chiang provides an excellent introduction to synchronous grammars in (Chiang, 2006).", "startOffset": 107, "endOffset": 121}, {"referenceID": 21, "context": "For overviews of different applications of T-family tree transducers and their various properties, in a very approachable style, (Knight, 2007) and (Knight and Graehl, 2005) are very helpful.", "startOffset": 129, "endOffset": 143}, {"referenceID": 20, "context": "For overviews of different applications of T-family tree transducers and their various properties, in a very approachable style, (Knight, 2007) and (Knight and Graehl, 2005) are very helpful.", "startOffset": 148, "endOffset": 173}, {"referenceID": 15, "context": "Additionally (Graehl et al., 2008) contains excellent examples for understanding xT transduction (one of which is in this paper in simplified form, though the original example is worth working through and understanding fully), along with a set of algorithms that can be computed over xT transducers, including an EM procedure that can be used to estimate the weights for an xT grammar given a parallel treebank.", "startOffset": 13, "endOffset": 34}, {"referenceID": 3, "context": "However, cross-language phrase-sense disambiguation with classifiers, like in the work of Carpuat and Wu (Carpuat and Wu, 2007), has proved useful for phrase-based SMT.", "startOffset": 105, "endOffset": 127}, {"referenceID": 27, "context": "For phrasebased SMT in general, discriminative approaches such as Minimum Error-Rate Training (MERT) (Och, 2003) have become quite typical.", "startOffset": 101, "endOffset": 112}, {"referenceID": 13, "context": "Other work has presented methods for learning tree-to-string transduction rules, for example (Galley et al., 2004) and (DeNeefe and Knight, 2009).", "startOffset": 93, "endOffset": 114}, {"referenceID": 9, "context": ", 2004) and (DeNeefe and Knight, 2009).", "startOffset": 12, "endOffset": 38}, {"referenceID": 12, "context": "One proposed approach for learning relations over trees is given in (Eisner, 2003), in which Eisner presents algorithms for both extracting an STSG grammar and training its weights; STSGs can then be expressed as xT transducers as described by Maletti in (Maletti, 2010a).", "startOffset": 68, "endOffset": 82}, {"referenceID": 23, "context": "One proposed approach for learning relations over trees is given in (Eisner, 2003), in which Eisner presents algorithms for both extracting an STSG grammar and training its weights; STSGs can then be expressed as xT transducers as described by Maletti in (Maletti, 2010a).", "startOffset": 255, "endOffset": 271}, {"referenceID": 6, "context": "Additionally, approaches for leaning tree transduction rules have been suggested for tasks other than machine translation, particularly in the summarization work of Cohn and Lapata (Cohn and Lapata, 2007), (Cohn and Lapata, 2008), who work with a corpus that not only has parse trees for both source and target languages (in their case, pairs of longer and paraphrased sentences, both in English), but has also been word-aligned.", "startOffset": 181, "endOffset": 204}, {"referenceID": 7, "context": "Additionally, approaches for leaning tree transduction rules have been suggested for tasks other than machine translation, particularly in the summarization work of Cohn and Lapata (Cohn and Lapata, 2007), (Cohn and Lapata, 2008), who work with a corpus that not only has parse trees for both source and target languages (in their case, pairs of longer and paraphrased sentences, both in English), but has also been word-aligned.", "startOffset": 206, "endOffset": 229}, {"referenceID": 8, "context": "Given that many grammar formalisms are expressible in terms of tree transducers, one wonders if constraint-based dependency frameworks, such as Extensible Dependency Grammar (Debusmann, 2006), which has been used by Michael Gasser for machine translation (Gasser, 2011), could be expressed in terms of tree transducers.", "startOffset": 174, "endOffset": 191}, {"referenceID": 14, "context": "Given that many grammar formalisms are expressible in terms of tree transducers, one wonders if constraint-based dependency frameworks, such as Extensible Dependency Grammar (Debusmann, 2006), which has been used by Michael Gasser for machine translation (Gasser, 2011), could be expressed in terms of tree transducers.", "startOffset": 255, "endOffset": 269}, {"referenceID": 10, "context": "Transducers over dependency trees have already been used for machine translation, for example by Ding and Palmer (Ding and Palmer, 2005).", "startOffset": 113, "endOffset": 136}], "year": 2012, "abstractText": "Tree transducers are formal automata that transform trees into other trees. Many varieties of tree transducers have been explored in the automata theory literature, and more recently, in the machine translation literature. In this paper I review T and xT transducers, situate them among related formalisms, and show how they can be used to implement rules for machine translation systems that cover all of the cross-language structural divergences described in Bonnie Dorr\u2019s influential article on the topic. I also present an implementation of xT transduction, suitable and convenient for experimenting with translation rules.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}