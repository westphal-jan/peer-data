{"id": "1204.0566", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Apr-2012", "title": "The Kernelized Stochastic Batch Perceptron", "abstract": "We present a novel approach for training kernel Support Vector Machines, establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach, and show that our method works well in practice compared to existing alternatives. For instance, we can easily implement a non-memory and memory-efficient algorithm to perform multiple iterations to gain the same results:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Tue, 3 Apr 2012 00:33:53 GMT  (505kb)", "https://arxiv.org/abs/1204.0566v1", null], ["v2", "Thu, 21 Jun 2012 12:14:24 GMT  (1043kb)", "http://arxiv.org/abs/1204.0566v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andrew cotter", "shai shalev-shwartz", "nathan srebro"], "accepted": true, "id": "1204.0566"}, "pdf": {"name": "1204.0566.pdf", "metadata": {"source": "META", "title": "The Kernelized Stochastic Batch Perceptron", "authors": ["Andrew Cotter"], "emails": ["cotter@ttic.edu", "shais@cs.huji.ac.il", "nati@ttic.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 4.\n05 66\nv2 [\ncs .L\nG ]\n2 1\nJu n"}, {"heading": "1. Introduction", "text": "We present a novel algorithm for training kernel Support Vector Machines (SVMs). One may view a SVM as the bi-criterion optimization problem of seeking a predictor with large margin (low norm) on the one hand, and small training error on the other. Our approach is a stochastic gradient method on a nonstandard scalarization of this bi-criterion problem. In particular, we use the \u201cslack constrained\u201d scalarized optimization problem introduced by Hazan et al. (2011) where we seek to maximize the classification margin, subject to a constraint on the total amount of \u201cslack\u201d, i.e. sum of the violations of this margin. Our approach is based on an efficient method for computing unbiased gradient estimates on the objective. Our algorithm can be seen as a generalization of the \u201cBatch Perceptron\u201d to the non-separable case (i.e. when errors are allowed), made possible by introducing stochasticity, and we therefore refer to it as the \u201cStochastic Batch Perceptron\u201d (SBP).\nThe SBP is fundamentally different from Pegasos (Shalev-Shwartz et al., 2011) and other stochastic gradient approaches to the problem of training SVMs, in\nThis is the \u201clong version\u201d of our ICML 2012 paper. The only difference between the two versions is that this one includes appendices.\nthat calculating each stochastic gradient estimate still requires considering the entire data set. In this regard, despite its stochasticity, the SBP is very much a \u201cbatch\u201d rather than \u201conline\u201d algorithm. For a linear SVM, each iteration would require runtime linear in the training set size, resulting in an unacceptable overall runtime. However, in the kernel setting, essentially all known approaches already require linear runtime per iteration. A more careful analysis reveals the benefits of the SBP over previous kernel SVM optimization algorithms.\nIn order to compare the SBP runtime to the runtime of other SVM optimization algorithms, which typically work on different scalarizations of the bi-criterion problem, we follow Bottou & Bousquet (2008); Shalev-Shwartz & Srebro (2008) and compare the runtimes required to ensure a generalization error of L\u2217 + \u01eb, assuming the existence of some unknown predictor u with norm \u2016u\u2016 and expected hinge loss L\u2217. The main advantage of the SBP is in the regime in which \u01eb = \u2126(L\u2217), i.e. we seek a constant factor approximation to the best achievable error (e.g. we would like an error of 1.01L\u2217). In this regime, the overall SBP runtime is \u2016u\u20164 /\u01eb, compared with \u2016u\u20164 /\u01eb3 for Pegasos and \u2016u\u20164 /\u01eb2 for the best known dual decomposition approach."}, {"heading": "2. Setup and Formulations", "text": "Training a SVM amounts to finding a vector w defining a classifier x 7\u2192 sign(\u3008w,\u03a6 (x)\u3009), that on the one hand has small norm (corresponding to a large classification margin), and on the other has a small training error, as measured through the average hinge loss on the training sample: L\u0302(w) = 1n \u2211n i=1 \u2113 (yi \u3008w,\u03a6 (xi)\u3009), where each (xi, yi) is a labeled example, and \u2113 (a) = max (0, 1\u2212 a) is the hinge loss. This is captured by\nthe following bi-criterion optimization problem:\nmin w\u2208Rd\n\u2016w\u2016 , L\u0302(w). (2.1)\nWe focus on kernelized SVMs, where the feature map \u03a6(x) is specified implicitly via a kernel K (x, x\u2032) = \u3008\u03a6 (x),\u03a6 (x\u2032)\u3009, and assume that K(x, x\u2032) \u2264 1. We consider only \u201cblack box\u201d access to the kernel (i.e. our methods work for any kernel, as long as we can compute K(x, x\u2032) efficiently), and in our runtime analysis treat kernel evaluations as requiring O(1) runtime. Since kernel evaluations dominate the runtime of all methods studied (ours as well as previous methods), one can also interpret the runtimes as indicating the number of required kernel evaluations. To simplify our derivation, we often discuss the explicit SVM, using \u03a6(x), and refer to the kernel only when needed.\nA typical approach to the bi-criterion Problem 2.1 is to scalarize it using a parameter \u03bb controlling the tradeoff between the norm (inverse margin) and the empirical error:\nmin w\u2208Rd\n\u03bb 2 \u2016w\u20162 + 1 n\nn \u2211\ni=1\n\u2113 (yi \u3008w,\u03a6 (xi)\u3009) (2.2)\nDifferent values of \u03bb correspond to different Pareto optimal solutions of Problem 2.1, and the entire Pareto front can be explored by varying \u03bb.\nWe instead consider the \u201cslack constrained\u201d scalarization (Hazan et al., 2011), where we maximize the \u201cmargin\u201d subject to a constraint of \u03bd on the total allowed \u201cslack\u201d, corresponding to the average error. That is, we aim at maximizing the margin by which all points are correctly classified (i.e. the minimal distance between a point and the separating hyperplane), after allowing predictions to be corrected by a total amount specified by the slack constraint:\nmax w\u2208Rd max \u03be\u2208Rn min i\u2208{1,...,n}\n(yi \u3008w,\u03a6 (xi)\u3009+ \u03bei) (2.3)\nsubject to: \u2016w\u2016 \u2264 1, \u03be 0, 1T \u03be \u2264 n\u03bd In this scalarization, varying \u03bd explores different Pareto optimal solutions of Problem 2.1. This is captured by the following Lemma, which also quantifies how suboptimal solutions of the slack-constrained objective correspond to Pareto suboptimal points:\nLemma 2.1. (Hazan et al., 2011, Lemma 2.1) For any u 6= 0, consider Problem 2.3 with \u03bd = L\u0302 (u) / \u2016u\u2016. Let w\u0304 be an \u01eb\u0304-suboptimal solution to this problem with objective value \u03b3, and consider the rescaled solution w = w\u0304/\u03b3. Then:\n\u2016w\u2016 \u2264 1 1\u2212 \u01eb\u0304 \u2016u\u2016 \u2016u\u2016 , L\u0302 (w) \u2264\n1\n1\u2212 \u01eb\u0304 \u2016u\u2016L\u0302 (u)"}, {"heading": "3. The Stochastic Batch Perceptron", "text": "In this section, we will develop the Stochastic Batch Perceptron. We consider Problem 2.3 as optimization of the variable w with a single constraint \u2016w\u2016 \u2264 1, with the objective being to maximize:\nf (w) = max \u03be 0,1T \u03be\u2264n\u03bd min p\u2208\u2206n\nn \u2211\ni=1\npi (yi \u3008w,\u03a6 (xi)\u3009+ \u03bei)\n(3.1) Notice that we replaced the minimization over training indices i in Problem 2.3 with an equivalent minimization over the probability simplex, \u2206n = {p 0 : 1T p = 1}, and that we consider p and \u03be to be a part of the objective, rather than optimization variables. The objective f(w) is a concave function of w, and we are maximizing it over a convex constraint \u2016w\u2016 \u2264 1, and so this is a convex optimization problem in w.\nOur approach will be to perform a stochastic gradient update on w at each iteration: take a step in the direction specified by an unbiased estimator of a (super)gradient of f(w), and project back to \u2016w\u2016 \u2264 1. To this end, we will need to identify the (super)gradients of f(w) and understand how to efficiently calculate unbiased estimates of them."}, {"heading": "3.1. Warmup: The Separable Case", "text": "As a warmup, we first consider the separable case, where \u03bd = 0 and no errors are allowed. The objective is then:\nf(w) = min i\nyi \u3008w,\u03a6 (xi)\u3009 , (3.2)\nThis is simply the \u201cmargin\u201d by which all points are correctly classified, i.e. \u03b3 s.t. \u2200i yi \u3008w,\u03a6(xi)\u3009 \u2265 \u03b3. We seek a linear predictor w with the largest possible margin. It is easy to see that (super)gradients with respect to w are given by yi\u03a6(xi) for any index i attaining the minimum in Equation 3.2, i.e. by the \u201cmost poorly classified\u201d point(s). A gradient ascent approach would then be to iteratively find such a point, update w \u2190 w+ \u03b7yi\u03a6(xi), and project back to \u2016w\u2016 \u2264 1. This is akin to a \u201cbatch Perceptron\u201d update, which at each iteration searches for a violating point and adds it to the predictor.\nIn the separable case, we could actually use exact supergradients of the objective. As we shall see, it is computationally beneficial in the non-separable case to base our steps on unbiased gradient estimates. We therefore refer to our method as the \u201cStochastic Batch Perceptron\u201d (SBP), and view it as a generalization of the batch Perceptron which uses stochasticity and is applicable in the non-separable setting. In the same\nway that the \u201cbatch Perceptron\u201d can be used to maximize the margin in the separable case, the SBP can be used to obtain any SVM solution along the Pareto front of the bi-criterion Problem 2.1.\n3.2. Supergradients of f(w)\nFor a fixed w, we define c \u2208 Rn be the vector of \u201cresponses\u201d:\nci = yi \u3008w,\u03a6 (xi)\u3009 (3.3)\nSupergradients of f(w) at w can be characterized explicitly in terms of minimax-optimal pairs p\u2217 and \u03be\u2217 such that p\u2217 = argminp\u2208\u2206n pt(c + \u03be\u2217) and \u03be\u2217 = argmax\u03be 0,1T \u03be\u2264n\u03bd(p \u2217)T (c+ \u03be).\nLemma 3.1 (Proof in Appendix C). For any w, let p\u2217, \u03be\u2217 be minimax optimal for Equation 3.1. Then \u2211n\ni=1 p \u2217 i yi\u03a6 (xi) is a supergradient of f(w) at w.\nThis suggests a simple method for obtaining unbiased estimates of supergradients of f(w): sample a training index i with probability p\u2217i , and take the stochastic supergradient to be yi\u03a6 (xi). The only remaining question is how one finds a minimax optimal p\u2217.\nIt is possible to find a minimax optimal p\u2217 in O(n) time. For any \u03be, a solution of minp\u2208\u2206n pT (x+ \u03be) must put all of the probability mass on those indices i for which ci + \u03bei is minimized. Hence, an optimal \u03be\n\u2217 will maximize the minimal value of ci + \u03be \u2217 i . This is illustrated in Figure 1. The intuition is that the total mass n\u03bd available to \u03be is distributed among the indices as if this volume of water were poured into a basin with height ci. The result is that the indices i with the lowest responses have columns of water above them such that the common surface level of the water is \u03b3.\nOnce the \u201cwater level\u201d \u03b3 has been determined, the optimal p\u2217 must be uniform on those indices i for which \u03be\u2217i > 0, i.e. for which ci < \u03b3, must be zero on all i s.t. ci > \u03b3, and could take any intermediate value when ci = \u03b3 (that is, for some q > 0, we must have ci < \u03b3 \u2192 p\u2217i = q, ci = \u03b3 \u2192 0 \u2264 p\u2217i \u2264 q, and ci > \u03b3 \u2192 p\u2217i = 0\u2014see Figure 1). In particular, the uniform distribution over all indices such that ci \u2264 \u03b3 is minimax optimal. Notice that in the separable case, where no slack is allowed, \u03b3 = mini ci and any distribution supported on the minimizing point(s) is minimax optimal, and yi\u03a6(xi) is an exact supergradient for such an i, as discussed in Section 3.1.\nIt is straightforward to find the water level \u03b3 in linear time once the responses ci are sorted (as in Figure 1), i.e. with a total runtime of O(n log n) due to sorting. It is also possible to find the water level \u03b3 in linear time, without sorting the responses, using a divideand-conquer algorithm, further of which may be found in Appendix B."}, {"heading": "3.3. Kernelized Implementation", "text": "In a kernelized SVM, w is an element of an implicit space, and cannot be represented explicitly. We therefore represent w as w =\n\u2211n i=1 \u03b1iyi\u03a6 (xi), and main-\ntain not w itself, but instead the coefficients \u03b1i. Our stochastic gradient estimates are always of the form yi\u03a6(xi) for an index i. Taking a step in this direction amounts to simply increasing the corresponding \u03b1i.\nWe could calculate all the responses ci at each iteration as ci = \u2211n j=1 \u03b1jyiyjK(xi, xj). However, this would require a quadratic number of kernel evaluations per iteration. Instead, as is typically done in kernelized SVM implementations, we keep the responses ci on hand, and after each stochastic gradient step of the form w \u2190 w + \u03b7yj\u03a6 (xj), we update the responses as:\nci \u2190 ci + \u03b7yiyjK(xi, xj) (3.4) This involves only n kernel evaluations per iteration.\nIn order to project w onto the unit ball, we must either track \u2016w\u2016 or calculate it from the responses as \u2016w\u2016 = \u2211ni=1 \u03b1ici. Rescaling w so as to project it back into \u2016w\u2016 \u2264 1 is performed by rescaling all coefficients \u03b1i and responses ci, again taking time O(n) and no additional kernel evaluations."}, {"heading": "3.4. Putting it Together", "text": "We are now ready to summarize the SBP algorithm. Starting from w(0) = 0 (so both \u03b1(0) and all responses are zero), each iteration proceeds as follows:\n1. Find p\u2217 by finding the \u201cwater level\u201d \u03b3 from the re-\nsponses (Section 3.2), and taking p\u2217 to be uniform on those indices for which ci \u2264 \u03b3.\n2. Sample j \u223c p\u2217. 3. Update w(t+1) \u2190 P ( w(t) + \u03b7tyj\u03a6 (xj) )\n, where P projects onto the unit ball and \u03b7t =\n1\u221a t . This is\ndone by first increasing \u03b1 \u2190 \u03b1 + \u03b7t and updating the responses as in Equation 3.4, then calculating \u2016w\u2016 (Section 3.3) and scaling \u03b1 and c by min(1, 1/ \u2016w\u2016).\nUpdating the responses as in Equation 3.4 requires O(n) kernel evaluations (the most computationally expensive part) and all other operations require O(n) scalar arithmetic operations.\nSince at each iteration we are just updating using an unbiased estimator of a supergradient, we can rely on the standard analysis of stochastic gradient descent to bound the suboptimality after T iterations:\nLemma 3.2 (Proof in Appendix C). For any T, \u03b4 > 0, after T iterations of the Stochastic Batch Perceptron, with probability at least 1 \u2212 \u03b4, the average iterate w\u0304 = 1T \u2211T t=1 w (t) (corresponding to \u03b1\u0304 = 1T \u2211T t=1 \u03b1 (t)), satisfies: f (w\u0304) \u2265 sup\u2016w\u2016\u22641 f (w) \u2212O (\u221a 1 T log 1 \u03b4 ) .\nSince each iteration is dominated by n kernel evaluations, and thus takes linear time (we take a kernel evaluation to require O(1) time), the overall runtime to achieve \u01eb suboptimality for Problem 2.3 is O(n/\u01eb2)."}, {"heading": "3.5. Learning Runtime", "text": "The previous section has given us the runtime for obtaining a certain suboptimality of Problem 2.3. However, since the suboptimality in this objective is not directly comparable to the suboptimality of other scalarizations, e.g. Problem 2.2, we follow Bottou & Bousquet (2008); Shalev-Shwartz & Srebro (2008), and analyze the runtime required to achieve a desired generalization performance, instead of that to achieve a certain optimization accuracy on the empirical optimization problem.\nRecall that our true learning objective is to find a predictor with low generalization error L0/1(w) = Pr(x,y) {y \u3008w,\u03a6(x)\u3009 \u2264 0} with respect to some unknown distribution over x, y based on a training set drawn i.i.d. from this distribution. We assume that there exists some (unknown) predictor u that has norm \u2016u\u2016 and low expected hinge loss L\u2217 = L(u) = E [\u2113(y \u3008u,\u03a6(x)\u3009)] (otherwise, there is no point in training a SVM), and analyze the runtime to find a predictor w with generalization error L0/1(w) \u2264 L\u2217 + \u01eb. In order to understand the SBP runtime, we must\ndetermine both the required sample size and optimization accuracy. Following Hazan et al. (2011), and based on the generalization guarantees of Srebro et al. (2010), using a sample of size:\nn = O\u0303\n(\n(L\u2217 + \u01eb \u01eb ) \u2016u\u20162 \u01eb\n)\n(3.5)\nand optimizing the empirical SVM bi-criterion Problem 2.1 such that:\n\u2016w\u2016 \u2264 2 \u2016u\u2016 ; L\u0302 (w)\u2212 L\u0302 (u) \u2264 \u01eb/2 (3.6)\nsuffices to ensure L0/1(w) \u2264 L\u2217 + \u01eb with high probability. Referring to Lemma 2.1, Equation 3.6 will be satisfied for w\u0304/\u03b3 as long as w\u0304 optimizes the objective of Problem 2.3 to within:\n\u01eb\u0304 = \u01eb/2\n\u2016u\u2016 (L\u0302(u) + \u01eb/2) \u2265 \u2126\n(\n\u01eb\n\u2016u\u2016 (L\u0302(u) + \u01eb)\n)\n(3.7)\nwhere the inequality holds with high probability for the sample size of Equation 3.5. Plugging this sample size and the optimization accuracy of Equation 3.7 into the SBP runtime of O(n/\u01eb\u03042) yields the overall runtime:\nO\u0303\n(\n(L\u2217 + \u01eb \u01eb )3 \u2016u\u20164 \u01eb\n)\n(3.8)\nfor the SBP to find w\u0304 such that its rescaling satisfies L0/1(w) \u2264 L(u) + \u01eb with high probability. In the realizable case, where L\u2217 = 0, or more generally when we would like to reach L\u2217 to within a small constant multiplicative factor, we have \u01eb = \u2126(L\u2217), the first factor in Equation 3.8 is a constant, and the runtime simplifies to O\u0303(\u2016u\u20164 /\u01eb). As we will see in Section 4, this is a better guarantee than that enjoyed by any other SVM optimization approach."}, {"heading": "3.6. Including an Unregularized Bias", "text": "It is possible to use the SBP to train SVMs with a bias term, i.e. where one seeks a predictor of the form x 7\u2192 (\u3008w,\u03a6(x)\u3009+ b). We then take stochastic gradient steps on:\nf(w) = (3.9)\nmax b\u2208R,\u03be 0 1 T \u03be\u2264n\u03bd min p\u2208\u2206n\nn \u2211\ni=1\npi (yi \u3008w,\u03a6(xi)\u3009+ yib+ \u03bei)\nLemma 3.1 still holds, but we must now find minimax optimal p\u2217,\u03be\u2217 and b\u2217. This can be accomplished using a modified \u201cwater filling\u201d involving two basins,\none containing the positively-classified examples, and the other the negatively-classified ones. As in the case without an unregularized bias, this can be accomplished in O(n) time\u2014see Appendix B for details."}, {"heading": "4. Relationship to Other Methods", "text": "We discuss the relationship between the SBP and several other SVM optimization approaches, highlighting similarities and key differences, and comparing their performance guarantees."}, {"heading": "4.1. SIMBA", "text": "Recently, Hazan et al. (2011) presented SIMBA, a method for training linear SVMs based on the same \u201cslack constrained\u201d scalarization (Problem 2.3) we use here. SIMBA also fully optimizes over the slack variables \u03be at each iteration, but differs in that, instead of fully optimizing over the distribution p (as the SBP does), SIMBA updates p using a stochastic mirror descent step. The predictor w is then updated, as in the SBP, using a random example drawn according to p. A SBP iteration is thus in a sense more \u201cthorough\u201d then a SIMBA iteration. The SBP theoretical guarantee (Lemma 3.2) is correspondingly better by a logarithmic factor (compare to Hazan et al. (2011, Theorem 4.3)). All else being equal, we would prefer performing a SBP iteration over a SIMBA iteration.\nFor linear SVMs, a SIMBA iteration can be performed in time O(n + d). However, fully optimizing p as described in Section 3.2 requires the responses ci, and calculating or updating all n responses would require time O(nd). In this setting, therefore, a SIMBA iteration is much more efficient than a SBP iteration.\nIn the kernel setting, calculating even a single response requires O(n) kernel evaluation, which is the same cost as updating all responses after a change to a single coordinate \u03b1i (Section 3.3). This makes the responses essentially \u201cfree\u201d, and gives an advantage to methods\nsuch as the SBP (and the dual decomposition methods discussed below) which make use of the responses.\nAlthough SIMBA is preferable for linear SVMs, the SBP is preferable for kernelized SVMs. It should also be noted that SIMBA relies heavily on having direct access to features, and that it is therefore not obvious how to apply it directly in the kernel setting."}, {"heading": "4.2. Pegasos and SGD on L\u0302(w)", "text": "Pegasos (Shalev-Shwartz et al., 2011) is a SGD method optimizing the regularized scalarization of Problem 2.2. Alternatively, one can perform SGD on L\u0302(w) subject to the constraint that \u2016w\u2016 \u2264 B, yielding similar learning guarantees (e.g. (Zhang, 2004)). At each iteration, these algorithms pick an example uniformly at random from the training set. If the margin constraint is violated on the example, w is updated by adding to it a scaled version of yi\u03a6(xi). Then, w is scaled and possibly projected back to \u2016w\u2016 \u2264 B. The actual update performed at each iteration is thus very similar to that of the SBP. The main difference is that in Pegasos and related SGD approaches, examples are picked uniformly at random, unlike the SBP which samples from the set of violating examples.\nIn a linear SVM, where \u03a6(xi) \u2208 Rd are given explicitly, each Pegasos (or SGD on L\u0302(w)) iteration is extremely simple and requires runtime which is linear in the dimensionality of \u03a6(xi). A SBP update would require calculating and referring to all O(n) responses. However, with access only to kernel evaluations, even a Pegasos-type update requires either considering all support vectors, or alternatively updating all responses, and might also take O(n) time, just like the much \u201csmarter\u201d SBP step.\nTo understand the learning runtime of such methods in the kernel setting, recall that SGD converges to an \u01eb-accurate solution of the optimization problem after at most \u2016u\u20162 /\u01eb2 iterations. Therefore, the overall runtime is n \u2016u\u20162 /\u01eb2. Combining this with Equation 3.5 yields that the runtime requires by SGD to achieve a learning accuracy of \u01eb is O\u0303 ( ((L\u2217 + \u01eb)/\u01eb) \u2016u\u20164 /\u01eb3 ) . When \u01eb = \u2126(L\u2217), this scales as 1/\u01eb3 compared with the 1/\u01eb scaling for the SBP (see also Table 1)."}, {"heading": "4.3. Dual Decomposition Methods", "text": "Many of the most popular packages for optimizing kernel SVMs, including LIBSVM (Chang & Lin, 2001) and SVM-Light (Joachims, 1998), use dualdecomposition approaches. This family of algorithms\nworks on the dual of the scalarization 2.2, given by:\nmax \u03b1\u2208[0, 1\u03bbn ] n\nn \u2211\ni=1\n\u03b1i \u2212 1\n2\nn \u2211\ni,j=1\n\u03b1i\u03b1jyiyjK(xi, xj) (4.1)\nand proceed by iteratively choosing a small working set of dual variables \u03b1i, and then optimizing over these variables while holding all other dual variables fixed. At an extreme, SMO (Platt, 1998) uses a working set of the smallest possible size (two in problems with an unregularized bias, one in problems without). Most dual decomposition approaches rely on having access to all the responses ci (as in the SBP), and employ some heuristic to select variables \u03b1i that are likely to enable a significant increase in the dual objective.\nOn an objective without an unregularized bias the structure of SMO is similar to the SBP: the responses ci are used to choose a single point j in the training set, then \u03b1j is updated, and finally the responses are updated accordingly. There are two important differences, though: how the training example to update is chosen, and how the change in \u03b1j is performed.\nSMO updates \u03b1j so as to exactly optimize the dual Problem 4.1, while the SBP takes a step along \u03b1j so as to improve the primal Problem 2.3. Dual feasibility is not maintained, so the SBP has more freedom to use large coefficients on a few support vectors, potentially resulting in sparser solutions.\nThe use of heuristics to choose the training example to update makes SMO very difficult to analyze. Although it is known to converge linearly after some number of iterations (Chen et al., 2006), the number of iterations required to reach this phase can be very large (see a detailed discussion in Appendix E). To the best of our knowledge, the most satisfying analysis for a dual decomposition method is the one given in Hush et al. (2006). In terms of learning runtime, this analysis yields a runtime of O\u0303 ( ((L(u) + \u01eb) /\u01eb)2 \u2016u\u20164 /\u01eb2 ) to guarantee L0/1(w) \u2264 L(u) + \u01eb. When \u01eb = \u2126(L\u2217), this runtime scales as 1/\u01eb2, compared with the 1/\u01eb guarantee for the SBP."}, {"heading": "4.4. Stochastic Dual Coordinate Ascent", "text": "Another variant of the dual decomposition approach is to choose a single \u03b1i randomly at each iteration and update it so as to optimize Equation 4.1 (Hsieh et al., 2008). The advantage here is that we do not need to use all of the responses at each iteration, so that if it is easy to calculate responses on-demand, as in the case of linear SVMs, each SDCA iteration can be calculated in time O(d) (Hsieh et al., 2008). In a sense, SDCA relates to SMO in a similar fashion that Pegasos re-\nlates to the SBP: SDCA and Pegasos are preferable on linear SVMs since they choose working points at random; SMO and the SBP choose working points based on more information (namely, the responses), which are unnecessarily expensive to compute in the linear case, but, as discussed earlier, are essentially \u201cfree\u201d in kernelized implementations. Pegasos and the SBP both work on the primal (though on different scalarizations), while SMO and SDCA work on the dual and maintain dual feasibility.\nThe current best analysis of the runtime of SDCA is not satisfying, and yields the bound n/\u03bb\u01eb on the number of iterations, which is a factor of n larger than the bound for Pegasos. Since the cost of each iteration is the same, this yields a significantly worse guarantee. We do not know if a better guarantee can be derived for SDCA. See a detailed discussion in Appendix E."}, {"heading": "4.5. The Online Perceptron", "text": "We have so far considered only the problem of optimizing the bi-criterion SVM objective of Problem 2.1. However, because the online Perceptron achieves the same form of learning guarantee (despite not optimizing the bi-criterion objective), it is reasonable to consider it, as well.\nThe online Perceptron makes a single pass over the training set. At each iteration, if w errs on the point under consideration (i.e. yi \u3008w,\u03a6(xi)\u3009 \u2264 0), then yi\u03a6(xi) is added into w. Let M be the number of mistakes made by the Perceptron on the sequence of examples. Support vectors are added only when a mistake is made, and so each iteration of the Perceptron involves at most M kernel evaluations. The total runtime is therefore Mn.\nWhile the Perceptron is an online learning algorithm, it can also be used for obtaining guarantees on the generalization error using an online-to-batch conversion (e.g. (Cesa-Bianchi et al., 2001)).\nFrom a bound on the number of mistakes M (e.g. Shalev-Shwartz (2007, Corollary 5)), it is possible to show that the expected number of mistakes the Perceptron makes is upper bounded by nL(u)+\u2016u\u2016 \u221a\nnL(u)+ \u2016u\u20162. This implies that the total runtime required by the Perceptron to achieve L0/1(w) \u2264 L(u) + \u01eb is O ( ((L(u) + \u01eb) /\u01eb)3 \u2016u\u20164 /\u01eb ) . This is of the same or-\nder as the bound we have derived for SBP. However, the Perceptron does not converge to a Pareto optimal solution to the bi-criterion Problem 2.1, and therefore cannot be considered a SVM optimization procedure. Furthermore, the online Perceptron generalization analysis relies on an \u201conline-to-batch\u201d conver-\nsion technique (e.g. (Cesa-Bianchi et al., 2001)), and is therefore valid only for a single pass over the data. If we attempt to run the Perceptron for multiple passes, then it might begin to overfit uncontrollably. Although the worst-case theoretical guarantee obtained after a single pass is indeed similar to that for an optimum of the SVM objective, in practice an optimum of the empirical SVM optimization problem does seem to have significantly better generalization performance."}, {"heading": "5. Experiments", "text": "We compared the SBP to other SVM optimization approaches on the datasets in Table 2. We compared to Pegasos (Shalev-Shwartz et al., 2011), SDCA (Hsieh et al., 2008), and SMO (Platt, 1998) with a second order heuristic for working point selection (Fan et al., 2005). These approaches work on the regularized formulation of Problem 2.2 or its dual (Problem 4.1). To enable comparison, the parameter \u03bd for the SBP was derived from \u03bb as \u2016w\u0302\u2217\u2016 \u03bd = 1 n \u2211n i=1 \u2113 (yi \u3008w\u2217,\u03a6 (xi)\u3009), where w\u0302\u2217 is the known (to us) optimum.\nWe first compared the methods on a SVM formulation without an unregularized bias, since Pegasos and SDCA do not naturally handle one. So that this comparison would be implementation-independent, we measure performance in terms of the number of kernel evaluations. As can be seen in Figure 2, the SBP outperforms Pegasos and SDCA, as predicted by the upper bounds. The SMO algorithm has a dramatically different performance profile, in line with the known analysis: it makes relatively little progress, in terms of generalization error, until it reaches a certain critical point, after which it converges rapidly. Unlike the other methods, terminating SMO early in order to obtain a cruder solution does not appear to be advisable.\nWe also compared to the online Perceptron algorithm. Although use of the Perceptron is justified for nonseparable data only if run for a single pass over the training set, we did continue running for multiple passes. The Perceptron\u2019s generalization performance is similar to that of the SBP for the first epoch, but the SBP continues improving over additional passes. As discussed in Section 4.5, the Perceptron is unsafe and might overfit after the first epoch, an effect which is clearly visible on the Adult dataset.\nTo give a sense of actual runtime, we compared our implementation of the SBP1 to the SVM package LIBSVM, running on an Intel E7500 processor. We\n1Source code is available from http://ttic.uchicago.edu/~cotter/projects/SBP\nallowed an unregularized bias (since that is what LIBSVM uses), and used the parameters in Table 2. For these experiments, we replaced the Reuters dataset with the version of the Forest dataset used by Nguyen et al. (2010), using their parameters. LIBSVM converged to a solution with 14.9% error in 195s on Adult, 0.44% in 1980s on MNIST, and 1.8% in 35 hours on Forest. In one-quarter of each of these runtimes, SBP obtained 15.0% error on Adult, 0.46% on MNIST, and 1.6% on Forest. These results of course depend heavily on the specific stopping criterion used."}, {"heading": "6. Summary and Discussion", "text": "The Stochastic Batch Perceptron is a novel approach for training kernelized SVMs. The SBP fares well empirically, and, as summarized in Table 1, our runtime guarantee for the SBP is the best of any existing guarantee for kernelized SVM training. An interesting open question is whether this runtime is optimal, i.e. whether any algorithm relying only on black-box kernel accesses must perform \u2126 ( ((L\u2217 + \u01eb)/\u01eb)3 \u2016u\u20164 /\u01eb )\nkernel evaluations.\nAs with other stochastic gradient methods, deciding when to terminate SBP optimization is an open issue. The most practical approach seems to be to terminate when a holdout error stabilizes. We should note that even for methods where the duality gap can be used (e.g. SMO), this criterion is often too strict, and the use of cruder criteria may improve training time.\nAcknowledgements: S. Shalev-Shwartz is supported by the Israeli Science Foundation grant number 590-10."}, {"heading": "Blum, M., Floyd, R. W., Pratt, V., Rivest, R. L., and", "text": "Tarjan, R. E. Time bounds for selection. JCSS, 7(4): 448\u2013461, August 1973.\nBottou, L. and Bousquet, O. The tradeoffs of large scale learning. In NIPS\u201908, pp. 161\u2013168, 2008.\nCesa-Bianchi, N., Conconi, A., and Gentile, C. On the generalization ability of on-line learning algorithms. IEEE Trans. on Inf. Theory, 50:2050\u20132057, 2001.\nChang, C-C. and Lin, C-J. LIBSVM: a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.\nChen, P-H., Fan, R-E., and Lin, C-J. A study on smo-type decomposition methods for support vector machines. IEEE Transactions on Neural Networks, 17(4):893\u2013908, 2006."}, {"heading": "Collins, M., Globerson, A., Koo, T., Carreras, X., and", "text": "Bartlett, P. Exponentiated gradient algorithms for conditional random fields and max-margin markov net-\nworks. JMLR, 9:1775\u20131822, 2008.\nFan, R-E., Chen, P-S., and Lin, C-J. Working set selection using second order information for training support vector machines. JMLR, 6:1889\u20131918, 2005.\nHazan, E., Koren, T., and Srebro, N. Beating SGD: Learning SVMs in sublinear time. In NIPS\u201911, 2011."}, {"heading": "Hsieh, C-J., Chang, K-W., Lin, C-J., Keerthi, S. S., and", "text": "Sundararajan, S. A dual coordinate descent method for large-scale linear SVM. In ICML\u201908, pp. 408\u2013415, 2008.\nHush, D., Kelly, P., Scovel, C., and Steinwart, I. QP algorithms with guaranteed accuracy and run time for support vector machines. JMLR, 7:733\u2013769, 2006.\nJoachims, T. Making large-scale support vector machine learning practical. In Scho\u0308lkopf, B., Burges, C., and Smola, A. J. (eds.), Advances in Kernel Methods - Support Vector Learning. MIT Press, 1998.\nKakade, S. M. and Tewari, A. On the generalization ability of online strongly convex programming algorithms. In NIPS\u201909, 2009."}, {"heading": "Nguyen, D D, Matsumoto, K., Takishima, Y., and", "text": "Hashimoto, K. Condensed vector machines: learning fast machine for large data. Trans. Neur. Netw., 21(12): 1903\u20131914, Dec 2010.\nPlatt, J. C. Fast training of support vector machines us-\ning Sequential Minimal Optimization. In Scho\u0308lkopf, B., Burges, C., and Smola, A. J. (eds.), Advances in Kernel Methods - Support Vector Learning. MIT Press, 1998.\nRahimi, A. and Recht, B. Random features for large-scale kernel machines. In NIPS\u201907, 2007.\nScovel, C., Hush, D., and Steinwart, I. Approximate duality. JOTA, 2008.\nShalev-Shwartz, S. Online Learning: Theory, Algorithms, and Applications. PhD thesis, The Hebrew University of Jerusalem, July 2007.\nShalev-Shwartz, S. and Srebro, N. SVM optimization: Inverse dependence on training set size. In ICML\u201908, pp. 928\u2013935, 2008."}, {"heading": "Shalev-Shwartz, S., Singer, Y., Srebro, N., and Cotter,", "text": "A. Pegasos: Primal Estimated sub-GrAdient SOlver for SVM. Mathematical Programming, 127(1):3\u201330, March 2011."}, {"heading": "Srebro, N., Sridharan, K., and Tewari, A. Smoothness,", "text": "low-noise and fast rates. In NIPS\u201910, 2010.\nZhang, T. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In ICML\u201904, 2004.\nZinkevich, M. Online convex programming and generalized infinitesimal gradient ascent. In ICML\u201903, 2003."}, {"heading": "A. Additional Experiments", "text": "While our focus in this paper is on optimization of the kernel SVM objective, and not on the broader problem of large-scale learning, one may wonder how well the SBP compares to techniques which accelerate the training of kernel SVMs through approximation. One such is the random Fourier projection algorithm of Rahimi & Recht (2007), which can be used to transform a kernel SVM problem into an approximatelyequivalent linear SVM. The resulting problem may then be optimized using one of the many existing fast linear SVM solvers, such as Pegasos, SDCA or SIMBA. Unlike methods (such as the SBP) which rely only on black-box kernel accesses, Rahimi and Recht\u2019s projection technique can only be applied on a certain class of kernel functions (shift-invariant kernels), of which the Gaussian kernel is a member.\nFor d-dimensional feature vectors, and using a Gaussian kernel with parameter \u03c32, Rahimi and Recht\u2019s approach is to sample v1, . . . , vk \u2208 Rd independently according to vi \u223c N (0, I), and then define the mapping P : Rd \u2192 R2k as:\nP (x)2i = 1\u221a k cos\n(\n1 \u03c3 \u3008vi, x\u3009\n)\nP (x)2i+1 = 1\u221a k sin\n(\n1 \u03c3 \u3008vi, x\u3009\n)\nThen \u3008P (xi),P (xj)\u3009 \u2248 K (xi, xj), with the quality of this approximation improving with increasing k (see Rahimi & Recht (2007, Claim 1) for details).\nNotice that computing each pair of Fourier features requires computing the d-dimensional inner product \u3008v, x\u3009. For comparison, let us write the Gaussian kernel in the following form:\nK (xi, xj) = exp\n(\n\u2212 1 2\u03c32 \u2016xi \u2212 xj\u20162 )\n=exp\n(\n\u2212 1 2\u03c32 ( \u2016xi\u20162 + \u2016xj\u20162 \u2212 2 \u3008xi, xj\u3009 )\n)\nThe norms \u2016xi\u2016 may be cheaply precomputed, so the dominant cost of performing a single Gaussian kernel evaluation is, likewise, that of the d-dimensional inner product \u3008xi, xj\u3009. This observation suggests that the computational cost of the use of Fourier features may be directly compared with that of a kernel-evaluation-based SVM optimizer in terms of d-dimensional inner products. Figure 3 contains such a comparison. In this figure, the computational cost of a 2k-dimensional Fourier linearization is taken to be the cost of computing P (xi) on the entire training set (kn inner products, where n is the\nnumber of training examples)\u2014we ignore the cost of optimizing the resulting linear SVM entirely. The plotted testing error is that of the optimum of the resulting linear SVM problem, which approximates the original kernel SVM. We can see that at least on Reuters and MNIST, the SBP is preferable to (i.e. faster than) approximating the kernel with random Fourier features.\nB. Implementation Details\nWe begin this appendix by providing complete pseudocode, which may be found in Algorithm 1, for the SBP algorithm which we outlined in Section 3.4. This implementation requires that we be able to find a minimax-optimal probability distribution p\u2217 to the objective of Equation 3.1.\nAs was discussed in Section 3.2, in a problem without an unregularized bias, such a probability distribution can be derived from the \u201cwater level\u201d \u03b3, which can be found in O(n) time using Algorithm 2. This algorithm works by subdividing the set of responses into those less than, equal to and greater than a pivot value (if one uses the median, which can be found in linear time using e.g. the median-of-medians algorithm (Blum et al., 1973), then the overall will be linear in n). Then, it calculates the size, minimum and sum of each of these subsets, from which the total volume of the water required to cover the subsets can be easily calculated. It then recurses into the subset containing the point at which a volume of n\u03bd just suffices to cover the responses, and continues until \u03b3 is found.\nIn Section 3.6, we mentioned that a similar result holds for the objective of Equation 3.9, which adds an unregularized bias.\nAs before, finding the water level \u03b3 reduces to finding minimax-optimal values of p\u2217, \u03be\u2217 and b\u2217. The characterization of such solutions is similar to that in the case without an unregularized bias. In particular, for a fixed value of b, we may still think about \u201cpouring water into a basin\u201d, except that the height of the basin is now ci + yib, rather than ci.\nWhen b is not fixed it is easier to think of two basins, one containing the positive examples, and the other the negative examples. These basins will be filled with water of a total volume of n\u03bd, to a common water level \u03b3. The relative heights of the two basins are determined by b: increasing b will raise the basin containing the positive examples, while lowering that containing the negative examples by the same amount. This is illustrated in Figure 4.\nIt remains only to determine what characterizes a\nminimax-optimal value of b. Let k+ and k\u2212 be the number of elements covered by water in the positive and negative basins, respectively, for some b. If k+ > k\u2212, then raising the positive basin and lowering the negative basin by the same amount (i.e. increasing b) will raise the overall water level, showing that b is not optimal. Hence, for an optimal b, water must cover an equal number of indices in each basin. Similar reasoning shows that an optimal p\u2217 must place equal probability mass on each of the two classes.\nOnce more, the resulting problem is amenable to a divide-and-conquer approach. The water level \u03b3 and bias b will be found in O(n) time by Algorithm 3, provided that the partition function chooses the median as the pivot."}, {"heading": "C. Proofs of Lemmas 3.1 and 3.2", "text": "Lemma 3.1. For any w, let p\u2217, \u03be\u2217 be minimax optimal for Equation 3.1. Then\n\u2211n i=1 p \u2217 i yi\u03a6 (xi) is a supergra-\ndient of f(w) at w.\nProof. By the definition of f , for any v \u2208 Rd:\nf (w + v) =\nmax \u03be 0,1T \u03be\u2264n\u03bd min p\u2208\u2206n\nn \u2211\ni=1\npi (yi \u3008w + v,\u03a6 (xi)\u3009+ \u03bei)\nSubstituting the particular value p\u2217 for p can only in-\ncrease the RHS, so:\nf (w + v) \u2264 max \u03be 0,1T \u03be\u2264n\u03bd\nn \u2211\ni=1\np\u2217i (yi \u3008w + v,\u03a6 (xi)\u3009+ \u03bei)\n\u2264 max \u03be 0,1T \u03be\u2264n\u03bd\nn \u2211\ni=1\np\u2217i (yi \u3008w,\u03a6 (xi)\u3009+ \u03bei)\n+\nn \u2211\ni=1\np\u2217i yi \u3008v,\u03a6 (xi)\u3009\nBecause p\u2217 is minimax-optimal at w:\nf (w + v) \u2264f (w) + n \u2211\ni=1\np\u2217i yi \u3008v,\u03a6 (xi)\u3009\n\u2264f (w) + \u2329 v, n \u2211\ni=1\np\u2217i yi\u03a6 (xi)\n\u232a\nSo \u2211n i=1 p \u2217 i yi\u03a6 (xi) is a supergradient of f .\nLemma 3.2. For any T, \u03b4 > 0, after T iterations of the Stochastic Batch Perceptron, with probability at least 1\u2212 \u03b4, the average iterate w\u0304 = 1T \u2211T t=1 w (t) (corresponding to \u03b1\u0304 = 1T \u2211T t=1 \u03b1 (t)), satisfies: f (w\u0304) \u2265 sup\u2016w\u2016\u22641 f (w) \u2212O (\u221a 1 T log 1 \u03b4 ) .\nProof. Define h = \u2212 1r f , where f is as in Equation 3.1. Then the stated update rules constitute an instance of Zinkevich\u2019s algorithm, in which steps are taken in the direction of stochastic subgradients g(t) of h at w(t) =\n\u2211n i=1 \u03b1iyi\u03a6 (xi).\nThe claimed result follows directly from Zinkevich (2003, Theorem 1) combined with an online-to-batch conversion analysis in the style of Cesa-Bianchi et al. (2001, Lemma 1).\nAlgorithm 1 Stochastic gradient ascent algorithm for optimizing the kernelized version of Problem 2.3, as described in Section 3.3. Here, ei is the ith standard unit basis vector. The find gamma subroutine finds the \u201cwater level\u201d \u03b3 from the vector of responses c and total volume n\u03bd.\noptimize ( n : N, x1, . . . , xn : R d, y1, . . . , yn : {\u00b11} , T0 : N, T : N, \u03bd : R+,K : Rd \u00d7 Rd \u2192 R+ )\n1 \u03b70 := 1/ \u221a maxi K (xi, xi); 2 \u03b1(0) := 0n; c(0) := 0n; r0 := 0; 3 for t := 1 to T 4 \u03b7t := \u03b70/ \u221a t; 5 \u03b3 := find gamma ( c(t\u22121), n\u03bd ) ; 6 sample i \u223c uniform {\nj : c (t\u22121) j < \u03b3\n}\n;\n7 \u03b1(t) := \u03b1(t\u22121) + \u03b7tei; 8 r2t := r 2 t\u22121 + 2\u03b7tc (t\u22121) i + \u03b7 2 tK (xi, xi); 9 for j = 1 to n\n10 c (t) j := c (t\u22121) j + \u03b7tyiyjK (xi, xj); 11 if (rt > 1) then 12 \u03b1(t) := (1/rt)\u03b1 (t); c(t) := (1/rt) c (t); rt := 1; 13 \u03b1\u0304 := 1T \u2211T t=1 \u03b1 (t); c\u0304 := 1T \u2211T t=1 c\n(t); \u03b3 := find gamma (c\u0304, n\u03bd); 14 return \u03b1\u0304/\u03b3;"}, {"heading": "D. Data-Laden Analyses", "text": "We\u2019ll begin by presenting a bound on the sample size n required to guarantee good generalization performance (in terms of the 0/1 loss) for a classifier which is \u01eb-suboptimal in terms of the empirical hinge loss. The following result, which follows from Srebro et al. (2010, Theorem 1), is a vital building block of the bounds derived in the remainder of this appendix:\nLemma D.1. Consider the expected 0/1 and hinge losses:\nL0/1 (w) = Ex,y [ 1y\u3008w,x\u3009\u22640 ]\nL (w) = Ex,y [max (0, 1\u2212 y \u3008w, x\u3009)]\nLet u be an arbitrary linear classifier, and suppose that we sample a training set of size n, with n given by the following equation, for parameters B \u2265 \u2016u\u2016, \u01eb > 0 and \u03b4 \u2208 (0, 1):\nn = O\u0303\n\n  (L (u) + \u01eb \u01eb )\n( B + \u221a\nlog 1\u03b4\n)2\n+ rB log 1\u03b4\n\u01eb\n\n \n(D.1) where r \u2265 \u2016x\u2016 is an upper bound on the radius of the data. Then, with probability 1\u2212 \u03b4 over the i.i.d. training sample xi, yi : i \u2208 {1, . . . , n}, uniformly for all linear classifiers w satisfying:\n\u2016w\u2016 \u2264 B L\u0302 (w) \u2212 L\u0302 (u) \u2264 \u01eb\nwhere L\u0302 is the empirical hinge loss:\nL\u0302 (w) = 1 n\nn \u2211\ni=1\nmax (0, 1\u2212 yi \u3008w, xi\u3009)\nwe have that:\nL\u0302 (u) \u2264 L (u) + \u01eb L0/1 (w) \u2264 L\u0302 (u) + \u01eb\nand in particular that:\nL0/1 (w) \u2264 L (u) + 2\u01eb\nIn the remainder of this appendix, we will apply the above result to derive generalization bounds on the performance of the various algorithms under consideration, in the data-laden setting."}, {"heading": "D.1. Stochastic Batch Perceptron", "text": "We will here present a more careful derivation of the main result of Section 3.5, bounding the generalization performance of the SBP.\nTheorem D.2. Let u be an arbitrary linear classifier in the RKHS, let \u01eb > 0 be given, and suppose that K (x, x) \u2264 r2 with probability 1. There exist values of the training size n, iteration count T and parameter \u03bd such that Algorithm 1 finds a solution w =\n\u2211n i=1 \u03b1iyi\u03a6 (xi) satisfying:\nL0/1 (w) \u2264 L (u) + \u01eb\nAlgorithm 2 Divide-and-conquer algorithm for finding the \u201cwater level\u201d \u03b3 from an array of responses C and total volume n\u03bd. The partition function chooses a pivot value from the array it receives as an argument (the median would be ideal), places all values less than the pivot at the start of the array, all values greater at the end, and returns the index of the pivot in the resulting array.\nfind gamma (C : Rn, n\u03bd : R) 1 lower := 1; upper := n; 2 lower max := \u2212\u221e; lower sum := 0; 3 while lower < upper 4 while lower < upper; 5 middle := partition(C [lower : upper]); 6 middle max := max (lower max,C [lower : (middle\u2212 1)]); 7 middle sum := lower sum+ \u2211\nC [lower : (middle\u2212 1)]; 8 if middle max \u00b7 (middle\u2212 1)\u2212middle sum \u2265 n\u03bd then 9 upper := middle\u2212 1;\n10 else 11 lower := middle; lower max := middle max; lower sum := middle sum; 12 return (n\u03bd \u2212 lower max \u00b7 (lower \u2212 1) + lower sum) / (lower \u2212 1) + lower max;\nwhere L0/1 and L are the expected 0/1 and hinge losses, respectively, after performing the following number of kernel evaluations:\n#K = O\u0303\n(\n(L (u) + \u01eb \u01eb )3 r3 \u2016u\u20164 \u01eb log2 1 \u03b4\n)\nwith the size of the support set of w (the number nonzero elements in \u03b1) satisfying:\n#S = O\n(\n(L (u) + \u01eb \u01eb\n)2\nr2 \u2016u\u20162 log 1 \u03b4\n)\nthe above statements holding with probability 1\u2212 \u03b4.\nProof. For a training set of size n, where:\nn = O\u0303 ((L (u) + \u01eb \u01eb ) rB2 \u01eb log 1 \u03b4 )\ntaking B = 2 \u2016u\u2016 in Lemma D.1 gives that L\u0302 (u) \u2264 L (u) + \u01eb and L0/1 (w) \u2264 L (u) + 2\u01eb with probability 1\u2212 \u03b4 over the training sample, uniformly for all linear classifiers w such that \u2016w\u2016 \u2264 B and L\u0302 (w)\u2212L\u0302 (u) \u2264 \u01eb, where L\u0302 is the empirical hinge loss. We will now show that these inequalities are satisfied by the result of Algorithm 1. Define:\nw\u0302\u2217 = argmin w:\u2016w\u2016\u2264\u2016u\u2016 L\u0302 (w)\nBecause w\u0302\u2217 is a Pareto optimal solution of the bicriterion objective of Problem 2.1, if we choose the parameter \u03bd to the slack-constrained objective (Problem 2.3) such that \u2016w\u0302\u2217\u2016 \u03bd = L\u0302 (w\u0302\u2217), then the optimum of the slack-constrained objective will be equivalent to\nw\u0302\u2217 (Lemma 2.1). As was discussed in Section 3.5, We will use Lemma 3.2 to find the number of iterations T required to satisfy Equation 3.7 (with u = w\u0302\u2217). This yields that, if we perform T iterations of Algorithm 1, where T satisfies the following:\nT \u2265 O\n\n\n(\nL\u0302 (w\u0302\u2217) + \u01eb \u01eb\n)2\nr2 \u2016w\u0302\u2217\u20162 log 1 \u03b4\n\n (D.2)\nthen the resulting solution w = w\u0304/\u03b3 will satisfy:\n\u2016w\u2016 \u2264 2 \u2016w\u0302\u2217\u2016 L\u0302 (w)\u2212 L\u0302 (w\u0302\u2217) \u2264 \u01eb\nwith probability 1\u2212 \u03b4. That is:\n\u2016w\u2016 \u2264 2 \u2016w\u0302\u2217\u2016 \u2264 B\nand:\nL\u0302 (w) \u2264 L\u0302 (w\u0302\u2217) + \u01eb \u2264 L\u0302 (u) + \u01eb\nThese are precisely the bounds on \u2016w\u2016 and L\u0302 (w) which we determined (at the start of the proof) to be necessary to permit us to apply Lemma D.1. Each of the T iterations requires n kernel evaluations, so the product of the bounds on T and n bounds the number of kernel evaluations (we may express Equation D.2 in terms of L (u) and \u2016u\u2016 instead of L\u0302 (w\u0302\u2217) and \u2016w\u0302\u2217\u2016, since L\u0302 (w\u0302\u2217) \u2264 L\u0302 (u) \u2264 L (u) + \u01eb and \u2016w\u0302\u2217\u2016 \u2264 \u2016u\u2016). Because each iteration will add at most one new element to the support set, the size of the support set is bounded by the number of iterations, T .\nThis discussion has proved that we can achieve suboptimality 2\u01eb with probability 1\u2212 2\u03b4 with the given #K and #S. Because scaling \u01eb and \u03b4 by 1/2 only changes the resulting bounds by constant factors, these results apply equally well for suboptimality \u01eb with probability 1\u2212 \u03b4."}, {"heading": "D.2. Pegasos / SGD on L\u0302", "text": "If w is the result of a call to the Pegasos algorithm (Shalev-Shwartz et al., 2011) without a projection step, then the analysis of Kakade & Tewari (2009, Corollary 7) permits us to bound the suboptimality relative to an arbitrary reference classifier u, with probability 1\u2212 \u03b4, as:\n(\n\u03bb 2 \u2016w\u20162 + L\u0302 (w)\n) \u2212 ( \u03bb\n2 \u2016u\u20162 + L\u0302 (u)\n)\n\u2264 (D.3)\n84r2 logT\n\u03bbT log\n1\n\u03b4\nEquation D.3 implies that, if one performs the following number of iterations, then the resulting solution will be \u01eb/2-suboptimal in the regularized objective, with probability 1\u2212 \u03b4:\nT = O\u0303\n(\n1 \u01eb \u00b7 r\n2\n\u03bb log\n1\n\u03b4\n)\nHere, \u01eb bounds the suboptimality not of the empirical hinge loss, but rather of the regularized objective (hinge loss + regularization). Although the\ndependence on 1/\u01eb is linear, accounting for the \u03bb dependence results in a bound which is not nearly good as the above appears. To see this, we\u2019ll follow Shalev-Shwartz & Srebro (2008) by decomposing the suboptimality in the empirical hinge loss as:\nL\u0302 (w)\u2212 L\u0302 (u) = \u01eb 2 \u2212 \u03bb 2 \u2016w\u20162 + \u03bb 2 \u2016u\u20162\n\u2264 \u01eb 2 + \u03bb 2 \u2016u\u20162\nIn order to have both terms bounded by \u01eb/2, we choose \u03bb = \u01eb/ \u2016u\u20162, which reduces the RHS of the above to \u01eb. Continuing to use this choice of \u03bb, we next decompose the squared norm of w as:\n\u03bb 2 \u2016w\u20162 = \u01eb 2 \u2212 L\u0302 (w) + L\u0302 (u) + \u03bb 2 \u2016u\u20162\n\u2264 \u01eb 2 + L\u0302 (u) + \u03bb 2 \u2016u\u20162\n\u2016w\u20162 \u2264 2 ( L\u0302 (u) + \u01eb \u01eb ) \u2016u\u20162\nHence, we will have that:\n\u2016w\u20162 \u2264 2 ( L\u0302 (u) + \u01eb \u01eb ) \u2016u\u20162 (D.4)\nL\u0302 (w) \u2212 L\u0302 (u) \u2264 \u01eb with probability 1 \u2212 \u03b4, after performing the following number of iterations:\nT = O\u0303\n(\nr2 \u2016u\u20162 \u01eb2 log 1 \u03b4\n)\n(D.5)\nThere are two ways in which we will use this bound on T to find bound on the number of kernel evaluations required to achieve some desired regularization error. The easiest is to note that the bound of Equation D.5 exceeds that of Lemma D.1, so that if we take T = n, then with high probability, we\u2019ll achieve generalization error 2\u01eb after Tn = T 2 kernel evaluations:\n#K = O\u0303\n(\nr4 \u2016u\u20164 \u01eb4 log2 1 \u03b4\n)\n(D.6)\nBecause we take the number of iterations to be precisely the same as the number of training examples, this is essentially the online stochastic setting.\nAlternatively, we may combine our bound on T with Lemma D.1. This yields the following bound on the generalization error of Pegasos in the data-laden batch setting.\nTheorem D.3. Let u be an arbitrary linear classifier in the RKHS, let \u01eb > 0 be given, and suppose that K (x, x) \u2264 r2 with probability 1. There exist values of the training size n, iteration count T and parameter \u03bd such that kernelized Pegasos finds a solution w = \u2211n\ni=1 \u03b1iyi\u03a6 (xi) satisfying:\nL0/1 (w) \u2264 L (u) + \u01eb\nwhere L0/1 and L are the expected 0/1 and hinge losses, respectively, after performing the following number of kernel evaluations:\n#K = O\u0303\n(\n(L (u) + \u01eb \u01eb )2 r3 \u2016u\u20164 \u01eb3 log2 1 \u03b4\n)\nwith the size of the support set of w (the number nonzero elements in \u03b1) satisfying:\n#S = O\u0303\n(\nr2 \u2016u\u20162 \u01eb2 log 1 \u03b4\n)\nthe above statements holding with probability 1\u2212 \u03b4.\nProof. Same proof technique as in Theorem D.2.\nBecause of the extra term in the bound on \u2016w\u2016 in Equation D.4, theorem D.3 gives a bound which is worse by a factor of (L (u) + \u01eb) /\u01eb than what we might have hoped to recover. When \u01eb \u226a L (u), this extra factor results in the bound going as 1/\u01eb5 rather than 1/\u01eb4. We need to use Equation D.6 to get a 1/\u01eb4 bound in this case.\nAlthough this bound on the generalization performance of Pegasos is not quite what we expected, for\nthe related algorithm which performs SGD on the following objective:\nmin w\u2208Rd\n1\nn\nn \u2211\ni=1\n\u2113 (yi \u3008w,\u03a6 (xi)\u3009)\nsubject to: \u2016w\u20162 \u2264 B2\nthe same proof technique yields the desired bound (i.e. without the extra (L (u) + \u01eb) /\u01eb factor). This is the origin of the \u201cSGD on L\u0302\u201d row in Table 1."}, {"heading": "D.3. Perceptron", "text": "Analysis of the venerable online Perceptron algorithm is typically presented as a bound on the number of mistakes made by the algorithm in terms of the hinge loss of the best classifier\u2014this is precisely the form which we consider in this document, despite the fact that the online Perceptron does not optimize any scalarization of the bi-criterion SVM objective of Problem 2.1. Interestingly, the performance of the Perceptron matches that of the SBP, as is shown in the following theorem:\nTheorem D.4. Let u be an arbitrary linear classifier in the RKHS, let \u01eb > 0 be given, and suppose that K (x, x) \u2264 r2 with probability 1. There exists a value of the training size n such that when the Perceptron algorithm is run for a single \u201cpass\u201d over the dataset, the result is a solution w =\n\u2211n i=1 \u03b1iyi\u03a6 (xi) satisfying:\nL0/1 (w) \u2264 L (u) + \u01eb\nwhere L0/1 and L are the expected 0/1 and hinge losses, respectively, after performing the following number of kernel evaluations:\n#K = O\u0303\n(\n(L (u) + \u01eb \u01eb )3 r4 \u2016u\u20164 \u01eb 1 \u03b4\n)\nwith the size of the support set of w (the number nonzero elements in \u03b1) satisfying:\n#S = O\n(\n(L (u) + \u01eb \u01eb\n)2\nr2 \u2016u\u20162 1 \u03b4\n)\nthe above statements holding with probability 1\u2212 \u03b4.\nProof. If we run the online Perceptron algorithm for a single pass over the dataset, then Corollary 5 of (Shalev-Shwartz, 2007) gives the following mistake bound, for M being the set of iterations on which a\nmistake is made:\n|M| \u2264 \u2211\ni\u2208M \u2113 (yi \u3008u,\u03a6 (xi)\u3009) (D.7)\n+ r \u2016u\u2016 \u221a \u2211\ni\u2208M \u2113 (yi \u3008u,\u03a6 (xi)\u3009) + r2 \u2016u\u20162\nn \u2211\ni=1\n\u21130/1 (yi \u3008wi,\u03a6 (xi)\u3009) \u2264 n \u2211\ni=1\n\u2113 (yi \u3008u,\u03a6 (xi)\u3009)+\n+ r \u2016u\u2016\n\u221a \u221a \u221a \u221a n \u2211\ni=1\n\u2113 (yi \u3008u,\u03a6 (xi)\u3009) + r2 \u2016u\u20162\nHere, \u2113 is the hinge loss and \u21130/1 is the 0/1 loss. Dividing through by n:\n1\nn\nn \u2211\ni=1\n\u21130/1 (yi \u3008wi,\u03a6 (xi)\u3009) \u2264 1\nn\nn \u2211\ni=1\n\u2113 (yi \u3008u,\u03a6 (xi)\u3009)\n+ r \u2016u\u2016\u221a\nn\n\u221a \u221a \u221a \u221a 1\nn\nn \u2211\ni=1\n\u2113 (yi \u3008u,\u03a6 (xi)\u3009) + r2 \u2016u\u20162\nn\nIf we suppose that the xi, yis are i.i.d., and that w \u223c Unif (w1, . . . , wn) (this is a \u201csampling\u201d online-to-batch conversion), then:\nE [ L0/1 (w) ] \u2264 L (u) + r \u2016u\u2016\u221a n \u221a L (u) + r 2 \u2016u\u20162 n\nHence, the following will be satisfied:\nE [ L0/1 (w) ] \u2264 L (u) + \u01eb (D.8)\nwhen:\nn \u2264 O ( (L (u) + \u01eb \u01eb ) r2 \u2016u\u20162 \u01eb )\nThe expectation is taken over the random sampling of w. The number of kernel evaluations performed by the ith iteration of the Perceptron will be equal to the number of mistakes made before iteration i. This quantity is upper bounded by the total number of mistakes made over n iterations, which is given by the\nmistake bound of equation D.7:\n|M| \u2264nL (u) + r \u2016u\u2016 \u221a nL (u) + r2 \u2016u\u20162\n\u2264O (( 1\n\u01eb (L (u) + \u01eb \u01eb ) L (u)\n+\n\u221a\n1\n\u01eb (L (u) + \u01eb \u01eb ) L (u) + 1 ) r2 \u2016u\u20162 )\n\u2264O (( (L (u) + \u01eb \u01eb )2 \u2212 (L (u) + \u01eb \u01eb )\n+\n\u221a\n(L (u) + \u01eb \u01eb\n)2\n\u2212 (L (u) + \u01eb\n\u01eb\n)\n+ 1\n\n\n\u00b7r2 \u2016u\u20162 )\n\u2264O ( (L (u) + \u01eb \u01eb )2 r2 \u2016u\u20162 )\nThe number of mistakes |M| is necessarily equal to the size of the support set of the resulting classifier. Substituting this bound into the number of iterations:\n#K =n |M|\n\u2264O ( (L (u) + \u01eb \u01eb )3 r4 \u2016u\u20164 \u01eb )\nThis holds in expectation, but we can turn this into a high-probability result using Markov\u2019s inequality, resulting in in a \u03b4-dependence of 1\u03b4 .\nAlthough this result has a \u03b4-dependence of 1/\u03b4, this is merely a relic of the simple online-to-batch conversion which we use in the analysis. Using a more complex algorithm (e.g. Cesa-Bianchi et al. (2001)) would likely improve this term to log 1\u03b4 ."}, {"heading": "E. Convergence rates of dual optimization methods", "text": "In this section we discuss existing analyses of dual optimization methods. We first underscore possible gaps between dual sub-optimality and primal suboptimality. Therefore, to relate existing analyzes in the literature of the dual sub-optimality, we must find a way to connect between the dual sub-optimality and primal sub-optimality. We do so using a result due to Scovel et al. (2008), and based on this result, we derive convergence rates on the primal sub-optimality.\nThroughout this section, the \u201cSVM problem\u201d is taken to be the regularized objective of Problem 2.2. We\ndenote the primal objective by:\nP (w) = \u03bb 2 \u2016w\u20162 + 1 n\nn \u2211\ni=1\n\u2113(yi \u3008w, xi, )\u3009\nThe dual objective can be written as:\nD(\u03b1) = \u03bb\n\n\nn \u2211\ni=1\n\u03b1i \u2212 1\n2\nn \u2211\ni,j=1\n\u03b1i\u03b1jQij\n\n\nwhere Qij = yiyj \u3008xi, xj\u3009, and the dual constraints are \u03b1 \u2208 [0, 1/(\u03bbn)]n. Finally, by strong duality we have:\nP \u2217 = argmax w\nP (w) = argmax \u03b1\u2208[0,1/(\u03bbn)]n\nD(\u03b1) = D\u2217"}, {"heading": "E.1. Dual gap vs. Primal gap", "text": "Several authors analyzed the convergence rate of dual optimization algorithms. For example, Hsieh et al. (2008); Collins et al. (2008) analyzed the convergence rate of SDCA and Chen et al. (2006) analyzed the convergence rate of SMO-type dual decomposition methods. In both cases, the number of iterations required so that the dual sub-optimality will be at most \u01eb is analyzed. This is not satisfactory since our goal is to understand how many iterations are required to achieve a primal sub-optimality of at most \u01eb. Indeed, the following lemma shows that a guarantee on a small dual sub-optimality might yield a trivial guarantee on the primal sub-optimality.\nLemma E.1. For every \u01eb > 0, there exists a SVM problem with a dual solution \u03b1 that is \u01eb-accurate, while the corresponding primal solution, w = \u2211\ni \u03b1iyixi, is at least (1 \u2212 \u01eb) sub-optimal. Furthermore, the distribution is such that there exists u with \u2016u\u2016 = 1 and L(u) = 0, while L(w) = 1 and L0,1(w) = 1/2.\nProof. Fix some u with \u2016u\u2016 = 1 and choose any distribution such that L(u) = 0. Take a sample of size n from this distribution. A reasonable choice for the regularization parameter of SVM in this case is to set \u03bb = 2\u01eb. We have: P \u2217 \u2264 P (u) = \u03bb2 \u2016u\u20162 = \u01eb. Now, for \u03b1 = 0 we have D\u2217 \u2212 D(\u03b1) = P \u2217 \u2212 0 \u2264 \u01eb. Therefore, the dual sub-optimality of \u03b1 = 0 is at most \u01eb. On the other hand, the corresponding primal solution is w = 0, which gives P (0) \u2212 P \u2217 = 1 \u2212 P \u2217 \u2265 1 \u2212 \u01eb. Furthermore, L(0) = 1 and L0,1(0) = 1/2, assuming that we break ties at random.\nIn an attempt to connect between dual and primal sub-optimality, Scovel et al. (2008) derived approximate duality theorems. This was used by Hush et al. (2006, Theorem 2) to show the following:\nTheorem E.2. (Hush et al., 2006, Theorem 2) To achieve \u01ebp sub-optimality in the primal, it suffices to require a sub-optimality in the dual of \u01eb \u2264 \u03bb \u01eb 2 p\n118 .\nThere is no contradiction to Lemma E.1 above since in the proof of the lemma we set \u03bb = 2\u01eb, which yields \u01ebp \u2265 1.\nE.2. Analyzing the primal sub-optimality of dual methods\nChen et al. (2006) derived the linear convergence of SMO-type algorithms. However, the analysis takes the following form:\nThere are c < 1 and k\u0304, such that for all k \u2265 k\u0304 it holds that D(\u03b1(k+1)) \u2212D\u2217 \u2264 c(D(\u03b1(k)) \u2212 D\u2217).\nIn the above, \u03b1(k) is the dual solution after performing k iterations, and D\u2217 is the optimal dual solution.\nThis type of analysis is not satisfactory since k\u0304 can be extremely large and c can be extremely close to 1. As an extreme example, suppose that k\u0304 is exponential in n. Then, in any practical implementation of the method, we will never reach the regime in which the linear convergence result holds. As a less extreme example, suppose that k\u0304 \u2265 n2. It follows that we might need to calculate the entire Gram matrix before the linear convergence analysis kicks in. To make more satisfactory statements, we therefore seek convergence analyses which demonstrate good performance not only asymptotically, but also for reasonably small values of k.\nHush et al. (2006) combined explicit convergence rate analysis of the dual sub-optimality of certain decomposition methods with Theorem E.2. The end result is an algorithm with a bound of O(n) on the number of dual iterations, and a total number of kernel evaluations at training time of O(n2). It also follows that the number of support vectors can be order of n.\nHsieh et al. (2008) analyzed the convergence rate of SDCA and derived a bound on the duality suboptimality after performing T iterations. Translating their results to our notation and ignoring low order terms we obtain:\n\u01ebD \u2264 n\nT + n\n( (\u03bb/2)\u2016\u03b1\u2217\u20162 + P \u2217 ) .\nwhere \u03b1\u2217 is such that w\u2217 = \u2211 i \u03b1 \u2217 i yixi. Combining this with Theorem E.2 yields that the number of iterations, according to this analysis, should be at least\nT \u2265 \u2126 ( nP \u2217\n\u03bb\u01eb2P\n)\n.\nSo, even if we set \u01ebP = P \u2217 we still need\nT \u2265 \u2126 ( n\n\u03bb\u01ebP\n)\n.\nEach iteration of SDCA cost roughly the same as a single iteration of Pegasos. However, Pegasos needs order of 1/(\u03bb\u01ebP ) iterations, while according to the analysis above, SDCA requires factor of n more iterations. We suspect that this analysis is not tight.\nAlgorithm 3 Divide-and-conquer algorithm for finding the \u201cwater level\u201d \u03b3 and bias b from an array of labels y, array of responses C and total volume n\u03bd, for a problem with an unregularized bias. The partition function is as in Algorithm 2.\nfind gamma and bias (y : {\u00b11}n , C : Rn, n\u03bd : R) 1 C+ := {C[i] : y[i] = +1}; n+ := |C+|; lower+ := 1; upper+ := n+; lower max+ := \u2212\u221e; lower sum+ := 0; 2 C\u2212 := {C[i] : y[i] = \u22121}; n\u2212 := |C\u2212|; lower\u2212 := 1; upper\u2212 := n\u2212; lower max\u2212 := \u2212\u221e; lower sum\u2212 := 0; 3 middle+ := partition(C+ [lower+ : upper+]); 4 middle\u2212 := partition(C\u2212 [lower\u2212 : upper\u2212]); 5 middle max+ := max (C [lower+ : (middle+ \u2212 1)]); middle sum+ := \u2211C [lower+ : (middle+ \u2212 1)]; 6 middle max\u2212 := max (C [lower\u2212 : (middle\u2212 \u2212 1)]); middle sum\u2212 := \u2211C [lower\u2212 : (middle\u2212 \u2212 1)]; 7 while (lower+ < upper+) or (lower\u2212 < upper\u2212) 8 direction+ := 0; direction\u2212 := 0; 9 if middle+ < lower\u2212 then direction+ = 1;\n10 else if middle+ > upper\u2212 then direction+ = \u22121; 11 if middle\u2212 < lower+ then direction\u2212 = 1; 12 else if middle\u2212 > upper+ then direction\u2212 = \u22121; 13 if direction+ = direction\u2212 = 0 then 14 volume+ := middle max+ \u00b7 (middle+ \u2212 1)\u2212middle sum+; 15 volume\u2212 := middle max\u2212 \u00b7 (middle\u2212 \u2212 1)\u2212middle sum\u2212; 16 if volume+ + volume\u2212 \u2265 n\u03bd then 17 if middle+ > middle\u2212 then direction+ = \u22121; 18 else if middle\u2212 > middle+ then direction\u2212 = \u22121; 19 else if upper+ \u2212 lower+ > upper\u2212 \u2212 lower\u2212 then direction+ = \u22121; 20 else direction\u2212 = \u22121; 21 else 22 if middle+ < middle\u2212 then direction+ = 1; 23 else if middle\u2212 < middle+ then direction\u2212 = 1; 24 else if upper+ \u2212 lower+ > upper\u2212 \u2212 lower\u2212 then direction+ = 1; 25 else direction\u2212 = 1; 26 if direction+ 6= 0 then 27 if direction+ > 0 then upper+ := middle+ \u2212 1; 28 else lower+ := middle+; lower max+ := middle max+; lower sum+ := middle sum+; 29 middle+ := partition(C+ [lower+ : upper+]); 30 middle max+ := max (lower max+, C [lower+ : (middle+ \u2212 1)]); 31 middle sum+ := lower sum+ + \u2211\nC [lower+ : (middle+ \u2212 1)]; 32 if direction\u2212 6= 0 then 33 if direction\u2212 > 0 then upper\u2212 := middle\u2212 \u2212 1; 34 else lower\u2212 := middle\u2212; lower max\u2212 := middle max\u2212; lower sum\u2212 := middle sum\u2212; 35 middle\u2212 := partition(C\u2212 [lower\u2212 : upper\u2212]); 36 middle max\u2212 := max (lower max\u2212, C [lower\u2212 : (middle\u2212 \u2212 1)]); 37 middle sum\u2212 := lower sum\u2212 + \u2211\nC [lower\u2212 : (middle\u2212 \u2212 1)]; 38 // at this point lower+ = lower\u2212 = upper+ = upper\u2212 39 \u2206\u03b3 := (n\u03bd + lower sum+ + lower sum\u2212) / (lower+ \u2212 1)\u2212 lower max+ \u2212 lower max\u2212; 40 if lower+ < n+ then \u2206\u03b3+ := min (\u2206\u03b3, C+[lower+]\u2212 lower max+) else \u2206\u03b3+ := \u2206\u03b3; 41 if lower\u2212 < n\u2212 then \u2206\u03b3\u2212 := min (\u2206\u03b3, C\u2212[lower\u2212]\u2212 lower max\u2212) else \u2206\u03b3\u2212 := \u2206\u03b3; 42 \u03b3+ := lower max+ + 0.5 \u00b7 (\u2206\u03b3 +\u2206\u03b3+ \u2212\u2206\u03b3\u2212); \u03b3\u2212 := lower max\u2212 + 0.5 \u00b7 (\u2206\u03b3 \u2212\u2206\u03b3+ +\u2206\u03b3\u2212); 43 \u03b3 := 0.5 \u00b7 (\u03b3+ + \u03b3\u2212); b := 0.5 \u00b7 (\u03b3\u2212 \u2212 \u03b3+); 44 return (\u03b3, b);"}], "references": [{"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "In NIPS\u201908,", "citeRegEx": "Bottou and Bousquet,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet", "year": 2008}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "IEEE Trans. on Inf. Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2001}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C-C", "Lin", "C-J"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Working set selection using second order information for training support vector machines", "author": ["Fan", "R-E", "Chen", "P-S", "Lin", "C-J"], "venue": null, "citeRegEx": "Fan et al\\.,? \\Q1889\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1889}, {"title": "Beating SGD: Learning SVMs in sublinear time", "author": ["E. Hazan", "T. Koren", "N. Srebro"], "venue": "In NIPS\u201911,", "citeRegEx": "Hazan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2011}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["Hsieh", "C-J", "Chang", "K-W", "Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "In ICML\u201908,", "citeRegEx": "Hsieh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsieh et al\\.", "year": 2008}, {"title": "QP algorithms with guaranteed accuracy and run time for support vector machines", "author": ["D. Hush", "P. Kelly", "C. Scovel", "I. Steinwart"], "venue": null, "citeRegEx": "Hush et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hush et al\\.", "year": 2006}, {"title": "Making large-scale support vector machine learning practical", "author": ["T. Joachims"], "venue": null, "citeRegEx": "Joachims,? \\Q1998\\E", "shortCiteRegEx": "Joachims", "year": 1998}, {"title": "On the generalization ability of online strongly convex programming algorithms", "author": ["S.M. Kakade", "A. Tewari"], "venue": "In NIPS\u201909,", "citeRegEx": "Kakade and Tewari,? \\Q2009\\E", "shortCiteRegEx": "Kakade and Tewari", "year": 2009}, {"title": "Condensed vector machines: learning fast machine for large data", "author": ["D D Nguyen", "K. Matsumoto", "Y. Takishima", "K. Hashimoto"], "venue": "Trans. Neur. Netw.,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS\u201907,", "citeRegEx": "Rahimi and Recht,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht", "year": 2007}, {"title": "Online Learning: Theory, Algorithms, and Applications", "author": ["S. Shalev-Shwartz"], "venue": "PhD thesis,", "citeRegEx": "Shalev.Shwartz,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz", "year": 2007}, {"title": "SVM optimization: Inverse dependence on training set size", "author": ["S. Shalev-Shwartz", "N. Srebro"], "venue": "In ICML\u201908,", "citeRegEx": "Shalev.Shwartz and Srebro,? \\Q2008\\E", "shortCiteRegEx": "Shalev.Shwartz and Srebro", "year": 2008}, {"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter"], "venue": "Mathematical Programming,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2011}, {"title": "Smoothness, low-noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In NIPS\u201910,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["T. Zhang"], "venue": "In ICML\u201904,", "citeRegEx": "Zhang,? \\Q2004\\E", "shortCiteRegEx": "Zhang", "year": 2004}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML\u201903,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}, {"title": "Dual gap vs. Primal gap Several authors analyzed the convergence rate of dual optimization algorithms", "author": ["D(\u03b1) = D\u2217 E"], "venue": "For example, Hsieh et al", "citeRegEx": "E.1.,? \\Q2008\\E", "shortCiteRegEx": "E.1.", "year": 2008}, {"title": "analyzed the convergence rate of SMO-type dual decomposition methods. In both cases, the number of iterations required so that the dual sub-optimality will be at most \u01eb is analyzed", "author": ["SDCA", "Chen"], "venue": null, "citeRegEx": "SDCA and Chen,? \\Q2006\\E", "shortCiteRegEx": "SDCA and Chen", "year": 2006}, {"title": "combined explicit convergence rate analysis of the dual sub-optimality of certain decomposition methods with Theorem E.2. The end result is an algorithm with a bound of O(n) on the number", "author": ["k. Hush"], "venue": null, "citeRegEx": "Hush,? \\Q2006\\E", "shortCiteRegEx": "Hush", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "The SBP is fundamentally different from Pegasos (Shalev-Shwartz et al., 2011) and other stochastic gradient approaches to the problem of training SVMs, in", "startOffset": 48, "endOffset": 77}, {"referenceID": 4, "context": "In particular, we use the \u201cslack constrained\u201d scalarized optimization problem introduced by Hazan et al. (2011) where we seek to maximize the classification margin, subject to a constraint on the total amount of \u201cslack\u201d, i.", "startOffset": 92, "endOffset": 112}, {"referenceID": 11, "context": "In order to compare the SBP runtime to the runtime of other SVM optimization algorithms, which typically work on different scalarizations of the bi-criterion problem, we follow Bottou & Bousquet (2008); Shalev-Shwartz & Srebro (2008) and compare the runtimes required to ensure a generalization error of L\u2217 + \u01eb, assuming the existence of some unknown predictor u with norm \u2016u\u2016 and expected hinge loss L\u2217.", "startOffset": 203, "endOffset": 234}, {"referenceID": 4, "context": "We instead consider the \u201cslack constrained\u201d scalarization (Hazan et al., 2011), where we maximize the \u201cmargin\u201d subject to a constraint of \u03bd on the total allowed \u201cslack\u201d, corresponding to the average error.", "startOffset": 58, "endOffset": 78}, {"referenceID": 10, "context": "2, we follow Bottou & Bousquet (2008); Shalev-Shwartz & Srebro (2008), and analyze the runtime required to achieve a desired generalization performance, instead of that to achieve a certain optimization accuracy on the empirical optimization problem.", "startOffset": 39, "endOffset": 70}, {"referenceID": 4, "context": "Following Hazan et al. (2011), and based on the generalization guarantees of Srebro et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 4, "context": "Following Hazan et al. (2011), and based on the generalization guarantees of Srebro et al. (2010), using a sample of size:", "startOffset": 10, "endOffset": 98}, {"referenceID": 4, "context": "SIMBA Recently, Hazan et al. (2011) presented SIMBA, a method for training linear SVMs based on the same \u201cslack constrained\u201d scalarization (Problem 2.", "startOffset": 16, "endOffset": 36}, {"referenceID": 13, "context": "Pegasos and SGD on L\u0302(w) Pegasos (Shalev-Shwartz et al., 2011) is a SGD method optimizing the regularized scalarization of Problem 2.", "startOffset": 33, "endOffset": 62}, {"referenceID": 15, "context": "(Zhang, 2004)).", "startOffset": 0, "endOffset": 13}, {"referenceID": 7, "context": "Dual Decomposition Methods Many of the most popular packages for optimizing kernel SVMs, including LIBSVM (Chang & Lin, 2001) and SVM-Light (Joachims, 1998), use dualdecomposition approaches.", "startOffset": 140, "endOffset": 156}, {"referenceID": 6, "context": "To the best of our knowledge, the most satisfying analysis for a dual decomposition method is the one given in Hush et al. (2006). In terms of learning runtime, this analysis yields a runtime of \u00d5 (", "startOffset": 111, "endOffset": 130}, {"referenceID": 5, "context": "1 (Hsieh et al., 2008).", "startOffset": 2, "endOffset": 22}, {"referenceID": 5, "context": "The advantage here is that we do not need to use all of the responses at each iteration, so that if it is easy to calculate responses on-demand, as in the case of linear SVMs, each SDCA iteration can be calculated in time O(d) (Hsieh et al., 2008).", "startOffset": 227, "endOffset": 247}, {"referenceID": 1, "context": "(Cesa-Bianchi et al., 2001)).", "startOffset": 0, "endOffset": 27}, {"referenceID": 1, "context": "(Cesa-Bianchi et al., 2001)), and is therefore valid only for a single pass over the data.", "startOffset": 0, "endOffset": 27}, {"referenceID": 13, "context": "We compared to Pegasos (Shalev-Shwartz et al., 2011), SDCA (Hsieh et al.", "startOffset": 23, "endOffset": 52}, {"referenceID": 5, "context": ", 2011), SDCA (Hsieh et al., 2008), and SMO (Platt, 1998) with a second order heuristic for working point selection (Fan et al.", "startOffset": 14, "endOffset": 34}, {"referenceID": 3, "context": ", 2008), and SMO (Platt, 1998) with a second order heuristic for working point selection (Fan et al., 2005). These approaches work on the regularized formulation of Problem 2.2 or its dual (Problem 4.1). To enable comparison, the parameter \u03bd for the SBP was derived from \u03bb as \u2016\u0175\u2217\u2016 \u03bd = 1 n \u2211n i=1 l (yi \u3008w\u2217,\u03a6 (xi)\u3009), where \u0175\u2217 is the known (to us) optimum. We first compared the methods on a SVM formulation without an unregularized bias, since Pegasos and SDCA do not naturally handle one. So that this comparison would be implementation-independent, we measure performance in terms of the number of kernel evaluations. As can be seen in Figure 2, the SBP outperforms Pegasos and SDCA, as predicted by the upper bounds. The SMO algorithm has a dramatically different performance profile, in line with the known analysis: it makes relatively little progress, in terms of generalization error, until it reaches a certain critical point, after which it converges rapidly. Unlike the other methods, terminating SMO early in order to obtain a cruder solution does not appear to be advisable. We also compared to the online Perceptron algorithm. Although use of the Perceptron is justified for nonseparable data only if run for a single pass over the training set, we did continue running for multiple passes. The Perceptron\u2019s generalization performance is similar to that of the SBP for the first epoch, but the SBP continues improving over additional passes. As discussed in Section 4.5, the Perceptron is unsafe and might overfit after the first epoch, an effect which is clearly visible on the Adult dataset. To give a sense of actual runtime, we compared our implementation of the SBP to the SVM package LIBSVM, running on an Intel E7500 processor. We Source code is available from http://ttic.uchicago.edu/~cotter/projects/SBP allowed an unregularized bias (since that is what LIBSVM uses), and used the parameters in Table 2. For these experiments, we replaced the Reuters dataset with the version of the Forest dataset used by Nguyen et al. (2010), using their parameters.", "startOffset": 90, "endOffset": 2049}, {"referenceID": 13, "context": "Pegasos / SGD on L\u0302 If w is the result of a call to the Pegasos algorithm (Shalev-Shwartz et al., 2011) without a projection step, then the analysis of Kakade & Tewari (2009, Corollary 7) permits us to bound the suboptimality relative to an arbitrary reference classifier u, with probability 1\u2212 \u03b4, as:", "startOffset": 74, "endOffset": 103}, {"referenceID": 11, "context": "To see this, we\u2019ll follow Shalev-Shwartz & Srebro (2008) by decomposing the suboptimality in the empirical hinge loss as:", "startOffset": 26, "endOffset": 57}, {"referenceID": 11, "context": "If we run the online Perceptron algorithm for a single pass over the dataset, then Corollary 5 of (Shalev-Shwartz, 2007) gives the following mistake bound, for M being the set of iterations on which a", "startOffset": 98, "endOffset": 120}, {"referenceID": 1, "context": "Cesa-Bianchi et al. (2001)) would likely improve this term to log 1 \u03b4 .", "startOffset": 0, "endOffset": 27}, {"referenceID": 5, "context": "For example, Hsieh et al. (2008); Collins et al.", "startOffset": 13, "endOffset": 33}, {"referenceID": 5, "context": "For example, Hsieh et al. (2008); Collins et al. (2008) analyzed the convergence rate of SDCA and Chen et al.", "startOffset": 13, "endOffset": 56}, {"referenceID": 5, "context": "For example, Hsieh et al. (2008); Collins et al. (2008) analyzed the convergence rate of SDCA and Chen et al. (2006) analyzed the convergence rate of SMO-type dual decomposition methods.", "startOffset": 13, "endOffset": 117}, {"referenceID": 5, "context": "Hush et al. (2006) combined explicit convergence rate analysis of the dual sub-optimality of certain decomposition methods with Theorem E.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "Hsieh et al. (2008) analyzed the convergence rate of SDCA and derived a bound on the duality suboptimality after performing T iterations.", "startOffset": 0, "endOffset": 20}], "year": 2012, "abstractText": "We present a novel approach for training kernel Support Vector Machines, establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach, and show that our method works well in practice compared to existing alternatives.", "creator": "LaTeX with hyperref package"}}}