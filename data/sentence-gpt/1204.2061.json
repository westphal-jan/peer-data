{"id": "1204.2061", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2012", "title": "A Fuzzy Similarity Based Concept Mining Model for Text Classification", "abstract": "Text Classification is a challenging and a red hot field in the current scenario and has great importance in text categorization applications. A lot of research work has been done in this field but there is a need to categorize a collection of text documents into mutually exclusive categories by extracting the concepts or features using supervised learning paradigm and different classification algorithms. In this paper, a new Fuzzy Similarity Based Concept Mining Model (FSCMM) is proposed to classify a set of text documents into pre - defined Category Groups (CG) by providing them training and preparing on the sentence, document and integrated corpora levels along with feature reduction, ambiguity removal on each level to achieve high system performance. As well as generating a full-body data model, this framework is available online and can be downloaded here .\n\nA similar technique has been applied in both the traditional and digital sources of data but also in the Digital Media Library . The method involves filtering the words used in a given sentence using the FSCMM model and replacing the original sentence with a separate category or category, then creating a partial group of data.\nAn alternative approach can be applied in the CFCM algorithm by using a custom classification system and by using a multi-line classification model using a single classification model that has a separate classification and classification level. It can be found in the following sections.\nFigure 1: The FSCMM model with the FSCMM model\nFigure 2: The FSCMM model\nThe classification of a sequence is done using the FSCMM model and using a randomizer, a real-time classifier with the FSCMM model. The FSCMM model is shown in the following table.\nFigure 3: The FSCMM model\nFigure 4: The FSCMM model\nThe classification of a sequence is done using the FSCMM model and using a real-time classifier. It can be found in the following section.\nFigure 5: The FSCMM model\nFigure 6: The FSCMM model\nThe classification of a sequence is done using the FSCMM model and using a randomizer, a real-time classifier with the FSCMM model. The FSCMM model is shown in the following table.\nFigure 7: The FSCMM model\nThe classification of a sequence is done using the FSCMM model and using a randomizer, a real-time classifier with the FSCMM model. The FSCMM model is shown in the following table.\nFigure 8:", "histories": [["v1", "Tue, 10 Apr 2012 07:05:20 GMT  (528kb)", "http://arxiv.org/abs/1204.2061v1", "7 Pages, 3 Figures, 2 Tables, International Journal of Advanced Computer Science and Applications(IJACSA)"]], "COMMENTS": "7 Pages, 3 Figures, 2 Tables, International Journal of Advanced Computer Science and Applications(IJACSA)", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["shalini puri"], "accepted": false, "id": "1204.2061"}, "pdf": {"name": "1204.2061.pdf", "metadata": {"source": "META", "title": "A Fuzzy Similarity Based Concept Mining Model for Text Classification", "authors": ["Shalini Puri"], "emails": [], "sections": [{"heading": null, "text": "115 | P a g e www.ijacsa.thesai.org\nin the current scenario and has great importance in text categorization applications. A lot of research work has been done in this field but there is a need to categorize a collection of text documents into mutually exclusive categories by extracting the concepts or features using supervised learning paradigm and different classification algorithms. In this paper, a new Fuzzy Similarity Based Concept Mining Model (FSCMM) is proposed to classify a set of text documents into pre - defined Category Groups (CG) by providing them training and preparing on the sentence, document and integrated corpora levels along with feature reduction, ambiguity removal on each level to achieve high system performance. Fuzzy Feature Category Similarity Analyzer (FFCSA) is used to analyze each extracted feature of Integrated Corpora Feature Vector (ICFV) with the corresponding categories or classes. This model uses Support Vector Machine Classifier (SVMC) to classify correctly the training data patterns into two groups; i. e., + 1 and \u2013 1, thereby producing accurate and correct results. The proposed model works efficiently and effectively with great performance and high - accuracy results.\nKeywords-Text Classification; Natural Language Processing; Feature Extraction; Concept Mining; Fuzzy Similarity Analyzer; Dimensionality Reduction; Sentence Level; Document Level; Integrated Corpora Level Processing.\nI. INTRODUCTION\nFrom the long time, the discipline of Artificial Intelligence (AI) is growing up on the map of science with psychology and computer science. It is an area of study that embeds the computational techniques and methodologies of intelligence, learning and knowledge [1] to perform complex tasks with great performance and high accuracy. This field is fascinating because of its complementarities of art and science. It contributes to increase the understanding of reasoning, learning and perception. Natural Language Processing (NLP) is the heart of AI and has text classification as an important problem area to process different textual data and documents by finding out their grammatical syntax and semantics and representing them in the fully structured form [1] [2]. AI provides many learning methods and paradigms to represent, interpret and acquire domain knowledge to further help other documents in learning.\nText Mining (TM) is a new, challenging and multidisciplinary area, which includes spheres of knowledge like Computing, Statistics, Predictive, Linguistics and Cognitive Science [3] [4]. TM has been applied in a variety of concerns and applications. Some applications are summary creation, clustering, language identification [5], term extraction [6] and categorization [5] [6], electronic mail management, document management, and market research with an investigation [3] [4] [5].\nTM consists of extracting regularities, patterns, and categorizing text in large volume of texts written in a natural language; therefore, NLP is used to process such text by segmenting it into its specific and constituent parts for further processing [2]. Text segmentation is also an important concern of TM. Many researches go on for the work of text classification [3] [4] [6] [7] [8] just for English language text. Text classification is performed on the textual document sets written in English language, one of the European Language, where words can be simply separated out using many delimiters like comma, space, full stop, etc. Most of the developed techniques work efficiently with European languages where the words, the unit of representation, can be clearly determined by simple tokenization techniques. Such text is referred as segmented text [10]. It does not always happen. There are some Asian Languages in which textual document does not follow word separation schemes and techniques. These languages contain un-segmented text. There are so many other languages in the world like Chinese and Thai Languages in which there is no delimiter to separate out the words. These languages are written as a sequence of characters without explicit word boundary delimiters [10]. So, they use different mechanisms for segmentation and categorization. Here, the proposed effort is to work only on the English text documents.\nTherefore, TM categorization is used to analyze and comprise of large volume of non - structured textual documents. Its purpose is to identify the main concepts in a text document and classifying it to one or more pre-defined categories [3]-[12]. NLP plays an important and vital role to convert unstructured text [4] [5] [6] into the structured one by performing a number of text pre-processing steps. This processing results into the extraction of specific and exclusive\n116 | P a g e www.ijacsa.thesai.org\nconcepts or features as words. These features help in categorizing text documents into classified groups.\nSection II discusses the thematic background and related research work done on the concept mining, feature extraction, and similarity measure. In section III, the proposed model and methodology is discussed in detail. It discusses Text Learning Phase which includes Text Document Training Processor (TDTP), Pseudo Thesaurus, Class Feeder (CF) and Fuzzy Feature Category Similarity Analyzer (FFCSA). Next, Support Vector Machine Classifier (SVMC) and Text Classification Phase are discussed. Finally, section IV concludes the paper with the suggested future work and scope of the paper.\nII. THEMATIC BACKGROUND AND RELATED WORK\nSupervised learning techniques and methodologies for automated text document categorization into known and predefined classes have received much attention in recent years. There are some reasons behind it. Firstly, in the unsupervised learning methods, the document collections have neither predefined classes nor labeled document\u2019s availability. Furthermore, there is a big motivation to uncover hidden category structure in large corpora. Therefore, text classification algorithms are booming up for word based representation of text documents and for text categorization."}, {"heading": "A. Concept Mining", "text": "Concept Mining is used to search or extract the concepts embedded in the text document. These concepts can be either words or phrases and are totally dependent on the semantic structure of the sentence. When a new text document is introduced to the system, the concept mining can detect a concept match from this document to all the previously processed documents in the data set by scanning the new document and extracting the matching concepts [5]. In this way, the similarity measure is used for concept analysis on the sentence, document, and corpus levels.\nThese concepts are originally extracted by the semantic role labeler [5] and analyzed with respect to the sentence, document, and corpus levels. Thus, the matching among these concepts is less likely to be found in non - related documents. If these concepts show matching in unrelated documents, then they produce errors in terms of noise. Therefore, when text document similarity is calculated, the concepts become insensitive to noise."}, {"heading": "B. Feature Extraction", "text": "In text classification, the dimensionality of the feature vector is usually huge. Even more, there is the problem of Curse of Dimensionality, in which the large collection of features takes very much dimension in terms of execution time and storage requirements. This is considered as one of the problems of Vector Space Model (VSM) where all the features are represented as a vector of n - dimensional data. Here, n represents the total number of features of the document. This features set is huge and high dimensional.\nThere are two popular methods for feature reduction: Feature Selection and Feature Extraction. In feature selection methods, a subset of the original feature set is obtained to make the new feature set, which is further used for the text\nclassification tasks with the use of Information Gain [5]. In feature extraction methods, the original feature set is converted into a different reduced feature set by a projecting process. So, the number of features is reduced and overall system performance is improved [6].\nFeature extraction approaches are more effective than feature selection techniques but are more computationally expensive. Therefore, development of scalable and efficient feature extraction algorithms is highly demanded to deal with high-dimensional document feature sets. Both feature reduction approaches are applied before document classification tasks are performed."}, {"heading": "C. Similarity Measure", "text": "In recent years, fuzzy logic [1] [2] [4] [6]-[15] has become an upcoming and demanding field of text classification. It has its strong base of calculating membership degree, fuzzy relations, fuzzy association, fuzzy production rules, fuzzy kmeans, fuzzy c-means and many more concerns. As such, a great research work has been done on the fuzzy similarity and its classifiers for text categorization.\nThe categorizer based on fuzzy similarity methodology is used to create categories with a basis on the similarity of textual terms [4]. It improves the issues of linguistic ambiguities present in the classification of texts. So, it creates the categories through an analysis of the degree of similarity of the text documents that are to be classified. The similarity measure is used to match these documents with pre-defined categories [4] - [15]. Therefore, the document feature matrix is formed to check that a document satisfies how many defined features of the reduced feature set and categorized into which category or class [4] [6] [7]. The fuzzy similarity measure can be used to compute such different matrices.\nIII. THE FUZZY SIMILARITY BASED CONCEPT MINING MODEL (FSCMM)\nIn this section, the proposed Fuzzy Similarity Based Concept Mining Model (FSCMM) is discussed. This model automatically classifies a set of known text documents into a set of category groups. The model shows that how these documents are trained step by step and classified by the Support Vector Machine Classifier (SVMC). SVMC is further used to classify various new and unknown text documents categorically.\nThe proposed model is divided into the two phases: Text Learning Phase (TLP) and Text Classification Phase (TCP). TLP performs the learning function on a given set of text documents. It performs the steps of first stage; i. e., Text Document Training Processor (TDTP) and then the steps of second stage; i. e., Fuzzy Feature Category Similarity Analyzer (FFCSA). The TDTP is used to process the text document by converting it into its small and constituent parts or chunks by using NLP concepts at the Sentence, Document and Integrated Corpora Levels. Then, it searches and stores the desired, important and non-redundant concepts by removing stop words, invalid words and extra words. In the next step, it performs word stemming and feature reduction. The result of sentence level preparation is low dimensional Reduced Feature Vector (RFV). Each RFV of a document is sent for document\n117 | P a g e www.ijacsa.thesai.org\nlevel preparation, so that Integrated Reduced Feature Vector (IRFV) is obtained. To obtain IRFV, all the RFVs are integrated into one. Now, Reduced Feature Frequency Calculator (RFFC) is used to calculate the total frequency of each different word occurred in the document. Finally, all redundant entries of each exclusive word are removed and all the words with their associated frequencies are stored in decreasing order of their frequencies. At the integrated corpora level, the low dimension Integrated Corpora Feature Vector (ICFV) is resulted.\nIn such a way, feature vectors at each level are made low dimensional by processing and updating step by step. Such functionality helps a lot to search the appropriate concepts with reduced vector length to improve system performance and accuracy.\nFFCSA performs similarity measure based analysis for feature pattern (TD \u2013 ICFV) using the enriched fuzzy logic base. The membership degree of each feature is associated with it. Therefore, an analysis is performed between every feature of a text document and class.\nSVMC is used for the categorization of the text documents. It uses the concept of hyper planes to identify the suitable category. Furthermore, SVMC accuracy is checked by providing some new and unknown text documents to be classified into the respective Category Group (CG). This task is performed in TCP.\nThe proposed Fuzzy Similarity Based Concept Mining Model (FSCMM) is shown in Fig. 1. In the next sections, this model and its components are discussed in detail.\n118 | P a g e www.ijacsa.thesai.org"}, {"heading": "A. Text Learning Phase (TLP)", "text": "Consider a set of n text documents,\nTD = {TD1, TD2, TD3,\u2026,TDn} (1)\nWhere TD1, TD2, TD3,\u2026,TDn are the individual and independent text documents which are processed, so that they can be categorized into the required category.\n1) Text Document Training Processor (TDTP) Text Document Training Processor (TDTP) prepares the given text document set TD of n text documents by performing many operations on the sentence, document, and integrated corpora levels. Firstly, each text document TDi, 1 \u2264 i \u2264 n, is processed at its sentence level. The result of such sentence level pre-processing for all the sentences of TDi is integrated into one, which is further processed and refined to make available for the integrated corpora. Integrated corpora accept and integrate all the refined text documents and perform more processing. Its result is sent to FFCSA.\na) At Sentence Level\nThis section describes that how a sentence is pre-processed and finds out the feature vector, each feature\u2019s frequency and finally, the conversion of the Feature Vector (FV) into the Reduced Feature Vector (RFV).\nA text document TDi is composed of a set of sentences, so consider\nTDi = {si1, si2, si3,\u2026,sim} (2)\nWhere i denotes the text document number and m denotes the total number of sentences in TDi. Sentence Extractor (SE) is used to extract the sentence si1 from TDi. Each sentence has its well-defined and non-overlapping boundaries, which makes the sentence extraction a simple task for SE.\nWhen the sentence is extracted, a verb \u2013 argument structure is made for si1. The syntax tree is drawn using the pre-defined syntactic grammar to separate the verbs and the arguments of the sentence. The sentence can be composed of the nouns, proper nouns, prepositions, verbs, adverbs, adjectives, articles, numerals, punctuation and determiners. So, with the construction of the syntax tree, the stop word and other extra terms are removed except the nouns, proper nouns and numerals which are considered as the concepts. To remove the invalid and extra words, the Pseudo Thesaurus is used. It also helps in word stemming.\nThe next step is to make the Feature Vector (FV) of the sentence sij of text document TDi as\nFV = {Fi11, Fi12, Fi13, . . . ,Fi1r} (3)\nWhere 1 \u2264 i \u2264 n, 1 \u2264 j\u2264 m, and r depicts the total number of present features in the sij. Feature Frequency Calculator (FFC) calculates the frequency of each different feature occurred in FV. Frequency represents the number of occurrences of a feature in the sentence. So, each different feature is associated with its frequency in the form of a Feature Frequency pair as <Fijk, freq (Fijk)>, where i is the text document number, j is the sentence number of the sentence, k is the feature number, freq () is a function to calculate the frequency of a feature, and 1 \u2264 k \u2264 r.\nThe next step is to convert the high dimensional FV into low dimensional Reduced Feature Vector (RFV) to reduce the storage and execution time complexities. So, a counter loop is invoked to remove the duplicate or redundant entries of a feature. Therefore, only one instance of each different feature occurred is stored in RFV. It highly reduces the FV dimension and increases the efficiency of the system with good performance. The complete sentence level processing is shown in the Fig. 2.\nSuch sentence level preprocessing for the TDP is performed for each sentence of each document and then progressing toward the integrated corpora.\nb) At Document Level\nThis step accepts the resultant RFV which is sent for document level pre-processing. Firstly, a counter loop is invoked to match the similar features in each of the RFVj with every other RFVq of a TDi where 1 \u2264 j, q \u2264 m, j \u2260 q and 1 \u2264 i \u2264 n. Refined Feature Vector Calculator (RFVC) updates each feature\u2019s frequency for those features which are present two or more times in more than two sentences. These updates are done by adding up their frequencies in terms of combined calculated frequency of that feature only. In this way, it updates the count of each different occurred feature with more than one occurrence. The features that have occurred only once in the document will not update their frequency.\n119 | P a g e www.ijacsa.thesai.org\nThe next step is that all RFVs of a TDi are integrated into one as\nIRFV = Integrat (RFV1, RFV2,\u2026,RFVm) (4)\nWhere Integrat () is a function to combine all RFVs.\nNow each RFVj is compared with every other RFVq where 1 \u2264 j, q \u2264 m, and j \u2260 q. In this way, each feature of RFVj is compared with the every other feature of the RFVq and thereby, the duplicate and redundant features are removed. The complete procedure on the document level is shown in Fig. 3.\nc) At Integrated Corpora\nIn integrated corpora, all the IRFVs of n documents are integrated into one. This step is used to calculate and update the final frequency of each different feature occurred in the corpora. Firstly, it removes duplicate and redundant feature entries of IRFV, and then removes all ambiguous words with the help of the pseudo thesaurus. In such a way, Integrated Corpora Feature Vector (ICFV) is generated. A Threshold Value (TV) is defined for ICFV. TV cuts off those features whose total frequency is less than TV. Finally, it reduces the dimension of ICFV.\nTherefore, Integrated Corpora Feature Vector (ICFV) is constructed as\nICFV = {F1, F2,\u2026,Fn} (5)\nWhere F1, F2,\u2026,Fn represent the features.\nSuch features with their associated frequencies show the statistical similarity between the text documents. Each feature is counted for each document and represented as the feature and its frequency as follows.\nTD1 = {<F1, f (F1)>, <F2, f (F2)>,\u2026,<Fn, f (Fn)>} (6)\nWhere f(Fi) is the function to calculate the frequency of feature Fi in the text document.\nThe next step is to generate the matrix of TD and ICFV in the given form of table 1. In this matrix, the 0 represents the absence of that feature in TD and the numerical value represents the total number of occurrences of the feature in the TD.\n2) Pseudo Thesaurus The Pseudo Thesaurus is a predefined English Vocabulary Set which is used to check the invalid words or to remove extra words from a sentence while processing the sentence in the TDTP. It is also used for word stemming so that the exact word can be obtained. For example, consider three different words for the word research - researching, researcher and researches. When the word stemming is performed, research is the final resulting word with the feature frequency counted as 3.\n3) Class Feeder (CF) Text Classification is the process of assigning the name of the class to a particular input, to which it belongs. The classes, from which the classification procedure can choose, can be described in many ways. So classification is considered as the essential part of many problems solving tasks or recognition tasks. Before classification can be done, the desired number of classes must be defined."}, {"heading": "4) Fuzzy Fetaure Category Similarity Analyzer (FFCSA)", "text": "In FFCSA, firstly the Feature Pattern (FP) is made in the form of the membership degree of each feature with respect to every class. Consider text document set TD of n text documents as per given in equation 1, together with the ICFV F of y features f1, f2,\u2026fy and e classes c1, c2, \u2026, ce. To construct the FP for each feature fk in F, its FP fpi is defined, by\nfpi = < fpi1, fpi2, fpi3,\u2026, fpie> (7)\n= <\u00b5 (fi, c1), \u00b5 (fi, c2), \u00b5 (fi, c3),\u2026,\u00b5 (fi, ce)>\nWhere,\nfi represents the number of occurrences of fi in the text document TDg where 1 \u2264 g \u2264 n.\n\u00b5 (fi, ce) is defined as the sum of product of the feature value of fi present in n text documents TD, w. r. t. a column vector and the 1 or 0 as the presence or absence of that feature in class ce / Sum of the feature value for the class ce only, where 1 \u2264 j \u2264 e. So,\n\u00b5 (fi, ce) \u2211 ( )\n\u2211 ( ) (8)\nbi is represented as\nbi = 1, if document \u0454 class ce (9) = 0, otherwise\nEach text document TD belongs to only one class c. In this way, each class can belong to one or more text documents. A\n120 | P a g e www.ijacsa.thesai.org\nset of n documents and their related categories or classes are represented as an ordered pair as shown\nTD = {<TD1, C (TD1)>, <TD2, C (TD2)>,\u2026,<TDm, C (TDm)>} (10)\nWhere the class of the text document TDi: C(TDi) \u0454 C, C (TD) is a categorization function whose domain is TD and range is C.\nEach document belongs to one of the classes in the C (TD).The resulted text documents that have many features are stored with their relevant classes as shown in the table 2. They are in the form of <doc no, number of occurrences of each feature, class no>.\nIn such a way, the relation between a feature and a class is made. Sometimes, it is quite possible that one document belongs to two or more classes that concern has to be considered by making the more presences of the text document in the table with the cost of increased complexity, so it is required to check each feature\u2019s distribution among them."}, {"heading": "B. Support Vector Machine Classifier (SVMC)", "text": "The next step is to use the Support Vector Machine Classifier (SVMC). SVMC is a popular and better method than other methods for text categorization. It is a kernel method which finds the maximum margin hyper plane in the feature space paradigm separating the data of training patterns into two groups like Boolean Logic 1 and 0. If any training pattern is not correctly classified by the hyper plane, then the concept of slack measure is used to get rid out of it.\nUsing this idea, SVMC can only separate apart two classes for h = +1 and h = -1. For e classes, one SVM for each class is created. For the SVM of class cl, 1 \u2264 l \u2264 e, the training patterns of class cl are for the h = +1 and of other classes are h = -1. The SVMC is then the aggregation of these SVMs.\nSVM provides good results then KNN method because it directly divide the training data according to the hyper planes."}, {"heading": "C. Text Classification Phase", "text": "To check the predictive accuracy of the SVMC, new and unknown text document is used, which is independent of the training text documents and is not used to construct the SVMC. The accuracy of this document is compared with the learned SVMC\u2019s class. If the accuracy of the SVMC is acceptable and good, then it can be used further to classify the future unseen text documents for which the class label is not known.\nTherefore, they can be categorized into the appropriate and a suitable category group.\nIV. CONCLUSION AND FUTURE SCOPE\nText classification is expected to play an important role in future search services or in the text categorization. It is an essential matter to focus on the main subjects and significant content. It is becoming important to have the computational methods that automatically classify available text documents to obtain the categorized information or groups with greater speed and fidelity for the content matter of the texts.\nAs the proposed FSCMM model is made for text document categorization, it works well with high efficiency and effectiveness. Although this model and methodology seem very complex, yet it achieves the task of text categorization with high performance, and good accuracy and prediction. Feature Reduction is performed on the sentence, document and integrated corpora levels to highly reduce feature vector dimension. Such reduction improves the system performance greatly in terms of space and time. Result shows that the feature reduction reduces the space complexity by20%.\nFuzzy similarity measure and methodology are used to make the matching connections among text documents, feature vectors and pre-defined classes. It provides the mathematical framework for finding out the membership degrees as feature frequency.\nThis model shows better results than other text categorization techniques. SVM classifier gives better results than KNN method. The system performance does not show high information gain and prediction results when KNN is used because it produces noise sensitive contents.\nIn the future, such model can be further extended to include the non-segmented text documents. It can also be extended to categorize the images, audio and video - related data."}], "references": [{"title": "Artificial Intelligence, 3  rd ed", "author": ["Eliane Rich", "Kevin Knight", "Shivashankar B Nair"], "venue": "Mc Graw Hill,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Data Mining: Concepts and Techniques, 2  nd ed", "author": ["Jiawei Han", "MicheLine Kamber"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "An Analysis of Constructed Categories for Textual Classification Using Fuzzy Similarity and Agglomerative Hierarchical Methods", "author": ["Marcus Vinicius", "C. Guelpeli", "Ana Cristina", "Bicharra Garcia"], "venue": "Third International IEEE Conference Signal-Image Technologies and Internet- Based System, September 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "An Efficient Concept-Based Mining Model for Enhancing Text Clustering", "author": ["S. Shehata", "F. Karray", "M.S. Kamel"], "venue": "IEEE Transactions On Knowledge And Data Engineering,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A Fuzzy Self- Constructing Feature Clustering Algorithm for Text Classification", "author": ["Jung-Yi Jiang", "Ren-Jia Liou", "Shie-Jue Lee"], "venue": "IEEE Transactions On Knowledge And Data Engineering, Vol. 23, No. 3, March 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Web Document Classification Based on Fuzzy Association", "author": ["Choochart Haruechaiyasak", "Mei-Ling Shyu", "Shu-Ching Chen", "Xiuqi Li"], "venue": "IEEE, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "A Comparative Study of Web-pages Classification Methods using Fuzzy Operators  (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 2, No. 11, 2011 121 | P a g e www.ijacsa.thesai.org  Applied to Arabic Web-pages", "author": ["Ahmad T. Al-Taani", "Noor Aldeen K. Al-Awad"], "venue": "World Academy of Science, Engineering and Technology, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Fuzzy Ontology Generation Model using Fuzzy Clustering for Learning Evaluation", "author": ["Qing YANG", "Wei CHEN", "Bin WEN"], "venue": "2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Exploring The Use of Fuzzy Signature for Text Mining", "author": ["Kok WaiWong", "Todsanai Chumwatana", "Domonkos Tikk"], "venue": "IEEE, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient Fuzzy Rule Generation: A New Approach Using Data Mining Principles and Rule Weighting", "author": ["O. Dehzangi", "M.J. Zolghadri", "S. Taheri", "S.M. Fakhrahmad"], "venue": "Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Facial Emotional Expressions of Life-like Character Based on Text Classifier and Fuzzy Logic", "author": ["Surya Sumpeno", "Mochamad Hariadi", "Mauridhi Hery Purnomo"], "venue": "IAENG International Journal of Computer Science, May, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Classifying Modality Learning Styles based on Production Fuzzy  Rules", "author": ["Rahmah Mokhtar", "Siti Norul Huda Sheikh Abdullah", "Nor Azan Mat Zin"], "venue": "International Conference on Pattern Analysis and Intelligent Robotics, June 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Concept Mining of Semantic Web Services By Means Of Extended Fuzzy Formal Concept Analysis (FFCA)", "author": ["Giuseppe Fenza", "Vincenzo Loia", "Sabrina Senatore"], "venue": "IEEE, Feb. 2008.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Advances in Research of Fuzzy C-means Clustering Algorithm", "author": ["Chen Yanyun", "Qiu Jianlin", "Gu Xiang", "Chen Jianping", "Ji Dan", "Chen Li"], "venue": "International Conference on Network Computing and Information Security, 2011. AUTHORS PROFILE Shalini Puri is pursuing M.Tech in Computer Science at Birla Institute of Technology, Mesra, Ranchi, India. She is currently working as an Assistant Professor in a reputed engineering college in India. Her research areas include Artificial Intelligence, Data Mining, Soft Computing, Graph Theory, and Software Engineering.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Natural Language Processing (NLP) is the heart of AI and has text classification as an important problem area to process different textual data and documents by finding out their grammatical syntax and semantics and representing them in the fully structured form [1] [2].", "startOffset": 267, "endOffset": 270}, {"referenceID": 1, "context": "Text Mining (TM) is a new, challenging and multidisciplinary area, which includes spheres of knowledge like Computing, Statistics, Predictive, Linguistics and Cognitive Science [3] [4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 2, "context": "Text Mining (TM) is a new, challenging and multidisciplinary area, which includes spheres of knowledge like Computing, Statistics, Predictive, Linguistics and Cognitive Science [3] [4].", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "Some applications are summary creation, clustering, language identification [5], term extraction [6] and categorization [5] [6], electronic mail management, document management, and market research with an investigation [3] [4] [5].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "Some applications are summary creation, clustering, language identification [5], term extraction [6] and categorization [5] [6], electronic mail management, document management, and market research with an investigation [3] [4] [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 3, "context": "Some applications are summary creation, clustering, language identification [5], term extraction [6] and categorization [5] [6], electronic mail management, document management, and market research with an investigation [3] [4] [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 4, "context": "Some applications are summary creation, clustering, language identification [5], term extraction [6] and categorization [5] [6], electronic mail management, document management, and market research with an investigation [3] [4] [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Some applications are summary creation, clustering, language identification [5], term extraction [6] and categorization [5] [6], electronic mail management, document management, and market research with an investigation [3] [4] [5].", "startOffset": 220, "endOffset": 223}, {"referenceID": 2, "context": "Some applications are summary creation, clustering, language identification [5], term extraction [6] and categorization [5] [6], electronic mail management, document management, and market research with an investigation [3] [4] [5].", "startOffset": 224, "endOffset": 227}, {"referenceID": 3, "context": "Some applications are summary creation, clustering, language identification [5], term extraction [6] and categorization [5] [6], electronic mail management, document management, and market research with an investigation [3] [4] [5].", "startOffset": 228, "endOffset": 231}, {"referenceID": 0, "context": "TM consists of extracting regularities, patterns, and categorizing text in large volume of texts written in a natural language; therefore, NLP is used to process such text by segmenting it into its specific and constituent parts for further processing [2].", "startOffset": 252, "endOffset": 255}, {"referenceID": 1, "context": "Many researches go on for the work of text classification [3] [4] [6] [7] [8] just for English language text.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Many researches go on for the work of text classification [3] [4] [6] [7] [8] just for English language text.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Many researches go on for the work of text classification [3] [4] [6] [7] [8] just for English language text.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Many researches go on for the work of text classification [3] [4] [6] [7] [8] just for English language text.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "Many researches go on for the work of text classification [3] [4] [6] [7] [8] just for English language text.", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "Such text is referred as segmented text [10].", "startOffset": 40, "endOffset": 44}, {"referenceID": 8, "context": "These languages are written as a sequence of characters without explicit word boundary delimiters [10].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "Its purpose is to identify the main concepts in a text document and classifying it to one or more pre-defined categories [3]-[12].", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "Its purpose is to identify the main concepts in a text document and classifying it to one or more pre-defined categories [3]-[12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 2, "context": "NLP plays an important and vital role to convert unstructured text [4] [5] [6] into the structured one by performing a number of text pre-processing steps.", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "NLP plays an important and vital role to convert unstructured text [4] [5] [6] into the structured one by performing a number of text pre-processing steps.", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "NLP plays an important and vital role to convert unstructured text [4] [5] [6] into the structured one by performing a number of text pre-processing steps.", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "When a new text document is introduced to the system, the concept mining can detect a concept match from this document to all the previously processed documents in the data set by scanning the new document and extracting the matching concepts [5].", "startOffset": 243, "endOffset": 246}, {"referenceID": 3, "context": "These concepts are originally extracted by the semantic role labeler [5] and analyzed with respect to the sentence, document, and corpus levels.", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "In feature selection methods, a subset of the original feature set is obtained to make the new feature set, which is further used for the text classification tasks with the use of Information Gain [5].", "startOffset": 197, "endOffset": 200}, {"referenceID": 4, "context": "So, the number of features is reduced and overall system performance is improved [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 0, "context": "In recent years, fuzzy logic [1] [2] [4] [6]-[15] has become an upcoming and demanding field of text classification.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "In recent years, fuzzy logic [1] [2] [4] [6]-[15] has become an upcoming and demanding field of text classification.", "startOffset": 37, "endOffset": 40}, {"referenceID": 4, "context": "In recent years, fuzzy logic [1] [2] [4] [6]-[15] has become an upcoming and demanding field of text classification.", "startOffset": 41, "endOffset": 44}, {"referenceID": 13, "context": "In recent years, fuzzy logic [1] [2] [4] [6]-[15] has become an upcoming and demanding field of text classification.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "The categorizer based on fuzzy similarity methodology is used to create categories with a basis on the similarity of textual terms [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "The similarity measure is used to match these documents with pre-defined categories [4] - [15].", "startOffset": 84, "endOffset": 87}, {"referenceID": 13, "context": "The similarity measure is used to match these documents with pre-defined categories [4] - [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 2, "context": "Therefore, the document feature matrix is formed to check that a document satisfies how many defined features of the reduced feature set and categorized into which category or class [4] [6] [7].", "startOffset": 182, "endOffset": 185}, {"referenceID": 4, "context": "Therefore, the document feature matrix is formed to check that a document satisfies how many defined features of the reduced feature set and categorized into which category or class [4] [6] [7].", "startOffset": 186, "endOffset": 189}, {"referenceID": 5, "context": "Therefore, the document feature matrix is formed to check that a document satisfies how many defined features of the reduced feature set and categorized into which category or class [4] [6] [7].", "startOffset": 190, "endOffset": 193}], "year": 2011, "abstractText": "Text Classification is a challenging and a red hot field in the current scenario and has great importance in text categorization applications. A lot of research work has been done in this field but there is a need to categorize a collection of text documents into mutually exclusive categories by extracting the concepts or features using supervised learning paradigm and different classification algorithms. In this paper, a new Fuzzy Similarity Based Concept Mining Model (FSCMM) is proposed to classify a set of text documents into pre defined Category Groups (CG) by providing them training and preparing on the sentence, document and integrated corpora levels along with feature reduction, ambiguity removal on each level to achieve high system performance. Fuzzy Feature Category Similarity Analyzer (FFCSA) is used to analyze each extracted feature of Integrated Corpora Feature Vector (ICFV) with the corresponding categories or classes. This model uses Support Vector Machine Classifier (SVMC) to classify correctly the training data patterns into two groups; i. e., + 1 and \u2013 1, thereby producing accurate and correct results. The proposed model works efficiently and effectively with great performance and high accuracy results. Keywords-Text Classification; Natural Language Processing; Feature Extraction; Concept Mining; Fuzzy Similarity Analyzer; Dimensionality Reduction; Sentence Level; Document Level; Integrated Corpora Level Processing.", "creator": "Microsoft Word 2010"}}}