{"id": "1204.3436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2012", "title": "Explaining Adaptation in Genetic Algorithms With Uniform Crossover: The Hyperclimbing Hypothesis", "abstract": "The hyperclimbing hypothesis is a hypothetical explanation for adaptation in genetic algorithms with uniform crossover (UGAs). Hyperclimbing is an intuitive, general-purpose, non-local search heuristic applicable to discrete product spaces with rugged or stochastic cost functions. The strength of this heuristic lie in its insusceptibility to local optima when the cost function is deterministic, and its tolerance for noise when the cost function is stochastic. Hyperclimbing works by decimating a search space, i.e. by iteratively fixing the values of small numbers of variables. The hyperclimbing hypothesis holds that UGAs work by implementing efficient hyperclimbing. Proof of concept for this hypothesis comes from the use of a novel analytic technique involving the exploitation of algorithmic symmetry. We have also obtained experimental results that show that a simple tweak inspired by the hyperclimbing hypothesis dramatically improves the performance of a UGA on large, random instances of MAX-3SAT and the Sherrington Kirkpatrick Spin Glasses problem. The researchers used a non-optimal model of the hyperclimbing hypothesis for the new hyperclimbing theory, which includes a simpler (non-optimal) model for UGA in addition to a more sophisticated approach. In order to investigate the hyperclimbing hypothesis, they measured the total number of UGA cases per row with their fixed (0.4 percent). The results confirm the hypothesis that UGA is a better fit for a non-optimal model than for an optimal model. In a previous paper, we examined the results of several other recent attempts at this problem, including the creation of a hyperclimbing hypothesis based on a recent experiment involving the Hirschmann-Schwartz Hypothesis. It also examined a recent study by a theoretical computer group who used a non-optimal model of the hyperclimbing hypothesis. In the last two years, this paper has provided some preliminary insights into the implementation of hyperclimbing.\n\n\n\nThis paper aims to develop a theoretical approach to the hyperclimbing hypothesis, using the hyperclimbing hypothesis. The idea is that an optimal model of the hyperclimbing hypothesis is a solution to a large-scale problem in which large-scale optimization has already been demonstrated. The authors propose that such a model can be used for applications such as neural networks in a machine learning model.\n\nTo perform a non-optimal simulation of the hyperclimbing", "histories": [["v1", "Mon, 16 Apr 2012 10:53:06 GMT  (625kb,D)", "http://arxiv.org/abs/1204.3436v1", "22 pages, 5 figures"]], "COMMENTS": "22 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["keki m burjorjee"], "accepted": false, "id": "1204.3436"}, "pdf": {"name": "1204.3436.pdf", "metadata": {"source": "CRF", "title": "Explaining Adaptation in Genetic Algorithms With Uniform Crossover: The Hyperclimbing Hypothesis", "authors": ["Keki M. Burjorjee"], "emails": ["kekib@cs.brandeis.edu"], "sections": [{"heading": null, "text": "The hyperclimbing hypothesis is a hypothetical explanation for adaptation in genetic algorithms with uniform crossover (UGAs). Hyperclimbing is an intuitive, general-purpose, non-local search heuristic applicable to discrete product spaces with rugged or stochastic cost functions. The strength of this heuristic lies in its insusceptibility to local optima when the cost function is deterministic, and its tolerance for noise when the cost function is stochastic. Hyperclimbing works by decimating a search space, i.e. by iteratively fixing the values of small numbers of variables. The hyperclimbing hypothesis holds that UGAs work by implementing efficient hyperclimbing. Proof of concept for this hypothesis comes from the use of a novel analytic technique involving the exploitation of algorithmic symmetry. We have also obtained experimental results that show that a simple tweak inspired by the hyperclimbing hypothesis dramatically improves the performance of a UGA on large, random instances of MAX-3SAT and the Sherrington Kirkpatrick Spin Glasses problem."}, {"heading": "1. Introduction", "text": "Over several decades of use in diverse scientific and engineering fields, evolutionary optimization has acquired a reputation for being a kind of universal acid\u2014a general purpose approach that routinely procures useful solutions to optimization problems with rugged, dynamic, and stochastic cost functions over search spaces consisting of strings, vectors, trees, and instances of other kinds of data structures (Fogel, 2006). Remarkably, the means by which evolutionary algorithms work is still the subject of much debate. An abiding mystery of the field is the widely observed utility of genetic algorithms with uniform crossover (Syswerda, 1989; Rudnick et al., 1994; Pelikan, 2008; Huifang and Mo, 2010). The use of uniform crossover (Ackley, 1987; Syswerda, 1989) in genetic algorithms causes genetic loci to be unlinked, i.e. recombine freely. It is generally acknowledged that the adaptive capacity of genetic algorithms with this kind of crossover cannot be explained within the rubric of the building block hypothesis, the reigning explanation for adaptation in genetic algorithms with strong linkage between loci (Goldberg, 2002). Yet, no alternate, scientifically rigorous explanation for adaptation in genetic algorithms with uniform crossover (UGAs) has been proposed. The hyperclimbing hypothesis, presented in this paper, addresses this gap. This hypothesis holds that UGAs perform adaptation by implicitly and efficiently implementing a global search heuristic called hyperclimbing.\nIf the hyperclimbing hypothesis is sound, then the UGA is in good company. Hyperclimbing belongs to a class of heuristics that perform global decimation. Global decimation, it turns out, is the state of the art approach to solving large, hard instances of SAT (Kroc\nar X\niv :1\n20 4.\n34 36\nv1 [\ncs .N\nE ]\n1 6\nA pr\n2 01\n2\net al., 2009). Conventional global decimation strategies\u2014e.g. Survey Propagation (Me\u0301zard et al., 2002), Belief Propagation, Warning Propagation (Braunstein et al., 2002)\u2014use message passing algorithms to obtain statistical information about the space being searched. This information is then used to fix the values of one, or a small number, of search space attributes, effectively reducing the size of the search space. The decimation strategy is then recursively applied to the smaller search space. And so on. Survey Propagation, perhaps the best known global decimation strategy, has been used along with Walksat (Selman et al., 1993) to solve instances of SAT with upwards of a million variables. The hyperclimbing hypothesis holds that in practice, UGAs also perform adaptation by decimating the search spaces to which they are applied. Unlike conventional decimation strategies, however, a UGA obtains statistical information about the search space implicitly, by means other than message passing.\nThe rest of this paper is organized as follows: Section 2 provides an informal description of the hyperclimbing heuristic. A more formal description appears in Section A of the online appendix. Section 3, presents proof of concept, i.e. it describes a stochastic fitness function1 on which a UGA behaves as described in the hyperclimbing hypothesis. Exploiting certain symmetries inherent within uniform crossover and a containing class of fitness functions, we argue that the adaptive capacity of a UGA scales extraordinarily well as the size of the search space increases. We follow up with experimental tests that validate this conclusion. One way for the hyperclimbing hypothesis to gain credibility is by inspiring modifications to the genetic algorithm that improve performance. Section 4 presents the results of experiments that show that a simple tweak called clamping, inspired by the hyperclimbing hypothesis, dramatically improves the performance of a genetic algorithm on large, randomly generated instances of MAX-3SAT, and the Sherrington Kirkpatric Spin Glasses problem. While not conclusive, this validation does lend considerable support to the hyperclimbing hypothesis2. We conclude in Section 5 with a brief discussion of the generalizability of the hyperclimbing hypothesis and its ramifications for Evolutionary Computation and Evolutionary Biology."}, {"heading": "2. The Hyperclimbing Heuristic", "text": "For a sketch of the workings of a hyperclimbing heuristic, consider a search space S = {0, 1}`, and a (possibly stochastic) fitness function that maps points in S to real values. Let us define the order of a schema partition Mitchell (1996) to simply be the order of the schemata that comprise the partition. Clearly then, schema partitions of lower order are coarser than schema partitions of higher order. The effect of a schema partition is defined to be the variance of the expected fitness of the constituent schemata under sampling from the uniform distribution over each schema. So for example, the effect of the schema partition # \u2217 \u2217# \u2217 \u2217 = {0 \u2217 \u22170 \u2217 \u2217, 0 \u2217 \u22171 \u2217 \u2217, 1 \u2217 \u22170 \u2217 \u2217, 1 \u2217 \u22171 \u2217 \u2217} is\n1\n4 1\u2211 i=0 1\u2211 j=0 (F (i \u2217 \u2217j \u2217 \u2217)\u2212 F (\u2217 \u2217 \u2217 \u2217 \u2217\u2217))2\n1. A fitness function is nothing but a cost function with a small twist: the goal is, not to minimize fitness, but to maximize it. 2. Then again, no scientific theory can be conclusively validated. The best one can hope for is pursuasive forms of validation (Popper, 2007b,a).\nwhere the operator F gives the expected fitness of a schema under sampling from the uniform distribution. A hyperclimbing heuristic starts by sampling from the uniform distribution over the entire search space. It subsequently identifies a coarse schema partition with a non-zero effect, and limits future sampling to a schema in this partition with above average expected fitness. In other words the hyperclimbing heuristic fixes the defining bits Mitchell (1996) of this schema in the population. This schema constitutes a new (smaller) search space to which the hyperclimbing heuristic is recursively applied. Crucially, the act of fixing defining bits in a population has the potential to \u201cgenerate\u201d a detectable non-zero effect in a schema partition that previously had a negligible effect. For example, the schema partition \u2217#\u2217\u2217\u2217# can have a negligible effect, while the schema partition 1#\u22170\u2217# has a detectable non-zero effect. A more formal description of the hyperclimbing heuristic can be found in Appendix A.\nAt each step in its progression, hyperclimbing is sensitive, not to the fitness value of any individual point, but to the sampling means of relatively coarse schemata. This heuristic is, therefore, natively able to tackle optimization problems with stochastic cost functions. Considering the intuitive simplicity of hyperclimbing, this heuristic has almost certainly been toyed with by other researchers in the general field of discrete optimization. In all likelihood it was set aside each time because of the seemingly high cost of implementation for all but the smallest of search spaces or the coarsest of schema partitions. Given a search space comprised by ` binary variables, there are ( ` o ) schema partitions of order o. For any\nfixed value of o, ( ` o ) \u2208 \u2126(`o) (Cormen et al., 1990). The exciting finding presented in this paper is that UGAs can implement hyperclimbing cheaply for large values of `, and values of o that are small, but greater than one."}, {"heading": "3. Proof of Concept", "text": "We introduce a parameterized stochastic fitness function, called a staircase function, and provide experimental evidence that a UGA can perform hyperclimbing on a particular parameterization of this function. Then, using symmetry arguments, we conclude that the running time and the number of fitness queries required to achieve equivalent results scale surprisingly well with changes to key parameters. An experimental test validates this conclusion.\nDefinition 1 A staircase function descriptor is a 6-tuple (h, o, \u03b4, `, L, V ) where h, o and ` are positive integers such that ho \u2264 `, \u03b4 is a positive real number, and L and V are matrices with h rows and o columns such that the values of V are binary digits, and the elements of L are distinct integers in [`].\nFor any positive integer `, let [`] denote the set {1, . . . , `}, and let B` denote the set of binary strings of length `. Given any k-tuple, x, of integers in [`], and any binary string g \u2208 B`, let \u039ex(g) denote the string b1, . . . , bk such that for any i \u2208 [k], bi = gxi . For any m\u00d7 n matrix M , and any i \u2208 [m], let Mi: denote the n-tuple that is the ith row of M . Let N (a, b) denote the normal distribution with mean a and variance b. Then the function, f , described by the staircase function descriptor (h, o, \u03b4, `, L, V ) is the stochastic function over the set of binary strings of length ` given by Algorithm 1. The parameters h, o, \u03b4, and ` are called the height, order, increment and span, respectively, of f . For any i \u2208 [h], we define\nAlgorithm 1: A staircase function with descriptor (h, o, \u03b4, \u03c3, `, L, V )\nInput: g is a chromosome of length `\nx\u2190 some value drawn from the distribution N (0, 1) for i\u2190 1 to h do\nif \u039eLi:(g) = Vi1 . . . Vio then x\u2190 x+ \u03b4 else x\u2190 x\u2212 (\u03b4/(2o \u2212 1)) break end\nend return x\nstep i of f to be the schema {g \u2208 B`|\u039eLi:(g) = Vi1 . . . Vio}, and define stage i of f to be the schema {g \u2208 B`|(\u039eL1:(g) = V11 . . . V1o) \u2227 . . . \u2227 (\u039eLi:(g) = Vi1 . . . Vio)}.\nA step of the staircase function is said to have been climbed when future sampling of the search space is largely limited to that step. Just as it is hard to climb higher steps of a physical staircase without climbing lower steps first, it is computationally expensive to identify higher steps of a staircase function without identifying lower steps first (Theorem 1, Appendix C). In this regard, it is possible that staircase functions capture a feature that is widespread within the fitness functions resulting from the representational choices of GA users. The difficulty of climbing step i \u2208 [h] given stage i \u2212 1, however, is non-increasing with respect to i (Corollary 1, Appendix C). Readers seeking to ways to visualize staircase functions are refered to Appendix B."}, {"heading": "3.1 UGA Specification", "text": "The pseudocode for the UGA used in this paper is given in Algorithm 2. The free parameters of the UGA are N (the size of the population), pm (the per bit mutation probability), and Evaluate-Fitness (the fitness function). Once these parameters are fixed, the UGA is fully specified. The specification of a fitness function implicitly determines the length of the chromosomes, `. Two points deserve further elaboration:\n1. The function SUS-Selection takes a population of size N , and a corresponding set of fitness values as inputs. It returns a set of N parents drawn by fitness proportionate stochastic universal sampling (SUS). Instead of selecting N parents by spinning a roulette wheel with one pointer N times, stochastic universal sampling selects N parents by spinning a roulette wheel with N equally spaced pointers just once. Selecting parents this way has been shown to reduce sampling error (Baker, 1985; Mitchell, 1996).\n2. When selection is fitness proportionate, an increase in the average fitness of the population causes a decrease in selection pressure. The UGA in Algorithm 2 combats\nthis effect by using sigma scaling (Mitchell, 1996, p 167) to adjust the fitness values returned by Evaluate-Fitness. These adjusted fitness values, not the raw ones, are used when selecting parents. Let f (t) x denote the raw fitness of some chromosome x in some generation t, and let f (t) and \u03c3(t) denote the mean and standard deviation of the raw fitness values in generation t respectively. Then the adjusted fitness of x in generation t is given by h (t) x where, if \u03c3(t) = 0 then h (t) x = 1, otherwise,\nh(t)x = min(0, 1 + f (t) x \u2212 f (t) \u03c3(t) )\nThe use of sigma scaling also entails that negative fitness values are handled appropriately.\n."}, {"heading": "3.2 Performance of a UGA on a class of Staircase Functions", "text": "Let f be a staircase function with descriptor (h, o, \u03b4, `, L, V ), we say that f is basic if ` = ho, Lij = o(i\u22121)+j, (i.e. if L is the matrix of integers from 1 to ho laid out row-wise), and V is a matrix of ones. If f is known to be basic, then the last three elements of the descriptor of f are fully determinable from the first three, and its descriptor can be shortened to (h, o, \u03b4). Given some staircase function f with descriptor (h, o, \u03b4, `, L, V ), we define the basic form of f to be the (basic) staircase function with descriptor (h, o, \u03b4).\nLet \u03c6\u2217 be the basic staircase function with descriptor (h = 50, o = 4, \u03b4 = 0.3), and let U denote the UGA defined in section 3.1 with a population size of 500, and a per bit mutation probability of 0.003 (i.e, pm = 0.003). Figure 1a shows that U is capable of robust adaptation when applied to \u03c6\u2217 (We denote the resulting algorithm by U\u03c6 \u2217 ). Figure 1c shows that under the action of U , the first four steps of \u03c6\u2217 go to fixation3 in ascending order. When a step gets fixed, future sampling will largely be confined to that step\u2014in effect, the hyperplane associated with the step has been climbed. Note that the UGA does not need to \u201cfully\u201d climb a step before it begins climbing the subsequent step (Figure 1c)."}, {"heading": "3.3 Symmetry Analysis and Experimental Confirmation", "text": "Formal models of SGAs with finite populations and non-trivial fitness functions (Nix and Vose, 1992), are notoriously unwieldy (Holland, 2000), which is why most theoretical analyses of SGAs assume an infinite population (Liepins and Vose, 1992; Stephens and Waelbroeck, 1999; Wright et al., 2003; Burjorjee, 2007). Unfortunately, since the running time and the number of fitness evaluations required by such models is always infinite, their use precludes the identification of computational efficiencies of the SGA. In the present case, we circumvent the difficulty of formally analyzing finite population SGAs by exploiting some simple symmetries introduced through our definition of staircase functions, and through our use of a crossover operator with no positional bias. The absence of positional bias in uniform crossover was highlighted by Eshelman et al. (1989). Essentially, permuting the bits\n3. The terms \u2018fixation\u2019 and \u2018fixing\u2019 are used loosely here. Clearly, as long as the mutation rate is non-zero, no locus can ever be said to go to fixation in the strict sense of the word.\nAlgorithm 2: Pseudocode for the UGA used. The population size is an even number, denoted N , the length of the chromosomes is `, and for any chromosomal bit, the probability that the bit will be flipped during mutation (the per bit mutation probability) is pm. The population is represented internally as an N by ` array of bits, with each row representing a single chromosome. Generate-UX-Masks(x, y) creates an x by y array of bits drawn from the uniform distribution over {0, 1}. GenerateMut-Masks(x, y, z) returns an x by y array of bits such that any given bit is 1 with probability z.\npop\u2190 Initialize-Population(N ,`) while some termination condition is unreached do\nfitnessV alues\u2190 Evaluate-Fitness(pop) adjustedF itV als\u2190 Sigma-Scale(fitnessV alues) parents\u2190 SUS-Selection(pop, adjustedF itV als) crossMasks\u2190Generate-UX-Masks(N/2, `) for i \u2190 1 to N/2 do\nfor j \u2190 1 to ` do if crossMasks[i, j] = 0 then\nnewPop[i, j]\u2190 parents[i, j] newPop[i+N/2, j]\u2190 parents[i+N/2, j]\nelse newPop[i, j]\u2190 parents[i+N/2, j] newPop[i+N/2, j]\u2190 parents[i, j] end\nend\nend mutMasks\u2190Generate-Mut-Masks(N , `, pm) for i \u2190 1 to N do\nfor j \u2190 1 to ` do newPop[i, j]\u2190 xor(newPop[i, j], mutMasks[i, j]) end\nend pop\u2190 newPop\nend\nof all strings in each generation using some permutation \u03c0 before crossover, and permuting the bits back using \u03c0\u22121 after crossover has no effect on the dynamics of a UGA. Another way to elucidate this symmetry is by noting that any homologous crossover operator can be modeled as a string of binary random variables. Only in the case of uniform crossover, however, are these random variables all independent and identically distributed.\nIt is easily seen that loci that are not part of any step of a staircase function are immaterial during fitness evaluation. The absence of positional bias in uniform crossover entails that such loci can also be ignored during recombination. Effectively, then, these loci can be \u201cspliced out\u201d without affecting the expected average fitness of the population in any generation. This, and other observations of this type lead to the conclusion below.\nin each of 5000 generations.The error bars show five standard errors above and below the mean every 200 generations. (c) Going from the top plot to the bottom plot, the mean frequencies, across 20 trials, of the first four steps of the staircase function U\u03c6 \u2217 in each of the first 250 generations. The error bars show three standard errors above and below the mean every 12 generations. (b,d) Same as the plots on the left, but for U\u03c6\n.\nLet W be some UGA. For any staircase function f , and any x \u2208 [0, 1], let p(t) (W f ,i) (x) denote the probability that the frequency of stage i of f in generation t of W f is x. Let f\u2217 be the basic form of f . Then, by appreciating the symmetries between the UGAs W f \u2217\nand W f one can conclude the following:\nConclusion 1 For any generation t, any i \u2208 [h], and any x \u2208 [0, 1], p(t) (W f ,i) (x) = p (t)\n(W f\u2217 ,i) (x)\nThis conclusion straightforwardly entails that to raise the average fitness of a population to some attainable value,\n1. The expected number of generations required is constant with respect to the span of a staircase function\n2. The running time required scales linearly with the span of a staircase function\n3. The running time and the number of generations are unaffected by the last two elements of the descriptor of a staircase function\nLet f be some staircase function with basic form \u03c6\u2217 (defined in Section 3.2). Then, given the above, the application of U to f should, discounting deviations due to sampling, produce results identical to those shown in Figures 1a and 1c. We validated this \u201ccorollary\u201d by applying U to the staircase function \u03c6 with descriptor (h = 50, o = 4, \u03b4 = 0.3, ` = 20000, L, V ) where L and V were randomly generated. The results are shown in Figures 1b and 1d. Note that gross changes to the matrices L and V , and an increase in the span of the staircase function by two orders of magnitude did not produce any statistically significant changes. It is hard to think of another algorithm with better scaling properties on this non-trivial class of fitness functions."}, {"heading": "4. Validation", "text": "Let us pause to consider a curious aspect of the behavior of U\u03c6 \u2217 . Figure 1 shows that the growth rate of the average fitness of the population of U\u03c6 \u2217\ndecreases as evolution proceeds, and the average fitness of the population plateaus at a level that falls significantly short of the maximum expected average population fitness of 15. As discussed in the previous section, the difficulty of climbing step i given stage i \u2212 1 is non-increasing with respect to i. So, given that U successfully identifies the first step of \u03c6\u2217, why does it fail to identify all remaining steps? To understand why, consider some binary string that belongs to the ith stage of \u03c6\u2217. Since the mutation rate of U is 0.003, the probability that this binary string will still belong to stage i after mutation is 0.997io. This entails that as i increases, U\u03c6 \u2217 is less able to \u201chold\u201d a population within stage i. In light of this observation, one can infer that as i increases the sensitivity of U to the conditional fitness signal of step i given stage i \u2212 1 will decrease. This loss in sensitivity explains the decrease in the growth rate of the average fitness of U\u03c6 \u2217 . We call the \u201cwastage\u201d of fitness queries described here mutational drag. To curb mutational drag in UGAs, we conceived of a very simple tweak called clamping. This tweak relies on parameters flagFreqThreshold \u2208 [0.5, 1], unflagFreqThreshold \u2208 [0.5, flagFreqThreshold], and the positive integer waitingPeriod. If the one-frequency or the zero-frequency of some locus (i.e. the frequency of the bit 1 or the frequency of the bit 0, respectively, at that locus) at the beginning of some generation is greater than flagFreqThreshold, then the locus is flagged. Once flagged, a locus remains\nflagged as long as the one-frequency or the zero-frequency of the locus is greater than unflagFreqThreshold at the beginning of each subsequent generation. If a flagged locus in some generation t has remained constantly flagged for the last waitingPeriod generations, then the locus is considered to have passed our fixation test, and is not mutated in generation t. This tweak is called clamping because it is expected that in the absence of mutation, a locus that has passed our fixation test will quickly go to strict fixation, i.e. the one-frequency, or the zero-frequency of the locus will get \u201cclamped\u201d at one for the remainder of the run.\nLet Uc denote a UGA that uses the clamping mechanism described above and is identical to the UGA U in every other way. The clamping mechanism used by Uc is parameterized as follows: flagFreqThreshold = 0.99, unflagFreqThreshold = 0.9, waitingPeriod=200. The performance of U\u03c6 \u2217 c is displayed in figure 2a. Figure 2b shows the number of loci that the clamping mechanism left unmutated in each generation. These two figures show that the clamping mechanism effectively allowed Uc to climb all the stages of \u03c6\n\u2217. If the hyperclimbing hypothesis is accurate, then mutational drag is likely to be an issue when UGAs are applied to other problems, especially large instances that require the use of long chromosomes. In such cases, the use of clamping should improve performance. We now present the results of experiments where the use of clamping clearly improves the performance of a UGA on large instances of MAX-3SAT and the Sherrington Kirkpatrik Spin Glasses problem."}, {"heading": "4.1 Validation on MAX-3SAT", "text": "MAX-kSAT (Hoos and Stu\u0308tzle, 2004) is one of the most extensively studied combinatorial optimization problems. An instance of this problem consists of n boolean variables, and m clauses. The literals of the instance are the n variables and their negations. Each clause is a disjunction of k of the total possible 2n literals. Given some MAX-kSAT instance, the value of a particular setting of the n variables is simply the number of the m clauses that evaluate to true. In a uniform random MAX-kSAT problem, the clauses are generated by picking each literal at random (with replacement) from amongst the 2n literals. Generated clauses containing multiple copies of a variable, and ones containing a variable and its negation, are discarded and replaced.\nLet Q denote the UGA defined in section 3.1 with a population size of 200 (N = 200) and a per bit mutation probability of 0.01 (i.e., pm = 0.01). We applied Q to a randomly generated instance of the Uniform Random 3SAT problem, denoted sat, with 1000 binary variables and 4000 clauses. Variable assignments were straightforwardly encoded, with each bit in a chromosome representing the value of a single variable. The fitness of a chromosome was simply the number of clauses satisfied under the variable assignment represented. Figure 3a shows the average fitness of the population of Qsat over 7000 generations. Note that the growth in the maximum and average fitness of the population tapered off by generation 1000.\nThe UGA Q was applied to sat once again; this time, however, the clamping mechanism described above was activated in generation 2000. The resulting UGA is denoted Qsatc . The clamping parameters used were as follows: flagFreqThreshold = 0.99, unflagFreqthreshold = 0.8, waitingPeriod = 200. The average fitness of the population of Qsatc over 7000 generations is shown in Figure 3b, and the number of loci that the clamping mechanism left unmutated in each generation is shown in Figure 3c. Once again, the growth in the maximum and average fitness of the population tapered off by generation 1000. However, the maximum and average fitness began to grow once again starting at generation 2200. This growth coincides with the commencement of the clamping of loci (compare Figures 3b and 3c)."}, {"heading": "4.2 Validation on an SK Spin Glasses System", "text": "A Sherrington Kirkpatrick Spin Glasses system is a set of coupling constants Jij , with 1 \u2264 i < j \u2264 `. Given a configuration of \u201cspins\u201d (\u03c31, . . . , \u03c3`), where each spin is a value in {+1,\u22121}, the \u201cenergy\u201d of the system is given by\nE(\u03c3) = \u2212 \u2211\n1\u2264i<j\u22641 Jij\u03c3i\u03c3j\n. The goal is to find a spin configuration that minimizes energy. By defining the fitness of some spin configuration \u03c3 to be \u2212E(\u03c3) we remain true to the conventional goal in genetic algorithmics of maximizing fitness. The coupling constants in J can either be drawn from the set {\u22121, 0,+1}, or from the gaussian distribution N (0, 1). Following Pelikan et al. (2008), we used coupling constants drawn from N (0, 1). Each chromosome in the evolving population straightforwardly represented a spin configuration, with the bits 1 and\n0 denoting the spins +1 and \u22121 respectively4. The UGAs Q and Qc (described in the previous subsection) were applied to a randomly generated Sherrington Kirkpatrik spin glasses system over 1000 spins, denoted spin. The results obtain (Figures 3d, 3e, and 3f) were similar to the results described in the previous subsection.\nIt should be said that clamping by itself does not cause decimation. It merely enforces strict decimation once a high degree of decimation has already occurred along some dimension. In other words, clamping can be viewed as a decimation \u201clock-in\u201d mechanism as opposed to a decimation enforcing mechanism. Thus, the occurrence of clamping shown in Figure 3 entails the occurrence of decimation. The effectiveness of clamping demonstrated above lends considerable support to the hyperclimbing hypothesis. More support of this kind can be found in the work of Huifang and Mo (2010) where the use of clamping improved the performance of a UGA on a completely different problem (optimizing the weights of a quantum neural network). A fair portion of the scientific usefulness of these experiments is attributable to the utter simplicity of clamping. Reasoning within the rubric of the hypercling hypothesis, it not difficult to think of adjustments to the UGA that are more effective, but also more complex. From an engineering standpoint the additional complexity would indeed be warranted. From a scientific perspective, however, the additional complexity is a liability because it might introduce suspicion that the adjustments work for reasons other than the one offered here."}, {"heading": "5. Conclusion", "text": "Simple genetic algorithms with uniform crossover (UGAs) perform adaptation by implicitly exploiting one or more features common to the fitness distributions arising in practice. Two key questions are i) What type of features? and ii) How are these features exploited by the UGA (i.e. what heuristic does the UGA implicitly implement)? The hyperclimbing hypothesis is the first scientific theory to venture answers to these questions. In doing so it challenges two commonly held views about the conditions necessary for a genetic algorithm to be effective: First, that the fitness distribution must have a building block structure (Goldberg, 2002; Watson, 2006). Second, that a genetic algorithm will be ineffective unless it makes use of a \u201clinkage learning\u201d mechanism (Goldberg, 2002). Support for the hyperclimbing hypothesis was presented in the proof of concept and validation sections of this article. Additional support for this hypothesis can be found in i) the weakness of the assumptions undergirding this hypothesis (compared to the building block hypothesis, the hyperclimbing hypothesis rests on weaker assumptions about the distribution of fitness over the search space; see Burjorjee 2009), ii) the computational efficiencies of the UGA rigorously identified in an earlier work (Burjorjee, 2009, Chapter 3), and iii) the utility of clamping reported by Huifang and Mo (2010).\nIf the hyperclimbing heuristic is sound, then the idea of a landscape (Wright, 1932; Kauffman, 1993) is not very useful for intuiting the behavior of UGAs. Far more useful is the notion of a hyperscape. Landscapes and hyperscapes are both just ways of conceptualizing fitness functions geometrically. Whereas landscapes draw one\u2019s attention to the\n4. Given an n\u00d7 ` matrix P representing a population of n spin configurations, each of size `, the energies of the spin configurations can be expressed compactly as \u2212PJTPT where J is an `\u00d7 ` upper triangular matrix containing the coupling constants of the SK system.\ninterplay between the fitness function and the neighborhood structure of individual points, hyperscapes are about the statistical fitness properties of individual hyperplanes, and the \u201cspatial\u201d relationships between hyperplanes\u2014lower order hyperplanes can contain higher order hyperplanes, hyperplanes can intersect each other, and disjoint hyperplanes belonging\nto the same hyperplane partition can be regarded as parallel. The use of hyperscapes for intuiting GA dynamics originated with Holland (1975) and was popularized by Goldberg (1989).\nUseful as it may be as an explanation for adaptation in UGAs, the ultimate value of the hyperclimbing hypothesis may lie in its generalizability. In a previous work (Burjorjee, 2009), the notion of a unit of inheritance\u2014i.e. a gene\u2014was used to generalize this hypothesis to account for adaptation in simple genetic algorithms with strong linkage between chromosomal loci. It may be possible for the hyperclimbing hypothesis to be generalized further to account for adaptation in other kinds of evolutionary algorithms, In general, such algorithms may perform adaptation by efficiently identifying and progressively fixing above average \u201caspects\u201d\u2014units of selection in evolutionary biology speak\u2014of the chromosomes under evolution. The precise nature of the unit of selection in each case would need to be determined.\nIf the hyperclimbing hypothesis and its generalizations are sound we would finally have a unified explanation for adaptation in evolutionary algorithms. Fundamental advances in the invention, application, and further analysis of these algorithms can be expected to follow. The field of global optimization would be an immediate beneficiary. In turn, a range of fields, including machine learning, drug discovery, and operations research stand to benefit. Take machine learning for instance. Machine learning problems that can be tackled today are, in large part, ones that are reducible in practice to convex optimization problems (Bennett and Parrado-Herna\u0301ndez, 2006). The identification of an intuitive, efficiently implementable, general purpose meta-heuristic for optimization over rugged, dynamic, and stochastic cost functions promises to significantly extend the reach of this field.\nFinally, we briefly touch on the interdisciplinary contribution that the hyperclimbing hypothesis makes to a longstanding debate about the units of selection in biological populations (Okasha, 2006; Dawkins, 1999a,b). The material presented in the proof of concept section of this paper, and especially the material in Chapter 3 of an earlier work (Burjorjee, 2009) suggest that the most basic unit of selection is, not the individual gene as is commonly thought, but a small set of genes. Chapter 3 of the earlier work (Burjorjee, 2009) demonstrates conclusively that as a unit of selection, the latter is not always reducible to instances of the former. In other words, it gives the lie to the common refrain in Population Genetics that multi-gene interactions can be ignored when studying adaptation in biological populations because \u201cadditive effects are the basis for selection\u201d (Wagner, 2002)."}, {"heading": "Appendix A. The Hyperclimbing Heuristic: Formal Description", "text": "Introducing new terminology and notation where necessary, we present a formal description of the hyperclimbing heuristic. For any positive integer `, let [`] denote the set {1, . . . , `}, and let B` denote the set of all binary strings of length `. For any binary string g, let gi denote the ith bit of g. We define the schema partition model set of `, denoted SPM`, to be the power set of [`], and define the schema model set of `, denoted SM`, to be the set {h : D \u2192 {0, 1}|D \u2208 SPM`}. Let S` and SP` be the set of all schemata and schema partitions Mitchell (1996), respectively, of the set B`. Given some schema \u03b3 \u2282 B`, let \u03c0(\u03b3) denote the set {i \u2208 [`] | \u2200x, y \u2208 \u03b3, xi = yi}. We define a schema modeling function SMF` : S` \u2192 SM` as follows: for any \u03b3 \u2208 S`, SMF` maps \u03b3 to the function h : \u03c0(\u03b3) \u2192 {0, 1} such that for any g \u2208 \u03b3 and any i \u2208 \u03c0(\u03b3), h(i) = gi. We define a schema partition modeling function SPMF` : SP` \u2192 SPM` as follows: for any \u0393 \u2208 SP`, SPMF`(\u0393) = \u03c0(\u03b3), where \u03b3 \u2208 \u0393. As \u03c0(\u03c8) = \u03c0(\u03be) for all \u03c8, \u03be \u2208 \u0393, the schema partition modeling function is well defined. It is easily seen that SPF` and SPMF` are both bijective. For any schema model h \u2208 SM`, we denote SMF\u22121` (h) by JhK`. Likewise, for any schema partition model S \u2208 SPM` we denote SPMF\u22121` (S) by JSK`. Going in the forward direction, for any schema \u03b3 \u2208 S`, we denote SMF`(\u03b3) by \u3008\u03b3\u3009. Likewise, for any schema partition \u0393 \u2208 SP`, we denote SPMF`(\u0393) by \u3008\u0393\u3009. We drop the ` when going in this direction, because its value in each case is ascertainable from the operand. For any schema partition \u0393, and any schema \u03b3 \u2208 \u0393, the order of \u0393, and the order of \u03b3 is |\u3008\u0393\u3009|.\nFor any two schema partitions \u03931,\u03932 \u2208 SP`, we say that \u03931 and \u03932 are orthogonal if the models of \u03931 and \u03932 are disjoint (i.e., \u3008\u03931\u3009\u2229\u3008\u03932\u3009 = \u2205). Let \u03931 and \u03932 be orthogonal schema partitions in SP`, and let \u03b31 \u2208 \u03931 and \u03b32 \u2208 \u03932 be two schemata. Then the concatenation \u03931\u03932 denotes the schema partition J\u3008\u03931\u3009 \u222a \u3008\u03932\u3009K`, and the concatenation \u03b31\u03b32 denotes the schema Jh : \u3008\u03931\u3009 \u222a \u3008\u03932\u3009 \u2192 {0, 1}K` such that for any i \u2208 \u3008\u03931\u3009, h(i) = \u3008\u03b31\u3009(i), and for any i \u2208 \u3008\u03932\u3009, h(i) = \u3008\u03b32\u3009(i). Since \u3008\u03931\u3009 and \u3008\u03932\u3009 are disjoint, \u03b31\u03b32 is well defined. Let \u03931 and \u03932 be orthogonal schema partitions, and let \u03b31 \u2208 \u03931 be some schema. Then \u03b3.\u03932 denotes the set {\u03b3\u03be \u2208 \u03931\u03932|\u03be \u2208 \u03932}.\nGiven some (possibly stochastic) fitness function f over the set B`, and some schema \u03b3 \u2208 S`, we define the fitness of \u03b3, denoted F (f) \u03b3 , to be a random variable that gives the fitness value of a binary string drawn from the uniform distribution over \u03b3. For any schema partition \u0393 \u2208 SP`, we define the effect of \u0393, denoted Effect[\u0393], to be the variance5 of the expected fitness values of the schemata of \u0393. In other words,\nEffect[\u0393] = 2\u2212|\u3008\u0393\u3009| \u2211 \u03b3\u2208\u0393 E[F (f)\u03b3 ] \u2212 2\u2212|\u3008\u0393\u3009|\u2211 \u03be\u2208\u0393 E[F (f) \u03be ] 2\nLet \u03931,\u03932 \u2208 SP` be schema partitions such that \u3008\u03931\u3009 \u2282 \u3008\u03932\u3009. It is easily seen that Effect[\u03931] \u2264 Effect[\u03932]. With equality if and only if F ((f)\u03b32 = F ((f) \u03b31 for all schemata \u03b31 \u2208 \u03931 and \u03b32 \u2208 \u03932 such that \u03b32 \u2282 \u03b31. This condition is unlikely to arise in practice; therefore, for all practical purposes, the effect of a given schema partition decreases as the partition becomes coarser. The schema partition J [l] K` has the maximum effect. Let \u0393 and \u03a8 be two 5. We use variance because it is a well known measure of dispersion. Other measures of dispersion may\nwell be substituted here without affecting the discussion\northogonal schema partitions, and let \u03b3 \u2208 \u0393 be some schema . We define the conditional effect of \u03a8 given \u03b3, denoted Effect[\u03a8|\u03b3], as follows:\nEffect[\u03a8|\u03b3] = 2\u2212|\u3008\u03a8\u3009| \u2211 \u03c8\u2208\u03a8 E[F (f)\u03b3\u03c8 ] \u2212 2\u2212|\u3008\u03a8\u3009|\u2211 \u03be\u2208\u03a8 E[F (f) \u03b3\u03be ] 2\nA hyperclimbing heuristic works by evaluating the fitness of samples drawn initially from the uniform distribution over the search space. It finds a coarse schema partition \u0393 with a non-zero effect, and limits future sampling to some schema \u03b3 of this partition whose average sampling fitness is greater then the mean of the average sampling fitness values of the schemata in \u0393. By limiting future sampling in this way, the heuristic raises the expected fitness of all future samples. The heuristic limits future sampling to some schema by fixing the defining bits Mitchell (1996) of that schema in all future samples. The unfixed loci constitute a new (smaller) search space to which the hyperclimbing heuristic is then recursively applied. Crucially, coarse schema partitions orthogonal to \u0393 that have undetectable unconditional effects, may have detectable effects when conditioned by \u03b3."}, {"heading": "Appendix B. Visualizing Staircase Functions", "text": "The stages of a staircase function can be visualized as a progression of nested hyperplanes6, with hyperplanes of higher order and higher expected fitness nested within hyperplanes of lower order and lower expected fitness. By choosing an appropriate scheme for mapping a high-dimensional hypercube onto a two dimensional plot, it becomes possible to visualize this progression of hyperplanes in two dimensions (Appendix B).\nDefinition 2 A refractal addressing system is a tuple (m,n,X, Y ), where m and n are positive integers, and X and Y are matrices with m rows and n columns such that the elements in X and Y are distinct positive integers from the set [2mn], such that for any k \u2208 [2mn], k is in X \u21d0\u21d2 k is not in Y (i.e. the elements of [2mn] are evenly split between X and Y ).\nThe refractal addressing system (m, o,X, Y ) determines how the set B2mn gets mapped onto a 2mn\u00d7 2mn grid of pixels. For any bitstring g \u2208 B2mn the xy-address (a tuple of two values, each between 1 and 2mn) of the pixel representing g is given by Algorithm 3. Example: Let (h = 4, o = 2, \u03b4 = 3, ` = 16, L, V ) be the descriptor of a staircase function f , such that\nV =  1 0 0 1 0 0 1 1  Let A = (m = 4, n = 2, X, Y ) be a refractal addressing system such that X1: = L1:, Y1: = L2:, X2: = L3:, and Y2: = L4:. A refractal plot 7 of f is shown in Figure 4a.\n6. A hyperplane is a geometrical representation of a schema (Goldberg, 1989, p 53). 7. The term \u201crefractal plot\u201d describes the images that result when dimensional stacking is combined with\npixelation Langton et al. (2006).\nAlgorithm 3: The algorithm for determining the (x, y)-address of a chromosome under the refractal addressing system (m,n,X, Y ). The function Bin-To-Int returns the integer value of a binary string.\nInput: g is a chromosome of length 2mn\ngranularity \u2190 2mn/2n x\u2190 1 y \u2190 1 for i\u2190 1 to m do\nx\u2190 x+ granularity \u2217Bin-To-Int (\u039eXi:(g)) y \u2190 y + granularity \u2217Bin-To-Int (\u039eYi:(g)) granularity \u2190 granularity/2n\nend return x, y\nThis image was generated by querying f with every bitstring in B16, and plotting the resulting fitness value of each chromosome as a greyscale pixel at the chromosome\u2019s refractal address under the addressing system A. The fitness values returned by f have been scaled to use the full range of possible greyscale shades8. Lighter shades signify greater fitness. The four stages of f can easily be discerned.\nSuppose we generate another refractal plot of f using the same addressing system A, but a different random number generator seed; because f is stochastic, the greyscale value of any pixel in the resulting plot will then most likely differ from that of its homolog in the plot shown in Figure 4a. Nevertheless, our ability to discern the stages of f would not be affected. In the same vein, note that when specifying A, we have not specified the values of the last two rows of X and Y ; given the definition of f it is easily seen that these values are immaterial to the discernment of its \u201cstaircase structure\u201d.\nOn the other hand, the values of the first two rows of X and Y are highly relevant to the discernment of this structure. Figure 4b shows a refractal plot of f that was obtained using a refractal addressing system A\u2032 = (m = 4, n = 2, X \u2032, Y \u2032) such that X \u20324: = L1:, Y \u2032 4: = L2:, X \u20323: = L3:, and Y \u2032\n3: = L4:. Nothing remotely resembling a staircase is visible in this plot. The lesson here is that the discernment of the fitness staircase inherent within a staircase function depends critically on how one \u2018looks\u2019 at this function. In determining the \u2018right\u2019 way to look at f we have used information about the descriptor of f , specifically the values of h, o, and L. This information will not be available to an algorithm which only has query access to f .\nEven if one knows the right way to look at a staircase function, the discernment of the fitness staircase inherent within this function can still be made difficult by a low value of the increment parameter. Figure 5 lets us visualize the decrease in the salience of the fitness staircase of f that accompanies a decrease in the increment parameter of this staircase function. In general, a decrease in the increment results in a decrease in the \u2018contrast\u2019\n8. We used the Matlab function imagesc()\nbetween the stages of that function, and an increase the amount of computation required to discern these stages."}, {"heading": "Appendix C. Analysis of Staircase Functions", "text": "Let ` be some positive integer. Given some (possibly stochastic) fitness function f over the set B`, and some schema \u03b3 \u2286 B` we define the fitness signal of \u03b3, denoted S(\u03b3), to be E[F\n(f) \u03b3 ] \u2212 E[F (f)B` ]. Let \u03b31 \u2286 B` and \u03b32 \u2286 B` be schemata in two orthogonal schema partitions. We define the conditional fitness signal of \u03b31 given \u03b32, denoted S(\u03b31 | \u03b32), to be the difference between the fitness signal of \u03b31\u03b32 and the fitness signal of \u03b32, i.e. S(\u03b31 | \u03b32) = S(\u03b31\u03b32) \u2212 S(\u03b32). Given some staircase function f we denote the ith step of f by bfci and denote the ith stage of f by dfei.\nLet f be a staircase function with descriptor (h, o, \u03b4, `, L, V ). For any integer i \u2208 [h], the fitness signal of bfci is one measure of the difficulty of \u201cdirectly\u201d identifying step i (i.e., the difficulty of determining step i without first determining any of the preceding steps 1, . . . , i \u2212 1). Likewise, for any integers i, j in [h] such that i > j, the conditional fitness signal of step i given stage j is one measure of the difficulty of \u201cdirectly\u201d identifying step i given stage j (i.e. the difficulty of determining bfci given dfej without first determining any of the intermediate steps bfcj+1, . . . , bfci\u22121.\nFor any i \u2208 [h], by Theorem 1 (see below), the unconditional fitness signal of step i is\n\u03b4\n2o(i\u22121)\nThis value decreases exponentially with i and o. It is reasonable, therefore, to suspect that the direct identification of step i of f quickly becomes infeasible with increases in i and o. Consider, however, that by Corollary 1, for any i \u2208 {2, . . . , h}, the conditional fitness signal of step i given stage (i \u2212 1) is \u03b4, a constant with respect to i. Therefore, if some algorithm can identify the first step of f , one should be able to use it to indirectly identify all remaining steps in time and fitness queries that scale linearly with the height of f .\nLemma 1 For any staircase function f with descriptor (h, o, \u03b4, `, L, V ), and any integer i \u2208 [h], the fitness signal of stage i is i\u03b4.\nProof: Let x be the expected fitness of B` under uniform sampling. We first prove the following claim:\nClaim 1 The fitness signal of stage i is i\u03b4 \u2212 x\nThe proof of the claim follows by induction on i. The base case, when i = h is easily seen to be true from the definition of a staircase function. For any k \u2208 {2, . . . , h}, we assume that the hypothesis holds for i = k, and prove that it holds for i = k \u2212 1. For any j \u2208 [h], let \u0393j \u2208 SP` denote the schema partition containing step i. The fitness signal of stage k\u2212 1 is given by\n1\n2o S(dfek) + \u2211 \u03c8 \u2208\u0393k\\{bfck} S(dfek\u22121\u03c8)  = k\u03b4 \u2212 x\n2o + 2o \u2212 1 2o\n( \u03b4(k \u2212 1)\u2212 \u03b4 2o \u2212 1 \u2212 x )\nwhere the first term of the right hand side of the above expression follows from the inductive hypothesis, and the second term follows from the definition of a staircase function. Manipulation of this expression yields\nk\u03b4 + (2o \u2212 1)\u03b4(k \u2212 1)\u2212 \u03b4 \u2212 2ox 2o\nwhich, upon further manipulation, yields (k \u2212 1)\u03b4 \u2212 x. This completes the proof of the claim. To prove the lemma, we must prove that x is zero. By claim 1, the fitness signal of the first stage is \u03b4\u2212x. By the definition of a staircase function then,\nx = \u03b4 \u2212 x\n2o + 2o \u2212 1 2o\n( \u2212 \u03b4\n2o \u2212 1 ) Which reduces to\nx = \u2212 x 2o\nClearly, x is zero.\nCorollary 1 For any i \u2208 {2, . . . , h}, the conditional fitness signal of step i given stage i\u22121 is \u03b4\nProof The conditional fitness signal of step i given stage i\u2212 1 is given by\nS(bfci | dfei\u22121) = S(dfei)\u2212 S(dfei\u22121) = (i\u03b4 \u2212 (i\u2212 1)\u03b4) = \u03b4\nTheorem 1 For any staircase function f with descriptor (h, o, \u03b4, \u03c3, `, L, V ), and any integer i \u2208 [h], the fitness signal of step i is \u03b4/2o(i\u22121).\nProof: For any j \u2208 [h], let \u039bj \u2208 SP` denote of the partition containing stage j, and let \u0393j \u2208 SP` denote of the partition containing step j. We first prove the following claim\nClaim 2 For any i \u2208 [h],\u2211 \u03be \u2208\u039bi\\{dfei} S(\u03be) = \u2212i\u03b4\nThe proof of the claim follows by induction on i. The proof for the base case (i = 1) is as follows: \u2211\n\u03be \u2208\u039b1\\{dfe1}\nS(\u03be) = (2o \u2212 1) ( \u2212\u03b4\n2o \u2212 1\n) = \u2212\u03b4\nFor any k \u2208 [h\u2212 1] we assume that the hypothesis holds for i = k, and prove that it holds for i = k + 1.\u2211\n\u03be \u2208\u039bk+1\\{dfek+1}\nS(\u03be)\n= \u2211\n\u03c8\u2208\u0393k+1\\{bfck+1}\nS(dfek\u03c8) + \u2211\n\u03be \u2208\u039bk\\{dfek} \u2211 \u03c8 \u2208\u0393k+1 S(\u03be\u03c8)\n= \u2211\n\u03c8 \u2208\u0393k+1\\{dfek+1}\nS(dfek\u03c8) + \u2211\n\u03c8 \u2208\u0393k+1 \u2211 \u03be \u2208\u039bk\\{dfek} S(\u03be\u03c8)\n= (2o \u2212 1)S(dfek) + 2o  \u2211 \u03be \u2208\u039bk\\{dfek} S(\u03be)  where the first and last equalities follow from the definition of a staircase function. Using Lemma 1 and the inductive hypothesis, the right hand side of this expression can be seen to equal\n(2o \u2212 1) ( k\u03b4 \u2212 \u03b4\n2o \u2212 1\n) \u2212 2ok\u03b4\nwhich, upon manipulation, yields \u2212\u03b4(k + 1). For a proof of the theorem, observe that step 1 and stage 1 are the same schema. So, by Lemma 1, S(bfc1) = \u03b4. Thus, the theorem holds for i = 1. For any i \u2208 {2, . . . , h},\nS(bfci) = 1\n(2o)i\u22121 S(dfei) + \u2211 \u03be \u2208\u039bi\u22121\\{dfei\u22121} S(\u03bebfck) \n= 1\n(2o)i\u22121 S(dfei) + \u2211 \u03be \u2208\u039bi\u22121\\{dfei\u22121} S(\u03be)  where the last equality follows from the definition of a staircase function. Using Lemma 1 and Claim 2, the right hand side of this equality can be seen to equal\ni\u03b4 \u2212 (i\u2212 1)\u03b4 (2o)i\u22121\n= \u03b4\n2o(i\u22121)"}], "references": [{"title": "A connectionist machine for genetic hillclimbing", "author": ["D.H. Ackley"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Ackley.,? \\Q1987\\E", "shortCiteRegEx": "Ackley.", "year": 1987}, {"title": "Adaptive selection methods for genetic algorithms", "author": ["James E. Baker"], "venue": "Proceedings of the First International Conference on Genetic Algorithms and Their Applications. Lawrence Erlbaum Associates, Publishers,", "citeRegEx": "Baker.,? \\Q1985\\E", "shortCiteRegEx": "Baker.", "year": 1985}, {"title": "The interplay of optimization and machine learning research", "author": ["Kristin P. Bennett", "Emilio Parrado-Hern\u00e1ndez"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bennett and Parrado.Hern\u00e1ndez.,? \\Q2006\\E", "shortCiteRegEx": "Bennett and Parrado.Hern\u00e1ndez.", "year": 2006}, {"title": "Survey propagation: an algorithm for satisfiability", "author": ["Alfredo Braunstein", "Marc Mzard", "Riccardo Zecchina"], "venue": "CoRR, cs.CC/0212002,", "citeRegEx": "Braunstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Braunstein et al\\.", "year": 2002}, {"title": "Sufficient conditions for coarse-graining evolutionary dynamics", "author": ["Keki Burjorjee"], "venue": "In Foundations of Genetic Algorithms 9 (FOGA IX),", "citeRegEx": "Burjorjee.,? \\Q2007\\E", "shortCiteRegEx": "Burjorjee.", "year": 2007}, {"title": "Generative Fixation: A Unifed Explanation for the Adaptive Capacity of Simple Recombinative Genetic Algorithms", "author": ["Keki M. Burjorjee"], "venue": "PhD thesis, Brandeis University,", "citeRegEx": "Burjorjee.,? \\Q2009\\E", "shortCiteRegEx": "Burjorjee.", "year": 2009}, {"title": "The Extended Phenotype", "author": ["Richard Dawkins"], "venue": null, "citeRegEx": "Dawkins.,? \\Q1999\\E", "shortCiteRegEx": "Dawkins.", "year": 1999}, {"title": "The Selfish Gene", "author": ["Richard Dawkins"], "venue": null, "citeRegEx": "Dawkins.,? \\Q1999\\E", "shortCiteRegEx": "Dawkins.", "year": 1999}, {"title": "Biases in the crossover landscape", "author": ["L.J. Eshelman", "R.A. Caruana", "J.D. Schaffer"], "venue": "Proceedings of the third international conference on Genetic algorithms table of contents,", "citeRegEx": "Eshelman et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Eshelman et al\\.", "year": 1989}, {"title": "Evolutionary Computation : Towards a New Philosophy of Machine Intelligence", "author": ["D.B. Fogel"], "venue": "IEEE press,", "citeRegEx": "Fogel.,? \\Q2006\\E", "shortCiteRegEx": "Fogel.", "year": 2006}, {"title": "Genetic Algorithms in Search, Optimization & Machine Learning", "author": ["David E. Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q1989\\E", "shortCiteRegEx": "Goldberg.", "year": 1989}, {"title": "The Design Of Innovation", "author": ["David E. Goldberg"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "Goldberg.,? \\Q2002\\E", "shortCiteRegEx": "Goldberg.", "year": 2002}, {"title": "Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence", "author": ["John H. Holland"], "venue": null, "citeRegEx": "Holland.,? \\Q1975\\E", "shortCiteRegEx": "Holland.", "year": 1975}, {"title": "Building blocks, cohort genetic algorithms, and hyperplane-defined functions", "author": ["John H. Holland"], "venue": "Evolutionary Computation,", "citeRegEx": "Holland.,? \\Q2000\\E", "shortCiteRegEx": "Holland.", "year": 2000}, {"title": "Stochastic Local Search: Foundations and Applications", "author": ["Holger H. Hoos", "Thomas St\u00fctzle"], "venue": null, "citeRegEx": "Hoos and St\u00fctzle.,? \\Q2004\\E", "shortCiteRegEx": "Hoos and St\u00fctzle.", "year": 2004}, {"title": "A new method of image compression based on quantum neural network", "author": ["Li Huifang", "Li Mo"], "venue": "In International Conference of Information Science and Management Engineering,", "citeRegEx": "Huifang and Mo.,? \\Q2010\\E", "shortCiteRegEx": "Huifang and Mo.", "year": 2010}, {"title": "The Origins of Order: Self-Organization and Selection in Evolution", "author": ["S.A. Kauffman"], "venue": "Biophysical Soc,", "citeRegEx": "Kauffman.,? \\Q1993\\E", "shortCiteRegEx": "Kauffman.", "year": 1993}, {"title": "Message-passing and local heuristics as decimation strategies for satisfiability", "author": ["L. Kroc", "A. Sabharwal", "B. Selman"], "venue": "In Proceedings of the 2009 ACM symposium on Applied Computing,", "citeRegEx": "Kroc et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kroc et al\\.", "year": 2009}, {"title": "Combining pixelization and dimensional stacking", "author": ["J.T. Langton", "A.A. Prinz", "T.J. Hickey"], "venue": "In Advances in Visual Computing,", "citeRegEx": "Langton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Langton et al\\.", "year": 2006}, {"title": "Evolution and the Levels of Selection", "author": ["Martin Pelikan"], "venue": "Mathematics and Artificial Intelligence,", "citeRegEx": "Pelikan.,? \\Q1992\\E", "shortCiteRegEx": "Pelikan.", "year": 1992}], "referenceMentions": [{"referenceID": 9, "context": "Introduction Over several decades of use in diverse scientific and engineering fields, evolutionary optimization has acquired a reputation for being a kind of universal acid\u2014a general purpose approach that routinely procures useful solutions to optimization problems with rugged, dynamic, and stochastic cost functions over search spaces consisting of strings, vectors, trees, and instances of other kinds of data structures (Fogel, 2006).", "startOffset": 425, "endOffset": 438}, {"referenceID": 15, "context": "An abiding mystery of the field is the widely observed utility of genetic algorithms with uniform crossover (Syswerda, 1989; Rudnick et al., 1994; Pelikan, 2008; Huifang and Mo, 2010).", "startOffset": 108, "endOffset": 183}, {"referenceID": 0, "context": "The use of uniform crossover (Ackley, 1987; Syswerda, 1989) in genetic algorithms causes genetic loci to be unlinked, i.", "startOffset": 29, "endOffset": 59}, {"referenceID": 11, "context": "It is generally acknowledged that the adaptive capacity of genetic algorithms with this kind of crossover cannot be explained within the rubric of the building block hypothesis, the reigning explanation for adaptation in genetic algorithms with strong linkage between loci (Goldberg, 2002).", "startOffset": 273, "endOffset": 289}, {"referenceID": 3, "context": ", 2002), Belief Propagation, Warning Propagation (Braunstein et al., 2002)\u2014use message passing algorithms to obtain statistical information about the space being searched.", "startOffset": 49, "endOffset": 74}, {"referenceID": 1, "context": "Selecting parents this way has been shown to reduce sampling error (Baker, 1985; Mitchell, 1996).", "startOffset": 67, "endOffset": 96}, {"referenceID": 13, "context": "3 Symmetry Analysis and Experimental Confirmation Formal models of SGAs with finite populations and non-trivial fitness functions (Nix and Vose, 1992), are notoriously unwieldy (Holland, 2000), which is why most theoretical analyses of SGAs assume an infinite population (Liepins and Vose, 1992; Stephens and Waelbroeck, 1999; Wright et al.", "startOffset": 177, "endOffset": 192}, {"referenceID": 4, "context": "3 Symmetry Analysis and Experimental Confirmation Formal models of SGAs with finite populations and non-trivial fitness functions (Nix and Vose, 1992), are notoriously unwieldy (Holland, 2000), which is why most theoretical analyses of SGAs assume an infinite population (Liepins and Vose, 1992; Stephens and Waelbroeck, 1999; Wright et al., 2003; Burjorjee, 2007).", "startOffset": 271, "endOffset": 364}, {"referenceID": 4, "context": ", 2003; Burjorjee, 2007). Unfortunately, since the running time and the number of fitness evaluations required by such models is always infinite, their use precludes the identification of computational efficiencies of the SGA. In the present case, we circumvent the difficulty of formally analyzing finite population SGAs by exploiting some simple symmetries introduced through our definition of staircase functions, and through our use of a crossover operator with no positional bias. The absence of positional bias in uniform crossover was highlighted by Eshelman et al. (1989). Essentially, permuting the bits", "startOffset": 8, "endOffset": 580}, {"referenceID": 14, "context": "1 Validation on MAX-3SAT MAX-kSAT (Hoos and St\u00fctzle, 2004) is one of the most extensively studied combinatorial optimization problems.", "startOffset": 34, "endOffset": 58}, {"referenceID": 19, "context": "Following Pelikan et al. (2008), we used coupling constants drawn from N (0, 1).", "startOffset": 10, "endOffset": 32}, {"referenceID": 15, "context": "More support of this kind can be found in the work of Huifang and Mo (2010) where the use of clamping improved the performance of a UGA on a completely different problem (optimizing the weights of a quantum neural network).", "startOffset": 54, "endOffset": 76}, {"referenceID": 11, "context": "In doing so it challenges two commonly held views about the conditions necessary for a genetic algorithm to be effective: First, that the fitness distribution must have a building block structure (Goldberg, 2002; Watson, 2006).", "startOffset": 196, "endOffset": 226}, {"referenceID": 11, "context": "Second, that a genetic algorithm will be ineffective unless it makes use of a \u201clinkage learning\u201d mechanism (Goldberg, 2002).", "startOffset": 107, "endOffset": 123}, {"referenceID": 16, "context": "If the hyperclimbing heuristic is sound, then the idea of a landscape (Wright, 1932; Kauffman, 1993) is not very useful for intuiting the behavior of UGAs.", "startOffset": 70, "endOffset": 100}, {"referenceID": 4, "context": "Additional support for this hypothesis can be found in i) the weakness of the assumptions undergirding this hypothesis (compared to the building block hypothesis, the hyperclimbing hypothesis rests on weaker assumptions about the distribution of fitness over the search space; see Burjorjee 2009), ii) the computational efficiencies of the UGA rigorously identified in an earlier work (Burjorjee, 2009, Chapter 3), and iii) the utility of clamping reported by Huifang and Mo (2010). If the hyperclimbing heuristic is sound, then the idea of a landscape (Wright, 1932; Kauffman, 1993) is not very useful for intuiting the behavior of UGAs.", "startOffset": 281, "endOffset": 482}, {"referenceID": 5, "context": "In a previous work (Burjorjee, 2009), the notion of a unit of inheritance\u2014i.", "startOffset": 19, "endOffset": 36}, {"referenceID": 2, "context": "Machine learning problems that can be tackled today are, in large part, ones that are reducible in practice to convex optimization problems (Bennett and Parrado-Hern\u00e1ndez, 2006).", "startOffset": 140, "endOffset": 177}, {"referenceID": 5, "context": "The material presented in the proof of concept section of this paper, and especially the material in Chapter 3 of an earlier work (Burjorjee, 2009) suggest that the most basic unit of selection is, not the individual gene as is commonly thought, but a small set of genes.", "startOffset": 130, "endOffset": 147}, {"referenceID": 5, "context": "Chapter 3 of the earlier work (Burjorjee, 2009) demonstrates conclusively that as a unit of selection, the latter is not always reducible to instances of the former.", "startOffset": 30, "endOffset": 47}, {"referenceID": 5, "context": "The use of hyperscapes for intuiting GA dynamics originated with Holland (1975) and was popularized by Goldberg (1989).", "startOffset": 65, "endOffset": 80}, {"referenceID": 5, "context": "The use of hyperscapes for intuiting GA dynamics originated with Holland (1975) and was popularized by Goldberg (1989). Useful as it may be as an explanation for adaptation in UGAs, the ultimate value of the hyperclimbing hypothesis may lie in its generalizability.", "startOffset": 103, "endOffset": 119}], "year": 2012, "abstractText": "The hyperclimbing hypothesis is a hypothetical explanation for adaptation in genetic algorithms with uniform crossover (UGAs). Hyperclimbing is an intuitive, general-purpose, non-local search heuristic applicable to discrete product spaces with rugged or stochastic cost functions. The strength of this heuristic lies in its insusceptibility to local optima when the cost function is deterministic, and its tolerance for noise when the cost function is stochastic. Hyperclimbing works by decimating a search space, i.e. by iteratively fixing the values of small numbers of variables. The hyperclimbing hypothesis holds that UGAs work by implementing efficient hyperclimbing. Proof of concept for this hypothesis comes from the use of a novel analytic technique involving the exploitation of algorithmic symmetry. We have also obtained experimental results that show that a simple tweak inspired by the hyperclimbing hypothesis dramatically improves the performance of a UGA on large, random instances of MAX-3SAT and the Sherrington Kirkpatrick Spin Glasses problem.", "creator": "LaTeX with hyperref package"}}}