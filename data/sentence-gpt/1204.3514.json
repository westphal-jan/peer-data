{"id": "1204.3514", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2012", "title": "Distributed Learning, Communication Complexity and Privacy", "abstract": "We consider the problem of PAC-learning from distributed data and analyze fundamental communication complexity questions involved. In addition to providing general upper and lower bounds on the amount of communication needed for learning, we also present tight results for a number of common concept classes including conjunctions, parity functions, and decision lists. For linear separators, we show that for non-concentrated distributions, we can use a version of the Perceptron algorithm to learn with much less communication than the number of updates given by the usual margin bound. Our general results show that in addition to VC-dimension and covering number, quantities such as the teaching-dimension and mistake-bound of a class play an important role in determining communication requirements. We also show that boosting can be performed in a generic manner in the distributed setting to achieve communication with only logarithmic dependence on 1/epsilon for any concept class. We additionally present an analysis of privacy, considering both differential privacy and a notion of distributional privacy that is especially appealing in this context. Furthermore, we provide examples of the general applicability of the Perceptron algorithm to model the importance of data retention in this context. Finally, we also offer an analysis of the effects of the L-dimensional model, namely that it allows a generic representation of the data in the form of \"L\" to measure how the data is exchanged in the distributed context.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Mon, 16 Apr 2012 15:10:32 GMT  (24kb)", "https://arxiv.org/abs/1204.3514v1", "18 pages"], ["v2", "Tue, 17 Apr 2012 21:42:21 GMT  (24kb)", "http://arxiv.org/abs/1204.3514v2", "18 pages"], ["v3", "Fri, 25 May 2012 15:53:51 GMT  (25kb)", "http://arxiv.org/abs/1204.3514v3", "19 pages"]], "COMMENTS": "18 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["maria-florina balcan", "avrim blum", "shai fine", "yishay mansour"], "accepted": false, "id": "1204.3514"}, "pdf": {"name": "1204.3514.pdf", "metadata": {"source": "CRF", "title": "Distributed Learning, Communication Complexity and Privacy", "authors": ["Maria-Florina Balcan", "Avrim Blum", "Shai Fine", "Yishay Mansour"], "emails": ["ninamf@cc.gatech.edu.", "avrim@cs.cmu.edu.", "shai@il.ibm.com.", "mansour@tau.ac.il."], "sections": [{"heading": null, "text": "ar X\niv :1\n20 4.\n35 14\nv3 [\ncs .L\nG ]\n2 5\nM ay\n2 01"}, {"heading": "1 Introduction", "text": "Suppose you have two databases: one with the positive examples and another with the negative examples. How much communication between them is needed to learn a good hypothesis? In this paper we consider this question and its generalizations, as well as related issues such as privacy. Broadly, we consider a framework where information is distributed between several locations, and our goal is to learn a low-error hypothesis with respect to the overall distribution of data using as little communication, and as few rounds of communication, as possible. Motivating examples include:\n1. Suppose k research groups around the world have collected large scientific datasets, such as genomic sequence data or sky survey data, and we wish to perform learning over the union of all these different datasets without too much communication.\n\u2217School of Computer Science, Georgia Institute of Technology. ninamf@cc.gatech.edu. This work was supported by NSF grant CCF-0953192, by ONR grant N00014-09-1-0751, and by a Microsoft Faculty Fellowship.\n\u2020Computer Science Department, Carnegie Mellon University. avrim@cs.cmu.edu. This work was supported in part by the National Science Foundation under grants CCF-1116892 and IIS-1065251, and by a grant from the United States-Israel Binational Science Foundation (BSF).\n\u2021IBM Research. shai@il.ibm.com. \u00a7School of Computer Science, Tel Aviv University. mansour@tau.ac.il. Supported in part by The Israeli Centers of Research Excellence (I-CORE) program, (Center No. 4/11), by the Google Inter-university center for Electronic Markets and Auctions, by a grant from the Israel Science Foundation, by a grant from United States-Israel Binational Science Foundation (BSF), and by a grant from the Israeli Ministry of Science (MoS).\n2. Suppose we are a sociologist and want to understand what distinguishes the clientele of two retailers (Macy\u2019s vs Walmart). Each retailer has a large database of its own customers and we want to learn a classification rule that distinguishes them. This is an instance of the case of each database corresponding to a different label. It also brings up natural privacy issues.\n3. Suppose k hospitals with different distributions of patients want to learn a classifier to identify a common misdiagnosis. Here, in addition to the goal of achieving high accuracy, low communication, and privacy for patients, the hospitals may want to protect their own privacy in some formal way as well.\nWe note that we are interested in learning a single hypothesis h that performs well overall, rather than separate hypotheses hi for each database. For instance, in the case that one database has all the positive examples and another has all the negatives, the latter problem becomes trivial. More generally, we are interested in understanding the fundamental communication complexity questions involved in distributed learning, a topic that is becoming increasingly relevant to modern learning problems. These issues, moreover, appear to be quite interesting even for the case of k = 2 entities."}, {"heading": "1.1 Our Contributions", "text": "We consider and analyze fundamental communication questions in PAC-learning from distributed data, providing general upper and lower bounds on the amount of communication needed to learn a given class, as well as broadly-applicable techniques for achieving communication-efficient learning. We also analyze a number of important specific classes, giving efficient learning algorithms with especially good communication performance, as well as in some cases counterintuitive distinctions between proper and non-proper learning.\nOur general upper and lower bounds show that in addition to VC-dimension and covering number, quantities such as the teaching-dimension and mistake-bound of a class play an important role in determining communication requirements. We also show how boosting can be performed in a communication-efficient manner, achieving communication depending only logarithmically on 1/\u01eb for any class, along with tradeoffs between total communication and number of communication rounds. Further we show that, ignoring computation, agnostic learning can be performed to error O(opt(H)) + \u01eb with logarithmic dependence on 1/\u01eb, by adapting results of Balcan and Hanneke [2012].\nIn terms of specific classes, we present several tight bounds including a \u0398(d log d) bound on the communication in bits needed for learning the class of decision lists over {0, 1}d. For learning linear separators, we show that for non-concentrated distributions, we can use a version of the Perceptron algorithm to learn using only O( \u221a\nd log(d/\u01eb)/\u01eb2) rounds of communication, each round sending only a single hypothesis vector, much less than the O(d/\u01eb2) total number of updates performed by the Perceptron algorithm. For parity functions, we give a rather surprising result. For the case of two entities, while proper learning has an \u2126(d2) lower bound based on classic results in communication complexity, we show that non-proper learning can be done efficiently using only O(d) bits of communication. This is a by-product of a general result regarding concepts learnable in the reliable-useful framework of Rivest and Sloan [1988]. For a table of results, see Appendix A.\nWe additionally present an analysis of communication-efficient privacy-preserving learning algorithms, considering both differential privacy and a notion of distributional privacy that is especially appealing in this context. We show that in many cases we can achieve privacy without incurring any additional communication penalty.\nMore broadly, in this work we propose and study communication as a fundamental resource for PAClearning in addition to the usual measures of time and samples. We remark that all our algorithms for specific classes address communication while maintaining efficiency along the other two axes."}, {"heading": "1.2 Related Work", "text": "Related work in computational learning theory has mainly focused on the topic of learning and parallel computation. Bshouty [1997] shows that many simple classes that can be PAC learned can not be efficiently learned in parallel with a polynomial number of processors. Long and Servedio [2011] show a parallel algorithm for large margin classifiers running in time O(1/\u03b3) compared to more naive implementations costing of \u2126(1/\u03b32), where \u03b3 is the margin. They also show an impossibility result regarding boosting, namely that the ability to call the weak learner oracle multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required. Collins et al. [2002] give an online algorithm that uses a parallel-update method for the logistic loss, and Zinkevich et al. [2010] give a detailed analysis of a parallel stochastic gradient descent in which each machine processes a random subset of the overall data, combining hypotheses at the very end. All of the above results are mainly interested in reducing the time required to perform learning when data can be randomly or algorithmically partitioned among processors; in contrast, our focus is on a setting in which we begin with data arbitrarily partitioned among the entities. Dekel et al. [2011] consider distributed online prediction with arbitrary partitioning of data streams, achieving strong regret bounds; however, in their setting the goal of entities is to perform well on their own sequence of data.\nIn very recent independent work, Daume III et al. [2012a] examine a setting much like that considered here, in which parties each have an arbitrary partition of an overall dataset, and the goal is to achieve low error over the entire distribution. They present comunication-efficient learning algorithms for axis-parallel boxes as well as for learning linear separators in R2. Daume III et al. [2012b], also independently of our work, extend this to the case of linear separators in Rd, achieving bounds similar to those obtained via our distributed boosting results. Additionally, they consider a range of distributed optimization problems, give connections to streaming algorithms, and present a number of experimental results. Their work overall is largely complementary to ours."}, {"heading": "2 Model and Objectives", "text": "Our model can be viewed as a distributed version of the PAC model. We have k entities (also called \u201cplayers\u201d) denoted by K and an instance space X. For each entity i \u2208 K there is a distribution Di over X that entity i can sample from. These samples are labeled by an unknown target function f . Our goal is to find a hypothesis h which approximates f well on the joint mixture D(x) = 1k \u2211k i=1Di(x). In the realizable case, we are given a concept class H such that f \u2208 H; in the agnostic case, our goal is to perform nearly as well as the best h\u2032 \u2208 H.\nIn order to achieve our goal of approximating f well with respect to D, entities can communicate with each other, for example by sending examples or hypotheses. At the end of the process, each entity should have a hypothesis of low error over D. In the center version of the model there is also a center, with initially no data of its own, mediating all the interactions. In this case the goal is for the center to obtain a low error hypothesis h. In the no-center version, the players simply communicate directly. In most cases, the two models are essentially equivalent; however (as seen in Section 5), the case of parity functions forms a notable exception. We assume the Di are not known to the center or to any entity j 6= i (in fact, Di is not\nknown explicitly even by entity i, and can be approximated only via sampling). Finally, let d denote the VC dimension of H, and \u01eb denote our target error rate in the realizable case, or our target gap with respect to opt(H) in the agnostic case.1 We will typically think of k as much smaller than d.\nRemark: We are assuming all players have the same weight, but all results extend to players with different given weights. We also remark that except for our generic results, all our algorithms for specific classes will be computationally efficient (see Appendix A).\nCommunication Complexity\nOur main focus is on learning methods that minimize the communication needed in order to learn well. There are two critical parameters, the total communication (either in terms of bits transmitted or examples or hypotheses transmitted ) and latency (number of rounds required). Also, in comparison to the baseline algorithm of having each database send all (or a random sample of) its data to a center, we will be looking both at methods that improve over the dependence on \u01eb and that improve over the dependence on d in terms of the amount of communication needed (and in some cases we will be able to improve in both parameters). In both cases, we will be interested in the tradeoffs between total communication and the number of communication rounds. The interested reader is referred to Kushilevitz and Nisan [1997] for an excellent exposition of communication complexity.\nWhen defining the exact communication model, it is important to distinguish whether entities can learn information from not receiving any data. For the most part we assume an asynchronous communication model, where the entities can not deduce any information when they do not receive the data (and there is no assumption about the delay of a message). In a few places we use a much stronger model of locksynchronous communication, where the communication is in time slots (so you can deduce that no one sent a message in a certain time slot) and if multiple entities try to transmit at the same time only one succeeds. Note that if we have an algorithm with T time steps and C communication bits in the lock-synchronous model, using an exponential back-off mechanism [Herlihy and Shavit, 2008] and a synchronizer [Peleg, 2000], we can convert it to an asynchronous communication with O(T log k) rounds and O((T +C) log k) communication bits.\nPrivacy\nIn addition to minimizing communication, it is also natural in this setting to consider issues of privacy, which we examine in Section 10. In particular, we will consider privacy of three forms: differential privacy for the examples (the standard form of privacy considered in the literature), differential privacy for the databases (viewing each entity as an individual deserving of privacy, which requires k to be large for any interesting statements), and distributional privacy for the databases (a weaker form of privacy that we can achieve even for small values of k). See Dwork [2008] for an excellent survey of differential privacy."}, {"heading": "3 Baseline approaches and lower bounds", "text": "We now describe two baseline methods for distributed learning as well as present general lower bounds.\nSupervised Learning: The simplest baseline approach is to just have each database send a random sample of size O( 1k ( d \u01eb log 1 \u01eb )) to the center, which then performs the learning. This implies we have a total communication cost of O(d\u01eb log 1 \u01eb ) in terms of number of examples transmitted. Note that while the sample received by\n1We will suppress dependence on the confidence parameter \u03b4 except in cases where it behaves in a nontrivial manner.\nthe center is not precisely drawn from D (in particular, it contains the same number of points from each Di), the standard double-sample VC-dimension argument still applies, and so with high probability all consistent h \u2208 H have low error. Similarly, for the agnostic case it suffices to use a total of O( d\n\u01eb2 log 1\u01eb ) examples. In\nboth cases, there is just one round of communication.\nEQ/online algorithms: A second baseline method is to run an Equivalence Query or online Mistake-Bound algorithm at the center. This method is simpler to describe in the lock-synchronization model. In each round the center broadcasts its current hypothesis. If any of the entities has a counter-example, it sends the counterexample to the center. If not, then we are done. The total amount of communication measured in terms of examples and hypotheses transmitted is at most the mistake bound M of the algorithm for learning H; in fact, by having each entity run a shadow copy of the algorithm, one needs only to transmit the examples and not the hypotheses. Note that in comparison to the previous baseline, there is now no dependence on \u01eb in terms of communication needed; however, the number of rounds may now be as large as the mistake bound M for the class H. Summarizing,\nTheorem 1. Any class H can be learned to error \u01eb in the realizable case using 1 round and O(d\u01eb log 1\u01eb ) total examples communicated, or M rounds and M total examples communicated, where M is the optimal mistake bound for H. In the agnostic case, we can learn to error opt(H) + \u01eb using 1 round and O( d\n\u01eb2 log 1\u01eb )\ntotal examples communicated.\nAnother baseline approach is for each player to describe an approximation to the joint distribution induced by Di and f to the center, in cases where that can be done efficiently. See Appendix B.1 for an example.\nWe now present a general lower bound on communication complexity for learning a class H. Let N\u01eb,D(H) denote the size of the minimum \u01eb-cover of H with respect to D, and let N\u01eb(H) = supD N\u01eb,D(H). Let dT (H) denote the teaching dimension of class H.2\nTheorem 2. Any class H requires \u2126(logN2\u01eb(H)) bits of communication to learn to error \u01eb. This implies \u2126(d) bits are required to learn to error \u01eb \u2264 1/8. For proper learning, \u2126(log |H|) bits are required to learn to error \u01eb < 12dT (H) . These hold even for k = 2.\nProof. Consider a distribution D1 such that N = N2\u01eb,D1(H) is maximized. Let D2 be concentrated on a single (arbitrary) point x. In order for player 2 to produce a hypothesis h of error at most \u01eb over D, h must have error at most 2\u01eb over D1. If player 2 receives fewer than log2(N2\u01eb(H)) \u2212 1 bits from player 1, then (considering also the two possible labels of x) there are less than N2\u01eb(H) possible hypotheses player 2 can output. Thus, there must be some f \u2208 H that has distance greater than 2\u01eb from all such hypotheses with respect to D1, and so player 2 cannot learn that function. The \u2126(d) lower bound follows from applying the above argument to the uniform distribution over d points shattered by H.\nFor the \u2126(log |H|) lower bound, again let D2 be concentrated on a single (arbitrary) point. If player 2 receives fewer than 12 log |H| bits then there must be some h\u2217 \u2208 H it cannot output. Consider f = h\u2217 and let D1 be uniform over dT (H) points uniquely defining f within H. Since player 2 is a proper learner, it must therefore have error greater than 2\u01eb over D1, implying error greater than \u01eb over D.\nNote that there is a significant gap between the above upper and lower bounds. For instance, if data lies in {0, 1}d, then in terms of d the upper bound in bits is O(d2) but the lower bound is \u2126(d) (or in examples, the upper bound is O(d) but the lower bound is \u2126(1)). In the following sections, we describe our algorithmic\n2dT (H) is defined as maxf\u2208H dT (f) where dT (f) is the smallest number of examples needed to uniquely identify f within H [Goldman and Kearns, 1991].\nresults for improving upon the above baseline methods, as well as stronger communication lower bounds for certain classes. We also show how boosting can be used to generically get only a logarithmic dependence of communication on 1/\u01eb for any class, using a logarithmic number of rounds."}, {"heading": "4 Intersection-closed classes and version-space algorithms", "text": "One simple case where one can perform substantially better than the baseline methods is that of intersectionclosed (or union-closed) classes H, where the functions in H can themselves be compactly described. For example, the class of conjunctions and the class of intervals on the real line are both intersection-closed. For such classes we have the following.\nTheorem 3. If H is intersection-closed, then H can be learned using one round and k hypotheses of total communication.\nProof. Each entity i draws a sample of size O(1\u01eb (d log( 1 \u01eb )+log(k/\u03b4))) and computes the smallest hypothesis hi \u2208 H consistent with its sample, sending hi to the center. The center then computes the smallest hypothesis h such that h \u2287 hi for all i. With probability at least 1 \u2212 \u03b4, h has error at most \u01eb on each Di and therefore error at most \u01eb on D overall.\nExample (conjunctions over {0, 1}d): In this case, the above procedure corresponds to each player sending the bitwise-and of all its positive examples to the center. The center then computes the bitwise-and of the results. The total communication in bits is O(dk). Notice this may be substantially smaller than the O(d2) bits used by the baseline methods.\nExample (boxes in d-Dimensions): In this case, each player can send its smallest consistent hypothesis using 2d values. The center examines the minimum and maximum in each coordinate to compute the minimal h \u2287 hi for all i. Total communication is O(dk) values. In Appendix B.2 we discuss related algorithms based on version spaces."}, {"heading": "5 Reliable-useful learning, parity, and lower bounds", "text": "A classic lower bound in communication complexity states that if two entities each have a set of linear equalities over n variables, then \u2126(n2) bits of communication are needed to determine a feasible solution, based on Ja\u0301Ja\u0301 and Prasanna [1984]. This in turn implies that for proper learning of parity functions, \u2126(n2) bits of communication are required even in the case k = 2, matching the baseline upper bound given via Equivalence Query algorithms.\nInterestingly, however, if one drops the requirement that learning be proper, then for k = 2, parity functions can be learned using only O(n) bits of communication. Moreover, the algorithm is efficient. This is in fact a special case of the following result for classes that are learnable in the reliable-useful learning model of Rivest and Sloan [1988].\nDefinition 1. [Rivest and Sloan, 1988] An algorithm reliably and usefully learns a class H if given poly(n, 1/\u01eb, 1/\u03b4) time and samples, it produces a hypothesis h that on any given example outputs either a correct prediction or the statement \u201cI don\u2019t know\u201d; moreover, with probability at least 1\u2212 \u03b4 the probability mass of examples for which it answers \u201cI don\u2019t know\u201d is at most \u01eb.\nTheorem 4. Suppose H is properly PAC-learnable and is learnable (not necessarily properly) in the reliable-useful model. Then for k = 2, H can be learned in one round with 2 hypotheses of total communication (or 2b bits of communication if each h \u2208 H can be described in b = O(log |H|) bits). Proof. The algorithm is as follows. First, each player i properly PAC-learns f under Di to error \u01eb, creating hypothesis hi \u2208 H. It also learns f reliably-usefully to create hypothesis gi having don\u2019t-know probability mass at most \u01eb under Di. Next, each player i sends hi to the other player (but not gi, because gi may take too many bits to communicate since it is not guaranteed to belong to H). Finally, each player i produces the overall hypothesis \u201cIf my own gi makes a prediction, then use it; else use the hypothesis h3\u2212i that I received from the other player\u201d. Note that each player i\u2019s final hypothesis has error at most \u01eb under both Di (because of gi) and D3\u2212i (because h3\u2212i has error at most \u01eb under D3\u2212i and gi never makes a mistake) and therefore has error at most \u01eb under D.\nExample (parity functions): Parity functions are properly PAC learnable (by an arbitrary consistent solution to the linear equations defined by the sample). They are also learnable in the reliable-useful model by a (non-proper) algorithm that behaves as follows: if the given test example x lies in the span of the training data, then write x as a sum of training examples and predict the corresponding sum of labels. Else output \u201cI don\u2019t know\u201d. Therefore, for k = 2, parity functions are learnable with only O(n) bits of communication.\nInterestingly, the above result does not apply to the case in which there is a center that must also learn a good hypothesis. The reason is that the output of the reliable-useful learning procedure might have large bit-complexity, for example, in the case of parity it has a complexity of \u2126(n2). A similar problem arises when there are more than two entities.3\nHowever, we can extend the result to the case of a center if the overall distribution D over unlabeled data is known to the players. In particular, after running the above protocol to error \u01eb/d, each player can then draw O(d/\u01eb) fresh unlabeled points from D, label them using its learned hypothesis, and then perform proper learning over this data to produce a new hypothesis h\u2032 \u2208 H to send to the center."}, {"heading": "6 Decision Lists", "text": "We now consider the class H of decision lists over d attributes. The best mistake-bound known for this class is O(d2), and its VC-dimension is O(d). Therefore, the baseline algorithms give a total communication complexity, in bits, of O\u0303(d2/\u01eb) for batch learning and O(d3) for the mistake-bound algorithm.4 Here, we present an improved algorithm, requiring a total communication complexity of only O(dk log d) bits. This is a substantial savings over both baseline algorithms, especially when k is small. Note that for constant k and for \u01eb = o(1/d), this bound matches the proper-learning \u2126(d log d) lower bound of Theorem 2.\nTheorem 5. The class of decision lists can be efficiently learned with a total of at most O(dk log d) bits of communication and a number of rounds bounded by the number of alternations in the target decision list f .\nProof. The algorithm operates as follows.\n1. First, each player i draws a sample Si of size O(1\u01eb (d log( 1 \u01eb ) + log(k/\u03b4))), which is sufficient so that\nconsistency with Si is sufficient for achieving low error over Di.\n3It is interesting to note that if we allow communication in the classification phase (and not only during learning) then the center can simply send each test example to all entities, and any entity that classifies it has to be correct.\n4One simple observation is the communication complexity of the mistake-bound algorithm can be reduced to O(d2 log d) by having each player, in the event of a mistake, send only the identity of the offending rule rather than the entire example; this requires only O(log d) bits per mistake. However we will be able to beat this bound substantially.\n2. Next, each player i computes the set Ti of all triplets (j, bj , cj) such that the rule \u201cif xj = bj then cj\u201d is consistent with all examples in Si. (For convenience, use j = 0 to denote the rule \u201celse cj\u201d.) Each player i then sends its set Ti to the center.\n3. The center now computes the intersection of all sets Ti received and broadcasts the result T = \u2229iTi to all players, i.e., the collection of triplets consistent with every Si.\n4. Each player i removes from Si all examples satisfied by T .\n5. Finally, we repeat steps 2,3,4 but in Step 2 each player only sending to the center any new rules that have become consistent since the previous rounds (the center will add them into Ti\u2014note that there is never a need to delete any rule from Ti); similarly in Step 3 the center only sends new rules that have entered the intersection T . The process ends once an \u201celse cj\u201d rule has entered T . The final hypothesis is the decision list consisting of the rules broadcast by the center, in the order they were broadcast.\nTo analyze the above procedure, note first that since each player announces any given triplet at most once, and any triplet can be described using O(log d) bits, the total communication in bits per player is at most O(d log d), for a total of O(dk log d) overall. Next, note that the topmost rule in f will be consistent with each Si, and indeed so will all rules appearing before the first alternation in f . Therefore, these will be present in each Ti and thus contained in T . Thus, each player will remove all examples exiting through any such rule. By induction, after k rounds of the protocol, all players will have removed all examples in their datasets that exit in one of the top k alternations of f , and therefore in the next round all rules in the k+ 1st alternation of f that have not been broadcast already will be output by the center. This implies the number of rounds will be bounded by the number of alternations of f . Finally, note that the hypothesis produced will by design be consistent with each Si since a new rule is added to T only when it is consistent with every Si."}, {"heading": "7 Linear Separators", "text": "We now consider the case of learning homogeneous linear separators in Rd. For this problem, we will for convenience discuss communication in terms of the number of vectors transmitted, rather than bits. However, for data of margin \u03b3, all vectors transmitted can be given using O(d log 1/\u03b3) bits each.\nOne simple case is when D is a radially symmetric distribution such as the symmetric Gaussian distribution centered at the origin, or the uniform distribution on the sphere. In that case, it is known that Ex\u223cD[\u2113(x)x/||x||], is a vector exactly in the direction of the target vector, where \u2113(x) is the label of x. Moreover, an average over O(d/\u01eb2) samples is sufficient to produce an estimate of error at most \u01eb with high probability [Servedio, 2002]. Thus, so long as each player draws a sufficiently large sample Si, we can learn to any desired error \u01eb with a total communication of only k examples: each database simply computes an average over its own data and sends it to the center, which combines the results.\nThe above result, however, requires very precise conditions on the overall distribution. In the following we consider several more general scenarios: learning a large-margin separator when data is \u201cwell-spread\u201d, learning over non-concentrated distributions, and learning linear separators without any additional assumptions."}, {"heading": "7.1 Learning large-margin separators when data is well-spread", "text": "We say that data is \u03b1-well-spread if for all datapoints xi and xj we have |xi\u00b7xj |\n||xi||||xj|| < \u03b1. In the following we\nshow that if data is indeed \u03b1-well-spread for a small value of \u03b1, then the Perceptron algorithm can be used to learn with substantially less communication than that given by just using its mistake-bound directly as in Theorem 1.\nTheorem 6. Suppose that data is \u03b1-well-spread and furthermore that all points have margin at least \u03b3 with the target w\u2217. Then we can find a consistent hypothesis with a version of the Perceptron algorithm using at most O(k(1 + \u03b1/\u03b32)) rounds of communication, each round communicating a single hypothesis.\nProof. We will run the algorithm in meta-rounds. Each meta-round will involve a round robin communication between the players 1, . . . , k. Starting from initial hypothesis w0 = ~0, each player i will in turn run the Perceptron algorithm on its data until it finds a consistent hypothesis wt,i that moreover satisfies |wt \u00b7xi| > 1 for all of its examples xi. It then sends the hypothesis wt,i produced to player i+1 along with the number of updates it performed, who continues this algorithm on its own data, starting from the most recent hypothesis wt,i. When player k sends wt,k to player 1, we start meta-round t + 1. At the start of meta-round t + 1, player 1 counts the number of updates made in the previous meta-round, and if it is less than 1/\u03b1 we stop and output the current hypothesis.\nIt is known that this \u201cMargin Perceptron\u201d algorithm makes at most 3/\u03b32 updates in total.5 Note that if in a meta-round all the players make less than 1/\u03b1 updates in total, then we know the hypothesis will still be consistent with all players\u2019 data. That is because each update can decrease the inner product of the hypothesis with some xi of another player by at most \u03b1. So, if less than 1/\u03b1 updates occur, it implies that every player\u2019s examples are still classified correctly. This implies that the total number of communication meta-rounds until a consistent hypothesis is produced will be at most 1 + 3\u03b1/\u03b32. In particular, this follows because the total number of updates is at most 3/\u03b32, and each round, except the last, makes at least 1/\u03b1 updates."}, {"heading": "7.2 Learning linear separators over non-concentrated distributions", "text": "We now use the analysis of Section 7.1 to achieve good communication bounds for learning linear separators over non-concentrated distributions. Specifically, we say a distribution over the d-dimensional unit sphere is non-concentrated if for some constant c, the probability density on any point x is at most c times greater than that of the uniform distribution over the sphere. The key idea is that in a non-concentrated distribution, nearly all pairs of points will be close to orthogonal, and most points will have reasonable margin with respect to the target.\nTheorem 7. For any non-concentrated distribution D over Rd we can learn to error O(\u01eb) using only O(k2 \u221a d log(dk/\u01eb)/\u01eb2) rounds of communication, each round communicating a single hypothesis vector.\nProof. Note that for any non-concentrated distribution D, the probability that two random examples x, x\u2032 from D satisfy |x \u00b7 x\u2032| > t/ \u221a d is e\u2212O(t\n2). This implies that in a polynomial-size sample (polynomial in d and 1/\u01eb), with high probability, any two examples xi, xj in the sample satisfy |xi \u00b7 xj | \u2264 \u221a\nc\u2032 log(d/\u01eb)/n for some constant c\u2032. Additionally, for any such distribution D there exists another constant c\u2032\u2032 such that for any \u01eb > 0, there is at most \u01eb probability mass of D that lies within margin \u03b3\u01eb = c\u2032\u2032\u01eb/ \u221a d of the target.\n5Because after update \u03c4 we get ||w\u03c4+1||2 \u2264 ||w\u03c4 ||2 + 2\u2113(xi)(w\u03c4 \u00b7 xi) + 1 \u2264 ||w\u03c4 ||2 + 3.\nThese together imply that using the proof idea of Theorem 6, we can learn to error O(\u01eb) using only O(k2 \u221a\nd log(dk/\u01eb)/\u01eb2) communication rounds. Specifically, each player acts as follows. If the hypothesis w given to it has error at most \u01eb on its own data, then it makes no updates and just passes w along. Otherwise, it makes updates using the margin-perceptron algorithm by choosing random examples x from its own distribution Di satisfying \u2113(x)(w \u00b7x) < 1 until the fraction of examples x under Di for which \u2113(x)(w \u00b7x) < 1 is at most \u01eb, sending the final hypothesis produced to the next player. Since before each update, the probability mass under Di of {x : \u2113(x)(w \u00b7 x) < 1} is at least \u01eb, the probability mass of this region under D is at least \u01eb/(2k). This in turn means there is at least a 1/2 probability that the example used for updating has margin at least 12\u03b3\u01eb/(2k) = \u2126(\u01eb/(k \u221a d)) with respect to the target. Thus, the total number of updates made over the entire algorithm will be only O(dk2/\u01eb2). Since the process will halt if all players make fewer than 1/\u03b1 updates in a meta-round, for \u03b1 = \u221a\nc\u2032 log(2dk/\u01eb)/n, this implies the total number of communication meta-rounds is O(k2 \u221a d log(d/\u01eb)/\u01eb2).\nNote that in Section 8 we show how boosting can be implemented communication-efficiently so that any class learnable to constant error rate from a sample of size O(d) can be learned to error \u01eb with total communication of only O(d log 1/\u01eb) examples (plus a small number of additional bits). However, as usual with boosting, this requires a distribution-independent weak learner. The \u201c1/\u01eb2\u201d term in the bound of Theorem 7 comes from the margin that is satisfied by a 1 \u2212 \u01eb fraction of points under a non-concentrated distribution, and so the results of Section 8 do not eliminate it."}, {"heading": "7.3 Learning linear separators without any additional assumptions", "text": "If we are willing to have a bound that depends on the dimension d, then we can run a mistake-bound algorithm for learning linear separators, using Theorem 1. Specifically, we can use a mistake-bound algorithm based on reducing the volume of the version space of consistent hypotheses (which is a polyhedra). The initial volume is 1 and the final volume is \u03b3d, where \u03b3 is the margin of the sample. In every round, each player checks if it has an example that reduces the volume by half (volume of hypotheses consistent with all examples broadcast so far). If it does, it sends it (we are using here the lock-synchronization model). If no player has such an example, then we are done. The hypothesis we have is for each x to predict with the majority of the consistent hypotheses. This gives a total of O(d log 1/\u03b3) examples communicated. In terms of bits, each example has d dimensions, and we can encode each dimension with O(log 1/\u03b3) bits, thus the total number of bits communicated is O(d2 log2 1/\u03b3). Alternatively, we can replace the log 1/\u03b3 term with a log 1/\u01eb term by using a PAC-learning algorithm to learn to constant error rate, and then applying the boosting results of Theorem 10 in Section 8 below.\nIt is natural to ask whether running the Perceptron algorithm in a round-robin fashion could be used to improve the generic O(1/\u03b32) communication bound given by the baseline results of Theorem 1, for general distributions of margin \u03b3. However, in Appendix C we present an example where the Perceptron algorithm indeed requires \u2126(1/\u03b32) rounds.\nTheorem 8. There are inputs for k = 2 with margin \u03b3 such that the Perceptron algorithm takes \u2126(1/\u03b32) rounds."}, {"heading": "8 Boosting for Logarithmic Dependence on 1/\u01eb", "text": "We now consider the general question of dependence of communication on 1/\u01eb, showing how boosting can be used to achieve O(log 1/\u01eb) total communication in O(log 1/\u01eb) rounds for any concept class, and more\ngenerally a tradeoff between communication and rounds. Boosting algorithms provide a mechanism to produce an \u01eb-error hypothesis given access only to a weak learning oracle, which on any distribution finds a hypothesis of error at most some value \u03b2 < 1/2 (i.e., a bias \u03b3 = 1/2 \u2212 \u03b2 > 0). Most boosting algorithms are weight-based, meaning they assign weights to each example x based solely on the performance of the hypotheses generated so far on x, with probabilities proportional to weights.6 We show here that any weight-based boosting algorithm can be applied to achieve strong learning of any class with low overall communication. The key idea is that in each round, players need only send enough data to the center for it to produce a weak hypothesis. Once the weak hypothesis is constructed and broadcast to all the players, the players can use it to separately re-weight their own distributions and send data for the next round. No matter how large or small the weights become, each round only needs a small amount of data to be transmitted. Formally, we show the following:\nLemma 9. Given any weight-based boosting algorithm that achieves error \u01eb by making r(\u01eb, \u03b2) calls to a \u03b2-weak learning oracle for H, we can construct a distributed learning algorithm achieving error \u01eb that uses O(r(\u01eb, \u03b2)) rounds, each involving O((d/\u03b2) log(1/\u03b2)) examples and an additional O(k log(d/\u03b2)) bits of communication per round.\nProof. The key property of weight-based boosting algorithms that we will use is that they maintain a current distribution such that the probability mass on any example x is solely a function of the performance of the weak-hypotheses seen so far on x, except for a normalization term that can be communicated efficiently. This will allow us to perform boosting in a distributed fashion. Specifically, we run the boosting algorithm in rounds, as follows.\nInitialization: Each player i will have a weight wi,t for round t. We begin with wi,0 = 1 for all i. Let Wt = \u2211k i=1 wi,t so initially W0 = k. These weights will all be known to the center. Each player i\nwill also have a large weighted sample Si, drawn from Di, known only to itself. Si will be weighted according to the specific boosting algorithm (and for all standard boosting algorithms, the points in Si begin with equal weights). We now repeat the following three steps for t = 1, 2, 3, . . ..\n1. Pre-sampling The center determines the number of samples ni,t to request from each player i by sampling O( d\u03b2 log 1 \u03b2 ) times from the multinomial distribution wi,t\u22121/Wt\u22121. It then sends each player i\nthe number ni,t, which requires only O(log d\u03b2 ) bits.\n2. Sampling Each player i samples ni,t examples from its local sample Si in proportion to its own internal example weights, and sends them to the center.\n3. Weak-learning The center takes the union of the received examples and uses these O( d\u03b2 log 1 \u03b2 ) samples\nto produce a weak hypothesis ht of error at most \u03b2/2 over the current weighted distribution, which it then sends to the players.7\n4. Updating Each player i, given ht, computes the new weight of each example in Si using the underlying boosting algorithm and sends their sum wi,t to the center. This sum can be sent to sufficient accuracy using O(log 1\u03b2 ) bits.\n6E.g., Schapire [1990], Freund [1990], Freund and Schapire [1997]. (For Adaboost, we are considering the version that uses a fixed upper bound \u03b2 on the error of the weak hypotheses.) Normalization may of course be based on overall performance.\n7In fact, because we have a broadcast model, technically the players each can observe all examples sent in step (2) and so can simulate the center in this step.\nIn each round, steps (1) and (2) ensure that the center receives O((d/\u03b2) log(1/\u03b2)) examples distributed according to a distribution D\u2032 matching that given by the boosting algorithm, except for small rounding error due to the number of bits sent in step (4). Specifically, the variation distance between D\u2032 and the distribution given by the boosting algorithm is at most \u03b2/2. Therefore, in step (3), it computes a hypothesis ht with error at most \u03b2/2+\u03b2/2 = \u03b2 with respect to the current distribution given by the boosting algorithm. In step (4), the examples in all sets Si then have their weights updated as determined by the boosting algorithm, and the values wi,t transmitted ensure that the normalizations are correct. Therefore, we are simulating the underlying boosting algorithm having access to a \u03b2-weak learner, and so the number of rounds is r(\u01eb, \u03b2). The overall communication per round is O((d/\u03b2) log(1/\u03b2)) examples plus O(k log(d/\u03b2)) bits for communicating the numbers ni,t and wi,t, as desired.\nBy adjusting the parameter \u03b2, we can trade off between the number of rounds and communication complexity. In particular, using Adaboost [Freund and Schapire, 1997] in Lemma 9 yields the following result (plugging in \u03b2 = 1/4 or \u03b2 = \u01eb1/c respectively):\nTheorem 10. Any class H can be learned to error \u01eb in O(log 1\u01eb ) rounds and O(d) examples plus O(k log d) bits of communication per round. For any c \u2265 1, H can be learned to error \u01eb in O(c) rounds and O( d\n\u01eb1/c log 1\u01eb ) examples plus O(k log d \u01eb ) bits communicated per round.\nThus, any class of VC-dimension d can be learned using O(log 1\u01eb ) rounds and a total of O(d log 1 \u01eb )\nexamples, plus a small number of extra bits of communication."}, {"heading": "9 Agnostic Learning", "text": "Balcan and Hanneke [2012] show that any class H can be agnostically learned to error O(opt(H))+ \u01eb using only O\u0303(d log 1/\u01eb) label requests, in an active learning model where class-conditional queries are allowed. We can use the core of their result to agnostically learn any finite class H to error O(opt(H)) + \u01eb in our setting, with a total communication that depends only (poly)logarithmically on 1/\u01eb. The key idea is that we can simulate their robust generalized halving algorithm using communication proportional only to the number of class-conditional queries their algorithm makes.\nTheorem 11. Any finite class H can be learned to error O(opt(H)) + \u01eb with a total communication of O(k log(|H|) log log(|H|) log(1/\u01eb)) examples and O(k log(|H|) log log(|H|) log2(1/\u01eb)) additional bits. The latter may be eliminated if shared randomness is available.\nProof. We prove this result by simulating the robust generalized halving algorithm of Balcan and Hanneke [2012], for the case of finite hypothesis spaces, in a communication-efficient manner.8 In particular, the algorithm operates as follows. For this procedure, N = O(log log |H|) and s = O(1/(opt(H) + \u01eb)) is such that the probability that the best hypothesis in H will have some error on a set of s examples is a small constant..\n1. We begin by drawing N sets S1, . . . , SN of size s from D. This can be implemented communicationefficiently as follows. For j = 1, . . . , N , player 1 makes s draws from {1, . . . , k} to determine the number nij of points in Sj that should come from each Di. Player 1 then sends each player i the list (ni1, ni2, . . . , niN ), who draws (but keeps internally and does not send) nij examples of Sj for each\n8The algorithm of Balcan and Hanneke [2012] for the case of infinite hypothesis spaces begins by using a large unlabeled sample to determine a small \u01eb-cover of H. This appears to be difficult to simulate communication-efficiently.\n1 \u2264 j \u2264 N . Total communication: O(kN log(s)) bits. Note that if shared randomness is available, then the computation of nij can be simulated by each player and so in that case no communication is needed in this step.\n2. Next we determine which sets Sj contain an example on which the majority-vote hypothesis over H, maj(H), makes a mistake, and identify one such example (x\u0303j, y\u0303j) for each such set. We can implement this communication-efficiently by having each player i evaluate maj(H) on its own portion of each set Sj and broadcast a mistake for each set on which at least one mistake is made. Total communication: O(kN) examples.\n3. If no more than N/3 sets Sj contained a mistake for maj(H) then halt. Else, remove from H each h that made mistakes on more than N/9 of the identified examples (x\u0303j , y\u0303j), and go to (1). This step can be implemented separately by each player without any communication.\nBalcan and Hanneke [2012] show that with high probability the above process halts within O(log |H|) rounds, does not remove the optimal h \u2208 H, and furthermore that when it halts, maj(H) has error O(opt(H)) + \u01eb. The total amount of communication is therefore O(k log(|H|) log log(|H|)) examples and O(k log(|H|) log log(|H|) log(1/\u01eb)) additional bits. The above has been assuming that the value of opt(H) is known; if not then one can perform binary search, multiplying the above quantities by an additional O(log(1/\u01eb)) term. Thus, we achieve the desired error rate within the desired communication bounds."}, {"heading": "10 Privacy", "text": "In the context of distributed learning, it is also natural to consider the question of privacy. We begin by considering the well-studied notion of differential privacy with respect to the examples, showing how this can be achieved in many cases without any increase in communication costs. We then consider the case that one would like to provide additional privacy guarantees for the players themselves. One option is to view each player as a single (large) example, but this requires many players to achieve any nontrivial accuracy guarantees. Thus, we also consider a natural notion of distributional privacy, in which players do not view their distribution Di as sensitive, but rather only the sample Si drawn from it. We analyze how large a sample is sufficient so that players can achieve accurate learning while not revealing more information about their sample than is inherent in the distribution it was drawn from. We now examine each notion in turn, and for each we explore how it can be achieved and the effect on communication."}, {"heading": "10.1 Differential privacy with respect to individual examples", "text": "In this setting we imagine that each entity i (e.g., a hospital) is responsible for the privacy of each example x \u2208 Si (e.g., its patients). In particular, suppose \u03c3 denotes a sequence of interactions between entity i and the other entities or center, and \u03b1 > 0 is a given privacy parameter. Differential privacy asks that for any Si and any modification S\u2032i of Si in which any one example has been arbitrarily changed, for all \u03c3 we have e\u2212\u03b1 \u2264 PrSi(\u03c3)/PrS\u2032i(\u03c3) \u2264 e\n\u03b1, where probabilities are over internal randomization of entity i. (See Dwork [2006, 2008, 2009] for a discussion of motivations and properties of differential privacy and a survey of results).\nIn our case, one natural approach for achieving privacy is to require that all interaction with each entity i be in the form of statistical queries [Kearns, 1998]. It is known that statistical queries can be implemented in a privacy-preserving manner [Dwork and Nissim, 2004, Blum et al., 2005, Kasiviswanathan et al., 2008],\nand in particular that a sample of size O(max[M\u03b1\u03c4 , M \u03c42 ] log(M/\u03b4)) is sufficient to preserve privacy while answering M statistical queries to tolerance \u03c4 with probability 1\u2212 \u03b4. For completness, we present the proof below.\nTheorem 12. [Dwork and Nissim, 2004, Blum et al., 2005, Kasiviswanathan et al., 2008] If H is learnable using M statistical queries of tolerance \u03c4 , then H is learnable preserving differential privacy with privacy parameter \u03b1 from a sample S of size O(max[M\u03b1\u03c4 , M \u03c42 ] log(M/\u03b4)).\nProof. For a single statistical query, privacy with parameter \u03b1\u2032 can be achieved by adding Laplace noise of width O( 1\u03b1\u2032|S|) to the empirical answer of the query on S. That is because changing a single entry in S can change the empirical answer by at most 1/|S|, so by adding such noise we have that for any v, PrS(v)/PrS\u2032(v) \u2264 e\u03b1 \u2032 . Note that with probability at least 1 \u2212 \u03b4\u2032, the amount of noise added to any given answer is at most O( 1\u03b1\u2032|S| log(1/\u03b4 \u2032)). Thus, if the overall algorithm requires M queries to be answered to tolerance \u03c4 , then setting \u03b1\u2032 = \u03b1/M, \u03b4\u2032 = \u03b4/(2M), \u03c4 = O( 1\u03b1\u2032|S| log(1/\u03b4 \u2032)), privacy can be achieved so long as we have |S| = O(max[M\u03b1\u03c4 , M\u03c42 ] log(M/\u03b4)), where the second term of the max is the sample size needed to achieve tolerance \u03c4 for M queries even without privacy considerations. As described in Dwork et al. [2010], one can achieve a somewhat weaker privacy guarantee using \u03b1\u2032 = O(\u03b1/ \u221a M).\nHowever, this generic approach may involve significant communication overhead over the best nonprivate method. Instead, in many cases we can achieve privacy without any communication overhead at all by performing statistical queries internally to the entities. For example, in the case of intersection-closed classes, we have the following privacy-preserving version of Theorem 3.\nTheorem 13. If H can be properly learned via statistical queries to D+ only, then H can be learned using one round and k hypotheses of total communication while preserving differential privacy.\nProof. Each entity i learns a hypothesis hi \u2208 H using privacy-preserving statistical queries to its own D+i , and sends hi to the center. Note that hi \u2286 f because the statistical query algorithm must succeed for any possible D\u2212. Therefore, the center can simply compute the minimal h \u2208 H such that h \u2287 hi for all i, which will have error at most \u01eb over each Di and therefore error at most \u01eb over D.\nFor instance, the class of conjunctions can be learned via statistical queries to D+ only by producing the conjunction of all variables xj such that PrD+i\n[xj = 0] \u2264 \u01eb2n \u00b1 \u03c4 , for \u03c4 = \u01eb2n . Thus, Theorem 13 implies that conjunctions can be learned in a privacy-preserving manner without any communication overhead.\nIndeed, in all the algorithms for specific classes given in this paper, except for parity functions, the interaction between entities and their data can be simulated with statistical queries. For example, the decision list algorithm of Theorem 5 can be implemented by having each entity identify rules to send to the center via statistical queries to Di. Thus, in these or any other cases where the information required by the protocol can be extracted by each entity using statistical queries to its own data, there is no communication overhead due to preserving privacy."}, {"heading": "10.2 Differential privacy with respect to the entities", "text": "One could also ask for a stronger privacy guarantee, that each entity be able to plausibly claim to be holding any other dataset it wishes; that is, to require e\u2212\u03b1 \u2264 PrSi(\u03c3)/PrS\u2032(\u03c3) \u2264 e\u03b1 for all Si and all (even unrelated) S\u2032. This in fact corresponds precisely to the local privacy notion of Kasiviswanathan et al. [2008],\nwhere in essence the only privacy-preserving mechanisms possible are via randomized-response.9 They show that any statistical query algorithm can be implemented in such a setting; however, because each entity is now viewed as essentially a single datapoint, to achieve any nontrivial accuracy, k must be quite large."}, {"heading": "10.3 Distributional privacy", "text": "If the number of entities is small, but we still want privacy with respect to the entities themselves, then one type of privacy we can achieve is a notion of distributional privacy. Here we guarantee that that each player i reveals (essentially) no more information about its own sample Si than is inherent in Di itself. That is, we think of Si as \u201csensitive\u201d but Di as \u201cnon-sensitive\u201d. Specifically, let us say a probabilistic mechanism A for answering a request q satisfies (\u03b1, \u03b4) distributional privacy if\nPr S,S\u2032\u223cDi\n[\n\u2200v, e\u2212\u03b1 \u2264 Pr A (A(S, q) = v)/Pr A (A(S\u2032, q) = v) \u2264 e\u03b1\n]\n\u2265 1\u2212 \u03b4.\nIn other words, with high probability, two random samples S, S\u2032 from Di have nearly the same probability of producing any given answer to request q. Blum et al. [2008] introduce a similar privacy notion,10 which they show is strictly stronger than differential privacy, but do not provide efficient algorithms. Here, we show how distributional privacy can be implemented efficiently.\nNotice that in this context, an ideal privacy preserving mechanism would be for player i to somehow use its sample to reconstruct Di perfectly and then draw a \u201cfake\u201d sample from Di to use in its communication protocol. However, since reconstructing Di perfectly is not in general possible, we instead will work via statistical queries.\nTheorem 14. If H is learnable using M statistical queries of tolerance \u03c4 , then H is learnable preserving distributional privacy from a sample of size O(M\n2 log3(M/\u03b4) \u03b12\u03c42 ).\nProof. We will show that we can achieve distributional privacy using statistical queries by adding additional Laplace noise beyond that required solely for differential privacy of the form in Section 10.1.\nSpecifically, for any statistical query q, Hoeffding bounds imply that with probability at least 1 \u2212 \u03b4\u2032, two random samples of size N will produce answers within \u03b2 = O( \u221a\nlog(1/\u03b4\u2032)/N ) of each other (because each will be within \u03b2/2 of the expectation with probability at least 1 \u2212 \u03b4\u2032/2). This quantity \u03b2 can now be viewed as the \u201cglobal sensitivity\u201d of query q for distributional privacy. In particular, it suffices to add Laplace noise of width O(\u03b2/\u03b1\u2032) in order to achieve privacy parameter \u03b1\u2032 for this query q because we have that with probability at least 1 \u2212 \u03b4\u2032, for two random samples S, S\u2032 of size N , for any v, Pr(A(S, q) = v)/Pr(A(S\u2032, q) = v) \u2264 e\u03b2/(\u03b2/\u03b1\u2032) = e\u03b1\u2032 . Note that this has the property that with probability at least 1\u2212 \u03b4\u2032, the amount of noise added to any given answer is at most O((\u03b2/\u03b1\u2032) log(1/\u03b4\u2032)).\nIf we have a total of M queries, then it suffices for preserving privacy over the entire sequence to set \u03b1\u2032 = \u03b1/M and \u03b4\u2032 = \u03b4/M . In order to have each query answered with high probability to within \u00b1\u03c4 , it suffices to have \u03b2 + (\u03b2/\u03b1\u2032) log(1/\u03b4\u2032) \u2264 c\u03c4 for some constant c, where the additional (low-order) \u03b2 term is just the statistical estimation error without added noise. Solving for N , we find that a sample of size N = O(M 2 log3(M/\u03b4)\n\u03b12\u03c42 ) is sufficient to maintain distributional privacy while answering each query to\ntolerance \u03c4 , as desired.\n9For example, if an entity is asked a question such as \u201cdo you have an example with xi = 1\u201d, then it flips a coin and with probability 1/2 + \u03b1\u2032 gives the correct answer and with probability 1/2\u2212 \u03b1\u2032 gives the incorrect answer, for some appropriate \u03b1\u2032.\n10In the notion of Blum et al. [2008], Di is uniform over some domain and sampling is done without replacement.\nAs in the results of Section 10.1, Theorem 14 implies that if each player can run its portion of a desired communication protocol while only interacting with its own data via statistical queries, then so long as |Si| is sufficiently large, we can implement distributional privacy without any communication penalty by performing internal statistical queries privately as above. For example, combining Theorem 14 with the proof of Theorem 13 we have:\nTheorem 15. If H can be properly learned via statistical queries to D+ only, then H can be learned using one round and k hypotheses of total communication while preserving distributional privacy."}, {"heading": "A Table of results", "text": "Class / Category Communication Efficient? Conjunctions over {0, 1}n O(nk) bits yes Parity functions over {0, 1}n , k = 2 O(n) bits yes Decision lists over {0, 1}n O(nk log n) bits yes Linear separators in Rd O(d log(1/\u01eb)) examples\u2217 yes\nunder radially-symmetric D O(k) examples yes under \u03b1-well-spread D O(k(1 + \u03b1/\u03b32)) hypotheses yes under non-concentrated D O(k2 \u221a\nd log(dk/\u01eb)/\u01eb2) hyps yes General Intersection-Closed k hypotheses see Note 1 below Boosting O(d log 1/\u01eb) examples\u2217 see Note 2 below Agnostic learning O\u0303(k log(|H|) log(1/\u01eb)) exs\u2217 see Note 3 below\n\u2217: plus low-order additional bits of communication. Note 1: Efficient if can compute the smallest consistent hypothesis in H efficiently, and for any given h1, . . . , hk , can efficiently compute the minimum h \u2287 hi for all i. Note 2: Efficient if can efficiently weak-learn with O(d) examples. Note 3: Efficient if can efficiently run robust halving algorithm for H."}, {"heading": "B Additional simple cases", "text": "B.1 Distribution-based algorithms\nAn alternative basic approach, in settings where it can be done succinctly, is for each entity i to send to the center a representation of its (approximate) distribution over labeled data. Then, given the descriptions, the center can deduce an approximation of the overall distribution over labeled data and search for a near optimal hypothesis. This example is especially relevant for the agnostic 1-dimensional case, e.g., a union of d intervals over X = [0, 1]. Each entity first simply sorts the points, and determines d/\u01eb border points defining regions of probability mass (approximately) \u01eb/d. For each segment between two border points, the entity reports the fraction of positive versus negative examples. It additionally sends the border points themselves. This communication requires O(d/\u01eb) border points and an additional O(log d/\u01eb) bits to report the fractions within each such interval, per entity. Given this information, the center can approximate the best union of d intervals with error O(\u01eb). Note that the supervised learning baseline algorithm would have a bound of O\u0303(d/\u01eb2) in terms of the number of points communicated.\nTheorem 16. There is an algorithm for agnostically learning a union of d intervals that uses one round and O(kd/\u01eb) values (each either a datapoint or a log d/\u01eb bit integer), such that the final hypothesis produced has error opt(H) + \u01eb.\nB.2 Version space algorithms\nAnother simple case where one can perform well is when the version space can be compactly described. The version space of H given a sample Si is the set of all h \u2208 H which are consistent with Si. Denote this set by V erSp(H, Si). Generic Version Space Algorithm: Each entity sends V erSp(H, Si) to the center. The center computes V = \u2229iV erSp(H, Si). Note that V = V erSp(H,\u222aiSi). The center can send either V or some h \u2208 V .\nExample (linear separators in [0, 1]2): Assume that the points have margin \u03b3. We can cover a convex set in [0, 1]2 using 1/\u03b32 rectangles, whose union completely covers the convex set, and is completely covered by the convex set extended by \u03b3. Each entity does this for its positive and negative regions, sending this (approximate) version space to the center. This gives a one-round algorithm with communication cost of O(1/\u03b32) points."}, {"heading": "C Linear Separators: Margin lower bound", "text": "Proof. (Theorem 8) Suppose we have two players, each with their own set of examples, such that the combined dataset has a linear separator of margin \u03b3. Suppose furthermore we run the perceptron algorithm where each player performs updates on their own dataset until consistent (or at least until low-error) and then passes the hypothesis on to the other player, with the process continuing until one player receives a hypothesis that is already low-error on its own data. How many rounds can this take in the worst case?\nBelow is an example showing a problematic case where this can indeed result in \u2126(1/\u03b32) rounds. In this example, there are 3 dimensions and the target vector is (0, 1, 0). Player 1 has the positive examples, with 49% of its data points at location (1, \u03b3, 3\u03b3) and 49% of its data points are at location (1, \u03b3,\u2212\u03b3). The remainder of player 1\u2019s points are at location (1, \u03b3, \u03b3). Player 2 has the negative examples. Half of its data points are at location (1,\u2212\u03b3,\u22123\u03b3) and half of its data points are at location (1,\u2212\u03b3, \u03b3).\nThe following demonstrates a bad sequence of events that can occur, with the two players essentially fighting over the first coordinate:\nplayer updates using producing hypothesis player 1 (1, \u03b3, \u03b3), + (1, \u03b3, \u03b3) player 2 (1,\u2212\u03b3,\u22123\u03b3), \u2212 (0, 2\u03b3, 4\u03b3) player 2 (1,\u2212\u03b3, \u03b3), \u2212 (\u22121, 3\u03b3, 3\u03b3) player 1 (1, \u03b3, 3\u03b3), + (0, 4\u03b3, 6\u03b3) player 1 (1, \u03b3,\u2212\u03b3), + (1, 5\u03b3, 5\u03b3) player 2 (1,\u2212\u03b3,\u22123\u03b3), \u2212 (0, 6\u03b3, 8\u03b3) player 2 (1,\u2212\u03b3, \u03b3), \u2212 (\u22121, 7\u03b3, 7\u03b3) player 1 (1, \u03b3, 3\u03b3), + (0, 8\u03b3, 10\u03b3) player 1 (1, \u03b3,\u2212\u03b3), + (1, 9\u03b3, 9\u03b3)\n...\nNotice that when the hypothesis looks like (\u22121, k\u03b3, k\u03b3), then the dot-product with the example (1, \u03b3, 3\u03b3) from player 1 is \u22121+4k\u03b32. So long as this is negative, player 1 will make two updates producing hypothesis (1, (k + 2)\u03b3, (k + 2)\u03b3). Then, so long as 4(k + 2)\u03b32 < 1, player 2 will make two updates producing hypothesis (\u22121, (k + 4)\u03b3, (k + 4)\u03b3). Thus, this procedure will continue for \u2126(1/\u03b32) rounds."}], "references": [{"title": "Robust interactive learning", "author": ["Maria-Florina Balcan", "Steve Hanneke"], "venue": "In Proc. 25th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Balcan and Hanneke.,? \\Q2012\\E", "shortCiteRegEx": "Balcan and Hanneke.", "year": 2012}, {"title": "Practical privacy: the SuLQ framework", "author": ["Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim"], "venue": "In Proc. 24th ACM Symposium on Principles of Database Systems (PODS),", "citeRegEx": "Blum et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2005}, {"title": "A learning theory approach to non-interactive database privacy", "author": ["Avrim Blum", "Katrina Ligett", "Aaron Roth"], "venue": "In Proc. 40th Annual ACM Symp. Theory of Computing,", "citeRegEx": "Blum et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2008}, {"title": "Exact learning of formulas in parallel", "author": ["Nader H. Bshouty"], "venue": "Machine Learning,", "citeRegEx": "Bshouty.,? \\Q1997\\E", "shortCiteRegEx": "Bshouty.", "year": 1997}, {"title": "Logistic regression, adaboost and bregman distances", "author": ["Michael Collins", "Robert E. Schapire", "Yoram Singer"], "venue": "Machine Learning,", "citeRegEx": "Collins et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2002}, {"title": "Protocols for learning classifiers on distributed data", "author": ["Hal Daume III", "Jeff Phillips", "Avishek Saha", "Suresh Venkatasubramanian"], "venue": "In International Conference on Artificial Intelligence and Statistics (AIStats),", "citeRegEx": "III et al\\.,? \\Q2012\\E", "shortCiteRegEx": "III et al\\.", "year": 2012}, {"title": "Efficient protocols for distributed classification and optimization", "author": ["Hal Daume III", "Jeff Phillips", "Avishek Saha", "Suresh Venkatasubramanian"], "venue": "CoRR, abs/1204.3523,", "citeRegEx": "III et al\\.,? \\Q2012\\E", "shortCiteRegEx": "III et al\\.", "year": 2012}, {"title": "Optimal distributed online prediction", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Dekel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2011}, {"title": "Differential privacy", "author": ["Cynthia Dwork"], "venue": null, "citeRegEx": "Dwork.,? \\Q2006\\E", "shortCiteRegEx": "Dwork.", "year": 2006}, {"title": "Differential privacy: A survey of results", "author": ["Cynthia Dwork"], "venue": "In TAMC, pages", "citeRegEx": "Dwork.,? \\Q2008\\E", "shortCiteRegEx": "Dwork.", "year": 2008}, {"title": "The differential privacy frontier (extended abstract)", "author": ["Cynthia Dwork"], "venue": "In TCC, pages 496\u2013502,", "citeRegEx": "Dwork.,? \\Q2009\\E", "shortCiteRegEx": "Dwork.", "year": 2009}, {"title": "Privacy-preserving datamining on vertically partitioned databases", "author": ["Cynthia Dwork", "Kobbi Nissim"], "venue": "In Proceedings of CRYPTO, Lecture Notes in Computer Science,", "citeRegEx": "Dwork and Nissim.,? \\Q2004\\E", "shortCiteRegEx": "Dwork and Nissim.", "year": 2004}, {"title": "Boosting and differential privacy", "author": ["Cynthia Dwork", "Guy N. Rothblum", "Salil P. Vadhan"], "venue": "In FOCS,", "citeRegEx": "Dwork et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2010}, {"title": "Boosting a weak learning algorithm by majority", "author": ["Yoav Freund"], "venue": "In COLT, pages 202\u2013216,", "citeRegEx": "Freund.,? \\Q1990\\E", "shortCiteRegEx": "Freund.", "year": 1990}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "J. Comput. Syst. Sci.,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "On the complexity of teaching", "author": ["Sally A. Goldman", "Michael J. Kearns"], "venue": "In Proceedings of COLT", "citeRegEx": "Goldman and Kearns.,? \\Q1991\\E", "shortCiteRegEx": "Goldman and Kearns.", "year": 1991}, {"title": "The art of multiprocessor programming", "author": ["Maurice Herlihy", "Nir Shavit"], "venue": null, "citeRegEx": "Herlihy and Shavit.,? \\Q2008\\E", "shortCiteRegEx": "Herlihy and Shavit.", "year": 2008}, {"title": "Information transfer in distributed computing with applications to vlsi", "author": ["Joseph J\u00e1J\u00e1", "Viktor K. Prasanna"], "venue": "J. ACM,", "citeRegEx": "J\u00e1J\u00e1 and Prasanna.,? \\Q1984\\E", "shortCiteRegEx": "J\u00e1J\u00e1 and Prasanna.", "year": 1984}, {"title": "What Can We Learn Privately", "author": ["Shiva Kasiviswanathan", "Homin Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith"], "venue": "In Proc. 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "Kasiviswanathan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kasiviswanathan et al\\.", "year": 2008}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael Kearns"], "venue": "Journal of the ACM,", "citeRegEx": "Kearns.,? \\Q1998\\E", "shortCiteRegEx": "Kearns.", "year": 1998}, {"title": "Communication complexity", "author": ["Eyal Kushilevitz", "Noam Nisan"], "venue": null, "citeRegEx": "Kushilevitz and Nisan.,? \\Q1997\\E", "shortCiteRegEx": "Kushilevitz and Nisan.", "year": 1997}, {"title": "Algorithms and hardness results for parallel large margin learning", "author": ["Phil Long", "Rocco Servedio"], "venue": "In NIPS,", "citeRegEx": "Long and Servedio.,? \\Q2011\\E", "shortCiteRegEx": "Long and Servedio.", "year": 2011}, {"title": "Distributed computing: a locality-sensitive approach", "author": ["David Peleg"], "venue": "Society for Industrial and Applied Mathematics, Philadelphia, PA,", "citeRegEx": "Peleg.,? \\Q2000\\E", "shortCiteRegEx": "Peleg.", "year": 2000}, {"title": "Learning complicated concepts reliably and usefully", "author": ["Ronald L. Rivest", "Robert Sloan"], "venue": "In Proceedings AAAI-88,", "citeRegEx": "Rivest and Sloan.,? \\Q1988\\E", "shortCiteRegEx": "Rivest and Sloan.", "year": 1988}, {"title": "The strength of weak learnability", "author": ["Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Schapire.,? \\Q1990\\E", "shortCiteRegEx": "Schapire.", "year": 1990}, {"title": "Perceptron, Winnow, and PAC learning", "author": ["Rocco Servedio"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Servedio.,? \\Q2002\\E", "shortCiteRegEx": "Servedio.", "year": 2002}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Alexander J. Smola", "Lihong Li"], "venue": "In NIPS,", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Further we show that, ignoring computation, agnostic learning can be performed to error O(opt(H)) + \u01eb with logarithmic dependence on 1/\u01eb, by adapting results of Balcan and Hanneke [2012]. In terms of specific classes, we present several tight bounds including a \u0398(d log d) bound on the communication in bits needed for learning the class of decision lists over {0, 1}d.", "startOffset": 161, "endOffset": 187}, {"referenceID": 23, "context": "This is a by-product of a general result regarding concepts learnable in the reliable-useful framework of Rivest and Sloan [1988]. For a table of results, see Appendix A.", "startOffset": 106, "endOffset": 130}, {"referenceID": 3, "context": "Bshouty [1997] shows that many simple classes that can be PAC learned can not be efficiently learned in parallel with a polynomial number of processors.", "startOffset": 0, "endOffset": 15}, {"referenceID": 3, "context": "Bshouty [1997] shows that many simple classes that can be PAC learned can not be efficiently learned in parallel with a polynomial number of processors. Long and Servedio [2011] show a parallel algorithm for large margin classifiers running in time O(1/\u03b3) compared to more naive implementations costing of \u03a9(1/\u03b32), where \u03b3 is the margin.", "startOffset": 0, "endOffset": 178}, {"referenceID": 3, "context": "Bshouty [1997] shows that many simple classes that can be PAC learned can not be efficiently learned in parallel with a polynomial number of processors. Long and Servedio [2011] show a parallel algorithm for large margin classifiers running in time O(1/\u03b3) compared to more naive implementations costing of \u03a9(1/\u03b32), where \u03b3 is the margin. They also show an impossibility result regarding boosting, namely that the ability to call the weak learner oracle multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required. Collins et al. [2002] give an online algorithm that uses a parallel-update method for the logistic loss, and Zinkevich et al.", "startOffset": 0, "endOffset": 620}, {"referenceID": 3, "context": "Bshouty [1997] shows that many simple classes that can be PAC learned can not be efficiently learned in parallel with a polynomial number of processors. Long and Servedio [2011] show a parallel algorithm for large margin classifiers running in time O(1/\u03b3) compared to more naive implementations costing of \u03a9(1/\u03b32), where \u03b3 is the margin. They also show an impossibility result regarding boosting, namely that the ability to call the weak learner oracle multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required. Collins et al. [2002] give an online algorithm that uses a parallel-update method for the logistic loss, and Zinkevich et al. [2010] give a detailed analysis of a parallel stochastic gradient descent in which each machine processes a random subset of the overall data, combining hypotheses at the very end.", "startOffset": 0, "endOffset": 731}, {"referenceID": 3, "context": "Bshouty [1997] shows that many simple classes that can be PAC learned can not be efficiently learned in parallel with a polynomial number of processors. Long and Servedio [2011] show a parallel algorithm for large margin classifiers running in time O(1/\u03b3) compared to more naive implementations costing of \u03a9(1/\u03b32), where \u03b3 is the margin. They also show an impossibility result regarding boosting, namely that the ability to call the weak learner oracle multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required. Collins et al. [2002] give an online algorithm that uses a parallel-update method for the logistic loss, and Zinkevich et al. [2010] give a detailed analysis of a parallel stochastic gradient descent in which each machine processes a random subset of the overall data, combining hypotheses at the very end. All of the above results are mainly interested in reducing the time required to perform learning when data can be randomly or algorithmically partitioned among processors; in contrast, our focus is on a setting in which we begin with data arbitrarily partitioned among the entities. Dekel et al. [2011] consider distributed online prediction with arbitrary partitioning of data streams, achieving strong regret bounds; however, in their setting the goal of entities is to perform well on their own sequence of data.", "startOffset": 0, "endOffset": 1208}, {"referenceID": 3, "context": "Bshouty [1997] shows that many simple classes that can be PAC learned can not be efficiently learned in parallel with a polynomial number of processors. Long and Servedio [2011] show a parallel algorithm for large margin classifiers running in time O(1/\u03b3) compared to more naive implementations costing of \u03a9(1/\u03b32), where \u03b3 is the margin. They also show an impossibility result regarding boosting, namely that the ability to call the weak learner oracle multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required. Collins et al. [2002] give an online algorithm that uses a parallel-update method for the logistic loss, and Zinkevich et al. [2010] give a detailed analysis of a parallel stochastic gradient descent in which each machine processes a random subset of the overall data, combining hypotheses at the very end. All of the above results are mainly interested in reducing the time required to perform learning when data can be randomly or algorithmically partitioned among processors; in contrast, our focus is on a setting in which we begin with data arbitrarily partitioned among the entities. Dekel et al. [2011] consider distributed online prediction with arbitrary partitioning of data streams, achieving strong regret bounds; however, in their setting the goal of entities is to perform well on their own sequence of data. In very recent independent work, Daume III et al. [2012a] examine a setting much like that considered here, in which parties each have an arbitrary partition of an overall dataset, and the goal is to achieve low error over the entire distribution.", "startOffset": 0, "endOffset": 1479}, {"referenceID": 3, "context": "Bshouty [1997] shows that many simple classes that can be PAC learned can not be efficiently learned in parallel with a polynomial number of processors. Long and Servedio [2011] show a parallel algorithm for large margin classifiers running in time O(1/\u03b3) compared to more naive implementations costing of \u03a9(1/\u03b32), where \u03b3 is the margin. They also show an impossibility result regarding boosting, namely that the ability to call the weak learner oracle multiple times in parallel within a single boosting stage does not reduce the overall number of successive stages of boosting that are required. Collins et al. [2002] give an online algorithm that uses a parallel-update method for the logistic loss, and Zinkevich et al. [2010] give a detailed analysis of a parallel stochastic gradient descent in which each machine processes a random subset of the overall data, combining hypotheses at the very end. All of the above results are mainly interested in reducing the time required to perform learning when data can be randomly or algorithmically partitioned among processors; in contrast, our focus is on a setting in which we begin with data arbitrarily partitioned among the entities. Dekel et al. [2011] consider distributed online prediction with arbitrary partitioning of data streams, achieving strong regret bounds; however, in their setting the goal of entities is to perform well on their own sequence of data. In very recent independent work, Daume III et al. [2012a] examine a setting much like that considered here, in which parties each have an arbitrary partition of an overall dataset, and the goal is to achieve low error over the entire distribution. They present comunication-efficient learning algorithms for axis-parallel boxes as well as for learning linear separators in R2. Daume III et al. [2012b], also independently of our work, extend this to the case of linear separators in Rd, achieving bounds similar to those obtained via our distributed boosting results.", "startOffset": 0, "endOffset": 1823}, {"referenceID": 16, "context": "Note that if we have an algorithm with T time steps and C communication bits in the lock-synchronous model, using an exponential back-off mechanism [Herlihy and Shavit, 2008] and a synchronizer [Peleg, 2000], we can convert it to an asynchronous communication with O(T log k) rounds and O((T +C) log k) communication bits.", "startOffset": 148, "endOffset": 174}, {"referenceID": 22, "context": "Note that if we have an algorithm with T time steps and C communication bits in the lock-synchronous model, using an exponential back-off mechanism [Herlihy and Shavit, 2008] and a synchronizer [Peleg, 2000], we can convert it to an asynchronous communication with O(T log k) rounds and O((T +C) log k) communication bits.", "startOffset": 194, "endOffset": 207}, {"referenceID": 19, "context": "The interested reader is referred to Kushilevitz and Nisan [1997] for an excellent exposition of communication complexity.", "startOffset": 37, "endOffset": 66}, {"referenceID": 8, "context": "See Dwork [2008] for an excellent survey of differential privacy.", "startOffset": 4, "endOffset": 17}, {"referenceID": 15, "context": "dT (H) is defined as maxf\u2208H dT (f) where dT (f) is the smallest number of examples needed to uniquely identify f within H [Goldman and Kearns, 1991].", "startOffset": 122, "endOffset": 148}, {"referenceID": 17, "context": "A classic lower bound in communication complexity states that if two entities each have a set of linear equalities over n variables, then \u03a9(n2) bits of communication are needed to determine a feasible solution, based on J\u00e1J\u00e1 and Prasanna [1984]. This in turn implies that for proper learning of parity functions, \u03a9(n2) bits of communication are required even in the case k = 2, matching the baseline upper bound given via Equivalence Query algorithms.", "startOffset": 220, "endOffset": 245}, {"referenceID": 17, "context": "A classic lower bound in communication complexity states that if two entities each have a set of linear equalities over n variables, then \u03a9(n2) bits of communication are needed to determine a feasible solution, based on J\u00e1J\u00e1 and Prasanna [1984]. This in turn implies that for proper learning of parity functions, \u03a9(n2) bits of communication are required even in the case k = 2, matching the baseline upper bound given via Equivalence Query algorithms. Interestingly, however, if one drops the requirement that learning be proper, then for k = 2, parity functions can be learned using only O(n) bits of communication. Moreover, the algorithm is efficient. This is in fact a special case of the following result for classes that are learnable in the reliable-useful learning model of Rivest and Sloan [1988].", "startOffset": 220, "endOffset": 806}, {"referenceID": 23, "context": "[Rivest and Sloan, 1988] An algorithm reliably and usefully learns a class H if given poly(n, 1/\u01eb, 1/\u03b4) time and samples, it produces a hypothesis h that on any given example outputs either a correct prediction or the statement \u201cI don\u2019t know\u201d; moreover, with probability at least 1\u2212 \u03b4 the probability mass of examples for which it answers \u201cI don\u2019t know\u201d is at most \u01eb.", "startOffset": 0, "endOffset": 24}, {"referenceID": 25, "context": "Moreover, an average over O(d/\u01eb2) samples is sufficient to produce an estimate of error at most \u01eb with high probability [Servedio, 2002].", "startOffset": 120, "endOffset": 136}, {"referenceID": 22, "context": ", Schapire [1990], Freund [1990], Freund and Schapire [1997].", "startOffset": 2, "endOffset": 18}, {"referenceID": 13, "context": ", Schapire [1990], Freund [1990], Freund and Schapire [1997].", "startOffset": 19, "endOffset": 33}, {"referenceID": 13, "context": ", Schapire [1990], Freund [1990], Freund and Schapire [1997]. (For Adaboost, we are considering the version that uses a fixed upper bound \u03b2 on the error of the weak hypotheses.", "startOffset": 19, "endOffset": 61}, {"referenceID": 14, "context": "In particular, using Adaboost [Freund and Schapire, 1997] in Lemma 9 yields the following result (plugging in \u03b2 = 1/4 or \u03b2 = \u01eb1/c respectively):", "startOffset": 30, "endOffset": 57}, {"referenceID": 0, "context": "We prove this result by simulating the robust generalized halving algorithm of Balcan and Hanneke [2012], for the case of finite hypothesis spaces, in a communication-efficient manner.", "startOffset": 79, "endOffset": 105}, {"referenceID": 0, "context": ", niN ), who draws (but keeps internally and does not send) nij examples of Sj for each The algorithm of Balcan and Hanneke [2012] for the case of infinite hypothesis spaces begins by using a large unlabeled sample to determine a small \u01eb-cover of H.", "startOffset": 105, "endOffset": 131}, {"referenceID": 19, "context": "In our case, one natural approach for achieving privacy is to require that all interaction with each entity i be in the form of statistical queries [Kearns, 1998].", "startOffset": 148, "endOffset": 162}, {"referenceID": 1, "context": "[Dwork and Nissim, 2004, Blum et al., 2005, Kasiviswanathan et al., 2008] If H is learnable using M statistical queries of tolerance \u03c4 , then H is learnable preserving differential privacy with privacy parameter \u03b1 from a sample S of size O(max[ \u03b1\u03c4 , M \u03c42 ] log(M/\u03b4)). Proof. For a single statistical query, privacy with parameter \u03b1\u2032 can be achieved by adding Laplace noise of width O( 1 \u03b1\u2032|S|) to the empirical answer of the query on S. That is because changing a single entry in S can change the empirical answer by at most 1/|S|, so by adding such noise we have that for any v, PrS(v)/PrS\u2032(v) \u2264 e\u03b1 \u2032 . Note that with probability at least 1 \u2212 \u03b4\u2032, the amount of noise added to any given answer is at most O( 1 \u03b1\u2032|S| log(1/\u03b4 \u2032)). Thus, if the overall algorithm requires M queries to be answered to tolerance \u03c4 , then setting \u03b1\u2032 = \u03b1/M, \u03b4\u2032 = \u03b4/(2M), \u03c4 = O( 1 \u03b1\u2032|S| log(1/\u03b4 \u2032)), privacy can be achieved so long as we have |S| = O(max[ \u03b1\u03c4 , M\u03c42 ] log(M/\u03b4)), where the second term of the max is the sample size needed to achieve tolerance \u03c4 for M queries even without privacy considerations. As described in Dwork et al. [2010], one can achieve a somewhat weaker privacy guarantee using \u03b1\u2032 = O(\u03b1/ \u221a M).", "startOffset": 25, "endOffset": 1122}, {"referenceID": 18, "context": "This in fact corresponds precisely to the local privacy notion of Kasiviswanathan et al. [2008],", "startOffset": 66, "endOffset": 96}, {"referenceID": 1, "context": "Blum et al. [2008] introduce a similar privacy notion,10 which they show is strictly stronger than differential privacy, but do not provide efficient algorithms.", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "In the notion of Blum et al. [2008], Di is uniform over some domain and sampling is done without replacement.", "startOffset": 17, "endOffset": 36}], "year": 2012, "abstractText": "We consider the problem of PAC-learning from distributed data and analyze fundamental communication complexity questions involved. We provide general upper and lower bounds on the amount of communication needed to learn well, showing that in addition to VC-dimension and covering number, quantities such as the teaching-dimension and mistake-bound of a class play an important role. We also present tight results for a number of common concept classes including conjunctions, parity functions, and decision lists. For linear separators, we show that for non-concentrated distributions, we can use a version of the Perceptron algorithm to learn with much less communication than the number of updates given by the usual margin bound. We also show how boosting can be performed in a generic manner in the distributed setting to achieve communication with only logarithmic dependence on 1/\u01eb for any concept class, and demonstrate how recent work on agnostic learning from class-conditional queries can be used to achieve low communication in agnostic settings as well. We additionally present an analysis of privacy, considering both differential privacy and a notion of distributional privacy that is especially appealing in this context.", "creator": "LaTeX with hyperref package"}}}