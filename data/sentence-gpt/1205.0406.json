{"id": "1205.0406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2012", "title": "Minimax Classifier for Uncertain Costs", "abstract": "Many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem. However, this assumption may not hold for many real-world problems. For example, a classifier might need to be applied in several circumstances, each of which associates with a different cost matrix. However, a classifier's current cost matrix might be similar to the ones in previous work (such as these, in general), and a classifier's current cost matrix might not be equally accurate. In other words, the problem with the fact that a cost matrix's current cost matrix can be compared to the same classifier's current cost matrix, it does not always be true.\n\n\n\n\nIn this work, we will examine a set of variables that are associated with the cost matrix in two different situations:\n(1) an increase in the cost of a classifier\n(2) an increase in the cost of a classifier\n(3) an increase in the cost of a classifier\nIn each case, a classifier is able to use the assumption that the cost matrix of a classifier is identical to a typical model.\nTo analyze, we will examine an optimization principle, which has a clear effect on the cost matrix.\nIn conclusion, an optimization principle should be applied to the cost matrix. In general, a classifier should use the assumption that the cost matrix of a classifier is identical to a typical model. In particular, a classifier should use the assumption that the cost matrix of a classifier is similar to a typical model. For example, a classifier is able to apply the assumption that the cost matrix of a classifier is similar to a typical model.", "histories": [["v1", "Wed, 2 May 2012 12:38:11 GMT  (1506kb,D)", "http://arxiv.org/abs/1205.0406v1", "6 pages, more materials will be added into the manuscript"]], "COMMENTS": "6 pages, more materials will be added into the manuscript", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rui wang", "ke tang"], "accepted": false, "id": "1205.0406"}, "pdf": {"name": "1205.0406.pdf", "metadata": {"source": "CRF", "title": "Minimax Classifier for Uncertain Costs", "authors": ["Rui Wang", "Ke Tang"], "emails": ["wrui1108@mail.ustc.edu.cn,", "tang@ustc.edu.cn."], "sections": [{"heading": null, "text": "I. INTRODUCTION\nIn many real world classification problems, different types of misclassifications commonly result in different costs. For example, in fraud detection problem, predicting a normal client as fraud will cut the profit, while predicting a fraud client as normal would usually lead to great loss [1]. In these scenarios, it would be more desirable to minimize the total cost rather than the classification error. This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].\nSo far, the majority of previous research on cost-sensitive learning assumes that the costs for different types of misclassifications, typically represented as a cost matrix, are uniquely specified before the classifier is applied to new data. Specifically, if the cost matrix is known before the training procedure, it can be integrated into the the learning algorithm to obtain a classifier with minimum total cost. This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9]. In addition to specialized methods, some alternative approaches, which are motivated by other learning problems, could also be employed to address cost-sensitive learning problems. This category of methods, including calibration methods [10], threshold moving [5] and its variants [11], typically post process the output of a classifier to optimize its performance with respect to a objective (e.g., minimize the total cost or classification error). In this sense, it is not necessary to know the cost matrix in prior to the training phase, as long as it becomes available before testing 1.\nNature Inspired Computation and Applications Laboratory (NICAL), Hefei, Anhui 230027 China, E-mail: wrui1108@mail.ustc.edu.cn, ketang@ustc.edu.cn.\n1Sometimes, post-processing can also be considered as a part of the training procedure. From this point of view, the cost matrix still needs to be specified before training phase is finished.\nAll the above-mentioned approaches assume that a unique cost matrix is known for a given cost-sensitive problem. Unfortunately, in the real world, it could be very difficult for a practitioner to specify the cost matrix uniquely, for the reason that one may do not have much sense about the exact values of misclassification costs, or that the costs may vary under different circumstances and thus is uncertain in nature. In one word, the cost matrix for a real-world problem may be uncertain throughout both training and testing.\nAs a matter of fact, the difficulty of specifying a cost matrix has been acknowledged by many researchers. In the context of ROC analysis [12], it is claimed that a classifier can be built without any cost information, while still performs well in the scenarios where the cost matrix changes. Nevertheless, an underlying assumption behind this statement is that threshold moving (or any other similar methods) is employed to finetune the output of the classifier. Hence, as discussed above, the specified cost matrix is still required in the post-processing phase. Zadrozny and Elkan [13] considered the scenario where example-based misclassification costs are static but unknown. More recently, Liu and Zhou [14] investigates the problem of learning with cost intervals. Specifically, the misclassification cost is assumed as taking a value within a predefined interval, and an approach is developed to train a SVM that performs well for every possible value of cost.\nRather than striving to achieve satisfactory performance over all possible cost matrices, the aim of this work is to minimize the largest total cost over a finite set of possible cost matrices, i.e., to find the minimax classifier. Under mild assumptions, we prove that the minimax classifier over multiple cost matrices can be achieved by solving a set of standard cost-sensitive learning problems and a set of sub-problems involves only two cost matrices. This finding immediately suggests a general framework for seeking minimax classifier over arbitrary number of cost matrices. Moreover, since an interval can be transformed into a finite set of values via discretization, the framework is also applicable to the scenarios where only the largest and smallest costs for misclassification are available.\nThe rest of this paper is organized as follows. Preliminary backgrounds and related works are introduced with more details in Section 2. Section 3 presents the theoretical analysis of the minimax problem. Experimental studies are in following section, and we conclude the paper in Section 5."}, {"heading": "II. PRELIMINARIES AND RELATED WORKS", "text": "In this section, we introduce the basic notations and backgrounds at first, and then review two works that are closely related to this study. One is the work from Liu and Zhou [14], which also deals with the uncertain cost problem, but with\nar X\niv :1\n20 5.\n04 06\nv1 [\ncs .L\nG ]\n2 M\nay 2\n01 2\n2 different formulation and learning target, and the other focus on finding the minimax classifier for uncertain class prior [15]."}, {"heading": "A. Preliminaries", "text": "Given a dataset S = {(x1, y1), . . . , (xn, yn)}, xi = {x1i , . . . , xmi } \u2208 Rm is the feature vector of instance (xi, yi) and yi \u2208 {0, 1} is the class label. Suppose that there is no cost with correct classification, a cost matrix C can be represented by two values c0 and c1, denoting the cost of misclassifying an instance from class 0 and class 1 respectively. Also, we use p0 and p1 = 1\u2212 p0 to represent the class priors, so there are n0 = np1 and n1 = np1 instances in each class. For any classifier h from the hypothesis space H, its total cost is,\nL = np0p10c0 + np1p01c1\n= n0p10c0 + n1p01c1, (1)\nwhere p10 (p01) is the probability that h misclassifies instances from class 0 (1) to class 1 (0)."}, {"heading": "B. Learning with Cost Intervals", "text": "In a recent work, Liu and Zhou [14] considered a special form of the uncertain cost problem where c0 is 1, and c1 is uncertain but within a predefined interval [cmin, cmax]. Their objective is to construct a classifier that performs well for every individual cost within [cmin, cmax]. Technically, the problem was transformed as finding the best surrogate cost cs to trained with, i.e., their learning target is,\nmin h\u2208H L(h, S, cs) s.t. p(L(h, S, c) < ) > 1\u2212 \u03b4, \u2200 c \u2208 [cmin, cmax] cmin \u2264 cs \u2264 cmax (2)\nA SVM-based algorithm was proposed there, which primarily minimizes the largest total cost (i.e., L(h, S, cmax)) and secondarily minimizes the total cost at mean cost c\u00b5 = (cmax + cmin)/2, i.e., L(h, S, c\u00b5). Solid experimental results reported there confirmed the efficacy of the method.\nHowever, to fit in with the interval formulation, one needs to artificially re-scale original cost matrices by different factors to assure every c0 is 1. Although this re-scaling process does no harm to traditional cost-sensitive learning as well as the study in [14], it makes the comparison of total costs across different cost matrices meaningless. Considering that it is generally hard or even impossible to find a classifier that performances well on all costs over the interval (as suggested by [14] itself), the best classifier they built may lead to very big total cost on original cost matrices for real-world problems."}, {"heading": "C. Minimax Classifier for Uncertain Class Priors", "text": "In the many studies involving the minimax criterion [16], those focused their attention on building minimax total cost classifier for uncertain class prior [15] are of particular interest to this study.\nFormally, in case of uncertain class prior, the minimax classification problem is to find the following classifier,\nhp = argmin h\u2208H max P L(h, P,C) (3)\nIt is well known that the total cost of a fixed classifier is a linear function of prior, while the optimal total cost (i.e., the Bayesian cost) is a concave function of prior [17]. Therefore, suppose the best classifier is h\u2217 for a given class prior P \u2217, then the total cost function of h\u2217 w.r.t. prior would be a tangent line of the Bayesian total cost curve at P \u2217. Based on these elegant properties, Alaiz-Rodriguez et al proposed two algorithms based on neural networks model to find the minimax classifier iteratively in [15]. Readers interested in the details of the algorithms are referred to that paper.\nNotice the deceptively symmetrical positions of prior and cost in Eq. (1), one may think that all the analysis and algorithms w.r.t. the uncertain prior problem can be employed directly for the uncertain cost problem concerned in this study. Unfortunately, that is not the case. For the reason that both c0 and c1 are free variables (i.e., the sum-to-one property of prior does not applied to cost), the concavity of Bayesian total cost for prior can not be transformed to cost. In the following, we consider the minimax problem for uncertain cost along a different way."}, {"heading": "III. MINIMAX CLASSIFIER FOR UNCERTAIN COST", "text": ""}, {"heading": "A. Problem Formulation", "text": "As mentioned above, this study focuses on minimizing the largest total cost over a finite set of possible cost matrices. Formally, given a set of cost matrices U = {C1, . . . Ck}, where Ci = {ci0, ci1} is the i-th cost matrix, the learning target is to find,\nhU = argmin h\u2208H max C\u2208U L(h, S,C). (4)\nSince the uncertain cost is formulate as a set directly, the problem is widely applicable in practice, ready for future study on multi-class problems, and facilitating theoretical analysis. On the other hand, the best classifier selected by the minimax criterion is much more reliable."}, {"heading": "B. Problem Analysis", "text": "For two different cost matrices Ci and Cj in U , if both ci0 \u2264 c j 0, and c i 1 \u2264 c j 1, then the total cost of any classifier h obtained on Ci will be smaller than that on Cj . In this case, we say that Ci is dominated by Cj . Furthermore, if there exist a cost matrix Cd that dominates all others in U , the above minimax problem can be simplified as a standard cost-sensitive learning problem with fixed cost matrix Cd. Therefore, given a minimax classification problem over a set of cost matrices, the first step one should take is to check and delete cost matrices that are dominated by any other cost matrix in U .\nOn the other hand, the performance of a classifier h from the hypothesis space H can be mapped to a point in the 2-D space with p10 as the x-axis and p01 as the y-axis. Similarly, for two different classifiers ha and hb, if pha10 \u2264 p hb 10 , and p ha 01 \u2264 p hb 01 , then Lha \u2264 Lhb , no matter what the cost matrix is. In this case, we say that ha dominates hb. If a classifier is not dominated by any other classifier in H, it is a non-dominated classifier. Following the concept in economics [18], the front formed by all non-dominated classifiers in H is named as the Pareto front (see Fig. 1). When H is an infinite hypothesis space and\n3 dataset S consist of enough samples, the front is continues. Obviously, for both standard cost-sensitive learning problem and the minimax problem concerned in this study, the optimal classifiers must be on the Pareto front.\nLet us firstly consider the situation with only one cost matrix C, Lemma 1 reveals the relative order between the total costs of any two classifiers on the front.\nLemma 1. For any two classifiers h1, h2 on the Pareto front, with ph110 \u2264 p h2 10 and p h1 01 \u2265 p h2 01 , the following conclusions hold,\n\u2022 p h1 01\u2212p h2 01\np h2 10\u2212p h1 10\n> n0c0n1c1 \u21d0\u21d2 Lh1 > Lh2 ,\n\u2022 p h1 01\u2212p h2 01\np h2 10\u2212p h1 10\n= n0c0n1c1 \u21d0\u21d2 Lh1 = Lh2 ,\n\u2022 p h1 01\u2212p h2 01\np h2 10\u2212p h1 10\n< n0c0n1c1 \u21d0\u21d2 Lh1 < Lh2 .\nProof: According to Eq. (1)\nLh1 = n0c0p h1 10 + n1c1p h1 01\nLh2 = n0c0p h2 10 + n1c1p h2 01\nTherefore,\nLh1 > Lh2 \u21d0\u21d2 n0c0p h1 10 + n1c1p h1 01 > n0c0p h2 10 + n1c1p h2 01 \u21d0\u21d2\np h1 01\u2212p h2 01 p h2 10\u2212p h1 10 > n0c0n1c1\nSimilarly, the other two cases also validated. Notice that the left hand of each case in Lemma 1, (ph101 \u2212 p h2 01)/(p h2 10 \u2212 p h1 10), is the abstract value of the slope of the segment connected h1 and h2, which is determined by classifiers\u2019 performance, and (n0c0)/(n1c1), on the other hand, is a constant given dataset S and cost matrix C. That is, geometrically, the relative order between the total costs of a pair of classifiers on the front is determined by slope of the segment connected these two classifiers.\nFurthermore, since all the dominated classifiers can be ignored w.r.t. our problem, total cost Eq.(1) can be treated as a function of the classifiers on the Pareto front. For briefness, we further consider it as a function of p10, and keep in mind that p01 is determined correspondingly. Hereafter, we denote the\ntotal cost function as LC(p10) for cost matrix C. The following lemma describes the track of LC(p10) along the front.\nLemma 2. Assume the Pareto front is convex2, then the total cost function LC(p10) decreases monotonically to its minimum at first, and then increases monotonically over the front.\nProof: Given any three adjacent classifiers on the front, h1, h2, h3, without loss of generality, we suppose ph110 < p h2 10 < ph310 . Since the curve of the Pareto front is decreasing and convex, h3 must lay on the right-side of the line passes h1 and h2. Plus the fact that ph110 < p h2 10 < p h3 10 and p h1 01 > p h2 01 > p h3 01 , we have ph101 \u2212 p h2 01\nph210 \u2212 p h1 10\n\u2265 p h2 01 \u2212 p h3 01\nph310 \u2212 p h2 10\n.\nSince h1, h2, h3 are three arbitrary adjacent classifiers on the front, it comes that the abstract value of the slope is adjacently and monotonically non-increasing along the front. Suppose the classifier of minimal total cost for cost matrix C is hC , with the total cost LC(p10 = phC10 ), then according to Lemma 1, LC(p10) decreases monotonically to (phC10 , LC(p10)) at first, and then increases monotonically.\nIn fact, Lemma 2 describes the behavior of total cost function for standard cost-sensitive learning problem (i.e., with only one cost matrix), and many cost-sensitive learning methods published in the literature could be used, hopefully, to find the minimum point. See Fig. 2 for an illustration3.\nNow, we are ready for considering the situation with multiple cost matrices. For a set of k cost matrices, there are k total cot curves correspondingly. Each of them decreases to its own minimum at first and then increases. Fig. 3 shows a example consist of two cost matrices. We can see that, in this case, the minimax total cost locates at the cross point of these two\n2Analogous to the ROCCH technique, in case that the Pareto front is not convex, one can construct the convex hull of all non-dominated classifiers as the surrogate Pareto front. Please refer to [12], particularly Theorem 7 there, for further details.\n3Note the total cost curve was drawn for illustration purpose, the convexity it appears is not implied nor has been proved.\n4 0 p10 T o ta l C o s t hU\nhc2p10 hc1p10 hup10\nh 2Ch 1C\n1\nL 1C\nL 2C\nFig. 3. The total cost curves for C1 and C2. Each of them decreases at first, and then increases monotonously. h1 and h2 are the best classifiers for C1 and C2, and hc is the minimax classifier for these two cost matrices.\ncurves. Generally, the position of the minimax classifier for multiple cost matrices is confined by the following theorem.\nTheorem 1. For k different total cost curves, each of them decreases to its own minimum at first and then increases monotonically, the minimax total cost locates at one of the two types of positions,\n1) minimum point of an individual curve, 2) point where curves get crossed.\nProof: Suppose the minimax total cost locates at neither one of the two types of positions, without loss of generality, we assume it is on the total cost curve of Ci (i.e., LCi ). Note that the minimax classifier is hU , we have,\nLCi(p hU 10 ) > LCj (p hU 10 ) for i 6= j.\nThere is no equality because the the minimax total cost is not obtained at a type-2 point. On the other hand, since the minimax total cost is obtained neither at a type-1 point, according to Lemma 2, we can find another classifier h\u2032U such that,\nLCi(p hU 10 ) > LCi(p h\u2032U 10 ) > LCj (p h\u2032U 10 ) for i 6= j.\nThis means the minimax total cost can be reduced, which conflicts with the definition of minimax. Therefore, the theorem is validated.\nSo, in order to find the minimax classifier, we just need to examine every classifier corresponds to the two types of positions. However, without further information, any pair of total cost curves may cross each other several times in practice, hence it would be very expensive or even impossible to examine all these points without omission. Fortunately, this obstacle can be removed elegantly by the following corollary.\nCorollary 1. For a set of k cost matrices U = {C1, . . . , Ck}, the minimax total cost classifier hU belongs to one of following two categories,\n1) classifiers that minimize the total cost for an individual cost matrix, 2) classifiers that minimax the total cost for a pair of cost matrices.\nProof: According to Theorem 1, if the minimax total cost is obtained at one of the type-1 positions, then the minimax classifier fall into the first category, thus the corollary is true. Otherwise, the minimax total cost is obtained at a cross point of total cost curves. We know that there are at least two total cost curves with different monotonic property at the cross point, otherwise, we can move hU in the direction that all involved curves are decreasing, leading to reduced minimax total cost. Let the LCi is decreasing, and LCj is increasing at hU , then we know from Lemma 2 that the maximal total cost for (Ci, Cj) is bigger with all other classifiers. Hence, the cross point is the also the minimax total cost for (Ci, Cj). So, the corollary is also true in this case.\nAccording to Corollary 1, the minimax classification problem over multiple cost matrices is reduced to solving a set of standard cost-sensitive learning problems and a set of subproblems involves only two cost matrices, saving the bother to consider the tradeoff among multiple cost matrices. Finally, the framework for solving the minimax classification problem over a set of cost matrices is summarized in Algorithm 1.\nAlgorithm 1 Framework of solving the minimax classification problem over a set of cost matrices\nInput: dataset S, a set of cost matrices U = {C1, . . . , Ck}\ndeletes all dominated cost matrices in U , if U = {C1} then hU = argminh\u2208H L(h, S,C1) else if U = {C1, C2} then hU = argminh\u2208HmaxC\u2208{C1,C2} L(h, S,C) else V = \u2205 for i = 1 to |U | do\nfind hCi = argminh\u2208H L(h, S,Ci) V = V \u22c3 {hCi}\nend for for i = 1 to |U | do\nfor j = 1 + 1 to |U | do find hij = argminh\u2208HmaxC\u2208{Ci,Cj} L(h, S,C) V = V \u22c3 {hij}\nend for end for hU = argminh\u2208V maxC\u2208U L(h, S,C)\nend if Return: the minimax classifier hU"}, {"heading": "IV. EXPERIMENTS", "text": "In the experiments, we compared three frameworks for solving the minimax problem. The first is to build the minimum total cost classifier for each possible cost matrix without considering any tradeoff among cost matrices at first, and then picks out the minimax classifier, the second is our framework described above, and the third one is to build the minimax classifier directly with all the possible cost matrices are under consideration simultaneously. For briefness, we denote these three frameworks as S, SP, and M respectively.\n5\nA. Implementation\nAlthough there are many standard cost-sensitive learning methods, striving to minimize the total cost for one cost matrix, can be used to implement S and one part of SP, to the best of our knowledge, there is no particular method that can be used to implement M or the the other part of SP (i.e., minimax the total cost for two or more cost matrices). Hence, for the comparison purpose, we adopted a simplified form of the Generalized Additive Model (GAM) to implement all the three frameworks. Therefore, the empirical studies presented underneath are preliminary, and only intend to serve as a baseline for future study.\nThe GAM used to implement all the compared frameworks is,\nF (x) = sign( T\u2211 i=1 fi(x)) (5)\nwhere T is the number of iterations, and fi is a decision stump, whose output is 1 or \u22121. At each generation, we add one decision stump such that the current ensemble of decision stump Fi get improved performance over Fi\u22121 on the predefined objective. This process repeats until the iteration number is ran out or there is no improvement.\nWith this simple GAM procedure, we are able to implement the three above-mentioned frameworks. That is, all necessary building blocks can be generated by setting the \u201cpredefined objective\u201d to minimize the total cost for a single cost matrix, or minimax the total cost for a pair of cost matrices, or minimax the total cost for a set of cost matrices."}, {"heading": "B. Experimental Setup", "text": "Ten datasets from the UCI machine learning repository [19] were used in the experiments. Brief information about these datasets is summarized in Table I. Most of these ten classification problems are originally real-world cost-sensitive problems, for example the australian, crx, and german problems are fraud detection problems, while the heart, mushroom and wdbc problems are related to health of people. For these problems, the misclassification cost matrix is usually hard if not impossible to specified by practitioners, so the experiments on them are appropriate.\nFor each of the datasets, we compared the 3 frameworks on 4 set of cost matrices of different cardinalities. They are sets of 3 cost matrices, 5 cost matrices, 10 cost matrices, and\n20 cost matrices. The value of each element of the matrices is randomly generated within [0, 10). Besides, it is assured in advance that there is no dominated cost matrix in each set.\nThe iteration number in the GAM is set to 50, and 20 times 5-fold cross validation procedure was employed to obtain stationary results. Hence, for each of 10\u00d74\u00d73 configurations of (dataset, set of cost matrices, compared method), there are 20\u00d75 total cost values. Based on these values, we furthermore conducted Wilcoxon signed rank test between SP method and the other two methods with significance level 5%."}, {"heading": "C. Results", "text": "Table II and Table III present the comparisons over each dataset on training and testing respectively. The value in each cell is the average total cost over 20 times 5-fold, and the best performance for each (dataset, cost set) configuration is in boldface. Moreover, the results of Wilcoxon signed rank test are denoted as superscripts on the values of S and M methods, a superscript of 1 indicates the performance of SP is significantly better than that of corresponding method, \u22121 for significantly worse, and no superscript means there is no statistically significant difference between SP and corresponding method.\nIn summary, we can see that SP outperforms the other two methods in almost all cases, and keeps statistically comparable for the rest few cases. There is no case that SP is statistically worse (i.e., there is no \u22121 on the superscripts).\nOf course, it is not surprising at all that SP defeats S completely in the experiments, since SP always checks a superset of classifiers compared to S. But these results at least provide a evidence that the S framework is not adequate for uncertain costs problems. Moreover, with a closer examination of the results in each fold, we can see that the performance of S and SP are identical sometimes, and SP is better if they are not. This is consistent with Corollary 1, since the classifier obtained by S could be the optimal minimax classifier in theory.\nOn the other hand, the superior of SP over M is more interesting. Unlike the S framework, M searches the hypothesis space with the true learning target directly (i.e., the minimax target). Therefore, the most plausible explanation is that the implementation of the M framework is no effective enough. Since ideally it could perform as good as the SP framework. However, as the similar problem encountered in multi-class classification problems, designing algorithms that can handle multiple tradeoff simultaneously is never a trivial work.\nIn summary, although we implemented the three compared frameworks with a preliminary and less effective model, the result reported in the paper confirms the efficacy of Corollary 1. Once we are equipped with particular designed method can solve the minimax problem over only two cost matrices effectively, it would be very exciting to see the full advantage of the SP framework."}, {"heading": "V. CONCLUSIONS AND DISCUSSIONS", "text": "For many real-world cost-sensitive learning problems, the costs associated with misclassifications are uncertain in nature. Many existing cost-sensitive learning algorithms, which\n6\nrequire the exact cost information (e.g., a unique cost matrix) being available, are not applicable for these problems. In this paper, we consider the situation where the cost information is provided as a set of cost matrices, and aim to achieve the minimax classifier over the cost matrices. It is theoretically proved that the classifier with minimax total cost is either the optimal classifier for a single cost matrix in the set, or the minimax classifier over a pair of cost matrices in the set. This result immediately leads to a framework for achieving minimax classifier over arbitrary number of cost matrices. Furthermore, it is also applicable in case that the cost information is provided as an infinite set, e.g., intervals, by combining with an appropriate sampling/discretization procedure. Preliminary empirical study has justified the efficacy of the framework.\nAlthough there exist a lot of algorithms for standard costsensitive learning problems, achieving minimax classifier over a pair of cost matrices remains the major technical obstacle. Therefore, novel algorithms should be developed for this purpose to exploit the usefulness of our framework to the full extent. Furthermore, the theoretical analysis conducted in this work needs to be extended to multi-class problems so that the resultant framework can be generalized. These issues will be investigated in the future."}], "references": [{"title": "Statistical fraud detection: A review", "author": ["R.J. Bolton", "D.J. Hand"], "venue": "Statistical Science, vol. 17, no. 3, pp. 235\u2013255, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "The Foundations of Cost-Sensitive Learning", "author": ["C. Elkan"], "venue": "Proceedings of 17th International Joint Conference on Artificial Intelligence, 2001, pp. 973\u2013978.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "An iterative method for multiclass cost-sensitive learning", "author": ["N. Abe", "B. Zadrozny", "J. Langford"], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004, pp. 3\u201311.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "On Multi-Class Cost-Sensitive Learning", "author": ["Z.H. Zhou", "X.Y. Liu"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence, 2006, pp. 567\u2013572.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Maximum likelihood in cost-sensitive learning: Model specification, approximations, and upper bounds", "author": ["J.P. Dmochowski", "P. Sajda", "L.C. Parra"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3313\u2013 3332, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Metacost: A general method for making classifiers costsensitive", "author": ["P. Domingos"], "venue": "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, 1999, pp. 155\u2013 164.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "An instance-weighting method to induce cost-sensitive trees", "author": ["K.M. Ting"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 14, no. 3, pp. 659\u2013665, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Training cost-sensitive neural networks with methods addressing the class imbalance problem", "author": ["Z.H. Zhou", "X.Y. Liu"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 18, no. 1, pp. 63\u201377, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Cost-sensitive boosting", "author": ["H. Masnadi-shirazi", "N. Vasconcelos", "S. Member"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 2, pp. 294\u2013309, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, 2002, pp. 694\u2013699.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "On reoptimizing multi-class classifiers", "author": ["C. Bourke", "K. Deng", "S.D. Scott", "R.E. Schapire", "N.V. Vinodchandran"], "venue": "Machine Learning, vol. 71, no. 2-3, pp. 219\u2013242, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust classification for imprecise environments", "author": ["F. Provost", "T. Fawcett"], "venue": "Machine Learning, vol. 42, pp. 203\u2013231, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning and making decisions when costs and probabilities are both unknown", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, 2001, pp. 204\u2013213.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning with cost intervals", "author": ["X.Y. Liu", "Z.H. Zhou"], "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2010, pp. 403\u2013412.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimax classifiers based on neural networks", "author": ["R. Alaiz Rodriguez", "A. Guerrero Curieses", "J. Cid Sueiro"], "venue": "Pattern Recognition, vol. 38, no. 1, pp. 29\u201339, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Pattern Classification, 2nd ed", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "New York: John Wiley,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "d\u2019Economie Politique", "author": ["V. Pareto", "Cour"], "venue": "Geneve: Librarie Droz,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1964}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "http://www.ics.uci.edu/\u223cmlearn/MLRepository.html, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "For example, in fraud detection problem, predicting a normal client as fraud will cut the profit, while predicting a fraud client as normal would usually lead to great loss [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 4, "context": "This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "This category of methods, including calibration methods [10], threshold moving [5] and its variants [11], typically post process the output of a classifier to optimize its performance with respect to a objective (e.", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "This category of methods, including calibration methods [10], threshold moving [5] and its variants [11], typically post process the output of a classifier to optimize its performance with respect to a objective (e.", "startOffset": 79, "endOffset": 82}, {"referenceID": 10, "context": "This category of methods, including calibration methods [10], threshold moving [5] and its variants [11], typically post process the output of a classifier to optimize its performance with respect to a objective (e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "In the context of ROC analysis [12], it is claimed that a classifier can be built without any cost information, while still performs well in the scenarios where the cost matrix changes.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "Zadrozny and Elkan [13] considered the scenario where example-based misclassification costs are static but unknown.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "More recently, Liu and Zhou [14] investigates the problem of learning with cost intervals.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "One is the work from Liu and Zhou [14], which also deals with the uncertain cost problem, but with ar X iv :1 20 5.", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "different formulation and learning target, and the other focus on finding the minimax classifier for uncertain class prior [15].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "In a recent work, Liu and Zhou [14] considered a special form of the uncertain cost problem where c0 is 1, and c1 is uncertain but within a predefined interval [cmin, cmax].", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "Although this re-scaling process does no harm to traditional cost-sensitive learning as well as the study in [14], it makes the comparison of total costs across different cost matrices meaningless.", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "Considering that it is generally hard or even impossible to find a classifier that performances well on all costs over the interval (as suggested by [14] itself), the best classifier they built may lead to very big total cost on original cost matrices for real-world problems.", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "In the many studies involving the minimax criterion [16], those focused their attention on building minimax total cost classifier for uncertain class prior [15] are of particular interest to this study.", "startOffset": 156, "endOffset": 160}, {"referenceID": 15, "context": ", the Bayesian cost) is a concave function of prior [17].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Based on these elegant properties, Alaiz-Rodriguez et al proposed two algorithms based on neural networks model to find the minimax classifier iteratively in [15].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "Following the concept in economics [18], the front formed by all non-dominated classifiers in H is named as the Pareto front (see Fig.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "Please refer to [12], particularly Theorem 7 there, for further details.", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "Ten datasets from the UCI machine learning repository [19] were used in the experiments.", "startOffset": 54, "endOffset": 58}], "year": 2012, "abstractText": "Many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem. However, this assumption may not hold for many real-world problems. For example, a classifier might need to be applied in several circumstances, each of which associates with a different cost matrix. Or, different human experts have different opinions about the costs for a given problem. Motivated by these facts, this study aims to seek the minimax classifier over multiple cost matrices. In summary, we theoretically proved that, no matter how many cost matrices are involved, the minimax problem can be tackled by solving a number of standard cost-sensitive problems and sub-problems that involve only two cost matrices. As a result, a general framework for achieving minimax classifier over multiple cost matrices is suggested and justified by preliminary empirical studies.", "creator": "LaTeX with hyperref package"}}}