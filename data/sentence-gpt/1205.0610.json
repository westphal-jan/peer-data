{"id": "1205.0610", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2012", "title": "Greedy Multiple Instance Learning via Codebook Learning and Nearest Neighbor Voting", "abstract": "Multiple instance learning (MIL) has attracted great attention recently in machine learning community. However, most MIL algorithms are very slow and cannot be applied to large datasets. In this paper, we propose a greedy strategy to speed up the multiple instance learning process. Our contribution is two fold. First, we propose a density ratio model, and show that maximizing a density ratio function is the low bound of the DD model under certain conditions. Secondly, we make use of a histogram ratio between positive bags and negative bags to represent the density ratio function and find codebooks separately for positive bags and negative bags by a greedy strategy. For testing, we make use of a nearest neighbor strategy to classify new bags. We test our method on both small benchmark datasets and the large TRECVID MED11 dataset. The experimental results show that our method yields comparable accuracy to the current state of the art, while being up to at least one order of magnitude faster. Finally, we show that our algorithm should perform much faster than the current state of the art.", "histories": [["v1", "Thu, 3 May 2012 04:09:19 GMT  (384kb,D)", "http://arxiv.org/abs/1205.0610v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen", "jason corso"], "accepted": false, "id": "1205.0610"}, "pdf": {"name": "1205.0610.pdf", "metadata": {"source": "CRF", "title": "Greedy Multiple Instance Learning via Codebook Learning and Nearest Neighbor Voting", "authors": ["Gang Chen", "Jason Corso"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Traditional supervised learning methods require a training dataset, consisting of input and label pairs, to construct a classifier that can predict outputs/labels for novel inputs. However, the requirement of input/label pairs in the training data is surprisingly prohibitive especially for large training data. Multiple instance learning (MIL) is a more flexible paradigm to learn a concept given positive and negative bags of instances. It assumes each bag may contain many instances, but a bag is labeled positive even if only one of the instances in it falls within the concept. And the bag is labeled negative only when all instances in it are negative. The aim of MIL is to induce a concept that will classify bags of instances based on the assumption defined above. This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].\nAlthough MIL has received an increasing amount of attention in recent years, the problem is still fairly undeveloped and there are many interesting open questions. The MIL problem is harder than traditional supervised learning methods because the learner receives a set of bags instead of a set of instances that are labeled positive or negative. Moreover, the learner needs to deal with the false positives in positive bags. Recently research [8] also shows that the halfspaces finding problem for MIL is NP-complete. And most current MIL algorithms are still very slow and cannot be applied to large data sets. Thus, an approximate algorithm to efficiently find implicit or explicit decision boundary is vital to solve the MIL problem.\nIn this paper, we propose a greedy multiple instance learning method (GMIL) via codebook learning and nearest neighbor voting. Our approach is inspired by the definition of training bags, as well as the Diverse Density (DD) [2] and Citation kNN models [9]. If there\u2019s one true concept t in positive bags, it must be the intersection of all positive bags and it must be far from negative bags. In other words, it has higher density on positive bags and lower density on negative bags. Thus, we present the density ratio model and derive\n\u2217VPML, University at Buffalo, SUNY\nar X\niv :1\n20 5.\n06 10\nv1 [\ncs .L\nG ]\n3 M\nay 2\n01 2\nthe relationship between the DD model and our method. Instead of maximizing a likelihood function, we maximize a density ratio between positive bags and negative bags. Then, we take a greedy strategy to select codebooks for positive targets and negative candidates separately by sorting the histogram ratio between positive bags and negative bags. Our algorithm is very fast, which is linear in the number of total instances from all training bags. See Fig. (1) for the training process. As for classification, we take a nearest neighbor strategy. We test our method on benchmark dataset and the TRECVID MED11 dataset, and our results demonstrate that our method is comparable to the state of the art.\nThe rest of the paper is structured as follow: In Section 2, we surveyed the related work. We describe the the greedy multiple instance learning algorithm in Section 3. Details of the experimental setup and the results of the experiment are given in Section 4, and we conclude in Section 5."}, {"heading": "2 Related work", "text": "One of the earliest algorithms for learning from multiple instances was developed by Dietterich et al. [1] for drug activity prediction. Their algorithm, the axis-parallel rectangle (APR) method, expands or shrinks a hyper-rectangle in the instance feature space with the goal of finding the smallest box that covers at least one instance from each positive bag and no instances from any negative bag. Following this seminal work, there has been a significant amount of research devoted to MIL problems using different learning models, such as DD [2], EM-DD [10], and extended Citation kNN [9].\nDue to the success of the SVM algorithm [5], and the various positive theoretical results behind it, maximum margin methods have become extremely popular in machine learning. Moreover, to improve classification accuracy, many variations of SVM have been proposed by changing constraints, the objective functions, space projection, kernels, etc. Andrews et al. [5] combined MIL with SVM first to cope with MIL problems. Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold\nlearning [25]. To our knowledge, most work on MIL has paid little attention to efficiency or testing MIL approaches on large datasets. There is also no work on learning codebooks for both positive bags and negative bags. As mentioned in [2], the assumption that all bags intersect at a single point is not necessary, DD assumes more complicated concepts. However, it was not generalized to learn negative targets to deal with false positives. Recently, discriminative dictionary learning [26] greatly improves accuracy for visual object recognition. Thus, learning a good dictionary is vital for a discriminative approach. We argue that learning discriminative targets, including both positive and negative centers, can yield a better representation for training data. But how to choose discriminative clusters for MIL problems? In this paper, we propose a greedy multiple instance learning via codebook learning and nearest neighbor voting. We introduce the density ratio function which is low bound of the DD model. We also generalize DD model to learn both positive and negative centers, which will help reduce the false positives in classification. Moreover, our method is very fast for training, which is linear in the number of all instances, at least an order of magnitude faster than comparable methods, and can be applied to large dataset."}, {"heading": "3 Greedy multiple instance learning", "text": "In this section, we present a greedy strategy for multiple instance learning. We maximize the density (histogram) ratio between positive bags and negative bags to find codebooks for both targets and negative candidate centers, which can be used to classify novel bags. The notation used in this paper for bags and instances is the one introduced by Maron and Lozano-Perez [2]."}, {"heading": "3.1 Density ratio function", "text": "Let D be the labeled data which consists of a set of positive bags and negative bags. We denote all positive bags as B+ and all negative bags as B\u2212. We denote B+i as the i\nth positive bag, the jth point (instance) in that bag as B+ij , and the value of the k th feature of that point as B+ijk. Likewise, B \u2212 ij represents a negative point. For training data D = {B+1 , ..., B+n ;B \u2212 1 , ..., B \u2212 m}, assuming for now that the true concept is a single point t (using Bayes\u2019 rule and assuming i.i.d for observations and a uniform prior over concept location). Thus, to find the true concept is equivalent to finding a point, which has high density on positive bags, and low density on negative bags. As a result, we maximize the following density ratio:\narg max x\n\u2211n i=1 \u2211 j Pr(x = t | B\n+ ij)\u2211m\ni=1 \u2211 j Pr(x = t | B \u2212 ij)\n(1)\nThen, we can employ kernel density estimation strategy [27] to calculate the probability at the point x, and rewrite Eq. (1) as the following\narg max x\n\u2211n i=1 \u2211 j \u03d5(\nx\u2212t hn |B+ij)\u2211m\ni=1 \u2211 j \u03d5( x\u2212t hn |B\u2212ij)\n(2)\nwhere hn is the edge length in a d-dimension volume, \u03d5 is a Parzen windowing function defined on d-dimension vector u:\n\u03d5(u) = { 1 |uj | < 12 , j = 1, ..., d 0 otherwise\nNote that we ignore the constants in Eq. (2). We also argue that we can find the true concept by maximizing Eq. (2). Intuitively, if one of the instances in a positive bag is close to t, then \u03d5(x\u2212thn |B + ij) \u2248 1, and \u03d5(x\u2212thn |B \u2212 ij) \u2248 0. Further, if every positive bag has an instance x near to t and no negative bags are close to t, then x will have high density in Eq. (2). Let us consider the case where \u2200x \u2208 B+i is not close to\nthe true concept t; then the numerator will be approximately zero, but the denominator will be larger than zero because it must fall into negative bags. If x is close to the target, it means the denominator will be approximately zero, while the numerator will approximate the total number of positive bags because each of them contains at least one positive instances. Hence, Eq. (2) can help find the target hypothesis t.\nHowever, it is always difficult to define the length of an edge hn in a d-dimension hypercube space. In addition, maximizing Eq. (2) is a tough problem because it is not a continuous and differentiable function. We can use a brute force searching strategy to find the target, but it is time-consuming. Because all positive bags contains at least one instance x close to t, and no negative bags are close to t, thus we can make use of clustering methods to allocate all instances into K bins. Then, we can approximately calculate Eq. (2) by maximizing the ratio between positive and negative histograms.\nWe make use of K-means to cluster all instances into K clusters, denoted as C1, C2, ..., CK . For positive bags, we calculate the following and count the instances (frequency) falling into K bins\nbin(k)+ = #(B+ij \u2208 Ck), k = 1, ...,K (3)\nBy normalizing over all K bins, we can get the histogram for B+ as: h(k)+ = bin(k) +\u2211K\nk=1 bin(k) + .\nLikewise, we can count instances falling into K bins for negative bags\nbin(k)\u2212 = #(B\u2212ij \u2208 Ck), k = 1, ...,K (4)\nSimilarly, we can calculate the histogram for B\u2212 as: h(k)\u2212 = bin(k) \u2212\u2211K\nk=1 bin(k) \u2212 , k = 1, ...,K.\nThus, we can make use of the following formula to approximate Eq. (2)\narg max x\n\u2211n i=1 \u2211 j \u03d5(\nx\u2212t hn |B+ij)\u2211m\ni=1 \u2211 j \u03d5( x\u2212t hn |B\u2212ij) \u221d arg max k h(k)+ h(k)\u2212 (5)\nwhere k = 1, ...,K."}, {"heading": "3.2 Target codebook discovery", "text": "It is possible that the denominator in Eq. (5) might be equal to 0. We introduce a small positive constant to avoid such a situation. In addition, we want larger bin(k)+ in positive bags to have higher priority chosen as target centers; we introduce the Sigmoid function as weights and reformulate Eq. (5) as follows:\narg max k\nh(k)+\nh(k)\u2212 + \u00d7 \u03c3(bin(k) + \u2212 n/K n/K ), k = 1, ...,K (6)\nwhere \u03c3(x) = 11+e\u2212x , and n is the total number of positive bags B +. Remember that n/K is just the average number of positive instances in each bin if there is only one positive instance in each positive bag. Roughly speaking, if x is the intersection of n bags, it should aggregate into one bin, and that bin\u2019s frequency should be larger than n/K. Thus, we want to increase the weights of bins which have higher numbers of instances from positive bags. It is straightforward to choose k that maximize Eq. (6) for one target point. Furthermore, rather than having just one target point t, it is also straightforward to select the second center (or bin) with the next largest histogram ratio. To find more target candidates, one can sort the ratio of each bin from largest to smallest in Eq. (6), and then greedily select the largest for example p centers as the target codebooks, denoted as C+ = {C+i | i = 1, ...,p}, where p \u2265 1. Note that such greedy strategy to search codebooks makes sure our targets appear with higher probability in positive bags and lower probability in negative bags.\nAs for negative candidate centers, we take a different strategy. Note that since all instances from negative bags are negative, it means that the most counted cluster centers in Eq. (4) can be used as the codebooks for all negative instances. In this paper, we sort the negative histogram h(k)\u2212 descendingly, with k = 1, ...,K, and choose the first q centers as the negative representatives, denoted as C\u2212 = {C\u2212i | i = 1, ...,q}, where q \u2265 0. In a sense, our approach generalizes the DD model to learning both positive and negative targets."}, {"heading": "3.3 Nearest neighbor for classification", "text": "Assuming we have learned data centers: C+ with p positive centers and C\u2212 with q negative centers, we employ nearest neighbor to classify new bags. For each new bag, we calculate its distance to both p positive centers and q negative centers, and find the minimum Hausdorff distance [28] as the distance between the bag and the learned codebooks.\nLet us consider two situations below. (1) Assume every positive bag shares only one target, namely p = 1 and q = 0. In this situation, we set a threshold \u03c4 to measure \u201ccloseness\u201d for a new unlabeled bag B. In this paper, we define \u03c4 as the mean distance from input bags to the target. Such a thresholding strategy is similar to the probability threshold in the DD model. (2) Note that all bags intersecting at a single point is not necessary. We can assume more complicated concepts, for example, p \u2265 1 and q \u2265 1. For a new bag B with instances x = {xi, i = 1, ..., n}, we label the bag\nlabel(B) =\n{ 1 dist(x, C+) < dist(x, C\u2212)\n0 otherwise (7)\nwhere dist(x,y) is the minimum Hausdorff distance between two sets x and y. C+ and C\u2212 are positive and negative centers respectively.\nWe prefer kNN to SVM, although kNNs have similar \u201cbehavior\u201d to SVMs (both of them need to learn an explicit or implicit decision boundary from training data). Since, we have learned discriminative centers, we can directly apply kNN for classification. Additionally, kNN has good Bayesian bound; the error rate of a kNN classifier tends to the Bayes optimal theoretically as the sample size tends to infinity."}, {"heading": "3.4 Relationship with Diverse Density model", "text": "For one target hypothesis problem, we derive that the density ratio model is the low bound of the Diverse Density model.\nDefinition For two classes problem with balanced training data (m = n), if the positive training data is separable from the negative ones in Euclidean space, then we say the training data is well distributed. Furthermore, if the desired target in positive bags is separable from all negative instances (contained in both positive and negative bags), we say the multiple instance learning problem is well distributed.\nOne of the well known example is the Gaussian distribution. In order to find the desired target, in other words the intersection of the positive bags, we hope it obeys Gaussian distribution and is separable from the negative instances.\nLemma 1. If the multiple instance learning problem is well distributed, then for \u2200x \u2208 B+ that is close to the target t, we have \u03d5(x\u2212thn |B + ij) \u2265 \u03d5(x\u2212thn |B \u2212 ij).\nThis is straightforward from the definition.\nTheorem 2. For the 2n variables v+i , v \u2212 i \u2208 (0, 1], i = 1, ..., n, if v + i \u2265 v \u2212 j ,\u2200i, j \u2208 [1, n], then we have\u2211n\ni=1 vi\u22112n i=n+1 vi\n\u2264 \u220fn\ni=1 vi\u220f2n i=n+1 vi .\nProof. We only prove this for n=2, and it is easy to extend it to more general situation. Assume we have only 4 variable, v+1 , v + 2 , v \u2212 1 and v \u2212 2 , where v + i \u2265 v \u2212 j , i, j \u2208 [1, 2], then we have to prove v+1 v + 2\nv\u22121 v \u2212 2\n\u2265 v1+v2v3+v4 .\nv+1 v + 2 v\u22121 v \u2212 2 \u2212 v + 1 + v + 2 v\u22121 + v \u2212 2\n(8)\n= v+1 v + 2 (v \u2212 1 + v \u2212 2 )\u2212 v \u2212 1 v \u2212 2 (v + 1 + v + 2 )\nv\u22121 v \u2212 2 (v \u2212 1 + v \u2212 2 )\n(9)\n= v+2 v \u2212 1 (v + 1 \u2212 v \u2212 2 ) + v + 1 v \u2212 2 (v + 2 \u2212 v \u2212 1 )\nv\u22121 v \u2212 2 (v \u2212 1 + v \u2212 2 )\n(10)\n\u22650 (11)\nLemma 3. For well distributed MIL problem, if there are balanced training number of bags, and further \u2200i,\u2211 j \u03d5( x\u2212t hn |B+ij) \u2265 \u2211 j \u03d5( x\u2212t hn |B\u2212ij), then density ratio is the low bound of Diverse density model. Refer Appendices for the proof. Note that the condition \u2200i, \u2211\nj \u03d5( x\u2212t hn |B+ij) \u2265 \u2211 j \u03d5( x\u2212t hn |B\u2212ij) is very weak.\nBecause there\u2019s at least one positive instance in positive bags, thus \u2203x \u2208 B+i close to the target t, such that \u03d5(x\u2212thn |B + ij) \u2248 1. While \u2200x \u2208 B \u2212 i , it is far from t, thus \u03d5( x\u2212t hn |B\u2212ij) \u2248 0. According to such analysis, we can conclude the condition is weak."}, {"heading": "3.5 Algorithm", "text": "We summarize the above discussion in pseudo code. Considering that the traditional K-means depends on the initial clusters, we use more robust K-means++ [29] in our experiment."}, {"heading": "3.6 Complexity", "text": "Suppose we have a total n positive bags and m negative bags, with N total instances. The complexity of our algorithm consists of K-means clustering and computing histograms for both positive bags and negative bags. The other steps for training only operate on constant numbers, so their corresponding time can be ignored. Note that the complexity of our method is dominated by K-means, which can be finished in O(NK). For large datasets, we randomly sample 10000 instances from the training data, and employ K-means++ to partition them into K clusters, and then assign all other instances into the K centers."}, {"heading": "4 Experiments", "text": "We conducted experimental evaluation on five benchmark datasets including the traditional MUSK datasets (Musk1 and Musk2) [1] and image datasets (Tiger, Elephant, and Fox)1. We also evaluate our method on a large dataset, TRECVID MED11 dataset2. Ten-fold cross-validation was used and the per-fold average\n1http://www.cs.columbia.edu/~andrews/mil/datasets.html 2http://www.nist.gov/itl/iad/mig/med11.cfm\nAlgorithm 1\n1: Initialize K, , p, q, \u03c4 ; 2: Partition D = D1, D2, ..., D10; // 10-fold cross validation 3: for i = 1; i <= 10; i+ + do 4: Dt = D \u2212Di; //Dt training data, Di for testing 5: Do k-means++ and cluster Dt into K bins; 6: Count instances which fall into K bins from B+ using Eq. (3); 7: Normalize items in K bins from B+;// compute histogram for B+ 8: Count instances which fall into K bins from B\u2212 using Eq. (4); 9: Normalize items in K bins from B\u2212; // compute histogram for B\u2212\n10: Compute histogram ratio between B+ and B\u2212 according to Eq. (6); 11: Find the first p targets as positive codebook according to Eq. (6); 12: Find the first q negative candidate centers as negative codebook according to Eq. (4); 13: Compute accuracy for Di by nearest neighbor strategy; 14: end for 15: Return average accuracy;\ntest classification performance was calculated for evaluation. For parameter settings, in general, K is related to dataset size. Larger dataset, larger K. \u03c4 is related to variance of each cluster after K-means clustering. p and q are decided by the numbers of the positive bags and negative bags. Specifically, we set p \u2265 q for balanced training dataset, and p \u2264 q for unbalanced training data (more negative data). All parameters in the experiments are determined empirically."}, {"heading": "4.1 Benchmark MIL datasets", "text": "The MUSK datasets have widely served as the benchmark dataset for the MIL algorithms. The feature vector for both Musk1 and Musk2 is 166-dimensional. The MUSK1 contains a total of 92 bags (47 positive and 45 negative), with approximately 6 instances per bag. The Musk2 dataset contains 102 bags (39 positive/63 negative), with 65 instances per bag on average. The COREL image dataset is 230 dimensional, containing three object categories: tiger, elephant, and fox. Each of the three categories consist of 200 bags (100 positive and 100 negative), with about 6 instances per bag. We compare our method with many others, ranging from classical MIL algorithms (the Matlab codes3 of DD, EM-DD, Citation-KNN and mi-SVM are available) to recent models (miGraph4, CRF-MIL, GPMIL) in multiple instance learning. Tab. (1) summarizes the performance of twelve MIL algorithms in the literature. There are five parameters K, , p, q, \u03c4 need to be specified for our method. We set K = 10, p = 2, q = 0 and \u03c4 = 0.082 for Musk1 dataset. As for Musk2 dataset, we set K = 22, p = 2, q = 2. In order to describe how K, p and q influence accuracy, we vary K and q respectively on Musk2 dataset. We demonstrate that learning negative codebooks is necessary, while too large q will decrease accuracy, see Fig. (2) for an intuitive understanding. As for COREL image dataset, we set K = 20, p = 4, q = 4 for all three objects. Our method can yield accuracy that is comparable to most MIL methods on the benchmark dataset. For the fox dataset, our highest accuracy is 70%, which is higher than previous methods. See Table. (1). The comparisons of different methods demonstrate again that no single MIL algorithm outperforms the others across all data sets [6].\nWe also compare efficiency between different methods conducted in the same environment (PC + Matlab2010a). The efficiency of an algorithm was measured based on the running time, i.e., the CPU time measured in seconds until the algorithm meets the stopping criterion. Note that in order to speed up the DD and EM-DD algorithms, we set parameters, such as iterations and F-count for Matlab function fmincon\n3http://www.cs.cmu.edu/~juny/MILL/ 4http://lamda.nju.edu.cn/Data.ashx\nsmaller than default value in the source code. We also test the MIL-Ensemble source code5. Because it is equally slow as DD, we do not include its results in Table (2). A significant advantage of our method is its speed: it is faster than all other methods by at least an order of magnitude. Overall our method is ranked fifth among those in the evaluation, making it comparable to the other global techniques. However these other techniques all run one order of magnitude more slowly than our method. It is also important to note that our results are based only on the simple K-means method, whereas other methods use additional information as well as more sophisticated data costs. We used simple K-means with Euclidean distance because our focus here is on the algorithmic techniques, and demonstrating that our method produces similar quality results much more quickly.\n5http://lamda.nju.edu.cn/code_MIL-Ensemble.ashx"}, {"heading": "4.2 Experiments on the TRECVID MED11", "text": "The TRECVID MED11 was used for evaluating our performance on event recognition. We use the first five events containing 813 video clips with millions of frames, representing complex activities and events, see Fig. (3) for sample images. We extract local features using HOG3D [31]. To represent videos using local features, we apply a common bag-of-words model with a 1000-word codebook. Each frame is represented as a histogram of occurrences of the codebook elements. Because this dataset requires large memory, we run it on 24 cores machine (Intel(R) Xeon(R) CPU X5650 @2.67GHz), with memory 48GB.\nIn this experiment, we only have video level labels. Thus, we treat event recognition as a MIL problem. Each video can be seen as a bag, and its frames in the corresponding bag as instances. To evaluate the performance of our method, we use MI-SVM as baseline because structural SVM (svmlight 6 or liblinear SVM) requires linear time for training (just linear kernel). Thus it is faster compared to other methods; for example, mi-Graph has the time complexity O(N2(m+ n)2). In the experiment, MI-SVM deploys liblinear7 for training. For evaluation, we take one-vs-all strategy. For our method, we set K = 50, p = 3, and q = 9 to train on the unbalanced dataset. The results in Table (3) demonstrate that our method yields competitive results. We also compare the running time between our method and MI-SVM, see Fig. 4 for details."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a greedy multiple instance learning method that leverages codebook learning and nearest neighbor voting. Instead of maximizing a likelihood function, we take a greedy strategy to maximize the histogram ratio between positive bags and negative bags. By learning codebooks for both targets and negative centers, we can use them to classify novel bags based on nearest neighbor strategy. The primary contribution of this paper is to maximize the density ratio to speed up the learning process. Another contribution is learning both targets and negative candidate centers to reduce false positives. Experimental results show that our method is significantly faster and effective compared to the state of the art. In future work, we will consider weight to cluster instances to learn the codebooks. For example, we can use Mahalanobis distance as a distance measure in K-means. We also plan to investigate spectral clustering or kernel k-means methods to learn the codebooks. It is worth mentioning that we can use a more theoretical and more sophisticated kernel function for kernel density estimation in order to improve classification accuracy."}], "references": [{"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artif. Intell", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "A framework for multiple-instance learning", "author": ["O. Maron", "T. Lozano-Prez"], "venue": "In: NIPS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "TPAMI", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Multiple instance boosting for object detection", "author": ["P. Viola", "J.C. Platt", "C. Zhang"], "venue": "In: NIPS", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Supervised versus multiple instance learning: An empirical comparison", "author": ["S. Ray", "M. Craven"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Multi-instance multilabel learning with application to scene classification", "author": ["Zhou", "Z.h", "Zhang", "M.l"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Multiple instance learning via margin maximization", "author": ["O.E. Kundakcioglu", "O. Seref", "P.M. Pardalos"], "venue": "Appl. Numer. Math", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J.D. Zucker"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Em-dd: An improved multiple-instance learning technique", "author": ["Q. Zhang", "S.A. Goldman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Multiple instance learning for sparse positive bags", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "Wang", "J.Z.: Miles"], "venue": "TPAMI", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A regularization framework for multiple-instance learning", "author": ["Cheung", "P.m", "J.T. Kwok"], "venue": "In: ICML", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Deterministic annealing for multiple-instance learning", "author": ["P.V. Gehler", "O. Chapelle"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Svm-based generalized multiple-instance learning via approximate box counting", "author": ["Q. Tao", "S. Scott", "N.V. Vinodchandran", "T.T. Osugi"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Multi-instance learning by treating instances as non-i.i.d. samples", "author": ["Z.H. Zhou", "Y.Y. Sun", "Y.F. Li"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Integrated segmentation and recognition of hand-printed numerals", "author": ["J.D. Keeler", "D.E. Rumelhart", "W.K. Leow"], "venue": "In: NIPS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Multi instance neural networks", "author": ["J. Ramon", "L.D. Raedt"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Multiple instance learning via disjunctive programming boosting", "author": ["S. Andrews", "T. Hofmann"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Bayesian multiple instance learning: automatic feature selection and inductive transfer", "author": ["V.C. Raykar", "B. Krishnapuram", "J. Bi", "M. Dundar", "R.B. Rao"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Solving multiple-instance and multiple-part learning problems with decision trees and decision rules. application to the mutagenesis", "author": ["J.D.Z. Yann"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Adaptive p-posterior mixture-model kernels for multiple instance learning", "author": ["H. Wang", "Q. Yang", "H. Zha"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "F.D.L.: Gaussian processes multiple instance learning", "author": ["M. Kim", "Torre"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A conditional random field for multiple-instance learning", "author": ["T. Deselaers", "V. Ferrari"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Multiple instance learning with manifold bags", "author": ["B. Babenko", "N. Varma", "P. Doll\u00e1r", "S. Belongie"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "In: NIPS", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA. SODA \u201907,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Multiple instance learning for computer aided diagnosis", "author": ["G. Fung", "M. Dundar", "B. Krishnapuram", "R.B. Rao"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "A spatio-temporal descriptor based on 3d-gradients", "author": ["A. Kl\u00e1ser", "M. Marszalek", "C. Schmid"], "venue": "Pr(x = t | B", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 3, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 226, "endOffset": 229}, {"referenceID": 4, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 251, "endOffset": 257}, {"referenceID": 5, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 251, "endOffset": 257}, {"referenceID": 1, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 283, "endOffset": 289}, {"referenceID": 6, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 283, "endOffset": 289}, {"referenceID": 7, "context": "Recently research [8] also shows that the halfspaces finding problem for MIL is NP-complete.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Our approach is inspired by the definition of training bags, as well as the Diverse Density (DD) [2] and Citation kNN models [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "Our approach is inspired by the definition of training bags, as well as the Diverse Density (DD) [2] and Citation kNN models [9].", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "[1] for drug activity prediction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Following this seminal work, there has been a significant amount of research devoted to MIL problems using different learning models, such as DD [2], EM-DD [10], and extended Citation kNN [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "Following this seminal work, there has been a significant amount of research devoted to MIL problems using different learning models, such as DD [2], EM-DD [10], and extended Citation kNN [9].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "Following this seminal work, there has been a significant amount of research devoted to MIL problems using different learning models, such as DD [2], EM-DD [10], and extended Citation kNN [9].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "Due to the success of the SVM algorithm [5], and the various positive theoretical results behind it, maximum margin methods have become extremely popular in machine learning.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "[5] combined MIL with SVM first to cope with MIL problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 11, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 12, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 13, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 14, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 15, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 16, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 136, "endOffset": 144}, {"referenceID": 17, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 136, "endOffset": 144}, {"referenceID": 18, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 155, "endOffset": 162}, {"referenceID": 3, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 155, "endOffset": 162}, {"referenceID": 5, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 197, "endOffset": 204}, {"referenceID": 19, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 197, "endOffset": 204}, {"referenceID": 20, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 221, "endOffset": 225}, {"referenceID": 21, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 248, "endOffset": 252}, {"referenceID": 22, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 273, "endOffset": 277}, {"referenceID": 23, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 305, "endOffset": 309}, {"referenceID": 24, "context": "learning [25].", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "As mentioned in [2], the assumption that all bags intersect at a single point is not necessary, DD assumes more complicated concepts.", "startOffset": 16, "endOffset": 19}, {"referenceID": 25, "context": "Recently, discriminative dictionary learning [26] greatly improves accuracy for visual object recognition.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "The notation used in this paper for bags and instances is the one introduced by Maron and Lozano-Perez [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "Assume we have only 4 variable, v 1 , v + 2 , v \u2212 1 and v \u2212 2 , where v + i \u2265 v \u2212 j , i, j \u2208 [1, 2], then we have to prove v 1 v + 2 v\u2212 1 v \u2212 2 \u2265 v1+v2 v3+v4 .", "startOffset": 93, "endOffset": 99}, {"referenceID": 1, "context": "Assume we have only 4 variable, v 1 , v + 2 , v \u2212 1 and v \u2212 2 , where v + i \u2265 v \u2212 j , i, j \u2208 [1, 2], then we have to prove v 1 v + 2 v\u2212 1 v \u2212 2 \u2265 v1+v2 v3+v4 .", "startOffset": 93, "endOffset": 99}, {"referenceID": 26, "context": "Considering that the traditional K-means depends on the initial clusters, we use more robust K-means++ [29] in our experiment.", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "We conducted experimental evaluation on five benchmark datasets including the traditional MUSK datasets (Musk1 and Musk2) [1] and image datasets (Tiger, Elephant, and Fox).", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "The comparisons of different methods demonstrate again that no single MIL algorithm outperforms the others across all data sets [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 0, "context": "Method Accuracy for benchmark dataset Musk1 Musk2 Elephant Fox Tiger APR[1] 92.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "2 N/A N/A N/A DD[2] 88.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "5 N/A N/A N/A EM-DD[10] 84.", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "1 Citation kNN[9] 92.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "3 N/A N/A N/A mi-SVM[5] 87.", "startOffset": 20, "endOffset": 23}, {"referenceID": 27, "context": "9 MICA[30] 84.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "0 MI-SVM + DA[14] 85.", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "0 PPMM Kernel[22] 95.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "2 miGraph[16] 90.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "9 CRF-MIL[24] 88.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "0 GP-MIL[23] 89.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "Note that except Citation kNN [9], the results of all other methods are based on 10 fold cross-validation.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "rank Code DD [2] 1725.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "9 >3600 >3600 >3600 >3600 Matlab EM-DD [10] 1693.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "3 >3600 >3600 >3600 >3600 Matlab Citation kNN [9] 18.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "5 4 Matlab MI-SVM [5] 3.", "startOffset": 18, "endOffset": 21}, {"referenceID": 15, "context": "1 2 Matlab/C++ miGraph [16] 21.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "4 3 Matlab/C++ CRF-MIL[24] 200.", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "We extract local features using HOG3D [31].", "startOffset": 38, "endOffset": 42}, {"referenceID": 4, "context": "Accuracy Time (Days) MI-SVM [5] 37.", "startOffset": 28, "endOffset": 31}], "year": 2012, "abstractText": "Multiple instance learning (MIL) has attracted great attention recently in machine learning community. However, most MIL algorithms are very slow and cannot be applied to large datasets. In this paper, we propose a greedy strategy to speed up the multiple instance learning process. Our contribution is two fold. First, we propose a density ratio model, and show that maximizing a density ratio function is the low bound of the DD model under certain conditions. Secondly, we make use of a histogram ratio between positive bags and negative bags to represent the density ratio function and find codebooks separately for positive bags and negative bags by a greedy strategy. For testing, we make use of a nearest neighbor strategy to classify new bags. We test our method on both small benchmark datasets and the large TRECVID MED11 dataset. The experimental results show that our method yields comparable accuracy to the current state of the art, while being up to at least one order of magnitude faster.", "creator": "LaTeX with hyperref package"}}}