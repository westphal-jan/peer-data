{"id": "1205.1638", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2012", "title": "Document summarization using positive pointwise mutual information", "abstract": "The degree of success in document summarization processes depends on the performance of the method used in identifying significant sentences in the documents. The collection of unique words characterizes the major signature of the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive Pointwise Mutual Information, which works well for measuring semantic similarity in the Term-Sentence-Matrix, is used in our method to assign weights for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated from this weighted TSM, is then used to extract a summary from the document. Our experiments show that such a method would outperform most of the existing methods in producing summaries from large documents. Using these measures, we can estimate the number of words a document has to write. The number of words that are labeled in the document is not dependent on the number of words and the number of words it can write. This means that most of the words that are labeled in the document are not necessarily identified in the document, but are identified in the document. This means that the total number of words the document has to write, for example, is roughly 5,000 words per word. The number of words that are labeled in the document is not dependent on the number of words it can write. The number of words that are labeled in the document is not dependent on the number of words it can write. The number of words that are labeled in the document is not dependent on the number of words it can write. This means that the number of words that are labeled in the document is not dependent on the number of words it can write. This means that the number of words that are labeled in the document is not dependent on the number of words it can write. This means that the number of words that are labeled in the document is not dependent on the number of words it can write. This means that the number of words that are labeled in the document is not dependent on the number of words it can write. This means that the number of words that are labeled in the document is not dependent on the number of words it can write. This means that the number of words that are labeled in the document is not dependent on the number of words it can write. This means that the number of words that are labeled in the document is not dependent on the number of words it can write. This means that the number of words that are labeled in the document is not dependent on the number of words it can write. This means that", "histories": [["v1", "Tue, 8 May 2012 09:19:10 GMT  (232kb)", "http://arxiv.org/abs/1205.1638v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["aji s", "ramachandra kaimal"], "accepted": false, "id": "1205.1638"}, "pdf": {"name": "1205.1638.pdf", "metadata": {"source": "CRF", "title": "DOCUMENT SUMMARIZATION USING POSITIVE POINTWISE MUTUAL INFORMATION", "authors": ["Ramachandra Kaimal"], "emails": ["aji_12345@yahoo.com", "mrkaimal@yahoo.com"], "sections": [{"heading": null, "text": "DOI : 10.5121/ijcsit.2012.4204 47\nThe degree of success in document summarization processes depends on the performance of the method used in identifying significant sentences in the documents. The collection of unique words characterizes the major signature of the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive Pointwise Mutual Information, which works well for measuring semantic similarity in the TermSentence-Matrix, is used in our method to assign weights for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated from this weighted TSM, is then used to extract a summary from the document. Our experiments show that such a method would outperform most of the existing methods in producing summaries from large documents.\nKEYWORDS\nData mining, text mining, document summarization, Positive Pointwise Mutual Information, TermSentence-Matrix"}, {"heading": "1. INTRODUCTION", "text": "The escalation of the computer networks and easy access methods to information has led to increasing amount of storage of information, mostly textual. According to the latest report from IDC [1], the world\u2019s information is doubling every two years. In 2011, the information created around the world was more than 1.8 zettabytes. By 2020 the world will generate 50 times the amount of information and 75 times the number of \"information containers\" while IT staff to manage it will grow less than 1.5 times. The report also points out the necessity of new \"information taming\" technologies for information processing and storage.\nTo speedup the accessing, the flow of information needs to be filtered and stored systematically. For example, the working of Information Retrieval Systems (IRS) can be made effective by summarizing the entire collection of documents. Automatic text summarization can help by providing condensed versions of text documents. Expected summarization holds a list of applications like information extraction, document retrieval [2], evaluation of answer books [3], etc.\nSince the first study on text extraction by Luhn appeared, the text summarization process has attracted lot of research activities [14,16,17]. Depending on the purpose and intended users, a summary can be generic or user-focused [4]. A generic summary covers all major themes or aspects of the original document to serve a broad readership community rather than a particular group. A user-focused (or topic-focused, query oriented) summary favors specific themes.\nSummarization processes are traditionally confined to ad-hoc and simple techniques, without any symbolic or linguistic processing, and this limits the quality of summary that can be produced. Semantic similarity is a concept whereby a set of words within identified unique words are assigned a metric based on the worthiness/ correctness of their meaning or semantic content. In this paper we suggest a method based on Positive Pointwise Mutual Information (PPMI) [5] an extension of Pointwise Mutual Information PMI[6] which gives more importance to measure the semantic similarity between the words in a document for document summarization."}, {"heading": "2. METHOD", "text": "In linguistics, morphology [7] deals with the arrangement and relationships between the words in a document. In any type of text processing application, the first step will be morphological analysis. Tokenization, stop words elimination [8] and stemming [9] are the sub tasks that are followed in our method."}, {"heading": "2.1 Tokenization and stop words elimination", "text": "Even though characters are the smallest unit, words are considered as the useful and informative building blocks of a document for processing. As depicted in the figure 1, the\nsentences in the document are separated and will be treated as the samples niS i ,...1, = for the\nexperiment. Words in iS are separated in the next step and the punctuation marks and other irrelevant notations will be removed from those words.\nStop words are very commonly used words like \u2018the\u2019, \u2018of\u2019, \u2018and\u2019, \u2018a\u2019, \u2018in\u2019, \u2018to\u2019, \u2018is\u2019, \u2018for\u2019, \u2018with\u2019, etc that do not contribute anything to the informational content of a document and hence it can be removed. These stop words have much meaning in natural language processing techniques that evaluate grammatical structures, but they have less importance in statistical analysis."}, {"heading": "2.2 Stemming", "text": "Generally the morphological variants of words separated from a document have analogous semantic understandings and can be considered as equivalent in IR system. A couple of algorithms [Lovins Stemming, Porter Stemming] for stemming [10,11] have been developed to reduce a word to its stem or root. After the stemming process, the terms of a document are the stems rather than the original words. Stemming algorithms not only reduce a word into stem, but also reduce the size of the list of words that has to be considered for analysis.\nWe are following the Porter Stemming [11] method, which is a rule based algorithm that works with both suffixes and prefixes. The algorithm defines five successive steps each consisting of a set of rules for transformation.\nHere a word is represented as combination of consonants and vowels in the form\n[ ] [ ] )1(......VVCVCC\nwhere the sequence bracket denotes arbitrary presence of their content and this can be written as\n[ ]( ) [ ] )2(VVCC m\n,where m is the number of occurrence of VC.\nThe further processing of stripping is decided by the rules applied in various steps in the algorithm.\nAt the end of stemming process, the unique words,\nij psjU ,....1, =\nwhere ips is the number of unique words, will be separated from iS . After processing\neach sentence, the collection of unique words in the entire document tiTi ,....1, = , where t is the total number of unique words identified for the document is obtained."}, {"heading": "2.3 Term-Sentence-Matrix", "text": "The occurrence of t words in the document is represented by a Term-Sentence-Matrix (TSM) of n columns and t rows, where t is the number of unique words and n is the number of sentences\nin the entire document. Each element ijF of the matrix is suitably measure the importance of\nterm i with respect to the sentence and the entire document. Initially ijF is the frequency of that\nith term in the jth sentence."}, {"heading": "2.4 Weighting the Elements", "text": "TSM alone is not adequate for analyzing the feature of a document; terms that have a large frequency are not necessarily more imperative. A weight derived in respect of the local and document context can give more information than a frequency.\nMutual Information (MI)[12] of an entry measures the amount of information contributed by that entry in the entire document. Consider a pair of outcomes x and y, say the occurrence of words x and y, the MI is defined as:\n)5( )(\n)/( log\n)4( )(\n)/( log\n)3( )()(\n),( log),(\nyp\nxyp\nxp\nyxp\nypxp\nyxp yxMI\n=\n=\n=\nThe measure is symmetric and can be positive or negative values, but is zero if x and y are independent.\n[ ])(log),(logmin),( ypxpyxMI \u2212\u2212\u2264\u2264\u221e\u2212\nThe value of MI maximizes when X and Y are perfectly associated. The negative MI shows that the co-occurrence is too small. The Positive PMI (PPMI) [12] is a modified version of PMI, in which all MI values that are less than zero are replaced with zero [13].\nConsider the TSM, F, with t rows and n columns. The row vector iW corresponds to the ith\nword and the column vector jS corresponds to the jth sentence..\nAn element ijF gives the number of occurrence of i th word in the j th sentence. The row f :i\ncorresponds to a word iw and the column f j: corresponds to a context jS .The PPMI value of\nan element can be calculated as\n)10( 0\n0\n)9( . log\n)8(\n)7(\n)6(\n1 1\n1\n1 1\n1\n1 1\n   > =\n      =\n=\n=\n=\n\u2211\u2211\n\u2211\n\u2211\u2211\n\u2211\n\u2211\u2211\n= =\n=\n= =\n=\n= =\notherwise\nPifp ppmi\npspwi\npw p\nf\nf\nps\nf\nf\npwi\nf\nf pw\nijij\nij\nij\nt\ni\nn\nj\nij\nt\ni\nij\nt\ni\nn\nj\nij\nn\nj\nij\nt\ni\nn\nj\nij\nij\nwhere pw is the probability that the word i w occurs in the sentence j with respect to the entire\ndocument, i pw is the probability of word i w in the entire documents and ps is the probability\nof a sentence in the entire document. If iw and js are statistically independent,\nthen pwpspwi =. , and thus ji ppmi is zero (since log(1) = 0). The product pspwi. is what\nwe would expect for pw if iw occurs in js by pure random chance. If there is semantic relation\nbetween iw and js , then the pw should be larger than it would be if iw and js were\nindependent; hence pwi.ps pw > , and ji ppmi is positive; otherwise ji ppmi should have a\nvalue zero."}, {"heading": "2.5 Ranking the sentence", "text": "The total significance of kth sentence, sk, can be calculated from the PPMI matrix as\n)11(. 1 k\nt\ni kik psPPMIs \u2211 = =\n,where kps is the probability of kth sentence in context of document to be summarized.\nThe sentences in the entire documents are ranked according to the ks . The sentences with required percentage weight is identified, and arranged in the order of as it in the original document."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": "A bunch of top hit articles in the online edition of Washington post are collected for the experiment. The articles contain an average of 850 words and 45 sentences. These articles are stored as plain text. The implementation strategy of our method is explained in the figure 1.\nHere we are considering seven documents for discussing the implementation details. The figure 2 explains the status of feature extracting process after the first phase.\nEven if the total number of words before and after stemming has a well defined relation, the number of words after stemming has considerably decreased in each document.\nAn average of 50% of words is eliminated from each document in the first phase.\nFigure 2: The total number of words that has to be considered for the next phases is decreased significantly after the first phase.\nFigure1: There are three phases in the implementation; the document to be summarized is given to the tokenization process of first phase. The summary of the document will be outputted from the identify sentence process of the third phase.\nThe unique words identified in the first phase are used to create Term-Sentence-Matrix. Number of occurrence of ith word in jth sentence is the initial value of an entry, and naturally it will be 1 in most of the cases. The weight of each term in context of corresponding sentence and document are derived from the TSM using equations 6, 7, 8, 9 and 10.\nThe least significant elements in the TSM are eliminated while calculating the PPMI. The sentences are ranked according to the weight obtained in PPMI.\nWeight of kth sentence, ks is calculated from the matrix PPMI using equation 11.\nWeight of a sentence is the direct measure of relevance of a sentence in a document. It is quite clear from the figure3 that in some cases, the weight of the sentence is not proportional to the number of words in it. For example, title is the first sentence in all documents used in the experiments, and the relevance of the words in the title is comparatively larger than other words in the remaining sentences.\nNumber of sentences required in the abstract is identified and extracts the sentences with higher importance from the original document. These sentences are arranged in order of original document to obtain the desired summary."}, {"heading": "4. EVALUATION", "text": "There is no clear and standardized explanation for the question, what constitutes a good summary. Evaluation of summary is a major challenge in summarization systems. Researchers are working over the last decades to answer that complex question. Evaluation based on Latent Semantic Analysis[15] is new method in this area. This method evaluates the quality of summary through the content similarity between the document and its summary."}, {"heading": "4.1 Measure of Main Topic", "text": "In addition to the existing PPMI matrix, we have constructed another matrix, SMI, for the summary from PPMI. SMI consist of t rows and l columns, where l is the number of sentences in the summary. The SVD method decomposes PPMI into three components as\n)12(TVdSdUdPPMI =\nand the SMI will be transformed as\n)13(TVsSsUsMI =\nThe first left singular vector of Ud is called the main topic[18] of the article. In this approach the main topic of both summary and document are calculated.\nThese vectors are the most significant features of the document and summary.\nThe classical cosine, \u03c6cos , between the \u201cmain topic vector\u201d of document and the \u201csummary\u201d\nreveals the degree of quality of the abstract.\n\u2211 =\n= t\ni ii usud 1 )14(.cos\u03c6\nWhere ud and us are the main topic of Ud and Us respectively. The following figure shows the final result of evaluation.\nTABLE2:\n\u03c6cos BETWEEN THE MAIN TOPIC OF DOCUMENT AND ITS SUMMARY\nDoc Abstract in %\n10 15 20 25 30\nD1 0.9994 0.9998 0.9999 1 1 D2 0.9637 0.9981 0.9981 0.9983 1 D3 0.9942 0.9972 0.9972 0.9998 1 D4 0.9973 0.999 1 1 1 D5 0.971 0.9716 0.9696 0.9772 0.9985 D6 0.9971 0.9947 0.9998 0.9999 1 D7 0.9422 0.9348 0.8318 0.9981 0.9981 AVG 0.9807 0.985 0.9709 0.9962 0.9995\nThe result given in table2 says that, as a general trend the difference between the features of documents and its abstract reduces on increasing the size of the abstract.\nThe average value of the similarity, the overall degree of success of the method, measure \u03c6cos\nfor the entire documents in the five test cases (% of abstract - 10 to 30) is 0.98646, which shows that the positive point wise mutual information technique gives a promising result in the connection with the main topic evaluation strategy."}, {"heading": "5. CONCLUSION", "text": "The proposed summarization method contains three separate phases. The porter stemming algorithm in the morphological analysis phase has reduced the feature matrix considerably. The Positive Point Mutual Information technique is used to find out the weight of sentences in a document. It is shown here, that the Latent Semantic Analysis is a reliable summary evaluation mechanism. It is noted that summary of some document reaches its maximum result in the very initial stages of experiments. The overall average value of \u03c6cos , the distance measure between\nthe main topics of summary and document, reveals that the importance of Positive Point Mutual Information in text data analysis and especially in summarization process."}], "references": [{"title": "Multi-Document Summarization as Applied in Information Retrieval", "author": ["Zhou", "D. Lei Li"], "venue": "Natural Language Processing and Knowledge Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Methods for Automatically Evaluating Answers to Complex Questions", "author": ["Jimmy Lin", "Dina Demner-Fushman"], "venue": "Information Retrieval,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Automated Text Summarization", "author": ["I. Mani"], "venue": "John Benjimans, Amsterdam. Journals,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Co-occurrence vectors from corpora vs. distance vectors from dictionaries", "author": ["Y. Niwa", "Y. Nitta"], "venue": "In Proceedings of the 15th International Conference On Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Word association norms, mutual information, and lexicography", "author": ["K. Church", "P. Hanks"], "venue": "In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Word recognition by morphological analysis", "author": ["Hankyu Lim", "Ungmo Kim"], "venue": "Intelligent Information Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "The importance of stop word removal on recall values in text categorization", "author": ["C. Silva", "B. Ribeiro"], "venue": "Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Information Retrieval: Data Structures and Algorithms", "author": ["Frakes", "R. Baeza-Yates", "(ed"], "venue": "Englewood Cliffs, NJ: Prentice-Hall,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Stemming in the language modeling framework", "author": ["James Allan", "Giridhar Kumaran"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "An algorithm for suffix stripping, Program, 14(3) pp 130\u2212137", "author": ["M.F. Porter"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1980}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Arti_cial Intelligence Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Co-occurrence vectors from corpora vs. distance vectors from dictionaries", "author": ["Y. Niwa", "Y. Nitta"], "venue": "In Proceedings of the 15th International Conference On Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "The Challenges of Automatic Summarization", "author": ["U. Hahn", "I. Mani"], "venue": "Journal of Computer Science & Information Technology (IJCSIT)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Using Latent Semantic Analysis in Text Summarization and Summary Evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proceedings of ISIM", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Expected summarization holds a list of applications like information extraction, document retrieval [2], evaluation of answer books [3], etc.", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Expected summarization holds a list of applications like information extraction, document retrieval [2], evaluation of answer books [3], etc.", "startOffset": 132, "endOffset": 135}, {"referenceID": 12, "context": "Since the first study on text extraction by Luhn appeared, the text summarization process has attracted lot of research activities [14,16,17].", "startOffset": 131, "endOffset": 141}, {"referenceID": 2, "context": "Depending on the purpose and intended users, a summary can be generic or user-focused [4].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "In this paper we suggest a method based on Positive Pointwise Mutual Information (PPMI) [5] an extension of Pointwise Mutual Information PMI[6] which gives more importance to measure the semantic similarity between the words in a document for document summarization.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "In this paper we suggest a method based on Positive Pointwise Mutual Information (PPMI) [5] an extension of Pointwise Mutual Information PMI[6] which gives more importance to measure the semantic similarity between the words in a document for document summarization.", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "METHOD In linguistics, morphology [7] deals with the arrangement and relationships between the words in a document.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Tokenization, stop words elimination [8] and stemming [9] are the sub tasks that are followed in our method.", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "Tokenization, stop words elimination [8] and stemming [9] are the sub tasks that are followed in our method.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "A couple of algorithms [Lovins Stemming, Porter Stemming] for stemming [10,11] have been developed to reduce a word to its stem or root.", "startOffset": 71, "endOffset": 78}, {"referenceID": 9, "context": "A couple of algorithms [Lovins Stemming, Porter Stemming] for stemming [10,11] have been developed to reduce a word to its stem or root.", "startOffset": 71, "endOffset": 78}, {"referenceID": 9, "context": "We are following the Porter Stemming [11] method, which is a rule based algorithm that works with both suffixes and prefixes.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Mutual Information (MI)[12] of an entry measures the amount of information contributed by that entry in the entire document.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "The Positive PMI (PPMI) [12] is a modified version of PMI, in which all MI values that are less than zero are replaced with zero [13].", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "The Positive PMI (PPMI) [12] is a modified version of PMI, in which all MI values that are less than zero are replaced with zero [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "Evaluation based on Latent Semantic Analysis[15] is new method in this area.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "The first left singular vector of Ud is called the main topic[18] of the article.", "startOffset": 61, "endOffset": 65}], "year": 2012, "abstractText": "The degree of success in document summarization processes depends on the performance of the method used in identifying significant sentences in the documents. The collection of unique words characterizes the major signature of the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive Pointwise Mutual Information, which works well for measuring semantic similarity in the TermSentence-Matrix, is used in our method to assign weights for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated from this weighted TSM, is then used to extract a summary from the document. Our experiments show that such a method would outperform most of the existing methods in producing summaries from large documents.", "creator": "PScript5.dll Version 5.2.2"}}}