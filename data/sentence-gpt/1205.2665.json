{"id": "1205.2665", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Lower Bound Bayesian Networks - An Efficient Inference of Lower Bounds on Probability Distributions in Bayesian Networks", "abstract": "We present a new method to propagate lower bounds on conditional probability distributions in conventional Bayesian networks. Our method guarantees to provide outer approximations of the exact lower bounds. A key advantage is that we can use any available algorithms and tools for Bayesian networks in order to represent and infer lower bounds. This new method yields results that are provable exact for trees with binary variables, and results which are competitive to existing approximations in credal networks for all other network structures. Our method is not limited to a specific kind of network structure. Basically, it is also not restricted to a specific kind of inference, but we restrict our analysis to prognostic inference in this article. The computational complexity is superior to that of other existing approaches.\n\n\n\n\n\n\n\n\n\nWe consider a problem in a similar way to the C-style generalizations described above. In this paper, we explore an approach in which the number of sublet sizes is proportional to the number of nodes in the distribution (or relative relative number of nodes), which are dependent on the number of nodes in each sublet size (with the degree of node weight (the smallest node) and the degree of node weight (the largest node).\n\nIn this paper, we show that an initial assumption that has a number of sublet sizes does not account for the proportion of nodes in each sublet size (with the degree of node weight (the smallest node) and the degree of node weight (the smallest node).\nTo determine which sublet size is proportional to the number of nodes in each sublet size (with the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree of node weight (the smallest node) and the degree", "histories": [["v1", "Wed, 9 May 2012 14:40:39 GMT  (197kb)", "http://arxiv.org/abs/1205.2665v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel", "rade", "bernhard sick"], "accepted": false, "id": "1205.2665"}, "pdf": {"name": "1205.2665.pdf", "metadata": {"source": "CRF", "title": "Lower Bound Bayesian Networks \u2013 An Efficient Inference of Lower Bounds on Probability Distributions in Bayesian Networks", "authors": ["Daniel Andrade"], "emails": ["daniel.andrade@is.s.u-tokyo.ac.jp", "sick@fim.uni-passau.de"], "sections": [{"heading": null, "text": "We present a new method to propagate lower bounds on conditional probability distributions in conventional Bayesian networks. Our method guarantees to provide outer approximations of the exact lower bounds. A key advantage is that we can use any available algorithms and tools for Bayesian networks in order to represent and infer lower bounds. This new method yields results that are provable exact for trees with binary variables, and results which are competitive to existing approximations in credal networks for all other network structures. Our method is not limited to a specific kind of network structure. Basically, it is also not restricted to a specific kind of inference, but we restrict our analysis to prognostic inference in this article. The computational complexity is superior to that of other existing approaches."}, {"heading": "1 INTRODUCTION", "text": "A Bayesian network is a popular means to represent joint probability distributions over a set of random variables. It consists of a graphical network which specifies dependencies between random variables by a directed graph, and of conditional probability tables (CPT) which specify for each variable a conditional probability distribution. Calculating the marginal probability distribution of a variable, given a set of observed variables (variables with evidence), is called inference in a Bayesian network.\nA special case of inference is prognostic inference which is the calculation of P (X|E1, . . . , Ev) if all evidence nodes E1, . . . , Ev are (direct or indirect) predecessors of node X and a predecessor of an evidence node is also an evidence node. Calculating any\nmarginal distribution P (X) without evidence is, therefore, considered as a special case of prognostic inference. The inference result can be used, for example, to determine expected profits, or to make important decisions. Prognostic reasoning can be an important tool, for instance, in medicine where temporal dependencies are explicitly modeled with a Bayesian network. For example, the outcome of a treatment, measured by the life expectancy of a patient, can be modeled as a consequence of the observed symptoms before treatment, and a sequence of treatments [Lucas et al., 2004].\nObviously, outcomes and decisions of an inference are sensitive to the choice of the CPT in the Bayesian network. Since most of these probabilities are typically estimated, it makes sense to check how sensitive the outcome is regarding a change of these probabilities. A method which expresses the posterior as a function of a node probability is described in [Kj\u00e6rulff and van der Gaag, 2000]. Various other types of approaches have been suggested to tackle this problem. A general idea is to model the uncertainty about the probabilities of each node and to propagate this uncertainty through the Bayesian network. A natural extension is to define a continuous probability distribution over the discrete probability distributions at each node (second-order probabilities). In [Borsotto et al., 2006] an analytic solution is derived to propagate expectation and variance of second-order probabilities. However, the computational complexity of this approach is high. Another approach to model uncertainty about a discrete probability distribution is to define probability intervals (or, more general, convex sets) instead of point probabilities which leads to the idea of credal networks [de Campos and Cozman, 2005]. How could probability intervals be found? One could, for example, ask several experts to estimate a probability interval. For that task it turns out that the imprecise Dirichlet model is a suitable means which has several desirable properties (see [Andrade et al., 2008]). Assume, for example, we want to find a lower and upper bound for the prior P (X). Further assume that ni\nstatements were given by experts favoring the statement [X = xi], and the total number of statements is n, then the lower and upper bounds for P (X = xi) can be calculated by\nP (X = xi) = ni\nd + n ,\nP (X = xi) = ni + d d + n ,\nrespectively, where d is a hyperparameter which is set to a strictly positive value, preferably 1 or 2 (see [Walley, 1996]). If we define the constant dd+n as the degree of ignorance about the probability distribution P (X), we can see that the upper bound is actually induced by the lower bound. That is, given the lower bound for any P (X = xi), we can calculate the upper bound by adding the degree of ignorance. The consequence is that in this case it is sufficient to have a framework which allows to specify lower bounds (and not explicit lower and upper bounds). One might think that being able to propagate lower bounds in a Bayesian network without error is sufficient to infer exact upper bounds by calculating the induced upper bounds. Unfortunately, this is not true since an exact upper bound can be lower than an upper bound induced by the correponding exact lower bounds. But it is easy to see that in the case where all nodes have only two states, the exact upper bound always equals the induced upper bound.\nBasically, the idea of specifying only lower bounds inspired us to define the new approach outlined in this article, which we call the Lower Bound Bayesian Network (LBBN ) method. LBBN provide an elegant and efficient solution to approximate exact lower bounds which can be run on any Bayesian network structure. The inferred lower bound and the induced upper bound are guaranteed to form an interval which includes the exact lower and upper bound, i.e., we guarantee an outer approximation. In the case that the network structure is a tree and all nodes are binary, our approach yields exact solutions for lower and upper bounds\u2014and it does this faster than the 2U algorithm [Ide and Cozman, 2008]. Whereas existing approximations for interval propagation in graphical models often set limitations on the structure of the graphical model or on the number of states, our method can be run on any Bayesian network using any existing algorithm for inference in standard Bayesian networks. Therefore, our method is complementary to some existing methods (e.g., in the case of prognostic inference in multi-connected networks) and competing with some existing methods (e.g., in the case of prognostic inference in binary multi-connected networks). In cases where it is competing, we will demonstrate (Sections 4.3 and 5) that our method always provides the best\ncomputational complexity, and it even leads to exact results in trees with binary variables. In all other cases it yields the second-best accuracy among the other existing approximations analyzed in this article.\nIn the following section we will give a short overview of the existing work which is closely related to our method. In Section 3 we will introduce LBBN in detail. In Section 4 we will sketch the proof that our method delivers outer approximations for lower and upper bounds which is a very desirable property, since only this property allows for making prudent decisions based on the approximated estimates. Furthermore, we will sketch the proof that our method yields exact solutions when the network is a tree and all variables are binary. In Section 4.3 we will show the superiority of LBBN in terms of computational complexity, and in Section 5 we demonstrate with some simulation experiments that our method is competitive to approximations for credal networks in terms of accuracy. Section 6 finally summarizes the major findings."}, {"heading": "2 RELATED WORK", "text": "Currently, the most important means for representing and propagating lower and upper bounds on probabilities in Bayesian networks are credal networks. They generalize the concept of Bayesian networks by propagating convex sets of discrete probability distributions instead of point probabilities (see, e.g., [de Campos and Cozman, 2005]). However, inference even in polytrees, except if all nodes have only two states, is shown to be NP-hard [de Campos and Cozman, 2005]. Therefore, various approximations have been suggested (see [da Rocha et al., 2003, Ide and Cozman, 2008, Tessem, 1992]). We will compare the performance of inference methods in credal networks to our method in Sections 4.3 and 5.\nRegarding the idea to concentrate on specifying only lower bounds instead of lower and upper bounds, our method is closely related to the work of Fertig and Breese [Fertig and Breese, 1990]. Their method yields outer approximations, too. However, the run-time of their algorithm is exponential with respect to the number of nodes (see Section 4.3). We will also make use of one result of Fertig and Breese\u2019s work in Section 4.2 in order to prove that our method yields exact lower bounds in trees with binary variables (and, thus, exact upper bounds, too)."}, {"heading": "3 LOWER BOUND BAYESIAN NETWORKS (LBBN)", "text": "In this section, Lower Bound Bayesian Networks are introduced. LBBN provide a novel way to approximate\nlower bounds. They deliver outer approximations such as the methods which were mentioned in the preceding section. In contrast to those, LBBN does not require to implement new algorithms or to extend given data structures. The reason is that they are actually standard Bayesian networks, where only the interpretation of the entries in the CPT is modified."}, {"heading": "3.1 DEFINITION OF LBBN", "text": "First, let us briefly recall the definition of a Bayesian network. It is defined by a directed non-cyclical graph. Furthermore, for each node Xi there is a discrete conditional probability distribution P (Xi|\u03a0(Xi)), where \u03a0(Xi) are the predecessor nodes of node Xi. The conditional probability distribution for each node is defined in the conditional probability tables (CPT). Let us refer to the state space of Xi by Si.\nIn an LBBN, the structure equals the structure of a Bayesian network, however, the meaning of the entries in the CPT is changed. First, each entry of the CPT P (X = x|\u03a0(X)) is replaced by a lower bound P (X = x|\u03a0(X)). After this step, the entries in a CPT do not sum up to one anymore. The mass 1 \u2212 \u2211 x P (X = x|\u03a0(X)) is the degree of ignorance and it can also be interpreted as a kind of free mass which is not assigned to any state of X. In order to recreate probabilities, a new state, called N is introduced, which collects this free mass in each CPT. Finally, having adapted the CPT appropriately, any Bayesian network algorithm can be used to infer any marginal distribution in this newly created Bayesian network.\nLet us now describe this transformation in some more detail. First, the LBBN has the same structure as the Bayesian network, but each node gets an additional state N . The node Xi of the original Bayesian network will be named X \u2032i in the new Bayesian network, and the state space of node X \u2032i will be named S \u2032 i = Si \u222a {N}. The conditional probability distribution for a node X \u2032i is then defined as follows: First, assume that xi \u2208 Si. If none of the states of the nodes in \u03a0(X \u2032i) is N , we set P (X \u2032i = xi|\u03a0(X \u2032i)) to the given lower bound of the probability P (Xi = xi|\u03a0(Xi)), i.e.,\nP (X \u2032i = xi|\u03a0(X \u2032i)) := P (Xi = xi|\u03a0(Xi)) .\nIf there are e nodes X \u2032u1 , . . . , X \u2032 ue \u2208 \u03a0(X \u2032 i), e \u2265 1, with their states being set to N , then\nP (X \u2032i = xi| . . . , X \u2032u1 = N, X \u2032 u2 = N, . . . , X \u2032 ue = N, . . .)\n:= min xu1\u2208Su1 ,...,xue\u2208Sue P (Xi = xi|\u03a0(Xi)) ,\nwhere P (X \u2032i = xi| . . . , X \u2032u1 = N, X \u2032 u2 = N, . . . , X \u2032 ue = N, . . .) is the conditional probability associated with node X \u2032i, given that the predecessors X \u2032 u1 , . . . , X \u2032 ue are\nset to N and all its other predecessors are set to a state which is not N . X \u2032u = N means that node Xu can be in any state Su. As a consequence, the previously defined probability can be interpreted as the minimal chance of being in state Xi = xi, given our ignorance about the current states of Xu1 , Xu2 , . . . , Xue . This interpretation of probability mass in state N is comparable to that in Dempster-Shafer theory, when mass is allocated to the whole state space \u2126. Finally, consider xi being N . In that case, we set P (X \u2032i = xi|\u03a0(X \u2032i)) to the free mass, i.e.,\nP (X \u2032i = N |\u03a0(X \u2032i)) := 1\u2212 \u2211\nxi\u2208Si\nP (X \u2032i = xi|\u03a0(X \u2032i)).\nIn other words, if we are uncertain about whether we are in a certain state of Xi, we represent this by Xi = N , meaning we might be in any state of Xi. Note that in the case that none of the states in \u03a0(X \u2032i) is N , P (X \u2032i = N |\u03a0(X \u2032i)) = 1\u2212 \u2211 xi\u2208Si P (Xi = xi|\u03a0(Xi))."}, {"heading": "3.2 EXAMPLE", "text": "Consider the Bayesian network shown in Figure 1. Each node is binary and the state spaces of the nodes E, F and G are {e1, e2}, {f1, f2}, and {g1, g2}, respectively. Given the lower bounds on the conditional probability distributions of each node, we are interested in finding the exact lower bound for P (G = g1). The lower bounds are defined according to Table 1.\nFirst, we create a new Bayesian network which has the same structure, but each node gets an additional state N . In order to distinguish between the old nodes and the new nodes with one extra state, we add the symbol\n\u2032 to the nodes, i.e., node E, becomes E\u2032 and so forth. The CPT of this LBBN are then defined according to Section 3.1 which results in Table 2. Note that probability tables for the nodes X, Y , and Z are not stated since they are irrelevant for calculating P (G = g1)\u2014these nodes are also called barren, see [Shachter, 1990]. We can now determine P (G\u2032 = g1) using any available algorithm for inference in Bayesian networks. In this case we might just sum over all other nodes which results in P (G\u2032 = g1) = 0.752. This value is the exact lower bound for the probability P (G\u2032 = g1).\nIn the following we will show that in certain special cases this procedure always yields exact lower bounds, and in other cases it yields conservative approximations."}, {"heading": "4 MAJOR PROPERTIES OF LBBN", "text": "In this section we prove two important properties of LBBN. The first is that inference in LBBN yields outer approximations, which is a fundamental justification for our method. The second is that we can even deliver exact solutions if we restrict the network to trees with binary variables. We also comment on the computational complexity of inference in LBBN."}, {"heading": "4.1 OUTER APPROXIMATION", "text": "What we want to show is that when calculating P (X \u2032|E\u20321, . . . , E\u2032v) with any existing Bayesian network inference algorithm, P (X \u2032|E\u20321, . . . , E\u2032v) is a conservative approximation of the exact lower bound P (X|E1, . . . , Ev). Note that if P (X \u2032|E\u20321, . . . , E\u2032v) is a conservative approximation of the exact lower bound, we can easily find a conservative approximation of the exact upper bound P (X|E1, . . . , Ev) by using the upper bound which is induced by the approximated lower bounds, i.e., P (X = a|E1, . . . , Ev) \u2264 1\u2212 \u2211 x\u2208S\\{a} P (X \u2032 = x|E\u20321, . . . , E\u2032v).\nIn order to facilitate the notation, let us make the\nfollowing assumptions without loss of generality. Let the n nodes in the network X1, . . . , Xn be ordered such that observed variables (E1, . . . Ev) correspond to Xr+2,. . . ,Xn, and X corresponds to Xr+1. We assume further that the indices of the other nodes are chosen such that X1, . . . , Xr are sorted using their top sort rank. Top sort can be run on the network since a Bayesian network does not contain cycles. For the observed variables we shortly write E (and when we refer to the corresponding nodes in the LBBN we write E\u2032). Therefore,\nP (X|E) =\n\u2211 x1 . . . \u2211 xr n\u220f i=1\nP (Xi|\u03a0(Xi))\u2211 x1 . . . \u2211 xr+1 n\u220f i=1 P (Xi|\u03a0(Xi)) .\nIn the general inference case, we could not simplify further, and using our proposed method we would have to find a lower bound for the nominator and an upper bound for the denominator independently. In our future work we must, therefore, investigate the additional approximation error introduced in the case of general inference. Here, we will limit the discussion to prognostic inference, which equals to Xi \u2208 E \u21d2 \u03a0(Xi) \u2208 E, and we simplify the above equation to:\nP (X|E) = \u2211 x1 . . . \u2211 xr n\u220f i=1 P (Xi|\u03a0(Xi))\n\u220fn i=r+2 P (Xi|\u03a0(Xi)) \u2211 x1 . . . \u2211 xr+1 r+1\u220f i=1 P (Xi|\u03a0(Xi))\nand thus\nP (X|E) = \u2211 x1 . . . \u2211 xr r+1\u220f i=1 P (Xi|\u03a0(Xi)) .\nNext, we will use that we sorted the indices according to top sort, and factor out the conditional probabilities which leads to\nP (X|E) = \u2211 x1 P (X1|\u03a0(X1)) . . .\n. . . \u2211 xr P (Xr|\u03a0(Xr))\u00b7P (Xr+1|\u03a0(Xr+1)).\nWe can show by induction over r (see Appendix A) that\u2211\nx1\u2208S1\nP (X1|\u03a0(X1)) . . .\n. . . \u2211\nxr\u2208Sr P (Xr|\u03a0(Xr)) \u00b7 P (Xr+1|\u03a0(Xr+1)) \u2265\u2211 x1\u2208S\u20321 P (X \u20321|\u03a0(X \u20321)) . . .\n. . . \u2211\nxr\u2208S\u2032r\nP (X \u2032r|\u03a0(X \u2032r)) \u00b7 P (X \u2032r+1|\u03a0(X \u2032r+1)) .\n(1)\nTherefore, P (X|E) \u2265 P (X \u2032|E\u2032). Finally, P (X|E) \u2265 P (X|E) \u2265 P (X \u2032|E\u2032) which proves our claim."}, {"heading": "4.2 EXACT LOWER BOUNDS", "text": "Making the same assumptions as above and assuming additionally that the given network is a tree and all variables are binary, we will now show that our method yields exact lower bounds, i.e., P (X \u2032|E\u2032) inferred in the LBBN equals P (X|E). Note that in this case, we can then easily calculate exact upper bounds using P (X = a|E) = 1\u2212 \u2211 x\u2208S\\{a} P (X = x|E) and |S| = 2.\nLet us use the same notation as above. Since we have a tree and consider only prognostic inference, the only part of the network which is relevant for calculating P (X|E) is the chain from X1 to Xr+1, where all predecessors of X1 are observed variables (variables with evidence). One way to retrieve the exact lower bound for P (Xr+1|E) is to use a sequence of \u201cnode reduction\u201d operations [Shachter, 1986, Fertig and Breese, 1990]. A node reduction operation is used to remove a node Y from the network in the following way: Let us assume that the node Y is connected with a directed edge to node X and that X has no other predecessors (which is true since we consider only trees). After removing Y , one has to connect its predecessors to its successor X and can calculate the exact lower bound for a new conditional probability associated with X in the following way: Let \u03a0 new(X) be the new set of predecessors of X, i.e., \u03a0new(X) = (\u03a0(X) \\ {Y }) \u222a \u03a0(Y ) = \u03a0(Y ). Then,\nP (X|\u03a0new(X)) := P (X|Y = y\u2217) \u00b7 u(Y = y\u2217|\u03a0(Y ))\n+ \u2211\ny\u2208(SY \\{y\u2217})\nP (Y = y|\u03a0(Y ))P (X|Y = y) ,\nwhere y\u2217 := argminy\u2208SY P (X|Y = y) and u(Y = y\u2217|\u03a0(Y )) := 1 \u2212 \u2211 y 6=y\u2217 P (Y = y|\u03a0(Y )). In other words, for a node removal operation it is guaranteed that the resulting lower bound is again the exact lower bound. Since all nodes are assumed to be binary, the exact upper bound must be the induced upper bound. As a consequence, another node removal operation will again yield the exact lower bound, and so forth. Therefore, one way to calculate P (Xr+1|E) is to perform a sequence of node removal operations starting with X1 and ending with the removal of Xr.\nIf we can show that one node removal operation equals a summation over the corresponding node in the LBBN, it is then easy to show (proof by induction) that a summation over the nodes X \u20321, . . . , X \u2032 r yields the exact lower bound for P (X|E). In the following, for the sake of brevity, we limit the proof to one node\nremoval operation, and assume that r = 1. Let us denote with P (X2|\u03a0 new(X2)) the exact lower bound for P (X2|\u03a0 new(X2)), where \u03a0 new(X2) denotes the predecessors of X2 after the node removal of X1. In other words, by removing X1 we retrieve P (X2|\u03a0 new(X2)). Then, P (X2|\u03a0 new(X2)) = P (X2|E). Now let us prove that P (X \u20322|E\u2032) = P (X2|\u03a0 new(X2)):\nP (X \u20322|E\u2032) = \u2211\nx1\u2208S\u20321\nP (X \u20321 = x1|E\u2032)P (X \u20322|X \u20321 = x1)\n= \u2211\nx1\u2208S1\nP (X \u20321 = x1|E\u2032)P (X \u20322|X \u20321 = x1)\n+ P (X \u20321 = N |E\u2032) \u00b7 P (X \u20322|X \u20321 = N) 1) = \u2211 x1\u2208S1 P (X1 = x1|E)P (X2|X1 = x1)\n+ (1\u2212 \u2211\nx1\u2208S1\nP (X1 = x1|E)) \u00b7 P (X2|X1 = x\u22171)\n= \u2211\nx1\u2208(S1\\{x\u22171})\nP (X1 = x1|E)P (X2|X1 = x1)\n+ P (X1 = x\u22171|E) \u00b7 P (X2|X1 = x\u22171) + (1\u2212 \u2211\nx1\u2208(S1\\{x\u22171})\nP (X1 = x1|E)) \u00b7 P (X2|X1 = x\u22171)\n\u2212 P (X1 = x\u22171|E) \u00b7 P (X2|X1 = x\u22171) = \u2211\nx1\u2208(S1\\{x\u22171})\nP (X1 = x1|E)P (X2|X1 = x1)\n+ u(X1 = x\u22171|E) \u00b7 P (X2|X1 = x\u22171) 2) = P (X2|\u03a0 new(X2)) .\nIn this proof,\n1) x\u22171 := argminx1\u2208SX1 P (X2|X1 = x1), and\n2) uses the definition of the node removal operation.\nThe consequence is that P (X \u2032|E\u2032) = P (X|E) which proves our claim."}, {"heading": "4.3 COMPUTATIONAL COMPLEXITY", "text": "Table 3 shows the computational complexity of LBBN and various approximations which are available for credal networks. Since an LBBN is just a Bayesian network, we can run any available inference algorithm for Bayesian networks in order to determine lower and upper bounds. If the network structure is a polytree, we can, for example, use Pearl\u2018s algorithm [Pearl, 1988]. This way\u2014assuming s is the number of states of a variable and p is the number of predecessors in the network\u2014we get a computational complexity of O((s+1)p) per state of a node1 , since each node in the\n1Thus, for all states of a node we get O((s + 1)p+1).\nresulting Bayesian network will have s+ 1 states\u2014one extra state for modeling N , the degree of ignorance.\nThe first row shows the complexity of algorithms which can be run efficiently for polytrees with binary variables. Though the A/R [Tessem, 1992] and A/R+ [da Rocha et al., 2003] can be run in this case as well, we excluded them in this row since it is of little interest to compare them in this case with LBBN or 2U. The 2U algorithm (see [Ide and Cozman, 2008]) is actually not an approximation but yields exact solutions in O(4p) per node. However, our algorithm is faster and yields also exact results when the polytree is a tree.\nAnother important point that must be mentioned is that inference with LBBN can currently provide the best computational complexity of all outer approximations in polytrees. The computational complexity of A/R, including the log factor, is due to the descending and ascending sorting during the annihilation and reinforcement operations, respectively. The proof for the computational complexity of A/R+ has been omitted for the sake of brevity.\nIn multi-connected networks, all approximations are NP-hard. Inference in a multi-connected LBBN is NPhard due to the fact that inference in a multi-connected Bayesian network is NP-hard. Since fast inference in Bayesian networks can be crucial, there are numerous approaches for approximations of marginal distributions in multi-connected Bayesian networks (see, for example, [Dagum and Luby, 1997]). All of them can be used to approximate marginal distributions in LBBN as well. This way, approximative lower bounds in multi-connected networks can be found in polynomial time. Thus, LBBN can be used to infer approximative lower bounds in large multi-connected Bayesian networks, where all other algorithms might be infeasible. In the corresponding row of Table 3 we consider the costs for one node evaluation, whereas several iterations over several nodes might be necessary till convergence (see [Draper and Hanks, 1994]). This\nalso holds for the other algorithms for multi-connected networks which are mentioned in the following.\nThere are several approximations available for the special case of inference in multi-connected Bayesian networks where all nodes are binary, e.g., L2U, IPE and 2V2U, see [Ide and Cozman, 2008]. The approximations L2U, IPE, and 2V2U are all based on 2U, which explains, for example, the identical complexity of L2U and IPE which contains the factor 4p. 2V2U also makes use of 2U. In general, the computational costs for the summation over the Markov blanket with G nodes, which are proportional to 2G, will outweigh the factor 4p.\nThe last row in Table 3 shows the computational complexity of the LBBN method for inference in any multiconnected Bayesian network, whereas t is the treewidth. We found some algorithms which can be used to approximate inference in general multi-connected credal networks (e.g., [Cozman et al., 2004, Ide and Cozman, 2005, Alessandro Antonucci and de Campos, 2008]), but we did not find clear complexity statements and performance evaluations for those. Therefore, it must be investigated in the future whether our method can also outperform these approximations in terms of computational complexity or accuracy.\nFinally, we want to remark that we excluded the approach from Fertig and Breese from the table since no efficient algorithms are known even for polytrees and, thus, the best computational complexity which can be stated for their algorithm is O(sO(n)). This is due to the fact that the algorithm uses a sequence of \u201cnode reduction\u201d and \u201carc reversal\u201d operations [Shachter, 1986] to infer a posterior distribution. Finding an optimal sequence of these operations is in general intractable, even for polytrees.\nOne can see from Table 3 that in all cases the computational complexity per node is best for the LBBN method. Although comprehensive run-time experiments are necessary to assure the superiority of the LBBN method in terms of actual run-time, we expect that our method outperforms all other methods since any available, highly-optimized implementation for inference in Bayesian networks can be used to infer probabilities in LBBN.\nIn the following section we will investigate the accuracy of the LBBN method with some simulation experiments. All approximations that were compared to LBBN in this section with regard to computational complexity will be compared to LBBN regarding accuracy."}, {"heading": "5 EXPERIMENTS", "text": "In order to calculate the deviation from the exact solution we used for all experiments Cozman\u2019s credal network implementation (see [Cozman, 2002])."}, {"heading": "5.1 BENCHMARK NETWORK", "text": "In [da Rocha et al., 2003], the accuracy of the A/R and the A/R+ algorithms is investigated using the network displayed in Figure 2.2 In this network, we infer approximations for the lower bounds with our method and then use the induced upper bounds to compare our results to the ones in [da Rocha et al., 2003].\nEach node has 3 states and the credal set for each node is defined by 3 vertices. Choosing random vertices for the definition of each credal set, 15 different instances of this network were created in [da Rocha et al., 2003]. For each of those networks the upper bound for P (E = e1) was calculated using A/R and A/R+. These approximated upper bounds were then compared to the exact upper bounds. The results can be seen in Table 4. In order to make the testing conditions for the approximation with LBBN close to the ones used in [da Rocha et al., 2003], we randomly generated 3 vertices which were then used to retrieve the lower bounds for the conditional probability distribution of a node. Furthermore, we found that 15 tests are quite few, since the variation is significant when running ten trials each consisting of 15 tests. As a consequence, we decided to run 200 tests in order to get a more stable assessment. The results in Table 4, where MEA denotes mean error in absolute terms, show that LBBN clearly outperforms A/R. The much slower A/R+ (see previous section) is able to produce better results.\n2The actual network in [da Rocha et al., 2003], Figure 1, is larger than the one here. However, the other nodes depicted there are barren for the calculation of the marginal probability P (E = e0)."}, {"heading": "5.2 RANDOMLY GENERATED NETWORKS", "text": "In order to compare our results to the ones presented in [Ide and Cozman, 2008], we randomly generated Bayesian networks using the BNGenerator [Ide, 2004]. Except for the number of nodes, the number of states, and the number of edges, we used the default parameter settings. These parameters were set in order to create five types of multi-connected networks with 10 nodes. The first 3 types have an edge density of 1.2 (i.e., 12 edges), 1.4, and 1.6, and each network contains only binary nodes. The last 2 types have an edge density of 1.2 and 1.4, respectively, and all their nodes have four states. We randomly created 10 different network structures for each type, and for each network structure we created 100 instances with randomly chosen lower bounds. For each instance we calculated the Mean Square Error (MSE). The averages over all instances for all structures of one type are set out in Table 5. We calculated the MSE in accordance with the calculation set out in [Ide and Cozman, 2008]:\u221a\n1 Q \u2211 X \u2211 x\u2208SX (P \u2217(x)\u2212 P (x))2 + (P \u2217(x)\u2212 P (x))2 ,\nwhere the exact lower and upper bounds for P (X = x) are denoted by P (x) and P (x), respectively. Furthermore, \u2217 denotes the approximations of the exact lower and upper bounds with the LBBN method. Q is the number of summands. In our experiments, the number of states s is the same for all nodes, and, thus, Q equals 2 \u00b7 n \u00b7 s, where n is the number of nodes in the network.\nIn Table 5, these results can be compared to the results for L2U, IPE, and SV2U published in [Ide and Cozman, 2008]. As we can see, our simple LBBN method based on standard Bayesian networks is the second best method in terms of accuracy, and can, therefore, directly compete with L2U whose accuracy is better but whose computational costs per node are higher (see previous section). Furthermore, the last two rows of Table 5 show that our method can provide reasonable good approximations for non-binary networks,\nwhere none of the other approximation methods can provide a solution. Note that this is due to the fact that the other methods are, by design, not applicable to multi-connected networks which have a node with more than two states.\nIn the future, we plan to compare our method to other approximations for large networks, in particular to the GL2U [Alessandro Antonucci and de Campos, 2008] which also provides approximations for general network structures."}, {"heading": "6 CONCLUSIONS", "text": "In this article, we presented a novel way to propagate lower bounds on conditional probabilities in Bayesian networks. We focused on prognostic inference and proved that our method guarantees to provide outer approximations which we showed to be competitive concerning accuracy to existing approximations, while providing the best computational complexity of all approximations. For the special case of prognostic inference in Bayesian networks with binary variables, our method provides exact solutions even faster than the fastest currently available method for exact inference in binary Bayesian networks, the 2U algorithm [Ide and Cozman, 2008]. In large networks, exact propagation is intractable\u2014in our experiments we reached the limit of computing exact lower and upper bounds with a network having only 10 nodes and 16 edges\u2014 and other approximations are either computationally expensive, limited to certain network structures, or both. The LBBN method provides a feasible solution for lower bound propagation in large networks. Finally, we want to emphasize that our method can be run using any existing algorithm and implementation for inference in Bayesian networks. Therefore, it allows for a sensitivity analysis in any large Bayesian network at virtually no extra costs."}, {"heading": "Acknowledgment", "text": "We would like to thank the anonymous reviewers of the article for their very constructive suggestions which helped us to improve the quality of this article. We also highly appreciate the comments of Sebastian Riedel."}, {"heading": "A Appendix", "text": "Proof. We will show that inequality (1) holds.\nLet us denote the term P (Xj |\u03a0(Xj)) as pij , for 1 \u2264 j \u2264 r. And, analogously, P (X \u2032j |\u03a0(X \u2032j)) as p\u2032ij . Finally, the term P (Xr+1|\u03a0(Xr+1)) is written short as b, and P (X \u2032r+1|\u03a0(X \u2032r+1)) is written analogously short as b\u2032. We enumerate the states Sj from 1 to sj , and the states from S\u2032j from 1 to sj + 1, whereas sj + 1 corresponds to state N .\nFor the following proof, we will exploit the following properties: For all j \u2208 {1, . . . , r} it holds that sj\u2211\nij=1 pij = 1 = sj+1\u2211 ij=1 p\u2032ij . Furthermore, note that b is dependent on i1, i2, . . . ir, and pij is also dependent on i1, i2, . . . ij\u22121. Furthermore, it holds that \u2200ij \u2208 {1, . . . , sj} : pij \u2265 p\u2032ij \u2227 b \u2265 b\n\u2032. It also holds that for ij 6= sj + 1 and 1 \u2264 d < j, then\np\u2032ij (id = sd + 1) \u2264 p \u2032 ij (id 6= sd + 1). Moreover, b\u2032(ij = sj + 1) \u2264 b\u2032(ij 6= sj + 1).\nAssuming the above properties holds, we can state s1\u2211\ni1=1\npi1 s2\u2211 i2=1 pi2 . . . sr\u2211 ir=1 (pir \u00b7 b) \u2265\ns1+1\u2211 i1=1 p\u2032i1 s2+1\u2211 i2=1 p\u2032i2 . . . sr+1\u2211 ir=1 (p\u2032ir \u00b7 b \u2032) .\nProof by induction over r. Induction basis, r = 1:\ns1\u2211 i1=1 pi1 \u00b7 b \u2265 s1+1\u2211 i1=1 p\u2032i1 \u00b7 b \u2032 ,\nThis inequality holds, since in the term of the right hand side more mass of pi1 is assigned to the state of b\u2032, for which b\u2032 is minimal.\nInduction step, from r to r + 1: To show is that\ns1\u2211 i1=1 pi1 s2\u2211 i2=1 pi2 . . . sr+1\u2211 ir+1=1 (pir+1 \u00b7 b) \u2265 s1+1\u2211 i1=1 p\u2032i1 s2+1\u2211 i2=1 p\u2032i2 . . . sr+1+1\u2211 ir+1=1 (p\u2032ir+1 \u00b7 b \u2032) .\nLet us denote\nbnew := sr+1\u2211\nir+1=1\npir+1 \u00b7 b\nand\nb\u2032new := sr+1+1\u2211 ir+1=1 p\u2032ir+1 \u00b7 b \u2032 .\nUsing the same argument like in the basis, one can see that bnew \u2265 b\u2032new . We now have to show that b\u2032new(ij = sj + 1) \u2264 b\u2032new(ij 6= sj + 1), for all 1 \u2264 j \u2264 r. This holds since, b\u2032(ij = sj + 1) \u2264 b\u2032(ij 6= sj + 1), and thus b\u2032new(ij = sj + 1) is a weighted average with more weight on the smallest value of b\u2032. We can now use the inductive assumption, to conclude that\ns1\u2211 i1=1 pi1 s2\u2211 i2=1 pi2 . . . sr\u2211 ir=1 (pir \u00b7 bnew) \u2265\ns1+1\u2211 i1=1 p\u2032i1 s2+1\u2211 i2=1 p\u2032i2 . . . sr+1\u2211 ir=1 (p\u2032ir \u00b7 b \u2032new) ."}], "references": [{"title": "Knowledge fusion using Dempster-Shafer theory and the imprecise Dirichlet model", "author": ["D. Andrade", "T. Horeis", "B. Sick"], "venue": "In Proceedings of the IEEE Conference on Soft Computing in Industrial Applications,", "citeRegEx": "Andrade et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Andrade et al\\.", "year": 2008}, {"title": "URL http://www.pmr.poli.usp.br/ltd/Software/ JavaBayes", "author": ["F.G. Cozman"], "venue": "Java Bayes. Website,", "citeRegEx": "Cozman.,? \\Q2002\\E", "shortCiteRegEx": "Cozman.", "year": 2002}, {"title": "Inference in polytrees with sets of probabilities", "author": ["J.C. da Rocha", "F.G. Cozman", "C.P. de Campos"], "venue": "In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Rocha et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rocha et al\\.", "year": 2003}, {"title": "An optimal approximation algorithm for Bayesian inference", "author": ["P. Dagum", "M. Luby"], "venue": "Artificial Intelligence,", "citeRegEx": "Dagum and Luby.,? \\Q1997\\E", "shortCiteRegEx": "Dagum and Luby.", "year": 1997}, {"title": "The inferential complexity of Bayesian and credal networks", "author": ["C.P. de Campos", "F.G. Cozman"], "venue": "In International Joint Conference On Artificial Intelligence,", "citeRegEx": "Campos and Cozman.,? \\Q2005\\E", "shortCiteRegEx": "Campos and Cozman.", "year": 2005}, {"title": "Localized partial evaluation of belief networks", "author": ["D.L. Draper", "S. Hanks"], "venue": "In Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Draper and Hanks.,? \\Q1994\\E", "shortCiteRegEx": "Draper and Hanks.", "year": 1994}, {"title": "Interval influence diagrams", "author": ["K.W. Fertig", "J.S. Breese"], "venue": "In Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Fertig and Breese.,? \\Q1990\\E", "shortCiteRegEx": "Fertig and Breese.", "year": 1990}, {"title": "Approximate algorithms for credal networks with binary variables", "author": ["J.S. Ide", "F.G. Cozman"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Ide and Cozman.,? \\Q2008\\E", "shortCiteRegEx": "Ide and Cozman.", "year": 2008}, {"title": "Approximate inference in credal networks by variational mean field methods", "author": ["J.S. Ide", "F.G. Cozman"], "venue": "In Proceedings of the Fourth International Symposium on Imprecise Probabilities and Their Applications,", "citeRegEx": "Ide and Cozman.,? \\Q2005\\E", "shortCiteRegEx": "Ide and Cozman.", "year": 2005}, {"title": "Making sensitivity analysis computationally efficient", "author": ["U. Kj\u00e6rulff", "L.C. van der Gaag"], "venue": "In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Kj\u00e6rulff and Gaag.,? \\Q2000\\E", "shortCiteRegEx": "Kj\u00e6rulff and Gaag.", "year": 2000}, {"title": "Bayesian networks in biomedicine and health-care", "author": ["P.J.F. Lucas", "L.C. van der Gaag", "A. Abu-Hanna"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "Lucas et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lucas et al\\.", "year": 2004}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Intelligent probabilistic inference", "author": ["R.D. Shachter"], "venue": "Uncertainty in Artificial Intelligence,", "citeRegEx": "Shachter.,? \\Q1986\\E", "shortCiteRegEx": "Shachter.", "year": 1986}, {"title": "Evidence absorption and propagation through evidence reversals", "author": ["R.D. Shachter"], "venue": "In Proceedings of the Fifth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Shachter.,? \\Q1990\\E", "shortCiteRegEx": "Shachter.", "year": 1990}, {"title": "Interval probability propagation", "author": ["B. Tessem"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Tessem.,? \\Q1992\\E", "shortCiteRegEx": "Tessem.", "year": 1992}, {"title": "Inferences from multinomial data: Learning about a bag of marbles", "author": ["P. Walley"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Walley.,? \\Q1996\\E", "shortCiteRegEx": "Walley.", "year": 1996}], "referenceMentions": [{"referenceID": 10, "context": "For example, the outcome of a treatment, measured by the life expectancy of a patient, can be modeled as a consequence of the observed symptoms before treatment, and a sequence of treatments [Lucas et al., 2004].", "startOffset": 191, "endOffset": 211}, {"referenceID": 0, "context": "For that task it turns out that the imprecise Dirichlet model is a suitable means which has several desirable properties (see [Andrade et al., 2008]).", "startOffset": 126, "endOffset": 148}, {"referenceID": 15, "context": "respectively, where d is a hyperparameter which is set to a strictly positive value, preferably 1 or 2 (see [Walley, 1996]).", "startOffset": 108, "endOffset": 122}, {"referenceID": 7, "context": "In the case that the network structure is a tree and all nodes are binary, our approach yields exact solutions for lower and upper bounds\u2014and it does this faster than the 2U algorithm [Ide and Cozman, 2008].", "startOffset": 184, "endOffset": 206}, {"referenceID": 6, "context": "Regarding the idea to concentrate on specifying only lower bounds instead of lower and upper bounds, our method is closely related to the work of Fertig and Breese [Fertig and Breese, 1990].", "startOffset": 164, "endOffset": 189}, {"referenceID": 13, "context": "Note that probability tables for the nodes X, Y , and Z are not stated since they are irrelevant for calculating P (G = g1)\u2014these nodes are also called barren, see [Shachter, 1990].", "startOffset": 164, "endOffset": 180}, {"referenceID": 11, "context": "If the network structure is a polytree, we can, for example, use Pearl\u2018s algorithm [Pearl, 1988].", "startOffset": 83, "endOffset": 96}, {"referenceID": 14, "context": "Though the A/R [Tessem, 1992] and A/R+ [da Rocha et al.", "startOffset": 15, "endOffset": 29}, {"referenceID": 7, "context": "The 2U algorithm (see [Ide and Cozman, 2008]) is actually not an approximation but yields exact solutions in O(4) per node.", "startOffset": 22, "endOffset": 44}, {"referenceID": 3, "context": "Since fast inference in Bayesian networks can be crucial, there are numerous approaches for approximations of marginal distributions in multi-connected Bayesian networks (see, for example, [Dagum and Luby, 1997]).", "startOffset": 189, "endOffset": 211}, {"referenceID": 5, "context": "In the corresponding row of Table 3 we consider the costs for one node evaluation, whereas several iterations over several nodes might be necessary till convergence (see [Draper and Hanks, 1994]).", "startOffset": 170, "endOffset": 194}, {"referenceID": 7, "context": ", L2U, IPE and 2V2U, see [Ide and Cozman, 2008].", "startOffset": 25, "endOffset": 47}, {"referenceID": 12, "context": "This is due to the fact that the algorithm uses a sequence of \u201cnode reduction\u201d and \u201carc reversal\u201d operations [Shachter, 1986] to infer a posterior distribution.", "startOffset": 109, "endOffset": 125}, {"referenceID": 1, "context": "In order to calculate the deviation from the exact solution we used for all experiments Cozman\u2019s credal network implementation (see [Cozman, 2002]).", "startOffset": 132, "endOffset": 146}, {"referenceID": 7, "context": "In order to compare our results to the ones presented in [Ide and Cozman, 2008], we randomly generated Bayesian networks using the BNGenerator [Ide, 2004].", "startOffset": 57, "endOffset": 79}, {"referenceID": 7, "context": "We calculated the MSE in accordance with the calculation set out in [Ide and Cozman, 2008]: \u221a 1 Q \u2211", "startOffset": 68, "endOffset": 90}, {"referenceID": 7, "context": "In Table 5, these results can be compared to the results for L2U, IPE, and SV2U published in [Ide and Cozman, 2008].", "startOffset": 93, "endOffset": 115}, {"referenceID": 7, "context": "For the special case of prognostic inference in Bayesian networks with binary variables, our method provides exact solutions even faster than the fastest currently available method for exact inference in binary Bayesian networks, the 2U algorithm [Ide and Cozman, 2008].", "startOffset": 247, "endOffset": 269}], "year": 2009, "abstractText": "We present a new method to propagate lower bounds on conditional probability distributions in conventional Bayesian networks. Our method guarantees to provide outer approximations of the exact lower bounds. A key advantage is that we can use any available algorithms and tools for Bayesian networks in order to represent and infer lower bounds. This new method yields results that are provable exact for trees with binary variables, and results which are competitive to existing approximations in credal networks for all other network structures. Our method is not limited to a specific kind of network structure. Basically, it is also not restricted to a specific kind of inference, but we restrict our analysis to prognostic inference in this article. The computational complexity is superior to that of other existing approaches.", "creator": "TeX"}}}