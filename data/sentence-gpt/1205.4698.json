{"id": "1205.4698", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2012", "title": "The Role of Weight Shrinking in Large Margin Perceptron Learning", "abstract": "We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels.\n\n\nThe first step of the update is to define the minimum-size bounds for the fixed width of the vector using the weighted weight vectors. The current threshold for the width of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors.\nWe then proceed to define the minimum-size bounds of the vector using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. We then proceed to define the minimum-size bounds of the vector by applying the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the weighted weight vectors. The new approximation to the minimum-size bounds of the vector is fixed using the", "histories": [["v1", "Mon, 21 May 2012 19:19:49 GMT  (106kb)", "https://arxiv.org/abs/1205.4698v1", "15 pages"], ["v2", "Thu, 7 Feb 2013 19:10:14 GMT  (107kb)", "http://arxiv.org/abs/1205.4698v2", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["constantinos panagiotakopoulos", "petroula tsampouka"], "accepted": false, "id": "1205.4698"}, "pdf": {"name": "1205.4698.pdf", "metadata": {"source": "CRF", "title": "The Role of Weight Shrinking in Large Margin Perceptron Learning", "authors": ["Constantinos Panagiotakopoulos"], "emails": ["costapan@eng.auth.gr,", "petroula@gen.auth.gr"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n46 98\nv2 [\ncs .L\nG ]\n7 F\neb 2"}, {"heading": "1 Introduction", "text": "It is widely accepted that the generalization ability of learning machines improves as the margin of the solution hyperplane increases [23]. The simplest online learning algorithm for binary linear classification, the perceptron [18, 13], does not aim at any margin. The problem, instead, of finding the optimal separating hyperplane is central to Support Vector Machines (SVMs) [23, 2].\nSVMs obtain large margin solutions by solving a constrained quadratic optimization problem using dual variables. In the early days, however, efficient implementation of SVMs was hindered by the quadratic dependence of their memory requirements on the number of training examples. To overcome this obstacle decomposition methods [17, 7] were developed that apply optimization only to a subset of the training set. Although such methods led to considerable improvement the problem of excessive runtimes when processing very large datasets remained. Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.\nThe drawbacks of the dual formulation motivated research long before the advent of linear SVMs in alternative large margin classifiers naturally formulated in primal space. Having the perceptron as a prototype they focus on the primal problem by updating a weight vector which represents their current state whenever a data point presented to them satisfies a specific condition. By exploiting\ntheir ability to process one example at a time1 they save time and memory and acquire the potential to handle large datasets. The first algorithm of the kind is the perceptron with margin [4] the solutions of which provably possess only up to 1/2 of the maximum margin [11]. Subsequently, various others succeeded in approximately attaining maximum margin by employing modified perceptron-like update rules. For ROMMA [12] such a rule is the result of a relaxed optimization which reduces all constraints to just two. In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a \u201cprojection\u201d mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates. Very recently, the margitron [15] and the perceptron with dynamic margin (PDM) [16] using modified conditions managed to approximately reach maximum margin solutions while maintaining the original perceptron update.\nA somewhat different approach from the hard margin one adopted by most of the algorithms above was also developed which focuses on the minimization of the regularized 1-norm soft margin loss through stochastic gradient descent. Notable representatives of this approach are the pioneer NORMA [10] and Pegasos [19]. Stochastic gradient descent gives rise to perceptron-like updates an important ingredient of which is the \u201cshrinking\u201d of the current weight vector. Shrinking is always imposed when a pattern is presented to the algorithm with it being the only modification suffered by the weight vector in the event that its condition is violated and as a consequence no loss is incurred. The cummulative effect of shrinking is to gradually diminish the impact of the earlier contributions to the weight vector. Shrinking has also been employed by algorithms which do not have their origin in stochastic gradient descent as an accompanying mechanism in perceptron-based budget scenarios for classification [3] or tracking [1].\nOur purpose in the present work is to investigate the role that shrinking of the weight vector might play in large margin perceptron learning. This is motivated by the observation that such a mechanism naturally emerges in attempts to attack the 1-norm soft margin task through stochastic gradient descent. If we accept that algorithms like NORMA succeed in minimizing the regularized 1-norm soft margin loss they should be able to solve the hard margin problem as well for sufficiently small non-zero values of the regularization parameter which also controls the strength of shrinking. Thus shrinking, as weak as it may be, when introduced into the perceptron algorithm with margin might prove beneficial. Another factor to be taken into account is that the shrinking mechanism in the algorithms considered here is operative only for erroneous trials, a feature that offers them the possibility to terminate in a finite number of steps. Therefore, shrinking in such algorithms may need to be strengthened relative to algorithms like NORMA to compensate for the fact that the latter shrink the weight vector even when the condition is violated. In conclusion, the amount of shrinking that a perceptron with margin could tolerate without it destroying the conservativeness of the update might be sufficient to raise the theoretically\n1 The conversion of online algorithms to the batch setting is done by cycling repeatedly through the dataset and using the last hypothesis for prediction.\nguaranteed fraction of the maximum margin achieved to a value larger than 1/2. It turns out that this is actually the case.\nThe remaining of this paper is organized as follows. Section 2 contains some preliminaries and a description of the algorithms. In Sect. 3 we present a theoretical analysis of the algorithms. Section 4 is devoted to implementational issues and a brief experimental evaluation while Sect. 5 contains our conclusions."}, {"heading": "2 The Algorithms", "text": "Let us consider a linearly separable training set {(xk, lk)}mk=1, with vectors xk \u2208 IRd and labels lk \u2208 {+1,\u22121}. This training set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality [23, 2]. By placing xk in the same position at a distance \u03c1 in an additional dimension, i.e., by extending xk to [xk, \u03c1], we construct an embedding of our data into the socalled augmented space [4]. This way, we construct hyperplanes possessing bias in the non-augmented feature space. Following the augmentation, a reflection with respect to the origin of the negatively labeled patterns is performed. This allows for a uniform treatment of both categories of patterns. Also, R \u2261 max\nk \u2016yk\u2016 with\nyk \u2261 [lkxk, lk\u03c1] the kth augmented and reflected pattern. The relation characterizing optimally correct classification of the training patterns yk by a weight vector u of unit norm in the augmented space is\nu \u00b7 yk \u2265 \u03b3d \u2261 max u\u2032:\u2016u\u2032\u2016=1 min i {u\u2032 \u00b7 yi} \u2200k . (1)\nWe shall refer to \u03b3d as the maximum directional margin. It coincides with the maximum margin in the augmented space with respect to hyperplanes passing through the origin. The maximum directional margin \u03b3d is upper bounded by the maximum geometric margin \u03b3 in the non-augmented space and tends to it as \u03c1 \u2192 \u221e [20].\nWe consider algorithms in which the augmented weight vector ast is initially set to zero, i.e. as0 = 0, and is updated according to the perceptron-like rule\na s t+1 = c s ta s t + \u03b7yk (2)\neach time the \u201cmisclassification\u201d condition\nc\u0304sta s t \u00b7 yk \u2264 b (3)\nis satisfied by a training pattern yk, i.e., whenever a margin error is made on yk. Here 0 < cst , c\u0304 s t \u2264 1 are \u201cshrinking factors\u201d which may vary with the number t of updates, \u03b7 > 0 is a constant learning rate and b > 0 acts as a margin threshold in the misclassification condition. If we set cst = c\u0304 s t = 1 we recover the classical perceptron algorithm with margin. The role of cst is to shrink the current weight vector as a first step of the update, thereby enhancing the importance of the current update relative to the previous one. At the same time such a shrinking\nacts as a mechanism of effectively increasing the margin threshold of the condition, an effect that may be further strengthened through the introduction of the factor c\u0304st in (3). In fact, for appropriate choices of c s t , c\u0304 s t , to which we confine our interest here, it is possible to equivalently introduce shrinking into the perceptron with margin via a learning rate and margin threshold which both increase with t. Notice that we denote by ast the weight vector of the algorithms with shrinking to reserve the notation at for the weight vector of the equivalent algorithms with variable learning rate and margin threshold.\nThe Margin Perceptron with Constant Shrinking\nInput: A linearly separable augmented dataset S = (y1, . . . ,yk, . . . ,ym) with reflection assumed Fix: \u03b7, \u03bb, b Define: cs = 1\u2212 \u03b7\u03bb Initialize: t = 0, a0 = 0, \u03b70 = \u03b7, b0 = c\ns b repeat\nfor k = 1 to m do if at \u00b7 yk \u2264 bt then\nat+1 = at + \u03b7tyk \u03b7t+1 = \u03b7t/c s bt+1 = bt/c s t\u2190 t+ 1\nuntil no update made within the for loop\nWe investigate the impact of shrinking on large margin perceptron learning by considering both constant and variable shrinking factors. If shrinking does not depend on t we set c\u0304st = 1 since a constant c\u0304st may be absorbed into a redefinition of b. We also express cst in terms of a \u201cshrinking parameter\u201d \u03bb < 1/\u03b7 as cst = 1 \u2212 \u03b7\u03bb. Then (2) becomes the update of NORMA for ast \u00b7yk \u2264 b. NORMA, however, updates its weight vector even when ast \u00b7 yk > b. In this case the update reduces to pure shrinking ast+1 = (1\u2212\u03b7\u03bb)ast . This\nis the important difference from our algorithm in which an update occurs only if the misclassification condition is satisfied, thereby making convergence in a finite number of steps possible.\nLet us divide the update rule (2) with (1 \u2212 \u03b7\u03bb)t and condition (3) with (1\u2212\u03b7\u03bb)t\u22121. Also let at = ast/(1\u2212\u03b7\u03bb)t\u22121. Then, we obtain a completely equivalent algorithm with update\nat+1 = at + \u03b7\n(1\u2212 \u03b7\u03bb)t yk (4)\nand condition\nat \u00b7 yk \u2264 b\n(1\u2212 \u03b7\u03bb)t\u22121 . (5)\nAn algorithm with variable shrinking is obtained if we choose cst = c\u0304 s t = (t/(t+ 1)) n , where n \u2265 0 is an integer. For n = 1 the shrinking factor cst entering the update is the one encountered in Pegasos. Pegasos, however, has variable learning rate, c\u0304st = 1 and performs, just like NORMA, a pure shrinking update when its condition is violated. In addition, its update ends with a projection step. A variable shrinking factor t/(t+ \u03bb) is also employed by SPA [1] in which b = 0. Such a factor is related to ours for \u03bb = n even if n 6= 1 since for t \u226b n\nt\nt+ n =\nn\u22121 \u220f\nk=0\nt+ k\nt+ k + 1 =\n(\nt\nt+ 1\n)n n\u22121 \u220f\nk=0\n(\n1 + k\nt(t+ k + 1)\n) \u2248 ( t\nt+ 1\n)n\n.\nLet us multiply both the update rule (2) and condition (3) with (t+1)n and set at = t n a s t . Then, we obtain a completely equivalent algorithm with update\nat+1 = at + \u03b7(t+ 1) n yk (6)\nand condition at \u00b7 yk \u2264 b(t+ 1)n . (7)\nThe Margin Perceptron with Variable Shrinking\nInput: A linearly separable augmented dataset S = (y1, . . . ,yk, . . . ,ym) with reflection assumed Fix: \u03b7, b, n Initialize: t = 0, a0 = 0 repeat\nfor k = 1 to m do tn = (t+ 1) n\nbt = btn \u03b7t = \u03b7tn if at \u00b7 yk \u2264 bt then\nat+1 = at + \u03b7tyk t\u2190 t+ 1\nuntil no update made within the for loop\nIf we had chosen c\u0304st = 1 we should have multiplied (3) with tn. As a result the threshold in (7) would have been btn, a difference that does not seem to be of paramount importance. However, the choice c\u0304st = (t/(t+ 1)) n prevailed for the sake of convenience. The choice, instead, cst = c\u0304 s t = t/(t + n)= Pnt /P n t+1 with P n t \u2261 \u220fn\u22121 k=0 (t+ k) would have led to at = P n t a s t and to the replacement of (t+1)n with Pnt+1 in (6) and (7).\nWe shall refer to the algorithm with update (4) and condition (5)\nas the margin perceptron with constant shrinking. The algorithm, instead, with update (6) and condition (7) will be called the margin perceptron with variable shrinking. The above formulations of the algorithms are the ones that will henceforth be considered in place of the original formulations of (2) and (3)."}, {"heading": "3 Theoretical Analysis", "text": "We begin with the analysis of the margin perceptron with constant shrinking.\nTheorem 1. The margin perceptron with constant shrinking converges in a finite number tc of updates satisfying the bound\ntc \u2264 1 \u03b4(1\u2212 \u01eb) R2\n\u03b32d ln\n4\u2212 (2 + \u03b4)\u01eb+ \u03b4 (2 + \u03b4)\u01eb \u2212 \u03b4 (8)\nprovided \u03b4 \u2261 \u03b7R2/b \u2264 2 and \u01eb \u2261 1\u2212\u03bbb/\u03b32d obey the constraint \u03b4/(2+ \u03b4) < \u01eb < 1. Moreover, the zero-threshold solution hyperplane possesses margin \u03b3\u2032d which is a fraction f of the maximum margin \u03b3d obeying the inequality\nf \u2261 \u03b3 \u2032 d\n\u03b3d >\n1\n2 + \u03b4 + 1\u2212 \u01eb 2 . (9)\nFinally, an after-run lower bound on f involving the margin \u03b3\u2032d achieved, the length \u2016atc\u2016 of the solution weight vector atc and the number tc of updates is given by\nf \u2265 1\u2212 (1 \u2212 \u03b7\u03bb) tc \u03bb(1\u2212 \u03b7\u03bb)tc\u22121 \u03b3\u2032d \u2016atc\u2016 . (10)\nProof. Taking the inner product of (4) with the optimal direction u and using (1) we get\nu \u00b7 at+1 \u2212 u \u00b7 at = \u03b7 (1\u2212 \u03b7\u03bb)tu \u00b7 yk \u2265 \u03b7 (1 \u2212 \u03b7\u03bb)t \u03b3d\na repeated application of which, taking into account that a0 = 0, gives\n\u2016at\u2016 \u2265 u \u00b7 at \u2265 t\u22121 \u2211\nk=0\n\u03b7\u03b3d (1 \u2212 \u03b7\u03bb)k = 1\u2212 (1\u2212 \u03b7\u03bb)t \u03bb(1\u2212 \u03b7\u03bb)t\u22121 \u03b3d . (11)\nHere we made use of \u2211t\u22121 k=0 \u03b1 k = (\u03b1t \u2212 1)/(\u03b1\u2212 1). From (4) and (5) we obtain\n\u2016at+1\u20162 \u2212 \u2016at\u20162 = \u03b72\n(1\u2212 \u03b7\u03bb)2t \u2016yk\u2016 2 +\n2\u03b7 (1\u2212 \u03b7\u03bb)tat \u00b7 yk \u2264 \u03b72R2 + 2\u03b7(1\u2212 \u03b7\u03bb)b (1 \u2212 \u03b7\u03bb)2t .\nA repeated application of the above inequality, assuming a0 = 0, leads to\n\u2016at\u20162 \u2264 t\u22121 \u2211\nk=0\n\u03b72R2 + 2\u03b7(1\u2212 \u03b7\u03bb)b (1 \u2212 \u03b7\u03bb)2k =\n( 1\u2212 (1\u2212 \u03b7\u03bb)2t ) ( \u03b7R2 + 2(1\u2212 \u03b7\u03bb)b )\n\u03bb(2 \u2212 \u03b7\u03bb)(1 \u2212 \u03b7\u03bb)2(t\u22121) .\n(12)\nComparing the lower bound on \u2016at\u20162 from (11) with its upper bound (12) we get\n(1\u2212 (1\u2212 \u03b7\u03bb)t)2\n\u03bb \u03b32d \u2264 1\u2212 (1\u2212 \u03b7\u03bb)2t 2\u2212 \u03b7\u03bb ( \u03b7R2 + 2(1\u2212 \u03b7\u03bb)b )\n(13)\nor, noticing that 1\u2212 (1\u2212 \u03b7\u03bb)2t = (1\u2212 (1 \u2212 \u03b7\u03bb)t) (1 + (1\u2212 \u03b7\u03bb)t), we obtain\n1\u2212 (1\u2212 \u03b7\u03bb)t \u2264 ( 1 + (1\u2212 \u03b7\u03bb)t ) A . (14)\nHere\nA \u2261 \u03bb ( b\n\u03b32d\n)\n\u03b7R2/b+ 2(1\u2212 \u03b7\u03bb) 2\u2212 \u03b7\u03bb < 1 . (15)\nThe condition A < 1 ensures that (14) does lead to an upper bound on the number of updates since otherwise (14) is always satisfied. This translates into a very restrictive upper bound on the shrinking parameter \u03bb. This upper bound depends on the values of the remaining parameters but is never larger than \u03b32d/b. From (14), provided A < 1, we easily derive the following upper bound on the number of updates\nt \u2264 tb \u2261 1 ln(1\u2212 \u03b7\u03bb)\u22121 ln 1 +A 1\u2212A . (16)\nFor \u03b4 \u2264 2 it holds that \u03b4 + 2(1\u2212 \u03b7\u03bb)\n2\u2212 \u03b7\u03bb \u2264 1 + \u03b4 2 (17)\nand\nA = (1\u2212 \u01eb) ( \u03b4 + 2(1\u2212 \u03b7\u03bb) 2\u2212 \u03b7\u03bb ) \u2264 (1\u2212 \u01eb) ( 1 + \u03b4 2 ) = 1\u2212 (2 + \u03b4)\u01eb\u2212 \u03b4 2 . (18)\nAs a consequence, \u01eb > \u03b4/(2 + \u03b4) ensures that A < 1. In addition\nln(1\u2212 \u03b7\u03bb)\u22121 \u2265 \u03b7\u03bb = \u03b4(1\u2212 \u01eb) \u03b3 2 d\nR2 . (19)\nCombining (16), (18) and (19) we finally arrive at the slightly simplified upper bound on the number of updates given by (8).\nUpon convergence of the algorithm in tc updates condition (5) is violated by all patterns. Therefore, the achieved margin \u03b3\u2032d > b/ ( (1\u2212 \u03b7\u03bb)tc\u22121 \u2016atc\u2016 ) . Thus,\nf2 = \u03b3\u2032d\n2\n\u03b32d >\nb2 (1\u2212 \u03b7\u03bb)2(tc\u22121) \u2016atc\u20162 \u03b32d \u2265 \u03bb(2\u2212 \u03b7\u03bb)b 2 (1\u2212 (1 \u2212 \u03b7\u03bb)2tc) (\u03b7R2 + 2(1\u2212 \u03b7\u03bb)b) \u03b32d\n\u2265 \u03bb(2 \u2212 \u03b7\u03bb)b 2\n(1\u2212 (1\u2212 \u03b7\u03bb)2tb ) (\u03b7R2 + 2(1\u2212 \u03b7\u03bb)b) \u03b32d =\n(\n\u03bbb\n(1\u2212 (1\u2212 \u03b7\u03bb)tb ) \u03b32d\n)2\n,\nwhere use has been made of the upper bound (12) on \u2016atc\u20162 and of the fact that (13) at t = tb holds as an equality. Taking the square root and making use of the definition of tb from (16) the previous inequality becomes\nf > \u03bb b\n\u03b32d\n(\n1 +A 2A\n)\n= 1\n2\n(\n\u03bbb\nA\u03b32d + \u03bb\nb\n\u03b32d\n)\n= 1\n2\n(\n2\u2212 \u03b7\u03bb \u03b4 + 2(1\u2212 \u03b7\u03bb) + 1\u2212 \u01eb\n)\n.\nFor \u03b4 \u2264 2 the above inequality gives rise to (9) because of (17). Finally, (10) is readily obtained if in the ratio \u03b3\u2032d/\u03b3d we employ the upper bound on \u03b3d derivable from (11). \u2293\u2294\nRemark 1. The parameters \u03b4 and \u01eb are independent. Therefore, we may consider choosing \u03b4 \u226a 1 while keeping \u01eb fixed. In this case the upper bound (8) on the number of updates becomes O (\n\u03b4\u22121R2/\u03b32d )\nand from (9) the before-run lower bound on f approaches as \u03b4 \u2192 0 the value 1 \u2212 \u01eb/2. This generalizes the wellknown result that the classical perceptron algorithm with margin (obtainable when \u03bb \u2192 0 or \u01eb \u2192 1) has in the limit \u03b4 \u2192 0 a theoretically guaranteed beforerun value of f equal to 1/2. By subsequently letting \u01eb \u2192 0 (i.e., \u03bb \u2192 \u03b32d/b) we may approach solutions with maximum margin.\nRemark 2. To facilitate comparison with other large margin classifiers we may relate the independent parameters \u03b4 and \u01eb and obtain a single parameter \u03b6 < 1/ \u221a 2 through the relations \u03b4 = 2\u03b6, \u01eb = \u03b4(1+\u03b4)/(2+\u03b4) = \u03b6(1+2\u03b6)/(1+\u03b6). Then, from (8) and (9) we have that the margin perceptron with constant shrinking achieves \u201caccuracy\u201d \u03b6, i.e., f > 1\u2212 \u03b6 , in a number tc of updates satisfying the bound\ntc \u2264 1\n\u03b6\n(\n1 + \u03b6 1\u2212 2\u03b62 ) R2\n\u03b32d ln\n\u221a\n1\u2212 \u03b62 \u03b6 .\nNotice that the quantity R/\u03b3d does not enter the logarithm. In this sense the above bound, which is O (\n(\u03b6\u22121R2/\u03b32d) ln \u03b6 \u22121 ) for \u03b6 \u226a 1, is the best among the\nbounds of perceptron-like maximum margin algorithms. Typically, algorithms which require at least an approximate knowledge of the value of \u03b3d to tune their parameters have bounds O (\n(\u03b6\u22121R2/\u03b32d) ln(\u03b6 \u22121 (R/\u03b3d)\nk ) ) with k = 1, 2 while\nalgorithms which do not assume such a knowledge have bounds O ( \u03b6\u22122R2/\u03b32d ) .\nRemark 3. Suppose we are given \u03b3\u0304d < \u03b3d. It may be expressed as \u03b3\u0304d = (1\u2212\u03be)\u03b3d. Setting \u03bb = (2/(2+ \u03b4))\u03b3\u03042d/b it holds that \u01eb = 1\u2212 (2/(2+ \u03b4))(1\u2212 \u03be)2 > \u03b4/(2+ \u03b4). Then (9) gives \u03b3\u2032d/\u03b3d > 1\u2212 \u03be + ( \u03be2 \u2212 \u03b4(1\u2212 \u03be) )\n/(2 + \u03b4). Thus, for \u03b4(1\u2212 \u03be) \u2264 \u03be2 a solution with margin \u03b3\u2032d > \u03b3\u0304d is obtained which provides a better lower bound on \u03b3d than the one used as an input. A repeated application of this procedure starting, e.g., with \u03b3\u0304d = 0, \u03be = 1, \u03bb = 0 gives solutions possessing margin which is any desirable approximation of \u03b3d without prior knowledge of its value. An estimate of the quality of the approximation at each stage may be obtained via the after-run lower bound (10) on \u03b3\u2032d/\u03b3d which provides an upper bound on \u03b3d. In practice, only a few repetitions of this procedure are required to obtain a satisfactory approximation of the optimal solution because the margin actually achieved by the algorithm is considerably larger than the one suggested by (9).\nRemark 4. From (12) we see that for the algorithm described by (2) and (3) with cst = 1 \u2212 \u03b7\u03bb, c\u0304st = 1 it holds that \u2016ast\u20162 = \u2016at\u20162 (1 \u2212 \u03b7\u03bb)2(t\u22121) \u2264 (\n\u03b7R2 + 2(1\u2212 \u03b7\u03bb)b ) / (\u03bb(2\u2212 \u03b7\u03bb)). Thus, it is confirmed in this context the wellknown fact that constant shrinking leads to bounded length of the weight vector.\nTo proceed with our analysis of the margin perceptron with variable shrinking we need some inequalities involving sums of powers of integers which we present in the form of lemmas. Their proofs can be found in the Appendix.\nLemma 1. Let n \u2265 0 be an integer. Then, it holds that\n(n+ 1)\nt \u2211\nk=1\nkn \u2264 t(t+ 1)n . (20)\nLemma 2. Let n \u2265 0 be an integer. Then, it holds that\n(n+ 1) t \u2211\nk=1\nkn \u2265 (t+ 1)n+1 \u2212 (n+ 1) 2\n2n+ 1 (t+ 1)n . (21)\nLemma 3. Let n \u2265 0 be an integer. Then, it holds that\n(2n+ 1)t\nt \u2211\nk=1\nk2n \u2264 (n+ 1)2 ( t \u2211\nk=1\nkn\n)2\n. (22)\nNow we are ready to move on with the analysis of the variable shrinking case.\nTheorem 2. The margin perceptron with variable shrinking converges in a finite number tc of updates satisfying the bound\ntc \u2264 tb \u2261 (n+ 1)2\n2n+ 1\n(\n1 + 2b\n\u03b7R2\n)\nR2 \u03b32d . (23)\nMoreover, the zero-threshold solution hyperplane possesses margin \u03b3\u2032d which is a fraction f of the maximum margin \u03b3d obeying the inequality\nf \u2261 \u03b3 \u2032 d\n\u03b3d >\n2n+ 1\n2n+ 2\n(\n1 + \u03b7R2\n2b\n)\u22121\n. (24)\nFinally, an after-run lower bound on f involving the margin \u03b3\u2032d achieved, the length \u2016atc\u2016 of the solution weight vector atc and the number tc of updates is given by\nf \u2265 \u03b7 tc \u2211\nk=1\nkn \u03b3\u2032d\n\u2016atc\u2016 . (25)\nProof. Taking the inner product of (6) with the optimal direction u and using (1) we get u \u00b7 at+1 \u2212 u \u00b7 at = \u03b7(t+ 1)nu \u00b7 yk \u2265 \u03b7(t+ 1)n\u03b3d a repeated application of which, taking into account that a0 = 0, gives\n\u2016at\u2016 \u2265 u \u00b7 at \u2265 \u03b7\u03b3d t \u2211\nk=1\nkn . (26)\nFrom (6) and (7) we obtain\n\u2016at+1\u20162\u2212\u2016at\u20162 = \u03b72(t+1)2n \u2016yk\u20162+2\u03b7(t+1)nat \u00b7yk \u2264 (\u03b72R2+2\u03b7b)(t+1)2n .\nA repeated application of the above inequality, assuming a0 = 0, leads to\n\u2016at\u20162 \u2264 (\u03b72R2 + 2\u03b7b) t \u2211\nk=1\nk2n . (27)\nCombining (26) and (27) we obtain\n\u03b72\u03b32d\n(\nt \u2211\nk=1\nkn\n)2\n\u2264 \u2016at\u20162 \u2264 (\u03b72R2 + 2\u03b7b) t \u2211\nk=1\nk2n (28)\nor\nt \u2264 ( R2 + 2b/\u03b7\n\u03b32d\n)\n(\nt t \u2211\nk=1\nk2n\n)(\nt \u2211\nk=1\nkn\n)\u22122\nwhich by virtue of (22) gives (23). Upon convergence of the algorithm in tc updates condition (7) is violated by all patterns. Therefore, the margin \u03b3\u2032d achieved satisfies \u03b3 \u2032 d > b(tc + 1)\nn/\u2016atc\u2016. Thus,\nf2 = \u03b3\u2032d\n2\n\u03b32d >\nb2(tc + 1) 2n\n\u03b32d \u2016atc\u2016 2 \u2265\nb2(tc + 1) 2n\n\u03b32d(\u03b7 2R2 + 2\u03b7b) \u2211tc k=1 k\n2n \u2265 (2n+ 1)b\n2\n\u03b32d(\u03b7R 2 + 2b)\u03b7tc\n.\n(29)\nHere we replaced \u2016atc\u20162 with its upper bound (\u03b72R2 +2\u03b7b) \u2211tc k=1 k 2n from (27) and \u2211tc\nk=1 k 2n with its upper bound tc(tc + 1) 2n/(2n + 1) from (20). Overapproximating tc by tb in (29) and substituting the value of the latter from (23) we get\nf2 > (2n+ 1)b2\n\u03b32d(\u03b7R 2 + 2b)\u03b7tb\n=\n(\n(2n+ 1)\n(n+ 1)\nb\n(\u03b7R2 + 2b)\n)2\nfrom where by taking the square root we obtain (24). Finally, (25) is readily obtained if in the ratio \u03b3\u2032d/\u03b3d we employ the upper bound on \u03b3d derivable from (26). \u2293\u2294\nRemark 5. Let us define \u03b4 \u2261 \u03b7R2/b and \u01eb \u2261 (n + 1)\u22121. Then, (23) and (24) become\ntc \u2264 1\n\u01eb\u03b4\n(\n1 + \u03b4/2 1\u2212 \u01eb/2\n)\nR2 \u03b32d\nand\nf > 1\u2212 \u01eb/2 1 + \u03b4/2 ,\nrespectively. The perceptron with margin corresponds to n = 0 or \u01eb = 1. If we choose \u03b4 \u226a 1 while keeping \u01eb (i.e., n) fixed the upper bound on the number of updates becomes O (\n\u03b4\u22121R2/\u03b32d )\nand the before-run lower bound on f approaches as \u03b4 \u2192 0 the value 1 \u2212 \u01eb/2. Then, by allowing \u01eb \u2192 0 (i.e., n \u2192 \u221e) maximum margin solutions are approximated. If we set, instead, \u03b4 = \u01eb \u226a 1 then f > 1\u2212 \u01eb and the algorithm achieves \u201caccuracy\u201d \u01eb in at most \u01eb\u22122R2/\u03b32d + O ( \u01eb\u22121R2/\u03b32d ) updates. This is among the best bounds of perceptron-like approximate maximum margin classifiers which do not assume knowledge of the value of \u03b3d in any way. For comparison, ALMA\u2019s bound is \u2243 8\u01eb\u22122R2/\u03b32d.\nRemark 6. Given that f2 \u2264 1 (29) leads to a lower bound on the number tc of updates required for convergence of the margin perceptron with variable shrinking which in terms of the parameters \u03b4 and \u01eb reads\ntc > 1\n\u01eb\u03b4\n(\n1\u2212 \u01eb/2 1 + \u03b4/2\n)\nR2 \u03b32d .\nAs \u03b4, \u01eb \u2192 0 the ratio of the above lower bound to the upper bound tends to 1 and the algorithm approaches the optimal solution in \u2243 (\u01eb\u03b4)\u22121R2/\u03b32d updates.\nRemark 7. Theorems 1 and 2 hold also for the algorithms described by (2) and (3) as appropriate provided, of course, that \u2016atc\u2016 is replaced in (10) and (25) with \u2225\n\u2225a s tc\n\u2225 \u2225 by making use of the relation connecting these two quantities.\nRemark 8. The after-run lower bounds on f given by (10) and (25) typically provide estimates of the margin achieved which are much more accurate than the ones obtained from the before-run bounds of (9) and (24), respectively. Our experience based on such estimates suggests that a satisfactory approximation of the maximum margin solution can be obtained without the need to resort to\nvery small values of the parameter \u01eb. In other words, although the theoretically guaranteed before-run fraction of the maximum margin for \u03b4 \u226a 1 is close to 1\u2212 \u01eb/2 both the estimated after-run fraction and the one actually achieved are larger. This is a generic feature of the perceptron with margin and its generalizations. It turns out that in most cases \u01eb \u2243 0.2\u2212 0.3 is sufficiently small for the algorithm to obtain for \u03b4 \u226a 1 solutions possessing 99% of the maximum margin. Thus, for constant shrinking a very accurate knowledge of the value of \u03b3d is not required while for variable shrinking very low values of n are sufficient."}, {"heading": "4 Implementation and Experiments", "text": "To reduce the computational cost we adopt a two-member nested sequence of reduced \u201cactive sets\u201d of data points as described in detail in [15]. The parameter c\u0304 which multiplies the threshold of the misclassification condition when this condition is used to select the points of the first-level active set is given the value c\u0304 = 1.01. The parameters, instead, determining the number of times the active sets are presented to the algorithm are set to the values Nep1 = Nep2 = 5.\nAn additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14\u201316] once a data point is presented to the algorithm. It is understood, of course, that a multiple update should be equivalent to a certain number of updates occurring as a result of repeatedly presenting to the algorithm the data point in question. Thus, the maximal multiplicity of such an update will be determined by the requirement that the pattern yk which satisfies the misclassification condition will just violate it as a result of the multiple update. For constant shrinking a multiple update is\nat+\u00b5 = at + 1\u2212 (1\u2212 \u03b7\u03bb)\u00b5 \u03bb(1 \u2212 \u03b7\u03bb)t+\u00b5\u22121 yk\nwith\n\u00b5 \u2264 [\n1 ln(1\u2212 \u03b7\u03bb)\u22121 ln ( 1 + \u03bb b \u2212 (1\u2212 \u03b7\u03bb)t\u22121at \u00b7 yk\n\u2016yk\u20162 \u2212 \u03bbb\n)]\n+ 1 .\nHere [x] is the integer part of x \u2265 0. For variable shrinking, instead, finding the maximal multiplicity of the update involves solving a (n+1)-th degree equation for which there is no general formula unless n \u2264 3. However, this does not pose a serious problem for several reasons. First of all, as we have already pointed out in Remark 8, we may reach very good approximations of the maximal margin hyperplane with low values of n. In addition, even if we choose a larger n we may obtain satisfactory performance with updates having multiplicity lower than the maximal one. Thus, it suffices to find a lower bound on the relevant root of the (n+1)-th degree equation. Moreover, even when the exact root is available it is often preferable to set an upper bound \u2113up on the multiplicity of the updates.\nThe aim of our experiments is to assess the ability of the margin perceptron with constant shrinking (MPCS) and the margin perceptron with variable shrinking (MPVS) to achieve fast convergence to a certain approximation of the\noptimal solution in the feature space where the patterns are linearly separable. For linearly separable data the feature space is the initial instance space. For inseparable data, instead, a space extended by m dimensions, as many as the instances, is considered where each instance is placed at a distance \u2206 from the origin in the corresponding dimension2 [5]. This extension generates a margin of at least\u2206/ \u221a m and its employment relies on the well-known equivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function \u2016w\u20162 +\u2206\u22122 \u2211\ni\u03bei 2\ninvolving the weight vector w and the 2-norm of the slacks \u03bei [2].\nIn the experiments the augmentation parameter \u03c1 was set to the value \u03c1 = 1. The values of the parameter \u2206 together with the number of instances and attributes of the datasets used are given in Table 1. Further details may be found in [16]. The experiments, like the ones of [16], were conducted on a 2.5 GHz Intel Core 2 Duo processor with 3 GB RAM running Windows Vista. Therefore, the runtimes reported here can be directly compared to the ones of [16]. Our codes written in C++ were compiled using the g++ compiler under Cygwin. They are available at http://users.auth.gr/costapan.\nIn the numerical experiments the results of which we report in Table 1 the algorithms MPCS and MPVS were required to obtain solutions possessing 99% of the maximum margin \u03b3d. Additionally, we imposed a cut-off value \u2113up = 1000 on the multiplicity of the updates. We set b = R2 such that \u03b4 = \u03b7R2/b = \u03b7 for both algorithms. For MPCS assuming knowledge of \u03b3d we chose \u03bb \u2243 0.75\u03b32d/b such that \u01eb \u2243 0.25. In the case of MPVS we set n = 3 giving \u01eb = (n+1)\u22121 = 0.25. Thus, for both algorithms the asymptotic value of the theoretically guaranteed fraction of \u03b3d that they were able to achieve in the limit \u03b4 \u2192 0 was 1 \u2212 \u01eb/2 \u2243 0.875. The lower bound on the fraction f reported is the after-run bound of (10)\n2 yk = [y\u0304k, lk\u2206\u03b41k, . . . , lk\u2206\u03b4mk], where \u03b4ij is Kronecker\u2019s \u03b4 and y\u0304k the projection of the kth extended instance yk (multiplied by its label lk) onto the initial instance space. The feature space mapping defined by the extension commutes with a possible augmentation (with parameter \u03c1) in which case y\u0304k = [lkx\u0304k, lk\u03c1]. Here x\u0304k represents the kth data point.\nand (25) which turns out in most cases to be \u2243 0.99 and certainly much larger than the before-run fraction \u2243 0.875 in accordance with our earlier discussion in Remark 8. The required value of the margin was achieved by sufficiently lowering the value of \u03b7 having knowledge of the target value. However, even if such a knowledge were not available we could have reached our goal guided by the after-run lower bound on f . From Table 1 we see that the runtimes (in seconds) of MPCS and MPVS for the same value \u03b3\u2032d of the margin achieved are comparable. More important, though, is a comparison with the results obtained with other large margin classifiers as reported in [16]. We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVMlight [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14]. We should note, however, that unlike our algorithms linear SVMs are not primal and strictly online.\nFinally, we would like to point out that in practice it is possible to set at one stage the parameter \u03bb of MPCS without prior knowledge of the value of \u03b3d. A preliminary run of MPCS with an almost vanishing \u03bb provides a lower bound on \u03b3d which is the margin \u03b3 \u2032 d achieved and an upper bound from the after-run lower bound on f . Actually, \u03b3d usually lies closer to its upper bound. This information is sufficient to choose \u03bb given that the algorithm is not extremely sensitive to this choice provided, of course, that \u03bb remains below its maximal allowed value."}, {"heading": "5 Conclusions", "text": "Motivated by the presence of weight shrinking in most attempts at solving the L1-SVM problem via stochastic gradient descent we introduced this feature into the classical perceptron algorithm with margin. In the case of constant weight decay parameter \u03bb and constant learning rate we demonstrated that convergence to solutions with approximately maximum margin requires \u03bb to approach a margin-dependent maximal allowed value. Scenarios with variable shrinking strength were also considered and proven not to be subject to such limitations. The theoretical analysis was corroborated by an experimental investigation with massive datasets which involved searching for large margin solutions in an extended feature space, a problem equivalent to the 2-norm soft margin one. As a final conclusion of our study we may say that shrinking of the current weight vector as a first step of the update is able to elevate the margin perceptron to a very effective primal online large margin classifier."}, {"heading": "A Proof of Lemma 1", "text": "Proof. We proceed by induction in the integer t. For t = 1 inequality (20) reduces to (n + 1) \u2264 2n which holds since 2n = (1 + 1)n \u2265 1 + n. Now let us assume that (20) holds and prove that (n + 1)\n\u2211t+1 k=1 k n \u2264 (t + 1)(t + 2)n\nor (n + 1) ( (t+ 1)n + \u2211t k=1 k n ) \u2264 (t + 1)(t + 2)n. Given that (20) holds it\nsuffices to prove that (n + 1)(t + 1)n + t(t + 1)n \u2264 (t + 1)(t + 2)n or that (t+ 2)n \u2265 (t + 1)n\u22121(n+ 1 + t). Indeed, (t + 2)n = (t+ 1)n ( 1 + (t+ 1)\u22121 )n \u2265 (t+ 1)n ( 1 + n(t+ 1)\u22121 ) = (t+ 1)n\u22121(t+ 1 + n). \u2293\u2294"}, {"heading": "B Proof of Lemma 2", "text": "Proof. We proceed by induction in the integer t. For t = 1 inequality (21) reduces to (n + 1)(2n+ 1) \u2265 2n (1\u2212 n(n\u2212 2)) which holds \u2200n \u2265 0. Now let us assume that (21) holds and prove that\n\u2211t+1 k=1 k n \u2265 1 n+1 (t+ 2) n+1 \u2212 n+12n+1 (t+ 2)n. Using (21) we have\n\u2211t+1 k=1 k n = (t + 1)n + \u2211t k=1 k n \u2265 (t + 1)n + 1 n+1 (t + 1) n+1 \u2212\nn+1 2n+1 (t + 1) n = 1 n+1 (t + 1) n+1 + n2n+1 (t + 1) n. Thus, it suffices to prove that F (t) \u2261 n+12n+1 (t+ 2)n + n2n+1 (t+ 1)n \u2212 1n+1 ( (t+ 2)n+1 \u2212 (t+ 1)n+1 )\n\u2265 0 or that F (t)/tn \u2265 0. By virtue of the binomial formula F (t)/tn admits the expansion\nF (t)\ntn =\nn \u2211\nl=0\nn!\nl!(n\u2212 l)!\n(\n(n+ 1)2l + n\n2n+ 1 \u2212 2 l+1 \u2212 1 l + 1\n)\nt\u2212l .\nGiven that ((n + 1)2l + n)(l + 1) \u2212 (2l+1 \u2212 1)(2n + 1) = ((l \u2212 3)2l + l + 3)n + (l \u2212 1)2l + 1 \u2265 0 \u2200l \u2265 0 the terms in the above expansion are all non-negative implying F (t)/tn \u2265 0. \u2293\u2294"}, {"heading": "C Proof of Lemma 3", "text": "Proof. We proceed by induction in the integer t. For t = 1 inequality (22) reduces to 2n+1 \u2264 (n+1)2 which obviously holds \u2200n \u2265 0. Now let us assume that (22) holds and prove that (2n+1)(t+1)\n\u2211t+1 k=1 k\n2n \u2264 (n+1)2 (\n\u2211t+1 k=1 k\nn )2\n. Using (22)\nwe have (2n + 1)(t + 1) \u2211t+1 k=1 k 2n = (2n + 1)(t + 1)\n(\n(t+ 1)2n + \u2211t k=1 k 2n ) =\n(2n + 1)(t + 1)2n+1 + t+1 t (2n + 1)t\n\u2211t\nk=1 k 2n \u2264 (2n + 1)(t + 1)2n+1 + t+1 t (n +\n1)2 ( \u2211t k=1 k n )2 . Also (n+1)2 ( \u2211t+1 k=1 k n )2 = (n+1)2 ( (t+ 1)n + \u2211t k=1 k n )2 = (n + 1)2(t+ 1)2n + (n+ 1)2 ( \u2211t k=1 k n )2 + 2(n + 1)2(t+ 1)n \u2211t k=1 k n. Thus, it\nsuffices to prove that (2n+1)(t+1)2n+1+ t+1 t (n+1)2\n(\n\u2211t k=1 k n )2 \u2264 (n+1)2(t+\n1)2n + (n + 1)2 ( \u2211t k=1 k n )2 + 2(n + 1)2(t + 1)n \u2211t k=1 k n or, equivalently, that (n + 1) ( 2(n+ 1)t(t+ 1)n \u2212 (n+ 1) \u2211t\nk=1 k n ) \u2211t k=1 k n + (n + 1)2t(t + 1)2n \u2212\n(2n+1)t(t+1)2n+1 \u2265 0. Replacing in the above inequality (n+1) \u2211t k=1 k n with its upper bound t(t+ 1)n from (20) we end up with the inequality (n+ 1)(2n+ 1)t(t+1)n \u2211t\nk=1 k n+(n+1)2t(t+1)2n\u2212(2n+1)t(t+1)2n+1 \u2265 0 to prove which,\nhowever, is equivalent to (21). \u2293\u2294"}], "references": [{"title": "Tracking the best hyperplane with a simple budget perceptron", "author": ["N. Cesa-Bianchi", "C. Gentile"], "venue": "COLT, pp. 483-498", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to support vector machines", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "The forgetron: A kernel-based perceptron on a fixed budget", "author": ["O. Dekel", "S. Shalev-Shwartz", "Singer. Y."], "venue": "NIPS, 18 pp. 259-266", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Pattern classsification and scene analysis", "author": ["R.O. Duda", "P.E. Hart"], "venue": "Wiley, Chichester", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1973}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Y. Freund", "R.E. Shapire"], "venue": "Machine Learning 37(3), 277-296", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "A new approximate maximal margin classification algorithm", "author": ["C. Gentile"], "venue": "Journal of Machine Learning Research 2, 213-242", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods-Support Vector Learning. MIT Press, Cambridge", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "KDD pp. 217-226", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["Hsieh", "C.-J.", "Chang", "K.-W.", "Lin", "C.-J.", "S.S. Keerthi", "S. Sundararajan"], "venue": "ICML pp. 408-415", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A. Smola", "R. Williamson"], "venue": "IEEE Transactions on Signal Processing 52(8), 2165-2176", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning algorithms with optimal stability in neural networks", "author": ["W. Krauth", "M. M\u00e9zard"], "venue": "Journal of Physics A20, L745-L752", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "The relaxed online maximummargin algorithm", "author": ["Y. Li", "P. Long"], "venue": "Machine Learning, 46(1-3), 361-387", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "On convergence proofs on perceptrons", "author": ["A.B.J. Novikoff"], "venue": "Proc. Symp. Math. Theory Automata, vol. 12, pp. 615-622", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1962}, {"title": "The margin perceptron with unlearning", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "ICML pp. 855\u2013862", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "The margitron: A generalized perceptron with margin", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "IEEE Transactions on Neural Networks 22(3), 395-407", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "The perceptron with dynamic margin", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "Kivinen, J., et. al. (eds.) ALT 2011. LNCS (LNAI) vol. 6925, pp. 204-218. Springer, Heidelberg", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["J.C. Platt"], "venue": "Microsoft Res. Redmond WA, Tech. Rep. MSR-TR-98-14", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65(6), 386-408", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1958}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Schwartz", "Y. Singer", "N. Srebro"], "venue": "ICML pp. 807-814", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Analysis of generic perceptron-like large margin classifiers", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "Gama, J., et. al. (eds.) ECML 2005. LNCS (LNAI) vol. 3720, pp. 750-758. Springer, Heidelberg", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Constant rate approximate maximum margin algorithms", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "F\u00fcrnkranz, J., et. al. (eds.) ECML 2006. LNCS (LNAI) vol. 4212, pp. 437-448. Springer, Heidelberg", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximate maximum margin algorithms with rules controlled by the number of mistakes", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "ICML pp. 903\u2013910", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistical learning theory", "author": ["V. Vapnik"], "venue": "Wiley, Chichester", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 22, "context": "It is widely accepted that the generalization ability of learning machines improves as the margin of the solution hyperplane increases [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "The simplest online learning algorithm for binary linear classification, the perceptron [18, 13], does not aim at any margin.", "startOffset": 88, "endOffset": 96}, {"referenceID": 12, "context": "The simplest online learning algorithm for binary linear classification, the perceptron [18, 13], does not aim at any margin.", "startOffset": 88, "endOffset": 96}, {"referenceID": 22, "context": "The problem, instead, of finding the optimal separating hyperplane is central to Support Vector Machines (SVMs) [23, 2].", "startOffset": 112, "endOffset": 119}, {"referenceID": 1, "context": "The problem, instead, of finding the optimal separating hyperplane is central to Support Vector Machines (SVMs) [23, 2].", "startOffset": 112, "endOffset": 119}, {"referenceID": 16, "context": "To overcome this obstacle decomposition methods [17, 7] were developed that apply optimization only to a subset of the training set.", "startOffset": 48, "endOffset": 55}, {"referenceID": 6, "context": "To overcome this obstacle decomposition methods [17, 7] were developed that apply optimization only to a subset of the training set.", "startOffset": 48, "endOffset": 55}, {"referenceID": 7, "context": "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.", "startOffset": 40, "endOffset": 50}, {"referenceID": 8, "context": "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.", "startOffset": 40, "endOffset": 50}, {"referenceID": 13, "context": "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.", "startOffset": 40, "endOffset": 50}, {"referenceID": 3, "context": "The first algorithm of the kind is the perceptron with margin [4] the solutions of which provably possess only up to 1/2 of the maximum margin [11].", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "The first algorithm of the kind is the perceptron with margin [4] the solutions of which provably possess only up to 1/2 of the maximum margin [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "For ROMMA [12] such a rule is the result of a relaxed optimization which reduces all constraints to just two.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a \u201cprojection\u201d mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.", "startOffset": 18, "endOffset": 21}, {"referenceID": 20, "context": "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a \u201cprojection\u201d mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a \u201cprojection\u201d mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Very recently, the margitron [15] and the perceptron with dynamic margin (PDM) [16] using modified conditions managed to approximately reach maximum margin solutions while maintaining the original perceptron update.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "Very recently, the margitron [15] and the perceptron with dynamic margin (PDM) [16] using modified conditions managed to approximately reach maximum margin solutions while maintaining the original perceptron update.", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "Notable representatives of this approach are the pioneer NORMA [10] and Pegasos [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "Notable representatives of this approach are the pioneer NORMA [10] and Pegasos [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "Shrinking has also been employed by algorithms which do not have their origin in stochastic gradient descent as an accompanying mechanism in perceptron-based budget scenarios for classification [3] or tracking [1].", "startOffset": 194, "endOffset": 197}, {"referenceID": 0, "context": "Shrinking has also been employed by algorithms which do not have their origin in stochastic gradient descent as an accompanying mechanism in perceptron-based budget scenarios for classification [3] or tracking [1].", "startOffset": 210, "endOffset": 213}, {"referenceID": 22, "context": "This training set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality [23, 2].", "startOffset": 126, "endOffset": 133}, {"referenceID": 1, "context": "This training set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality [23, 2].", "startOffset": 126, "endOffset": 133}, {"referenceID": 3, "context": ", by extending xk to [xk, \u03c1], we construct an embedding of our data into the socalled augmented space [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "The maximum directional margin \u03b3d is upper bounded by the maximum geometric margin \u03b3 in the non-augmented space and tends to it as \u03c1 \u2192 \u221e [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "A variable shrinking factor t/(t+ \u03bb) is also employed by SPA [1] in which b = 0.", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "To reduce the computational cost we adopt a two-member nested sequence of reduced \u201cactive sets\u201d of data points as described in detail in [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14\u201316] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 141}, {"referenceID": 14, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14\u201316] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 141}, {"referenceID": 15, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14\u201316] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 141}, {"referenceID": 4, "context": "For inseparable data, instead, a space extended by m dimensions, as many as the instances, is considered where each instance is placed at a distance \u2206 from the origin in the corresponding dimension [5].", "startOffset": 198, "endOffset": 201}, {"referenceID": 1, "context": "This extension generates a margin of at least\u2206/ \u221a m and its employment relies on the well-known equivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function \u2016w\u2016 +\u2206 \u2211 i\u03bei 2 involving the weight vector w and the 2-norm of the slacks \u03bei [2].", "startOffset": 331, "endOffset": 334}, {"referenceID": 15, "context": "Further details may be found in [16].", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "The experiments, like the ones of [16], were conducted on a 2.", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "Therefore, the runtimes reported here can be directly compared to the ones of [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "More important, though, is a comparison with the results obtained with other large margin classifiers as reported in [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].", "startOffset": 188, "endOffset": 191}, {"referenceID": 13, "context": "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].", "startOffset": 200, "endOffset": 204}], "year": 2013, "abstractText": "We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}