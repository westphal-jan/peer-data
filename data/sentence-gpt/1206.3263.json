{"id": "1206.3263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Sparse Stochastic Finite-State Controllers for POMDPs", "abstract": "Bounded policy iteration is an approach to solving infinite-horizon POMDPs that represents policies as stochastic finite-state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming. In the original algorithm, the size of the linear programs, and thus the complexity of policy improvement, depends on the number of parameters of each node, which grows with the size of the controller. But in practice, the number of parameters of a node with non-zero values is often very small, and does not grow with the size of the controller. Based on this observation, we develop a version of bounded policy iteration that leverages the sparse structure of a stochastic finite-state controller. In each iteration, it improves a policy by the same amount as the original algorithm, but with much better scalability. In our new algorithm, the number of parameters is increased, thus reducing the complexity of policy improvement in the design process. In each iteration, it reduces the complexity of policy improvement, which, due to its lower linear program structure, makes it easier to perform the calculations on the entire application.\n\n\n\nThe main drawback of the implementation of bounded policy iteration is that it is much simpler to run the code while simultaneously testing the program. In the previous version, the function defined by a linear program was defined as \"probability\" that increases the overall complexity of the implementation, but which increases the number of parameters of the program. This is because it improves the computation of the program for a longer time. This is due to the smaller number of parameters being added, so that the size of the program remains small and the program is executed in a more stable state. Furthermore, because the program is not limited to multiple program states, it is more likely to run more computationally, since the program is not constrained to multiple program states, and therefore the execution of all the parameters in the program is slower. We also introduce a new class of algorithms for the application to compare the results of a model with a set of model parameters, which will optimize performance. For example, we set the number of parameters for a model by comparing the maximum number of parameters in the model to a set of parameters in the model.\nWe use this method to analyze the algorithm of a particular program state for a more recent version, and then estimate that it will perform better, but not slower, in a simpler way. Using the new method, we can observe a significant decrease in the overall performance of the program. We use", "histories": [["v1", "Wed, 13 Jun 2012 15:34:42 GMT  (112kb)", "http://arxiv.org/abs/1206.3263v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eric a hansen"], "accepted": false, "id": "1206.3263"}, "pdf": {"name": "1206.3263.pdf", "metadata": {"source": "CRF", "title": "Sparse Stochastic Finite-State Controllers for POMDPs", "authors": ["Eric A. Hansen"], "emails": ["hansen@cse.msstate.edu"], "sections": [{"heading": null, "text": "Bounded policy iteration is an approach to solving infinite-horizon POMDPs that represents policies as stochastic finite-state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming. In the original algorithm, the size of the linear programs, and thus the complexity of policy improvement, depends on the number of parameters of each node, which grows with the size of the controller. But in practice, the number of parameters of a node with non-zero values is often very small, and does not grow with the size of the controller. Based on this observation, we develop a version of bounded policy iteration that leverages the sparse structure of a stochastic finite-state controller. In each iteration, it improves a policy by the same amount as the original algorithm, but with much better scalability."}, {"heading": "1 Introduction", "text": "Partially observable Markov decision processes (POMDPs) provide a framework for decision-theoretic planning problems where actions need to be taken based on imperfect state information. Many researchers have shown that a policy for an infinite-horizon POMDP can be represented by a finite-state controller. In some cases, this is a deterministic controller in which a single action is associated with each node, and an observation results in a deterministic transition to a successor node (Kaelbling, Littman, & Cassandra, 1998; Hansen, 1998; Meuleau, Kim, Kaelbling, & Cassandra, 1999a). In other cases, it is a stochastic controller in which actions are selected based on a probability distribution associated with each node, and an observation results in a probabilistic transition to a successor node (Platzman, 1981; Meuleau, Peshkin, Kim, & Kaelbling, 1999b; Baxter & Bartlett, 2001; Poupart & Boutilier, 2004; Amato, Bernstein, & Zilberstein, 2007).\nBounded policy iteration (BPI) is an approach to solving infinite-horizon POMDPs that represents policies as stochastic finite-state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming, where the parameters specify the action selection and node transition probabilities of the node (Poupart & Boutilier, 2004). BPI is related to an exact policy iteration algorithm for POMDPs due to Hansen (1998), but differs from it by providing an elegant and effective approach to approximation in which bounding the size of the controller allows a tradeoff between planning time and plan quality. Originally developed as an approach to solving single-agent POMDPs, BPI has also been generalized for use in solving decentralized POMDPs (Bernstein, Hansen, & Zilberstein, 2005).\nIn BPI, the complexity of policy improvement depends on the size of the linear programs used to adjust the parameters of each node of the controller. In turn, this depends on the number of parameters of each node (as well as the size of the state space). In the original algorithm, each node of the controller has |A|+|A||Z||N | parameters, where |A| in the number of actions, |Z| is the number of observations, and |N | is the number of nodes of the controller. This assumes a fully-connected stochastic controller. In practice, however, most of these parameters have zero probabilities, and the number of parameters of a node with non-zero probabilities remains relatively constant as the number of nodes of the controller increases. Based on this observation, we propose a modified version of BPI that leverages a sparse representation of a stochastic finite-state controller. In each iteration, it improves the controller by the same amount as the original algorithm. But it does so by solving much smaller linear programs, where the number of variables in each linear program depends on the number of parameters of a node with non-zero values. Because the number of parameters of a node with non-zero values tends to remain relatively constant as the size of the controller grows, the complexity of each iteration of the modified version of BPI tends to grow only linearly with the number of nodes of a controller, which is a dramatic improvement in scalability compared to the original algorithm."}, {"heading": "2 Background", "text": "We consider a discrete-time infinite-horizon POMDP with a finite set of states, S , a finite set of actions, A, and a finite set of observations, Z . Each time period, the environment is in some state s \u2208 S, the agent takes an action a \u2208 A, the environment makes a transition to state s\u2032 \u2208 S with probability P (s\u2032|s, a), and the agent observes z \u2208 Z with probability P (z|s\u2032, a). In addition, the agent receives an immediate reward with expected value R(s, a) \u2208 <. We assume the objective is to maximize expected total discounted reward, where \u03b2 \u2208 (0, 1] is the discount factor. Since the state of the environment cannot be directly observed, we let b denote an |S|-dimensional vector of state probabilities, called a belief state, where b(s) denotes the probability that the system is in state s. If action a is taken and followed by observation z, the successor belief state, denoted baz , is determined using Bayes\u2019 rule."}, {"heading": "2.1 Policy representation and evaluation", "text": "A policy for a POMDP can be represented by a finite-state controller (FSC). A stochastic finite-state controller is a tuple < N , \u03c8, \u03b7 > where N is a finite set of nodes, \u03c8 is an action selection function that specifies the probability, \u03c8n(a) = P (a|n), of selecting action a \u2208 A when the FSC is in node n \u2208 N , and \u03b7 is a node transition function that specifies the probability, \u03b7n(a, z, n\u2032) = P (n\u2032|n, a, z), that the FSC will make a transition from node n \u2208 N to node n\u2032 \u2208 N after taking action a \u2208 A and receiving z \u2208 Z. The value function of a policy represented by a FSC is piecewise linear and convex, and can be computed exactly by solving the following system of linear equations, with one equation for each pair of node n \u2208 N and state s \u2208 S:\nVn(s) = \u2211\na\u2208A \u03c8n(a)R(s, a) + (1)\n\u03b2 \u2211\na,z,s,n\u2032 \u03b7n(a, z, n\u2032)P (s\u2032|s, a)P (z|s\u2032, a)Vn\u2032(s\u2032).\nIn this representation of the value function, there is one |S|-dimensional vector Vn for each node n \u2208 N of the controller. The value of any belief state b is determined as follows,\nV (b) = max n\u2208N\n\u2211 s\u2208S b(s)Vn(s), (2)\nand the controller is assumed to start in the node that maximizes the value of the initial belief state. The value function of an optimal policy satisfies the optimality equation,\nV (b) = max a\u2208A\n{ R(b, a) + \u03b2 \u2211\nz\u2208Z P (z|b, a)V (baz)\n} , (3)\nwhere R(b, a) = \u2211\ns\u2208S b(s)R(s, a) and P (z|b, a) =\u2211 s\u2208S b(s) \u2211 s\u2032\u2208S P (s \u2032|s, a)P (z|s\u2032, a)."}, {"heading": "2.2 Bounded policy iteration", "text": "Policy iteration algorithms iteratively improve a policy by alternating between two steps: policy evaluation and policy improvement. Hansen (1998) proposed a policy iteration algorithm for POMDPs that represents a policy as a deterministic finite-state controller. In the policy improvement step, it uses the dynamic programming update for POMDPs to add, merge and prune nodes of the controller. The algorithm is guaranteed to converge to an \u00b2-optimal controller and can detect convergence to an optimal policy.\nA potential problem is that the number of nodes added in the policy improvement step can be large, and the controller can grow substantially in size from one iteration to the next. Because the complexity of the policy improvement step increases with the size of the controller, allowing the size of the controller to grow too fast can slow the rate of further improvement and limit scalability. The same problem occurs in value iteration, where the number of vectors in the piecewise linear and convex representation of the value function can grow too fast from one iteration to the next. Feng and Hansen (2001) describe a method for performing approximate dynamic programming updates that has the effect of reducing the number of vectors added to a value function in value iteration, or the number of nodes added to a controller in policy iteration.\nFor policy iteration, Poupart and Boutilier (2004) propose another approach that also has the benefit of avoiding an expensive dynamic programming update. Their bounded policy iteration algorithm controls growth in the size of the controller in two ways. First, a policy is represented as a stochastic finite-state controller that can be improved without increasing its size, by adjusting the action and nodetransition probabilities of each node of the controller using linear programming. Second, when the controller cannot be improved further in this way, k nodes are added to the controller, where k is some small number greater than or equal to one. In the rest of this background section, we review each of these two steps in further detail.\nAlgorithm 1 Bounded policy iteration repeat\nrepeat {Policy evaluation} Solve the linear system given by Equation (1) {Policy improvement without adding nodes} for each node n of the controller do\nsolve the linear program in Table 1 if \u00b2 > 0 then\nupdate parameters and value vector of node end if\nend for until no improvement of controller {Policy improvement by adding nodes} find belief states reachable from tangent points create k new nodes that improve their value\nuntil no new node is created\nImproving nodes The first step attempts to improve a controller while keeping its size fixed. For each node n of the controller, the linear program in Table 1 is solved. The linear program searches for action probabilities and node transition probabilities for the node that improve the value vector Vn associated with the node by some amount \u00b2 for each state, where \u00b2 is the objective maximized by the linear program. If an improvement is found, the parameters of the node are updated accordingly. (The value vector may also be updated, and will be further improved during the policy evaluation step.)\nPoupart and Boutilier (2004) show that the linear program can be interpreted as follows: it implicitly considers the vectors of the backed-up value function that would be created by performing a dynamic programming update. In particular, the linear program searches for a convex combination of these backed-up vectors that pointwise dominates the value vector currently associated with the node. If an improvement is found, the parameters of the convex combination become the new parameters of the node. This interpretation is illustrated in Figure 1, which is adapted from a similar figure from Poupart\u2019s dissertation (2005). As Figure 1 shows, the new value vector is parallel to the old vector (i.e., the value of each component is improved by same amount) and it is tangent to the backed-up value function.\nAdding nodes Eventually, no node can be improved further by solving its corresponding linear program. At this point, BPI is at a local optimum. It can escape such a local optimum by adding one or more nodes to the controller.\nThe key insight is that when a local optimum is reached, the value vector of each node of the controller is tangent to the backed-up value function at one or more belief states. Moreover, the solution of the linear program that is the dual of the linear program in Table 1 is a belief state that is tan-\ngent to the backed-up value function, and so is called the tangent belief state. (Since most linear program solvers return both the primal and dual solutions when they solve a linear program, we assume that we get the tangent belief state when we solve the linear program in Table 1, in addition to the value of \u00b2 and the action and node transition probabilities.)\nTo escape a local optimum, it is necessary to improve the value of the tangent belief state. This leads to a method for adding nodes to a controller. Given a tangent belief state b, the algorithm considers every belief state b\u2032 that can be reached from it in one step (i.e., by taking some action a followed by some observation z). For each reachable belief state, a backup is performed (as defined by Equation 3). If the backed-up value is better than the value of the belief state based on the current value function, a deterministic node is added to the controller that has the same action, and, for each observation, the same successor node, that created the improved backed-up value. Often, it is only necessary to add a single node to the controller to escape a local optimum. But because a value vector may be tangent to the backed-up value function for a linear portion of belief space, it may be necessary to add more than one node to escape the local optimum. As we will see, this method for adding nodes to escape local optima is related to the approach that we develop in this paper.\nTwo sources of complexity Most of the computation time of BPI is spent in solving the linear programs that are used to adjust the parameters that specify the action selection probabilities and node transition probabilities of each node of a controller. The size of the linear programs, and thus the complexity of BPI, depends on two things: the size of the controller (which determines the number of variables in the linear program) and the size of the state space (which determines the number of constraints).\nIn this paper, we focus on the first source of complexity, that is, we focus on improving the complexity of BPI with respect to controller size. (By controller size, we mean not only the number of nodes of the controller, but also the number of actions and observations, since these together determine the number of parameters of a node.) Coping with POMDPs with large state spaces is an orthogonal research issue, and several approaches have been developed that can be combined with BPI. For example, Poupart and Boutilier (2005) describe a technique called value-directed compression and report that it allows BPI to solve POMDPs with millions of states. Because the test problems used in this paper have small state spaces, it is important to keep in mind that the techniques developed in the next section for improving the scalability of BPI with respect to controller size can be combined with techniques for coping with large state spaces."}, {"heading": "3 Sparse bounded policy iteration", "text": "In this section, we describe a modified version of BPI that we call Sparse BPI. In each iteration, it improves a FSC by the same amount as the original algorithm, but with much improved scalability. To motivate our approach, we begin with a discussion of the sparse structure of stochastic finitestate controllers found by BPI."}, {"heading": "3.1 Sparse stochastic finite-state controllers", "text": "As we have seen, each iteration of BPI solves |N | linear programs, and each linear program has |A| + |A||Z||N |\nvariables and |S| + |A||Z| constraints (in addition to the constraints that the variables have non-negative values). Even for small FSCs, the number of variables in the linear programs can be very large, and the fact that the number of variables grows with the number of nodes in the controller significantly limits the scalability of BPI.\nIf we look at the controllers produced by BPI, however, we find that most of the parameters of each node (i.e., most of the variables in the solutions of the linear program) have values of zero. Table 2 illustrates this vividly for four benchmark POMDPs from (Cassandra, 2004). The overwhelming majority of each node\u2019s parameters have zero probabilities. This is despite the fact that all of the nodes of these controllers are stochastic. Note that a deterministic node has 1 + |Z| non-zero probabilities, one for a choice of action, and |Z| to specify the successor node for each observation. Table 2 shows that the minimum number of non-zero parameters for any node is always greater than this, which indicates that all of the nodes are stochastic. For these problems and many others, BPI is very effective in leveraging the possibility of stochastic nodes to improve a controller without increasing its size. Nevertheless, the actual number of parameters with non-zero probabilities is a very small fraction of the total number of parameters.\nBesides the sparsity of the stochastic finite-state controllers found by BPI, an equally important observation about the results in Table 2 is that the number of parameters with non-zero probabilities tends to remain the same as the controller grows in size, whereas the total number of parameters grows significantly. In the following, we develop a modified version of BPI that exploits this sparse structure."}, {"heading": "3.2 Sparse policy improvement algorithm", "text": "We begin by describing the main idea of the algorithm. We have seen that the number of parameters of a node with non-zero probabilities in the solution of the linear program in Table 1 is very small relative to the total number of parameters. Let us call these the useful parameters. If we could somehow identify the useful parameters of a node, we could improve a node by solving a linear program that only includes these variables. We call a linear program that only includes some of the parameters of a node a reduced linear program. Our approach will be to solve a series of reduced linear programs, where the last one is guaranteed to include all of the useful parameters of the node. This approach will achieve the same result as solving the full linear program of Table 1, but will be more efficient if the reduced linear programs are much smaller.\nWe next describe an iterative method for identifying a small set of parameters that includes all of the useful parameters. Our method starts with the set of parameters of the node that currently have non-zero probabilities. This guarantees a solution that is at least as good as the current policy for the node. Then we add parameters using a technique that is inspired by the technique BPI uses to add nodes to a controller in order to escape local optima. As shown in Figure 1, the value vector that is created by solving the linear program in Table 1 is tangent to the backed-up value function, and the solution of the dual linear program is a tangent belief state. In the case of a reduced linear program, however, we have a partial backed-up value function that does not include vectors corresponding to parameters that are not included in the reduced linear program. (For example, if only one action parameter is included in the reduced linear program, the partial backed-up value function does not include any vectors created by taking a different action.)\nIf the reduced linear program is missing some useful parameters, it must be possible to improve the value of the tangent belief state b by considering the vectors of the full backed-up value function. So, to identify potentially useful parameters, we perform a backup for the tangent belief state b. If the backed-up value, which is defined as\nmax a\u2208A\n{ R(b, a) + \u03b2 \u2211\nz\u2208Z P (z|b, a)max n\u2208N\n(\u2211\ns\u2208S baz(s)Vn(s)\n)} ,\nis greater than the value of the tangent belief state based on the current value vector Vn\u2032 , which is defined as\u2211\ns\u2208S b(s)Vn\u2032(s), we add to our set of parameters the parameters used to create the backed-up value: the best action a, and, for each observation z, the best successor node n. Then we solve another linear program that includes these parameters in addition to the parameters contained in the previous reduced linear program.\nIf the value vector is tangent to the partial backed-up value function at a single belief state, adding the parameters that improve the backed-up value of this tangent belief state improves the value vector for this node. But since the value vector may be tangent to the partial backed-up value function along a linear segment, adding parameters does not guarantee improvement of the node. In this case, however, it does change the result of the new reduced linear program in an important way. Because adding these parameters to the reduced linear program improves the partial backed-up value function that is searched by the linear program, the value vector is no longer tangent to the partial backed-up value function at the same belief state. In other words, even if the solution of the reduced linear program is not changed by adding these parameters, the solution of its dual linear program changes in one important way: there must be a new tangent belief state. This is illustrated in Figure 2.\nThis points to an iterative algorithm for improving a node. At each step, the algorithm solves a reduced linear program, and then performs a backup for the tangent belief state. If the backed-up value of the tangent belief state is better than its value based on the current value vector, the parameters that produced the improved backed-up value are added to the reduced linear program. Since the backedup value of the tangent belief state is improved, it must be the case that at least some of these parameters are not in the current linear program; if they were, the current value vector would not be tangent to the partial backed-up value function at this belief state. The condition for terminating this iterative process of adding parameters to a reduced linear program is based on the following lemma.\nLemma 1 The difference between the backed-up value of a tangent belief state and its value based on the current value vector bounds the amount by which the linear program in Table 1 can improve the value vector of a node.\nProof: The linear program in Table 1 implicitly uses the backed-up value function to improve the value vector associated with a node by an amount \u00b2 for each state value of the vector; in turn, this improves by \u00b2 the value of any belief state based on the value vector. So, if the difference between the backed-up value of any belief state and its value based on the current value vector is \u03b4, then \u00b2 \u2264 \u03b4.\u00a4 It follows that if the backed-up value of any belief state is not an improvement of its value based on the current value vector, the linear program cannot improve the value vector.\nBased on this lemma, we can prove the following.\nTheorem 1 This procedure of improving a stochastic node by solving a sequence of reduced linear programs is guaranteed to terminate and the last reduced linear program in the sequence produces the same result as solving the full linear program in Table 1.\nProof: The procedure is guaranteed to terminate because whenever the backed-up value of the tangent belief state is greater than its value based on the value vector created by solving the current reduced linear program, at least one (and no more than (1 + |Z|) parameters will be added to the linear program, and the total number of parameters is finite.\nTo see that the last reduced linear program solved in this sequence has the same result as solving the full linear program in Table 1, note that the procedure terminates when the difference between the backed-up value of the tangent belief state (for the last reduced linear program) and the value of the tangent belief state based on the best value vector found so far for this node is zero. From Lemma 1, it follows that no further improvement is possible.\u00a4 We call this iterative approach to improving the nodes of a stochastic FSC sparse policy improvement. The high-level\nAlgorithm 2 Sparse policy improvement for each node of controller do\nCreate initial reduced linear program {its variables correspond to the parameters of the node that currently have non-zero probabilities} threshold \u2190 0 repeat\nSolve the reduced linear program if \u00b2 > threshold then\nUse solution of LP to update parameters of node threshold \u2190 \u00b2\nend if Do backup for tangent belief state if backed-up value > (current value + \u00b2) then\nAdd variables to linear program that correspond to parameters that produced backed-up value\nend if until no new variables are added to linear program increase value vector of node by \u00b2\nend for\npseudocode is shown in Algorithm 2. Although it can solve multiple linear programs for each linear program solved by the original BPI algorithm, it has an advantage if the number of variables in each of these reduced linear programs is very small compared to the number of variables in the full linear program of Table 1. This is the case whenever the FSCs found by BPI are very sparse.\nAs mentioned before, this algorithm is inspired by the way the original BPI algorithm adds nodes to a controller. In both cases, the technique for breaking a local optimum at a tangent belief state is to improve the backed-up value function of the tangent belief state. In the original BPI algorithm, the value of the tangent belief state is improved by adding nodes to the controller. In sparse policy improvement, it is improved by adding parameters to a node.\nThere is also an interesting relationship between this algorithm and column generation methods in linear programming (Bertsekas & Tsitsiklis, 1997). Column generation is a useful strategy for solving linear programs where the number of variables is very large, the number of constraints is relatively small, and most variables have a value of zero in the optimal solution. It begins by considering a small subset of the variables (i.e., the columns) of a problem, and solves a reduced linear program with only these variables. Then it identifies unused variables that can be added to the linear program to improve the result. Use of this strategy requires some way of determining if the current solution is optimal, and if it is not, some way of generating one or more unused variables that can improve the solution. Our algorithm can be viewed as an application of this general strategy to the problem of solving POMDPs using BPI."}, {"heading": "4 Experimental results", "text": "We implemented sparse bounded policy iteration and tested it on several benchmark POMDPs. Table 3 shows results for the four POMDPs considered earlier in Table 2. The experiments ran on a 3.0 GHz processor using a linear program solver in CPLEX version 9.0. We solved each POMDP beginning with an initial controller with |A| nodes, and adding up to five new nodes each time the policy improvement step reached a local optimum. Table 3 reports the average running time to improve a single node because this dominates overall computation time, and also because it measures performance independently of design decisions such as how and when to add nodes to a controller.\nTable 3 shows that Sparse BPI is much more efficient than the original BPI algorithm in improving sparse stochastic FSCs. Even for relatively small controllers of 300 nodes, Sparse BPI runs between 20 and 80 times faster than BPI in solving these particular POMDPs. More importantly, its relative advantage grows with the size of the controller. The size of the reduced linear programs solved by Sparse BPI remains about the same as the controller grows in size; as a result, the time it takes to improve a single node of the controller remains relatively constant as the size of the controller grows, in contrast to BPI.\nAn interesting difference between Sparse BPI and BPI is that the running time of an iteration of Sparse BPI depends on how much improvement of the controller is possible. If the nodes of a controller are already at a local optimum, Sparse BPI often needs to solve only a couple reduced linear programs per node in order to determine that further improvement is not possible. In this case, an iteration of Sparse BPI terminates relatively quickly. But if much improvement of the FSC is possible, Sparse BPI often needs to solve ten or twenty or even more reduced linear programs for some nodes in order to add all of the parameters that are needed to maximize improvement of the node. To obtain reliable running times for Sparse-BPI, the results in Table 3 are averaged over several iterations of Sparse-BPI.\nTable 3 shows that a larger state space slows both algorithms, but it slows the sparse algorithm more. This is because the sparse algorithm solves multiple linear programs for each linear program solved by the original algorithm, and more constraints makes these more difficult to solve.\nThe running time of Sparse BPI could be reduced further by finding a way to reduce the number of iterations it takes to improve a node. For the hallway2 problem, for example, the sparse algorithm can take ten or more iterations to improve a node \u2013 occasionally, as many as thirty or forty. Each iteration requires solving a reduced linear program. But in fact, it is only necessary to solve one linear program to change the parameters of a node. The other linear programs in the sequence of reduced linear programs solved by the sparse algorithm are used to identify a sequence of tangent belief states; for each tangent belief state, a backup is performed in order to identify parameters to add to the stochastic node. It seems possible to identify a set of belief states for which to perform backups without needing to solve a linear program to identify each one.\nPoint-based methods solve POMDPs by performing a backup for each of a finite set of belief states (Pineau, Gordon, & Thrun, 2003). Recent work shows that this approach can be combined with policy iteration algorithms that represent a policy as a finite-state controller (Ji, Parr, Li, Liao, & Carin, 2007). An interesting direction of research is to use point-based backups to identify parameters that can be added to nodes; after many backups are performed, a reduced linear program could be solved for each node in order to improve its action selection and node transition probabilities \u2013 integrating point-based methods with policy iteration for stochastic finite-state controllers.\nAnother possible way to reduce the number of iterations is early termination. Instead of continuing to add parameters until the difference between the backed-up value of the tangent belief state and its value based on the current value vector is zero, Sparse-BPI could stop adding parameters as soon as the difference is small enough to demonstrate that only a small amount of further improvement is possible."}, {"heading": "5 Conclusion", "text": "We have presented a modified bounded policy iteration algorithm for POMDPs called sparse bounded policy iteration. The new algorithm exploits the sparse structure of stochastic finite-state controllers found by bounded policy iteration. Each iteration of the algorithm produces the identical improvement of a controller that an iteration of the original bounded policy iteration algorithm produces, but with improved scalability. Whereas the time it takes for the original algorithm to improve a single node grows with the size of the controller, the time it takes for the new algorithm to improve a single node is typically independent of the size of the controller. This makes it practical to use bounded policy iteration to find larger controllers for POMDPs."}, {"heading": "Acknowledgements", "text": "This work was supported in part by the Air Force Office of Scientific Research under Award No. 05-003202A05."}], "references": [{"title": "Solving POMDPs using quadratically constrained linear programs", "author": ["C. Amato", "D. Bernstein", "S. Zilberstein"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence", "citeRegEx": "Amato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2007}, {"title": "Infinite-horizon policygradient estimation", "author": ["J. Baxter", "P. Bartlett"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Baxter and Bartlett,? \\Q2001\\E", "shortCiteRegEx": "Baxter and Bartlett", "year": 2001}, {"title": "Bounded policy iteration for decentralized POMDPs", "author": ["D. Bernstein", "E. Hansen", "S. Zilberstein"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence(IJCAI-05),", "citeRegEx": "Bernstein et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2005}, {"title": "Introduction to Linear Optimization", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific", "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1997\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1997}, {"title": "Tony\u2019s POMDP file repository page.. http://pomdp.org/pomdp/examples/index.shtml", "author": ["A. Cassandra"], "venue": null, "citeRegEx": "Cassandra,? \\Q2004\\E", "shortCiteRegEx": "Cassandra", "year": 2004}, {"title": "Approximate planning for factored POMDPs", "author": ["Z. Feng", "E. Hansen"], "venue": "In Proceedings of the 6th European Conference on Planning", "citeRegEx": "Feng and Hansen,? \\Q2001\\E", "shortCiteRegEx": "Feng and Hansen", "year": 2001}, {"title": "Solving POMDPs by searching in policy space", "author": ["E. Hansen"], "venue": "In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hansen,? \\Q1998\\E", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Point-based policy iteration", "author": ["S. Ji", "R. Parr", "H. Li", "X. Liao", "L. Carin"], "venue": "In Proceedings of the 22nd National Conference on Artificial Intelligence", "citeRegEx": "Ji et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2007}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Solving POMDPs by searching the space of finite policies", "author": ["N. Meuleau", "K. Kim", "L. Kaelbling", "A. Cassandra"], "venue": "In Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "Learning finite-state controllers for partially observable environments", "author": ["N. Meuleau", "L. Peshkin", "K. Kim", "L. Kaelbling"], "venue": "In Proceedings ofthe 15th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "A feasible computational approach to infinite-horizon partially-observed Markov decision problems", "author": ["L. Platzman"], "venue": "Tech. rep., Georgia Institute of Technology. Reprinted in Working Notes of the AAAI Fall Symposium on Planning using Partially Observable", "citeRegEx": "Platzman,? \\Q1981\\E", "shortCiteRegEx": "Platzman", "year": 1981}, {"title": "Exploiting Structure to Efficiently Solve Large Scale Partially Observable Markov Decision Processes", "author": ["P. Poupart"], "venue": "Ph.D. thesis,", "citeRegEx": "Poupart,? \\Q2005\\E", "shortCiteRegEx": "Poupart", "year": 2005}, {"title": "Bounded finite state controllers", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference,", "citeRegEx": "Poupart and Boutilier,? \\Q2004\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2004}, {"title": "VDCBPI: An approximate scalable algorithm for large POMDPs", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Poupart and Boutilier,? \\Q2005\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2005}], "referenceMentions": [{"referenceID": 6, "context": "In some cases, this is a deterministic controller in which a single action is associated with each node, and an observation results in a deterministic transition to a successor node (Kaelbling, Littman, & Cassandra, 1998; Hansen, 1998; Meuleau, Kim, Kaelbling, & Cassandra, 1999a).", "startOffset": 182, "endOffset": 280}, {"referenceID": 12, "context": "In other cases, it is a stochastic controller in which actions are selected based on a probability distribution associated with each node, and an observation results in a probabilistic transition to a successor node (Platzman, 1981; Meuleau, Peshkin, Kim, & Kaelbling, 1999b; Baxter & Bartlett, 2001; Poupart & Boutilier, 2004; Amato, Bernstein, & Zilberstein, 2007).", "startOffset": 216, "endOffset": 366}, {"referenceID": 6, "context": "BPI is related to an exact policy iteration algorithm for POMDPs due to Hansen (1998), but differs from it by providing an elegant and effective approach to approximation in which bounding the size of the controller allows a tradeoff between planning time and plan quality.", "startOffset": 72, "endOffset": 86}, {"referenceID": 6, "context": "Hansen (1998) proposed a policy iteration algorithm for POMDPs that represents a policy as a deterministic finite-state controller.", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "Feng and Hansen (2001) describe a method for performing approximate dynamic programming updates that has the effect of reducing the number of vectors added to a value function in value iteration, or the number of nodes added to a controller in policy iteration.", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "For policy iteration, Poupart and Boutilier (2004) propose another approach that also has the benefit of avoiding an expensive dynamic programming update.", "startOffset": 22, "endOffset": 51}, {"referenceID": 4, "context": ") The four test POMDPs are from (Cassandra, 2004).", "startOffset": 32, "endOffset": 49}, {"referenceID": 13, "context": "For example, Poupart and Boutilier (2005) describe a technique called value-directed compression and report that it allows BPI to solve POMDPs with millions of states.", "startOffset": 13, "endOffset": 42}, {"referenceID": 4, "context": "Table 2 illustrates this vividly for four benchmark POMDPs from (Cassandra, 2004).", "startOffset": 64, "endOffset": 81}], "year": 2008, "abstractText": "Bounded policy iteration is an approach to solving infinite-horizon POMDPs that represents policies as stochastic finite-state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming. In the original algorithm, the size of the linear programs, and thus the complexity of policy improvement, depends on the number of parameters of each node, which grows with the size of the controller. But in practice, the number of parameters of a node with non-zero values is often very small, and does not grow with the size of the controller. Based on this observation, we develop a version of bounded policy iteration that leverages the sparse structure of a stochastic finite-state controller. In each iteration, it improves a policy by the same amount as the original algorithm, but with much better scalability.", "creator": " TeX output 2008.05.15:0148"}}}