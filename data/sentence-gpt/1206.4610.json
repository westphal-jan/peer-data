{"id": "1206.4610", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Manifold Relevance Determination", "abstract": "In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear(in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a \"softly\" shared latent space that can be \"expressed\" to obtain a uniform fit for the individual views of the data.\n\n\n\n\n\nA Bayesian latent model allows for a linear linear(n) model to be fully quantified by the partitioning, averaging, and minimizing the \"uniform\" distribution. This model provides a highly simplified model of the residuals of multiple views. By reducing the variance in the partitioning and minimizing the \"uniform\" distribution we are able to obtain the \"distribution\" for each view of the data. For this reason, we apply the covariation model to a Bayesian linear model using Bayesian covariation and apply the latent space to gain general uniformity.\n\nIn the final section of this paper we will also discuss the posterior-dimensional distribution in the posterior-dimensional model by identifying an important factor and then applying the latent space to gain general uniformity to gain general uniformity.\nGeneral covariation\nIn the previous section, we present a Bayesian latent model with a function for each view of the data. In our previous post, we used Bayesian covariation models for multiple view-specific views of the data. In our previous post, we have shown that Bayesian covariation models can be expressed in a Bayesian model. We have shown that Bayesian covariation models can be expressed in a Bayesian model for both views and the one view, as well as an optimal Bayesian model for each view of the data. As part of our model evaluation, we also use Bayesian covariation models to obtain an optimal Bayesian model for each view of the data. These models can be expressed as follows:\nFor every view of the data, the Bayesian covariation model uses Bayesian covariation models to obtain an optimal Bayesian model for each view of the data. For each view of the data, the Bayesian covariation model uses Bayesian covariation models to obtain an optimal Bayesian model for each view of the data. In addition to the covariation model, a Bayesian model for all views of the data is also used to obtain a optimal Bayesian model for each view of the data", "histories": [["v1", "Mon, 18 Jun 2012 14:45:37 GMT  (2585kb)", "http://arxiv.org/abs/1206.4610v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.ML", "authors": ["andreas c damianou", "carl henrik ek", "michalis k titsias", "neil d lawrence"], "accepted": true, "id": "1206.4610"}, "pdf": {"name": "1206.4610.pdf", "metadata": {"source": "META", "title": "Manifold Relevance Determination", "authors": ["Andreas C. Damianou", "Carl Henrik Ek", "Michalis K. Titsias", "Neil D. Lawrence"], "emails": ["ANDREAS.DAMIANOU@SHEFFIELD.AC.UK", "CHEK@CSC.KTH.SE", "MTITSIAS@WELL.OX.AC.UK", "N.LAWRENCE@SHEFFIELD.AC.UK"], "sections": [{"heading": "1. Introduction", "text": "Multiview learning is characterised by data which contain observations from several different modalities: for example depth cameras provide colour and depth images from the same scene, or a meeting might be represented by both\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nan audio and a video feed. This motivates latent variable models which align the different views by assuming that a portion of the data variance is shared between the modalities, whilst explaining the remaining variance with latent spaces that are private to each modality. This model structure allows inference when only a subset of the modalities is available and, because the observation spaces have been aligned, it is possible to transfer information between modalities by conditioning the model through the underlying concept.\nSeveral approaches that combine multiple views have been suggested. One line of work aims to find a low-dimensional representation of the observations by seeking a transformation of each view. Different approaches exploit different characteristics of the data such as, correlation (Kuss & Graepel, 2003; Ham et al., 2005), or mutual information (Memisevic et al., 2011). However, these methods only aim to encode the shared variance and do not provide a probabilistic model. To address these shortcomings different generative models have been suggested. In particular, approaches formulated as Gaussian Processes Latent Variable Models (GP-LVMs) (Lawrence, 2005) have been especially successful (Shon et al., 2006; Ek et al., 2007). However, these models assume that a single latent variable is capable of representing each modality, implying that the modalities can be fully aligned. To overcome this, the idea of a factorized latent space was presented in (Ek et al., 2008) where each view is associated with an additional private space, representing the variance which cannot be aligned, in addition to the shared space (Ek, 2009), an idea independently suggested by Klami & Kaski (2006). The main challenge for the applicability of the proposed models is that the factorization of the latent variable is a structural and essentially discrete property of the model, making it very challenging to learn. Salzmann et al. (2010) intro-\nduced a set of regularizers allowing the dimensionality of the factorization to be learned. However, the regularizers were motivated out of necessity rather than principle and introduced several additional parameters to the model.\nWe present a new principled approach to learning a factorized latent variable representation of multiple observation spaces. We introduce a relaxation of the structural factorization of the model from the original hard discrete representation, where each latent variable is either associated with a private space or a shared space, to a smooth continuous representation, where a latent variable may be more important to the shared space than the private space. In contrast to previous approaches the model is fully Bayesian, allowing estimation of both the dimensionality and the structure of the latent representation to be done automatically. Further, it provides an approximation to the full posterior of the latent points given the data.We describe the model and the variational approximation in the next section. The model is capable of handling extremely high dimensional data. We illustrate this by modelling image data directly in the pixel space in section 3. We also demonstrate the model\u2019s ability to reconstruct pose from silhouette in a human motion example and, finally, by considering class labels to be a second \u2018view\u2019 of a dataset we show how the model can be used to improve classification performance in a well known visualization benchmark: the \u201coil data\u201d."}, {"heading": "2. The Model", "text": "We wish to relate two views Y \u2208 RN\u00d7DY and Z \u2208 RN\u00d7DZ of a dataset within the same model. We assume the existence of a single latent variable X \u2208 RN\u00d7Q which, through the mappings {fYd } DY d=1 : X 7\u2192 Y and {fZd } DZ d=1 : X 7\u2192 Z (Q < D), gives a low dimensional representation of the data. Our assumption is that the data is generated from a low dimensional manifold and corrupted by additive Gaussian noise {Y,Z} \u223c N (0, \u03c3{Y,Z} I),\nynd = f Y d (xn) + Y nd znd = f Z d (xn) + Z nd, (1)\nwhere {y, z}nd represents dimension d of point n. This leads to the likelihood under the model, P (Y,Z|X,\u03b8), where \u03b8 = {\u03b8Y ,\u03b8Z} collectively denotes the parameters of the mapping functions and the noise variances \u03c3{Y,Z} . Finding the latent representation X and the mappings fY and fZ is an ill-constrained problem. Lawrence (2005) suggested regularizing the problem by placing Gaussian process (GP) (Rasmussen & Williams, 2006) priors over the mappings and the resulting models are known as Gaussian Process latent variable models (GP-LVMs).\nIn the GP-LVM framework each generative mapping is modeled as a product of independent GP\u2019s parametrized by\na (typically shared) covariance function k{Y,Z} evaluated over the latent variable X , so that\np(FY |X,\u03b8Y ) = DY\u220f d=1 N (fYd |0,KY ), (2)\nwhere FY = {fYd } DY d=1 with f Y nd = f Y d (xn), and similarly for FZ . This allows for general non-linear mappings to be marginalised out analytically leading to a likelihood as a product of Gaussian densities,\nP (Y,Z|X,\u03b8) = \u220f K={Y,Z} \u222b p(K|FK)p(FK|X,\u03b8K)dFK. (3)\nA fully Bayesian treatment requires integration over the latent variable X in equation (3) which is intractable, as X appears non-linearly in the inverse of the covariance matrices KY and KZ of the GP priors for fY and fZ . In practice, a maximum a posteriori solution (Shon et al., 2006; Ek et al., 2007; Salzmann et al., 2010) was often used. However, failure to marginalize out the latent variables means that it is not possible to automatically estimate the dimensionality of the latent space or the parameters of any prior distributions used in the latent space. We show how we can obtain an approximate Bayesian training and inference procedure by variationally marginalizing out X . We achieve this by building on recent variational approximations for standard GP-LVMs (Titsias & Lawrence, 2010; Damianou et al., 2011). We then introduce automatic relevance determination (ARD) priors (Rasmussen & Williams, 2006) so that each view of the data is allowed to estimate a separate vector of ARD parameters. This allows the views to determine which of the emerging private and shared latent spaces are relevant to them. We refer to this idea as manifold relevance determination (MRD)."}, {"heading": "2.1. Manifold Relevance Determination", "text": "We wish to recover a factorized latent representation such that the variance shared between different observation spaces can be aligned and separated from variance that is specific (private) to the separate views. In manifold relevance determination the notion of a hard separation between private and shared spaces is relaxed to a continuous setting. The model is allowed to (and indeed often does) completely allocate a latent dimension to private or shared spaces, but may also choose to endow a shared latent dimension with more or less relevance for a particular dataview. Importantly, this factorization is learned from data by maximizing a variational lower bound on the model evidence, rather than through construction of bespoke regularizers to achieve the same effect. The model we propose can be seen as a generalisation of the traditional approach to manifold learning; we still assume the existence of a low-dimensional representation encoding the underlying phenomenon, but the variance contained in an observation space does not necessarily need to be governed by the\nfull manifold, as traditionally assumed, nor by a subspace geometrically orthogonal to that, as assumed in Salzmann et al. (2010).\nThe expressive power of our model comes from the ability to consider non-linear mappings within a Bayesian framework. Specifically, ourDY latent functions fYd are selected to be independent draws of a zero-mean GP with an ARD covariance function of the form:\nkY (xi,xj) = (\u03c3 Y ard) 2e\u2212 1 2\n\u2211Q q=1 w Y q (xi,q\u2212xj,q) 2\n, (4)\nand similarly for fZ . Accordingly, we can learn a common latent space1 but we allow the two sets of ARD weights wY = {wYq } Q q=1 and w Z = {wZq } Q q=1 to automatically infer the responsibility of each latent dimension for generating points in the Y and Z spaces respectively. We can then automatically recover a segmentation of the latent space X = ( XY , Xs, XZ ) , where Xs \u2208 RN\u00d7Qs is the shared subspace, defined by the set of dimensions q \u2208 [1, ..., Q] for which wYq , w Z q > \u03b4, with \u03b4 being a number close to zero and Qs \u2264 Q. This equips the model with further flexibility, because it allows for a \u201csoftly\u201d shared latent space, if the two sets of weights are both greater than \u03b4 but dissimilar, in general. As for the two private spaces, XY and XZ , they are also being inferred automatically along with their dimensionalities QY and QZ 2. More precisely:\nXY = {xq}QYq=1 : xq \u2208 X, wYq > \u03b4, wZq < \u03b4 (5)\nand analogously for XZ . Here, xq denotes columns of X , while we assume that data are stored by rows. All of the above are summarised in the graphical model of figure 1."}, {"heading": "2.2. Bayesian training", "text": "The fully Bayesian training procedure requires maximisation of the logarithm of the joint marginal likelihood\n1As we will see in the next section, we actually learn a common distribution of latent points.\n2In general, there will also be dimensions of the initial latent space which are considered unnecessary by both sets of weights.\np(Y,Z|\u03b8) = \u222b p(Y,Z|X,\u03b8)p(X)dX , where a prior distribution is placed onX . This prior may be a standard normal distribution or may generally depend on a set of parameters \u03b8X . By looking again at (3) we see that the above integral is intractable due to the nonlinear way in which X appears in p(F {Y,Z}|X,\u03b8{Y,Z}). Standard variational approximations are also intractable in this situation. Here, we describe a non-standard method which leads to an analytic solution.\nAs a starting point, we consider the mean field methodology and seek to maximise a variational lower bound Fv(q,\u03b8) on the logarithm of the true marginal likelihood by relying on a variational distribution which factorises as q(\u0398)q(X), where we assume that q(X) \u223c N (\u00b5, S). As will be explained later more clearly, in our approach q(\u0398) is a distribution which depends on additional variational parameters \u0398 = {\u0398Y ,\u0398Z} so that q(\u0398) = q(\u0398Y )q(\u0398Z). These additional parameters \u0398 as well as the exact form of q(\u0398) will be defined later on, as they constitute the most crucial ingredient of our non-standard variational approach.\nBy dropping the model hyperparameters \u03b8 from our expressions, for simplicity, we can use Jensen\u2019s inequality and obtain a variational bound Fv(q) \u2264 log p(Y,Z):\nFv(q) =\n\u222b q(\u0398)q(X) log ( p(Y |X)p(Z|X)\nq(\u0398)\np(X)\nq(X)\n) dX\n= LY + LZ \u2212 KL [q(X) \u2016 p(X)] , (6)\nwhere LY = \u222b q(\u0398Y )q(X) log p(Y |X)\nq(\u0398Y ) dX and similarly\nfor LZ . However, this does not solve the problem of intractability since the challenging terms still appear in LY and LZ . To circumvent this problem, we follow Titsias & Lawrence (2010) and apply the \u201cdata augmentation\u201d principle, i.e. we expand the joint probability space with M extra samples UY and UZ of the latent functions fY and fZ evaluated at a set of pseudo-inputs (known as \u201cinducing points\u201d) X\u0304Y and X\u0304Z respectively. Here, UY \u2208 RMY \u00d7DY , UZ \u2208 RMZ\u00d7DZ , X\u0304Y \u2208 RMY \u00d7Q, X\u0304Z \u2208 RMZ\u00d7Q and M = MY +MZ . The expression of the joint probability is\nas before except for the term p(Y |X) which now becomes:\np(Y |X, X\u0304Y ) = \u222b p(Y |FY )p(FY |UY , X, X\u0304Y )\u00b7\np(UY |X\u0304Y )dFY dUY (7)\nand similarly for p(Z|X). The integrations over U{Y,Z} are tractable if we assume Gaussian prior distributions for these variables. As we shall see, the inducing points are variational rather than model parameters. More details on the variational learning of inducing variables in GPs can be found in Titsias (2009).\nAnalogously to Titsias & Lawrence (2010), we are now able to define q(\u0398) = q(\u0398Y )q(\u0398Z) as\nq(\u0398) = \u220f\nK={Y,Z}\nq(UK)p(FK|UK, X, X\u0304K), (8)\nwhere q(U{Y,Z}) are free form distributions. In that way, the p(FK|UK, X, X\u0304K) factors cancel out with the \u201cdifficult\u201d terms of LY and LZ , as can be seen by replacing equations (8) and (7) back to (6), which now becomes our final objective function and can be trivially extended for more than two observed datasets. This function is jointly maximised with respect to the model parameters, involving the latent space weights wY and wZ , and the variational parameters {\u00b5, S, X\u0304}. As in standard variational inference, this optimisation gives, as a by-product, an approximation of p(X|Y,Z) by q(X), i.e. we obtain a distribution over the latent space. This adds extra robustness to our model, since previous approaches rely on MAP estimates for the latent points. More detailed derivation of the variational bound can be found in the suppl. material.\nDynamical Modelling: The model formulation described previously is also covering the case when we wish to additionally model correlations between datapoints of the same output space, e.g. when Y and Z are multivariate timeseries. For the dynamical scenario we follow Damianou et al. (2011); Lawrence & Moore (2007) and choose the prior on the latent space to depend on the observation times t \u2208 RN , e.g. a GP with a covariance function k = k(t, t\u2032). With this approach, we are also allowed to learn the structure of multiple independent sequences which share some commonality by learning a common latent space for all timeseries while, at the same time, ignoring correlations between datapoints belonging to different sequences.\nInference: Given a model which is trained to jointly represent two output spaces Y and Z with a common but factorised input space X , we wish to generate a new (or infer a training) set of outputs Z\u2217 \u2208 RN\u2217\u00d7DZ given a set of (potentially partially) observed test points Y \u2217 \u2208 RN\u2217\u00d7DY . This is done in three steps. Firstly, we predict the set of latent points X\u2217 \u2208 RN\u2217\u00d7Q which is most likely to have\ngenerated Y \u2217. For this, we use an approximation to the posterior p(X\u2217|Y \u2217, Y ), which has the same form as for the standard Bayesian GP-LVM model (Titsias & Lawrence, 2010) and is given by a variational distribution q(X,X\u2217). To find q(X,X\u2217) we optimise a variational lower bound on the marginal likelihood p(Y, Y \u2217) which has analogous form with the training objective function (6). Specifically, we ignore Z and replace Y with (Y, Y \u2217) and X with (X,X\u2217) in (6). In the second step, we find the training latent points XNN which are closest to X\u2217 in the shared latent space. In the third step, we find outputs Z from the likelihood p(Z|XNN ). This procedure returns the set of training points Z which best match the observed test points Y \u2217. If we wish to generate novel outputs, we have to propagate the information recovered when predicting X\u2217. Since the shared latent space encodes the same kind of information for both datasets, we can achieve the above by simply replacing the features of XNN corresponding to the shared latent space, with those of X\u2217.\nComplexity: As in common sparse methods in Gaussian processes (Titsias, 2009), the typical cubic complexity reduces to O(NM2), where N and M is the total number of training and inducing points respectively. In our experiments we set M = 100. Further, the model scales only linearly with the data dimensionality. Indeed, the Gaussian densities in equation (6) result in an objective function which only involves the data matrices Y and Z in expressions of the form Y Y > and ZZ> which are N \u00d7N matrices no matter how many features DY and DZ are used to describe the original data. Also, these quantities are constant and can be precomputed. Consequently, our approach can model datasets with very large numbers of features."}, {"heading": "3. Experiments", "text": "The MRD method is designed to represent multiple views of a data set as a set of factorized latent spaces. In this section we will show experiments which exploit this factorized structure. Source code for recreating these experiments is included as supplementary material.\nYale faces: To show the ability of our method to model very high-dimensional spaces our first experiment is applied to the Yale dataset (Georghiades et al., 2001; Lee et al., 2005) which contains images of several human faces under different poses and 64 illumination conditions. We consider a single pose for each subject such that the only variations are the location of the light source and the subject\u2019s appearance. Since our model is capable of working with very high-dimensional data, it can be directly applied to the raw pixel values (in this case 192 \u00d7 168 = 32, 256 pixels/image) so that we do not have to rely on image feature extraction to pre-process the data, and we can directly sample novel outputs. From the full Yale database, we con-\nstructed a dataset Y containing the pictures corresponding to all 64 different illumination conditions for each one of 3 subjects and similarly for Z, for 3 different subjects. In this way, we formed two datasets, Y and Z, each consisting of all 64 \u00d7 3 images corresponding to a set of three different faces, under all possible illumination conditions, therefore, Y,Z \u2208 RN\u00d7D, N = 192, D = 32, 256. We then aligned the order of the images in each dataset, so that each image yn from the first one was randomly set to correspond to one of the 3 possible zn\u2019s of the second dataset which are depicted in the same illumination condition as yn. In that way, we matched datapoints between the two datasets only according to the illumination condition and not the identity of the faces, so that the model is not explicitly forced to learn the correspondence between face characteristics.\nThe latent space variational means were initialised by concatenating the two datasets and performing PCA. An alternative approach would be to perform PCA on each dataset separately and then concatenate the two low dimensional representations to initialise X . We found that both initializations achieved similar results. The optimized relevance weights {wY ,wZ} are visualized as bar graphs in figure 2.\nThe latent space is clearly segmented into a shared part, consisting of dimensions indexed as 1,2 and 3 3 two private and an irrelevant part (dimension 9). The two data views allocated approximately equal weights to the shared latent dimensions, which are visualized in figures 3(a) and 3(b). Interaction with these three latent dimensions reveals that the structure of the shared subspace resembles a hollow hemisphere. This corresponds to the shape of the space defined by the fixed locations of the light source.\nThis indicates that the shared space successfully encodes the information about the position of the light source and not the face characteristics. This indication is enhanced by the results found when we performed dimensionality reduction with the standard Bayesian GP-LVM for pictures corresponding to all illumination conditions of a single face\n3Dimension 6 also encodes shared information, but of almost negligible amount (wY6 and wZ6 are almost zero).\n(i.e. a dataset with one modality). Specifically, the latent space discovered by the Bayesian GP-LVM and the shared subspace discovered by MRD have the same dimensionality and similar structure, as can be seen in figure 4.\nAs for the private manifolds discovered by MRD, these correspond to subspaces for disambiguating between faces of the same dataset. Indeed, plotting the largest two dimensions of the first latent private subspace against each other reveals three clusters, corresponding to the three different faces within the dataset. Similarly to the standard Bayesian GP-LVM applied to a single face, here the private dimensions with very small weight model slight changes across faces of the same subject (blinking etc).\nWe can also confirm visually the subspaces\u2019 properties by sampling a set of novel inputs Xsamp from each subspace and then mapping back to the observed data space using the likelihoods p(Y |Xsamp) or p(Z|Xsamp), thus obtaining novel outputs (images). To better understand what kind of information is encoded in each of the dimensions of the shared or private spaces, we sampled new latent points by varying only one dimension at a time, while keeping the rest fixed. The first two rows of figure 5 show some of the outputs obtained after sampling across each of the shared dimensions 1 and 3 respectively, which clearly encode the coordinates of the light source, whereas dimension 2 was\nfound to model the overall brightness. The sampling procedure can intuitively be thought as a walk in the space shown in figure 3(b) from left to right and from the bottom to the top. Although the set of learned latent inputs is discrete, the corresponding latent subspace is continuous, and we can interpolate images in new illumination conditions by sampling from areas where there are no training inputs (i.e. in between the red crosses shown in figure 3).\nSimilarly, we can sample from the private subspaces and obtain novel outputs which interpolate the non-shared characteristics of the involved data. This results in a morphing effect across different faces, which is shown in the last row of figure 5. Example videos can be found in the supplementary material.\nAs a final test, we confirm the efficient segmentation of the latent space into private and shared parts by automatically recovering all the illumination similarities found in the training set. More specifically, given a datapoint yn from the first dataset, we search the whole space of training inputs X to find the 6 Nearest Neigbours to the latent representation xn of yn, based only on the shared dimensions. From these latent points, we can then obtain points in the output space of the second dataset, by using the likelihood p(Z|X). As can be seen in figure 6, the model returns images with matching illumination condition. Moreover, the fact that, typically, the first neighbours of each given point correspond to outputs belonging to different faces, indicates that the shared latent space is \u201cpure\u201d, and is not polluted by information that encodes the face appearance.\nHuman motion data: For our second experiment, we consider a set of 3D human poses and associated silhouettes, coming from the dataset of Agarwal and Triggs (Agarwal & Triggs, 2006). We used a subset of 5 sequences, totalling\n649 frames, corresponding to walking motions in various directions and patterns. A separate walking sequence of 158 frames was used as a test set. Each pose is represented by a 63\u2212dimensional vector of joint locations and each silhouette is represented by a 100\u2212dimensional vector of HoG (histogram of oriented gradients) features.\nGiven the test silhouette features, we used our model to generate the corresponding poses. This is challenging, as the data are multi-modal, i.e. a silhouette representation may be generated from more than one poses (e.g. fig. 7).\nAs described in the inference section, given y\u2217, one of the N\u2217 test silhouettes, our model optimises a test latent point x\u2217 and finds a series of K candidate initial training inputs {x(k)NN}Kk=1, sorted according to their similarity to x\u2217, taking into account only the shared dimensions. Based on these initial latent points, it then generates a sorted series of K novel poses {z(k)}Kk=1. For the dynamical version of our model, all test points are considered together and the predicted N\u2217 outputs are forced to form a smooth sequence. Our experiments show that the initial training inputs xNN typically correspond to silhouettes similar to the given one, something which confirms that the segmentation of the latent space is efficient. However, when ambiguities arise, as the example shown in figure 7, the non-dynamical version of our model has no way of selecting the correct input, since all points of the test sequence are treated independently. But when the dynamical version is employed, the model forces the whole set of training and test inputs to create smooth paths in the latent space. In other words, the dynamics disambiguate the model.\nIndeed, as can be seen in figure 8, our method is forced to select a candidate training input xNN for initialisation which does not necessarily correspond to the training silhouette that is most similar to the test one. What is more, if we assume that the test pose z\u2217 is known and seek for its nearest training neighbour in the pose space, we find that the corresponding silhouette is very similar to the one\nfound by our model, which is only given information in the silhouette space.\nGiven the above, we quantify the results and compare our method with linear and Gaussian process regression and Nearest Neighbour in the silhouette space. We also compared against the shared GP-LVM (Ek et al., 2008; Ek, 2009) which optimises the latent points using MAP and, therefore, requires an initial factorisation of the inputs to be given a priori. Finally, we compared to a dynamical ver-\nsion of Nearest Neighbour where we kept multiple nearest neighbours and selected the coherent ones over a sequence. The errors shown in table 1 as well as the video provided as supplementary material show that MRD performs better than the other methods in this task.\nClassification: As a final experiment, we demonstrate the flexibility of our model in a supervised dimensionality reduction scenario for a classification task. The training dataset was created such that a matrix Y contained the actual observations and a matrix Z the corresponding class labels in 1-of-K encoding. We used the \u2018oil\u2019 database (Bishop & James, 1993) which contains 1000 12\u2212dimensional examples split in 3 classes. We selected 10 random subsets of the data with increasing number of training examples and compared to the nearest neighbor (NN) method in the data space. As can be seen in figure 9, MRD successfully determines the shared information between the data and the label space and outperforms NN."}, {"heading": "4. Conclusions", "text": "We have presented a new factorized latent variable model for multi view data. The model automatically factorizes the data using variables representing variance that exists in each view separately from variance being specific to a particular view. The model learns a distribution over the latent points variationally. This allows us to to automatically find the dimensionality of the latent space as well as to incorporate prior knowledge about its structure. As an example, we showed how dynamical priors can be included on the latent space. This allowed us to use temporal continuity to disambiguate the model\u2019s predictions in an ambiguous human pose estimation problem. The model is capable of learning from extremely high-dimensional data. We illustrated this by learning a model directly on the pixel representation of an image. Our model is capable of learning a compact an intuitive representation of such data which we exemplified by generating novel images by sampling from the latent representation in a structured manner. Finally, we showed how a generative model with discriminative capabilities can be obtained by treating the observations and class labels of a dataset as separate modalities."}, {"heading": "Acknowledgments", "text": "Research was partially supported by the University of Sheffield Moody endowment fund and the Greek State Scholarships Foundation (IKY). We would like to thank the reviewers for their useful feedback."}], "references": [{"title": "Recovering 3D human pose from monocular images", "author": ["Agarwal", "Ankur", "Triggs", "Bill"], "venue": "doi: 10.1109/TPAMI", "citeRegEx": "Agarwal et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2006}, {"title": "Analysis of multiphase flows using dual-energy gamma densitometry and neural networks", "author": ["Bishop", "Christopher M", "James", "Gwilym D"], "venue": "Nuclear Instruments and Methods in Physics Research,", "citeRegEx": "Bishop et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1993}, {"title": "Variational gaussian process dynamical systems", "author": ["Damianou", "Andreas C", "Titsias", "Michalis", "Lawrence", "Neil D"], "venue": "In NIPS, pp", "citeRegEx": "Damianou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Damianou et al\\.", "year": 2011}, {"title": "Shared Gaussian Process Latent Variable Models", "author": ["Ek", "Carl Henrik"], "venue": "PhD Thesis,", "citeRegEx": "Ek and Henrik.,? \\Q2009\\E", "shortCiteRegEx": "Ek and Henrik.", "year": 2009}, {"title": "Gaussian process latent variable models for human pose estimation", "author": ["Ek", "Carl Henrik", "Torr", "Phil", "Lawrence", "Neil"], "venue": "Proceedings of the 4th international conference on Machine learning for multimodal interaction,", "citeRegEx": "Ek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ek et al\\.", "year": 2007}, {"title": "Ambiguity modeling in latent spaces", "author": ["Ek", "Carl Henrik", "J Rihan", "Torr", "Phil", "G Rogez", "Lawrence", "Neil"], "venue": "Machine Learning and Multimodal Interaction,", "citeRegEx": "Ek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ek et al\\.", "year": 2008}, {"title": "From few to many: Illumination cone models for face recognition under variable lighting and pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "Georghiades et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Georghiades et al\\.", "year": 2001}, {"title": "Semisupervised alignment of manifolds", "author": ["J Ham", "D Lee", "Saul", "Lawrence K"], "venue": "In Annual Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Ham et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2005}, {"title": "Generative models that discover dependencies between data sets", "author": ["Klami", "Arto", "Kaski", "Samuel"], "venue": "In Proceedings of MLSP\u201906, IEEE International Workshop on Machine Learning for Signal Processing,", "citeRegEx": "Klami et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Klami et al\\.", "year": 2006}, {"title": "The Geometry Of Kernel Canonical Correlation Analysis", "author": ["Kuss", "Malte", "Graepel", "Thore"], "venue": null, "citeRegEx": "Kuss et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Kuss et al\\.", "year": 2003}, {"title": "Probabilistic non-linear principal component analysis with Gaussian process latent variable models", "author": ["Lawrence", "Neil"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lawrence and Neil.,? \\Q2005\\E", "shortCiteRegEx": "Lawrence and Neil.", "year": 2005}, {"title": "Hierarchical Gaussian process latent variable models", "author": ["Lawrence", "Neil D", "Moore", "Andrew J"], "venue": "In ICML,", "citeRegEx": "Lawrence et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2007}, {"title": "Acquiring linear subspaces for face recognition under variable lighting", "author": ["K.C. Lee", "J. Ho", "D. Kriegman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intelligence,", "citeRegEx": "Lee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2005}, {"title": "Shared Kernel Information Embedding for Discriminative Inference", "author": ["Memisevic", "Roland", "Sigal", "Leonid", "Fleet", "David J"], "venue": "Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Memisevic et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Memisevic et al\\.", "year": 2011}, {"title": "Gaussian Processes for Machine Learning", "author": ["Rasmussen", "Carl Edward", "Williams", "Christopher K. I"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "Factorized Orthogonal Latent Spaces", "author": ["Salzmann", "Mathieu", "Ek", "Carl Henrik", "Urtasun", "Raquel", "Darrell", "Trevor"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Salzmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Salzmann et al\\.", "year": 2010}, {"title": "Learning shared latent structure for image synthesis and robotic imitation", "author": ["A Shon", "K Grochow", "A. Hertzmann"], "venue": "In Neural Information Processing,", "citeRegEx": "Shon et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shon et al\\.", "year": 2006}, {"title": "Bayesian Gaussian Process Latent Variable Model", "author": ["Titsias", "Michalis", "Lawrence", "Neil"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Titsias et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Titsias et al\\.", "year": 2010}, {"title": "Variational learning of inducing variables in sparse Gaussian processes", "author": ["Titsias", "Michalis K"], "venue": "In Proceedings of the Twelfth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Titsias and K.,? \\Q2009\\E", "shortCiteRegEx": "Titsias and K.", "year": 2009}], "referenceMentions": [{"referenceID": 7, "context": "Different approaches exploit different characteristics of the data such as, correlation (Kuss & Graepel, 2003; Ham et al., 2005), or mutual information (Memisevic et al.", "startOffset": 88, "endOffset": 128}, {"referenceID": 13, "context": ", 2005), or mutual information (Memisevic et al., 2011).", "startOffset": 31, "endOffset": 55}, {"referenceID": 16, "context": "In particular, approaches formulated as Gaussian Processes Latent Variable Models (GP-LVMs) (Lawrence, 2005) have been especially successful (Shon et al., 2006; Ek et al., 2007).", "startOffset": 141, "endOffset": 177}, {"referenceID": 4, "context": "In particular, approaches formulated as Gaussian Processes Latent Variable Models (GP-LVMs) (Lawrence, 2005) have been especially successful (Shon et al., 2006; Ek et al., 2007).", "startOffset": 141, "endOffset": 177}, {"referenceID": 5, "context": "To overcome this, the idea of a factorized latent space was presented in (Ek et al., 2008) where each view is associated with an additional private space, representing the variance which cannot be aligned, in addition to the shared space (Ek, 2009), an idea independently suggested by Klami & Kaski (2006).", "startOffset": 73, "endOffset": 90}, {"referenceID": 4, "context": ", 2006; Ek et al., 2007). However, these models assume that a single latent variable is capable of representing each modality, implying that the modalities can be fully aligned. To overcome this, the idea of a factorized latent space was presented in (Ek et al., 2008) where each view is associated with an additional private space, representing the variance which cannot be aligned, in addition to the shared space (Ek, 2009), an idea independently suggested by Klami & Kaski (2006). The main challenge for the applicability of the proposed models is that the factorization of the latent variable is a structural and essentially discrete property of the model, making it very challenging to learn.", "startOffset": 8, "endOffset": 484}, {"referenceID": 4, "context": ", 2006; Ek et al., 2007). However, these models assume that a single latent variable is capable of representing each modality, implying that the modalities can be fully aligned. To overcome this, the idea of a factorized latent space was presented in (Ek et al., 2008) where each view is associated with an additional private space, representing the variance which cannot be aligned, in addition to the shared space (Ek, 2009), an idea independently suggested by Klami & Kaski (2006). The main challenge for the applicability of the proposed models is that the factorization of the latent variable is a structural and essentially discrete property of the model, making it very challenging to learn. Salzmann et al. (2010) intro-", "startOffset": 8, "endOffset": 722}, {"referenceID": 16, "context": "In practice, a maximum a posteriori solution (Shon et al., 2006; Ek et al., 2007; Salzmann et al., 2010) was often used.", "startOffset": 45, "endOffset": 104}, {"referenceID": 4, "context": "In practice, a maximum a posteriori solution (Shon et al., 2006; Ek et al., 2007; Salzmann et al., 2010) was often used.", "startOffset": 45, "endOffset": 104}, {"referenceID": 15, "context": "In practice, a maximum a posteriori solution (Shon et al., 2006; Ek et al., 2007; Salzmann et al., 2010) was often used.", "startOffset": 45, "endOffset": 104}, {"referenceID": 2, "context": "We achieve this by building on recent variational approximations for standard GP-LVMs (Titsias & Lawrence, 2010; Damianou et al., 2011).", "startOffset": 86, "endOffset": 135}, {"referenceID": 16, "context": "the observations was shared (Shon et al., 2006).", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "Secondly, Ek et al. (2008) introduced private latent spaces to explain variance specific", "startOffset": 10, "endOffset": 27}, {"referenceID": 15, "context": "full manifold, as traditionally assumed, nor by a subspace geometrically orthogonal to that, as assumed in Salzmann et al. (2010).", "startOffset": 107, "endOffset": 130}, {"referenceID": 2, "context": "For the dynamical scenario we follow Damianou et al. (2011); Lawrence & Moore (2007) and choose the prior on the latent space to depend on the observation times t \u2208 R , e.", "startOffset": 37, "endOffset": 60}, {"referenceID": 2, "context": "For the dynamical scenario we follow Damianou et al. (2011); Lawrence & Moore (2007) and choose the prior on the latent space to depend on the observation times t \u2208 R , e.", "startOffset": 37, "endOffset": 85}, {"referenceID": 6, "context": "Yale faces: To show the ability of our method to model very high-dimensional spaces our first experiment is applied to the Yale dataset (Georghiades et al., 2001; Lee et al., 2005) which contains images of several human faces under different poses and 64 illumination conditions.", "startOffset": 136, "endOffset": 180}, {"referenceID": 12, "context": "Yale faces: To show the ability of our method to model very high-dimensional spaces our first experiment is applied to the Yale dataset (Georghiades et al., 2001; Lee et al., 2005) which contains images of several human faces under different poses and 64 illumination conditions.", "startOffset": 136, "endOffset": 180}, {"referenceID": 5, "context": "We also compared against the shared GP-LVM (Ek et al., 2008; Ek, 2009) which optimises the latent points using MAP and, therefore, requires an initial factorisation of the inputs to be given a priori.", "startOffset": 43, "endOffset": 70}], "year": 2012, "abstractText": "In this paper we present a fully Bayesian latent variable model which exploits conditional nonlinear (in)-dependence structures to learn an efficient latent representation. The latent space is factorized to represent shared and private information from multiple views of the data. In contrast to previous approaches, we introduce a relaxation to the discrete segmentation and allow for a \u201csoftly\u201d shared latent space. Further, Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces. The model is capable of capturing structure underlying extremely high dimensional spaces. This is illustrated by modelling unprocessed images with tenths of thousands of pixels. This also allows us to directly generate novel images from the trained model by sampling from the discovered latent spaces. We also demonstrate the model by prediction of human pose in an ambiguous setting. Our Bayesian framework allows us to perform disambiguation in a principled manner by including latent space priors which incorporate the dynamic nature of the data.", "creator": "LaTeX with hyperref package"}}}