{"id": "1206.4635", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Deep Mixtures of Factor Analysers", "abstract": "An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models based on simple convolutional methods that reduce the required memory space.\n\nIn an ideal world, a machine learning solution is used to solve a single problem and solve it successfully. In this model, the model's latent variable is the number of inputs (or inputs) that need to be represented at a time with constant values. In order to learn the next layer, we need to learn the next layer using a method that works at a time in the system, and then a layer of latent variables.\nIn this case, the machine learning solution was the following:\nWe have to learn the next layer, because it is too slow. If we can increase the speed of the next layer, it will only work on the second layer. If we can increase the time required to learn the next layer by 20% using a model with variable input. We can reduce the number of inputs by 20% using a model with variable input. The machine learning solution is similar:\nThe second layer is a deep learning model (where the input value is a fixed number of inputs), which is trained using a model with random number of inputs and a constant number of inputs. This process is called a \u2018deep learning\u2019. In this case, the machine learning solution is the \u2018deep learning\u2019 model.\nThe deep learning model is not very efficient. However, when we solve the next layer using a model using a model with constant values, we can use a model that does not use multiple input points to solve the problem. Since we can learn a model that does not have single input points (it must have a variable input point), we can choose to have multiple inputs for each machine learning problem.\nThe machine learning solution is quite easy to implement. We have to define parameters that represent the values that the machine learning solution uses. To do this, we have to define parameters that represent the values that the machine learning solution uses.\nTo perform the deep learning algorithm, we have to create", "histories": [["v1", "Mon, 18 Jun 2012 15:14:57 GMT  (3090kb)", "http://arxiv.org/abs/1206.4635v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["yichuan tang", "ruslan salakhutdinov", "geoffrey e hinton"], "accepted": true, "id": "1206.4635"}, "pdf": {"name": "1206.4635.pdf", "metadata": {"source": "META", "title": "Deep Mixtures of Factor Analysers", "authors": ["Yichuan Tang", "Ruslan Salakhutdinov", "Geoffrey Hinton"], "emails": ["tang@cs.toronto.edu", "rsalakhu@cs.toronto.edu", "hinton@cs.toronto.edu"], "sections": [{"heading": "1. Introduction", "text": "Unsupervised learning is important for revealing structure in the data and for discovering features that can be used for subsequent discriminative learning. It is also useful for creating a good prior that can be used for tasks such as image denoising and inpainting or tracking animate motion.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nA recent latent variable density model based on Markov Random Fields is the Gaussian Restricted Boltzmann Machine (GRBM) (Hinton & Salakhutdinov, 2006). A GRBM can be viewed as a mixture of diagonal Gaussians with the number of components exponential in the number of hidden variables, but with a lot of parameter sharing between the exponentially many Gaussians. In (Hinton et al., 2006), it was shown that a trained RBM model can be improved by using a second RBM to create a model of the \u201caggregated posterior\u201d (Eq. 9) of the first RBM, where the aggregated posterior is the equally weighted average of the posterior distributions over the hidden units of the first RBM for each training case. The second RBM is then used to replace the prior over the hidden units of the first RBM that is implicitly defined by the weights and biases of the first RBM. With mild assumptions on how training is performed at the higher layer, it was proven that a variational lower bound on the loglikelihood is guaranteed to improve. The second level RBM can do a better job of modeling the first RBM\u2019s aggregated posterior than the first level RBM because its parameters are not also being used to model the conditional distribution of the data given the states of the units in the first hidden layer.\nA rival model for real-valued high-dimensional data is the Mixture of Factor Analyzers (MFA) (Ghahramani & Hinton, 1996). MFAs simultaneously perform clustering and dimensionality reduction of the data by making locally linear assumptions (Verbeek, 2006). Unlike RBMs, MFAs are directed graphical models where a multivariate standard normal prior is specified for the latent factors for all components. Learning typically uses the EM algorithm to maximize the data log-likelihood. Each FA in the mixture has an isotropic Gaussian prior over its factors and a Gaussian posterior for each training case, but when the posterior is aggregated over many training cases it will typically be non-Gaussian. We can, therefore, improve a variational lower bound on the log probability of the training data by replacing the prior of each FA by a\nseparate, second-level MFA that learns to model the aggregated posterior of that FA better than it is modeled by an isotropic Gaussian. Empirically, the average test log-likelihood also increases for models of both low and high-dimensional data.\nWhile it is true that a two layer MFA can be collapsed back into a standard one layer MFA, learning the two models is nevertheless quite different due to the sharing of factor loadings among the second layer components of the Deep MFA. Parameter sharing helps to reduce overfitting and greatly reduces the computational cost of learning. The EM algorithm also benefits from an easier objective function due to the greedy layer-wise learning, so it is less likely to get stuck in poor local optima.\nMultilayer factor analysis was also part of the model in (Chen et al., 2011). However, that work mainly focused on learning convolutional features with nonparametric Bayesian priors on the parameters. By using max-pooling and decimation of the first layer factors, their model was designed to learn discriminative features, rather than a top-down generative model of pixel values."}, {"heading": "2. Mixture of Factor Analysers", "text": "Factor analysis was first introduced in psychology as a latent variable model to find the \u201cunderlying factor\u201d behind covariates. The latent variables are called factors and are of lower dimension than the covariates. Factor analyzers are linear models as the factor loadings span a linear subspace within the vector space of the covariates. To deal with non-linear data distributions, Mixtures of Factor Analyzers (MFA) (Ghahramani & Hinton, 1996) can be used. MFAs approximate nonlinear manifolds by making local linear assumptions.\nLet x \u2208 RD denote the D-dimensional data, {z \u2208 Rd : d \u2264 D} denote the d-dimensional latent variable, and c \u2208 {1, . . . , C} denote the component indicator variable of C total components. The MFA is a directed generative model, defined as follows:\np(c) = \u03c0c, C\u2211 c=1 \u03c0c = 1, (1)\np(z|c) = p(z) = N (z; 0, I), (2)\np(x|z, c) = N (x; Wcz + \u00b5c,\u03a8c), (3)\nwhere I is the d \u00d7 d identity matrix. The parameters of the c-th component include a mixing proportion \u03c0c, a factor loading matrix Wc \u2208 RD\u00d7d, mean \u00b5c, and\na diagonal matrix \u03a8c \u2208 RD\u00d7D, which represents the independent noise variances for each of the variables.\nBy integrating out the latent variable z, a MFA model becomes a mixture of Gaussians with constrained covariance:\np(x|c) = \u222b z p(x|z, c)p(z|c)dz = N (x;\u00b5c,\u0393c) (4)\n\u0393c = WcW T c + \u03a8c\np(x) = C\u2211 c=1 \u03c0c N (x;\u00b5c,\u0393c). (5)\nInference\nFor inference, we are interested in the posterior:\np(z, c|x) = p(z|x, c)p(c|x) (6)\nThe posterior over the components can be found using Bayes rule:\np(c|x) = p(x|c)p(c)\u2211C \u03b3=1 p(x|\u03b3)p(\u03b3)\n(7)\nGiven component c, the posterior over the latent factors is also a multivariate Gaussian:\np(z|x, c) = N (z; mc,V\u22121c ), (8)\nwhere\nVc = I + W T c \u03a8 \u22121 c Wc, mc = V \u22121 c W T c \u03a8 \u22121 c (x\u2212 \u00b5).\nMaximum likelihood learning of a MFA model is straightforward using the EM algorithm. During the E-step, Eqs. 7, 8 are used to compute the posterior over the latent variables given the current setting of the model parameters. During the M-step, the expected complete-data log-likelihood is maximized with respect to the model parameters \u03b8 = {\u03c0c,Wc,\u00b5c,\u03a8c}Cc=1:\nEp(z,c|x;\u03b8old)[log p(x, z, c; \u03b8)]"}, {"heading": "3. Deep Mixtures of Factor Analysers", "text": "After MFA training reaches convergence, the model can be improved by increasing the number C of mixture components or the dimensionality d of the latent factors per component. This amounts to adjusting the conditional distributions p(x|z, c). However, as we demonstrate in our experimental results, this approach\nquickly leads to overfitting, particularly when modeling high-dimensional data.\nAn alternative is to replace the standard multivariate normal prior on the latent factors: p(z|c) = N (0, I). The \u201caggregated posterior\u201d is the empirical average over the data of the posteriors over the factors: 1N \u2211N n=1 \u2211C c=1 p(zn, c|xn) and a componentspecific aggregated posterior is:\n1\nN N\u2211 n=1 p(zn, cn = c|xn) (9)\nIf each factor analyser in the mixture was a perfect model of the data assigned to it, the componentspecific aggregated posterior would be distributed according to an isotropic Gaussian, but in practice, it is non-Gaussian. Figure 1 (left panel) shows a component-specific aggregated posterior (with d = 2), which is highly non-Gaussian. In this case, we wish to replace a simple standard normal prior by a more powerful MFA prior:\np(z|c) = MFA(\u03b8(2)c ) (10)\nHere, \u03b8(2)c emphasizes that the new MFA\u2019s parameters are at the second layer and are specific to component c of the first layer MFA.\nMore concretely, the variational lower bound on the\nlog-likelihood of the model given data x is:\nL(x;\u03b8) = C\u2211 c=1 \u222b z q(z, c|x;\u03b8) log p(x, z, c;\u03b8)dz +H(q)\n= C\u2211 c=1 \u222b z q(z, c|x;\u03b8) { log p(x|z, c;\u03b8) (11)\n+ log p(z|c) + log \u03c0c } dz +H(q),\nwhere H(\u00b7) is the entropy of the posterior distribution q and \u03b8 represent the first layer MFA parameters. The DMFA formulation seeks to find a better prior log p(z|c) (using Eq. 10), while holding the first layer parameters fixed. Initially, when q(z, c|x;\u03b8) \u2261 p(z, c|x;\u03b8), the bound is tight. Therefore, any increase in the bound will lead to an increase in the true likelihood of the model. Maximizing the bound of Eq. 11 with respect to \u03b8(2) is equivalent to maximizing:\nC\u2211 c=1 \u222b z q(z, c|x;\u03b8) log p(z|c;\u03b8(2)) (12)\naveraged over the training data vectors. This is equivalent to fitting component-specific second-layer MFAs with vectors drawn from q(z, c|x;\u03b8) as data. The same scheme can be extended to training third-layer MFAs. With proper initialization, we are guaranteed to improve the lower bound on the log-likelihood, but the log-likelihood itself can fall (Hinton et al., 2006).\nFig. 1 (middle panel) shows a schematic representation of our model. Using \u03c0 (2) kc to denote the second layer\nmixing proportion of component kc, we have:\n\u2200 c : Kc\u2211 kc=1 \u03c0 (2) kc = 1 (13)\nA DMFA replaces the old MFA prior pMFA(z, c) = p(c)p(z|c) with a better prior:\npDMFA(z, c) = p(c)p(kc|c)p(z|kc) (14)\nTherefore, when sampling from a DMFA, we first sample c using \u03c0c, followed by sampling the second layer component kc using \u03c0 (2) kc\n. Finally, we can sample z using the Gaussian of component kc, as in Eq. 4.\nA simpler, but completely equivalent DMFA formulation is to enumerate over all possible second layer components kc. We use a new component indicator variable s = 1, . . . , S to denote a specific second layer component, where S = \u2211C c=1Kc. The mixing proportions are defined as \u03c0 (2) s = p(c(s))p(kc(s)|c(s)), where c(s) and kc(s) denotes the first and second layer components c and kc to which s corresponds. For example c(2) = 1 and c(5) = 2. We note that the size of S is exponential in the number of DMFA layers. The generative process of this formulation is very intuitive and we shall use it throughout the remaining sections.\nFig. 1 (right panel) shows the graphical model for a 2 layer DMFA. Specifically,\np(s) = \u03c0(2)s (15) p(z(2)|s) = N (z(2); 0, I) (16) p(z(1)|z(2), s) = N ( z(1); W(2)s z (2) + \u00b5(2)s ,\u03a8 (2) s ) (17)\nc\u2190 c(s), (deterministic) (18) p(x|z(1), c) = N ( x; W(1)c z (1) + \u00b5(1)c ,\u03a8 (1) c ) (19)\nEq. 18 is fully deterministic as every s belongs to one and only one c. z(1) \u2208 Rd(1) , z(2) \u2208 Rd(2) , W(1)c \u2208 RD\u00d7d(1) , W(2)s \u2208 Rd (1)\u00d7d(2) , \u00b5 (1) c \u2208 Rd (1) , and \u00b5 (2) s \u2208 Rd(2) . Finally, \u03a8(1)c and \u03a8 (2) s are d\n(1)\u00d7d(1) and d(2)\u00d7 d(2) diagonal matrices of the first and second layers respectively.\nDMFA has an equivalent shallow form, which is obtained by integrating out the latent factors. If we integrate out the first layer factors z(1), we obtain:\np(x|z(2), s) = N ( x; W(1)c (W (2) s z (2) + \u00b5(2)s ) + \u00b5 (1) c ,\n\u03a8(1)c + W (1) c \u03a8 (2) s W (1) c\nT ) (20)\nBy further integrating out z(2):\np(x|s) = N (x; W(1)c \u00b5(2)s + \u00b5(1)c , (21)\n\u03a8c + W (1) c (\u03a8 (2) s + W (2) d W (2) d T )W(1)c T )\nFrom Eq. 20, we can see that a DMFA can be reduced to a standard MFA where z(2) are the factors and s indicates the mixture component. This \u201ccollapsed\u201d MFA is regularized due to its parameter sharing. In particular, the means of the components s with the same first layer component c all must lie on a hyperplane spanned by W (1) c . The covariance of these components all share the same outer product factorization (W (1) c W (1) c T ) but with different \u201ccore matrices\u201d (\u03a8 (2) s + W (2) s W (2) s T ).\nAssuming that the number of the second layer components are equal, i.e. \u2200c : Kc = K, a standard shallow MFA with S = C \u00d7 K mixture components and d(1) factors per component would require O(DKd(1)C) parameters. A DMFA with two layers, on the other hand, would require O(Dd(1)C + d(1)d(2)CK) = O((D + d(2)K)d(1)C) parameters. Note that a DMFA requires a much smaller number of effective parameters than an equivalent shallow MFA, since d(2) << D. As we shall see in Sec. 4.1, this sharing of parameters is critical for preventing overfitting."}, {"heading": "3.1. Inference", "text": "Exact inference in a collapsed DMFA model is of order O(CK) since the data likelihood must be computed for each mixture component. We can incur a lower cost by using an approximate inference, which is O(C + K). First, we compute the posterior p(z(1), c|x) = p(z(1)|x, c)p(c|x) using Eq. 7. This posterior is exact if we had a standard normal prior over z(1), but it is an approximation of the exact posterior of the DMFA model. The entropy of the posterior p(c|x) is likely to be very low in high dimensional spaces. We therefore make a point estimate by selecting the component c with maximum posterior probability:\nc\u0302 = arg max c\np(c)p(c|x) (22)\np(z(1)|x) = \u2211 c p(z(1)|x, c)p(c|x)dc\n\u2248 p(z(1)|x, c\u0302) (23)\nFor the second layer, we treat c\u0302 and z(1) as data, and compute the posterior distribution p(z(2), s|z(1), c\u0302) in a similar fashion."}, {"heading": "3.2. Learning", "text": "A DMFA can be trained efficiently using a greedy layer-wise algorithm. The first layer MFA is trained in a standard way. We then use Eq. 23 to infer the component c\u0302 and the factors associated with that component for each training case {xn}. We then freeze the first layer parameters and treat the sampled first\nAlgorithm 1 Learning DMFAs\nGiven data: X = {x1,x2, . . . ,xN}. //Layer 1 training Train 1st layer MFA on X with C components and d factors using EM \u2192 MFA1.\n//Layer 2 training Create dataset Yc for each of the C components. Yc \u2190 \u2205 for i = 1 to N do\nfor c = 1 to C do compute p(c|xi) and p(z(1)|xi, c), Eqs. 7 & 8. end for Find c\u0302 = arg maxc p(c|xi). Sample z\n(1) i from N (z(1); mc\u0302,V \u22121 c\u0302 ).\nAdd z (1) i to dataset Yc\u0302: Yc\u0302 = Yc\u0302 \u222a {z (1) i }.\nend for\nd(2) and Kc: # of 2nd layer factors and components. for c = 1 to C do\nTrain a separate 2nd layer MFA on Yc with d(2) factors and Kc components using EM \u2192 MFA2{c}.\nend for\nlayer factor values for every component { {z(1)n }c } as training data for the second layer MFAs. Algorithm 1 details this layer-wise training algorithm. After greedy learning, \u201cbackfitting\u201d by collapsing a DMFA and running additional EM steps is also possible. However, more care is needed to prevent overfitting."}, {"heading": "4. Experiments", "text": "We demonstrate the advantages of learning DMFAs on both low dimensional and high dimensional datasets, including face images, natural image patches, and speech acoustic data. Toronto Face Database (TFD): The Toronto Face Database is a collection of aligned faces from a variety of (mostly) publicly available face image databases (Susskind, 2011). From the original resolution of 108\u00d7 108, we downsampled to resolutions of 48\u00d7 48 or 24\u00d7 24. We then randomly selected 30,000 images for training, 10,000 for validation, and 10,000 for testing. CIFAR-10: The CIFAR-10 dataset (Krizhevsky, 2009) consists of 60,000 32\u00d7 32\u00d73 color images of 10 object classes. There are 50,000 training images and 10,000 test images. Out of 50,000 training images, 10,000 were set aside for validation. TIMIT Speech: TIMIT is a corpus of phonemically and lexically transcribed speech of American English\nspeakers of different sexes and dialects1. The corpus contains a 462-speaker training set, a 50-speaker validation set, and a 24-speaker core test set. For our purposes, we extracted data vectors every 10-ms from the continuous speech data. Each frame analyzes a 25-ms Hamming window using a set of filter banks based on the Fast Fourier Transform. Concatenating 11 frames, we obtain 1353 dimensional input vectors. We randomly selected 30,000 vectors for training, 10,000 for validation, and 10,000 for testing. Berkeley Natural Images: The Berkeley segmentation database (Martin et al., 2001) contain 300 images from natural scenes. We randomly extracted 2 million 8 \u00d7 8 image patches for training, 50,000 patches for validation, and 50,000 for testing. UCI: We used 4 datasets from the UCI repository (Murphy & Aha, 1995). These are low dimensional datasets and have relatively few training examples. These were the only UCI datasets we tried.\nFor all image datasets, the DC component of each image was removed: x\u2190 x\u2212mean(x). This removes the huge illumination variations across data samples. No other preprocessing steps were used. For the TIMIT and UCI datasets, we normalize input vectors to zero mean and scale the entire input by a single number to make the average standard deviation be one. For evaluating the log probabilities of DMFAs, we always first collapsed it to a shallow MFA in order to obtain the exact data log-likelihood."}, {"heading": "4.1. Overfitting", "text": "We first trained a 20 component MFA on 24 \u00d7 24 faces until convergence2, which took 33 iterations. The number of factors was set to half of the input dimensionality, d(1) = D/2 = 288. Fig. 2 shows the corresponding training and validation log-likelihoods3. We next stacked a second MFA layer with five second layer components (Kc = 5) for each of the first layer components and d(2) = 50 second layer factors. The DMFA (MFA2) model improved as learning continued for an additional 20 iterations (see red and blue lines in Fig. 2). As a comparison, immediately after we initially formed the two-layer MFA, we collapsed it into its equivalent shallow representation and performed additional training (magenta and black lines in Fig. 2). Observe that the shallow MFA starts overfitting due to its extra capacity (5 times more parameters). MFA2,\n1 www.ldc.upen.edu/Catalog/CatalogEntry.jsp?catalogId=LDC93S1\n2Convergence is achieved when the log-likelihood changed by less than 0.01% from the previous EM iteration.\n3Similar results were obtained for different numbers of components and factors.\non the other hand, shows improvements on both the training and validation data. We note that training a shallow MFA with 100 components from random initialization is significantly worse (see Table. 1).\nTo give a sense of the computation costs, training the first layer MFA took 1600 seconds on a multi-core Xeon machine. The second layer MFA training took an additional 580 seconds."}, {"heading": "4.2. Qualitative Results", "text": "We next demonstrate qualitative improvements of the samples from a DMFA over a standard MFA model. As the baseline, we first trained a MFA model on 30,000 24\u00d724 face images from the TFD, with 288 factors and 100 components. We then trained a DMFA with 20 first layer components and 5 second layer components for each of the 20 first layer components. The DMFA has the same number of parameters as the baseline MFA. The two-layer MFA (MFA2) performs better compared to the standard MFA by around 20 nats on the test set. Fig 3 further shows samples from the two models. Qualitatively, the DMFA appears to generate better samples compared to the shallow MFA model."}, {"heading": "4.3. High Dimensional Data", "text": "Next, we explore the benefits of DMFAs on the high dimensional CIFAR and TIMIT datasets. We first trained a MFA model with the number of factors equal to half of the input dimensionality. The number of mixture components was set to 20. For MFA2, 5 components with 50 latent factors were used. For the 3rd layer MFA (MFA3) 3 factors with 30 latent factors were used.\nTable 1 shows the average training and test loglikelihood. In addition, we provide results for two\ntypes of RBM models that are commonly used when modeling high-dimensional real-valued data, including image patches and speech. The SSU model is a type of RBM with Gaussian visible and stepped sigmoid hidden units (Nair & Hinton, 2010). By using rectified linear activations, SSU can be viewed as a mixture of linear models with the number of components exponential in the number of hidden variables. A simpler Gaussian RBM (GRBM) model uses Gaussian visible and binary hidden units. It can also be viewed as a mixture of diagonal Gaussians with exponential number of components. For both the GRBM and SSU, we used Fast Persistent Contrastive Divergence (Tieleman & Hinton, 2009) for learning and AIS (Salakhutdinov & Murray, 2008) to estimate their log-partition functions. The AIS estimators have standard errors of around 5 nats, which are too small to affect the conclusions we can draw from Table 1.\nThe number of parameters for the GRBMs and SSU are matched to the MFA model, which means that approximately 6,000 hidden nodes are used. Increasing the number of hidden units did not result in any significant improvements of GRBM and SSU models. Hyperparameters are selected using the validation set. After MFA learning converged, a MFA2 model is initialized. The means of the MFA-2 components were slightly perturbed from zero so as to break symmetry. Shallow1 results were obtained by collapsing these newly initialized MFA2 models and further training using EM with early stopping. Shallow2 results were obtained by starting at random initialization (with multiple restarts) with the equivalent number of parameters as the corresponding Shallow1 models. We note the significant gains by DMFAs for the TIMIT and TFD-48 datasets.\nFig. 4 displays gains of 2 and 3 layer MFA as we vary the number of the first layer mixture components. It\nis interesting to observe that MFA and DMFA significantly outperformed various RBM models. This result suggests that it may be possible to improve many of the existing deep networks for modeling real-valued data that use GRBMs for the first hidden layer, though better density models do not necessarily learn features that are better for discrimination."}, {"heading": "4.4. Low Dimensional Data", "text": "DMFAs can also be used with low dimensional data. Following (Silva et al., 2011), we used 4 continuous datasets from the UCI repository. We removed the discrete variables from all datasets. For the Parkinsons dataset, one variable from any pair whose Pearson correlation coefficient is greater than 0.98 was also removed (for details see (Silva et al., 2011)). Table 2 reports the averaged test results using 10-fold cross validation. Compared to the recently introduced Copula Networks, MFAs give much better test predictive performance. Adding a second layer produced significant gains in model performance. The improvements from adding a second layer on all datasets were statistically significant using the paired t-test at p = 0.01."}, {"heading": "4.5. Natural Images", "text": "One important application of generative models is in the task of image restoration which can be formulated as a MAP estimation problem. As confirmed by Zoran & Weiss (2011), a better prior almost certainly leads to a better signal to noise ratio of the restored image. In addition, Zoran & Weiss (2011) have shown that combining a mixture of Gaussians model trained on 8\u00d7 8 patches of natural images with a patch-based denoising algorithm, allowed them to achieve state-of-the-art results. Following their work, we trained a two-layer MFA on 8\u00d78 patches from the Berkeley database. Two million training and 50,000 test patches were extracted from the 200 training and 100 test images, respectively. Table 3 shows results. Note that the DMFA improves upon the current state-of-the-art GMMs model of Zoran & Weiss (2011) by about 2 nats, while substantially outperforming other commonly used models including PCA and ICA. Finally, we trained a shallow equivalent to MFA-2 (5 times more parameters than MFA) from random initialization and achieved only 164.9 nats, thereby demonstrating that DMFAs are necessary in order to achieve the extra gain."}, {"heading": "4.6. Allocating more components to more popular factor analysers", "text": "Until now, we have given every higher level MFA the same number of components to model the aggregated posterior of its lower level factor analyser (\u2200c : Kc = K). While simple to implement, this is not optimal. An alternative is to use more second layer components for the first layer components with bigger mixing proportions. We tested this hypothesis by first training a MFA model on 48 \u00d7 48 TFD faces, which achieved an average test log-likelihood of 5159 nats. For the two-layer MFA, instead of assigning 5 components to each of the first layer components, we let Kc \u221d \u03c0c, with min(Kc) = 2 and \u2211C c Kc = 5 \u00d7 C. With all other learning hyper-parameters held constant, the resulting DMFA achieved 5246 nats on the test set. Compared to 5242 nats of our previous model (c.f. Table 1), the new method accounted for a gain of 4 nats. As another alternative, a measure of Gaussianity of the aggregated posterior could be used to determine Kc."}, {"heading": "5. Discussions", "text": "As density models, MFAs significantly outperform undirected RBM models for real-valued data and by using second layer MFAs to model the aggregated posterior of each first layer factor analyser, we can achieve substantial gains in performance. Higher input dimensionality leads to bigger gains from learning DMFAs. However, adding a third MFA layer appears to be of little value. Another possible extension of our work is to train a mixture of linear dynamical systems and then to train a higher-level mixture of linear dynamical systems to model the aggregated posterior of each component of the first level mixture."}, {"heading": "Acknowledgements", "text": "We thank Iain Murray for discussions and Jakob Verbeek for sharing his MFA code. This research was supported by NSERC & CIFAR."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables. After learning each layer, samples from the posterior distributions for that layer are used as training data for learning the next layer. This approach is commonly used with Restricted Boltzmann Machines, which are undirected graphical models with a single hidden layer, but it can also be used with Mixtures of Factor Analysers (MFAs) which are directed graphical models. In this paper, we present a greedy layer-wise learning algorithm for Deep Mixtures of Factor Analysers (DMFAs). Even though a DMFA can be converted to an equivalent shallow MFA by multiplying together the factor loading matrices at different levels, learning and inference are much more efficient in a DMFA and the sharing of each lower-level factor loading matrix by many different higher level MFAs prevents overfitting. We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machine on a wide variety of datasets.", "creator": "LaTeX with hyperref package"}}}