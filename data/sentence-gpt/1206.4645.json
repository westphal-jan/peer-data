{"id": "1206.4645", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design", "abstract": "Convex regression is a promising area for bridging statistical estimation and deterministic convex optimization. New piecewise linear convex regression methods are fast and scalable, but can have instability when used to approximate constraints or objective functions for optimization. Ensemble methods, like bagging, smearing and random partitioning, can alleviate this problem and maintain the theoretical properties of the underlying estimator.\n\n\n\nHere's a look at our previous piece on modeling the following problems:\nOne important problem we had with the previous analysis was that the original model was very linear and could not do all of the computations in order to perform the same. Now it is impossible to implement that model. Let's look at the model that did not include the data.\nFirst we have to perform a regression method on the data and use it to calculate the number of iterations that we need. This time we will be looking at the first function that does this:\n(function(s, a, b) { return a , b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b + b", "histories": [["v1", "Mon, 18 Jun 2012 15:19:58 GMT  (349kb)", "http://arxiv.org/abs/1206.4645v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.NA stat.ME stat.ML", "authors": ["lauren hannah", "david b dunson"], "accepted": true, "id": "1206.4645"}, "pdf": {"name": "1206.4645.pdf", "metadata": {"source": "META", "title": "Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design", "authors": ["Lauren A. Hannah", "David B. Dunson"], "emails": ["lh140@duke.edu", "dunson@stat.duke.edu"], "sections": [{"heading": "1. Introduction", "text": "Convex regression, which is regression subject to a convexity or concavity constraint on the mean function, has received renewed attention. The regression problem is x \u2208 X \u2282 Rp and y \u2208 R,\ny = f0(x) + ,\nwhere is a mean 0 random variable and f0 is convex,\n\u03bbf0(x1) + (1\u2212 \u03bb)f0(x2) \u2265 f0(\u03bbx1 + (1\u2212 \u03bb)x2),\nfor every x1,x2 \u2208 X and \u03bb \u2208 (0, 1). Regression problems with known convexity or concavity constraints occur in many areas, including economic production,\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nconsumption and preference functions (Allon et al., 2007), options pricing (A\u0131\u0304t-Sahalia and Duarte, 2003; Hannah and Dunson, 2011), value function approximation in operations research and reinforcement learning (Powell, 2007; Lim, 2010) and device modeling for geometric programming based circuit design in electrical engineering (Kim et al., 2004; Roy et al., 2007).\nConvex regression is particularly promising for the machine learning community as a way to bridge statistical estimation and deterministic convex optimization. In particular, data can be used to estimate constraints or objective functions for convex optimization problems. For instance, many reinforcement learning problems that involve resource allocation or storage have concave value functions. If value function estimates are concave, vector-valued continuous action spaces can easily be searched. Similarly, geometric programming and other deterministic convex optimization problems require known constraint and objective functions. However, in many situations only noisy samples are available; convex regression can be used to generate those functions from samples.\nAlthough convex regression has been studied since the 1950\u2019s (Hildreth, 1954), computationally feasible methods for the multivariate setting have only recently been proposed by Magnani and Boyd (2009) and Hannah and Dunson (2011). Both methods fit a piecewise linear model to the data, (x1, y1), . . . , (xn, yn), under a least squares objective function by adaptively partitioning the dataset. While efficient, the method of Magnani and Boyd (2009) does not always converge; the Convex Adaptive Partitioning (CAP) method of Hannah and Dunson (2011), however, converges, is consistent and has a worst case computational complexity of O(n log(n)2).\nWhile piecewise linear methods are computationally efficient, the number of components and hyperplane\nparameters can be sensitive to training data. Moreover, the resulting piecewise linear models may not be appropriate for approximating functions for convex optimization. When a piecewise linear function is used in an optimization setting, it defines a polyhedral constraint region. When the objective function is linear, a solution lies on a vertex of the polyhedral constraint region. A vertex is created by the intersection of p+ 1 hyperplanes. Because all of the parameters have estimation error, the location of the vertex can be highly sensitive to training data.\nThese problems can be addressed by using ensemble methods based on the CAP estimator. Ensemble methods combine multiple models to produce a new predictive model. We average over multiple piecewise linear estimators to create a new estimator that is less sensitive to individual hyperplane parameters. Efficient estimators can be created by using many traditional ensemble methods, like bagging (Breiman, 1996), smearing (Breiman, 2000) and random forests (Breiman, 2001), that maintain the properties of the underlying estimator, like consistency and computational complexity. We compare these methods with CAP and the piecewise linear model of Magnani and Boyd (2009); the ensemble methods have better predictive error and produce functions that are much more stable in an optimization setting.\nWe apply ensemble methods to device and constraint modeling for circuit optimization via geometric programming. Circuits are an interconnection of electrical devices, including capacitors, resistors, inductors and logic gates. Circuit optimization selects appropriate sizes for devices, gates, wires and other design variables, such as threshold and power supply voltage to minimize a given objective like circuit delay or physical area, subject to a set of constraints, usually on area, power, noise or delay. Many circuit design problems can be well modeled by a geometric program (GP), which minimizes a posynomial objective function subject to posynomial inequality and monomial equality constraints. Geometric programming allows the efficient computation of optimal global solutions, even for large problems; see Boyd, Kim, Patil and Horowitz (2005) for a tutorial. However, constraint functions are often not available in a posynomial form and must be approximated from observational data or a known but non-posynomial function for each device. We use ensemble methods to compute device models that are more accurate and more stable than other convex regression methods in this setting.\nThe contributions of this paper are 1) new ensemble methods for convex regression that are resistant\nto overfitting and produce a better estimator for optimization than non-ensemble methods, 2) conditions for consistency when CAP is the underlying estimator, 3) strong empirical results, and 4) an application to device and constraint modeling for geometric programming based circuit optimization."}, {"heading": "2. Ensemble convex regression", "text": ""}, {"heading": "2.1. Convex regression", "text": "Regression subject to a convexity constraint has been the subject of renewed interest in the past few years. One approach has been approximation by a function with a positive definite Hessian (Roy et al., 2007; Aguilera and Morin, 2009; Yongqiao and He, 2012). These methods generally result in a problem that is solved by a semidefinite program with n semidefinite constraints; solution methods are prohibitively slow for more than 1 or 2 thousand observations. Another approach relies on an alternate definition of convexity:\nf0(x1) \u2265 f0(x2) + g0(x1)T (x1 \u2212 x2), (1)\nfor every x1,x2 \u2208 X , where g0(x) \u2208 \u2202f0(x) is a subgradient of f0 at x. Equation (1) means that a convex function lies above all of its supporting hyperplanes; with enough supporting hyperplanes, f0 can be approximately reconstructed arbitrarily well by taking the maximum over those hyperplanes.\nThe least squares estimator (LSE) directly projects a least squares objective function onto the cone of convex functions (Hildreth, 1954),\nmin n\u2211 i=1 (yi \u2212 y\u0302i)2 s.t. : y\u0302j \u2265 y\u0302i + gTi (xj \u2212 xi), (2)\nfor i, j = 1, . . . , n. Here, y\u0302i and gi are the estimated values of f0(xi) and the subgradient of f0 at xi, respectively. Equation (2) is a quadratic program with O(n2) constraints and cannot be solved efficiently for more than 1 or 2 thousand observations.\nTo combat these computational difficulties, some recent methods (Magnani and Boyd, 2009; Aguilera et al., 2011; Hannah and Dunson, 2011) estimate a small set of hyperplanes by approximately solving\n(\u03b1\u2217, \u03b2\u2217,K\u2217) = arg min \u03b1,\u03b2,K n\u2211 i=1 [ yi \u2212 max k=1,...,K \u03b1k + \u03b2 T k xi ]2 where (\u03b1, \u03b2) \u2208 R \u00d7 Rp defines a hyperplane. The regression function f\u0302 is defined as the maximum over the set of hyperplanes for a convex function,\nf\u0302(x) = max k=1,...,K\u2217\n\u03b1\u2217k + \u03b2 \u2217 k Tx.\nThe most computationally efficient methods are given by Magnani and Boyd (2009) and Hannah and Dunson (2011)."}, {"heading": "2.2. Combining ensemble methods with convex regression", "text": "Ensemble methods reduce overfitting by averaging over a collection of estimates. Here we overview traditional ensemble methods and discuss how they can be combined with convex regression.\nBagging. Bagging methods (Breiman, 1996) subsample the training data with replacement, which acts as a random re-weighting of the training set. We study the situation where M new training sets are created by subsampling n observations, denoted by (x(m),y(m))Mm=1. An estimator is created by averaging the function estimates for each subsample,\nf\u0302avg(x) = 1\nM M\u2211 m=1 f\u0302 ( x |x(m),y(m) ) , (3)\nwhere f\u0302 ( x |x(m),y(m) ) is a convex regression estimator trained on (x(m),y(m)). Bagging can be used with both CAP and the method proposed in Magnani and Boyd (2009).\nSmearing. Smearing (Breiman, 2000) works by adding mean zero, i.i.d. noise to the training data responses. These \u201csmeared\u201d datasets are then fit with a regression method and the results are averaged to produce an estimator. M new training response sets, (y (m) 1:n ) M m=1, are created by adding i.i.d. Gaussian noise,\n\u03be(m) \u223c Nn(0, \u03c32I), y(m)i = yi + \u03be (m) i .\nThe ensemble estimator is then created by averaging convex estimators for each of the M random training sets as in Equation (3). In Breiman (2000), the noise level was chosen to be 2.5 times the standard deviation of the estimator residuals, y\u2212 f\u0302(x). However, we also consider the situation where it is chosen by crossvalidation. Both CAP and the method of Magnani and Boyd (2009) can be used with smearing.\nRandom Search Directions. Random forests (Breiman, 2001) are used in tree regression settings; instead of fully exploring each of the subset split directions, a split is generated in a random direction. Since the subsets in both CAP and Magnani and Boyd (2009) are defined by the hyperplane parameters, there is no direct analogy between random forests and these methods. However, we propose a method in the same spirit.\nThe partitions of the CAP estimator are created in a two step process. In the first step, subsets are split along cardinal (CAP) or random directions. In the second step, the subsets are redefined by the maximal hyperplanes. Searching over a set of random directions in the first step produces a random estimator. This is done M times and an ensemble estimator is produced by averaging the estimators.\nBoosting. Boosting (Freund and Schapire, 1997; Friedman, 2002) is a popular ensemble method that constructs additive models in a greedy, forward stepwise manner. This exact method is not appropriate for convex regression since the residuals left after fitting a convex function may not maintain convexity. However, methods that iteratively weight a set of basis functions may prove useful for convex regression."}, {"heading": "3. Theoretical results", "text": "Bagging, smearing and random search directions maintain consistency if CAP is used as the convex estimator and a few mild conditions are imposed. Each CAP covariate subset Ak has diameter dnk, where dnk = supx1,x2\u2208Ak ||x1 \u2212 x2||2. Define the empirical mean for index subset Ck as x\u0304k =\n1 |Ck| \u2211 i\u2208Ck xi. For\nxi \u2208 Ak, define\n\u0393 (m) i =\n[ [1, . . . , 1]\ndnk \u22121 (xi \u2212 x\u0304k)\n] , Gk = \u2211 i\u2208Ck \u0393i\u0393i T .\nLet x1, . . . ,xn be i.i.d. random variables and let a superscript of (m) denote that a quantity is associated with random estimator m = 1, . . . ,M . Let f\u0302(x |Z(m), Dn) be a random estimator based on data Dn and random variable Z\n(m). We make the following assumptions, which are the original CAP conditions for consistency applied to each random dataset:\nA1. X is compact and f0 is Lipschitz continuous and continuously differentiable on X with Lipschitz parameter \u03b6.\nA2. There is an a > 0 such that E [ ea|Y\u2212f0(x)| |X = x ] is bounded on X .\nA3. For m = 1, . . . ,M , the diameter of the partition\nmaxk d (m) nk\n\u22121 \u2192 0 in probability as n\u2192\u221e.\nA4. Let \u03bb (m) k be the smallest eigenvalue of\n|C(m)k |\u22121G (m) k and \u03bb (m) n = mink \u03bb (m) k . Then for m = 1, . . . ,M , \u03bb (m) n remains bounded away from 0 in probability as n\u2192\u221e.\nA5. For m = 1, . . . ,M , the number of observations in each subset satisfies\nmin k=1,...,K (m) n\nd (m) nk 2 |C(m)k |/ log(n) \u2192 0 in\nprobability as n\u2192\u221e.\nProposition 3.1. Suppose that\nf\u0302avg(x |Dn) = 1\nM M\u2211 m=1 f\u0302(x |Z(m), Dn)\nand supx\u2208X \u2223\u2223\u2223f\u0302(x |Z(m), Dn)\u2212 f0(x)\u2223\u2223\u2223 \u2192 0 in probability. Then for every fixed M ,\nsupx\u2208X \u2223\u2223\u2223f\u0302avg(x |Dn)\u2212 f0(x)\u2223\u2223\u2223\u2192 0 in probability. Proof. By the triangle inequality, \u2223\u2223\u2223f\u0302avg(x |Dn)\u2212 f0(x)\u2223\u2223\u2223 \u2264 1 M M\u2211 m=1\n\u2223\u2223\u2223f\u0302(x |Z(m), Dn)\u2212 f0(x)\u2223\u2223\u2223 , The result follows from the assumption for each m.\nTheorem 3.2. If f\u0302(x) is generated by the CAP estimator, (x(m),y(m))Mm=1 are generated by bagging, E[Y 2 |X = x] < \u221e a.s. for all x \u2208 X , and A1. to A5. hold, then for every fixed M ,\nsup x\u2208X \u2223\u2223\u2223\u2223\u2223 1M M\u2211 m=1 f\u0302(x |x(m)1:n ,y (m) 1:n )\u2212 f0(x) \u2223\u2223\u2223\u2223\u2223\u2192 0 in probability as n\u2192\u221e.\nProof. Because of the bounded second moment and A1., the plugin estimate f\u0302(x |x(m),y(m)) is consistent and the result follows from Prop. 3.1.\nTheorem 3.3. If f\u0302(x) is generated by the CAP estimator, Y (m) i = Yi + \u03be (m) i with \u03be (m) i \u223c N(0, B) iid for some B < \u221e, and assumptions A1. through A5. hold, then for every fixed M ,\nsup x\u2208X \u2223\u2223\u2223\u2223\u2223 1M M\u2211 m=1 f\u0302(x |x1:n,y(m)1:n )\u2212 f0(x) \u2223\u2223\u2223\u2223\u2223\u2192 0 in probability as n\u2192\u221e.\nProof. Fix m and consider the estimator f\u0302(x |x1:n,y(m)1:n ). Note that Y (m) = Y + \u03be(m), so\nE [ ea|Y (m)\u2212f0(x)| |x ] \u2264 E [ ea(|Y\u2212f0(x)|+|\u03be (m)|) |x ]\n\u2264 2e 12a 2B2E [ ea|Y\u2212f0(x)| |x ] <\u221e.\nSince A1. through A5. hold for that estimator, it is consistent and the result follows from Prop. 3.1.\nTheorem 3.4. If f\u0302(x) is generated by the CAP estimator with random search directions Z(m) and A1. to A5. hold, then for every fixed M ,\nsup x\u2208X \u2223\u2223\u2223\u2223\u2223 1M M\u2211 m=1 f\u0302(x |x1:n,y1:n, Z(m))\u2212 f0(x) \u2223\u2223\u2223\u2223\u2223\u2192 0 in probability as n\u2192\u221e.\nProof. Each estimator is consistent and the result follows from Prop. 3.1."}, {"heading": "4. Experiments on synthetic data", "text": ""}, {"heading": "4.1. Prediction", "text": "Here x \u2208 R5 with X \u223c N5(0, I). Set\ny = (x1 + .5x2 + x3) 2 \u2212 x4 + .25x25 + , \u223c N(0, 1).\nWe compared CAP, linear fitting (Magnani and Boyd, 2009) (MB), cross-validated smearing (Sm CAP, Sm MB), smearing with 2.5 times residual noise (Sm 2.5 CAP, Sm 2.5 MB), bagging (Bag CAP, Bag MB) and random search directions (RD). All ensemble methods except RD were implemented with CAP and MB. 10 training sets and one testing set were generated; the number of training samples was varied between 100 and 5,000. For Sm CAP and Sm MB, the noise level was chosen by 5-fold cross validation from \u03c3 = {0, 10\u22122s, 10\u22121s, 5\u22121s, 2.5\u22121s, s, 2.5s, 5s, 10s, 102s}, where s is the standard deviation of the residuals; each level was approximated with M = 25. Appropriate noise levels were then probabilistically chosen for each m for M = 200. The number of hyperplanes in linear fitting was chosen through 5-fold cross validation. Results are given in Table 2.\nEnsemble methods substantially reduced CAP prediction error for all sample sizes except for n = 5, 000. Smearing with cross-validated noise, random search directions and bagging produced similar results. Ensemble methods produced smaller reduction in prediction error for linear fitting, with bagging producing the best results. Smearing with 2.5x standard deviation noise produced worse results than the other ensemble methods, likely because the noise levels for cross-validated smearing were lower."}, {"heading": "4.2. Optimization", "text": "Approximating objective functions or constraints for use in convex optimization is one of the most promising applications for convex regression. In this subsection, we use convex regression for response surface methods in stochastic optimization; see Lim (2010) for\nan overview. We would like to minimize an unknown function f(x) with respect to x given n noisy observations, (xi, yi) n i=1, where yi = f(xi, i),\nmin x\u2208X\nE {f(x, ) | (xi, yi)ni=1} . (4)\nWe tested the regression methods with\nYi = xiQx T i + i, Q =\n[ 1 0.2\n0.2 1\n] , i \u223c N(0, 0.1).\nThe constraint set is \u22121 \u2264 xj \u2264 1 for j = 1, 2, and xi \u223c Unif [\u22121, 1]2. We used the above methods as well as the Least Squares Estimator (LSE). We generated 50 training sets, solved Equation (4) using each regression method, and then calculated the root mean squared error (RMSE) for the functional estimators; the number of training data was set at 100 and 500. Solutions to Equation (4) were evaluated with respect to the true function; RMSE was calculated over a grid on the constraint space. Results are given in Table 1. The ensemble methods produced significantly better quality results both in terms of solution selection and RMSE than the competing methods. The extra noise of 2.5x smearing acted as a smoother and produced a more accurate and stable minimum."}, {"heading": "5. Circuit design", "text": ""}, {"heading": "5.1. Geometric programming and circuit design", "text": "Geometric programming is a mathematical optimization problem where the objective function and con-\nstraints are defined in terms of monomials, posynomials and generalized posynomials. A monomial g(x) and posynomial f(x) have the forms\ng(x) = cxa11 x a2 2 . . . x ap p , f(x) = K\u2211 k=1 ckx ak1 1 . . . x akp p ,\nfor x > 0. A generalized posynomial is created through positive powers, addition, multiplication or the maximum of posynomials. A GP minimizes a generalized posynomial subject to a set of generalized posynomial inequality and monomial equality constraints,\nmin f0(x), subject to fi(x) \u2264 1, gj(x) = 1, x > 0,\nwhere fi are generalized posynomials for i = 0, . . . ,m and gj are monomials for j = 1, . . . , k. GPs can be reformulated as convex optimization problems through a change of variables, zi = log(xi),\nf(x) = K\u2211 k=1 ckx ak1 1 . . . x akp p , f(z) = K\u2211 k=1 ea T k z+bk .\nIf we take the log of the transformed function, f(z), we get a function that is convex in z.\nMany circuit design problems, both analog and digital, can be modeled as GPs (Kim et al., 2004; Boyd, Kim, Patil and Horowitz, 2005; Roy et al., 2007). Geometric programming offers a fast, global solution method for design problems that scales well even to large problems. To use geometric programming, however, devices and constraints need to be modeled by generalized posynomials. Sometimes device models are not known and need to be inferred from data in standard cell libraries; other times, constraints or device models are given, but not in a form that can be expressed as a generalized posynomial. In each of these cases, piecewise linear convex regression can be used to produce generalized posynomial representations of these models and constraints. Ensemble methods can produce models that have lower error and are more stable in an optimization setting than existing methods."}, {"heading": "5.2. Device and constraint modeling with convex regression", "text": "Functions without explicit generalized posynomial representation occur in two settings. In the device modeling setting, device parameters such as the inverse of transconductance, gate-source voltage, the inverse of output resistance and the intrinsic gate capacitance need to be modeled as generalized posynomial functions of input parameters such as device width, length, terminal voltages and drain current. These relationships need to be inferred from data generated by circuit simulation or contained in standard cell libraries.\nLog convexity of such data is not guaranteed and the goal is to find a low error generalized posynomial approximation. In the constraint modeling setting, some constraints, such as the power supply voltage for different gates, are known but do not have a generalized posynomial form. In this case, the goal is to create a low error generalized posynomial approximation based on samples from the true function. While posynomials can be directly fit through Gauss-Newton type methods, these methods only reach local optima and are sensitive to algorithm initialization. Piecewise linear convex regression offers a more appealing alternative.\nMonomials, posynomials and generalized posynomials are closely related to affine (linear) and convex functions. Using the transformation, z = log(x), if a function f is a monomial, then log(f(ez)) is affine. If a function f is a generalized posynomial, then log(f(ez)) is convex. Conversely, if log(f(ez)) is convex, then f can be approximated arbitrarily well by a posynomial, generalized posynomial or the maximum of a set of monomials. We use this fact and piecewise linear convex regression to approximate what should be posynomial functions by the maximum of a set of monomials.\nLet (xi, yi) n i=1 be a set of observations of f(x) and let (zi, y\u0302i) n i=1 be the set of transformed observations, zi = log(xi) and y\u0302i = log(yi). If a piecewise linear convex model is fit to the transformed data,\nf\u0302n(z) = max k=1,...,K\n\u03b1k + \u03b2 T k z,\nthen a generalized posynomial can be constructed for the original function,\nf\u0303n(x) = max k=1,...,K\ne\u03b1kx\u03b2k11 . . . x \u03b2kp p .\nIn an ensemble setting, the coefficients can either be constructed directly or with dummy variables."}, {"heading": "5.3. Power modeling", "text": "Here we fit a generalized posynomial to a known, but non-posynomial, function, the power dissipated as a function of gate supply and threshold voltages, Vdd and Vth; this example is studied in Boyd, Kim, Patil and Horowitz (2005). The total power dissipated for a gate is the sum of the average static power and the dynamic power dissipated. The dynamic power is a function of the gate supply voltage, Vdd,\nPdyn = f ( Cint + CL ) V 2dd,\nwhere f is the frequency, Cint is the intrinsic capacitance and CL is the load capacitance. The static leakage is a function of the supply voltage and the average current leakage, I\u0304 leak, Pstat = I\u0304\nleakVdd. The average current leakage is a function of both the supply and threshold voltages; a standard model is\nI\u0304 leak \u221d e\u2212(Vth\u2212\u03b3DVdd)/V0 ,\nwhere \u03b3D and V0 are constants, typically around 0.06 and 0.04, respectively. To get the total power dissipates, we set P = Pstat + Pdyn. Note that while Pdyn is a posynomial, Pstat is not; moreover, it is not even convex under the log transformation. Previous methods have modeled power dissipated through hand-tuned monomial and generalized posynomial approximations. As a numerical example, we would like to model\nP = V 2dd + 30Vdde \u2212(Vth\u22120.06Vdd)/0.039 (5)\nfor 1.0 \u2264 Vdd \u2264 2.0 and 0.2 \u2264 Vth \u2264 0.4 with a generalized posynomial. The goal is to produce a model that has low overall error.\nWe produce a generalized posynomial model using n covariate samples that are drawn uniformly from x1 \u2208 [log(1.0), log(2.0)] and x2 \u2208 [log(0.2), log(0.4)]; responses are generated by evaluating those values in Equation (5). The number of observations was varied between 200 and 5,000, with 10 i.i.d. training sets. Methods were the same as in Section 4.1. All were tested by calculating RMSE from Equation (5) on a 10,000 sample testing set. For the ensemble methods, M = 200. Results are given in Table 2. The gains using ensemble methods were smaller for power modeling than for the synthetic problem, likely because the power modeling problem is noiseless."}, {"heading": "5.4. LC oscillator design", "text": "Here we compare convex regression methods for device modeling in geometric programming based LC oscillator design. Oscillators generate an oscillating output at a constant frequency. An LC oscillator (L and C represent inductor and capacitor, respectively) sends electrons from one plate of a capacitor through a coil, or loop inductor, to reach the other plate. However, when the electrons travel around a coil, a magnetic field is created that generates a voltage across the coil in the opposite direction of the electron flow. Once the capacitor is fully discharged, the magnetic field around the coil collapses and the voltage recharges the capacitor in the opposite direction. Additional voltage is applied to compensate for that lost to resistance.\nWe implement the LC oscillator design problem given in Boyd, Kim and Mohan (2005). The goal is to minimize power consumption subject to upper bound constraints on the phase noise, area of the loop inductor and lower bounds on the loop gain and the self resonance frequency, and some other loop inductor and transistor-specific constraints. The variables to be optimized are the width and diameter of the loop inductor, the self resonance frequency, the length and width and maximum current of the CMOS transistor, the differential voltage amplitude, the total capacitance of the oscillator, the maximum switching capacitance, the minimum variable capacitance, the bias current and the capacitor max frequency.\nWe used the methods in Section 4.1 to approximate the resistance of the loop inductor. EM-based posynomial fitting gave\nR = 0.1DW\u22121 + 3 \u00b7 10\u22126DW\u22120.84f0.5 (6) + 5 \u00b7 10\u22129DW\u22120.76f0.75 + 0.02DWf,\nwhere f , D, W are the loop inductor frequency, diameter and width. We used this model as truth to compare the suboptimality of the regression methods in a non-trivial optimization setting. To generate approximations, we sampled uniformly across logtransformed covariates, where \u221210 \u2264 log(D) \u2264 \u22125, \u221213 \u2264 log(W ) \u2264 \u221210 and 22 \u2264 log(f) \u2264 23 for n = 500 and n = 5, 000, although f is fixed for optimization. 50 different training sets were generated and the optimization problem was solved using the approximated functions in ggplab (Mutapcic et al., 2006). For all ensemble methods, M = 50 to limit the number of non-sparse GP constraints. We compared optimal power consumption for each model as a function of phase noise (dBc/Hz), which was varied from \u2212122 to \u2212110. Due to differences in function scale, percentage error from optimal values and percentage deviations from the true resistance values are used instead of RMSE. Results are given in Table 3.\nFor both methods, cross-validated smearing provided lower error estimators and produced solutions with comparable mean deviation and lower maximum deviation. Random search directions produced similar results with CAP. The different measurement metric highlights the differences between ensemble methods. The Magnani and Boyd (2009) estimator became more unstable with 2.5x standard deviation smearing. CAP became more unstable when used with bagging. Both of these methods likely do poorly because they add significant noise into a noiseless situation."}, {"heading": "6. Conclusions", "text": "In this paper, we combine convex regression (CAP and Magnani and Boyd (2009)) and ensemble methods to produce a piecewise linear approximation method. CAP is a consistent, stable estimator that uses a tree-like search to produce a piecewise linear model. Ensemble methods like bagging, smearing and random search directions add uncertainty in the partition boundaries. When averaged, these uncertain estimates produce a better fit. The Magnani and Boyd (2009) method is an unstable estimator that can produce a very good fit by aligning a piecewise linear model with the data used to produce it. Smearing and bagging average over a large number of models and reduce the likelihood that the estimator will be determined by a few poorly fitting models. Although device modeling is a natural setting for ensemble convex regression, the low computational complexity, theoretical guarantees and strong empirical performance in optimization settings, make ensemble convex regression a promising tool for combining estimation and optimization."}, {"heading": "Acknowledgments", "text": "Lauren A. Hannah is partially supported by the Duke Provost\u2019s Postdoctoral Fellowship. This work was supported by Award Number R01ES17240 from the National Institute of Environmental Health Sciences. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institute of Environmental Health Sciences or the National Institutes of Health."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Convex regression is a promising area for bridging statistical estimation and deterministic convex optimization. New piecewise linear convex regression methods (Hannah and Dunson, 2011; Magnani and Boyd, 2009) are fast and scalable, but can have instability when used to approximate constraints or objective functions for optimization. Ensemble methods, like bagging, smearing and random partitioning, can alleviate this problem and maintain the theoretical properties of the underlying estimator. We empirically examine the performance of ensemble methods for prediction and optimization, and then apply them to device modeling and constraint approximation for geometric programming based circuit design.", "creator": "LaTeX with hyperref package"}}}