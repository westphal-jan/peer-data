{"id": "1206.4648", "review": {"conference": "icml", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Two-Manifold Problems with Applications to Nonlinear System Identification", "abstract": "Recently, there has been much interest in spectral approaches to learning manifolds---so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data. We then show that two-manifold problems are less common among two-manifold systems than one-manifold systems, and that these two-manifold problems are in the same domain as their counterparts.\n\n\nIn this section, we describe two-manifold problem and illustrate its possible advantages. First, we show that the spectral decomposition of the two-manifold and related problems are not trivial, because we can not directly define a specific set of features in the Hilbert space: instead, we are forced to use one-manifold data to explore the generalizations of other features in our system. The two-manifold problems are only used in discrete domains, like parallelism, and so on. In this section, we describe how we can learn and test the generalizations of several other functions, so that it will be possible to find specific features in our system. Our approach thus describes two-manifold problems in the Hilbert space: first, we show how we can achieve a unified picture of the Hilbert space: first, we demonstrate how we can obtain generalizations of different features in our system in a similar manner. Second, we show that our two-manifold problems are very common among two-manifold systems: first, we show how we can obtain generalizations of different features in our system in a different way, and second, we demonstrate the generalizations of several different types of features in our system in a different way. Our approach also shows that the generalizations of different types of features in our", "histories": [["v1", "Mon, 18 Jun 2012 15:23:02 GMT  (7860kb)", "http://arxiv.org/abs/1206.4648v1", "ICML2012. arXiv admin note: text overlap witharXiv:1112.6399"]], "COMMENTS": "ICML2012. arXiv admin note: text overlap witharXiv:1112.6399", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["byron boots", "geoff gordon"], "accepted": true, "id": "1206.4648"}, "pdf": {"name": "1206.4648.pdf", "metadata": {"source": "META", "title": "Two-Manifold Problems with Applications to Nonlinear System Identification", "authors": ["Byron Boots", "Geoffrey J. Gordon"], "emails": ["beb@cs.cmu.edu", "ggordon@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Manifold learning algorithms are nonlinear methods for embedding a set of data points into a low-dimensional space while preserving local geometry. Recently, there has been a great deal of interest in spectral approaches to learning manifolds. These kernel eigenmap methods include Isomap (Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis & Saul, 2000), Laplacian Eigenmaps (LE) (Belkin & Niyogi, 2002), Maximum Variance Unfolding (MVU) (Weinberger et al., 2004), and Maximum Entropy Unfolding (MEU) (Lawrence, 2011). Despite the popularity of kernel eigenmap\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nmethods, they are limited in one important respect: they generally only perform well when there is little or no noise. Several authors have attacked the problem of learning manifolds in the presence of noise using methods like neighborhood smoothing (Chen et al., 2008) and robust principal components analysis (Zhan & Yin, 2009; 2011), with some success when noise is limited. Unfortunately, the problem is fundamentally ill posed without some sort of side information about the true underlying signal: by design, manifold methods will recover extra latent dimensions which \u201cexplain\u201d the noise.\nWe take a different approach to the problem of learning manifolds from noisy observations. We assume access to instrumental variables, which are correlated with the true latent variables, but uncorrelated with the noise in observations. Such instrumental variables can be used to separate signal from noise, as described in Section 3. Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression (Pearl, 2000), principal component analysis (PCA) (Jolliffe, 2002), and temporal difference learning (Bradtke & Barto, 1996). Here we extend the scope of this technique to manifold learning. We will pay particular attention to the two-manifold problem, in which two sets of observations each serve as instruments for the other. We propose algorithms for twomanifold problems based on spectral decompositions related to cross-covariance operators; and, we show that the instrumental variable idea suppresses noise in practice.\nFinally we look at a detailed example of how twomanifold methods can help solve difficult machine learning problems. Subspace identification approaches to learning nonlinear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinite-dimensional) covariance operators (Hsu et al., 2009; Boots et al., 2010; Song et al., 2010). Two-manifold problems are\na natural fit: by relating the spectral decomposition to our two-manifold method, subspace identification techniques can be forced to identify a manifold state space, and consequently, to learn a dynamical system that is both accurate and interpretable, outperforming the current state of the art."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Kernel PCA", "text": "Kernel PCA (Scho\u0308lkopf et al., 1998) generalizes PCA to high- or infinite-dimensional input data, represented implicitly using a reproducing-kernel Hilbert space (RKHS). If the kernel K(x,x\u2032) is sufficiently expressive, kernel PCA can find structure that regular PCA misses. Conceptually, if we write the \u201cfeature function\u201d \u03c6(x) = K(x, \u00b7), and define an infinitely-tall \u201cmatrix\u201d \u03a6 with columns \u03c6(xi), our goal is to recover the eigenvalues and eigenvectors of the centered covariance operator \u03a3\u0302XX = 1 n\u03a6H\u03a6 T. Here H is the centering matrix H = In \u2212 1n11 T. For efficient computation, we work with the Gram matrix G = 1n\u03a6 T\u03a6 instead of the large or infinite \u03a3\u0302XX . The centered Gram matrix C = HGH has the same nonzero eigenvalues as \u03a3\u0302XX ; the eigenvectors of \u03a3\u0302XX are \u03a6Hvi\u03bb \u22121/2 i , where (\u03bbi,vi) are the eigenpairs of C (Scho\u0308lkopf et al., 1998)."}, {"heading": "2.2. Manifold Learning", "text": "Kernel eigenmap methods seek a nonlinear function that maps a high-dimensional set of data points to a lower-dimensional space while preserving the manifold on which the data lies. The main insight behind these methods is that large distances in input space are often meaningless due to the large-scale curvature of the manifold; so, ignoring these distances can lead to a significant improvement in dimensionality reduction by \u201cunfolding\u201d the manifold.\nInterestingly, these algorithms can be viewed as special cases of kernel PCA where the Gram matrix G is constructed over the finite domain of the training data in a particular way (Ham et al., 2003). For example, in Laplacian Eigenmaps (LE), we first compute an adjacency matrix W by nearest neighbors: wi,j is nonzero if point i is one of the nearest neighbors of point j, or vice versa. We can either set non-zero weights to 1, or compute them with a kernel such as a Gaussian RBF. Next we let Si,i = \u2211 j wi,j , and set L = (S\u2212W). Finally, we eigendecompose L = V\u039bVT and set the LE embedding to be E = V2:k+1, the k smallest eigenvectors of L excluding the vector corresponding to the 0 eigenvalue. (Optionally we can scale according to eigenvalues, E = V2:k+1\u039b \u22121/2 2:k+1.) To relate LE to kernel PCA, Ham et al. (2003) showed that one can build a Gram matrix from L, G = L\u2020; the LE embedding is\ngiven by the top k eigenvectors (and optionally eigenvalues) of G.\nIn general, many manifold learning methods (including all the kernel eigenmap methods mentioned above) can be viewed as constructing a matrix E of embedding coordinates. From any such method, we can extract an equivalent Gram matrix G = EET. So, for the rest of the paper, we view a manifold learner simply as a black box which accepts data and produces a Gram matrix that encodes the learned manifold structure. This view greatly simplifies the description of our twomanifold algorithms below."}, {"heading": "3. Bias and Instrumental Variables", "text": "Kernel eigenmap methods are very good at dimensionality reduction when the original data points sample a high-dimensional manifold relatively densely, and when the noise in each sample is small compared to the local curvature of the manifold. In practice, however, observations are frequently noisy, and manifoldlearning algorithms applied to these datasets usually produce biased embeddings. See Figures 1\u20132, the \u201cnoisy swiss rolls,\u201d for an example.\nOur goal is therefore to design a more noise-resistant algorithm for the two-manifold problem. We begin by examining PCA, a linear special case of manifold learning, and studying why it produces biased embeddings in the presence of noise. We next show how to overcome this problem in the linear case, and then use these same ideas to generalize kernel PCA, a nonlinear algorithm. Finally, in Sec. 4, we extend these ideas to fully general kernel eigenmap methods."}, {"heading": "3.1. Bias in Finite-Dimensional Linear Models", "text": "Suppose that xi is a noisy view of some underlying low-dimensional latent variable zi: xi = Mzi + i for a linear transformation M and i.i.d. zero-mean noise term i.\n1 Without loss of generality, we assume that xi and zi are centered, and that Cov[zi] and M both have full column rank. In this case, PCA on X will generally fail to recover Z: the expectation of \u03a3\u0302XX = 1 nXX\nT is M Cov[zi] M T + Cov[ i], while we need M Cov[zi] M T to be able to recover a transformation of M or Z. The unwanted term Cov[ i] will, in general, affect all eigenvalues and eigenvectors of \u03a3\u0302XX , causing us to recover a biased answer even in the limit of infinite data."}, {"heading": "3.1.1. Instrumental Variables", "text": "We can fix this problem for linear embeddings: instead of plain PCA, we can use what might be called\n1Note that each i is a vector, and we make no assumption that its coordinates are independent from one another (i.e., we do not assume that Cov[ i] is spherical).\ntwo-subspace PCA. This method finds a statistically consistent solution through the use of an instrumental variable (Pearl, 2000; Jolliffe, 2002), an observation yi that is correlated with the true latent variables, but uncorrelated with the noise in xi. Importantly, picking an instrumental variable is not merely a statistical aid, but rather a value judgement about the nature of the latent variable and the noise in the observations. We are defining the noise to be that part of the variability which is uncorrelated with the instrumental variable, and the signal to be that part which is correlated.\nIn our example above, a good instrumental variable yi is a different (noisy) view of the same underlying low-dimensional latent variable: yi = Nzi + \u03b6i for some full-column-rank linear transformation N and i.i.d. zero-mean noise term \u03b6i. The expectation of the empirical cross covariance \u03a3\u0302XY = 1 nXY\nT is then M Cov(zi) N\nT: the noise terms, being independent and zero-mean, cancel out. (And the variance of each element of \u03a3\u0302XY goes to 0 as n\u2192\u221e.)\nNow, we can identify the embedding by computing the truncated singular value decomposition (SVD) of the covariance: \u3008U,D,V\u3009 = SVD(\u03a3\u0302XY , k). If we set k to be the true dimension of z, then as n \u2192 \u221e, U will converge to an orthonormal basis for the range of M, and V will converge to an orthonormal basis for the range of N. The corresponding embeddings are then given by UTX and VTY.2 Interestingly, we can equally well view xi as an instrumental variable for yi: we simultaneously find consistent embeddings of both xi and yi, using each to unbias the other."}, {"heading": "3.2. Bias in Nonlinear Models", "text": "We now extend the analysis of Section 3.1 to nonlinear models. We assume noisy observations xi = f(zi)+ i, where zi is the desired low-dimensional latent variable, i is an i.i.d. noise term, and f is a smooth function with smooth inverse (so that f(zi) lies on a manifold). Our goal is to recover f and zi up to identifiability.\nKernel PCA (Sec. 2.1) is a common approach to this problem. In the (restrictive) realizable case, kernel PCA gets the right answer: that is, suppose that zi has dimension k, that i has zero variance, and that we have at least k independent samples. And, suppose that \u03c6(f(z)) is a linear function of z. Then, the Gram matrix or the covariance \u201cmatrix\u201d will have rank k, and we can reconstruct a basis for the range of \u03c6 \u25e6\n2Several spectral decompositions of cross-covariance matrices can be viewed as special cases of two-subspace PCA that involve transforming the variables xi and yi before applying a singular value decomposition. Two popular examples are reduced-rank regression (Reinsel & Velu, 1998) and canonical correlation analysis (Hotelling, 1935).\nf from the top k eigenvectors of the Gram matrix. (Similarly, if \u03c6 \u25e6 f is near linear and the variance of i is small, we can expect kernel PCA to work well, if not perfectly.)\nHowever, just as PCA recovers a biased answer when the variance of i is nonzero, kernel PCA will also recover a biased answer under noise, even in the limit of infinite data. The bias of kernel PCA follows immediately from the example at the beginning of Section 3: if we use a linear kernel, kernel PCA will simply reproduce the bias of ordinary PCA.\n3.2.1.Instrumental Variables in Hilbert Space\nBy analogy to two-subspace PCA, a natural generalization of kernel PCA is two-subspace kernel PCA, which we can accomplish via a kernelized SVD of a cross-covariance operator in Hilbert space. Given a joint distribution P[X,Y ] over two variables X on X and Y on Y, with feature maps \u03c6 and \u03c5 (corresponding to kernels Kx and Ky), the cross-covariance operator \u03a3XY is E[\u03c6(x) \u2297 \u03c5(y)]. The cross-covariance operator reduces to an ordinary cross-covariance matrix in the finite-dimensional case; in the infinite-dimensional case, it can be viewed as a kernel mean map descriptor (Smola et al., 2007) for the joint distribution P[X,Y ]. The concept of a cross-covariance operator allows us to extend the methods of instrumental variables to infinite dimensional RKHSs. In our example above, a good instrumental variable yi is a different (noisy) view of the same underlying latent variable: yi = g(zi) + \u03b6i for some smoothly invertible function g and i.i.d. zero-mean noise term \u03b6i."}, {"heading": "3.2.2. Two-subspace PCA in RKHSs", "text": "We proceed now to derive the kernel SVD for a crosscovariance operator.3 Conceptually, our inputs are \u201cmatrices\u201d \u03a6 and \u03a5 whose columns are respectively \u03c6(xi) and \u03c5(yi). The centered empirical covariance operator is then \u03a3\u0302XY = 1 n (\u03a6H)(\u03a5H) T. The goal of the kernel SVD is then to factor \u03a3\u0302XY so that we can recover the desired bases for \u03c6(xi) and \u03c5(yi). Unfortunately, this conceptual algorithm is impractical, since \u03a3\u0302XY can be high- or infinite-dimensional. So, instead, we perform an SVD on the covariance operator in Hilbert space via a trick analogous to kernel PCA.\nTo understand SVD in general Hilbert spaces, we start by looking at a Gram matrix formulation of finite dimensional SVD. Recall that the singular values of \u03a3\u0302XY = 1 n (XH)(YH) T are the square roots of the\n3The kernel SVD algorithm previously appeared as an intermediate step in (Song et al., 2010; Fukumizu et al., 2005); here we give a more complete description.\neigenvalues of \u03a3\u0302XY \u03a3\u0302Y X (where \u03a3\u0302Y X = \u03a3\u0302 T XY ), and the left singular vectors are defined to be the corresponding eigenvectors. We can find identical eigenvectors and eigenvalues using only centered Gram matrices CX = 1 n (XH) T(XH) and CY = 1 n (YH)\nT(YH). Let vi be a right eigenvector of CY CX , so that CY CXvi = \u03bbivi. Premultiplying by (XH) yields\n1\nn2 (XH)(YH)T(YH)(XH)T(XH)vi = \u03bbi(XH)vi\nand regrouping terms gives us \u03a3\u0302XY \u03a3\u0302Y Xwi = \u03bbiwi where wi = (XH)vi. So, \u03bbi is an eigenvalue of \u03a3\u0302XY \u03a3\u0302Y X , \u221a \u03bbi is a singular value of \u03a3\u0302XY , and (XH)vi\u03bb \u22121/2 i is the corresponding unit length left singular vector. An analogous argument shows that, if w\u2032i is a unit-length right singular vector of \u03a3\u0302XY , then w\u2032i = (YH)v \u2032 i\u03bb \u22121/2 i , where v \u2032 i is a unit-length left eigenvector of CY CX .\nThis machinery allows us to solve the two-subspace kernel PCA problem by computing the singular values of the empirical covariance operator \u03a3\u0302XY . We define GX and GY to be the Gram matrices whose elements are Kx(xi,xj) and Ky(yi,yj) respectively, and then compute the eigendecomposition of CY CX = (HGY H)(HGXH). This method avoids any computations in infinite-dimensional spaces; and, it gives us compact representations of the left and right singular vectors.\nUnder appropriate assumptions, we can show that the SVD of the empirical cross-covariance operator \u03a3\u0302XY = 1 n\u03a6H\u03a5\nT converges to the desired value. Suppose that E[\u03c6(xi) | zi] is a linear function of zi, and similarly, that E[\u03c5(yi) | zi] is a linear function of zi.\n4 The noise terms \u03c6(xi) \u2212 E[\u03c6(xi) | zi] and \u03c5(yi) \u2212 E[\u03c5(yi) | zi] are by definition zero-mean; and they are independent of each other, since the first depends only on i and the second only on \u03b6i. So, the noise terms cancel out, and the expectation of \u03a3\u0302XY is the true covariance \u03a3XY . If we additionally assume that the noise terms have finite variance, the product-RKHS norm of the error \u03a3\u0302XY \u2212 \u03a3XY vanishes as n\u2192\u221e.\nThe remainder of the proof follows from the proof of Theorem 1 in (Song et al., 2010) (the convergence of the empirical estimator of the kernel covariance oper-\n4The assumption of linearity is restrictive, but appears necessary: in order to learn a representation of a manifold using factorization-based methods, we need to pick a kernel which flattens out the manifold into a subspace. This is why kernel eigenmap methods are generally more successful than plain kernel PCA: by learning an appropriate kernel, they are able to adapt their nonlinearity to the shape of the target manifold.\nator). In particular, the top k left singular vectors of \u03a3\u0302XY converge to a basis for the range of E[\u03c6(xi) | zi] (considered as a function of zi); similarly, the top right singular vectors of \u03a3\u0302XY converge to a basis for the range of E[\u03c5(yi) | zi]."}, {"heading": "4. Two-Manifold Problems", "text": "Now that we have extended the instrumental variable idea to RKHSs, we can also expand the scope of manifold learning to two-manifold problems, where we want to simultaneously learn two manifolds for two covarying lists of observations, each corrupted by uncorrelated noise.5 The idea is simple: we view manifold learners as constructing Gram matrices as in Sec. 2.2, then apply the RKHS instrumental variable idea of Sec. 3. As we will see, this procedure allows us to regain good performance when observations are noisy.\nSuppose we are given two set of observations residing on (or near) two different manifolds: x1, . . . ,xn \u2208 Rd1 on MX and y1, . . . ,yn \u2208 Rd2 on MY . Further suppose that both xi and yi are noisy functions of a latent\n5The uncorrelated noise assumption is extremely mild: if some latent variable causes correlated changes in our measurements on the two manifolds, then we are making the definition that it is part of the desired signal to be recovered. No other definition seems reasonable: if there is no difference in statistical behavior between signal and noise, then it is impossible to use a statistical method to separate signal from noise.\nAlgorithm 1 Instrumental Eigenmaps In: n i.i.d. pairs of observations {xi,yi}ni=1 Out: embeddings EX and EY 1: Compute Gram matrices: GX and GY from x1:n\nand y1:n respectively, using for example LE. 2: Compute centered Gram matrices:\nCX = HGXH and CY = HGY H 3: Perform a singular value decomposition and trun-\ncate the top k singular values: \u3008U,\u039b,VT\u3009 = SVD(CXCY , k)\n4: Find the embeddings from the singular values:\nEX = U1:k\u039b 1/2 1:k and EY = V1:k\u039b 1/2 1:k\nvariable zi, itself residing on a latent k-dimensional manifoldMZ : xi = f(zi)+ i and yi = g(zi)+\u03b6i. We assume that the functions f and g are smooth, so that f(zi) and g(zi) trace out submanifolds f(MZ) \u2286MX and g(MZ) \u2286MY . We further assume that the noise terms i and \u03b6i move xi and yi within their respective manifolds MX and MY : this assumption is without loss of generality, since we can can always increase the dimension of the manifolds MX and MY to allow an arbitrary noise term. See Figure 1 for an example.\nIf the variance of the noise terms i and \u03b6i is too high, or if MX and MY are higher-dimensional than the latent MZ manifold (i.e., if the noise terms move xi and yi away from f(MZ) and g(MZ)), then it may be difficult to reconstruct f(MZ) or g(MZ) separately from xi or yi. Our goal, therefore, is to use xi and yi together to reconstruct both manifolds simultaneously: the extra information from the correspondence between xi and yi will make up for noise, allowing success in the two-manifold problem where the individual one-manifold problems are intractable.\nGiven samples of n i.i.d. pairs {xi,yi}ni=1 from two manifolds, we propose a two-step spectral learning algorithm for two-manifold problems: first, use either a given kernel or an ordinary one-manifold algorithm such as LE or LLE to compute centered Gram matrices CX and CY from xi and yi separately. Second, use kernel SVD to recover the embedding of points in MZ . The procedure, called instrumental eigenmaps, is summarized in Algorithm 1.\nAs shown in Figure 2, computing eigenvalues of CXCY instead of CX or CY alone alters the eigensystem: it promotes directions within each individual learned manifold that are useful for predicting coordinates on the other learned manifold, and demotes directions that are not useful. This effect strengthens our ability to recover relevant dimensions in the face of noise."}, {"heading": "5. Two-Manifold Detailed Example: Nonlinear System Identification", "text": "A fundamental problem in machine learning and robotics is dynamical system identification. This task requires two related subtasks: 1) learning a low dimensional state space, which is often known to lie on a manifold ; and 2) learning the system dynamics.\nWe propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010). Here, we focus on a specific example: we show how to combine HSE-HMMs (Song et al., 2010), a powerful nonparametric approach to system identification, with manifold learning. We demonstrate that the resulting manifold HSE-HMM can outperform standard HSE-HMMs (and many other wellknown methods for learning dynamical systems): the manifold HSE-HMM accurately discovers a curved low-dimensional manifold which contains the state space, while other methods discover only a (potentially much higher-dimensional) subspace which contains this manifold."}, {"heading": "5.1. Hilbert Space Embeddings of HMMs", "text": "The key idea behind spectral learning of dynamical systems is that a good latent state is one that lets us\npredict the future. HSE-HMMs implement this idea by finding a low-dimensional embedding of the conditional probability distribution of sequences of future observations, and using the embedding coordinates as state. Song et al. (2010) suggest finding this lowdimensional state space as a subspace of an infinite dimensional RKHS.\nIntuitively, we might think that we could find the best state space by performing PCA or kernel PCA of sequences of future observations. That is, we would sample n sequences of future observations x1, . . . ,xn \u2208 Rd1 from a dynamical system. We would then construct a Gram matrix GX , whose (i, j) element is Kx(xi,xj). Finally, we would find the eigendecomposition of the centered Gram matrix CX = HGXH as in Section 2.1. The resulting embedding coordinates would be tuned to predict future observations well, and so could be viewed as a good state space. However, the state space found by kernel PCA is biased : it typically includes noise, information that cannot be predicted from past observations. We would like instead to find a low dimensional state space that is uncorrelated with the noise in the future observations.\nSo, in addition to sampling sequences of future observations, we sample corresponding sequences of past observations y1, . . . ,yn \u2208 Rd2 : sequence yi ends at time ti \u2212 1. We view features of the past as instrumental variables to unbias the future. We therefore construct a Gram matrix GY , whose (i, j) element is Ky(yi,yj). From GY we construct the centered Gram matrix CY = HGY H. Finally, we identify the state space using a kernel SVD as in Section 3.2.2: \u3008U,\u039b,VT\u3009 = SVD(CXCY , k). The left singular \u201cvectors\u201d (reconstructed from U as in Section 3.2.2) now identify a subspace in which the system evolves. From this subspace, we can proceed to identify the parameters of the system as in Song et al. (2010)."}, {"heading": "5.2. Manifold HSE-HMMs", "text": "In contrast with ordinary HSE-HMMs, we are interested in modeling a dynamical system whose state space lies on a low-dimensional manifold, even if this manifold is curved to occupy a higher-dimensional subspace (an example is given in Section 5.3, below). We want to use this additional knowledge to constrain the learning algorithm and produce a more accurate model for a given amount of training data. To do so, we replace the kernel SVD by a two-manifold method. That is, we learn centered Gram matrices CX and CY for the future and past observations, using a manifold method like LE or LLE (see Section 2.2). Then we apply a SVD to CXCY in order to recover the latent state space."}, {"heading": "5.3. Slotcar: A Real-World Dynamical System", "text": "To evaluate two-manifold HSE-HMMs we look at the problem of tracking and predicting the position of a slotcar with attached inertial measurement unit (IMU) racing around a track. Figure 3(A) shows setup.\nWe collected 3000 successive observations of 3D acceleration and angular velocity at 10 Hz while the slot car circled the track controlled by a constant policy (with varying speeds). The goal was to learn a dynamical model of the noisy IMU data, and, after filtering, to predict current and future 2-dimensional locations. We used the first 2000 data points as training data, and held out the last 500 data points for testing the learned models. We trained four models, and evaluated these models based on prediction accuracy, and, where appropriate, the learned latent state.\nFirst, we trained a 20-dimensional embedded HMM with the spectral algorithm of Song et al. (2010), using sequences of 150 consecutive observations and Gaussian RBF kernels. Second, we trained a similar 20- dimensional embedded HMM with normalized LE kernels. (Normalized LE differs from LE by utilizing the normalized graph Laplacian instead of the standard graph Laplacian.) The number of nearest neighbors was selected to be 50, and the other parameters were set to be identical to the first model. (So, the only difference is that the first model performs a kernel SVD, while the second model solves a two-manifold problem.) Third, we trained a 20-dimensional Kalman filter using the N4SID algorithm (Van Overschee & De Moor, 1996) with Hankel matrices of 150 time steps; and finally, we learned a 20-state HMM (with 400 levels of discretization for observations) via the EM algorithm.\nWe compared the learned state spaces of the first three models. These models differ mainly in their kernel: Gaussian RBF, learned manifold from normalized LE, or linear. As a test, we tried to reconstruct the 2- dimensional locations of the car (collected from an overhead camera, and not used in learning the dynamical systems) from each of the three latent state spaces: the more accurate the learned state space, the better we expect to be able to reconstruct the locations. Results are shown in Figure 3(B).\nFinally we examined the prediction accuracy of each model. We performed filtering for different extents t1 = 100, . . . , 350, then predicted the car location for a further t2 steps in the future, for t2 = 1, . . . , 100. The root-mean-squared error of this prediction in the 2-dimensional location space is plotted in Figure 3(C). The Manifold HMM learned by the method detailed in Section 5.2 consistently yields lower prediction error\nfor the duration of the prediction horizon."}, {"heading": "6. Related Work", "text": "While preparing this manuscript, we learned of the simultaneous and independent work of Mahadevan et al. (2011). That paper defines one particular twomanifold algorithm, maximum covariance unfolding (MCU). We believe the current paper will help to elucidate why two-manifold methods like MCU work well.\nA similar problem to the two-manifold problem is manifold alignment (Ham et al., 2005; Wang & Mahadevan, 2009), which builds connections between two or more data sets by aligning their underlying manifolds. Our aim is different: we assume paired data, where manifold alignments do not; and, we focus on learning algorithms that simultaneously discover manifold structure and connections between manifolds (as provided by, e.g., a top-level learning problem defined between two manifolds).\nInterconnected dimensionality reduction has been explored before in sufficient dimension reduction (SDR) (Li, 1991; Cook & Yin, 2001; Fukumizu et al., 2004). In SDR, the goal is to find a linear subspace of covariates xi that makes response vectors yi conditionally independent of the xis. The formulation is in terms of conditional independence. Unfortunately, the solution to SDR problems usually requires a difficult nonlinear non-convex optimization. A related method is manifold kernel dimension reduction (Nilsson et al., 2007), which finds an embedding of covariates xi using a kernel eigenmap method, and then attempts to find a linear transformation of some of the dimensions of the embedded points to predict response variables yi. The response variables are constrained to be linear in\nthe manifold, so the problem is quite different from a two-manifold problem."}, {"heading": "7. Conclusion", "text": "In this paper we study two-manifold problems, where two sets of corresponding data points, generated from a single latent manifold and corrupted by noise, lie on or near two different higher dimensional manifolds. We design algorithms by relating two-manifold problems to cross-covariance operators in RKHSs, and show that these algorithms result in a significant improvement over standard manifold learning approaches in the presence of noise. This is an appealing result: manifold learning algorithms typically assume that observations are (close to) noiseless, an assumption that is rarely satisfied in practice.\nFurthermore, we demonstrate the utility of twomanifold problems by extending a recent dynamical system identification algorithm to learn a system with a state space that lies on a manifold. The resulting algorithm learns a model that outperforms the current state of the art in predictive accuracy. To our knowledge this is the first combination of system identification and manifold learning that accurately identifies a latent time series manifold and is competitive with the best system identification algorithms at learning accurate predictive models."}, {"heading": "Acknowledgements", "text": "Byron Boots and Geoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052. Byron Boots was supported by the NSF under grant number EEEC-0540865."}], "references": [{"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Belkin", "Mikhail", "Niyogi", "Partha"], "venue": "Neural Computation,", "citeRegEx": "Belkin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2002}, {"title": "Predictive state temporal difference learning", "author": ["Boots", "Byron", "Gordon", "Geoff"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Boots et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2010}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J"], "venue": "In Proceedings of Robotics: Science and Systems VI,", "citeRegEx": "Boots et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2010}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["Boots", "Byron", "Siddiqi", "Sajid", "Gordon", "Geoffrey"], "venue": "In Proceedings of the 25th National Conference on Artificial Intelligence (AAAI2011),", "citeRegEx": "Boots et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "Linear leastsquares algorithms for temporal difference learning", "author": ["Bradtke", "Steven J", "Barto", "Andrew G"], "venue": "In Machine Learning,", "citeRegEx": "Bradtke et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bradtke et al\\.", "year": 1996}, {"title": "Neighborhood smoothing embedding for noisy manifold learning", "author": ["Chen", "Guisheng", "Yin", "Junsong", "Li", "Deyi"], "venue": "In GrC, pp", "citeRegEx": "Chen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2008}, {"title": "Theory and methods: Special invited paper: Dimension reduction and visualization in discriminant analysis (with discussion)", "author": ["Cook", "R. Dennis", "Yin", "Xiangrong"], "venue": "Australian and New Zealand Journal of Statistics,", "citeRegEx": "Cook et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cook et al\\.", "year": 2001}, {"title": "Consistency of kernel canonical correlation analysis", "author": ["K. Fukumizu", "F. Bach", "A. Gretton"], "venue": "Technical Report 942,", "citeRegEx": "Fukumizu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2005}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces", "author": ["Fukumizu", "Kenji", "Bach", "Francis R", "Jordan", "Michael I", "Williams", "Chris"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "A kernel view of the dimensionality reduction of manifolds", "author": ["Ham", "Jihun", "Lee", "Daniel D", "Mika", "Sebastian", "Schlkopf", "Bernhard"], "venue": null, "citeRegEx": "Ham et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2003}, {"title": "Semisupervised alignment of manifolds", "author": ["Ham", "Jihun", "Lee", "Daniel", "Saul", "Lawrence"], "venue": "10th International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Ham et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ham et al\\.", "year": 2005}, {"title": "The most predictable criterion", "author": ["Hotelling", "Harold"], "venue": "Journal of Educational Psychology,", "citeRegEx": "Hotelling and Harold.,? \\Q1935\\E", "shortCiteRegEx": "Hotelling and Harold.", "year": 1935}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Hsu", "Daniel", "Kakade", "Sham", "Zhang", "Tong"], "venue": "In COLT,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Principal component analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "Jolliffe,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe", "year": 2002}, {"title": "Spectral dimensionality reduction via maximum entropy", "author": ["Lawrence", "Neil D"], "venue": "In Proc. AISTATS,", "citeRegEx": "Lawrence and D.,? \\Q2011\\E", "shortCiteRegEx": "Lawrence and D.", "year": 2011}, {"title": "Sliced inverse regression for dimension reduction", "author": ["Li", "Ker-Chau"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Li and Ker.Chau.,? \\Q1991\\E", "shortCiteRegEx": "Li and Ker.Chau.", "year": 1991}, {"title": "Maximum covariance unfolding: Manifold learning for bimodal data", "author": ["Mahadevan", "Vijay", "Wong", "Chi Wah", "Pereira", "Jose Costa", "Liu", "Tom", "Vasconcelos", "Nuno", "Saul", "Lawrence"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mahadevan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mahadevan et al\\.", "year": 2011}, {"title": "Regression on manifolds using kernel dimension reduction", "author": ["Nilsson", "Jens", "Sha", "Fei", "Jordan", "Michael I"], "venue": "In ICML, pp", "citeRegEx": "Nilsson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nilsson et al\\.", "year": 2007}, {"title": "Causality: models, reasoning, and inference", "author": ["Pearl", "Judea"], "venue": null, "citeRegEx": "Pearl and Judea.,? \\Q2000\\E", "shortCiteRegEx": "Pearl and Judea.", "year": 2000}, {"title": "Multivariate Reduced-rank Regression: Theory and Applications", "author": ["Reinsel", "Gregory C", "Velu", "Rajabather Palani"], "venue": null, "citeRegEx": "Reinsel et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Reinsel et al\\.", "year": 1998}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "Sam T", "Saul", "Lawrence K"], "venue": null, "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["Sch\u00f6lkopf", "Bernhard", "Smola", "Alex J", "M\u00fcller", "KlausRobert"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Reduced-rank hidden Markov models", "author": ["Siddiqi", "Sajid", "Boots", "Byron", "Gordon", "Geoffrey J"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "Siddiqi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Siddiqi et al\\.", "year": 2010}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "Algorithmic Learning Theory, Lecture Notes on Computer Science. Springer,", "citeRegEx": "Smola et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Smola et al\\.", "year": 2007}, {"title": "Hilbert space embeddings of hidden Markov models", "author": ["L. Song", "B. Boots", "S.M. Siddiqi", "G.J. Gordon", "A.J. Smola"], "venue": "In Proc. 27th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Tenenbaum", "Joshua B", "Silva", "Vin De", "Langford", "John"], "venue": "Science, 290:2319\u20132323,", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Subspace Identification for Linear Systems: Theory, Implementation, Applications", "author": ["P. Van Overschee", "B. De Moor"], "venue": null, "citeRegEx": "Overschee and Moor,? \\Q1996\\E", "shortCiteRegEx": "Overschee and Moor", "year": 1996}, {"title": "A general framework for manifold alignment", "author": ["Wang", "Chang", "Mahadevan", "Sridhar"], "venue": "In Proc. AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2009}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["Weinberger", "Kilian Q", "Sha", "Fei", "Saul", "Lawrence K"], "venue": "Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "Weinberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2004}, {"title": "Robust local tangent space alignment via iterative weighted PCA", "author": ["Zhan", "Yubin", "Yin", "Jianping"], "venue": null, "citeRegEx": "Zhan et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Zhan et al\\.", "year": 1985}], "referenceMentions": [{"referenceID": 25, "context": "These kernel eigenmap methods include Isomap (Tenenbaum et al., 2000), Locally Linear Embedding (LLE) (Roweis & Saul, 2000), Laplacian Eigenmaps (LE) (Belkin & Niyogi, 2002), Maximum Variance Unfolding (MVU) (Weinberger et al.", "startOffset": 45, "endOffset": 69}, {"referenceID": 28, "context": ", 2000), Locally Linear Embedding (LLE) (Roweis & Saul, 2000), Laplacian Eigenmaps (LE) (Belkin & Niyogi, 2002), Maximum Variance Unfolding (MVU) (Weinberger et al., 2004), and Maximum Entropy Unfolding (MEU) (Lawrence, 2011).", "startOffset": 146, "endOffset": 171}, {"referenceID": 5, "context": "Several authors have attacked the problem of learning manifolds in the presence of noise using methods like neighborhood smoothing (Chen et al., 2008) and robust principal components analysis (Zhan & Yin, 2009; 2011), with some success when noise is limited.", "startOffset": 131, "endOffset": 150}, {"referenceID": 13, "context": "Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression (Pearl, 2000), principal component analysis (PCA) (Jolliffe, 2002), and temporal difference learning (Bradtke & Barto, 1996).", "startOffset": 206, "endOffset": 222}, {"referenceID": 12, "context": "Subspace identification approaches to learning nonlinear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinite-dimensional) covariance operators (Hsu et al., 2009; Boots et al., 2010; Song et al., 2010).", "startOffset": 209, "endOffset": 266}, {"referenceID": 1, "context": "Subspace identification approaches to learning nonlinear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinite-dimensional) covariance operators (Hsu et al., 2009; Boots et al., 2010; Song et al., 2010).", "startOffset": 209, "endOffset": 266}, {"referenceID": 24, "context": "Subspace identification approaches to learning nonlinear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinite-dimensional) covariance operators (Hsu et al., 2009; Boots et al., 2010; Song et al., 2010).", "startOffset": 209, "endOffset": 266}, {"referenceID": 21, "context": "Kernel PCA (Sch\u00f6lkopf et al., 1998) generalizes PCA to high- or infinite-dimensional input data, represented implicitly using a reproducing-kernel Hilbert space (RKHS).", "startOffset": 11, "endOffset": 35}, {"referenceID": 21, "context": "\u03a3\u0302XX ; the eigenvectors of \u03a3\u0302XX are \u03a6Hvi\u03bb \u22121/2 i , where (\u03bbi,vi) are the eigenpairs of C (Sch\u00f6lkopf et al., 1998).", "startOffset": 89, "endOffset": 113}, {"referenceID": 9, "context": "Interestingly, these algorithms can be viewed as special cases of kernel PCA where the Gram matrix G is constructed over the finite domain of the training data in a particular way (Ham et al., 2003).", "startOffset": 180, "endOffset": 198}, {"referenceID": 9, "context": ") To relate LE to kernel PCA, Ham et al. (2003) showed that one can build a Gram matrix from L, G = L\u2020; the LE embedding is given by the top k eigenvectors (and optionally eigenvalues) of G.", "startOffset": 30, "endOffset": 48}, {"referenceID": 13, "context": "This method finds a statistically consistent solution through the use of an instrumental variable (Pearl, 2000; Jolliffe, 2002), an observation yi that is correlated with the true latent variables, but uncorrelated with the noise in xi.", "startOffset": 98, "endOffset": 127}, {"referenceID": 23, "context": "The cross-covariance operator reduces to an ordinary cross-covariance matrix in the finite-dimensional case; in the infinite-dimensional case, it can be viewed as a kernel mean map descriptor (Smola et al., 2007) for the joint distribution P[X,Y ].", "startOffset": 192, "endOffset": 212}, {"referenceID": 24, "context": "The kernel SVD algorithm previously appeared as an intermediate step in (Song et al., 2010; Fukumizu et al., 2005); here we give a more complete description.", "startOffset": 72, "endOffset": 114}, {"referenceID": 7, "context": "The kernel SVD algorithm previously appeared as an intermediate step in (Song et al., 2010; Fukumizu et al., 2005); here we give a more complete description.", "startOffset": 72, "endOffset": 114}, {"referenceID": 24, "context": "The remainder of the proof follows from the proof of Theorem 1 in (Song et al., 2010) (the convergence of the empirical estimator of the kernel covariance oper-", "startOffset": 66, "endOffset": 85}, {"referenceID": 24, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 22, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 3, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 12, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 1, "context": "We propose tackling this problem by combining twomanifold methods (for task 1) with spectral learning algorithms for nonlinear dynamical systems (for task 2) (Song et al., 2010; Siddiqi et al., 2010; Boots et al., 2011; Boots & Gordon, 2010; Hsu et al., 2009; Boots et al., 2010).", "startOffset": 158, "endOffset": 279}, {"referenceID": 24, "context": "Here, we focus on a specific example: we show how to combine HSE-HMMs (Song et al., 2010), a powerful nonparametric approach to system identification, with manifold learning.", "startOffset": 70, "endOffset": 89}, {"referenceID": 24, "context": "Song et al. (2010) suggest finding this lowdimensional state space as a subspace of an infinite dimensional RKHS.", "startOffset": 0, "endOffset": 19}, {"referenceID": 24, "context": "From this subspace, we can proceed to identify the parameters of the system as in Song et al. (2010).", "startOffset": 82, "endOffset": 101}, {"referenceID": 24, "context": "First, we trained a 20-dimensional embedded HMM with the spectral algorithm of Song et al. (2010), using sequences of 150 consecutive observations and Gaussian RBF kernels.", "startOffset": 79, "endOffset": 98}, {"referenceID": 16, "context": "While preparing this manuscript, we learned of the simultaneous and independent work of Mahadevan et al. (2011). That paper defines one particular twomanifold algorithm, maximum covariance unfolding (MCU).", "startOffset": 88, "endOffset": 112}, {"referenceID": 10, "context": "A similar problem to the two-manifold problem is manifold alignment (Ham et al., 2005; Wang & Mahadevan, 2009), which builds connections between two or more data sets by aligning their underlying manifolds.", "startOffset": 68, "endOffset": 110}, {"referenceID": 8, "context": "Interconnected dimensionality reduction has been explored before in sufficient dimension reduction (SDR) (Li, 1991; Cook & Yin, 2001; Fukumizu et al., 2004).", "startOffset": 105, "endOffset": 156}, {"referenceID": 17, "context": "A related method is manifold kernel dimension reduction (Nilsson et al., 2007), which finds an embedding of covariates xi using a kernel eigenmap method, and then attempts to find a linear transformation of some of the dimensions of the embedded points to predict response variables yi.", "startOffset": 56, "endOffset": 78}], "year": 2012, "abstractText": "Recently, there has been much interest in spectral approaches to learning manifolds\u2014 so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space, and discuss when two-manifold problems are useful. Finally, we demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.", "creator": "LaTeX with hyperref package"}}}