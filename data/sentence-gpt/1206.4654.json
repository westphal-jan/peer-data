{"id": "1206.4654", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Generalized Loop Correction Method for Approximate Inference in Graphical Models", "abstract": "Belief Propagation (BP) is one of the most popular methods for inference in probabilistic graphical models. BP is guaranteed to return the correct answer for tree structures, but can be incorrect or non-convergent for loopy graphical models. Recently, several new approximate inference algorithms based on cavity distribution have been proposed. These methods can account for the effect of loops by incorporating the dependency between BP messages. Alternatively, region-based approximations (that lead to methods such as Generalized Belief Propagation) improve upon BP by considering interactions within small clusters of variables, thus taking small loops within these clusters into account. This paper introduces an approach, Generalized Loop Correction (GLC), that benefits from both of these types of loop correction. We show how GLC relates to these two families of inference methods, then provide empirical evidence that GLC works effectively in general, and can be significantly more accurate than both correction schemes. This paper aims to demonstrate the properties of gLC, in particular by introducing two techniques that are used in the optimization of a graph of data to perform the first inference using the standard Gaussian distribution. These results will serve as a demonstration of the utility of GLC as a simple distribution.\n\n\n\nP.S. If you have any questions, please feel free to reach me at [email protected].", "histories": [["v1", "Mon, 18 Jun 2012 15:25:04 GMT  (543kb)", "http://arxiv.org/abs/1206.4654v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["siamak ravanbakhsh", "chun-nam yu", "russell greiner"], "accepted": true, "id": "1206.4654"}, "pdf": {"name": "1206.4654.pdf", "metadata": {"source": "META", "title": "A Generalized Loop Correction Method  for Approximate Inference in Graphical Models", "authors": ["Siamak Ravanbakhsh", "Russell Greiner"], "emails": ["mravanba@ualberta.ca", "chunnam@ualberta.ca", "rgreiner@ualberta.ca"], "sections": [{"heading": "1. Introduction", "text": "Many real-world applications require probabilistic inference from some known probabilistic model (Koller & Friedman, 2009). This paper will use probabilistic graphical models, focusing on factor graphs (Kschischang et al., 1998), that can represent both Markov Networks and Bayesian Networks. The basic challenge of such inference is marginalization (or maxmarginalization) over a large number of variables. For discrete variables, computing the exact solutions is Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nNP-hard, typically involving a computation that is exponential in the number of variables.\nWhen the conditional dependencies of the variables form a tree structure (i.e., no loops), this exact inference is tractable, and can be done by a message passing procedure, Belief Propagation (BP) (Pearl, 1988). The Loopy Belief Propagation (LBP) system applies BP repeatedly to graph structures that are not trees (called \u201cloopy graphs\u201d); however, this provides only an approximately correct solution (when it converges).\nLBP is related to the Bethe approximation to free energy (Heskes, 2003), which is the basis for minimization of more sophisticated energy approximations and provably convergent methods (Yedidia et al., 2005; Heskes, 2006; Yuille, 2002). A representative class of energy approximations is the region-graph methods (Yedidia et al., 2005), which deal with a set of connected variables (called \u201cregions\u201d); these methods subsume both the Cluster Variation Method (CVM) (Pelizzola, 2005; Kikuchi, 1951) and the Junction Graph Method (Aji & McEliece, 2001). Such region-based methods deal with the short loops of the graph by incorporating them into overlapping regions (see Figure 1(a)), and perform exact inference over each region. Note a valid region-based methods is exact if its region graph has no loops.\nA different class of algorithms, loop correction methods, tackles the problem of inference in loopy graphical models by considering the cavity distribution of variables. A cavity distribution is defined as the marginal distribution on Markov blanket of a single (or a cluster of) variable(s), after removing all factors that depend on those initial variables. Figure 1(b) illustrates cavity distribution, and also shows that the cavity variables can interact. The key observation in these methods is that, by removing a variable xi in a graphical model, we break all the loops that involve the variable xi, resulting in a simplified problem of finding\nthe cavity distribution. The marginals around xi can then be recovered by considering the cavity distribution and its interaction with xi. This is the basis for the loop correction schemes by Montanari & Rizzo\u2019s (2005) on pairwise dependencies over binary variables, and also Mooij & Kappen\u2019s (2007) extension to general factor graphs \u2013 called Loop Corrected Belief Propagation (LCBP).\nThis paper defines a new algorithm for probabilistic inference, called Generalized Loop Correction (GLC), that uses a more general form of cavity distribution, defined over regions, and also a novel message passing scheme between these regions that uses cavity distributions to correct the types of loops that result from exact inference over each region. GLC\u2019s combination of loop corrections is well motivated, as regionbased methods can deal effectively with short loops in the graph, and the approximate cavity distribution is known to produce superior results when dealing with long influencial loops (Mooij & Kappen, 2007).\nIn its simplest form, GLC produces update equations similar to LCBP\u2019s; indeed, under a mild assumption, GLC reduces to LCBP for pairwise factors. In its general form, when not provided with information on cavity variable interactions, GLC produces results similar to region-based methods. We theoretically establish the relation between GLC and region-based approxi-\nmations, for a limited setting.\nSection 2 explains the notation, factor graph representation and preliminaries for GLC. Section 3 introduces a simple version of GLC that works with regions that partition the set of variables; followed by its extension to the more general algorithm. Section 4 presents empirical results, comparing our GLC against other approaches."}, {"heading": "2. Framework", "text": ""}, {"heading": "2.1. Notation", "text": "Let X = {X1, X2, . . . , XN} be a set of N discretevalued random variables, where Xi \u2208 Xi. Suppose their joint probability distribution factorizes into a product of non-negative functions:\nP (X = x) := 1\nZ \u220f I\u2208F \u03c8I(xI)\nwhere each I \u2286 {1, 2, . . . , N} is a subset of the variable indices, and xI = {xi | i \u2208 I} is the set of values in x indexed by the subset I. Each factor \u03c8I : \u220f i\u2208I Xi \u2192 [0,\u221e) is a non-negative function, and F is the collection of indexing subsets I for all the factors \u03c8I . Below we will use the term \u201cfactor\u201d interchangeably with the function \u03c8I and subset I, and the term \u201cvariable\u201d interchangeably for the value xi and index i. Here Z is the partition function.\nThis model can be conveniently represented as a bipartite graph, called the factor graph (Kschischang et al., 1998), which includes two sets of nodes: variable nodes xi, and factor nodes \u03c8I . A variable node xi is connected to a factor node \u03c8I if and only if i \u2208 I. We use the notation N(i) to denote the neighbors of variable xi in the factor graph \u2013 i.e., the set of factors defined by N(i) := {I \u2208 F | i \u2208 I}. To illustrate, using Figure 1(a): N(j) = {I, T, S} and T = {j, s, w}.\nWe use the shorthand \u03c8A(x) := \u220f\nI\u2208A(xI) to denote the product of factors in a set of factors A. For marginalizing all possible values of x except the ith variable, we define the notation:\u2211\nx\\i\nf(x) := \u2211\nxj\u2208Xj ,j 6=i\nf(x).\nSimilarly for a set of variables r, we use the notation\u2211 x\\r to denote marginalization of all variables apart from those in r."}, {"heading": "2.2. Generalized Cavity Distribution", "text": "The notion of cavity distribution is borrowed from socalled cavity methods from statistical physics (Me\u0301zard & Montanari, 2009), and has been used in analysis and optimization of important combinatorial problems (Me\u0301zard et al., 2002; Braunstein et al., 2002). The basic idea is to make a cavity by removing a variable xi along with all the factors around it, from the factor graph (Figure 1(b)). We will use a more general notion of regional cavity, around a region.\nDefinition A cavity region is a subset of variables r \u2286 {1, . . . , N} that are connected by a set of factors \u2013 i.e., the set of variable nodes r and the associated factors N(r) := {N(i) | i \u2208 r} forms a connected component on the factor graph.\nFor example in Figure 1(a), the variables indexed by r1 = {j, k, s} define a cavity region with factors N(r1) = {I, T, Y, S,W,K}\nRemark A \u201ccavity region\u201d is different from common notion of region in region-graph methods, in that a cavity region includes all factors in N(r) (and nothing more), while common regions allow a factor I to be a part of a region only if I \u2286 r.\nThe notation \u2295r := {i \u2208 I | I \u2208 N(r)} denotes the cavity region r with its surrounding variables, and r := \u2295r \\ r denotes just the perimeter of the cavity region r. In Figure 1(c), the dotted circles show the indices r1 = {o, i,m, t, u, v, w} and their union with the pale circles defines \u2295r1.\nDefinition The Cavity Distribution, for cavity region\nr, is defined over the variables indexed by r, as: P \\r(x r) \u221d \u2211 x\\ r \u03c8F\\N(r)(x) = \u2211 x\\ r \u220f I /\u2208N(r) \u03c8I(xI) Here the summation is over all variables but the ones indexed by r.\nIn Figure 1(c), this is the distribution obtained by removing factors N(r1) = {I, T, Y,K, S,W} from the factor gaph and marginalizing the rest over dotted circles, r1.\nThe core idea to our approach is that each cavity region r can produce reliable probability distribution over r, given an accurate cavity distribution estimate over the surrounding variables r. Given the exact cavity distribution P \\r over r, we can recover the exact joint distribution Pr over \u2295r by:\nPr(x\u2295r) \u221d P \\r(x r)\u03c8N(r)(x) = P \\r(x r) \u220f\nI\u2208N(r)\n\u03c8I(xI) .\nIn practice, we can only obtain estimates P\u0302 \\r(x r) of the true cavity distribution P \\r(x r). However, suppose we have multiple cavity regions r1, r2, . . . , rM that collectively cover all the variables {x1, . . . , xN}. If rp intersects with rq, we can improve the estimate of P\u0302 \\rp(x rp) by enforcing marginal consistency of P\u0302rp(x\u2295rp) with P\u0302rq (x\u2295rq ) over the variables in their intersection. This suggests an iterative correction scheme that is very similar to message passing.\nIn Figure 1(a), let each hexagon (over variables and factors) define a cavity region, here r1, . . . , r5. Note r1 can provide good estimates over {j, s, k}, given good approximation to cavity distribution over {o, i,m, t, u, v, w}. This in turn can be improved by neighboring regions; e.g., r2 gives a good approximation over {i, o}, and r3 over {i,m}. Starting from an initial cavity distribution P\u0302\n\\r\u03b1 0 , for each cavity region\n\u03b1 \u2208 {1, . . . , 14}, We perform this improvement for all cavity regions, in iterations until convergence.\nWhen we start with a uniform cavity distribution P\u0302 \\rp 0 for all regions, the results are very similar to those of CVM. The accuracy of this approximation depends on the accuracy of the initial P\u0302 \\rp 0 .\nFollowing Mooij (2008), we use variable clamping to estimate higher-order interactions in r: Here, we estimate the partition function Zx r after removing factors in N(r) and fixing x r to each possible assignment. Doing this calculation, we have P\u0302 \\r(x r) \u221d Zx r . In our experiments, we use the approximation to the partition function provided using LBP. However there are some alternatives to clamping: conditioning scheme Rizzo et al. (2007) makes it possible to use\nany method capable of marginalization for estimation of cavity distribution (clamping requires estimation of partition function). It is also possible to use techniques in answering joint queries for this purpose (Koller & Friedman (2009)).\nUsing clamping for this purpose also means that, if the resulting network, after clamping, has no loops, then P\u0302r(x\u2295r) is exact \u2013 hence GLC produces exact results if for every cluster r, removing \u2295r results in a tree."}, {"heading": "3. Generalized Loop Correction", "text": ""}, {"heading": "3.1. Simple Case: Partitioning Cavity Regions", "text": "To introduce our approach, first consider a simpler case where the cavity regions r1, . . . , rM form a (disjoint and exhaustive) partition of the variables {1, . . . , N}.\nLet rp,q := ( rp)\u2229 rq denote the intersection of the perimeter rp of rp with another cavity region rq. (Note rp,q 6= rq,p). As r1, . . . , rM is a partition, each perimeter rp is a disjoint union of rp,q for q = 1 . . .M (some of which might be empty if rp and rq are not neighbors). Let Nb(p) denote the set of regions q with rp,q 6= \u2205. We now consider how to improve the cavity distribution estimate over rp through update messages sent to each of the rp,q.\nIn Figure 1(a), the regions r2, r4, r5, r7, r11, r14 form a partitioning. Here, r2 with {m, k, s, w} \u2282 r2, receives updates over r2,7 = {m} from r7 and updates over r2,4 = {k} from r4. This last update ensures \u2211 x\\{k} P\u0302r2(x\u2295r2) = \u2211 x\\{k} P\u0302r4(x\u2295r4). Towards enforcing this equality, we introduce a message m4\u21922(x r2,4) into distribution over \u2295r2.\nHere, the distribution over \u2295rp becomes: P\u0302rp(x\u2295rp) \u221d P\u0302 \\rp 0 (x rp)\u03c8N(rp)(x\u2295rp) \u220f q\u2208Nb(p) mq\u2192p(x rp,q ), (1)\nwhere P\u0302rp denotes our estimate of the true distribution Prp .\nThe messages mq\u2192p can be recovered by considering marginalization constraints. When rp and rq are neighbors, their distributions P\u0302rp(x\u2295rp) and P\u0302rq (x\u2295rq ) should satisfy\u2211 x\\\u2295rp\u2229\u2295rq P\u0302rp(x\u2295rp) = \u2211 x\\\u2295rp\u2229\u2295rq P\u0302rq (x\u2295rq ). We can divide both sides by the factor product \u03c8N(rp)\u2229N(rq)(x), as the domain of the factors in N(rp) \u2229 N(rq) is completely contained in \u2295rp \u2229 \u2295rq and independent of the summation. Hence we have\u2211 x\\\u2295rp\u2229\u2295rq P\u0302rp(x\u2295rp) \u03c8N(rp)\u2229N(rq)(x) = \u2211 x\\\u2295rp\u2229\u2295rq P\u0302rq(x\u2295rq) \u03c8N(rp)\u2229N(rq)(x)\nAs rp,q \u2282 \u2295rp \u2229 \u2295rq , this implies the weaker con-\nsistency condition:\u2211 x\\ rp,q P\u0302rp(x\u2295rp)\u03c8N(rp)\u2229N(rq)(x) \u22121= \u2211 x\\ rp,q P\u0302rq(x\u2295rq)\u03c8N(rp)\u2229N(rq)(x) \u22121,\n(2)\nwhich we can use to derive update equations formq\u2192p.\nStarting from the LHS of Eqn (2),\u2211 x\\ rp,q P\u0302rp(x\u2295rp)\u03c8N(rp)\u2229N(rq)(x) \u22121\n\u221d \u2211\nx\\ rp,q\nP\u0302 \\rp 0 (x rp)\u03c8N(rp)\\N(rq)(x) \u220f q\u2032\u2208Nb(p) mq\u2032\u2192p(x rp,q\u2032 )\n\u221d mq\u2192p(x rp,q) \u2211\nx\\ rp,q\nP\u0302 \\rp 0 (x rp)\u03c8N(rp)\\N(rq)(x) \u220f q\u2032\u2208Nb(p) q\u2032 6=q mq\u2032\u2192p(x rp,q\u2032).\nSetting this proportional to the RHS of Eqn (2), we have the update equation\nmnewq\u2192p(x rp,q )\n\u221d\n\u2211 x\\ rp,q P\u0302rq (x\u2295rq )\u03c8N(rp)\u2229N(rq)(x) \u22121\n\u2211 x\\ rp,q P\u0302 \\rp 0 (x rp)\u03c8N(rp)\\N(rq)(x) \u220f q\u2032\u2208Nb(p) q\u2032 6=q mq\u2192p(x rp,q\u2032 )\n\u221d\n\u2211 x\\ rp,q P\u0302rq (x\u2295rq )\u03c8N(rp)\u2229N(rq)(x) \u22121\n\u2211 x\\ rp,q P\u0302rp(x\u2295rp)\u03c8N(rp)\u2229N(rq)(x) \u22121 mq\u2192p(x rp,q ) (3)\nThe last line follows from multiplying the numerator and denominator by the current version of the message mq\u2192p. At convergence, when mq\u2192p equals m new q\u2192p, the consistency constraints are satisfied. By repeating this update in any order, after convergence, the P\u0302r(x\u2295r)s represent approximate marginals over each region.\nThe following theorem stablishes the relation between GLC and CVM in a limited setting.\nTheorem 1 If the cavity regions partition the variables and all the factors involve no more than 2 variables, then any GBP fixed point of a particular CVM construction (details in Appendix A) is also a fixed point for GLC, starting from uniform cavity distributions P\u0302 \\r 0 = 1. (Proof in Appendix A.)\nCorollary 1 If the factors have size two and there are no loops of size 4 in the factor graph, for single variable partitioning with uniform cavity distribution, any fixed points of LBP can be mapped to fixed points of GLC.\nProof If there are no loops of size 4 then no two factors have identical domain. Thus the factors are all maximal and GBP applied to CVM with maximal factor domains, is the same as LBP. On the other hand (refering to CVM construction of Appendix A) under the given condition, GLC with single variable partitioning shares the fixed points of GBP applied to CVM\nwith maximal factors. Therefore GLC shares the fixed points of LBP.\nTheorem 2 If all factors have size two and no two factors have the same domain, GLC is identical to LCBP under single variable partitioning.\nProof Follows from comparison of two update equations \u2013 i.e., Eqn (3) and Eqn (5) in (Mooij & Kappen, 2007)\u2013 under the assumptions of the theorem."}, {"heading": "3.2. General Cavity Regions", "text": "When cavity regions do not partition the set of variables, the updates are more involved. As the perimeter rp is no longer partitioned, the rp,q\u2019s are no longer disjoint.\nFor example in Figure 1, for r1 we have r1,2 ={o, i}, r1,3 = {i,m}, r1,4 = {t, u}, r1,5 = {v, w} and also r1,6 = {i}, r1,7 = {m}, r1,8 = {m, t}, r1,9 = {t}, etc. This means xi appears in messages m2\u21921, m3\u21921 and m6\u21921.\nDirectly adopting the correction formula for P\u0302r in Eqn (1) as a product of messsages over rp,q could double-count variables. To avoid this problem, we adopt a strategy similar to CVM to discount extra contributions from overlapping variables in rp. For each cavity region rp, we form a rp-region graph (Figure 2) with the incoming messages forming the distributions over top regions. For computational reasons, we only consider maximal rp,q domains.1 here, this means dropping m6\u21921 as r1,6\u2282 r1,2 and so on.\nOur region-graph construction is similar to CVM (Pelizzola, 2005) \u2013 i.e., we construct new sub-regions as the intersection of rp,q\u2019s, and we repeat this recursively until no new region can be added. We then connect each sub-region to its immediate parent. Figure 2 shows the r1-region graph for the example of Figure 1(a). If the cavity regions are a partition, the rp-region graph includes only the top regions. Below we use Rp to denote the rp-region graph for rp; ROp to denote its top (outer) regions; and brp(x\u03c1) to denote the belief over region \u03c1 in rp-region graph. For top-regions, the initial belief is equal to the basic messages obtained using Eqn (3).\nNext we assign \u201ccounting numbers\u201d to regions, in a way similar to CVM: top regions are assigned cn( rp,q) = 1, and each sub-region \u03c1 is assigned using\n1This does not noticably affect the accuracy in our experiments. When using uniform cavity distributions, the results are identical.\nthe Mo\u0308bius formula: cn(\u03c1) := 1\u2212 \u2211 \u03c1\u2032\u2208A(\u03c1) cn(\u03c1\u2032) where A(\u03c1) is the set of ancestors of \u03c1.\nWe can now define the belief over cavity regions rp as:\nP\u0302rp(x\u2295rp) \u221d P\u0302 \\rp 0 (x rp)\u03c8N(rp)(x\u2295rp) \u220f \u03c1\u2208 Rp brp(x\u03c1) cn(\u03c1) (4)\nThis avoids any double-counting of variables, and reduces to Eqn (1) in the case of partitioning cavity regions.\nTo apply Eqn (4) effectively, we need to enforce marginal consistency of the intersection regions with their parents, which can be accomplished via message passing in a downward pass, Each region \u03c1\u2032 sends to each of its child \u03c1, its marginal over the child\u2019s variables:\n\u00b5\u03c1\u2032\u2192\u03c1(x\u03c1) := \u2211\nx\\\u03c1 brp(x\u03c1\u2032)\nThen set the belief over each child region to be the geometric average of the incoming messages: brp(x\u03c1) := \u220f\n\u03c1\u2032\u2208pr(\u03c1) \u00b5\u03c1\u2032\u2192\u03c1(x\u03c1)\n1 |pr(\u03c1)|\nThe downward pass updates the child regions in Rp\\ ROp . We update the beliefs at the top regions using a modified version of Eqn (3): brp(x rp,q ) \u221d\u2211 x\\ rp,q P\u0302rq(x\u2295rq)\u03c8N(rq)\u2229N(rp)(x\u2295rq) \u22121\n\u2211 x\\ rp,q P\u0302rp(x\u2295rp)\u03c8N(rp)\u2229N(rq)(x\u2295rp) \u22121 beffrp (x rp,q) cn(\u03c1), (5)\nfor all top regions rp,q \u2208 ROp .\nHere beffrp (x rp,q ) is the effective old message over rp,q:\nbeffrp (x rp,q ) = \u2211\nx\\ rp,q \u220f \u03c1\u2208 Rp brp(x\u03c1)\nThat is, in the update equation, we need the calculation of the new message to assume this value as the old message from q to p. This marginalization is important because it allows the belief at the top region\nbrp(x rp,q ) to be influenced by the beliefs brp(x\u03c1) of the sub-regions after a downward pass. It enforces marginal consistency between the top regions, and at convergence we have beffrp (x rp,q ) = brp(x rp,q ). Notice also Eqn (5) is equivalent to the old update Eqn (3) in the partitioning case.\nTo calculate this marginalization more efficiently, GLC uses an upward pass in the rp-region-graph. Starting from the parents of the lowest regions, we define beffrp (x\u03c1) as:\nbeffrp (x\u03c1\u2032) := brp(x\u03c1\u2032) \u220f\n\u03c1\u2208ch(\u03c1\u2032)\nbeffr (x\u03c1)\n\u00b5\u03c1\u2192\u03c1\u2032(x\u03c1)\nReturning to the example, the previous text provides a method to update P\u0302r1(x\u2295r1). GLC performs this for the remaining regions as well, and then iterates the entire process until convergence \u2013 i.e., until the change in all distributions is less than a threshold."}, {"heading": "4. Experiments", "text": "This section compares different variations of our method against LBP as well as CVM, LCBP and TreeEP (Minka & Qi, 2003) methods, each of which performs some kind of loop correction. For CVM, we use the double-loop algorithm of (Heskes, 2006), which is slower than GBP but has better convergence properties. All methods are applied without any damping. We stop each method after a maximum of 1E4 iterations or if the change in the probability distribution (or messages) is less than 1E-9. We report the time in seconds and the error for each method as the average of absolute error in single variable marginals \u2013 i.e., \u2211 xi,v |P\u0302 (xi =v)\u2212P (xi =v)|. For each setting, we report the average results over 10 random instances of the problem. We experimented with grids, 3-regular random graphs, and the ALARM network as typical benchmark problems.2\nBoth LCBP and GLC can be used with a uniform initial cavity or with an initial cavity distribution estimated via clamping cavity variables. In the experiments, full and uniform refer to the kind of cavity distribution used. We use GLC to denote the partitioning case, and GLC+ when overlapping clusters of some form are used. For example, GLC+(Loop4, full) refers to a setting with full cavity that contains all overlapping loop clusters of length up to 4. If a factor does not appear in any loops, it forms its own cluster. The same form of clusters are used for CVM.\n2The evaluations are based on implementation in libdai inference toolbox (Mooij, 2010)."}, {"heading": "4.1. Grids", "text": "We experimented with periodic Ising grids in which xi \u2208 {\u22121,+1} is a binary variable and the probability distribution of a setting when xi and xj are connected in the graph is given by P (x) \u221d exp( \u2211 i \u03b8ixi + 1 2 \u2211 i,j\u2208I Ji,jxixj ) where Ji,j controls variable interactions and \u03b8i defines a single node potential \u2013 a.k.a. a local field. In general, smaller local fields and larger variable interactions result in more difficult problems. We sampled local fields independently from N (0, 1) and interactions from N (0, \u03b22). Figure 3(left) summarize the results for 6x6 grids for different values of \u03b2.\nWe also experimented with periodic grids of different sizes, generated by sampling all factor entries independently from N (0, 1). Figure 3(middle) compares the computation time and error of different methods for grids of sizes that range from 4x4 to 10x10."}, {"heading": "4.2. Regular Graphs", "text": "We generated two sets of experiments with random 3-regular graphs (all nodes have degree 3) over 40 variables. Here we used Ising model when both local fields and couplings are independently sampled from N (0, \u03b22). Figure 3(right) show the time and error for different values of \u03b2. Figure 4 shows time versus error for graph size between 10 to 100 nodes for \u03b2 = 1. For larger \u03b2s, few instances did not converge within allocated number of iterations. The results are for cases in which all methods converged."}, {"heading": "4.3. Alarm Network", "text": "Alarm is a Bayesian network with 37 variables and 37 factors. Variables are discrete, but not all are binary, and most factors have more than two variables. Table(1) compares the accuracy versus run-time of different methods. GLC with factor domains as regions \u2013 i.e., rp = I for I \u2208 F \u2013 and all loopy clusters produces exact results up to the convergence threshold."}, {"heading": "4.4. Discussions", "text": "These results show that GLC consistently provides more accurate results than both CVM and LCBP, although often at the cost of more computation time. They also suggest that one may not achieve this tradeoff between time and accuracy simply by including larger loops in CVM regions. When used with uniform cavity, the performance of GLC (specifically GLC+) is similar to CVM, and GLC appears stable, which is\nlacking in general single-loop GBP implementations.\nGLC\u2019s time complexity (when using full cavity, and using LBP to estimate the cavity distribution) is O(\u03c4MN |X |u + \u03bbM |X |v)), where \u03bb is the number of iterations of GLC, \u03c4 is the maximum number of iterations for LBP, M is the number of clusters, N is the number of variables, u = maxp | rp| and v = maxp | \u2295 rp|. Here the first term is the cost of estimating the cavity distributions and the second is the cost of exact inference on clusters. This makes GLC especially useful when regional Markov blankets are not too large."}, {"heading": "5. Conclusions", "text": "We introduced GLC, an inference method that provide accurate inference by utilizing the loop correction schemes of both region-based and recent cavitybased methods. Experimental results on benchmarks support the claim that, for difficult problems, these schemes are complementary and our GLC can successfully exploit both. We also believe that our scheme motivates possible variations that can also deal with graphical models with large Markov blankets."}, {"heading": "6. Acknowledgements", "text": "We thank the anonymous reviewers for their excellent detailed comments. This research was partly funded by NSERC, Alberta Innovates \u2013 Technology Futures (AICML) and Alberta Advanced Education and Technology."}, {"heading": "A. Appendix", "text": "We prove the equality of GLC to CVM, in the setting where each factor involves no more than 2 variables and the cavity distributions P\u0302 \\r(x r) is uniform. 3\nConsider the following CVM region-graph:\n\u2022 internal region (Rintp ): it contains all the variables in rp, and factors that are internal to rp \u2013 i.e., {I \u2208 F | I \u2286 rp}. \u2022 bridge region (Rbrp,q): it contains all the variables and factors that connect rp and rq \u2014 i.e., variables \u2295rp\u2229 \u2295rq and factors N(rp) \u2229N(rq). \u2022 sub region (Rsubp,q ): the intersection of internal Rintp and bridge Rbrp,q. It contains only variables and no\nfactors. (Note Rsubp,q = rq,p)\nNote each internal and bridge region has a counting number\n3 To differentiate from GLC\u2019s cavity regions r, we use the capital notation R to denote the corresponding region in the CVM region graph construction.\nof 1, while each subregion has a counting number of \u22121. Since we assume the cavity regions rp form a partition and each factor contains no more than 2 variables, this region graph construction counts each variable and each factor exactly once.\nWe focus on the parent-to-child algorithm for GBP. For the specific region graph construction outlined, we have 2 types of messages: internal region to subregion message (\u00b5isq\u2192p sent from R int q to R sub q,p ), and bridge region to subregion message (\u00b5bsq\u2192p sent from R br q,p to R sub q,p ). Note that Rsubq,p and R sub p,q are the intersection of R br p,q with Rintq and R int p respectively. We use the notation \u00b5 to differentiate from messages m used in GLC. Below we drop the arguments to make the equations more readable. The parent-to-child algorithm uses the following fixed-point equations:\n\u00b5isq\u2192p \u221d \u2211 x\\Rsubq,p \u03c8Rintq \u220f q\u2032\u2208Nb(q),q\u2032 6=p \u00b5 bs q\u2032\u2192q\n\u00b5bsq\u2192p \u221d \u2211 x\\Rsubq,p \u03c8Rbrq,p\u00b5 is q\u2192p\nSuppose GBP converges to a fixed point with messages \u00b5isq\u2192p and \u00b5 bs q\u2192p satisfying the fixed point conditions above; we show that messages defined by mq\u2192p := \u00b5 is q\u2192p are fixed points of update Eqn (3) \u2013 i.e., satisfy the consistency condition of Eqn (2)\nAssuming uniform initial cavity P\u0302 \\r 0 = 1, for LHS of Eqn (2), we have\u2211 x\\ rp,q P\u0302rp(x\u2295rp)\u03c8N(rp)\u2229N(rq)(x) \u22121\n\u221d mq\u2192p \u2211 x\\ rp,q \u03c8N(rp)\\N(rq) \u220f q\u2032\u2208Nb(p),q\u2032 6=qmq\u2032\u2192p\n\u221d mq\u2192p = \u00b5isq\u2192p, as the domain of the expression inside the summation sign is disjoint from rp,q.\nAs for the RHS of Eqn (2) we have\u2211 x\\ rp,q P\u0302rq (x\u2295rq )\u03c8N(rp)\u2229N(rq)(x) \u22121\n\u221d \u2211\nx\\ rp,q\n\u03c8N(rq)\u03c8N(rp)\u2229N(rq)(x) \u22121 \u220f q\u2032\u2208Nb(q) mq\u2032\u2192q\n\u221d \u2211\nx\\Rsubq,p\n(\u03c8Rintq \u220f q\u2032\u2208Nb(q) \u03c8Rbr q\u2032,q )\u03c8Rbrp,q (x) \u22121 \u220f q\u2032\u2208Nb(q) \u00b5isq\u2032\u2192q\n\u221d \u2211\nx\\Rsubq,p\n\u03c8Rintq \u220f q\u2032\u2208Nb(q),q\u2032 6=p \u03c8Rbr q\u2032,q \u00b5isq\u2032\u2192q (6)\n\u221d \u2211\nx\\Rsubq,p\n\u03c8Rintq \u220f q\u2032\u2208Nb(q),q\u2032 6=p \u2211 x\\Rsub\nq\u2032,q\n\u03c8Rbr q\u2032,q\n\u00b5isq\u2032\u2192q (7)\n\u221d \u2211\nx\\Rsubq,p\n\u03c8Rintq \u220f q\u2032\u2208Nb(q),q\u2032 6=p \u00b5bsq\u2032\u2192q \u221d \u00b5isq\u2192p\nRemoving \u00b5isp\u2192q in line (6) is valid because, in the absence of \u03c8Rbrp,q , its domain is disjoint from the rest of the terms. Moving the summation inside the product in line (7) is valid because partitioning guarantees that product terms\u2019 domains have no overlap and they are also disjoint from \u03c8Rintq .\nThus the LHS and RHS of Eqn (2) agrees and mq\u2192p := \u00b5isq\u2192p is a fixed point of GLC."}], "references": [{"title": "Stable fixed points of loopy belief propagation are local minima of the Bethe free energy", "author": ["T. Heskes"], "venue": "In NIPS,", "citeRegEx": "Heskes,? \\Q2003\\E", "shortCiteRegEx": "Heskes", "year": 2003}, {"title": "Convexity arguments for efficient minimization of the Bethe and Kikuchi free energies", "author": ["T. Heskes"], "venue": null, "citeRegEx": "Heskes,? \\Q2006\\E", "shortCiteRegEx": "Heskes", "year": 2006}, {"title": "A theory of cooperative phenomena", "author": ["R. Kikuchi"], "venue": "Phys. Rev.,", "citeRegEx": "Kikuchi,? \\Q1951\\E", "shortCiteRegEx": "Kikuchi", "year": 1951}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F Kschischang", "B Frey", "H. Loeliger"], "venue": "IEEE Info Theory,", "citeRegEx": "Kschischang et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kschischang et al\\.", "year": 1998}, {"title": "Analytic and algorithmic solution of random satisfiability problems", "author": ["M M\u00e9zard", "G Parisi", "R. Zecchina"], "venue": null, "citeRegEx": "M\u00e9zard et al\\.,? \\Q2002\\E", "shortCiteRegEx": "M\u00e9zard et al\\.", "year": 2002}, {"title": "Tree-structured approximations by expectation propagation", "author": ["T Minka", "Y. Qi"], "venue": "In NIPS,", "citeRegEx": "Minka and Qi,? \\Q2003\\E", "shortCiteRegEx": "Minka and Qi", "year": 2003}, {"title": "How to compute loop corrections to the Bethe approximation", "author": ["A Montanari", "T. Rizzo"], "venue": "J Statistical Mechanics,", "citeRegEx": "Montanari and Rizzo,? \\Q2005\\E", "shortCiteRegEx": "Montanari and Rizzo", "year": 2005}, {"title": "Understanding and Improving Belief Propagation", "author": ["J. Mooij"], "venue": "PhD thesis, Radboud U,", "citeRegEx": "Mooij,? \\Q2008\\E", "shortCiteRegEx": "Mooij", "year": 2008}, {"title": "libDAI: A free and open source C++ library for discrete approximate inference in graphical models", "author": ["J. Mooij"], "venue": null, "citeRegEx": "Mooij,? \\Q2010\\E", "shortCiteRegEx": "Mooij", "year": 2010}, {"title": "Loop corrections for approximate inference on factor graphs", "author": ["J Mooij", "H. Kappen"], "venue": "JRML,", "citeRegEx": "Mooij and Kappen,? \\Q2007\\E", "shortCiteRegEx": "Mooij and Kappen", "year": 2007}, {"title": "Probabilistic reasoning in intelligent systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Cluster variation method in statistical physics and probabilistic graphical models", "author": ["A. Pelizzola"], "venue": "J Physics A,", "citeRegEx": "Pelizzola,? \\Q2005\\E", "shortCiteRegEx": "Pelizzola", "year": 2005}, {"title": "On cavity approximations for graphical models", "author": ["T Rizzo", "B Wemmenhove", "H. Kappen"], "venue": "J Physical Review,", "citeRegEx": "Rizzo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Rizzo et al\\.", "year": 2007}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["J Yedidia", "W Freeman", "Y. Weiss"], "venue": "IEEE Info Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "CCCP algorithms to minimize the Bethe and Kikuchi free energies", "author": ["A. Yuille"], "venue": "Neural Computation,", "citeRegEx": "Yuille,? \\Q2002\\E", "shortCiteRegEx": "Yuille", "year": 2002}], "referenceMentions": [{"referenceID": 4, "context": "This paper will use probabilistic graphical models, focusing on factor graphs (Kschischang et al., 1998), that can represent both Markov Networks and Bayesian Networks.", "startOffset": 78, "endOffset": 104}, {"referenceID": 11, "context": ", no loops), this exact inference is tractable, and can be done by a message passing procedure, Belief Propagation (BP) (Pearl, 1988).", "startOffset": 120, "endOffset": 133}, {"referenceID": 0, "context": "LBP is related to the Bethe approximation to free energy (Heskes, 2003), which is the basis for minimization of more sophisticated energy approximations and provably convergent methods (Yedidia et al.", "startOffset": 57, "endOffset": 71}, {"referenceID": 14, "context": "LBP is related to the Bethe approximation to free energy (Heskes, 2003), which is the basis for minimization of more sophisticated energy approximations and provably convergent methods (Yedidia et al., 2005; Heskes, 2006; Yuille, 2002).", "startOffset": 185, "endOffset": 235}, {"referenceID": 1, "context": "LBP is related to the Bethe approximation to free energy (Heskes, 2003), which is the basis for minimization of more sophisticated energy approximations and provably convergent methods (Yedidia et al., 2005; Heskes, 2006; Yuille, 2002).", "startOffset": 185, "endOffset": 235}, {"referenceID": 15, "context": "LBP is related to the Bethe approximation to free energy (Heskes, 2003), which is the basis for minimization of more sophisticated energy approximations and provably convergent methods (Yedidia et al., 2005; Heskes, 2006; Yuille, 2002).", "startOffset": 185, "endOffset": 235}, {"referenceID": 14, "context": "A representative class of energy approximations is the region-graph methods (Yedidia et al., 2005), which deal with a set of connected variables (called \u201cregions\u201d); these methods subsume both the Cluster Variation Method (CVM) (Pelizzola, 2005; Kikuchi, 1951) and the Junction Graph Method (Aji & McEliece, 2001).", "startOffset": 76, "endOffset": 98}, {"referenceID": 12, "context": ", 2005), which deal with a set of connected variables (called \u201cregions\u201d); these methods subsume both the Cluster Variation Method (CVM) (Pelizzola, 2005; Kikuchi, 1951) and the Junction Graph Method (Aji & McEliece, 2001).", "startOffset": 136, "endOffset": 168}, {"referenceID": 2, "context": ", 2005), which deal with a set of connected variables (called \u201cregions\u201d); these methods subsume both the Cluster Variation Method (CVM) (Pelizzola, 2005; Kikuchi, 1951) and the Junction Graph Method (Aji & McEliece, 2001).", "startOffset": 136, "endOffset": 168}, {"referenceID": 8, "context": "This is the basis for the loop correction schemes by Montanari & Rizzo\u2019s (2005) on pairwise dependencies over binary variables, and also Mooij & Kappen\u2019s (2007) extension to general factor graphs \u2013 called Loop Corrected Belief Propagation (LCBP).", "startOffset": 137, "endOffset": 161}, {"referenceID": 4, "context": "This model can be conveniently represented as a bipartite graph, called the factor graph (Kschischang et al., 1998), which includes two sets of nodes: variable nodes xi, and factor nodes \u03c8I .", "startOffset": 89, "endOffset": 115}, {"referenceID": 5, "context": "The notion of cavity distribution is borrowed from socalled cavity methods from statistical physics (M\u00e9zard & Montanari, 2009), and has been used in analysis and optimization of important combinatorial problems (M\u00e9zard et al., 2002; Braunstein et al., 2002).", "startOffset": 211, "endOffset": 257}, {"referenceID": 8, "context": "Following Mooij (2008), we use variable clamping to estimate higher-order interactions in r: Here, we estimate the partition function Zx r after removing factors in N(r) and fixing x r to each possible assignment.", "startOffset": 10, "endOffset": 23}, {"referenceID": 8, "context": "Following Mooij (2008), we use variable clamping to estimate higher-order interactions in r: Here, we estimate the partition function Zx r after removing factors in N(r) and fixing x r to each possible assignment. Doing this calculation, we have P\u0302 \\r(x r) \u221d Zx r . In our experiments, we use the approximation to the partition function provided using LBP. However there are some alternatives to clamping: conditioning scheme Rizzo et al. (2007) makes it possible to use", "startOffset": 10, "endOffset": 446}, {"referenceID": 12, "context": "Our region-graph construction is similar to CVM (Pelizzola, 2005) \u2013 i.", "startOffset": 48, "endOffset": 65}, {"referenceID": 1, "context": "For CVM, we use the double-loop algorithm of (Heskes, 2006), which is slower than GBP but has better convergence properties.", "startOffset": 45, "endOffset": 59}, {"referenceID": 9, "context": "The evaluations are based on implementation in libdai inference toolbox (Mooij, 2010).", "startOffset": 72, "endOffset": 85}], "year": 2012, "abstractText": "Belief Propagation (BP) is one of the most popular methods for inference in probabilistic graphical models. BP is guaranteed to return the correct answer for tree structures, but can be incorrect or non-convergent for loopy graphical models. Recently, several new approximate inference algorithms based on cavity distribution have been proposed. These methods can account for the effect of loops by incorporating the dependency between BP messages. Alternatively, regionbased approximations (that lead to methods such as Generalized Belief Propagation) improve upon BP by considering interactions within small clusters of variables, thus taking small loops within these clusters into account. This paper introduces an approach, Generalized Loop Correction (GLC), that benefits from both of these types of loop correction. We show how GLC relates to these two families of inference methods, then provide empirical evidence that GLC works effectively in general, and can be significantly more accurate than both correction schemes.", "creator": "LaTeX with hyperref package"}}}