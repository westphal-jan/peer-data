{"id": "1206.4673", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Group Sparse Additive Models", "abstract": "We consider the problem of sparse variable selection in nonparametric additive models, with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly. Previous works either study the group sparsity in the parametric setting (e.g., in a case of a family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family-size family", "histories": [["v1", "Mon, 18 Jun 2012 15:35:38 GMT  (470kb)", "http://arxiv.org/abs/1206.4673v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["junming yin", "xi chen", "eric p xing"], "accepted": true, "id": "1206.4673"}, "pdf": {"name": "1206.4673.pdf", "metadata": {"source": "META", "title": "Group Sparse Additive Models", "authors": ["Junming Yin", "Xi Chen", "Eric P. Xing"], "emails": ["junmingy@cs.cmu.edu", "xichen@cs.cmu.edu", "epxing@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "The problem of sparse variable selection for highdimensional data arises in a wide spectrum of domains, including signal processing, bioinformatics and computer vision. `1-regularized methods, such as lasso (Tibshirani, 1996), are among the most widely used approaches for variable selection in linear models. Despite the popularity of linear models, a reliance on rigid parametric forms limits their ability to model\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nnonlinear covariate effects. Additive models (Hastie & Tibshirani, 1990), where each additive component is a univariate smooth function of a single covariate, are nonparametric extensions of linear models and can offer a higher degree of flexibility. Variable selection in nonparametric additive models is more challenging as one needs to simultaneously select and fit component functions. While some recent progress has been made on this problem by proposing various functional penalties (Lin & Zhang, 2006; Bach, 2008; Ravikumar et al., 2009), none of these methods is capable of exploiting the structural information among the covariates that may exist as prior knowledge in many applications. The simplest case is a group structure where the covariates are partitioned into disjoint groups, and it is desirable to choose relevant covariates that are sparse at the group level. In the parametric setting, it has been shown that if such a group structure exists and is consistent with the true sparsity pattern of covariates, treating the whole group of covariates as a single unit in variable selection has the potential to increase the accuracy of the estimator (Huang & Zhang, 2010). However, to the best of our knowledge, there has been no successful attempts to investigate the benefit of considering the group sparsity in estimating nonparametric additive models. One example of such a situation occurs in biology, where the effects of multiple genes on the phenotype are nonlinear, and the goal is to identify a few genes from the same functional groups that are predictive of the phenotype.\nIn this paper, we fill this gap by presenting a new approach for variable selection in nonparametric additive models that can take advantage of the group structure among the covariates. The proposed method, called GroupSpAM, achieves functional sparsity at the level of groups by integrating the spirit of group lasso (Yuan & Lin, 2006) and the idea of sparse additive models (SpAM) (Ravikumar et al., 2009). Therefore, the GroupSpAM estimate combines the predictive power of SpAM that can model nonlinear covariate effects, with the advantage of group lasso that can incorpo-\nrate group structure to achieve better support recovery accuracy. Our empirical results provide convincing evidence of the expected benefits inherited from both threads of research. Although our main focus here is on regression, the framework can be easily extended to classification setting via generalized additive models.\nIn contrast to the ordinary group lasso, where there has been much confusion regarding orthonormality of covariates within a group, GroupSpAM is highly flexible in that no assumptions are made on the design matrices or on the covariance of component functions in each group. In this sense, GroupSpAM can be viewed as a nonparametric extension of the generalized group lasso (Friedman et al., 2010), which allows non-orthonormal covariates within a group, to additive models. However, while allowing nonlinear covariate effects and arbitrary covariance of component functions within a group gives rise to a new method that enjoys superior flexibility, there are new technical challenges in characterizing and computing the GroupSpAM estimate. To resolve these difficulties, we propose a novel optimization procedure to simultaneously conduct component selection and fitting at the level of groups. Specifically, our major contributions in this work include: (1) generalization of `1/`2 norm to L2 function spaces as the sparsity-inducing penalty in GroupSpAM; (2) a necessary and sufficient condition for identifying functional sparsity at the group level (Theorem 2); (3) an efficient block coordinate descent algorithm tailored to handle the covariance of component functions within a group; (4) extension of GroupSpAM to the case of overlapping groups and a successful application to a real dataset."}, {"heading": "2. Related Work", "text": "Lin & Zhang (2006) proposed the COSSO estimator, which uses the sum of the reproducing kernel Hilbert space (RKHS) norms of the component functions as the regularization penalty for simultaneous variable selection and model fitting in smoothing spline ANOVA models. Ravikumar et al. (2009) introduced a sparse version of additive models called SpAM by penalizing the sum of the L2(\u00b5) norms of the component functions. While these methods have been shown to be effective in estimating sparse nonparametric models, neither of them is able to take advantage of structural information. Liu et al. (2009a) extended SpAM to the multi-task setting; however, the group structure is imposed on the tasks instead of on the covariates being considered here.\nFinally, it is important to emphasize that our work is substantially different from another line of research\nthat also mixes group lasso and additive models (Bach, 2008; Meier et al., 2009; Huang et al., 2010). These approaches, in which each component function is expanded into a group of basis functions constructed from a single covariate, can only perform variable selection at an individual level. In contrast, in this paper, each group consists of a set of several covariates and our goal is to encourage those variables within a group to be selected jointly.\nNotation Vectors and matrices are denoted by boldface letters, and estimates are denoted with a hat. For a random variable X with distribution \u00b5 and a measurable function f of x, \u2016f\u2016 denotes the L2(\u00b5) norm of f : \u2016f\u2016 = \u221a E[f2(X)]. With some slight abuse of notation we also use \u2016 \u00b7 \u2016 to denote the `2 norm of a real vector. For a set of random variables X1, . . . , Xp, let Hj = {fj | E[fj(Xj)] = 0, \u2016fj\u2016 < \u221e}, j = 1 . . . , p, with inner product on the space defined as \u3008fj , gj\u3009 = E[fj(Xj)gj(Xj)]. Sometimes we also use fj := fj(Xj) for simplicity. We use f\u0302j \u2208 Rn to represent the vector of evaluations of the fitted function f\u0302j at the n observed values of Xj , i.e., f\u0302j = [f\u0302j(x1j), . . . , f\u0302j(xnj)] T . A group of covariates is a subset g \u2282 {1, . . . , p} and a set of groups is denoted as G. For any group g and any vector v, vg = {vj}j\u2208g. \u2016fg\u2016 = \u221a\u2211 j\u2208g \u2016fj\u20162. dg = |g| represents the cardinality of group g. We assume G is available in advance as prior knowledge and all covariates are covered by at least one group in G."}, {"heading": "3. Background", "text": "We begin by reviewing some important concepts on nonparametric regression and (sparse) additive models to set the stage for our method."}, {"heading": "3.1. Smoothing for Nonparametric Regression", "text": "Let X1, . . . , Xp be a set of random covariate variables and Y be a real valued random response variable. Nonparametric regression is concerned with estimating the regression function\nm(X) = m(X1, . . . , Xp) = E[Y | X1, . . . , Xp]\nfrom a set of n data samples {(x(i), y(i)) : x(i) \u2208 Rp, y(i) \u2208 R, i = 1, . . . , n}, without assuming any parametric form of m(X), such as linearity in X1, . . . , Xp. We can write Y = m(X) + , where E[ ] = 0.\nIn the case of a single covariate (p = 1), m(X) = E[Y | X] is known as the orthogonal projection of Y onto the linear space of all measurable functions of X and can be written as\nm(X) = PY, (1)\nwhere P is the conditional expectation operator E[\u00b7 |\nGroup Sparse Additive Models\nX]. By making the assumption that m(x) = E[Y | X = x] is a smooth function of x, we can estimate m(x) using a class of smoothing estimators called the kernel smoothers\nm\u0302(x) = n\u2211 i=1 `i(x)y (i) = `(x)Ty, (2)\nwhere `i(x) \u221d Kh(|x(i)\u2212x|) andKh is a smoothing kernel function with the bandwidth h. Kernel smoothers are examples of linear smoothers because, for each x, the estimator m\u0302(x) in (2) is a linear combination of y(i). Let y\u0302 \u2208 Rn be a vector of fitted values y\u0302(i) at the observed x(i), one consequence of linear smoother is\ny\u0302 = Sy, (3)\nwhere S is the so-called smoother matrix with Sij = `j(x\n(i)), i, j = 1, . . . , n. A comparison of equations (1) and (3) reveals that the conditional expectation operator P plays the role of smoother in the population setting, and consequently a natural estimate of P is a linear smoother with smoother matrix S. This onedimensional smoother, called scatter smoother, is a building block for fitting more complicated models."}, {"heading": "3.2. Additive Models", "text": "Although it is straightforward to generalize the onedimensional smoothers to p-dimension case, it is wellknown that smoothers break down in high dimensions due to the curse of dimensionality. This shortcoming motivates the study of additive models (Hastie & Tibshirani, 1990),\nm(X1, . . . , Xp) = \u03b1+ p\u2211 j=1 fj(Xj), (4)\nwhere f1, . . . , fp are one-dimensional smooth component functions, one for each covariate. For simplicity and identification purposes, we assume \u03b1 = 0 and fj \u2208 Hj so that E[fj(Xj)] = 0 for each j. The optimization problem of additive models in the population setting is to minimize\nL(f) = 1 2 E [( Y \u2212 p\u2211 j=1 fj(Xj) )2] , (5)\nover {f : fj \u2208 Hj}. The minimizers of (5) can be shown to satisfy\nfj = E [( Y \u2212 \u2211 k 6=j fk ) | Xj ] := Pj ( Y \u2212 \u2211 k 6=j fk ) , (6)\nwhere Pj = E[\u00b7 | Xj ] is the projection operator onto Hj . Replacing Pj by a linear smoother with smoother matrix Sj in (6) immediately leads to a sample version of the above iterative procedure for fitting additive model:\nf\u0302j \u2190 Sj ( y \u2212 \u2211 k 6=j f\u0302k ) , j = 1, . . . , p, 1, . . . , p, . . . (7)\nThis simple algorithm is known as backfitting and is essentially a coordinate descent algorithm."}, {"heading": "3.3. SpAM", "text": "However, additive models work well only in a lowdimensional p n setting. Ravikumar et al. (2009) proposed a new approach called SpAM for component selection in high-dimensional additive models. The idea is to impose a sparsity constraint on the index set of non-zero component functions via regularization:\nmin f :fj\u2208Hj L(f) + \u03bb\u2126(f), (8)\nwhere \u03bb > 0 is the regularization parameter and \u2126(f) = \u2211p j=1 \u2016fj\u2016 behaves like an `1 ball across different components to encourage functional sparsity. The stationary condition of (8) is given by\nfj = [ 1\u2212 \u03bb \u2016PjRj\u2016 ] + PjRj , (9)\nwhere Rj = Y \u2212 \u2211 k 6=j fk is the partial residual and [\u00b7]+ = max{\u00b7, 0}. A sample version of the algorithm can be obtained by inserting sample estimates into (9)."}, {"heading": "4. GroupSpAM", "text": "We are now equipped to present GroupSpAM. In this section, we assume G is a partition of {1, \u00b7 \u00b7 \u00b7 , p}, i.e., the groups in G do not overlap. The optimization problem of GroupSpAM in the population setting is formulated as\nmin f :fj\u2208Hj L(f) + \u03bb\u2126group(f), (10)\nwhere L(f) is the expected square error as in (5) and \u2126group(f) is the regularization functional penalty defined as\n\u2126group(f) = \u2211 g\u2208G \u221a dg\u2016fg\u2016 = \u2211 g\u2208G \u221a dg \u221a\u2211 j\u2208g E [ f2j (Xj) ] .\nThe regularization term is the generalization of (scaled) `1/`2 penalty norm used in group lasso to L2 function spaces. As in group lasso, this mixed norm induces sparsity at the level of groups: the whole group of functions fg = {fj}j\u2208g are encouraged to be set to zero. If each group g is a singleton, this formulation reduces to SpAM (Ravikumar et al., 2009); if each component function fj has a linear parametric form, the optimization problem in (10) is just the population setting of the ordinary group lasso formulation (Yuan & Lin, 2006).\nTo solve the optimization problem (10), a seemingly natural strategy is to employ a block coordinate descent algorithm: we optimize the objective functional in (10) with respect to a particular group of functions fg at a time while all other groups of functions are kept fixed. However, as we allow arbitrary covariance of component functions within a group (see remark below for further discussion), there are two main obstacles for applying such algorithm to our problem: (1) it creates a new difficulty to characterize the thresholding condition for functional sparsity at the group level; (2) unlike the ordinary group lasso and SpAM, there is no longer a closed-form solution to the stationary condition for each group of functions fg, in the form of a soft-thresholding operator. Before describing the details of the algorithm, we first state the stationary condition that characterizes the optimum of fg.\nTheorem 1. Let Rg = Y \u2212 \u2211 g\u2032 6=g \u2211 j\u2032\u2208g\u2032 fj\u2032(Xj\u2032) be the partial residual after removing all functions from the group g. The stationary condition of the problem (10) with respect to fg while fixing all other groups {fg\u2032 : g\u2032 6= g} is\nfj+ \u2211\nj\u2032\u2208g:j\u2032 6=j\nPjfj\u2032\u2212PjRg+\u03bb \u221a dgsj = 0,\u2200j \u2208 g, (11)\nwhere sg = {sj \u2208 Hj}j\u2208g is a vector of functions belonging to the subgradient of \u2016fg\u2016:\nsg =\n{ {fj/\u2016fg\u2016}j\u2208g if \u2016fg\u2016 6= 0,\n{ug : \u2016ug\u2016 \u2264 1} if \u2016fg\u2016 = 0.\nProof. The proof uses the calculus of variations in Hilbert space. See Appendix for details.\nNext, we prove a necessary and sufficient thresholding condition at the group level so that we can set a whole group of functions fg to zeros if (12) holds. Theorem 2. fj = 0 \u2200j \u2208 g if and only if\u221a\u2211 j\u2208g E[(PjRg)2] \u2264 \u03bb \u221a dg. (12)\nProof. Necessity. If fj = 0 \u2200j \u2208 g, then (11) reduces to PjRg = \u03bb \u221a dgsj \u2200j \u2208 g, with \u2016sg\u2016 \u2264 1. Thus\u221a\u2211\nj\u2208g E[(PjRg)2] = \u03bb\n\u221a dg \u221a\u2211 j\u2208g E[s2j ] \u2264 \u03bb \u221a dg.\nSufficiency. We prove by contradiction. If there exists an fj 6= 0, j \u2208 g hence \u2016fg\u2016 6= 0, equation (11) becomes\nPjRg = fj + \u2211\nj\u2032\u2208g:j\u2032 6=j\nPjfj\u2032 + \u03bb \u221a dg\n\u2016fg\u2016 fj ,\u2200j \u2208 g.\nWithout loss of generality, we assume g = {1, \u00b7 \u00b7 \u00b7 , |g|}. The set of equations in (11) can be succinctly written in the following equivalent form:\nQRg = Jfg + \u03bb \u221a dg\n\u2016fg\u2016 fg, (13)\nwhere Q and J are a vector and matrix of conditional expectation operators, respectively, defined as\nQ =  P1 P2 ...\nP|g|\n , J =  I P1 \u00b7 \u00b7 \u00b7 P1 P2 I \u00b7 \u00b7 \u00b7 P2 ... ... . . .\n... P|g| P|g| \u00b7 \u00b7 \u00b7 I  . (14) Hence\u2211 j\u2208g E[(PjRg)2] = \u2016QRg\u20162 = \u2225\u2225\u2225\u2225\u2225Jfg + \u03bb \u221a dg \u2016fg\u2016 fg \u2225\u2225\u2225\u2225\u2225 2\n= \u2016Jfg\u20162 + \u03bb2dg + 2 \u03bb \u221a dg\n\u2016fg\u2016 \u3008Jfg, fg\u3009 .\nBy the fact that \u3008Jfg, fg\u3009 = \u2211 j\u2208g E [ f2j + fj \u2211 j\u2032\u2208g:j\u2032 6=j Pjfj\u2032 ]\n= \u2211 j\u2208g E [ f2j ] + \u2211 j\u2208g \u2211 j\u2032\u2208g:j\u2032 6=j E [fjE[fj\u2032 | Xj ]]\n= \u2211 j\u2208g E [ f2j ] + \u2211 j\u2208g \u2211 j\u2032\u2208g:j\u2032 6=j E [fjfj\u2032 ]\n= E [(\u2211 j\u2208g fj )2] \u2265 0,\nwe conclude \u2211 j\u2208g E[(PjRg)2] \u2265 \u03bb2dg.\nFollowing the standard additive models, we replace the conditional expectation operator Pj by a linear smoother with smoother matrix Sj in the population conditions (12) and (13) to obtain a sample version of estimation procedure. Specifically, if the sample estimate of norm \u03c9\u0302g in (15) is below the threshold \u03bb \u221a dg, the whole group of functions are thresholded to zeros; otherwise, we estimate f\u0302g by solving the sample version of (13) in equation (16). See Algorithm 1 for details. The full block coordinate descent algorithm of the GroupSpAM procedure is described in Algorithm 2. Performing prediction on new data is essentially the same as in the standard additive models (Hastie & Tibshirani, 1990).\nRemark As we don\u2019t restrict the covariance of component functions, for two distinct functions fj and fj\u2032\nAlgorithm 1 Thresholding\n1: Input: Partial residual R\u0302g, smoother matrices {Sj : j \u2208 g}, and tuning parameter \u03bb. 2: Output: f\u0302g = {f\u0302j : j \u2208 g}.\n3: Estimate PjRg by smoothing: P\u0302j = SjR\u0302g, \u2200j \u2208 g. 4: Estimate \u221a\u2211\nj\u2208g E[(PjRg)2] by\n\u03c9\u0302g =\n\u221a 1\nn \u2211 j\u2208g \u2016P\u0302j\u20162. (15)\n5: if \u03c9\u0302g \u2264 \u03bb \u221a dg then 6: Set f\u0302j = 0, \u2200j \u2208 g. 7: else 8: Estimate f\u0302g by solving the sample version of (13):\niterate the following fixed point equation over t until convergence\nf\u0302 (t+1)g = ( J\u0302 + \u03bb \u221a dg\n\u2016f\u0302 (t)g \u2016/ \u221a n I\n)\u22121 Q\u0302R\u0302g, (16)\nwhere Q\u0302 and J\u0302 are matrices obtained by replacing each Pj with Sj in Q and J (equation (14)), respectively.\n9: end if 10: Center each f\u0302j by subtracting its mean\nAlgorithm 2 Block Coordinate Descent\n1: Input: Data X \u2208 Rn\u00d7p,y \u2208 Rn, a partition G of {1, . . . , p}, and tuning parameter \u03bb. 2: Output: Fitted functions f\u0302 = {f\u0302j : j = 1, . . . , p}. 3: Initialize f\u0302j = 0 \u2200j; pre-compute smoother matrices\nSj \u2200j. 4: Cycle though group g \u2208 G until convergence:\n(1) Compute the partial residual R\u0302g = y \u2212\u2211 g\u2032 6=g \u2211 j\u2032\u2208g\u2032 f\u0302j\u2032\n(2) f\u0302g \u2190\u2212 Thresholding(R\u0302g, {Sj}j\u2208g, \u03bb)\nin the same group, their covariance Cov(fj , fj\u2032) is not forced to be zero. By the fact that\nCov(fj , fj\u2032) = E[fjfj\u2032 ] = E[fjE[fj\u2032 | Xj ]],\nPjfj\u2032 = E[fj\u2032 | Xj ] is not restricted to be zero as well. Therefore, no closed-form solution to the stationary condition (11) is available and we solve its sample version by fixed point iteration in (16). On the other hand, if Pjfj\u2032 is assumed to be zero for all distinct j, j\u2032 \u2208 g in (11), the solution of f\u0302g can be directly obtained by applying a soft-thresholding op-\nerator: f\u0302j =\n[ 1\u2212 \u03bb \u221a dg\n\u03c9\u0302g ] + P\u0302j , \u2200j \u2208 g. However, such\nan assumption, though leading to a simple closed-form solution of f\u0302g, is unrealistic in practice for within-group component functions."}, {"heading": "5. GroupSpAM with Overlap", "text": "In this section, we allow overlap between the groups in G and we are interested in estimating an additive model whose support is a union of groups. We adopt the approach of Jacob et al. (2009) by introducing a set of latent functions hg = {hgj \u2208 Hj}j\u2208g, one set for each group, and solve the following optimization problem\nmin f ,{hg}g\u2208G\nL(f) + \u03bb \u2211 g\u2208G \u221a dg\u2016hg\u2016\ns.t. \u2211 g:j\u2208g hgj = fj , j = 1, . . . , p. (17)\nThe idea is to decompose each original component function to be the sum of a set of latent functions and then apply the functional group penalty to the decomposition. As a consequence, the covariates that are not in any selected group are removed and the resulting support is a union of groups. Once we eliminate f from (17), the problem reduces to\nmin {hg}g\u2208G\n1 2 E [( Y \u2212 \u2211 g\u2208G \u2211 j\u2208g hgj (Xj) )2] + \u03bb \u2211 g\u2208G \u221a dg\u2016hg\u2016.\n(18) Algorithm 2 can be directly applied to solve (18) by treating each hg as a block."}, {"heading": "6. Experiments", "text": ""}, {"heading": "6.1. Simulation Study", "text": "We generate covariates with compound symmetry covariance structure as follows: each covariate Xj = (Wj + tU)/(1 + t), j = 1, . . . , p, where W1, . . . ,Wp and U are i.i.d from Uni(-2.5,2.5). For distinct covariates Xj and Xk, corr(Xj , Xk) = t\n2/(1 + t2). The setting t = 0 corresponds to the case of independent covariates. The sample size n = 150, and the dimension of covariates p = 200 and 1000.\nWe then generate responses from an additive model in Rp with two groups of relevant component functions, each of size four (Table 1): Y = m(X) + =\u22118 j=1 fj(Xj) + , where \u223c N (0, \u03c32). The component functions are drawn from Lin & Zhang (2006) and Ravikumar et al. (2009), and are appropriately scaled so that each group contains a function with relatively low variance. The standard deviation \u03c3 of the noise is carefully chosen to give a signal-to-noise ratio \u221a\nVar(m(X))/\u03c32 = 3 in the case of uncorrelated covariates (t = 0). For each training data, we also generate equal-sized validation data and test data in the same manner. Validation datasets are used to choose the value of the parameter \u03bb, and test datasets are used to measure the predication accuracy of the estimated models in terms of mean squared error (MSE).\nTable 2 summarizes the results of applying different methods to 100 simulated data for each value of p = 200, p = 1000 and t = 0, 1, 2. As expected, as the dimension (p) or the correlation (t) increases, the problem in general becomes more difficult. For all reported results, we assume a group structure with 50 or 250 blocks of 4 neighboring covariates for GroupSpAM and group lasso, and use Gaussian kernel smoothers with the plug-in bandwidths hj = 0.6s\u0302d(Xj)n\n\u22121/5 for jth covariate to implement GroupSpAM and SpAM. For group lasso, we use the implementation in the state-of-art package SLEP (Liu et al., 2009b); for COSSO1, we use the Matlab code available from http:\n1For p = 1000, we get many warning messages in running COSSO, which suggests that COSSO might not scale well to high-dimensional settings (see also Table 2).\n//www4.stat.ncsu.edu/~hzhang/cosso.html. In almost all cases, GroupSpAM is able to recover all the true supports of covariates with higher precisions and have lower MSE on test data. Compared to the group lasso, GroupSpAM has significantly improved the predictive power of estimates, and also leads to much better support recovery when t increases. The improvement is mainly in terms of variable selection as compared to SpAM, showing the benefit of considering the group sparsity in estimating additive models. It is particularly interesting to notice that the supports of component functions with relatively low variances, i.e., f3 and f7, are hard to be recovered as individual units by SpAM, but are much easier for GroupSpAM to select when combined with other components in the same group. Figure 1 shows the estimated component functions by GroupSpAM (solid blue line) versus the true component functions (dashed red line) in one typical simulation with p = 200, t = 0."}, {"heading": "6.2. Breast Cancer Data", "text": "In our second experiment, we apply GroupSpAM with overlap to a breast cancer dataset (van de Vijver et al., 2002) to demonstrate the potential advantage of using additive models with group sparsity in a real-world problem. The dataset consists of gene expression measurements for 8,141 genes collected from 295 breastcancer tumors (78 metastatic and 217 non-metastatic). We are interested in finding a sparse set of genes that can discriminate the two types of tumors. Instead of considering individual genes independently, a more powerful way is to build a predictive model that takes into account their pathway information. Some genes in a biological pathway are known to perform the same\nfunctionality in the cell, hence are more likely to be involved in the studied phenomenon in a group manner. Furthermore, each gene can participate in multiple pathways, and the studied phenomenon may possibly depend on the behavior of genes in a complex way. The GroupSpAM with overlap provides us with a natural and flexible way to incorporate these prior information into the biological analysis, where each group consists of the set of genes in a pathway and groups are potentially overlapping.\nAmong all 8, 141 genes, we focus on 3, 510 of them that belong to at least one canonical pathway in the Molecular Signatures Database (Subramanian et al., 2005). We further reduce the gene set to top 300 genes most correlated with the type of tumor by applying the sure independence screening (Fan & Lv, 2008). This step, which excludes the most irrelevant genes, is a common practice for analyzing microarray data.\nOverall, we obtain 1, 369 covariates (gene expression levels) in 432 groups after covariate duplication. Since the dataset is heavily unbalanced, we adopt a balanced loss function, where each positive (negative) sample is weighted by the proportion of negative (positive)\nsamples. To get the classification label from the nonparametric regression analysis, we simply take the sign of the predicted responses. Table 3 shows the results of GroupSpAM, SpAM and group lasso with overlap (Jacob et al., 2009) based on the balanced loss function by a 3-fold cross validation2. As we can see\n2When running COSSO, we ran into the same problem as in the simulation. Hence we left the results of COSSO.\nfrom Table 3, compared to SpAM, it achieves similar balanced error rates but with less selected genes and pathways, which could lead to an easier interpretation for genetic functional analysis. As compared to group lasso, GroupSpAM has an improved balanced error rate (P = 0.054), suggesting that a better predictive model can be built by using the more flexible additive model class. The functional relationship of the identified genes and pathways to breast cancer merits further investigation."}, {"heading": "7. Conclusions", "text": "In this paper, we propose a novel method for variable selection in nonparametric additive models when there exists a potentially overlapping group structure among the covariates. An efficient optimization algorithm is developed and promising results are obtained on both simulated and real data. An interesting future direction is to design new functional penalties to incorporate more rich structures among the covariates (e.g., hierarchical tree structure). Another future work is to investigate the asymptotic properties of the method, such as model selection and prediction consistency."}, {"heading": "Acknowledgments", "text": "This work is supported by NIH 1R01GM087694, NIH 1R01GM093156, and a Ray and Stephanie Lane Research Fellowship to JY. We would like to thank Guillaume Obozinski for helpful discussion of MKL and the anonymous reviewers for their valuable comments."}, {"heading": "Appendix: Proof of Theorem 1", "text": "Proof. Writing L(f) in (5) as a functional that depends on fg only, we obtain\nL(fg) = 1 2 E [( Rg \u2212 \u2211 j\u2208g fj(Xj) )2] .\nConsider a perturbation of L(fg) along the direction \u03b7g = {\u03b7j \u2208 Hj}j\u2208g,\nL(fg + \u03b7g) = 1 2 E [( Rg \u2212 \u2211 j\u2208g (fj(Xj) + \u03b7j(Xj)) )2] .\nThe first order approximation of L(fg + \u03b7g)\u2212 L(fg) is\n\u2211 j\u2208g E [ \u03b7j(Xj) (\u2211 j\u2032\u2208g fj\u2032(Xj\u2032)\u2212Rg )]\n= \u2211 j\u2208g E [ \u03b7j(Xj)E [(\u2211 j\u2032\u2208g fj\u2032(Xj\u2032)\u2212Rg ) | Xj ]] = \u2211 j\u2208g \u2329 \u03b7j(Xj),E [(\u2211 j\u2032\u2208g fj\u2032(Xj\u2032)\u2212Rg ) | Xj ]\u232a .\nIn the second step, we use the iterated expectation rule to condition on Xj ; in the last step, noting E [(\u2211 j\u2032\u2208g fj\u2032(Xj\u2032)\u2212Rg ) | Xj ] \u2208 Hj , we express the expectation in the form of inner product in Hj and thus obtain the gradient of L(fg) as\n\u2207L(fg) = { E [(\u2211\nj\u2032\u2208g\nfj\u2032(Xj\u2032)\u2212Rg ) | Xj ]} j\u2208g .\nDenote \u2126group(fg) = \u221a dg\u2016fg\u2016. The stationary condition of fg for minimizing L(fg) + \u03bb\u2126group(fg) is\nE [(\u2211\nj\u2032\u2208g\nfj\u2032(Xj\u2032)\u2212Rg ) | Xj ] + \u03bb \u221a dgsj = 0,\u2200j \u2208 g\nand can be rewritten in the form of conditional expectation operator Pj = E[\u00b7 | Xj ], as in equation (11)."}], "references": [{"title": "Consistency of the group lasso and multiple kernel learning", "author": ["F. Bach"], "venue": "JMLR, 9:1179\u20131225,", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Sure independence screening for ultrahigh dimensional feature space. JRSSB", "author": ["J. Fan", "J. Lv"], "venue": null, "citeRegEx": "Fan and Lv,? \\Q2008\\E", "shortCiteRegEx": "Fan and Lv", "year": 2008}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Technical report, Stanford University,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Generalized Additive Models", "author": ["T. Hastie", "R. Tibshirani"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Hastie and Tibshirani,? \\Q1990\\E", "shortCiteRegEx": "Hastie and Tibshirani", "year": 1990}, {"title": "The benefit of group sparsity", "author": ["J. Huang", "T. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Huang and Zhang,? \\Q2010\\E", "shortCiteRegEx": "Huang and Zhang", "year": 2010}, {"title": "Variable selection in nonparametric additive models", "author": ["J. Huang", "J. Horowitz", "F. Wei"], "venue": "Annals of Statistics,", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J. Vert"], "venue": "In ICML", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "Component selection and smoothing in multivariate nonparametric regression", "author": ["Y. Lin", "H. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Lin and Zhang,? \\Q2006\\E", "shortCiteRegEx": "Lin and Zhang", "year": 2006}, {"title": "Nonparametric regression and classification with joint sparsity constraints", "author": ["H. Liu", "J. Lafferty", "L. Wasserman"], "venue": "In NIPS", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "SLEP: Sparse Learning with Efficient Projections", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "HighDimensional additive modeling", "author": ["L. Meier", "S. van de Geer", "P. Buehlmann"], "venue": "Annals of Statistics,", "citeRegEx": "Meier et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meier et al\\.", "year": 2009}, {"title": "Gene set enrichment analysis: A knowledge-based approach for interpreting genome-wide expression profiles", "author": ["A Subramanian"], "venue": "PNAS, 102(43):15545\u201315550,", "citeRegEx": "Subramanian,? \\Q2005\\E", "shortCiteRegEx": "Subramanian", "year": 2005}, {"title": "Regression shrinkage and selection via the lasso. JRSSB", "author": ["R. Tibshirani"], "venue": null, "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "A gene-expression signature as a predictor of survival in breast cancer", "author": ["M van de Vijver"], "venue": "N. Engl. J. Med.,", "citeRegEx": "Vijver,? \\Q1999\\E", "shortCiteRegEx": "Vijver", "year": 1999}, {"title": "Model selection and estimation in regression with grouped variables. JRSSB", "author": ["M. Yuan", "Y. Lin"], "venue": null, "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}], "referenceMentions": [{"referenceID": 12, "context": "`1-regularized methods, such as lasso (Tibshirani, 1996), are among the most widely used approaches for variable selection in linear models.", "startOffset": 38, "endOffset": 56}, {"referenceID": 0, "context": "While some recent progress has been made on this problem by proposing various functional penalties (Lin & Zhang, 2006; Bach, 2008; Ravikumar et al., 2009), none of these methods is capable of exploiting the structural information among the covariates that may exist as prior knowledge in many applications.", "startOffset": 99, "endOffset": 154}, {"referenceID": 2, "context": "In this sense, GroupSpAM can be viewed as a nonparametric extension of the generalized group lasso (Friedman et al., 2010), which allows non-orthonormal covariates within a group, to additive models.", "startOffset": 99, "endOffset": 122}, {"referenceID": 8, "context": "Liu et al. (2009a) extended SpAM to the multi-task setting; however, the group structure is imposed on the tasks instead of on the covariates being considered here.", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "Finally, it is important to emphasize that our work is substantially different from another line of research that also mixes group lasso and additive models (Bach, 2008; Meier et al., 2009; Huang et al., 2010).", "startOffset": 157, "endOffset": 209}, {"referenceID": 10, "context": "Finally, it is important to emphasize that our work is substantially different from another line of research that also mixes group lasso and additive models (Bach, 2008; Meier et al., 2009; Huang et al., 2010).", "startOffset": 157, "endOffset": 209}, {"referenceID": 5, "context": "Finally, it is important to emphasize that our work is substantially different from another line of research that also mixes group lasso and additive models (Bach, 2008; Meier et al., 2009; Huang et al., 2010).", "startOffset": 157, "endOffset": 209}, {"referenceID": 6, "context": "We adopt the approach of Jacob et al. (2009) by introducing a set of latent functions h = {hgj \u2208 Hj}j\u2208g, one set for each group, and solve the following optimization problem min f ,{h}g\u2208G L(f) + \u03bb \u2211 g\u2208G \u221a dg\u2016h\u2016", "startOffset": 25, "endOffset": 45}, {"referenceID": 6, "context": "Table 3 shows the results of GroupSpAM, SpAM and group lasso with overlap (Jacob et al., 2009) based on the balanced loss function by a 3-fold cross validation.", "startOffset": 74, "endOffset": 94}], "year": 2012, "abstractText": "We consider the problem of sparse variable selection in nonparametric additive models, with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly. Previous works either study the group sparsity in the parametric setting (e.g., group lasso), or address the problem in the nonparametric setting without exploiting the structural information (e.g., sparse additive models). In this paper, we present a new method, called group sparse additive models (GroupSpAM), which can handle group sparsity in additive models. We generalize the `1/`2 norm to Hilbert spaces as the sparsityinducing penalty in GroupSpAM. Moreover, we derive a novel thresholding condition for identifying the functional sparsity at the group level, and propose an efficient block coordinate descent algorithm for constructing the estimate. We demonstrate by simulation that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models, and also conduct a comparative experiment on a real breast cancer dataset.", "creator": "LaTeX with hyperref package"}}}