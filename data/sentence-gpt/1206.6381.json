{"id": "1206.6381", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Shortest path distance in random k-nearest neighbor graphs", "abstract": "Consider a weighted or unweighted k-nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R^d. We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity. We prove that for unweighted kNN graphs, this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning. We also study the behavior of the shortest path distance in weighted kNN graphs. We also report that it is possible to measure the correlation between time variance in the distribution of shortest path distance with the corresponding data points. We conclude that the similarity in the distance of shortest path distance to shortest path distance is similar for unweighted kNN graphs, as shown in the figure above. Finally, we conclude that the distribution of shortest path distance in weighted kNN graphs has a long-term effect on machine learning.\n\n\n\nA number of important caveats apply to the model-driven optimization of KNN algorithms, as well as other statistical techniques used to train data sets. A number of important caveats apply to the model-driven optimization of KNN algorithms, as well as other statistical techniques used to train data sets.\n\nIt is currently unknown whether such a modeling technique could produce a similar results if it were implemented using a more rigorous algorithm in the near future. We recommend an approach that would allow the optimization of KNN algorithms to be implemented for the purposes of model-driven optimization.\nThe following papers were presented from the March 6th, 2014 paper on KNN algorithms.\nA detailed analysis of all the KNN algorithms used for the comparison of kNN algorithm, the probability of KNN algorithms being used in the comparison of kNN algorithm, and the probability of an algorithm being used as an alternative.\nThe following papers discussed the optimization of KNN algorithms using a different set of KNN algorithms.\nIn the original paper, KNN algorithm algorithm was implemented by J.J.J. (2008).\nA further paper on KNN algorithms by B.K. (2004).\nThe final paper, Part III of the paper, and Appendix I of the paper is included in the following Appendix A of the paper:\nThe algorithm is described as:\nKNN algorithms in KNN are implemented as a simple, fully interactive and efficient algorithm to understand how the most efficient data sets in KNN algorithms are constructed. The main difference is the KNN algorithm is very efficient and it has the best", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (669kb)", "http://arxiv.org/abs/1206.6381v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"], ["v2", "Mon, 9 Jul 2012 08:36:42 GMT  (578kb,D)", "http://arxiv.org/abs/1206.6381v2", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["morteza alamgir", "ulrike von luxburg"], "accepted": true, "id": "1206.6381"}, "pdf": {"name": "1206.6381.pdf", "metadata": {"source": "CRF", "title": "Shortest path distance in random k-nearest neighbor graphs", "authors": ["Morteza Alamgir", "Ulrike von Luxburg"], "emails": ["morteza@tuebingen.mpg.de", "ulrike.luxburg@tuebingen.mpg.de"], "sections": [{"heading": "1. Introduction", "text": "The shortest path distance is the most fundamental distance function between vertices in a graph, and it is widely used in computer science and machine learning. In this paper we want to understand the geometry induced by the shortest path distance in randomly generated geometric graphs like k-nearest neighbor graphs.\nConsider a neighborhood graph G built from an i.i.d. sample X1, ..., Xn drawn according to some density p on X \u2282 Rd (for exact definitions see Section 2). Assume that the sample size n goes to infinity. Two questions arise about the behavior of the shortest path distance between fixed points in this graph:\n1. Weight assignment: Given a distance measure D on X , how can we assign edge weights such that the shortest path distance in the graph converges to D?\n2. Limit distance: Given a function h that assigns weights of the form h(\u2016Xi\u2212Xj\u2016) to edges in G, what is the limit of the shortest path distance in this weighted graph as n\u2192\u221e?\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nThe first question has already been studied in some special cases. Tenenbaum et al. (2000) discuss the case of \u03b5- and kNN graphs when p is uniform and D is the geodesic distance. Sajama & Orlitsky (2005) extend these results to \u03b5-graphs from a general density p by introducing edge weights that depend on an explicit estimate of the underlying density. In a recent preprint, Hwang & Hero (2012) consider completely connected graphs whose vertices come from a general density p and whose edge weights are powers of distances.\nThere is little work regarding the second question. Tenenbaum et al. (2000) answer the question for a very special case with h(x) = x and uniform p. Hwang & Hero (2012) study the case h(x) = xa, a > 1 for arbitrary density p.\nWe have a more general point of view. In Section 4 we show that depending on properties of the function h(x), the shortest path distance operates in different regimes, and we find the limit of the shortest path distance for particular function classes of h(x). Our method also reveals a direct way to answer the first question without explicit density estimation.\nAn interesting special case is the unweighted kNN graph, which corresponds to the constant weight function h(x) = 1. We show that the shortest path distance on unweighted kNN-graphs converges to a limit distance on X that does not conform to the natural intuition and induces a geometry on X that can be detrimental for machine learning applications.\nOur results have implications for many machine learning algorithms, see Section 5 for more discussion. (1) The shortest paths based on unweighted kNN graphs prefer to go through low density regions, and they even accept large detours if this avoids passing through high density regions (see Figure 1 for an illustration). This is exactly the opposite of what we would like to achieve in most applications. (2) For manifold learning algorithms like Isomap, unweighted kNN graphs introduce a fundamental bias that leads to huge distortions in the estimated manifold structure (see Figure 2 for an\nillustration). (3) In the area of semi-supervised learning, a standard approach is to construct a graph on the sample points, then compute a distance between vertices of the graph, and finally use a standard distancebased classifier to label the unlabeled points (e.g., Sajama & Orlitsky, 2005 and Bijral et al., 2011). The crucial property exploited in this approach is that distances between points should be small if they are in the same high-density region. Shortest path distances in unweighted kNN graphs and their limit distances do exactly the opposite, so they can be misleading for this approach."}, {"heading": "2. Basic definitions", "text": "Consider a closed, connected subset X \u2286 Rd that is endowed with a density function p with respect to the Lebesgue measure. For the ease of presentation we assume for the rest of the paper that the density p is Lipschitz continuous with Lipschitz constant L and bounded away from 0 by pmin > 0. To simplify notation later on, we define the shorthand q(x) := (p(x))1/d.\nWe will consider different metrics on X . A ball with respect to a particular metric d in X will be written as B(x, r, d) := {y \u2208 X | d(x, y) \u2264 r}. We denote the Euclidean volume of the unit ball in Rd by \u03b7d.\nAssume the finite dataset X1, ..., Xn has been drawn i.i.d according to p. We build a geometric graph G = (V,E) that has the data points as vertices and connects vertices that are close. Specifically, for the kNN graph we connect Xi with Xj if Xi is among the k nearest neighbors of Xj or vice versa. For the \u03b5-graph, we connect Xi and Xj whenever their Euclidean distance\nsatisfies \u2016Xi \u2212 Xj\u2016 \u2264 \u03b5. In this paper, all graphs are undirected, but might carry edge weights wij \u2265 0. In unweighted graphs, we define the length of a path by its number of edges, in weighted graphs we define the length of a path by the sum of the edge weights along the path. In both cases, the shortest path (SP) distance Dsp(x, y) between two vertices x, y \u2208 V is the length of the shortest path connecting them.\nLet f be a positive continuous scalar function defined on X . For a given path \u03b3 in X that connects x with y and is parameterized by t, we define the f -length of the path as\nDf,\u03b3 = \u222b \u03b3 f(\u03b3(t))|\u03b3\u2032(t)|dt.\nThis expression is also known as the line integral along \u03b3 with respect to f . The f -geodesic path between x and y is the path with minimum f -length.\nThe f -length of the geodesic path is called the f - distance between x and y. We denote it by df (x, y). If f(x) is a function of the density p at x, then the f - distance is sometimes called a density based distance (Sajama & Orlitsky, 2005).\nThe f -distance on X is a metric, and in particular it satisfies the triangle inequality. Another useful property is that for a point u on the f -geodesic path between x and y we have Df (x, y) = Df (x, u)+Df (u, y).\nThe function f determines the behavior of the f - distance. When f(x) is a monotonically decreasing function of density p(x), passing through a high density region will cost less than passing through a low density region. It works the other way round when f is a monotonically increasing function of density. A constant function does not impose any preference between low and high density regions.\nThe main purpose of this paper is to study the relationship between the SP distance in various geometric graphs and particular f -distances on X . For example, in Section 3 we show that the SP distance in unweighted kNN graphs converges to the f -distance with f(x) = p(x)1/d.\nIn the rest of the paper, all statements refer to points x and y in the interior of X such that their f -geodesic path is bounded away from the boundary of X ."}, {"heading": "3. Shortest paths in unweighted graphs", "text": "In this section we study the behavior of the shortest path distance in the family of unweighted kNN graphs. We show that the rescaled graph SP distance converges to the q-distance in the original space X .\nTheorem 1 (SP limit in unweighted kNN graphs) Consider the unweighted kNN graph Gn based on the i.i.d. sample X1, ..., Xn \u2208 X from the density p. Choose \u03bb and a such that\n\u03bb \u2265 4L \u03b7 1/d d p 1+1/d min\n(k n )1/d , a < 1\u2212 logk ( 4d(1 + \u03bb)2 ) .\nFix two points x = Xi and y = Xj. Then there exist e1(\u03bb, k),e2(\u03bb, k, n),e3(\u03bb) (see below for explicit definitions) such that with probability at least 1 \u2212 3e3n exp(\u2212\u03bb2ka/6) we have\ne1Dq(x, y) \u2264 e2Dsp(x, y) \u2264 Dq(x, y)\u2212 e2.\nMoreover if n \u2192 \u221e, k \u2192 \u221e, k/n \u2192 0, \u03bb \u2192 0 and \u03bb2ka/ log(n) \u2192 \u221e, then the probability converges to 1 and (k/(\u03b7dn))\n1/dDsp(x, y) converges to Dq(x, y) in probability.\nThe convergence conditions on n and k are the ones to be expected for random geometric graphs. The condition \u03bb2ka/ log(n) \u2192 \u221e is slightly stronger than the usual k/ log(n)\u2192\u221e condition. This condition is satisfied as soon as k is of the order a bit larger than log(n). For example k \u2248 log(n)1+\u03b1 with a small \u03b1 will work. For k smaller than log(n), the graphs are not connected anyway (see e.g. Penrose, 1999) and are unsuitable for machine learning applications.\nBefore proving Theorem 1, we need to state a couple of propositions and lemmas. We start by introducing some ad-hoc notation:\nDefinition 2 (Connectivity parameters) Consider a geometric graph based on a fixed set of points X1, ..., Xn \u2208 Rd. Let rlow be a real number such that Df (Xi, Xj) \u2264 rlow implies that Xi is connected to Xj in the graph. Analogously, consider rup to be a real number such that Df (Xi, Xj) \u2265 rup implies that Xi is not connected to Xj in the graph.\nDefinition 3 (Dense sampling assumption) Consider a graph G with connectivity parameters rlow and rup. We say that it satisfies the dense sampling assumption if there exists an \u03c2 < rlow/4 such that for all x \u2208 X there exists a vertex y in the graph with Df (x, y) \u2264 \u03c2.\nProposition 4 (Bounding Dsp by Df) Consider any unweighted geometric graph based on a fixed set X1, ..., Xn \u2208 X \u2282 Rd that satisfies the dense sampling assumption. Fix two vertices x and y of the graph and set\ne1 = (rlow \u2212 2\u03c2)/rup , e2 = rlow \u2212 2\u03c2.\nThen the following statement holds:\ne1Df (x, y) \u2264 e2Dsp(x, y) \u2264 Df (x, y)\u2212 e2.\nProof. Right hand side. Consider the f -geodesic path \u03b3\u2217x,y connecting x to y. Divide \u03b3 \u2217 x,y to segments by u0 = x, u1, ..., ut, ut+1 = y such that Df (ui, ui+1) = rlow\u22122\u03c2 for i = 0, ..., t\u22121 and Df (ut, ut+1) \u2264 rlow\u22122\u03c2 (see Figure 3). Because of the dense sampling assumption, for all i = 1, ..., t there exists a vertex vi in the ball B(ui, \u03c2;Df ) and we have\nDf (vi, ui) \u2264 \u03c2 Df (ui, ui+1) \u2264 rlow \u2212 2\u03c2\nDf (ui+1, vi+1) \u2264 \u03c2.\nApplying the triangle inequality gives Df (vi, vi+1) \u2264 rlow, which shows that vi and vi+1 are connected. By summing up along the path we get\n(rlow \u2212 2\u03c2)(Dsp(x, y)\u2212 1) \u2264 (rlow \u2212 2\u03c2)t\n= t\u22121\u2211 i=0 Df (ui, ui+1) (a) \u2264 Df (x, y).\nIn step (a) we use the simple fact that if u is on the f -geodesic path from x to y, then\nDf (x, y) = Df (x, u) +Df (u, y).\nLeft hand side. Assume that the graph SP between x and y consists of vertices z0 = x, z1, ..., zs = y. By Df (zi, zi+1) \u2264 rup we can write\n(rlow \u2212 2\u03c2)Dsp(x, y) \u2265 rlow \u2212 2\u03c2 rup\n\u2211s\u22121 i=0 Df (zi, zi+1)\n\u2265 rlow \u2212 2\u03c2 rup Df (x, y).\n2\nThe next lemma uses the Lipschitz continuity and boundedness of p to show that q(x)\u2016x \u2212 y\u2016 is a good approximation of Dq(x, y) in small intervals.\nLemma 5 (Approximating Dq in small balls) Consider any given \u03bb < 1. If \u2016x\u2212 y\u2016 \u2264 pmin\u03bb/L then the following statements hold:\n1. We can approximate p(y) by the density at x:\np(y)(1\u2212 \u03bb) \u2264 p(x) \u2264 p(y)(1 + \u03bb).\n2. We can approximate Dq(x, y) by q(x)\u2016x\u2212 y\u2016:\n(1\u2212\u03bb)1/dq(x)\u2016x\u2212y\u2016 \u2264 Dq(x, y) \u2264 (1+\u03bb)1/dq(x)\u2016x\u2212y\u2016.\nProof. Part (1). By the Lipschitz continuity of p for \u2016x\u2212 y\u2016 \u2264 \u03b4 we have\n|p(x)\u2212 p(y)| \u2264 L\u2016x\u2212 y\u2016 \u2264 L\u03b4.\nSetting \u03b4 = \u03bbpmin/L leads to the result.\nPart (2). The previous part can be written as\n(1\u2212 \u03bb)1/dq(x) \u2264 q(y) \u2264 (1 + \u03bb)1/dq(x).\nDenote the q-geodesic path between x and y by \u03b3\u2217 and the line segment connecting x to y by l. Using the definition of a q-geodesic path, we can write\u222b\n\u03b3\u2217 q(\u03b3\u2217(t))|\u03b3\u2217(t)\u2032|dt \u2264 \u222b l q(l(t))|l(t)\u2032|dt \u2264\n(1 + \u03bb)1/d \u222b l q(x)|l(t)\u2032|dt = (1 + \u03bb)1/dq(x)\u2016x\u2212 y\u2016.\nAlso,\u222b \u03b3\u2217 q(\u03b3\u2217(t))|\u03b3\u2217(t)\u2032|dt \u2265 (1\u2212 \u03bb)1/d \u222b \u03b3\u2217 q(x)|\u03b3\u2217(t)\u2032|dt\n\u2265 (1\u2212 \u03bb)1/dq(x)\u2016x\u2212 y\u2016.\n2\nNow we are going to show how the quantities rlow and rup introduced in Definition 2 can be bounded in random unweighted kNN graphs and how they are related to the metric Dq.\nTo this end, we define the kNN q-radii at vertex x as Rq,k(x) = Dq(x, y) and the approximated kNN q-radii at vertex x as R\u0302q,k(x) = q(x)\u2016x \u2212 y\u2016, where y is the k-nearest neighbor of x. The minimum and maximum values of kNN q-radii are defined as\nRminq,k = min u Rq,k(u) , R max q,k = max u Rq,k(u).\nAccordingly we define R\u0302minq,k and R\u0302 max p,k for the approximated q-radii.\nThe following proposition is a direct adaptation of Proposition 31 from von Luxburg et al. (2010).\nProposition 6 (Bounding Rminp,k and R max p,k ) Given \u03bb < 1/2 define rlow and rup as\nrlow := ( k\n(1 + \u03bb)n\u03b7d\n)1/d , rup := ( k (1\u2212 \u03bb)n\u03b7d )1/d and radius r\u0302low and r\u0302up as\nr\u0302low = rlow\n(1 + \u03bb)1/d , r\u0302up = rup (1\u2212 \u03bb)1/d .\nAssume that r\u0302up \u2264 \u03bbp1+1/dmin /L. Then\nP ( Rminp,k \u2264 rlow ) \u2264 n exp(\u2212\u03bb2k/6)\nP ( Rmaxp,k \u2265 rup ) \u2264 n exp(\u2212\u03bb2k/6).\nProof. Consider a ball Bx with radius r\u0302low/q(x) around x. Note that r\u0302low/q(x) \u2264 pmin\u03bb/L , so we can bound the density of points in Bx by (1 + \u03bb)p(x) using Lemma 5. Denote the probability mass of the ball by \u00b5(x), which is bounded by\n\u00b5(x) = \u222b Bx p(s)ds \u2264 (1 + \u03bb)p(x) \u222b Bx ds\n= (1 + \u03bb)r\u0302dlow\u03b7d =: \u00b5max.\nObserve that R\u0302q,k(x) \u2264 r\u0302low if and only if there are at least k data points in Bx. Let Q \u223c Binomial(n, \u00b5(x)) and S \u223c Binomial(n, \u00b5max). By the choice of r\u0302low we have E(S) = k/(1 + \u03bb). It follows that\nP ( R\u0302q,k(x) \u2264 r\u0302low ) = P ( Q \u2265 k ) \u2264 P ( S \u2265 k ) = P ( S \u2265 (1 + \u03bb)E(S) ) .\nNow we apply a concentration inequality for binomial random variables (see Prop. 28 in von Luxburg et al., 2010) and a union bound to get\nP ( R\u0302minq,k \u2264 r\u0302low ) \u2264 P ( \u2203i : R\u0302q,k(Xi) \u2264 r\u0302low ) \u2264 n exp ( \u2212\u03bb2k 3(1 + \u03bb)\n) \u2264 n exp(\u2212\u03bb2k/6).\nBy a similar argument we can prove the analogous statement for Rmaxp,k . Finally, Lemma 5 gives\nR\u0302minq,k \u2265 Rminq,k\n(1 + \u03bb)1/d , R\u0302maxq,k \u2264 Rmaxq,k (1\u2212 \u03bb)1/d .\n2\nThe following proposition shows how the sampling parameter \u03c2 can be chosen to satisfy the dense sampling\nassumption. Note that we decided to choose \u03c2 in a form that keeps the statements in Theorem 1 simple, rather than optimizing over the parameter \u03c2 to maximize the probability of success.\nLemma 7 (Sampling lemma) Assume X1, ..., Xn are sampled i.i.d. from a probability distribution p and a constant a < 1 is given. Set \u03bb as in Theorem 1,\n\u03c2 := (1 + \u03bb)1/d ( ka \u03b7dn )1/d ,\nand e3(\u03bb) := 2 d/(1 \u2212 \u03bb)2. Then with probability at least 1 \u2212 e3n exp(\u2212ka/6), for every x \u2208 X exists a y \u2208 X1, ..., Xn such that Dq(x, y) \u2264 \u03c2.\nProof. Define \u03c20 = (1 + \u03bb) \u22121/d\u03c2. We prove that for every x \u2208 X , there exist a vertex y such that q(x)\u2016x\u2212 y\u2016 \u2264 \u03c20. Then using Lemma 5 will give the result.\nThe proof idea is a generalization of the covering argument in the proof of the sampling lemma in Tenenbaum et al. (2000). We first construct a covering of X that consists of balls with approximately the same probability mass. The centers of the balls are chosen by an iterative procedure that ensures that no center is contained in any of the balls we have so far. We choose the radius \u03c20/q(x) for the ball at point x and call it Bq(x, \u03c20). The probability mass of this ball can be bounded by\nV(Bq(x, \u03c20)) \u2265 (1\u2212 \u03bb)\u03c2d0\u03b7d\nNote that smaller balls Bq(u, (1 \u2212 \u03bb)1/d\u03c20/2) are all disjoint. To see this, consider two balls Bq(x, (1 \u2212 \u03bb)1/d\u03c20/2), Bq(y, (1\u2212 \u03bb)1/d\u03c20/2). Observe that\n(1\u2212 \u03bb)1/d\u03c20 2q(x) + (1\u2212 \u03bb)1/d\u03c20 2q(y) \u2264 \u03c20 q(x) .\nWe can bound the total number of balls by\nS \u2264 1 V(Bq(x, (1\u2212 \u03bb)1/d\u03c20/2)) \u2264 2 d \u03b7d(1\u2212 \u03bb)2\u03c2d0 .\nNow we sample points from the underlying space and apply the same concentration inequality as above. We bound the probability that a ball Bq(u, \u03c20) does not contain any sample point (\u201cis empty\u201d) by\nPr(Ball i is empty) \u2264 exp(\u2212n\u03c2d0\u03b7d/6).\nRewriting and Substituting the value of \u03c20 gives Pr(no ball is empty) \u2265 1\u2212 \u2211 iPr(Bi is empty)\n\u2265 1\u2212 S \u00b7 e\u2212n\u03c2 d 0 \u03b7d/6 \u2265 1\u2212 2\ndne\u2212k a/6 (1\u2212 \u03bb)2ka\n\u2265 1\u2212 2 dne\u2212k a/6\n(1\u2212 \u03bb)2 = 1\u2212 e3ne\u2212k a/6.\n2\nProof of Theorem 1. Set rlow and rup as in Proposition 6. The assumption on \u03bb ensures that r\u0302up \u2264 \u03bbp1+1/dmin /L. It follows from Proposition 6 that the statements about rlow and rup in Definition 2 both hold for Gn with probability at least \u00b51 = 1 \u2212 2n exp(\u2212\u03bb2k/6). Set \u03c2 as in Lemma 7 and define the constant a < 1\u2212 logk ( 4d(1 + \u03bb)2 ) . By this choice we have rlow > 4\u03c2. Lemma 7 shows that the sampling assumption holds in Gn for the selected \u03c2 with probability at least \u00b52 = 1\u2212e3n exp(\u2212ka/6). Together, all these statements about Gn hold with probability at least \u00b5 := 1\u2212 3e3n exp(\u2212\u03bb2ka/6).\nUsing Proposition 4 completes the first part of the theorem. For the convergence we have\ne1 = rlow \u2212 2\u03c2 rup\n= (1\u2212 \u03bb\n1 + \u03bb )1/d \u2212 2 (1\u2212 \u03bb2 k1\u2212a )1/d .\nThis shows that e1 \u2192 1 as \u03bb \u2192 0 and k \u2192 \u221e. For \u03bb \u2192 0 and k \u2192 \u221e we can set a to any constant smaller than 1. Finally it is easy to check that e2 \u2192 0 and \u03c2/rlow \u2192 0. 2"}, {"heading": "4. Shortest paths in weighted graphs", "text": "In this section we discuss both questions from the Introduction. We also extend our results from the previous section to weighted kNN graphs and \u03b5-graphs."}, {"heading": "4.1. Weight assignment problem", "text": "Consider a graph based on the i.i.d. sample X1, ..., Xn \u2208 X from the density p. We are given a positive scalar function f which is only a function of the density: f(x) = f\u0303(p(x)). We want to assign edge weights such that the graph SP distance converges to the f -distance in X .\nIt is well known that the f -length of a curve \u03b3 : [a, b]\u2192 X can be approximated by a Riemann sum over a partition of [a, b] to subintervals [xi, xi+1]:\nD\u0302f,\u03b3 = \u2211 i f ( \u03b3(xi)+\u03b3(xi+1) 2 ) \u2016\u03b3(xi)\u2212 \u03b3(xi+1)\u2016.\nAs the partition gets finer, the approximation D\u0302f,\u03b3 converges to Df,\u03b3 (cf. Chapter 3 of Gamelin, 2007). This suggests using edge weights\nwij = f\u0303 ( p( Xi+Xj 2 ) ) \u2016Xi \u2212Xj\u2016.\nHowever the underlying density p(x) is not known in many machine learning applications. Sajama & Orlitsky (2005) already proved that the plug-in approach\nusing a kernel density estimator p\u0302(x) for p(x) will lead to the convergence of the SP distance to f -distance in \u03b5-graphs. Our next result hows how to choose edge weights in kNN graphs without estimating the density. It is a corollary from a theorem that will be presented in Section 4.2.\nWe use a notational convention to simplify our arguments and hide approximation factors that will eventually go to 1 as the sample size goes to infinity. We say that f is approximately larger than g (f <\u03bb g) if there exists a function e(\u03bb) such that f \u2265 e(\u03bb)g and e(\u03bb)\u2192 1 as n\u2192\u221e and \u03bb\u2192 0. The symbol 4\u03bb is defined similarly. We use the notation f \u2248\u03bb g if f 4\u03bb g and f <\u03bb g.\nCorollary 8 (Weight assignment) Consider the kNN graph based on the i.i.d. sample X1, ..., Xn \u2208 X from the density p. Let f be of the form f(x) = f\u0303(p(x)) with f\u0303 increasing. We assume that f\u0303 is Lipschitz continuous and f is bounded away from 0. Set the edge weights\nwij = \u2016Xi \u2212Xj\u2016f\u0303 ( rd \u2016Xi \u2212Xj\u2016d ) . (1)\nFix two points x = Xi and y = Xj. Choose \u03bb and a as in Theorem 9. Then with probability at least 1 \u2212 3e3n exp(\u2212\u03bb2ka/6) we have Dsp(x, y) \u2248\u03bb Df (x, y)."}, {"heading": "4.2. Limit distance problem", "text": "Consider a weighted graph based on the i.i.d. sample X1, ..., Xn \u2208 X from the density p. We are given a increasing edge weight function h : R+ \u2192 R+ which assigns weight h(\u2016x \u2212 y\u2016) to the edge (x, y). We are interested in finding the limit of the graph SP distance with respect to edge weight function h as the sample size goes to infinity. In particular we are looking for a distance function f such that the SP distance converges to the f -distance.\nAssume we knew the solution f\u2217 = f\u0303\u2217(p(x)) of this problem. To guarantee the convergence of the distances, f\u2217 should assign weights of the form of wij \u2248 f\u0303\u2217(p(Xi))\u2016Xi \u2212Xj\u2016. This would mean\nf\u0303\u2217(p(Xi)) \u2248 h(\u2016Xi \u2212Xj\u2016) \u2016Xi \u2212Xj\u2016 ,\nwhich shows that determining f\u0303\u2217 is closely related to finding a density based estimation for \u2016Xi \u2212Xj\u2016.\nDepending on h, we distinguish two regimes for this problem: subadditive and superadditive."}, {"heading": "4.2.1. Subadditive weights", "text": "A function h(x) is called subadditive if \u2200x, y \u2265 0 : h(x)+h(y) \u2265 h(x+y). Common examples of subadditive functions are f(x) = xa, a < 1 and f(x) = xe\u2212x. For a subadditive h, the SP in the graph will satisfy the triangle inequality and it will prefer jumping along distant vertices. Based on this intuition, we come up with the following guess for vertices along the SP: For \u03b5-graphs we have the approximation \u2016Xi \u2212 Xj\u2016 \u2248 \u03b5 and f(x) = h(\u03b5)/\u03b5. For kNN-graphs we have \u2016Xi \u2212Xj\u2016 \u2248 r/q(Xi) with r = (k/(n\u03b7d))1/d and\nf(x) = h( r q(x) ) q(x) r , f\u0303(x) = h( r x1/d ) x1/d r .\nWe formally prove this statement for kNN graphs in the next theorem. In contrast to Theorem 1, the scaling factor is moved into f . The proof for \u03b5-graphs is much simpler and can be adapted by setting r = \u03b5, q(x) = 1, and rlow = rup = \u03b5.\nTheorem 9 (Limit of SP in weighted graphs) Consider the kNN graph based on the i.i.d. sample X1, ..., Xn \u2208 X from the density p. Let h be an increasing, Lipschitz continuous and subadditive function, and define the edge weights wij = h(\u2016Xi \u2212Xj\u2016). Fix two points x = Xi and y = Xj. Define r = (k/(n\u03b7d)) 1/d and set\nf(x) = h( r q(x) ) q(x) r .\nChoose \u03bb and a such that\n\u03bb \u2265 4L \u03b7 1/d d p 1+1/d min\n(k n )1/d , a < 1\u2212 logk ( 4d(1 + \u03bb)2 ) .\nThen with probability at least 1 \u2212 3e3n exp(\u2212\u03bb2ka/6) we have Dsp(x, y) \u2248\u03bb Df (x, y).\nProof. The essence of the proof is similar to the one in Theorem 1, we present a sketch only. The main step is to adapt Proposition 4 to weighted graphs with weight function h. Adapting Lemma 5 for general f is straightforward. The lemma states that Df (x, y) \u2248\u03bb f(x)\u2016x \u2212 y\u2016 for nearby points. We set rlow and \u03c2 as in the sampling lemma and Proposition 6 (these are properties of kNN graphs and hold for any f). Proposition 6 says that in kNN graphs, x is connected to y with high probability iff \u2016x\u2212y\u2016 4\u03bb r/q(x). The probabilistic argument and the criteria on choosing \u03bb are similar to Theorem 1.\nFirst we show that Dsp(x, y) 4\u03bb Df (x, y). Consider the f -geodesic path \u03b3\u2217x,y connecting x to y. Divide \u03b3\u2217x,y into segments u0 = x, u1, ..., ut, ut+1 = y such that Dq(ui, ui+1) = rlow \u2212 2\u03c2 for i = 0, ..., t \u2212 1\nand Dq(ut, ut+1) \u2264 rlow \u2212 2\u03c2 (see Figure 3). There exists a vertex vi near to ui such that vi and vi+1 are connected. We show that the length of the path x, v1, ..., vt, y is approximately smaller than Df (x, y). From the path construction we have\n\u2016vi \u2212 vi+1\u2016 \u2248\u03bb \u2016ui \u2212 ui+1\u2016 \u2248\u03bb r/q(ui).\nBy summing up along the path we get Dsp(x, y) \u2264 \u2211 i h(\u2016vi \u2212 vi+1\u2016)\n\u2248\u03bb \u2211 i h(\u2016ui \u2212 ui+1\u2016) \u2248\u03bb \u2211 i h( r q(ui) )\n= \u2211 i f(ui) r q(ui) \u2248\u03bb \u2211 i f(ui)\u2016ui \u2212 ui+1\u2016\nFrom the adaptation of Lemma 5 we have Df (ui, ui+1) \u2248\u03bb f(ui)\u2016ui \u2212 ui+1\u2016, which gives\u2211\ni f(ui)\u2016ui \u2212 ui+1\u2016 \u2248\u03bb \u2211 iDf (ui, ui+1) = Df (x, y).\nThis shows that Dsp(x, y) 4\u03bb Df (x, y).\nFor the other way round, we use a technique different from Proposition 4. Denote the graph shortest path between x and y by \u03c0 : z0 = x, z1, ..., zs, zs+1 = y. Consider \u03c0\u2032 as a continuous path in X corresponding to \u03c0. As in the previous part, divide \u03c0\u2032 into segments u0 = x, u1, ..., ut, ut+1 = y (see Figure 3). From Dq(zi, zi+1) 4\u03bb r and Dq(ui, ui+1) \u2248\u03bb r we have s <\u03bb t. Using this and the subadditivity of h we get\nDsp(x, y) = \u2211 i h(\u2016zi \u2212 zi+1\u2016) <\u03bb \u2211 i h(\u2016ui \u2212 ui+1\u2016).\nTo prove Dsp(x, y) <\u03bb Df (x, y), we can write\u2211 i h(\u2016ui \u2212 ui+1\u2016) \u2248\u03bb \u2211 i h( r q(ui) ) = \u2211 i f(ui)r q(ui)\n\u2248\u03bb \u2211 i f(ui)\u2016ui \u2212 ui+1\u2016\n\u2248\u03bb \u2211 iDf (ui, ui+1) \u2265 Df (x, y).\n2 The proof of Theorem 8 is a direct consequence of this theorem. It follows by choosing h(t) = tf\u0303(rd/td) (which is subadditive if f\u0303 is increasing) and setting wij = h(\u2016Xi \u2212Xj\u2016)."}, {"heading": "4.2.2. Supperadditive weights", "text": "A function h is called superadditive if \u2200x, y \u2265 0 : h(x)+h(y) \u2264 h(x+y). Examples are f(x) = xa; a > 1 and f(x) = xex. To get an intuition on the behavior of the SP for a superadditive h, take an example of three vertices x, y, z which are all connected in the graph and sit on a straight line such that \u2016x \u2212 y\u2016 + \u2016y \u2212 z\u2016 = \u2016x \u2212 z\u2016. By the superadditivity, the SP between x and z will prefer going through y rather than directly jumping to z. More generally, the graph SP will prefer taking many \u201csmall\u201d edges\nrather than fewer \u201clong\u201d edges. For this reason, we do not expect a big difference between superadditive weighted kNN graphs and \u03b5-graphs: the long edges in the kNN graph will not be used anyway. However, due to technical problems we did not manage to prove a formal theorem to this effect.\nThe special case of the superadditive family h(x) = xa, a > 1 is treated in Hwang & Hero (2012) by completely different methods. Although their results are presented for complete graphs, we believe that it can be extended to \u03b5 and kNN graphs. We are not aware of any other result for the limit of SP distance in the superadditive regime."}, {"heading": "5. Consequences in applications", "text": "In this section we study the consequences of our results on manifold embedding using Isomap and on a particular semi-supervised learning method.\nThere are two cases where we do not expect a drastic difference between the SP in weighted and unweighted kNN graph: (1) If the underlying density p is close to uniform. (2) If the intrinsic dimensionality of our data d is high. The latter is because in the q-distance, the underlying density arises in the form of p(x)1/d, where the exponent flattens the distribution for large d."}, {"heading": "5.1. Isomap", "text": "Isomap is a widely used method for low dimensional manifold embedding (Tenenbaum et al., 2000). The main idea is to use metric multidimensional scaling on the matrix of pairwise geodesic distances. Using the Euclidean length of edges as their weights will lead to the convergence of the SP distance to the geodesic distance. But what would be the effect of applying Isomap to unweighted graphs?\nOur results of the last section already hint that there is no big difference between unweighted and weighted \u03b5-graphs for Isomap. However, the case of kNN graphs is different because weighted and unweighted shortest paths measure different quantities. The effect of applying Isomap to unweighted kNN graphs can easily be demonstrated by the following simulation. We sample 2000 points in R2 from a distribution that has two uniform high-density squares, surrounded by a uniform low density region. An unweighted kNN graph is constructed with k = 10, and we apply Isomap with target dimension 2. The result is depicted in Figure 2. We can see that the Isomap embedding heavily distorts the original data: it stretches high density regions and compacts low density regions to make the vertex distribution close to uniform."}, {"heading": "5.2. Semi-supervised learning", "text": "Our work has close relationship to some of the literature on semi-supervised learning (SSL). In regularization based approaches, the underlying density is either exploited implicitly as attempted in Laplacian regularization (Zhu et al., 2003 but see Nadler et al., 2009; Alamgir & von Luxburg, 2011 and Zhou & Belkin, 2011), or more explicitly as in measure based regularization (Bousquet et al., 2004). Alternatively, one defines new distance functions on the data that take the density of the unlabeled points into account. Here, the papers by Sajama & Orlitsky (2005) and Bijral et al. (2011) are most related to our paper. Both papers suggest different ways to approximate the density based distance from the data. In Sajama & Orlitsky (2005) it is achieved by estimating the underlying density while in Bijral et al. (2011), the authors omit the density estimation and use an approximation.\nOur work shows a simpler way to converge to a similar distance function for a specific family of f -distances, namely constructing a kNN graph and assigning edge weights as in Equation 1."}, {"heading": "6. Conclusions and outlook", "text": "We have seen in this paper that the shortest path distance on unweighted kNN graphs has a very funny limit behavior: it prefers to go through regions of low density and even takes large detours in order to avoid the high density regions. In hindsight, this result seems obvious, but most people are surprised when they first hear about it. In particular, we believe that it is important to spread this insight among machine learning practitioners, who routinely use unweighted kNN-graphs as a simple, robust alternative to \u03b5-graphs.\nIn some sense, unweighted \u03b5-graphs and unweighted kNN graphs behave as \u201cduals\u201d of each other: while degrees in \u03b5-graphs reflect the underlying density, they are independent of the density in kNN graphs. While the shortest path in \u03b5-graphs is independent of the underlying density and converges to the Euclidean distance, the shortest paths in kNN graphs take the density into account.\nCurrent practice is to use \u03b5 and kNN graphs more or less interchangeably in many applications, and the decision for one or the other graph is largely driven by robustness or convenience considerations. However, as our results show it is important to be aware of the implicit consequences of this choice. Each graph carries different information about the underlying density, and depending on how a particular machine learning\nalgorithms makes use of the graph structures, it might either miss out or benefit from this information."}], "references": [{"title": "Phase transition in the familiy of p-resistances", "author": ["M. Alamgir", "U. von Luxburg"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Alamgir and Luxburg,? \\Q2011\\E", "shortCiteRegEx": "Alamgir and Luxburg", "year": 2011}, {"title": "Semi-supervised learning with density based distances", "author": ["A. Bijral", "N. Ratliff", "N. Srebro"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Bijral et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bijral et al\\.", "year": 2011}, {"title": "Measure based regularization", "author": ["O. Bousquet", "O. Chapelle", "M. Hein"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Bousquet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bousquet et al\\.", "year": 2004}, {"title": "Shortest path through random points", "author": ["Hwang", "S.J", "Damelin S. B", "A.O. Hero"], "venue": "In Preprint available at Arxiv,", "citeRegEx": "Hwang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2012}, {"title": "Semi-supervised learning with the graph Laplacian: The limit of infinite unlabelled data", "author": ["B. Nadler", "N. Srebro", "X. Zhou"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "Nadler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nadler et al\\.", "year": 2009}, {"title": "A strong law for the longest edge of the minimal spanning tree", "author": ["M. Penrose"], "venue": "Ann. of Prob.,", "citeRegEx": "Penrose,? \\Q1999\\E", "shortCiteRegEx": "Penrose", "year": 1999}, {"title": "Estimating and computing density based distance metrics", "author": ["Sajama", "A. Orlitsky"], "venue": "In International Conference on Machine learning (ICML),", "citeRegEx": "Sajama and Orlitsky,? \\Q2005\\E", "shortCiteRegEx": "Sajama and Orlitsky", "year": 2005}, {"title": "Supplementary material to \u201dA Global Geometric Framework for Nonlinear Dimensionality Reduction", "author": ["J. Tenenbaum", "V. de Silva", "J. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Hitting times, commute distances and the spectral gap in large random geometric graphs", "author": ["U. von Luxburg", "M. Hein", "A. Radl"], "venue": "In Preprint available at Arxiv,", "citeRegEx": "Luxburg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Luxburg et al\\.", "year": 2010}, {"title": "Semi-supervised learning by higher order regularization", "author": ["X. Zhou", "M. Belkin"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Zhou and Belkin,? \\Q2011\\E", "shortCiteRegEx": "Zhou and Belkin", "year": 2011}, {"title": "SemiSupervised Learning Using Gaussian Fields and Harmonic Functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty"], "venue": "In International Conference of Machine Learning (ICML),", "citeRegEx": "Zhu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 7, "context": "Tenenbaum et al. (2000) discuss the case of \u03b5- and kNN graphs when p is uniform and D is the geodesic distance.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Tenenbaum et al. (2000) discuss the case of \u03b5- and kNN graphs when p is uniform and D is the geodesic distance. Sajama & Orlitsky (2005) extend these results to \u03b5-graphs from a general density p by introducing edge weights that depend on an explicit estimate of the underlying density.", "startOffset": 0, "endOffset": 137}, {"referenceID": 7, "context": "Tenenbaum et al. (2000) discuss the case of \u03b5- and kNN graphs when p is uniform and D is the geodesic distance. Sajama & Orlitsky (2005) extend these results to \u03b5-graphs from a general density p by introducing edge weights that depend on an explicit estimate of the underlying density. In a recent preprint, Hwang & Hero (2012) consider completely connected graphs whose vertices come from a general density p and whose edge weights are powers of distances.", "startOffset": 0, "endOffset": 328}, {"referenceID": 7, "context": "Tenenbaum et al. (2000) answer the question for a very special case with h(x) = x and uniform p.", "startOffset": 0, "endOffset": 24}, {"referenceID": 7, "context": "Tenenbaum et al. (2000) answer the question for a very special case with h(x) = x and uniform p. Hwang & Hero (2012) study the case h(x) = x, a > 1 for arbitrary density p.", "startOffset": 0, "endOffset": 117}, {"referenceID": 8, "context": "The following proposition is a direct adaptation of Proposition 31 from von Luxburg et al. (2010). Proposition 6 (Bounding R p,k and R max p,k ) Given \u03bb < 1/2 define rlow and rup as", "startOffset": 76, "endOffset": 98}, {"referenceID": 7, "context": "The proof idea is a generalization of the covering argument in the proof of the sampling lemma in Tenenbaum et al. (2000). We first construct a covering of X that consists of balls with approximately the same probability mass.", "startOffset": 98, "endOffset": 122}, {"referenceID": 7, "context": "Isomap is a widely used method for low dimensional manifold embedding (Tenenbaum et al., 2000).", "startOffset": 70, "endOffset": 94}, {"referenceID": 2, "context": ", 2009; Alamgir & von Luxburg, 2011 and Zhou & Belkin, 2011), or more explicitly as in measure based regularization (Bousquet et al., 2004).", "startOffset": 116, "endOffset": 139}, {"referenceID": 1, "context": ", 2009; Alamgir & von Luxburg, 2011 and Zhou & Belkin, 2011), or more explicitly as in measure based regularization (Bousquet et al., 2004). Alternatively, one defines new distance functions on the data that take the density of the unlabeled points into account. Here, the papers by Sajama & Orlitsky (2005) and Bijral et al.", "startOffset": 117, "endOffset": 308}, {"referenceID": 1, "context": "Here, the papers by Sajama & Orlitsky (2005) and Bijral et al. (2011) are most related to our paper.", "startOffset": 49, "endOffset": 70}, {"referenceID": 1, "context": "Here, the papers by Sajama & Orlitsky (2005) and Bijral et al. (2011) are most related to our paper. Both papers suggest different ways to approximate the density based distance from the data. In Sajama & Orlitsky (2005) it is achieved by estimating the underlying density while in Bijral et al.", "startOffset": 49, "endOffset": 221}, {"referenceID": 1, "context": "Here, the papers by Sajama & Orlitsky (2005) and Bijral et al. (2011) are most related to our paper. Both papers suggest different ways to approximate the density based distance from the data. In Sajama & Orlitsky (2005) it is achieved by estimating the underlying density while in Bijral et al. (2011), the authors omit the density estimation and use an approximation.", "startOffset": 49, "endOffset": 303}], "year": 2012, "abstractText": "Consider a weighted or unweighted k-nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R. We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity. We prove that for unweighted kNN graphs, this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning. We also study the behavior of the shortest path distance in weighted kNN graphs.", "creator": "TeX"}}}