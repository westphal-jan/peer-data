{"id": "1206.6412", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound", "abstract": "In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. As a result, the algorithm is capable of identifying the optimal error bound by the integral operator. The algorithm consists of two functions (labeling the first variable) and the second function (labeling the last variable). The results of the algorithm are calculated using the regression function. The first function is the second function:\n\n\n\n\n\n\n\nThe second function is the second function:\n\n\nThe first function is the third function:\n\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\n\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe third function is the third function:\nThe", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (558kb)", "http://arxiv.org/abs/1206.6412v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ming ji", "tianbao yang", "binbin lin", "rong jin", "jiawei han 0001"], "accepted": true, "id": "1206.6412"}, "pdf": {"name": "1206.6412.pdf", "metadata": {"source": "CRF", "title": "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound", "authors": ["Ming Ji", "Tianbao Yang", "Binbin Lin", "Rong Jin", "Jiawei Han"], "emails": ["mingji1@illinois.edu", "yangtia1@msu.edu", "binbinlin@zju.edu.cn", "rongjin@cse.msu.edu", "hanj@illinois.edu"], "sections": [{"heading": "1. Introduction", "text": "Although numerous algorithms have been developed for semi-supervised learning (Zhu (2008) and references therein), most of them do not have theoretical guarantee on improving the generalization performance of supervised learning. A number of theories have been proposed for semi-supervised learning, and most of them are based on one of the two assumptions: (1) the cluster assumption (Seeger, 2001; Rigollet, 2007; Lafferty & Wasserman, 2007; Singh et al., 2008; Sinha & Belkin, 2009) which assumes that two data points should have the same class label or similar values if they are connected by a path passing through a high density region; (2) the manifold assumption (Lafferty & Wasserman, 2007; Niyogi, 2008)\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nwhich states that the prediction function lives in a low dimensional manifold of the marginal distribution PX .\nIt has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies (Castelli & Cover, 1995; 1996), the authors show that under the assumption that the marginal distribution PX is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet (2007) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples. Furthermore, Singh et al. (2008) show that the mixture components can be identified if PX is a mixture of a finite number of smooth density functions and the separation/overlap between different mixture components is significantly large. Despite the encouraging results, one major problem of the cluster assumption is that it is difficult to be verified given a limited number of labeled examples. In addition, the learning algorithms suggested in (Rigollet, 2007; Singh et al., 2008; Zhang & Ando, 2005) are difficult to implement efficiently even if the cluster assumption holds, making them unpractical\nfor real-world problems.\nIn this work, we aim to develop a simple algorithm for semi-supervised learning that on one hand is easy to implement, and on the other hand is guaranteed to improve the generalization performance of supervised learning under appropriate assumptions. The main idea of the proposed algorithm is to estimate the top eigenfunctions of the integral operator from the both labeled and unlabeled examples, and learn from the labeled examples the best prediction function in the subspace spanned by the estimated eigenfunctions. Unlike the previous studies of exploring eigenfunctions for semi-supervised learning (Fergus et al., 2009; Sinha & Belkin, 2009), we show that under appropriate assumptions, the proposed algorithm achieves a better generalization error bound than supervised learning algorithms.\nTo derive the generalization error bound, we make a different set of assumptions from previous studies. First, we assume a skewed eigenvalue distribution and bounded eigenfunctions of the integral operator. The assumption of skewed eigenvalue distributions has been verified and used in multiple studies of kernel learning (Koltchinskii, 2011; Steinwart et al., 2006; Minh, 2010; Zhang & Ando, 2005), while the assumption of bounded eigenvectors was mostly found in the study of compressive sensing (Cande\u0300s & Tao, 2006). Second, we assume that a sufficient number of labeled examples are available, which is also used by the other analysis of semi-supervised learning (Rigollet, 2007). It is the combination of these assumptions that allow us to derive better generalization error bound for semi-supervised learning.\nThe rest of the paper is arranged as follows. Section 2 presents the proposed algorithm and verifies its effectiveness by an empirical study. Section 3 shows the improved generalization error bound for the proposed semi-supervised learning, and Section 4 outlines the proofs. Section 5 concludes with future work."}, {"heading": "2. Algorithm and Empirical Validation", "text": "Let X be a compact domain or a manifold in the Euclidean space Rd. Let D = {xi, i = 1, . . . , N |xi \u2208 X} be a collection of training examples. We randomly select n examples from D for labeling. Without loss of generality, we assume that the first n examples are labeled by yl = (y1, . . . , yn)\n\u22a4 \u2208 Rn. We denote by y = (y1, . . . , yN )\n\u22a4 \u2208 RN the true labels for all the examples in D. In this study, we assume y = f(x) is decided by an unknown deterministic function f(x). Our goal is to learn an accurate prediction function by\nAlgorithm 1 A Simple Algorithm for Semisupervised Learning\n1: Input \u2022 D = {x1, . . . ,xN}: labeled and unlabeled examples\n\u2022 yl = (y1, . . . , yn)\u22a4: labels for the first n examples in D\n\u2022 s: the number of eigenfunctions to be used 2: Compute (\u03d5\u0302i, \u03bb\u0302i), i = 1, . . . , s, the first s eigen-\nfunctions and eigenvalues for the integral operator L\u0302N defined in (4). 3: Compute the prediction g\u0302(x) in (5), where \u03b3\u2217 = (\u03b3\u22171 , . . . , \u03b3 \u2217 s )\n\u22a4 is given by solving the following regression problem\n\u03b3\u2217 = argmin \u03b3\u2208Rs n\u2211 i=1  s\u2211 j=1 \u03b3j \u03d5\u0302j(xi)\u2212 yi 2 (1) 4: Output prediction function g\u0302(\u00b7)\nexploiting both labeled and unlabeled examples. Below we first present our algorithm and then verify its empirical performance by comparing to the state-ofthe-art algorithms for supervised and semi-supervised learning."}, {"heading": "2.1. A Simple algorithm for Semi-Supervised Learning", "text": "Let \u03ba(\u00b7, \u00b7) : X \u00d7 X \u2192 R be a Mercer kernel, and let H\u03ba be a Reproducing Kernel Hilbert space (RKHS) of functions X \u2192 R endowed with kernel \u03ba(\u00b7, \u00b7). We assume that \u03ba is a bounded function, i.e., |\u03ba(x,x)| \u2264 1, \u2200x \u2208 X . Similar to most semi-supervised learning algorithms, in order to effectively exploit the unlabeled data, we need to relate the prediction function f(x) to the unlabeled examples (or the marginal distribution PX ). To this end, we assume there exists an accurate prediction function g(x) \u2208 H\u03ba with \u2225g\u2225H\u03ba \u2264 R. More specifically, we define\n\u03b52 = min h\u2208H\u03ba,\u2225h\u2225H\u03ba\u2264R Ex[(f(x)\u2212 h(x))2], (2)\ng(x) = argmin h\u2208H\u03ba,\u2225h\u2225H\u03ba\u2264R\nEx[(f(x)\u2212 h(x))2]. (3)\nOur basic assumption (A0) is that the regression error \u03b52 \u226a R2 is small, and the maximum regression error of g(x) for any x \u2208 X is also small, i.e.,\nsup x\u2208X\n(f(x)\u2212 g(x))2 , \u03b52max = O(n\u03b52/ lnN).\nTo present our algorithm, we define an integral oper-\nator over the examples in D:\nL\u0302N (f)(\u00b7) = 1\nN N\u2211 i=1 \u03ba(xi, \u00b7)f(xi), (4)\nwhere f \u2208 H\u03ba. Let (\u03d5\u0302i(x), \u03bb\u0302i), i = 1, 2, . . . , N be the eigenfunctions and eigenvalues of L\u0302N ranked in the descending order of eigenvalues, where \u27e8\u03d5\u0302i(\u00b7), \u03d5\u0302j(\u00b7)\u27e9H\u03ba = \u03b4(i, j) for any 1 \u2264 i, j \u2264 N . According to (Guo & Zhou, 2011), the prediction function g(x) can be well approximated by a function in the subspace spanned by the top eigenfunctions of L\u0302N . Hence, we propose to learn a target prediction function g\u0302(x) as a linear combination of the first s eigenfunctions, i.e.,\ng\u0302(x) = s\u2211\nj=1\n\u03b3\u2217j \u03d5\u0302j(x), (5)\nwhere s is a parameter that needs to be determined empirically. Coefficients {\u03b3\u2217i }si=1 in (5) are learned through a simple regression by minimizing the squared error of the labeled examples as shown in (1). Algorithm 1 shows the basic steps of the proposed algorithm.\nImplementation In step 2 of Algorithm 1, we need to compute the eigenvalues and eigenfunctions of L\u0302N , which is given as follows (Smale & Zhou, 2009). Let K = [\u03ba(xi,xj)]N\u00d7N be the kernel matrix for the examples in D, and let {(vi, \u03c3i)}si=1 be the first s eigenvectors and eigenvalues of K. Then, the eigenvalues and eigenfunctions of L\u0302N are given by\n\u03bb\u0302i = \u03c3i N , \u03d5\u0302i(\u00b7) = 1 \u221a \u03c3i N\u2211 j=1 vij\u03ba(xj , \u00b7), i = 1, . . . , s,\nwhere vij is the j-th element of vector v i. Finally, in step 3 of Algorithm 1, we need to compute the optimal coefficient \u03b3\u2217, which, according to (Bishop, 2006), is given by\n\u03b3\u2217 = D1/2[V \u22a4KBK \u22a4 BV ] \u22121V \u22a4KByl,\nwhere D = diag(\u03c31, . . . , \u03c3s), KB = [\u03ba(xi,xj)]N\u00d7n includes the kernel similarity between all the examples in D and labeled examples, and V = (v1, . . . ,vs)."}, {"heading": "2.2. Empirical study", "text": "Three real-world data sets, i.e., insurance, wine, and temperature 1, are used in our empirical study. The statistics of these datasets are given in Table 1. The first two datasets are from the UC Irvine Machine\n1http://www.remss.com/msu\nLearning Repository (Frank & Asuncion, 2010), while the task of the last dataset is to predict the temperature based on the coordinates (latitude, longitude) on the earth surface. All three datasets are designed for regression tasks with real-valued outputs. We choose these three datasets because they fit in with our assumptions that will be elaborated in section 3.2.\nWe randomly choose 90% of the data for training, and use the rest 10% for testing. We randomly select 2%, 3%, . . . , 9% of the entire dataset as labeled examples. We evaluate the performance by measuring the regression error of the testing data. Each experiment is repeated ten times and the regression errors averaged over the ten trials are reported. Two supervised regression algorithms, i.e., Kernel Ridge Regression (KRR) (Saunders et al., 1998) and Support Vector Regression (SVR) (Drucker et al., 1996), and a state-of-the-art algorithm for semi-supervised regression, i.e., Laplacian Regularized Least Squares (LapRLS) (Belkin et al., 2006), are used as the baselines. We did not include other baseline algorithms for semi-supervised learning because Laplacian regularization yields the state-of-the-art performance of semi-supervised learning. More importantly, our goal is to verify that the proposed algorithm can effectively improve the generalization performance of supervised learning. We refer to the proposed algorithm as Simple Semi-Supervised Learning, or SSSL for short. A RBF kernel function is used for all algorithms, and all the parameters are chosen by cross validation.\nTables 2-4 show the regression errors for the three datasets, respectively. First, as we expected, the performance of all learning algorithms improves as the number of labeled examples increases. It is also not surprising to see that the two semi-supervised learning algorithms perform better than the two supervised learning algorithms. Second, the proposed algorithm (SSSL) outperforms the baseline semi-supervised learning algorithm for almost all the cases, indicating that it is effective for semi-supervised learning. Note that SVR does not perform well on the temperature dataset since this dataset has a perfect manifold structure (the earth surface is a sphere), and SVR fails to capture the manifold structure when the percentage of labeled data is very small."}, {"heading": "3. Generalization Error Bounds", "text": "To analyze the generalization performance of the proposed algorithm, we first consider the simple scenario where we have access to an infinite number of unlabeled examples (i.e., the marginal distribution PX ). We then present the generalization error bound for a finite number of unlabeled examples. Detailed analysis can be found in Section 4."}, {"heading": "3.1. Generalization error for an infinite number of unlabeled examples", "text": "Given the marginal distribution PX , we define an integral operator L as L(f)(\u00b7) = Ex[\u03ba(x, \u00b7)f(x)]. We denote by {(\u03d5i(\u00b7), \u03bbi), i = 1, 2, . . .} the eigenfunctions and eigenvalues of L ranked in the descending order of the eigenvalues, where the eigenfunctions are normalized according to the distribution, i.e.,\u222b x\u2208X \u03d5i(x)\u03d5j(x)dPX = \u03b4ij . We note that L\u0302N , defined in (4), is the empirical version of L, and \u2225L\u2212 L\u0302N\u2225HS approaches to zero as the number of examples goes to infinity, where \u2225 \u00b7 \u2225HS is Hilbert Schmidt norm of a linear operator (Smale & Zhou, 2009).\nIn order to achieve a better generalization error bound\nfor the proposed semi-supervised learning algorithm, we make the following assumptions about eigenvalues and eigenfunctions:\n\u2022 A1 Skewed eigenvalue distribution. Similar to many studies (Koltchinskii & Yuan, 2010; Steinwart et al., 2006; Minh, 2010), we assume the eigenvalues follow a power law distribution, i.e., there exists a small constant a > 0 and a power index p > 2, such that\n\u03bbk \u2264 a2k\u2212p, k = 1, 2, . . . .\n\u2022 A2 Bounded eigenfunctions. There exists a small constant C such that maxx\u2208X max\ni |\u03d5i(x)| \u2264 C.\nThis is similar to the incoherence condition specified in compressive sensing (Cande\u0300s & Tao, 2006).\n\u2022 A3 Sufficient number of labeled examples. We require the number of labeled examples to be larger than n0 which is defined as\nn0 = 64C 2 ln2(2N3)\n( Ra\n\u03b5\n)4/(p\u22121) , (6)\nwhere N > 0 is some large number that corresponds to the number of unlabeled examples when we come to the case of finite samples.\nRemark 1 Assumption (A1) ensures that the target function can be approximated, with a small error, by a function in the subspace spanned by the top eigenfunctions of L. This is the foundation behind Algorithm 1.\nRemark 2 Assumptions (A2) and (A3) are introduced to ensure that all the coefficients {\u03b3\u2217i }si=1 in (5) can be estimated accurately. More specifically, assumption (A3) makes it possible to obtain an accurate estimation of the coefficients {\u03b3\u2217i }si=1. Assumption A2 ensures that labeled examples are associated with all the top eigenfunctions, and therefore a reliable estimation can be obtained for all the coefficients through the regression analysis. Intuitively, assumption (A2) ensures that |\u03d5i(xj)|, j \u2208 [n] on the labeled examples are not zeros, which is due to E[\u03d5i(x)] is fixed and maxx |\u03d5i(x)| is small, otherwise we cannot obtain an accurate estimation of \u03b3\u2217. Actually, it is notable that we only need to bound the first s eigenfunctions in M(s) = maxx \u2211s i=1 \u03d5 2 i (x), a key quantity in Proposition 2. From another point of view, if we bound maxx |\u03d5i(x)| \u2264 \u2225\u03d5i\u2225H\u03ba = 1/ \u221a \u03bbi (Smale & Zhou, 2009, pg. 9), then if the first s eigenvalues are large, we can expect the maximum value of the first s eigenfuncitons is small. An example satisfying this property is the Sobolev space of functions defined on the domain [0, 1]d with uniform distribution (see (Koltchinskii, 2011, pg. 16)).\nThe following theorem shows the generalization error of Algorithm 1 for an infinite number of unlabeled examples provided that assumptions (A0\u223cA3) hold. Theorem 1. Assume (A0 \u223c A3) hold. Set s = (aR/\u03b5)2/(p\u22121). Then, with a probability 1\u2212 2N\u22123, we have\nEx\n[ (g\u0302(x)\u2212 f(x))2 ] \u2264 O(\u03b52),\nwhere g\u0302(\u00b7) is the function learned by Algorithm 1.\nRemark 3 According to (2), \u03b52 is the optimal regression error that can be achieved by a prediction function in H\u03ba. Hence, Theorem 1 shows that given an infinite number of unlabeled examples, the prediction function learned by Algorithm 1 achieves almost the optimal performance (up to a constant).\nRemark 4 It is also useful to compare the bound in Theorem 1 to the generalization error bound of supervised learning. According to (Tsybakov, 2008), the\nminimax optimal error if supervised regression (i.e., the best possible regression error of the worst possible distribution) is bounded by \u2126(n\u2212p/(p+1)) 2. So if we take the value in assumption (A3) for n \u221d \u03f5\u22124/(p\u22121), then the generalization error for supervised regression is \u2126(\u03b54p/(p\n2\u22121)). Compared to our bound (i.e., O(\u03b52)), when p > 1 + \u221a 2, we have 4p/(p2 \u2212 1) < 2, implying that the generalization error bound of Algorithm 1 is better than that for supervised regression."}, {"heading": "3.2. Generalization error for a finite number of unlabeled examples", "text": "We now consider the scenario where only a finite number (i.e., N) of unlabeled examples are available. The key challenge arising from the finite sample analysis is that we do not have access to the eigenfunctions and eigenvalues of L. Instead, we have to approximate the eigenfunctions and eigenvalues of L by its empirical counterpart L\u0302N . These approximation errors make the analysis more involved. To ensure that the approximation does not significantly increase the regression error, we make the following assumptions:\n\u2022 B1 Skewed eigenvalue distribution of L\u0302N . We assume eigenvalues \u03bb\u0302i, i = 1, 2, . . . follow a power law distribution, i.e., there exists a small constant a and power index p > 2, such that\n\u03bb\u0302k \u2264 a2k\u2212p, k = 1, 2, . . . ..\n\u2022 B2 Bounded eigenfunctions. There exists a small constant C\u0302 such that maxx\u2208X max\ni |\u03d5\u0302i(x)/\n\u221a \u03bbi| \u2264\nC\u0302.\n\u2022 B3 Sufficient number of labeled examples. We require the number of labeled examples to be larger than n0 where n0 is defined as\nn0 = 64C\u0302 2 ln2(2N3)\n( Ra\n\u03b5\n)4/(p\u22121) .\n\u2022 B4 Sufficiently large eigengap. Let rs = \u03bbs\u2212\u03bbs+1 be the gap between the s-th eigenvalue and (s+1)th eigenvalue of L. We assume the eigengap rs is sufficiently large for s = (Ra/\u03b5)2/(p\u22121), i.e., rs \u2265 3\u03c4\n2/3 N , where \u03c4N = 12 lnN\u221a N .\nRemark 5 Assumptions (B1\u223cB3) are the \u201cempirical\u201d versions of assumptions (A1\u223cA3). Note that unlike assumption (A2) where |\u03d5i(x)| is assumed to be\n2We use \u2126(\u00b7), instead of O(\u00b7), since it is a minimax optimal bound.\nbounded, in assumption (B2), we assume |\u03d5\u0302i(x)/ \u221a \u03bbi| to be bounded. This is because \u03d5i(x) is normalized with respect to the distribution PX , while \u03d5\u0302i(x) is normalized with respect to the functional norm since the marginal distribution PX is unknown. The most important feature of the finite sample analysis is that we introduce a new assumption (B4), where the number of unlabeled examples N plays an important role to bound the eigengap. This additional assumption is designed to address the approximation error in replacing the eigenfunctions of L with the eigenfunctions of L\u0302N . Theorem 2. Assume (A0) and (B1\u223cB3) hold. Set s = (aR/\u03b5)2/(p\u22121), and assume\nN \u2265 max ( 144R2[lnN ]2r\u22122s \u03b5 \u22122, 144R4a2[lnN ]2\u03b5\u22124 ) .\nThen, with a probability 1\u2212 4N\u22123, we have\nEx[(g\u0302(x)\u2212 f(x))2] \u2264 O(\u03b52).\nAs indicated by Theorem 2, the prediction function learned by Algorithm 1 achieves almost the optimal regression error (up to a constant) provided that all the assumptions hold and the number of unlabeled examples is sufficiently large.\nFinally, to partially verify the assumptions, we examine the eigenvalue distributions for the chosen datasets (described in Section 2.2), as shown in Figure 1. Due to space limitation, we put the figure for the temperature dataset in the supplementary material. We also show in Figure 1 the curves of a2k\u2212p with p = 2.1. It is very clear that the eigenvalues follow a skewed distribution with the power index p > 2."}, {"heading": "4. Analysis", "text": "We present the full analysis for the case of infinite number of unlabeled examples, and only sketch the analysis for finite number of unlabeled examples due to lack of space. More detailed analysis can be found in the supplementary materials."}, {"heading": "4.1. Analysis for an infinite number of unlabeled examples", "text": "When we have an infinite number of unlabeled examples, the learned prediction function is given by g\u0302(x) = \u2211s j=1 \u03b3 \u2217 j \u03d5j(x), where \u03b3\n\u2217 = (\u03b31, \u00b7 \u00b7 \u00b7 , \u03b3\u2217s )\u22a4 is obtained by solving the following optimization problem:\n\u03b3\u2217 = argmin \u03b3 L(\u03b3) = n\u2211 i=1  s\u2211 j=1 \u03b3j\u03d5j(xi)\u2212 f(xi) 2  . (7)\nUsing the eigenfunctions of L, we write g(x), the optimal prediction function defined in (3), as g(x) =\u2211\nj \u03b1j\u03d5j(x). We define gs(x), the projection of g(x) into the subspace spanned by the top s eigenfunctions, as\ngs(x) = s\u2211\nj=1\n\u03b1j\u03d5j(x).\nUsing gs(x), we decompose the generalization error of g\u0302(x) into two parts, i.e.,\nEx[(g\u0302(x)\u2212 f(x))2] \u2264 2Ex[(g\u0302(x)\u2212 gs(x))2] + 2Ex[(gs(x)\u2212 f(x))2].\nThe following lemmas bound the two terms on the R.H.S. of the above inequality, separately.\nLemma 1. Under assumption (A1), for any s \u2265 1, we have\nEx [ (gs(x)\u2212 f(x))2 ] \u2264 2\u03b52 + 2a 2R2\nsp\u22121 , \u03b52s.\nLemma 2. Under assumptions (A2\u223cA3) and s = (aR/\u03f5)2/(p\u22121), with a probability at least 1\u22122N\u22123, we have\nEx [ (g\u0302(x)\u2212 gs(x))2 ] \u2264 2\u03b72,\nwhere \u03b72 = 2 ( \u03b52s + 2\u03b5s\u03b5max \u221a 3 lnN\nn +\n\u03b52max lnN\nn\n) .\nAs indicated by Lemma 1, assumption (A1) guarantees an additional small regression error when constraining the solution to the subspace spanned by the top eigenfunctions of L. As indicated by Lemma 2, assumptions (A2\u223cA3) ensure that gs(x), the projection of g(x) into the subspace spanned by the top eigenfunctions, can be accurately estimated from the labeled examples. It is easy to see that Theorem 1 immediately follows Lemma 1 and Lemma 2 by noting that \u03b52s = O(\u03b5\n2) and \u03b72 = O(\u03b52) when we set s = (Ra/\u03b5)2/(p\u22121). Below, we show how to prove both lemmas.\nProof of Lemma 1 We first show that \u2211\u221e\ni=s+1 \u03b1 2 i\nis bounded. Since \u2225g\u2225H\u03ba \u2264 R, we have\nR2 \u2265 \u27e8g, g\u27e9H\u03ba = \u221e\u2211 i=1 \u03b12i \u2225\u03d5i\u22252H\u03ba = \u221e\u2211 i=1 \u03b12i \u03bbi ,\nand therefore\n\u221e\u2211 i=s+1 \u03b12i \u2264 R2 +\u221e\u2211 i=s+1 \u03bbi \u2264 a2R2 (p\u2212 1)sp\u22121 \u2264 a 2R2 sp\u22121 .\nThen we bound the regression error of gs(x) as follows: Ex [ (gs(x)\u2212 f(x))2 ] \u2264 2Ex [ (g(x)\u2212 f(x))2 ] +2Ex\n \u221e\u2211 i,j=s+1 \u03b1i\u03b1j\u03d5i(x)\u03d5j(x)  = 2\u03b52 + 2\n\u221e\u2211 i=s+1 \u03b12i \u2264 2\u03b52 + 2a2R2 sp\u22121 , \u03b52s.\nProof of Lemma 2 The proof of Lemma 2 is significantly more involved. We first introduce some notations. Let zi = (\u03d51(xi), . . . , \u03d5s(xi))\n\u22a4 be the vector representation of xi derived from the first s eigenfunctions. Let Z = (z1, . . . , zn) include the representations of all labeled examples, and let yl = (f(x1), . . . , f(xn))\n\u22a4. Using Z, we rewrite L(\u03b3) in (7) as L(\u03b3) = \u03b3\u22a4ZZ\u22a4\u03b3 \u2212 2\u03b3\u22a4Zyl + \u2225yl\u222522. The following proposition bounds Ex[(g\u0302(x)\u2212 gs(x))2] using the minimum eigenvalue of ZZ\u22a4. Proposition 1. Assume ZZ\u22a4 is nonsingular. With a probability at least 1\u2212N\u22123, we have\nEx [ (g\u0302(x)\u2212 gs(x))2 ] = \u2225\u03b1s \u2212 \u03b3\u2217\u222522 \u2264\nn\u03b72\n\u03bbmin(ZZ\u22a4) .\nThe following proposition bounds the minimum eigenvalue of ZZ\u22a4. Proposition 2. With a probability at least 1 \u2212N\u22123, where N > 0 is a large number, we have\n1 n \u03bbmin(ZZ \u22a4) \u2265 1\u2212 4M(s) ln(2N 3)\u221a n ,\nwhere M(s) = maxx\u2208X \u2211s i=1 \u03d5 2 i (x).\nThe proof for Proposition 1 and 2 can be found in the supplementary materials. Now we are ready to prove Lemma 2.\nAccording to assumptions A2\u223cA3 and Proposition 2, we have, with a probability at least 1\u2212N\u22123\n1 n \u03bbmin(ZZ \u22a4) \u2265 1\u2212 4M(s) ln(2N 3)\u221a n \u2265 1 2 .\nCombining the above inequality with Proposition 1, we have, with a probability at least 1\u2212 2N\u22123,\nEx [ (g\u0302(x)\u2212 gs(x))2 ] \u2264 2\u03b72."}, {"heading": "4.2. Analysis for a finite number of unlabeled examples", "text": "Define \u03b3\u2217 the optimal solution that minimizes the regression error using the eigenfunctions of L\u0302N , i.e.,\n\u03b3\u2217 = argmin \u03b3\u2208Rs n\u2211 i=1\n( f(xi)\u2212\ns\u2211 k=1 \u03b3k\u03d5\u0302k(xi)\n)2 .\nWe further define \u03b3\u0302\u2217i = \u03b3 \u2217 i \u221a \u03bbi, i = 1, \u00b7 \u00b7 \u00b7 , s, and write g\u0302(x) learned in the presence of a finite number of un-\nlabeled examples as g\u0302(x) = \u2211s\ni=1 \u03b3\u0302 \u2217 i \u03d5\u0302i(x)\u221a\n\u03bbi . We also in-\ntroduce hs(x) as follows\nhs(x) = s\u2211\ni=1\n\u03b1i \u03d5\u0302i(x)\u221a\n\u03bbi .\nwhere {\u03b1i}si=1 are the coefficients defined in g(x). Similar to the previous analysis, we bound the generalization error of g\u0302(x) by\nEx[(g\u0302(x)\u2212 f(x))2] \u2264 2Ex[(g\u0302(x)\u2212 hs(x))2] + 2Ex[(hs(x)\u2212 f(x))2].\nWe follow the same path as in the infinite case and present two lemmas to bound the two terms on R.H.S. of the above inequality.\nLemma 3. Under assumptions B1, B3 and N \u2265 144s2p\u22122[lnN ]2a\u22122, with a probability at least 1 \u2212 2N\u22123, we have\nEx[(hs(x)\u2212 f(x))2] \u2264 4\u03b52s + 36R2\u03c42N\nr2s , \u03b5\u03022s.\nLemma 4. Under assumptions B1\u223cB3, with a probability at least 1\u2212 4N\u22123, we have\nEx [ (g\u0302(x)\u2212 hs(x))2 ] \u2264 4\u03b7\u03022.\nwhere \u03b7\u03022 = 2 ( \u03b5\u03022s + 2\u03b5\u0302s\u03b5max \u221a 3 lnN\nn +\n\u03b52max lnN\nn\n) .\nThe proof for Lemma 4 and Lemma 3 can be found in the supplementary materials.\nProof of Theorem 2. Using the condition N \u2265 144R2[lnN ]2/[r2s\u03b5 2], we have 36R2\u03c42N/r 2 s \u2264 O(\u03b52). When we set s = (Ra/\u03b5)2/(p\u22121), we have \u03b52s = O(\u03b5 2), \u03b5\u03022s = O(\u03b5 2) and \u03b7\u0302 = O(\u03b52) . By Lemma 3 and Lemma 4, we have, with a probability 1\u2212 4N\u22123,\nEx[(g\u0302(x)\u2212 f(x))2] \u2264 2\u03b5\u03022s + 8\u03b7\u03022 = O(\u03b52)."}, {"heading": "5. Conclusions", "text": "In this work, we present a very simple algorithm for semi-supervised learning. Our analysis shows that under appropriate assumptions about the integral operator, the proposed algorithm achieves a better generalization error than a supervised learning algorithm. In the future, we plan to further improve the scalability of the proposed algorithm by exploring different approaches (e.g., the Nystro\u0308m method) for efficiently estimating eigenfunctions from a large number of unlabeled examples."}, {"heading": "Acknowledgments", "text": "The work was supported in part by the U.S. Army Research Laboratory under Cooperative Agreement No. W911NF-09-2-0053 (NS-CTA), NSF IIS-0905215, NSF IIS-0643494, U.S. Air Force Office of Scientific Research MURI award FA9550-08-1-0265, and Office of Navy Research (ONR Award N00014-09-1-0663 and N00014-12-1-0431). The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies."}], "references": [{"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["Belkin", "Mikhail", "Niyogi", "Partha", "Sindhwani", "Vikas"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Belkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2006}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Bishop", "Christopher M"], "venue": null, "citeRegEx": "Bishop and M.,? \\Q2006\\E", "shortCiteRegEx": "Bishop and M.", "year": 2006}, {"title": "Near-optimal signal recovery from random projections: Universal encoding strategies", "author": ["Cand\u00e8s", "Emmanuel J", "Tao", "Terence"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "On the exponential value of labeled samples", "author": ["Castelli", "Vittorio", "Cover", "Thomas M"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Castelli et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Castelli et al\\.", "year": 1995}, {"title": "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter", "author": ["Castelli", "Vittorio", "Cover", "Thomas M"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Castelli et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Castelli et al\\.", "year": 1996}, {"title": "Support vector regression machines", "author": ["Drucker", "Harris", "Burges", "Christopher J. C", "Kaufman", "Linda", "Smola", "Alex J", "Vapnik", "Vladimir"], "venue": "In NIPS, pp", "citeRegEx": "Drucker et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Drucker et al\\.", "year": 1996}, {"title": "Semisupervised learning in gigantic image collections", "author": ["Fergus", "Rob", "Weiss", "Yair", "Torralba", "Antonio"], "venue": "In NIPS, pp", "citeRegEx": "Fergus et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fergus et al\\.", "year": 2009}, {"title": "An empirical featurebased learning algorithm producing sparse approximations", "author": ["Guo", "Xin", "Zhou", "Ding-Xuan"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Guo et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2011}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems", "author": ["Koltchinskii", "Vladimir"], "venue": null, "citeRegEx": "Koltchinskii and Vladimir.,? \\Q2011\\E", "shortCiteRegEx": "Koltchinskii and Vladimir.", "year": 2011}, {"title": "Sparsity in multiple kernel learning", "author": ["Koltchinskii", "Vladimir", "Yuan", "Ming"], "venue": "Annuals of Statistics,", "citeRegEx": "Koltchinskii et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koltchinskii et al\\.", "year": 2010}, {"title": "Statistical analysis of semi-supervised regression", "author": ["Lafferty", "John D", "Wasserman", "Larry A"], "venue": "In NIPS,", "citeRegEx": "Lafferty et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2007}, {"title": "Some properties of gaussian reproducing kernel hilbert spaces and their implications for function approximation and learning theory", "author": ["Minh", "Ha Quang"], "venue": "Constructive Approximation,", "citeRegEx": "Minh and Quang.,? \\Q2010\\E", "shortCiteRegEx": "Minh and Quang.", "year": 2010}, {"title": "Statistical analysis of semi-supervised learning: The limit of infinite unlabelled data", "author": ["Nadler", "Boaz", "Srebro", "Nathan", "Zhou", "Xueyuan"], "venue": "In NIPS,", "citeRegEx": "Nadler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nadler et al\\.", "year": 2009}, {"title": "Manifold regularization and semi-supervised learning: Some theoretical analyses", "author": ["P. Niyogi"], "venue": "Technical report,", "citeRegEx": "Niyogi,? \\Q2008\\E", "shortCiteRegEx": "Niyogi", "year": 2008}, {"title": "Generalization error bounds in semisupervised classification under the cluster assumption", "author": ["Rigollet", "Philippe"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Rigollet and Philippe.,? \\Q2007\\E", "shortCiteRegEx": "Rigollet and Philippe.", "year": 2007}, {"title": "Ridge regression learning algorithm in dual variables", "author": ["Saunders", "Craig", "Gammerman", "Alexander", "Vovk", "Volodya"], "venue": "In ICML, pp", "citeRegEx": "Saunders et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Saunders et al\\.", "year": 1998}, {"title": "Learning with labeled and unlabeled data", "author": ["Seeger", "Matthias"], "venue": "Technical report,", "citeRegEx": "Seeger and Matthias.,? \\Q2001\\E", "shortCiteRegEx": "Seeger and Matthias.", "year": 2001}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["Singh", "Aarti", "Nowak", "Robert D", "Zhu", "Xiaojin"], "venue": "In NIPS, pp", "citeRegEx": "Singh et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2008}, {"title": "Semi-supervised learning using sparse eigenfunction bases", "author": ["Sinha", "Kaushik", "Belkin", "Mikhail"], "venue": "In NIPS, pp", "citeRegEx": "Sinha et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sinha et al\\.", "year": 2009}, {"title": "Geometry on probability spaces", "author": ["Smale", "Steve", "Zhou", "Ding-Xuan"], "venue": "Constructive Approximation,", "citeRegEx": "Smale et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Smale et al\\.", "year": 2009}, {"title": "An explicit description of the reproducing kernel hilbert spaces of gaussian rbf kernels", "author": ["Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Steinwart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Steinwart et al\\.", "year": 2006}, {"title": "Introduction to Nonparametric Estimation", "author": ["Tsybakov", "Alexandre B"], "venue": "Springer, 1st edition,", "citeRegEx": "Tsybakov and B.,? \\Q2008\\E", "shortCiteRegEx": "Tsybakov and B.", "year": 2008}, {"title": "Analysis of spectral kernel design based semi-supervised learning", "author": ["Zhang", "Tong", "Ando", "Rie Kubota"], "venue": "In NIPS, pp", "citeRegEx": "Zhang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2005}, {"title": "Semi-supervised learning literature survey", "author": ["Zhu", "Xiaojin"], "venue": "Technical report, Computer Sciences, University of Wisconsin-Madison,", "citeRegEx": "Zhu and Xiaojin.,? \\Q2008\\E", "shortCiteRegEx": "Zhu and Xiaojin.", "year": 2008}], "referenceMentions": [{"referenceID": 17, "context": "A number of theories have been proposed for semi-supervised learning, and most of them are based on one of the two assumptions: (1) the cluster assumption (Seeger, 2001; Rigollet, 2007; Lafferty & Wasserman, 2007; Singh et al., 2008; Sinha & Belkin, 2009) which assumes that two data points should have the same class label or similar values if they are connected by a path passing through a high density region; (2) the manifold assumption (Lafferty & Wasserman, 2007; Niyogi, 2008)", "startOffset": 155, "endOffset": 255}, {"referenceID": 13, "context": ", 2008; Sinha & Belkin, 2009) which assumes that two data points should have the same class label or similar values if they are connected by a path passing through a high density region; (2) the manifold assumption (Lafferty & Wasserman, 2007; Niyogi, 2008)", "startOffset": 215, "endOffset": 257}, {"referenceID": 12, "context": "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning.", "startOffset": 43, "endOffset": 92}, {"referenceID": 13, "context": "However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples.", "startOffset": 44, "endOffset": 58}, {"referenceID": 17, "context": "In addition, the learning algorithms suggested in (Rigollet, 2007; Singh et al., 2008; Zhang & Ando, 2005) are difficult to implement efficiently even if the cluster assumption holds, making them unpractical", "startOffset": 50, "endOffset": 106}, {"referenceID": 12, "context": "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies (Castelli & Cover, 1995; 1996), the authors show that under the assumption that the marginal distribution PX is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet (2007) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples.", "startOffset": 72, "endOffset": 932}, {"referenceID": 12, "context": "It has been pointed out by several studies (Lafferty & Wasserman, 2007; Nadler et al., 2009) that the manifold assumption by itself is insufficient to reduce the generalization error bound of supervised learning. However, on the other hand, it was found in (Niyogi, 2008) that for certain learning problems, no supervised learner can learn effectively, while a manifold based learner (that knows the manifold or learns it from unlabeled examples) can learn well with relatively few labeled examples. Compared to the manifold assumption, theoretical results based on cluster assumption appear to be more encouraging. In the early studies (Castelli & Cover, 1995; 1996), the authors show that under the assumption that the marginal distribution PX is a mixture of class conditional distributions, the generalization error will be reduced exponentially in the number of labeled examples if the mixture is identifiable. Rigollet (2007) defines the cluster assumption in terms of density level sets, and shows a similar exponential convergence rate given a sufficiently large number of unlabeled examples. Furthermore, Singh et al. (2008) show that the mixture components can be identified if PX is a mixture of a finite number of smooth density functions and the separation/overlap between different mixture components is significantly large.", "startOffset": 72, "endOffset": 1134}, {"referenceID": 6, "context": "Unlike the previous studies of exploring eigenfunctions for semi-supervised learning (Fergus et al., 2009; Sinha & Belkin, 2009), we show that under appropriate assumptions, the proposed algorithm achieves a better generalization error bound than supervised learning algorithms.", "startOffset": 85, "endOffset": 128}, {"referenceID": 20, "context": "The assumption of skewed eigenvalue distributions has been verified and used in multiple studies of kernel learning (Koltchinskii, 2011; Steinwart et al., 2006; Minh, 2010; Zhang & Ando, 2005), while the assumption of bounded eigenvectors was mostly found in the study of compressive sensing (Cand\u00e8s & Tao, 2006).", "startOffset": 116, "endOffset": 192}, {"referenceID": 15, "context": ", Kernel Ridge Regression (KRR) (Saunders et al., 1998) and Support Vector Regression (SVR) (Drucker et al.", "startOffset": 32, "endOffset": 55}, {"referenceID": 5, "context": ", 1998) and Support Vector Regression (SVR) (Drucker et al., 1996), and a state-of-the-art algorithm for semi-supervised regression, i.", "startOffset": 44, "endOffset": 66}, {"referenceID": 0, "context": ", Laplacian Regularized Least Squares (LapRLS) (Belkin et al., 2006), are used as the baselines.", "startOffset": 47, "endOffset": 68}, {"referenceID": 20, "context": "Similar to many studies (Koltchinskii & Yuan, 2010; Steinwart et al., 2006; Minh, 2010), we assume the eigenvalues follow a power law distribution, i.", "startOffset": 24, "endOffset": 87}], "year": 2012, "abstractText": "In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. We also verify the effectiveness of the proposed algorithm by an empirical study.", "creator": " TeX output 2012.05.20:2113"}}}