{"id": "1206.6413", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A convex relaxation for weakly supervised classifiers", "abstract": "This paper introduces a general multi-class approach to weakly supervised classification. Inferring the labels and learning the parameters of the model is usually done jointly through a block-coordinate descent algorithm such as expectation-maximization (EM), which may lead to local minima. To avoid this problem, we propose a cost function based on a convex relaxation of the soft-max loss. We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program (SDP). Empirically, our method compares favorably to standard ones on different datasets for multiple instance learning and semi-supervised learning as well as on clustering tasks. We demonstrate this approach by introducing a network algorithm for deep learning using MNN and using an algorithm specifically designed to identify the predicted residuals of the predictions.\n\n\n\n\n\n\nThe results are presented in a paper in this paper.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (877kb)", "http://arxiv.org/abs/1206.6413v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["armand joulin", "francis r bach"], "accepted": true, "id": "1206.6413"}, "pdf": {"name": "1206.6413.pdf", "metadata": {"source": "META", "title": "A convex relaxation for weakly supervised classifiers", "authors": ["Armand Joulin", "Francis Bach"], "emails": ["armand.joulin@ens.fr", "francis.bach@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "Discriminative supervised classifiers have proved to be very accurate data-driven tools for learning the relationship between input variables and certain labels. Usually, for these methods to work, the labeling of the training data needs to be complete and precise. However, in many practical situations, this requirement is impossible to meet because of the challenges posed by the acquisition of detailed data annotations. This typically leads to partial or ambiguous labelings.\nDifferent weakly supervised methods have been proposed to tackle this issue. In the semi-supervised framework (Chapelle et al., 2006), only a small number of points are labeled, and the goal is to use the unlabeled points to improve the performance of the classifier. In the multiple-instance learning (MIL) framework introduced by Dietterich & Lathrop (1997), bags of instances are labeled together instead of individually, and some instances belonging to the same bag\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nmay have different true labels. Finally, in the ambiguous labeling setting (Jin & Ghahramani, 2003; Hullermeier & Beringer, 2006), each point is associated with multiple potential labels.\nMore generally, in all these frameworks, the points are associated with observable partial labels and the implicit or explicit goal is to jointly estimate their true latent labels and learn a classifier based on these labels. This usually leads to a non-convex cost function which is often optimized with a greedy method or a coordinate descent algorithm such as the expectationmaximization (EM) procedure. These methods usually converge to a local minimum, and their initialization remains an open practical problem.\nIn this paper, we propose a simple and general framework which can be used for any of the aforementioned problems. We explicitly learn the true latent label and the classifier parameters. We also propose a convex relaxation of our cost function and an efficient algorithm to minimize it. More precisely, we use a discrimative classifier with a soft-max loss, and our convex relaxation extends the work of Guo & Schuurmans (2008). Our main contributions are:\n\u2022 a full convex relaxation of the soft-max loss function with intercept, which can be applied to a large set of multiclass problems with any level of supervision,\n\u2022 a novel convex cost function for weakly supervised and unsupervised problems and,\n\u2022 a dedicated and efficient optimization procedure.\nWe develop our framework for the general weakly supervised case. We propose results on both toy examples as proof of concept of our claims, and on standard MIL and semi-supervised learning (SSL) datasets."}, {"heading": "1.1. Related work", "text": "Multiple instance learning (MIL) has received much attention because of its wide range of applications. First used for drug activity prediction, it has also been used in the vision community for different problems such as scene classification (Maron & Ratan,\n1998), object detection (Viola et al., 2006), object tracking in video (Babenko et al., 2009), and image database retrieval (Yang, 2000). Many MIL methods have been developed in the past decade. For example, some are based on boosting (Auer & Ortner, 2004), others on nearest neighbors (Wang & Zucker, 2000), on neural networks (Zhang & Zhou, 2006), on decision trees (Blockeel et al., 2005), or the construction of an appropriate kernel (Wang et al., 2008; Ga\u0308rtner et al., 2002; Kwok & Cheung, 2007). Much of the work in the MIL community has focused on the use of discriminative classifiers, the most popular one being the support vector machine (SVM) (Andrews et al., 2003; Chen & Wang, 2004; Gehler & Chapelle, 2007). In this paper, we concentrate on the logisitic loss which makes little difference with the hinge loss with the additional advantage of being twice differentiable. Note that this loss has already been used in the context of MIL (Xu & Frank, 2004; Ray & Craven, 2005), but with different optimization schemes.\nMany semi-supervised learning (SSL) methods have also been proposed in the past decade (see, e.g., Chapelle et al., 2006; Zhu, 2006). For example, some are based on maximizing the margin with an SVM framework (Joachims, 1999; Bennett & Demiriz, 1998; Xu & Schuurmans, 2005), and others use the unlabeled data for regularization (Belkin et al., 2004) or co-training of weak classifiers (Blum & Mitchell, 1998).\nDiscriminative clustering provides a principled way to reuse existing supervised learning machinery while explicitly estimating the latent labels. For example, following the SVM approach of Xu et al. (2005), algorithms using linear discriminant analysis (De la Torre & Takeo, 2006) or ridge regression (Bach & Harchaoui, 2007) have been proposed. These methods often fail in the multiclass case, whereas we show that the soft-max loss with intercept works well in this setting. A common issue for discriminative clustering is that a perfect separation is reached by assigning the same label to all of the points. In most of the previously cited methods, this issue is adressed by adding linear constraints on the size of each cluster. In this paper we use instead a natural cluster-size balancing term corresponding to an entropy penalization (Chapelle et al., 2006; Joulin et al., 2010).\nThe link between SSL and MIL has been widely studied in the community. For example, in the context of image segmentation with text annotation, Barnard et al. (2003) propose a general weakly supervised model based on a multi-modal extension to a mixture of latent Dirichlet allocation. An important issue with this family of generative models is that learning the parameters is often untractable. Another example is Zhou & Xu (2007) who use the relation between MIL and SSL to develop a method for MIL.\nThe idea of using a convex cost function in the weakly supervision context has been already studied in different contexts such as, for example, ambiguous labeling (Cour et al., 2009) or discriminative clustering (Xu et al., 2005; Bach & Harchaoui, 2007). In this paper, we are interested in the convex relaxation of a general multiclass loss function, i.e., the soft-max loss. Guo & Schuurmans (2008) propose a related relaxation but do not consider the intercept in the linear classifier. We extend their work to the case of linear classifiers with an intercept and show in the experiment section, why this difference is crucial when it comes to classification. Note that by using kernels, we can use non-linear classifiers as well. Also, our dedicated optimization scheme is more scalable than the one developed in Guo & Schuurmans (2008) and could be applied to their problem as well."}, {"heading": "2. Proposed model", "text": ""}, {"heading": "2.1. Notations", "text": "We suppose that we observe I bags of instances. For i in {1, . . . , I}, Ni is the set of instances in the i-th bag, and Ni = |Ni| is its cardinality. We denote by N = \u2211\ni Ni the total number of instances. In each bag i, an instance n in Ni is associated with a feature xn \u2208 X and a label yn in L, in certain feature and label space. In this paper, we suppose that this label is common to all the instances of a same bag and explain only partially the instances contained in the bag. We are thus interested in finding a latent label zn \u2208 P which would give a better understanding of the data. We denote by P and L the cardinalities of P and L. We also assume that the latent label zn of an instance n can only take its values in a subset Pyn of P which depends on the label yn of the bag. The variables yn and zn are associated with their canonical vectorial representation, i.e., znp = 1 if the instance n has a latent label of p and 0 otherwise. We denote by z the N \u00d7 P matrix with rows zn.\nInstance reweighting. In many problems, a set of instances can be bigger than the other, this is the case for example in a one-vs-all classifer where the number of positive instances is often very small compared to the number of negative examples. A side-contribution of this work is to consider explicitly a reweighting of the data to avoid undesired side effects: Each point is associated with a weight \u03c0n \u2265 0 which denotes its importance compared to others. Some examples are the uniform case, i.e., \u03c0n = 1 N or when bags have to be reweighted, i.e., \u03c0n = 1\nINi for n in the bag i. We\ndenote by \u03c0 the vector with entries equal to \u03c0n. Note that \u03c0 \u2265 0 and \u2211\nn \u03c0n = 1.\nThis setting is very general, so let us now show how it applies to several concrete settings.\nSemi-supervised learning. Given a set of true labels P and Nl points with known label, there are Nl+1 bags, i.e., one for each labeled point and one for all the unlabeled instances. The set L is equal to P plus a label for the unlabeled bag (i.e., L = P + 1). The true label of an instance in a positive bag is fixed whereas in the unlabeled bag it can take any value in P.\nUnsupervised learning. This is an extreme case of the semi-supervised framework with only the unlabeled bag.\nMultiple instance learning. There are two possible labels for a bag (L = 2), i.e., positive (yn = 1) or negative (yn = 0). The true label zn of an instance n in a negative bag is necessarily negative (zn = 0) and in a positive bag it can be either positive or negative (P1 = {0, 1}).\nAmbiguous labelling. Each bag is associated with a set of possible true labels Pl. The set of partial labels is thus the combination of all possible subsets of P, i.e., each label l \u2208 L represents a subset of P (L = 2P )."}, {"heading": "2.2. Problem formulation", "text": "The goal of a discriminative weakly supervised classifier is to find the latent labels z that minimize the value of a regularized discriminative loss function. More precisely, given some latent label z and some feature map \u03c6 : X 7\u2192 IRd (note that \u03c6 could be explicitly defined or implicitly given through a positive-definite kernel), we train a multi-class discriminative classifier to find the parameters w \u2208 IRP\u00d7d and b \u2208 IRP that minimize:\nL(z, w, b) =\nN \u2211\nn=1\n\u03c0n\u2113(zn, w T\u03c6(xn) + b),\nwhere \u2113 : IRP \u00d7 IRP 7\u2192 IR is a loss function. In this paper, we are interested in the multi-class setting where a natural choice for \u2113 is the soft-max loss function (Hastie et al., 2001). Note that for a given instance n, the set of possible true labels depends on the the label y of its bag, our loss function \u2113(zn, w T\u03c6(xn) + b) then takes the following form:\n\u2212 \u2211\nl\u2208L\nynl \u2211\np\u2208Pl\nznp log\n( exp(wTp \u03c6(xn) + bp) \u2211\nk\u2208Pl exp(wTk \u03c6(xn) + bk)\n)\n,\nwhere wTp is the p\u2013th row of w T and bp the p\u2013th entry of b.\nCluster-size balancing term. In many unsupervised or weakly supervised problems, a common issue is that assigning the same label to all the instances leads to perfect separation. In the MIL community, this is equivalent to considering all the bags as negative and a common solution is to add a non-convex constraint which enforces at least one point per positive bag to be positive. Another solution used in\nthe discriminative clustering community is to add constraints on the number of elements per class and per bag (Xu et al., 2005; Bach & Harchaoui, 2007). Despite good results, this solution introduces extra parameters and may be hard to extend to other frameworks such as MIL, where a positive bag may not have any negative instances. Another common technique is to encourage the proportion of points per class and per bag to be close to uniform. An appropriate penalty term for achieving this is the entropy (i.e., h(v) = \u2212 \u2211\nk vk log(vk)) of the proportions of points per bag and per latent label, leading to:\nH(z) = \u2211\ni\u2208I\nh\n(\n\u2211\nn\u2208Ni\n\u03c0nzn\n)\n.\nPenalizing by this entropy turns out to be equivalent to maximizing the log-likelihood of a graphical model where the features xn explain the labels yn through the latent labels zn (Joulin et al., 2010). An important consequence is that the natural weight of this penalty in the cost function is 1, so we do not add any extra parameters.\nTo avoid over-fitting, we penalize the norm of w, leading to the following cost function:\nf(z, w, b) = L(z, w, b)\u2212H(z) + \u03bb\n2P \u2016w\u20162F ,\nwhere \u03bb > 0 is the regularization parameter and the problem thus takes the following form:\nmin \u2200n\u2264N, zn\u2208SPyn min w\u2208IRd\u00d7P , b\u2208IRP f(z, w, b), (1)\nwhere SP = {t \u2208 IR P | t \u2265 0, tT 1P = 1} is the simplex in IRP . To avoid cumbersome double subscripts, we suppose that any instance n in a bag with a label yn (which is common to the entire bag), has a latent label zn in P instead of Pyn .\nIn the next section we show how to obtain a convex relaxation of this problem."}, {"heading": "3. Convex relaxation", "text": "An interesting feature of the soft-max cost function is its link to the entropy through the Fenchel conjugate (Boyd & Vandenberghe, 2003), i.e., given a P - dimensional vector t, the log-partition can be written as log (\n\u2211P p=1 exp(tp)\n)\n= maxv\u2208SP \u2211P p=1 vptp + h(v).\nSubstituting in the loss function, the weakly supervised problem defined in Eq. (1) can be reformulated as:\nmin z\u2208SN\nP\nmax q\u2208SN\nP\n\u2211\ni\u2208I\n\u2211\nn\u2208Ni\n\u03c0nh(qn)\u2212H(z) + g(z, q), (2)\nwhere q is an N \u00d7 P matrix with n-th row qTn , and g(z, q) is equal to:\nmin w\u2208IRP\u00d7d\nb\u2208IRP\n\u2211\ni\u2208I\n\u2211\nn\u2208Ni\n\u03c0n(qn \u2212 zn) T (wT\u03c6(xn) + b) +\n\u03bb\n2P \u2016w\u20162F .\nMinimizing this function w.r.t. the intercept b leads to an intercept constraint on the dual variables, i.e, (q \u2212 z)T\u03c0 = 0. The minimization w.r.t. w leads to a closed-form expression for g:\ng(z, q) = \u2212 P\n2\u03bb tr ( (q \u2212 z)(q \u2212 z)TK ) ,\nwhere K is the positive definite kernel matrix associated with the reweighted mapping \u03c6, i.e., with entries equal to Knm = \u03c0n\u03c6(xn)\nT\u03c6(xm)\u03c0m. The cost function is not convex in general in z since it is the maximum over a set indexed by q of concave functions in z. A common way of dealing with this issue is to relax the problem into a semidefinite program (SDP) in zzT . Unfortunately, our cost function does not directly depend on zzT , but a reparametrization in terms of q inspired by Guo & Schuurmans (2008) allows us to get around this technical difficulty.\nReparametrization in q. We reparametrize the problem by introducing an N \u00d7 N matrix \u2126 such that q = \u2126z (Guo & Schuurmans, 2008). The intercept constraint and the normalization constraint on q (i.e., q1K = 1N ) become constraints over \u2126, i.e., respectively \u2126T\u03c0 = \u03c0 and \u21261N = 1N . Translating the addition of an intercept to a linear classifier into a simple constraint on the columns of \u2126 provides a significant improvement over Guo & Schuurmans (2008), as shown in Section 5.1. This reparametrization has the side-effect of introducing a non-convex term in the cost function since the entropies over qn in Eq. (2) is replaced by an entropy over the n\u2013th row of \u2126z which is not jointly concave/convex in \u2126 and z.\nTight upper-bound on the entropy. We show in the supplementary material that the entropy in q can be bounded by a difference of entropy in \u2126 and z, up to an additive constant C0:\n\u2211\ni\u2208I\n\u2211\nn\u2208Ni\n\u03c0nh(qn) \u2264 \u2212 \u2211\nn\n\u03c0nh(\u2126n) +H(z) + C0. (3)\nThis upper-bound is tight in the sense that given a discrete value of z (i.e., before the relaxation), the maximum of the left part among discrete values of q is equal to the maximum of the right part among corresponding discrete values of \u2126. Note also that the term in z appearing in Eq. (3) cancels out with the entropy term\nin Eq. (2). This relaxation leads to the minimizition of the following function of z:\nmax \u2126\u2208O\n\u2212 P\n2\u03bb tr ( zzT (I \u2212 \u2126)TK(I \u2212 \u2126) ) \u2212 \u2211\nn\n\u03c0nh(\u2126n),\nwhere O = {\u2126 | \u21261N = 1N , \u2126T\u03c0 = \u03c0, \u2126 \u2265 0}. This problem depends on z solely through the matrix zzT , and can thus be relaxed into an SDP in zzT .\nReparametrization in z. With the change of variable Z = zzT , we have the maximum of a set of linear functions of Z, which is convex. However, the set Z of possible values for Z is non-convex since it is defined by:\n{\ndiag(Z) = 1N , Z \u2265 0, Z 0, rank(Z) = k \u2212 1. (4)\nLet us review these constraints:\n\u2022 In practice, the piecewise-positivity constraint is not necessary and removing it leads to a matrix Z with entries in [\u22121, 1] since Z is positive semidefinite with ones on the diagonal.\n\u2022 The rank constraint is the main source of nonconvexity, and will be removed, thus leading to a convex relaxation.\n\u2022 The rest of the constraints defines the elliptope:\nEN = {Z \u2208 IR N\u00d7N | diag(Z) = 1N , Z 0}.\nNote that an additional linear constraint may be needed depending on the considered weakly supervised problem. We give below some examples:\n\u2022 In the case of MIL, this constraint takes the form of Z\u2212 = 1N\u22121 T N\u2212 , where N\u2212 is the number of\nnegative examples, and Z\u2212 is the restriction of Z to the negative bags.\n\u2022 \u201cMust-not-link\u201d constraints on the instances can be handled: If two bags i and j have labels yi and yj such that the set of possible latent labels are dissimilar (i.e., Pli \u2229 Plj = \u2205), we can constrain the submatrix Zij to be equal to 0. These constraints are of particular interest in the case of SSL, where labeled bags with different labels should not be assigned to the same latent label.\nIn the rest of this paper, we consider the specific cases of SSL, MIL and discriminative clustering:\n\u2022 In SSL, we can reduce the dimensionality of Z: Since all the values of z with a same known label are equal, it is equivalent to replace them by a\nsingle element in Z. Denoting by Nu is the number of unlabeled points, P the number of labels and NR = Nu + P , this is equivalent to considering a matrix RTZR instead of Z, where R is a N \u00d7 NR matrix whose restriction to the unlabeled bags is the identity and all other entries are zero except for Rn(Nu+l) which is equal to 1 if the instance n has a known label l.\n\u2022 In MIL, the same reduction can be done with P = 1 and Nu denoting the total number of positive instances.\n\u2022 Discriminative clustering is similar to SSL with P = 0.\nBy taking into account all of these modifications and by dropping the rank constraint, we replace the nonconvex set Z by the elliptope ENR , leading to the minimization of g(Z) over ENR , where g(Z) is equal to:\nmax \u2126\u2208O\n\u2212 P\n2\u03bb tr ( ZR(I \u2212 \u2126)TK(I \u2212 \u2126)RT ) \u2212 \u2211\nn\n\u03c0nh(\u2126n). (5)\nIn the next section we propose an efficient algorithm to solve this convex optimization problem."}, {"heading": "4. Optimization", "text": "Since our optimization involves a maximization in our inner loop, it cannot be solved directly by a generalpurpose toolbox. We propose an algorithm dedicated to our case. In the rest of this paper we refer to the maximization as the inner loop and the overall minimization of our cost function as the outer loop."}, {"heading": "4.1. Inner loop", "text": "Evaluating the cost function defined in Eq. (5) involves the maximization of the sum of the entropy of \u2126 and a function T defined as:\nT (\u2126) = \u2212 1\n2\u03bb tr ( (I \u2212 \u2126)RTZR(I \u2212 \u2126)TK ) .\nWe use a proximal method with a reweighted Kullback-Leibler (KL) divergence which naturally enforces the point-wise positivity contraint in W, and leads to an efficient Bregman projection with a KL divergence (an I-projection to be more precise) on the rest of the constraints defining W. More precisely, given a point \u21260, the proximal update is given by maximizing the following function:\nlD(\u2126)=tr ( \u2126T\u2207T (\u21260) ) \u2212 \u2211\nn\n\u03c0nh(\u2126n)\u2212LD\u03c0(\u2126 \u2016\u2126 0), (6)\nwhere L is the Lipschitz constant of \u2207T and D\u03c0 is a reweighted KL divergence defined as:\nD\u03c0(\u2126 \u2016 \u2126 0) =\n\u2211\ni\n\u2211\nn\u2208Ni\n\u03c0n\nN \u2211\nm=1\n\u2126nm log\n(\n\u2126nm \u21260nm\n)\n.\nThe I-projection can be done efficiently with an iterative proportional fitting procedure (IPFP), which is guaranteed to converge to the global minimum with linear convergence rate (Fienberg, 1970).\nNote that to obtain a faster convergence of the inner loop, we may take advantage of a low-rank decomposition of K and RTZR and we use an accelerated proximal scheme on the logarithm of \u2126 (Beck & Teboulle, 2009). To control the distance from the optimum \u2126\u2217, we can use a provably correct duality gap which can be computed efficiently (details are in the supplementary material)."}, {"heading": "4.2. Outer loop", "text": "The outer loop minimizes g(Z) as defined in Eq. (5) over the elliptope ENR . Many approaches have been proposed to solve this type of problems (Goemans & Williamson, 1995; Burer & Monteiro, 2003; Journe\u0301e et al., 2010) but, to the best of our knowledge, they all assume that the function and its gradient can be computed efficiently and put the emphasis on the projection. This is not the case in our problem, and we thus propose a method adapted to our particular setting.\nFirst, to simplify the projection on the ENR , we replace our cost function g(Z) by its diagonally rescaled version gR(Z) = g(diag(Z)\n\u22121/2Zdiag(Z)\u22121/2). Note that even if this function is in general non-convex, it coincides with g(Z) on ENR , making its restriction to this set convex. This modification allows us to rescale the diagonal of any update Z to a diagonal equal to 1N without modifying the value of our cost function.\nOur minimization of gR over the elliptope is also based on a proximal method with a Bregman divergence to guarantee updates that stay in the feasible set. A natural choice for the Bregman divergence is the KL divergence based on the von Neumann entropy, i.e, the entropy of the eigenvalues of a matrix (see more details in the supplementary material). This divergence guarantees that each update has nonnegative eigenvalues. Given a point Z0, its update can then be obtained in closed-form as the diagonally rescaled version of VDiag(exp(diag( 1tE)))V\nT , where V and E are the eigenvectors and the eigenvalues of \u2212\u2207gR(Z0)+t log(Z0) and t is a positive step size computed using a line-search with backtracking.\nAs in the inner loop, we use a computationnally tractable provable duality gap, i.e., \u2212NR\u03bbmin, where \u03bbmin is the lowest eigenvalue of \u2207gR(Z) (see details in the supplementary material)."}, {"heading": "4.3. Rounding", "text": "Many rounding schemes can be applied with similar performances. Following Bach & Harchaoui (2007)\nand Joulin et al. (2010), we use k-means clustering on the eigenvectors associated with the k highest eigenvalues (Ng et al., 2001) to obtain a rounded solution z\u2217. This z\u2217 is then used to initialize an EM procedure to solve the problem defined in Eq. (1) and obtain the parameters (w, b) of the classifier, leading to finer details not caught by the convex relaxation.\nA specificity of the MIL framework is that strictly no point from a negative bag should be classified as positive, which leads to adding to Eq. (1), the following linear constraints on the parameters of the classifier:\n\u2200i \u2208 I\u2212, n \u2208 Ni, w T 0 \u03c6(xn) + b0 \u2265 w T 1 \u03c6(xn) + b1. (7)\nWe add these hard constraints in the M-step (optimization over w and b) of the EM procedure. The projection over this set of linear constraints is performed efficiently with an homotopy algorithm in the dual (Mairal et al., 2010)."}, {"heading": "5. Results", "text": "Implementation. Our algorithm is implemented in MATLAB and takes from 1 to 5 minutes for 500 points. Note that we can efficiently compute the solutions for different values of \u03bb using warm restarts. Our overall complexity is O(N3) but we can scale up to several thousands of points. The complexity of the different steps in our algorithm is given in Figure 1. On larger datasets, we can use our relaxation on subsets of instances or on pre-clustering the instances (with kmeans) and use it to initialize the EM on the complete\ndataset."}, {"heading": "5.1. Discriminative clustering", "text": "In this section, we compare our method to two different discriminative clustering methods for the multiclass case: the SDP relaxation of the soft-max problem with no intercept (Guo & Schuurmans, 2008) and the discriminative clustering framework introduced by Bach and Harchaoui (2007). The latter comparison is relevant since they propose a convex cost function based on the square loss with intercept.\nWe consider in Figure 2, as a proof of concept, two toy examples where the goal is to find 3 and 5 clusters with linear kernels and N = 500. Even if the clusters are linearly separable, the set of values of w and b which leads to a perfect separation is very small (Figure 2, panel (a)), making the problem challenging. For fair comparison, we test different regularization parameters and show the one leading to the best performances. We show the matrix Z obtained for the three methods as well as the matrix K = xxT in Figure 2. We see that our method clearly obtains a better estimation of the class assignment compared to the others, showing the importance of both the soft-max loss and the intercept.\nIn panels (a) and (b) of Figure 3, we also show that our method works with non-linear kernels in a multiclass setting. Finally, in the panel (c) of Figure 3, we show a comparison with k-means as we increase the number of dimensions containing only noise, following the setup of Bach & Harchaoui (2007). Our setting is the 3-cluster problem shown in Figure 2 with an RBF kernel and N = 300. We see that our algorithm is more robust than k-means."}, {"heading": "5.2. Multiple instance learning", "text": "In Figure 4, we show some comparisons with other MIL methods on standard datasets (Dietterich & Lathrop, 1997; Andrews et al., 2003) for a variety of tasks:\na drug activity prediction (musk), image classification (fox, tiger and elephant), and text classification (trec1 ).\nFor comparison, we use the setting described by Andrews et al. (2003), where we create 10 random splits of the data, train on 90% of them and test on the remaining 10%. We test our algorithm with and without the intercept and with uniform or bag-specific (i.e., 1 INi for instances in the bag i) weights, and compare it to some classical MIL algorithms. Note that we have only tried a linear kernel, and we select the regularization parameter using a 2-fold cross-validation for each split. Our algorithm obtains comparable performances with methods dedicated to the MIL problem."}, {"heading": "5.3. Semi-supervised learning", "text": "For the SSL setting, we choose the standard SSL datasets and we compare with methods proposed in Chapelle et al. (2006). The benchmarks (Linear and Nonlinear) are based on a SVM formulation and the benchmark (Entropy-Reg.) uses an entropy regularization. We use our method with either a linear or a RBF kernel. To fix our parameters, we follow the experimental setup of Chapelle et al. (2006). Each set contains 1500 points and either l = 10 or 100 of them are labeled. We show the results in Figure 5. As expected, since the benchmarks and our formulation\nare very related, the performances are mostly similar when l = 100. However, when l = 10, our method is more robust and its performances get significantly higher showing that a convex relaxation is less sensible to noise and poorly labeled data."}, {"heading": "6. Conclusion", "text": "In this paper, we propose a convex relaxation of a general cost function for weakly supervised problems. We show the importance of a tight convex relaxation compared to relaxation where either the related linear classifier has been approximated (absence of intercept) or the loss function (square-loss instead of the softmax loss). Our comparison with standard non-convex methods for MIL and SSL shows the importance of the initialization for robustness of the approach. We believe that convex relaxation is a powerful tool to obtain good initializations to non-convex problems. The trade-off is that these methods are usually not scalable which suggest to use them on subsets of points or after a quantization step to initialize a more efficient algorithm, such as EM.\nAcknowledgements. This paper was partially supported by the European Research Council (SIERRA and VIDEOWORLD projects)."}], "references": [{"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "In NIPS,", "citeRegEx": "Andrews et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2003}, {"title": "A boosting approach to multiple instance learning", "author": ["P. Auer", "R. Ortner"], "venue": "In ECML,", "citeRegEx": "Auer and Ortner,? \\Q2004\\E", "shortCiteRegEx": "Auer and Ortner", "year": 2004}, {"title": "Visual tracking with online multiple instance learning", "author": ["B. Babenko", "Yang", "M-H", "S. Belongie"], "venue": "In CVPR,", "citeRegEx": "Babenko et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Babenko et al\\.", "year": 2009}, {"title": "Diffrac : a discriminative and flexible framework for clustering", "author": ["F. Bach", "Z. Harchaoui"], "venue": "In NIPS,", "citeRegEx": "Bach and Harchaoui,? \\Q2007\\E", "shortCiteRegEx": "Bach and Harchaoui", "year": 2007}, {"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2009}, {"title": "Regularization and semi-supervised learning on large graphs", "author": ["M. Belkin", "I. Matveeva", "P. Niyogi"], "venue": "In COLT,", "citeRegEx": "Belkin et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2004}, {"title": "Semi-supervised support vector machines", "author": ["K. Bennett", "A. Demiriz"], "venue": "In NIPS,", "citeRegEx": "Bennett and Demiriz,? \\Q1998\\E", "shortCiteRegEx": "Bennett and Demiriz", "year": 1998}, {"title": "Multi-instance tree learning", "author": ["H. Blockeel", "D. Page", "A. Srinivasan"], "venue": "In ICML,", "citeRegEx": "Blockeel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blockeel et al\\.", "year": 2005}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "In COLT,", "citeRegEx": "Blum and Mitchell,? \\Q1998\\E", "shortCiteRegEx": "Blum and Mitchell", "year": 1998}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "author": ["S. Burer", "R.D.C. Monteiro"], "venue": "Mathematical Programming,", "citeRegEx": "Burer and Monteiro,? \\Q2003\\E", "shortCiteRegEx": "Burer and Monteiro", "year": 2003}, {"title": "Image categorization by learning and reasoning with regions", "author": ["Y. Chen", "Wang", "James Z"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2004}, {"title": "Learning from ambiguously labeled images", "author": ["T. Cour", "Sapp", "Ben", "Jordan", "Chris", "Taskar"], "venue": "In CVPR,", "citeRegEx": "Cour et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cour et al\\.", "year": 2009}, {"title": "Discriminative cluster analysis", "author": ["F. De la Torre", "K. Takeo"], "venue": "In ICML,", "citeRegEx": "Torre and Takeo,? \\Q2006\\E", "shortCiteRegEx": "Torre and Takeo", "year": 2006}, {"title": "Solving the multipleinstance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop"], "venue": "Artificial Intelligence,", "citeRegEx": "Dietterich and Lathrop,? \\Q1997\\E", "shortCiteRegEx": "Dietterich and Lathrop", "year": 1997}, {"title": "An iterative procedure for estimation in contingency tables", "author": ["S.E. Fienberg"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Fienberg,? \\Q1970\\E", "shortCiteRegEx": "Fienberg", "year": 1970}, {"title": "Deterministic annealing for multiple-instance learning", "author": ["P.V. Gehler", "O. Chapelle"], "venue": "In AISTATS,", "citeRegEx": "Gehler and Chapelle,? \\Q2007\\E", "shortCiteRegEx": "Gehler and Chapelle", "year": 2007}, {"title": "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming", "author": ["M.X. Goemans", "D.P. Williamson"], "venue": "Journal of the ACM,", "citeRegEx": "Goemans and Williamson,? \\Q1995\\E", "shortCiteRegEx": "Goemans and Williamson", "year": 1995}, {"title": "Convex relaxations of latent variable training", "author": ["Y. Guo", "D. Schuurmans"], "venue": "In NIPS,", "citeRegEx": "Guo and Schuurmans,? \\Q2008\\E", "shortCiteRegEx": "Guo and Schuurmans", "year": 2008}, {"title": "Learning from ambiguously labeled examples", "author": ["E. Hullermeier", "J. Beringer"], "venue": "In IDA,", "citeRegEx": "Hullermeier and Beringer,? \\Q2006\\E", "shortCiteRegEx": "Hullermeier and Beringer", "year": 2006}, {"title": "Learning with multiple labels", "author": ["R. Jin", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Jin and Ghahramani,? \\Q2003\\E", "shortCiteRegEx": "Jin and Ghahramani", "year": 2003}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": null, "citeRegEx": "Joachims,? \\Q1999\\E", "shortCiteRegEx": "Joachims", "year": 1999}, {"title": "Efficient optimization for discriminative latent class models", "author": ["A. Joulin", "F. Bach", "J. Ponce"], "venue": "In NIPS,", "citeRegEx": "Joulin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2010}, {"title": "Lowrank optimization on the cone of positive semidefinite matrices", "author": ["M. Journ\u00e9e", "F. Bach", "Absil", "P.-A", "R. Sepulchre"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "Marginalized multi-instance kernels", "author": ["J.T. Kwok", "P. Cheung"], "venue": "In IJCAI,", "citeRegEx": "Kwok and Cheung,? \\Q2007\\E", "shortCiteRegEx": "Kwok and Cheung", "year": 2007}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": null, "citeRegEx": "Mairal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2010}, {"title": "Multiple-instance learning for natural scene classification", "author": ["O. Maron", "A.L. Ratan"], "venue": "In ICML,", "citeRegEx": "Maron and Ratan,? \\Q1998\\E", "shortCiteRegEx": "Maron and Ratan", "year": 1998}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Supervised Versus Multiple Instance Learning: An Empirical Comparison", "author": ["S. Ray", "M. Craven"], "venue": "In ICML,", "citeRegEx": "Ray and Craven,? \\Q2005\\E", "shortCiteRegEx": "Ray and Craven", "year": 2005}, {"title": "Multiple instance boosting for object detection", "author": ["P. Viola", "Platt", "John C", "C. Zhang"], "venue": "In NIPS,", "citeRegEx": "Viola et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2006}, {"title": "Adaptive p-posterior mixture-model kernels for multiple instance learning", "author": ["Wang", "H.-Y", "Q. Yang", "H. Zha"], "venue": "In ICML,", "citeRegEx": "Wang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2008}, {"title": "Solving multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J. Zucker"], "venue": "In ICML,", "citeRegEx": "Wang and Zucker,? \\Q2000\\E", "shortCiteRegEx": "Wang and Zucker", "year": 2000}, {"title": "Unsupervised and semisupervised multi-class support vector machines", "author": ["L. Xu", "D. Schuurmans"], "venue": "In AAAI,", "citeRegEx": "Xu and Schuurmans,? \\Q2005\\E", "shortCiteRegEx": "Xu and Schuurmans", "year": 2005}, {"title": "Maximum margin clustering", "author": ["L. Xu", "J. Neufeld", "B. Larson", "D. Schuurmans"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2005}, {"title": "Logistic regression and boosting for labeled bags of instances", "author": ["X. Xu", "E. Frank"], "venue": "In KDDM,", "citeRegEx": "Xu and Frank,? \\Q2004\\E", "shortCiteRegEx": "Xu and Frank", "year": 2004}, {"title": "Image database retrieval with multiple-instance learning techniques", "author": ["C. Yang"], "venue": "In ICDE,", "citeRegEx": "Yang,? \\Q2000\\E", "shortCiteRegEx": "Yang", "year": 2000}, {"title": "Adapting rbf neural networks to multi-instance learning", "author": ["M. Zhang", "Z. Zhou"], "venue": "Neural Processing Letters,", "citeRegEx": "Zhang and Zhou,? \\Q2006\\E", "shortCiteRegEx": "Zhang and Zhou", "year": 2006}, {"title": "Em-dd: An improved multiple-instance learning technique", "author": ["Q. Zhang", "S.A. Goldman"], "venue": "In NIPS,", "citeRegEx": "Zhang and Goldman,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Goldman", "year": 2001}, {"title": "On the relation between multi-instance learning and semi-supervised learning", "author": ["Z. Zhou", "J. Xu"], "venue": "In ICML,", "citeRegEx": "Zhou and Xu,? \\Q2007\\E", "shortCiteRegEx": "Zhou and Xu", "year": 2007}, {"title": "Semi-supervised learning literature", "author": ["X. Zhu"], "venue": null, "citeRegEx": "Zhu,? \\Q2006\\E", "shortCiteRegEx": "Zhu", "year": 2006}], "referenceMentions": [{"referenceID": 28, "context": "1998), object detection (Viola et al., 2006), object tracking in video (Babenko et al.", "startOffset": 24, "endOffset": 44}, {"referenceID": 2, "context": ", 2006), object tracking in video (Babenko et al., 2009), and image database retrieval (Yang, 2000).", "startOffset": 34, "endOffset": 56}, {"referenceID": 34, "context": ", 2009), and image database retrieval (Yang, 2000).", "startOffset": 38, "endOffset": 50}, {"referenceID": 7, "context": "For example, some are based on boosting (Auer & Ortner, 2004), others on nearest neighbors (Wang & Zucker, 2000), on neural networks (Zhang & Zhou, 2006), on decision trees (Blockeel et al., 2005), or the construction of an appropriate kernel (Wang et al.", "startOffset": 173, "endOffset": 196}, {"referenceID": 29, "context": ", 2005), or the construction of an appropriate kernel (Wang et al., 2008; G\u00e4rtner et al., 2002; Kwok & Cheung, 2007).", "startOffset": 54, "endOffset": 116}, {"referenceID": 0, "context": "Much of the work in the MIL community has focused on the use of discriminative classifiers, the most popular one being the support vector machine (SVM) (Andrews et al., 2003; Chen & Wang, 2004; Gehler & Chapelle, 2007).", "startOffset": 152, "endOffset": 218}, {"referenceID": 38, "context": "Many semi-supervised learning (SSL) methods have also been proposed in the past decade (see, e.g., Chapelle et al., 2006; Zhu, 2006).", "startOffset": 87, "endOffset": 132}, {"referenceID": 20, "context": "For example, some are based on maximizing the margin with an SVM framework (Joachims, 1999; Bennett & Demiriz, 1998; Xu & Schuurmans, 2005), and others use the unlabeled data for regularization (Belkin et al.", "startOffset": 75, "endOffset": 139}, {"referenceID": 5, "context": "For example, some are based on maximizing the margin with an SVM framework (Joachims, 1999; Bennett & Demiriz, 1998; Xu & Schuurmans, 2005), and others use the unlabeled data for regularization (Belkin et al., 2004) or co-training of weak classifiers (Blum & Mitchell, 1998).", "startOffset": 194, "endOffset": 215}, {"referenceID": 21, "context": "In this paper we use instead a natural cluster-size balancing term corresponding to an entropy penalization (Chapelle et al., 2006; Joulin et al., 2010).", "startOffset": 108, "endOffset": 152}, {"referenceID": 11, "context": "The idea of using a convex cost function in the weakly supervision context has been already studied in different contexts such as, for example, ambiguous labeling (Cour et al., 2009) or discriminative clustering (Xu et al.", "startOffset": 163, "endOffset": 182}, {"referenceID": 32, "context": ", 2009) or discriminative clustering (Xu et al., 2005; Bach & Harchaoui, 2007).", "startOffset": 37, "endOffset": 78}, {"referenceID": 30, "context": "For example, following the SVM approach of Xu et al. (2005), algorithms using linear discriminant analysis (De la Torre & Takeo, 2006) or ridge regression (Bach & Harchaoui, 2007) have been proposed.", "startOffset": 43, "endOffset": 60}, {"referenceID": 20, "context": ", 2006; Joulin et al., 2010). The link between SSL and MIL has been widely studied in the community. For example, in the context of image segmentation with text annotation, Barnard et al. (2003) propose a general weakly supervised model based on a multi-modal extension to a mixture of latent Dirichlet allocation.", "startOffset": 8, "endOffset": 195}, {"referenceID": 20, "context": ", 2006; Joulin et al., 2010). The link between SSL and MIL has been widely studied in the community. For example, in the context of image segmentation with text annotation, Barnard et al. (2003) propose a general weakly supervised model based on a multi-modal extension to a mixture of latent Dirichlet allocation. An important issue with this family of generative models is that learning the parameters is often untractable. Another example is Zhou & Xu (2007) who use the relation between MIL and SSL to develop a method for MIL.", "startOffset": 8, "endOffset": 462}, {"referenceID": 11, "context": "The idea of using a convex cost function in the weakly supervision context has been already studied in different contexts such as, for example, ambiguous labeling (Cour et al., 2009) or discriminative clustering (Xu et al., 2005; Bach & Harchaoui, 2007). In this paper, we are interested in the convex relaxation of a general multiclass loss function, i.e., the soft-max loss. Guo & Schuurmans (2008) propose a related relaxation but do not consider the intercept in the linear classifier.", "startOffset": 164, "endOffset": 401}, {"referenceID": 11, "context": "The idea of using a convex cost function in the weakly supervision context has been already studied in different contexts such as, for example, ambiguous labeling (Cour et al., 2009) or discriminative clustering (Xu et al., 2005; Bach & Harchaoui, 2007). In this paper, we are interested in the convex relaxation of a general multiclass loss function, i.e., the soft-max loss. Guo & Schuurmans (2008) propose a related relaxation but do not consider the intercept in the linear classifier. We extend their work to the case of linear classifiers with an intercept and show in the experiment section, why this difference is crucial when it comes to classification. Note that by using kernels, we can use non-linear classifiers as well. Also, our dedicated optimization scheme is more scalable than the one developed in Guo & Schuurmans (2008) and could be applied to their problem as well.", "startOffset": 164, "endOffset": 841}, {"referenceID": 32, "context": "Another solution used in the discriminative clustering community is to add constraints on the number of elements per class and per bag (Xu et al., 2005; Bach & Harchaoui, 2007).", "startOffset": 135, "endOffset": 176}, {"referenceID": 21, "context": "Penalizing by this entropy turns out to be equivalent to maximizing the log-likelihood of a graphical model where the features xn explain the labels yn through the latent labels zn (Joulin et al., 2010).", "startOffset": 181, "endOffset": 202}, {"referenceID": 14, "context": "The I-projection can be done efficiently with an iterative proportional fitting procedure (IPFP), which is guaranteed to converge to the global minimum with linear convergence rate (Fienberg, 1970).", "startOffset": 181, "endOffset": 197}, {"referenceID": 22, "context": "Many approaches have been proposed to solve this type of problems (Goemans & Williamson, 1995; Burer & Monteiro, 2003; Journ\u00e9e et al., 2010) but, to the best of our knowledge, they all assume that the function and its gradient can be computed efficiently and put the emphasis on the projection.", "startOffset": 66, "endOffset": 140}, {"referenceID": 26, "context": "(2010), we use k-means clustering on the eigenvectors associated with the k highest eigenvalues (Ng et al., 2001) to obtain a rounded solution z.", "startOffset": 96, "endOffset": 113}, {"referenceID": 21, "context": "and Joulin et al. (2010), we use k-means clustering on the eigenvectors associated with the k highest eigenvalues (Ng et al.", "startOffset": 4, "endOffset": 25}, {"referenceID": 24, "context": "The projection over this set of linear constraints is performed efficiently with an homotopy algorithm in the dual (Mairal et al., 2010).", "startOffset": 115, "endOffset": 136}, {"referenceID": 3, "context": "In this section, we compare our method to two different discriminative clustering methods for the multiclass case: the SDP relaxation of the soft-max problem with no intercept (Guo & Schuurmans, 2008) and the discriminative clustering framework introduced by Bach and Harchaoui (2007). The latter comparison is relevant since they propose a convex cost function based on the square loss with intercept.", "startOffset": 259, "endOffset": 285}, {"referenceID": 0, "context": "In Figure 4, we show some comparisons with other MIL methods on standard datasets (Dietterich & Lathrop, 1997; Andrews et al., 2003) for a variety of tasks:", "startOffset": 82, "endOffset": 132}, {"referenceID": 0, "context": "8 mi-SVM (Andrews et al., 2003) 87.", "startOffset": 9, "endOffset": 31}, {"referenceID": 0, "context": "6 MI-SVM (Andrews et al., 2003) 77.", "startOffset": 9, "endOffset": 31}, {"referenceID": 29, "context": "9 PPMM Kernel (Wang et al., 2008) 95.", "startOffset": 14, "endOffset": 33}, {"referenceID": 0, "context": "For comparison, we use the setting described by Andrews et al. (2003), where we create 10 random splits of the data, train on 90% of them and test on the remaining 10%.", "startOffset": 48, "endOffset": 70}], "year": 2012, "abstractText": "This paper introduces a general multi-class approach to weakly supervised classification. Inferring the labels and learning the parameters of the model is usually done jointly through a block-coordinate descent algorithm such as expectation-maximization (EM), which may lead to local minima. To avoid this problem, we propose a cost function based on a convex relaxation of the soft-max loss. We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program (SDP). Empirically, our method compares favorably to standard ones on different datasets for multiple instance learning and semi-supervised learning, as well as on clustering tasks.", "creator": "LaTeX with hyperref package"}}}