{"id": "1206.6426", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A fast and simple algorithm for training neural probabilistic language models", "abstract": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. As NPLMs are only trained in 1 word in NPLMs, they are not only used on a finite scale but also on the face of significant bias in the performance. The problem, however, is that NPLMs are also used to train in small numbers with a limited set of constraints.\n\n\n\nIt is now apparent that the term NPLMs, in many of these languages, is a common and often misunderstood term. While this study was not performed solely to compare the quality of NPLMs and the data (NPLMs vs. NPLMs), it was aimed to examine the effect of the word NPLM on learning strategies in the language. This, however, raises the question of whether NPLMs represent the most important learning strategies in the language (at least in this paper), and whether they represent the most important learning strategies in the language.\nIn other words, NPLMs are generally considered more useful and thus have no value in any of the major learning skills of the language. A similar study of learning strategies in the languages, namely NPLMs, has been performed for a while and the data presented here is relatively comparable (Figure 1).\nGiven that the NPLMs are known to have a limited range of language acquisition techniques, the study team would like to assess whether NPLMs represent a particular language learning strategy (Figure 1). Specifically, we would like to examine whether the term NPLM represent the most important learning strategies (Figure 1). The results of the NPLM study can be interpreted as a measure of the number of learning strategies that can be used to train the language over time. However, the study did not provide a good way of comparing the language learning strategies across the linguistic domain. This research was conducted as part of a larger study of learning strategies in the English language (Manssen et al., 2002). We therefore would want to examine whether these skills are most important in the language (Figure 1). This study provides more generalizations about NPLM.\nIn conclusion, our findings of this study indicate that NPLM may be the", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (325kb)", "http://arxiv.org/abs/1206.6426v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["andriy mnih", "yee whye teh"], "accepted": true, "id": "1206.6426"}, "pdf": {"name": "1206.6426.pdf", "metadata": {"source": "META", "title": "A fast and simple algorithm for training neural probabilistic language models", "authors": ["Andriy Mnih", "Yee Whye Teh"], "emails": ["amnih@gatsby.ucl.ac.uk", "ywteh@gatsby.ucl.ac.uk"], "sections": [{"heading": null, "text": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.\nWe propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well.\nWe demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-ofthe-art results on the Microsoft Research Sentence Completion Challenge dataset.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s)."}, {"heading": "1. Introduction", "text": "By assigning probabilities to sentences, language models allow distinguishing between probable and improbable sentences, which makes such models an important component of speech recognition, machine translation, and information retrieval systems. Probabilistic language models are typically based on the Markov assumption, which means that they model the conditional distribution of the next word in a sentence given some fixed number of words that immediately precede it. The group of words conditioned on is called the context, denoted h, while the word being predicted is called the target word, denoted w. n-gram models, which are effectively smoothed tables of normalized word/context co-occurrence counts, have dominated language modelling for decades due to their simplicity and excellent performance.\nIn the last few years neural language models have become competitive with n-grams and now routinely outperform them (Mikolov et al., 2011). NPLMs model the distribution for the next word as a smooth function of learned multi-dimensional real-valued representations of the context words and the target word. Similar representations are learned for words that are used in similar ways, ensuring that the network outputs similar probability values for them. Word representations learned by language models are also used for natural language processing applications such as semantic role labelling (Collobert & Weston, 2008), sentiment analysis (Maas & Ng, 2010), named entity recognition (Turian et al., 2010), and parsing (Socher et al., 2011).\nUnfortunately, NPLMs are very slow to train, which makes them unappealing for large-scale applications. This is a consequence of having to consider the entire vocabulary when computing the probability of a single word or the corresponding gradient. In fact, the time complexity of this computation scales as the product\nof the vocabulary size and the word feature dimensionality. One way to accelerate this computation is to reduce the vocabulary size for the NPLM by using it to predict only the most frequent words and handling the rest using an n-gram model (Schwenk & Gauvain, 2005).\nAlternatively, the vocabulary can be structured into a tree with words at the leaves, allowing exponentially faster computation of word probabilities and their gradients (Morin & Bengio, 2005). Unfortunately, the predictive performance of the resulting model is heavily dependent on the tree used and finding a good tree is a difficult problem (Mnih & Hinton, 2009).\nPerhaps a more elegant approach is to keep the model the same and to approximate the expensive gradient computations using importance sampling (Bengio & Sene\u0301cal, 2003). Unfortunately, the variance in the importance sampling estimates can make learning unstable, unless it is carefully controlled (Bengio & Sene\u0301cal, 2008).\nIn this paper we propose an efficient algorithm for training NPLMs based on noise-contrastive estimation (Gutmann & Hyva\u0308rinen, 2010), which is much more stable than importance sampling. Though it also uses sampling to approximate the gradients needed for learning, neither the number of samples nor the proposal distribution require dynamic adaptation for achieving performance on par with maximum likelihood learning."}, {"heading": "2. Neural probabilistic language models", "text": "A statistical language model is simply a collection of conditional distributions for the next word, indexed by its context.1 In a neural language model the conditional distribution corresponding to context h, Ph(w), is defined as\nPh\u03b8 (w) = exp(s\u03b8(w, h))\u2211 w\u2032 exp(s\u03b8(w \u2032, h)) , (1)\nwhere s\u03b8(w, h) is the scoring function with parameters \u03b8 which quantifies the compatibility of word w with context h. The negated scoring function is sometimes referred to as the energy function (Bengio et al., 2000).\nDepending on the form of s\u03b8(w, h), Eq. 1 can describe both neural and maximum entropy language models (Berger et al., 1996). The main difference between these two model classes lies in the features\n1Though almost all statistical language models predict the next word, it is also possible to model the distribution of the word preceding the context or surrounded by the context.\nthey use: neural language models learn their features jointly with other parameters, while maximum entropy models use fixed hand-engineered features and only learn the weights for those features. A neural language model represents each word in the vocabulary using a real-valued feature vector and defines the scoring function in terms of the feature vectors of the context words and the next word. In some models, different feature vector tables are used for the context and the next word vocabularies (Bengio et al., 2000), while in others the table is shared (Bengio et al., 2003; Mnih & Hinton, 2007).\nThe feature vectors account for the vast majority of parameters in neural language models, which means that their memory requirements are linear in the vocabulary size. This compares favourably to the memory requirements of the n-gram models, which are typically linear in the training set size."}, {"heading": "2.1. Log-bilinear model", "text": "The training method we propose is directly applicable to all neural probabilistic and maximum-entropy language models. For simplicity, we will perform our experiments using the log-bilinear language (LBL) model (Mnih & Hinton, 2007), which is the simplest neural language model. The LBL model performs linear prediction in the semantic word feature space and does not have non-linearities. In spite of its simplicity, the LBL model has been shown to outperform n-grams, though the more complex neural language models (Mikolov et al., 2010; Mnih et al., 2009) can outperform it.\nIn this paper we will use a slightly extended version of the LBL model that uses separate feature vector tables for the context words and the target words. Thus a context word w will be represented with feature vector rw, while a target word w will be represented with feature vector qw. Given a context h, the model computes the predicted representation for the target word by linearly combining the feature vectors for the context words using position-dependent context weight matrices Ci:\nq\u0302 = n\u22121\u2211 i=1 Cirwi . (2)\nThe score for the match between the context and the next word is computed by taking the dot product between the predicted representation and the representation of the candidate target word w:\ns\u03b8(w, h) = q\u0302 >qw + bw. (3)\nHere bw is the base rate parameter used to model the popularity of w. The probability of w in context h is then obtained by plugging the above score function into Eq. 1."}, {"heading": "2.2. Maximum likelihood learning", "text": "Maximum likelihood training of neural language models is tractable but expensive because computing the gradient of log-likelihood takes time linear in the vocabulary size. The contribution of a single context/word observation to the gradient of the loglikelihood is given by\n\u2202\n\u2202\u03b8 logPh\u03b8 (w) =\n\u2202\n\u2202\u03b8 s\u03b8(w, h)\u2212 \u2211 w\u2032 Ph\u03b8 (w \u2032) \u2202 \u2202\u03b8 s\u03b8(w \u2032, h)\n(4)\n= \u2202\n\u2202\u03b8 s\u03b8(w, h)\u2212 EPh\u03b8\n[ \u2202\n\u2202\u03b8 s\u03b8(w, h)\n] .\nThe expectation w.r.t. Ph\u03b8 (w \u2032) is expensive to evaluate because it requires computing s\u03b8(w, h) for all words in the vocabulary. Since vocabularies typically contain tens of thousands of words, maximum likelihood learning tends to be very slow."}, {"heading": "2.3. Importance sampling", "text": "Bengio and Sene\u0301cal (2003) have proposed a method for speeding up training of neural language models based on approximating the expectation in Eq. 4 using importance sampling. The idea is to generate k samples x1, ..., xk from an easy-to-sample-from distribution Qh(w) and estimate the gradient with\n\u2202\n\u2202\u03b8 logPh\u03b8 (w) \u2248\n\u2202\n\u2202\u03b8 s\u03b8(w, h)\u2212\n1\nV k\u2211 j=1 v(xj) \u2202 \u2202\u03b8 s\u03b8(xj , h),\n(5)\nwhere v(x) = exp(s\u03b8(x,h)) Qh(w=x) and V = \u2211k j=1 v(xj). The normalization by V is necessary here because the importance weights v are computed using the unnormalized model distribution exp(s\u03b8(x, h)). Typically the proposal distribution is an n-gram model fit to the training set, possibly with a context size different from the neural model\u2019s.\nThough this approach is conceptually simple, it is nontrivial to use in practice because the high variance of the importance sampling estimates can make learning unstable. The variance tends to grow as learning progresses, because the model distribution moves away from the proposal distribution.2 One way to\n2Bengio and Sene\u0301cal (2008) argue that this happens be-\ncontrol the variance is to keep increasing the number of samples during training so that the effective sample size stays above some predetermined value (Bengio & Sene\u0301cal, 2003). Alternatively, the n-gram proposal distribution can be adapted to track the model distribution throughout training (Bengio & Sene\u0301cal, 2008). The first approach is simpler but less efficient because the increasing number of samples makes learning slower. The second approach leads to greater speedups but is considerably more difficult to implement and requires additional memory for storing the adaptive proposal distribution."}, {"heading": "3. Noise-contrastive estimation", "text": "We propose using noise-contrastive estimation (NCE) as a more stable alternative to importance sampling for efficient training of neural language models and other models defined by Eq. 1. NCE has recently been introduced by Gutmann and Hyva\u0308rinen (2010) for training unnormalized probabilistic models. Though it has been developed for estimating probability densities, we are interested in applying it to discrete distributions and so will assume discrete distributions and use probability mass functions instead of density functions.\nThe basic idea of NCE is to reduce the problem of density estimation to that of binary classification, discriminating between samples from the data distribution and samples from a known noise distribution. In the language modelling setting, the data distribution Phd (w) will be the distribution of words that occur after a particular context h. Though it is possible to use context-dependent noise distributions, for simplicity we will use a context-independent (unigram) Pn(w). We are interested in fitting the context-dependent model Ph\u03b8 (w) to P h d (w).\nFollowing Gutmann and Hyva\u0308rinen (2012), we assume that noise samples are k times more frequent than data samples, so that datapoints come from the mixture 1 k+1P h d (w)+ k k+1Pn(w). Then the posterior probability that sample w came from the data distribution is\nPh(D = 1|w) = P h d (w)\nPhd (w) + kPn(w) . (6)\nSince we would like to fit Ph\u03b8 to P h d , we use P h \u03b8 in place of Phd in Eq. 6, making the posterior probability a function of the model parameters \u03b8:\nPh(D = 1|w, \u03b8) = P h \u03b8 (w)\nPh\u03b8 (w) + kPn(w) . (7)\ncause neural language models and n-gram models learn very different distributions.\nThis quantity can be too expensive to compute, however, because of the normalization required for evaluating Ph\u03b8 (w) (Eq. 1). NCE sidesteps this issue by avoiding explicit normalization and treating normalization constants as parameters. Thus the model is parameterized in terms of an unnormalized distribution Ph0\u03b80 and a learned parameter c\nh corresponding to the logarithm of the normalizing constant:\nPh\u03b8 (w) = P h0 \u03b80 (w) exp(c h). (8)\nHere \u03b80 are the parameters of the unnormalized distribution and \u03b8 = {\u03b80, ch}.\nTo fit the context-dependent model to the data (for the moment ignoring the fact that it shares parameters with models for other contexts), we simply maximize the expectation of logPh(D|w, \u03b8) under the mixture of the data and noise samples. This leads to the objective function\nJh(\u03b8) =EPhd\n[ log\nPh\u03b8 (w)\nPh\u03b8 (w) + kPn(w)\n] + (9)\nkEPn\n[ log\nkPn(w)\nPh\u03b8 (w) + kPn(w) ] with the gradient\n\u2202\n\u2202\u03b8 Jh(\u03b8) =EPhd\n[ kPn(w)\nPh\u03b8 (w) + kPn(w)\n\u2202\n\u2202\u03b8 logPh\u03b8 (w)\n] \u2212\n(10)\nkEPn\n[ Ph\u03b8 (w)\nPh\u03b8 (w) + kPn(w)\n\u2202\n\u2202\u03b8 logPh\u03b8 (w)\n] .\nNote that the gradient can also be expressed as\n\u2202\n\u2202\u03b8 Jh(\u03b8) = \u2211 w\nkPn(w)\nPh\u03b8 (w) + kPn(w) \u00d7 (11)\n(Phd (w)\u2212 Ph\u03b8 (w)) \u2202\n\u2202\u03b8 logPh\u03b8 (w),\nand that as k \u2192\u221e, \u2202\n\u2202\u03b8 Jh(\u03b8)\u2192 \u2211 w (Phd (w)\u2212 Ph\u03b8 (w)) \u2202 \u2202\u03b8 logPh\u03b8 (w), (12)\nwhich is the maximum likelihood gradient. Thus as the ratio of noise samples to observations increases, the NCE gradient approaches the maximum likelihood gradient.\nIn practice, given a word w observed in context h, we compute its contribution to the gradient by generating k noise samples x1, ..., xk and using the formula\n\u2202\n\u2202\u03b8 Jh,w(\u03b8) =\nkPn(w)\nPh\u03b8 (w) + kPn(w)\n\u2202\n\u2202\u03b8 logPh\u03b8 (w)\u2212 (13)\nk\u2211 i=1 [ Ph\u03b8 (xi) Ph\u03b8 (xi) + kPn(xi) \u2202 \u2202\u03b8 logPh\u03b8 (xi) ] .\nNote that the weights Ph\u03b8 (xi)\nPh\u03b8 (xi)+kPn(xi) are always be-\ntween 0 and 1, which makes NCE-based learning very stable (Gutmann & Hyva\u0308rinen, 2010). In contrast, the weights produced by importance sampling can be arbitrarily large.\nSince the distributions for different contexts share parameters, we cannot learn these distributions independently of each other by optimizing one Jh(\u03b8) at a time. Instead, we define a global NCE objective by combining the per-context NCE objectives using the empirical context probabilities P (h) as weights:\nJ(\u03b8) = \u2211 h P (h)Jh(\u03b8). (14)\nNote that this is the traditional approach for combining the per-context ML objectives for training neural language models."}, {"heading": "3.1. Dealing with normalizing constants", "text": "Our initial implementation of NCE training learned a (log-)normalizing constant (c in Eq. 8) for each context in the training set, storing them in a hash table indexed by the context.3 Though this approach works well for small datasets, it requires estimating one parameter per context, making it difficult to scale to huge numbers of observed contexts encountered by models with large context sizes. Surprisingly, we discovered that fixing the normalizing constants to 1,4 instead of learning them, did not affect the performance of the resulting models. We believe this is because the model has so many free parameters that meeting the approximate per-context normalization constraint encouraged by the objective function is easy."}, {"heading": "3.2. Potential speedup", "text": "We will now compare the gradient computation costs for NCE and ML learning. Suppose c is the context size, d is the word feature vector dimensionality, and V is the vocabulary size of the model. Then computing the predicted representation using Eq. 2 takes about cd2 operations for both NCE and ML. For ML, computing the distribution of the next word from the predicted representation takes about V d operations. For NCE, evaluating the probability of k noise samples under the model takes about kd operations. Since the gradient computation in each model has the same complexity as computing the probabilities, the speedup for\n3We did not use the learned normalizing constants when computing the validation and test set perplexities. Rather we normalized the probabilities explicitly.\n4This amounts to setting the normalizing parameters c to 0.\neach parameter update due to using NCE is about\nSpeedup = cd2 + V d cd2 + kd = cd+ V cd+ k . (15)\nFor a model with a 2-word context, 100D feature vectors, and a vocabulary size of 10K, an NCE update using 25 noise samples should be about 45 times faster than an ML update.\nSince the time complexity of computing the predicted representation is quadratic in the feature vector dimensionality, it can dominate the cost of the parameter update, making learning slow even for a small number of noise samples. We can avoid this by making context weight matrices Ci diagonal, reducing the complexity of computing the predicted representation to cd, and making the speedup factor c+Vc+k . For the model above this would amount to a factor of 370. The use of diagonal context matrices was introduced by Mnih & Hinton (2009) to speed up their hierarchical LBL-like model.\nSince the cost of a parameter update for importancesampling-based learning is the same as for NCE with the same number of noise samples, the algorithm that needs fewer samples to perform well will be faster."}, {"heading": "4. Penn Treebank results", "text": "We investigated the properties of the proposed algorithm empirically on the Penn Treebank corpus. As is common practice, we trained on sections 0-20 (930K\nwords) and used sections 21-22 (74k words) as the validation set and sections 23-24 (82k words) as the test set. The standard vocabulary of 10K most frequent words was used with the remaining words replaced by a special token. We chose to use this dataset to keep the training time for exact maximum likelihood learning reasonable.\nThe learning rates were adapted at the end of each epoch based on the change in the validation set perplexity since the end of the previous epoch. The rates were halved when the perplexity increased and were left unchanged otherwise. Parameters were updated based on mini-batches of 1000 context/word pairs each. Except when stated otherwise, NCE training generated 25 noise samples from the empirical unigram distribution per context/word observation. Noise samples were generated anew for each update. We did not use a weight penalty as the validation-score-based learning rate reduction appeared to be sufficient to avoid overfitting. All models used a two-word context and different 100D feature vector tables for context and target words.\nOur first experiment compared ML learning to NCE learning for various numbers of noise samples. The resulting test perplexities and training times are shown in Table 1. It is clear that increasing the number of noise samples produces better-performing models, with 25 samples being sufficient to match the MLtrained model. In terms of training time, NCE was 14 times faster than ML. The number of noise samples did not have a significant effect on the running time because computing the predicted representation was considerably more expensive than computing the probability of (at most) 100 samples. The main reason the speedup factor was less than 45 (the value predicted in Section 3.2) is because NCE took about twice as many epochs as ML to converge. Our NCE implementation is also less optimized than the ML implementation which takes greater advantage of the BLAS matrix routines.\nTo explore the effect of the noise distribution on the performance of the algorithm, we tried generating noise samples from the unigram as well as the uniform distribution. For each noise distribution we trained models using 1, 5, 25, and 100 noise samples per datapoint. As shown in Table 2, the unigram noise distribution leads to much better test set perplexity in all cases. However, the perplexity gap shrinks as the number of noise samples increases, from almost 100 for a single noise sample down to under 15 for 100 noise samples. In spite of poor test set performance, a uniform noise distribution did not lead to learning\ninstability even when a single noise sample was used.\nIn addition to the ML and NCE algorithms, we also implemented the importance sampling training algorithm from (Bengio & Sene\u0301cal, 2003) to use as a baseline, but found it very unstable. It diverged in virtually all of our experiments, even with adaptive sample size and the target effective sample size set to hundreds. The only run that did not diverge involved learning a unigram model using the target unigram as the proposal distribution, which is the ideal situation for importance sampling. The cause of failure in all cases was the appearance of extremely large importance weights once the model distribution became sufficiently different from the unigram proposal distribution5, which is a known problem with importance sampling. Since IS-based methods seem to require well over a hundred samples per gradient computation (Bengio & Sene\u0301cal, 2008), even when an adaptive proposal distribution is used, we expect IS-based training to be considerably slower than NCE, which, as we have shown, can achieve ML-level performance with only 25 noise samples."}, {"heading": "5. Sentence Completion Challenge", "text": "To demonstrate the scalability and effectiveness of our approach we used it to train several large neural language models for the Microsoft Research Sentence Completion Challenge (Zweig & Burges, 2011). The challenge was designed as a benchmark for semantic models and consists of SAT-style sentence completion problems. Given 1,040 sentences, each of which is missing a word, the task is to select the correct word out of the five candidates provided for each sentence. Candidate words have been chosen from relatively infrequent words using a maximum entropy model to ensure that the resulting complete sentences were not too improbable. Human judges then picked the best four candidates for each sentence so that all completions were grammatically correct but the correct answer was unambiguous. Though humans can achieve over 90% accuracy on the task, statistical models fare much worse with the best result of 49% produced by a whole-sentence LSA model, and n-gram models achieving only about 39% accuracy (Zweig & Burges, 2011).\nNeural language models are a natural choice for this task because they can take advantage of larger contexts than traditional n-gram models, which we expect\n5Though using a unigram proposal distribution might appear naive, Bengio and Sene\u0301cal (2003) reported that higher-order n-gram proposal distributions worked much worse than the unigram.\nto be important for sentence completion. We used a slightly modified LBL architecture for our models for this task. In the interests of scalability, we used diagonal context weight matrices which reduced the time complexity of gradient computations from quadratic to linear in the dimensionality of word feature vectors and allowed us to use more feature dimensions. Since the task was sentence completion, we made the models aware of sentence boundaries by using a special \u201cout-of-sentence\u201d token for words in context positions outside of the sentence containing the word being predicted. For example, this token would be used as the context word when predicting the first word in a sentence using a model with a single-word context.\nWe score a candidate sentence with a language model by using it to compute the probability of each word in the sentence and taking the product of those probabilities as the sentence score. We then pick the candidate word that produces the highest-scoring sentence as our answer. Note that this way of using a model with a cword context takes into account c words on both sides of the candidate word because the probabilities of the c words following the candidate word depend on it.\nThe models were trained on the standard training set for the challenge containing 522 works from Project Gutenberg. After removing the Project Gutenberg headers and footers from the files, we split them into sentences and then tokenized the sentences into words. We used the Punkt sentence tokenizer and the Penn Treebank word tokenizer from NLTK (Bird et al., 2009). We then converted all words to lowercase and replaced the ones that occurred fewer than 5 times with an \u201cunknown word\u201d token, resulting in a vocabulary size of just under 80,000. The sentences to be completed were preprocessed in the same manner. The resulting dataset was then randomly split at the sentence level into a test and validation sets of 10K words (500 sentences) each and a 47M-word training set.\nWe used the training procedure described in Section 4, with the exception of using a small weight penalty to avoid overfitting. Each model took between one and two days to train on a single core of a modern CPU. As a baseline for comparison, we also trained several ngram models (with modified Kneser-Ney smoothing) using the SRILM toolkit (Stolcke, 2002), obtaining results similar to those reported by Zweig & Burges (2011).\nSince we selected hyperparameters based on the (Gutenberg) validation set perplexity, we report the scores on the entire collection of 1,040 sentences, which means that our results are directly comparable to those of Zweig & Burges (2011). As can be seen from Ta-\nble 3, more word features and larger context leads to better performance in LBL models in terms of both accuracy and perplexity. The LBL models perform considerably better on sentence completion than n-gram models, in spite of having higher test perplexity. Even the LBL model with a two-word context performs better than any n-gram model. The LBL model with a five-word context, matches the best published result on the dataset. Note that the LSA model that produced that result considered all words in a sentence, while an LBL model with a c-word contexts considers only the 2c words that surround the candidate word. The model with a 10-word context and 300D feature vectors outperforms the LSA model by a large margin and sets a new accuracy record for the dataset at 54.7%.\nLanguage models typically use the words preceding the word of interest as the context. However, since we are interested in filling in a word in the middle of the sentence, it makes sense to use both the preceding and the following words as the context for the language model, making the context bidirectional. We trained several LBL models with bidirectional context to see whether such models are superior to their unidirectional-context counterparts for sentence completion. Scoring a sentence with a bidirectional model is both simpler and faster: we simply compute the probability of the candidate word under the model using the context surrounding the word. Thus a model is applied only once per sentence, instead of c+1 times required by the unidirectional models.\nAs Table 3 shows, the LBL models with bidirectional\ncontexts achieve much lower test perplexity than their unidirectional counterparts, which is not surprising because they also condition on words that follow the word being predicted. What is surprising, however, is that bidirectional contexts appear to be considerably less effective for sentence completion than unidirectional contexts. Though the c-word context model and c\u00d7 2-word context model look at the same words when using the scoring procedures we described above, the unidirectional model seems to make better use of the available information."}, {"heading": "6. Discussion", "text": "We have introduced a simple and efficient method for training statistical language models based on noisecontrastive estimation. Our results show that the learning algorithm is very stable and can produce models that perform as well as the ones trained using maximum likelihood in less than one-tenth of the time. In a large-scale test of the approach, we trained several neural language models on a collection of Project Gutenberg texts, achieving state-of-the-art performance on the Microsoft Research Sentence Completion Challenge dataset.\nThough we have shown that the unigram noise distribution is sufficient for training neural language models efficiently, context-dependent noise distributions are worth investigating because they might lead to even faster training by reducing the number of noise samples needed.\nRecently, Pihlaja et al. (2010) introduced a family of estimation methods for unnormalized models that includes NCE and importance sampling as special cases. Other members of this family might be of interest for training language models, though our preliminary results suggest that none of them outperform NCE.\nFinally, we believe that NCE can be applied to many models other than neural or maximum-entropy language models. Probabilistic classifiers with many classes are a prime candidate."}, {"heading": "Acknowledgments", "text": "We thank Vinayak Rao and Lloyd Elliot for their helpful comments. We thank the Gatsby Charitable Foundation for generous funding."}], "references": [{"title": "Adaptive importance sampling to accelerate training of a neural probabilistic language model", "author": ["Bengio", "Yoshua", "Sen\u00e9cal", "Jean-S\u00e9bastien"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "R\u00e9jean", "Vincent", "Pascal"], "venue": "In NIPS, pp", "citeRegEx": "Bengio et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2000}, {"title": "A neural probabilistic language model", "author": ["Bengio", "Yoshua", "Ducharme", "Rejean", "Vincent", "Pascal", "Jauvin", "Christian"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "V.J.D. Pietra", "S.A.D. Pietra"], "venue": "Computational linguistics,", "citeRegEx": "Berger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "In Proc. Int. Conf. on Artificial Intelligence and Statistics (AISTATS2010),", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? \\Q2010\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2010}, {"title": "Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics", "author": ["M.U. Gutmann", "A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gutmann and Hyv\u00e4rinen,? \\Q2012\\E", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen", "year": 2012}, {"title": "A probabilistic model for semantic word vectors", "author": ["A.L. Maas", "A.Y. Ng"], "venue": "In NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Maas and Ng,? \\Q2010\\E", "shortCiteRegEx": "Maas and Ng", "year": 2010}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. \u010cernock\u1ef3", "S. Khudanpur"], "venue": "In Eleventh Annual Conference of the International Speech Communication Association,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. \u010cernock\u1ef3"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th International Conference on Machine Learning,", "citeRegEx": "Mnih and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "Improving a statistical language model through non-linear prediction", "author": ["A. Mnih", "Z. Yuecheng", "G. Hinton"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Andriy", "Hinton", "Geoffrey"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Mnih et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2009}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Morin", "Frederic", "Bengio", "Yoshua"], "venue": "In AISTATS\u201905,", "citeRegEx": "Morin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Morin et al\\.", "year": 2005}, {"title": "A family of computationally efficient and simple estimators for unnormalized statistical models", "author": ["M. Pihlaja", "M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "In Proc. Conf. on Uncertainty in Artificial Intelligence (UAI2010).,", "citeRegEx": "Pihlaja et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Pihlaja et al\\.", "year": 2010}, {"title": "Training neural network language models on very large corpora", "author": ["Schwenk", "Holger", "Gauvain", "Jean-Luc"], "venue": "In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Schwenk et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schwenk et al\\.", "year": 2005}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["R. Socher", "C.C. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "SRILM \u2013 an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "Stolcke,? \\Q2002\\E", "shortCiteRegEx": "Stolcke", "year": 2002}, {"title": "Word representations: A simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "The Microsoft Research Sentence Completion Challenge", "author": ["G. Zweig", "C.J.C. Burges"], "venue": "Technical Report MSR-TR-2011-129, Microsoft Research,", "citeRegEx": "Zweig and Burges,? \\Q2011\\E", "shortCiteRegEx": "Zweig and Burges", "year": 2011}], "referenceMentions": [{"referenceID": 9, "context": "In the last few years neural language models have become competitive with n-grams and now routinely outperform them (Mikolov et al., 2011).", "startOffset": 116, "endOffset": 138}, {"referenceID": 18, "context": "Word representations learned by language models are also used for natural language processing applications such as semantic role labelling (Collobert & Weston, 2008), sentiment analysis (Maas & Ng, 2010), named entity recognition (Turian et al., 2010), and parsing (Socher et al.", "startOffset": 230, "endOffset": 251}, {"referenceID": 16, "context": ", 2010), and parsing (Socher et al., 2011).", "startOffset": 21, "endOffset": 42}, {"referenceID": 1, "context": "The negated scoring function is sometimes referred to as the energy function (Bengio et al., 2000).", "startOffset": 77, "endOffset": 98}, {"referenceID": 3, "context": "1 can describe both neural and maximum entropy language models (Berger et al., 1996).", "startOffset": 63, "endOffset": 84}, {"referenceID": 1, "context": "In some models, different feature vector tables are used for the context and the next word vocabularies (Bengio et al., 2000), while in others the table is shared (Bengio et al.", "startOffset": 104, "endOffset": 125}, {"referenceID": 2, "context": ", 2000), while in others the table is shared (Bengio et al., 2003; Mnih & Hinton, 2007).", "startOffset": 45, "endOffset": 87}, {"referenceID": 8, "context": "In spite of its simplicity, the LBL model has been shown to outperform n-grams, though the more complex neural language models (Mikolov et al., 2010; Mnih et al., 2009) can outperform it.", "startOffset": 127, "endOffset": 168}, {"referenceID": 11, "context": "In spite of its simplicity, the LBL model has been shown to outperform n-grams, though the more complex neural language models (Mikolov et al., 2010; Mnih et al., 2009) can outperform it.", "startOffset": 127, "endOffset": 168}, {"referenceID": 5, "context": "NCE has recently been introduced by Gutmann and Hyv\u00e4rinen (2010) for training unnormalized probabilistic models.", "startOffset": 36, "endOffset": 65}, {"referenceID": 5, "context": "Following Gutmann and Hyv\u00e4rinen (2012), we assume that noise samples are k times more frequent than data samples, so that datapoints come from the mixture 1 k+1P h d (w)+ k k+1Pn(w).", "startOffset": 10, "endOffset": 39}, {"referenceID": 17, "context": "As a baseline for comparison, we also trained several ngram models (with modified Kneser-Ney smoothing) using the SRILM toolkit (Stolcke, 2002), obtaining results similar to those reported by Zweig & Burges (2011).", "startOffset": 128, "endOffset": 143}, {"referenceID": 17, "context": "As a baseline for comparison, we also trained several ngram models (with modified Kneser-Ney smoothing) using the SRILM toolkit (Stolcke, 2002), obtaining results similar to those reported by Zweig & Burges (2011).", "startOffset": 129, "endOffset": 214}, {"referenceID": 14, "context": "Recently, Pihlaja et al. (2010) introduced a family of estimation methods for unnormalized models that includes NCE and importance sampling as special cases.", "startOffset": 10, "endOffset": 32}], "year": 2012, "abstractText": "In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients. We propose a fast and simple algorithm for training NPLMs based on noise-contrastive estimation, a newly introduced procedure for estimating unnormalized continuous distributions. We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models. The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well. We demonstrate the scalability of the proposed approach by training several neural language models on a 47M-word corpus with a 80K-word vocabulary, obtaining state-ofthe-art results on the Microsoft Research Sentence Completion Challenge dataset. Appearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).", "creator": "LaTeX with hyperref package"}}}