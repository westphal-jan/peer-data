{"id": "1206.6447", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Small-sample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering", "abstract": "Functional neuroimaging can measure the brain?s response to an external stimulus. It is used to perform brain mapping: identifying from these observations the brain regions involved. This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus. Brain mapping is then seen as a support recovery problem. On functional MRI (fMRI) data, this problem is particularly challenging as i) the number of samples is small due to limited acquisition time and ii) the variables are strongly correlated. We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables. The use of randomization techniques, e.g. bootstrap samples, and clustering of the variables improves the recovery properties of sparse methods. We demonstrate the benefit of our approach on an extensive simulation study as well as two fMRI datasets. This study provides a useful framework to test the neuroimaging properties of individual brain regions and to provide a more robust model for prediction in the neuroimaging context. Finally, it provides a new way to measure the neural activity associated with a stimulus. We demonstrate a simple and general model of the brain. We suggest that each model has a positive or negative predictive power and use the model to perform a specific task in addition to a normal stimulus. For example, we show that a positive or negative model may indicate that the brain activity associated with the stimulus might differ significantly from a normal stimulus. Furthermore, we show that, as a rule, we cannot perform normal stimulus task at all in an individual brain region. We argue that this is a very important area for our model to explore. Moreover, we suggest that using the model to model the whole brain, we are able to evaluate the ability of the model to perform a specific task in addition to a normal stimulus. This paper also provides a new way to analyze the effect of this model on the neural activity and the brain activity in the individual brain region. Further, we suggest that we are able to predict the neural activity of the individual brain region using the model.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (1437kb)", "http://arxiv.org/abs/1206.6447v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.CV stat.AP stat.ML", "authors": ["ga\u00ebl varoquaux", "alexandre gramfort", "bertrand thirion"], "accepted": true, "id": "1206.6447"}, "pdf": {"name": "1206.6447.pdf", "metadata": {"source": "META", "title": "Small-sample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering", "authors": ["Ga\u00ebl Varoquaux", "Alexandre Gramfort"], "emails": ["gael.varoquaux@inria.fr", "alexandre.gramfort@inria.fr", "bertrand.thirion@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "Functional brain imaging, for instance using functional MRI, is nowadays central to human neuroscience research and the corresponding medical applications. Learning statistical links between the observed brain images and the corresponding subject\u2019s behavior can be formulated as a machine learning problem (Mitchell et al., 2004). Indeed, prediction from neuroimaging data has lead to impressive results, such as brain reading, e.g. guessing which image a subject is looking at\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nfrom his brain activity (Haxby et al., 2001).\nHowever, the main goal of a functional neuroimaging study is most often not prediction per se, but brain mapping, that is identifying the brain regions involved in the cognitive processing of an external stimuli. With fMRI, the data at hand is the brain activity amplitude measured on a grid on voxels. It is classically modeled as a linear effect driven by the cognitive task performed by the subjects. Detecting regions of the brain active in a specific task can be formalized as identifying the non-zero coefficients of a linear model predicting the external stimuli from the neuroimaging data.\nFrom a statistical standpoint, this estimation is very challenging because of the dimensionality of the problem: the behavior must be linked to the brain activity measured on more than 20 000 voxels, often 50 000, with a few hundred observations. For this reason, the problem is most-often tackled as a mass-univariate approach: a model is fitted separately on each voxel. Detection of active voxels then suffers of a multiple comparison problem; the detection power decreases linearly with the number of variables tested. Yet, only a few brain regions are active for a given task and the linear model is sparse. There is great potential interest to use sparse recovery techniques (Carroll et al., 2009), that can recover active voxels suffering only a loss in detection power sub-linear in the number of voxels. However, with brain mapping as with many other experimental fields, the design matrix is imposed by the problem, and due to the strong correlations across regressors, univariate approaches are often more effective than multivariate approaches (Haury et al., 2011).\nThe main contribution of this paper is to propose an efficient sparse recovery procedure well-suited to the specificities of brain mapping situations: spatiallyclustered weights in very high-dimensional correlated designs. Unlike previous work in fMRI (Carroll et al., 2009), we focus on recovery and not predictive power.\nWe also provide a detailed empirical study of sparse estimation in the case of large spatial configurations for the weight and correlated designs. The organization of the paper is the following: in section 2 we review results related to small sample sparse estimation, in section 3, we expose our contributed method, and in section 4 we report experimental results on spatiallycorrelated synthetic data and brain images."}, {"heading": "2. Preliminaries: small-sample estimation of sparse linear models", "text": "The problem at hand is the recovery of sparse models in the presence of noise. Formally, given n observed brain images composed of p voxels, X \u2208 Rn\u00d7p and the corresponding behavioral variable y \u2208 Rn related to the stimulus, the data and the target variable are related by a linear model y = X \u03b2 + e with e \u2208 Rn the measurement noise and \u03b2 \u2208 Rp the coefficients of the model to recover. More specifically, we want to find the sparsity pattern of \u03b2, S = supp(\u03b2) = {i \u2208 [1, . . . p] s.t. \u03b2i 6= 0}. For the application at hand, n will typically be a few hundreds of images while p can be thousands of voxels.\nIn general, the sample complexity \u2013that is the number of samples requires to bound the estimation error\u2013 of a linear model with n observations and p variables is O ( p ) . However, under certain conditions, for sparse signals, i.e. linear models for which k p coefficients are non-zeros, only O ( k log(p\u2212k) ) observations are required to identify these coefficients (Wainwright, 2009; Candes et al., 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).\nTwo types of conditions govern the success of the recovery of a k-sparse vector from noisy measurements, i.e. model consistency of the estimator:\ni) subsets of the columns of design matrix X larger than k should be well conditioned, as for instance implied by the Restricted Isometry Property (RIP) (Candes et al., 2006). In particular, on the signal subspace, the design matrix XS should be sufficiently well conditioned.\nii) the regressors on the signal subspace XS should not be too correlated to regressors on the noise subspace XS , as formalized by Tropp\u2019s Exact Recovery Condition (ERC) (Tropp, 2004) or for the `1-penalized regression the irrepresentable condition (Zhao & Yu, 2006), or mutual incoherence (Wainwright, 2009).\nIn addition, the smallest non-zero coefficient of \u03b2 must scale as the standard deviation of noise and the inverse\nof the minimum singular value of XS . The number of observations necessary and sufficient to almost surely recover the sparsity grows as nmin = 2 \u03b8 k log(p \u2212 k) where \u03b8 depends on the various factors listed previously (Wainwright, 2009). In the specific case of X i.i.d Gaussian distributed with identity covariance, \u03b8 \u2248 1. Importantly, the Lasso estimator yields at most n non-zero coefficients, and for n below nmin, sparse methods will often fail dramatically to recover a sparsity pattern close to the ground truth.\nA common cause for failure of sparse recovery is multicolinearity: the presence of strong correlations in the design X. In practice, if several columns of X are strongly correlated, sparse estimators will often select arbitrarily one of them and not the others, leading for instance to a high rate of false negatives in the support estimation if they all correspond to the signal subspace. In the extreme case of exactly collinear regressors, the Lasso, which is a non-strictly convex optimization, does not admit a unique solution. For this reason, the elastic net (Zou & Hastie, 2005) adds a strongly convex term to the Lasso in the form of a `2 penalty. This term has the effect of grouping together correlated regressors. It opens the door to sparse recovery in relaxed conditions, in particular with illconditioned signal-subspace design matrices XS (Jia & Yu, 2010). In addition, while the sample complexity of the elastic net scales similarly to that of the Lasso, the former can select more than n coefficients. Another approach to ensuring strong convexity in the presence of correlated variables is based on mixed norms, imposing an `2 penalty on sub-groups of features known a priori to covary (Yuan & Lin, 2006). When the group structure is unknown, Grave et al. (2011) proposed the trace-lasso, adapting the penalization to the design matrix. The limitation of this approach is that in the small-sample limit, the design matrix can only define a small number of strictly convex directions. For image data, correlation across neighboring voxels can be modeled using overlapping groups (Jacob et al., 2009).\nAnother challenge of high-dimensional sparsity recovery from noisy observations is the difficulty to control false positives, i.e. inclusion in the estimated support S\u0302 of variables not present in the true support S. For the Lasso, theoretical bounds on false detections are conditional on choosing the right value for the parameter \u03bb, controlling the amount of `1 regularization. This optimum depends on the noise level and structure, and is generally considered challenging to set in the smallsample settings. For these reasons, Bach (2008) and Meinshausen & Bu\u0308hlmann (2010) introduce resampled estimators based on the Lasso. As it is well known in mass-univariate analysis, such sampling of the poste-\nrior yields control of the probability of false selection with weaker constraints on the model, here \u03bb. In addition, different variables in a strongly correlated group can be selected across the resampling estimates, and as a result these schemes can in theory recover correlated variables and more variables than observations. Another Lasso variant, the randomized Lasso (Meinshausen & Bu\u0308hlmann, 2010) introduces a random perturbation of the design X by rescaling the regressors. It achieves sparse recovery under a RIP-like condition, bounds on sparse eigenvalues of the design, without the need of an irrepresentable-like condition imposing a strong separation between signal and noise regressors. Provided \u03bb is chosen large enough, the randomized lasso yields good control of the inclusion of noise variables even with a small number of observations.\nWhile most of the theoretical results on sparse recovery have been established for the square loss, these techniques can carry over to other losses, for instance the logistic loss, which is strongly convex as the square loss, but is well suited to classification problems, rather than regression. Bach (2010) extends non-asymptotic least-square results to logistic regression by approximating it as a weighted least square and show that similar conditions on the design apply for sparse recovery with `1 penalization. Small sample learning rates for `1-penalized logistic regression were established earlier by Ng (2004) based on the non rotational invariance of the `1 ball. This general argument gives a simple necessary condition for sub-linear sample complexity of empirical risk minimizers: the corresponding optimization problem should not be rotational invariant. In addition the rotational asymmetries of the learning problem should correspond to a good representation of the data. Indeed, adapting the representation of a signal such that its decomposition is more sparse is key to the performance of sparse methods."}, {"heading": "3. Randomization and clustering for sparse recovery in brain imaging", "text": "Our approach to sparse recovery on spatially correlated images can be summarized by i) clustering highly-correlated variables before sparse regression ii) randomizing the clustering and sparse estimation. Our work is based on the randomized lasso (Meinshausen & Bu\u0308hlmann, 2010), which we first briefly present.\nThe randomized lasso The randomized lasso consists in randomly perturbing the design matrix by taking only a fraction of the training samples and randomly scaling each variable, in our case each voxel. By repeating the later procedure and then counting how\noften each variable is selected across the repetitions, each variable can be assigned a score. Higher scores denote variables likely to belong to the true support.\nLet i \u2208 [1 . . . l] denote the repetition and \u03b2i be the corresponding estimated coefficients. The design matrix Xi is formed by a random fraction \u03c0 of the training data. Each column of Xi is then randomly scaled using a Bernoulli distribution to 1 or to 1 \u2212 \u03b1 with equal probability. The procedure is a subsampling of the data and a random perturbation of each column. The stability score of each voxel v is then the percentage of the repetitions for which the voxel has a non-zero weight as estimated by sparse regression, i.e. is used for the prediction: av = #{i s.t. v \u2208 supp(\u03b2i)}/l \u2208 [0, 1]. The estimated support is then defined as {v s.t. av \u2265 \u03c4}. In the following, we set \u03b1 = 0.5 and \u03c0 = 75% as suggested by (Meinshausen & Bu\u0308hlmann, 2010), and use l = 200.\nThe randomized lasso does not fully address the problem of large groups of correlated variables. Indeed, the selection score will undoubtedly tend to zero as the size of group increase, and be confounded by selection scores of uninformative variables due to chance.\nAdding randomized clustering To improve the stability of the estimation we propose to work with clusters of correlated variables. The motivation is that clustering variables and replacing groups of variables by their mean value will reduce the number of variables and improve the properties of the resulting design Xred: less variables and less correlation between them. As the variables are voxels defined over a 3 dimensional grid and as correlation between voxels is strongly local, we can further constrain the clustering procedure to cluster only neighboring voxels. Following Michel et al. (2012), we use spatially-constrained Ward hierarchical clustering to cluster the voxels in q spatially connected regions in which the signal is averaged. When merging two singleton clusters, the Ward criteria chooses the samples with largest crosscovariance. The benefit of this algorithm is thus that it is a bottom-up approach that captures well local correlations into spatial clusters. In addition, it gives a multi-scale representation: a tree of nested regions.\nWe run the clustering algorithm in the randomization loop of the randomized lasso: after sub-sampling the observations and rescaling the variables. As the hierarchical tree is estimated each time on a random fraction of the data, the tree is different for every randomization. Note that a similar randomization on trees by resampling is performed in the Random Forests algorithm (Breiman, 2001). It is motivated\nAlgorithm 1 Randomized ward lasso Require: Input: datasets X \u2208 Rn\u00d7p, target variable y \u2208 Rn, number of clusters q, penalization \u03bb, number of resampling l, scaling \u03b1 \u2208 [0, 1], sub-sampling fraction \u03c0 \u2208 [0, 1]\n1: for i = 1 to l do 2: Sub-sample: X\u0303 \u2190XJ , y\u0303 \u2190 yJ where J \u2282 {1 . . . n}, card(J) = \u03c0 n, X\u0303 \u2208 R\u03c0 n\u00d7p, y\u0303 \u2208 R\u03c0 n 3: Randomize feature scaling: X\u0303 \u2190 X\u0303\u00b7w, where w \u2208 Rp,wi = 1\u2212 \u03b1\u00b5i, \u00b5i \u223c Bern(0.5) 4: Cluster features, and use the clusters mean: X\u0303red \u2190 ward(X\u0303), X\u0303red \u2208 R\u03c0 n\u00d7q 5: Estimate \u03b2ired \u2208 Rq from X\u0303red and y\u0303 with sparse linear regression 6: Label initial features with estimated coefficient on corresponding cluster: \u03b2i \u2190 ward\u22121(\u03b2ired), \u03b2i \u2208 Rp 7: end for 8: return stability scores av = #{k s.t. v \u2208 supp(\u03b2i)}/l, av \u2208 [0, 1]\nby the high-variance of estimators based on greedy tree construction. Similarly, in consensus clustering (Monti et al., 2003) resampling is applied to the construction of clusters to account for the sensibility of the algorithms, that are most often based on greedy approaches to non-convex problems. After variable clustering, the sparse estimator can be fitted on a qdimensional dataset. A variable is marked as selected in repetition k if it belongs to a cluster with a non-zero weight. As the estimated supp(\u03b2i) is in Rq, we will write v \u2208 supp(ward\u22121(\u03b2i)), where ward\u22121 amounts to assigning to the initial features the coefficient estimated for the center of the cluster that they belong to. The full estimation procedure, that we denote by randomized ward lasso, is summarized in Algorithm 1.\nThe main benefit of the additional clustering step is to reduce the correlations in the design matrix, therefore improving the behavior of sparse methods. Indeed, as correlated features are merged, the conditioning of sub-matrices of Xred improves, and thus the conditions for control of false detections with the randomized lasso are fulfilled for q small enough. The key to the success of our method is this tight control: with high probability, clusters selected during each iteration all contain an original non zero feature. When backprojecting the selection frequency in the non-clustered space, because of the randomness of the cluster shapes in the different iterations, the selection frequency score can outline structures of a smaller size than the cluster size. There is an inherent trade-off in the choice of q: large clusters will most likely not match the geometry of the support of \u03b2 but are more easily selected by sparse regression and are therefore more stable. Note that the separation between the signal and noise subspace may not be improved by the clustering, but it is not central to the success of the randomized lasso.\nImportantly, thanks to the clustering, our method can select more variables than the number of observations, which would be impossible with a standard Lasso-type estimator and difficult with only randomization. For-\nmally, the randomized ward lasso can be seen as applying a sparse estimator on random projections, learned by clustering. Importantly, those random projections are made of a small number of voxels, and thus are consistent with the assumed sparse representation of our problem. In addition, as they are not performed in rotationally-invariant directions, the resulting estimator is not rotationally invariant, and we suggest that it can achieve sub-linear sample complexity.\nFinally, the randomized ward lasso is computationally tractable in the large p settings. Indeed, agglomerative clustering can be implemented in linear time with regards to the number of possible agglomerations tested (Mu\u0308llner, 2011). As we constrain clustering on the spatial grid, this number of agglomeration grows only linearly with p. The cost of a Lasso estimate on an n \u00d7 p problem is O ( n p k ) if it selects k non-zero coefficients. Thus, for l resampling iterations q = \u03ba p clusters, \u03ba \u2208 [0, 1], the computational complexity of the randomized ward lasso scales as O ( l \u03ba2 n p k ) . Importantly for our applications, the scaling in p is linear. This is to be contrasted with sparse methods imposing a spatial regularization, such as group Lasso with overlapping spatial groups."}, {"heading": "4. Experimental results", "text": ""}, {"heading": "4.1. Synthetic data", "text": "Data generation We compare different sparse estimators on simulated data for which the ground truth is known. We simulate data according to a simple model reflecting the key elements of brain imaging data: local correlations of the design matrix X, i.e. the observed brain images, and spatially clustered non-zero weights. Specifically, we work on a (32 \u00d7 64) grid of 2048 features. We generate an i.i.d Gaussian designX that we spatially smooth with a 2D Gaussian filter of standard deviation \u03c3. The weight vectors \u03b2 to recover have k = 64 non-zero coefficients. We split them into spatial clusters of size c, evenly-separated on the spa-\ntial grid. Non-zero coefficients are uniformly sampled between \u03b2min = .2 and 1 + \u03b2min. Finally, the target variable y is generated according to the linear model with additive Gaussian i.i.d noise. The amplitude of the noise is set to ensure that X explains 80% of the variance of y under the true model.\nThe two parameters of our procedure, the smoothing and the cluster size, control to which point the synthetic data violate the conditions for recovery. Sufficient smoothing can render arbitrarily ill conditioned groups of regressors located in a spatial neighborhood, thus violating sparse eigenvalue properties or the RIP. In particular, if the clusters are large, the design matrix restricted to the signal subspace XS is ill conditioned, whereas if they are small, the non-zero coefficients are well separated and XS is well conditioned. Finally, as the smoothing increases, so does the coupling between the signal and noise subspaces, although this coupling is less important for large clusters.\nWe investigate sample sizes n = 128 and 256, which are standard numbers for fMRI datasets. Note that with no smoothing, elements of X are Gaussian i.i.d distributed, with identity covariance, and nmin \u2248 1000.\nSuccess metrics We compare the ability to recover the true support of different estimators. Each estimator yields a score for each variable. The higher the score, the more likely is the variable to be in the support. We use precision-recall curves to quantify the ability of each estimator to recover the non-zero coefficients as the threshold on the scores is varied. We summarize the precision-recall by the area under the curve (AUC). An AUC of .5 is chance, while 1 is perfect recovery: there exists a threshold such that active and non-active features are perfectly separated. In practice, we consider that an AUC above 0.95 is a near-perfect recovery, and an AUC above .75 as usable. To match the context of real data where ground truth is unknown, note that we report performance for automatically selected parameters, and not best possible performance, unlike e.g. Grave et al. (2011).\nResults We investigated two non-convex approaches: a greedy algorithm, orthogonal matching pursuit (Mallat & Zhang, 1993), and an iterative approach, the adaptive Lasso (Zou, 2006). We do not report the results as they did not achieve useful recovery. This failure can be understood because of the low number of samples, below nmin in non smoothed design, and the fragility of non-convex estimators when working with correlated designs. In addition, we studied screening based on F-tests, the elastic net, the randomized lasso, and our contribution, the random-\nized ward lasso. Each time, we set the regularization parameters using 6-fold cross-validation to maximize the explained variance of left out data. For elastic net, we also set by cross-validation the amount of `2 regularization, and for randomized ward lasso, the number of clusters. Setting penalties by minimizing test error gives no guaranties on estimation error in terms of recovery, but alternative methods like information-based criterion (IC) such as BIC or AIC do not hold in theory for small-sample settings. In our simulations cross-validation was superior over IC.\nResults can be seen on figure 1. With no smoothing, all methods fail to give satisfactory recovery, although very large clusters can be partially recovered by randomized ward lasso with n = 256. For large cluster sizes, adding a small amount of correlation to the design improves the recovery. Indeed, the added correlation is consistent with the sparsity pattern to recover. Elastic net and randomized lasso can then recover even small clusters. Due to the small sample size, no method performs well in case of small clusters without any smoothing, nor in the case of small clusters and large smoothing, which corresponds to an\nill-posed deconvolution problem. In the large cluster and large smoothing case, the univariate approach outperforms all others. In this situation, sub-matrices of the design matrix are too ill-conditioned for sparse methods to succeed, but the effective number of degrees of freedom to be estimated is strongly reduced by the inter-feature correlations: the univariate approach does not perform many independent hypothesis tests and thus the number of false positive is reduced. In a large region of the parameter space explored, for clusters composed of 8 or more features, and smoothing standard deviation of the order of a pixel, our proposed method, the randomized ward lasso, outperforms other methods. We note that in the small sample case (n = 128) parameter selection is particularly difficult for this method. Interestingly, we find that, in the settings that we explore, the randomized lasso is most often outperformed by the elastic net, suggesting that the elastic net\u2019s relaxation of the conditions on sparse eigenvalues is better suited to the problem than the randomized lasso\u2019s relaxation of the irrepresentable condition. Finally, we confirmed empirically that, in the randomized ward lasso, randomizing the clusters at each iteration was important: fully-random clusters outperform using a non-random clusters, and our randomized approach performs best (Fig. 2)."}, {"heading": "4.2. FMRI data", "text": "To assess the performance of our approach on real fMRI data, we investigated two datasets. The first one from Tom et al. (2007) is a gambling task where the subject is asked to accept or reject gambles that offered a 50/50 chance of gaining or losing money. Each gamble has an amount that can be used as target in a regression setting. The second dataset from Haxby et al. (2001) is a visual object recognition task. Each object category can then be used as target in a classification setting. In this setting, we use a sparse logistic regression model instead of a Lasso. We refer to Tom et al. (2007) and Haxby et al. (2001) for a detailed description of the experimental protocol. Data are publicly available on http://openfmri.org.\nRegression After standard preprocessing (slice timing, motion correction, first level analysis with a general linear model, inter-subject spatial normalization), the regression dataset consists of 17 subjects with 48 fMRI observations per subject. Only the gain condition was used (see (Jimura & Poldrack, 2012)). The 48 observations contain 6 repetitions of the experiment with 8 levels of gain. FMRI volumes were downsampled to 4\u00d74\u00d74 mm voxels. When learning from the full set of 17 subjects, the training data therefore consist\nof 816 samples with approximately 26 000 voxels. The prediction here is inter-subject: the estimator learns on some subjects and predicts on left out subjects.\nClassification In the object recognition experiment, 5 subjects were asked to recognize 8 different types of objects (face, house, cat, shoe, scissors etc.). We focus here on the binary classification task that consists in predicting whether the subject was viewing one object or the other (e.g. a house or a face). The data consist of 12 sessions, each with 9 volumes per category. When considering N sessions for training, data consist of 18N volumes each containing about 30 000 voxels. The problem is here intra-subject: some sessions are used for learning and prediction is performed on left out sessions of the same subject.\nAs for such real data the ground truth is not known, we test these qualitative observations by training a predictive model on the voxels with the highest scores as in (Haury et al., 2011). The rationale is that if voxels with the highest F-scores contain more false positives than voxels with the highest stability scores then a predictive model trained with the first set should give significantly worse performance than a model trained with the second one. To validate this hypothesis, we train a `2 logistic regression model on the best voxels as scored by the methods, for different object contrasts and numbers of training sessions. The number of selected voxels and the regularization parameter is optimized by cross-validation. For T training sessions (4 \u2264 T \u2264 10) we obtain a prediction score on 12\u2212T left out sessions, unseen by the feature-selection method. For completeness, we also benchmark a linear SVM, as well as `1 and `2 logistic regressions trained on the whole brain. Experiments were performed with the scikit-learn (Pedregosa et al., 2011), using LIBLINEAR (Fan et al., 2008) for logistic regression and LIBSVM (Chang & Lin, 2011) for SVM.\nResults Figure 3 shows maps of F values, the scores obtained with standard stability selection, and the scores obtained using our approach. The scores are not thresholded for visualization purposes, meaning that the zeros obtained are actually zeros. On both datasets, one can observe that despite being fairly noisy F-tests highlight well localized regions of the brain. Very similar regions are clearly outlined by our approach, whereas the standard stability selection procedure yields more spatially scattered selection scores. We also ran elastic-net in the regression settings (not displayed). The cross-validation lead setting a compromise between `2 and `1 penalty in favor of `1: \u03bb (0.1`2 + 0.9`1). As a result, the maps were\noverly sparse, displaying single scattered voxels. Finally, in the regression and classification settings, we find \u2248 1000 and 2700 non-zero scores, which corresponds to a ratio n/nmin respectively \u2248 25 and 780 for nmin = 2p log(p \u2212 k). While visual inspection of the figures suggests that thresholding the scores at 0 probably leads to false detections and thus over estimating k, the estimation problems tackled here can be safely considered as well beyond the reach of conventional sparse estimators in terms of number of samples.\nThese prediction results are reported in Fig. 4. First we observe that a linear SVM and an `2-logistic regression yield very similar results which is expected due to the similarity between the logistic loss and the hinge loss. Second, as this learning problem displays many non-informative features, using an `1 penalization actually helps the prediction. Finally, we observe that using the best voxels as scored by the randomized ward logistic leads to significantly better performance than when using the voxels with the highest F-score. This suggests that it actually achieves a better identification of the voxels involved in the cognitive task."}, {"heading": "5. Conclusion", "text": "Motivated by the problem of brain mapping using functional MRI data we address the problem of support identification when working with strongly spa-\ntially correlated designs as well as low number of samples. We propose to use a clustering of variables as well as randomization techniques, in order to position the problem in settings where a good recovery is achievable, i.e., low correlation between variables in the support and limited number of variables compared to the number of samples. Further informed by the strong spatial correlations between voxels in fMRI data we constrain spatially the clusters. This constraint injects additional prior in the estimation to facilitate the recovery of spatially contiguous supports. Our formulation enables the use of computationally-efficient sparse linear regression models and yields an overall algorithmic complexity linear in the number of features.\nResults on simulations highlight the settings in which our approach outperforms techniques like elastic net or univariate feature screening. Results on two publicly available fMRI datasets demonstrate how our approach is able to zero out some non-informative regions while outlining relevant clusters of voxels. Although the true support is unknown for real data, prediction scores obtained on the best voxels outperforms alternative supervised methods suggesting that our approach is not only better in terms of support identification but can also significantly improve prediction on such data.\nThe success of our approach with very small sample sizes lies in exploiting the characteristics of the prob-\nlem tackled: recovery of contiguous clusters from image measurements with local correlations. Indeed by clustering variables, we exploit the fact that their observed correlation shares common structure with the sparsity pattern to recover. Further work include investigating other clustering algorithms and comparing to more computationally costly but convex estimators such as using overlapping group penalties."}], "references": [{"title": "Bolasso: model consistent lasso estimation through the bootstrap", "author": ["F. Bach"], "venue": "In ICML, pp", "citeRegEx": "Bach,? \\Q2008\\E", "shortCiteRegEx": "Bach", "year": 2008}, {"title": "Self-concordant analysis for logistic regression", "author": ["F. Bach"], "venue": "Elec J Stat,", "citeRegEx": "Bach,? \\Q2010\\E", "shortCiteRegEx": "Bach", "year": 2010}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E.J. Candes", "J.K. Romberg", "T. Tao"], "venue": "Comm. Pure Appl. Math.,", "citeRegEx": "Candes et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2006}, {"title": "Prediction and interpretation of distributed neural activity with sparse models", "author": ["M.K. Carroll", "G.A. Cecchi", "I. Rish", "R. Garg", "A.R. Rao"], "venue": null, "citeRegEx": "Carroll et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Carroll et al\\.", "year": 2009}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chang", "C-C", "Lin", "C-J"], "venue": "ACM Trans Intell Syst Tech,", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Trace lasso: a trace norm regularization for correlated designs", "author": ["E. Grave", "G.R. Obozinski", "F. Bach"], "venue": "In Adv NIPS,", "citeRegEx": "Grave et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grave et al\\.", "year": 2011}, {"title": "The influence of feature selection methods on accuracy, stability and interpretability of molecular signatures", "author": ["Haury", "A-C", "P Gestraud", "Vert", "J-P"], "venue": "PLoS ONE, pp", "citeRegEx": "Haury et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Haury et al\\.", "year": 2011}, {"title": "Distributed and overlapping representations of faces and objects in ventral temporal cortex", "author": ["J.V. Haxby", "I.M. Gobbini", "Furey", "M.L"], "venue": "Science, 293:2425,", "citeRegEx": "Haxby et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Haxby et al\\.", "year": 2001}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J.P. Vert"], "venue": "In ICML, pp", "citeRegEx": "Jacob et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jacob et al\\.", "year": 2009}, {"title": "On model consistency of the elastic net when p n", "author": ["J. Jia", "B. Yu"], "venue": "Statistica Sinica,", "citeRegEx": "Jia and Yu,? \\Q2010\\E", "shortCiteRegEx": "Jia and Yu", "year": 2010}, {"title": "Analyses of regional-average activation and multivoxel pattern information tell complementary", "author": ["K. Jimura", "R. Poldrack"], "venue": "stories. Neuropsychologia,", "citeRegEx": "Jimura and Poldrack,? \\Q2012\\E", "shortCiteRegEx": "Jimura and Poldrack", "year": 2012}, {"title": "Matching pursuits with timefrequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "Trans Signal Process,", "citeRegEx": "Mallat and Zhang,? \\Q1993\\E", "shortCiteRegEx": "Mallat and Zhang", "year": 1993}, {"title": "A supervised clustering approach for fMRI-based inference of brain states", "author": ["V. Michel", "A. Gramfort", "G Varoquaux"], "venue": "Pattern Recognition,", "citeRegEx": "Michel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Michel et al\\.", "year": 2012}, {"title": "Learning to decode cognitive states from brain images", "author": ["T.M. Mitchell", "R. Hutchinson", "R.S. Niculescu", "F Pereira"], "venue": "Machine Learning,", "citeRegEx": "Mitchell et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2004}, {"title": "Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data", "author": ["S. Monti", "P. Tamayo", "J. Mesirov", "T. Golub"], "venue": "Machine learning,", "citeRegEx": "Monti et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Monti et al\\.", "year": 2003}, {"title": "Modern hierarchical, agglomerative clustering algorithms", "author": ["D. M\u00fcllner"], "venue": "Arxiv preprint arXiv:1109.2378,", "citeRegEx": "M\u00fcllner,? \\Q2011\\E", "shortCiteRegEx": "M\u00fcllner", "year": 2011}, {"title": "Feature selection, l1 vs. l2 regularization, and rotational invariance", "author": ["A. Ng"], "venue": "In ICML, pp", "citeRegEx": "Ng,? \\Q2004\\E", "shortCiteRegEx": "Ng", "year": 2004}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A Gramfort"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Roy. Statistical Society B,", "citeRegEx": "Tibshirani,? \\Q1994\\E", "shortCiteRegEx": "Tibshirani", "year": 1994}, {"title": "The neural basis of loss aversion in decision-making under risk", "author": ["S.M. Tom", "C.R. Fox", "C. Trepel", "R. Poldrack"], "venue": "Science, 315:515,", "citeRegEx": "Tom et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Tom et al\\.", "year": 2007}, {"title": "Greed is good: Algorithmic results for sparse approximation", "author": ["J.A. Tropp"], "venue": "Trans Inf Theory,", "citeRegEx": "Tropp,? \\Q2004\\E", "shortCiteRegEx": "Tropp", "year": 2004}, {"title": "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming", "author": ["M. Wainwright"], "venue": "Trans Inf Theory,", "citeRegEx": "Wainwright,? \\Q2009\\E", "shortCiteRegEx": "Wainwright", "year": 2009}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Roy. Statistical Society B,", "citeRegEx": "Yuan and Lin,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Lin", "year": 2006}, {"title": "On model selection consistency of lasso", "author": ["P. Zhao", "B. Yu"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Zhao and Yu,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu", "year": 2006}, {"title": "The adaptive lasso and its oracle properties", "author": ["H. Zou"], "venue": "J. Amer. Statistical Assoc.,", "citeRegEx": "Zou,? \\Q2006\\E", "shortCiteRegEx": "Zou", "year": 2006}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "J. Roy. Statistical Society B,", "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 14, "context": "Learning statistical links between the observed brain images and the corresponding subject\u2019s behavior can be formulated as a machine learning problem (Mitchell et al., 2004).", "startOffset": 150, "endOffset": 173}, {"referenceID": 8, "context": "from his brain activity (Haxby et al., 2001).", "startOffset": 24, "endOffset": 44}, {"referenceID": 3, "context": "There is great potential interest to use sparse recovery techniques (Carroll et al., 2009), that can recover active voxels suffering only a loss in detection power sub-linear in the number of voxels.", "startOffset": 68, "endOffset": 90}, {"referenceID": 7, "context": "However, with brain mapping as with many other experimental fields, the design matrix is imposed by the problem, and due to the strong correlations across regressors, univariate approaches are often more effective than multivariate approaches (Haury et al., 2011).", "startOffset": 243, "endOffset": 263}, {"referenceID": 3, "context": "Unlike previous work in fMRI (Carroll et al., 2009), we focus on recovery and not predictive power.", "startOffset": 29, "endOffset": 51}, {"referenceID": 22, "context": "linear models for which k p coefficients are non-zeros, only O ( k log(p\u2212k) ) observations are required to identify these coefficients (Wainwright, 2009; Candes et al., 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).", "startOffset": 135, "endOffset": 174}, {"referenceID": 2, "context": "linear models for which k p coefficients are non-zeros, only O ( k log(p\u2212k) ) observations are required to identify these coefficients (Wainwright, 2009; Candes et al., 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).", "startOffset": 135, "endOffset": 174}, {"referenceID": 19, "context": ", 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).", "startOffset": 146, "endOffset": 164}, {"referenceID": 21, "context": ", 2006), and the corresponding estimation problem can be solved using tractable algorithms such as `1-penalized square loss regression \u2013the Lasso (Tibshirani, 1994)\u2013 or greedy approaches (Tropp, 2004).", "startOffset": 187, "endOffset": 200}, {"referenceID": 2, "context": "i) subsets of the columns of design matrix X larger than k should be well conditioned, as for instance implied by the Restricted Isometry Property (RIP) (Candes et al., 2006).", "startOffset": 153, "endOffset": 174}, {"referenceID": 21, "context": "ii) the regressors on the signal subspace XS should not be too correlated to regressors on the noise subspace XS , as formalized by Tropp\u2019s Exact Recovery Condition (ERC) (Tropp, 2004) or for the `1-penalized regression the irrepresentable condition (Zhao & Yu, 2006), or mutual incoherence (Wainwright, 2009).", "startOffset": 171, "endOffset": 184}, {"referenceID": 22, "context": "ii) the regressors on the signal subspace XS should not be too correlated to regressors on the noise subspace XS , as formalized by Tropp\u2019s Exact Recovery Condition (ERC) (Tropp, 2004) or for the `1-penalized regression the irrepresentable condition (Zhao & Yu, 2006), or mutual incoherence (Wainwright, 2009).", "startOffset": 291, "endOffset": 309}, {"referenceID": 22, "context": "The number of observations necessary and sufficient to almost surely recover the sparsity grows as nmin = 2 \u03b8 k log(p \u2212 k) where \u03b8 depends on the various factors listed previously (Wainwright, 2009).", "startOffset": 180, "endOffset": 198}, {"referenceID": 9, "context": "For image data, correlation across neighboring voxels can be modeled using overlapping groups (Jacob et al., 2009).", "startOffset": 94, "endOffset": 114}, {"referenceID": 6, "context": "When the group structure is unknown, Grave et al. (2011) proposed the trace-lasso, adapting the penalization to the design matrix.", "startOffset": 37, "endOffset": 57}, {"referenceID": 0, "context": "For these reasons, Bach (2008) and Meinshausen & B\u00fchlmann (2010) introduce resampled estimators based on the Lasso.", "startOffset": 19, "endOffset": 31}, {"referenceID": 0, "context": "For these reasons, Bach (2008) and Meinshausen & B\u00fchlmann (2010) introduce resampled estimators based on the Lasso.", "startOffset": 19, "endOffset": 65}, {"referenceID": 0, "context": "Bach (2010) extends non-asymptotic least-square results to logistic regression by approximating it as a weighted least square and show that similar conditions on the design apply for sparse recovery with `1 penalization.", "startOffset": 0, "endOffset": 12}, {"referenceID": 0, "context": "Bach (2010) extends non-asymptotic least-square results to logistic regression by approximating it as a weighted least square and show that similar conditions on the design apply for sparse recovery with `1 penalization. Small sample learning rates for `1-penalized logistic regression were established earlier by Ng (2004) based on the non rotational invariance of the `1 ball.", "startOffset": 0, "endOffset": 324}, {"referenceID": 13, "context": "Following Michel et al. (2012), we use spatially-constrained Ward hierarchical clustering to cluster the voxels in q spatially connected regions in which the signal is averaged.", "startOffset": 10, "endOffset": 31}, {"referenceID": 15, "context": "Similarly, in consensus clustering (Monti et al., 2003) resampling is applied to the construction of clusters to account for the sensibility of the algorithms, that are most often based on greedy approaches to non-convex problems.", "startOffset": 35, "endOffset": 55}, {"referenceID": 16, "context": "Indeed, agglomerative clustering can be implemented in linear time with regards to the number of possible agglomerations tested (M\u00fcllner, 2011).", "startOffset": 128, "endOffset": 143}, {"referenceID": 6, "context": "Grave et al. (2011).", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "Results We investigated two non-convex approaches: a greedy algorithm, orthogonal matching pursuit (Mallat & Zhang, 1993), and an iterative approach, the adaptive Lasso (Zou, 2006).", "startOffset": 169, "endOffset": 180}, {"referenceID": 19, "context": "The first one from Tom et al. (2007) is a gambling task where the subject is asked to accept or reject gambles that offered a 50/50 chance of gaining or losing money.", "startOffset": 19, "endOffset": 37}, {"referenceID": 8, "context": "The second dataset from Haxby et al. (2001) is a visual object recognition task.", "startOffset": 24, "endOffset": 44}, {"referenceID": 8, "context": "The second dataset from Haxby et al. (2001) is a visual object recognition task. Each object category can then be used as target in a classification setting. In this setting, we use a sparse logistic regression model instead of a Lasso. We refer to Tom et al. (2007) and Haxby et al.", "startOffset": 24, "endOffset": 267}, {"referenceID": 8, "context": "The second dataset from Haxby et al. (2001) is a visual object recognition task. Each object category can then be used as target in a classification setting. In this setting, we use a sparse logistic regression model instead of a Lasso. We refer to Tom et al. (2007) and Haxby et al. (2001) for a detailed description of the experimental protocol.", "startOffset": 24, "endOffset": 291}, {"referenceID": 7, "context": "As for such real data the ground truth is not known, we test these qualitative observations by training a predictive model on the voxels with the highest scores as in (Haury et al., 2011).", "startOffset": 167, "endOffset": 187}, {"referenceID": 18, "context": "Experiments were performed with the scikit-learn (Pedregosa et al., 2011), using LIBLINEAR (Fan et al.", "startOffset": 49, "endOffset": 73}, {"referenceID": 5, "context": ", 2011), using LIBLINEAR (Fan et al., 2008) for logistic regression and LIBSVM (Chang & Lin, 2011) for SVM.", "startOffset": 25, "endOffset": 43}], "year": 2012, "abstractText": "Functional neuroimaging can measure the brain\u2019s response to an external stimulus. It is used to perform brain mapping: identifying from these observations the brain regions involved. This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus. Brain mapping is then seen as a support recovery problem. On functional MRI (fMRI) data, this problem is particularly challenging as i) the number of samples is small due to limited acquisition time and ii) the variables are strongly correlated. We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables. The use of randomization techniques, e.g. bootstrap samples, and clustering of the variables improves the recovery properties of sparse methods. We demonstrate the benefit of our approach on an extensive simulation study as well as two fMRI datasets.", "creator": "LaTeX with hyperref package"}}}