{"id": "1206.6461", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model", "abstract": "We consider the problem of learning the optimal action-value function in the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on the sample-complexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor \\gamma\\in[0,1) only O(N\\log(N/\\delta)/((1-\\gamma)^3\\epsilon^2)) samples are required to find an \\epsilon-optimal estimation of the action-value function with the probability 1-\\delta. We also prove a matching lower bound of \\Theta (N\\log(N/\\delta)/((1-\\gamma)^3\\epsilon^2)) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action-) value function in which the upper bound matches the lower bound of RL in terms of N, \\epsilon, \\delta and 1/(1-\\gamma). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1-\\gamma). The optimization of N values in the range of 0.000 to 2.000 to 6.000 means that we know that this optimization is very efficient for estimating the optimal action-value function and thus that we can find a reliable way to improve it. This model is very general and is highly general. A recent paper in Nature shows that the probability of an MDP with n state-action pairs and the discount factor \\gamma\\in[1] are highly efficient for estimating the optimal action-value function in which the lower bound matches the lower bound. In the case of n-state-action pairs, this approach is not very efficient for estimating the optimal action-value function in which the upper bound matches the lower bound.\n\n\n\n\n\nIn our previous paper, we propose that the reduction-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-precision-pre", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (468kb)", "http://arxiv.org/abs/1206.6461v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["mohammad gheshlaghi azar", "r\u00e9mi munos", "bert kappen"], "accepted": true, "id": "1206.6461"}, "pdf": {"name": "1206.6461.pdf", "metadata": {"source": "META", "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model ", "authors": ["Mohammad Gheshlaghi Azar", "R\u00e9mi Munos"], "emails": ["m.azar@science.ru.nl", "remi.munos@inria.fr", "b.kappen@science.ru.nl"], "sections": [{"heading": null, "text": "( N log(N/\u03b4)/ ( (1 \u2212 \u03b3)3\u03b52 ))\nsamples are required to find an \u03b5-optimal estimation of the action-value function with the probability 1 \u2212 \u03b4. We also prove a matching lower bound of \u0398 ( N log(N/\u03b4)/ ( (1 \u2212 \u03b3)3\u03b52 )) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action) value function in which the upper bound matches the lower bound of RL in terms ofN , \u03b5, \u03b4 and 1/(1\u2212\u03b3). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1\u2212 \u03b3)."}, {"heading": "1. Introduction", "text": "Model-based value iteration (VI) (Kearns & Singh, 1999; Bus\u0327,oniu et al., 2010) is a well-known reinforcement learning (RL) (Szepesva\u0301ri, 2010; Sutton & Barto, 1998) algorithm which relies on an empirical estimate of the state-transition dis-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\ntributions to estimate the optimal (action-)value function through the Bellman recursion. In the finite state-action problems, it has been shown that an action-value based variant of VI, model-based Q-value iteration (QVI), finds an \u03b5-optimal estimate of the action-value function with high probability using only T = O\u0303(N/ ( (1 \u2212 \u03b3)4\u03b52 ) ) samples (Kearns & Singh, 1999; Kakade, 2004, chap. 9.1), where N and \u03b3 denote the size of state-action space and the discount factor, respectively.1 Although this bound matches the best existing upper bound on the sample complexity of estimating the action-value function (Azar et al., 2011a), it has not been clear, so far, whether this bound is a tight bound on the performance of QVI or it can be improved by a more careful analysis of QVI algorithm. This is mainly due to the fact that there is a gap of order 1/(1\u2212 \u03b3)2 between the upper bound of QVI and the state-of-the-art result for lower bound, which is of \u2126 ( N/ ( (1\u2212 \u03b3)2\u03b52 )) (Azar et al., 2011b).\nIn this paper, we focus on the problems which are formulated as finite state-action discounted infinitehorizon Markov decision processes (MDPs), and prove a new tight bound of O ( N log(N/\u03b4)/ ( (1\u2212 \u03b3)3\u03b52 )) on the sample complexity of the QVI algorithm. The new upper bound improves on the existing bound of QVI by an order of 1/(1\u2212\u03b3).2 We also present a new matching lower bound of \u0398 ( N log(N/\u03b4)/ ( (1\u2212\u03b3)3\u03b52 )) , which also improves on the best existing lower bound of RL by an order of 1/(1\u2212 \u03b3). The new results, which close\n1The notation g = O\u0303(f) implies that there are constants c1 and c2 such that g \u2264 c1f logc2(f).\n2In this paper, to keep the presentation succinct, we only consider the value iteration algorithm. However, one can prove upper-bounds of a same order for other modelbased methods such as policy iteration and linear programming using the results of this paper.\nthe above-mentioned gap between the lower bound and the upper bound of RL, guarantee that no learning method, given a generative model of the MDP, can be significantly more efficient than QVI in terms of the sample complexity of estimating the action-value function.\nThe main idea to improve the upper bound of QVI is to express the performance loss of QVI in terms of the variance of the sum of discounted rewards as opposed to the maximum Vmax = Rmax/(1\u2212 \u03b3) in the previous results. For this we make use of Bernstein\u2019s concentration inequality (Cesa-Bianchi & Lugosi, 2006, appendix, pg. 361), which bounds the estimation error in terms of the variance of the value function. We also rely on the fact that the variance of the sum of discounted rewards, like the expected value of the sum (value function), satisfies a Bellman-like equation, in which the variance of the value function plays the role of the instant reward (Munos & Moore, 1999; Sobel, 1982), to derive a sharp bound on the variance of the value function. In the case of lower bound, we improve on the result of Azar et al. (2011b) by adding some structure to the class of MDPs for which we prove the lower bound: In the new model, there is a high probability for transition from every intermediate state to itself. This adds to the difficulty of estimating the value function, since even a small estimation error may propagate throughout the recursive structure of the MDP and inflict a big performance loss especially for \u03b3\u2019s close to 1.\nThe rest of the paper is organized as follows. After introducing the notation used in the paper in Section 2, we describe the model-based Q-value iteration (QVI) algorithm in Subsection 2.1. We then state our main theoretical results, which are in the form of PAC sample complexity bounds in Section 3. Section 4 contains the detailed proofs of the results of Sections 3, i.e., sample complexity bound of QVI and a general new lower bound for RL. Finally, we conclude the paper and propose some directions for the future work in Section 5."}, {"heading": "2. Background", "text": "In this section, we review some standard concepts and definitions from the theory of Markov decision processes (MDPs). We then present the modelbased Q-value iteration algorithm of Kearns & Singh (1999). We consider the standard reinforcement learning (RL) framework (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998) in which a learning agent interacts with a stochastic environment and this interaction is modeled as a discrete-time discounted MDP. A dis-\ncounted MDP is a quintuple (X,A, P,R, \u03b3), where X and A are the set of states and actions, P is the state transition distribution, R is the reward function, and \u03b3 \u2208 (0, 1) is a discount factor. We denote by P (\u00b7|x, a) and r(x, a) the probability distribution over the next state and the immediate reward of taking action a at state x, respectively.\nRemark 1. To keep the representation succinct, in the sequel, we use the notation Z for the joint stateaction space X\u00d7A. We also make use of the shorthand notations z and \u03b2 for the state-action pair (x, a) and 1/(1\u2212 \u03b3), respectively. Assumption 1 (MDP Regularity). We assume Z and, subsequently, X and A are finite sets with cardinalities N , |X| and |A|, respectively. We also assume that the immediate reward r(x, a) is taken from the interval [0, 1].\nA mapping \u03c0 : X \u2192 A is called a stationary and deterministic Markovian policy, or just a policy in short. Following a policy \u03c0 in an MDP means that at each time step t the control action At \u2208 A is given by At = \u03c0(Xt), where Xt \u2208 X. The value and the action-value functions of a policy \u03c0, denoted respectively by V \u03c0 : X \u2192 R and Q\u03c0 : Z \u2192 R, are defined as the expected sum of discounted rewards that are encountered when the policy \u03c0 is executed. Given an MDP, the goal is to find a policy that attains the best possible values, V \u2217(x) , sup\u03c0 V\n\u03c0(x), \u2200x \u2208 X. Function V \u2217 is called the optimal value function. Similarly the optimal action-value function is defined as Q\u2217(x, a) = sup\u03c0 Q\n\u03c0(x, a). We say that a policy \u03c0\u2217 is optimal if it attains the optimal V \u2217(x) for all x \u2208 X. The policy \u03c0 defines the state transition kernel P\u03c0 as: P\u03c0(y|x) , P (y|x, \u03c0(x)) for all x \u2208 X. The right-linear operators P\u03c0\u00b7, P \u00b7 and P\u03c0\u00b7 are then defined as (P\u03c0Q)(z) , \u2211 y\u2208XP (y|z)Q(y, \u03c0(y)), (PV )(z) , \u2211\ny\u2208XP (y|z)V (y) for all z \u2208 Z and (P\u03c0V )(x) , \u2211 y\u2208X P\u03c0(y|x)V (y) for all x \u2208 X, respectively. Finally, \u2016 \u00b7 \u2016 shall denote the supremum (\u2113\u221e) norm which is defined as \u2016g\u2016 , maxy\u2208Y |g(y)|, where Y is a finite set and g : Y \u2192 R is a real-valued function.3"}, {"heading": "2.1. Model-based Q-value Iteration (QVI)", "text": "The algorithm makes n transition samples from each state-action pair z \u2208 Z for which it makes n calls to the generative model.4 It then builds an empirical model of the transition probabilities as: P\u0302 (y|z) , m(y, z)/n,\n3For ease of exposition, in the sequel, we remove the dependence on z and x , e.g., writing Q for Q(z) and V for V (x), when there is no possible confusion.\n4The total number of calls to the generative model is given by T = nN .\nwhere m(y, z) denotes the number of times that the state y \u2208 X has been reached from z \u2208 Z. The algorithm then makes an empirical estimate of the optimal action-value function Q\u2217 by iterating some actionvalue function Qk, with the initial value of Q0, through the empirical Bellman optimality operator T\u0302.5"}, {"heading": "3. Main Results", "text": "Our main results are in the form of PAC (probably approximately correct) bounds on the \u2113\u221e-norm of the difference of the optimal action-value function Q\u2217 and its sample estimate:\nTheorem 1 (PAC-bound for model-based Q-value iteration). Let Assumption 1 hold and T be a positive integer. Then, there exist some constants c and c0 such that for all \u03b5 \u2208 (0, 1) and \u03b4 \u2208 (0, 1) a total sampling budget of\nT = \u2308c\u03b2 3N\n\u03b52 log\nc0N\n\u03b4 \u2309,\nsuffices for the uniform approximation error \u2016Q\u2217 \u2212 Qk\u2016 \u2264 \u03b5, w.p. (with the probability) at least 1 \u2212 \u03b4, after only k = \u2308log(6\u03b2/\u03b5)/ log(1/\u03b3)\u2309 iteration of QVI algorithm.6 In particular, one may choose c = 68 and c0 = 12.\nThe following general result provides a tight lower bound on the number of transitions T for every RL algorithm to achieve an \u03b5-optimal estimate of the actionvalue function w.p. 1 \u2212 \u03b4, under the assumption that the algorithm is (\u03b5, \u03b4, T )-correct:\nDefinition 1 ((\u03b5, \u03b4, T )-correct algorithm). Let QAT be the estimate of Q\u2217 by an RL algorithm A after observing T \u2265 0 transition samples. We say that A is (\u03b5, \u03b4, T )-correct on the class of MDPs M if\u2225\u2225Q\u2217 \u2212QAT\n\u2225\u2225 \u2264 \u03b5 with probability at least 1 \u2212 \u03b4 for all M \u2208 M.7 Theorem 2 (Lower bound on the sample complexity of estimating the optimal action-value function). There exists some constants \u03b50, \u03b40, c1, c2, and a class of MDPs M, such that for all \u03b5 \u2208 (0, \u03b50), \u03b4 \u2208 (0, \u03b40/N), and every (\u03b5, \u03b4, T )-correct RL algorithm A on the class of MDPs M the number of transitions needs to be at least\nT = \u2308\u03b2 3N\nc1\u03b52 log\nN\nc2\u03b4 \u2309.\n5The operator T\u0302 is defined on the action-value function Q, for all z \u2208 Z, by T\u0302Q(z) = r(z)+\u03b3P\u0302V (z), where V (x) = maxa\u2208A(Q(x, a)) for all x \u2208 X.\n6For every real number u, \u2308u\u2309 is defined as the smallest integer number not less than u.\n7The algorithm A, unlike QVI, does not need to generate a same number of transition samples for every stateaction pair and can generate samples in an arbitrarily way."}, {"heading": "4. Analysis", "text": "In this section, we first provide the full proof of the finite-time PAC bound of QVI, reported in Theorem 1, in Subsection 4.1. We then prove Theorem 2, the RL lower bound, in Subsection 4.2. Also, we need to emphasize that we discard some of the proofs of the technical lemmas due to the lack of space. We provide those results in a long version of this paper."}, {"heading": "4.1. Poof of Theorem 1", "text": "We begin by introducing some new notation. Consider the stationary policy \u03c0. We define V\u03c0(z) , E[| \u2211 t\u22650\u03b3 tr(Zt) \u2212 Q\u03c0(z)|2] as the variance of the sum of discounted rewards starting from z \u2208 Z under the policy \u03c0. Also, define \u03c3\u03c0(z) , \u03b32 \u2211 y\u2208ZP \u03c0(y|z)|Q\u03c0(y) \u2212 P\u03c0Q\u03c0(z)|2 as the immediate variance at z \u2208 Z, i.e., \u03b32VY\u223cP\u03c0(\u00b7|z)[Q\u03c0(Y )]. Also, we shall denote v\u03c0 and v\u2217 as the immediate variance of the value function V \u03c0 and V \u2217 defined as v\u03c0(z) , \u03b32VY\u223cP (\u00b7|z)[V \u03c0(Y )] and v\u2217(z) , \u03b32VY\u223cP (\u00b7|z)[V \u2217(Y )], for all z \u2208 Z, respectively. Further, we denote the immediate variance of the action-value function Q\u0302\u03c0, V\u0302 \u03c0 and V\u0302 \u2217 by \u03c3\u0302\u03c0, v\u0302\u03c0 and v\u0302\u2217, respectively.\nWe now state our first result which indicates that Qk is very close to Q\u0302\u2217 up to an order of O(\u03b3k). Therefore, to prove bound on \u2016Q\u2217\u2212Qk\u2016, one only needs to bound \u2016Q\u2217 \u2212 Q\u0302\u2217\u2016 in high probability. Lemma 1. Let Assumption 1 hold and Q0(z) be in the interval [0, \u03b2] for all z \u2208 Z. Then we have\n\u2016Qk \u2212 Q\u0302\u2217\u2016 \u2264 \u03b3k\u03b2.\nIn the rest of this subsection, we focus on proving a high probability bound on \u2016Q\u2217 \u2212 Q\u0302\u2217\u2016. One can prove a crude bound of O\u0303(\u03b22/ \u221a n) on \u2016Q\u2217 \u2212 Q\u0302\u2217\u2016 by first proving that \u2016Q\u2217 \u2212 Q\u0302\u2217\u2016 \u2264 \u03b2\u2016(P \u2212 P\u0302 )V \u2217\u2016 and then using the Hoeffding\u2019s tail inequality (Cesa-Bianchi & Lugosi, 2006, appendix, pg. 359) to bound the random variable \u2016(P \u2212 P\u0302 )V \u2217\u2016 in high probability. Here, we follow a different and more subtle approach to bound \u2016Q\u2217 \u2212 Q\u0302\u2217\u2016, which leads to a tight bound of O\u0303(\u03b21.5/ \u221a n): (i) We prove in Lemma 2 component-wise upper and lower bounds on the error Q\u2217 \u2212 Q\u0302\u2217 which are expressed in terms of (I\u2212\u03b3P\u0302\u03c0\u2217)\u22121 [ P \u2212 P\u0302 ] V \u2217 and (I\u2212\u03b3P\u0302 \u03c0\u0302\u2217)\u22121 [ P \u2212 P\u0302 ] V \u2217, respectively. (ii) We make use of the sharp result of Bernstein\u2019s inequality to bound [ P \u2212 P\u0302 ] V \u2217 in terms of the squared root of the variance of V \u2217 in high probability. (iii) We prove the key result of this subsection (Lemma 5) which shows that the variance of the sum of discounted rewards satisfies a Bellman-like recursion, in which the instant reward r(z) is replaced by\n\u03c3\u03c0(z). Based on this result we prove an upper-bound of order O(\u03b21.5) on (I \u2212 \u03b3P\u03c0)\u22121 \u221a V(Q\u03c0) for any policy \u03c0, which combined with the previous steps leads to the sharp upper bound of O\u0303(\u03b21.5/ \u221a n) on \u2016Q\u2217 \u2212 Q\u0302\u2217\u2016.\nWe proceed by the following lemma which boundsQ\u2217\u2212 Q\u0302\u2217 from above and below:\nLemma 2 (Component-wise bounds on Q\u2217 \u2212 Q\u0302\u2217 ).\nQ\u2217 \u2212 Q\u0302\u2217 \u2264 \u03b3(I \u2212 \u03b3P\u0302\u03c0\u2217)\u22121 [ P \u2212 P\u0302 ] V \u2217, (1) Q\u2217 \u2212 Q\u0302\u2217 \u2265 \u03b3(I \u2212 \u03b3P\u0302 \u03c0\u0302\u2217)\u22121 [ P \u2212 P\u0302 ] V \u2217. (2)\nWe now concentrate on bounding the RHS (right hand sides) of (1) and (2). We first state the following technical result which relates v\u2217 to \u03c3\u0302\u03c0 \u2217 and \u03c3\u0302\u2217.\nLemma 3. Let Assumption 1 hold and 0 < \u03b4 < 1. Then, w.p. at least 1\u2212 \u03b4:\nv\u2217 \u2264 \u03c3\u0302\u03c0\u2217 + bv1, (3) v\u2217 \u2264 \u03c3\u0302\u2217 + bv1, (4)\nwhere bv is defined as\nbv ,\n\u221a 18\u03b34\u03b24 log 3N\u03b4\nn + 4\u03b32\u03b24 log 3N\u03b4 n ,\nand 1 is a function which assigns 1 to all z \u2208 Z.\nBased on Lemma 3 we prove the following sharp bound on \u03b3(P \u2212 P\u0302 )V \u2217, for which we also rely on Bernstein\u2019s inequality (Cesa-Bianchi & Lugosi, 2006, appendix, pg. 361).\nLemma 4. Let Assumption 1 hold and 0 < \u03b4 < 1. Define cpv , 2 log(2N/\u03b4) and bpv as:\nbpv ,\n( 6(\u03b3\u03b2)4/3 log 6N\u03b4\nn\n)3/4 +\n5\u03b3\u03b22 log 6N\u03b4 n .\nThen w.p. 1\u2212 \u03b4 we have\n\u03b3(P \u2212 P\u0302 )V \u2217 \u2264 \u221a cpv\u03c3\u0302\u03c0 \u2217\nn + bpv1, (5)\n\u03b3(P \u2212 P\u0302 )V \u2217 \u2265 \u2212 \u221a cpv\u03c3\u0302\u2217\nn \u2212 bpv1. (6)\nProof. For all z \u2208 Z and all 0 < \u03b4 < 1, Bernstein\u2019s inequality implies that w.p. at least 1\u2212 \u03b4:\n(P \u2212 P\u0302 )V \u2217(z) \u2264\n\u221a 2v\u2217(z) log 1\u03b4\n\u03b32n + 2\u03b2 log 1\u03b4 3n ,\n(P \u2212 P\u0302 )V \u2217(z) \u2265 \u2212\n\u221a 2v\u2217(z) log 1\u03b4\n\u03b32n \u2212 2\u03b2 log\n1 \u03b4\n3n .\nWe deduce (using a union bound):\n\u03b3(P \u2212 P\u0302 )V \u2217 \u2264 \u221a c\u2032pv v\u2217\nn + b\u2032pv1, (7)\n\u03b3(P \u2212 P\u0302 )V \u2217 \u2265 \u2212 \u221a c\u2032pv v\u2217\nn \u2212 b\u2032pv1, (8)\nwhere c\u2032pv , 2 log(N/\u03b4) and b \u2032 pv , 2\u03b3\u03b2 log(N/\u03b4)/3n. The result then follows by plugging (3) and (4) into (7) and (8), respectively, and then taking a union bound.\nWe now state the key lemma of this section which shows that for any policy \u03c0 the variance V\u03c0 satisfies the following Bellman-like recursion. Later, we use this result, in Lemma 6, to bound (I \u2212 \u03b3P\u03c0)\u22121\u03c3\u03c0. Lemma 5. V\u03c0 satisfies the Bellman equation\nV \u03c0 = \u03c3\u03c0 + \u03b32P\u03c0V\u03c0.\nProof. For all z \u2208 Z we have\nV \u03c0(z) = E [\u2223\u2223\u2223\u2223 \u2211\nt\u22650\n\u03b3tr(Zt)\u2212Q\u03c0(z) \u2223\u2223\u2223\u2223 2]\n= EZ1\u223cP\u03c0(.|z)E [\u2223\u2223\u2223\u2223 \u2211\nt\u22651\n\u03b3tr(Zt)\u2212 \u03b3Q\u03c0(Z1)\n\u2212 (Q\u03c0(z)\u2212 r(z)\u2212 \u03b3Q\u03c0(Z1)) \u2223\u2223\u2223\u2223 2]\n= \u03b32EZ1\u223cP\u03c0(.|z)E [\u2223\u2223\u2223\u2223 \u2211\nt\u22651\n\u03b3t\u22121r(Zt)\u2212Q\u03c0(Z1) \u2223\u2223\u2223\u2223 2]\n\u2212 2EZ1\u223cP\u03c0(.|z) [ (Q\u03c0(z)\u2212 r(z)\u2212 \u03b3Q\u03c0(Z1)) \u00d7 E (\u2211\nt\u22651\n\u03b3tr(Zt)\u2212 \u03b3Q\u03c0(Z1) \u2223\u2223\u2223\u2223Z1 )]\n+ EZ1\u223cP\u03c0(\u00b7|z)(|Q\u03c0(z)\u2212 r(z)\u2212 \u03b3Q\u03c0(Z1)|2)\n= \u03b32EZ1\u223cP\u03c0(.|z)E [\u2223\u2223\u2223\u2223 \u2211\nt\u22651\n\u03b3t\u22121r(Zt)\u2212Q\u03c0(Z1) \u2223\u2223\u2223\u2223 2]\n+ \u03b32VZ1\u223cP\u03c0(\u00b7|z)(Q \u03c0(Z1))\n= \u03b32 \u2211\ny\u2208Z\nP\u03c0(y|z)V\u03c0(y) + \u03c3\u03c0(z),\nin which we rely on E( \u2211\nt\u22651 \u03b3 tr(Zt)\u2212 \u03b3Q\u03c0(Z1)|Z1) =\n0.\nBased on Lemma 5, one can prove the following result on the immediate variance.\nLemma 6.\n\u2016(I \u2212 \u03b32P\u03c0)\u22121\u03c3\u03c0\u2016 \u2264 \u03b22, (9) \u2016(I \u2212 \u03b3P\u03c0)\u22121 \u221a \u03c3\u03c0\u2016 \u2264 2 log(2)\u03b21.5. (10)\nProof. The first inequality follows from Lemma 5 by solving (5) in terms of V\u03c0 and taking the sup-norm over both sides of the resulted equation. In the case of Eq.(10) we have 8\n\u2016(I \u2212 \u03b3P\u03c0)\u22121 \u221a \u03c3\u03c0\u2016 = \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nk\u22650\n(\u03b3P\u03c0)k \u221a \u03c3\u03c0 \u2225\u2225\u2225\u2225\u2225\u2225\n= \u2225\u2225\u2225\u2225\u2225\u2225 \u2211\nl\u22650\n(\u03b3P\u03c0)tl t\u22121\u2211\nj=0\n(\u03b3P\u03c0)j \u221a \u03c3\u03c0 \u2225\u2225\u2225\u2225\u2225\u2225\n\u2264 \u2211\nl\u22650\n(\u03b3t)l \u2225\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211\nj=0\n(\u03b3P\u03c0)j \u221a \u03c3\u03c0 \u2225\u2225\u2225\u2225\u2225\u2225\n= 1\n1\u2212 \u03b3t \u2225\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211\nj=0\n(\u03b3P\u03c0)j \u221a \u03c3\u03c0 \u2225\u2225\u2225\u2225\u2225\u2225 ,\n(11) in which we write k = tl + j with t is a positive integer. We now prove a bound on \u2225\u2225\u2211 t\u22121 j=0(\u03b3P \u03c0)j \u221a \u03c3\u03c0 \u2225\u2225 by making use of Jensen\u2019s inequality as well as CauchySchwarz inequality:\n\u2225\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211\nj=0\n(\u03b3P\u03c0)j \u221a \u03c3\u03c0 \u2225\u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211\nj=0\n\u03b3j \u221a (P\u03c0)j\u03c3\u03c0 \u2225\u2225\u2225\u2225\u2225\u2225\n\u2264 \u221a t \u2225\u2225\u2225\u2225\u2225\u2225 \u221a\u221a\u221a\u221a t\u22121\u2211\nj=0\n(\u03b32P\u03c0)j\u03c3\u03c0 \u2225\u2225\u2225\u2225\u2225\u2225\n\u2264 \u221a t \u2225\u2225\u2225 \u221a (I \u2212 \u03b32P\u03c0)\u22121\u03c3\u03c0 \u2225\u2225\u2225 \u2264 \u03b2 \u221a t,\n(12)\nwhere in the last step we rely on (9). The result then follows by plugging (12) into (11) and optimizing the bound in terms of t to achieve the best dependency on \u03b2.\nNow we make use of Lemma 6 and Lemma 4 to bound \u2016Q\u2217 \u2212 Q\u0302\u2217\u2016 in high probability. Lemma 7. Let Assumption 1 hold. Then, for any 0 < \u03b4 < 1: \u2016Q\u2217 \u2212 Q\u0302\u2217\u2016 \u2264 \u03b5\u2032, w.p. 1\u2212 \u03b4, where \u03b5\u2032 is defined as:\n\u03b5\u2032 ,\n\u221a 17\u03b23 log 4N\u03b4\nn +\n( 6(\u03b3\u03b22)4/3 log 12N\u03b4\nn\n)3/4\n+ 5\u03b3\u03b23 log 12N\u03b4\nn .\n(13)\n8 For any real-valued function f , \u221a f is defined as a\ncomponent wise squared-root operator on f .\nProof. By incorporating the result of Lemma 4 and Lemma 6 into Lemma 2, we deduce that:\nQ\u2217 \u2212 Q\u0302\u2217 \u2264 b1, Q\u2217 \u2212 Q\u0302\u2217 \u2265 \u2212b1,\nw.p. 1\u2212 \u03b4. The scalar b is given by:\nb ,\n\u221a 17\u03b23 log 2N\u03b4\nn +\n( 6(\u03b3\u03b22)4/3 log 6N\u03b4\nn\n)3/4\n+ 5\u03b3\u03b23 log 6N\u03b4\nn .\nThe result then follows by combining these two bounds and taking the \u2113\u221e norm.\nProof of Theorem 1. We combine the proof of Lemma 7 and Lemma 1 in order to bound Q\u2217 \u2212 Qk in high probability. We then solve the resulted bound w.r.t. n and k.9"}, {"heading": "4.2. Proof of the Lower-bound", "text": "In this section, we provide the proof of Theorem 2. In our analysis, we rely on the likelihood-ratio method, which has been previously used to prove a lower bound for multi-armed bandits (Mannor & Tsitsiklis, 2004), and extend this approach to RL and MDPs. We begin by defining a class of MDPs for which the proposed lower bound will be obtained (see Figure 1). We define the class of MDPs M as the set of all MDPs with the state-action space of cardinality N = 3KL, where K and L are positive integers. Also, we assume that for all M \u2208 M, the state space X consists of three smaller sets S, Y1 and Y2. The set S includes K states, each of those states corresponds with the set of actions A = {a1, a2, . . . , aL}, whereas the states in Y1 and Y2 are single-action states. By taking the action a \u2208 A from every state x \u2208 S, we move to the next state y(z) \u2208 Y1 with the probability 1, where z = (x, a). The transition probability from Y1 is characterized by the transition probability pM from every y(z) \u2208 Y1 to itself and with the probability 1 \u2212 pM to the corresponding y(z) \u2208 Y2.10 Further, for all M \u2208 M, Y2 consists of only absorbing states, i.e., for all y \u2208 Y2, P (x|x) = 1. The instant reward r is set to 1 for every state in Y1 and 0 elsewhere. For this class of MDPs, the optimal action-value function Q\u2217 can be solved in close form from the Bellman equation:\nQ\u2217(z) = \u03b3V \u2217(y(z)) = \u03b3\n1\u2212 \u03b3pM , \u2200z \u2208 S\u00d7A,\n9Note that the total number of samples is then computed by T = Nn.\n10Every state y \u2208 Y2 is only connected to one state in Y1 and S, i.e., there is no overlapping path in the MDP.\nIn the rest of the proof, we concentrate on proving the lower bound for \u2016Q\u2217 \u2212 QAT \u2016 for all z \u2208 S\u00d7A. Now, let us consider a set of two MDPs M\u2217 = {M0,M1} in M with the transition probabilities\npM =\n{ p M = M0,\np+ \u03b1 M = M1,\nwhere \u03b1 and p are some positive numbers such that 0 < p < p + \u03b1 \u2264 1, which will be quantified later in this section. We assume that the discount factor \u03b3 is bounded from below by some positive constant \u03b30 for the class of MDP M\u2217. We denote by Em ad Pm the expectation and the probability under the model Mm in the rest of this section.\nWe follow the following steps in the proof: (i) we prove a lower bound on the sample-complexity of learning the value function for every state y \u2208 Y on the class of MDP M\u2217 (ii) we then make use of the fact that the estimates of Q\u2217(z) for different z \u2208 S\u00d7A are independent of each others to combine these bounds and prove the tight result of Theorem 2.\nWe now introduce some new notation: Let QAt (z) be an empirical estimate of the action-value function Q\u2217(z) by the RL algorithm A using t > 0 transition\nsamples from the state y(z) \u2208 Y1 for z \u2208 X\u00d7A. We define the event E1(z) , {|Q\u22170(z) \u2212 QAt (z)| \u2264 \u03b5} for all z \u2208 S\u00d7A, where Q\u22170 , \u03b3/(1 \u2212 \u03b3p) is the optimal action-value function for all z \u2208 S\u00d7A under the MDP M0. We then define k , r1 + r2 + \u00b7 \u00b7 \u00b7+ rt as the sum of rewards of making t transitions from y(z) \u2208 Y1. We also introduce the event E2(z), for all z \u2208 S\u00d7A as:\nE2(z) ,\n{ pt\u2212 k \u2264 \u221a 2p(1\u2212 p)t log c \u2032 2\n2\u03b8\n} .\nFurther, we define E(z) , E1(z)\u2229E2(z). We then state the following lemmas required for our analysis.\nWe begin our analysis of the lower bound by the following lemma: Lemma 8. Define \u03b8 , exp ( \u2212c\u20321\u03b12t/(p(1\u2212p)) ) . Then, for every RL algorithm A, there exists an MDP Mm \u2208 M\n\u2217 and constants c\u20321 > 0 and c \u2032 2 > 0 such that\nPm(|Q\u2217(z)\u2212QAt (z)|) > \u03b5) > \u03b8\nc\u20322 , (14)\nby the choice of \u03b1 = 2(1\u2212 \u03b3p)2\u03b5/(\u03b32).\nProof. To prove this result we make use of a contradiction argument, i.e., we assume that there exists an algorithm A for which:\nPm((|Q\u2217(z)\u2212QAt (z)|) > \u03b5) \u2264 \u03b8\nc\u20322 , or\nPm((|Q\u2217(z)\u2212QAt (z)|) \u2264 \u03b5) \u2265 1\u2212 \u03b8\nc\u20322 ,\n(15)\nfor all Mm \u2208 M\u2217 and show that this assumption leads to a contradiction. We now state the following technical lemmas required for the proof:\nLemma 9. For all p > 12 :\nP0(E2(z)) > 1\u2212 2\u03b8\nc\u20322 .\nNow, by the assumption that Pm(|Q\u2217(z) \u2212QAt (z)|) > \u03b5) \u2264 \u03b8/c\u20322 for all Mm \u2208 M\u2217, we have P0(E1(z)) \u2265 1\u2212\u03b8/c\u20322 \u2265 1\u22121/c\u20322. This combined with Lemma 9 and with the choice of c\u20322 = 6 implies that P0(E(z)) > 1/2, for all z \u2208 S\u00d7A. Lemma 10. Let \u03b5 \u2264 1\u2212p4\u03b32(1\u2212\u03b3p)2 . Then, for all z \u2208 S\u00d7A: P1(E1(z)) > \u03b8/c\u20322.\nNow by the choice of \u03b1 = 2(1 \u2212 \u03b3p)2\u03b5/(\u03b32), we have that Q\u22171(z) \u2212 Q\u22170(z) = \u03b31\u2212\u03b3(p+\u03b1) \u2212 \u03b3 1\u2212\u03b3p > 2\u03b5, thus Q\u22170(z) + \u03b5 < Q \u2217 1(z) \u2212 \u03b5. In words, the random event\n{|Q\u22170(z)\u2212QAt (z)| \u2264 \u03b5} does not overlap with the event {|Q\u22171(z)\u2212QAt (z)| \u2264 \u03b5}. Now let us return to the assumption of Eq. (15), which states that for all Mm \u2208 M\u2217, Pm(|Q\u2217(z) \u2212 QAt (z)|) \u2264 \u03b5) \u2265 1\u2212\u03b8/c\u20322 under Algorithm A. Based on Lemma 10 we have P1(|Q\u22170(z) \u2212 QAt (z)| \u2264 \u03b5) > \u03b8/c\u20322. This combined with the fact that {|Q\u22170(y) \u2212 QAt (z)|} and {|Q\u22171(z) \u2212 QAt (z)|} do not overlap implies that P1(|Q\u2217(z)\u2212QAt (z)|) \u2264 \u03b5) \u2264 1\u2212\u03b8/c\u20322, which violates the assumption of Eq. (15). The contradiction between the result of Lemma 10 and the assumption which leads to this result proves the lower bound of Eq. (14).\nNow by the choice of p = 4\u03b3\u221213\u03b3 and c1 = 8100, we have that for every \u03b5 \u2208 (0, 3] and for all 0.4 = \u03b30 \u2264 \u03b3 < 1 there exists an MDP Mm \u2208 M\u2217 such that\nPm(|Q\u2217(z)\u2212QATz (z)|) > \u03b5) > 1\nc\u20322 e\n\u2212c1Tz\u03b5 2\n6\u03b23 ,\nThis result implies that for any state-action z \u2208 S\u00d7A, the probability of making an estimation error of \u03b5 is at least \u03b4 onM0 orM1 whenever the number of transition samples Tz from z \u2208 Z is less that \u03be(\u03b5, \u03b4) , 6\u03b2 3\nc1\u03b52 log 1c\u2032 2 \u03b4 .\nWe now extend this result to the whole state-action space S\u00d7A. Lemma 11. Assume that for every algorithm A, for every state-action z \u2208 S\u00d7A we have11\nPm(|Q\u2217(z)\u2212QATz (z)| > \u03b5|Tz = tz) > \u03b4, (16) Then for any \u03b4\u2032 \u2208 (0, 1/2), for any algorithm A using a total number of transition samples less than T = N 6 \u03be ( \u03b5, 12\u03b4 \u2032 N ) , there exists an MDP Mm \u2208 M\u2217 such that\nPm ( \u2016Q\u2217 \u2212QAT \u2016 > \u03b5 ) > \u03b4\u2032, (17)\nwhere QAT denotes the empirical estimate of the optimal action-value function Q\u2217 by A using T transition samples.\nProof. First note that if the total number of observed transitions is less than KL/2\u03be(\u03b5, \u03b4) = (N/6)\u03be(\u03b5, \u03b4), then there exists at least KL/2 = N/6 state-action pairs that are sampled at most \u03be(\u03b5, \u03b4) times. Indeed, if this was not the case, then the total number of transitions would be strictly larger than N/6\u03be(\u03b5, \u03b4), which implies a contradiction). Now let us denote those states as z(1), . . . , z(N/6).\nWe consider the specific class of MDPs described in Figure 1. In order to prove that (17) holds for any algorithm, it is sufficient to prove it for the class of algorithms that return an estimate QATz (z) for each stateaction z based on the transition samples observed from\n11Note that we allow Tz to be random.\nz only (indeed, since the samples from z and z\u2032 are independent, the samples collected from z\u2032 do not bring more information about Q\u2217(z) than the information brought by the samples collected from z). Thus, by defining Q(z) , {|Q\u2217(z)\u2212QATz (z)| > \u03b5}, we have that for such algorithms, the events Q(z) and Q(z\u2032) are conditionally independent given Tz and Tz\u2032 . Thus, there exists an MDP Mm \u2208 M\u2217 such that:\nPm ( {Q(z(i))c}1\u2264i\u2264N\n6 \u2229 {Tz(i) \u2264 \u03be(\u03b5, \u03b4)}1\u2264i\u2264N 6\n)\n=\n\u03be(\u03b5,\u03b4)\u2211\nt1=0\n\u00b7 \u00b7 \u00b7 \u03be(\u03b5,\u03b4)\u2211\ntN/6=0\nPm ( {Tz(i) = ti}1\u2264i\u2264N\n6\n)\nPm ( {Q(z(i))c}1\u2264i\u2264N/6 \u2229 {Tz(i) = ti}1\u2264i\u2264N\n6\n)\n=\n\u03be(\u03b5,\u03b4)\u2211\nt1=0\n\u00b7 \u00b7 \u00b7 \u03be(\u03b5,\u03b4)\u2211\ntN/6=0\nPm ( {Tz(i) = ti}1\u2264i\u2264N\n6\n)\n\u220f\n1\u2264i\u2264N/6\nPm ( Q(z(i)) c \u2229 Tz(i) = ti )\n\u2264 \u03be(\u03b5,\u03b4)\u2211\nt1=0\n\u00b7 \u00b7 \u00b7 \u03be(\u03b5,\u03b4)\u2211\ntN/6=0\nPm ( {Tz(i) = ti}1\u2264i\u2264N\n6\n) (1\u2212 \u03b4)N6 ,\nfrom Eq. (16), thus\nPm ( {Q(z(i))c}1\u2264i\u2264N/6 \u2223\u2223{Tz(i) \u2264 \u03be(\u03b5, \u03b4)}1\u2264i\u2264N/6 )\n\u2264(1\u2212 \u03b4)N/6.\nWe finally deduce that if the total number of transition samples is less than N6 \u03be(\u03b5, \u03b4), then\nPm(\u2016Q\u2217 \u2212QAT \u2016 > \u03b5 ) \u2265 Pm ( \u22c3\nz\u2208S\u00d7A\nQ(z) )\n\u2265 1\u2212 Pm ( {Q(z(i))c}1\u2264i\u2264N\n6 \u2223\u2223{Tz(i) \u2264 \u03be(\u03b5, \u03b4)}1\u2264i\u2264N 6\n)\n\u2265 1\u2212 (1\u2212 \u03b4)N6 \u2265 \u03b4N 12 ,\nwhenever \u03b4N6 \u2264 1. Setting \u03b4\u2032 = \u03b4N12 , we obtain the desired result.\nLemma 11 implies that if the total number of samples T is less than \u03b23N/(c1\u03b5\n2) log(N/(c2\u03b4)), with the choice of c1 = 8100 and c2 = 72, then the probability of \u2016Q\u2217\u2212QAT \u2016 \u2264 \u03b5 is at maximum 1\u2212\u03b4 on either M0 or M1. This is equivalent to the statement that for every RL algorithm A to be (\u03b5, \u03b4, T )-correct on the set M\u2217, and subsequently on the class of MDPs M, the total number of transitions T needs to satisfy the inequality T > \u03b23N/(c1\u03b5\n2) log(N/(c2\u03b4)), which concludes the proof of Theorem 2."}, {"heading": "5. Conclusion and Future Works", "text": "In this paper, we have presented the first minimax bound on the sample complexity of estimating the optimal action-value function in discounted reward MDPs. We have proven that the model-based Q-value iteration algorithm (QVI) is an optimal learning algorithm since it minimizes the dependencies on 1/\u03b5, N , \u03b4 and \u03b2. Also, our results have significantly improved on the state-of-the-art in terms of dependency on \u03b2. Overall, we conclude that QVI is an efficient RL algorithm which completely closes the gap between the lower and upper bound of the sample complexity of RL in the presence of a generative model of the MDP.\nIn this work, we are only interested in the estimation of the optimal action-value function and not the problem of exploration. Therefore, we did not compare our results with the-state-of-the-art of PAC-MDP (Strehl et al., 2009; Szita & Szepesva\u0301ri, 2010) and upper-confidence bound based algorithms (Bartlett & Tewari, 2009; Jaksch et al., 2010), in which the choice of the exploration policy has an influence on the behavior of the learning algorithm. However, we believe that it would be possible to improve on the state-of-the-art in PAC-MDP, based on the results of this paper. This is mainly due to the fact that most PAC-MDP algorithms rely on an extended variant of model-based Q-value iteration to estimate the action-value function, but they use the naive result of Hoeffding\u2019s inequality for concentration of measure which leads to non-tight sample complexity results. One can improve on those results, in terms of dependency on \u03b2, using the improved analysis of this paper which makes use of the sharp result of Bernstein\u2019s inequality as opposed to the Hoeffding\u2019s inequality in the previous works. Also, we believe that the existing lower bound on the sample complexity of exploration of any reinforcement learning algorithm (Strehl et al., 2009) can be significantly improved in terms of dependency on \u03b2 using the new \u201chard\u201d class of MDPs presented in this paper."}, {"heading": "Acknowledgments", "text": "The authors appreciate supports from the European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement no 231495. We also thank Bruno Scherrer for useful discussion and the anonymous reviewers for their valuable comments."}], "references": [{"title": "Reinforcement learning with a near optimal rate of convergence", "author": ["Azar", "M. Gheshlaghi", "R. Munos", "M. Ghavamzadeh", "H.J. Kappen"], "venue": "Technical Report inria00636615,", "citeRegEx": "Azar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2011}, {"title": "REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs", "author": ["P.L. Bartlett", "A. Tewari"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Bartlett and Tewari,? \\Q2009\\E", "shortCiteRegEx": "Bartlett and Tewari", "year": 2009}, {"title": "Reinforcement Learning and Dynamic Programming Using Function Approximators", "author": ["Bu\u015f", "L. oniu", "Babu\u0161", "R. ka", "B. De Schutter", "D. Ernst"], "venue": null, "citeRegEx": "Bu\u015f et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bu\u015f et al\\.", "year": 2010}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["S.M. Kakade"], "venue": "PhD thesis, Gatsby Computational Neuroscience Unit,", "citeRegEx": "Kakade,? \\Q2004\\E", "shortCiteRegEx": "Kakade", "year": 2004}, {"title": "Finite-sample convergence rates for Q-learning and indirect algorithms", "author": ["M. Kearns", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kearns and Singh,? \\Q1999\\E", "shortCiteRegEx": "Kearns and Singh", "year": 1999}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mannor and Tsitsiklis,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis", "year": 2004}, {"title": "Influence and variance of a Markov chain : Application to adaptive discretizations in optimal control", "author": ["R. Munos", "A. Moore"], "venue": "In Proceedings of the 38th IEEE Conference on Decision and Control,", "citeRegEx": "Munos and Moore,? \\Q1999\\E", "shortCiteRegEx": "Munos and Moore", "year": 1999}, {"title": "The variance of discounted markov decision processes", "author": ["M.J. Sobel"], "venue": "Journal of Applied Probability,", "citeRegEx": "Sobel,? \\Q1982\\E", "shortCiteRegEx": "Sobel", "year": 1982}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A.L. Strehl", "L. Li", "M.L. Littman"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["Szepesv\u00e1ri", "Cs"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "Szepesv\u00e1ri and Cs.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Cs.", "year": 2010}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["I. Szita", "Szepesv\u00e1ri", "Cs"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Szita et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Szita et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 9, "context": "We also rely on the fact that the variance of the sum of discounted rewards, like the expected value of the sum (value function), satisfies a Bellman-like equation, in which the variance of the value function plays the role of the instant reward (Munos & Moore, 1999; Sobel, 1982), to derive a sharp bound on the variance of the value function.", "startOffset": 246, "endOffset": 280}, {"referenceID": 0, "context": "In the case of lower bound, we improve on the result of Azar et al. (2011b) by adding some structure to the class of MDPs for which we prove the lower bound: In the new model, there is a high probability for transition from every intermediate state to itself.", "startOffset": 56, "endOffset": 76}, {"referenceID": 10, "context": "Therefore, we did not compare our results with the-state-of-the-art of PAC-MDP (Strehl et al., 2009; Szita & Szepesv\u00e1ri, 2010) and upper-confidence bound based algorithms (Bartlett & Tewari, 2009; Jaksch et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 4, "context": ", 2009; Szita & Szepesv\u00e1ri, 2010) and upper-confidence bound based algorithms (Bartlett & Tewari, 2009; Jaksch et al., 2010), in which the choice of the exploration policy has an influence on the behavior of the learning algorithm.", "startOffset": 78, "endOffset": 124}, {"referenceID": 10, "context": "Also, we believe that the existing lower bound on the sample complexity of exploration of any reinforcement learning algorithm (Strehl et al., 2009) can be significantly improved in terms of dependency on \u03b2 using the new \u201chard\u201d class of MDPs presented in this paper.", "startOffset": 127, "endOffset": 148}], "year": 2012, "abstractText": "We consider the problem of learning the optimal action-value function in the discountedreward Markov decision processes (MDPs). We prove a new PAC bound on the samplecomplexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor \u03b3 \u2208 [0, 1) only O ( N log(N/\u03b4)/ ( (1 \u2212 \u03b3)\u03b5 )) samples are required to find an \u03b5-optimal estimation of the action-value function with the probability 1 \u2212 \u03b4. We also prove a matching lower bound of \u0398 ( N log(N/\u03b4)/ ( (1 \u2212 \u03b3)\u03b5 )) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action) value function in which the upper bound matches the lower bound of RL in terms ofN , \u03b5, \u03b4 and 1/(1\u2212\u03b3). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1\u2212 \u03b3).", "creator": "LaTeX with hyperref package"}}}