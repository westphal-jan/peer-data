{"id": "1206.6480", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Dantzig Selector Approach to Temporal Difference Learning", "abstract": "LSTD is a popular algorithm for value function approximation. Whenever the number of features is larger than the number of samples, it must be paired with some form of regularization. In particular, L1-regularization methods tend to perform feature selection by promoting sparsity, and thus, are well-suited for high-dimensional problems. However, since LSTD is not a simple regression algorithm, but it solves a fixed--point problem, its integration with L1-regularization is not straightforward and might come with some drawbacks (e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector. We investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches, and show how it addresses some of their drawbacks.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (549kb)", "http://arxiv.org/abs/1206.6480v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["matthieu geist", "bruno scherrer", "alessandro lazaric", "mohammad ghavamzadeh"], "accepted": true, "id": "1206.6480"}, "pdf": {"name": "1206.6480.pdf", "metadata": {"source": "META", "title": "A Dantzig Selector Approach to Temporal Difference Learning", "authors": ["Matthieu Geist", "Bruno Scherrer", "Alessandro Lazaric", "Mohammad Ghavamzadeh"], "emails": ["matthieu.geist@supelec.fr", "bruno.scherrer@inria.fr", "firstname.lastname@inria.fr"], "sections": [{"heading": "1. Introduction", "text": "An important problem in reinforcement learning (RL) (Sutton & Barto, 1998) is to estimate the quality of a given policy through the computation of its value function (e.g., in the policy evaluation step of a policy iteration). Oftentimes, the state space is to large, and thus, approximation schemes must be used to represent the value function. Furthermore, whenever the model (reward function and probability transitions) is unknown, the best approximation should be computed using a set of sampled transitions. Many algorithms\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nhave been designed to solve this approximation problem. Among them, LSTD (Least-Squares Temporal Differences) (Bradtke & Barto, 1996) is the most popular. Using a linear parametric representation, LSTD computes the fixed-point of the Bellman operator composed with the orthogonal projection.\nIn many practical scenarios, the number of features of the linear approximation is much larger than the number of available samples. For example, one may want to consider a very rich function space, such that the actual value function lies in it. Unfortunately, in this case, learning is prone to overfitting. A standard approach to face this problem is to introduce some form of regularization. While LSTD has been often paired with `2-regularization, only recently `1-regularization (see Sec. 2.2 for a thorough review of the main `1- regularized algorithms) has been considered to deal with high\u2013dimensional problems. This approach is particularly appealing since `1-regularization implicitly performs feature selection and targets sparse solutions. In particular, LASSO-TD (Kolter & Ng, 2009) can be seen as en extension of LASSO (Tibshirani, 1996) to temporal difference learning, to which it reduces when setting the discount factor to zero. However, LASSO-TD is not derived from a proper convex optimization problem, and thus, it requires some assumptions that might not hold in an off-policy setting. Although other algorithms have been proposed to overcome these drawbacks (e.g., `1-PBR by Geist & Scherrer (2011)), other disadvantages may appear.\nThis paper introduces a new algorithm, Dantzig-LSTD (D-LSTD for short, see Sec. 3), which extends the Dantzig Selector (DS) (Candes & Tao, 2007) to temporal difference learning. Instead of solving a fixed-point problem as in LASSO-TD, it can simply be cast as a linear program, thus allowing to use any off-the-shelf solver. Furthermore, since the underlying optimiza-\ntion problem is convex, it can handle off-policy learning in a principled way. Yet, when LASSO-TD is well defined, both algorithms provide similar solutions (see Prop. 2), as DS does w.r.t. LASSO. We show that for some oracle choice of the regularization factor, the DLSTD solution converges quickly to the LSTD solution (at a rate depending only logarithmically on the number of features), as shown in Theorem 1. This new algorithm also opens some issues, namely how well is the true value function estimated and how to efficiently choose the regularization factor. These points are discussed in Sec. 4. Finally, we report some illustrative empirical results in Sec. 5."}, {"heading": "2. LSTD and Related Work", "text": "A Markov reward process1 (MRP) is a tuple {S, P,R, \u03b3}, where S is a finite state space, P = (p(s\u2032|s))1\u2264s,s\u2032\u2264|S| is the transition matrix, R = (r(s))1\u2264s\u2264|S| with \u2016R\u2016\u221e \u2264 rmax is the reward vector, and \u03b3 is a discount factor. The value function V is defined as the expected cumulative reward from a given state s, V (s) = E[ \u2211\u221e t=0 \u03b3\ntrt|s0 = s]. It is the unique fixed-point of the Bellman operator T : V \u2192 R+\u03b3PV .\nIn many practical applications, the model of the MRP (i.e., the reward R and transitions P ) is unknown and only a set of n transitions {(si, ri, s\u2032i)1\u2264i\u2264n} is available. In general, we assume that states s1, . . . , sn are sampled from a sampling distribution \u00b5 (not necessarily the stationary distribution of the MRP) and the next states s\u20321, . . . , s \u2032 n are generated according to the transition probabilities p(\u00b7|si). Whenever the state space is too large, the value function cannot be computed exactly at each state and a function approximation scheme is needed. We consider value functions V\u0302\u03b8 defined as a linear combination of p basis functions \u03c6i(s), that is V\u0302\u03b8(s) = \u2211p i=1 \u03b8i\u03c6i(s) = \u03b8\n>\u03c6(s). We denote by \u03a6 \u2208 R|S|\u00d7p the feature matrix whose rows contain the feature vectors \u03c6(s)> for any s \u2208 S. This defines a hypothesis space H = {\u03a6\u03b8|\u03b8 \u2208 Rp}, which contains all the value functions that can be represented by the features \u03c6. The objective is to find the function V\u0302\u03b8\u2217 that approximates V the best."}, {"heading": "2.1. LSTD", "text": "Let \u03a0\u00b5 denote the orthogonal projection onto H w.r.t. the sampling distribution \u00b5. If D\u00b5 is the diagonal matrix with elements \u00b5(s) and M\u00b5 = \u03a6>D\u00b5\u03a6 is the Gram matrix, then the projection operator is \u03a0\u00b5 = \u03a6M\u22121\u00b5 \u03a6 >D\u00b5. Motivated by the fact that the value\n1This can easily be extended to Markovian decision processes that reduce to MRPs for fixed policies.\nfunction is the fixed point of the Bellman operator T , the LSTD algorithm computes the fixed\u2013point of the joint \u03a0\u00b5T operator: V\u0302\u03b8\u2217 = \u03a0\u00b5T V\u0302\u03b8\u2217 . Let us define A \u2208 Rp\u00d7p and b \u2208 Rp as A = \u03a6>D\u00b5(I\u2212\u03b3P )\u03a6 and b = \u03a6>D\u00b5R. In the following we assume that A and M\u00b5 are invertible. It can be shown through simple algebra that V\u0302\u03b8\u2217 is the fixed-point of \u03a0\u00b5T if and only if \u03b8\u2217 is the (unique) solution to A\u03b8\u2217 = b. This relationship is particularly interesting since it shows that computing the fixed point \u03a0\u00b5T is equivalent to solving a linear system of equations defined by A and b.\nSince P and R are not usually known, we have to rely on sample\u2013based estimates. In particular, we define \u03a6\u0303 (resp. \u03a6\u0303\u2032) \u2208 Rn\u00d7p the empirical feature matrices whose rows contain the feature vectors \u03c6(si)> (resp \u03c6(s\u2032i) >), and R\u0303 \u2208 Rn the reward vector of rowcomponents ri. The random matrices A\u0303 and b\u0303 are then defined as A\u0303 = 1n \u03a6\u0303 >\u2206\u03a6\u0303 and b\u0303 = 1n \u03a6\u0303 >R\u0303 with \u2206\u03a6\u0303 = \u03a6\u0303 \u2212 \u03b3\u03a6\u0303\u2032. LSTD computes the solution \u03b80 of the sample\u2013based linear system A\u0303\u03b80 = b\u0303. We notice that both A\u0303 and b\u0303 are unbiased estimators of the model\u2013based matrices A and b (i.e., E[A\u0303] = A and E[b\u0303] = b), thus suggesting that as the number of samples increases, the solution of LSTD \u03b80 converges to the model\u2013based solution \u03b8\u2217. Since LSTD computes the fixed point of the joint operator \u03a0\u00b5T , then the sample\u2013based LSTD solution can also be formulated in an equivalent form as the solution of two nested optimization problems:{\n\u03c9\u03b8 = argmin\u03c9 \u2016R\u0303+ \u03b3\u03a6\u0303\u2032\u03b8 \u2212 \u03a6\u0303\u03c9\u201622 \u03b80 = argmin\u03b8 \u2016\u03a6\u0303\u03b8 \u2212 \u03a6\u0303\u03c9\u03b8\u201622 , (1)\nwhere the first equation projects the image of the estimated value function V\u0302\u03b8 under the Bellman operator T onto the hypothesis space H, and the second one solves the related fixed-point problem."}, {"heading": "2.2. Related Work", "text": "When the number of samples is close or smaller than the number of features, the matrix A\u0303 is ill\u2013conditioned and some form of regularization should be employed to solve the LSTD problem. In this section, we review the state\u2013of\u2013the\u2013art regularized LSTD algorithms.\nThe formulation of LSTD in Eq. 1 is particularly helpful in understanding the different regularization schemes that could be applied to LSTD. In particular, each of the minimizations relative to the operators \u03a0\u00b5 and T can be regularized, thus obtaining:{\n\u03c9\u03b8 = argmin\u03c9 \u2016R\u0303+ \u03b3\u03a6\u0303\u2032\u03b8 \u2212 \u03a6\u0303\u03c9\u201622 + \u03bb1pen1(\u03c9) \u03b8\u03bb1,\u03bb2 = argmin\u03b8 \u2016\u03a6\u0303\u03b8 \u2212 \u03a6\u0303\u03c9\u03b8\u201622 + \u03bb2pen2(\u03b8) .\nWith this formulation, all the regularization schemes for LSTD (except `1-LSTD, which we discuss at the end of this section) can be summarized as in Tab. 1.\nRidge regression (i.e., `2-regularization) is the most common form of regularization and it simply adds a term \u03bbI to A\u0303. This corresponds to \u03bb1pen1(\u03c9) = \u03bb\u2016\u03c9\u201622 and \u03bb2 = 0 and it has been generalized by Farahmand et al. (2008) with `2,2-LSTD, where both penalty terms use an `2-norm regularization. Although these approaches can help in dealing with ill\u2013defined A\u0303 matrices, they are not specifically designed for the case of n p, where the optimal solution is sparse. In fact, it is well-known that, unlike `1\u2013regularization, `2 does not promote sparsity, and thus, it might fail when the number of samples is much smaller than the number of features.\nThe `1-regularization has been introduced more recently with LASSO-TD, where the projection is replaced by an `1-penalized projection. In this case, the nested optimization problem in Eq. 1 reduces to solving the fixed-point problem (if well defined): \u03b8l,\u03bb = argmin\u03b8 \u2016R\u0303+\u03b3\u03a6\u0303\u2032\u03b8l,\u03bb\u2212\u03a6\u0303\u03b8\u201622+\u03bb\u2016\u03b8\u20161. This algorithm has been first introduced by Kolter & Ng (2009) under the name LARS-TD, where it is solved using an ad\u2013hoc variation of the LARS algorithm (Efron et al., 2004). For LARS-TD to find a solution, A\u0303 must be a P-matrix.2 Unfortunately, this may not be true when the sampling and stationary distributions are different (off-policy learning). Although this does not always affect the performance in practice (see some of the experiments reported in Kolter & Ng 2009), it would be desirable to remove or relax this condition. The LARSTD idea is further developed by Johns et al. (2010), where LASSO-TD is reframed as a linear complementary problem. This allows using any off-the-shelf LCP solver (notably some of them allow warm-starts, which may be of interest in a policy iteration context), but the P-matrix condition is still required, since it is inherent to the optimization problem and not to how it is actually solved. Finally, the theoretical properties of LASSO-TD were analyzed by Ghavamzadeh et al.\n2A P-matrix is a matrix that all its principal minors are positive (generalizing positive definite matrices).\n(2011), who provided prediction error bounds in the on-policy fixed design setting (i.e., the performance is evaluated on the points in the training set). In particular, they show that, similarly to LASSO in regression, the prediction error depends on the sparsity of the projection of the value function (i.e., the `0-norm of the \u03b8 parameter of \u03a0\u00b5V ), and it scales only logarithmically with the number of features. This implies that even if the dimensionality of H is much larger than the number of samples, the LASSO-TD accurately approximates the true value function in the on\u2013 policy setting. In order to alleviate the P-matrix problem, the `1-PBR (Projected Bellman residual) (Geist & Scherrer, 2011) and the `2,1-LSTD (Hoffman et al., 2011) algorithms have been proposed. The idea is to place the `1-regularization term in the fixed-point equation instead of the projection equation. This corresponds to adding an `1-penalty term to the projected Bellman residual minimization (writing \u03a0\u0303 the empirical projection and T\u0303 the sampled Bellman operator): \u03b8pbr,\u03bb = argmin\u03b8 \u2016\u03a0\u0303(\u03a6\u0303\u03b8\u2212T\u0303 (\u03a6\u0303\u03b8))\u20162+\u03bb\u2016\u03b8\u20161. Since this is a convex optimization problem, there is no problem for A\u0303 not being a P-matrix, and off-the-shelf LASSO solvers can be used. However, this comes at the cost of a high computational cost if n p (notably in the computation the empirical projection, which could be as bad as O(p3)), and there is no theoretical analysis.\nFinally, a novel approach has been introduced by Pires (2011). The idea is to consider the linear system formulation of LSTD (i.e., A\u03b8 = b) and to add an `1- penalty term to it: \u03b81,\u03bb = argmin\u03b8 \u2016A\u0303\u03b8\u2212 b\u0303\u201622 + \u03bb\u2016\u03b8\u20161. We refer to this algorithm as `1-LSTD. Being defined as a proper convex optimization problem, it does not have theoretical problems in the off-policy setting and any standard solver can be used. Notice that for \u03b3 = 0, `1-LSTD does not reduce to a known algorithm."}, {"heading": "3. Dantzig-LSTD", "text": "The Dantzig-LSTD (D-LSTD for short) algorithm that we propose in this paper returns an estimate \u03b8d,\u03bb (i.e., a value function V\u03b8d,\u03bb) with a low `1-norm under the constraint that the Bellman residual (R\u0303 + \u03b3\u03a6\u0303\u2032\u03b8 \u2212 \u03a6\u0303\u03b8), namely the correlated Bellman residual (\u03a6\u0303>(R\u0303 + \u03b3\u03a6\u0303\u2032\u03b8 \u2212 \u03a6\u0303\u03b8) = b\u0303\u2212 A\u0303\u03b8), is smaller than a parameter \u03bb. Formally, D-LSTD solves:\n\u03b8d,\u03bb = argmin \u03b8\u2208Rp \u2016\u03b8\u20161 subject to \u2016A\u0303\u03b8 \u2212 b\u0303\u2016\u221e \u2264 \u03bb. (2)\nThis optimization problem is convex and can be easily recast as a linear program (LP):\nmin u,\u03b8\u2208Rp\n1>u subject to { \u2212u \u2264 \u03b8 \u2264 u \u2212\u03bb1 \u2264 A\u0303\u03b8 \u2212 b\u0303 \u2264 \u03bb1 .\nThis algorithm is closely related to DS (Candes & Tao, 2007), to which it reduces when \u03b3 = 0. Being a convex optimization problem, it does not require A\u0303 to be a P-matrix and it can be solved using any LP solver (notably the efficient primal\u2013dual interior point method of Candes & Tao 2007, which makes use of the Woodbury matrix identity when n p)."}, {"heading": "3.1. A Finite Sample Analysis", "text": "In this section we study how well the D-LSTD solution \u03b8d,\u03bb compares to \u03b8\u2217, i.e., the model\u2013based LSTD solution satisfying A\u03b8\u2217 = b. The analysis follows similar steps as in Pires (2011) for `1-LSTD. In the following, we use the assumption that the samples are generated i.i.d. from an arbitrary sampling distribution \u00b5. We leave as future work the extension to Markov design (i.e., when the samples are generated from a single trajectory of the policy under evaluation).\nTheorem 1. Let B\u221e,\u03c6 = maxs\u2208S \u2016\u03c6(s)\u2016\u221e, the DLSTD solution \u03b8d,\u03bb (Eq. 2) satisfies\ninf \u03bb \u2016A\u03b8d,\u03bb \u2212 b\u2016\u221e \u2264 (3)\n2 (\u2016\u03b8\u2217\u20161(1 + \u03b3)B\u221e,\u03c6 + rmax)B\u221e,\u03c6 \u221a 4 n ln 8p \u03b4 ,\nwith probability at least 1\u2212 \u03b4.\nProof. (sketch) We first need a concentration result for the `\u221e-norm. Let x1, . . . , xn be i.i.d. random vectors with mean x\u0304 \u2208 Rd and bounded by \u2016xi\u2016\u221e \u2264 B. Using Hoeffding inequality and a union bound, it is easy to show that with probability greater that\n1 \u2212 \u03b4, one has \u2016 1n \u2211n i=1 xi \u2212 x\u0304\u2016\u221e \u2264 B \u221a 2 n ln 2d \u03b4 . Let \u2206A,max = \u2016A \u2212 A\u0303\u2016max (entrywise max norm) and \u2206b,max = \u2016b \u2212 b\u0303\u2016\u221e. We have the following consistency inequality: \u2016A\u03b8\u2016\u221e \u2264 \u2016A\u2016max\u2016\u03b8\u20161. Combined with the triangle inequality, this gives: |\u2016A\u03b8 \u2212 b\u2016\u221e \u2212 \u2016A\u0303\u03b8 \u2212 b\u0303\u2016\u221e| \u2264 \u2206A,max\u2016\u03b8\u20161 + \u2206b,max. Let us choose \u03bb = \u2206A,max\u2016\u03b8\u2217\u20161 + \u2206b,max. The previous inequality implies that \u2016A\u0303\u03b8\u2217 \u2212 b\u0303\u2016\u221e \u2264 \u03bb (recall that A\u03b8\u2217 = b). Combined with the fact that \u03b8d,\u03bb minimizes Eq. 2, we have that \u2016\u03b8d,\u03bb\u20161 \u2264 \u2016\u03b8\u2217\u20161. Combining the previous results, we obtain \u2016A\u03b8d,\u03bb \u2212 b\u2016\u221e \u2264 2\u2206A,max\u2016\u03b8\u2217\u20161 + 2\u2206b,max. The concentration result for \u2016.\u2016\u221e can be used to bound \u2206A,max and \u2206b,max, which gives the stated result, using the fact that \u2016\u03c6(si)(\u03c6(si) \u2212 \u03b3\u03c6(s\u2032i))T \u2016max \u2264 B2\u221e,\u03c6(1 + \u03b3) and that \u2016\u03c6(si)ri\u2016\u221e \u2264 B\u221e,\u03c6rmax.\nSince the algorithm is specifically designed for the high\u2013dimensional setting (n p), it is critical to study\nthe dependency of the performance on n and p. Up to constant terms, the previous bound can be written as\ninf \u03bb \u2016A\u03b8d,\u03bb \u2212 b\u2016\u221e \u2264 O\n( \u2016\u03b8\u2217\u20161 \u221a 1 n ln p \u03b4 ) .\nFirst we notice that as the number of samples increases, the error of \u03b8d,\u03bb tends to zero, thus implying that it matches the performance of the model\u2013based LSTD solution \u03b8\u2217. Furthermore, the dependency on the number of features p is just logarithmic, while the `1-norm of \u03b8\u2217 is assumed to be small whenever the solution is sparse. This suggests that D-LSTD could work well even in the case n p whenever the problem admits a sparse LSTD solution. Finally, we also notice that there is no specific assumption regarding the learning setting, except that A should be invertible. This is particularly important because it means that, unlike most of the other results available for LSTD (e.g., see Ghavamzadeh et al. 2011), this result holds also in the off-policy setting. The main drawback of this analysis is that it holds for an oracle choice of \u03bb. We postpone a discussion about how to choose the regularizer in practice to Sec. 4."}, {"heading": "3.2. Comparison to Other Algorithms", "text": "Similar to `1-PBR and `2,1-LSTD, D-LSTD is based on a well-defined standard convex optimization problem, which does not require A\u0303 to be a P-matrix (unlike LASSO-TD) and that can be solved using any off-theshelf solvers. Nonetheless, D-LSTD has only one metaparameter (instead of two), and in general, it has a smaller computational cost w.r.t. solving the nested optimization problems of `1-PBR and `2,1-LSTD.\nD-LSTD is also related to LASSO-TD: Proposition 2. The LASSO-TD solution \u03b8l,\u03bb (if it exists) satisfies the D-LSTD constraints:\n\u2016A\u0303\u03b8l,\u03bb \u2212 b\u0303\u2016\u221e \u2264 \u03bb.\nProof. The optimality conditions of LASSO-TD can be obtained by ensuring that 0 belongs to the subgradient of 12\u2016\u03a6\u0303\u03b8 \u2212 (R\u0303 + \u03b3\u03a6\u0303\n\u2032\u03b8l,\u03bb)\u201622 + \u03bb\u2016\u03b8\u20161 and then substituting \u03b8 by \u03b8l,\u03bb (Kolter & Ng, 2009). This notably implies that for all 1 \u2264 i \u2264 p, we have \u2212\u03bb \u2264 (b\u0303\u2212 A\u0303\u03b8l,\u03bb)i \u2264 \u03bb, which is the stated result.\nTherefore, D-LSTD and LASSO-TD satisfy the same constraints, but \u2016\u03b8l,\u03bb\u20161 \u2265 \u2016\u03b8d,\u03bb\u20161, thus suggesting a more sparse solution. This is not surprising, since DLSTD relates to LASSO-TD in a similar way as DS does to LASSO (Bickel et al., 2009). However, thanks to its definition as a convex optimization problem, DLSTD avoids the main drawbacks of LASSO-TD (notably the P-matrix requirement).\nSimilar to `1-LSTD, D-LSTD is built on the linear system of equations formulation of LSTD. Both approaches relax the condition A\u0303\u03b8 = b\u0303 (using an `2- norm of the error for `1-LSTD and an `\u221e-norm for D-LSTD) while penalizing model complexity through the `1-norm of the parameter vector. Both algorithms have the same advantages compared to LASSO-TD and to `1-PBR/`2,1-LSTD. Their main difference lies in their convergence rate. A result similar to Theorem 1 exists for `1-LSTD (Pires, 2011):\ninf \u03bb \u2016A\u03b81,\u03bb \u2212 b\u20162 \u2264 O\n( \u2016\u03b8\u2217\u20161 \u221a p2\nn ln 1 \u03b4\n) .\nAlthough controlling the `2-norm (in `1-LSTD) may be harder than the `\u221e-norm (as in D-LSTD), `1-LSTD has a very poor dependency on p, which makes the bound not informative as n p. On the other hand, D-LSTD just has a logarithmic dependency on p."}, {"heading": "4. Discussion", "text": "In this section, we discuss how the error ||A\u03b8 \u2212 b|| relates to the value function prediction error and how to choose the regularizer \u03bb in practice."}, {"heading": "4.1. From the Parameters to the Value", "text": "Similar to Yu & Bertsekas (2010), we can link V \u2212 V\u0302\u03b8 to A\u03b8 \u2212 b as in the next theorem. Theorem 3. For any V\u0302\u03b8 = \u03a6\u03b8, we have the component\u2013wise equality:\nV \u2212 V\u0302\u03b8 = (I \u2212 \u03b3\u03a0\u00b5P )\u22121((V \u2212\u03a0\u00b5V ) + \u03a6M\u22121\u00b5 (A\u03b8\u0302 \u2212 b)).\nProof. Recall that V = TV (for the true value function) and that V\u0302\u03b8 = \u03a0\u00b5V\u0302\u03b8 (for an estimate V\u0302\u03b8 = \u03a6\u03b8 belonging to the hypothesis space). We have that:\nV \u2212\u03a0\u00b5V = V \u2212\u03a0\u00b5TV \u2212 (V\u0302\u03b8 \u2212\u03a0\u00b5T V\u0302\u03b8) + (V\u0302\u03b8 \u2212\u03a0\u00b5T V\u0302\u03b8) = (I \u2212 \u03b3\u03a0\u00b5P )(V \u2212 V\u0302\u03b8) + \u03a0\u00b5(V\u0302\u03b8 \u2212 T V\u0302\u03b8),\nV \u2212 V\u0302\u03b8 = (I \u2212 \u03b3\u03a0\u00b5P )\u22121 ` (V \u2212\u03a0\u00b5V ) + \u03a0\u00b5(T V\u0302\u03b8 \u2212 V\u0302\u03b8) \u00b4 .\nNote that \u03a0\u00b5(T V\u0302\u03b8 \u2212 V\u0302\u03b8) = \u03a6M\u22121\u00b5 (b \u2212 A\u03b8) gives the result.\nIn order to have the final prediction error, we apply the `\u221e-norm to Theorem 3. Let L\u03c6\u00b5 = maxs \u2016M\u22121\u00b5 \u03c6(s)\u20161, using Theorem 1, we obtain\ninf \u03bb \u2016V \u2212 V\u0302\u03b8d,\u03bb\u2016\u221e \u2264 \u2016(I \u2212 \u03b3\u03a0\u00b5P )\u22121\u2016\u221e\u00d7( \u2016V \u2212\u03a0\u00b5V \u2016\u221e +O ( \u2016\u03b8\u2217\u20161L\u03c6\u00b5 \u221a 1 n ln p \u03b4 )) .\nIn general, the previous expression cannot be simplified any further. Nonetheless, under a high\u2013 dimensional assumption \u03a0\u00b5P = P and \u03a0\u00b5R = R. Therefore, the hypothesis space H is stable by the Bellman operator T and V \u2208 H. In this case, we have \u2016V \u2212 \u03a0\u00b5V \u2016\u221e = 0 and it can be shown that \u2016(I \u2212 \u03b3\u03a0\u00b5P )\u22121\u2016\u221e = 11\u2212\u03b3 . Thus, we obtain the bound (valid also in the off-policy case):\ninf \u03bb \u2016V \u2212 V\u0302\u03b8d,\u03bb\u2016\u221e \u2264 O ( \u2016\u03b8\u2217\u20161L\u03c6\u00b5 1\u2212 \u03b3 \u221a 1 n ln p \u03b4 ) .\nThe main critical term in this bound is L\u03c6\u00b5, which might hide a dependency on the number of features p. In fact, although the specific value of L\u03c6\u00b5 depends on the feature space, it is possible to find cases when it grows as \u221a p (consider an orthonormal basis), thus potentially neutralizing the low dependency on p in Theorem 1. It is an open question whether this dependency on p is intrinsic to the algorithm or is an artifact of the proof. In fact, if \u03b8d,\u03bb solves the linear system of equations accurately, then we expect that the corresponding function V\u0302\u03b8d,\u03bb performs almost as well as the model\u2013based solution V\u0302\u03b8\u2217 . The experiments of Section 5 seem to confirm this conjecture."}, {"heading": "4.2. Cross Validation", "text": "The result of Theorem 1 holds for an oracle value of \u03bb. In practice, the choice of \u03bb can only be directed by the available data. This issue is of great practical importance, though not often discussed in the RL literature (especially for `1-penalized LSTD variations). In supervised learning, algorithms minimize a risk being defined as the (empirical) expectation of some loss function. Cross-validation consists in using an independent sample to estimate the true risk function, and the meta-parameter is selected as the one minimizing the estimated true risk. However, for value function estimation, there is no such risk, and crossvalidation cannot be used. A general model selection method has been derived for value function estimation by Farahmand & Szepesva\u0301ri (2011). However, we may devise an ad\u2013hoc (and simple) solution for D-LSTD. Since D-LSTD is defined as a proper convex optimization problem (which reduces to a supervised learning problem when \u03b3 tends to 0), one may be tempted to use standard cross-validation. Unfortunately, this is not directly possible. Indeed, \u2016A\u0303\u03b8 \u2212 b\u0303\u2016\u221e is the loss (\u2016.\u2016\u221e) of an empirical average (A\u0303 and b\u0303) rather than the empirical expectation of a loss. However, we can still consider some heuristics. Assume that we want to estimate \u2016A\u03b8 \u2212 b\u2016\u221e for some fixed parameter vector \u03b8. Let A\u0303, b\u0303 be unbiased estimates of A, b, then\n\u2016A\u03b8\u2212b\u2016\u221e \u2264 E[\u2016A\u0303\u03b8\u2212b\u0303\u2016\u221e] (Jensen\u2019s inequality). Thus, given an independent set of samples, we have access to an unbiased estimate of an upper\u2013bound of \u2016A\u03b8\u2212b\u2016\u221e. Based on this evidence, we propose a K-fold crossvalidation-based heuristic for D-LSTD. Assume that the training set is split in K folds Fk. Let \u03b8(\u2212k)d,\u03bb denote the estimate trained without Fk, and A\u0303Fk and b\u0303Fk be the quantities computed with only the samples in Fk. A heuristic is to choose the \u03bb that minimizes\nJ1(\u03bb) = 1 K K\u2211 i=1 \u2016A\u0303Fk\u03b8 (\u2212k) d,\u03bb \u2212 b\u0303Fk\u2016\u221e. (4)\nHowever, since we are interested in the case n p and the estimate A\u0303Fk is computed with n K samples, it may have a high variance. An alternative (which we empirically found to be more efficient), at the cost of adding some bias, is to choose \u03bb by minimizing\nJ2(\u03bb) = 1 K K\u2211 i=1 \u2016A\u0303\u03b8(\u2212k)d,\u03bb \u2212 b\u0303\u2016\u221e. (5)\nA similar heuristic can be devised for `1-LSTD. Although the previous heuristic worked well in our experiments, it does not have any theoretical guarantees. A different model selection strategy has been devised for `1-LSTD by Pires (2011). It consists in choosing \u03bb\u0302 = argmin[a,b] \u2016A\u0303\u03b81,\u03bb \u2212 b\u0303\u201622 + \u03bb\u2032\u2016\u03b81,\u03bb\u20161 with [a, b] an exponential grid and \u03bb\u2032 can be computed from data (no oracle choice). This does not require splitting the learning set while ensuring a bound for \u2016A\u03b81,\u03bb\u0302 \u2212 b\u20162. We leave the adaptation of this model selection strategy to D-LSTD for future work."}, {"heading": "5. Illustration and Experiment", "text": "Sec. 5.1 presents an example that shows D-LSTD alleviates the potential problem of off-policy learning. Sec. 5.2 reports a more complex corrupted chain illustrating the case of n p, in an on- and off-policy setting, and studies (heuristic) cross-validation."}, {"heading": "5.1. A Pathological MDP", "text": "We consider a simple two-state MDP (e.g., see Kolter & Ng 2009). The transition matrix and reward vector are P = ( 0 1 0 1 ) and R = ( 0 \u22121\n)>. The optimal value function is therefore V = \u221211\u2212\u03b3 ( \u03b3 1\n)> with \u03b3 the discount factor. Let us consider the one-feature approximation \u03a6 = ( 1 2\n)>. We compare the (asymptotic) regularization paths of LASSO-TD (Kolter & Ng, 2009), `1-LSTD and D-LSTD, in the on-policy and off-policy cases (where LASSO-TD fails).\nOn-policy Case. In the on-policy case, the sampling distribution is \u00b5> = ( 0 1 ) . The regularization paths for each algorithm can be computed easily by solving analytically the optimality conditions (there is only one parameter) and they are reported in Fig. 1, top panels. LASSO-TD and D-LSTD have the same regularization path. This was expected, as there is only one parameter, but this is not true in general (recall that LASSO-TD and D-LSTD inherit the same differences as LASSO and DS).\nOff-policy Case. Let us now consider the uniform distribution \u00b5> = ( 1 2 1 2 ) . For \u03b3 > 56 , A is not a Pmatrix and LASSO-TD does not have a unique solution, nor a piecewise linear regularization path. Paths are shown on Fig. 1, bottom panels. The `1-LSTD\u2019s path is still well-defined. LASSO-TD has more than one solution. The interesting fact here is that DLSTD\u2019s path is well-defined, there is always a unique solution, and the path is piecewise linear. Note that both in the on- and off-policy cases all the algorithms provide the LSTD solution for \u03bb = 0."}, {"heading": "5.2. Corrupted Chain", "text": "We consider the same chain problem as in Kolter & Ng (2009) and Hoffman et al. (2011). The state s has s\u0304 + 1 components si. The first one is an integer (s1 \u2208 {1 . . . 20}) that evolves according to a 20-state, 2-action MDP (states are connected by a chain, the action chooses the direction, and the probability of success is 0.9). All other state components are random Gaussian noises si+1t \u223c N (0, 1). The reward is +1 if s1t = 1 or 20. The feature vector \u03c6(s) \u2208 Rs\u0304+6 consists of an intercept (constant function), 5 radial basis functions corresponding to the first state component, and s\u0304 identity functions corresponding to the irrelevant components: \u03c6(s) = ( 1 RBF1(s1) . . .RBF5(s1) s2 . . . ss\u0304+1 )> .We\ncompare LASSO-TD (with its LARS-like implementation), `1-LSTD, and D-LSTD (for which we used `1-magic (Romberg, 2005)).3 We standardize the data by removing the intercept, centering the observations, centering and standardizing the features \u03a6\u0303, and applying the same transformation (computed from \u03a6\u0303) to \u03a6\u0303\u2032. The intercept can be computed analytically, it is the mean Bellman error (without regularization, this allows recovering the LSTD solution). We also consider `2,\u2205-LSTD, i.e., the standard `2-penalized LSTD.\nOn-policy Evaluation. We first study the on-policy problem. The evaluated policy is the optimal one (going left if s1 \u2264 10, and right otherwise). We sample 400 transitions (20 trajectories of length 20 started randomly on {1 . . . 20}) and vary the number s\u0304 of irrelevant features between 800 and 1400. Results are presented in Fig. 2, averaged over 20 independent runs. For LARS-TD, we computed the whole regularization path (at least until too many features are added) and trained the other algorithms for a set of regularization parameters (logarithmically spaced between 10\u22123 and 10). Each time, we report the best prediction error (on 500 test points, such that the first state component is uniformly sampled from {1 . . . 20}), computed w.r.t. the true value function (therefore, this is an oracle choice). All `1-penalized approaches perform significantly better than the `2-penalization ones, showing that their performance have only a very mild dependency on the dimensionality p (as predicted by Theorem 1). Among them, LASSO-TD seems to be consistently better, followed closely by D-LSTD and `1- LSTD. For LASSO-TD and `2,\u2205-LSTD, these results are consistent with those published by Hoffman et al. (2011). Notice that there was more choice of regularization parameters for LASSO-TD, as the whole regularization path was computed. This may explain the\n3We also considered `1-PBR/`2,1-LSTD. Results are not reported for the sake of clarity, but they behave like `1- LSTD, so worse than LASSO-TD/D-LSTD.\nbetter results of LASSO-TD compared to D-LSTD.\nHeuristic Cross-validation. All results of Fig. 2 require an oracle to choose the right regularization parameter. This is not practical in a real setting. As explained in Sec. 4, `1-LSTD and D-LSTD can benefit from a heuristic cross-validation scheme. We tested K-fold cross-validation (with K = 5) on this problem, with the schemes J1 (Eq. 4) and J2 (Eq. 5) for n = 400 training samples and s\u0304 = 800 irrelevant features (results averaged over 20 independent runs). Results are reported in Tab. 2. The error is computed as before, but here it is not used to choose the regularization parameter. The results for J1 are quite bad, probably due to the high variance of the related estimator (still, for D-LSTD, the right regularization parameter is often chosen, apart from a few outliers). The J2 scheme is much better, comparable to the oracle scheme (see Fig. 2). Comparing the results of the J2 heuristic using a Behrens-Fisher t-test, `1-LSTD and LASSO-TD are different (5% risk), but not D-LSTD and LASSO-TD.\nOff-policy Evaluation. Here we test the off-policy evaluation problem. Let \u03c0opt be the optimal policy (going left if s1 \u2264 10 and right otherwise) and \u03c0worst = 1 \u2212 \u03c0opt (going right if s1 \u2264 10 and left otherwise). We define \u03c0\u03b1 = (1 \u2212 \u03b1)\u03c0opt + \u03b1\u03c0worst, with \u03b1 \u2208 [0, 12 ]. Let also \u00b5\u03b1 be the corresponding stationary distribution and recall V is the true value function. We consider the same problem as before, with s\u0304 = 800. For values of \u03b1 varying from 0 to 0.5, we sample n = 400 chain states according to \u00b5\u03b1 as well as the associated transitions according to the optimal policy. The regularization parameter is chosen to minimize the error between the true value function and the\nestimated one on the training set (thus, an oracle-like selection procedure), for all algorithms. Results are averaged over 50 independent runs. Fig. 3 shows the error \u2016V\u0302\u03b1 \u2212 V \u2016\u00b5\u03b1 as a function of \u03b1. The term 0 corresponds to the zero prediction, that is \u2016V \u2016\u00b5\u03b1 . In all cases, D-LSTD seems to be slightly better than the others, and things get worse as going away from the stationary distribution (as \u03b1 increases). In no case LASSO-TD seems to suffer from off-policy learning, suggesting that in this case the P -matrix condition is satisfied. Also, the difference between `2- and `1- schemes decreases as \u03b1 increases. An `1-schemes may help when there are much more features than samples, but there is little to do when the mismatch between distributions increases. Even if not reported, all approaches performed equally bad when \u03b1 tends to one, since there is no more valuable information in the data."}, {"heading": "6. Conclusion", "text": "In this paper, we introduced the Dantzig-LSTD algorithm with the objective of removing the drawbacks of existing `1-schemes for temporal difference learning. Since D-LSTD is defined as a standard linear program, it does not require A\u0303 to be a P-matrix and can be computed using any LP solver. The D-LSTD estimate is a good approximation of the asymptotic LSTD solution in the sense of Theorem 1. It is also close to the LASSO-TD estimate (whenever well defined) in the sense of Prop. 2. In fact, D-LSTD inherits the same difference that the Dantzig selector has w.r.t. LASSO. Also, our preliminary experiments show that D-LSTD performs at least as well as LASSO-TD.\nThere are still a number of issues that need further investigation. As discussed in Sec. 4, when moving from the linear system of equations to the prediction error, an additional dependency on the number of features seems to appear. To which extent this dependency is an artifact of the proof or a characteristic of the algorithm is not fully clear yet. As for the choice of the regularization parameter, we plan to adapt the model selection scheme of `1-LSTD (Pires, 2011) to D-LSTD and test it. Finally, we plan to test D-LSTD in control schemes (i.e., policy iteration).\nAcknowledgments The first author thanks the Re\u0301gion Lorraine for financial support. The third and fourth authors would like to thank French National Research Agency (ANR) under project LAMPADA n\u25e6 ANR-09-EMER-007, European Community\u2019s Seventh Framework Programme (FP7/2007-2013) under grant agreement n\u25e6 231495, and PASCAL2 European Network of Excellence for supporting their research."}, {"heading": "Bickel, P. J., Ritov, Y., and Tsybakov, A. B. Simultaneous", "text": "analysis of Lasso and Dantzig selector. The Annals of Statistics, 37(4):1705\u20131732, 2009.\nBradtke, S. J. and Barto, A. G. Linear Least-Squares algorithms for temporal difference learning. Machine Learning, 22:33\u201357, 1996.\nCandes, E. and Tao, T. The Dantzig selector: statistical estimation when p is much larger than n. Annals of Statistics, 35(6):2313\u20132351, 2007."}, {"heading": "Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R.", "text": "Least Angle Regression. Annals of Statistics, 32(2):407\u2013 499, 2004."}, {"heading": "Farahmand, A., Ghavamzadeh, M., Szepesva\u0301ri, C., and", "text": "Mannor, S. Regularized Policy Iteration. In Proc. of NIPS 21, 2008."}, {"heading": "Farahmand, A. M. and Szepesva\u0301ri, C. Model selection in", "text": "reinforcement learning. Machine Learning Journal, 85 (3):299\u2013332, 2011.\nGeist, M. and Scherrer, B. `1-penalized projected Bellman residual. In Proc. of EWRL 9, 2011."}, {"heading": "Ghavamzadeh, M., Lazaric, A., Munos, R., and Hoffman,", "text": "M. Finite-Sample Analysis of Lasso-TD. In Proc. of ICML, 2011."}, {"heading": "Hoffman, M. W., Lazaric, A., Ghavamzadeh, M., and", "text": "Munos, R. Regularized Least Squares Temporal Difference learning with nested `2 and `1 penalization. In Proc. of EWRL 9, 2011.\nJohns, J., Painter-Wakefield, C., and Parr, R. Linear Complementarity for Regularized Policy Evaluation and Improvement. In Proc. of NIPS 23, 2010."}, {"heading": "Kolter, J. Z. and Ng, A. Y. Regularization and Feature Selection in Least-Squares Temporal Difference Learning.", "text": "In Proc. of ICML, 2009.\nPires, B. A. Statistical analysis of `1-penalized linear estimation with applications. Master\u2019s thesis, University of Alberta, 2011.\nRomberg, J. `1-magic matlab library. http://users.ece. gatech.edu/~justin/l1magic/, 2005.\nSutton, R. S. and Barto, A. G. Reinforcement Learning: an Introduction. The MIT Press, 1998."}, {"heading": "Tibshirani, R. Regression Shrinkage and Selection via the", "text": "Lasso. Journal of the Royal Statistical Society, 58(1): 267\u2013288, 1996.\nYu, H. and Bertsekas, D. P. Error Bounds for Approximations from Projected Linear Equations. Mathematics of Operations Research, 35:306\u2013329, 2010."}], "references": [{"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Linear Least-Squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning,", "citeRegEx": "Bradtke and Barto,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto", "year": 1996}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["E. Candes", "T. Tao"], "venue": "Annals of Statistics,", "citeRegEx": "Candes and Tao,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao", "year": 2007}, {"title": "Least Angle Regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Regularized Policy Iteration", "author": ["A. Farahmand", "M. Ghavamzadeh", "C. Szepesv\u00e1ri", "S. Mannor"], "venue": "In Proc. of NIPS", "citeRegEx": "Farahmand et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2008}, {"title": "Model selection in reinforcement learning", "author": ["A.M. Farahmand", "C. Szepesv\u00e1ri"], "venue": "Machine Learning Journal,", "citeRegEx": "Farahmand and Szepesv\u00e1ri,? \\Q2011\\E", "shortCiteRegEx": "Farahmand and Szepesv\u00e1ri", "year": 2011}, {"title": "`1-penalized projected Bellman residual", "author": ["M. Geist", "B. Scherrer"], "venue": "In Proc. of EWRL", "citeRegEx": "Geist and Scherrer,? \\Q2011\\E", "shortCiteRegEx": "Geist and Scherrer", "year": 2011}, {"title": "Finite-Sample Analysis of Lasso-TD", "author": ["M. Ghavamzadeh", "A. Lazaric", "R. Munos", "M. Hoffman"], "venue": "In Proc. of ICML,", "citeRegEx": "Ghavamzadeh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ghavamzadeh et al\\.", "year": 2011}, {"title": "Regularized Least Squares Temporal Difference learning with nested `2 and `1 penalization", "author": ["M.W. Hoffman", "A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "In Proc. of EWRL", "citeRegEx": "Hoffman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2011}, {"title": "Linear Complementarity for Regularized Policy Evaluation and Improvement", "author": ["J. Johns", "C. Painter-Wakefield", "R. Parr"], "venue": "In Proc. of NIPS", "citeRegEx": "Johns et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Johns et al\\.", "year": 2010}, {"title": "Regularization and Feature Selection in Least-Squares Temporal Difference Learning", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "In Proc. of ICML,", "citeRegEx": "Kolter and Ng,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "Statistical analysis of `1-penalized linear estimation with applications", "author": ["B.A. Pires"], "venue": "Master\u2019s thesis, University of Alberta,", "citeRegEx": "Pires,? \\Q2011\\E", "shortCiteRegEx": "Pires", "year": 2011}, {"title": "Reinforcement Learning: an Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Regression Shrinkage and Selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Error Bounds for Approximations from Projected Linear Equations", "author": ["H. Yu", "D.P. Bertsekas"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Yu and Bertsekas,? \\Q2010\\E", "shortCiteRegEx": "Yu and Bertsekas", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "In particular, LASSO-TD (Kolter & Ng, 2009) can be seen as en extension of LASSO (Tibshirani, 1996) to temporal difference learning, to which it reduces when setting the discount factor to zero.", "startOffset": 81, "endOffset": 99}, {"referenceID": 13, "context": "In particular, LASSO-TD (Kolter & Ng, 2009) can be seen as en extension of LASSO (Tibshirani, 1996) to temporal difference learning, to which it reduces when setting the discount factor to zero. However, LASSO-TD is not derived from a proper convex optimization problem, and thus, it requires some assumptions that might not hold in an off-policy setting. Although other algorithms have been proposed to overcome these drawbacks (e.g., `1-PBR by Geist & Scherrer (2011)), other disadvantages may appear.", "startOffset": 82, "endOffset": 470}, {"referenceID": 4, "context": "This corresponds to \u03bb1pen1(\u03c9) = \u03bb\u2016\u03c9\u20162 and \u03bb2 = 0 and it has been generalized by Farahmand et al. (2008) with `2,2-LSTD, where both penalty terms use an `2-norm regularization.", "startOffset": 80, "endOffset": 104}, {"referenceID": 3, "context": "This algorithm has been first introduced by Kolter & Ng (2009) under the name LARS-TD, where it is solved using an ad\u2013hoc variation of the LARS algorithm (Efron et al., 2004).", "startOffset": 154, "endOffset": 174}, {"referenceID": 3, "context": "This algorithm has been first introduced by Kolter & Ng (2009) under the name LARS-TD, where it is solved using an ad\u2013hoc variation of the LARS algorithm (Efron et al., 2004). For LARS-TD to find a solution, \u00c3 must be a P-matrix. Unfortunately, this may not be true when the sampling and stationary distributions are different (off-policy learning). Although this does not always affect the performance in practice (see some of the experiments reported in Kolter & Ng 2009), it would be desirable to remove or relax this condition. The LARSTD idea is further developed by Johns et al. (2010), where LASSO-TD is reframed as a linear complementary problem.", "startOffset": 155, "endOffset": 592}, {"referenceID": 8, "context": "In order to alleviate the P-matrix problem, the `1-PBR (Projected Bellman residual) (Geist & Scherrer, 2011) and the `2,1-LSTD (Hoffman et al., 2011) algorithms have been proposed.", "startOffset": 127, "endOffset": 149}, {"referenceID": 11, "context": "Finally, a novel approach has been introduced by Pires (2011). The idea is to consider the linear system formulation of LSTD (i.", "startOffset": 49, "endOffset": 62}, {"referenceID": 11, "context": "The analysis follows similar steps as in Pires (2011) for `1-LSTD.", "startOffset": 41, "endOffset": 54}, {"referenceID": 0, "context": "This is not surprising, since DLSTD relates to LASSO-TD in a similar way as DS does to LASSO (Bickel et al., 2009).", "startOffset": 93, "endOffset": 114}, {"referenceID": 11, "context": "A result similar to Theorem 1 exists for `1-LSTD (Pires, 2011):", "startOffset": 49, "endOffset": 62}, {"referenceID": 11, "context": "A different model selection strategy has been devised for `1-LSTD by Pires (2011). It consists in choosing \u03bb\u0302 = argmin[a,b] \u2016\u00c3\u03b81,\u03bb \u2212 b\u0303\u20162 + \u03bb\u2016\u03b81,\u03bb\u20161 with [a, b] an exponential grid and \u03bb\u2032 can be computed from data (no oracle choice).", "startOffset": 69, "endOffset": 82}, {"referenceID": 8, "context": "We consider the same chain problem as in Kolter & Ng (2009) and Hoffman et al. (2011). The state s has s\u0304 + 1 components s.", "startOffset": 64, "endOffset": 86}, {"referenceID": 8, "context": "For LASSO-TD and `2,\u2205-LSTD, these results are consistent with those published by Hoffman et al. (2011). Notice that there was more choice of regularization parameters for LASSO-TD, as the whole regularization path was computed.", "startOffset": 81, "endOffset": 103}, {"referenceID": 11, "context": "As for the choice of the regularization parameter, we plan to adapt the model selection scheme of `1-LSTD (Pires, 2011) to D-LSTD and test it.", "startOffset": 106, "endOffset": 119}], "year": 2012, "abstractText": "LSTD is a popular algorithm for value function approximation. Whenever the number of features is larger than the number of samples, it must be paired with some form of regularization. In particular, `1-regularization methods tend to perform feature selection by promoting sparsity, and thus, are wellsuited for high\u2013dimensional problems. However, since LSTD is not a simple regression algorithm, but it solves a fixed\u2013point problem, its integration with `1-regularization is not straightforward and might come with some drawbacks (e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector. We investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches, and show how it addresses some of their drawbacks.", "creator": "LaTeX with hyperref package"}}}