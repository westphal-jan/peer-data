{"id": "1206.6482", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Modeling Images using Transformed Indian Buffet Processes", "abstract": "Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. In addition, as image model modeling is described, several models may not necessarily provide optimal results.\n\n\n\n\nThe most efficient way to model transformation-invariant features in images and image modeling is by drawing out two layers of black and white images: a color-coded image, and a white background. In such models, the image can be assigned as the main object or its component, either as a layer, a separate layer, a part of the image or a combination.\nThe color-coded image of the main object or its component, either as a layer, a separate layer, a part of the image or a combination.\nImage model models typically generate the most realistic image in the image model, but are often poorly understood. In some models, there is also an advantage of using black and white backgrounds.\nBecause image model models are not based in a natural way, the main image is not usually represented in the image model, but in images in other images that are more realistic.\nIn fact, this is not the case for a single image model:\nA single model is an image model, and the model is not usually represented in a larger color-coded image.\nTo avoid any confusion, the first step is to calculate the image and shape-to-shape transformations used in the image model. As such, we only take two color-coded images as the main object or its component. In some models, this will only be represented in a wider range of colors, especially in a lower-cost setting.\nThe second step is to find the most realistic image in the image model. As such, the colors in the image models can be represented in multiple colors or even in a smaller and smaller set of colors, including more realistic and less costly models. However, when using a different color-coded image model, we only consider the only color-coded image in the image model, and the models can be generated with different models.\nHere is a simplified image model:\nA second model consists of some colors that look better in any color-coded image.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (612kb)", "http://arxiv.org/abs/1206.6482v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CV cs.LG stat.ML", "authors": ["ke zhai 0001", "yuening hu", "jordan l boyd-graber", "sinead williamson"], "accepted": true, "id": "1206.6482"}, "pdf": {"name": "1206.6482.pdf", "metadata": {"source": "META", "title": "Modeling Images using Transformed Indian Buffet Processes", "authors": ["Yuening Hu", "Ke Zhai", "Sinead Williamson"], "emails": ["YNHU@CS.UMD.EDU", "ZHAIKE@CS.UMD.EDU", "SINEAD@CS.CMU.EDU", "JBG@UMIACS.UMD.EDU"], "sections": [{"heading": "1. Introduction", "text": "Latent feature models assume data are generated by combining latent features shared across the dataset and aim to learn this latent structure in an unsupervised manner. Such models typically assume all properties of a feature are common to all data points\u2014i.e., each feature appears in exactly the same way across all observations. This is often a reasonable assumption. For example, microarray data are designed so each cell consistently corresponds to a specific condition.\nThis does not hold for images. Consider a collection of images of a rolling ball. If a model must create new features to\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nexplain the ball\u2019s every position, it will devote less attention to other aspects of the image and will be unable to generalize across the ball\u2019s path. Instead, we would like some properties of a feature, e.g., shape, to be shared across data points but properties, e.g., location, to be observation-specific.\nModels that generalize across images to discover transformation-invariant features have many applications. Image tracking, for instance, discovers mislaid bags or illegally stopped cars. Image reconstruction restores partially corrupted images. Movie compression recognizes recurring image patches and caches them across frames.\nWe argue that latent feature models of images should:\n\u2022 Discover features needed to model data and add additional features to model new data. \u2022 Generalize across transformations so features can have different locations, scales, and orientations. \u2022 Handle properties of real images such as occlusion.\nA nonparametric model that comes close to our goals is the noisy-OR transformed Indian buffet process (NO-tIBP, Austerweil & Griffiths, 2010); however, its likelihood model is inappropriate for real images. Existing unsupervised models that handle realistic likelihoods (Jojic & Frey, 2001; Titsias & Williams, 2006) are parametric and cannot discover new features. In Section 2, we further describe these and other models that meet some, but not all, of our criteria.\nIn Section 3, we propose models that fulfill these properties by combining realistic likelihoods with nonparametric frameworks. In Section 4, we introduce novel inference algorithms that dramatically improve inference for transformed IBPs in larger datasets (Section 5). In Section 6, we show that our models can discover features and model data better than existing models. We discuss relationships with other nonparametric models and extensions in Section 7.\n\u2020 indicates equal contributions."}, {"heading": "2. Background", "text": "In this section, we review the Indian buffet process and how its extension, the transformed IBP, models simple images. We then describe likelihood models for images. These models are a prelude to the models we introduce in Section 3."}, {"heading": "2.1. The Indian Buffet Process", "text": "The Indian buffet process (IBP, Griffiths & Ghahramani, 2005) is a distribution over binary matrices with exchangeable rows and infinitely many columns. This can define a nonparametric latent feature model with an unbounded number of features. This often matches our intuitions. We do not know how many latent features we expect to find in our data; neither do we expect to see all possible latent features in a given dataset.\nTo use the IBP to model data, we must select a likelihood model that determines the form of features corresponding to columns of Z and how the subset of features selected by a row of Z combine to generate a data point.1 Many likelihoods have been proposed for the IBP, several of which are appropriate for modeling images."}, {"heading": "2.2. The Transformed IBP", "text": "Most IBP-based latent feature models assume a feature is identical in every data point in which it appears. This precludes image modeling, where (for example) a car moves\n1We follow the convention that zn is the nth row of a matrix Z, and znk is the kth element of the vector zn.\nfrom location to location or where a person may be in either the foreground or background. Na\u0131\u0308ve models would learn different features for each location a car appears in; a more appropriate model would learn that each observation is in fact a transformation of a common feature.\nThe transformed IBP (tIBP, Austerweil & Griffiths, 2010) extends the IBP to accommodate data with varying locations. In the tIBP, each column of an IBP-distributed matrix Z is (as before) associated with a feature. In addition, each nonzero element of Z is associated with a transformation rnk. Transforming the features and combining them according to a likelihood model produces observations. In the original tIBP paper, features were generated and combined using noisy-OR (Wood et al., 2006); we refer to this model as the noisy-OR tIBP (NO-tIBP), which allows the same feature to appear in different locations, scales, and orientations."}, {"heading": "2.3. Likelihoods for Latent Feature Image Models", "text": "In addition to the noisy-OR, another likelihood that has been used with the IBP is a linear Gaussian model, which assumes images are generated via a linear superposition of features (Griffiths & Ghahramani, 2005). Each IBP row selects a subset of features and generates an observation by additively superimposing these features and adding Gaussian noise. This is demonstrated in Figure 1(a). This model can be extended by adding weights to the non-zero elements of the IBP-distributed matrix (Knowles & Ghahramani, 2007) and incorporating a spiky noise model (Zhou et al., 2011) appropriate for corrupted images.\nIf we want to model images where features can occlude each other, linear Gaussian models are inappropriate. In the vision community, images are often represented via overlapping layers (Wang & Adelson, 1994), including in generative probabilistic models (Jojic & Frey, 2001; Titsias & Williams, 2006). In these \u201csprite\u201d models, features are Gaussian-distributed, and an ordering is defined over a set of features. In each image, every active feature has a transformation (as in the tIBP) and a binary mask for each pixel. Given the feature order, the image is generated by taking the value, at each pixel, of the uppermost unmasked feature.\nThis model is appealing. It is an intuitive occlusion model; features have a consistent ordering; and only the topmost feature is visible. However, this likelihood model has only been used for parametric feature sets and on data where the number of features is known a priori."}, {"heading": "3. Modeling Real-valued Images", "text": "While the NO-tIBP likelihood model is incompatible with real images, it provides a foundation for nonparametric models with transformed features. In this section, we use the tIBP to build models that combine nonparametric feature\nmodels with more useful and realistic likelihood functions for real images.\nWe begin by providing a general representation for the transformed IBP with an arbitrary likelihood.\n1. Sample a binary matrix Z \u223c IBP(\u03b1), determining the features (columns) present in observations (rows). 2. For k \u2208 N, sample a feature \u03c6k \u223c p(\u03c6). 3. For each image n \u2208 {1, . . . , N}\n\u2022 For k \u2208 N, sample a transformation rnk \u223c p(r). \u2022 Sample an image xn \u223c p(x|\u03a6, zn, rn).\nThe distribution over transformations p(r), the feature likelihood p(\u03c6), and the image likelihood p(x|\u03a6, zn, rn) can be defined in various ways. In the remainder of this section, we will use this generic framework to define concrete models with a parameterization of transformations and two different likelihood models.\nTransformations Following Austerweil & Griffiths (2010), we consider three categories of transformation: translation, rotation and scaling. We parameterize a transformation r : RD \u2192 RD using a vector (rx, ry, rr, rs). The parameters (rx, ry) parameterize translations, and the transformed feature r(ak) is obtained by shifting each pixel in ak by (rx, ry). Rotations are parameterized by rr \u2208 [0, 2\u03c0), and scaling is parameterized by rs \u2208 R+. In practice, we restrict the possible rotations and scaling factors to a finite set, and assume a uniform prior on transformations.\nLinear Gaussian transformed IBP Our first attempt to define a likelihood applicable to real data is based on the linear Gaussian likelihood for the IBP described in Section 2.3. Each feature \u03c6k is represented using a real-valued vector ak \u223c N (0, \u03c32aI). In each image, the transformed features are combined using superposition,\nxn \u223cN ( \u2211\u221e k=1znkrnk(ak), \u03c3 2 xI). (1)\nWe refer to the resulting model as the linear Gaussian transformed IBP (LG-tIBP).\nMasked transformed IBP While the LG-tIBP model is appropriate for real-valued data, it cannot handle feature occlusion. To address this problem, we propose a masked transformed IBP (M-tIBP), based on the sprite model (Section 2.3). In this model, each feature \u03c6k is represented by a Gaussian feature ak and a shape vector \u03c0k. Let \u03c9 be a permutation of N that imposes an ordering on the features. We can interpret feature i being \u201cbehind\u201d feature k if \u03c9i < \u03c9k. Each time a feature appears in an image, we sample a mask sn,k from the Bernoulli probabilities in the corresponding shape vector \u03c0k. These masks \u201cocclude\u201d lower layers so that at each pixel; only the uppermost unmasked feature contributes to the final image.\nThe generative process can be described as follows. For each image n and feature k, define an auxiliary variable Mn,k, the visibility indicator,\nMdn,k =  1 if argmaxj [ \u03c9jzn,j ( s r\u22121n,j(d) n,j )] = k and s r\u22121n,k(d) n,k > 0\n0 otherwise.\n(2)\nThe visibility indicator Mdn,k, is 1 when feature k is the uppermost unmasked feature at pixel d in image n. The image and feature likelihoods for the M-tIBP are\nak \u223cN (0, \u03c32aI)\n\u03c0dk \u223cBeta(\u03b2, \u03b2) \u03c9 \u223cUniform()\n\u03c6k :=(ak,\u03c0k, \u03c9k)\nsdn,k \u223cBernoulli(\u03c0dk) xn \u223cN ( \u2211\u221e k=1znk \u00b7 [rnk(ak) \u25e6Mn,k] , \u03c3 2 xI),\n(3)\nwhere the operator \u25e6 is the Hadamard product on matrices.\nFigure 1 shows how the IBP-distributed matrix Z and other transformations variables combine features to form images for the IBP, LG-tIBP, and M-tIBP."}, {"heading": "4. Inference", "text": "We perform inference of both LG-tIBP and M-tIBP using MCMC. At each iteration, we sample the Gaussiandistributed features A, the IBP-distributed binary matrix Z, the transformations R, the hyperparameters \u03b1, \u03c3x and \u03c3a, and, for M-tIBP, the binary masks S and ordering \u03c9."}, {"heading": "4.1. Sampling Indicators, Transformations, and Masks", "text": "In all models, the binary indicator matrix Z, the matrix of transformations R, and (where appropriate) the feature masks S are all closely coupled. Austerweil & Griffiths (2010) sampled each znk of Z by explicitly marginalizing over rnk, and then sampling rnk. However, explicitly computing the conditional distribution for all transformations for each feature cannot scale to even moderate-sized images (as discussed in Section 5). Instead, we sample znk, rnk and sn,k jointly via a Metropolis-Hastings step.\nThe efficacy of a Metropolis-Hastings sampler depends on the quality of the proposal distribution. We design a data-driven proposal distribution (Tu & Zhu, 2002) q(znk, rnk, snk) = qz(znk)qr(rnk)qs(snk) based on an established pattern matching technique that assigns high probability to plausible states.\nFeature Indicator Proposal Distribution LetK+ be the highest feature index represented in the data, excluding the\ncurrent data point. Our proposal distribution for znk, k \u2264 K+ is\nq(znk \u2192 z\u2217nk) = { 1 if z\u2217nk 6= znk 0 otherwise.\n(4)\nOur proposal distribution for previously unseen features follows Griffiths & Ghahramani (2005): sample K\u2217 new features according to Poisson(\u03b1/N ), whereN is the number of observations.\nTransformation Proposal Distribution To obtain a proposal distribution for translations rnk that matches our intuitions about the true posterior, we look at the crosscorrelation between the feature ak and the residual x\u0303n,k obtained by removing all but that feature from the image xn. Cross-correlation (Duda & Hart, 1973) is a standard tool in classical image analysis and pattern-matching. The crosscorrelation u ? v between two real-valued images u and v is a measure of the similarity between u and a translated version of v, i.e., (u ? v)(t) := \u2211T \u03c4=1 u(\u03c4)v(t+ \u03c4). Since our proposal distribution for r\u2217nk must be strictly positive, we use the exponentiated function\nq(r|ak, x\u0303n,k) \u221d exp {(x\u0303n,k ? ak)(r)} , (5)\nfor our proposal distribution,2 and define the residual x\u0303n,k\nx\u0303n,k = \u2211 j:\u03c9j<\u03c9k Mn,j \u25e6 xn (6)\nfor M-tIBP, and\nx\u0303n,k = xn \u2212 \u2211 j 6=k znjrnj(ak) (7)\nfor LG-tIBP.\nIn Figure 2, we show the proposal distribution for r\u2217nk for a feature and three data points. The proposal distribution peaks in the locations that best match the pattern of pixels in the feature. If no locations match the feature, the proposal distribution is relatively entropic. Thus, the crosscorrelation proposal distribution will cause us to consider good candidates for rnk.\nTo incorporate scaling and rotation in addition to translation, we must increase the space over which we define our Metropolis-Hastings proposal. For a small transformation space (e.g., multiples of \u03c02 rotation and half / double scaling) it remains practical to extend the proposal distribution to include all possible scaling and rotation combinations. We separately obtain cross-correlations of these transformed features with the residual image, and concatenate the resulting vectors to obtain a distribution over all possible transformations. For new features, rnk is set to be the identity transformation.\n2Of course, any R \u2192 R+ function would be a fair choice; however we found exponentiating works in practice.\nMask Proposal Distribution In the M-tIBP, we must also propose a binary mask sn,k. We use, as a proposal distribution, the conditional distribution\nqs(sn,k) = \u220fD d=1p(s d nk = v|s\u2212(n,k)) (8)\np(sdn,k = 1|s\u2212(n,k)) = \u2211 m 6=n s d m,k + \u03b2\u2211\nm 6=n zmk + 2\u03b2 . (9)\nUnseen Features For previously unseen features, we sample a new feature ak \u223c N (0, \u03c32a). Our proposal distribution for the corresponding mask is obtained by normalizing ak and sampling each pixel of the proposed mask s\u2217n,k according to a series of Bernoulli distributions parameterized by the normalized entries of ak."}, {"heading": "4.2. Resampling Transformation and Masks", "text": "In addition to sampling znk, rnk and sn,k jointly, we also resample rnk (and, for M-tIBP, sn,k) for values of n and k for which znk = 1. We jointly resample rnk using a Metropolis-Hastings step with proposal distribution qr(rnk) (or qr(rnk)qs(snk)). For the M-tIBP, we also Gibbs sample the binary masks using the conditional distribution\np(sdn,k|sd\u2212(n,k),xn, z, rn,A)\n\u221d p(xn|sdn,k, zn, rn,A) \u00b7 p(sdn,k|sd\u2212(n,k)), (10)\nwhere p(sdn,k|sd\u2212(n,k)) is given in Eqn. (9)."}, {"heading": "4.3. Sampling the Feature Order", "text": "We assume the feature order \u03c9 is sampled from a uniform distribution over permutations. We sample the feature order using a Metropolis-Hastings step where we uniformly choose two consecutive features and propose an order swap."}, {"heading": "4.4. Sampling Features and Hyperparameters", "text": "Conjugacy eases the sampling of ak. For the M-tIBP, we sample the dth pixel of the kth feature as\nakd|Z,R,S,X \u223c N ( F \u03c32x \u2211N n=1M d n,kxn,rnk(d), F ) , (11)\nwhere F = (\u03c3\u22122a + \u03c3 \u22122 x \u2211N n=1M d n,k) \u22121.\nThe hyperparameters \u03b1, \u03c3x and \u03c3a can be Gibbs sampled via closed form equations (Doshi-Velez, 2009)."}, {"heading": "4.5. Modeling Color Images", "text": "The derivation above assumes that each pixel is a single real number. However, natural images are typically have color information, represented as a three-dimensional vector for each pixel. In our model, all colors contribute to the image likelihoods. Similarly, the proposal distribution is an element-wise sum over all possible channels,\nq(r|ak, x\u0303n,k) \u221d exp {\u2211 c(x\u0303 c n,k ? a c k)(r) } , (12)\nwhere x\u0303cn,k and a c k are c-channel contribution of x\u0303n,k and ak, respectively.\nIn the M-tIBP case, for feature k in image n, we assume all channels share a common mask sn,k."}, {"heading": "5. Computational Complexity", "text": "The main motivation behind the algorithm proposed in Section 4 is to allow the transformed IBP to be applied to large data. Austerweil & Griffiths (2010) calculate the likelihood of the data for every possible transformation. Replacing this naive approach with the sampler presented above can achieve a speed-up of at least O(Dmin(SR,K/ logD)), where R is the number of rotations considered, S is the number of scales considered, D is the number of pixels, and K is the number of non-zero elements in zn.\nEvaluating the LG-tIBP and M-tIBP likelihoods for a single image requires O(DK) computations. Since the number of possible translations3 is O(D), calculating the likelihood for all possible translations in O(SRD2K), yielding a total per-iteration complexity of O(ND2K2) for the inference method used by Austerweil & Griffiths (2010). If we were to also sum over values of sn,k, this would scale as O(2D).\nBy contrast, calculating the cross-correlation between a feature and an image residual can be done using the fast Fourier transform in O(D logD), so the proposal distribution described in Section 4.1 can be calculated in O(SRD logD). The likelihood need only be evaluated twice in the Metropolis-Hastings step, so our sampler scales as O(NSRDKmax(K, logD))."}, {"heading": "6. Experimental Evaluation", "text": "We evaluate the LG-tIBP and M-tIBP models4 on both simulated and real-world data against the linear Gaussian IBP\n3Since features can be centered outside the image, the total number of translations is in fact greater than the number of pixels.\n4http://www.cs.umd.edu/\u02dcynhu/code/mtibp\n(IBP), the noisy-OR transformed IBP (NO-tIBP) and the sprite model (SPRITE, Jojic & Frey, 2001). Experiments on simulated data show that both LG-tIBP and M-tIBP recover the underlying features and locations more effectively than IBP. All data sets were scaled to have zero mean and unit variance for linear Gaussian models.\nSimulated Data To qualitatively assess the ability of LGtIBP and M-tIBP to find translated features, we generated data using four colorful features: \u201cO\u201d, \u201c>\u201d, \u201ct\u201d, and \u201c\u00d7\u201d. Each synthetic dataset contains 100 images generated by selecting features independently with probability 0.5 and sampling a transformation uniformly. Since the noisy-OR likelihood cannot process color images, data are binarized for NO-tIBP. Although the other models can cope with Gaussian noise, NO-tIBP cannot, so no noise was added. Each experiment ran 100 iterations; we present features and reconstructions from the final iteration.\nFigure 3 compares the performance of the four models on a dataset constructed by translating four features. NOtIBP achieves good results. While the IBP struggles to find common structure, both LG-tIBP and M-tIBP generalize across locations and discover features qualitatively similar to NO-tIBP\u2019s. Where features overlap, M-tIBP obtains the correct reconstruction; LG-tIBP does not.\nFigure 4 shows the training set likelihood at each iteration, plotted against accumulated CPU time, obtained using both the proposed Metropolis Hastings inference and Gibbs sampling in the LG-tIBP model on two datasets: 9 \u00d7 9 and 15 \u00d7 15 pixel images respectively. Each marker indicates a single iteration; each plot shows 100 iterations. Time was measured on a machine with 6-Core 2.8-GHz CPU and 16GB memory. The speed-up predicted in Section 5 is real-\nized in practice; while convergense requires slightly more iterations, it requires far less total CPU time.\nIn addition, we trained LG-tIBP and M-tIBP on a dataset where features have been scaled, rotated, and translated. This was not implemented by Austerweil & Griffiths (2010), presumably due to the computational cost. Figure 5 shows that our two models successfully detected the underlying features. The ordering learned by M-tIBP matches the true order, except in the case of the green \u201cO\u201d and the blue \u201c\u00d7\u201d, which did not often overlap.\nReal-world data To show that the performance on simulated data in Section 6 carries over to real images, we evaluated LG-tIBP and M-tIBP on four image datasets, chosen to reflect various levels of complexity from simple video games with static/dynamic background to real-world scenes.\n1. DNK: 171 screen shots from the 1981 video game \u201cDonkey Kong\u201d.5 2. SMB: 200 screen shots from the 1985 video game \u201cSuper Mario Brothers\u201d.6 3. TFC: 186 frames from an intersection traffic video.7\n5 http://www.youtube.com/watch?v=EhFV5-qbbIw 6 http://www.youtube.com/watch?v=xkD7L2QFwR0\n7Raw AVSS PV Easy data available at http://www.eecs.qmul.ac.uk/ \u02dcandrea/avss2007_d.html\n4. WLK: 226 frames from a video of people walking in a Lisbon shopping center.8\nAll images were resized to 101\u00d7101 pixels. We trained and tested the models using the full three-channel RGB data.\nFor each dataset, we trained LG-tIBP, M-tIBP, IBP and SPRITE9 on a randomly selected 80% of the images with the remaining 20% held out for testing. Since the NO-tIBP is only appropriate for binary data, we could not compare with this method. We used the features extracted from the training set to estimate Z and R on test data, and evaluated the reconstructions using test set RMSE. Table 1 shows that LG-tIBP and M-tIBP achieve better performance than IBP across all datasets; M-tIBP performs equally well as SPRITE on three datasets, and much better on the SMB dataset. M-tIBP performs better than LG-tIBP on SMB and WLK datasets, but worse than LG-tIBP on DNK. This is because DNK has limited occlusions and a black background, and so can be adequately represented using the simpler LG-tIBP.\nFigure 6 shows reconstructions and features obtained using the IBP, SPRITE, LG-tIBP, and M-tIBP. The IBP only matches the image background. In contrast, both LG-tIBP and M-tIBP identify shapes that appear in different locations. For example, in the first column of Figure 6, LG-tIBP identifies Donkey Kong (cyan) and a fireball (yellow), in addition to the background (green). Interestingly, LG-tIBP mis-identifies a pie10 as a fireball but missed the actual fireball. Our M-tIBP model detected the pie (red) and the fireball (blue), while Donkey Kong (cyan) and background\n8Raw WalkByShop1cor data available at http://groups.inf.ed.ac. uk/vision/CAVIAR/CAVIARDATA1/\n9The publicly available implementation of SPRITE could not detect any features in our datasets. To enable the fairest comparison possible, we compare against a finite version of M-tIBP with a fixed K (based on the \u201ctrue\u201d K based on inspecting the dataset, as in previous works using SPRITE) and an a \u201calways on\u201d Z. We believe that this is equivalent to the SPRITE model, although the inference implementation has tweaks and tricks that restrict the kinds of features that can be learned.\n10\u201cPies\u201d is the common name used for these sprites by Donkey Kong players; the designers\u2019 intent was to depict troughs of cement.\n(yellow) are also clearly identified. Though M-tIBP has slightly larger RMSE than LG-tIBP on this dataset, the features seems more intuitive.\nIn the Super Mario dataset, while LG-tIBP extracted the bush and brick clearly, M-tIBP managed to extract the text \u201c100\u201d, denoting points earned by the player (green). SPRITE performs poorly, possibly due to the large, sparsely observed feature set. M-tIBP identified the blue sky as two parts: one is the red feature and the other is the green feature. Because bricks often appear in the center of the screen, the model learns to \u201cocclude\u201d that location with a patch of sky.\nWhile LG-tIBP and M-tIBP can learn features and transformations, M-tIBP is, on the whole, more accurate and the reconstructions are clearer. SPRITE can generally reconstruct data as well as M-tIBP, but the extracted features are less clear. One possible reason is that SPRITE assumes all\nfeatures are present in each image. Moreover, in practice it is difficult to know a priori the number of features in a dataset. These two factors mean SPRITE is unlikely to scale to heterogeneous datasets such as SMB."}, {"heading": "7. Discussion and Future Work", "text": "We have presented two nonparametric latent feature models for real-valued images, and presented a novel and efficient inference scheme. In this section, we discuss further applications of this inference paradigm, and discuss possible extensions to our models.\nExploitation of Pattern Matching Algorithms This inference scheme uses scoring functions from classical image analysis as the proposal distribution in a MetropolisHastings algorithm and combines the robustness and compu-\ntational appeal of a well-established pattern recognition tool with the flexibility of probabilistic models. This approach, or similar methods based on other classical pattern recognition techniques (Tu & Zhu, 2002; Tu et al., 2005), can be applied across a range of Bayesian models to improve inference in large state spaces.\nAn alternative to modeling images as real-valued vectors is to use image codewords (Li Fei-Fei & Perona, 2005). Other techniques have used transformed Bayesian nonparametric models to build high-performing vision systems using fixed codewords (Sudderth et al., 2005); a combination of these models would allow for a joint model to infer transformations, codewords, and feature cooccurrence patterns.\nRotation and scaling are implemented by extending the space for our cross-correlation-based proposal distribution. One avenue for future work is to investigate how existing non-statistical models for pattern recognition can sample a broder class of transformations using Metropolis-Hastings.\nAdditional Modeling Directions Features can appear more than once in an image, contrary to the assumptions of the tIBP. One avenue for future work is to extend the model to allow multiple instances of a feature in a given image. The infinite gamma-Poisson process (Titsias, 2007) is a distribution over infinite non-negative integer valued matrices. It has been used for image modeling, but that application required presegmentation of images. This work would allow extension to non-segmented images.\nAs in the original tIBP paper, we assumed that transformations associated with each (data point, feature) pair are sampled i.i.d. from some distribution f(r) over possible transformations. One possible avenue for future research is to allow correlations (e.g., over time) between transformations in different images, leading to an image tracking model, which also leads to more efficient inference, by restricting the range in which an feature can appear in the tth to a neighborhood of the feature\u2019s location in the (t\u2212 1)th image. This idea was used to speed up inference in the SPRITE implementation of Titsias & Williams (2006). Incorporating spatial information into the mask distribution would also lead to more coherent feature appearances and counteract some of the \u201cspotty\u201d features observed for MtIBP.\nIn addition, a more informative prior on the features ak could be used to encode domain-specific knowledge\u2014for example, for data comparable to the Walker video, one might make use of vertically oriented ellipses to find humanshaped features."}, {"heading": "8. Acknowledgments", "text": "The authors would like to thank Joseph Austerweil, Frank Wood, Finale Doshi-Velez and Michalis K. Titsias for publishing implementations. This research was supported by NSF grant #1018625. Jordan Boyd-Graber is also supported by the Army Research Laboratory through ARL Cooperative Agreement W911NF-09-2-0072. Sinead Williamson is supported by NIH grant #R01GM087694 and AFOSR grant #FA9550010247. Any opinions, findings, conclusions, or recommendations expressed are the authors\u2019 and do not necessarily reflect those of the sponsors."}], "references": [{"title": "Learning invariant features using the transformed Indian buffet process", "author": ["J.L. Austerweil", "T.L. Griffiths"], "venue": "In NIPS,", "citeRegEx": "Austerweil and Griffiths,? \\Q2010\\E", "shortCiteRegEx": "Austerweil and Griffiths", "year": 2010}, {"title": "The Indian buffet process: Scalable inference and extensions", "author": ["F. Doshi-Velez"], "venue": "Master\u2019s thesis, University of Cambridge,", "citeRegEx": "Doshi.Velez,? \\Q2009\\E", "shortCiteRegEx": "Doshi.Velez", "year": 2009}, {"title": "Pattern classification and scene analysis", "author": ["R.O. Duda", "P.E. Hart"], "venue": null, "citeRegEx": "Duda and Hart,? \\Q1973\\E", "shortCiteRegEx": "Duda and Hart", "year": 1973}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "In NIPS,", "citeRegEx": "Griffiths and Ghahramani,? \\Q2005\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2005}, {"title": "Learning flexible sprites in video layers", "author": ["N. Jojic", "B.J. Frey"], "venue": "In CVPR,", "citeRegEx": "Jojic and Frey,? \\Q2001\\E", "shortCiteRegEx": "Jojic and Frey", "year": 2001}, {"title": "Infinite sparse factor analysis and infinite independent component analysis", "author": ["D. Knowles", "Z. Ghahramani"], "venue": "In ICA,", "citeRegEx": "Knowles and Ghahramani,? \\Q2007\\E", "shortCiteRegEx": "Knowles and Ghahramani", "year": 2007}, {"title": "A Bayesian hierarchical model for learning natural scene categories", "author": ["Li Fei-Fei", "Perona", "Pietro"], "venue": "In CVPR,", "citeRegEx": "Fei.Fei et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2005}, {"title": "Describing visual scenes using the transformed Dirichlet process", "author": ["E.B. Sudderth", "A. Torralba", "W.T. Freeman", "A.S. Willsky"], "venue": "In NIPS,", "citeRegEx": "Sudderth et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sudderth et al\\.", "year": 2005}, {"title": "The infinite gamma-Poisson feature model", "author": ["M. Titsias"], "venue": "In NIPS,", "citeRegEx": "Titsias,? \\Q2007\\E", "shortCiteRegEx": "Titsias", "year": 2007}, {"title": "Sequential learning of layered models from video", "author": ["M.K. Titsias", "C.K.I. Williams"], "venue": "In Toward Category-Level Object Recognition,", "citeRegEx": "Titsias and Williams,? \\Q2006\\E", "shortCiteRegEx": "Titsias and Williams", "year": 2006}, {"title": "Image segmentation by data-driven Markov chain Monte Carlo", "author": ["Z. Tu", "Zhu", "S.-C"], "venue": "TPAMI, 24:657\u2013673,", "citeRegEx": "Tu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2002}, {"title": "Image parsing: Unifying segmentation, detection, and recognition", "author": ["Z. Tu", "X. Chen", "A.L. Yuille", "S.C. Zhu"], "venue": "In ICCV,", "citeRegEx": "Tu et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2005}, {"title": "Representing moving images with layers", "author": ["J.Y.A. Wang", "E.H. Adelson"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Wang and Adelson,? \\Q1994\\E", "shortCiteRegEx": "Wang and Adelson", "year": 1994}, {"title": "A non-parametric Bayesian method for inferring hidden causes", "author": ["F. Wood", "T.L. Griffiths", "Z. Ghahramani"], "venue": "In UAI,", "citeRegEx": "Wood et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wood et al\\.", "year": 2006}, {"title": "Dependent hierarchical beta processes for image interpolation and denoising", "author": ["M. Zhou", "H. Yang", "G. Sapiro", "D. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "In the original tIBP paper, features were generated and combined using noisy-OR (Wood et al., 2006); we refer to this model as the noisy-OR tIBP (NO-tIBP), which allows the same feature to appear in different locations, scales, and orientations.", "startOffset": 80, "endOffset": 99}, {"referenceID": 14, "context": "This model can be extended by adding weights to the non-zero elements of the IBP-distributed matrix (Knowles & Ghahramani, 2007) and incorporating a spiky noise model (Zhou et al., 2011) appropriate for corrupted images.", "startOffset": 167, "endOffset": 186}, {"referenceID": 1, "context": "The hyperparameters \u03b1, \u03c3x and \u03c3a can be Gibbs sampled via closed form equations (Doshi-Velez, 2009).", "startOffset": 80, "endOffset": 99}, {"referenceID": 11, "context": "This approach, or similar methods based on other classical pattern recognition techniques (Tu & Zhu, 2002; Tu et al., 2005), can be applied across a range of Bayesian models to improve inference in large state spaces.", "startOffset": 90, "endOffset": 123}, {"referenceID": 7, "context": "Other techniques have used transformed Bayesian nonparametric models to build high-performing vision systems using fixed codewords (Sudderth et al., 2005); a combination of these models would allow for a joint model to infer transformations, codewords, and feature cooccurrence patterns.", "startOffset": 131, "endOffset": 154}, {"referenceID": 8, "context": "The infinite gamma-Poisson process (Titsias, 2007) is a distribution over infinite non-negative integer valued matrices.", "startOffset": 35, "endOffset": 50}, {"referenceID": 8, "context": "This idea was used to speed up inference in the SPRITE implementation of Titsias & Williams (2006). Incorporating spatial information into the mask distribution would also lead to more coherent feature appearances and counteract some of the \u201cspotty\u201d features observed for MtIBP.", "startOffset": 73, "endOffset": 99}], "year": 2012, "abstractText": "Latent feature models are attractive for image modeling, since images generally contain multiple objects. However, many latent feature models ignore that objects can appear at different locations or require pre-segmentation of images. While the transformed Indian buffet process (tIBP) provides a method for modeling transformation-invariant features in unsegmented binary images, its current form is inappropriate for real images because of its computational cost and modeling assumptions. We combine the tIBP with likelihoods appropriate for real images and develop an efficient inference, using the crosscorrelation between images and features, that is theoretically and empirically faster than existing inference techniques. Our method discovers reasonable components and achieve effective image reconstruction in natural images.", "creator": "LaTeX with hyperref package"}}}