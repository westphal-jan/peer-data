{"id": "1206.6484", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Apprenticeship Learning for Model Parameters of Partially Observable Environments", "abstract": "We consider apprenticeship learning, i.e., having an agent learn a task by observing an expert demonstrating the task in a partially observable environment when the model of the environment is uncertain or unreliable, or a teacher may have an incomplete understanding of the task. We also recommend that apprenticeship learning become more frequent and more frequent. For example, when learners are taught in a group of classes, when a teacher is teaching to the child, the teachers may be less likely to say that the child is working a difficult task when teachers are teaching to the child. Our primary purpose is to learn and understand the task in a way that is similar to learning the task in a group of classes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (138kb)", "http://arxiv.org/abs/1206.6484v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.AI stat.ML", "authors": ["takaki makino", "johane takeuchi"], "accepted": true, "id": "1206.6484"}, "pdf": {"name": "1206.6484.pdf", "metadata": {"source": "META", "title": "Apprenticeship Learning for Model Parameters of  Partially Observable Environments", "authors": ["Takaki Makino", "Johane Takeuchi"], "emails": ["mak@sat.t.u-tokyo.ac.jp", "johane.takeuchi@jp.honda-ri.com"], "sections": [{"heading": "1. Introduction", "text": "Learning from Demonstration (LfD) is a framework for learning to perform a complex task by observing demonstration (task execution) by an expert (Argall et al., 2009). LfD is particularly useful for domains where the expert knowledge of the domain is limited or difficult to represent, because demonstrations are much easier than designing a controller for the task.\nApprenticeship Learning via Inverse Reinforcement Learning (Abbeel & Ng, 2004), which is an application of LfD for reinforcement learning, is an algorithm that learns the reward function of the environment un-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nder the assumption that the expert is trying to maximize the reward. The idea is that, although reinforcement learning can produce an optimal policy with respect to a given reward function, designing a reward function that captures the desired task behavior is not always obvious and requires expert knowledge of the domain. Moreover, learning the reward function from demonstration requires much less amount of demonstration compared to learning the policy directly from the demonstration, because the reinforcement learning combines the reward function with the environment model for optimizing the policy for future rewards. Inverse reinforcement learning is successfully applied to tasks where the environment is fully observable, including aerobatic helicopter flight (Abbeel et al., 2010), robot hand control (Boularias et al., 2011) and prediction of linguistic structures (Neu & Szepesva\u0301ri, 2009). Inverse reinforcement learning in partially observable environments when an exact model is available has also been studied (Ziebart et al., 2010; Henry et al., 2010; Choi & Kim, 2011).\nHowever, the design bottleneck is not limited to the reward function. In many tasks, how to model the environment is not obvious as well, and requires expert knowledge of the domain, especially when the environment is partially observable. For example, dialogue system tasks are often represented as a Partially Observable Markov Decision Process (POMDP) in which the user\u2019s mental state is situated as a hidden state (Williams et al., 2005; Kim et al., 2008; Meguro et al., 2010), but designing such a model requires a considerable amount of work by domain experts, such as annotating dialogue corpus. Thus, there is a need for a way to estimate uncertain parameters of an environment model from non-annotated demonstration data.\nOne obvious way to estimate environmental parameters from the demonstration is to extract the environmental reaction to the expert\u2019s action (Thomson et al.,\n2010). In case of POMDP environment, This reduces the problem into a parameter estimation of an InputOutput Hidden Markov Model (IO-HMM) (Bengio & Frasconi, 1996) (Fig. 1(a)). However, this approach assumes nothing about the demonstrator, and it is applicable to cases where the demonstration is generated from the learning agent or even from a naive random policy. Our claim is that demonstration by an expert contains much richer information about the environment that comes from the expert\u2019s knowledge, and by extracting this information, we can reduce the burden of designing a model suitable for the task.\nOur proposal is to apply the framework of apprenticeship learning to estimate uncertain parameters of the environment (Fig. 1(b))1 . Assuming that the expert\u2019s behavior is based on a stochastic optimal policy with knowledge of the perfect POMDP model for the target environment, we can extract the expert\u2019s knowledge regarding the POMDP parameters from his demonstration. The extracted information of expert knowledge can be combined with IO-HMM estimation from the environmental response, to provide a better estimate of the POMDP parameters.\nWe present two straightforward estimation algorithms, maximum a posteriori (MAP) estimator and posterior sampler by Markov chain Monte Carlo (MCMC), combined with planning algorithms to achieve modelparameter apprenticeship learning In the experiments with short demonstrations, we show that our algorithms can achieve more accurate estimates of\n1We can use this approach for any environment model, such as fully-observable MDPs. However, the most effective cases are for POMDPs because they are hard to be learned from environmental reactions, so we focus on POMDPs in this paper.\nPOMDP parameters and better policies than can existing methods based on IO-HMM estimation."}, {"heading": "2. POMDP and its Parameterization", "text": "An Input-Output Hidden Markov Model (IO-HMM) (Bengio & Frasconi, 1996) is a framework for representing environments consisting of hidden states, inputs (actions that may affect the states), and outputs (observations from the states). Formally, an IO-HMM is defined as a tuple \u27e8S,A,Z, T,O, b0\u27e9, where S is the finite set of states, A is the finite set of actions, Z is the finite set of observations, T is the state transition function such that T (s, a, s\u2032) denotes probability P (s\u2032|s, a) of changing to state s\u2032 by taking action a at state s, O is the observation function such that O(a, s, z) denotes probability P (z|a, s) of perceiving observation z as a result of taking action a and arriving in state s, and b0 is the vector of initial state distribution such that b0(s) denotes the probability of starting in state s.\nSince the true state is hidden, we construct a belief about the state. We denote belief by a vector b where b(s) denotes the probability that the state is s at the current time step. The following update formula can be used to calculate the belief baz for the next time step from the belief at the current time step, given the action a at the current time step and the observation z at the next time step:\nbaz(s \u2032) \u221d O(a, s\u2032, z) \u2211 s T (s, a, s\u2032)b(s) . (1)\nA partially observable Markov decision process (POMDP) is a formulation of an action selection problem on an IO-HMM. A POMDP is defined as a tuple P = \u27e8S,A,Z, T,O, b0, R, \u03b3\u27e9, where S,A,Z, T,O, b0 are\ndefined as in the IO-HMM, R is the reward function so that R(s, a) denotes the immediate reward of taking action a in state s, and \u03b3 \u2208 [0, 1) is the discount factor. The goal of an agent is to maximize the expected discounted total reward E[ \u2211\u221e t=0 \u03b3\ntR(st, at)] by choosing a policy.\nSince the true state is hidden, a policy of agent action must be defined over past actions and observations. If a POMDP is specified, we can use a belief as a sufficient statistic of past actions and observations, where \u03c0(b, a) = P (a|b) is a probability of taking action a at belief b. A policy \u03c0 induces a value function V\u03c0(b) that represents the expected discounted total reward of executing policy \u03c0 starting from b. It is known (Smallwood & Sondik, 1973) that V\u03c0\u2217 , the value function associated with the optimal greedy policy \u03c0\u2217, can be approximated with an arbitrary accuracy by a convex, piecewise-linear function\nQ(b, a) = max \u03b1\u2208\u0393(a) (\u03b1 \u00b7 b) V (b) = max a Q(b, a) , (2)\nwhere \u0393(a) is a finite set of vectors called \u03b1-vectors associated to action a, and \u03b1 \u00b7b is the inner product of a \u03b1-vector and vector b. We consider soft-max policy for a given set of \u03b1-vectors:\n\u03c0\u0303(b, a) = exp(\u03b2Q(b, a))\u2211 a\u2032 exp(\u03b2Q(b, a \u2032)) (3)\nwhere \u03b2 is the inverse temperature parameter that controls the orderedness of the policy. We denote the softmax policy from the optimal action-value function Q\u2217 as soft-max optimal policy \u03c0\u0303\u2217.\nIn general, computing an approximately optimal solution within a given error bound \u03f5 is NP-hard. However, it is known that given a set of balls of radius \u03b4 \u2264 O(\u03f5) over beliefs that cover an optimal reachable space, an approximated solution can be computed in a polynomial time (Hsu et al., 2008). SARSOP (Kurniawati et al., 2008) is one of approximated POMDP solvers that implements elaborated point selection and a pruning algorithm.\nIn this paper, we consider situations in which some part of the environment model is uncertain. We introduce a K-element parameter vector \u03b8 with its prior distribution p(\u03b8), and consider a POMDP P\u03b8 = \u27e8S,A,Z, T\u03b8, O\u03b8, b0,\u03b8, R\u03b8, \u03b3\u27e9, where T , O, b0 and R are determined according to the given parameter \u03b8. An L-length sequence D = (a1z1 \u00b7 \u00b7 \u00b7 aLzL) of demonstration by an expert is given, assuming that the expert knows \u03b8true, the true parameter of the environment, and is following a soft-max optimal policy \u03c0\u0303\u2217\u03b8true under POMDP P\u03b8true with inverse temperature \u03b2.2 What we\n2For notational simplicity we assume \u03b2 is known and\nwant is to calculate p(\u03b8|D), the posterior distribution of the parameter, and to find an optimal policy over the posterior."}, {"heading": "3. Inferring Posterior", "text": "Bayes\u2019 theorem gives posterior distribution p(\u03b8|D) of parameter \u03b8 given demonstration D = (a1z1 \u00b7 \u00b7 \u00b7 aLzL):\np(\u03b8|D) \u221d p(D|\u03b8)p(\u03b8) . (4)\nLikelihood p(D|\u03b8) of the demonstration is the result of marginalizing expert\u2019s policy \u03c0:\np(D|\u03b8) = \u222b p(D|\u03b8, \u03c0)p(\u03c0|\u03b8)d\u03c0 . (5)\nNote that, from our assumption, the expert\u2019s policy \u03c0\u03b8 is equal to the soft-max optimal policy \u03c0\u0303 \u2217 \u03b8 for the POMDP P\u03b8 with parameter \u03b8, thus p(\u03c0 = \u03c0\u0303\u2217\u03b8|\u03b8) = 1.\nWe can further refactor the likelihood p(D|\u03b8, \u03c0\u03b8) as follows:\np(D|\u03b8, \u03c0) = p(a1|\u03b8, \u03c0)p(z1|\u03b8, a1)p(a2|\u03b8, \u03c0, a1z1) \u00b7 \u00b7 \u00b7 (6) = p(a1 \u00b7 \u00b7 \u00b7 aL|\u03b8, \u03c0, z1 \u00b7 \u00b7 \u00b7 zL\u22121)\n\u00b7 p(z1 \u00b7 \u00b7 \u00b7 zL|\u03b8, a1 \u00b7 \u00b7 \u00b7 aL) . (7)\nThe first factor of Eq. 7 corresponds to the likelihood that the expert performs action ai given the policy \u03c0\u03b8:\np(a1 \u00b7 \u00b7 \u00b7 aL|\u03c0, z1 \u00b7 \u00b7 \u00b7 zL\u22121) = L\u220f\ni=1\n\u03c0(bi,\u03b8,D, ai) , (8)\nwhere bi,\u03b8,D is the belief at time step i in a POMDP P\u03b8 with history D, calculated by applying Eq. 1 repeatedly to b0,\u03b8.\nOn the other hand, the second factor corresponds to the likelihood that the environment responds with observation zi to the performed actions:\np(z1 \u00b7 \u00b7 \u00b7 zL|\u03b8, a1 \u00b7 \u00b7 \u00b7 aL) = L\u220f\ni=1 \u2211 s\u2208S b\u03b8,D,i\u22121(s) \u2211 s\u2032\u2208S T (s, ai, s \u2032)O(ai, s \u2032, zi) . (9)\nTo our knowledge, previous studies that use the benefit of the first factor in the inference of parameter \u03b8 only consider the change of the reward function. In cases where only the rewards are uncertain, the inference is relatively easy since the value function V\u03c0 for a given policy is given as a linear function of the reward\nfixed. However it is easy to apply our methods to cases with unknown \u03b2, because it is equivalent to a fixed \u03b2 with an unknown scaling parameter of the reward function.\nvalues (Ramachandran & Amir, 2007). However, if we consider cases where transition and observation probabilities are uncertain, the inference is complex because of the nonlinear dependence between the parameters and the value function."}, {"heading": "3.1. Maximum A Posteriori Inference", "text": "Maximum a posteriori (MAP) inference is to find \u03b8 that maximizes the posterior (Eq. 4). Unfortunately, it is not easy to use sophisticated optimization techniques using gradients because changes in beliefs complicates obtaining gradients for either factor in Eq. 7. This is the major difference from the setting of inverse reinforcement learning, in which we can evaluate the gradient of expert action likelihood, and the observation likelihood is constant given D.\nWe take a straightforward approach to optimization by using the COBYLA algorithm (Powell, 1998), which does not require gradients. For each candidate parameter value \u03b8, we call a POMDP solver for POMDP P\u03b8 to obtain the optimal action-value function Q\u2217, which gives the soft-max optimal policy \u03c0\u0303\u2217\u03b8 for the POMDP that is used for evaluating expert action likelihood (Eq. 8). As for the observation likelihood (Eq. 9), we apply the standard forward algorithm for IO-HMM to POMDP P\u03b8 and sequence D.\nThis algorithm has no guarantee to find the MAP parameter because it is based on a local search. However, in practice it seems to find a reasonably good solution, and calculation is quick compared to the sampling approach which we will describe next."}, {"heading": "3.2. Inference by Sampling", "text": "We also employ a Markov chain Monte Carlo (MCMC) sampling approach (Gilks et al., 1996) to infer the posterior distribution of \u03b8. Unlike MAP inference, The approximation calculated by MCMC can be arbitrarily accurate with a sufficient computational time.\nThe traditional way of sampling parameters for IOHMM is to use the Markov chain Monte Carlo approach; that is by alternately sampling the hidden state sequence s given parameter \u03b8, and \u03b8 given s. By using a conjugate prior for the parameters, we can easily sample \u03b8 from the posterior given s.\nTo make the sample distribution follow the expert action likelihood, we introduce the Metropolis algorithm (Metropolis et al., 1953), which accepts the proposed sample \u03b8\u2032 with the probability min(1, p\u2032/p), where p\u2032 and p are the expert action likelihood for the proposed sample and for the previous sample, respectively.\nAlgorithm 1 MCMC sampler for posterior p(\u03b8|D) Require: D: demonstration 1: sample \u03b8 from the prior 2: p := infinitesimal positive value 3: loop 4: sample s = (s1 \u00b7 \u00b7 \u00b7 sL) from p(s|D,\u03b8) 5: for k := 1 to K, in random order do 6: \u03b8\u2032 := \u03b8 7: replace \u03b8\u2032k by a sampled value from IO-HMM\nposterior given s, D 8: call POMDP solver to find \u03c0\u0303\u2217\u03b8\u2032 9: p\u2032 := p(a1 \u00b7 \u00b7 \u00b7 aL|\u03c0\u0303\u2217\u03b8\u2032 , z1 \u00b7 \u00b7 \u00b7 zL) {Eq. 8}\n10: if with probability min(1, p\u2032/p) then 11: p := p\u2032, \u03b8 := \u03b8\u2032 {accept the sample} 12: end if 13: end for 14: end loop\nAlgorithm 1 shows the sampler for the posterior of model parameters. The algorithm is similar to the sampling algorithm for model parameters from IOHMM posterior, which deals with only the likelihood of an environmental response (the second term in Eq. 7). The difference lies in lines 8\u201312, that performs Metropolis algorithm for the expert action likelihood (the first term in Eq. 7). We run the algorithm until specified numberM of samples are collected, excluding burn-ins and interval samples."}, {"heading": "4. Planning with a Sampled Posterior", "text": "Our goal is to achieve model-parameter apprenticeship learning; that is, to make an optimal policy for the learned posterior of POMDP model parameters. In this section, we describe how to produce an optimal policy based on the sampled results. Note that, in case of a MAP estimate, we can obtain a policy by applying a solver to POMDP P\u03b8\u0302 with estimated parameter \u03b8\u0302.\nExisting planning methods for POMDP with Bayesian uncertainty (Ross et al., 2008) are not applicable, because they require that the uncertainty be represented in conjugate priors, which cannot represent the posterior distribution of parameters after observing demonstration. Instead, we employed a method to develop a POMDP policy based on the sampled parameters. The idea is to extend the hidden state of POMDP with a variable m, which is an index of the sampled parameters \u03b8m (m = 1, . . . ,M). At the beginning m is uniformly distributed, and never changes. This extended POMDP can be solved by a standard POMDP solver. We expect that the belief over sample index m converges to the index of the most likely parame-\nter while the agent interacts with the environment. In case the target POMDP is episodic, we want to retain belief overm beyond episodes, so we convert the target POMDP into non-episodic POMDPs before extension.\nFormally, given M sampled parameters \u03b81, . . . ,\u03b8M for the target POMDP, we create an extended POMDP P\u0303 = \u27e8S\u0303, A, Z, T\u0303 , O\u0303, b\u03030, R\u0303, \u03b3\u27e9, where\nS\u0303 = S \u00d7 {1, . . . ,M} O\u0303([s,m], a, z) = O\u03b8m(s, a, z) b\u03030([s,m]) = b0,\u03b8m(s)/M R\u0303([s,m], a) = R\u03b8m(s, a)\nT\u0303 ([s,m], a, [s\u2032,m\u2032]) =\n{ T\u03b8m(s, a, s \u2032) m = m\u2032\n0 m \u0338= m\u2032 .\nNote that the optimal policy of the extended POMDP becomes a good policy in the target POMDP only if the samples represent the target well. If we need a agent that learns by exploring the uncertainty in the target POMDP, we will need scheduled resampling as has been done in fully observable environments by the BOSS algorithm (Asmuth et al., 2009). In this paper we chose not to resample, because our purpose is to evaluate the posterior distribution p(\u03b8|D) obtained from demonstration."}, {"heading": "5. Experiments", "text": "To evaluate the proposed model-parameter apprenticeship learning algorithms, we performed experiments with two tasks: one is a simple environment based on the well-known Tiger problem (Kaelbling et al., 1998), and the other is a task designed for a dialog system. In the following experiments, we used APPL Toolkit which implements the SARSOP algorithm (Kurniawati et al., 2008) as a POMDP solver. We used COBYLA implementation in the NLopt library (Johnson, 2008)."}, {"heading": "5.1. Bayesian Tiger Problem", "text": "We introduced four unknown parameters to the Tiger problem, whose prior is represented as pi \u223c Beta(3, 3), pl, pr \u223c Beta(5, 3), rt \u223c N (\u221250, 502) as follows. An agent is standing in front of two doors. A tiger is hidden behind the left door with probability pi and behind the right door with probability 1 \u2212 pi. The agent can open one of the doors, and obtain reward rt if the agent sees the tiger and reward 10 otherwise. Alternatively, the agent can choose to listen with reward \u22121: if the tiger is behind the left door, the agent hears the tiger from the left with probability pl and from the right with probability 1 \u2212 pl; if the tiger is behind the right door, the agent hears it from the right with probability pr and from the left with probability 1\u2212 pr.\nWe set the true environment as pi = 0.6, pl = pr = 0.85 and rt = \u2212100, and we used \u03b3 = 0.9. We generated 100 demonstrations by the experts with soft-max policy \u03b2 = 0.3, each consisting of 100 steps of actions and observations (which contained 22 episodes on average). For each demonstration, we applied one of the learning algorithms to the demonstration to estimate the posterior. From the estimated posterior, an optimal greedy policy was derived, and tested by simulating 100,000 steps in the true environment, and the average reward was measured. For sampling algorithms, 1,000 MCMC steps were performed including 100-step burn-in, and parameters were sampled for every 10 steps (total M = 90 samples) to generate a greedy policy.\nTable 1 shows the distribution of the estimated parameters. Both of the proposed methods produce more accurate estimates of parameters compared to the meth-\nods based on IO-HMM3. We can see that the proposed methods provide better RMSE than do the IO-HMM methods. The estimates from IO-HMM methods are closer to the prior mean, suggesting that the provided demonstration is too short to obtain an accurate estimate. On the other hand, the estimates from the proposed methods are closer to the true value, which indicates that the proposed methods provide a better estimate using the same length of demonstration. We can also see that the proposed sampler produces a narrower posterior distribution (i.e., smaller standard deviation of the samples) than that of the IO-HMM\n3Note that we compare only state transition parameters because the rewards cannot be estimated by IO-HMM methods.\nsampler.\nAs shown in Figure 2, having an accurate estimate leads to better results in simulation by the learned policy. The results of the policies based on estimated posterior with our methods are not much worse than those of the expert policy who knows the true parameter values. On the other hand, policies based on IOHMM estimation occasionally result in very bad policies, as shown in \u201cLess\u201d average rewards in the figure. Considering that the demonstration is short and noisy, these results indicate that the model-parameter apprenticeship learning methods prevent agents from critical failures in learning to follow the demonstrated task."}, {"heading": "5.2. Dialog System", "text": "To show the effectiveness of our methods in a more realistic scenario, we developed a new task of dialog management for a ticket-vending system. A user asks the agent for a ticket with a certain origin and destination via an unreliable voice recognition interface; the task of the agent is to repeat the order correctly, and issue the ticket. We expect that the expert demonstration is useful to determine parameters, which represents user\u2019s preferred ticket routes and way of talking.\nThe task consists of 13 observations from voice recognition, including three place names and SIL (silence). The agent can choose from 11 actions, consisting of uttering one of 9 words, waiting for next word from the user, or issuing a ticket. The dialog is managed by a 32-state POMDP (Fig. 3) for each of 3\u00d72 = 6 ticket routes, resulting in the total of 192 hidden states.\nThe POMDP is parameterized with a 15-dimensional vector \u03b8; 4 parameters are assigned to route preferences (initial state distribution), 9 to ways of talking (transition probabilities), and 2 to voice recognition errors (observation probabilities). The agents are required to estimate the parameters from a 300-step demonstration generated by an expert. In the experiments we didn\u2019t use samplers since they require too much computational resources.\nWe generated 12 demonstrations by the experts (the result of solving the true POMDP model) with softmax policy \u03b2 = 0.4, each consisting of 300 steps of actions and observations (which contained 27 episodes on average). Using the learned parameters, we applied SARSOP POMDP solver to obtain a greedy policy, and measured average reward by testing the policy on the original environment. Since calculating the exact solution of the POMDP is too expensive, we set the timeout of 40 CPU seconds for each parameter can-\ndidate during MAP search, and 600 CPU seconds for calculating the expert policy and the final policy based on the estimated parameters;\nFigure 4 shows the results. We found that the agents based on the parameters estimated by the proposed MAP algorithm perform significantly better than the agents based on the parameters estimated by IO-HMM (P < .05). However, in this setting, we couldn\u2019t obtain the expert-level performance by the apprenticeship learning. One possible reason is that the optimization algorithm is disturbed by the approximation error of expert action likelihood, which is caused by the short timeout of the POMDP solver and random searching strategy of SARSOP. We believe that the\nresult can be improved if we use more computational resources; or, if we use a POMDP solver that can be started from the result of a similar POMDP, we may be able to improve the optimization process."}, {"heading": "6. Conclusion", "text": "We have shown that the apprenticeship learning approach can be used to estimate parameters of an unknown POMDP environment. Assuming that an expert knowing the perfect POMDP model of the target environment will try to maximize the reward, we can extract the expert\u2019s knowledge about the environment from his demonstration in terms of the posterior distribution of unknown parameters. Our proposed algorithms are simple but are capable of estimating POMDP parameters accurately even if the demonstration is short. We also showed that the extracted knowledge can be used to develop a policy that can act reasonably well in the target environment.\nOur approach is a generalization of inverse reinforcement learning, in a sense that the unknown parameters are not limited to those for the reward function but can also be for transition and observation functions. This approach can be particularly useful in the domain of applications that interact with human beings, whose model is unknown but demonstration by experts is available. One direct extension of the approach is to estimate other parameters, such as the discount factor of the expert, from the demonstration. Future work should also include the development of more efficient algorithms as has been done in the context of inverse reinforcement learning."}, {"heading": "Acknowledgment", "text": "This research is supported by the Aihara Innovative Mathematical Modelling Project, the Japan Society for the Promotion of Science (JSPS) through the \u201cFunding Program for World-Leading Innovative R&D on Science and Technology (FIRST Program),\u201d initiated by the Council for Science and Technology Policy (CSTP), and by JSPS Grant-in-Aid for Young Scientists (B) (20700126)."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "Proc. of 21st International Conference on Machine Learning (ICML", "citeRegEx": "Abbeel and Ng,? \\Q2004\\E", "shortCiteRegEx": "Abbeel and Ng", "year": 2004}, {"title": "A survey of robot learning from demonstration", "author": ["B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Argall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Argall et al\\.", "year": 2009}, {"title": "A bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M. Littman", "A. Nouri", "D. Wingate"], "venue": "In Proc. of the 25th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Asmuth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "Input-output HMM\u2019s for sequence processing", "author": ["Y. Bengio", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Bengio and Frasconi,? \\Q1996\\E", "shortCiteRegEx": "Bengio and Frasconi", "year": 1996}, {"title": "Relative entropy inverse reinforcement learning", "author": ["A. Boularias", "J. Kober", "J. Peters"], "venue": "Journal of Machine Learning Research: Workshop and Conference Proceedings (AISTATS", "citeRegEx": "Boularias et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boularias et al\\.", "year": 2011}, {"title": "Inverse reinforcement learning in partially observable environments", "author": ["J. Choi", "Kim", "K.-E"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Choi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2011}, {"title": "Learning to navigate through crowded environments", "author": ["P. Henry", "C. Vollmer", "B. Ferris", "D. Fox"], "venue": "In Proc. of 2010 IEEE International Conference of Robotics and Automation (ICRA", "citeRegEx": "Henry et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Henry et al\\.", "year": 2010}, {"title": "What makes some POMDP problems easy to approximate", "author": ["D. Hsu", "W.S. Lee", "N. Rong"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hsu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2008}, {"title": "The NLopt nonlinear-optimization package", "author": ["S.G. Johnson"], "venue": "http://ab-initio.mit.edu/nlopt,", "citeRegEx": "Johnson,? \\Q2008\\E", "shortCiteRegEx": "Johnson", "year": 2008}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "A frame-based probabilistic framework for spoken dialog management using dialog examples", "author": ["K. Kim", "C. Lee", "S. Jung", "G.G. Lee"], "venue": "In Proc. of the 9th SIGdial Workshop on Discourse and Dialogue,", "citeRegEx": "Kim et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2008}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S. Lee"], "venue": "In Proc. Robotics: Science and Systems,", "citeRegEx": "Kurniawati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "Controlling Listening-oriented Dialogue using Partially Observable Markov Decision Processes", "author": ["T. Meguro", "R. Higashinaka", "Y. Minami", "K. Dohsaka"], "venue": "In Proc. of the 23rd International Conference on Computational Linguistics (COLING", "citeRegEx": "Meguro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Meguro et al\\.", "year": 2010}, {"title": "Equations of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "Journal of Chemical Physics,", "citeRegEx": "Metropolis et al\\.,? \\Q1953\\E", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "Training parsers by inverse reinforcement learning", "author": ["G. Neu", "C. Szepesv\u00e1ri"], "venue": "Machine Learning,", "citeRegEx": "Neu and Szepesv\u00e1ri,? \\Q2009\\E", "shortCiteRegEx": "Neu and Szepesv\u00e1ri", "year": 2009}, {"title": "Direct search algorithms for optimization calculations", "author": ["M.J.D. Powell"], "venue": "Acta Numerica,", "citeRegEx": "Powell,? \\Q1998\\E", "shortCiteRegEx": "Powell", "year": 1998}, {"title": "Bayesian inverse reinforcement learning", "author": ["D. Ramachandran", "E. Amir"], "venue": "In Proc. of International Joint Conference of Artifical Intelligence", "citeRegEx": "Ramachandran and Amir,? \\Q2007\\E", "shortCiteRegEx": "Ramachandran and Amir", "year": 2007}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R. Smallwood", "E. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Parameter learning for POMDP spoken dialogue models", "author": ["B. Thomson", "F. Jur\u010d\u0301\u0131\u010dek", "M. Ga\u0161i\u0107", "S. Keizer", "F. Mairesse", "K. Yu", "S. Young"], "venue": "In Proc. of the 3rd IEEE Workshop on Spoken Language Technology (SLT", "citeRegEx": "Thomson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thomson et al\\.", "year": 2010}, {"title": "Partially observable Markov decision processes with continuous observations for dialogue management", "author": ["J.D. Williams", "P. Poupart", "S. Young"], "venue": "In Proc. of the 6th SIGdial Workshop on Discourse and Dialogue,", "citeRegEx": "Williams et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Williams et al\\.", "year": 2005}, {"title": "Modeling interaction via the principle of maximum causal entropy", "author": ["B. Ziebart", "J.A. Bragnell", "A.K. Dey"], "venue": "Proc. of the 27th International Conference on Machine Learning (ICML", "citeRegEx": "Ziebart et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "Learning from Demonstration (LfD) is a framework for learning to perform a complex task by observing demonstration (task execution) by an expert (Argall et al., 2009).", "startOffset": 145, "endOffset": 166}, {"referenceID": 4, "context": ", 2010), robot hand control (Boularias et al., 2011) and prediction of linguistic structures (Neu & Szepesv\u00e1ri, 2009).", "startOffset": 28, "endOffset": 52}, {"referenceID": 20, "context": "Inverse reinforcement learning in partially observable environments when an exact model is available has also been studied (Ziebart et al., 2010; Henry et al., 2010; Choi & Kim, 2011).", "startOffset": 123, "endOffset": 183}, {"referenceID": 6, "context": "Inverse reinforcement learning in partially observable environments when an exact model is available has also been studied (Ziebart et al., 2010; Henry et al., 2010; Choi & Kim, 2011).", "startOffset": 123, "endOffset": 183}, {"referenceID": 19, "context": "For example, dialogue system tasks are often represented as a Partially Observable Markov Decision Process (POMDP) in which the user\u2019s mental state is situated as a hidden state (Williams et al., 2005; Kim et al., 2008; Meguro et al., 2010), but designing such a model requires a considerable amount of work by domain experts, such as annotating dialogue corpus.", "startOffset": 178, "endOffset": 240}, {"referenceID": 10, "context": "For example, dialogue system tasks are often represented as a Partially Observable Markov Decision Process (POMDP) in which the user\u2019s mental state is situated as a hidden state (Williams et al., 2005; Kim et al., 2008; Meguro et al., 2010), but designing such a model requires a considerable amount of work by domain experts, such as annotating dialogue corpus.", "startOffset": 178, "endOffset": 240}, {"referenceID": 12, "context": "For example, dialogue system tasks are often represented as a Partially Observable Markov Decision Process (POMDP) in which the user\u2019s mental state is situated as a hidden state (Williams et al., 2005; Kim et al., 2008; Meguro et al., 2010), but designing such a model requires a considerable amount of work by domain experts, such as annotating dialogue corpus.", "startOffset": 178, "endOffset": 240}, {"referenceID": 7, "context": "However, it is known that given a set of balls of radius \u03b4 \u2264 O(\u03b5) over beliefs that cover an optimal reachable space, an approximated solution can be computed in a polynomial time (Hsu et al., 2008).", "startOffset": 180, "endOffset": 198}, {"referenceID": 11, "context": "SARSOP (Kurniawati et al., 2008) is one of approximated POMDP solvers that implements elaborated point selection and a pruning algorithm.", "startOffset": 7, "endOffset": 32}, {"referenceID": 15, "context": "We take a straightforward approach to optimization by using the COBYLA algorithm (Powell, 1998), which does not require gradients.", "startOffset": 81, "endOffset": 95}, {"referenceID": 13, "context": "To make the sample distribution follow the expert action likelihood, we introduce the Metropolis algorithm (Metropolis et al., 1953), which accepts the proposed sample \u03b8\u2032 with the probability min(1, p\u2032/p), where p\u2032 and p are the expert action likelihood for the proposed sample and for the previous sample, respectively.", "startOffset": 107, "endOffset": 132}, {"referenceID": 2, "context": "If we need a agent that learns by exploring the uncertainty in the target POMDP, we will need scheduled resampling as has been done in fully observable environments by the BOSS algorithm (Asmuth et al., 2009).", "startOffset": 187, "endOffset": 208}, {"referenceID": 9, "context": "To evaluate the proposed model-parameter apprenticeship learning algorithms, we performed experiments with two tasks: one is a simple environment based on the well-known Tiger problem (Kaelbling et al., 1998), and the other is a task designed for a dialog system.", "startOffset": 184, "endOffset": 208}, {"referenceID": 11, "context": "In the following experiments, we used APPL Toolkit which implements the SARSOP algorithm (Kurniawati et al., 2008) as a POMDP solver.", "startOffset": 89, "endOffset": 114}, {"referenceID": 8, "context": "We used COBYLA implementation in the NLopt library (Johnson, 2008).", "startOffset": 51, "endOffset": 66}], "year": 2012, "abstractText": "We consider apprenticeship learning \u2014 i.e., having an agent learn a task by observing an expert demonstrating the task \u2014 in a partially observable environment when the model of the environment is uncertain. This setting is useful in applications where the explicit modeling of the environment is difficult, such as a dialogue system. We show that we can extract information about the environment model by inferring action selection process behind the demonstration, under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment. Proposed algorithms can achieve more accurate estimates of POMDP parameters and better policies from a short demonstration, compared to methods that learns only from the reaction from the environment.", "creator": "LaTeX with hyperref package"}}}