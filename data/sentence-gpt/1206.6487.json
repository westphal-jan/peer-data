{"id": "1206.6487", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "An adaptive algorithm for finite stochastic partial monitoring", "abstract": "We present a new anytime algorithm that achieves near-optimal regret for any instance of finite stochastic partial monitoring. In particular, the new algorithm achieves the minimax regret, within logarithmic factors, for both \"easy\" and \"hard\" problems. For easy problems, it additionally achieves logarithmic individual regret. The new algorithm achieves all the problems in the classical linear equation (TNF) and all possible solutions to these problems, including the \"no-hob\" problems.\n\n\n\n\n\nThe new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation.\n\n\n\nThe new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation.\nThe new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation.\n\nThe new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation.\nThe new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation. The new algorithm improves the accuracy of the regression analysis of the logarithmic equation.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (697kb)", "http://arxiv.org/abs/1206.6487v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.GT stat.ML", "authors": ["g\u00e1bor bart\u00f3k", "navid zolghadr", "csaba szepesv\u00e1ri"], "accepted": true, "id": "1206.6487"}, "pdf": {"name": "1206.6487.pdf", "metadata": {"source": "META", "title": "An adaptive algorithm for finite stochastic partial monitoring", "authors": ["G\u00e1bor Bart\u00f3k", "Navid Zolghadr"], "emails": ["bartok@ualberta.ca", "zolghadr@ualberta.ca", "szepesva@ualberta.ca"], "sections": [{"heading": null, "text": "\u221a T )\nregret in Dynamic Pricing, proven to be hard by Barto\u0301k et al. (2011)."}, {"heading": "1. Introduction", "text": "Partial monitoring can be cast as a sequential game played by a learner and an opponent. In every time step, the learner chooses an action and simultaneously the opponent chooses an outcome. Then, based on the action and the outcome, the learner suffers some loss and receives some feedback. Neither the outcome nor the loss are revealed to the learner. Thus, a partialmonitoring game with N actions and M outcomes is defined with the pair G = (L,H), where L \u2208 RN\u00d7M is the loss matrix, and H \u2208 \u03a3N\u00d7M is the feedback matrix over some arbitrary set of symbols \u03a3. These matrices are announced to both the learner and the opponent before the game starts. At time step t, if It \u2208 N = {1, 2, . . . , N} and Jt \u2208 M = {1, 2, . . . ,M} denote the (possibly random) choices of the learner and the opponent, respectively then, the loss suffered by the learner in that time step is L[It, Jt], while the\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nfeedback received is H[It, Jt].\nThe goal of the learner (or player) is to minimize his cumulative loss \u2211T t=1 L[It, Jt]. The performance of the learner is measured in terms of the regret, defined as the excess cumulative loss he suffers compared to that of the best fixed action in hindsight:\nRT = T\u2211 t=1 L[It, Jt]\u2212min i\u2208N T\u2211 t=1 L[i, Jt] .\nThe regret usually grows with the time horizon T . What distinguishes between a \u201csuccessful\u201d and an \u201cunsuccessful\u201d learner is the growth rate of the regret. A regret linear in T means that the learner does not approach the performance of the optimal action. On the other hand, if the growth rate is sublinear, it is said that the learner can learn the game.\nIn this paper we restrict our attention to stochastic games, adding the extra assumption that the opponent generates the outcomes with a sequence of independent and identically distributed random variables. This distribution will be called the opponent strategy. As for the player, a player strategy (or algorithm) is a (possibly random) function from the set of feedback sequences (observation histories) to the set of actions.\nIn stochastic games, we use a slightly different notion of regret: we compare the cumulative loss with that of the action with the lowest expected loss.\nRT = T\u2211 t=1 L[It, Jt]\u2212 T min i\u2208N E[L[i, J1]] .\nThe \u201chardness\u201d of a game is defined in terms of the minimax expected regret (or minimax regret for short):\nRT (G) = min A max p\u2208\u2206M\nE[RT ] ,\nwhere \u2206M is the space of opponent strategies, and A is any strategy of the player. In other words, the\nminimax regret is the worst-case expected regret of the best algorithm.\nA question of major importance is how the minimax regret scales with the parameters of the game, such as the time horizon T , the number of actions N , the number of outcomes M . In the stochastic setting, another measure of \u201chardness\u201d is worth studying, namely the individual or problem-dependent regret, defined as the expected regret given a fixed opponent strategy."}, {"heading": "1.1. Related work", "text": "Two special cases of partial monitoring have been extensively studied for a long time: full-information games, where the feedback carries enough information for the learner to infer the outcome for any action-outcome pair, and bandit games, where the learner receives the loss of the chosen action as feedback. Since Vovk (1990) and Littlestone & Warmuth (1994) we know that for full-information games, the minimax regret scales as \u0398( \u221a T logN). For bandit games, the minimax regret has been proven to scale as \u0398( \u221a NT ) (Audibert & Bubeck, 2009).1 The individual regret of these kind of games has also been studied: Auer et al. (2002) showed that given any opponent strategy, the expected regret can be upper bounded by c \u2211 i\u2208N :\u03b4i 6=0 1 \u03b4i\nlog T , where \u03b4i is the expected difference between the loss of action i and an optimal action.\nFinite partial monitoring problems were introduced by Piccolboni & Schindelhauer (2001). They proved that a game is either \u201chopeless\u201d (that is, its minimax regret scales linearly with T ), or the regret can be upper bounded by O(T 3/4). They also give a characterization of hopeless games. Namely, a game is hopeless if it does not satisfy the global observability condition (see Definition 5 in Section 2). Their upper bound for non-hopeless games was tightened to O(T 2/3) by Cesa-Bianchi et al. (2006), who also showed that there exists a game with a matching lower bound.\nCesa-Bianchi et al. (2006) posted the problem of characterizing partial-monitoring games with minimax regret less than \u0398(T 2/3). This problem has been solved since then. The first steps towards classifying partialmonitoring games were made by Barto\u0301k et al. (2010), who characterized almost all games with two outcomes. They proved that there are only four categories: games with minimax regret 0, \u0398\u0303( \u221a T ), \u0398(T 2/3), and \u0398(T ), and named them trivial, easy, hard, and hopeless, re-\n1The Exp3 algorithm due to Auer et al. (2003) achieves almost the same regret, with an extra logarithmic term.\nspectively.2 They also found that there exist games that are easy, but can not easily be \u201ctransformed\u201d to a bandit or full-information game. Later, Barto\u0301k et al. (2011) proved the same results for finite stochastic partial monitoring, with any finite number of outcomes. The condition that separates easy games from hard games is the local observability condition (see Definition 6). The algorithm Balaton introduced there works by eliminating actions that are thought to be suboptimal with high confidence. They conjectured in their paper that the same classification holds for nonstochastic games, without changing the condition. Recently, Foster & Rakhlin (2011) designed the algorithm NeighborhoodWatch that proves this conjecture to be true. Foster & Rakhlin prove an upper bound on a stronger notion of regret, called internal regret."}, {"heading": "1.2. Contributions", "text": "In this paper, we extend the results of Barto\u0301k et al. (2011). We introduce a new algorithm, called CBP for \u201cConfidence Bound Partial monitoring\u201d, with various desirable properties. First of all, while Balaton only works on easy games, CBP can be run on any non-hopeless game, and it achieves (up to logarithmic factors) the minimax regret rates both for easy and hard games (see Corollaries 3 and 2). Furthermore, it also achieves logarithmic problem-dependent regret for easy games (see Corollary 1). It is also an \u201canytime\u201d algorithm, meaning that it does not have to know the time horizon, nor does it have to use the doubling trick, to achieve the desired performance.\nThe final, and potentially most impactful, aspect of our algorithm is that through additional assumptions on the set of opponent strategies, the minimax regret of even hard games can be brought down to \u0398\u0303( \u221a T )! While this statement may seem to contradict the result of Barto\u0301k et al. (2011), in fact it does not. For the precise statement, see Theorem 2. We call this property \u201cadaptiveness\u201d to emphasize that the algorithm does not even have to know that the set of opponent strategies is restricted."}, {"heading": "2. Definitions and notations", "text": "Recall from the introduction that an instance of partial monitoring with N actions and M outcomes is defined by the pair of matrices L \u2208 RN\u00d7M and H \u2208 \u03a3N\u00d7M , where \u03a3 is an arbitrary set of symbols. In each round t, the opponent chooses an outcome Jt \u2208M and simultaneously the learner chooses an action It \u2208 N . Then,\n2Note that these results do not concern the growth rate in terms of other parameters (like N).\nthe feedback H[It, Jt] is revealed and the learner suffers the loss L[It, Jt]. It is important to note that the loss is not revealed to the learner.\nAs it was previously mentioned, in this paper we deal with stochastic opponents only. In this case, the choice of the opponent is governed by a sequence J1, J2, . . . of i.i.d. random variables. The distribution of these variables p \u2208 \u2206M is called an opponent strategy, where \u2206M , also called the probability simplex, is the set of all distributions over the M outcomes. It is easy to see that, given opponent strategy p, the expected loss of action i can be expressed as `>i p, where `i is defined as the column vector consisting of the ith row of L.\nThe following definitions, taken from Barto\u0301k et al. (2011), are essential for understanding how the structure of L and H determines the \u201chardness\u201d of a game.\nAction i is called optimal under strategy p if its expected loss is not greater than that of any other action i\u2032 \u2208 N . That is, `>i p \u2264 `>i\u2032 p. Determining which action is optimal under opponent strategies yields the cell decomposition3 of the probability simplex \u2206M :\nDefinition 1 (Cell decomposition). For every action i \u2208 N , let Ci = {p \u2208 \u2206M : action i is optimal under p}. The sets C1, . . . , CN constitute the cell decomposition of \u2206M .\nNow we can define the following important properties of actions:\nDefinition 2 (Properties of actions). \u2022 Action i is called dominated if Ci = \u2205. If an action is not dominated then it is called non-dominated.\n\u2022 Action i is called degenerate if it is nondominated and there exists an action i\u2032 such that Ci ( Ci\u2032 .\n\u2022 If an action is neither dominated nor degenerate then it is called Pareto-optimal. The set of Pareto-optimal actions is denoted by P.\nFrom the definition of cells we see that a cell is either empty or it is a closed polytope. Furthermore, Paretooptimal actions have (M \u2212 1)-dimensional cells. The following definition, important for our algorithm, also uses the dimensionality of polytopes:\nDefinition 3 (Neighbors). Two Pareto-optimal actions i and j are neighbors if Ci \u2229 Cj is an (M \u2212 2)- dimensional polytope. Let N be the set of unordered pairs over N that contains neighboring action-pairs. The neighborhood action set of two neighboring actions i, j is defined as N+i,j = {k \u2208 N : Ci\u2229Cj \u2286 Ck}.\n3The concept of cell decomposition also appears in Piccolboni & Schindelhauer (2001).\nNote that the neighborhood action set N+i,j naturally contains i and j. If N+i,j contains some other action k then either Ck = Ci, Ck = Cj , or Ck = Ci \u2229 Cj .\nIn general, the elements of the feedback matrix H can be arbitrary symbols. Nevertheless, the nature of the symbols themselves does not matter in terms of the structure of the game. What determines the feedback structure of a game is the occurrence of identical symbols in each row of H. To \u201cstandardize\u201d the feedback structure, the signal matrix is defined for each action:\nDefinition 4. Let si be the number of distinct symbols in the ith row of H and let \u03c31, . . . , \u03c3si \u2208 \u03a3 be an enumeration of those symbols. Then the signal matrix Si \u2208 {0, 1}si\u00d7M of action i is defined as Si[k, l] = I{H[i,l]=\u03c3k}.\nThe idea of this definition is that if p \u2208 \u2206M is the opponent\u2019s strategy then Sip gives the distribution over the symbols underlying action i. In fact, it is also true that observing H[It, Jt] is equivalent to observing the vector SIteJt , where ek is the k\nth unit vector in the standard basis of RM . From now on we assume without loss of generality that the learner\u2019s observation at time step t is the random vector Yt = SIteJt . Note that the dimensionality of this vector depends on the action chosen by the learner, namely Yt \u2208 RsIt .\nThe following two definitions play a key role in classifying partial-monitoring games based on their difficulty.\nDefinition 5 (Global observability (Piccolboni & Schindelhauer, 2001)). A partial-monitoring game (L,H) admits the global observability condition, if for all pairs i, j of actions, `i \u2212 `j \u2208 \u2295k\u2208N ImS>k . Definition 6 (Local observability (Barto\u0301k et al., 2011)). A pair of neighboring actions i, j is said to be locally observable if `i \u2212 `j \u2208 \u2295k\u2208N+i,j ImS > k . We denote by L \u2282 N the set of locally observable pairs of actions (the pairs are unordered). A game satisfies the local observability condition if every pair of neighboring actions is locally observable, i.e., if L = N .\nThe main result of Barto\u0301k et al. (2011) is that locally observable games have O\u0303( \u221a T ) minimax regret. It is easy to see that local observability implies global observability. Also, from Piccolboni & Schindelhauer (2001) we know that if global observability does not hold then the game has linear minimax regret. From now on, we only deal with games that admit the global observability condition.\nA collection of the concepts and symbols introduced in this section is shown in Table 1."}, {"heading": "3. The proposed algorithm", "text": "Our algorithm builds on the core idea underlying algorithm Balaton of Barto\u0301k et al. (2011), so we start with a brief review of Balaton. Balaton uses sweeps to successively eliminate suboptimal actions. This is done by estimating the differences between the expected losses of pairs of actions, i.e., \u03b4i,j = (`i\u2212`j)>p\u2217 (i, j \u2208 N). In fact, Balaton exploits that it suffices to keep track of \u03b4i,j for neighboring pairs of actions (i.e., for action pairs i, j such that {i, j} \u2208 N ). This is because if an action i is suboptimal, it will have a neighbor j that has a smaller expected loss and so the action i will get eliminated when \u03b4i,j is checked. Now, to estimate \u03b4i,j for some {i, j} \u2208 N one observes that under the local observability condition, it holds that `i \u2212 `j = \u2211 k\u2208N+i,j S>i vi,j,k for some vectors vi,j,k \u2208 R\u03c3k . This yields that \u03b4i,j = (`i\u2212`j)>p\u2217 =\u2211 k\u2208N+i,j v>i,j,kSkp \u2217. Since \u03bdk def = Skp \u2217 is the vector of the distribution of symbols under action k, which can be estimated by \u03bdk(t), the empirical frequencies of the individual symbols observed under k up to time t, Balaton uses \u2211 k\u2208N+i,j v>i,j,k\u03bdk(t) to estimate \u03b4i,j . Since none of the actions in N+i,j can get eliminated before one of {i, j} gets eliminated, the estimate of \u03b4i,j gets refined until one of {i, j} is eliminated.\nThe essence of why Balaton achieves a low regret is as follows: When i is not a neighbor of the optimal action i\u2217 one can show that it will be eliminated before all neighbors j \u201cbetween i and i\u2217\u201d get eliminated. Thus, the contribution of such \u201cfar\u201d actions to the re-\ngret is minimal. When i is a neighbor of i\u2217, it will be eliminated in time proportional to \u03b4\u22122i,i\u2217 . Thus the contribution to the regret of such an action is proportional to \u03b4\u22121i , where \u03b4i def = \u03b4i,i\u2217 . It also holds that the contribution to the regret of i cannot be larger than \u03b4iT . Thus, the contribution of i to the regret is at most min(\u03b4iT, \u03b4 \u22121 i ) \u2264 \u221a T .\nWhen some pairs {i, j} \u2208 N are not locally observable, one needs to use actions other than those in N+i,j to construct an estimate of \u03b4i,j . Under global observability, `i\u2212`j = \u2211 k\u2208Vi,j S > i vi,j,k for an appropriate subset Vi,j \u2282 N and an appropriate set of vectors vi,j,\u00b7. Thus, if the actions in Vi,j are kept in play, one can estimate the difference \u03b4i,j as before, using \u2211 k\u2208N+i,j v>i,j,k\u03bdk(t). This motivates the following definition:\nDefinition 7 (Observer sets and observer vectors). The observer set Vi,j \u2282 N underlying a pair of neighboring actions {i, j} \u2208 N is a set of actions such that\n`i \u2212 `j \u2208 \u2295k\u2208Vi,j ImS>k .\nThe observer vectors (vi,j,k)k\u2208Vi,j are defined to satisfy the equation `i \u2212 `j = \u2211 k\u2208Vi,j S > k vi,j,k. In particular, vi,j,k \u2208 Rsk . In what follows, the choice of the observer sets and vectors is restricted so that Vi,j = Vj,i and vi,j,k = \u2212vj,i,k. Furthermore, the observer set Vi,j is constrained to be a superset of N + i,j and in particular when a pair {i, j} is locally observable, Vi,j = N + i,j must hold. Finally, for any action\nk \u2208 \u22c3 {i,j}\u2208N N + i,j, let Wk = maxi,j:k\u2208Vi,j \u2016vi,j,k\u2016\u221e be the confidence width of action k.\nThe reason of the particular choice Vi,j = N + i,j for lo-\ncally observable pairs {i, j} is that we plan to use Vi,j (and the vectors vi,j,\u00b7) in the case of locally observable pairs, too. For not locally observable pairs, the whole action set N is always a valid observer set (thus, Vi,j can be found). However, whenever possible, it is better to use a smaller set. The actual choice of Vi,j (and vi,j,k) is postponed until the effect of this choice on the regret becomes clear.\nWith the observer sets, the basic idea of the algorithm becomes as follows: (i) Eliminate the suboptimal actions in successive sweeps; (ii) In each sweep, enrich the set of remaining actions P(t) by adding the observer actions underlying the remaining neighboring pairs {i, j} \u2208 N (t): V(t) = \u22c3 {i,j}\u2208N (t) Vi,j ; (iii) Explore the actions in P(t) \u222a V(t) to update the symbol frequency estimate vectors \u03bdk(t). Another refinement is to eliminate the sweeps so as to make the algorithm enjoy an advantageous anytime property. This can be achieved by selecting in each step only one action. We propose the action to be chosen should be the one that maximizes the reduction of the remaining uncertainty. This algorithm could be shown to enjoy \u221a T regret for locally observable games. However, if we run it on a non-locally observable game and the opponent strategy is on Ci \u2229 Cj for {i, j} \u2208 N \\ L, it will suffer linear regret! The reason is that if both actions i and j are optimal, and thus never get eliminated, the algorithm will choose actions from Vi,j \\ N+i,j too often. Furthermore, even if the opponent strategy is not on the boundary the regret can be too high: say action i is optimal but \u03b4j is small, while {i, j} \u2208 N \\ L. Then a third action k \u2208 Vi,j with large \u03b4k will be chosen proportional to 1/\u03b42j times, causing high regret. To combat this we restrict the frequency with which an action can be used for \u201cinformation seeking purposes\u201d. For this, we introduce the set of rarely chosen actions,\nR(t) = {k \u2208 N : nk(t) \u2264 \u03b7kf(t)} ,\nwhere \u03b7k \u2208 R, f : N\u2192 R are tuning parameters to be chosen later. Then, the set of actions available at time t is restricted to P(t) \u222a N+(t) \u222a (V(t) \u2229 R(t)), where N+(t) = \u22c3 {i,j}\u2208N (t)N + i,j . We will show that with these modifications, the algorithm achieves O(T 2/3) regret in the general case, while it will also be shown to achieve an O( \u221a T ) regret when the opponent uses a benign strategy. A pseudocode for the algorithm is given in Algorithm 1.\nIt remains to specify the function getPolytope. It gets the array halfSpace as input. The array halfSpace stores which neighboring action pairs have a confident estimate on the difference of their expected losses, along with the sign of the difference (if confi-\nAlgorithm 1 CBP\nInput: L, H, \u03b1, \u03b71, . . . , \u03b7N , f = f(\u00b7) Calculate P, N , Vi,j , vi,j,k, Wk for t = 1 to N do\nChoose It = t and observe Yt {Initialization} nIt \u2190 1 {# times the action is chosen} \u03bdIt \u2190 Yt {Cumulative observations}\nend for for t = N + 1, N + 2, . . . do\nfor each {i, j} \u2208 N do \u03b4\u0303i,j \u2190 \u2211 k\u2208Vi,j v > i,j,k \u03bdk nk\n{Loss diff. estimate} ci,j \u2190 \u2211 k\u2208Vi,j \u2016vi,j,k\u2016\u221e \u221a \u03b1 log t nk\n{Confidence} if |\u03b4\u0303i,j | \u2265 ci,j then halfSpace(i, j)\u2190 sgn \u03b4\u0303i,j else halfSpace(i, j)\u2190 0\nend if end for [P(t),N (t)]\u2190 getPolytope(P,N , halfSpace) N+(t) = \u222a{i,j}\u2208N (t)N+ij V(t) = \u222a{i,j}\u2208N (t)Vij R(t) = {k \u2208 N : nk(t) \u2264 \u03b7kf(t)} S(t) = P(t) \u222aN+(t) \u222a (V(t) \u2229R(t)) Choose It = argmaxi\u2208S(t) W 2i ni\nand observe Yt \u03bdIt \u2190 \u03bdIt + Yt nIt \u2190 nIt + 1\nend for\ndent). Each of these confident pairs define an open halfspace, namely\n\u2206{i,j} = { p \u2208 \u2206M : halfSpace(i, j)(`i \u2212 `j)>p > 0 } .\nThe function getPolytope calculates the open polytope defined as the intersection of the above halfspaces. Then for all i \u2208 P it checks if Ci intersects with the open polytope. If so, then i will be an element of P(t). Similarly, for every {i, j} \u2208 N , it checks if Ci \u2229 Cj intersects with the open polytope and puts the pair in N (t) if it does.\nNote that it is not enough to compute P(t) and then drop from N those pairs {k, l} where one of k or l is excluded from P(t): it is possible that the boundary Ck \u2229 Cl between the cells of two actions k, l \u2208 P(t) is included in the rejected region. For an illustration of cell decomposition and excluding cells, see Figure 1.\nComputational complexity The computationally heavy parts of the algorithm are the initial calculation of the cell decomposition and the function getPolytope. All of these require linear programming. In the preprocessing phase we need to solve N + N2 linear\nprograms to determine cells and neighboring pairs of cells. Then in every round, at most N2 linear programs are needed. The algorithm can be sped up by \u201ccaching\u201d previously solved linear programs."}, {"heading": "4. Analysis of the algorithm", "text": "The first theorem in this section is an individual upper bound on the regret of CBP.\nTheorem 1. Let (L,H) be an N by M partialmonitoring game. For a fixed opponent strategy p\u2217 \u2208 \u2206M , let \u03b4i denote the difference between the expected loss of action i and an optimal action. For any time horizon T , algorithm CBP with parameters \u03b1 > 1, \u03bdk = W 2/3 k , f(t) = \u03b1 1/3t2/3 log1/3 t has expected regret\nE[RT ] \u2264 \u2211 {i,j}\u2208N 2|Vi,j | ( 1 + 1 2\u03b1\u2212 2 ) + N\u2211 k=1 \u03b4k\n+ N\u2211 k=1 \u03b4k>0 4W 2k d2k \u03b4k \u03b1 log T\n+ \u2211\nk\u2208V\\N+ \u03b4k min\n( 4W 2k d2l(k)\n\u03b42l(k) \u03b1 log T,\n\u03b11/3W 2/3 k T 2/3 log1/3 T ) +\n\u2211 k\u2208V\\N+ \u03b4k\u03b1 1/3W 2/3 k T 2/3 log1/3 T\n+ 2dk\u03b1 1/3W 2/3T 2/3 log1/3 T ,\nwhere W = maxk\u2208N Wk, V = \u222a{i,j}\u2208NVi,j, N+ = \u222a{i,j}\u2208NN+i,j, and d1, . . . , dN are game-dependent constants.\nThe proof is omitted for lack of space.4 Here we give a\n4For complete proofs we refer the reader to the supplementary material.\nshort explanation of the different terms in the bound. The first term corresponds to the confidence interval failure event. The second term comes from the initialization phase of the algorithm. The remaining four terms come from categorizing the choices of the algorithm by two criteria: (1) Would It be different if R(t) was defined as R(t) = N? (2) Is It \u2208 P(t) \u222a N+t ? These two binary events lead to four different cases in the proof, resulting in the last four terms of the bound.\nAn implication of Theorem 1 is an upper bound on the individual regret of locally observable games:\nCorollary 1. If G is locally observable then\nE[RT ] \u2264 \u2211 {i,j}\u2208N 2|Vi,j | ( 1 + 1 2\u03b1\u2212 2 )\n+ N\u2211 k=1 \u03b4k + 4W 2 k d2k \u03b4k \u03b1 log T .\nProof. If a game is locally observable then V \\N+ = \u2205, leaving the last two sums of the statement of Theorem 1 zero.\nThe following corollary is an upper bound on the minimax regret of any globally observable game.\nCorollary 2. Let G be a globally observable game. Then there exists a constant c such that the expected regret can be upper bounded independently of the choice of p\u2217 as\nE[RT ] \u2264 cT 2/3 log1/3 T .\nThe following theorem is an upper bound on the minimax regret of any globally observable game against \u201cbenign\u201d opponents. To state the theorem, we need a new definition. Let A be some subset of actions in G. We call A a point-local game in G if \u22c2 i\u2208A Ci 6= \u2205. Theorem 2. Let G be a globally observable game. Let \u2206\u2032 \u2286 \u2206M be some subset of the probability simplex such that its topological closure \u2206\u2032 has \u2206\u2032\u2229Ci\u2229Cj = \u2205 for every {i, j} \u2208 N \\ L. Then there exists a constant c such that for every p\u2217 \u2208 \u2206\u2032, algorithm CBP with parameters \u03b1 > 1, \u03bdk = W 2/3 k , f(t) = \u03b1\n1/3t2/3 log1/3 t achieves\nE[RT ] \u2264 cdpmax \u221a bT log T ,\nwhere b is the size of the largest point-local game, and dpmax is a game-dependent constant.\nIn a nutshell, the proof revisits the four cases of the proof of Theorem 1, and shows that the terms which would yield T 2/3 upper bound can be non-zero only for a limited number of time steps.\nRemark 1. Note that the above theorem implies that CBP does not need to have any prior knowledge about \u2206\u2032 to achieve \u221a T regret. This is why we say our algorithm is \u201cadaptive\u201d.\nAn immediate implication of Theorem 2 is the following minimax bound for locally observable games:\nCorollary 3. Let G be a locally observable finite partial monitoring game. Then there exists a constant c such that for every p \u2208 \u2206M ,\nE[RT ] \u2264 c \u221a T log T .\nRemark 2. The upper bounds in Corollaries 2 and 3 both have matching lower bounds up to logarithmic factors (Barto\u0301k et al., 2011), proving that CBP achieves near optimal regret in both locally observable and non-locally observable games."}, {"heading": "5. Experiments", "text": "We demonstrate the results of the previous sections using instances of Dynamic Pricing, as well as a locally observable game. We compare the results of CBP to two other algorithms: Balaton (Barto\u0301k et al., 2011) which is, as mentioned earlier in the paper, the first algorithm that achieves O\u0303( \u221a T ) minimax regret for all locally observable finite stochastic partial-monitoring games; and FeedExp3 (Piccolboni & Schindelhauer, 2001), which achieves O(T 2/3) minimax regret on all non-hopeless finite partial-monitoring games, even against adversarial opponents."}, {"heading": "5.1. A locally observable game", "text": "The game we use to compare CBP and Balaton has 3 actions and 3 outcomes. The game is described with the loss and feedback matrices:\nL = 1 1 00 1 1 1 0 1  ; H = a b bb a b b b a  . We ran the algorithms 10 times for 15 different stochastic strategies. We averaged the results for each strategy and then took pointwise maximum over the 15 strategies. Figure 2(a) shows the empirical minimax regret calculated the way described above. In addition, Figure 2(b) shows the regret of the algorithms against one of the opponents, averaged over 100 runs. The results indicate that CBP outperforms both FeedExp and Balaton. We also observe that, although the asymptotic performace of Balaton is proven to be better than that of FeedExp, a larger constant factor makes Balaton lose against FeedExp even at time step ten million."}, {"heading": "5.2. Dynamic Pricing", "text": "In Dynamic Pricing, at every time step a seller (player) sets a price for his product while a buyer (opponent) secretly sets a maximum price he is willing to pay. The feedback for the seller is \u201cbuy\u201d or \u201cno-buy\u201d, while his loss is either a preset constant (no-buy) or the difference between the prices (buy). The finite version of the game can be described with the following matrices:\nL =  0 1 \u00b7 \u00b7 \u00b7 N \u2212 1 c 0 \u00b7 \u00b7 \u00b7 N \u2212 2 ... . . . . . .\n... c \u00b7 \u00b7 \u00b7 c 0\n H =  y y \u00b7 \u00b7 \u00b7 y n y \u00b7 \u00b7 \u00b7 y ... . . . . . .\n... n \u00b7 \u00b7 \u00b7 n y  This game is not locally observable and thus it is \u201chard\u201d (Barto\u0301k et al., 2011). Simple linear algebra gives that the locally observable action pairs are the \u201cconsecutive\u201d actions (L = {{i, i+ 1} : i \u2208 N \u2212 1}), while quite surprisingly, all action pairs are neighbors.\nWe compare CBP with FeedExp on Dynamic Pricing with N = M = 5 and c = 2. Since Balaton is undefined on not locally observable games, we can not include it in the comparison. To demonstrate the adaptiveness of CBP, we use two sets of opponent strategies. The \u201cbenign\u201d setting is a set of opponents which are far away from \u201cdangerous\u201d regions, that is, from boundaries between cells of non-locally observable neighboring action pairs. The \u201charsh\u201d settings, however, include opponent strategies that are close or on the boundary between two such actions. For each setting we maximize over 15 strategies and average over 10 runs. We also compare the individual regret of the two algorithms against one benign and one harsh strategy. We averaged over 100 runs and plotted the 90 percent confidence intervals.\nThe results (shown in Figures 3 and 4) indicate that CBP has a significant advantage over FeedExp on benign settings. Nevertheless, for the harsh settings FeedExp slightly outperforms CBP, which we think is a reasonable price to pay for the benefit of adaptivity."}], "references": [{"title": "Minimax policies for adversarial and stochastic bandits", "author": ["Audibert", "J-Y", "S. Bubeck"], "venue": "COLT", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Mach. Learn.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Toward a classification of finite partial-monitoring games", "author": ["G. Bart\u00f3k", "D. P\u00e1l", "Szepesv\u00e1ri", "Cs"], "venue": "ALT", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2010}, {"title": "Minimax regret of finite partial-monitoring games in stochastic environments", "author": ["G. Bart\u00f3k", "D. P\u00e1l", "Szepesv\u00e1ri", "Cs"], "venue": "In COLT,", "citeRegEx": "Bart\u00f3k et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bart\u00f3k et al\\.", "year": 2011}, {"title": "Regret minimization under partial monitoring", "author": ["N. Cesa-Bianchi", "G. Lugosi", "G. Stoltz"], "venue": "In Information Theory Workshop,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2006}, {"title": "No internal regret via neighborhood watch", "author": ["D.P. Foster", "A. Rakhlin"], "venue": "CoRR, abs/1108.6088,", "citeRegEx": "Foster and Rakhlin,? \\Q2011\\E", "shortCiteRegEx": "Foster and Rakhlin", "year": 2011}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth", "year": 1994}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["A. Piccolboni", "C. Schindelhauer"], "venue": "COLT", "citeRegEx": "Piccolboni and Schindelhauer,? \\Q2001\\E", "shortCiteRegEx": "Piccolboni and Schindelhauer", "year": 2001}, {"title": "Aggregating strategies", "author": ["V.G. Vovk"], "venue": "In Annual Workshop on Computational Learning Theory,", "citeRegEx": "Vovk,? \\Q1990\\E", "shortCiteRegEx": "Vovk", "year": 1990}], "referenceMentions": [{"referenceID": 2, "context": "As an implication, we show that under some reasonable additional assumptions, the algorithm enjoys an O( \u221a T ) regret in Dynamic Pricing, proven to be hard by Bart\u00f3k et al. (2011).", "startOffset": 159, "endOffset": 180}, {"referenceID": 7, "context": "Since Vovk (1990) and Littlestone & Warmuth (1994) we know that for full-information games, the minimax regret scales as \u0398( \u221a T logN).", "startOffset": 6, "endOffset": 18}, {"referenceID": 7, "context": "Since Vovk (1990) and Littlestone & Warmuth (1994) we know that for full-information games, the minimax regret scales as \u0398( \u221a T logN).", "startOffset": 6, "endOffset": 51}, {"referenceID": 1, "context": "The individual regret of these kind of games has also been studied: Auer et al. (2002) showed that given any opponent strategy, the expected regret can be upper bounded by c \u2211 i\u2208N :\u03b4i 6=0 1 \u03b4i log T , where \u03b4i is the expected difference between the loss of action i and an optimal action.", "startOffset": 68, "endOffset": 87}, {"referenceID": 4, "context": "Their upper bound for non-hopeless games was tightened to O(T ) by Cesa-Bianchi et al. (2006), who also showed that there exists a game with a matching lower bound.", "startOffset": 67, "endOffset": 94}, {"referenceID": 2, "context": "The first steps towards classifying partialmonitoring games were made by Bart\u00f3k et al. (2010), who characterized almost all games with two outcomes.", "startOffset": 73, "endOffset": 94}, {"referenceID": 1, "context": "The Exp3 algorithm due to Auer et al. (2003) achieves almost the same regret, with an extra logarithmic term.", "startOffset": 26, "endOffset": 45}, {"referenceID": 1, "context": "The Exp3 algorithm due to Auer et al. (2003) achieves almost the same regret, with an extra logarithmic term. spectively. They also found that there exist games that are easy, but can not easily be \u201ctransformed\u201d to a bandit or full-information game. Later, Bart\u00f3k et al. (2011) proved the same results for finite stochastic partial monitoring, with any finite number of outcomes.", "startOffset": 26, "endOffset": 278}, {"referenceID": 1, "context": "The Exp3 algorithm due to Auer et al. (2003) achieves almost the same regret, with an extra logarithmic term. spectively. They also found that there exist games that are easy, but can not easily be \u201ctransformed\u201d to a bandit or full-information game. Later, Bart\u00f3k et al. (2011) proved the same results for finite stochastic partial monitoring, with any finite number of outcomes. The condition that separates easy games from hard games is the local observability condition (see Definition 6). The algorithm Balaton introduced there works by eliminating actions that are thought to be suboptimal with high confidence. They conjectured in their paper that the same classification holds for nonstochastic games, without changing the condition. Recently, Foster & Rakhlin (2011) designed the algorithm NeighborhoodWatch that proves this conjecture to be true.", "startOffset": 26, "endOffset": 775}, {"referenceID": 2, "context": "In this paper, we extend the results of Bart\u00f3k et al. (2011). We introduce a new algorithm, called CBP for \u201cConfidence Bound Partial monitoring\u201d, with various desirable properties.", "startOffset": 40, "endOffset": 61}, {"referenceID": 2, "context": "The final, and potentially most impactful, aspect of our algorithm is that through additional assumptions on the set of opponent strategies, the minimax regret of even hard games can be brought down to \u0398\u0303( \u221a T )! While this statement may seem to contradict the result of Bart\u00f3k et al. (2011), in fact it does not.", "startOffset": 271, "endOffset": 292}, {"referenceID": 2, "context": "The following definitions, taken from Bart\u00f3k et al. (2011), are essential for understanding how the structure of L and H determines the \u201chardness\u201d of a game.", "startOffset": 38, "endOffset": 59}, {"referenceID": 3, "context": "Definition 6 (Local observability (Bart\u00f3k et al., 2011)).", "startOffset": 34, "endOffset": 55}, {"referenceID": 2, "context": "The main result of Bart\u00f3k et al. (2011) is that lo-", "startOffset": 19, "endOffset": 40}, {"referenceID": 2, "context": "Our algorithm builds on the core idea underlying algorithm Balaton of Bart\u00f3k et al. (2011), so we start with a brief review of Balaton.", "startOffset": 70, "endOffset": 91}, {"referenceID": 3, "context": "The upper bounds in Corollaries 2 and 3 both have matching lower bounds up to logarithmic factors (Bart\u00f3k et al., 2011), proving that CBP achieves near optimal regret in both locally observable and non-locally observable games.", "startOffset": 98, "endOffset": 119}, {"referenceID": 3, "context": "We compare the results of CBP to two other algorithms: Balaton (Bart\u00f3k et al., 2011) which is, as mentioned earlier in the paper, the first algorithm that achieves \u00d5( \u221a T ) minimax regret for all locally observable finite stochastic partial-monitoring games; and FeedExp3 (Piccolboni & Schindelhauer, 2001), which achieves O(T ) minimax regret on all non-hopeless finite partial-monitoring games, even against adversarial opponents.", "startOffset": 63, "endOffset": 84}, {"referenceID": 3, "context": "This game is not locally observable and thus it is \u201chard\u201d (Bart\u00f3k et al., 2011).", "startOffset": 58, "endOffset": 79}], "year": 2012, "abstractText": "We present a new anytime algorithm that achieves near-optimal regret for any instance of finite stochastic partial monitoring. In particular, the new algorithm achieves the minimax regret, within logarithmic factors, for both \u201ceasy\u201d and \u201chard\u201d problems. For easy problems, it additionally achieves logarithmic individual regret. Most importantly, the algorithm is adaptive in the sense that if the opponent strategy is in an \u201ceasy region\u201d of the strategy space then the regret grows as if the problem was easy. As an implication, we show that under some reasonable additional assumptions, the algorithm enjoys an O( \u221a T ) regret in Dynamic Pricing, proven to be hard by Bart\u00f3k et al. (2011).", "creator": "LaTeX with hyperref package"}}}