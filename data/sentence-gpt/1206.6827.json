{"id": "1206.6827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Linear Algebra Approach to Separable Bayesian Networks", "abstract": "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node's neighbors, instead of the joint distributions. In terms of modeling, separable networks has rendered possible siginificant reduction in complexity, as the state space is only linear in the number of variables on the network, in contrast to a typical state space which is exponential. In this work, We describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable. We demonstrate that the conditional probability distribution, the coherence theory, and the PFC are separable networks. We apply this approach to the conditional probability distribution in an ordinary, nonparametric way. We explain the connection in this study by explaining the difference between the two, and explain the coherence theory with data about the coherence theory. This is related to the fact that the coherence theory is an important concept in computer simulation, and is supported by a number of other theoretical papers. The results demonstrate the relationship between the coherence theory and the coherence theory, with the fact that it is highly parsable on any given state space. As such, we show that an optimal conditional probability distribution is not the most parsable on any given state space. However, we show that the coherence theory is highly parsable on any given state space. We also show that the coherence theory is highly parsable on any given state space. Our model uses an even more parsable state space that is less parsable on any given state space. The results show that the coherence theory is highly parsable on any given state space. In fact, our model uses an even more parsable state space.\n\n\n\nIn this paper, we describe a computational method for testing whether a given CPT is separable in an ordinary, nonparametric way. The probability distribution, the coherence theory, and the PFC are separable networks. We demonstrate that the coherence theory is highly parsable on any given state space. We demonstrate that the coherence theory is highly parsable on any given state space. We explain the connection in this study by explaining the difference between the two, and explain the coherence theory with data about the coherence theory. This is related to the", "histories": [["v1", "Wed, 27 Jun 2012 15:41:47 GMT  (127kb)", "http://arxiv.org/abs/1206.6827v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["chalee asavathiratham"], "accepted": false, "id": "1206.6827"}, "pdf": {"name": "1206.6827.pdf", "metadata": {"source": "CRF", "title": "Linear Algebra Approach to Separable Bayesian Networks", "authors": ["Chalee Asavathiratham"], "emails": ["chalee@alum.mit.edu"], "sections": [{"heading": null, "text": "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node\u2019s parents, instead of the joint distributions. We describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof to [Pfeffer00] on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable."}, {"heading": "1 Introduction", "text": "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node\u2019s neighbors, instead of the joint distributions. In terms of modeling, separable networks has rendered possible siginificant reduction in complexity, as the state space is only linear in the number of variables on the network, in contrast to a typical state space which is exponential.\nWe describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof to [Pfeffer00] on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable."}, {"heading": "2 Results", "text": ""}, {"heading": "2.1 Probability Mass Functions", "text": "Suppose X , Y and Z are three random variables and let the size of their sample spaces be some finite integers mx, my and my respectively. Similar to [Pfeffer00], we denote the space of Probability Mass Functions (PMF) over the joint random variables (X,Y ) as \u2206XY , i.e.,\n\u2206XY = {q \u2208 Rmxmy | q \u2265 0, q\u20321mxmy = 1},\nwhere 1mxmy is the all-ones column vector of length (mxmy), and q \u2265 0 means each entry of the column vector q is non-negative. When there is no ambiguity, we will drop the subscript from 1."}, {"heading": "2.2 Event Matrix", "text": "We now introduce a particular class of matrices which would be useful for our discussion later. These matrices have been introduced in [Asavathiratham00] (Sec. 5.2.1) and is instrumental in some of the key theoretical results in the thesis.\nLet ei = [ 0 \u00b7 \u00b7 \u00b7 0 1 0 \u00b7 \u00b7 \u00b7 0 ]\u2032 be the vector in which the single 1-entry appears in the ith position. For random variable X , we can represent its ith outcome as ei \u2208 Rmx . Stacking these vectors into a matrix, we get the event matrix BX \u2013 in this case, the mx-by-mx identity matrix \u2013 whose rows form a sample space for X .\nFor joint variables (X,Y ), let us define the event matrix as\nBXY = [Imx \u2297 1my |1mx \u2297 Imy ],\nwhere J\u2297K denotes the Kronecker product of matrices\nJ and K.1 For example, if mx = 2 and my = 3, then\nBXY = \u23a1 \u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3\n1 0 1 0 1 0 0 1 0 1 0 1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 \u23a4 \u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 .\nFor convenience, let us define B = BXY . It can be seen that each row of B is an outcome of the joint random variable of (X,Y ). Moreover, the ordering of the row B also provides for us an index for elements in the sample space of (X,Y ). Note that\nB [ 1mx 0 ] = B [ 0 1my ] = 1mxmy . (1)\nWe shall also list some properties of B that will be used later on in the paper.\nTheorem 1\nrank(B) = mx +my \u2212 1\nProof: See Theorem 5.8 in [Asavathiratham00].\nLet N (K) and R(K) be the null space and the subspace of some matrix K.\nCorollary 2\nN (B) = R ( [ 1mx\n\u22121my\n])\nProof: See Corollary 5.10 in [Asavathiratham00].\nNote that for a given q \u2208 \u2206XY , the product q\u2032B = [ q\u2032X q \u2032 Y ],\nwhere qX and qY are marginal of q over X and Y respectively. That is, multiplication by B gives us the marginal distribution on X and Y from q. As such, we define the marginalization matrices as follows\nBXY,X = B [ B\u2032X 0 ] (2)\nBXY,Y = B [ 0 B\u2032Y ] , (3)\nso that q\u2032X = q \u2032BXY,X and q\u2032Y = q \u2032BXY,Y . These marginilization matrices shall be used below.\n1For example, if J is a 2-by-2 matrix with entries given by j11, . . . , j22 and if K is some arbitrary matrix, then\nJ \u2297 K = j11K j12K j21K j22K ."}, {"heading": "2.3 Sufficiency", "text": "For a given conditional PMF P (Z|XY ), we let CP (Z|XY ), or simply C, denote its Conditional Probability Table (CPT). That is, C is a (mxmy)-by-mz matrix such that each row of C corresponds to an outcome of (X,Y ), sorted in the order defined by B. Specifically, let the (i, j)th entry of C be defined as\n[C]ij = P (Z = j | (X,Y )is ith row of B )\nIt then follows that C has nonnegative entries and\nC1mz = 1mxmy .\nIn [Pfeffer00], the function \u03a6P : \u2206XY \u2192 \u2206Z was defined as \u03a6P (q) = \u2211 xy q(xy)P (Z|xy). Expressing this definition in a matrix form, we get\n\u03a6P (q) = q\u2032C.\nBecause \u03a6P , P (Z|XY ) and C completely describe one another, we will focus mainly on C.\nIn general, obtaining a marginal on Z requires that we know the full joint distribution on (X,Y ). Below we define a special condition such that the marginals on X and Y alone can uniquely determine the marginal on Z.\nDefinition 1 C is sufficient if for any q1,q2 \u2208 \u2206XY such that q\u20321B = q \u2032 2B, then q \u2032 1C = q \u2032 2C.\nTheorem 3 C is sufficient if and only if N (B\u2032) \u2282 N (C\u2032)\nProof: First suppose C is sufficient. Take any q\u0303 \u2208 N (B\u2032) so that q\u0303\u2032B = 0. If q\u0303 \u2265 0, then it must follow that q\u0303 = 0. Otherwise, if q\u0303 has any positive entry, q\u0303\u2032B would have at least some positive entries, because every row of B has some positive entries. This would contradict the fact q\u0303\u2032B = 0.\nThen q\u0303 must have at least one negative entry. This implies that it must also have at least one positive entry, because\nq\u0303\u20321 = q\u0303\u2032B [\n1mx 0\n] = 0. (4)\nwhere the second equality follows from (1). We can, therefore, separate the positive entries from the negative ones to obtain\nq\u0303 = q+ \u2212 q\u2212, where q+ and q\u2212 are the absolute values of the pos-\nitive and negative entries of q\u0303 respectively. Let d =\nq\u2032+1 = q \u2032 \u22121, where the second equality follows from (4). Because q+ has some positive entry, d > 0. Define q\u0303+ = 1dq+ and q\u0303\u2212 = 1 dq\u2212. It can then be shown that q\u0303+ and q\u0303\u2212 are both valid PMFs. Therefore,\nq\u0303\u2032B = 0 \u2194 d(q\u0303+ \u2212 q\u0303\u2212)\u2032B = 0 \u2194 q\u0303\u2032+B = q\u0303\u2032\u2212B \u2194 q\u0303\u2032+C = q\u0303\u2032\u2212C (5) \u2194 q\u0303\u2032C = 0\nwhere (5) follows from sufficiency of C. This completes the forward part of the proof. The converse of the proof is straightforward.\nLet R(A) denote the subspace spanned by the matrix A.\nCorollary 4 C is sufficient if and only if R(C) \u2282 R(B).\nProof: This follows from the fact that N (B\u2032) \u2282 N (C\u2032) is equivalent to R(C) \u2282 R(B)."}, {"heading": "2.4 Separability", "text": "First we state the definition of separability as given in [Pfeffer00].\nDefinition 2 P (Z|XY ) is separable if there exists condition distributions PX(Z|X), PY (Z|Y ) and a constant \u03b3 \u2208 [0, 1] such that P (Z|XY ) = \u03b3PX(Z|X)+(1\u2212 \u03b3)PY (Z|Y ).\nSeparability is the key to the reduction in complexity. To see this, consider, for example, the task of listing P (Z|XY ) for all possible values of X ,Y and Z. In general, we would require a table whose number of entries is on the order of O(mxmymz). Through separability, we only need to list the conditional probabilities P (Z|X) and P (Z|Y ) separately, and compute P (Z|XY ) upon needed. This would result in a table on the order of O(mzmx + mzmy). The reduction would be even greater as we extend this to multiple variables such as P (Z|X1X2 \u00b7 \u00b7 \u00b7Xn) later on.\nProposition 1 P (Z|XY ) is separable if and only if C can be expressed as\nC = \u03b3BXY,XCX + (1\u2212 \u03b3)BXY,Y CY (6)\nwhere CX and CY are mx-by-mz and my-by-mz nonnegative matrices with every row summing to 1.\nTo see why this is the case, one only has to recognize that that the rows of CX and CY are respectively the conditional PMFs PX(Z|X) and PY (Z|Y ) expressed in a matrix form, with one row for each different outcome\nof X and Y . If we multiply (6) by some q \u2208 \u2206XY , the left hand-side q\u2032C would be a marginal distribution P (Z). On the right-hand side, the first-term would be\nq\u2032BXY,XCX = q\u2032XCX ,\nwhich is PX(Z|X) in vector form. Similarly, the second-term on the right-hand side would be PY (Z|Y ) as desired.\nExample 1 Let (mx,my,mz) = (2, 3, 2) and let\nC = \u23a1 \u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 .65 .35 .35 .65 .20 .80 .60 .40\n.30 .70 .15 .85\n\u23a4 \u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 .\nOne way to factorize C into the form in (6) is to as follows.\n\u03b3 = 0.5, CX = [ .3 .7\n.2 .8\n] , CY = \u23a1 \u23a3 1 0.4 .6\n.1 .9\n\u23a4 \u23a6 .\nSince the factorization is not unique, another choice would be\n\u03b3 = 0.5, CX = [ .4 .6\n.3 .7\n] , CY = \u23a1 \u23a3 .9 .1.3 .7\n0 1\n\u23a4 \u23a6 .\nThe following result has been first shown in [Pfeffer00]. The proof we are presenting below is different in that it is based on linear algebra. By establishing this connection, we hope to apply some of the results from matrix analysis to simplify some theoretical and computational problems in the Bayesian Network area.\nTheorem 5 C is sufficient if and only if P (Z|XY ) is separable.\nProof: First suppose that C is sufficient. By Corollary 4, C = BF for some (mx +my)-by-mz matrix F . Because each row of C is a valid PMF, C1mz = 1mxmy , which means\nBF1mz = 1mxmy . (7)\nWe would like factorize F in (7) to show its separability. From (1), we know that\nB \u00b7 1 2 [ 1mx 1my ] = 1 2 (1mxmy + 1mxmy) = 1mxmy . (8)\nCombining (8) and Corollary 2, we can express the term F1 as a sum of particular and homogeneous solution, i.e.,\nF1mz = 1 2 [ 1mx 1my ] + \u03b4 [ 1mx \u22121my ] (9)\nwhere \u03b4 is some constant.\nCase A: If F \u2265 0, then (9) implies that \u2212 12 \u2264 \u03b4 \u2264 12 . For the special cases when \u2212 12 < \u03b4 < 12 , we let \u03b3 = 1 2 + \u03b4 and\nCX = 1\n1 2 + \u03b4\n[Imx 0]F\nCY = 1\n1 2 \u2212 \u03b4\n[ 0 Imy ] F\nIf, however, \u03b4 = 12 , then we let \u03b3 = 1 and CX = [Imx 0]F , and CY can be any CPT. Similarly, when \u03b4 = \u2212 12 , we let CY = [ 0 Imy ] F , and \u03b3 = 0.\nIt can easily verified that CX and CY as defined are valid CPTs, and that \u03b3 would lie in [0, 1]. Substituting these values into (6), we thus have proved that P (Z|XY ) is separable. Case B: If F has some negative entries, we can convert it to a non-negative matrix one column at a time as follows. Let f be a column that contains at least one negative entry. Let us partition it into blocks of mx and my entries so that f \u2032 = [f \u2032x f \u2032 y]. Each row of Bf can be written as\n(fx)i + (fy)j (10)\nwhere (fx)i and (fy)j represents some ith and jth entries of fx and fy respectively. Because BF = C \u2265 0, at most one of the terms in (10) can be negative. Indeed, if fx has a negative entry, there can be no negative value in fy at all, and vice versa. Assume without loss of generality that fy \u2265 0, and fx contains some negative entry. Let i\u2217 = argmini(fx)i, and j\u2217 = argminj(fy)j . Because Bf \u2265 0, (fy)j\u2217 \u2265 \u2212(fx)i\u2217 > 0. Then replace f with\nf\u0303 = f + \u03b1\n[ 1mx\n\u22121my\n] ,\nwhere \u2212(fx)i\u2217 \u2264 \u03b1 \u2264 (fy)j\u2217 . By construction, f\u0303 would have no negative entry. Furthermore, because of Corollary 2, Bf = Bf\u0303 . We can apply this to all other columns of F that has a negative entry until we arrive at F\u0303 \u2265 0. Then we can apply Case A to obtain CX and CY . This thus proves that P (Z|XY ) is separable. On the other hand, if P (Z|XY ) is separable, then we can factorize C according to Proposition 1. By (2)- (3), BXY,X , BXY,Y \u2208 R(B). Thus, C \u2208 R(B). By Corollary 4, C is sufficient."}, {"heading": "2.5 Extension to Multiple Variables", "text": "The results above can be extended to the multiplevariable case such as P (Z|X1 \u00b7 \u00b7 \u00b7Xn).\nLet m1, . . . ,mn be the size of the sample spaces of X1, . . . , Xn respectively. We construct a sequence of matrices {Bi} from i = 1 to i = n through a recursive procedure as follows:\nB1 = Im1\nBi = [ Bi\u22121 \u2297 1mi | 1\u00b5i\u22121 \u2297 Imi ] (11)\nwhere \u00b5i = \u220fi j=1 mj . The event matrix and, will be\ndenoted by B = Bn. Some properties of B are presented again for the general case.\nTheorem 6 For 1 \u2264 i \u2264 n,\nrank ( Bi ) = ( i\u2211 k=1 mk ) \u2212 i+ 1\nProof: See Theorem 5.8 in [Asavathiratham00].\nTheorem 7\nN (B) = { v \u2223\u2223\u2223 v = \u23a1 \u23a2\u23a3 a11m1 ...\nan1mn\n\u23a4 \u23a5\u23a6 with \u2211\ni\nai = 0 }\nProof: See Corollary 5.10 in [Asavathiratham00].\nDefinition 3 (General Sufficient Condition) C is sufficient if for any q1,q2 \u2208 \u2206X1\u00b7\u00b7\u00b7Xn such that q\u20321B = q \u2032 2B, then q \u2032 1C = q \u2032 2C.\nThereom 3 and Corollary 4 and their proofs still hold in the general case.\nDefinition 4 (General Sepability Condition) P (Z|X1 \u00b7 \u00b7 \u00b7Xn) is separable if there exist a set of distributions {Pi(Z|X1 \u00b7 \u00b7 \u00b7Xn)} and non-negative constants {\u03b3i} such that \u2211 i \u03b3i = 1 and\nP (Z|X1 \u00b7 \u00b7 \u00b7Xn) = n\u2211\ni=1\n\u03b3iPi(Z|Xi).\nLet B\u0302i = B \u00b7 (ei \u2297 Imi), i.e., B\u0302i extracts the columns corresponding to the ith variable from B.\nProposition 2 P (Z|X1 \u00b7 \u00b7 \u00b7Xn) is separable if and only if C can be written as\nC = n\u2211\ni=1\n\u03b3iB\u0302iCi (12)\nwhere each Ci is an mi-by-mz nonnegative matrix in which every row sums to 1, and the coefficients {\u03b3i} are non-negative and sum to 1.\nTheorem 8 C is sufficient if and only if P (Z|X1 \u00b7 \u00b7 \u00b7Xn) is separable.\nSketch of Proof: The proof would proceed similarly as the one in Theorem 5, except that (9) should be written as\nF1 = 1 n\n\u23a1 \u23a2\u23a3 1m1 ...\n1mn\n\u23a4 \u23a5\u23a6+ \u23a1 \u23a2\u23a3 a11m1 ...\nan1mn\n\u23a4 \u23a5\u23a6 ,\nin which \u2211\ni ai = 0 from Theorem 7.\nCase A: If F \u2265 0, we apply the same arguments and let Ci = 11\nn +ai\n[ 0 \u00b7 \u00b7 \u00b7 Imi \u00b7 \u00b7 \u00b7 0 ] F .\nCase B: If F has some negative entry, we would again add some columns to its to make it non-negative. Here we partition f into n blocks according to {mi}. Then there must be some row of Bf that is equal to\nn\u2211 i=1 min(fi), (13)\ni.e., some row of B would happen to select the minimal entry of every block {fi} into the sum. Let us group all the indices that contribute positive (or zero) value in (13) into A+, and let the rest be A\u2212. Now, for all i \u2208 A\u2212, let \u03b1i = |min(fi)|. For i \u2208 A+, let \u03b1i be some negative coefficients chosen in any way such that \u03b1i +min(fi) \u2265 0 and \u2211 i\u2208A+ \u03b1i = \u2212 \u2211 i\u2208A\u2212 \u03b1i. These {\u03b1i} must exist2 because\u2211 i\u2208A+ min(fi) \u2265 \u2212 \u2211 i\u2208A\u2212 min(f\u2212),\nwhich follows from the fact that Bf \u2265 0. Thus, by replacing f with\nf\u0303 = f + \u23a1 \u23a2\u23a3 \u03b111m1 ...\n\u03b1n1mn\n\u23a4 \u23a5\u23a6 ,\nwe would have a non-negative column as desired. The rest of the proof is the same."}, {"heading": "2.6 Test for Separability", "text": "Given a CPT matrix C we can test if it is separable by simply verifying that it lies in the subspace of B. One way to do this is to test whether the orthogonal projection C onto R(B) is equal to itself. If C \u2208 R(B), then the projection would only be the least-square approximation.\nTo achieve this, we would need a basis for R(B). Unfortunately, B itself does not have full column rank\n2For example, a greedy algorithm would work.\naccording to Theorem 6 and thus cannot be directly used to create an orthogonal projection. We propose the following slightly modified matrix A, which is defined in the following recursive way:\nA1 = Im1\nAi = [ Ai\u22121 \u2297 1mi | 1\u00b5i\u22121 \u2297 I\u0303mi ]\nwhere \u00b5i = \u220fi j=1 mj , and I\u0303m is the matrix consisting of the first m\u2212 1 columns of the m-by-m identity matrix. The resulting A = An would be a matrix with \u00b5n\nrows and ( \u2211\ni mi)\u2212n+1 columns. By inspection, one can see A can be obtained from B by dropping certain n\u2212 1 columns. This means that R(A) \u2282 R(B). Theorem 9 The columns of A are linearly independent and spans R(B). Proof: For i = 1 the claim is trivially true. Assume that the assertion is true up to some index i\u2212 1. Because B(i\u22121) \u2208 R(A(i\u22121)), there exists some matrix K such that B(i\u22121) = A(i\u22121)K. Thus, by the Kronecker product property, Bi\u22121\u22971mi = (A(i\u22121) \u22971mi)K. Define\nB\u0303i = [ B(i\u22121) \u2297 1mi | 1\u00b5i\u22121 \u2297 I\u0303mi ] (14)\nFrom Lemma B.1 in Appendix B of [Asavathiratham00], we have that\nrank ( B\u0303i ) = rank ( B(i\u22121) ) +mi \u2212 1\n= rank ( A(i\u22121) ) +mi \u2212 1 (15)\n= rank(Ai). (16)\nEq. (15) comes from assertion of the proof up to i\u22121, and (16) is due to the fact that mi \u2212 1 new columns are added in going from A(i\u22121) to Ai. 3\nDefine the orthogonal projection of C as\nP(C) = A(A\u2032A)\u22121A\u2032C. (17) Note that the inverse (A\u2032A)\u22121 must exist because A has full column rank.\nTheorem 10 (Separability Test) C is separable if and only if P(C) = C. Proof: This follows from standard linear algebra theory and from Corollary 4 and Theorem 8."}, {"heading": "2.7 Connection to the Influence Model", "text": "The Influence Model was first introduced in [Asavathiratham00] and succinctly described in [Asavathiratham01]. Originally intended to explain cascading\n3Can we find another basis that is both orthonormal and recursive?\nphenomena in power systems, the model has been applied in applications such as social interaction, viral marketing networks, information diffusion, and distributed control.\nRecall that an Influence Model is defined by an n-by-n stochastic matrix D called the influence matrix and a set of matrices {Aij} for all i, j = 1, . . . , n such that every Aij \u2265 0 and Aij1 = 1. Each site i (node i) at time k has a status that is represented by\nsi[k] = [ 0 \u00b7 \u00b7 \u00b7 1 \u00b7 \u00b7 \u00b7 0]\u2032\nwhere the position of the 1-entry represents the its current status. For each time step, given the states {si[k]} the next-state PMF, or the posterior belief, at time k + 1 state i is given by\np\u2032i[k + 1] = di1s \u2032 1[k]Ai1 + \u00b7 \u00b7 \u00b7+ dins\u2032n[k]Ain.\nThe state at time k + 1 would be realized randomly according to pi[k + 1].\nIt turns out that a DBN in which all CPT\u2019s are separable is an influence model. To make the relation explicit, one can see that the entries in each row of D is equivalent to a set of {\u03b3i} in (12), and {Aij} are the Ci\u2019s.\nApproximating a given DBN with an Influence Model should allow us computational savings in various ways. For example, the Influence Model allows us to compute the marginal distribution of a node at any given time efficiently. It also provides for a simple way for analyzing the recurrent classes based on reduced-order graph structure. Absorption probabilities of each recurrent classes can also be efficiently computed."}, {"heading": "3 Open Issues", "text": ""}, {"heading": "3.1 Approximating General DBN with a Separable System", "text": "So far, we have only focused on the CPT of a given variable Z, but the idea can be extended to a network of random variables. That is, if we are given a Dynamic Bayesian Network in which every node has an arbitrary CPT based on the joint distribution of its neighbors (including possibly itself), then we can appoximate it with a separable version. What would then be the optimal way to achieve this?\nOne possible procedure is to work with the CPT of each node separately. That is, one approximates the CPT table at each node with a separable version that is optimal in some sense. Once we have a separable CPT, we can factorize it into smaller CPT tables according to (12).\nWhat should be the optimality criterion in the approximation? Although P(C) is optimal in the least-square sense, i.e.,\nP(C) = arg min X\u2208R(B) ||X \u2212 C||2, (18)\nthere might be other objective function that serves us better such as the Kullback-Leibler distance between the original and the approximated distributions."}, {"heading": "3.2 Learning and Inference on Separable Systems", "text": "Separable systems may offer a significant advantage over general DBNs in terms of speed in parameter learning and state inference. Because the number of parameters for such systems is usually much smaller than that for a general DBN, the parameter learning should require much less data and behave much more stably. This benefit has been the key to the application demonstrated in [Basu01].\nFor a general DBN, the task of updating the posterior distribution still requires a computation that is exponential in number of variables in the system, for instance the forward-backward algorithm. Given that separable BNs are defined so that the marginal distributions are sufficient to predict themselves in the future, it seems plausible that there exists some method for the inference tasks with reduced computation."}, {"heading": "Acknowledgements", "text": "The author wishes to thank Prof. Benjamin Van Roy (Stanford), Prof. George C. Verghese (MIT), and Prof. Sandip Roy (Washington State) for many helpful discussions."}], "references": [{"title": "Influence Model: A Tractable Representations for the Dynamics of Networked Markov Chains, Ph.D", "author": ["C. Asavathiratham"], "venue": null, "citeRegEx": "Asavathiratham.,? \\Q2000\\E", "shortCiteRegEx": "Asavathiratham.", "year": 2000}, {"title": "The Influence Model, IEEE Control Systems", "author": ["C. Asavathiratham", "S. Roy", "B.C. Lesieutre", "G.C. Verghese"], "venue": null, "citeRegEx": "Asavathiratham et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Asavathiratham et al\\.", "year": 2001}, {"title": "Towards Measuring Human Interactions in Conversational Settings", "author": ["S. Basu", "T. Choudhury", "B. Clarkson", "A. Pentland"], "venue": "Proceedings of the IEEE Int\u2019l Workshop on Cues in Communication (CUES", "citeRegEx": "Basu et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Basu et al\\.", "year": 2001}, {"title": "Sufficiency, Separability, and Temporal Probabilistic Models. Uncertainty in Artificial Intelligence", "author": ["A. Pfeffer"], "venue": null, "citeRegEx": "Pfeffer,? \\Q2001\\E", "shortCiteRegEx": "Pfeffer", "year": 2001}], "referenceMentions": [], "year": 2006, "abstractText": "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node\u2019s parents, instead of the joint distributions. We describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof to [Pfeffer00] on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable.", "creator": "dvips(k) 5.94a Copyright 2003 Radical Eye Software"}}}