{"id": "1206.6831", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Pearl's Calculus of Intervention Is Complete", "abstract": "This paper is concerned with graphical criteria that can be used to solve the problem of identifying casual effects from nonexperimental data in a causal Bayesian network structure, i.e., a directed acyclic graph that represents causal relationships with data from other nodes in the same network. The paper explains the relationship, based on previous research on nonlinear linear-gradient models, which provide a detailed understanding of causal relationships. The approach used to study causal relationships in a causal network is that if the causal data were given in the same network, the data would correspond to linear paths (or paths, as the authors propose, that are similar in the order of 100 x 100 x 100 x 100 x 100 x 100). This approach will allow for a very high-level model of the causal relationships that is commonly used in the same network as an ordinary linear-gradient model, which uses linear paths and then works as a hierarchical model for linear paths and the hierarchical networks. This model will also include a set of general models (or paths) for the causal relationships that are considered to be correlated in the causal relationships (e.g., [1], [2], [3], [4], [5], [6], [7], [8], [9], [10] and [11]. The approach also provides a framework for estimating the relationship between data for the causal relationships and the causal relationships that are not. However, it is useful to be more familiar with the causal relationships of those that are associated with a linear-gradient model. We show that the causal relationships that are associated with the linear path, which are in general linear paths, have more similarity to linear paths than those that are associated with a linear path. We also show that these linear paths are used in the causal relationships and are associated with a linear path with the linear path. Specifically, a linear path for the causal relationships of the causal relationships with the linear path (or path) is associated with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the linear path with the", "histories": [["v1", "Wed, 27 Jun 2012 16:17:19 GMT  (119kb)", "http://arxiv.org/abs/1206.6831v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yimin huang", "marco valtorta"], "accepted": false, "id": "1206.6831"}, "pdf": {"name": "1206.6831.pdf", "metadata": {"source": "CRF", "title": "Pearl\u2019s Calculus of Intervention Is Complete", "authors": ["Yimin Huang", "Marco Valtorta"], "emails": ["mgv}@cse.sc.edu"], "sections": [{"heading": null, "text": "This paper is concerned with graphical criteria that can be used to solve the problem of identifying casual effects from nonexperimental data in a causal Bayesian network structure, i.e., a directed acyclic graph that represents causal relationships. We first review Pearl\u2019s work on this topic [Pearl, 1995], in which several useful graphical criteria are presented. Then we present a complete algorithm [Huang and Valtorta, 2006b] for the identifiability problem. By exploiting the completeness of this algorithm, we prove that the three basic do-calculus rules that Pearl presents are complete, in the sense that, if a causal effect is identifiable, there exists a sequence of applications of the rules of the do-calculus that transforms the causal effect formula into a formula that only includes observational quantities."}, {"heading": "1 Introduction", "text": "This paper focuses on graphical criteria used to infer the strength of cause-and-effect relationships from a causal Bayesian network [Pearl, 1995, Pearl, 2000], which is an acyclic directed graph representing nonexperimental data and causal relationships.\nIn the 1990s, some graphical conditions were given to show whether the causal effect, that is, the joint response of any set S of variables to interventions on a set T of action variables, denoted as PT (S)1, is identifiable or not. Those results are summarized in [Pearl, 2000]. For example, \u201cback-door\u201d and \u201cfront-door\u201d criteria and do-calculus [Pearl, 1995]; graphical criteria to identify PT (S) when T is a singleton [Galles and Pearl, 1995]; special graphical conditions under which it is possible\n1The notations P (s|do(t)) and P (s|t\u0302) are used in [Pearl, 2000], and the notation Pt(s) is used in [Tian and Pearl, 2002b, Tian and Pearl, 2003].\nto identify PT (S) [Pearl and Robins, 1995]. Some further study can be also found in [Robins, 1997] and [Kuroki and Miyakawa, 1999]. In all these graphical criteria, Pearl\u2019s three do-calculus (inference) rules are in the core position. All the other graphical rules can be obtained from them. Pearl conjectures that they are sufficient for the identification problem, but the conjecture has remained opened until now.\nIn the current decade, Tian and Pearl published a series of papers related to the identification problem [Tian and Pearl, 2002a, Tian and Pearl, 2002b, Tian and Pearl, 2003]. Their new methods combined the graphical character of causal graph and the algebraic definition of causal effect. They used both algebraic and graphical methods to identify causal effects.\nBased on their work, Huang and Valtorta proved that Tian and Pearl\u2019s identify algorithm for semi-Markovian graphs is complete [Huang and Valtorta, 2006a]. Here, semiMarkovian graphs are defined as causal graphs in which each unobservable variable is a root and has exactly two observable children; semi-Markovian graphs are sometimes defined differently. It has been shown that a transformation between general Bayesian networks and semi-Markovian graphs, defined, e.g., in [Tian and Pearl, 2002b], preserves identifiability [Huang and Valtorta, 2006b]. In [Huang and Valtorta, 2006b], the authors also present an algorithm on general causal Bayesian networks and prove that the algorithm is complete, which means a causal effect is identifiable if and only if the given algorithm runs successfully and returns an expression that is the target causal effect in terms of estimable quantities.\nWe have recently learned that the sufficiency of the three inference rules (with some minor technical limitations) has been proved in [Shpitser and Pearl, 2006]. Our independently obtained result applies to general causal Bayesian networks.\nIn this paper, we review the graphical rules and the complete identify algorithm of [Huang and Valtorta, 2006b]. We consider their relationship and prove that the identify\nalgorithm can be obtained by using the three inference rules. Because of the completeness of the identify algorithm, our proof shows that the three inference rules are sufficient, which confirms Pearl\u2019s conjecture. In the next section we give out the definitions and notations that we use in this paper. In section three, we review some graphical rules for identification problem. We discuss the complete identify algorithm in section four and prove the sufficiency of the three inference rules in section five. Conclusions are included in section six."}, {"heading": "2 Definitions and Notations", "text": "A causal Bayesian network consists of a DAG G over a set of variables V = {V1, . . . , Vn}, called a causal graph, and a probability distribution on V . The interpretation of this kind of model consists of two parts. One is the probabilistic interpretation, which says that each variable in the graph is independent of all its non-descendants given its direct parents; the other is the causal interpretation, which says that the directed edges in G represent causal influences between the corresponding variables [Pearl, 2000, Lauritzen, 2001].\nWe use V (G) to show that V is the variable set of graph G. If it is clear in the context, we also use V directly. Capital characters, like V , are used for variable sets; the lower characters, like v, stand for the instances of variable set V . Capital character like X , Y and Vi are also used for single variable, and their values can be x, y and vi. Normally, we use F (V ) to denote a function on variable set V . An instance of this function is denoted as F (V )(V = v), or F (V )(v), or just F (v). Because all the variables are in the causal graph, we sometimes use node and node set instead of variable and variable set. We use Pa(Vi) to denote parent node set of node Vi in graph G and pa(Vi) as an instance of variable set Pa(Vi). Ch(Vi) is Vi\u2019s children node set; ch(Vi) is an instance of Ch(Vi).\nBased on the probabilistic interpretation of causal Bayesian network, we have that the joint probability function P (v) = P (v1, . . . , vn) can be factorized as\nP (v) = \u220f\nVi\u2208V\nP (vi|pa(Vi)) (1)\nThe causal interpretation of Markovian model enables us to predict intervention effects. Here, intervention means some kind of modification of factors in product (1). The simplest kind of intervention is fixing a subset T \u2286 V of variables to some constants t, denoted by do(T = t) or just do(t), and then the post-intervention distribution\nPT (V )(T = t, V = v) = Pt(v) (2)\nis given by:\nPt(v) =\n{ \u220f\nVi\u2208V \\T P (vi|pa(Vi)) v consistent with t\n0 v inconsistent with t (3)\nWe note explicitly that the post-intervention distribution Pt(v) is a probability distribution.\nWhen all the variables in V are observable, since all P (vi|pai) can be estimated from nonexperimental data, as just indicated, all causal effects are computable. But when some variables in V are unobservable, things are much more complex.\nLet N and U stand for the sets of observable and unobservable variables in graph G respectively, that is V = N \u222a U . The observed probability distribution P (n) = P (N = n), is a mixture of products:\nP (n) = \u2211\nU\n\u220f\nVi\u2208N\nP (vi|pa(Vi)) \u220f\nVj\u2208U\nP (vj |pa(Vj)) (4)\nThe post-intervention distribution Pt(n) = PT=t(N = n) is defined as:\nPt(n) =\n\n  \n  \n\u2211\nU\n\u220f\nVi\u2208N\\T P (vi|pa(Vi))\u00d7\n\u220f\nVj\u2208U P (vj |pa(Vj))\nn consistent with t 0 n inconsistent with t\n(5)\nSometimes what we want to know is not the postintervention distribution for the whole N , but the postintervention distribution Pt(s) of an observable variable subset S \u2282 N , For those two observable variable set S and T , Pt(s) = PT=t(S = s) is given by:\nPt(s) =\n\n  \n  \n\u2211\nVl\u2208(N\\S)\\T\n\u2211\nU\n\u220f\nVi\u2208N\\T P (vi|pa(Vi))\n\u220f\nVj\u2208U P (vj |pa(Vj))\ns consistent with t 0 s inconsistent with t\n(6)\nThe identifiability question is defined as whether the causal effect PT (S), that is all Pt(s) given by (6), can be determined uniquely from the distribution P (N = n) given by (4), and thus independent of the unknown quantities P (vi|pa(Vi))s, where Vi \u2208 U or there are some Vj \u2208 Pa(Vi), Vj \u2208 U .\nWe give out a formal definition of identifiability below, which follows [Tian and Pearl, 2003].\nA Markovian model consists of four elements\nM =< N,U,GN\u222aU , P (vi|pa(Vi)) >\nwhere, (i) N = {N1, . . . , Nm} is a set of observable variables; (ii) U = {U1, . . . , Un} is a set of unobservable\nvariables; (iii) G is a directed acyclic graph with nodes corresponding to the elements of V = N \u222a U ; and (vi) P (vi|pa(Vi)), i = 1, . . . ,m + n, is the conditional probability of variable Vi \u2208 V given its parents Pa(Vi))in G.\nDefinition 1 The causal effect of a set of variables T on a disjoint set of variables S is said to be identifiable from a graph G if all the quantities Pt(s) can be computed uniquely from any positive probability of the observed variables \u2014 that is , if PM1t (s) = P M2 t (s) for every pair of models M1 and M2 with PM1(n) = PM2(n) > 0 and G(M1) = G(M2).\nThis definition means that, given the causal graph G, the quantity Pt(s) can be determined from the observed distribution P (n) alone.\nNormally, when we talk about S and T , we think they are both observable variable subsets of N and mutually disjoint. So, s is always consistent with t in Equation 6.\nWe are sometimes interested in the causal effect on a set of observable variables S due to all other observable variables. In this case, keeping the convention that N stands for the set of all observable variables, T = N\\S. For convenience and for uniformity with [Tian and Pearl, 2002b], we define\nQ[S] = PN\\S(S) (7)\nand interpret this equation as stating that Q[S] is the causal effect of N\\S on S.\nWe define the c-component relation on the unobserved variable set U of graph G as follow: for any U1 \u2208 U and U2 \u2208 U , they are related under the c-component relation if and only if at least one of conditions below is satisfied:\n(i) there is an edge between U1 and U2,\n(ii) U1 and U2 are both parents of the same observable node,\n(iii) both U1 and U2 are in the c-component relation with respect to another node U3 \u2208 U .\nObserve that the c-component relation in U is reflexive, symmetric and transitive, so it defines a partition of U . Based on this relationship, we can therefore divide U into disjoint and mutually exclusive c-component related parts.\nA c-component of variable set V on graph G consists of all the unobservable variables belonging to the same ccomponent related part of U and all observable variables that have an unobservable parent which is a member of that c-component. According to the definition of c-component relation, it is clear that an observable node can only appear in one c-component. If an observable node has no unobservable parent, then itself is a c-component on V . Therefore, the c-components form a partition on all of the variables.\nWe conclude this section by giving several simple graphical definitions that will be needed later.\nFor a given set of variables C, we define the directed unobservable parent set DUP (C). A node V belongs to DUP (C) if and only if both of these two conditions are satisfied: i) V is an unobservable node; ii) there is a directed path from V to an element of C, and all the internal nodes on that path are unobservable nodes.\nFor a given observable variable set C \u2286 N , let GC denote the subgraph of G composed only of variables in C \u222a DUP (C) and all the links between variable pairs in C \u222a DUP (C). Let An(C) be the union of C and the set of ancestors of the variables in C, and let De(C) be the union of C and the set of descendents of the variables in C. An observable variable set S \u2286 N in graph G is called an ancestral set if it contains all its own observed ancestors, i.e., S = An(S) \u2229N ."}, {"heading": "3 Graphical Criteria", "text": "In general, to solve the identifiability problem graphically, there are two things we need to know. The first is a set of inference rules, which can transfer causal effect expressions to equivalent expressions. The second is a sound and complete algorithm based on those rules. Here complete means that, for any causal effect question, we can use this algorithm to answer it, either by generating a final expression just involving ordinary conditional probabilities, which is assessable by empirical observation, or by reporting that the effect is unidentifiable.\nUsing causal Bayesian network, Pearl gives two graphical criteria to check identifiability, and called them back-door criterion and front-door criterion [Pearl, 1993, Pearl, 1995]. Following these results, in [Pearl, 1995] and also in his book Causality [Pearl, 2000], Pearl proposes three inference rules (the do-calculus rules) that allow transformations between sentences concerning interventions and observations. The aim of such rules is to lead to a calculus of interventions and observations on causal Bayesian networks, so that, whenever possible, sentences that involve interventions and observations may be transformed into sentences that involve only observations. Pearl proves that the three do-calculus rules are sound and conjectures that they are sufficient. We present these three inference rules below. We begin by reviewing several definitions from [Pearl, 2000].\nLet X , Y , Z be arbitrary disjoint sets of nodes in a causal graph G. We denote by GX the graph obtained by deleting from G all arrows pointing to nodes in X . Likewise, we denote by GX the graph obtained by deleting from G all arrows emerging from nodes in X . To represent the deletion of both incoming and outgoing arrows, we use the notation GXZ .\nThe expression P (y|x\u0302, z) \u2261 P (y, z|x\u0302)/P (z|x\u0302) stands for the probability of Y = y given that X is held constant at x and that (under this condition) Z = z is observed. Our notation for this probability is Px(y|z). In this section, we use Pearl\u2019s notation in [Pearl, 2000] when quoting his results directly.\nHere are the three inference rules of proposed calculus. Proofs of soundness can be found in [Pearl, 1995].\nTheorem (Rules of Do-Calculus) [Pearl, 2000] Let G be the directed acyclic graph associated with a causal model, and let P ()\u0307 stand for the probability distribution induced by that model. For any disjoint subsets of variables X ,Y ,Z, and W we have the following rules.\nRule 1 (Insertion/deletion of observations)\nP (y|x\u0302, z, w) = P (y|x\u0302, w) if(Y \u22a5 Z|X,W )GX (8)\nRule 2 (Action/observation exchange)\nP (y|x\u0302, z\u0302, w) = P (y|x\u0302, z, w) if(Y \u22a5 Z|X,W )GXZ (9)\nRule 3 (Insertion/deletion of actions)\nP (y|x\u0302, z\u0302, w) = P (y|x\u0302, w) if(Y \u22a5 Z|X,W )G X,Z(W )\n(10) where Z(W ) is the set of Z-nodes that are not ancestors of any W -node in GX .\nIn [Pearl, 2000], the author shows that both his backdoor and front-door criteria can be obtained from these three rules. In [Galles and Pearl, 1995], the authors give out a graphical criterion to identify the causal effect between a singleton variable X and a set of variables Y . Their algorithm works in time polynomial on the number of variables in the graph. This result is also showed in [Pearl, 2000]2. In [Pearl and Robins, 1995], the authors extend the results of [Galles and Pearl, 1995] to the case where T stands for a compound action, consisting of several atomic interventions that are implemented either concurrently or sequentially. They establish a graphical criterion for recognizing when the effect of X on Y is identifiable and, in case the diagram satisfies this criterion, they provide a closed-form expression for the distribution of an outcome variable S under the plan defined by the compound action setting T = t. Following Pearl and Robins\u2019 work, [Kuroki and Miyakawa, 1999] present an extension of the front door criterion.\nAll the criteria cited above are based the three inference rules. Therefore, the proof of sufficiency of the three rules that is provided in this paper paves the road for proofs of sufficiency of other graphical algorithms in this area.\n2We explicitly note that the algorithm given in Section 4.3.1 of [Pearl, 2000], while inspired by the do-calculus, is not complete, as shown in [Tian and Pearl, 2003]."}, {"heading": "4 A Sound and Complete Identification Algorithm", "text": "In this section we present a complete identification algorithm.\nFor a given model (causal Bayesian network) with graph G, We begin with removing all unobservable nodes that have no observable descendants. From the definitions in section two, it is easy to prove that this transformation does not change the identifiability of the model [Huang and Valtorta, 2006a].\nBelow are two lemmas proved by Tian and Pearl in [Tian and Pearl, 2002b].\nLemma 1 Let W \u2286 C \u2286 N . If W is an ancestral set in GC , then\n\u2211\nVi\u2208C\\W\nQ[C] = Q[W ] (11)\nLemma 2 Let H \u2286 N , and we have c-components H \u20321, . . . ,H \u2032 n in the sub graph GH , Hi = H \u2032 i \u2229 H , 1 6 i 6 n, then\n(i) Q[H] can be decomposed as\nQ[H] = n \u220f\ni=1\nQ[Hi] (12)\n(ii) Each Q[Hi] is computable from Q[H], in the following way. Let k be the number of variables in H , and let a topological order of variables in H be Vh1 < . . . < Vhk in GH . Let H(j) = {Vh1 , . . . , Vhj} be the set of variables in H ordered before Vhj ( including Vhj ), j = 1, . . . , k, and H(0) = \u03c6. Then each Q[Hi],i = 1, . . . , n, is given by\nQ[Hi] = \u220f\n{j|Vhj\u2208Hi}\nQ[H(j)]\nQ[H(j\u22121)] (13)\nwhere each Q[H(j)], j = 0, 1, . . . , k, is given by\nQ[H(j)] = \u2211\nh\\h(j)\nQ[H] (14)\nAssume that N(G) is partitioned into N1, . . . , Nk in G, where each Ni belongs to a c-component, and that we have c-components S\u20321, . . . , S \u2032 l in graph GS , Sj = S \u2032 j \u2229 S, 1 6 j 6 l. Based on lemma 2, for any model on graph G, we have\nQ[S] = l \u220f\nj=1\nQ[Sj ] (15)\nBecause each Sj ,j = 1, . . . , l, which is a c-component in GS , is a subgraph of G, it must be included in one Nj , Nj \u2208 {N1, . . . , Nk}.\nThe rest of this section is devoted to three algorithms.\nAlgorithm Computing Q[S]\nINPUT: S \u2286 N\nOUTPUT: Expression for Q[S] or FAIL\nLet N(G) be partitioned into N1, . . . , Nk, each of them belonging to a c-components in G, and S be partitioned into S1, . . . , Sl, each of them belonging to a c-components in GS , and Sj \u2286 Nj . We can\ni) Compute each Q[Nj ] with lemma 2\nii) Compute each Q[Sj ] with Identify algorithm below with C = Sj ,T = Nj ,Q = Q[Nj ]\niii) If in ii), we get Fail as return value of Identify algorithm of any Sj , then Q[S] is unidentifiable in graph G; else Q[S] is identifiable and equal to \u220fl\nj=1 Q[Sj ] (by lemma 2)\nAlgorithm Identify (C,T ,Q)\nINPUT: C \u2286 T \u2286 N , Q = Q[T ], GT and GC are both composed of one single c-component\nOUTPUT: Expression for Q[C] in terms of Q or FAIL\nLet A = An(C)GT \u2229 T\ni) If A = C, output Q[C], which is equal to \u2211\nT\\C Q[T ] by lemma 1\nii) Else if A = T , output FAIL\niii) Else (if C \u2282 A \u2282 T )\n1. Compute Q[A] = \u2211\nT\\A Q[T ] with lemma 1\n2. Assume that in GA, C is contained in a c-component T \u20321, T1 = T \u2032 1 \u2229A.\n3. Compute Q[T1] from Q[A] with lemma 2\n4. Output Identify(C,T1,Q[T1])\nTo compute PT (S), we can rewrite it as:\nPt(s) = \u2211\nN\\(T\u222aS)\nPt(n\\t) = \u2211\nN\\(T\u222aS)\nQ[N\\T ] (16)\nLet D = An(S)GN\\T \u2229 N . D is an ancestral set in graph GN\\T . Lemma 1 allows us to conclude that \u2211\nN\\(T\u222aD) Q[N\\T ] = Q[D]. Therefore, we have:\nPt(s) = \u2211\nD\\S\n\u2211\nN\\(T\u222aD)\nQ[N\\T ] = \u2211\nD\\S\nQ[D] (17)\nAlgorithm Computing PT (S)\nINPUT: two disjoint observable variable sets S, T \u2282 N\nOUTPUT: the expression for PT (S) or FAIL\n1. Let D = An(S)GN\\T \u2229N\n2. Use the Computing Q[S] algorithm to compute Q[D]\n3. If the algorithm returns FAIL, then output FAIL\n4. Else, output PT (S) = \u2211 D\\S Q[D]\nThe authors, in [Huang and Valtorta, 2006b], prove that:\nTheorem 1 The above algorithm for computing PT (S) is sound and complete.\nNote that the soundness of the algorithms above can be obtained from lemma 1, 2 and standard probability manipulations. We will exploit this property in the next section."}, {"heading": "5 Completeness of the Three Inference Rules of Pearl\u2019s Do-Calculus", "text": "In this section, we prove the three inference rules are complete. As already mentioned, the soundness of the three rules is proved in [Pearl, 1995]. Here we just need to prove their sufficiency.\nNote that the sound and complete algorithm for computing PT (S) in the last section is obtained by using lemma 1 and lemma 2. If we can show that lemma 1 and lemma 2 can be obtained through just using the three inference rules and standard probability manipulations, then the sufficiency of the three rules is proved.\nWe begin with the following observation:\nLemma 3 If any of the three rules can be used on a model with graph G, it can also be used on a model that is obtained by removing all unobservable nodes that have no observable descendants.\nProof: This follows from the well-known result that barren nodes can be removed without changing the d-separation relation for the other nodes [Shachter, 1986].\nWe also have:\nLemma 4 Rule 1 follows from rule 2 and rule 3.\nProof: Since removing an edge can only d-separate more nodes in a Bayesian network, the conditions for the application of rules 2 and 3 are satisfied if the condition for rule 1 is satisfied. We can replace the application of rule 1 by the application of rule 2 followed by the application of rule 3. In detail, by applying rule 2, we have that P (y|x\u0302, z, w) = P (y|x\u0302, z\u0302, w). By applying rule 3, we have that P (y|x\u0302, z\u0302, w) = P (y|x\u0302, w). So, P (y|x\u0302, z, w) = P (y|x\u0302, w), which is the result of applying rule 1.\nWe now show that lemma 1 and lemma 2 can be obtained through just using rule 2, rule 3 and standard probability manipulations. For convenience, we define V m1 =\n{V1, V2, . . . , Vm} and V\u0302 m1 = {V\u03021, V\u03022, . . . , V\u0302m}. Here, Vi, 1 6 i 6 m can be any variable or variable set; in the case of sets, the comma should be understood as the union operator.\nLemma 5 Lemma 1 follows from rule 3.\nProof: We recall the definition of ancestral set. An observable variable set S \u2286 N in graph G is called an ancestral set if it contains all its own observed ancestors, i.e., S = An(S) \u2229 N . Because G is a DAG, GC , which include all nodes in C \u222aDUP (C), is a DAG also. If W is an ancestral set in GC , then there is a topological order of nodes in GC that starts with all the nodes in W and continues with the other nodes. If W = N , the lemma is trivially true. Otherwise, consider the first node, say X , in the topological order just described and that is in C but not in W . So, what we need to prove is: If W \u2282 N , node X \u2208 N\\W , C = W \u222a {X}, and W is an ancestral set in GC , then\n\u2211\nX\nQ[C] = Q[W ] (18)\nRecall that for any S \u2282 N , by definition Q[S] = PN\\S(S). So, equation 18 can be rewritten as\n\u2211\nX\nPN\\C(C) = PN\\W (W ) (19)\nAssume W = {X1, . . . ,Xk} = Xk1 , Y = N\\(W \u222a{X}), where, unlike X , Y is a variable set here. Equation 19 becomes\n\u2211\nX\nPY (X k 1 ,X) = PY,X(X k 1 ) (20)\nUsing the graphical language Pearl used, what we want to prove is\n\u2211\nX\nP (Xk1 ,X|Y\u0302 ) = P (X k 1 |Y\u0302 , X\u0302) (21)\nWe know that \u2211\nX P (X k 1 ,X|Y\u0302 ) =\n\u2211\nX P (X|X k 1 , Y\u0302 )P (X k 1 |Y\u0302 )\n= ( \u2211 X P (X|X k 1 , Y\u0302 ))P (X k 1 |Y\u0302 )\n(22)\nNow, we use rule 3 of the do-calculus. Note that we can apply this rule, because for P (Xk1 |Y\u0302 ), we have\n({Xk1 } \u22a5 X|Y )GY ,X (23)\nThis is because in graph GY ,X , if there is a d-connected path from X to a node Xi, 1 6 i 6 k, that path could not include any node in Y , because Y nodes can only be divergent nodes and Y is given. If that path just goes from X through some unobservable nodes to Xi, it would mean\nthat topologically X is before Xi in graph GC , which could not be true. So, that kind of path does not exist.\nUsing rule 3 we obtain\nP (Xk1 |Y\u0302 ) = P (X k 1 |Y\u0302 , X\u0302) (24)\nSo, what we need to prove is just\n\u2211\nX\nP (X|Xk1 , Y\u0302 )) = 1 (25)\nThis is obvious, since Py is a probability distribution.\nLemma 6 Lemma 2 follows from rule 2 and rules 3.\nProof: When H just includes one observable variable, (i) and (ii) in lemma 2 are clearly true.\nAssume (i) and (ii) are still true for any observable variable set H \u2282 N , where the size of H is less than or equal to integer k.\nConsider an arbitrary observable variable set E of size k+ 1, and assume variable X \u2208 E is topologically after all the variables in H = E\\{X} in graph G. Assume H \u222a DUP (H) can be divided into c-component H \u20321, . . . ,H \u2032 n in graph GH , and Hi = H \u2032i \u2229 H ,1 6 i 6 n. Let Y = N\\E. Also assume X and H1,H2, . . . ,Hm, 0 6 m 6 n construct a c-component in graph GE . (If m = 0, then X is a c-component by itself.)\nSince the size of H is k, we have the inductive hypothesis\nQ[H] =\nn \u220f\ni=1\nQ[Hi] (26)\nwhich means:\nP (H|Y\u0302 , X\u0302) = \u220fn i=1 P (Hi|H\u0302 i\u22121 1 , H\u0302 n i+1, Y\u0302 , X\u0302) (27)\nWhat we want to prove is that (i) and (ii) are still true for E.\nFor (i), we want to prove\nP (H,X|Y\u0302 ) = P (Hm1 ,X|H\u0302 n m+1, Y\u0302 )\u00d7 \u220fn\nj=m+1 P (Hj |H\u0302 j\u22121 1 , H\u0302 n j+1, Y\u0302 , X\u0302)\n(28)\nNote that we have\nP (H,X|Y\u0302 ) = P (X|H, Y\u0302 )P (H|Y\u0302 ) (29)\nWe know that, in graph GY ,X , (H \u22a5 {X}|Y ). This is because if there is a d-connected path from X to any node of H in GY ,X , that path could not include any node in Y , since Y nodes can only be divergent nodes. Then that path must go from X to one node of H , and this is impossible because we assume X is topologically after all nodes in H .\nBased on rule 3, we have\nP (H|Y\u0302 ) = P (H|X\u0302, Y\u0302 ) (30)\nThen 29 can be rewritten as\nP (H,X|Y\u0302 ) = P (X|H, Y\u0302 )P (H|X\u0302, Y\u0302 ) = P (X|H, Y\u0302 )\u00d7 \u220fm\nj=1 P (Hj |H\u0302 j\u22121 1 , H\u0302 n j+1, Y\u0302 , X\u0302)\u00d7\n\u220fn j=m+1 P (Hj |H\u0302 j\u22121 1 , H\u0302 n j+1, Y\u0302 , X\u0302)\n(31) Based on the induction assumption, we have\n\u220fm j=1 P (Hj |H\u0302 j\u22121 1 , H\u0302 n j+1, Y\u0302 , X\u0302) = P (H m 1 |H\u0302 n m+1, Y\u0302 , X\u0302)\n(32) Just as before, we know that in graph GY,Hn\nm+1,X ,\n({Hm1 } \u22a5 {X}|{Y,H n m+1}). This is because if there is a d-connected path from X to any node of Hnm+1 in GY,Hn\nm+1,X , that path could not go through any node in\nY , for all Y nodes can only be divergent nodes. Then that path must go from X to one node of H . This is impossible because we assume X is topologically after all nodes in H .\nWith rule 3, we have\nP (Hm1 |H\u0302 n m+1, Y\u0302 , X\u0302) = P (H m 1 |H\u0302 n m+1, Y\u0302 ) (33)\nWe now show that (X \u22a5 {Hnm+1}|Y,H m 1 ) in graph GY ,Hn m+1 . If there is a d-connected path from X to a node Z in {Hnm+1} in GY ,Hn m+1 , that path could not go through any node in Y ; assume Z \u2032 is the nearest observable node to Z on that path, Z \u2032 \u2208 {Hm1 }. If there are some unobservable nodes on that path between Z and Z \u2032, then Z and Z \u2032 belong to the some c-component, (because Z \u2032 can only be a convergent node on that path and the path gets into Z), which is impossible; a link from Z to Z \u2032 is impossible because all links exiting from Z are removed, and a link from Z \u2032 to Z would not open the connection between Z \u2032 and Z, because Z \u2032 is known. So, Z \u2032 does not exist. If there are some unobservable nodes between X and Z, then X and Z belong to the same c-component in graph GE (because there must be a divergent unobservable node path between them, otherwise, X is topologically before Z), but this is impossible because Z is in {Hnm+1}. A link from X to Z is also impossible, because we assume X is topologically after all nodes in H . So, X and Z are d-separated.\nBased on rule 2, we have\nP (X|H, Y\u0302 ) = P (X|Hm1 ,H n m+1, Y\u0302 ) = P (X|Hm1 , H\u0302 n m+1, Y\u0302 )\n(34)\nPutting 31, 32, 33, and 34 together, we have:\nP (H,X|Y\u0302 ) = P (X|H, Y\u0302 ) \u220fm\nj=1 P (Hj |H\u0302 j\u22121 1 , H\u0302 n j+1, Y\u0302 , X\u0302)\u00d7\n\u220fn j=m+1 P (Hj |H\u0302 j\u22121 1 , H\u0302 n j+1, Y\u0302 , X\u0302) = P (X|Hm1 , H\u0302 n m+1, Y\u0302 )\u00d7 P (H m 1 |H\u0302 n m+1, Y\u0302 )\u00d7 \u220fn j=m+1 P (Hj |H\u0302 j\u22121 1 , H\u0302 n j+1, Y\u0302 , X\u0302) = P (Hm1 ,X|H\u0302 n m+1, Y\u0302 )\u00d7 \u220fn\nj=m+1 P (Hj |H\u0302 j\u22121 1 , H\u0302 n j+1, Y\u0302 , X\u0302)\n(35)\nNow let us consider the second part of this lemma.\nFrom lemma 1, we have\nQ[H] = \u2211\nX\nQ(H,X) = \u2211\nX\nQ[E] (36)\nOur inductive assumption is that H satisfies (ii), where H(j) = {Vh1 , . . . , Vhj} is the set of variables in H ordered before Vhj (including Vhj ). Then each Q[Hi],i = 1, . . . , n, is given by\nQ[Hi] = \u220f\n{j|Vhj\u2208Hi}\nQ[H(j)]\nQ[H(j\u22121)] (37)\nwhere each Q[H(j)], j = 0, 1, . . . , k, is given by\nQ[H(j)] = \u2211\nh\\h(j)\nQ[H] (38)\nFrom equation 36, we have\nQ[E(j)] = \u2211\n{h,x}\\h(j)\nQ[E] = \u2211\nh\\h(j)\nQ[H] = Q[H(j)]\n(39) From (i) we have\nQ[E] = Q[E(k+1)] = Q[Hm1 ,X]\nn \u220f\ni=m+1\nQ[Hi] (40)\nWe have\nQ[Hm1 ,X] = Q[E (k+1)]/\nn \u220f\ni=m+1\nQ[Hi] (41)\nThe chain decomposition allows us to write\nQ[Ek+1] =\nk+1 \u220f\nj=0\nQ[E(j)]/Q[E(j\u22121)] (42)\nand for each m+ 1 6 i 6 n,\nQ[Hi] = \u220f\n{j|Vhj\u2208Hi}\nQ[H(j)]\nQ[H(j\u22121)] =\n\u220f\n{j|Vhj\u2208Hi}\nQ[E(j)]\nQ[E(j\u22121)]\n(43)\nSo, equation 41 can be rewritten as\nQ[Hm1 ,X] = Q[E (k+1)]/\n\u220fn\ni=m+1 Q[Hi] = \u220f\n{j|Vhj\u2208H m 1 ,X}\nQ[E(j)] Q[E(j\u22121)]\n(44)\nPutting the lemmas in this section together, we have\nTheorem 2 The three inference rules, together with standard probability manipulations, are complete for determining identifiability of all effects of PT (S).\nTheorem 2 confirms Pearl\u2019s conjecture that the three rules are sufficient."}, {"heading": "6 Conclusion", "text": "In this paper, we prove that the do-calculus method of [Pearl, 1995] is complete, in the sense that, if a causal effect is identifiable, there exists a sequence of applications of the rules of the do-calculus that transforms the causal effect formula into one that only includes observational quantities.\nIn fact the constructive proofs in the fifth section do not just show us those rules are complete, but they also provide a formal recursive algorithms to do calculation with rule 2 and 3 when we need lemma 1 or 2. Together with the algorithm we gave in section four, we obtain a formal procedure to solve the identifiability problem and compute causal effects with graphical rules 2 and 3. Clearly, this procedure is complete too. We are not claiming that the procedure just outlined is guided by the structure of the causal graph in a way that would be easy to understand for a causal modeler: this remains an issue to be studied further."}], "references": [{"title": "Testing identifiability of causal effects", "author": ["Galles", "Pearl", "D. 1995] Galles", "J. Pearl"], "venue": "In Proceedings of UAI-95,", "citeRegEx": "Galles et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Galles et al\\.", "year": 1995}, {"title": "On the completeness of an identifiability algorithm for semi-Markovian models", "author": ["Huang", "Valtorta", "Y. 2006a] Huang", "M. Valtorta"], "venue": "Technical report,", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "A study of identifiability in causal Bayesian network", "author": ["Huang", "Valtorta", "Y. 2006b] Huang", "M. Valtorta"], "venue": "Technical report,", "citeRegEx": "Huang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2006}, {"title": "Identifiability criteria for causal effects", "author": ["Kuroki", "Miyakawa", "M. 1999] Kuroki", "M. Miyakawa"], "venue": null, "citeRegEx": "Kuroki et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kuroki et al\\.", "year": 1999}, {"title": "Probabilistic evaluation of sequential plans from causal models with hidden variables", "author": ["Pearl", "Robins", "J. 1995] Pearl", "J.M. Robins"], "venue": "In Proceedings of UAI-95,", "citeRegEx": "Pearl et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Pearl et al\\.", "year": 1995}, {"title": "Identification of joint interventional distributions in recursive semi-Markovian causal models. Technical report, Cognitive Systems Laboratory, University of California at Los Angeles", "author": ["Shpitser", "Pearl", "I. 2006] Shpitser", "J. Pearl"], "venue": null, "citeRegEx": "Shpitser et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shpitser et al\\.", "year": 2006}, {"title": "A general identification condition for causal effects", "author": ["Tian", "Pearl", "J. 2002a] Tian", "J. Pearl"], "venue": "In Proceedings of the Eighteenth National Conference on Artificial Intelligence", "citeRegEx": "Tian et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2002}, {"title": "On the testable implications of causal models with hidden variables", "author": ["Tian", "Pearl", "J. 2002b] Tian", "J. Pearl"], "venue": "In Proceedings of UAI-02,", "citeRegEx": "Tian et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2002}, {"title": "On the identification of causal effects, Technical report 290-L. Technical report, Cognitive Systems Laboratory, University of California at Los Angeles. Extended version available at http://www.cs.iastate.edu/ jtian/r290-L.pdf", "author": ["Tian", "Pearl", "J. 2003] Tian", "J. Pearl"], "venue": null, "citeRegEx": "Tian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tian et al\\.", "year": 2003}], "referenceMentions": [], "year": 2006, "abstractText": "This paper is concerned with graphical criteria that can be used to solve the problem of identifying casual effects from nonexperimental data in a causal Bayesian network structure, i.e., a directed acyclic graph that represents causal relationships. We first review Pearl\u2019s work on this topic [Pearl, 1995], in which several useful graphical criteria are presented. Then we present a complete algorithm [Huang and Valtorta, 2006b] for the identifiability problem. By exploiting the completeness of this algorithm, we prove that the three basic do-calculus rules that Pearl presents are complete, in the sense that, if a causal effect is identifiable, there exists a sequence of applications of the rules of the do-calculus that transforms the causal effect formula into a formula that only includes observational quantities.", "creator": "dvips(k) 5.94b Copyright 2004 Radical Eye Software"}}}