{"id": "1207.0099", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2012", "title": "Density-Difference Estimation", "abstract": "We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of first estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small error incurred in the first stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. The usefulness of the proposed method is also demonstrated experimentally. First, we used the finite-sample approximation to estimate the variance that is proportional to the first stage. We then apply a zero-sum distribution of a linear polynomial distribution of the predicted variance. We then apply the exponential sum in the first stage to the mean density (0.6%). It is equivalent to the probability density (0.7%), and the probability density (0.8%), and the probability density (0.9%) are the two-step estimates for the observed variance. The two-step procedure can be used to estimate the error density using an additional step. We can compute a two-step estimate by taking a first step. A double-wise linear polynomial distribution of the predicted variance. For the estimated number of possible error densities, the first step is used to estimate the value of the fixed-parametric polynomial distribution of the predicted variance. We can use the single-shot optimization (also known as the Boolim function) to estimate the distribution of the estimated variance.\n\n\n\nThe main problem in this paper is that one of the main differences in the number of possible error densities and the number of possible error densities is the fact that it is an estimate of the true probability densities by a single-shot algorithm. The problem is that an initial error from a single-shot algorithm is the same as an initial error from a single-shot algorithm. This means that a second error from the first step can be considered as the original error and thus the same number of possible error densities. The problem is that a second error from a single-shot algorithm is the same as an initial error from a single-shot algorithm, so the result is a single-shot algorithm. However, the two-step version", "histories": [["v1", "Sat, 30 Jun 2012 14:21:46 GMT  (3469kb)", "http://arxiv.org/abs/1207.0099v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["masashi sugiyama", "takafumi kanamori", "taiji suzuki", "marthinus christoffel du plessis", "song liu", "ichiro takeuchi"], "accepted": true, "id": "1207.0099"}, "pdf": {"name": "1207.0099.pdf", "metadata": {"source": "CRF", "title": "Density-Difference Estimation", "authors": ["Masashi Sugiyama"], "emails": ["sugi@cs.titech.ac.jp", "kanamori@is.nagoya-u.ac.jp", "s-taiji@stat.t.u-tokyo.ac.jp", "christo@sg.cs.titech.ac.jp", "song@sg.cs.titech.ac.jp", "takeuchi.ichiro@nitech.ac.jp"], "sections": [{"heading": null, "text": "Keywords\ndensity difference, L2-distance, robustness, Kullback-Leibler divergence, kernel density estimation."}, {"heading": "1 Introduction", "text": "When estimating a quantity consisting of two elements, a two-stage approach of first estimating the two elements separately and then approximating the target quantity based on the estimates of the two elements often performs poorly, because the first stage is carried out without regard to the second stage and thus a small error incurred in the first stage can cause a big error in the second stage. To cope with this problem, it would be more appropriate to directly estimate the target quantity in a single-shot process without separately estimating the two elements.\nA seminal example that follows this general idea is pattern recognition by the support vector machine (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998): Instead of separately estimating two probability distributions of patterns for positive and negative classes, the support vector machine directly learns the boundary between the two classes that is sufficient for pattern recognition. More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.\nIn this paper, we further explore this line of research, and propose a method for directly estimating the difference between two probability densities in a single-shot process. Density differences are useful for various purposes such as class-balance estimation under class-prior change (Saerens et al., 2002; Du Plessis & Sugiyama, 2012), change-point detection in time series (Kawahara & Sugiyama, 2012; Liu et al., 2012), feature extraction (Torkkola, 2003), video-based event detection (Matsugu et al., 2011), flow cytometric data analysis (Duong et al., 2009), ultrasound image segmentation (Liu et al., 2010), non-rigid image registration (Atif et al., 2003), and image-based target recognition (Gray\n& Principe, 2010).\nFor this density-difference estimation problem, we propose a single-shot method, called the least-squares density-difference (LSDD) estimator, that directly estimates the density difference without separately estimating two densities. LSDD is derived within a framework of kernel least-squares estimation, and its solution can be computed analytically in a computationally efficient and stable manner. Furthermore, LSDD is equipped with crossvalidation, and thus all tuning parameters such as the kernel width and the regularization parameter can be systematically and objectively optimized. We derive a finite-sample error bound for the LSDD estimator in a non-parametric setup and show that it achieves the optimal convergence rate.\nWe also apply LSDD to L2-distance estimation and show that it is more accurate than the difference of KDEs, which tends to severely under-estimate the L2-distance (Anderson et al., 1994). Compared with the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951), the L2-distance is more robust against outliers (Basu et al., 1998; Scott, 2001; Besbeas & Morgan, 2004).\nFinally, we experimentally demonstrate the usefulness of LSDD in semi-supervised\nclass-prior estimation and unsupervised change detection.\nThe rest of this paper is structured as follows. In Section 2, we derive the LSDD method and investigate its theoretical properties. In Section 3, we show how the L2distance can be approximated by LSDD. In Section 4, we illustrate the numerical behavior of LSDD. Finally, we conclude in Section 5."}, {"heading": "2 Density-Difference Estimation", "text": "In this section, we propose a single-shot method for estimating the difference between two probability densities from samples, and analyze its theoretical properties."}, {"heading": "2.1 Problem Formulation and Naive Approach", "text": "First, we formulate the problem of density-difference estimation.\nSuppose that we are given two sets of independent and identically distributed sam-\nples X := {xi}ni=1 and X \u2032 := {x\u2032i\u2032}n \u2032 i\u2032=1 drawn from probability distributions on R d with densities p(x) and p\u2032(x), respectively:\nX := {xi}ni=1 i.i.d.\u223c p(x),\nX \u2032 := {x\u2032i\u2032}n \u2032 i\u2032=1 i.i.d.\u223c p\u2032(x).\nOur goal is to estimate the difference f(x) between p(x) and p\u2032(x) from the samples X and X \u2032:\nf(x) := p(x)\u2212 p\u2032(x).\nA naive approach to density-difference estimation is to use kernel density estimators (KDEs) (Silverman, 1986). For Gaussian kernels, the KDE-based density-difference estimator is given by\nf\u0303(x) := p\u0302(x)\u2212 p\u0302\u2032(x),\nwhere\np\u0302(x) := 1\nn(2\u03c0\u03c32)d/2\nn\u2211\ni=1\nexp ( \u2212\u2016x\u2212 xi\u2016 2\n2\u03c32\n) ,\np\u0302\u2032(x) := 1\nn\u2032(2\u03c0\u03c3\u20322)d/2\nn\u2032\u2211\ni\u2032=1\nexp ( \u2212\u2016x\u2212 x \u2032 i\u2032\u20162\n2\u03c3\u20322\n) .\nThe Gaussian widths \u03c3 and \u03c3\u2032 may be determined based on cross-validation (Ha\u0308rdle et al.,\n2004).\nHowever, we argue that the KDE-based density-difference estimator is not the best approach because of its two-step nature: Small estimation error in each density estimate can cause a big error in the final density-difference estimate. More intuitively, good density estimators tend to be smooth and thus a density-difference estimator obtained from such smooth density estimators tends to be over-smoothed (Hall & Wand, 1988; Anderson et al., 1994, see also numerical experiments in Section 4.1.1).\nTo overcome this weakness, we give a single-shot procedure of directly estimating the\ndensity difference f(x) without separately estimating the densities p(x) and p\u2032(x)."}, {"heading": "2.2 Least-Squares Density-Difference Estimation", "text": "In our proposed approach, we fit a density-difference model g(x) to the true densitydifference function f(x) under the squared loss:\nargmin g\n\u222b (g(x)\u2212 f(x))2 dx. (1)\nWe use the following linear-in-parameter model as g(x):\ng(x) =\nb\u2211\n\u2113=1\n\u03b8\u2113\u03c8\u2113(x) = \u03b8 \u22a4\u03c8(x), (2)\nwhere b denotes the number of basis functions, \u03c8(x) = (\u03c81(x), . . . , \u03c8b(x)) \u22a4 is a bdimensional basis function vector, \u03b8 = (\u03b81, . . . , \u03b8b) \u22a4 is a b-dimensional parameter vector, and \u22a4 denotes the transpose. In practice, we use the following non-parametric Gaussian kernel model as g(x):\ng(x) = n+n\u2032\u2211\n\u2113=1\n\u03b8\u2113 exp\n( \u2212\u2016x\u2212 c\u2113\u2016 2\n2\u03c32\n) , (3)\nwhere (c1, . . . , cn, cn+1, . . . , cn+n\u2032) := (x1, . . . ,xn,x \u2032 1, . . . ,x \u2032 n\u2032) are Gaussian kernel centers. If n + n\u2032 is large, we may use only a subset of x1, . . . ,xn,x \u2032 1, . . . ,x \u2032 n\u2032 as Gaussian kernel centers.\nFor the model (2), the optimal parameter \u03b8\u2217 is given by\n\u03b8\u2217 := argmin \u03b8\n\u222b (g(x)\u2212 f(x))2 dx\n= argmin \u03b8\n[\u222b g(x)2dx\u2212 2 \u222b g(x)f(x)dx ]\n= argmin \u03b8\n[ \u03b8\u22a4H\u03b8 \u2212 2h\u22a4\u03b8 ]\n=H\u22121h,\nwhere H is the b\u00d7 b matrix and h is the b-dimensional vector defined as\nH := \u222b \u03c8(x)\u03c8(x)\u22a4dx,\nh := \u222b \u03c8(x)p(x)dx\u2212 \u222b \u03c8(x\u2032)p\u2032(x\u2032)dx\u2032.\nNote that, for the Gaussian kernel model (3), the integral in H can be computed analytically as\nH\u2113,\u2113\u2032 =\n\u222b exp ( \u2212\u2016x\u2212 c\u2113\u2016 2\n2\u03c32\n) exp ( \u2212\u2016x\u2212 c\u2113\u2032\u2016 2\n2\u03c32\n) dx\n= (\u03c0\u03c32)d/2 exp ( \u2212\u2016c\u2113 \u2212 c\u2113\u2032\u2016 2\n4\u03c32\n) ,\nwhere d denotes the dimensionality of x.\nReplacing the expectations in h by empirical estimators and adding an \u21132-regularizer\nto the objective function, we arrive at the following optimization problem:\n\u03b8\u0302 := argmin \u03b8\n[ \u03b8\u22a4H\u03b8 \u2212 2h\u0302\u22a4\u03b8 + \u03bb\u03b8\u22a4\u03b8 ] , (4)\nwhere \u03bb (\u2265 0) is the regularization parameter and h\u0302 is the b-dimensional vector defined as\nh\u0302 = 1\nn\nn\u2211\ni=1\n\u03c8(xi)\u2212 1\nn\u2032\nn\u2032\u2211\ni\u2032=1\n\u03c8(x\u2032i\u2032).\nTaking the derivative of the objective function in Eq.(4) and equating it to zero, we can obtain the solution \u03b8\u0302 analytically as\n\u03b8\u0302 = (H + \u03bbIb) \u22121 h\u0302,\nwhere Ib denotes the b-dimensional identity matrix.\nFinally, a density-difference estimator f\u0302(x) is given as\nf\u0302(x) = \u03b8\u0302 \u22a4 \u03c8(x). (5)\nWe call this the least-squares density-difference (LSDD) estimator."}, {"heading": "2.3 Theoretical Analysis", "text": "Here, we theoretically investigate the behavior of the LSDD estimator."}, {"heading": "2.3.1 Parametric Convergence", "text": "First, we consider a linear parametric setup where basis functions in our density-difference model (2) are fixed.\nSuppose that n/(n+ n\u2032) converges to \u03b7 \u2208 [0, 1]. Then the central limit theorem (Rao,\n1965) asserts that \u221a nn\u2032\nn+n\u2032 (\u03b8\u0302\u2212 \u03b8\u2217) converges in law to the normal distribution with mean\n0 and covariance matrix\nH\u22121((1\u2212 \u03b7)V p + \u03b7V p\u2032)H\u22121,\nwhere V p denotes the covariance matrix of \u03c8(x) under the probability density p(x):\nV p := \u222b ( \u03c8(x)\u2212\u03c8p ) ( \u03c8(x)\u2212\u03c8p )\u22a4 p(x)dx, (6)\nand \u03c8p denotes the expectation of \u03c8(x) under the probability density p(x):\n\u03c8p :=\n\u222b \u03c8(x)p(x)dx.\nThis result implies that the LSDD estimator has asymptotic normality with asymptotic\norder \u221a 1/n+ 1/n\u2032, which is the optimal convergence rate in the parametric setup."}, {"heading": "2.3.2 Non-Parametric Error Bound", "text": "Next, we consider a non-parametric setup where a density-difference function is learned in a Gaussian reproducing kernel Hilbert space (RKHS) (Aronszajn, 1950).\nLet H\u03b3 be the Gaussian RKHS with width \u03b3:\nk\u03b3(x,x \u2032) = exp ( \u2212\u2016x\u2212 x\n\u2032\u20162 \u03b32\n) .\nLet us consider a slightly modified LSDD estimator that is more suitable for non-\nparametric error analysis: For n\u2032 = n,\nf\u0302 := argmin g\u2208H\u03b3\n[ \u2016g\u20162L2 \u2212 2 ( 1\nn\nn\u2211\ni=1\ng(xi) + 1\nn\nn\u2211\ni\u2032=1\ng(x\u2032i\u2032) ) + \u03bb\u2016g\u20162H\u03b3 ] ,\nwhere \u2016 \u00b7 \u2016L2 denotes the L2-norm and \u2016 \u00b7 \u2016H\u03b3 denotes the norm in RKHS H\u03b3 .\nThen we can prove that, for all \u03c1, \u03c1\u2032 > 0, there exists a constant K such that, for all \u03c4 \u2265 1 and n \u2265 1, the non-parametric LSDD estimator with appropriate choice of \u03bb and \u03b3 satisfies1\n\u2016f\u0302 \u2212 f\u20162L2 + \u03bb\u2016f\u0302\u20162H\u03b3 \u2264 K ( n\u2212 2\u03b1 2\u03b1+d +\u03c1 + \u03c4n\u22121+\u03c1 \u2032 ) , (7)\nwith probability not less than 1\u22124e\u2212\u03c4 . Here, d denotes the dimensionality of input vector x, and \u03b1 \u2265 0 denotes the regularity of Besov space to which the true density-difference function f belongs (smaller/larger \u03b1 means f is \u201cless/more complex\u201d; see Appendix A for its precise definition). Because n\u2212 2\u03b1 2\u03b1+d is the optimal learning rate in this setup (Eberts & Steinwart, 2011), the above result shows that the non-parametric LSDD estimator achieves the optimal convergence rate.\nIt is known that, if the naive KDE with a Gaussian kernel is used for estimating a probability density with regularity \u03b1 > 2, the optimal learning rate cannot be achieved (Farrell, 1972; Silverman, 1986). To achieve the optimal rate by KDE, we should choose a kernel specifically tailored to each regularity \u03b1 (Parzen, 1962). But such a kernel is not non-negative and it is difficult to implement in practice. On the other hand, our LSDD estimator can always achieve the optimal learning rate with a Gaussian kernel without regard to regularity \u03b1.\n1Because our theoretical result is highly technical, we only describe a rough idea here. More precise statement of the result and its complete proof are provided in Appendix A, where we utilize the mathematical technique developed in Eberts and Steinwart (2011) for a regression problem."}, {"heading": "2.4 Model Selection by Cross-Validation", "text": "The above theoretical analyses showed the superiority of LSDD. However, the practical performance of LSDD depends on the choice of models (i.e., the kernel width \u03c3 and the regularization parameter \u03bb). Here, we show that the model can be optimized by crossvalidation (CV).\nMore specifically, we first divide the samples X = {xi}ni=1 and X \u2032 = {x\u2032i\u2032}n \u2032 i\u2032=1 into T disjoint subsets {Xt}Tt=1 and {X \u2032t}Tt=1, respectively. Then we obtain a density-difference estimate f\u0302t(x) from X\\Xt and X \u2032\\X \u2032t (i.e., all samples without Xt and X \u2032t ), and compute its hold-out error for Xt and X \u2032t as\nCV(t) := \u222b f\u0302t(x) 2dx\u2212 2|Xt| \u2211\nx\u2208Xt\nf\u0302t(x) + 2 |X \u2032t | \u2211\nx\u2032\u2208X \u2032t\nf\u0302t(x \u2032),\nwhere |X | denotes the number of elements in the set X . We repeat this hold-out validation procedure for t = 1, . . . , T , and compute the average hold-out error as\nCV := 1\nT\nT\u2211\nt=1\nCV(t).\nFinally, we choose the model that minimizes CV.\nA MATLABR\u00a9 implementation of LSDD is available from\nhttp://sugiyama-www.cs.titech.ac.jp/~sugi/software/LSDD/\u2019.\n(to be made public after acceptance)\n3 L2-Distance Estimation by LSDD\nIn this section, we consider the problem of approximating the L2-distance between p(x) and p\u2032(x),\nL2(p, p\u2032) := \u222b (p(x)\u2212 p\u2032(x))2 dx, (8)\nfrom samples X := {xi}ni=1 and X \u2032 := {x\u2032i\u2032}n \u2032 i\u2032=1 (see Section 2.1)."}, {"heading": "3.1 Basic Form", "text": "For an equivalent expression\nL2(p, p\u2032) = \u222b f(x)p(x)dx\u2212 \u222b f(x\u2032)p\u2032(x\u2032)dx\u2032,\nif we replace f(x) with an LSDD estimator f\u0302(x) and approximate the expectations by empirical averages, the following L2-distance estimator can be obtained:\nL2(p, p\u2032) \u2248 h\u0302\u22a4\u03b8\u0302. (9)\nSimilarly, for another expression\nL2(p, p\u2032) = \u222b f(x)2dx,\nreplacing f(x) with an LSDD estimator f\u0302(x) gives another L2-distance estimator:\nL2(p, p\u2032) \u2248 \u03b8\u0302\u22a4H\u03b8\u0302. (10)"}, {"heading": "3.2 Reduction of Bias Caused by Regularization", "text": "Eq.(9) and Eq.(10) themselves give approximations to L2(p, p\u2032). Nevertheless, we argue that the use of their combination, defined by\nL\u03022(X ,X \u2032) := 2h\u0302\u22a4\u03b8\u0302 \u2212 \u03b8\u0302\u22a4H\u03b8\u0302, (11)\nis more sensible. To explain the reason, let us consider a generalized L2-distance estimator of the following form:\n\u03b2h\u0302 \u22a4 \u03b8\u0302 + (1\u2212 \u03b2)\u03b8\u0302\u22a4H\u03b8\u0302, (12)\nwhere \u03b2 is a real scalar. If the regularization parameter \u03bb (\u2265 0) is small, then Eq.(12) can be expressed as\n\u03b2h\u0302 \u22a4 \u03b8\u0302 + (1\u2212 \u03b2)\u03b8\u0302\u22a4H\u03b8\u0302 = h\u0302\u22a4H\u22121h\u0302\u2212 \u03bb(2\u2212 \u03b2)h\u0302\u22a4H\u22122h\u0302+ op(\u03bb), (13)\nwhere op denotes the probabilistic order (its derivation is given in Appendix B).\nThus, the bias introduced by regularization (i.e., the second term in the right-hand side of Eq.(13) that depends on \u03bb) can be eliminated if \u03b2 = 2, which yields Eq.(11). Note that, if no regularization is imposed (i.e., \u03bb = 0), both Eq.(9) and Eq.(10) yield h\u0302 \u22a4 H\u22121h\u0302, the first term in the right-hand side of Eq.(13).\nEq.(11) is actually equivalent to the negative of the optimal objective value of the LSDD optimization problem without regularization (i.e., Eq.(4) with \u03bb = 0). This can be naturally interpreted through a lower bound of L2(p, p\u2032) obtained by Legendre-Fenchel\nconvex duality (Rockafellar, 1970):\nL2(p, p\u2032) = sup g\n[ 2 (\u222b g(x)p(x)dx\u2212 \u222b g(x)p\u2032(x)dx ) \u2212 \u222b g(x)2dx ] ,\nwhere the supremum is attained at g = f . If the expectations are replaced by empirical estimators and the linear-in-parameter model (2) is used as g, the above optimization problem is reduced to the LSDD objective function without regularization (see Eq.(4)). Thus, LSDD corresponds to approximately maximizing the above lower bound and Eq.(11) is its maximum value.\nThrough eigenvalue decomposition of H , we can show that\n2h\u0302 \u22a4 \u03b8\u0302 \u2212 \u03b8\u0302\u22a4H\u03b8\u0302 \u2265 h\u0302\u22a4\u03b8\u0302 \u2265 \u03b8\u0302\u22a4H\u03b8\u0302.\nThus, our approximator (11) is not less than the plain approximators (9) and (10).\n3.3 Further Bias Correction\nh\u0302 \u22a4 H\u22121h\u0302, the first term in Eq.(13), is an essential part of the L2-distance estimator (11). However, it is actually a slightly biased estimator of the target quantity h\u22a4H\u22121h (= \u03b8\u2217\u22a4H\u03b8\u2217 = h\u22a4\u03b8\u2217):\nE[h\u0302 \u22a4 H\u22121h\u0302] = h\u22a4H\u22121h+ tr ( H\u22121 ( 1\nn V p +\n1\nn\u2032 V p\u2032\n)) , (14)\nwhere E denotes the expectation over all samples X = {xi}ni=1 and X \u2032 = {x\u2032i\u2032}n \u2032 i\u2032=1, and V p and V p\u2032 are defined by Eq.(6) (its derivation is given in Appendix C).\nThe second term in the right-hand side of Eq.(14) is an estimation bias that is generally non-zero. Thus, based on Eq.(14), we can construct a bias-corrected L2-distance estimator\nas\nL\u03032(X ,X \u2032) := 2h\u0302\u22a4\u03b8\u0302 \u2212 \u03b8\u0302\u22a4H\u03b8\u0302 \u2212 tr ( H\u22121 ( 1\nn V\u0302 p +\n1\nn\u2032 V\u0302 p\u2032\n)) , (15)\nwhere V\u0302 p is an empirical estimator of covariance matrix V p:\nV\u0302 p := 1\nn\nn\u2211\ni=1\n( \u03c8(xi)\u2212 \u03c8\u0302p )( \u03c8(xi)\u2212 \u03c8\u0302p )\u22a4 ,\nand \u03c8\u0302p is an empirical estimator of the expectation \u03c8p:\n\u03c8\u0302p := 1\nn\nn\u2211\ni=1\n\u03c8(xi).\nThe true L2-distance is non-negative by definition (see Eq.(8)), but the above biascorrected estimate can take a negative value. Following the same line as Baranchik (1964), the positive-part estimator may be more accurate:\nL 2 (X ,X \u2032) := max { 0, L\u03032(X ,X \u2032) } .\nHowever, in our preliminary experiments, L 2 (X ,X \u2032) does not always perform well particularly when H is ill-conditioned. For this reason, we practically propose to use L\u03022(X ,X \u2032) defined by Eq.(11)."}, {"heading": "4 Experiments", "text": "In this section, we experimentally evaluate the performance of LSDD."}, {"heading": "4.1 Numerical Examples", "text": "First, we show numerical examples using artificial datasets."}, {"heading": "4.1.1 LSDD vs. KDE", "text": "We experimentally compare the behavior of LSDD and the KDE-based method. Let\np(x) = N(x; (\u00b5, 0, . . . , 0)\u22a4, (4\u03c0)\u22121Id),\np\u2032(x) = N(x; (0, 0, . . . , 0)\u22a4, (4\u03c0)\u22121Id),\nwhere N(x;\u00b5,\u03a3) denotes the multi-dimensional normal density with mean vector \u00b5 and variance-covariance matrix \u03a3 with respect to x, and Id denotes the d-dimensional identity matrix.\nWe first illustrate how LSDD and KDE behave under d = 1 and n = n\u2032 = 200. Figure 1 depicts the data samples, densities and density difference estimated by KDE, and density difference estimated by LSDD for \u00b5 = 0 (i.e., f(x) = p(x)\u2212 p\u2032(x) = 0). This shows that LSDD gives a more accurate estimate of the density difference f(x) than KDE. Figure 2 depicts the results for \u00b5 = 0.5 (i.e., f(x) 6= 0), showing again that LSDD performs well.\nNext, we compare the L2-distance approximator based on LSDD and that based on KDE. For \u00b5 = 0, 0.2, 0.4, 0.6, 0.8 and d = 1, 5, we draw n = n\u2032 = 200 samples from the above p(x) and p\u2032(x). Figure 3 depicts the mean and standard error of estimated L2-distances over 100 runs as functions of mean \u00b5. When d = 1, the LSDD-based L2distance estimator gives accurate estimates of the true L2-distance, whereas the KDEbased L2-distance estimator slightly underestimates the true L2-distance. This is caused by the fact that KDE tends to provide smoother density estimates (see Figure 2(c) again). Such smoother density estimates are accurate as density estimates, but the difference of smoother density estimates yields a smaller L2-distance estimate (Anderson et al., 1994).\nThis tendency is more significant when d = 5; the KDE-based L2-distance estimator severely underestimates the true L2-distance, which is a typical drawback of the twostep procedure. On the other hand, the LSDD-based L2-distance estimator still gives reasonably accurate estimates of the true L2-distance even when d = 5.\n4.1.2 L2-Distance vs. KL-Divergence\nThe Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951) is a popular divergence measure for comparing probability distributions. The KL-divergence from p(x) to p\u2032(x) is defined as\nKL(p\u2016p\u2032) := \u222b p(x) log p(x)\np\u2032(x) dx.\nFirst, we illustrate the difference between the L2-distance and the KL-divergence. For\nd = 1, let\np(x) = (1\u2212 \u03b7)N(x; 0, 12) + \u03b7N(x;\u00b5, 1/42),\np\u2032(x) = N(x; 0, 12).\nImplications of the above densities are that samples drawn from N(x; 0, 12) are inliers, whereas samples drawn from N(x;\u00b5, 1/42) are outliers. We set the outlier rate at \u03b7 = 0.1 and the outlier mean at \u00b5 = 0, 2, 4, . . . , 10 (see Figure 4).\nFigure 5 depicts the L2-distance and the KL-divergence for outlier mean \u00b5 = 0, 2, 4, . . . , 10. This shows that both the L2-distance and the KL-divergence increase as \u00b5 increases. However, the L2-distance is bounded from above, whereas the KL-divergence diverges to infinity as \u00b5 tends to infinity. This result implies that the L2-distance is less sensitive to outliers than the KL-divergence, which well agrees with the observation given in Basu et al. (1998).\nNext, we draw n = n\u2032 = 100 samples from p(x) and p\u2032(x), and estimate the L2-distance by LSDD and the KL-divergence by the Kullback-Leibler importance estimation procedure2 (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010). Figure 6 depicts the estimated L2-distance and KL-divergence for outlier mean \u00b5 = 0, 2, 4, . . . , 10 over 100 runs. This shows that both LSDD and KLIEP reasonably capture the profiles of the true L2-distance and the KL-divergence, although the scale of KLIEP values is much different from the true values (see Figure 5) because the estimated normalization factor was unreliable.\nFinally, based on the permutation test procedure (Efron & Tibshirani, 1993), we conduct hypothesis testing of the null hypothesis that densities p and p\u2032 are the same. More specifically, we first compute a distance estimator for the original datasets X and X \u2032 and obtain D\u0302(X ,X \u2032). Next, we randomly permute the |X \u222a X \u2032| samples, and assign the first |X | samples to a set X\u0303 and the remaining |X \u2032| samples to another set X\u0303 \u2032. Then we compute the distance estimator again using the randomly permuted datasets X\u0303 and X\u0303 \u2032 and obtain D\u0303(X\u0303 , X\u0303 \u2032). Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distri2Estimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; Pe\u0301rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010). Among them, KLIEP was shown to possess a superior convergence property and demonstrated to work well in practice. KLIEP is based on direct estimation of density ratio p(x)/p\u2032(x) without density estimation of p(x) and p\u2032(x).\nbution, D\u0303(X\u0303 , X\u0303 \u2032) would take a value close to zero. This random permutation procedure is repeated many times, and the distribution of D\u0303(X\u0303 , X\u0303 \u2032) under the null hypothesis (i.e., the two distributions are the same) is constructed. Finally, the p-value is approximated by evaluating the relative ranking of D\u0302(X ,X \u2032) in the histogram of D\u0303(X\u0303 , X\u0303 \u2032). We set the significance level at 5%.\nFigure 7 depicts the rejection rate of the null hypothesis for outlier rate \u03b7 = 0.1 and outlier mean \u00b5 = 0, 2, 4, . . . , 10, based on the L2-distance estimated by LSDD and the KL-divergence estimated by KLIEP. This shows that the KLIEP-based test rejects the null hypothesis more frequently for large \u00b5, whereas the rejection rate of the LSDD-based test is kept almost constant even when \u00b5 is changed. This result implies that the twosample test by LSDD is more robust against outliers (i.e., two distributions tend to be regarded as the same even in the presence of outliers) than the KLIEP-based test.\nFigure 8 depicts the rejection rate of the null hypothesis for outlier mean \u00b5 = 10 for outlier rate \u03b7 = 0, 0.05, 0.1, . . . , 0.35. When \u03b7 = 0 (i.e., no outliers), both the LSDD-based test and the KLIEP-based test accept the null hypothesis with the designated significance level approximately. When \u03b7 = 0.1, the LSDD-based test still keeps a low rejection rate, whereas the KLIEP-based test tends to reject the null hypothesis. When \u03b7 \u2265 0.3, the LSDD-based test and the KLIEP-based test tend to reject the null hypothesis in a similar way."}, {"heading": "4.2 Applications", "text": "Next, we apply LSDD to semi-supervised class-balance estimation under class prior change and change-point detection in time series."}, {"heading": "4.2.1 Semi-Supervised Class-Balance Estimation", "text": "In real-world pattern recognition tasks, changes in class balance are often observed. Then significant estimation bias can be caused since the class balance in the training dataset does not reflect that of the test dataset.\nHere, we consider a pattern recognition task of classifying pattern x \u2208 Rd to class y \u2208 {+1,\u22121}. Our goal is to learn the class balance of a test dataset in a semi-supervised learning setup where unlabeled test samples are provided in addition to labeled training samples (Chapelle et al., 2006). The class balance in the test set can be estimated by matching a mixture of class-wise training input densities,\n\u03c0ptrain(x|y = +1) + (1\u2212 \u03c0)ptrain(x|y = \u22121),\nwith the test input density ptest(x) (Saerens et al., 2002), where \u03c0 \u2208 [0, 1] is a mixing coefficient to learn. See Figure 9 for schematic illustration. Here, we use the L2-distance estimated by LSDD and the difference of KDEs for this distribution matching.\nWe use four UCI benchmark datasets3, where we randomly choose 20 labeled training samples from each class and 50 unlabeled test samples following true class-prior \u03c0\u2217 = 0.1, 0.2, . . . , 0.9. Figure 10 plots the mean and standard error of the squared difference between true and estimated class balances \u03c0 and the misclassification error by a weighted regularized least-squares classifier (Rifkin et al., 2003) over 1000 runs. The results show that LSDD tends to provide better class-balance estimates, which are translated into\n3http://archive.ics.uci.edu/ml/\nlower classification errors."}, {"heading": "4.2.2 Unsupervised Change Detection", "text": "The objective of change detection is to discover abrupt property changes behind timeseries data.\nLet y(t) \u2208 Rm be an m-dimensional time-series sample at time t, and let\nY (t) := [y(t)\u22a4,y(t+ 1)\u22a4, . . . ,y(t+ k \u2212 1)\u22a4]\u22a4 \u2208 Rkm\nbe a subsequence of time series at time t with length k. We treat the subsequence Y (t) as a sample, instead of a single point y(t), by which time-dependent information can be incorporated naturally (Kawahara & Sugiyama, 2012). Let Y(t) be a set of r retrospective subsequence samples starting at time t:\nY(t) := {Y (t),Y (t+ 1), . . . ,Y (t+ r \u2212 1)}.\nOur strategy is to compute a certain dissimilarity measure between two consecutive segments Y(t) and Y(t+r), and use it as the plausibility of change points (see Figure 11). As a dissimilarity measure, we use the L2-distance estimated by LSDD and the KL-divergence\nestimated by the KL importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010). We set k = 5 and r = 50.\nFirst, we use the IPSJ SIG-SLP Corpora and Environments for Noisy Speech Recognition (CENSREC) dataset4 provided by the National Institute of Informatics, Japan, which records human voice in a noisy environment such as a restaurant. The top graph in Figure 12(a) displays the original time-series, where true change points were manually annotated. The bottom two graphs in Figure 12(a) plot change scores obtained by KLIEP and LSDD, showing that the LSDD-based change score indicates the existence of change points more clearly than the KLIEP-based change score.\nNext, we use a dataset taken from the Human Activity Sensing Consortium (HASC) challenge 2011 5, which provides human activity information collected by portable threeaxis accelerometers. Because the orientation of the accelerometers is not necessarily fixed, we take the \u21132-norm of the 3-dimensional data. The top graph in Figure 12(b) displays the original time-series for a sequence of actions \u201cjog\u201d, \u201cstay\u201d, \u201cstair down\u201d, \u201cstay\u201d, and \u201cstair up\u201d (there exists 4 change points at time 540, 1110, 1728, and 2286). The bottom two graphs in Figure 12(b) depict the change scores obtained by KLIEP and LSDD, showing that the LSDD score is much more stable and interpretable than the KLIEP score."}, {"heading": "5 Conclusions", "text": "In this paper, we proposed a method for directly estimating the difference between two probability density functions without density estimation. The proposed method, called the least-squares density-difference (LSDD), was derived within a framework of kernel least-squares estimation, and its solution can be computed analytically in a computation-\n4http://research.nii.ac.jp/src/en/CENSREC-1-C.html 5http://hasc.jp/hc2011/\nally efficient and stable manner. Furthermore, LSDD is equipped with cross-validation, and thus all tuning parameters such as the kernel width and the regularization parameter can be systematically and objectively optimized. We showed the asymptotic normality of LSDD in a parametric setup and derived a finite-sample error bound for LSDD in a non-parametric setup. In both cases, LSDD achieves the optimal convergence rate.\nWe also proposed an L2-distance estimator based on LSDD, which nicely cancels a bias caused by regularization. The LSDD-based L2-distance estimator was experimentally shown to be more accurate than the difference of kernel density estimators and more robust against outliers than Kullback-Leibler divergence estimation.\nDensity-difference estimation is a novel research paradigm in machine learning, and we have given a simple but useful method for this emerging topic. Our future work will develop more powerful algorithms for density-difference estimation and explores a variety of applications."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Wittawat Jitkrittum for his comments. Masashi Sugiyama was supported by MEXT KAKENHI 23300069, Takafumi Kanamori was supported by MEXT KAKENHI 24500340, Taiji Suzuki was supported by MEXT KAKENHI 22700289 and the Aihara Project, the FIRST program from JSPS initiated by CSTP, Marthinus Christoffel du Plessis was supported by MEXT Scholarship, Song Liu was supported by the JST PRESTO program, and Ichiro Takeuchi was supported by MEXT KAKENHI 23700165."}, {"heading": "A Technical Details of Non-Parametric Convergence", "text": "Analysis in Section 2.3.2\nFirst, we define linear operators Pn, P, P \u2032 n, P \u2032, Qn, Q as\nPnf := 1\nn\nn\u2211\ni=1\nf(xi), P f :=\n\u222b\nRd\nf(x)p(x)dx,\nP \u2032nf := 1\nn\nn\u2211\ni=1\nf(x\u2032i), P \u2032f :=\n\u222b\nRd\nf(x)p\u2032(x)dx,\nQnf := Pnf \u2212 P \u2032nf, Qf := Pf \u2212 P \u2032f.\nLet H\u03b3 be an RKHS endowed with the Gaussian kernel with width \u03b3:\nk\u03b3(x,x \u2032) = exp ( \u2212\u2016x\u2212 x\n\u2032\u20162 \u03b32\n) .\nA density-difference estimator f\u0302 is obtained as\nf\u0302 := argmin f\u2208H\u03b3\n[ \u2016f\u20162L2(Rd) \u2212 2Qnf + \u03bb\u2016f\u20162H\u03b3 ] .\nWe assume the following conditions:\nAssumption 1. The densities are bounded: There exists M such that\n\u2016p\u2016\u221e \u2264 M and \u2016p\u2032\u2016\u221e \u2264 M.\nThe density difference f = p\u2212 p\u2032 is a member of Besov space with regularity \u03b1: f \u2208 B\u03b12,\u221e and, for r = \u230a\u03b1\u230b+ 1 where \u230a\u03b1\u230b denotes the largest integer less than or equal to \u03b1,\n\u2016f\u2016B\u03b12,\u221e := \u2016f\u2016L2(Rd) + sup t>0 (t\u2212\u03b1\u03c9r,L2(Rd)(f, t)) < c,\nwhere B\u03b12,\u221e is the Besov space with regularity \u03b1 and \u03c9r,L2(Rd) is the r-th modulus of smoothness (see Eberts and Steinwart (2011) for the definitions).\nThen we have the following theorem.\nTheorem 2. Suppose Assumption 1 is satisfied. Then, for all \u01eb > 0 and p \u2208 (0, 1), there exists a constant K > 0 depending on M, c, \u01eb, p such that for all n \u2265 1, \u03c4 \u2265 1, and \u03bb > 0, the LSDD estimator f\u0302 in H\u03b3 satisfies\n\u2016f\u0302 \u2212 f\u20162L2(Rd)+\u03bb\u2016f\u0302\u20162H\u03b3 \u2264 K ( \u03bb\u03b3\u2212d+\u03b32\u03b1+ \u03b3\u2212(1\u2212p)(1+\u01eb)d \u03bbpn + \u03b3\u2212 2(1\u2212p)d 1+p (1+\u01eb+ 1\u2212p 4 )\n\u03bb 3p\u2212p2 1+p n 2 1+p\n+ \u03c4 n2\u03bb + \u03c4 n\n) ,\nwith probability not less than 1\u2212 4e\u2212\u03c4 .\nTo prove this, we utilize the technique developed in Eberts and Steinwart (2011) for\na regression problem.\nProof. First, note that\n\u2016f\u0302\u20162L2(Rd) \u2212 2Qnf\u0302 + \u2016f\u20162L2(Rd) + \u03bb\u2016f\u0302\u20162H\u03b3 \u2264 \u2016f0\u20162L2(Rd) \u2212 2Qnf0 + \u2016f\u20162L2(Rd) + \u03bb\u2016f0\u20162H\u03b3 .\nTherefore, we have\n\u2016f\u0302 \u2212 f\u20162L2(Rd) + \u03bb\u2016f\u0302\u20162H\u03b3\n= \u2016f\u0302\u20162L2(Rd) \u2212 2Qnf\u0302 + \u2016f\u20162L2(Rd) + 2(Qn \u2212Q)f\u0302 + \u03bb\u2016f\u0302\u20162H\u03b3 \u2264 \u2016f0\u20162L2(Rd) \u2212 2Qnf0 + \u2016f\u20162L2(Rd) + 2(Qn \u2212Q)f\u0302 + \u03bb\u2016f\u0302\u20162H\u03b3 = \u2016f0\u20162L2(Rd) \u2212 2Qf0 + \u2016f\u20162L2(Rd) + 2(Qn \u2212Q)(f\u0302 \u2212 f0) + \u03bb\u2016f\u0302\u20162H\u03b3 = \u2016f0 \u2212 f\u20162L2(Rd) + 2(Qn \u2212Q)(f\u0302 \u2212 f) + 2(Qn \u2212Q)(f \u2212 f0) + \u03bb\u2016f\u0302\u20162H\u03b3 . (16)\nLet\nK(x) :=\nr\u2211\nj=1\n( r\nj\n) (\u22121)1\u2212j 1\njd\n( 2\n\u03b3 \u221a \u03c0\n) d 2\nexp ( \u22122\u2016x\u2016 2\nj2\u03b32\n) ,\nand f\u0303(x) := (\u03b3 \u221a \u03c0)\u2212 d 2 f . Using K and f\u0303 , we define\nf0 := K \u2217 f\u0303 := \u222b\nRd\nf\u0303(y)K(x\u2212 y)dy,\ni.e., f0 is the convolution of K and f\u0303 . Because of Lemma 2 in Eberts and Steinwart (2011), we have f0 \u2208 H\u03b3 and\n\u2016f0\u2016H\u03b3 \u2264 (2r \u2212 1)\u2016f\u0303\u2016L2(Rd) (\u2235 Lemma 2 of Eberts and Steinwart (2011))\n\u2264 (2r \u2212 1)(\u03b3 \u221a \u03c0)\u2212 d 2\u2016f\u2016L2(Rd) \u2264 (2r \u2212 1)(\u03b3 \u221a \u03c0)\u2212 d 2 (\u2016p\u2016L2(Rd) + \u2016p\u2032\u2016L2(Rd)) \u2264 (2r \u2212 1)(\u03b3 \u221a \u03c0)\u2212 d 22 \u221a M. (17)\nMoreover, Lemma 3 in Eberts and Steinwart (2011) gives\n\u2016f0\u2016\u221e \u2264 (2r \u2212 1)\u2016f\u2016\u221e \u2264 (2r \u2212 1)M, (18)\nand Lemma 1 in Eberts and Steinwart (2011) yields that there exists a constant Cr,2 such that\n\u2016f0 \u2212 f\u20162L2(Rd) \u2264 Cr,2\u03c92r,L2(Rd)(f, \u03b3\n2 ) \u2264 Cr,2c2\u03b32\u03b1. (19)\nNow, following a similar line to Theorem 3 in Eberts and Steinwart (2011), we can\nshow that, for all \u01eb > 0 and p \u2208 (0, 1), there exists a constant C\u01eb,p such that\n|(Pn \u2212 P )(f\u0302 \u2212 f)| \u2264 f\u0302 \u2212 f.\nTo bound this, we derive the tail probability of\n(Pn \u2212 P ) (\nf\u0302 \u2212 f \u2016f\u0302 \u2212 f\u20162\nL2(Rd) + \u03bb\u2016f\u0302\u20162H\u03b3 + r\n) ,\nwhere r > 0 is a positive real such that r > r\u2217 for\nr\u2217 = min f\u2208H\u03b3 \u2016f \u2212 f\u20162L2(Rd) + \u03bb\u2016f\u20162H\u03b3 .\nLet\ngf,r = f \u2212 f\n\u2016f \u2212 f\u20162 L2(Rd) + \u03bb\u2016f\u20162H\u03b3 + r\nfor f \u2208 H\u03b3 and r > r\u2217. Then we have\n\u2016gf,r\u2016\u221e \u2264 \u2016f\u2016\u221e + \u2016f\u2016\u221e\n\u2016f \u2212 f\u20162 L2(Rd) + \u03bb\u2016f\u20162H\u03b3 + r\n\u2264 \u2016f\u2016H\u03b3 + \u2016f\u2016\u221e\u2016f \u2212 f\u20162 L2(Rd) + \u03bb\u2016f\u20162H\u03b3 + r \u2264 1 \u03bb\u2016f\u2016H\u03b3 + r/\u2016f\u2016H\u03b3 + M r \u2264 1 2 \u221a r\u03bb + M r ,\nand\nPg2f,r = P (f \u2212 f)2\n(\u2016f \u2212 f\u20162 L2(Rd) + \u03bb\u2016f\u20162H\u03b3 + r)2 \u2264 M\u2016f \u2212 f\u20162L2(Rd) (\u2016f \u2212 f\u20162 L2(Rd) + \u03bb\u2016f\u20162H\u03b3 + r)2 \u2264 M r .\nHere, let\nFr := {f \u2208 H\u03b3 | \u2016f \u2212 f\u20162L2(Rd) + \u03bb\u2016f\u20162H\u03b3 \u2264 r},\nand we assume that there exists a function such that\nE [ sup f\u2208Fr |(Pn \u2212 P )(f \u2212 f)| ] \u2264 \u03d5n(r),\nwhere E denotes the expectation over all samples. Then, by the peeling device (see Theorem 7.7 in Steinwart & Christmann, 2008), we have\nE sup f\u2208H\u03b3\n|(Pn \u2212 P )gf,r| \u2264 8\u03d5(r)\nr .\nTherefore, by Talagrand\u2019s concentration inequality, we have\nPr [ sup f\u2208H\u03b3 |(Pn \u2212 P )gf,r| < 10\u03d5n(r) r + \u221a 2M\u03c4 nr + 14\u03c4 3n ( 1 2 \u221a r\u03bb + M r )] \u2265 1\u2212 e\u2212\u03c4 , (20)\nwhere Pr[\u00b7] denotes the probability of an event.\nFrom now on, we give an upper bound of \u03d5n. The RKHS H\u03b3 can embedded in arbitrary Sobolev space Wm(Rd). Indeed, by the proof of Theorem 3.1 in Steinwart and Scovel (2004), we have\n\u2016f\u2016Wm(Rd) \u2264 Cm\u03b3\u2212 m 2 + d 4 \u2016f\u2016H\u03b3\nfor all f \u2208 H\u03b3. Moreover, the theories of interpolation spaces give that, for all f \u2208 Wm(Rd), the supremum norm of f can be bounded as\n\u2016f\u2016\u221e \u2264 C \u2032m\u2016f\u2016 1\u2212 d 2m L2(Rd) \u2016f\u2016 d 2m Wm(Rd) ,\nif d < 2m. Here we set m = d 2p . Then we have\n\u2016f\u2016\u221e \u2264 C \u2032\u2032p\u2016f\u20161\u2212pL2(Rd)\u2016f\u2016 p H\u03b3 \u03b3\u2212 d(1\u2212p) 4 .\nNow, since Fr \u2282 (r/\u03bb)1/2BH\u03b3 and\nP (f \u2212 f)2 \u2264 M\u2016f \u2212 f\u20162L2(Rd) \u2264 Mr for f \u2208 Fr\nhold from Theorem 7.16 and Theorem 7.34 in Steinwart and Christmann (2008), we can take\n\u03d5n(r) = max { C1,p,\u01eb\u03b3 \u2212 (1\u2212p)(1+\u01eb)d 2\n( r \u03bb ) p 2 (Mr) 1\u2212p 2 n\u22121/2,\nC2,p,\u01eb\u03b3 \u2212\n(1\u2212p)(1+\u01eb)d 1+p ( r \u03bb ) p 1+p [( r \u03bb ) p 2 \u03b3\u2212 d(1\u2212p) 4 r 1\u2212p 2 ] 1\u2212p 1+p n\u22121/(1+p) } ,\nwhere \u01eb > 0 and p \u2208 (0, 1) are arbitrary and C1,p,\u01eb, C2,p,\u01eb are constants depending on p, \u01eb. In the same way, we can also obtain a bound of supf\u2208H\u03b3 |(P \u2032n \u2212 P \u2032)gf,r|.\nIf we set r to satisfy\n1 8 \u2265 10\u03d5n(r) r +\n\u221a 2M\u03c4\nnr +\n14\u03c4\n3n\n( 1\n2 \u221a r\u03bb\n+ M\nr\n) , (21)\nthen we have\n|(Qn \u2212Q)(f\u0302 \u2212 f)| \u2264 1\n4\n( r + \u2016f\u0302 \u2212 f\u20162L2(Rd) + \u03bb\u2016f\u0302\u2016H\u03b3 ) , (22)\nwith probability 1\u2212 2e\u2212\u03c4 . To satisfy Eq.(21), it suffices to set\nr = C\n( \u03b3\u2212(1\u2212p)(1+\u01eb)d\n\u03bbpn +\n\u03b3\u2212 2(1\u2212p)d 1+p (1+\u01eb+ 1\u2212p 4 )\n\u03bb 3p\u2212p2 1+p n 2 1+p\n+ \u03c4\nn2\u03bb +\n\u03c4 n\n) , (23)\nwhere C is a sufficiently large constant depending on M, \u01eb, p.\nFinally, we bound the term (Qn \u2212Q)(f0 \u2212 f). By Bernstein\u2019s inequality, we have\n|(Pn \u2212 P )(f0 \u2212 f)| \u2264 C ( \u2016f \u2212 f0\u2016L2(P ) \u221a \u03c4\nn +\n2rM\u03c4\nn\n)\n\u2264 C (\u221a 2M\u2016f \u2212 f0\u2016L2(Rd) \u221a \u03c4\nn +\n2rM\u03c4\nn\n)\n\u2264 C ( \u2016f \u2212 f0\u20162L2(Rd) + 2M\u03c4\nn +\n2rM\u03c4\nn\n) , (24)\nwith probability 1 \u2212 e\u2212\u03c4 , where C is a universal constant. In a similar way, we can also obtain\n|(P \u2032n \u2212 P \u2032)(f0 \u2212 f)| \u2264 C ( \u2016f \u2212 f0\u20162L2(Rd) + 2M\u03c4\nn +\n2rM\u03c4\nn\n) .\nCombining these inequalities, we have\n|(Qn \u2212Q)(f0 \u2212 f)| \u2264 C ( \u2016f \u2212 f0\u20162L2(Rd) + 2rM\u03c4\nn\n) , (25)\nwith probability 1\u2212 2e\u2212\u03c4 , where C is a universal constant.\nSubstituting Eqs.(22) and (25) into Eq.(16), we have\n\u2016f\u0302 \u2212 f\u20162L2(Rd) + \u03bb\u2016f\u0302\u20162H\u03b3\n\u2264 2 { \u2016f0 \u2212 f\u20162L2(Rd) + C ( \u2016f \u2212 f0\u20162L2(Rd) + 2rM\u03c4\nn\n) + r + \u03bb\u2016f0\u2016H\u03b3 } ,\nwith probability 1\u2212 4e\u2212\u03c4 . Moreover, by Eqs.(19) and (17), the right-hand side is further bounded by\n\u2016f\u0302 \u2212 f\u20162L2(Rd) + \u03bb\u2016f\u0302\u20162H\u03b3 \u2264 C { \u03b32\u03b1 + r + \u03bb\u03b3\u2212d + 1 + \u03c4\nn\n} ,\nFinally, substituting (23) into the right-hand side, we have\n\u2016f\u0302 \u2212 f\u20162L2(Rd) + \u03bb\u2016f\u0302\u20162H\u03b3\n\u2264 C { \u03b32\u03b1 + \u03b3\u2212(1\u2212p)(1+\u01eb)d\n\u03bbpn +\n\u03b3\u2212 2(1\u2212p)d 1+p (1+\u01eb+ 1\u2212p 4 )\n\u03bb 3p\u2212p2 1+p n 2 1+p\n+ \u03bb\u03b3\u2212d + \u03c4\n\u03bbn2 +\n\u03c4 n\n} ,\nwith probability 1\u2212 4e\u2212\u03c4 for \u03c4 \u2265 1. This gives the assertion.\nIf we set\n\u03bb = n\u2212 2\u03b1+d (2\u03b1+d)(1+p)+(\u01eb\u2212p+\u01ebp) , \u03b3 = n\u2212 1 (2\u03b1+d)(1+p)+(\u01eb\u2212p+\u01ebp) ,\nand take \u01eb, p sufficiently small, then we immediately have the following corollary.\nCorollary 1. Suppose Assumption 1 is satisfied. Then, for all \u03c1, \u03c1\u2032 > 0, there exists a constant K > 0 depending on M, c, \u03c1, \u03c1\u2032 such that for all n \u2265 1, \u03c4 \u2265 1, the densitydifference estimator f\u0302 with appropriate choice of \u03b3 and \u03bb satisfies\n\u2016f\u0302 \u2212 f\u20162L2(Rd) + \u03bb\u2016f\u0302\u20162H\u03b3 \u2264 K ( n\u2212 2\u03b1 2\u03b1+d +\u03c1 + \u03c4\nn1\u2212\u03c1\u2032\n) , (26)\nwith probability not less than 1\u2212 4e\u2212\u03c4 .\nNote that n\u2212 2\u03b1\n2\u03b1+d is the optimal learning rate to estimate a function in B\u03b12,\u221e (Eberts\n& Steinwart, 2011). Therefore, the density-difference estimator with a Gaussian kernel achieves the optimal learning rate by appropriately choosing the regularization parameter and the Gaussian width. Because the learning rate depends on \u03b1, the LSDD estimator has an adaptivity to the smoothness of the true function.\nOur analysis heavily relies on the techniques developed in Eberts and Steinwart (2011) for a regression problem. The main difference is that the analysis in their paper involves a clipping procedure, which stems from the fact that the analyzed estimator requires an empirical approximation of the expectation of the square term. The Lipschitz continuity of the square function f 7\u2192 f 2 is utilized to investigate this term, and the clipping procedure is used to ensure the Lipschitz continuity. On the other hand, in the current paper, we can exactly compute \u2016f\u20162 L2(Rd) so that we do not need the Lipschitz continuity.\nB Derivation of Eq.(13)\nWhen \u03bb (\u2265 0) is small, (H + \u03bbIb)\u22121 can be expanded as\n(H + \u03bbIb) \u22121 =H\u22121 \u2212 \u03bbH\u22122 + op(\u03bb),\nwhere op denotes the probabilistic order. Then Eq.(12) can be expressed as\n\u03b2h\u0302 \u22a4 \u03b8\u0302 + (1\u2212 \u03b2)\u03b8\u0302\u22a4H\u03b8\u0302\n= \u03b2h\u0302 \u22a4 (H + \u03bbIb) \u22121 h\u0302+ (1\u2212 \u03b2)h\u0302\u22a4 (H + \u03bbIb)\u22121H (H + \u03bbIb)\u22121 h\u0302 = \u03b2h\u0302 \u22a4 H\u22121h\u0302\u2212 \u03bb\u03b2h\u0302\u22a4H\u22122h\u0302\n+ (1\u2212 \u03b2)h\u0302\u22a4H\u22121h\u0302\u2212 2\u03bb(1\u2212 \u03b2)h\u0302\u22a4H\u22122h\u0302+ op(\u03bb)\n= h\u0302 \u22a4 H\u22121h\u0302\u2212 \u03bb(2\u2212 \u03b2)h\u0302\u22a4H\u22122h\u0302+ op(\u03bb),\nwhich concludes the proof.\nC Derivation of Eq.(14)\nBecause E[h\u0302] = h, we have\nE[h\u0302 \u22a4 H\u22121h\u0302\u2212 h\u22a4H\u22121h] = E[(h\u0302\u2212 h)\u22a4H\u22121(h\u0302\u2212 h)]\n= tr ( H\u22121E[(h\u0302\u2212 h)(h\u0302\u2212 h)\u22a4] )\n= tr ( H\u22121 ( 1\nn V p[\u03c8] +\n1 n\u2032 V p\u2032[\u03c8]\n)) ,\nwhich concludes the proof."}], "references": [{"title": "Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel-based density estimates", "author": ["N. Anderson", "P. Hall", "D. Titterington"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Anderson et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 1994}, {"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Transactions of the American Mathematical Society, 68, 337\u2013404.", "citeRegEx": "Aronszajn,? 1950", "shortCiteRegEx": "Aronszajn", "year": 1950}, {"title": "Non-rigid medical image registration by maximisation of quadratic mutual information", "author": ["J. Atif", "X. Ripoche", "A. Osorio"], "venue": "IEEE 29th Annual Northeast Bioengineering Conference (pp. 32\u201340)", "citeRegEx": "Atif et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Atif et al\\.", "year": 2003}, {"title": "Multiple regression and estimation of the mean of a multivariate normal distribution (Technical Report 51)", "author": ["A.J. Baranchik"], "venue": "Department of Statistics, Stanford University, Stanford, CA, USA.", "citeRegEx": "Baranchik,? 1964", "shortCiteRegEx": "Baranchik", "year": 1964}, {"title": "Robust and efficient estimation by minimising a density power divergence", "author": ["A. Basu", "I.R. Harris", "N.L. Hjort", "M.C. Jones"], "venue": null, "citeRegEx": "Basu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Basu et al\\.", "year": 1998}, {"title": "Integrated squared error estimation of normal mixtures", "author": ["P. Besbeas", "B.J.T. Morgan"], "venue": "Computational Statistics & Data Analysis,", "citeRegEx": "Besbeas and Morgan,? \\Q2004\\E", "shortCiteRegEx": "Besbeas and Morgan", "year": 2004}, {"title": "A training algorithm for optimal margin classifiers", "author": ["B.E. Boser", "I.M. Guyon", "V.N. Vapnik"], "venue": "Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (pp", "citeRegEx": "Boser et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Boser et al\\.", "year": 1992}, {"title": "Semi-supervised learning of class balance under class-prior change by distribution matching", "author": ["M.C. Du Plessis", "M. Sugiyama"], "venue": "Proceedings of 29th International Conference on Machine Learning", "citeRegEx": "Plessis and Sugiyama,? \\Q2012\\E", "shortCiteRegEx": "Plessis and Sugiyama", "year": 2012}, {"title": "Highest density difference region estimation with application to flow cytometric data", "author": ["T. Duong", "I. Koch", "M.P. Wand"], "venue": "Biometrical Journal,", "citeRegEx": "Duong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2009}, {"title": "Optimal learning rates for least squares SVMs using Gaussian kernels", "author": ["M. Eberts", "I. Steinwart"], "venue": "Advances in neural information processing systems", "citeRegEx": "Eberts and Steinwart,? \\Q2011\\E", "shortCiteRegEx": "Eberts and Steinwart", "year": 2011}, {"title": "An introduction to the bootstrap", "author": ["B. Efron", "R.J. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani", "year": 1993}, {"title": "On the best obtainable asymptotic rates of convergence in estimation of a density function at a point", "author": ["R.H. Farrell"], "venue": "The Annals of Mathematical Statistics, 43, 170\u2013180.", "citeRegEx": "Farrell,? 1972", "shortCiteRegEx": "Farrell", "year": 1972}, {"title": "Quadratic mutual information for dimensionality reduction and classification", "author": ["D.M. Gray", "J.C. Principe"], "venue": "Proceedings of SPIE (p", "citeRegEx": "Gray and Principe,? \\Q2010\\E", "shortCiteRegEx": "Gray and Principe", "year": 2010}, {"title": "Covariate shift by kernel mean matching", "author": ["A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Sch\u00f6lkopf"], "venue": "Dataset shift in machine learning,", "citeRegEx": "Gretton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2009}, {"title": "On nonparametric discrimination using density differences", "author": ["P. Hall", "M.P. Wand"], "venue": null, "citeRegEx": "Hall and Wand,? \\Q1988\\E", "shortCiteRegEx": "Hall and Wand", "year": 1988}, {"title": "Nonparametric and semiparametric models", "author": ["W. H\u00e4rdle", "M. M\u00fcller", "S. Sperlich", "A. Werwatz"], "venue": null, "citeRegEx": "H\u00e4rdle et al\\.,? \\Q2004\\E", "shortCiteRegEx": "H\u00e4rdle et al\\.", "year": 2004}, {"title": "A least-squares approach to direct importance estimation", "author": ["T. Kanamori", "S. Hido", "M. Sugiyama"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kanamori et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2009}, {"title": "Statistical analysis of kernel-based least-squares density-ratio estimation", "author": ["T. Kanamori", "T. Suzuki", "M. Sugiyama"], "venue": "Machine Learning,", "citeRegEx": "Kanamori et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kanamori et al\\.", "year": 2012}, {"title": "Sequential change-point detection based on direct density-ratio estimation", "author": ["Y. Kawahara", "M. Sugiyama"], "venue": "Statistical Analysis and Data Mining,", "citeRegEx": "Kawahara and Sugiyama,? \\Q2012\\E", "shortCiteRegEx": "Kawahara and Sugiyama", "year": 2012}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R.A. Leibler"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Kullback and Leibler,? \\Q1951\\E", "shortCiteRegEx": "Kullback and Leibler", "year": 1951}, {"title": "Probability density difference-based active contour for ultrasound image segmentation", "author": ["B. Liu", "H.D. Cheng", "J. Huang", "J. Tian", "X. Tang", "J. Liu"], "venue": "Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Change-point detection in timeseries data by relative density-ratio estimation (Technical Report 1203.0453)", "author": ["S. Liu", "M. Yamada", "N. Collier", "M. Sugiyama"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Detection of activities and events without explicit categorization. Proceedings of the 3rd International Workshop on Video Event Categorization, Tagging and Retrieval for Real-World Applications (VECTaR2011) (pp. 1532\u20131539)", "author": ["M. Matsugu", "M. Yamanaka", "M. Sugiyama"], "venue": null, "citeRegEx": "Matsugu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Matsugu et al\\.", "year": 2011}, {"title": "Estimating divergence functionals and the likelihood ratio by convex risk minimization", "author": ["X. Nguyen", "M.J. Wainwright", "M.I. Jordan"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Nguyen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2010}, {"title": "On the estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The Annals of Mathematical Statistics, 33, 1065\u20131076.", "citeRegEx": "Parzen,? 1962", "shortCiteRegEx": "Parzen", "year": 1962}, {"title": "Kullback-Leibler divergence estimation of continuous distributions", "author": ["F. P\u00e9rez-Cruz"], "venue": "Proceedings of IEEE International Symposium on Information Theory (pp. 1666\u20131670). Nice, France.", "citeRegEx": "P\u00e9rez.Cruz,? 2008", "shortCiteRegEx": "P\u00e9rez.Cruz", "year": 2008}, {"title": "Inferences for case-control and semiparametric two-sample density ratio models", "author": ["J. Qin"], "venue": "Biometrika, 85, 619\u2013630.", "citeRegEx": "Qin,? 1998", "shortCiteRegEx": "Qin", "year": 1998}, {"title": "Linear statistical inference and its applications", "author": ["C.R. Rao"], "venue": "New York, NY, USA: Wiley.", "citeRegEx": "Rao,? 1965", "shortCiteRegEx": "Rao", "year": 1965}, {"title": "Regularized least-squares classification. Advances in Learning Theory: Methods, Models and Applications (pp. 131\u2013154)", "author": ["R. Rifkin", "G. Yeo", "T. Poggio"], "venue": null, "citeRegEx": "Rifkin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rifkin et al\\.", "year": 2003}, {"title": "Convex analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton, NJ, USA: Princeton University Press.", "citeRegEx": "Rockafellar,? 1970", "shortCiteRegEx": "Rockafellar", "year": 1970}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: A simple procedure", "author": ["M. Saerens", "P. Latinne", "C. Decaestecker"], "venue": "Neural Computation,", "citeRegEx": "Saerens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2002}, {"title": "Parametric statistical modeling by minimum integrated square error", "author": ["D.W. Scott"], "venue": "Technometrics, 43, 274\u2013285.", "citeRegEx": "Scott,? 2001", "shortCiteRegEx": "Scott", "year": 2001}, {"title": "Information divergence estimation based on datadependent partitions", "author": ["J. Silva", "S.S. Narayanan"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Silva and Narayanan,? \\Q2010\\E", "shortCiteRegEx": "Silva and Narayanan", "year": 2010}, {"title": "Density estimation for statistics and data analysis", "author": ["B.W. Silverman"], "venue": "London, UK: Chapman and Hall.", "citeRegEx": "Silverman,? 1986", "shortCiteRegEx": "Silverman", "year": 1986}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": null, "citeRegEx": "Steinwart and Christmann,? \\Q2008\\E", "shortCiteRegEx": "Steinwart and Christmann", "year": 2008}, {"title": "Fast rates for support vector machines using Gaussian kernels", "author": ["I. Steinwart", "C. Scovel"], "venue": "The Annals of Statistics,", "citeRegEx": "Steinwart and Scovel,? \\Q2004\\E", "shortCiteRegEx": "Steinwart and Scovel", "year": 2004}, {"title": "Density ratio estimation in machine learning", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": null, "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "Density ratio matching under the Bregman divergence: A unified framework of density ratio estimation", "author": ["M. Sugiyama", "T. Suzuki", "T. Kanamori"], "venue": "Annals of the Institute of Statistical Mathematics", "citeRegEx": "Sugiyama et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2012}, {"title": "Direct importance estimation for covariate shift adaptation", "author": ["M. Sugiyama", "T. Suzuki", "S. Nakajima", "H. Kashima", "P. von B\u00fcnau", "M. Kawanabe"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Sugiyama et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2008}, {"title": "Feature extraction by non-parametric mutual information maximization", "author": ["K. Torkkola"], "venue": "Journal of Machine Learning Research, 3, 1415\u20131438.", "citeRegEx": "Torkkola,? 2003", "shortCiteRegEx": "Torkkola", "year": 2003}, {"title": "Statistical learning theory", "author": ["V.N. Vapnik"], "venue": "New York, NY, USA: Wiley.", "citeRegEx": "Vapnik,? 1998", "shortCiteRegEx": "Vapnik", "year": 1998}, {"title": "Divergence estimation of contiunous distributions based on data-dependent partitions", "author": ["Q. Wang", "S.R. Kulkarmi", "S. Verd\u00fa"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 6, "context": "A seminal example that follows this general idea is pattern recognition by the support vector machine (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998): Instead of separately estimating two probability distributions of patterns for positive and negative classes, the support vector machine directly learns the boundary between the two classes that is sufficient for pattern recognition.", "startOffset": 102, "endOffset": 159}, {"referenceID": 40, "context": "A seminal example that follows this general idea is pattern recognition by the support vector machine (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1998): Instead of separately estimating two probability distributions of patterns for positive and negative classes, the support vector machine directly learns the boundary between the two classes that is sufficient for pattern recognition.", "startOffset": 102, "endOffset": 159}, {"referenceID": 26, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 38, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 13, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 16, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 23, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 17, "context": "More recently, a problem of estimating the ratio of two probability densities was tackled in a similar fashion (Qin, 1998; Sugiyama et al., 2008; Gretton et al., 2009; Kanamori et al., 2009; Nguyen et al., 2010; Kanamori et al., 2012; Sugiyama et al., 2012b; Sugiyama et al., 2012a): The ratio of two probability densities is directly estimated without going through separate estimation of the two probability densities.", "startOffset": 111, "endOffset": 282}, {"referenceID": 30, "context": "Density differences are useful for various purposes such as class-balance estimation under class-prior change (Saerens et al., 2002; Du Plessis & Sugiyama, 2012), change-point detection in time series (Kawahara & Sugiyama, 2012; Liu et al.", "startOffset": 110, "endOffset": 161}, {"referenceID": 21, "context": ", 2002; Du Plessis & Sugiyama, 2012), change-point detection in time series (Kawahara & Sugiyama, 2012; Liu et al., 2012), feature extraction (Torkkola, 2003), video-based event detection (Matsugu et al.", "startOffset": 76, "endOffset": 121}, {"referenceID": 39, "context": ", 2012), feature extraction (Torkkola, 2003), video-based event detection (Matsugu et al.", "startOffset": 28, "endOffset": 44}, {"referenceID": 22, "context": ", 2012), feature extraction (Torkkola, 2003), video-based event detection (Matsugu et al., 2011), flow cytometric data analysis (Duong et al.", "startOffset": 74, "endOffset": 96}, {"referenceID": 8, "context": ", 2011), flow cytometric data analysis (Duong et al., 2009), ultrasound image segmentation (Liu et al.", "startOffset": 39, "endOffset": 59}, {"referenceID": 20, "context": ", 2009), ultrasound image segmentation (Liu et al., 2010), non-rigid image registration (Atif et al.", "startOffset": 39, "endOffset": 57}, {"referenceID": 2, "context": ", 2010), non-rigid image registration (Atif et al., 2003), and image-based target recognition (Gray", "startOffset": 38, "endOffset": 57}, {"referenceID": 0, "context": "We also apply LSDD to L-distance estimation and show that it is more accurate than the difference of KDEs, which tends to severely under-estimate the L-distance (Anderson et al., 1994).", "startOffset": 161, "endOffset": 184}, {"referenceID": 4, "context": "Compared with the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951), the L-distance is more robust against outliers (Basu et al., 1998; Scott, 2001; Besbeas & Morgan, 2004).", "startOffset": 126, "endOffset": 182}, {"referenceID": 31, "context": "Compared with the Kullback-Leibler (KL) divergence (Kullback & Leibler, 1951), the L-distance is more robust against outliers (Basu et al., 1998; Scott, 2001; Besbeas & Morgan, 2004).", "startOffset": 126, "endOffset": 182}, {"referenceID": 33, "context": "A naive approach to density-difference estimation is to use kernel density estimators (KDEs) (Silverman, 1986).", "startOffset": 93, "endOffset": 110}, {"referenceID": 27, "context": "Then the central limit theorem (Rao, 1965) asserts that \u221a nn n+n (\u03b8\u0302\u2212 \u03b8) converges in law to the normal distribution with mean 0 and covariance matrix", "startOffset": 31, "endOffset": 42}, {"referenceID": 1, "context": "2 Non-Parametric Error Bound Next, we consider a non-parametric setup where a density-difference function is learned in a Gaussian reproducing kernel Hilbert space (RKHS) (Aronszajn, 1950).", "startOffset": 171, "endOffset": 188}, {"referenceID": 11, "context": "It is known that, if the naive KDE with a Gaussian kernel is used for estimating a probability density with regularity \u03b1 > 2, the optimal learning rate cannot be achieved (Farrell, 1972; Silverman, 1986).", "startOffset": 171, "endOffset": 203}, {"referenceID": 33, "context": "It is known that, if the naive KDE with a Gaussian kernel is used for estimating a probability density with regularity \u03b1 > 2, the optimal learning rate cannot be achieved (Farrell, 1972; Silverman, 1986).", "startOffset": 171, "endOffset": 203}, {"referenceID": 24, "context": "To achieve the optimal rate by KDE, we should choose a kernel specifically tailored to each regularity \u03b1 (Parzen, 1962).", "startOffset": 105, "endOffset": 119}, {"referenceID": 9, "context": "More precise statement of the result and its complete proof are provided in Appendix A, where we utilize the mathematical technique developed in Eberts and Steinwart (2011) for a regression problem.", "startOffset": 145, "endOffset": 173}, {"referenceID": 29, "context": "convex duality (Rockafellar, 1970):", "startOffset": 15, "endOffset": 34}, {"referenceID": 3, "context": "Following the same line as Baranchik (1964), the positive-part estimator may be more accurate:", "startOffset": 27, "endOffset": 44}, {"referenceID": 0, "context": "Such smoother density estimates are accurate as density estimates, but the difference of smoother density estimates yields a smaller L-distance estimate (Anderson et al., 1994).", "startOffset": 153, "endOffset": 176}, {"referenceID": 38, "context": "Next, we draw n = n = 100 samples from p(x) and p(x), and estimate the L-distance by LSDD and the KL-divergence by the Kullback-Leibler importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 176, "endOffset": 220}, {"referenceID": 23, "context": "Next, we draw n = n = 100 samples from p(x) and p(x), and estimate the L-distance by LSDD and the KL-divergence by the Kullback-Leibler importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 176, "endOffset": 220}, {"referenceID": 41, "context": "Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distriEstimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; P\u00e9rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010).", "startOffset": 149, "endOffset": 255}, {"referenceID": 38, "context": "Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distriEstimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; P\u00e9rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010).", "startOffset": 149, "endOffset": 255}, {"referenceID": 25, "context": "Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distriEstimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; P\u00e9rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010).", "startOffset": 149, "endOffset": 255}, {"referenceID": 23, "context": "Since X\u0303 and X\u0303 \u2032 can be regarded as being drawn from the same distriEstimation of the KL-divergence from data has been extensively studied recently (Wang et al., 2005; Sugiyama et al., 2008; P\u00e9rez-Cruz, 2008; Silva & Narayanan, 2010; Nguyen et al., 2010).", "startOffset": 149, "endOffset": 255}, {"referenceID": 4, "context": "This result implies that the L-distance is less sensitive to outliers than the KL-divergence, which well agrees with the observation given in Basu et al. (1998). Next, we draw n = n = 100 samples from p(x) and p(x), and estimate the L-distance by LSDD and the KL-divergence by the Kullback-Leibler importance estimation procedure (KLIEP) (Sugiyama et al.", "startOffset": 142, "endOffset": 161}, {"referenceID": 30, "context": "\u03c0ptrain(x|y = +1) + (1\u2212 \u03c0)ptrain(x|y = \u22121), with the test input density ptest(x) (Saerens et al., 2002), where \u03c0 \u2208 [0, 1] is a mixing coefficient to learn.", "startOffset": 81, "endOffset": 103}, {"referenceID": 28, "context": "Figure 10 plots the mean and standard error of the squared difference between true and estimated class balances \u03c0 and the misclassification error by a weighted regularized least-squares classifier (Rifkin et al., 2003) over 1000 runs.", "startOffset": 197, "endOffset": 218}, {"referenceID": 38, "context": "estimated by the KL importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 60, "endOffset": 104}, {"referenceID": 23, "context": "estimated by the KL importance estimation procedure (KLIEP) (Sugiyama et al., 2008; Nguyen et al., 2010).", "startOffset": 60, "endOffset": 104}], "year": 2012, "abstractText": "We address the problem of estimating the difference between two probability densities. A naive approach is a two-step procedure of first estimating two densities separately and then computing their difference. However, such a two-step procedure does not necessarily work well because the first step is performed without regard to the second step and thus a small error incurred in the first stage can cause a big error in the second stage. In this paper, we propose a single-shot procedure for directly estimating the density difference without separately estimating two densities. We derive a non-parametric finite-sample error bound for the proposed single-shot density-difference estimator and show that it achieves the optimal convergence rate. The usefulness of the proposed method is also demonstrated experimentally.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}