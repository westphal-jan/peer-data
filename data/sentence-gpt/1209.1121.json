{"id": "1209.1121", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2012", "title": "Learning Manifolds with K-Means and K-Flats", "abstract": "We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by k-means and k-flats, and analyze their performance. We extend previous results for k-means in two separate directions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "histories": [["v1", "Wed, 5 Sep 2012 21:18:03 GMT  (240kb,D)", "https://arxiv.org/abs/1209.1121v1", "13 pages, 2 figures"], ["v2", "Fri, 7 Sep 2012 17:11:23 GMT  (240kb,D)", "http://arxiv.org/abs/1209.1121v2", "13 pages, 2 figures; Advances in Neural Information Processing Systems, NIPS 2012"], ["v3", "Tue, 11 Sep 2012 16:00:03 GMT  (240kb,D)", "http://arxiv.org/abs/1209.1121v3", "13 pages, 2 figures; Advances in Neural Information Processing Systems, NIPS 2012"], ["v4", "Tue, 19 Feb 2013 17:53:17 GMT  (257kb,D)", "http://arxiv.org/abs/1209.1121v4", "19 pages, 2 figures; Advances in Neural Information Processing Systems, NIPS 2012"]], "COMMENTS": "13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["guillermo d ca\u00f1as", "tomaso a poggio", "lorenzo rosasco"], "accepted": true, "id": "1209.1121"}, "pdf": {"name": "1209.1121.pdf", "metadata": {"source": "CRF", "title": "Learning Manifolds with K-Means and K-Flats", "authors": ["Guillermo D. Canas", "Tomaso Poggio", "Lorenzo A. Rosasco"], "emails": ["guilledc@mit.edu", "tp@ai.mit.edu", "lrosasco@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "Our study is broadly motivated by questions in high-dimensional learning. As is well known, learning in high dimensions is feasible only if the data distribution satisfies suitable prior assumptions. One such assumption is that the data distribution lies on, or is close to, a low-dimensional set embedded in a high dimensional space, for instance a low dimensional manifold. This latter assumption has proved to be useful in practice, as well as amenable to theoretical analysis, and it has led to a significant amount of recent work. Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.\nInterestingly, the problem of learning the manifold itself has received less attention: given samples from a d-manifoldM embedded in some ambient space X , the problem is to learn a set that approximates M in a suitable sense. This problem has been considered in computational geometry, but in a setting in which typically the manifold is a hyper-surface in a low-dimensional space (e.g. R3), and the data are typically not sampled probabilistically, see for instance [32, 30]. The problem of learning a manifold is also related to that of estimating the support of a distribution, (see [17, 18] for recent surveys.) In this context, some of the distances considered to measure approximation quality are the Hausforff distance, and the so-called excess mass distance.\nThe reconstruction framework that we consider is related to the work of [1, 38], as well as to the framework proposed in [37], in which a manifold is approximated by a set, with performance measured by an expected distance to this set. This setting is similar to the problem of dictionary learning (see for instance [36], and extensive references therein), in which a dictionary is found by minimizing a similar reconstruction error, perhaps with additional constraints on an associated encoding of the data. Crucially, while the dictionary is learned on the empirical data, the quantity of interest is the expected reconstruction error, which is the focus of this work.\nWe analyze this problem by focusing on two important, and widely-used algorithms, namely kmeans and k-flats. The k-means algorithm can be seen to define a piecewise constant approximation of M. Indeed, it induces a Voronoi decomposition on M, in which each Voronoi region is effectively approximated by a fixed mean. Given this, a natural extension is to consider higher order approximations, such as those induced by discrete collections of k d-dimensional affine spaces (k-flats), with possibly better\nar X\niv :1\n20 9.\n11 21\nv4 [\ncs .L\nG ]\n1 9\nFe b\n20 13\nresulting performance. Since M is a d-manifold, the k-flats approximation naturally resembles the way in which a manifold is locally approximated by its tangent bundle.\nOur analysis extends previous results for k-means to the case in which the data-generating distribution is supported on a manifold, and provides analogous results for k-flats. We note that the k-means algorithm has been widely studied, and thus much of our analysis in this case involves the combination of known facts to obtain novel results. The analysis of k-flats, however, requires developing substantially new mathematical tools.\nThe rest of the paper is organized as follows. In section 2, we describe the formal setting and the algorithms that we study. We begin our analysis by discussing the reconstruction properties of k-means in section 3. In section 4, we present and discuss our main results, whose proofs are postponed to the appendices."}, {"heading": "2 Learning Manifolds", "text": "Let X by a Hilbert space with inner product \u3008\u00b7, \u00b7\u3009, endowed with a Borel probability measure \u03c1 supported over a compact, smooth d-manifold M. We assume the data to be given by a training set, in the form of samples Xn = (x1, . . . , xn) drawn identically and independently with respect to \u03c1. Our goal is to learn a set Sn that approximates well the manifold. The approximation (learning error) is measured by the expected reconstruction error\nE\u03c1(Sn) := \u222b M d\u03c1(x) d2 X (x, Sn), (1)\nwhere the distance to a set S \u2286 X is d2 X (x, S) = infx\u2032\u2208S d 2 X (x, x\u2032), with dX (x, x \u2032) = \u2016x\u2212 x\u2032\u2016. This is the same reconstruction measure that has been the recent focus of [37, 5, 38]. It is easy to see that any set such that S \u2283M will have zero risk, withM being the \u201csmallest\u201d such set (with respect to set containment.) In other words, the above error measure does not introduce an explicit penalty on the \u201csize\u201d of Sn: enlarging any given Sn can never increase the learning error. With this observation in mind, we study specific learning algorithms that, given the data, produce a set belonging to some restricted hypothesis space H (e.g. sets of size k for k-means), which effectively introduces a constraint on the size of the sets. Finally, note that the risk of Equation 1 is non-negative and, if the hypothesis space is sufficiently rich, the risk of an unsupervised algorithm may converge to zero under suitable conditions."}, {"heading": "2.1 Using K-Means and K-Flats for Piecewise Manifold Approximation", "text": "In this work, we focus on two specific algorithms, namely k-means [34, 33] and k-flats [12]. Although typically discussed in the Euclidean space case, their definition can be easily extended to a Hilbert space setting. The study of manifolds embedded in a Hilbert space is of special interest when considering non-linear (kernel) versions of the algorithms [20]. More generally, this setting can be seen as a limit case when dealing with high dimensional data. Naturally, the more classical setting of an absolutely continuous distribution over d-dimensional Euclidean space is simply a particular case, in which X = Rd, and M is a domain with positive Lebesgue measure. K-Means. Let H = Sk be the class of sets of size k in X . Given a training set Xn and a choice of k, k-means is defined by the minimization over S \u2208 Sk of the empirical reconstruction error\nEn(S) := 1\nn n\u2211 i=1 d2 X (xi, S). (2)\nwhere, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].\nA minimizer of Equation 2 on Sk is a discrete set of k means Sn,k = {m1, . . . ,mk}, which induces a Dirichlet-Voronoi tiling of X : a collection of k regions, each closest to a common mean [4] (in our notation, the subscript n denotes the dependence of Sn,k on the sample, while k refers to its size.) By virtue of Sn,k being a minimizing set, each mean must occupy the center of mass of the samples in its Voronoi region.\nThese two facts imply that it is possible to compute a local minimum of the empirical risk by using a greedy coordinate-descent relaxation, namely Lloyd\u2019s algorithm [33]. Furthermore, given a finite sample Xn, the number of locally-minimizing sets Sn,k is also finite since (by the center-of-mass condition) there cannot be more than the number of possible partitions of Xn into k groups, and therefore the global minimum must be attainable. Even though Lloyd\u2019s algorithm provides no guarantees of closeness to the global minimizer, in practice it is possible to use a randomized approximation algorithm, such as kmeans++ [3], which provides guarantees of approximation to the global minimum in expectation with respect to the randomization. K-Flats. Let H = Fk be the class of collections of k flats (affine spaces) of dimension d. For any value of k, k-flats, analogously to k-means, aims at finding the set Fk \u2208 Fk that minimizes the empirical reconstruction (2) over Fk. By an argument similar to the one used for k-means, a global minimizer must be attainable, and a Lloyd-type relaxation converges to a local minimum. Note that, in this case, given a Voronoi partition ofM into regions closest to each d-flat, new optimizing flats for that partition can be computed by a d-truncated PCA solution on the samples falling in each region."}, {"heading": "2.2 Learning a Manifold with K-means and K-flats", "text": "In practice, k-means is often interpreted to be a clustering algorithm, with clusters defined by the Voronoi diagram of the set of means Sn,k. In this interpretation, Equation 2 is simply rewritten by summing over the Voronoi regions, and adding all pairwise distances between samples in the region (the intra-cluster distances.) For instance, this point of view is considered in [14] where k-means is studied from an information theoretic persepective. K-means can also be interpreted to be performing vector quantization, where the goal is to minimize the encoding error associated to a nearest-neighbor quantizer [23]. Interestingly, in the limit of increasing sample size, this problem coincides, in a precise sense [39], with the problem of optimal quantization of probability distributions (see for instance the excellent monograph of [24].)\nWhen the data-generating distribution is supported on a manifold M, k-means can be seen to be approximating points on the manifold by a discrete set of means. Analogously to the Euclidean setting, this induces a Voronoi decomposition of M, in which each Voronoi region is effectively approximated by a fixed mean (in this sense k-means produces a piecewise constant approximation of M.) As in the Euclidean setting, the limit of this problem with increasing sample size is precisely the problem of optimal quantization of distributions on manifolds, which is the subject of significant recent work in the field of optimal quantization [26, 27].\nIn this paper, we take the above view of k-means as defining a (piecewise constant) approximation of the manifold M supporting the data distribution. In particular, we are interested in the behavior of the expected reconstruction error E\u03c1(Sn,k), for varying k and n. This perspective has an interesting relation with dictionary learning, in which one is interested in finding a dictionary, and an associated representation, that allows to approximately reconstruct a finite set of data-points/signals. In this interpretation, the set of means can be seen as a dictionary of size k that produces a maximally sparse representation (the k-means encoding), see for example [36] and references therein. Crucially, while the dictionary is learned on the available empirical data, the quantity of interest is the expected reconstruction error, and the question of characterizing the performance with respect to this latter quantity naturally arises.\nSince k-means produces a piecewise constant approximation of the data, a natural idea is to consider higher orders of approximation, such as approximation by discrete collections of k d-dimensional affine spaces (k-flats), with possibly better performance. Since M is a d-manifold, the approximation induced by k-flats may more naturally resemble the way in which a manifold is locally approximated by its tangent bundle. We provide in Sec. 4.2 a partial answer to this question."}, {"heading": "3 Reconstruction Properties of k-Means", "text": "Since we are interested in the behavior of the expected reconstruction (1) of k-means and k-flats for varying k and n, before analyzing this behavior, we consider what is currently known about this problem,\nbased on previous work. While k-flats is a relatively new algorithm whose behavior is not yet well understood, several properties of k-means are currently known.\nRecall that k-means find an discrete set Sn,k of size k that best approximates the samples in the sense of (2). Clearly, as k increases, the empirical reconstruction error En(Sn,k) cannot increase, and typically decreases. However, we are ultimately interested in the expected reconstruction error, and therefore would like to understand the behavior of E\u03c1(Sn,k) with varying k, n.\nIn the context of optimal quantization, the behavior of the expected reconstruction error E\u03c1 has been considered for an approximating set Sk obtained by minimizing the expected reconstruction error itself over the hypothesis space H = Sk. The set Sk can thus be interpreted as the output of a population, or infinite sample version of k-means. In this case, it is possible to show that E\u03c1(Sk) is a non increasing function of k and, in fact, to derive explicit rates. For example in the case X = Rd, and under fairly general technical assumptions, it is possible to show that E\u03c1(Sk) = \u0398(k\u22122/d), where the constants depend on \u03c1 and d [24].\nIn machine learning, the properties of k-means have been studied, for fixed k, by considering the excess reconstruction error E\u03c1(Sn,k)\u2212 E\u03c1(Sk). In particular, this quantity has been studied for X = Rd, and shown to be, with high probability, of order \u221a kd/n, up-to logarithmic factors [37]. The case where X is a Hilbert space has been considered in [37, 10], where an upper-bound of order k/ \u221a n is proven to hold with high probability. The more general setting where X is a metric space has been studied in [9]. When analyzing the behavior of E\u03c1(Sn,k), and in the particular case that X = Rd, the above results can be combined to obtain, with high probability, a bound of the form\nE\u03c1(Sn,k) \u2264 |E\u03c1(Sn,k)\u2212 En(Sn,k)|+ En(Sn,k)\u2212 En(Sk) + |En(Sk)\u2212 E\u03c1(Sk)|+ E\u03c1(Sk)\n\u2264 C\n(\u221a kd\nn + k\u22122/d\n) (3)\nup to logarithmic factors, where the constant C does not depend on k or n (a complete derivation is given in the Appendix.) The above inequality suggests a somewhat surprising effect: the expected reconstruction properties of k-means may be described by a trade-off between a statistical error (of order\u221a\nkd n ) and a geometric approximation error (of order k \u22122/d.)\nThe existence of such a tradeoff between the approximation, and the statistical errors may itself not be entirely obvious, see the discussion in [5]. For instance, in the k-means problem, it is intuitive that, as more means are inserted, the expected distance from a random sample to the means should decrease, and one might expect a similar behavior for the expected reconstruction error. This observation naturally begs the question of whether and when this trade-off really exists or if it is simply a result of the looseness in the bounds. In particular, one could ask how tight the bound (3) is.\nWhile the bound on E\u03c1(Sk) is known to be tight for k sufficiently large [24], the remaining terms (which are dominated by |E\u03c1(Sn,k)\u2212En(Sn,k)|) are derived by controlling the supremum of an empirical process\nsup S\u2208Sk\n|En(S)\u2212 E\u03c1(S)| (4)\nand it is unknown whether available bounds for it are tight [37]. Indeed, it is not clear how close the distortion redundancy E\u03c1(Sn,k)\u2212 E\u03c1(Sk) is to its known lower bound of order d \u221a k1\u2212 4 d\nn (in expectation) [5]. More importantly, we are not aware of a lower bound for E\u03c1(Sn,k) itself. Indeed, as pointed out in [5], \u201cThe exact dependence of the minimax distortion redundancy on k and d is still a challenging open problem\u201d.\nFinally, we note that, whenever a trade-off can be shown to hold, it may be used to justify a heuristic for choosing k empirically as the value that minimizes the reconstruction error in a hold-out set.\nIn Figure 1 we perform some simple numerical simulations showing that the trade-off indeed occurs in certain regimes. The following example provides a situation where a trade-off can be easily shown to occur.\nExample 1. Consider a setup in which n = 2 samples are drawn from a uniform distribution on the unit d = 100-sphere, though the argument holds for other n much smaller than d. Because d n, with high probability, the samples are nearly orthogonal: < x1, x2 >X' 0, while a third sample x drawn uniformly on S100 will also very likely be nearly orthogonal to both x1, x2 [31]. The k-means solution on this dataset is clearly Sk=1 = {(x1 + x2)/2} (Fig 2(a)). Indeed, since Sk=2 = {x1, x2} (Fig 2(b)), it is E\u03c1(Sk=1) ' 1.5 < 2 ' E\u03c1(Sk=2) with very high probability. In this case, it is better to place a single mean closer to the origin (with E\u03c1({0}) = 1), than to place two means at the sample locations. This example is sufficiently simple that the exact k-means solution is known, but the effect can be observed in more complex settings."}, {"heading": "4 Main Results", "text": "Contributions. Our work extends previous results in two different directions:\n(a) We provide an analysis of k-means for the case in which the data-generating distribution is supported on a manifold embedded in a Hilbert space. In particular, in this setting: 1) we derive new results\non the approximation error, and 2) new sample complexity results (learning rates) arising from the choice of k by optimizing the resulting bound. We analyze the case in which a solution is obtained from an approximation algorithm, such as k-means++ [3], to include this computational error in the bounds.\n(b) We generalize the above results from k-means to k-flats, deriving learning rates obtained from new bounds on both the statistical and the approximation errors. To the best of our knowledge, these results provide the first theoretical analysis of k-flats in either sense.\nWe note that the k-means algorithm has been widely studied in the past, and much of our analysis in this case involves the combination of known facts to obtain novel results. However, in the case of k-flats, there is currently no known analysis, and we provide novel results as well as new performance bounds for each of the components in the bounds.\nThroughout this section we make the following technical assumption:\nAssumption 1. M is a smooth d-manifold with metric of class C1, contained in the unit ball in X , and with volume measure denoted by \u00b5I. The probability measure \u03c1 is absolutely continuous with respect to \u00b5I, with density p."}, {"heading": "4.1 Learning Rates for k-Means", "text": "The first result considers the idealized case where we have access to an exact solution for k-means.\nTheorem 1. Under Assumption 1, if Sn,k is a solution of k-means then, for 0 < \u03b4 < 1, there are constants C and \u03b3 dependent only on d, and sufficiently large n\u2032 such that, by setting\nkn = n d 2(d+2) \u00b7 ( C\n24 \u221a \u03c0 )d/(d+2) \u00b7 {\u222b M d\u00b5I(x)p(x) d/(d+2) } , (5)\nand Sn = Sn,kn , it is P [ E\u03c1(Sn) \u2264 \u03b3 \u00b7 n\u22121/(d+2) \u00b7 \u221a ln 1/\u03b4 \u00b7 {\u222b M d\u00b5I(x)p(x) d/(d+2) }] \u2265 1\u2212 \u03b4, (6)\nfor all n \u2265 n\u2032, where C \u223c d/(2\u03c0e) and \u03b3 grows sublinearly with d.\nRemark 1. Note that the distinction between distributions with density inM, and singular distributions is important. The bound of Equation (6) holds only when the absolutely continuous part of \u03c1 over M is non-vanishing. the case in which the distribution is singular over M requires a different analysis, and may result in faster convergence rates.\nThe following result considers the case where the k-means++ algorithm is used to compute the estimator.\nTheorem 2. Under Assumption 1, if Sn,k is the solution of k-means++ , then for 0 < \u03b4 < 1, there are constants C and \u03b3 that depend only on d, and a sufficiently large n\u2032 such that, by setting\nkn = n d 2(d+2) \u00b7 ( C\n24 \u221a \u03c0 )d/(d+2) \u00b7 {\u222b M d\u00b5I(x)p(x) d/(d+2) } , (7)\nand Sn = Sn,kn , it is P [ EZ E\u03c1(Sn) \u2264 \u03b3 \u00b7 n\u22121/(d+2) ( lnn+ ln \u2016p\u2016d/(d+2) ) \u00b7 \u221a ln 1/\u03b4 \u00b7 {\u222b M d\u00b5I(x)p(x) d/(d+2) }] \u2265 1\u2212 \u03b4, (8)\nfor all n \u2265 n\u2032, where the expectation is with respect to the random choice Z in the algorithm, and \u2016p\u2016d/(d+2) = {\u222b M d\u00b5I(x)p(x) d/(d+2) }(d+2)/d , C \u223c d/(2\u03c0e), and \u03b3 grows sublinearly with d.\nRemark 2. In the particular case that X = Rd and M is contained in the unit ball, we may further bound the distribution-dependent part of Equations 6 and 8. Using Ho\u0308lder\u2019s inequality, one obtains\u222b\nd\u03bd(x)p(x)d/(d+2) \u2264 [\u222b M d\u03bd(x)p(x) ]d/(d+2) \u00b7 [\u222b M d\u03bd(x) ]2/(d+2) \u2264 Vol(M)2/(d+2) \u2264 \u03c92/(d+2)d ,\n(9)\nwhere \u03bd is the Lebesgue measure in Rd, and \u03c9d is the volume of the d-dimensional unit ball. It is clear from the proof of Theorem 1 that, in this case, we may choose\nkn = n d 2(d+2) \u00b7 ( C\n24 \u221a \u03c0\n)d/(d+2) \u00b7 \u03c92/dd ,\nindependently of the density p, to obtain a bound E\u03c1(S\u2217n) = O ( n\u22121/(d+2) \u00b7 \u221a ln 1/\u03b4 ) with probability 1\u2212\u03b4\n(and similarly for Theorem 2, except for an additional lnn term), where the constant only depends on the dimension.\nRemark 3. Note that according to the above theorems, choosing k requires knowledge of properties of the distribution \u03c1 underlying the data, such as the intrinsic dimension of the support. In fact, following the ideas in [43] Section 6.3-5, it is easy to prove that choosing k to minimize the reconstruction error on a hold-out set, allows to achieve the same learning rates (up to a logarithmic factor), adaptively in the sense that knowledge of properties of \u03c1 are not needed."}, {"heading": "4.2 Learning Rates for k-Flats", "text": "To study k-flats, we need to slightly strengthen Assumption 1 by adding to it by the following:\nAssumption 2. Assume the manifoldM to have metric of class C3, and finite second fundamental form II [22].\nOne reason for the higher-smoothness assumption is that k-flats uses higher order approximation, whose analysis requires a higher order of differentiability. We begin by providing a result for k-flats on hypersurfaces (codimension one), and next extend it to manifolds in more general spaces.\nTheorem 3. Let, X = Rd+1. Under Assumptions 1,2, if Fn,k is a solution of k-flats, then there is a constant C that depends only on d, and sufficiently large n\u2032 such that, by setting\nkn = n d 2(d+4) \u00b7 ( C\n2 \u221a 2\u03c0d\n)d/(d+4) \u00b7 (\u03baM) 4/(d+4) , (10)\nand Fn = Fn,kn , then for all n \u2265 n\u2032 it is\nP [ E\u03c1(Fn) \u2264 2 (8\u03c0d)2/(d+4) Cd/(d+4) \u00b7 n\u22122/(d+4) \u00b7 \u221a 1\n2 ln 1/\u03b4 \u00b7 (\u03baM)\n4/(d+4) ] \u2265 1\u2212 \u03b4, (11)\nwhere \u03baM := \u00b5|II|(M) = \u222b M d\u00b5 I (x)|\u03ba1/2G (x)| is the total root curvature of M, \u00b5|II| is the measure associated with the (positive) second fundamental form, and \u03ba G is the Gaussian curvature on M.\nIn the more general case of a d-manifold M (with metric in C3) embedded in a separable Hilbert space X , we cannot make any assumption on the codimension of M (the dimension of the orthogonal complement to the tangent space at each point.) In particular, the second fundamental form II, which is an extrinsic quantity describing how the tangent spaces bend locally is, at every x \u2208 M, a map IIx : TxM 7\u2192 (TxM)\u22a5 (in this case of class C1 by Assumption 2) from the tangent space to its orthogonal complement (II(x) := B(x, x) in the notation of [22, p. 128].) Crucially, in this case, we may no longer assume the dimension of the orthogonal complement (TxM)\u22a5 to be finite. Denote by |IIx| = supr\u2208TxM\n\u2016r\u2016\u22641 \u2016IIx(r)\u2016X , the operator norm of IIx. We have:\nTheorem 4. Under Assumptions 1,2, if Fn,k is a solution to the k-flats problem, then there is a constant C that depends only on d, and sufficiently large n\u2032 such that, by setting\nkn = n d 2(d+4) \u00b7 ( C\n2 \u221a 2\u03c0d\n)d/(d+4) \u00b7 \u03ba4/(d+4) M , (12)\nand Fn = Fn,kn , then for all n \u2265 n\u2032 it is\nP [ E\u03c1(Fn) \u2264 2 (8\u03c0d)2/(d+4) Cd/(d+4) \u00b7 n\u22122/(d+4) \u00b7 \u221a 1\n2 ln 1/\u03b4 \u00b7 \u03ba4/(d+4) M\n] \u2265 1\u2212 \u03b4, (13)\nwhere \u03baM := \u222b M d\u00b5I(x) |IIx|2\nNote that the better k-flats bounds stem from the higher approximation power of d-flats over points. Although this greatly complicates the setup and proofs, as well as the analysis of the constants, the resulting bounds are of order O ( n\u22122/(d+4) ) , compared with the slower order O ( n\u22121/(d+2) ) of k-means."}, {"heading": "4.3 Discussion", "text": "In all the results, the final performance does not depend on the dimensionality of the embedding space (which in fact can be infinite), but only on the intrinsic dimension of the space on which the datagenerating distribution is defined. The key to these results is an approximation construction in which the Voronoi regions on the manifold (points closest to a given mean or flat) are guaranteed to have vanishing diameter in the limit of k going to infinity. Under our construction, a hypersurface is approximated efficiently by tracking the variation of its tangent spaces by using the second fundamental form. Where this form vanishes, the Voronoi regions of an approximation will not be ensured to have vanishing diameter with k going to infinity, unless certain care is taken in the analysis.\nAn important point of interest is that the approximations are controlled by averaged quantities, such as the total root curvature (k-flats for surfaces of codimension one), total curvature (k-flats in arbitrary codimensions), and d/(d+ 2)-norm of the probability density (k-means), which are integrated over the domain where the distribution is defined. Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].) Intuitively, it is easy to see that a constraint on an average quantity may be arbitrarily less restrictive than one on its maximum. A small difficult region (e.g. of very high curvature) may cause the bounds of the latter to substantially degrade, while the results presented here would not be adversely affected so long as the region is small.\nAdditionally, care has been taken throughout to analyze the behavior of the constants. In particular, there are no constants in the analysis that grow exponentially with the dimension, and in fact, many have polynomial, or slower growth. We believe this to be an important point, since this ensures that the asymptotic bounds do not hide an additional exponential dependence on the dimension."}, {"heading": "A Methodology and Derivation of Results", "text": "Although both k-means and k-flats optimize the same empirical risk, the performance measure we are interested in is that of Equation 1. We may bound it from above as follows:\nE\u03c1(Sn,k) \u2264 |E\u03c1(Sn,k)\u2212 En(Sn,k)|+ En(Sn,k)\u2212 En(S\u2217k) + |En(S\u2217k)\u2212 E\u2217\u03c1,k|+ E\u2217\u03c1,k (14) \u2264 2 \u00b7 sup\nS\u2208Sk |E\u03c1(S)\u2212 En(S)|\ufe38 \ufe37\ufe37 \ufe38\nStatistical error\n+ E\u2217\u03c1,k\ufe38\ufe37\ufe37\ufe38 Approximation error\n(15)\nwhere E\u2217\u03c1,k := infS\u2208Sk E\u03c1(S) is the best attainable performance over Sk, and S\u2217k is a set for which the best performance is attained. Note that En(Sn,k)\u2212En(S\u2217k) \u2264 0 by the definition of Sn,k. The same error decomposition can be considered for k-flats, by replacing Sn,k by Fn,k and Sk by Fk.\nEquation 14 decomposes the total learning error into two terms: a uniform (over all sets in the class Ck) bound on the difference between the empirical, and true error measures, and an approximation error term. The uniform statistical error bound will depend on the samples, and thus may hold with a certain probability.\nIn this setting, the approximation error will typically tend to zero as the class Ck becomes larger (as k increases.) Note that this is true, for instance, if Ck is the class of discrete sets of size k, as in the k-means problem.\nThe performance of Equation 14 is, through its dependence on the samples, a random variable. We will thus set out to find probabilistic bounds on its performance, as a function of the number n of samples, and the size k of the approximation. By choosing the approximation size parameter k to minimize these bounds, we obtain performance bounds as a function of the sample size."}, {"heading": "B K-Means", "text": "We use the above decomposition to derive sample complexity bounds for the performance of the k-means algorithm. To derive explicit bounds on the different error terms we have to combine in a novel way some previous results and some new observations.\nApproximation error. The error E\u2217\u03c1,k = infSk\u2208Sk E\u03c1(Sk) is related to the problem of optimal quantization. The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15]. In particular, it is known that, for distributions with finite moment of order 2 +\u03bb, for some \u03bb > 0, it is [24]\nlim k\u2192\u221e\nE\u2217\u03c1,k \u00b7 k2/d = C {\u222b d\u03bd(x)pa(x) d/(d+2) }(d+2)/d (16)\nwhere \u03bd is the Lebesgue measure, pa is the density of the absolutely continuous part of the distribution (according to its Lebesgue decomposition), and C is a constant that depends only on the dimension. Therefore, the approximation error decays at least as fast as k\u22122/d.\nWe note that, by setting \u00b5 to be the uniform distribution over the unit cube [0, 1]d, it clearly is\nlim k\u2192\u221e\nE\u2217\u00b5,k \u00b7 k2/d = C\nand thus, by making use of Zador\u2019s asymptotic formula [46], and combining it with a result of Bo\u0308ro\u0308czky (see [27], p. 491), we observe that C \u223c (d/(2\u03c0e))r/2 with d\u2192\u221e, for the r-th order quantization problem. In particular, this shows that the constant C only depends on the dimension, and, in our case (r = 2), has only linear growth in d, a fact that will be used in the sequel.\nThe approximation error E\u2217\u03c1,k = infSk\u2208Sk E\u03c1(Sk) of k-means is related to the problem of optimal quantization on manifolds, for which some results are known [26]. By calling E\u2217M,p,k the approximation error only among sets of means contained inM, Theorem 5 in Appendix C, implies in this case (letting r = 2) that\nlim k\u2192\u221e E\u2217\u03c1,k \u00b7 k2/d = C {\u222b M d\u00b5 I (x) p(x)d/(d+2) }(d+2)/d (17)\nwhere p is absolutely continuous over M and, by replacing M with a d-dimensional domain in Rd, it is clear that the constant C is the same as above.\nSince restricting the means to be onM cannot decrease the approximation error, it is E\u2217\u03c1,k \u2264 E\u2217M,p,k, and therefore the right-hand side of Equation 17 provides an (asymptotic) upper bound to E\u2217\u03c1,k \u00b7 k2/d.\nFor the statistical error we use available bounds. Statistical error. The statistical error of Equation 14, which uniformly bounds the difference between the empirical, and expected error, has been widely-studied in recent years in the literature [37, 38, 5]. In particular, it has been shown that, for a distribution p over the unit ball in Rd, it is\nsup S\u2208Sk\n|E\u03c1(S)\u2212 En(S)| \u2264 k \u221a\n18\u03c0\u221a n +\n\u221a 8 ln 1/\u03b4\nn (18)\nwith probability 1 \u2212 \u03b4 [37]. Clearly, this implies convergence En(S) \u2192 E\u03c1(S) almost surely, as n \u2192 \u221e; although this latter result was proven earlier in [39], under the less restrictive condition that p have finite second moment. By bringing together the above results, we obtain the bound in Theorem 1 on the performance of kmeans, whose proof is postponed to Appendix A.\nFurther, we can consider the error incurred by the actual optimization algorithm used to compute the k-means solution. Computational error. In practice, the k-means problem is NP-hard [2, 19, 35], with the original Lloyd relaxation algorithm providing no guarantees of closeness to the global minimum of Equation 2. However, practical approximations, such as the k-means++ algorithm [3], exist. When using k-means++, means are inserted one by one at samples selected with probability proportional to their squared distance to the\nset of previously-inserted means. This randomized seeding has been shown by [3] to output a set that is, in expectation, within a 8 (ln k + 2)-factor of the optimal. Once again, by combining these results, we obtain Theorem 2, whose proof is also in Appendix A.\nWe use the results discussed in Section A to obtain the proof of Theorem 1 as follows. Proof. Letting \u2016p\u2016d/(d+2) := {\u222b d\u00b5I(x)p(x) d/(d+2) }(d+2)/d , then with probability 1\u2212 \u03b4, it is\nE\u03c1(Sn,k) \u2264 2n\u22121/2 ( k \u221a 18\u03c0 + \u221a 8 ln 1/\u03b4 ) + Ck\u22122/d \u00b7 \u2016p\u2016d/(d+2)\n\u2264 2n\u22121/2k \u221a 18\u03c0 \u00b7 \u221a\n8 ln 1/\u03b4 + Ck\u22122/d \u00b7 \u2016p\u2016d/(d+2) = 24 \u221a \u03c0kn\u22121/2 \u221a ln 1/\u03b4 + Ck\u22122/d \u00b7 \u2016p\u2016d/(d+2)\n= 2 \u221a ln 1/\u03b4n\u22121/(d+2)Cd/(d+2) ( 24 \u221a \u03c0 )2/(d+2) \u00b7{\u222b d\u00b5I(x)p(x)d/(d+2)}\n(19)\nwhere the parameter\nkn = n d 2(d+2) \u00b7 ( C\n24 \u221a \u03c0\n)d/(d+2) \u00b7 {\u222b d\u00b5I(x)p(x) d/(d+2) } (20)\nhas been chosen to balance the summands in the third line of Equation 19.\nThe proof of Theorem 2 follows a similar argument.\nProof. In the case of Theorem 2, the additional multiplicative term Ak = 8(ln k + 2) corresponding to the computational error incurred by the k-means++ algorithm does not affect the choice of parameter kn since both summands in the third line of Equation 19 are multiplied by Ak in this case. Therefore, we may simply use the same choice of kn as in Equation 20 in this case to obtain\nEZ E\u03c1(Sn,k) \u2264 2n\u22121/2 ( k \u221a 18\u03c0 + \u221a 8 ln 1/\u03b4 ) + Ck\u22122/d \u00b7 \u2016p\u2016d/(d+2) \u00b7 8(ln k + 2)\n\u2264 16 \u221a ln 1/\u03b4n\u22121/(d+2)Cd/(d+2) ( 24 \u221a \u03c0 )2/(d+2) \u00b7{\u222b d\u00b5I(x)p(x)d/(d+2)}\n\u00b7 [ 2 + d\nd+ 2\n( 1\n2 lnn+ ln\nC\n12 \u221a \u03c0\n+ ln \u2016p\u2016d/(d+2) )] (21)\nwith probability 1 \u2212 \u03b4, where the expectation is with respect to the random choice Z in the algorithm. From this the bound of Theorem 2 follows."}, {"heading": "C K-Flats", "text": "Here we state a series of lemma that we prove in the next section. For the k-flats problem, we begin by introducing a uniform bound on the difference between empirical (Equation 2) and expected risk (Equation 1.)\nLemma 1. If Fk is the class of sets of k d-dimensional affine spaces then, with probability 1\u2212 \u03b4 on the sampling of Xn \u223c p, it is\nsup X\u2032\u2208Fk\n|E\u03c1(X \u2032)\u2212 En(X \u2032)| \u2264 k \u221a 2\u03c0d\nn +\n\u221a ln 1/\u03b4\n2n\nBy combining the above result with approximation error bounds, we may produce performance bounds on the expected risk for the k-flats problem, with appropriate choice of parameter kn. We distinguish between the codimension one hypersurface case, and the more general case of a smooth manifold M embedded in a Hilbert space. We begin with an approximation error bound for hypersurfaces in Euclidean space.\nLemma 2. Assume given M smooth with metric of class C3 in Rd+1. If Fk is the class of sets of k d-dimensional affine spaces, and E\u2217\u03c1,k is the minimizer of Equation 1 over Fk, then there is a constant C that depends on d only, such that\nlim k\u2192\u221e\nE\u2217\u03c1,k \u00b7 k4/d \u2264 C \u00b7 (\u03baM) 4/d\nwhere \u03baM := \u00b5|II|(M) is the total root curvature of M, and \u00b5|II| is the measure associated with the (positive) second fundamental form. The constant C grows as C \u223c (d/(2\u03c0e))2 with d\u2192\u221e.\nFor the more general problem of approximation of a smooth manifold in a separable Hilbert space, we begin by considering the definitions in Section 4 the second fundamental form II and its operator norm |IIq| at a point q \u2208M. The we have:\nLemma 3. Assume given a d-manifold M with metric in C3 embedded in a separable Hilbert space X . If Fk is the class of sets of k d-dimensional affine spaces, and E\u2217\u03c1,k is the minimizer of Equation 1 over Fk, then there is a constant C that depends on d only, such that\nlim k\u2192\u221e\nE\u2217\u03c1,k \u00b7 k4/d \u2264 C \u00b7 (\u03baM) 4/d\nwhere \u03baM := \u222b M d\u00b5I(x) 1 4 |IIx|\n2 and \u00b5I is the volume measure over M. The constant C grows as C \u223c (d/(2\u03c0e)) 2 with d\u2192\u221e.\nWe combine these two results into Theorems 3 and 4, whose derivation is in Appendix B.\nC.1 Proofs\nWe begin proving the bound on the statistical error given in Lemma 1.\nProof. We begin by finding uniform upper bounds on the difference between Equations 1 and 2 for the class Fk of sets of k d-dimensional affine spaces. To do this, we will first bound the Rademacher complexity Rn(Fk, p) of the class Fk.\nLet \u03a6 and \u03a8 be Gaussian processes indexed by Fk, and defined by\n\u03a6X\u2032 = n\u2211 i=1 \u03b3i k min j=1 d2 X (xi, \u03c0 \u2032 jxi)\n\u03a8X\u2032 = n\u2211 i=1 \u03b3i k\u2211 j=1 d2 X (xi, \u03c0 \u2032 jxi)\n(22)\nX \u2032 \u2208 Fk, X \u2032 is the union of k d-subspaces: X \u2032 = \u222akj=1Fj , where each \u03c0\u2032j is an orthogonal projection onto Fj , and \u03b3i are independent Gaussian sequences of zero mean and unit variance.\nNoticing that d2 X (x, \u03c0x) = \u2016x\u20162 \u2212 \u2016\u03c0x\u20162 = \u2016x\u20162 \u2212 \u3008xxt, \u03c0\u3009 F for any orthogonal projection \u03c0 (see for instance [11], Sec. 2.1), where \u3008\u00b7, \u00b7\u3009\nF is the Hilbert-Schmidt inner product, we may verify that:\nE\u03b3 (\u03a6X\u2032 \u2212 \u03a6X\u2032\u2032)2 = n\u2211 i=1 [ k min j=1 \u2016xi\u20162 \u2212 \u2329 xix t i, \u03c0 \u2032 j \u232a F \u2212 ( k min j=1 \u2016xi\u20162 \u2212 \u2329 xix t i, \u03c0 \u2032\u2032 j \u232a F )]2 \u2264\nn\u2211 i=1 k max j=1 (\u2329 xix t i, \u03c0 \u2032 j \u232a F \u2212 \u2329 xix t i, \u03c0 \u2032\u2032 j \u232a F )2 \u2264\nn\u2211 i=1 k\u2211 j=1 (\u2329 xix t i, \u03c0 \u2032 j \u232a F \u2212 \u2329 xix t i, \u03c0 \u2032\u2032 j \u232a F )2 = E\u03b3 (\u03a8X\u2032 \u2212\u03a8X\u2032\u2032)2\n(23)\nSince it is,\nE\u03b3 sup X\u2032\u2208Fk n\u2211 i=1 \u03b3i k\u2211 j=1 \u2329 xix t i, \u03c0 \u2032 j \u232a F = E\u03b3 sup X\u2032\u2208Fk k\u2211 j=1 \u2329 n\u2211 i=1 \u03b3ixix t i, \u03c0 \u2032 j \u232a F\n\u2264 kE\u03b3 sup \u03c0 \u2329 n\u2211 i=1 \u03b3ixix t i, \u03c0 \u232a F\n\u2264 k sup \u03c0 \u2016\u03c0\u2016 F E\u03b3 \u2016 n\u2211 i=1 \u03b3ixix t i\u2016F \u2264 k \u221a dn\n(24)\nwe may bound the Gaussian complexity \u0393n(Fk, p) as follows:\n\u0393n(Fk, p) = 2\nn E\u03b3 sup\nX\u2032\u2208Fk n\u2211 i=1 \u03b3i k min j=1 d2 X (xi, \u03c0 \u2032 jxi)\n\u2264 2 n E\u03b3 sup\nX\u2032\u2208Fk n\u2211 i=1 \u03b3i k\u2211 j=1 \u2329 xix t i, \u03c0 \u2032 j \u232a F \u2264 2k\n\u221a d\nn\n(25)\nwhere the first inequality follows from Equation 23 and Slepian\u2019s Lemma [41], and the second from Equation 24.\nTherefore the Rademacher complexity is bounded by\nRn(Fk, p) \u2264 \u221a \u03c0/2\u0393n(Fk, p) \u2264 k\n\u221a 2\u03c0d\nn (26)\nFinally, by Theorem 8 of [6], it is:\nsup X\u2032\u2208Fk\n|E\u03c1(X \u2032)\u2212 En(X \u2032)| \u2264 Rn(Fk, p) + \u221a ln 1/\u03b4\n2n \u2264 k\n\u221a 2\u03c0d\nn +\n\u221a ln 1/\u03b4\n2n (27)\nas desired.\nC.2 Approximation Error\nIn order to prove approximation bounds for the k-flats problem, we will begin by first considering the simpler setting of a smooth d-manifold in Rd+1 space (codimension 1), and later we will extend the analysis to the general case.\nApproximation Error: Codimension One\nAssume that it is X = Rd+1 with the natural metric, and M is a compact, smooth d-manifold with metric of class C2. Since M is of codimension one, the second fundamental form at each point is a map from the tangent space to the reals. Assume given \u03b1 > 0 and \u03bb > 0. At every point x \u2208 M, define the metric Qx := |IIx|+ \u03b1\u2032(x)Ix, where\na) I and II are, respectively, the first and second fundamental forms on M [22].\nb) |II| is the convexified second fundamental form, whose eigenvalues are those of II but in absolute value. If the second fundamental form II is written in coordinates (with respect to an orthonormal basis of the tangent space) as S\u039bST , with S orthonormal, and \u039b diagonal, then |II| is S|\u039b|ST in coordinates. Because |II| is continuous and positive semi-definite, it has an associated measure \u00b5|II| (with respect to the volume measure \u00b5I.)\nc) \u03b1\u2032(x) > 0 is chosen such that d\u00b5 Qx /d\u00b5I = d\u00b5|II|/d\u00b5I + \u03b1. Note that such \u03b1 \u2032(x) > 0 always exists since:\n\u00b7 \u03b1\u2032(x) = 0 implies d\u00b5 Qx /d\u00b5I = d\u00b5|II|/d\u00b5I, and \u00b7 d\u00b5 Qx /d\u00b5I can be made arbitrarily large by increasing \u03b1 \u2032(x).\nand therefore there is some intermediate value of \u03b1\u2032(x) > 0 that satisfies the constraint.\nIn particular, from condition c), it is clear that Q is everywhere positive definite. Let \u00b5I and \u00b5Q be the measures over M, associated with I and Q. Since, by its definition, \u00b5II is absolutely continuous with respect to I, then so must Q be. Therefore, we may define\n\u03c9 Q := d\u00b5 Q /d\u00b5I\nto be the density of \u00b5 Q with respect to \u00b5I. Consider the discrete set Pk \u2282M of size k that minimizes the quantity\nfQ,p(Pk) = \u222b M d\u00b5 Q (x) [ p(x) \u03c9 Q (x) ] min p\u2208Pk d4 Q (x, p) (28)\namong all sets of k points on M. fQ,p(Pk) is the (fourth-order) quantization error over M, with metric Q, and with respect to a weight function p/\u03c9\nQ . Note that, in the definition of fQ,p(Pk), it is crucial that\nthe measure (\u00b5 Q ), and distance (d Q ) match, in the sense that d Q is the geodesic distance with respect to the metric Q, whose associated measure is \u00b5\nQ .\nThe following theorem, adapted from [26], characterizes the relation between k and the quantization error fQ,p(Pk) on a Riemannian manifold.\nTheorem 5. [[26]] Given a smooth compact Riemannian d-manifold M with metric Q of class C1, and a continuous function w :M\u2192 R+, then\nmin P\u2208Pk \u222b M d\u00b5 Q (x)w(x) min p\u2208P dr Q (x, p) \u223c C {\u222b M d\u00b5 Q (x)w(x)d/(d+r) }(d+r)/d \u00b7 k\u2212r/d (29)\nas k \u2192\u221e, where the constant C depends only on d. Furthermore, for each connected M, there is a number \u03be > 1 such that each set Pk that minimizes\nEquation 29 is a ( k\u22121/d/\u03be ) -packing and ( \u03bek\u22121/d ) -cover of M, with respect to d\nQ . This last result, which shows that a minimizing set Pk of size k must be a ( \u03bek\u22121/d ) -cover, clearly\nimplies, by the definition of Voronoi diagram and the triangle inequality, the following key corollary.\nCorollary 1. Given M, there is \u03be > 1 such that each set Pk that minimizes Equation 29 has Voronoi regions of diameter no larger than 2\u03bek\u22121/d, as measured by the distance d\nQ .\nLet each Pk \u2282M be a minimizer of Equation 28 of size k, then, for each k, define Fk to be the union of (d-dimensional affine) tangent spaces to M at each q \u2208 Pk, that is, Fk := \u222aq\u2208PkTqM. We may now use the definition of Pk to bound the approximation error E\u03c1(Fk) on this set.\nWe begin by establishing some results that link distance to tangent spaces on manifolds to the geodesic distance d\nQ associated with Q. The following lemma appears (in a slightly different form) as Lemma 4.1\nin [16], and is borrowed from [26, 25].\nLemma 4. [[26, 25], [16]] Given M as above, and \u03bb > 0 then, for every p \u2208 M there is an open neighborhood V\u03bb(p) 3 p in M such that, for all x, y \u2208 V\u03bb(p), it is\nd2 X (x, TyM) \u2264 (1 + \u03bb)d4|II|(x, y) (30)\nwhere dX (x, TyM) is the distance from x to the tangent plane TyM at y, and d|II| is the geodesic distance associated with the convexified second fundamental form.\nFrom the definition of Q, it is clear that, because Q strictly dominates |II| then, for points x, y satisfying the conditions of Equation 30, it must be dX (x, TyM) \u2264 (1 + \u03bb)d|II|(x, y) \u2264 (1 + \u03bb)dQ(x, y).\nGiven our choice of \u03bb > 0, Lemma 4 implies that there is a collection of k neighborhoods, centered around the points p \u2208 Pk, such that Equation 30 holds inside each. However, these neighborhoods may\nbe too small for our purposes. In order to apply Lemma 4 to our problem, we will need to prove a stronger condition. We begin by considering the Dirichlet-Voronoi regions DM,Q(p;Pk) of points p \u2208 Pk, with respect to the distance d\nQ . That is,\nDM,Q(p;Pk) = {x \u2208M : dQ(x, p) \u2264 dQ(x, q),\u2200q \u2208 Pk}\nwhere, as before, Pk is a set of size k minimizing Equation 28.\nLemma 5. For each \u03bb > 0, there is k\u2032 such that, for all k \u2265 k\u2032, and all q \u2208 Pk, Equation 30 holds for all x, y \u2208 DM,Q(q;Pk).\nRemark Note that, if it were P \u2032k \u2282 Pk with k > k\u2032 (if each Pk+1 were constructed by adding one point to Pk), then Lemma 5 would follow automatically from Lemma 4 and Corollary 1. Since, in general, this not the case, the following proof is needed.\nProof. It suffices to show that every Voronoi region DM,Q(q;Pk), for sufficiently large k, is contained in a neighborhood V\u03bb(vq) of the type described in Lemma 4, for some vq \u2208M.\nClearly, by Lemma 4, the set C = {V\u03bb(x) : x \u2208 M} is an open cover of M. Since M is compact, C admits a finite subcover C \u2032. By the Lebesgue number lemma, there is \u03b4 > 0 such that every set inM of diameter less than \u03b4 is contained in some open set of C \u2032.\nNow let k\u2032 = d(\u03b4/2\u03be)\u2212de. By Corollary 1, every Voronoi region DM,Q(q;Pk), with q \u2208 Pk, k \u2265 k\u2032, has diameter less than \u03b4, and is therefore contained in some set of C \u2032. Since Equation 30 holds inside every set of C \u2032 then, in particular, it holds inside DM,Q(q;Pk).\nWe now have all the tools needed to prove:\nLemma 2 If Fk is the class of sets of k d-dimensional affine spaces, and E\u2217\u03c1,k is the minimizer of Equation 1 over Fk, then there is a constant C that depends on d only, such that\nlim k\u2192\u221e\nE\u2217\u03c1,k \u00b7 k4/d \u2264 C \u00b7 (\u03baM) 4/d\nwhere \u03baM := \u00b5|II|(M) is the total root curvature of M. The constant C grows as C \u223c (d/(2\u03c0e)) 2\nwith d\u2192\u221e.\nProof. Pick \u03b1 > 0 and \u03bb > 0. Given Pk minimizing Equation 28, if Fk is the union of tangent spaces at each p \u2208 Pk, by Lemmas 4 and 5, it is\nE\u03c1(Fk) = \u222b M d\u00b5 I (x)p(x) min p\u2208Pk d2 X (x, TpM)\n\u2264 (1 + \u03bb) \u222b M d\u00b5I(x)p(x) min p\u2208Pk d4 Q (x, p)\n= (1 + \u03bb) \u222b M d\u00b5 Q (x) p(x) \u03c9Q(x) min p\u2208Pk d4 Q (x, p)\nThm. 5, r=4\n\u2264 (1 + \u03bb)C {\u222b M d\u00b5 Q (x) [ p(x) \u03c9Q(x) ]d/(d+4)}(d+4)/d \u00b7 k\u22124/d\n(31)\nwhere the last line follows from the fact that Pk has been chosen to minimize Equation 28, and where, in order to apply Theorem 5, we use the fact that p is absolutely continuous in M.\nBy the definition of \u03c9Q, it follows that{\u222b M d\u00b5 Q (x) [ p(x) \u03c9Q(x) ]d/(d+4)}(d+4)/d = {\u222b M d\u00b5 I (x)\u03c9Q(x) 4/(d+4)p(x)d/(d+4) }(d+4)/d\n\u2264 {\u222b M d\u00b5 I (x)\u03c9 Q (x) }4/d (32)\nwhere the last line follows from Ho\u0308lder\u2019s inequality (\u2016fg\u20161 \u2264 \u2016f\u2016p\u2016g\u2016q with p = (d + 4)/d > 1, and q = (d+ 4)/4.)\nFinally, by the definition of Q and \u03b1\u2032, it is\u222b M d\u00b5 I (x)\u03c9 Q (x) \u2264 \u222b M d\u00b5 I (x)\u03b1+ \u222b M d\u00b5|II|(x) = \u03b1VM + \u03baM (33)\nwhere VM is the total volume of M, and \u03baM := \u00b5|II|(M) is the total root curvature of M. Therefore\nE\u03c1(Fk) \u2264 (1 + \u03bb)C {\u03b1VM + \u03baM} 4/d \u00b7 k\u22124/d (34)\nSince \u03b1 > 0 and \u03bb > 0 are arbitrary, Lemma 2 follows. Finally, we discuss an important technicality in the proof that we hadn\u2019t mentioned before in the interest of clarity of exposition. Because we are taking absolutely values in its definition, Q is not necessarily of class C1, even if II is. Therefore, we may not apply Theorem 5 directly. We may, however, use Weierstrass\u2019 approximation theorem (see for example [21] p. 133), to obtain a smooth -approximation to Q, which can be enforced to be positive definite by relating the choice of to that of \u03b1, and with \u2192 0 as \u03b1 \u2192 0. Since the -approximation Q only affects the final performance (Equation 34) by at most a constant times , then the fact that \u03b1 is arbitrarily small (and thus so is ) implies the lemma.\nApproximation Error: General Case\nAssume given a d-manifoldM with metric in C3 embedded in a separable Hilbert space X . Consider the definition in Section 4 of the second fundamental form II and its operator norm |II|.\nWe begin extending the results of Lemma 4 to the general case, where the manifold is embedded in a possibly infinite-dimensional ambient space. In this case, the orthogonal complement (TxM)\u22a5 to the tangent space at x \u2208 M may be infinite-dimensional (although, by the separability of X , it has a countable basis.)\nFor each x \u2208 M, consider the largest x-centered ball Bx(\u03b5) for which there is a smooth one-to-one Monge patch mx : Bx(\u03b5x) \u2282 TxM\u2192M. Since M is smooth, and II bounded, by the inverse function theorem it holds \u03b5x > 0. Because II \u2208 C1, we can always choose \u03b5x to be continuous in M, and thus by the compactness ofM there is a minimum 0 < \u03b5 such that 0 < \u03b5 \u2264 \u03b5x with x \u2208M. Let Nx(\u03b4) denote the geodesic neighborhood around x \u2208M of radius \u03b4. We begin by proving the following technical Lemma.\nLemma 6. For every q \u2208 M, there is \u03b4q such that, for all x, y \u2208 Nq(\u03b4q), it is x \u2208 my(By(\u03b5)) (x is in the Monge patch of y.)\nProof. The Monge function my : By( )\u2192M is such that r \u2208 By( ) implies my(r)\u2212 (y + r) \u2208 (TyM)\u22a5 (with the appropriate identification of vectors in X and in (TyM)\u22a5), and therefore for all r \u2208 By( ) it holds\ndI(y,my(r)) \u2265 \u2016my(r)\u2212 y\u2016X = \u2016my(r)\u2212 (y + r) + (y + r)\u2212 y\u2016X = \u2016my(r)\u2212 (y + r)\u2016X + \u2016r\u2016X \u2265 \u2016r\u2016X\nTherefore Ny(\u03b5) \u2282 my(By(\u03b5)). For each q \u2208M, the geodesic ballNq(\u03b5/2) is such that, by the triangle inequality, for all x, y \u2208 Nq(\u03b5/2) it is dI(x, y) \u2264 \u03b5. Therefore x \u2208 Ny(\u03b5) \u2282 my(By(\u03b5)).\nLemma 7. For all \u03bb > 0 and q \u2208M, there is a neighborhood V 3 q such that, for all x, y \u2208 V it is\nd2X (x, TyM) \u2264 (1 + \u03bb)d4I (x, y)|IIx|2 (35)\nProof. Let V be a geodesic neighborhood of radius smaller than \u03b5, so that Lemma 6 holds. Define the extension II\u2217x(r) = II \u2217 x(r t + r\u22a5) := IIx(r t) of the second fundamental form to X , where rt \u2208 TxM and r\u22a5 \u2208 (TxM)\u22a5 is the unique decomposition of r \u2208 X into tangent and orthogonal components.\nBy Lemma 6, given x, y \u2208 V , x is in the (one-to-one) Monge patch my of y. Let x\u2032 \u2208 TyM be the unique point such that my(x\n\u2032) = x, and let r := (x\u2032 \u2212 y)/\u2016x\u2032 \u2212 y\u2016X . Since the domain of my is convex, the curve \u03b3y,r : [0, \u2016x\u2032 \u2212 y\u2016X ]\u2192M given by\n\u03b3y,r(t) = y + tr +my(tr) = y + tr + 1\n2 t2IIy(r) + o(t 2)\nis well-defined, where the last equality follows from the smoothness of II. Clearly, \u03b3y,r(\u2016x\u2032 \u2212 y\u2016X ) = x. For 0 \u2264 t \u2264 \u2016x\u2032 \u2212 y\u2016X the length of \u03b3y,r([0, t]) is\nL(\u03b3y,r([0, t])) = \u222b t 0 d\u03c4\u2016 \u02d9\u03b3y,r(\u03c4)\u2016X = \u222b t 0 d\u03c4 (\u2016r\u2016X +O(t)) = t \u00b7 (1 + o(1)) (36)\n(where o(1) \u2192 0 as t \u2192 0.) This establishes the closeness of distances in TyM to geodesic distance on M. In particular, for any \u03b1 > 0, y \u2208 M, there is a sufficiently small geodesic neighborhood N 3 y such that, for x \u2208 N , it holds\n\u2016x\u2032 \u2212 y\u2016X \u2264 \u2016x\u2212 y\u2016X \u2264 dI(x, y) \u2264 (1 + \u03bb)\u2016x\u2032 \u2212 y\u2016X\nBy the smoothness of II, for y \u2208M and x \u2208 Ny(\u03b4y), with 0 < \u03b4y < \u03b5, it is\nd2X (x, TyM) = d2X (\u03b3y,r(\u2016x\u2032 \u2212 y\u2016X ), TyM) = \u2016 1\n2 IIy(r)\u2016x\u2032 \u2212 y\u20162X + o(\u2016x\u2032 \u2212 y\u20162X )\u20162\n= \u20161 2 II\u2217y(x\u2212 y) + o(\u03b42y)\u20162\nand therefore for any \u03b1 > 0, there is a sufficiently small 0 < \u03b4y,\u03b1 < \u03b5 such that, given any x \u2208 Ny(\u03b4y,\u03b1), it is\nd2X (x, TyM) \u2264 (1 + \u03b1)\u2016 1 2 II\u2217y(x\u2212 y)\u20162 (37)\nBy the smoothness of II, and the same argument as in Lemma 6, there is a continuous choice of 0 < \u03b4y,\u03b1, and therefore a minimum value 0 < \u03b4\u03b1 \u2264 \u03b4y,\u03b1, for y \u2208M.\nSimilarly, by the smoothness of II\u2217, for any \u03b1 > 0 and y \u2208 M, there is a sufficiently small \u03b2y,\u03b1 > 0 such that, for all x \u2208 Ny(\u03b2y,\u03b1), it holds\n\u20161 2 II\u2217y(y \u2212 x)\u20162 \u2264 (1 + \u03b1)\u2016 1 2 II\u2217x(y \u2212 x)\u20162 (38)\nBy the argument of Lemma 6, there is a continuous choice of 0 < \u03b2y,\u03b1, and therefore a minimum value 0 < \u03b2\u03b1 \u2264 \u03b2y,\u03b1, for y \u2208M.\nFinally, let \u03b1 = \u03bb/4, and restrict 0 < \u03bb < 1 (larger \u03bb are simply less restrictive.) For each q \u2208 M, let V = Nq(min{\u03b4\u03b1, \u03b2\u03b1}/2) 3 q be a sufficiently small geodesic neighborhood such that, for all x, y \u2208 V , Eqs. 37 and 38 hold.\nSince \u03b1 = \u03bb/4 < 1/4, it is clearly (1 + \u03b1)2 \u2264 (1 + \u03bb), and therefore\nd2X (x, TyM) \u2264 (1 + \u03b1)\u2016 1 2 II\u2217y(y \u2212 x)\u20162 \u2264 (1 + \u03b1)2\u2016 1 2 II\u2217x(y \u2212 x)\u20162\n\u2264 (1 + \u03bb)1 4 \u2016y \u2212 x\u20164|IIx|2 \u2264 (1 + \u03bb) 1 4 d4I (x, y)|IIx|2\n(39)\nwhere the second-to-last inequality follows from the definition of |II|.\nNote that the same argument as that of Lemma 5 can be used here, with the goal of making sure that, for sufficiently large k, every Voronoi region of each p \u2208 Pk in the approximation satisfies Equation 35. We may now finish the proof by using a similar argument to that of the codimension-one case.\nLet \u03bb > 0. Consider a discrete set Pk \u2282M of size k that minimizes\ng(Pk) = \u222b M d\u00b5I(x) 1 4 p(x)|IIx|2 min p\u2208Pk d4 I (x, p) (40)\nNote once again that the distance and measure in Equation 40 match and therefore, since p(x)|IIx|2/4 is continuous, we can apply Theorem 5 (with r = 4) in this case.\nLet Fk := \u222aq\u2208PkTqM. By Lemma 7 and Lemma 5, adapted to this case, there is k\u2032 such that for all k \u2265 k\u2032 it is\nE\u03c1(Fk) = \u222b M d\u00b5 I (x) 1 4 p(x) min p\u2208Pk d2 X (x, TpM)\n\u2264 (1 + \u03bb) \u222b M d\u00b5I(x) 1 4 p(x)|IIx|2 min p\u2208Pk d4 I (x, p)\nThm. 5,r=4\n\u2264 (1 + \u03bb)C {\u222b M d\u00b5 I (x) [ 1 4 p(x)|IIx|2 ]d/(d+4)}(d+4)/d \u00b7 k\u22124/d\n(41)\nwhere the last line follows from the fact that Pk has been chosen to minimize Equation 40. Finally, by Ho\u0308lder\u2019s inequality, it is{\u222b\nM d\u00b5 I (x)\n[ 1\n4 p(x)|IIx|2 ]d/(d+4)}(d+4)/d \u2264 {\u222b M d\u00b5 I (x)p(x) }{\u222b M d\u00b5 I (x) ( 1 4 |IIx\u20162 )d/4}4/d = \u20161\n4 |II|2\u2016d/4\nand thus E\u03c1(Fk) \u2264 (1 + \u03bb)C \u00b7 (\u03baM/k)4/d\nwhere the total curvature \u03baM := \u222b M d\u00b5 I (x) 1 4 |IIx|d/2 is the geometric invariant of the manifold (aside from the dimension) that controls the constant in the bound. Since \u03b1 > 0 and \u03bb > 0 are arbitrary, Lemma 3 follows.\nProofs of Theorems 3 and 4\nWe use the results discussed in Section A to obtain the proof of Theorem 3 as follows. The proof of Theorem 4 follows from the derivation in Section A, as well as the argument below, with \u03ba1\nM substituted\nby \u03baM , and is omitted in the interest of brevity.\nProof. By Lemmas 1 and 2, with probability 1\u2212 \u03b4, it is\nE\u03c1(Fn,k) \u2264 2n\u22121/2 ( k \u221a 2\u03c0d+ \u221a 1\n2 ln 1/\u03b4\n) + C(\u03ba1\nM /k)4/d\n\u2264 2n\u22121/2k \u221a 2\u03c0d \u00b7 \u221a 1\n2 ln 1/\u03b4 + C(\u03ba1 M /k)4/d\n= 2 (8\u03c0d) 2/(d+4) Cd/(d+4) \u00b7 n\u22122/(d+4) \u00b7 \u221a 1\n2 ln 1/\u03b4 \u00b7\n( \u03ba1 M\n)4/(d+4) (42)\nwhere the last line follows from choosing k to balance the two summands of the second line, as:\nkn = n d 2(d+4) \u00b7 ( C\n2 \u221a 2\u03c0d\n)d/(d+4) \u00b7 ( \u03ba1 M )4/(d+4)"}], "references": [{"title": "Multiscale geometric methods for data sets ii: Geometric multi-resolution analysis", "author": ["William K Allard", "Guangliang Chen", "Mauro Maggioni"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Np\u2013hardness of euclidean sum-of-squares clustering", "author": ["Daniel Aloise", "Amit Deshpande", "Pierre Hansen", "Preyas Popat"], "venue": "Mach. Learn.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "k\u2013means++: the advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Voronoi diagrams: A survey of a fundamental geometric data structure", "author": ["Franz Aurenhammer"], "venue": "ACM Comput. Surv.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "The minimax distortion redundancy in empirical quantizer design", "author": ["Peter L. Bartlett", "Tamas Linder", "Gabor Lugosi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["Peter L. Bartlett", "Shahar Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput., 15(6):1373\u20131396", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Manifold regularization: a geometric framework for learning from labeled and unlabeled examples", "author": ["Mikhail Belkin", "Partha Niyogi", "Vikas Sindhwani"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "A framework for statistical clustering with constant time approximation algorithms for k-median and k-means clustering", "author": ["Shai Ben-David"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "On the performance of clustering in hilbert spaces", "author": ["G\u00e9rard Biau", "Luc Devroye", "G\u00e1bor Lugosi"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Statistical properties of kernel principal component analysis", "author": ["Gilles Blanchard", "Olivier Bousquet", "Laurent Zwald"], "venue": "Mach. Learn.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "k-plane clustering", "author": ["P.S. Bradley", "O.L. Mangasarian"], "venue": "J. of Global Optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Empirical risk approximation: An induction principle for unsupervised learning", "author": ["Joachim M. Buhmann"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Information theoretic model validation for clustering", "author": ["Joachim M. Buhmann"], "venue": "In International Symposium on Information Theory, Austin Texas", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "On the optimization of weighted cubature formulae on certain classes of continuous functions", "author": ["E V Chernaya"], "venue": "East J. Approx", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Building triangulations using -nets", "author": ["Kenneth L. Clarkson"], "venue": "In Proceedings of the thirty-eighth annual ACM symposium on Theory of computing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Set estimation", "author": ["A. Cuevas", "R. Fraiman"], "venue": "New perspectives in stochastic geometry, pages 374\u2013397. Oxford Univ. Press, Oxford", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Set estimation: an overview and some recent developments", "author": ["A. Cuevas", "A. Rod\u0155\u0131guez-Casal"], "venue": "Recent advances and trends in nonparametric statistics, pages 251\u2013264. Elsevier B. V., Amsterdam", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Random projection trees for vector quantization", "author": ["Sanjoy Dasgupta", "Yoav Freund"], "venue": "IEEE Trans. Inf. Theor.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Kernel k-means: spectral clustering and normalized cuts", "author": ["Inderjit S. Dhillon", "Yuqiang Guan", "Brian Kulis"], "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Foundations of Modern Analysis", "author": ["J. Dieudonne"], "venue": "Pure and Applied Mathematics. Hesperides Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Riemannian geometry", "author": ["M.P. DoCarmo"], "venue": "Theory and Applications Series. Birkh\u00e4user", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1992}, {"title": "Vector quantization and signal compression", "author": ["Allen Gersho", "Robert M. Gray"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1991}, {"title": "Foundations of quantization for probability distributions", "author": ["Siegfried Graf", "Harald Luschgy"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "Asymptotic estimates for best and stepwise approximation of convex bodies i", "author": ["P.M. Gruber"], "venue": "Forum Mathematicum, 15:281\u2013297", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1993}, {"title": "Optimum quantization and its applications", "author": ["Peter M. Gruber"], "venue": "Adv. Math,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Convex and discrete geometry", "author": ["P.M. Gruber"], "venue": "Grundlehren der mathematischen Wissenschaften. Springer", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Intrinsic dimensionality estimation of submanifolds in rd", "author": ["Matthias Hein", "Jean-Yves Audibert"], "venue": "Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["V. De Silva J.B. Tenenbaum", "J.C. Langford"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2000}, {"title": "Spectral surface reconstruction from noisy point clouds", "author": ["Ravikrishna Kolluri", "Jonathan Richard Shewchuk", "James F. O\u2019Brien"], "venue": "In Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "The Concentration of Measure Phenomenon", "author": ["M. Ledoux"], "venue": "Mathematical Surveys and Monographs. American Mathematical Society", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Mesh-independent surface interpolation", "author": ["David Levin"], "venue": "Geometric Modeling for Scientific Visualization,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "Least squares quantization in pcm", "author": ["Stuart P. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "L. M. Le Cam and J. Neyman, editors, Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 281\u2013297. University of California Press", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1967}, {"title": "The planar k\u2013means problem is np-hard", "author": ["Meena Mahajan", "Prajakta Nimbhorkar", "Kasturi Varadarajan"], "venue": "In Proceedings of the 3rd International Workshop on Algorithms and Computation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Online dictionary learning for sparse coding", "author": ["Julien Mairal", "Francis Bach", "Jean Ponce", "Guillermo Sapiro"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "K\u2013dimensional coding schemes in hilbert spaces", "author": ["A. Maurer", "M. Pontil"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "Sample complexity of testing the manifold hypothesis", "author": ["Hariharan Narayanan", "Sanjoy Mitter"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Strong consistency of k-means clustering", "author": ["David Pollard"], "venue": "Annals of Statistics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1981}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["ST Roweis", "LK Saul"], "venue": "Science, 290:2323\u2013 2326", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2000}, {"title": "The one-sided barrier problem for Gaussian noise", "author": ["David Slepian"], "venue": "Bell System Tech. J.,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1962}, {"title": "Nonparametric regression between general Riemannian manifolds", "author": ["Florian Steinke", "Matthias Hein", "Bernhard Sch\u00f6lkopf"], "venue": "SIAM J. Imaging Sci.,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Support vector machines", "author": ["I. Steinwart", "A. Christmann"], "venue": "Information Science and Statistics. Springer, New York", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Sur la representation d \u0301une population in par une nombre d \u0301elements", "author": ["G Fejes Toth"], "venue": "Acta Math. Acad. Sci. Hungaricae", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1959}, {"title": "A tutorial on spectral clustering", "author": ["Ulrike von Luxburg"], "venue": "Stat. Comput.,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2007}], "referenceMentions": [{"referenceID": 28, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 14, "endOffset": 25}, {"referenceID": 39, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 14, "endOffset": 25}, {"referenceID": 6, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 14, "endOffset": 25}, {"referenceID": 41, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 180, "endOffset": 183}, {"referenceID": 44, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 199, "endOffset": 203}, {"referenceID": 6, "context": "Starting from [29, 40, 7], this set of ideas, broadly referred to as manifold learning, has been applied to a variety of problems from supervised [42] and semi-supervised learning [8], to clustering [45] and dimensionality reduction [7], to name a few.", "startOffset": 233, "endOffset": 236}, {"referenceID": 31, "context": "R), and the data are typically not sampled probabilistically, see for instance [32, 30].", "startOffset": 79, "endOffset": 87}, {"referenceID": 29, "context": "R), and the data are typically not sampled probabilistically, see for instance [32, 30].", "startOffset": 79, "endOffset": 87}, {"referenceID": 16, "context": "The problem of learning a manifold is also related to that of estimating the support of a distribution, (see [17, 18] for recent surveys.", "startOffset": 109, "endOffset": 117}, {"referenceID": 17, "context": "The problem of learning a manifold is also related to that of estimating the support of a distribution, (see [17, 18] for recent surveys.", "startOffset": 109, "endOffset": 117}, {"referenceID": 0, "context": "The reconstruction framework that we consider is related to the work of [1, 38], as well as to the framework proposed in [37], in which a manifold is approximated by a set, with performance measured by an expected distance to this set.", "startOffset": 72, "endOffset": 79}, {"referenceID": 37, "context": "The reconstruction framework that we consider is related to the work of [1, 38], as well as to the framework proposed in [37], in which a manifold is approximated by a set, with performance measured by an expected distance to this set.", "startOffset": 72, "endOffset": 79}, {"referenceID": 36, "context": "The reconstruction framework that we consider is related to the work of [1, 38], as well as to the framework proposed in [37], in which a manifold is approximated by a set, with performance measured by an expected distance to this set.", "startOffset": 121, "endOffset": 125}, {"referenceID": 35, "context": "This setting is similar to the problem of dictionary learning (see for instance [36], and extensive references therein), in which a dictionary is found by minimizing a similar reconstruction error, perhaps with additional constraints on an associated encoding of the data.", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "This is the same reconstruction measure that has been the recent focus of [37, 5, 38].", "startOffset": 74, "endOffset": 85}, {"referenceID": 4, "context": "This is the same reconstruction measure that has been the recent focus of [37, 5, 38].", "startOffset": 74, "endOffset": 85}, {"referenceID": 37, "context": "This is the same reconstruction measure that has been the recent focus of [37, 5, 38].", "startOffset": 74, "endOffset": 85}, {"referenceID": 33, "context": "1 Using K-Means and K-Flats for Piecewise Manifold Approximation In this work, we focus on two specific algorithms, namely k-means [34, 33] and k-flats [12].", "startOffset": 131, "endOffset": 139}, {"referenceID": 32, "context": "1 Using K-Means and K-Flats for Piecewise Manifold Approximation In this work, we focus on two specific algorithms, namely k-means [34, 33] and k-flats [12].", "startOffset": 131, "endOffset": 139}, {"referenceID": 11, "context": "1 Using K-Means and K-Flats for Piecewise Manifold Approximation In this work, we focus on two specific algorithms, namely k-means [34, 33] and k-flats [12].", "startOffset": 152, "endOffset": 156}, {"referenceID": 19, "context": "The study of manifolds embedded in a Hilbert space is of special interest when considering non-linear (kernel) versions of the algorithms [20].", "startOffset": 138, "endOffset": 142}, {"referenceID": 12, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 8, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 36, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 9, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 36, "context": "where, for any fixed set S, En(S) is an unbiased empirical estimate of E\u03c1(S), so that k-means can be seen to be performing a kind of empirical risk minimization [13, 9, 37, 10, 37].", "startOffset": 161, "endOffset": 180}, {"referenceID": 3, "context": ",mk}, which induces a Dirichlet-Voronoi tiling of X : a collection of k regions, each closest to a common mean [4] (in our notation, the subscript n denotes the dependence of Sn,k on the sample, while k refers to its size.", "startOffset": 111, "endOffset": 114}, {"referenceID": 32, "context": "These two facts imply that it is possible to compute a local minimum of the empirical risk by using a greedy coordinate-descent relaxation, namely Lloyd\u2019s algorithm [33].", "startOffset": 165, "endOffset": 169}, {"referenceID": 2, "context": "Even though Lloyd\u2019s algorithm provides no guarantees of closeness to the global minimizer, in practice it is possible to use a randomized approximation algorithm, such as kmeans++ [3], which provides guarantees of approximation to the global minimum in expectation with respect to the randomization.", "startOffset": 180, "endOffset": 183}, {"referenceID": 13, "context": ") For instance, this point of view is considered in [14] where k-means is studied from an information theoretic persepective.", "startOffset": 52, "endOffset": 56}, {"referenceID": 22, "context": "K-means can also be interpreted to be performing vector quantization, where the goal is to minimize the encoding error associated to a nearest-neighbor quantizer [23].", "startOffset": 162, "endOffset": 166}, {"referenceID": 38, "context": "Interestingly, in the limit of increasing sample size, this problem coincides, in a precise sense [39], with the problem of optimal quantization of probability distributions (see for instance the excellent monograph of [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "Interestingly, in the limit of increasing sample size, this problem coincides, in a precise sense [39], with the problem of optimal quantization of probability distributions (see for instance the excellent monograph of [24].", "startOffset": 219, "endOffset": 223}, {"referenceID": 25, "context": ") As in the Euclidean setting, the limit of this problem with increasing sample size is precisely the problem of optimal quantization of distributions on manifolds, which is the subject of significant recent work in the field of optimal quantization [26, 27].", "startOffset": 250, "endOffset": 258}, {"referenceID": 26, "context": ") As in the Euclidean setting, the limit of this problem with increasing sample size is precisely the problem of optimal quantization of distributions on manifolds, which is the subject of significant recent work in the field of optimal quantization [26, 27].", "startOffset": 250, "endOffset": 258}, {"referenceID": 35, "context": "In this interpretation, the set of means can be seen as a dictionary of size k that produces a maximally sparse representation (the k-means encoding), see for example [36] and references therein.", "startOffset": 167, "endOffset": 171}, {"referenceID": 27, "context": "For example [28] report an average intrinsic dimension d for each digit to be between 10 and 13.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "For example in the case X = R, and under fairly general technical assumptions, it is possible to show that E\u03c1(Sk) = \u0398(k\u22122/d), where the constants depend on \u03c1 and d [24].", "startOffset": 164, "endOffset": 168}, {"referenceID": 36, "context": "In particular, this quantity has been studied for X = R, and shown to be, with high probability, of order \u221a kd/n, up-to logarithmic factors [37].", "startOffset": 140, "endOffset": 144}, {"referenceID": 36, "context": "The case where X is a Hilbert space has been considered in [37, 10], where an upper-bound of order k/ \u221a n is proven to hold with high probability.", "startOffset": 59, "endOffset": 67}, {"referenceID": 9, "context": "The case where X is a Hilbert space has been considered in [37, 10], where an upper-bound of order k/ \u221a n is proven to hold with high probability.", "startOffset": 59, "endOffset": 67}, {"referenceID": 8, "context": "The more general setting where X is a metric space has been studied in [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "The existence of such a tradeoff between the approximation, and the statistical errors may itself not be entirely obvious, see the discussion in [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 23, "context": "While the bound on E\u03c1(Sk) is known to be tight for k sufficiently large [24], the remaining terms (which are dominated by |E\u03c1(Sn,k)\u2212En(Sn,k)|) are derived by controlling the supremum of an empirical process sup S\u2208Sk |En(S)\u2212 E\u03c1(S)| (4)", "startOffset": 72, "endOffset": 76}, {"referenceID": 36, "context": "and it is unknown whether available bounds for it are tight [37].", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "Indeed, it is not clear how close the distortion redundancy E\u03c1(Sn,k)\u2212 E\u03c1(Sk) is to its known lower bound of order d \u221a k1\u2212 4 d n (in expectation) [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 4, "context": "Indeed, as pointed out in [5], \u201cThe exact dependence of the minimax distortion redundancy on k and d is still a challenging open problem\u201d.", "startOffset": 26, "endOffset": 29}, {"referenceID": 30, "context": "Because d n, with high probability, the samples are nearly orthogonal: < x1, x2 >X' 0, while a third sample x drawn uniformly on S will also very likely be nearly orthogonal to both x1, x2 [31].", "startOffset": 189, "endOffset": 193}, {"referenceID": 2, "context": "We analyze the case in which a solution is obtained from an approximation algorithm, such as k-means++ [3], to include this computational error in the bounds.", "startOffset": 103, "endOffset": 106}, {"referenceID": 42, "context": "In fact, following the ideas in [43] Section 6.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "Assume the manifoldM to have metric of class C, and finite second fundamental form II [22].", "startOffset": 86, "endOffset": 90}, {"referenceID": 24, "context": "Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].", "startOffset": 133, "endOffset": 141}, {"referenceID": 15, "context": "Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].", "startOffset": 133, "endOffset": 141}, {"referenceID": 0, "context": "Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].", "startOffset": 271, "endOffset": 278}, {"referenceID": 37, "context": "Note that these types of quantities have been linked to provably tight approximations in certain cases, such as for convex manifolds [25, 16], in contrast with worst-case methods that place a constraint on a maximum curvature, or minimum injectivity radius (for instance [1, 38].", "startOffset": 271, "endOffset": 278}, {"referenceID": 0, "context": "[1] William K Allard, Guangliang Chen, and Mauro Maggioni.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] David Arthur and Sergei Vassilvitskii.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Franz Aurenhammer.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Peter L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Peter L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] Shai Ben-David.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] G\u00e9rard Biau, Luc Devroye, and G\u00e1bor Lugosi.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] Gilles Blanchard, Olivier Bousquet, and Laurent Zwald.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] Joachim M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] Joachim M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] E V Chernaya.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] Kenneth L.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] Sanjoy Dasgupta and Yoav Freund.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] Inderjit S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[21] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] Allen Gersho and Robert M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] Siegfried Graf and Harald Luschgy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26] Peter M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] Matthias Hein and Jean-Yves Audibert.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30] Ravikrishna Kolluri, Jonathan Richard Shewchuk, and James F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[31] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] David Levin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] Stuart P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[35] Meena Mahajan, Prajakta Nimbhorkar, and Kasturi Varadarajan.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[36] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[37] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[38] Hariharan Narayanan and Sanjoy Mitter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[39] David Pollard.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[40] ST Roweis and LK Saul.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[41] David Slepian.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] Florian Steinke, Matthias Hein, and Bernhard Sch\u00f6lkopf.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[43] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[44] G Fejes Toth.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] Ulrike von Luxburg.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 107, "endOffset": 115}, {"referenceID": 23, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 196, "endOffset": 212}, {"referenceID": 26, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 196, "endOffset": 212}, {"referenceID": 25, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 196, "endOffset": 212}, {"referenceID": 14, "context": "The classical optimal quantization problem is quite well understood, going back to the fundamental work of [46, 44] on optimal quantization for data transmission, and more recently by the work of [24, 27, 26, 15].", "startOffset": 196, "endOffset": 212}, {"referenceID": 23, "context": "In particular, it is known that, for distributions with finite moment of order 2 +\u03bb, for some \u03bb > 0, it is [24]", "startOffset": 107, "endOffset": 111}, {"referenceID": 0, "context": "We note that, by setting \u03bc to be the uniform distribution over the unit cube [0, 1], it clearly is", "startOffset": 77, "endOffset": 83}, {"referenceID": 26, "context": "and thus, by making use of Zador\u2019s asymptotic formula [46], and combining it with a result of B\u00f6r\u00f6czky (see [27], p.", "startOffset": 108, "endOffset": 112}, {"referenceID": 25, "context": "The approximation error E\u2217 \u03c1,k = infSk\u2208Sk E\u03c1(Sk) of k-means is related to the problem of optimal quantization on manifolds, for which some results are known [26].", "startOffset": 157, "endOffset": 161}, {"referenceID": 36, "context": "The statistical error of Equation 14, which uniformly bounds the difference between the empirical, and expected error, has been widely-studied in recent years in the literature [37, 38, 5].", "startOffset": 177, "endOffset": 188}, {"referenceID": 37, "context": "The statistical error of Equation 14, which uniformly bounds the difference between the empirical, and expected error, has been widely-studied in recent years in the literature [37, 38, 5].", "startOffset": 177, "endOffset": 188}, {"referenceID": 4, "context": "The statistical error of Equation 14, which uniformly bounds the difference between the empirical, and expected error, has been widely-studied in recent years in the literature [37, 38, 5].", "startOffset": 177, "endOffset": 188}, {"referenceID": 36, "context": "with probability 1 \u2212 \u03b4 [37].", "startOffset": 23, "endOffset": 27}, {"referenceID": 38, "context": "Clearly, this implies convergence En(S) \u2192 E\u03c1(S) almost surely, as n \u2192 \u221e; although this latter result was proven earlier in [39], under the less restrictive condition that p have finite second moment.", "startOffset": 123, "endOffset": 127}, {"referenceID": 1, "context": "In practice, the k-means problem is NP-hard [2, 19, 35], with the original Lloyd relaxation algorithm providing no guarantees of closeness to the global minimum of Equation 2.", "startOffset": 44, "endOffset": 55}, {"referenceID": 18, "context": "In practice, the k-means problem is NP-hard [2, 19, 35], with the original Lloyd relaxation algorithm providing no guarantees of closeness to the global minimum of Equation 2.", "startOffset": 44, "endOffset": 55}, {"referenceID": 34, "context": "In practice, the k-means problem is NP-hard [2, 19, 35], with the original Lloyd relaxation algorithm providing no guarantees of closeness to the global minimum of Equation 2.", "startOffset": 44, "endOffset": 55}, {"referenceID": 2, "context": "However, practical approximations, such as the k-means++ algorithm [3], exist.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "This randomized seeding has been shown by [3] to output a set that is, in expectation, within a 8 (ln k + 2)-factor of the optimal.", "startOffset": 42, "endOffset": 45}, {"referenceID": 10, "context": "Noticing that d X (x, \u03c0x) = \u2016x\u2016 \u2212 \u2016\u03c0x\u2016 = \u2016x\u2016 \u2212 \u3008xx, \u03c0\u3009 F for any orthogonal projection \u03c0 (see for instance [11], Sec.", "startOffset": 107, "endOffset": 111}, {"referenceID": 40, "context": "where the first inequality follows from Equation 23 and Slepian\u2019s Lemma [41], and the second from Equation 24.", "startOffset": 72, "endOffset": 76}, {"referenceID": 5, "context": "Finally, by Theorem 8 of [6], it is:", "startOffset": 25, "endOffset": 28}, {"referenceID": 21, "context": "At every point x \u2208 M, define the metric Qx := |IIx|+ \u03b1(x)Ix, where a) I and II are, respectively, the first and second fundamental forms on M [22].", "startOffset": 142, "endOffset": 146}, {"referenceID": 25, "context": "The following theorem, adapted from [26], characterizes the relation between k and the quantization error fQ,p(Pk) on a Riemannian manifold.", "startOffset": 36, "endOffset": 40}, {"referenceID": 25, "context": "[[26]] Given a smooth compact Riemannian d-manifold M with metric Q of class C, and a continuous function w :M\u2192 R, then", "startOffset": 1, "endOffset": 5}, {"referenceID": 15, "context": "1 in [16], and is borrowed from [26, 25].", "startOffset": 5, "endOffset": 9}, {"referenceID": 25, "context": "1 in [16], and is borrowed from [26, 25].", "startOffset": 32, "endOffset": 40}, {"referenceID": 24, "context": "1 in [16], and is borrowed from [26, 25].", "startOffset": 32, "endOffset": 40}, {"referenceID": 25, "context": "[[26, 25], [16]] Given M as above, and \u03bb > 0 then, for every p \u2208 M there is an open neighborhood V\u03bb(p) 3 p in M such that, for all x, y \u2208 V\u03bb(p), it is d X (x, TyM) \u2264 (1 + \u03bb)d|II|(x, y) (30)", "startOffset": 1, "endOffset": 9}, {"referenceID": 24, "context": "[[26, 25], [16]] Given M as above, and \u03bb > 0 then, for every p \u2208 M there is an open neighborhood V\u03bb(p) 3 p in M such that, for all x, y \u2208 V\u03bb(p), it is d X (x, TyM) \u2264 (1 + \u03bb)d|II|(x, y) (30)", "startOffset": 1, "endOffset": 9}, {"referenceID": 15, "context": "[[26, 25], [16]] Given M as above, and \u03bb > 0 then, for every p \u2208 M there is an open neighborhood V\u03bb(p) 3 p in M such that, for all x, y \u2208 V\u03bb(p), it is d X (x, TyM) \u2264 (1 + \u03bb)d|II|(x, y) (30)", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "We may, however, use Weierstrass\u2019 approximation theorem (see for example [21] p.", "startOffset": 73, "endOffset": 77}], "year": 2013, "abstractText": "We study the problem of estimating a manifold from random samples. In particular, we consider piecewise constant and piecewise linear estimators induced by k-means and k-flats, and analyze their performance. We extend previous results for k-means in two separate directions. First, we provide new results for k-means reconstruction on manifolds and, secondly, we prove reconstruction bounds for higher-order approximation (k-flats), for which no known results were previously available. While the results for k-means are novel, some of the technical tools are well-established in the literature. In the case of k-flats, both the results and the mathematical tools are new.", "creator": "LaTeX with hyperref package"}}}