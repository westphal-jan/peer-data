{"id": "1209.3056", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2012", "title": "Parametric Local Metric Learning for Nearest Neighbor Classification", "abstract": "We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this \"independence\" approach delivers an increased flexibility its downside is the considerable risk of overfitting a number of local metrics. For example, the value of the local measure may depend on the length of a network compared with a network. Another study by Wexler found that the relative local measure is most likely to be a low quality local measure and has a higher probability of being low quality or inaccurate. While this lack of specificity does not guarantee that the network should be more robust in terms of accuracy and accuracy the local measure is likely to be better than the average in the network.\n\n\n\n\nIn the future, we will use these results to determine the way that our global community has developed for our communities. As of 2012, the average local measure is the smallest in the developed world. We believe this approach is important for reducing our dependence on local measure data. The first report from the International Community for Data Analysis (ICARS) was a project designed to help citizens and other stakeholders make sure we maintain public knowledge on how the World Community works. The ICARS website offers comprehensive information about the development of our communities on how we interact with local metrics such as the average size of communities.", "histories": [["v1", "Thu, 13 Sep 2012 22:47:07 GMT  (2191kb,D)", "http://arxiv.org/abs/1209.3056v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jun wang 0017", "alexandros kalousis", "adam woznica"], "accepted": true, "id": "1209.3056"}, "pdf": {"name": "1209.3056.pdf", "metadata": {"source": "CRF", "title": "Parametric Local Metric Learning for Nearest Neighbor Classification", "authors": ["Jun Wang"], "emails": ["Jun.Wang@unige.ch", "Adam.Woznica@unige.ch", "Alexandros.Kalousis@hesge.ch"], "sections": [{"heading": null, "text": "We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this \u201dindependence\u201d approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several largescale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner."}, {"heading": "1 Introduction", "text": "The nearest neighbor (NN) classifier is one of the simplest and most classical non-linear classification algorithms. It is guaranteed to yield an error no worse than twice the Bayes error as the number of instances approaches infinity. With finite learning instances, its performance strongly depends on the use of an appropriate distance measure. Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric. It learns a global distance metric which determines the importance of the different input features and their correlations. However, since the discriminatory power of the input features might vary between different neighborhoods, learning a global metric cannot fit well the distance over the data manifold. Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that. It increases the expressive power of standard Mahalanobis metric learning by learning a number of local metrics (e.g. one per each instance).\nar X\niv :1\n20 9.\n30 56\nv1 [\ncs .L\nG ]\nLocal metric learning has been shown to be effective for different learning scenarios. One of the first local metric learning works, Discriminant Adaptive Nearest Neighbor classification [8], DANN, learns local metrics by shrinking neighborhoods in directions orthogonal to the local decision boundaries and enlarging the neighborhoods parallel to the boundaries. It learns the local metrics independently with no regularization between them which makes it prone to overfitting. The authors of LMNN-Multiple Metric (LMNN-MM) [15] significantly limited the number of learned metrics and constrained all instances in a given region to share the same metric in an effort to combat overfitting. In the supervised setting they fixed the number of metrics to the number of classes; a similar idea has been also considered in [3]. However, they too learn the metrics independently for each region making them also prone to overfitting since the local metrics will be overly specific to their respective regions. The authors of [16] learn local metrics using a least-squares approach by minimizing a weighted sum of the distances of each instance to apriori defined target positions and constraining the instances in the projected space to preserve the original geometric structure of the data in an effort to alleviate overfitting. However, the method learns the local metrics using a learning-order-sensitive propagation strategy, and depends heavily on the appropriate definition of the target positions for each instance, a task far from obvious. In another effort to overcome the overfitting problem of the discriminative methods [8, 15], Generative Local Metric Learning, GLML, [11], propose to learn local metrics by minimizing the NN expected classification error under strong model assumptions. They use the Gaussian distribution to model the learning instances of each class. However, the strong model assumptions might easily be very inflexible for many learning problems.\nIn this paper we propose the Parametric Local Metric Learning method (PLML) which learns a smooth metric matrix function over the data manifold. More precisely, we parametrize the metric matrix of each instance as a linear combination of basis metric matrices of a small set of anchor points; this parametrization is naturally derived from an error bound on local metric approximation. Additionally we incorporate a manifold regularization on the linear combinations, forcing the linear combinations to vary smoothly over the data manifold. We develop an efficient two stage algorithm that first learns the linear combinations of each instance and then the metric matrices of the anchor points. To improve scalability and efficiency we employ a fast first-order optimization algorithm, FISTA [2], to learn the linear combinations as well as the basis metrics of the anchor points. We experiment with the PLML method on a number of large scale classification problems with tens of thousands of learning instances. The experimental results clearly demonstrate that PLML significantly improves the predictive performance over the current state-of-the-art metric learning methods, as well as over multi-class SVM with automatic kernel selection."}, {"heading": "2 Preliminaries", "text": "We denote by X the n\u00d7dmatrix of learning instances, the i-th row of which is the xTi \u2208 Rd instance, and by y = (y1, . . . , yn)T , yi \u2208 {1, . . . , c} the vector of class labels. The squared Mahalanobis distance between two instances in the input space is given by:\nd2M(xi,xj) = (xi \u2212 xj)TM(xi \u2212 xj) where M is a PSD metric matrix (M 0). A linear metric learning method learns a Mahalanobis metric M by optimizing some cost function under the PSD constraints for M and a set of additional constraints on the pairwise instance distances. Depending on the actual metric learning method, different kinds of constraints on pairwise distances are used. The most successful ones are the large margin triplet constraints. A triplet constraint denoted by c(xi,xj ,xk), indicates that in the projected space induced by M the distance between xi and xj should be smaller than the distance between xi and xk.\nVery often a single metric M can not model adequately the complexity of a given learning problem in which discriminative features vary between different neighborhoods. To address this limitation in local metric learning we learn a set of local metrics. In most cases we learn a local metric for each learning instance [8, 11], however we can also learn a local metric for some part of the instance space in which case the number of learned metrics can be considerably smaller than n, e.g. [15]. We follow the former approach and learn one local metric per instance. In principle, distances should then be defined as geodesic distances using the local metric on a Riemannian manifold. However, this is computationally difficult, thus we define the distance between instances xi and xj as:\nd2Mi(xi,xj) = (xi \u2212 xj) TMi(xi \u2212 xj)\nwhere Mi is the local metric of instance xi. Note that most often the local metric Mi of instance xi is different from that of xj . As a result, the distance d2Mi(xi,xj) does not satisfy the symmetric property, i.e. it is not a proper metric. Nevertheless, in accordance to the standard practice we will continue to use the term local metric learning following [15, 11]."}, {"heading": "3 Parametric Local Metric Learning", "text": "We assume that there exists a Lipschitz smooth vector-valued function f(x), the output of which is the vectorized local metric matrix of instance x. Learning the local metric of each instance is essentially learning the value of this function at different points over the data manifold. In order to significantly reduce the computational complexity we will approximate the metric function instead of directly learning it.\nDefinition 1 A vector-valued function f(x) on Rd is a (\u03b1, \u03b2, p)-Lipschitz smooth function with respect to a vector norm \u2016\u00b7\u2016 if \u2016f(x)\u2212 f(x\u2032)\u2016 \u2264 \u03b1 \u2016x\u2212 x\u2032\u2016 and\u2225\u2225f(x)\u2212 f(x\u2032)\u2212\u2207f(x\u2032)T (x\u2212 x\u2032)\u2225\u2225 \u2264 \u03b2 \u2016x\u2212 x\u2032\u20161+p, where \u2207f(x\u2032)T is the derivative of the f function at x\u2032. We assume \u03b1, \u03b2 > 0 and p \u2208 (0, 1].\n[18] have shown that any Lipschitz smooth real function f(x) defined on a lower dimensional manifold can be approximated by a linear combination of function values f(u),u \u2208 U, of a set U of anchor points. Based on this result we have the following lemma that gives the respective error bound for learning a Lipschitz smooth vector-valued function.\nLemma 1 Let (\u03b3,U) be a nonnegative weighting on anchor points U in Rd. Let f be an (\u03b1, \u03b2, p)Lipschitz smooth vector function. We have for all x \u2208 Rd:\u2225\u2225\u2225\u2225\u2225f(x)\u2212\u2211\nu\u2208U\n\u03b3u(x)f(u) \u2225\u2225\u2225\u2225\u2225 \u2264 \u03b1 \u2225\u2225\u2225\u2225\u2225x\u2212\u2211\nu\u2208U\n\u03b3u(x)u \u2225\u2225\u2225\u2225\u2225+ \u03b2 \u2211 u\u2208U \u03b3u(x) \u2016x\u2212 u\u20161+p (1)\nThe proof of the above Lemma 1 is similar to the proof of Lemma 2.1 in [18]; for lack of space we omit its presentation. By the nonnegative weighting strategy (\u03b3,U), the PSD constraints on the approximated local metric is automatically satisfied if the local metrics of anchor points are PSD matrices.\nLemma 1 suggests a natural way to approximate the local metric function by parameterizing the metric Mi of each instance xi as a weighted linear combination, Wi \u2208 Rm, of a small set of metric basis, {Mb1 , . . . ,Mbm}, each one associated with an anchor point defined in some region of the instance space. This parametrization will also provide us with a global way to regularize the flexibility of the metric function. We will first learn the vector of weights Wi for each instance xi, and then the basis metric matrices; these two together, will give us the Mi metric for the instance xi.\nMore formally, we define a m \u00d7 d matrix U of anchor points, the i-th row of which is the anchor point ui, where uTi \u2208 Rd. We denote by Mbi the Mahalanobis metric matrix associated with ui. The anchor points can be defined using some clustering algorithm, we have chosen to define them as the means of clusters constructed by the k-means algorithm. The local metric Mi of an instance xi is parametrized by:\nMi = \u2211 bk WibkMbk , Wibk \u2265 0, \u2211 bk Wibk = 1 (2)\nwhere W is a n \u00d7m weight matrix, and its Wibk entry is the weight of the basis metric Mbk for the instance xi. The constraint \u2211 bk Wibk = 1 removes the scaling problem between different local metrics. Using the parametrization of equation (2), the squared distance of xi to xj under the metric Mi is:\nd2Mi(xi,xj) = \u2211 bk Wibkd 2 Mbk (xi,xj) (3)\nwhere d2Mbk (xi,xj) is the squared Mahalanobis distance between xi and xj under the basis metric Mbk . We will show in the next section how to learn the weights of the basis metrics for each instance and in section 3.2 how to learn the basis metrics.\nAlgorithm 1 Smoothl Local Linear Weight Learning Input: W0, X, U, G, L, \u03bb1, and \u03bb2 Output: matrix W define g\u0303\u03b2,Y(W) = g(Y) + tr(\u2207g(Y)T (W \u2212Y)) + \u03b22 \u2016W \u2212Y\u2016 2 F\ninitialize: t1 = 1, \u03b2 = 1,Y1 = W0, and i = 0 repeat i = i+ 1, Wi = Proj((Yi \u2212 1\u03b2\u2207g(Y i)))\nwhile g(Wi) > g\u0303\u03b2,Yi(Wi) do \u03b2 = 2\u03b2, Wi = Proj((Yi \u2212 1\u03b2\u2207g(Y i))) end while ti+1 = 1+ \u221a 1+4t2i 2 , Y i+1 = Wi + ti\u22121ti+1 (W i \u2212Wi\u22121)\nuntil converges;"}, {"heading": "3.1 Smooth Local Linear Weighting", "text": "Lemma 1 bounds the approximation error by two terms. The first term states that x should be close to its linear approximation, and the second that the weighting should be local. In addition we want the local metrics to vary smoothly over the data manifold. To achieve this smoothness we rely on manifold regularization and constrain the weight vectors of neighboring instances to be similar. Following this reasoning we will learn Smooth Local Linear Weights for the basis metrics by minimizing the error bound of (1) together with a regularization term that controls the weight variation of similar instances. To simplify the objective function, we use the term\n\u2225\u2225x\u2212\u2211u\u2208U \u03b3u(x)u\u2225\u22252 instead of\n\u2225\u2225x\u2212\u2211u\u2208U \u03b3u(x)u\u2225\u2225. By including the constraints on the W weight matrix in (2), the optimization problem is given by:\nmin W\ng(W) = \u2016X\u2212WU\u20162F + \u03bb1tr(WG) + \u03bb2tr(W TLW) (4)\ns.t. Wibk \u2265 0, \u2211 bk Wibk = 1,\u2200i, bk\nwhere tr(\u00b7) and \u2016\u00b7\u2016F denote respectively the trace norm of a square matrix and the Frobenius norm of a matrix. The m\u00d7 n matrix G is the squared distance matrix between each anchor point ui and each instance xj , obtained for p = 1 in (1), i.e. its (i, j) entry is the squared Euclidean distance between ui and xj . L is the n \u00d7 n Laplacian matrix constructed by D \u2212 S, where S is the n \u00d7 n symmetric pairwise similarity matrix of learning instances and D is a diagonal matrix with Dii =\u2211 k Sik. Thus the minimization of the tr(W\nTLW) term constrains similar instances to have similar weight coefficients. The minimization of the tr(WG) term forces the weights of the instances to reflect their local properties. Most often the similarity matrix S is constructed using k-nearest neighbors graph [19]. The \u03bb1 and \u03bb2 parameters control the importance of the different terms.\nSince the cost function g(W) is convex quadratic with W and the constraint is simply linear, (4) is a convex optimization problem with a unique optimal solution. The constraints on W in (4) can be seen as n simplex constraints on each row of W; we will use the projected gradient method to solve the optimization problem. At each iteration t, the learned weight matrix W is updated by:\nWt+1 = Proj(Wt \u2212 \u03b7\u2207g(Wt)) (5) where \u03b7 > 0 is the step size and \u2207g(Wt) is the gradient of the cost function g(W) at Wt. The Proj(\u00b7) denotes the simplex projection operator on each row of W. Such a projection operator can be efficiently implemented with a complexity of O(nm log(m)) [6]. To speed up the optimization procedure we employ a fast first-order optimization method FISTA, [2]. The detailed algorithm is described in Algorithm 1. The Lipschitz constant \u03b2 required by this algorithm is estimated by using the condition of g(Wi) \u2264 g\u0303\u03b2,Yi(Wi) [1]. At each iteration, the main computations are in the gradient and the objective value with complexity O(nmd+ n2m).\nTo set the weights of the basis metrics for a testing instance we can optimize (4) given the weight of the basis metrics for the training instances. Alternatively we can simply set them as the weights of its nearest neighbor in the training instances. In the experiments we used the latter approach."}, {"heading": "3.2 Large Margin Basis Metric Learning", "text": "In this section we define a large margin based algorithm to learn the basis metrics Mb1 , . . . ,Mbm . Given the W weight matrix of basis metrics obtained using Algorithm 1, the local metric Mi of an instance xi defined in (2) is linear with respect to the basis metrics Mb1 , . . . ,Mbm . We define the relative comparison distance of instances xi, xj and xk as: d2Mi(xi,xk) \u2212 d 2 Mi\n(xi,xj). In a large margin constraint c(xi,xj ,xk), the squared distance d2Mi(xi,xk) is required to be larger than d2Mi(xi,xj) + 1, otherwise an error \u03beijk \u2265 0 is generated. Note that, this relative comparison definition is different from that defined in LMNN-MM [15]. In LMNN-MM to avoid over-fitting, different local metrics Mj and Mk are used to compute the squared distance d2Mj (xi,xj) and d2Mk(xi,xk) respectively, as no smoothness constraint is added between metrics of different local regions.\nGiven a set of triplet constraints, we learn the basis metrics Mb1 , . . . ,Mbm with the following optimization problem:\nmin Mb1 ,...,Mbm ,\u03be\n\u03b11 \u2211 bl ||Mbl ||2F + \u2211 ijk \u03beijk + \u03b12 \u2211 ij \u2211 bl Wibld 2 Mbl (xi,xj) (6)\ns.t. \u2211 bl Wibl(d 2 Mbl (xi,xk)\u2212 d2Mbl (xi,xj)) \u2265 1\u2212 \u03beijk \u2200i, j, k\n\u03beijk \u2265 0; \u2200i, j, k Mbl 0; \u2200bl\nwhere \u03b11 and \u03b12 are parameters that balance the importance of the different terms. The large margin triplet constraints for each instance are generated using its k1 same class nearest neighbors and k2 different class nearest neighbors by requiring its distances to the k2 different class instances to be larger than those to its k1 same class instances. In the objective function of (6) the basis metrics are learned by minimizing the sum of large margin errors and the sum of squared pairwise distances of each instance to its k1 nearest neighbors computed using the local metric. Unlike LMNN we add the squared Frobenius norm on each basis metrics in the objective function. We do this for two reasons. First we exploit the connection between LMNN and SVM shown in [5] under which the squared Frobenius norm of the metric matrix is related to the SVM margin. Second because adding this term leads to an easy-to-optimize dual formulation of (6) [12].\nUnlike many special solvers which optimize the primal form of the metric learning problem [15, 13], we follow [12] and optimize the Lagrangian dual problem of (6). The dual formulation leads to an efficient basis metric learning algorithm. Introducing the Lagrangian dual multipliers \u03b3ijk, pijk and the PSD matrices Zbl to respectively associate with every large margin triplet constraints, \u03beijk \u2265 0 and the PSD constraints Mbl 0 in (6), we can easily derive the following Lagrangian dual form\nmax Zb1 ,...,Zbm ,\u03b3\n\u2211 ijk \u03b3ijk \u2212 \u2211 bl 1 4\u03b11 \u00b7 \u2016Zbl + \u2211 ijk \u03b3ijkWiblCijk \u2212 \u03b12 \u2211 ij WiblAij\u20162F (7)\ns.t. 1 \u2265 \u03b3ijk \u2265 0; \u2200i,j,k Zbl 0; \u2200bl\nand the corresponding optimality conditions: M\u2217bl = (Z\u2217bl\n+ \u2211\nijk \u03b3 \u2217 ijkWiblCijk\u2212\u03b12 \u2211 ij WiblAij)\n2\u03b11 and\n1 \u2265 \u03b3ijk \u2265 0, where the matrices Aij and Cijk are given by xTijxij and xTikxik\u2212xTijxij respectively, where xij = xi \u2212 xj . Compared to the primal form, the main advantage of the dual formulation is that the second term in the objective function of (7) has a closed-form solution for Zbl given a fixed \u03b3. To drive the optimal solution of Zbl , let Kbl = \u03b12 \u2211 ijWiblAij \u2212 \u2211 ijk \u03b3ijkWiblCijk. Then, given a fixed \u03b3, the optimal solution of Zbl is Z \u2217 bl\n= (Kbl)+, where (Kbl)+ projects the matrix Kbl onto the PSD cone, i.e. (Kbl)+ = U[max(diag(\u03a3)),0)]U T with Kbl = U\u03a3U T.\nNow, (7) is rewritten as:\nmin \u03b3\ng(\u03b3) = \u2212 \u2211 ijk \u03b3ijk + \u2211 bl 1 4\u03b11 \u2016(Kbl)+ \u2212Kbl\u2016 2 F (8)\ns.t. 1 \u2265 \u03b3ijk \u2265 0;\u2200i, j, k\nAnd the optimal condition for Mbl is M \u2217 bl = 12\u03b11 ((K \u2217 bl )+ \u2212K\u2217bl). The gradient of the objective function in (8), \u2207g(\u03b3ijk), is given by: \u2207g(\u03b3ijk) = \u22121 + \u2211 bl 1 2\u03b11 \u3008(Kbl)+ \u2212Kbl ,WiblCijk\u3009. At each iteration, \u03b3 is updated by:\n\u03b3i+1 = BoxProj(\u03b3i \u2212 \u03b7\u2207g(\u03b3i)) (9)\nwhere \u03b7 > 0 is the step size. The BoxProj(\u00b7) denotes the simple box projection operator on \u03b3 as specified in the constraints of (8). At each iteration, the main computational complexity lies in the computation of the eigen-decomposition with a complexity of O(md3) and the computation of the gradient with a complexity of O(m(nd2 + cd)), where m is the number of basis metrics and c is the number of large margin triplet constraints. As in the weight learning problem the FISTA algorithm is employed to accelerate the optimization process; for lack of space we omit the algorithm presentation."}, {"heading": "4 Experiments", "text": "In this section we will evaluate the performance of PLML and compare it with a number of relevant baseline methods on six datasets with large number of instances, ranging from 5K to 70K instances; these datasets are Letter, USPS, Pendigits, Optdigits, Isolet and MNIST. We want to determine whether the addition of manifold regularization on the local metrics improves the predictive performance of local metric learning, and whether the local metric learning improves over learning with single global metric. We will compare PLML against six baseline methods. The first, SML, is a variant of PLML where a single global metric is learned, i.e. we set the number of basis in (6) to one. The second, Cluster-Based LML (CBLML), is also a variant of PLML without weight learning. Here we learn one local metric for each cluster and we assign a weight of one for a basis metric Mbi if the corresponding cluster of Mbi contains the instance, and zero otherwise. Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2. The former two learn a single global metric and the latter two a number of local metrics. In addition to the different metric learning methods, we also compare PLML against multi-class SVMs in which we use the one-against-all strategy to determine the class label for multi-class problems and select the best kernel with inner cross validation.\nSince metric learning is computationally expensive for datasets with large number of features we followed [15] and reduced the dimensionality of the USPS, Isolet and MINIST datasets by applying PCA. In these datasets the retained PCA components explain 95% of their total variances. We preprocessed all datasets by first standardizing the input features, and then normalizing the instances to so that their L2-norm is one.\nPLML has a number of hyper-parameters. To reduce the computational time we do not tune \u03bb1 and \u03bb2 of the weight learning optimization problem (4), and we set them to their default values of \u03bb1 = 1 and \u03bb2 = 100. The Laplacian matrix L is constructed using the six nearest neighbors graph following [19]. The anchor points U are the means of clusters constructed with k-means clustering. The number m of anchor points, i.e. the number of basis metrics, depends on the complexity of the learning problem. More complex problems will often require a larger number of anchor points to better model the complexity of the data. As the number of classes in the examined datasets is 10 or 26, we simply set m = 20 for all datasets. In the basis metric learning problem (6), the number of the dual parameters \u03b3 is the same as the number of triplet constraints. To speedup the learning process, the triplet constraints are constructed only using the three same-class and the three different-class nearest neighbors for each learning instance. The parameter \u03b12 is set to 1, while the parameter \u03b11 is the only parameter that we select from the set {0.01, 0.1, 1, 10, 100} using 2-fold inner cross-validation. The above setting of basis metric learning for PLML is also used with the SML and CBLML methods. For LMNN and LMNN-MM we use their default settings, [15], in which the triplet constraints are constructed by the three nearest same-class neighbors and all different-class samples. As a result, the number of triplet constraints optimized in LMNN and LMNN-MM is much larger than those of PLML, SML, BoostMetric and CBLML. The local metrics are initialized by identity matrices. As in [11], GLML uses the Gaussian distribution to model the learning instances from the same class. Finally, we use the 1-NN rule to evaluate the performance\n1http://code.google.com/p/boosting 2http://www.cse.wustl.edu/\u223ckilian/code/code.html.\nof the different metric learning methods. In addition as we already mentioned we also compare against multi-class SVM. Since the performance of the latter depends heavily on the kernel with which it is coupled we do automatic kernel selection with inner cross validation to select the best kernel and parameter setting. The kernels were chosen from the set of linear, polynomial (degree 2,3 and 4), and Gaussian kernels; the width of the Gaussian kernel was set to the average of all pairwise distances. Its C parameter of the hinge loss term was selected from {0.1, 1, 10, 100}. To estimate the classification accuracy for Pendigits, Optdigits, Isolet and MNIST we used the default train and test split, for the other datasets we used 10-fold cross-validation. The statistical significance of the differences were tested with McNemar\u2019s test with a p-value of 0.05. In order to get a better understanding of the relative performance of the different algorithms for a given dataset we used a simple ranking schema in which an algorithm A was assigned one point if it was found to have a statistically significantly better accuracy than another algorithm B, 0.5 points if the two algorithms did not have a significant difference, and zero points if A was found to be significantly worse than B."}, {"heading": "4.1 Results", "text": "In Table 1 we report the experimental results. PLML consistently outperforms the single global metric learning methods LMNN, BoostMetric and SML, for all datasets except Isolet on which its accuracy is slightly lower than that of LMNN. Depending on the single global metric learning method with which we compare it, it is significantly better in three, four, and five datasets ( for LMNN, SML, and BoostMetric respectively), out of the six and never singificantly worse. When we compare PLML with CBLML and LMNN-MM, the two baseline methods which learn one local metric for each cluster and each class respectively with no smoothness constraints, we see that it is statistically significantly better in all the datasets. GLML fails to learn appropriate metrics on all datasets because its fundamental generative model assumption is often not valid. Finally, we see that PLML is significantly better than SVM in two out of the six datasets and it is never significantly worse; remember here that with SVM we also do inner fold kernel selection to automatically select the appropriate feature speace. Overall PLML is the best performing methods scoring 37 points over the different datasets, followed by SVM with automatic kernel selection and SML which score 32.5 and 28.5 points respectively. The other metric learning methods perform rather poorly.\nExamining more closely the performance of the baseline local metric learning methods CBLML and LMNN-MM we observe that they tend to overfit the learning problems. This can be seen by their considerably worse performance with respect to that of SML and LMNN which rely on a single\nglobal model. On the other hand PLML even though it also learns local metrics it does not suffer from the overfitting problem due to the manifold regularization. The poor performance of LMNNMM is not in agreement with the results reported in [15]. The main reason for the difference is the experimental setting. In [15], 30% of the training instance of each dataset were used as a validation set to avoid overfitting.\nTo provide a better understanding of the behavior of the learned metrics, we applied PLML LMNNMM, CBLML and GLML, on an image dataset containing instances of four different handwritten digits, zero, one, two, and four, from the MNIST dataset. As in [15], we use the two main principal components to learn. Figure 1 shows the learned local metrics by plotting the axis of their corresponding ellipses(black line). The direction of the longer axis is the more discriminative. Clearly PLML fits the data much better than LMNN-MM and as expected its local metrics vary smoothly. In terms of the predictive performance, PLML has the best with 82.76% accuracy. The CBLML, LMNN-MM and GLML have an almost identical performance with respective accuracies of 82.59%, 82.56% and 82.51%.\nFinally we investigated the sensitivity of PLML and CBLML to the number of basis metrics, we experimented with m \u2208 {5, 10, 15, 20, 25, 30, 35, 40}. The results are given in Figure 2. We see that the predictive performance of PLML often improves as we increase the number of the basis metrics. Its performance saturates when the number of basis metrics becomes sufficient to model the underlying training data. As expected different learning problems require different number of basis metrics. PLML does not overfit on any of the datasets. In contrast, the performance of CBLML gets worse when the number of basis metrics is large which provides further evidence that CBLML does indeed overfit the learning problems, demonstrating clearly the utility of the manifold regularization."}, {"heading": "5 Conclusions", "text": "Local metric learning provides a more flexible way to learn the distance function. However they are prone to overfitting since the number of parameters they learn can be very large. In this paper we presented PLML, a local metric learning method which regularizes local metrics to vary smoothly over the data manifold. Using an approximation error bound of the metric matrix function, we parametrize the local metrics by a weighted linear combinations of local metrics of anchor points. Our method scales to learning problems with tens of thousands of instances and avoids the overfitting problems that plague the other local metric learning methods. The experimental results show that PLML outperforms significantly the state of the art metric learning methods and it has a performance which is significantly better or equivalent to that of SVM with automatic kernel selection."}, {"heading": "Acknowledgments", "text": "This work was funded by the Swiss NSF (Grant 200021-137949). The support of EU projects DebugIT (FP7-217139) and e-LICO (FP7-231519), as well as that of COST Action BM072 (\u2019Urine and Kidney Proteomics\u2019) is also gratefully acknowledged"}], "references": [{"title": "Gradient-based algorithms with applications to signal-recovery problems", "author": ["A. Beck", "M. Teboulle"], "venue": "Convex Optimization in Signal Processing and Communications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Integrating constraints and metric learning in semisupervised clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "In ICML, page", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "A metric learning perspective of svm: on the relation of svm and lmnn", "author": ["H. Do", "A. Kalousis", "J. Wang", "A. Woznica"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Efficient projections onto the l 1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Image retrieval and classification using local distance functions", "author": ["A. Frome", "Y. Singer", "J. Malik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Discriminant adaptive nearest neighbor classification", "author": ["T. Hastie", "R. Tibshirani"], "venue": "IEEE Trans. on PAMI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Metric and kernel learning using a linear transformation", "author": ["P. Jain", "B. Kulis", "J.V. Davis", "I.S. Dhillon"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Generative local metric learning for nearest neighbor classification", "author": ["Y.K. Noh", "B.T. Zhang", "D.D. Lee"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "A scalable dual approach to semidefinite metric learning", "author": ["C. Shen", "J. Kim", "L. Wang"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Positive semidefinite metric learning using boostinglike", "author": ["C. Shen", "J. Kim", "L. Wang", "A. Hengel"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Metric learning with multiple kernels", "author": ["J. Wang", "H. Do", "A. Woznica", "A. Kalousis"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Locally smooth metric learning with application to image retrieval", "author": ["D.Y. Yeung", "H. Chang"], "venue": "In ICCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Sparse metric learning via smooth optimization", "author": ["Y. Ying", "K. Huang", "C. Campbell"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Nonlinear learning using local coordinate coding", "author": ["K. Yu", "T. Zhang", "Y. Gong"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Self-tuning spectral clustering", "author": ["L. Zelnik-Manor", "P. Perona"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}], "referenceMentions": [{"referenceID": 2, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 13, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 7, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 8, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 15, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 12, "context": "Mahalanobis metric learning [4, 15, 9, 10, 17, 14] improves the performance of the NN classifier if used instead of the Euclidean metric.", "startOffset": 28, "endOffset": 50}, {"referenceID": 6, "context": "Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that.", "startOffset": 96, "endOffset": 109}, {"referenceID": 1, "context": "Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that.", "startOffset": 96, "endOffset": 109}, {"referenceID": 13, "context": "Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that.", "startOffset": 96, "endOffset": 109}, {"referenceID": 5, "context": "Thus a more appropriate way is to learn a metric on each neighborhood and local metric learning [8, 3, 15, 7] does exactly that.", "startOffset": 96, "endOffset": 109}, {"referenceID": 6, "context": "One of the first local metric learning works, Discriminant Adaptive Nearest Neighbor classification [8], DANN, learns local metrics by shrinking neighborhoods in directions orthogonal to the local decision boundaries and enlarging the neighborhoods parallel to the boundaries.", "startOffset": 100, "endOffset": 103}, {"referenceID": 13, "context": "The authors of LMNN-Multiple Metric (LMNN-MM) [15] significantly limited the number of learned metrics and constrained all instances in a given region to share the same metric in an effort to combat overfitting.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "In the supervised setting they fixed the number of metrics to the number of classes; a similar idea has been also considered in [3].", "startOffset": 128, "endOffset": 131}, {"referenceID": 14, "context": "The authors of [16] learn local metrics using a least-squares approach by minimizing a weighted sum of the distances of each instance to apriori defined target positions and constraining the instances in the projected space to preserve the original geometric structure of the data in an effort to alleviate overfitting.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": "In another effort to overcome the overfitting problem of the discriminative methods [8, 15], Generative Local Metric Learning, GLML, [11], propose to learn local metrics by minimizing the NN expected classification error under strong model assumptions.", "startOffset": 84, "endOffset": 91}, {"referenceID": 13, "context": "In another effort to overcome the overfitting problem of the discriminative methods [8, 15], Generative Local Metric Learning, GLML, [11], propose to learn local metrics by minimizing the NN expected classification error under strong model assumptions.", "startOffset": 84, "endOffset": 91}, {"referenceID": 9, "context": "In another effort to overcome the overfitting problem of the discriminative methods [8, 15], Generative Local Metric Learning, GLML, [11], propose to learn local metrics by minimizing the NN expected classification error under strong model assumptions.", "startOffset": 133, "endOffset": 137}, {"referenceID": 0, "context": "To improve scalability and efficiency we employ a fast first-order optimization algorithm, FISTA [2], to learn the linear combinations as well as the basis metrics of the anchor points.", "startOffset": 97, "endOffset": 100}, {"referenceID": 6, "context": "In most cases we learn a local metric for each learning instance [8, 11], however we can also learn a local metric for some part of the instance space in which case the number of learned metrics can be considerably smaller than n, e.", "startOffset": 65, "endOffset": 72}, {"referenceID": 9, "context": "In most cases we learn a local metric for each learning instance [8, 11], however we can also learn a local metric for some part of the instance space in which case the number of learned metrics can be considerably smaller than n, e.", "startOffset": 65, "endOffset": 72}, {"referenceID": 13, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Nevertheless, in accordance to the standard practice we will continue to use the term local metric learning following [15, 11].", "startOffset": 118, "endOffset": 126}, {"referenceID": 9, "context": "Nevertheless, in accordance to the standard practice we will continue to use the term local metric learning following [15, 11].", "startOffset": 118, "endOffset": 126}, {"referenceID": 16, "context": "[18] have shown that any Lipschitz smooth real function f(x) defined on a lower dimensional manifold can be approximated by a linear combination of function values f(u),u \u2208 U, of a set U of anchor points.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "1 in [18]; for lack of space we omit its presentation.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "Most often the similarity matrix S is constructed using k-nearest neighbors graph [19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "Such a projection operator can be efficiently implemented with a complexity of O(nm log(m)) [6].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "To speed up the optimization procedure we employ a fast first-order optimization method FISTA, [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 13, "context": "Note that, this relative comparison definition is different from that defined in LMNN-MM [15].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "First we exploit the connection between LMNN and SVM shown in [5] under which the squared Frobenius norm of the metric matrix is related to the SVM margin.", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "Second because adding this term leads to an easy-to-optimize dual formulation of (6) [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "Unlike many special solvers which optimize the primal form of the metric learning problem [15, 13], we follow [12] and optimize the Lagrangian dual problem of (6).", "startOffset": 90, "endOffset": 98}, {"referenceID": 11, "context": "Unlike many special solvers which optimize the primal form of the metric learning problem [15, 13], we follow [12] and optimize the Lagrangian dual problem of (6).", "startOffset": 90, "endOffset": 98}, {"referenceID": 10, "context": "Unlike many special solvers which optimize the primal form of the metric learning problem [15, 13], we follow [12] and optimize the Lagrangian dual problem of (6).", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2.", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2.", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "Finally, we also compare against four state of the art metric learning methods LMNN [15], BoostMetric [13]1, GLML [11] and LMNN-MM [15]2.", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Since metric learning is computationally expensive for datasets with large number of features we followed [15] and reduced the dimensionality of the USPS, Isolet and MINIST datasets by applying PCA.", "startOffset": 106, "endOffset": 110}, {"referenceID": 17, "context": "The Laplacian matrix L is constructed using the six nearest neighbors graph following [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "For LMNN and LMNN-MM we use their default settings, [15], in which the triplet constraints are constructed by the three nearest same-class neighbors and all different-class samples.", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "As in [11], GLML uses the Gaussian distribution to model the learning instances from the same class.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "The poor performance of LMNNMM is not in agreement with the results reported in [15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "In [15], 30% of the training instance of each dataset were used as a validation set to avoid overfitting.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "As in [15], we use the two main principal components to learn.", "startOffset": 6, "endOffset": 10}], "year": 2012, "abstractText": "We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this \u201dindependence\u201d approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several largescale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner.", "creator": "LaTeX with hyperref package"}}}