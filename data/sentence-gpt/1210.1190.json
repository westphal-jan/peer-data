{"id": "1210.1190", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2012", "title": "Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization", "abstract": "The separability assumption (Donoho &amp; Stodden, 2003; Arora et al., 2012) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several other favorable properties in relation to existing methods. A parallel implementation of our algorithm demonstrates high scalability on shared- and distributed-memory machines.\n\n\n\n\nThe work of Kranz et al. demonstrates the importance of generality in computational computing. The two work are complementary in understanding the potential of a machine-defined problem. This is reflected in the way we apply the general notion of generality to complex data types: the notion of generality. The computational paradigm, according to Kranz and the current model, is in place, in this context. A computer program that creates and performs computations based on the computational paradigm, in this context, is able to produce an infinite list of sub-categories that are not constrained by the computational paradigm. In this case, Kranz et al. show that a machine-defined problem cannot be explained by the use of the mathematical model or a method that only has one sub-categories, and is able to produce one sub-categories that are not constrained by the computational paradigm.\nThe computational paradigm has four sub-categories:\n1) the universal definition of super-categories: the finite-dimensional definition of non-super-categories (a) the finite-dimensional definition of non-super-categories (b) the finite-dimensional definition of non-super-categories (c) the finite-dimensional definition of non-super-categories (d) the finite-dimensional definition of non-super-categories (e) the finite-dimensional definition of non-super-categories (e) the finite-dimensional definition of non-super-categories (f) the finite-dimensional definition of non-super-categories (g) the finite-dimensional definition of non-super-categories (g) the finite-dimensional definition of non-super-categories (g) the finite-dimensional definition of non-super", "histories": [["v1", "Wed, 3 Oct 2012 18:37:47 GMT  (142kb)", "http://arxiv.org/abs/1210.1190v1", "15 pages, 6 figures"]], "COMMENTS": "15 pages, 6 figures", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["abhishek kumar 0001", "vikas sindhwani", "prabhanjan kambadur"], "accepted": true, "id": "1210.1190"}, "pdf": {"name": "1210.1190.pdf", "metadata": {"source": "CRF", "title": "Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization", "authors": ["Abhishek Kumar", "Vikas Sindhwani", "Prabhanjan Kambadur"], "emails": ["abhishek@cs.umd.edu", "vsindhw@us.ibm.com", "pkambadu@us.ibm.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n21 0.\n11 90\nv1 [\nst at\n.M L\n] 3\nThe separability assumption (Donoho & Stodden, 2003; Arora et al., 2012) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several other favorable properties in relation to existing methods. A parallel implementation of our algorithm demonstrates high scalability on shared- and distributed-memory machines."}, {"heading": "1 Introduction", "text": "A data matrix X of size m \u00d7 n is said to admit a Non-negative Matrix Factorization (NMF) with inner-dimension r, if X can be expressed as X = WH where W,H are two non-negative matrices of dimensions m\u00d7 r and r \u00d7 n respectively. In many applications, a compact (i.e., small r) approximate NMF tends to provide a natural and interpretable part-based decomposition of the data (Lee & Seung, 1999), often more appealing than other low-rank factorizations. NMFs arise pervasively in a variety of signal separation problems, such as modeling topics in text and hyperspectral image analysis (Cichocki et al., 2009).\nFigure 1 shows the geometry of the NMF problem. As a point cloud in Rm, all the n columns of X are contained inside a cone that is generated by r non-negative vectors in Rm comprising the columns of W. For any matrix A, let cone(A) denote the set obtained by taking linear combinations of the columns of A with non-negative coefficients. Then, the goal is to find a nonnegative matrix W, with just r columns, such that: cone(X) \u2286 cone(W) \u2286 Rm+ , where R m + denotes the non-negative orthant in Rm. Such polyhedral nesting problems studied in computational geometry are known to be NP-hard, which makes the exact and approximate NMF problem also NP-hard (Vavasis, 2009). Faced with such results, almost the entire algorithmic focus in the NMF literature, e.g., (Cichocki et al., 2009; Lee & Seung, 1999; Lin, 2007; Hsieh & Dhillon, 2011), has centered on treating the problem as an instance of general non-convex programming, leading to\n\u2217\nThis work was done when AK was a summer intern at IBM Research.\nheuristic procedures that lack optimality guarantees beyond convergence to a stationary point of the objective function for approximate NMF. Very recently, in a series of elegant papers (Arora et al., 2012; Bittorf et al., 2012; Gillis & Vavasis, 2012; Esser et al., 2012; Elhamifar et al., 2012), promising alternative approaches have been developed based on certain separability assumption on the data which enables the NMF problem to be solved exactly. Geometrically, the assumption states the following:\nDefinition 1.1. Separability Assumption: The entire dataset, i.e. all columns of X, reside in a cone generated by a small subset of r columns of X.\nIn algebraic terms, X = WH = XAH so that the r columns of W are hidden among the columns of X (indexed by an unknown subset of indices A). Equivalently, a corresponding subset of r columns of H happen to constitute the r \u00d7 r identity matrix. We refer to these columns as anchors (Arora et al., 2012). Informally, in the context of topic modeling problems where X is a document-word matrix and W,H are document-topic and topic-term associations respectively, the separability assumption equivalently posits the existence of special anchor words in the vocabulary, whose occurence uniquely identifies the presence of a topic, and whose usage across the corpus is collectively predictive of the usage of all the other words. The separability assumption was investigated earlier by Donoho & Stodden (2003) who showed that it implied uniqueness of the NMF solution, modulo permutation and scaling. In order to place our contributions in the right context, we first briefly provide a flavor of recently proposed separable NMF algorithms.\nRelated Work: Assuming that the columns of X are normalized to have unit l1-norm, the separable NMF problem reduces to that of finding the extreme points (that is, points inexpressible as convex combinations of other points) of the convex hull of the columns (Arora et al., 2012). A Linear Program (LP) can be setup to attempt to express a given column as a convex combination of the other columns. If this LP declares infeasibility, an extreme point is identified. This approach (Arora et al., 2012, Section 5) requires solving n feasibility LP\u2019s each involving n\u22121 variables which is not scalable for many problems of interest. A noise-robust version of the procedure further requires knowledge of parameters that are hard to estimate apriori. Bittorf et al. (2012) formulate a single LP whose solution resolves the exactly separable NMF problem. An extension is also developed for noise-robustness. Instead of invoking a general LP solver, a specialized algorithm is derived based on an incremental stochastic gradient descent procedure, and its parallel (multithreaded) implementation is benchmarked on large datasets. On the other hand, this algorithm requires estimates of primal and dual step sizes, converges only asymptotically, and does not explicitly exploit the sparsity of the final solution. Gillis & Vavasis (2012) develop a highly scalable approach closely related to rank-revealing QR factorizations for column subset selection.\nA perturbation analysis of this algorithm under noise is also presented. In Esser et al. (2012), column subset selection is cast essentially as a form of multivariate regression with row-sparsity inducing norms, e.g., see Bien et al. (2010). Algorithms derived in this framework are asymptotically convergent, and sensitive to near-duplicate columns, making it necessary to perform certain adhoc preprocessing steps.\nContributions: We present a new family of highly scalable and empirically noise-robust algorithms for separable NMFs, with several favorable properties: \u25e6 The algorithms produce a correct solution for the separable case after exactly r iterations. They\nrequire no additional parameters. They are closely related to convex and conical hull finding procedures proposed in the computational geometry literature (Clarkson, 1994; Dula et al., 1998). Computationally, the algorithms bear some resemblence to simultaneous Orthogonal Matching Pursuit (Buhlmann & Geer, 2010; Tropp et al., 2006) for sparse greedy reconstruction of multiple target variables from the same subset of input variables. We also derive a variant based on this connection that performs quite well under noise. \u25e6 Under controlled noise conditions in synthetic datasets and on real-world topic modeling problems, our algorithms consistently outperform other separable NMF techniques with respect to multiple performance metrics. Our methods are highly competitive with existing non-convex NMF algorithms, but are free of sub-optimal local minima and associated initialization issues. \u25e6 The solution for (r \u2212 1) target anchors is contained in the solution for r target anchors (unlike non-convex NMF methods), which makes it easier to do model selection on real-world datasets by keeping track of performance on a validation set. \u25e6 The algorithms are highly scalable and have small memory footprint. The sparsity of the data, the intermediate variables and the final solution is carefully exploited in a high-performance parallel and distributed implementation which scales excellently on both shared- and distributed-memory machines. For example, a twitter corpus with 125-thousand tweets can be factorized for r = 100 in less than 10 seconds on a commodity 8-core machine. \u25e6 Unlike all existing algorithms, no column normalization is needed. Such normalization interferes with the TFIDF weightings routinely used in text modeling applications, leading to performance loss. \u25e6 Unlike Esser et al. (2012), the algorithms do not require any special preprocessing to eliminate duplicate or near-duplicate columns."}, {"heading": "2 Fast Conical Hull Algorithms", "text": "An informal description: Figure 2 provides some geometric intuition underlying the proposed approach. The algorithm executes r iterations. In each iteration a new anchor column is identified. This corresponds to expanding a cone one extreme ray at a time, until the entire dataset is eventually contained in the cone defined by the full set of anchors. Figure 2 illustrates one step of the algorithm where there is an existing cone defined by three extreme rays (marked 1 to 3). To identify the next extreme ray, the algorithm picks a point outside the current cone (a green point) and projects it to the current cone to compute a residual vector (we call this the projection step). This residual vector separates the current cone from at least one non-selected extreme ray that can be found by maximizing a specific selection criteria (we call this the detection step). Intuitively, the algorithm picks a face of the current cone (spanned by rays 1 and 3 in Figure 2) that \u201csees\u201d exterior points\nand rotates this face towards the exterior until it hits the \u201clast\u201d point. In the example shown in Figure 2, ray 4 is identified as a new extreme ray.\nThese geometric intuitions are inspired by Clarkson (1994); Dula et al. (1998) who present LPbased algorithms for general convex and conical hull problems. Their algorithms are also directly applicable in our NMF setting, provided the data satisfies the separability assumption exactly. In this case, the residual of any single exterior point can be used to correctly expand the cone as described above. However, anchor detection criteria derived from multiple residuals demonstrates radically superior noise robustness, as we report in the experimental section. The emphasis on scalability and noise-robustness thus leads us to a new family of algorithms whose implementation (and associated proof of correctness) is distinct from prior work.\nCones, Extreme Rays and Projection: Here, we provide a short background on relevant geometric concepts and set some notation. Recall that a cone C is a non-empty convex set that is closed with respect to taking conic combinations (i.e., linear combinations with non-negative coefficients) of its elements. A ray in C generated by a vector x 6= 0 \u2208 C is the set of all vectors {tx : t \u2265 0}. A ray R is an extreme ray if its generators cannot be expressed by taking conic combinations of elements in C that do not themselves belong to R. A cone is called finitely generated if its elements are conic combinations of a finite set of vectors, and pointed if it does not contain both a vector x as well as its negation \u2212x. A fundamental result (e.g., see Nemirovski (2010)) states: a pointed, finitely generated cone C possesses a finite and unique set of extreme rays, and C is the conical hull of the generators of these extreme rays. Furthermore, the generators of these extreme rays are a subset of the finite set of vectors used to originally express the cone. In the NMF context, note that any cone contained in Rm+ is pointed. This implies that cone(X) can also be described by a minimally compact set of generators, i.e., cone(X) = cone(XA) where A uniquely indexes the extreme rays (anchors). Thus, a non-negative matrix X admits a separable NMF with inner-dimension r if the number of extreme rays of cone(X), i.e. size of A, coincides with r. A face of a cone is the intersection between the cone and a supporting hyperplane. The projection of a point x onto the cone generated by columns of a matrix W, i.e. computing z\u22c6 = argminz\u2208cone(W) \u2016x\u2212 z\u2016 2 2, can be obtained by solving a non-negative least squares problem, i.e., computing h\u22c6 = argminh\u22650 \u2016x \u2212 Wh\u2016 2 2 and setting z\n\u22c6 = Wh\u22c6. All columns of X can be simultaneously projected by solving H\u22c6 = argminH\u22650 \u2016X \u2212 WH\u2016 2 2. We will use the notation R to denote the residual matrix after a projection operation, i.e., R = X \u2212WH\u22c6. We will use the\nAlgorithm 1 Xray : Algorithms for Separable NMF\nInput: X \u2208 Rm\u00d7n+ separable with inner dimension r Output: W \u2208 Rm\u00d7r,H \u2208 Rr\u00d7n, r indices in A such that: X = WH, W = XA Initialize: R \u2190 X, A \u2190 {} while |A| < r do\n1. Detection Step: Find an extreme ray. Below, p is a strictly positive vector (not collinear with Ri):\nj\u22c6 = argmax j\nRTi Xj pTXj for any i : \u2016Ri\u20162 > 0 (1)\nSome exterior point selection criteria:\nrand : any random i : \u2016Ri\u20162 > 0 (2) max : i = argmaxk \u2016Rk\u20162 (3)\ndist : i = argmaxk \u2016 ( RTkX ) + \u20162 (4)\nA greedy variant: j\u2217 = argmaxj \u2016(RTXj)+\u2016 2 2\n\u2016Xj\u201622 (5)\n2. Update: A \u2190 A \u222a {j\u2217} (see Remarks) 3. Projection Step: Project onto current cone.\nH = argmin B\u22650\n\u2016X\u2212XAB\u2016 2 2 (Algorithm 2) (6)\n4. Update Residuals (not explicitly): R = X\u2212XAH\nend while\nnotation Xi,Ri to denote the i th column of X and its corresponding residual. The notation q+ will denote the vector obtained by setting all negative entries of the vector q to 0."}, {"heading": "2.1 Algorithm Description, Correctness Result and Variations", "text": "Algorithm 1 details the steps of the proposed family of algorithms which we call Xray . Each iteration consists of two steps: (i) a detection step that finds a column(s) of X to be added as an anchor, and (ii) a projection step where all data points are projected onto the current cone to get the residuals. Projection is done by solving simultaneous nonnegative least squares problem using Algorithm 2. Every residual vector Ri obtained after the projection step is normal to one of the faces of the current cone. In the selection step, we pick a face of the current cone (identified by its normal Ri), normalize all the data points to lie on the hyperplane p Tx = 1 ( Yj = Xj\npTXj\n)\nfor a\nstrictly positive vector p, and expand the current cone by selecting an extreme ray that maximizes the inner product RTi Yj . The selection step can be implemented in various ways - some options are listed in Algorithm 1.\nTo show that Xray correctly identifies all the extreme rays, we need the following lemmas.\nLemma 2.1. The residual matrix R, obtained after projection of columns of X onto the current cone satisfies RTXA \u2264 0, where XA are the extreme rays of the current cone.\nAlgorithm 2 Solver for: argminB\u22650 \u2016X\u2212XAB\u2016 2 2\nInput: X \u2208 Rm\u00d7n, Index set A with r indices, Initial value for warm-starts: Binit \u2208 R r\u00d7n Convergence paramaters tol, maxcycles Initialize: B = Binit Set: C = XTX \u2208 Rn\u00d7n S = CA,A \u2208 R r\u00d7r s = diag(S) U = BTS \u2208 Rn\u00d7r while true do for i = 1 . . . r (//cyclic coordinate descent) do\nb = \u03b4 = (BT )i u = Ui \u2212 sib b = s\u22121i (Ci \u2212 u)+ \u03b4 = b\u2212 \u03b4 U = U+ \u03b4STi // sparse rank-1 update (BT )i = b\nend for objective = \u2016X\u20162 + \u2211r\ni=1(Ui +Ci) T (BT )i\nExit if \u2206objective < tol, or #iters > maxcycles.\nend while\nProof. Residuals are given by R = X\u2212XAH, where H = argminB\u22650\u2016X\u2212XAB\u2016 2 F. Forming the Lagrangian for Eq. 6, we get L(B,\u039b) = \u2016X \u2212XAB\u2016 2 F \u2212 tr(\u039b\nTB), where the matrix \u039b contains the nonnegative Lagrange multipliers. Differentiating w.r.t. B and evaluating at the optimum B = H, we have the following from the KKT conditions: 2XTA(XAH\u2212X)\u2212\u039b = 0 \u21d2 \u22122XTAR = \u039b \u2265 0 \u21d2 R TXA \u2264 0\nLemma 2.2. For any point Xi exterior to the current cone, we have R T i Xi > 0, where Ri is the residual of Xi obtained by projecting it onto the current cone.\nProof. Let R = X\u2212XAH, where H = argminB\u22650\u2016X\u2212XAB\u2016 2 F and XA are the extreme rays of the current cone. From the KKT conditions (used in the proof of Lemma 2.1) we have 2RTXA = \u2212\u039b T , where \u039b are the Lagrange multipliers. Hence, 2RTi XA = \u2212\u039b T i (ith row of both left and right side matrices). From the complementary slackness property, we have \u039bjiHji = 0 \u2200 j, i. Hence, 2RTi XAHi = \u2212\u039b T i Hi = 0. Hence we have RTi Xi = R T i (Ri +XAHi) = \u2016Ri\u2016 2 2 +R T i XAHi = \u2016Ri\u2016 2 2 > 0 since Ri 6= 0.\nUsing the above two lemmas, we prove the following theorem regarding the correctness of Algorithm 1.\nTheorem 2.1. The data point Xj\u2217 added at each iteration in the Detection step of Algorithm 1, if the maximizer in Eqn. 1 is unique, is an extreme ray of C that has not been selected in previous iterations.\nProof. Let the index set A identify all the extreme rays of C. Under the separability assumption, we have X = XAH. Let the index set A\nt identify the extreme rays of the current cone Ct. Let Yj =\nXj pTXj and YA = XA[diag(p TXA)] \u22121 (since p is strictly positive, the inverse exists).\nHence Yj = YA [diag(pTXA)]Hj\npTXj . Let Cj = [diag(pTXA)]Hj pTXj . We also have pTYj = 1 and p TYA = 1 T .\nHence, we have 1 = pTYj = p TYACj = 1 TCj.\nUsing Lemma 2.1, Lemma 2.2 and the fact that p is strictly positive, we have max1\u2264j\u2264nR T i Yj =\nmaxj /\u2208At R T i Yj . Indeed, for all j \u2208 A t we have RtiYj \u2264 0 using Lemma 2.1 and there is at least one j = i /\u2208 At for which RtiYj > 0 using Lemma 2.2. Hence the maximum lies in the set {j : j /\u2208 A\nt}. Further, we have maxj /\u2208At R T i Yj = maxj /\u2208At R T i YACj \u2264 maxj\u2208(A\\At)R \u2032 iYj. The second inequality is the result of the fact that \u2016Cj\u20161 = 1 and Cj \u2265 0. This implies that if there is a unique maximum at a j\u2217 = argmaxj /\u2208At R T i Yj , then Xj\u2217 is generator of an extreme ray of the cone C.\nRemarks: (1) If the maximum occurs at two points j\u22171 and j \u2217 2 , both these points Xj\u22171 and Xj\u22172 generate the extreme rays of the cone C. Hence both are added to anchor set A. If the maximum occurs at more than two points, some of these are the generators of the extreme rays of C and others are conic combinations of these generators. We can identify the extreme rays of this subset of points by calling Algorithm 1 recursively and add them to anchor set A. (2) Note that the algorithm is not influenced by presence of repeated anchors. (3) In the Algorithm, the vector p simply needs to satisfy pTxi > 0, i = 1 . . . n. In our implementation, we simply used p = [1, . . . 1] \u2208 R\nm, i.e., pTxi = \u2016xi\u20161. (4) Note that unlike Gillis & Vavasis (2012), we do not need XA to be full-rank.\nExterior Point Selection: It can be noted that residual of any point exterior to the current cone (i.e., any Ri 6= 0) can be used in the selection step of Algorithm 1. This gives us multiple ways of expanding the current cone depending on which i is chosen - all of which solve the separable problem but may behave very differently in the presence of noise. Some natural options are listed in Algorithm 1: choosing a random exterior point (Eqn. 2), one with maximum residual norm (Eqn. 3) or one which defines a normal to a supporting hyperplane of the current cone which \u201csees\u201d maximum \u201cmass\u201d of points in its positive halfspace, as measured by Eqn. 4. In the experiments, we will refer to these variants as Xray (rand), Xray (max) and Xray (dist) respectively.\nA Greedy variation for noisy data: In high dimensional noisy data almost all the points may masquerade as anchors. A natural choice is to expand the current cone greedily by selecting a point that best minimizes the current residual, i.e., j\u2217 = argminj minb>0\u2016R \u2212 Xjb\nT \u20162F . This selection criterion simplifies to Eqn.5 in Algorithm 1 (referred as Xray (greedy) henceforth). One may view this approach as implementing a nonnegative variant of simultaneous orthogonal matching pursuit (Tropp et al., 2006), which is a greedy approach to the problem of sparse regression of multiple response variables on the same subset of explanatory variables, i.e., for solving minB\u22650\u2016X \u2212 XB\u2016 2 F s.t. \u2016B\u20160,1 = r where \u2016B\u20160,1 pseudo-norm counts the number of non-zero rows in B. In the context of separable NMF, both response variables and explanatory variables are the columns data matrix X. A relaxed version of this problem is solved in Esser et al. (2012) (minB\u22650\u2016X \u2212 XB\u2016 2 F + \u03bb\u2016B\u20161,\u221e). It is also possible to have \u2016B\u20161,2 penalized variant (Tropp, 2006; Bien et al., 2010) which is natural for sparse multivariate regression problems. Note that the greedy approach is not guaranteed to solve the separable NMF problem, but may perform well in the noisy settings as we observe in our experiments. Intuitively, this variant is concerned with greedily optimizing all residuals on average at every iteration, instead of making a decision based on the residual of a single, albeit well-chosen, exterior point.\nSolution Refinement and Model Selection: In practice, the separable solution (W,H) as obtained from Algorithm 1 may be further refined with a few steps of alternating optimization with respect to a divergence measure of interest (e.g., Frobenius reconstruction \u2016X\u2212WH\u20162F). Also, in real-world datasets, the value of r is typically unknown. Since our algorithms build the solution\none anchor at a time, r can be set based on a performance measure evaluated on held-out data. Alternatively, Algorithm 1 can exit if the amount of improvement from introducing a new anchor falls below a prespecified threshold."}, {"heading": "2.2 Scalability and Parallelization", "text": "Here we describe various implementation details that allow us to gracefully scale to large sparse datasets (e.g., document-term matrices). The detection step can be parallelized by scoring the candidate anchors simultaneously. Likewise, the projection step involves solving Eqn. 6, which is separable in the columns of B and hence can be optimized in parallel.\nDetection Step: We avoid materializing the dense residual matrix R in the evaluation of the anchor selection criteria. Instead, we score candidate anchors on-the-fly as we compute (but not explicitly materialize) a matrix Q = ( RTX )\n+ =\n( C\u2212 (CAH) T ) + where C = XTX denotes\na covariance matrix (word-by-word for topic modeling applications). Here, the potential sparsity, symmetry of the covariance matrix C as well as the non-negativity of H can be further exploited. For example, if Cij = 0, the corresponding entry in the product (CAH) need not be computed, since the resulting negative value is anyway reset to zero by the (\u00b7)+ thresholding operator. On a P core machine, the selection criteria may be evaluated in O(nnz(C)rP ) time where nnz(C) is the number of non-zeros in C. If C is dense, we compute Q using parallel dense BLAS-3 operations. The one time computation of C is done via a parallel aggregation of rank-one outer-product terms defined by the rows of X.\nProjection Step: Algorithm 2 gives the steps of a cyclic block coordinate descent algorithm organized around very light-weight incremental sparsity-exploiting updates for solving Eqn. 6 (derivation omitted for brevity). The algorithm can be invoked in parallel on columns of X to compute the corresponding columns of B. The previous value of B is used to warm start the optimization and typically a very small number of iterations is needed for convergence."}, {"heading": "3 Empirical Observations", "text": "Here, we report extensive comparisons on synthetic and medium-scale topic modeling problems, and benchmark our parallel implementation on large text datasets on multicore machines and distributed systems. We compare with the methods proposed in Bittorf et al. (2012) (abbrv. as Hottopixx ) and Gillis & Vavasis (2012) (abbrv. as GV), as well as traditional NMFs based on alternating optimization (Cichocki et al., 2009). The source codes for Hottopixx and GV were taken from the respective authors\u2019 websites. In comparisons with Esser et al. (2012), it was observed that it tends to select near-duplicate anchors, as also mentioned in Esser et al. (2012). This characteristic causes it to consistently perform less favorably compared to other methods unless the data is preprocessed in an adhoc fashion to remove similar columns of X; hence we do not include it in our list of baselines. We also do not compare with Arora et al. (2012) since Hottopixx reportedly performs better (Bittorf et al., 2012) and the algorithm requires parameters which are hard to guess apriori."}, {"heading": "3.1 Synthetic experiments", "text": "We perform a synthetic experiment that injects controlled amount of noise to corrupt the separable structure. Each entry of the matrix W \u2208 R200\u00d720+ is generated i.i.d. according to a uniform distribution between 0 and 1. The matrix H \u2208 R20\u00d7210+ is taken to be [I20\u00d720 H\n\u2032] where each column of H\u2032 \u2208 R20\u00d7190+ is generated according to a Dirichlet distribution whose parameters are chosen uniformly in [0, 1]. The data matrix X is set to WH+N where each entry of noise matrix N is generated i.i.d. according to a Gaussian distribution with zero mean and std. dev. \u03b4. Fig. 3 plots the fraction of correctly recovered anchors (averaged over 10 runs for each value of \u03b4) against the noise level \u03b4 ranging from 0 to 1.5. The proposed Xray (max) shows the best noise-robustness in terms of anchor recovery, followed by Xray (dist) and GV. Although Xray (greedy) does not perfectly resolve the separable NMF problem (\u03b4 = 0), it performs better than Hottopixx and is competitive with GV for near-separable case (\u03b4 > 0). As described below, on real datasets it turns out to be highly competitive. Xray (rand), although solves the separable problem (\u03b4 = 0), degrades significantly under noise, which shows that proper selection of an exterior point to expand the current cone is crucial for noise-robustness."}, {"heading": "3.2 Medium-scale Topic modeling problems", "text": "We evaluate the proposed methods on three human-labeled text datasets that are commonly used in topic modeling literature: TDT-2 (TDT2) (m = 9394, n = 19528, r = 30), BBC (Greene & Cunningham, 2006) (m = 2225, n = 9635, r = 5) and Reuters (Reuters) (m = 7285, n = 18221, r = 10). We used standard tf-idf representation with document frequency thresholding in constructing the data matrix X. As required in Hottopixx and GV, we use \u21131-normalized columns of X (referred as matrix X(\u21131) henceforth) to identify the anchor column indices A(\u21131), and use the unnormalized data X (and the corresponding anchor columns XA(\u21131)) for classification and clustering tasks. The use of X (\u21131) A in clustering and classification (for any index set A) resulted in significantly worse performance uniformly for all methods so these results are not reported. For the sake of clarity in the figures, we do not show the results for Xray (max) which performed almost similar to Xray (dist) in these experiments.\nClassification experiments: Figure 4 shows the classification accuracy results obtained with the features (columns of the document-term matrix restricted to anchor words) selected by dif-\nferent methods on the three datasets. Black dotted line is the classification accuracy with full features (all the words). We use 5% of the documents for training and the rest 95% for testing to emulate a semi-supervised learning scenario where we view various methods as inducing a topical representation based on all (unlabeled) data. We use multiclass SVM classifier as implemented in LIBLINEAR (Fan et al., 2008) and use four-fold cross validation to select the parameter C. Among separable NMF techniques, the proposed Xray (greedy) and Xray (dist) (with exception on Reuters) outperform Hottopixx and GV on all the three datasets, more so on TDT. On average, traditional NMFs with local optimization perform quite well on these datasets especially when r is small, but can show significant performance variance (shown as error bars) with respect to initialization. As the number of topics increases, the performance gap between the proposed methods and the local optimization method rapidly diminishes. In this regime our techniques are a viable alternative to local optimization methods, and have the advantage of being local-minima-free, i.e., eliminating uncertainty with respect to initialization and therefore not requiring multiple runs.\nClustering experiments: We also evaluated clustering performance by assigning a cluster label to each document based on the maximum element in the corresponding row of W. We refine the solution with a few iterations of alternating optimization. Figure 5 shows the clustering performance in terms of Normalized Mutual Information (NMI) as these iterations proceed. We also show the NMI obtained with local search method after it has converged to a local optimum (averaged NMI from ten runs with different random initializations is shown; error-bar indicates the variation around the average). Again, the proposed Xray methods are among the best performing\nmethods in terms of clustering performance and do not require multiple runs as traditional NMFs do.\nEffect of column normalization: In text processing, tf-idf features are popular due to their good empirical performance in various tasks. However, most of the previously proposed methods for the separable NMF problem (Bittorf et al., 2012; Gillis & Vavasis, 2012; Arora et al., 2012) require the columns of X (i.e, words for text data) to be \u21131 normalized, which can disturb the tf-idf structure. We conduct a small experiment to study the effect of word normalization on the prediction performance of the proposed methods. We identify anchor sets A(\u21131), A(\u21132) and A by the proposed methods using the data matrices X(\u21131) (\u21131-normalized columns), X\n(\u21132) (\u21132-normalized columns) and X, respectively and use XA(\u21131) , XA(\u21132) and XA for classification (same SVM setup as described earlier). Table 1 shows the classification accuracy for 100 topics on the three datasets. These empirical results suggest that \u21131 normalization of words on top of tf-idf features, as required by other separable NMF methods, can actually adversely affect the predictive quality of the selected anchors.\nQuality of anchor words: Qualitatively, we found that anchor words selected by the proposed Xray methods tend to be more representative of the topics compared to those selected by Hottopixx and GV. Table 2 shows top words and anchors for a few topics (Lewinsky scandal, Iraq nuclear program, National Tobacco Settlement, Indonesia riots of 1998 and Columbia space shuttle) extracted from the TDT dataset."}, {"heading": "3.3 Large-scale Experiments", "text": "We implemented a shared- and distributed-memory parallel version of Xray in C++. That is, our implementation can exploit parallelism when running on multi-core machines, or on clusters of multi-core machines. For shared-memory parallelism, we use PFunc (Kambadur et al., 2009), a lightweight and portable library that provides C and C++ APIs to express task parallelism. For distributed-memory parallelism, we use MPI1, a popular library specification for message-passing that is used extensively in high-performance computing.\nTo test the shared-memory performance and scalability of Xray , we ran experiments on daniel, a dual-socket, quad-core Intel R\u00a9 XeonTM X5570 machine with 64GB of RAM running Linux Kernel 2.6.35-24 (total 8 cores). For compilation, we used GCC v4.4.5 with: \u201c-O3 -fomit-framepointer -funroll-loops\u201d in addition to PFunc 1.02, OpenMPI 1.4.5 and untuned ATLAS BLAS. We ran large-scale experiments on three datasets: RCV1 (Lewis et al., 2004), co-occurence matrix of people and places from ClueWeb09 dataset (Lemur), and IBM Twitter (IBMT) dataset. The statistics relating to these three large datasets are presented in Table 3.We report scalability results for Xray (greedy) - other variants are computationally very similar.\nFigure 6 depicts the multi-threaded performance of our implementation on daniel while detecting 100 topics. Our implementation is able to factorize RCV1 in 409 seconds on 8 cores and achieve 4.2x speedup over 8 threads when compared to the sequential implementation. Similarly, for IBMT we achieve 4.5x speedup, while completing the factorization in 9.8 seconds on 8 cores. For the dense XTX case, we are able to factorize PPL2 in 1147 seconds with just 8 cores. We\n1http://www.mpi-forum.org/\nbelieve that further speedup improvements can be demonstrated on these problems by (a) optimizing the data layout of various sparse matrices to alleviate memory contention amongst threads, and (b) in dense problems such as PPL2, by using a version of BLAS tuned to our architecture and by reorganizing our implementation around more BLAS-3 operations that have better memory to compute ratio than BLAS-1 or 2 operations. Our implementation showed good scalability on distributed-memory machines as well (details omitted for brevity).\nTo compare our performance against the state-of-the-art Hottopixx algorithm (Bittorf et al., 2012), we ran their algorithm on daniel with the options \u201c--dual 0.01 --epochs 10 --splits 8 --hott <R> --normse 1 --primal 1e-6\u201d set in close consultation with the authors. A detailed comparison is shown in Table 3.2. A head-to-head comparison is difficult because of the different performance characteristics of Hottopixx and Xray . For example, Hottopixx \u2019s per-epoch runtime is not dependent on r, the number of topics, but it\u2019s accuracy is dependent on E, the number of epochs, while our methods execute exactly r iterations, where each iteration has a superlinear dependence on r. Nonetheless, for all three datasets, we see that Xray performs better than Hottopixx even when Hottopixx is run only for 5 epochs. In particular, for the sparse datasets IBMT and RCV1, Xray runs to completion in significantly shorter amount of time than Hottopixx ."}, {"heading": "4 Conclusions and Future Work", "text": "Our methods perform favorably in comparison to other recently proposed separable NMF algorithms and offer highly scalable local-minima-free alternatives to existing local optimization techniques. Future work includes a formal noise analysis of the proposed algorithms, investigating the streaming setting where documents or words arrive in an online fashion, and using our models for social media content analysis.\nAcknowledgments: We thank Victor Bittorf and Ben Recht for graciously providing their code, associated parameters and technical support. We thank Haim Avron, Christos Boutsidis, Ken Clarkson, Rick Lawrence and Ankur Moitra for insightful and enthusiastic discussions.\nResearch was sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) under the Social Media in Strategic Communication (SMISC) program, Agreement Number W911NF12-C-0028. The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Defense Advanced Research Projects Agency or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon."}], "references": [{"title": "CUR from a sparse optimization viewpoint", "author": ["J. Bien", "Y. Xu", "M. Mahoney"], "venue": null, "citeRegEx": "Bien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bien et al\\.", "year": 2012}, {"title": "matrices with linear programs (arxiv:1206.1270v1)", "author": ["Buhlmann", "Peter", "Geer", "Sara Van De"], "venue": null, "citeRegEx": "Buhlmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Buhlmann et al\\.", "year": 2012}, {"title": "More output-sensitive geometric algorithms", "author": ["Wiley", "K. 2009. Clarkson"], "venue": null, "citeRegEx": "Wiley and Clarkson,? \\Q1994\\E", "shortCiteRegEx": "Wiley and Clarkson", "year": 1994}, {"title": "See all by looking at a few: Sparse modeling", "author": ["Elhamifar", "Ehsan", "Sapiro", "Guillermo", "Vidal", "Rene"], "venue": "hull. INFORMS Jour. on Comp.,", "citeRegEx": "Elhamifar et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Elhamifar et al\\.", "year": 1998}, {"title": "A convex model", "author": ["Esser", "Ernie", "Mller", "Michael", "Osher", "Stanley", "Sapiro", "Guillermo", "Xin", "Jack"], "venue": null, "citeRegEx": "Esser et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Esser et al\\.", "year": 2012}, {"title": "Liblinear: A library for large", "author": ["Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "Wang", "X.-R", "Lin"], "venue": "Transactions on Image Processing,", "citeRegEx": "Fan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2012}, {"title": "Fast and robust recursive algorithms for separable nonneg", "author": ["Gillis", "Nicolas", "Vavasis", "Stephen A"], "venue": null, "citeRegEx": "Gillis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Gillis et al\\.", "year": 2008}, {"title": "Practical solutions to the problem of diagonal dominance", "author": ["Greene", "Derek", "Cunningham", "P\u00e1draig"], "venue": null, "citeRegEx": "Greene et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Greene et al\\.", "year": 2012}, {"title": "Fast coordinate descent methods with variable selection", "author": ["C.J. Hsieh", "I.S. Dhillon"], "venue": null, "citeRegEx": "Hsieh and Dhillon,? \\Q2006\\E", "shortCiteRegEx": "Hsieh and Dhillon", "year": 2006}, {"title": "RCV1: A new benchmark collection for text categorization", "author": ["D Lewis", "Y Yang", "T Rose", "F. Li"], "venue": null, "citeRegEx": "Lewis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 1999}, {"title": "Projected gradient methods for non-negative matrix factorization", "author": ["Lin", "C.-J"], "venue": "Neural Computation,", "citeRegEx": "Lin and C..J.,? \\Q2004\\E", "shortCiteRegEx": "Lin and C..J.", "year": 2004}, {"title": "Lecture Notes: Introduction to Linear Optimization", "author": ["A. Nemirovski"], "venue": "Reuters. archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection", "citeRegEx": "2007", "shortCiteRegEx": "2007", "year": 2010}, {"title": "Algorithms for simultaneous sparse approximation", "author": ["J.A. Tropp", "A.C. Gilbert", "M.J. Strauss"], "venue": null, "citeRegEx": "Tropp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tropp et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "Very recently, in a series of elegant papers (Arora et al., 2012; Bittorf et al., 2012; Gillis & Vavasis, 2012; Esser et al., 2012; Elhamifar et al., 2012), promising alternative approaches have been developed based on certain separability assumption on the data which enables the NMF problem to be solved exactly.", "startOffset": 45, "endOffset": 155}, {"referenceID": 3, "context": "In Esser et al. (2012), column subset selection is cast essentially as a form of multivariate regression with row-sparsity inducing norms, e.", "startOffset": 3, "endOffset": 23}, {"referenceID": 0, "context": ", see Bien et al. (2010). Algorithms derived in this framework are asymptotically convergent, and sensitive to near-duplicate columns, making it necessary to perform certain adhoc preprocessing steps.", "startOffset": 6, "endOffset": 25}, {"referenceID": 12, "context": "Computationally, the algorithms bear some resemblence to simultaneous Orthogonal Matching Pursuit (Buhlmann & Geer, 2010; Tropp et al., 2006) for sparse greedy reconstruction of multiple target variables from the same subset of input variables.", "startOffset": 98, "endOffset": 141}, {"referenceID": 4, "context": "\u25e6 Unlike Esser et al. (2012), the algorithms do not require any special preprocessing to eliminate duplicate or near-duplicate columns.", "startOffset": 9, "endOffset": 29}, {"referenceID": 12, "context": "One may view this approach as implementing a nonnegative variant of simultaneous orthogonal matching pursuit (Tropp et al., 2006), which is a greedy approach to the problem of sparse regression of multiple response variables on the same subset of explanatory variables, i.", "startOffset": 109, "endOffset": 129}, {"referenceID": 3, "context": "A relaxed version of this problem is solved in Esser et al. (2012) (minB\u22650\u2016X \u2212 XB\u2016 2 F + \u03bb\u2016B\u20161,\u221e).", "startOffset": 47, "endOffset": 67}, {"referenceID": 4, "context": "In comparisons with Esser et al. (2012), it was observed that it tends to select near-duplicate anchors, as also mentioned in Esser et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 4, "context": "In comparisons with Esser et al. (2012), it was observed that it tends to select near-duplicate anchors, as also mentioned in Esser et al. (2012). This characteristic causes it to consistently perform less favorably compared to other methods unless the data is preprocessed in an adhoc fashion to remove similar columns of X; hence we do not include it in our list of baselines.", "startOffset": 20, "endOffset": 146}, {"referenceID": 4, "context": "In comparisons with Esser et al. (2012), it was observed that it tends to select near-duplicate anchors, as also mentioned in Esser et al. (2012). This characteristic causes it to consistently perform less favorably compared to other methods unless the data is preprocessed in an adhoc fashion to remove similar columns of X; hence we do not include it in our list of baselines. We also do not compare with Arora et al. (2012) since Hottopixx reportedly performs better (Bittorf et al.", "startOffset": 20, "endOffset": 427}], "year": 2012, "abstractText": "The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several other favorable properties in relation to existing methods. A parallel implementation of our algorithm demonstrates high scalability on sharedand distributed-memory machines.", "creator": "gnuplot 4.6 patchlevel 0"}}}