{"id": "1210.6001", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Oct-2012", "title": "Reducing statistical time-series problems to binary classification", "abstract": "We show how binary classification methods developed to work on i.i.d. data can be used for solving statistical problems that are seemingly unrelated to classification and concern highly-dependent time series. Specifically, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. The algorithms that we construct for solving these problems are based on a new metric between time-series distributions, which can be evaluated using binary classification methods. Universal consistency of the proposed algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data. In addition to this work, we recommend that we carefully consider the validity and relevance of statistical problems as an independent measure in the evaluation of computational problems.\n\n\n\n\n\n\nThe following conclusions will be drawn from empirical studies. We propose that we use binary classification methods for solving problems in the prediction of time series. The basic principle of a mathematical problem is to solve a problem that does not exist by a fixed time series and thus can be found.\nThe key components of the problem are:\nThe first variable to be determined is that the function in question is a constant. The second variable is the function, (a) (b) (c) (d) (e) (f) (g) (h) (i) (ii) (iii) (iv) (v) (vi) (v) (vi) The second variable is the function, (b) (c) (d) (e) (f) (g) (h) (i) (ii) (iii) (v) The third variable is the function, (d) (e) (f) (g) (h) (i) (ii) (iii) The fourth variable is the function, (f) (g) (h) (i) (ii) (iii) (v) (vi) (v) The fifth variable is the function, (h) (i) (ii) (v) (vi) (v) The fifth variable is the function, (f) (g) (h) (i) (iii) The sixth variable is the function, (h) (i) (ii) (v) (vi) (v) The seventh variable is the function, (h) (i) (ii) (v) The eighth variable is the function, (h) (i) (ii) (v) The seventh variable is the", "histories": [["v1", "Mon, 22 Oct 2012 19:02:21 GMT  (28kb,D)", "http://arxiv.org/abs/1210.6001v1", null], ["v2", "Mon, 28 Jan 2013 10:25:38 GMT  (28kb,D)", "http://arxiv.org/abs/1210.6001v2", null], ["v3", "Fri, 7 Jun 2013 09:45:45 GMT  (28kb,D)", "http://arxiv.org/abs/1210.6001v3", "In proceedings of NIPS 2012, pp. 2069-2077"]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["daniil ryabko", "j\u00e9r\u00e9mie mary"], "accepted": true, "id": "1210.6001"}, "pdf": {"name": "1210.6001.pdf", "metadata": {"source": "CRF", "title": "Reducing statistical time-series problems to binary classification", "authors": ["Daniil Ryabko"], "emails": ["daniil@ryabko.net", "Jeremie.Mary@inria.fr"], "sections": [{"heading": "1 Introduction", "text": "Binary classification is one of the most well-understood problems of machine learning and statistics: a wealth of efficient classification algorithms has been developed and applied to a wide range of applications. Perhaps one of the reasons for this is that binary classification is conceptually one of the simplest statistical learning problems. It is thus natural to try and use it as a building block for solving other, more complex, newer or just different problems; in other words, one can try to obtain efficient algorithms for different learning problems by reducing them to binary classification. This approach has been applied to many different problems, starting with multi-class classification, and including regression and ranking [3, 16], to give just a few examples. However, all of these problems are formulated in terms of independent and identically distributed (i.i.d.) samples. This is also the assumption underlying the theoretical analysis of most of the classification algorithms.\nIn this work we consider learning problems that concern time-series data for which independence assumptions do not hold. The series can exhibit arbitrary long-range dependence, and different timeseries samples may be interdependent as well. Moreover, the learning problems that we consider \u2014 the three-sample problem, time-series clustering, and homogeneity testing \u2014 at first glance seem completely unrelated to classification.\nWe show how the considered problems can be reduced to binary classification methods. The results include asymptotically consistent algorithms, as well as finite-sample analysis. To establish the consistency of the suggested methods, for clustering and the three-sample problem the only assumption that we make on the data is that the distributions generating the samples are stationary ergodic; this is one of the weakest assumptions used in statistics. For homogeneity testing we have to make some mixing assumptions in order to obtain consistency results (this is indeed unavoidable [22]). Mixing conditions are also used to obtain finite-sample performance guarantees for the first two problems.\nThe proposed approach is based on a new distance between time-series distributions (that is, between probability distributions on the space of infinite sequences), which we call telescope distance. This distance can be evaluated using binary classification methods, and its finite-sample estimates are shown to be asymptotically consistent. Three main building blocks are used to construct the tele-\nar X\niv :1\n21 0.\n60 01\nv1 [\ncs .L\nG ]\n2 2\nO ct\nscope distance. The first one is a distance on finite-dimensional marginal distributions. The distance we use for this is the following: dH(P,Q) := suph\u2208H |EPh \u2212 EQh| where P,Q are distributions and H is a set of functions. This distance can be estimated using binary classification methods, and thus can be used to reduce various statistical problems to the classification problem. This distance was previously applied to such statistical problems as homogeneity testing and change-point estimation [14]. However, these applications so far have only concerned i.i.d. data, whereas we want to work with highly-dependent time series. Thus, the second building block are the recent results of [1, 2], that show that empirical estimates of dH are consistent (under certain conditions on H) for arbitrary stationary ergodic distributions. This, however, is not enough: evaluating dH for (stationary ergodic) time-series distributions means measuring the distance between their finitedimensional marginals, and not the distributions themselves. Finally, the third step to construct the distance is what we call telescoping. It consists in summing the distances for all the (infinitely many) finite-dimensional marginals with decreasing weights.\nWe show that the resulting distance (telescope distance) indeed can be consistently estimated based on sampling, for arbitrary stationary ergodic distributions. Further, we show how this fact can be used to construct consistent algorithms for the considered problems on time series. Thus we can harness binary classification methods to solve statistical learning problems concerning time series.\nTo illustrate the theoretical results in an experimental setting, we chose the problem of time-series clustering, since it is a difficult unsupervised problem which seems most different from the problem of binary classification. Experiments on both synthetic and real-world data are provided. The real-world setting concerns brain-computer interface (BCI) data, which is a notoriously challenging application, and on which the presented algorithm demonstrates competitive performance.\nA related approach to address the problems considered here, as well some related problems about stationary ergodic time series, is based on (consistent) empirical estimates of the distributional distance, see [23, 21, 13] and [8] about the distributional distance. The empirical distance is based on counting frequencies of bins of decreasing sizes and \u201ctelescoping.\u201d A similar telescoping trick is used in different problems, e.g. sequence prediction [19]. Another related approach to time-series analysis involves a different reduction, namely, that to data compression [20].\nOrganisation. Section 2 is preliminary. In Section 3 we introduce and discuss the telescope distance. Section 4 explains how this distance can be calculated using binary classification methods. Sections 5 and 6 are devoted to the three-sample problem and clustering, respectively. In Section 7, under some mixing conditions, we address the problems of homogeneity testing, clustering with unknown k, and finite-sample performance guarantees. Section 8 presents experimental evaluation."}, {"heading": "2 Notation and definitions", "text": "Let (X ,FX ) be a measurable space (the domain). Time-series (or process) distributions are probability measures on the space (XN,FN) of one-way infinite sequences (where FN is the Borel sigmaalgebra of XN). We use the abbreviation X1..k for X1, . . . , Xk. All sets and functions introduced below (in particular, the setsHk and their elements) are assumed measurable. A distribution \u03c1 is stationary if \u03c1(X1..k \u2208 A) = \u03c1(Xn+1..n+k \u2208 A) for all A \u2208 FXk , k, n \u2208 N (with FXk being the sigma-algebra of Xk). A stationary distribution is called (stationary) ergodic if limn\u2192\u221e 1n \u2211 i=1..n\u2212k+1 IXi..i+k\u2208A = \u03c1(A) \u03c1-a.s. for every A \u2208 FXk , k \u2208 N. (This definition, which is more suited for the purposes of this work, is equivalent to the usual one expressed in terms of invariant sets, see e.g. [8].)"}, {"heading": "3 A distance between time-series distributions", "text": "We start with a distance between distributions on X , and then we will extend it to distributions on X\u221e. For two probability distributions P and Q on (X ,F) and a set H of measurable functions on X , one can define the distance\ndH(P,Q) := sup h\u2208H |EPh\u2212EQh|.\nSpecial cases of this distance are Kolmogorov-Smirnov [15], Kantorovich-Rubinstein [11] and Fortet-Mourier [7] metrics; the general case has been studied since at least [26].\nWe will be interested in the cases where dH(P,Q) = 0 implies P = Q. Note that in this case dH is a metric (the rest of the properties are easy to see). For reasons that will become apparent shortly (see Remark below), we will be mainly interested in the sets H that consist of indicator functions. In this case we can identify each f \u2208 H with the set {x : f(x) = 1} \u2282 X and (by a slight abuse of notation) write dH(P,Q) := suph\u2208H |P (h) \u2212 Q(h)|. It is easy to check that in this case dH is a metric if and only if H generates F . The latter property is often easy to verify directly. First of all, it trivially holds for the case where H is the set of halfspaces in a Euclidean X . It is also easy to check that it holds if H is the set of halfspaces in the feature space of most commonly used kernels (provided the feature space is of the same or higher dimension than the input space), such as polynomial and Gaussian kernels.\nBased on dH we can construct a distance between time-series probability distributions. For two time-series distributions \u03c11, \u03c12 we take the dH between k-dimensional marginal distributions of \u03c11 and \u03c12 for each k \u2208 N, and sum them all up with decreasing weights. Definition 1 (telescope distance D). For two time series distributions \u03c11 and \u03c12 on the space (X\u221e,F\u221e) and a sequence of sets of functions H = (H1,H2, . . . ) define the telescope distance\nDH(\u03c11, \u03c12) := \u221e\u2211 k=1 wk sup h\u2208Hk |E\u03c11h(X1, . . . , Xk)\u2212E\u03c12h(Y1, . . . , Yk)|, (1)\nwhere wk, k \u2208 N is a sequence of positive summable real weights (e.g. wk = 1/k2). Lemma 1. DH is a metric if and only if dHk is a metric for every k \u2208 N. Proof. The statement follows from the fact that two process distributions are the same if and only if all their finite-dimensional marginals coincide.\nDefinition 2 (empirical telescope distance D\u0302). For a pair of samples X1..n and Y1..m define empirical telescope distance as\nD\u0302H(X1..n, Y1..m) :=\nmin{m,n}\u2211 k=1 wk sup h\u2208Hk \u2223\u2223\u2223\u2223\u2223 1n\u2212 k + 1 n\u2212k+1\u2211 i=1 h(Xi..i+k\u22121)\u2212 1 m\u2212 k + 1 m\u2212k+1\u2211 i=1 h(Yi..i+k\u22121) \u2223\u2223\u2223\u2223\u2223 . (2) All the methods presented in this work are based on the empirical telescope distance. The key fact is that it is an asymptotically consistent estimate of the telescope distance, that is, the latter can be consistently estimated based on sampling.\nTheorem 1. Let H = (H1,H2, . . . ),Hk \u2282 X k, k \u2208 N be a sequence of separable sets of indicator functions of finite VC dimension such that Hk generates FXk . Then, for every stationary ergodic time series distributions \u03c1X and \u03c1Y generating samples X1..n and Y1..m we have\nlim n,m\u2192\u221e D\u0302H(X1..n, Y1..m) = DH(\u03c1X , \u03c1Y ) (3)\nNote that D\u0302H is a biased estimate of DH, and, unlike in the i.i.d. case, the bias may depend on the distributions; however, the bias is o(n).\nRemark. The condition that the sets Hk are sets of indicator function of finite VC dimension comes from [2], where it is shown that for any stationary ergodic distribution \u03c1, under these conditions, suph\u2208Hk 1 n\u2212k+1 \u2211n\u2212k+1 i=1 h(Xi..i+k\u22121) is an asymptotically consistent estimate of E\u03c1h(X1, . . . , Xk). This fact implies that dH can be consistently estimated, from which the theorem is derived.\nProof of Theorem 1. As is established in [2], under the conditions of the theorem we have\nlim n\u2192\u221e sup h\u2208Hk\n1\nn\u2212 k + 1 n\u2212k+1\u2211 i=1 h(Xi..i+k\u22121) = sup h\u2208Hk E\u03c1Xh(X1, . . . , Xk) \u03c1X -a.s. (4)\nfor all k \u2208 N, and likewise for \u03c1Y . Fix an \u03b5 > 0. We can find a T \u2208 N such that\u2211 k>T wk \u2264 \u03b5. (5)\nNote that T depends only on \u03b5. Moreover, as follows from (4), for each k = 1..T we can find an Nk such that \u2223\u2223\u2223 sup\nh\u2208Hk\n1\nn\u2212 k + 1 n\u2212k+1\u2211 i=1 h(Xi..i+k\u22121)\u2212 sup h\u2208Hk E\u03c1Xh(X1..k) \u2223\u2223\u2223 \u2264 \u03b5/T (6)\nLet Nk := maxi=1..T Ni and define analogously M for \u03c1Y . Thus, for n \u2265 N , m \u2265M we have\nD\u0302H(X1..n, Y1..m)\n\u2264 T\u2211 k=1 wk sup h\u2208Hk \u2223\u2223\u2223\u2223\u2223 1n\u2212 k + 1 n\u2212k+1\u2211 i=1 h(Xi..i+k\u22121)\u2212 1 m\u2212 k + 1 m\u2212k+1\u2211 i=1 h(Yi..i+k\u22121) \u2223\u2223\u2223\u2223\u2223+ \u03b5 \u2264\nT\u2211 k=1 wk sup h\u2208Hk {\u2223\u2223\u2223\u2223\u2223 1n\u2212 k + 1 n\u2212k+1\u2211 i=1 h(Xi..i+k\u22121)\u2212E\u03c11h(X1..k) \u2223\u2223\u2223\u2223\u2223 + |E\u03c11h(X1..k)\u2212E\u03c12h(Y1..k)|\n+ \u2223\u2223\u2223\u2223\u2223E\u03c12h(Y1..k)\u2212 1m\u2212 k + 1 m\u2212k+1\u2211 i=1 h(Yi..i+k\u22121) \u2223\u2223\u2223\u2223\u2223 } + \u03b5\n\u2264 3\u03b5+DH(\u03c1X , \u03c1Y ),\nwhere the first inequality follows from the definition of D\u0302H (2) and from (5) and the last inequality follows from (6). Since \u03b5 was chosen arbitrary the statement follows."}, {"heading": "4 Calculating D\u0302H using binary classification methods", "text": "The methods for solving various statistical problems that we suggest are all based on D\u0302H. The main appeal of this approach is that D\u0302H can be calculated using binary classification methods. Here we explain how to do it.\nThe definition (2) of DH involves calculating l summands (where l := min{n,m}), that is\nsup h\u2208Hk \u2223\u2223\u2223\u2223\u2223 1n\u2212 k + 1 n\u2212k+1\u2211 i=1 h(Xi..i+k\u22121)\u2212 1 m\u2212 k + 1 m\u2212k+1\u2211 i=1 h(Yi..i+k\u22121) \u2223\u2223\u2223\u2223\u2223 (7) for each k = 1..l. Assuming that h \u2208 Hk are indicator functions, calculating each of the summands amounts to solving the following k-dimensional binary classification problem. Consider Xi..i+k\u22121, i = 1..n \u2212 k + 1 as class-1 examples and Yi..i+k\u22121, i = 1..m \u2212 k + 1 as class-0 examples. The supremum (7) is attained on h \u2208 Hk that minimizes the empirical risk, with examples wighted with respect to the sample size. Indeed, then we can define the weighted empirical risk of any h \u2208 Hk as\u2223\u2223\u2223\u2223\u2223 1n\u2212 k + 1 n\u2212k+1\u2211 i=1 (1\u2212 h(Xi..i+k\u22121)) + 1 m\u2212 k + 1 m\u2212k+1\u2211 i=1 h(Yi..i+k\u22121)\n\u2223\u2223\u2223\u2223\u2223 , which is obviously minimized by any h \u2208 Hk that attains (7). Thus, as long as we have a way to find h \u2208 Hk that minimizes empirical risk, we have a consistent estimate of DH(\u03c1X , \u03c1Y ), under the mild conditions on H required by Theorem 1. Since the dimension of the resulting classification problems grows with the length of the sequences, one should prefer methods that work in high dimensions, such as soft-margin SVMs [6].\nA particularly remarkable feature is that the choice of Hk is much easier for the problems that we consider than in the binary classification problem. Specifically, if (for some fixed k) the classifier that achieves the minimal (Bayes) error for the classification problem is not in Hk, then obviously the error of an empirical risk minimizer will not tend to zero, no matter how much data we have. In\ncontrast, all we need to achieve asymptotically 0 error in estimating D\u0302 (and therefore, in the learning problems considered below) is that the sets Hk asymptotically generate FXk and have a finite VC dimension (for each k). This is the case already for the set of hyperplanes in Rk! Thus, while the choice ofHk (or, say, of the kernel to use in SVM) is still important from the practical point of view, it is almost irrelevant for the theoretical consistency results. Thus, we have the following.\nClaim 1. The approximation error |DH(P,Q)\u2212 D\u0302H(X,Y )|, and thus the error of the algorithms below, can be much smaller than the error of classification algorithms used to calculate DH(X,Y ).\nFinally, we remark that while in (2) the number of summands is l, it can be replaced with any \u03b3l such that \u03b3l \u2192 \u221e, without affecting any asymptotic consistency results. A practically viable choice is \u03b3l = log l; in fact, there is no reason to choose faster growing \u03b3n since the estimates for higher-order summands will not have enough data to converge. This is also the value we use in the experiments."}, {"heading": "5 The three-sample problem", "text": "We start with a conceptually simple problem known in statistics as the three-sample problem (some times also called time-series classification). We are given three samples X = (X1, . . . , Xn), Y = (Y1, . . . , Ym) and Z = (Z1, . . . , Zl). It is known that X and Y were generated by different time-series distributions, whereas Z was generated by the same distribution as either X or Y . It is required to find out which one is the case. Both distributions are assumed to be stationary ergodic, but no further assumptions are made about them (no independence, mixing or memory assumptions). The three sample-problem for dependent time series has been addressed in [9] for Markov processes and in [23] for stationary ergodic time series. The latter work uses an approach based on the distributional distance.\nIndeed, to solve this problem it suffices to have consistent estimates of some distance between time series distributions. Thus, we can use the telescope distance. The following statement is a simple corollary of Theorem 1.\nTheorem 2. Let the samples X = (X1, . . . , Xn), Y = (Y1, . . . , Ym) and Z = (Z1, . . . , Zl) be generated by stationary ergodic distributions \u03c1X , \u03c1Y and \u03c1Z , with \u03c1X 6= \u03c1Y and either (i) \u03c1Z = \u03c1X or (ii) \u03c1Z = \u03c1Y . Assume that the sets Hk \u2282 X k, k \u2208 N are separable sets of indicator functions of finite VC dimension such thatHk generates FXk . A test that declares (i) if D\u0302H(Z,X) \u2264 D\u0302H(Z, Y ) and (ii) otherwise makes only finitely many errors with probability 1 as n,m, l\u2192\u221e.\nIt is straightforward to extend this theorem to more than two classes; in other words, instead of X and Y one can have an arbitrary number of samples from different stationary ergodic distributions."}, {"heading": "6 Clustering time series", "text": "We are given N samples X1 = (X11 , . . . , X 1 n1), . . . , X N = (XN1 , . . . , X N nN ) generated by k different stationary ergodic time-series distributions \u03c11, . . . , \u03c1k. The number k is known, but the distributions are not. It is required to group the N samples into k groups (clusters), that is, to output a partitioning of {X1..XN} into k sets. While there may be many different approaches to define what is a good clustering (and, in general, deciding what is a good clustering is a difficult problem), for the problem of classifying time-series samples there is a natural choice, proposed in [21]: those samples should be put together that were generated by the same distribution. Thus, define target clustering as the partitioning in which those and only those samples that were generated by the same distribution are placed in the same cluster. A clustering algorithm is called asymptotically consistent if with probability 1 there is an n\u2032 such that the algorithm produces the target clustering whenever maxi=1..N ni \u2265 n\u2032. Again, to solve this problem it is enough to have a metric between time-series distributions that can be consistently estimated. Our approach here is based on the telescope distance, and thus we use D\u0302.\nThe clustering problem is relatively simple if the target clustering has what is called the strict separation property [4]: every two points in the same target cluster are closer to each other than to any point from a different target cluster. The following statement is an easy corollary of Theorem 1.\nTheorem 3. Assume that the setsHk \u2282 X k, k \u2208 N are separable sets of indicator functions of finite VC dimension, such that Hk generates FXk . If the distributions \u03c11, . . . , \u03c1k generating the samples X1 = (X11 , . . . , X 1 n1), . . . , X N = (XN1 , . . . , X N nN ) are stationary ergodic, then with probability 1 from some n := maxi=1..N ni on the target clustering has the strict separation property with respect to D\u0302H.\nWith the strict separation property at hand, it is easy to find asymptotically consistent algorithms. We will give some simple examples, but the theorem below can be extended to many other distancebased clustering algorithms.\nThe average linkage algorithm works as follows. The distance between clusters is defined as the average distance between points in these clusters. First, put each point into a separate cluster. Then, merge the two closest clusters; repeat the last step until the total number of clusters is k. The farthest point clustering works as follows. Assign c1 := X1 to the first cluster. For i = 2..k, find the point Xj , j \u2208 {1..N} that maximizes the distance mint=1..i D\u0302H(Xj , ct) (to the points already assigned to clusters) and assign ci := Xj to the cluster i. Then assign each of the remaining points to the nearest cluster. The following statement is a corollary of Theorem 3. Theorem 4. Under the conditions of Theorem 3, average linkage and farthest point clusterings are asymptotically consistent.\nNote that we do not require the samples to be independent; the joint distributions of the samples may be completely arbitrary, as long as the marginal distribution of each sample is stationary ergodic. These results can be extended to the online setting in the spirit of [13]."}, {"heading": "7 Speed of convergence", "text": "The results established so far are asymptotic out of necessity: they are established under the assumption that the distributions involved are stationary ergodic, which is too general to allow for any meaningful finite-time performance guarantees. Moreover, some statistical problems, such as homogeneity testing or clustering when the number of clusters is unknown, are provably impossible to solve under this assumption [22].\nWhile it is interesting to be able to establish consistency results under such general assumptions, it is also interesting to see what results can be obtained under stronger assumptions. Moreover, since it is usually not known in advance whether the data at hand satisfies given assumptions or not, it appears important to have methods that have both asymptotic consistency in the general setting and finite-time performance guarantees under stronger assumptions.\nIn this section we will look at the speed of convergence of D\u0302 under certain mixing conditions, and use it to construct solutions for the problems of homogeneity and clustering with an unknown number of clusters, as well as to establish finite-time performance guarantees for the methods presented in the previous sections.\nA stationary distribution on the space of one-way infinite sequences (XN,FN) can be uniquely extended to a stationary distribution on the space of two-way infinite sequences (X Z,FZ) of the form . . . , X\u22121, X0, X1, . . . . Definition 3 (\u03b2-mixing coefficients). For a process distribution \u03c1 define the mixing coefficients\n\u03b2(\u03c1, k) := sup A\u2208\u03c3(X\u2212\u221e..0), B\u2208\u03c3(Xk..\u221e)\n|\u03c1(A \u2229B)\u2212 \u03c1(A)\u03c1(B)|\nwhere \u03c3(..) denotes the sigma-algebra of the random variables in brackets.\nWhen \u03b2(\u03c1, k) \u2192 0 the process \u03c1 is called absolutely regular; this condition is much stronger than ergodicity, but is much weaker than the i.i.d. assumption."}, {"heading": "7.1 Speed of convergence of D\u0302", "text": "Assume that a sample X1..n is generated by a distribution \u03c1 that is uniformly \u03b2-mixing with coefficients \u03b2(\u03c1, k) Assume further thatHk is a set of indicator functions with a finite VC dimension dk, for each k \u2208 N.\nThe general tool that we use to obtain performance guarantees in this section is the following bound that can be obtained from the results of [12].\nqn(\u03c1,Hk, \u03b5) := \u03c1 (\nsup h\u2208Hk | 1 n\u2212 k + 1 n\u2212k+1\u2211 i=1 h(Xi..i+k\u22121)\u2212E\u03c11h(X1..k)| > \u03b5 )\n\u2264 n\u03b2(\u03c1, tn \u2212 k) + 8tdk+1m e\u2212ln\u03b5 2/8, (8)\nwhere tn and ln are any integers in 1..n. The parameters tn, ln should be set according to the values of \u03b2 in order to optimize the bound.\nOne can use similar bounds for classes of finite Pollard dimension [18] or more general bounds expressed in terms of covering numbers, such as those given in [12]. Here we consider classes of finite VC dimension only for the ease of the exposition and for the sake of continuity with the previous section (where it was necessary).\nFurthermore, for the rest of this section we assume geometric \u03b2-mixing distributions, that is, \u03b2(\u03c1, t) \u2264 \u03b3t for some \u03b3 < 1. Letting ln = tn = \u221a n the bound (8) becomes\nqn(\u03c1,Hk, \u03b5) \u2264 n\u03b3 \u221a n\u2212k + 8n(dk+1)/2e\u2212 \u221a n\u03b52/8. (9)\nLemma 2. Let two samples X1..n and Y1..m be generated by stationary distributions \u03c1X and \u03c1Y whose \u03b2-mixing coefficients satisfy \u03b2(\u03c1., t) \u2264 \u03b3t for some \u03b3 < 1. Let Hk, k \u2208 N be some sets of indicator functions on X k whose VC dimension dk is finite and non-decreasing with k. Then\nP (|D\u0302H(X1..n, Y1..m)\u2212DH(\u03c1X , \u03c1Y )| > \u03b5) \u2264 2\u2206(\u03b5/4, n\u2032) (10) where n\u2032 := min{n1, n2}, the probability is with respect to \u03c1X \u00d7 \u03c1Y and\n\u2206(\u03b5, n) := \u2212 log \u03b5(n\u03b3 \u221a n+log(\u03b5) + 8n(d\u2212 log \u03b5+1)/2e\u2212 \u221a n\u03b52/8). (11)\nProof. Note that \u2211\u221e k=\u2212 log \u03b5/2 wk < \u03b5/2. Using this and the definitions (1) and (2) of DH and D\u0302H we obtain\nP (|D\u0302H(X1..n1 , Y1..n2)\u2212DH(\u03c1X , \u03c1Y )| > \u03b5) \u2264 \u2212 log(\u03b5/2)\u2211\nk=1\n(qn(\u03c1X ,Hk, \u03b5/4) + qn(\u03c1Y ,Hk, \u03b5/4)),\nwhich, together with (6)implies the statement."}, {"heading": "7.2 Homogeneity testing", "text": "Given two samples X1..n and Y1..m generated by distributions \u03c1X and \u03c1Y respectively, the problem of homogeneity testing (or the two-sample problem) consists in deciding whether \u03c1X = \u03c1Y . A test is called (asymptotically) consistent if its probability of error goes to zero as n\u2032 := min{m,n} goes to infinity. In general, for stationary ergodic time series distributions, there is no asymptotically consistent test for homogeneity [22], so stronger assumptions are in order.\nHomogeneity testing is one of the classical problems of mathematical statistics, and one of the most studied ones. Vast literature exits on homogeneity testing for i.i.d. data, and for dependent processes as well. We do not attempt to survey this literature here. Our contribution to this line of research is to show that this problem can be reduced (via the telescope distance) to binary classification, in the case of strongly dependent processes satisfying some mixing conditions.\nIt is easy to see that under the mixing conditions of Lemma 1 a consistent test for homogeneity exists, and finite-sample performance guarantees can be obtained. It is enough to find a sequence \u03b5n \u2192 0 such that \u2206(\u03b5n, n) \u2192 0 (see (11). Then the test can be constructed as follows: say that the two sequencesX1..n and Y1..m were generated by the same distribution if D\u0302H(X1..n, Y1..m) < \u03b5min{n,m}; otherwise say that they were generated by different distributions. The following statement is an immediate consequence of Lemma 2. Theorem 5. Under the conditions of Lemma 2 the probability of Type I error (the distributions are the same but the test says they are different) of the described test is upper-bounded by 4\u2206(\u03b5/8, n\u2032). The probability of Type II error (the distributions are different but the test says they are the same) is upper-bounded by 4\u2206(\u03b4 \u2212 \u03b5/8, n\u2032) where \u03b4 := 1/2DH(\u03c1X , \u03c1Y ).\nThe optimal choice of \u03b5n may depend on the speed at which dk (the VC dimension ofHk) increases; however, for most natural cases (recall that Hk are also parameters of the algorithm) this growth is polynomial so the main term to control is e\u2212 \u221a n\u03b52/8.\nFor example, if Hk is the set of halfspaces in X k = Rk then dk = k + 1 and one can chose \u03b5n := n \u22121/8. The resulting probability of Type I error decreases as exp(\u2212n1/4)."}, {"heading": "7.3 Clustering with a known or unknown number of clusters", "text": "If the distributions generating the samples satisfy certain mixing conditions, then we can augment Theorems 3 and 4 with finite-sample performance guarantees. Theorem 6. Let the distributions \u03c11, . . . , \u03c1k generating the samples X1 = (X11 , . . . , X 1 n1), . . . , X N = (XN1 , . . . , X N nN ) satisfy the conditions of Lemma 2. Define \u03b4 := mini,j=1..N,i6=j DH(\u03c1i, \u03c1j) and n := mini=1..N ni. Then with probability at least\n1\u2212N(N \u2212 1)\u2206(\u03b4/4, n)/2\nthe target clustering of the samples has the strict separation property. In this case single linkage and farthest point algorithms output the target clustering.\nProof. Note that a sufficient condition for the strict separation property to hold is that for every one out of N(N \u2212 1)/2 pairs of samples the estimate D\u0302H(Xi, Xj) i, j = 1..N is within \u03b4/4 of the DH distance between the corresponding distributions. It remains to apply Lemma 2 to obtain the first statement, and the second statement is obvious (cf. Theorem 4).\nAs with homogeneity testing, while in the general case of stationary ergodic distributions it is impossible to have a consistent clustering algorithm when the number of clusters k is unknown, the situation changes if the distributions satisfy certain mixing conditions. In this case a consistent clustering algorithm can be obtained as follows. Assign to the same cluster all samples that are at most \u03b5n-far from each other, where the threshold \u03b5n is selected the same way as for homogeneity testing: \u03b5n \u2192 0 and \u2206(\u03b5n, n) \u2192 0. The optimal choice of this parameter depends on the choice of Hk through the speed of growth of the VC dimension dk of these sets. Theorem 7. Given N samples generated by k different stationary distributions \u03c1i, i = 1..k (unknown k) all satisfying the conditions of Lemma 2, the probability of error (misclustering at least one sample) of the described algorithm is upper-bounded by\n2N(N \u2212 1) max{\u2206(\u03b5/8, n),\u2206(\u03b4 \u2212 \u03b5/8, n)}\nwhere \u03b4 := mini,j=1..k,i 6=j DH(\u03c1i, \u03c1j) and n = mini=1..N ni, with ni, i = 1..N being lengths of the samples."}, {"heading": "8 Experiments", "text": "For experimental evaluation we chose the problem of time-series clustering. Average-linkage clustering is used, with the telescope distance between samples calculated using an SVM, as described in Section 4. In all experiments, SVM is used with radial basis kernel, with default parameters of libsvm [5]."}, {"heading": "8.1 Synthetic data", "text": "For the artificial setting we have chosen highly-dependent time series distributions which have the same single-dimensional marginals and which cannot be well approximated by finite- or countablestate models. The distributions \u03c1(\u03b1), \u03b1 \u2208 (0, 1), are constructed as follows. Select r0 \u2208 [0, 1] uniformly at random; then, for each i = 1..n obtain ri by shifting ri\u22121 by \u03b1 to the right, and removing the integer part. The time series (X1, X2, . . . ) is then obtained from ri by drawing a point from a distribution lawN1 if ri < 0.5 and fromN2 otherwise. N1 is a 3-dimensional Gaussian with mean of 0 and covariance matrix Id\u00d71/4. N2 is the same but with mean 1. If \u03b1 is irrational1 then the\n1in experiments simulated by a longdouble with a long mantissa\ndistribution \u03c1(\u03b1) is stationary ergodic, but does not belong to any simpler natural distribution family [25]. The single-dimensional marginal is the same for all values of \u03b1. The latter two properties make all parametric and most non-parametric methods inapplicable to this problem.\nIn our experiments, we use two process distributions \u03c1(\u03b1i), i \u2208 {1, 2}, with \u03b11 = 0.31..., \u03b12 = 0.35...,. The dependence of error rate on the length of time series is shown on Figure 1. One clustering experiment on sequences of length 1000 takes about 5 min. on a standard laptop."}, {"heading": "8.2 Real data", "text": "To demonstrate the applicability of the proposed methods to realistic scenarios, we chose the braincomputer interface data from BCI competition III [17]. The dataset consists of (pre-processed) BCI recordings of mental imagery: a person is thinking about one of three subjects (left foot, right foot, a random letter). Originally, each time series consisted of several consecutive sequences of different classes, and the problem was supervised: three time series for training and one for testing. We split each of the original time series into classes, and then used our clustering algorithm in a completely unsupervised setting. The original problem is 96-dimensional, but we used only the first 3 dimensions (using all 96 gives worse performance). The typical sequence length is 300. The performance is reported in Table 1, labeled TSSVM. All the computation for this experiment takes approximately 6 minutes on a standard laptop.\nThe following methods were used for comparison. First, we used dynamic time wrapping (DTW) [24] which is a popular base-line approach for time-series clustering. The other two methods in Table 1 are from [10]. The comparison is not fully relevant, since the results in [10] are for different settings; the method KCpA was used in change-point estimation method (a different but also unsupervised setting), and SVM was used in a supervised setting. The latter is of particular interest since the classification method we used in the telescope distance is also SVM, but our setting is unsupervised (clustering).\nAcknowledgments. This research was funded by the Ministry of Higher Education and Research, Nord-Pasde-Calais Regional Council and FEDER (Contrat de Projets Etat Region CPER 2007-2013), ANR projects EXPLO-RA (ANR-08-COSI-004), Lampada (ANR-09-EMER-007) and CoAdapt, and by the European Community\u2019s FP7 Program under grant agreements n\u25e6 216886 (PASCAL2) and n\u25e6 270327 (CompLACS)."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We show how binary classification methods developed to work on i.i.d. data can<lb>be used for solving statistical problems that are seemingly unrelated to classifi-<lb>cation and concern highly-dependent time series. Specifically, the problems of<lb>time-series clustering, homogeneity testing and the three-sample problem are ad-<lb>dressed. The algorithms that we construct for solving these problems are based<lb>on a new metric between time-series distributions, which can be evaluated using<lb>binary classification methods. Universal consistency of the proposed algorithms<lb>is proven under most general assumptions. The theoretical results are illustrated<lb>with experiments on synthetic and real-world data.", "creator": "LaTeX with hyperref package"}}}