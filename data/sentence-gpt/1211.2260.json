{"id": "1211.2260", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Nov-2012", "title": "No-Regret Algorithms for Unconstrained Online Convex Optimization", "abstract": "Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is R^n. Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x^* are known in advance. We present algorithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of x^*. In particular, regret with respect to x^* = 0 is constant. We then prove lower bounds showing that our guarantees are near-optimal in this setting. However, a better understanding of the limitations of this subset of algorithms is warranted as the algorithm and its dependencies are not necessarily independent of the original state of the program.\n\n\n\nA new version of R and related algorithms (C, D) are being developed for distributed computation of the program. The software can therefore serve as a framework for distributed computation of the program. The resulting machine-readable output is stored and shared by other users, which is then shared by other users.\n\nProgramming R and related algorithms are also available in different formats. In our previous work, we defined an R model that can be expressed as a model using the data and computing methods discussed in this section. The model is described in the following format, as follows:\nModel R\nIn a case where the computer has some information that it can use, it can then be generalized to express that the model has the best accuracy of the prediction of the prediction. For example, if the model has the best accuracy of the prediction, it can then be generalized to express that the model has the best accuracy of the prediction. In this case, a model with the best accuracy of the prediction can also be generalized to express that the model has the best accuracy of the prediction. For example, if the model has the best accuracy of the prediction, it can then be generalized to express that the model has the best accuracy of the prediction. For example, if the model has the best accuracy of the prediction, it can then be generalized to express that the model has the best accuracy of the prediction. For example, if the model has the best accuracy of the prediction, it can then be generalized to express that the model has the best accuracy of the prediction. For example, if the model has the best accuracy of the prediction, it can then be generalized to express that the model has the best accuracy of the prediction. In this case, the model has the best accuracy of the", "histories": [["v1", "Fri, 9 Nov 2012 22:13:10 GMT  (25kb,D)", "http://arxiv.org/abs/1211.2260v1", "To appear"]], "COMMENTS": "To appear", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthew j streeter", "h brendan mcmahan"], "accepted": true, "id": "1211.2260"}, "pdf": {"name": "1211.2260.pdf", "metadata": {"source": "CRF", "title": "No-Regret Algorithms for Unconstrained Online Convex Optimization", "authors": ["Matthew Streeter", "H. Brendan McMahan"], "emails": ["matt@duolingo.com", "mcmahan@google.com"], "sections": [{"heading": "1 Introduction", "text": "Over the past several years, online convex optimization has emerged as a fundamental tool for solving problems in machine learning (see, e.g., [3, 12] for an introduction). The reduction from general online convex optimization to online linear optimization means that simple and efficient (in memory and time) algorithms can be used to tackle large-scale machine learning problems. The key theoretical techniques behind essentially all the algorithms in this field are the use of a fixed or increasing strongly convex regularizer (for gradient descent algorithms, this is equivalent to a fixed or decreasing learning rate sequence). In this paper, we show that a fundamentally different type of algorithm can offer significant advantages over these approaches. Our algorithms adjust their learning rates based not just on the number of rounds, but also based on the sum of gradients seen so far. This allows us to start with small learning rates, but effectively increase the learning rate if the problem instance warrants it.\nThis approach produces regret bounds of the formO ( R \u221a T log((1+R)T ) ) , whereR = \u2016x\u030a\u20162 is the L2 norm of an arbitrary comparator. Critically, our algorithms provide this guarantee simultaneously for all x\u030a \u2208 Rn, without any need to know R in advance. A consequence of this is that we can guarantee at most constant regret with respect to the origin, x\u030a = 0. This technique can be applied to any online convex optimization problem where a fixed feasible set is not an essential component of the problem. We discuss two applications of particular interest below:\nOnline Prediction Perhaps the single most important application of online convex optimization is the following prediction setting: the world presents an attribute vector at \u2208 Rn; the prediction algorithm produces a prediction \u03c3(at \u00b7 xt), where xt \u2208 Rn represents the model parameters, and \u03c3 : R \u2192 Y maps the linear prediction into the appropriate label space. Then, the adversary reveals the label yt \u2208 Y , and the prediction is penalized according to a loss function ` : Y \u00d7 Y \u2192 R. For appropriately chosen \u03c3 and `, this becomes a problem of online convex optimization against functions ft(x) = `(\u03c3(at \u00b7x), yt). In this formulation, there are no inherent restrictions on the model coefficients x \u2208 Rn. The practitioner may have prior knowledge that \u201csmall\u201d model vectors are more\n\u2217This work was performed while the author was at Google.\nar X\niv :1\n21 1.\n22 60\nv1 [\ncs .L\nG ]\n9 N\nov 2\nlikely than large ones, but this is rarely best encoded as a feasible set F , which says: \u201call xt \u2208 F are equally likely, and all other xt are ruled out.\u201d A more general strategy is to introduce a fixed convex regularizer: L1 and L22 penalties are common, but domain-specific choices are also possible. While algorithms of this form have proved very effective at solving these problems, theoretical guarantees usually require fixing a feasible set of radius R, or at least an intelligent guess of the norm of an optimal comparator x\u030a.\nThe Unconstrained Experts Problem and Portfolio Management In the classic problem of predicting with expert advice (e.g., [3]), there are n experts, and on each round t the player selects an expert (say i), and obtains reward gt,i from a bounded interval (say [\u22121, 1]). Typically, one uses an algorithm that proposes a probability distribution pt on experts, so the expected reward is pt \u00b7 gt. Our algorithms apply to an unconstrained version of this problem: there are still n experts with payouts in [\u22121, 1], but rather than selecting an individual expert, the player can place a \u201cbet\u201d of xt,i on each expert i, and then receives reward \u2211 i xt,igt,i = xt \u00b7 gt. The bets are unconstrained (betting a negative value corresponds to betting against the expert). In this setting, a natural goal is the following: place bets so as to achieve as much reward as possible, subject to the constraint that total losses are bounded by a constant (which can be set equal to some starting budget which is to be invested). Our algorithms can satisfy constraints of this form because regret with respect to x\u030a = 0 (which equals total loss) is bounded by a constant.\nIt is useful to contrast our results in this setting to previous applications of online convex optimization to portfolio management, for example [6] and [2]. By applying algorithms for exp-concave loss functions, they obtain log-wealth within O(log(T )) of the best constant rebalanced portfolio. However, this approach requires a \u201cno-junk-bond\u201d assumption: on each round, for each investment, you always retain at least an \u03b1 > 0 fraction of your initial investment. While this may be realistic (though not guaranteed!) for blue-chip stocks, it certainly is not for bets on derivatives that can lose all their value unless a particular event occurs (e.g., a stock price crosses some threshold). Our model allows us to handle such investments: if we play xi > 0, an outcome of gi = \u22121 corresponds exactly to losing 100% of that investment. Our results imply that if even one investment (out of exponentially many choices) has significant returns, we will increase our wealth exponentially.\nNotation and Problem Statement For the algorithms considered in this paper, it will be more natural to consider reward-maximization rather than loss-minimization. Therefore, we consider online linear optimization where the goal is to maximize cumulative reward given adversarially selected linear reward functions ft(x) = gt \u00b7 x. On each round t = 1 . . . T , the algorithm selects a point xt \u2208 Rn, receives reward ft(xt) = gt \u00b7 xt, and observes gt. For simplicity, we assume gt,i \u2208 [\u22121, 1], that is, \u2016gt\u2016\u221e \u2264 1. If the real problem is against convex loss functions `t(x), they can be converted to our framework by taking gt = \u2212O`t(xt) (see pseudo-code for REWARD-DOUBLING), using the standard reduction from online convex optimization to online linear optimization [13].\nWe use the compressed summation notation g1:t = \u2211t s=1 gs for both vectors and scalars. We study the reward of our algorithms, and their regret against a fixed comparator x\u030a:\nReward \u2261 T\u2211 t=1 gt \u00b7 xt and Regret(\u030ax) \u2261 g1:T \u00b7 x\u030a\u2212 T\u2211 t=1 gt \u00b7 xt.\nComparison of Regret Bounds The primary contribution of this paper is to establish matching upper and lower bounds for unconstrained online convex optimization problems, using algorithms that require no prior information about the comparator point x\u030a. Specifically, we present an algorithm that, for any x\u030a \u2208 Rn, guarantees Regret(\u030ax) \u2264 O ( \u2016x\u030a\u20162 \u221a T log((1 + \u2016x\u030a\u20162) \u221a T ) ) . To obtain this guarantee, we show that it is sufficient (and necessary) that reward is \u2126(exp(|g1:T |/ \u221a T )) (see Theorem 1). This shift of emphasis from regret-minimization to reward-maximization eliminates the quantification on x\u030a, and may be useful in other contexts.\nTable 1 compares the bounds for REWARD-DOUBLING (this paper) to those of two previous algorithms: online gradient descent [13] and projected exponentiated gradient descent [8, 12]. For each\nOur bounds are not directly comparable to the bounds cited above: a O(log(T )) regret bound on logwealth implies wealth at least O ( OPT/T ) , whereas we guarantee wealth like O ( OPT\u2019 \u2212 \u221a T ) . But more importantly, the comparison classes are different.\nalgorithm, we consider a fixed choice of parameter settings and then look at how regret changes as we vary the comparator point x\u030a.\nGradient descent is minimax-optimal [1] when the comparator point is contained in a hypershere whose radius is known in advance (\u2016x\u030a\u20162 \u2264 R) and gradients are sparse (\u2016gt\u20162 \u2264 1, top table). Exponentiated gradient descent excels when gradients are dense (\u2016gt\u2016\u221e \u2264 1, bottom table) but the comparator point is sparse (\u2016x\u030a\u20161 \u2264 R for R known in advance). In both these cases, the bounds for REWARD-DOUBLING match those of the previous algorithms up to logarithmic factors, even when they are tuned optimally with knowledge of R.\nThe advantage of REWARD-DOUBLING shows up when the guess of R used to tune the competing algorithms turns out to be wrong. When x\u030a = 0, REWARD-DOUBLING offers constant regret compared to \u2126( \u221a T ) for the other algorithms. When x\u030a can be arbitrary, only REWARD-DOUBLING offers sub-linear regret (and in fact its regret bound is optimal, as shown in Theorem 8).\nIn order to guarantee constant origin-regret, REWARD-DOUBLING frequently \u201cjumps\u201d back to playing the origin, which may be undesirable in some applications. In Section 4 we introduce SMOOTH-REWARD-DOUBLING, which achieves similar guarantees without resetting to the origin.\nRelated Work Our work is related, at least in spirit, to the use of a momentum term in stochastic gradient descent for back propagation in neural networks [7, 11, 9]. These results are similar in motivation in that they effectively yield a larger learning rate when many recent gradients point in the same direction.\nIn Follow-The-Regularized-Leader terms, the exponentiated gradient descent algorithm with unnormalized weights of Kivinen and Warmuth [8] plays xt+1 = arg minx\u2208Rn+ g1:t \u00b7 x+ 1 \u03b7 (x log x\u2212 x), which has closed-form solution xt+1 = exp(\u2212\u03b7g1:t). Like our algorithm, this algorithm moves away from the origin exponentially fast, but unlike our algorithm it can incur arbitrarily large regret with respect to x\u030a = 0. Theorem 9 shows that no algorithm of this form can provide bounds like the ones proved in this paper.\nHazan and Kale [5] give regret bounds in terms of the variance of the gt. Letting G = |g1:t| and H = \u2211T t=1 g 2 t , they prove regret bounds of the form O( \u221a V ) where V = H \u2212 G2/T . This result\nhas some similarity to our work in that G/ \u221a T = \u221a H \u2212 V , and so if we hold H constant, then when V is low, the critical ratio G/ \u221a T that appears in our bounds is large. However, they consider the case of a known feasible set, and their algorithm (gradient descent with a constant learning rate) cannot obtain bounds of the form we prove."}, {"heading": "2 Reward and Regret", "text": "In this section we present a general result that converts lower bounds on reward into upper bounds on regret, for one-dimensional online linear optimization. In the unconstrained setting, this result will be sufficient to provide guarantees for general n-dimensional online convex optimization.\nTheorem 1. Consider an algorithm for one-dimensional online linear optimization that, when run on a sequence of gradients g1, g2, . . . , gT , with gt \u2208 [\u22121, 1] for all t, guarantees\nReward \u2265 \u03ba exp (\u03b3|g1:T |)\u2212 , (1) where \u03b3, \u03ba > 0 and \u2265 0 are constants. Then, against any comparator x\u030a \u2208 [\u2212R,R], we have\nRegret(\u030ax) \u2264 R \u03b3\n( log ( R\n\u03ba\u03b3\n) \u2212 1 ) + , (2)\nletting 0 log 0 = 0 when R = 0. Further, any algorithm with the regret guarantee of Eq. (2) must guarantee the reward of Eq. (1).\nWe give a proof of this theorem in the appendix. The duality between reward and regret can also be seen as a consequence of the fact that exp(x) and y log y \u2212 y are convex conjugates. The \u03b3 term typically contains a dependence on T like 1/ \u221a T . This bound holds for all R, and so for some small R the log term becomes negative; however, for real algorithms the term will ensure the regret bound remains positive. The minus one can of course be dropped to simplify the bound further."}, {"heading": "3 Gradient Descent with Increasing Learning Rates", "text": "In this section we show that allowing the learning rate of gradient descent to sometimes increase leads to novel theoretical guarantees.\nTo build intuition, consider online linear optimization in one dimension, with gradients g1, g2, . . . , gT , all in [\u22121, 1]. In this setting, the reward of unconstrained gradient descent has a simple closed form: Lemma 2. Consider unconstrained gradient descent in one dimension, with learning rate \u03b7. On round t, this algorithm plays the point xt = \u03b7g1:t\u22121. Letting G = |g1:t| and H = \u2211T t=1 g 2 t , the cumulative reward of the algorithm is exactly\nReward = \u03b7\n2\n( G2 \u2212H ) .\nWe give a simple direct proof in Appendix A. Perhaps surprisingly, this result implies that the reward is totally independent of the order of the linear functions selected by the adversary. Examining the expression in Lemma 2, we see that the optimal choice of learning rate \u03b7 depends fundamentally on two quantities: the absolute value of the sum of gradients (G), and the sum of the squared gradients (H). If G2 > H , we would like to use as large a learning rate as possible in order to maximize reward. In contrast, if G2 < H , the algorithm will obtain negative reward, and the best it can do is to cut its losses by setting \u03b7 as small as possible.\nOne of the motivations for this work is the observation that the state-of-the-art online gradient descent algorithms adjust their learning rates based only on the observed value ofH (or its upper bound T ); for example [4, 10]. We would like to increase reward by also accounting for G. But unlike H , which is monotonically increasing with time, G can both increase and decrease. This makes simple guess-and-doubling tricks fail when applied to G, and necessitates a more careful approach."}, {"heading": "3.1 Analysis in One Dimension", "text": "In this section we analyze algorithm REWARD-DOUBLING-1D (Algorithm 1), which consists of a series of epochs. We suppose for the moment that an upper bound H\u0304 on H = \u2211T t=1 g 2 t is known in advance. In the first epoch, we run gradient descent with a small initial learning rate \u03b7 = \u03b71. Whenever the total reward accumulated in the current epoch reaches \u03b7H\u0304 , we double \u03b7 and start a new epoch (returning to the origin and forgetting all previous gradients except the most recent one).\nLemma 3. Applied to a sequence of gradients g1, g2, . . . , gT , all in [\u22121, 1], whereH = \u2211T t=1 g 2 t \u2264 H\u0304 , REWARD-DOUBLING-1D obtains reward satisfying\nReward = T\u2211 t=1 xtgt \u2265 1 4 \u03b71H\u0304 exp ( a |g1:T |\u221a H\u0304 ) \u2212 \u03b71H\u0304, (3)\nfor a = log(2)/ \u221a 3.\nAlgorithm 1 REWARD-DOUBLING-1D Parameters: initial learning rate \u03b71, upper bound H\u0304 \u2265 \u2211T t=1 g 2 t .\nInitialize x1 \u2190 0, i\u2190 1, and Q1 \u2190 0. for t = 1, 2, . . . , T do\nPlay xt, and receive reward xtgt. Qi \u2190 Qi + xtgt. if Qi < \u03b7iH\u0304 then xt+1 \u2190 xt + \u03b7igt. else i\u2190 i+ 1. \u03b7i \u2190 2\u03b7i\u22121; Qi \u2190 0. xt+1 \u2190 0 + \u03b7igt.\nAlgorithm 2 REWARD-DOUBLING Parameters: maximum origin-regret i for 1 \u2264 i \u2264 n. for i = 1, 2, . . . , n do\nLet Ai be a copy of algorithm REWARD-DOUBLING-1D-GUESS (see Theorem 4), with parameter i. for t = 1, 2, . . . , T do Play xt, with xt,i selected by Ai. Receive gradient vector gt = \u2212Oft(xt). for i = 1, 2, . . . , n do\nFeed back gt,i to Ai.\nProof. Suppose round T occurs during the k\u2019th epoch. Because epoch i can only come to an end if Qi \u2265 \u03b7iH\u0304 , where \u03b7i = 2i\u22121\u03b71, we have\nReward = k\u2211 i=1 Qi \u2265 ( k\u22121\u2211 i=1 2i\u22121\u03b71H\u0304 ) +Qk = ( 2k\u22121 \u2212 1 ) \u03b71H\u0304 +Qk . (4)\nWe now lower bound Qk. For i = 1, . . . , k let ti denote the round on which Qi is initialized to 0, with t1 \u2261 1, and define tk+1 \u2261 T . By construction, Qi is the total reward of a gradient descent algorithm that is active on rounds ti through ti+1 inclusive, and that uses learning rate \u03b7i (note that on round ti, this algorithm gets 0 reward and we initialize Qi to 0 on that round). Thus, by Lemma 2, we have that for any i,\nQi = \u03b7i 2\n( (gti:ti+1) 2 \u2212 ti+1\u2211 s=ti g2s ) \u2265 \u2212\u03b7i 2 H\u0304 .\nApplying this bound to epoch k, we have Qk \u2265 \u2212 12\u03b7kH\u0304 = \u22122 k\u22122\u03b71H\u0304 . Substituting into (4) gives\nReward \u2265 \u03b71H\u0304(2k\u22121 \u2212 1\u2212 2k\u22122) = \u03b71H\u0304(2k\u22122 \u2212 1) . (5)\nWe now show that k \u2265 |g1:T |\u221a 3H\u0304 . At the end of round ti+1\u22121, we must have hadQi < \u03b7iH\u0304 (otherwise epoch i+ 1 would have begun earlier). Thus, again using Lemma 2,\n\u03b7i 2\n( (gti:ti+1\u22121) 2 \u2212 H\u0304 ) \u2264 \u03b7iH\u0304\nso |gti:ti+1\u22121| \u2264 \u221a 3H\u0304 . Thus,\n|g1:T | \u2264 k\u2211 i=1 |gti:ti+1\u22121| \u2264 k \u221a 3H\u0304 .\nRearranging gives k \u2265 |g1:T |\u221a 3H\u0304 , and combining with Eq. (5) proves the lemma.\nWe can now apply Theorem 1 to the reward (given by Eq. (3)) of REWARD-DOUBLING-1D to show\nRegret(\u030ax) \u2264 bR \u221a H\u0304 ( log ( 4Rb \u221a H\u0304\n\u03b71\n) \u2212 1 ) + \u03b71H\u0304 (6)\nfor any x\u030a \u2208 [\u2212R,R], where b = a\u22121 = \u221a\n3/ log(2) < 2.5. When the feasible set is also fixed in advance, online gradient descent with a fixed learning obtains a regret bound of O(R \u221a T ). Suppose we use the estimate H\u0304 = T . By choosing \u03b71 = 1T , we guarantee constant regret against the origin, x\u030a = 0 (equivalently, constant total loss). Further, for any feasible set of radius R, we still have\nworst-case regret of at most O(R \u221a T log((1 + R)T )), which is only modestly worse than that of gradient descent with the optimal R known in advance.\nThe need for an upper bound H\u0304 can be removed using a standard guess-and-doubling approach, at the cost of a constant factor increase in regret (see appendix for proof). Theorem 4. Consider algorithm REWARD-DOUBLING-1D-GUESS, which behaves as follows. On each era i, the algorithm runs REWARD-DOUBLING-1D with an upper bound of H\u0304i = 2i\u22121, and initial learning rate \u03b7i1 = 2\n\u22122i. An era ends when H\u0304i is no longer an upper bound on the sum of squared gradients seen during that era. Letting c = \u221a 2\u221a\n2\u22121 , this algorithm has regret at most\nRegret \u2264 cR \u221a H + 1 ( log ( R (2H + 2)5/2 ) \u2212 1 ) + ."}, {"heading": "3.2 Extension to n dimensions", "text": "To extend our results to general online convex optimization, it is sufficient to run a separate copy of REWARD-DOUBLING-1D-GUESS for each coordinate, as is done in REWARD-DOUBLING (Algorithm 2). The key to the analysis of this algorithm is that overall regret is simply the sum of regret on n one-dimensional subproblems which can be analyzed independently. Theorem 5. Given a sequence of convex loss functions f1, f2, . . . , fT from Rn to R, REWARD-DOUBLING with i = n has regret bounded by\nRegret(\u030ax) \u2264 + c n\u2211 i=1 |\u030axi| \u221a Hi + 1 ( log (n |\u030axi|(2Hi + 2)5/2 ) \u2212 1 )\n\u2264 + c\u2016x\u030a\u20162 \u221a H + n ( log (n \u2016x\u030a\u201622(2H + 2)5/2 ) \u2212 1 )\nfor c = \u221a\n2\u221a 2\u22121 , where Hi = \u2211T t=1 g 2 t,i and H = \u2211T t=1 \u2016gt\u201622.\nProof. Fix a comparator x\u030a. For any coordinate i, define\nRegreti = T\u2211 t=1 x\u030aigt,i \u2212 T\u2211 t=1 xt,igt,i .\nObserve that n\u2211 i=1 Regreti = T\u2211 t=1 x\u030a \u00b7 gt \u2212 T\u2211 t=1 xt \u00b7 gt = Regret(\u030ax) .\nFurthermore, Regreti is simply the regret of REWARD-DOUBLING-1D-GUESS on the gradient sequence g1,i, g2,i, . . . , gT,i. Applying the bound of Theorem 4 to each Regreti term completes the proof of the first inequality. For the second inequality, let ~H be a vector whose ith component is\u221a Hi + 1, and let ~x \u2208 Rn where ~xi = |\u030axi|. Using the Cauchy-Schwarz inequality, we have\nn\u2211 i=1 |\u030axi| \u221a Hi + 1 = ~x \u00b7 ~H \u2264 \u2016x\u030a\u20162 \u2016 ~H\u20162 = \u2016x\u030a\u20162 \u221a H + n .\nThis, together with the fact that log(|\u030axi|(2Hi + 2)5/2) \u2264 log(\u2016x\u030a\u201622(2H + 2)5/2), suffices to prove second inequality.\nIn some applications, n is not known in advance. In this case, we can set i = i2 for the ith coordinate we encounter, and get the same bound up to constant factors."}, {"heading": "4 An Epoch-Free Algorithm", "text": "In this section we analyze SMOOTH-REWARD-DOUBLING, a simple algorithm that achieves bounds comparable to those of Theorem 4, without guessing-and-doubling. We consider only the 1-d problem, as the technique of Theorem 5 can be applied to extend to n dimensions. Given a parameter\n\u03b7 > 0, we achieve\nRegret \u2264 R \u221a T ( log ( RT 3/2\n\u03b7\n) \u2212 1 ) + 1.76\u03b7, (7)\nfor all T and R, which is better (by constant factors) than Theorem 4 when gt \u2208 {\u22121, 1} (which implies T = H). The bound can be worse on a problems where H < T .\nThe idea of the algorithm is to maintain the invariant that our cumulative reward, as a function of g1:t and t, satisfies Reward \u2265 N(g1:t, t), for some fixed function N . Because reward changes by gtxt on round t, it suffices to guarantee that for any g \u2208 [\u22121, 1],\nN(g1:t, t) + gxt+1 \u2265 N(g1:t + g, t+ 1) (8)\nwhere xt+1 is the point the algorithm plays on round t+ 1, and we assume N(0, 1) = 0.\nThis inequality is approximately satisfied (for small g) if we choose\nxt+1 = \u2202N(g1:t + g, t) \u2202g \u2248 N(g1:t + g, t)\u2212N(g1:t, t) g \u2248 N(g1:t + g, t+ 1)\u2212N(g1:t, t) g .\nThis suggests that if we want to maintain reward at least N(g1:t, t) = 1t (exp(|g1:t|/ \u221a t) \u2212 1) , we should set xt+1 \u2248 sign(g1:t)t\u22123/2 exp ( |g1:t|\u221a t ) . The following theorem (proved in the appendix) provides an inductive analysis of an algorithm of this form. Theorem 6. Fix a sequence of reward functions ft(x) = gtx with gt \u2208 [\u22121, 1], and let Gt = |g1:t|. We consider SMOOTH-REWARD-DOUBLING, which plays 0 on round 1 and whenever Gt = 0; otherwise, it plays xt+1 = \u03b7 sign(g1:t)B(Gt, t+ 5) (9) with \u03b7 > 0 a learning-rate parameter and\nB(G, t) = 1\nt3/2 exp ( G\u221a t ) . (10)\nThen, at the end of each round t, this algorithm has\nReward(t) \u2265 \u03b7 1 t+ 5 exp ( Gt\u221a t+ 5 ) \u2212 1.76\u03b7.\nTwo main technical challenges arise in the proof: first, we prove a result like Eq. (8) forN(g1:t, t) = (1/t) exp ( |g1:t|/ \u221a t ) . However, this Lemma only holds for t \u2265 6 and when the sign of g1:t doesn\u2019t change. We account for this by showing that a small modification toN (costing only a constant over all rounds) suffices.\nBy running this algorithm independently for each coordinate using an appropriate choice of \u03b7, one can obtain a guarantee similar to that of Theorem 5."}, {"heading": "5 Lower Bounds", "text": "As with our previous results, it is sufficient to show a lower bound in one dimension, as it can then be replicated independently in each coordinate to obtain an n dimensional bound. Note that our lower bound contains the factor log(|\u030ax| \u221a T ), which can be negative when x\u030a is small relative to T , hence it is important to hold x\u030a fixed and consider the behavior as T \u2192 \u221e. Here we give only a proof sketch; see Appendix A for the full proof. Theorem 7. Consider the problem of unconstrained online linear optimization in one dimension, and an online algorithm that guarantees origin-regret at most . Then, for any fixed comparator x\u030a, and any integer T0, there exists a gradient sequence {gt} \u2208 [\u22121, 1]T of length T \u2265 T0 for which the algorithm\u2019s regret satisfies\nRegret(\u030ax) \u2265 0.336|\u030ax| \u221a\u221a\u221a\u221aT log( |\u030ax|\u221aT ) .\nProof. (Sketch) Assume without loss of generality that x\u030a > 0. Let Q be the algorithm\u2019s reward when each gt is drawn independently uniformly from {\u22121, 1}. We have E[Q] = 0, and because the algorithm guarantees origin-regret at most , we haveQ \u2265 \u2212 with probability 1. LettingG = g1:T , it follows that for any threshold Z = Z(T ),\n0 = E[Q]\n= E[Q|G < Z] \u00b7 Pr[G < Z] + E[Q|G \u2265 Z] \u00b7 Pr[G \u2265 Z] \u2265 \u2212 Pr[G < Z] + E[Q|G \u2265 Z] \u00b7 Pr[G \u2265 Z] > \u2212 + E[Q|G \u2265 Z] \u00b7 Pr[G \u2265 Z] .\nEquivalently, E[Q|G \u2265 Z] <\nPr[G \u2265 Z] .\nWe choose Z(T ) = \u221a kT , where k = \u230a log(R \u221a T )/ log(p \u22121) \u230b\n. Here R = |\u030ax| and p > 0 is a constant chosen using binomial distribution lower bounds so that Pr[G \u2265 Z] \u2265 pk. This implies\nE[Q|G \u2265 Z] < p\u2212k = exp ( k log p\u22121 ) \u2264 R \u221a T .\nThis implies there exists a sequence with G \u2265 Z and Q < R \u221a T . On this sequence, regret is at least Gx\u030a\u2212Q \u2265 R \u221a kT \u2212R \u221a T = \u2126(R \u221a kT ).\nTheorem 8. Consider the problem of unconstrained online linear optimization in Rn, and consider an online algorithm that guarantees origin-regret at most . For any radius R, and any T0, there exists a gradient sequence gradient sequence {gt} \u2208 ([\u22121, 1]n)T of length T \u2265 T0, and a comparator x\u030a with \u2016x\u030a\u20161 = R, for which the algorithm\u2019s regret satisfies\nRegret(\u030ax) \u2265 0.336 n\u2211 i=1 |\u030axi|\n\u221a\u221a\u221a\u221aT log( |\u030axi|\u221aT ) .\nProof. For each coordinate i, Theorem 7 implies that there exists a T \u2265 T0 and a sequence of gradients gt,i such that\nT\u2211 t=1 x\u030aigt,i \u2212 T\u2211 t=1 xt,igt,i \u2265 0.336|\u030axi|\n\u221a\u221a\u221a\u221aT log( |\u030axi|\u221aT ) .\n(The proof of Theorem 7 makes it clear that we can use the same T for all i.) Summing this inequality across all n coordinates then gives the regret bound stated in the theorem.\nThe following theorem presents a stronger negative result for Follow-the-Regularized-Leader algorithms with a fixed regularizer: for any such algorithm that guarantees origin-regret at most T after T rounds, worst-case regret with respect to any point outside [\u2212 T , T ] grows linearly with T . Theorem 9. Consider a Follow-The-Regularized-Leader algorithm that sets\nxt = arg min x (g1:t\u22121x+ \u03c8T (x))\nwhere \u03c8T is a convex, non-negative function with \u03c8T (0) = 0. Let T be the maximum origin-regret incurred by the algorithm on a sequence of T gradients. Then, for any x\u030a with |\u030ax| > T , there exists a sequence of T gradients such that the algorithm\u2019s regret with respect to x\u030a is at least T\u221212 (|\u030ax| \u2212 T ).\nIn fact, it is clear from the proof that the above result holds for any algorithm that selects xt+1 purely as a function of g1:t (in particular, with no dependence on t)."}, {"heading": "6 Future Work", "text": "This work leaves open many interesting questions. It should be possible to apply our techniques to problems that do have constrained feasible sets; for example, it is natural to consider the unconstrained experts problem on the positive orthant. While we believe this extension is straightforward, handling arbitrary non-axis-aligned constraints will be more difficult. Another possibility is to develop an algorithm with bounds in terms of H rather than T that doesn\u2019t use a guess and double approach."}, {"heading": "A Proofs", "text": "This appendix gives the proofs omitted in the body of the paper, with the corresponding lemmas and theorems restated for convenience.\nTheorem 1. Consider an algorithm for one-dimensional online linear optimization that, when run on a sequence of gradients g1, g2, . . . , gT , with gt \u2208 [\u22121, 1] for all t, guarantees\nReward \u2265 \u03ba exp (\u03b3|g1:T |)\u2212 , (1)\nwhere \u03b3, \u03ba > 0 and \u2265 0 are constants. Then, against any comparator x\u030a \u2208 [\u2212R,R], we have\nRegret(\u030ax) \u2264 R \u03b3\n( log ( R\n\u03ba\u03b3\n) \u2212 1 ) + , (2)\nletting 0 log 0 = 0 when R = 0. Further, any algorithm with the regret guarantee of Eq. (2) must guarantee the reward of Eq. (1).\nProof. Let GT = |g1:T |. By definition, given the reward guarantee of Eq. (1) we have\nRegret \u2264 RGT \u2212 \u03ba exp (\u03b3GT ) + . (11)\nIf R = 0, then Eq. (2) follows immediately. Otherwise, note this is a concave function in GT , and setting the first derivative equal to zero shows\nG\u2217 = 1\n\u03b3 log\n( R\n\u03b3\u03ba\n) .\nmaximizes regret (for large enough R we could have G\u2217 > T , and so this G\u2217 is not actually achievable by the adversary, but this is fine for lower bounding regret). Plugging G\u2217 into Eq. (11) and simplifying yields the bound of Eq. (2). For the second claim, suppose Eq. (2) holds. Then, again by definition, we must have\nReward \u2265 RG\u2212 R \u03b3 log\n( R\n\u03b3\u03ba\n) + R\n\u03b3 \u2212 . (12)\nThis bound is a concave function of R, and since it holds for any R \u2265 0 by assumption, we can choose the R that maximizes the bound, namely R\u2217 = \u03b3\u03ba exp(\u03b3G). Note\nR\u2217\n\u03b3 log\n( R\u2217\n\u03b3\u03ba\n) = R\u2217\n\u03b3 log (exp (\u03b3G)) = R\u2217G,\nand so plugging R\u2217 into Eq. (12) yields\nReward \u2265 1 \u03b3 R\u2217 \u2212 = \u03ba exp (\u03b3G)\u2212 .\nLemma 2. Consider unconstrained gradient descent in one dimension, with learning rate \u03b7. On round t, this algorithm plays the point xt = \u03b7g1:t\u22121. Letting G = |g1:t| and H = \u2211T t=1 g 2 t , the cumulative reward of the algorithm is exactly\nReward = \u03b7\n2\n( G2 \u2212H ) .\nProof. The algorithm\u2019s cumulative reward after T rounds is\nT\u2211 t=1 xtgt = T\u2211 t=1 gt\u03b7g1:t\u22121 = \u03b7 2\n( (g1:T )\n2 \u2212 T\u2211 t=1 g2t\n) . (13)\nTo verify the second equality, note that (g1:T )2 \u2212 (g1:T\u22121)2 = g2T + 2gT (g1:T\u22121), so on round T the right hand side increases by \u03b7gT (g1:T\u22121), as does the left hand side. The equality then follows by induction on T .\nIt is worth noting that the standard R \u221a T bound can be derived from the above result fairly easily. We have\nRegret \u2264 RG\u2212 \u03b7 2 (G2 \u2212H)\n\u2264 \u03b7 2 H + max G\n( RG\u2212 \u03b7G 2\n2 ) \u2264 \u03b7\n2 H +\nR2 2\u03b7 ,\nwhere the max is achieved by taking G = R/\u03b7. Taking \u03b7 = R/ \u221a T then gives the standard bound. However, this bound significantly underestimates the performance of constant-learning-rate gradient descent whenG is large. This is in contrast to our regret bounds, which are always tight with respect to their matching reward bounds. Theorem 4. Consider algorithm REWARD-DOUBLING-1D-GUESS, which behaves as follows. On each era i, the algorithm runs REWARD-DOUBLING-1D with an upper bound of H\u0304i = 2i\u22121, and initial learning rate \u03b7i1 = 2\n\u22122i. An era ends when H\u0304i is no longer an upper bound on the sum of squared gradients seen during that era. Letting c = \u221a 2\u221a\n2\u22121 , this algorithm has regret at most\nRegret \u2264 cR \u221a H + 1 ( log ( R (2H + 2)5/2 ) \u2212 1 ) + .\nProof. Suppose round T occurs in era k, and let ti be the round on which era i starts, with tk+1 \u2261 T + 1. Define Hi = \u2211ti+1\u22121 s=ti\ng2s . To prove the theorem we will need several inequalities. First, note that H = \u2211k i=1Hi \u2265 \u2211k\u22121 i=1 H\u0304i = 2\nk\u22121 \u2212 1, or 2k\u22121 \u2264 H + 1. Thus, k\u2211 i=1 \u221a H\u0304i = k\u22121\u2211 i=0 \u221a 2i = \u221a 2 k \u2212 1\u221a 2\u2212 1 \u2264 \u221a 2k\u221a 2\u2212 1 \u2264 \u221a 2(H + 1)\u221a 2\u2212 1 = c \u221a H + 1 .\nNext, note that for any i we have\u221a H\u0304i \u03b7i1 = 1 2 i\u22121 2 +2i \u2264 1 22.5k \u2264 1 (2(H + 1))(5/2).\nNote that the bound of Lemma 3 applies for all T where H \u2264 H\u0304 , and thus so does Eq. (6). Thus, we can apply this bound to the regret in era k on rounds tk through T , as well as on the regret in each earlier era. Then, total regret with respect to the best point in [\u2212R,R] is at most the sum of the regret in each era, so\nRegret \u2264 k\u2211 i=1 R \u221a H\u0304i\n( log ( R \u221a H\u0304i\n\u03b7i1\n) \u2212 1 ) + \u03b7i1Hi\n\u2264 k\u2211 i=1 R \u221a H\u0304i ( log ( R (2H + 2)5/2 ) \u2212 1 ) + \u03b7i1Hi\n\u2264 cR \u221a H + 1 ( log ( R (2H + 2)5/2 ) \u2212 1 ) + k\u2211 i=1 \u03b7i1Hi\nFinally, because Hi \u2264 H\u0304i + 1 \u2264 2H\u0304i = 2i, we have \u2211k i=1 \u03b7 i 1Hi \u2264 \u2211k i=1 2\n\u2212i \u2264 , which completes the proof.\nTheorem 6. Fix a sequence of reward functions ft(x) = gtx with gt \u2208 [\u22121, 1], and let Gt = |g1:t|. We consider SMOOTH-REWARD-DOUBLING, which plays 0 on round 1 and whenever Gt = 0; otherwise, it plays xt+1 = \u03b7 sign(g1:t)B(Gt, t+ 5) (9) with \u03b7 > 0 a learning-rate parameter and\nB(G, t) = 1\nt3/2 exp ( G\u221a t ) . (10)\nThen, at the end of each round t, this algorithm has\nReward(t) \u2265 \u03b7 1 t+ 5 exp ( Gt\u221a t+ 5 ) \u2212 1.76\u03b7.\nProof. We present a proof for the case where \u03b7 = 1; since \u03b7 simply scales all of the xt played by the algorithm (and hence, reward), the result for general \u03b7 follows immediately. We use the minimum reward function\nN(G, t) = 1\nt exp ( G\u221a t ) . (14)\nThe proof will be by induction on t, with the induction hypothesis that the cumulative reward of the algorithm at the end of round t satisfies\nReward(t) \u2265 N(Gt, t+ 5)\u2212 1:t, (15)\nwhere 1 = N(1, 6) and for t > 1, t+1 = \u0303(t+ 5) with\n\u0303(\u03c4) = 1\n\u03c4 + 1 exp ( 1\u221a \u03c4 + 1 ) \u2212 1 \u03c4 + 1 \u03c43/2 .\nWe will then show that the sum of t\u2019s is always bounded by a constant.\nFor the base case, t = 1, we play x = 0 so end the round with zero reward, while the RHS of Eq. (15) is N(|g1|, 6)\u2212N(1, 6) \u2264 0. Now, suppose the induction hypothesis holds at the end of some round t \u2265 1. Without loss of generality, suppose g1:t \u2265 0 so Gt = g1:t. We consider two cases. First, suppose Gt > 0 and Gt + gt+1 > 0 (so gt+1 > \u2212Gt). In this case, g1:t does not change sign when we add gt+1; thus, an invariant like that of Eq. (8) is sufficient; we prove such a result in Lemma 10 (given below). More precisely, we play xt+1 according to Eq. (9), and\nReward(t+ 1) \u2265 N(Gt, t+ 5)\u2212 1:t + gt+1xt+1 IH and update rule \u2265 N(Gt + gt+1, t+ 5 + 1)\u2212 1:t Lemma 10 with \u03c4 = t+ 5. \u2265 N(Gt+1, t+ 5 + 1)\u2212 1:t+1, since t+1 > 0.\nFor the remaining case, we have Gt + gt+1 \u2264 0, implying gt+1 \u2264 \u2212Gt \u2264 0. In this case, we suffer some loss and arrive at Gt+1 = |Gt + gt+1| = \u2212gt+1 \u2212 Gt. Lemma 11 (below) provides the key bound on the additional loss when the sign of g1:t changes. If Gt > 0, we have\nReward(t+ 1) \u2265 N(Gt, t+ 5)\u2212 1:t + gt+1xt+1 IH and update rule \u2265 N(\u2212gt+1 \u2212Gt, t+ 5 + 1)\u2212 1:t+1 Lemma 11 with \u03c4 = t+ 5 = N(Gt+1, t+ 5 + 1)\u2212 1:t+1.\nIf Gt = 0, we can take gt+1 non-positive without loss of generality, and playing xt+1 = 0 is no worse than playing B(0, t+ 5), and so we conclude Eq. (15) holds for all t. Finally,\n\u221e\u2211 t=2 t \u2264 \u222b \u221e \u03c4=6 \u0303(\u03c4) = \u221a 2 3 \u2212 2\u03b3 + 2 Ei ( 1\u221a 7 ) + log(6) \u2264 1.50.\nwhere \u03b3 is the Euler gamma constant and Ei is the exponential integral. The upper bound can be found easily using numerical methods. Adding 1 = exp(1/ \u221a 6)/6 \u2264 0.26 gives 1:T \u2264 1.76 for any T .\nLemma 10. Let G > 0 and \u03c4 \u2265 6. Then, for any g \u2208 [\u22121, 1] such that G+ g \u2265 0,\nN(G, \u03c4) + gB(G, \u03c4)\u2212N(G+ g, \u03c4 + 1) \u2265 0\nwhere N is defined by Eq. (14) and B is defined by Eq. (10).\nProof. We need to show\n1 \u03c4 exp ( G\u221a \u03c4 ) + g \u03c43/2 exp ( G\u221a \u03c4 ) \u2212 1 \u03c4 + 1 exp ( G+ g\u221a \u03c4 + 1 ) \u2265 0.\nor equivalently, multiplying by \u03c43/2(1 + \u03c4)/ exp(G/ \u221a \u03c4) \u2265 0,\n\u2206 = \u221a \u03c4(1 + \u03c4) + g(1 + \u03c4)\u2212 \u03c43/2 exp ( G+ g\u221a \u03c4 + 1 \u2212 G\u221a \u03c4 ) \u2265 0.\nSince \u03c4 + 1 \u2265 \u03c4 , the exp term is maximized when G = 0, so\n\u2206 \u2265 (g + \u221a \u03c4)(1 + \u03c4)\u2212 \u03c43/2 exp ( g\u221a \u03c4 + 1 ) . (16)\nNow, we consider the cases where g \u2265 0 and g < 0 separately. First, suppose g > 0, so g/ \u221a \u03c4 + 1 \u2208 [0, 1], and we can use the inequality exp(x) \u2264 1 + x+ x2 for x \u2208 [0, 1], which gives\n\u2206 \u2265 g + g\u03c4 + \u221a \u03c4 + \u03c43/2 \u2212 \u03c43/2 ( 1 +\ng\u221a \u03c4 + 1 + g2 \u03c4 + 1 ) \u2265 g + g\u03c4 + \u221a \u03c4 + \u03c43/2 \u2212 \u03c43/2 ( 1 +\ng\u221a \u03c4 + 1 \u03c4 ) = g + g\u03c4 + \u221a \u03c4 + \u03c43/2 \u2212 \u03c43/2 \u2212 g\u03c4 \u2212 \u221a \u03c4\n= g > 0.\nNow, we consider the case where g < 0. In order to show \u2206 \u2265 0 in this case, we need a tight upper bound on exp(y) for y \u2208 [\u22121, 0]. To derive one, we note that for x \u2265 0, exp(x) \u2265 1 + x + 12x 2 from the series representation of ex, and so exp(\u2212x) \u2264 (1 + x+ 12x 2)\u22121. Thus, for y \u2208 [\u22121, 0] we have exp(y) \u2264 (1\u2212 y + 12y 2)\u22121 = Q(y). Then, starting from Eq. (16),\n\u2206 \u2265 (g + \u221a \u03c4)(1 + \u03c4)\u2212 \u03c43/2Q ( g\u221a \u03c4 + 1 ) .\nLet \u22062 = \u2206Q (\ng\u221a \u03c4+1\n)\u22121 . Because \u22062 and \u2206 have the same sign, it suffices to show \u22062 \u2265 0. We\nhave\n\u22062 =\n( 1\u2212 g\u221a\n\u03c4 + 1 +\ng2\n2(t+ 1)\n) (g + \u221a \u03c4)(1 + \u03c4)\u2212 \u03c43/2\n= ( 1 + \u03c4 \u2212 g \u221a \u03c4 + 1 + 1 2 g2 ) (g + \u221a \u03c4)\u2212 \u03c43/2.\nFirst, note d\ndg \u22062 = 1 +\n3g2 2 + g \u221a \u03c4 + \u03c4 \u2212 2g \u221a 1 + \u03c4 \u2212 \u221a \u03c4 \u221a 1 + \u03c4 .\nSince g \u2264 0, we have \u22122g \u221a \u03c4 + 1 + g \u221a \u03c4 \u2265 0, and (t+ 1)\u2212 \u221a \u03c4 \u221a \u03c4 + 1 \u2265 0, and so we conclude that \u22062 is increasing in g, and so taking g = \u22121 we have\n\u22062 \u2265 (3\n2 + \u03c4 +\n\u221a \u03c4 + 1 ) (\u22121 + \u221a \u03c4)\u2212 \u03c43/2\nTaking the derivative with respect to \u03c4 reveals this expression is increasing in \u03c4 , and taking \u03c4 = 6 produces a positive value, proving this case.\nLemma 11. For any g \u2208 [\u22121, 0] and G \u2265 0 such that G+ g \u2264 0, and any \u03c4 \u2265 1, N(G, \u03c4) + gB(G, \u03c4) \u2265 N(\u2212g \u2212G, \u03c4 + 1)\u2212 \u0303(\u03c4)\nwhere N is defined by Eq. (14) and B is defined by Eq. (10), and\n\u0303(\u03c4) \u2261 1 \u03c4 + 1 exp ( 1\u221a \u03c4 + 1 ) \u2212 1 \u03c4 + 1 \u03c43/2 .\nProof. We have\nN(\u2212g \u2212G, \u03c4 + 1)\u2212N(G, \u03c4)\u2212 gB(G, \u03c4)\n= 1\n\u03c4 + 1 exp ( \u2212g \u2212G\u221a \u03c4 + 1 ) \u2212 1 \u03c4 exp ( G\u221a \u03c4 ) \u2212 g \u03c43/2 exp ( G\u221a \u03c4 ) ,\nand since this expression is increasing as g decreases, and g \u2265 \u22121 in any case,\n\u2264 1 \u03c4 + 1 exp ( 1\u2212G\u221a \u03c4 + 1 ) \u2212 1 \u03c4 exp ( G\u221a \u03c4 ) + 1 \u03c43/2 exp ( G\u221a \u03c4 ) ,\nand since \u03c43/2 > \u03c4 , taken together the second two terms increase as G decreases, as does the first term, so since G \u2265 0,\n\u2264 1 \u03c4 + 1 exp ( 1\u221a \u03c4 + 1 ) \u2212 1 \u03c4 + 1 \u03c43/2 = \u0303(\u03c4),\nand re-arranging proves the lemma.\nTheorem 9. Consider a Follow-The-Regularized-Leader algorithm that sets\nxt = arg min x (g1:t\u22121x+ \u03c8T (x))\nwhere \u03c8T is a convex, non-negative function with \u03c8T (0) = 0. Let T be the maximum origin-regret incurred by the algorithm on a sequence of T gradients. Then, for any x\u030a with |\u030ax| > T , there exists a sequence of T gradients such that the algorithm\u2019s regret with respect to x\u030a is at least T\u221212 (|\u030ax| \u2212 T ).\nProof. For simplicity, we will prove that regret is at least T2 (|\u030ax| \u2212 T ) when T is even; if T is odd, we simply take gT = 0 and consider the first T \u2212 1 rounds. Let T = 2M . We will consider two gradient sequences. First, suppose gt = 1 for t \u2264 M , and gt = \u22121 otherwise. Observe that for any r, we have g1:M\u2212r = g1:M+r, which implies xM\u2212r+1 = xM+r+1. Thus, the algorithm\u2019s total reward is\nT\u2211 t=1 xtgt = M\u2211 t=1 xt \u2212 T\u2211 t=M+1 xt\n= x1 \u2212 xM+1 + M\u22121\u2211 r=1 xM\u2212r+1 \u2212 xM+r+1\n= x1 \u2212 xM+1 Because x1 = 0, we get that on this sequence the algorithm has origin-regret x\u0302 \u2261 xM+1, and so by assumption x\u0302 \u2264 T . Next, suppose g1 = 1 for t \u2264 M , and gt = 0 otherwise. For this sequence, we will have xt \u2264 x\u0302 \u2264 T for all t, so total reward is at most M T . For any positive x\u030a with x\u030a > T , this means that regret with respect to x\u030a is at least x\u030aM \u2212M T = M(|\u030ax| \u2212 T ) . For x\u030a < \u2212 T , we can use a similar argument with the sign of the gradients reversed (for both gradient sequences) to get the same bound.\nIn proving Theorem 7, we will use the following lemma. Lemma 12. Let GT = \u2211T i=1 gi be the sum of T random variables, each drawn uniformly from {\u22121, 1}. Then, for any integer k that is a factor of T , we have\nPr[GT \u2265 \u221a kT ] \u2265 pk .\nwhere p = 726 = 0.109375.\nProof. First, for any T define pT = Pr[GT \u2265 \u221a T ], and define\np = inf T\u2208Z+ pT .\nFor any T , we have pT \u2265 2\u2212T trivially, and by the Central Limit Theorem, limT\u2192\u221e pT = 1 \u2212 N0,1(1) > 0, where N0,1 is the standard normal cumulative distribution function. It follows that p > 0, and using numerical methods we find p = p6 = 726 = 0.109375.\nNow, divide the length T sequence into k sequences of length Tk . Let Zi be the sum of gradients for the ith of these sequences. Observe that if Zi \u2265 \u221a T k for all i, then GT = \u2211k i=1 Zi \u2265 k \u221a T k =\u221a\nkT . Furthermore, for any i, we have\nPr [ Zi \u2265 \u221a T\nk\n] = Pr [ GT k \u2265 \u221a T\nk\n] \u2265 p .\nThus,\nPr [ G \u2265 \u221a kT ] \u2265 k\u220f i=1 Pr [ Zi \u2265 \u221a T k ] \u2265 pk .\nTheorem 7. Consider the problem of unconstrained online linear optimization in one dimension, and an online algorithm that guarantees origin-regret at most . Then, for any fixed comparator x\u030a, and any integer T0, there exists a gradient sequence {gt} \u2208 [\u22121, 1]T of length T \u2265 T0 for which the algorithm\u2019s regret satisfies\nRegret(\u030ax) \u2265 0.336|\u030ax| \u221a\u221a\u221a\u221aT log( |\u030ax|\u221aT ) .\nProof. Let k = k(T ) = \u230a log(R \u221a T )/ log(p \u22121) \u230b\n, and choose T \u2265 T0 large enough so that 4 \u2264 k \u2264 T and also so that T is a multiple of k (the latter is possible since k(T ) grows much more slowly than T ). Let Q be the algorithm\u2019s reward when each gt is drawn uniformly from {\u22121, 1}. Let G = g1:T . As shown in the proof sketch, we have\nE[Q|G \u2265 \u221a kT ] <\nPr[G \u2265 \u221a kT ] .\nBy Lemma 12, Pr[G \u2265 \u221a kT ] \u2265 pk. Thus,\nE[Q|G \u2265 \u221a kT ] < p\u2212k = exp ( k log p\u22121 ) \u2264 R \u221a T .\nIf the algorithm guaranteed Q \u2265 R \u221a T whenever G \u2265 \u221a kT , then we would have E[Q|G \u2265\u221a\nkT ] \u2265 R \u221a T , a contradiction. Thus, there exists a sequence where G \u2265 \u221a kT and Q < R \u221a T , so\non this sequence we have\nRegret \u2265 R \u221a kT \u2212R \u221a T = R \u221a T ( \u221a k \u2212 1)\nBecause k \u2265 4, we have 12 \u221a k \u2265 1 or \u221a k \u2212 1 \u2265 12 \u221a k, so regret is at least 12R \u221a kT =\nbR \u221a T log ( R \u221a T ) , where b = 12 \u221a 1 log p\u22121 > 0.336 (and p is the constant from Lemma 12)."}], "references": [{"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Algorithms for portfolio management based on the Newton method", "author": ["Amit Agarwal", "Elad Hazan", "Satyen Kale", "Robert E. Schapire"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "In COLT,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In COLT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "On stochastic and worst-case models for investing", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["Robert A. Jacobs"], "venue": "Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1987}, {"title": "Exponentiated Gradient Versus Gradient Descent for Linear Predictors", "author": ["Jyrki Kivinen", "Manfred Warmuth"], "venue": "Journal of Information and Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Optimal stochastic search and adaptive momentum", "author": ["Todd K. Leen", "Genevieve B. Orr"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H. Brendan McMahan", "Matthew Streeter"], "venue": "In COLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Gradient descent: Second order momentum and saturating error", "author": ["Barak Pearlmutter"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": ", [3, 12] for an introduction).", "startOffset": 2, "endOffset": 9}, {"referenceID": 11, "context": ", [3, 12] for an introduction).", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": ", [3]), there are n experts, and on each round t the player selects an expert (say i), and obtains reward gt,i from a bounded interval (say [\u22121, 1]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "It is useful to contrast our results in this setting to previous applications of online convex optimization to portfolio management, for example [6] and [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "It is useful to contrast our results in this setting to previous applications of online convex optimization to portfolio management, for example [6] and [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 7, "context": "Table 1 compares the bounds for REWARD-DOUBLING (this paper) to those of two previous algorithms: online gradient descent [13] and projected exponentiated gradient descent [8, 12].", "startOffset": 172, "endOffset": 179}, {"referenceID": 11, "context": "Table 1 compares the bounds for REWARD-DOUBLING (this paper) to those of two previous algorithms: online gradient descent [13] and projected exponentiated gradient descent [8, 12].", "startOffset": 172, "endOffset": 179}, {"referenceID": 0, "context": "Gradient descent is minimax-optimal [1] when the comparator point is contained in a hypershere whose radius is known in advance (\u2016x\u030a\u20162 \u2264 R) and gradients are sparse (\u2016gt\u20162 \u2264 1, top table).", "startOffset": 36, "endOffset": 39}, {"referenceID": 6, "context": "Related Work Our work is related, at least in spirit, to the use of a momentum term in stochastic gradient descent for back propagation in neural networks [7, 11, 9].", "startOffset": 155, "endOffset": 165}, {"referenceID": 10, "context": "Related Work Our work is related, at least in spirit, to the use of a momentum term in stochastic gradient descent for back propagation in neural networks [7, 11, 9].", "startOffset": 155, "endOffset": 165}, {"referenceID": 8, "context": "Related Work Our work is related, at least in spirit, to the use of a momentum term in stochastic gradient descent for back propagation in neural networks [7, 11, 9].", "startOffset": 155, "endOffset": 165}, {"referenceID": 7, "context": "In Follow-The-Regularized-Leader terms, the exponentiated gradient descent algorithm with unnormalized weights of Kivinen and Warmuth [8] plays xt+1 = arg minx\u2208Rn+ g1:t \u00b7 x+ 1 \u03b7 (x log x\u2212 x), which has closed-form solution xt+1 = exp(\u2212\u03b7g1:t).", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "Hazan and Kale [5] give regret bounds in terms of the variance of the gt.", "startOffset": 15, "endOffset": 18}, {"referenceID": 3, "context": "One of the motivations for this work is the observation that the state-of-the-art online gradient descent algorithms adjust their learning rates based only on the observed value ofH (or its upper bound T ); for example [4, 10].", "startOffset": 219, "endOffset": 226}, {"referenceID": 9, "context": "One of the motivations for this work is the observation that the state-of-the-art online gradient descent algorithms adjust their learning rates based only on the observed value ofH (or its upper bound T ); for example [4, 10].", "startOffset": 219, "endOffset": 226}], "year": 2012, "abstractText": "Some of the most compelling applications of online convex optimization, including online prediction and classification, are unconstrained: the natural feasible set is R. Existing algorithms fail to achieve sub-linear regret in this setting unless constraints on the comparator point x\u030a are known in advance. We present algorithms that, without such prior knowledge, offer near-optimal regret bounds with respect to any choice of x\u030a. In particular, regret with respect to x\u030a = 0 is constant. We then prove lower bounds showing that our guarantees are near-optimal in this setting.", "creator": "LaTeX with hyperref package"}}}