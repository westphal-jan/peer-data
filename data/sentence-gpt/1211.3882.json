{"id": "1211.3882", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2012", "title": "Gliders2012: Development and Competition Results", "abstract": "The RoboCup 2D Simulation League incorporates several challenging features, setting a benchmark for Artificial Intelligence (AI). In this paper we describe some of the ideas and tools around the development of our team, Gliders2012. In our description, we focus on the evaluation function as one of our central mechanisms for action selection. We also point to a new framework for watching log files in a web browser that we release for use and further development by the RoboCup community. Finally, we also summarize results of the group and final matches we played during RoboCup 2012, with Gliders2012 finishing 4th out of 19 teams.", "histories": [["v1", "Fri, 16 Nov 2012 13:20:59 GMT  (121kb,D)", "https://arxiv.org/abs/1211.3882v1", null], ["v2", "Wed, 21 Nov 2012 04:03:21 GMT  (121kb,D)", "http://arxiv.org/abs/1211.3882v2", "10 pages"]], "reviews": [], "SUBJECTS": "cs.AI cs.MA cs.RO", "authors": ["edward moore", "oliver obst", "mikhail prokopenko", "peter wang", "jason held"], "accepted": false, "id": "1211.3882"}, "pdf": {"name": "1211.3882.pdf", "metadata": {"source": "CRF", "title": "Gliders2012: Development and Competition Results", "authors": ["Edward Moore", "Oliver Obst", "Mikhail Prokopenko", "Peter Wang", "Jason Held"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The RoboCup Simulation League [10] incorporates several challenging features, setting a benchmark for Artificial Intelligence (AI). The following list includes some of the most prominent characteristics of the RoboCup 2D Simulation League:\n\u2013 distributed client/server system running on a network, leading to fragmented, localized and imprecise (noisy and latent) information about the environment (field) [12]; \u2013 concurrent communication with a medium-sized number of agents [21]; \u2013 heterogeneous sensory data (visual, auditory, kinetic) and limited range of basic com-\nmands/effectors (turn, kick, dash, . . .) [19]; \u2013 asynchronous perception-action activity and limited window of opportunity to perform\nan action [6]; \u2013 autonomous decision-making under constraints enforced by teamwork (collaboration)\nand opponent (competition) [20]; \u2013 conflicts between reactivity and deliberation [18]; \u2013 no centralized controllers and centralized world model (no global vision, etc.) [16,17].\nFrom the onset of the RoboCup effort it was recognized that, as a benchmark, RoboCup is fairly different from another classical AI problem \u2014 chess. As pointed out by Asada et al. [4], chess and RoboCup differ in a few key elements: environment (static vs dynamic), state change (turn-taking vs real-time), information accessibility (complete vs incomplete), sensor readings (symbolic vs non-symbolic), and control (central vs distributed). This difference has been well understood over the last decade. Nevertheless, there are some similarities, for example, efficient evaluation functions used by the RoboCup agents are conceptually similar\n? CSIRO authors are listed in alphabetical order.\nar X\niv :1\n21 1.\n38 82\nv2 [\ncs .A\nI] 2\n1 N\nov 2\n01 2\nto evaluation functions used by chess computers: in either case the agent is attempting to consider multiple future states, assign some values to the alternative outcomes, and choose an action optimizing the evaluations. One may argue that superior performance of recent world champions in the RoboCup 2D Simulation League [3,5] may be attributed, at least partially, to sophisticated evaluation functions employed by these teams. In this short paper we describe a novel mechanism utilizing action-dependent evaluation functions, comparing it to some well known constructive models used by belief revision and belief update [14].\nThe experiments are carried out using a new simulated soccer team for the RoboCup soccer 2D simulator [7], Gliders2012. The team code is written by C++ using agent2d: the well-known base code developed by Akiyama et al. [1]. Other software packages are used as well:\n\u2013 librcsc: a base library for RCSS with various utilities describing relevant geometrical constructs, world model, etc.; \u2013 soccerwindow2: a viewer program for RCSS, working as a monitor client, a log player and a visual debugger; \u2013 fedit2: a formation editor for agent2d, allowing to design a team formation."}, {"heading": "2 Motivation and approach", "text": ""}, {"heading": "2.1 Chess analogy", "text": "As argued by Larame\u0301e, in chess \u201cthe evaluation function, is unique in a very real sense: while search techniques are pretty much universal and move generation can be deducted from a game\u2019s rules and no more, evaluation requires a deep and thorough analysis of strategy\u201d [11]. He lists several main board evaluation metrics: material balance (an account of which pieces are on the board for each side), mobility (a measure of how many move options are available, especially for powerful chess pieces), board control (a side controls a square if it has more pieces attacking it than the opponent), development (minor pieces should be brought into the game as quickly as possible), pawn formations, king safety and tropism (a measure of how easy it is for a piece to attack the opposing king; usually measured in terms of distance).\nOne may draw some parallels with RoboCup Simulation. For example, the goal safety and distances to the opposing goal are analogous to king safety and tropism, pawn formations may give some hints to team formations, development is somewhat similar to developing an attack from within your own half, board control is akin to blocking and marking opponent players (i.e., field control), mobility is achieved by either positioning teammates to receive a pass, or creating multiple directions for dribble, and material balance can be computed by accounting for heterogeneous player types and remaining stamina values. All these analogies are, of course, not direct \u2014 nevertheless, they may be provide some inspiration for an evaluation function relevant for RoboCup Simulation."}, {"heading": "2.2 Basic evaluation", "text": "The evaluation function of agent2D is, however, quite simple. Using the chess analogy, it implements tropism only, and is intended to make the basic client play in a goal-oriented\nfashion. For a player controlling the ball, it considers two features of each possible resultant state s: its X-coordinate (the larger the better) and the distance from it to the opponent\u2019s goal (the smaller the better). That is, the opponent\u2019s goal is the ultimate desirable resultant state S, and each action a is rated in terms of a single distance metric D\nr(a) = D(s = result(a), S) . (1)\nThe action that is selected is simply the one that minimizes the distance between resultant and desirable states, i.e., minimizes this metric:\na\u2217 = arg min a r(a) . (2)\nFor the players who are not controlling the ball and are engaged in intercept behavior, the evaluation function is not specified explicitly. These players select positions on the field according to their roles in the team formation.\nThe evaluation function (2) has reached a significant aim: all types of actions (dribbles, passes, etc.) can be directly compared to each other in terms of a single metric. At the same time, the simple computation is not adequate to support a very sophisticated tactical play in mid-field, or even near opponent\u2019s penalty area. Another drawback is that all the actions are judged in relation to a single point: the opponent goal."}, {"heading": "2.3 Multiple desirable states: tactics", "text": "Our main objective was to retain the advantage of a single metric, but diversify the evaluation by considering multiple points as desirable states. Moreover, we suggested [15] not only that the most desirable state can change from one cycle to another, but also that a player may entertain multiple desirable states at any given time (cycle). This diversity is brought about by different tactics. For example, a player may consider one desirable state S1 if passing to the left (action a1) pursuing one tactic, another resultant state S2 if passing to the right (action a2) guided by another tactic, and yet another desirable state S3 if dribbling to the center (action a3) suggested by a third tactic. Each of the considered actions is evaluated with respect to the corresponding desirable state that represents one of possible tactical ways to develop the play.\nIn other words, at any given time, there is a number m of tactics represented by a set of desirable states: {S1, . . . , Sm}, and the feasible actions are partitioned into m sets: A1, . . . , Am, so that for every action ai, there is a set Aj such that ai \u2208 Aj . We denote the function mapping an action to its tactical state by\ntactics : a\u2192 S . (3)\nThen each feasible action is rated with respect to the corresponding desirable state:\nr(a) = D(s = result(a), S = tactics(a)) (4)\nfollowed by selection according to the optimization (2). This approach does not impose tactics in a top-down fashion, selecting one tactic and the sub-selecting the best action for the chosen tactic. Rather, all feasible actions are considered, and tactics contribute to the\nevaluation via the desirable states suggested by the tactics. In certain cases the opponent\u2019s goal becomes one of possible desirable states (one of the tactics), keeping the goal-oriented behavior of agents.\nThe difference between definitions (1) and (4) is simply that the desirable states that the player is trying to reach are not independent of actions, but rather are action-dependent, and this dependence is tactical. To re-iterate, the comparison between two actions a1 and a2 according to the first definition (1) always assumes the same action-independent state S that is evaluated against, while the proposed definition (4) allows for different desirable states S1 = tactics(a1) and S2 = tactics(a2). The metric D is the same for all actions, retaining the advantage of a uniform comparison across different action types.\nWe would like to point out at this stage a difference between the proposed actiondependent evaluation function and other action-dependent formalisms, e.g., with actiondependent features generalizing state space proposed by Stone and Veloso [22]. The latter study described a multi-agent learning paradigm called team-partitioned, opaque-transition reinforcement learning (TPOT-RL). TPOT-RL introduced the concept of using actiondependent features to generalize the state space. However, regardless of action-dependent features, each possible action a is evaluated by TPOT-RL based on the current state of the world using a fixed function e : (S,A)\u2192 U . That is, the function e is the same for all actions in TPOT-RL.\nAnother interesting point is the analogy between multiple desirable states unified by the proposed evaluation function and the constructive model for belief update and belief revision [14]. Belief revision is the process by which a rational agent changes their beliefs about a static world in the light of new data. Belief update on the other hand is the process by which an agent maintains their beliefs up to date with an evolving world. The constructive model for belief revision includes a single similarity structure centered on all possible worlds consistent with current beliefs (a single system of nested spheres), and identifies the nearest sphere which is consistent with the new data. Peppas et al. [14] have shown that the model for belief update uses multiple systems of spheres (one for each possible world), finds in parallel the spheres consistent with the new data that are nearest to their respective central possible worlds, and collects possible worlds within these spheres. Arguably, the action-dependent evaluation proposed here is akin to the constructive model of belief update."}, {"heading": "2.4 Mobility and field control", "text": "The function tactics implements the mobility aspect of evaluation, by diversifying options of the player controlling the ball in continuing the game. The other teammates can also use this function in selecting a desirable state for their positioning. That is, a player choosing a position on the field does not have to have a single best point, given the current state. It may consider multiple points, each of which is again dependent on the action. For example, the player may consider state (point) S1 when moving to the left wing with action a1, and state (point) S2 when blocking a nearest opponent with action a2. Each of the resultant states s1 = result(a1) and s2 = result(a2) are compared with the corresponding desirable states suggested by the tactics S1 and S2, and the action achieving the best proximity in terms of the metric D is selected. The diversification in positioning achieves both mobility (by enabling better passes to these teammates) and field control \u2014 by taking key points and blocking key directions.\nThe idea of field control can be traced to a generic framework describing abstract spatiotemporal relationships described by Dylla et al. [8]. The latter work did not menion field control explicitly but argued that a reachability relation is needed to express spatial relationships between the players and the ball. They suggested to use Voronoi diagrams: a Voronoi diagram is the partitioning of a plane with n points into n convex polygons such that each polygon contains exactly one point and every point in the given polygon is closer to its central point than any other [8]. This was further developed by Akiyama et al. who used a dual representation of Voronoi diagrams \u2014 the Delaunay triangulation [2,3]."}, {"heading": "2.5 Example", "text": "Figure 1 illustrates the concept of action-dependent evaluation. The player controlling the ball (left team, number 11) has several options available: it can dribble in a general forwardleft direction, pass to teammates 7 and 10 (in a number of ways, including direct and lead passes), etc. We consider three choices (shown by arrows): dribble forward-left, pass to the left to teammate 7, and pass to the right to teammate 10. The agent2d\u2019s evaluation function would most likely rated the dribble higher, as the resultant state (the arrow-head) has a larger X-coordinate and a smaller distance from the opponent\u2019s goal than the alternatives. The new evaluation function identifies two desirable states instead, shown by a small rectangle to the left of player 11, and a small circle to its right. The rectangle defines the tactic suggesting to develop an attack to the left and through the center, and the circle corresponds to the tactic preferring the right wing. The dribble and pass to teammate 7 are partitioned to the first tactic (rectangle), and the pass to teammate 10 belongs to the second tactic (circle). Each of these actions is rated by proximity of their resultant states (the arrow-heads) to the rectangle and circle respectively. The pass to number 10 has the smaller distance between the resultant and desirable states, and is then selected."}, {"heading": "3 Development and Results", "text": "The proposed approach was implemented in Gliders2012 \u2014 a new team based on agent2d [1]. We carried out multiple iterative experiments, matching Gliders2012 up against the agent2d (HELIOS Base team), and achieving \u2248 +4.0 goal difference, typically averaged over 100 games. In doing that, some of the most important tools for development were a set of scripts to automate running tournaments. Average results of such a tournament are a useful indicator if a proposed change in the team is really helpful or possibly hurts performance. Oftentimes, it is helpful to briefly watch some of the games that are stored on the simulation server. To make these matches available for viewing over web browser, the current existing solution is to convert them into flash (SWF) format. Even though source code for this program is available [9], a required library to create SWF files is proprietary and not anymore distributed. As a result, this option is not available for new teams.\nWe created a new set of programs to visualize matches in a browser window. This time, the main implementation uses HTML5 / javascript, with a pre-processing step to create simple-to-parse log files that can be conveniently transferred over the internet. The current state of the visualization basic but functional, and released by us to the RoboCup community\nin the hope that it will be improved over time, re-released, and be used for showing games to general public in future competitions. Figure 2 shows a screenshot.\nThe steps required to produce a HTML5 visualization from a stored log file (*.rcg.gz) are as follows:\n1. Convert the log file into an intermediate text file. The required tool for this is distributed with the soccer server (rcg2txt).\n2. Convert the intermediate text file into a *.replay file. This is also a text file, but with only the essential information to display the match.\n3. The *.replay file then can be loaded from a PHP / HTML file over a browser. The entire log player consists of a PHP / HTML script and a few javascript and CSS files.\nThe log player is available (along with the necessary converter) at the Gliders2012 web page www.oliverobst.eu/research/robotics-gliders2012-simulation-league-robocup-team."}, {"heading": "3.1 Results at RoboCup 2012", "text": "Eventually, our team also participated in RoboCup 2012 with convincing results. The tournament was played over several rounds, where Gliders2012 proceeded to the semi-finals, resulting in a 4th place. Our detailed results are as follows:\nSeeding round (Round 0) A seeding round was played, with the 18 participating teams distributed over 4 groups of 4 or 5 teams each. The results of these groups were to be used to seed all teams more fairly for the subsequent round 1.\nGliders2012 were allocated to group C with 4 other teams: GPR-2D (Brazil), MarIik (Iran), AUT 2D (Iran), and Riton (also Iran). Gliders2012 ended up leading the group, with one draw against MarIik, and all other matches won (Table 1).\nRound 1 All teams were to proceed to round 1. Round 1 was played in two groups of 9 (groups E and F), with Gliders2012 in group E. Other teams in group E were: Warthog, GPR2D, and ITAndroids (all Brazil), Oxsy (Romania), GDUT TiJi, YuShan2012, and WrightEagle (all China), as well as Riton (Iran).\nGliders2012 won 3 matches, drew 3, and lost 2, resulting in a 3rd place ranking in this group (Table 2).\nRound 2 The top 6 teams of both round 1 groups proceeded to round 2, groups G and H. Gliders2012 played in group G, against Riton and MarIik (Iran), FCPortugal (Portugal), robOTTO (Germany), and WrightEagle (China). Gliders2012 won 3 matches, and lost 2, and ended up ranked 4th with an equal number of points to the second and third place.\nRound 3 The top 4 teams of both groups in round 2 proceeded to round 3, groups I and J. The group I teams were HELIOS2012 (Japan), Gliders2012, AUT 2D (Iran), and robOTTO\nTable 1. Round 0, group C results.\nPlace Team Points Total Score W D L\n1 Gliders2012 10 9:2 3 1 0 2 MarIiK 8 4:2 2 2 0 3 GPR-2D 4 2:3 1 1 2 4 AUT 2D 4 2:3 1 1 2 5 Riton 1 0:7 0 1 3\nTable 2. Round 1, group E results.\nPlace Team Points Total Score W D L\n1 WrightEagle 24 53 : 7 8 0 0 2 YuShan2012 15 15 : 12 4 3 1 3 Gliders2012 12 18 : 17 3 3 2 4 ITAndroids 11 13 : 19 3 2 3 5 Riton 10 10 : 10 2 4 2 6 GDUT TiJi 10 18 : 19 3 1 4 7 GPR-2D 9 5 : 10 2 3 3 8 Oxsy 6 10 : 27 2 0 6 9 Warthog 2 4 : 25 0 2 6\nTable 3. Round 2, group G results.\nPlace Team Points Total Score W D L\n1 WrightEagle 15 19 : 3 5 0 0 2 robOTTO 9 9 : 7 3 0 2 3 MarIiK 9 7 : 8 3 0 2 4 Gliders2012 9 9 : 11 3 0 2 5 FCPortugal 3 7 : 11 1 0 4 6 Riton 0 8 : 19 0 0 5\nTable 4. Round 3, group I results.\nPlace Team Points Total Score W D L\n1 HELIOS2012 9 4 : 0 3 0 0 2 Gliders2012 6 5 : 1 2 0 1 3 AUT 2D 3 1 : 3 1 0 2 4 robOTTO 0 0 : 6 0 0 3\n(Germany). With two won matches and one lost match in this group, Gliders2012 proceeded to the semi-finals.\nSemi-Finals and 3rd place match Gliders played WrightEagle for the semi-finals (2 matches), both 0:2. The subsequent match for the third place was lost against MarIiK (0:1). HELIOS2012 won the tournament, WrightEagle became runner-up."}, {"heading": "4 Conclusion", "text": "We described a novel mechanism utilizing action-dependent evaluation functions, having applied it in the RoboCup Simulation 2D. The mechanism can be contrasted with some well known constructive models used by belief revision and belief update [14]. The approach also allowed us to draw parallels with evaluation functions employed by chess-playing computers, in terms of mobility, field control, tropism, etc. The evaluation function that varies desirable states dependent on contemplated actions is applicable in both ball-controlling and positioning scenarios. The tactics that correspond to multiple desirable states are not imposed in a top-down fashion, but rather contribute to the evaluation via these desirable states.\nOur proposed approach and its implementation has shown to be successful both in many experiments as well as during tournament.\nAcknowledgments The Authors are thankful to Valentina Cupac, Andrew Curline, Tim D\u2019Adam, Ivan Duong, James Nugent, Tom Stewart for their contribution. Team logo was created by Matthew Chadwick. Some of the Authors have been involved with RoboCup Simulation 2D in the past, however the code of their previous teams (Cyberoos and RoboLog, see, e.g., [17,13]) is not used in Gliders2012."}], "references": [{"title": "Multi-agent positioning mechanism in the dynamic environment", "author": ["Hidehisa Akiyama", "Itsuki Noda"], "venue": "RoboCup 2007: Robot Soccer World Cup XI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Helios2010 team description", "author": ["Hidehisa Akiyama", "Hiroki Shimora"], "venue": "In RoboCup 2010: Robot Soccer World Cup XIV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "RoboCup: Today and tomorrow \u2013 What we have have learned", "author": ["Minoru Asada", "Hiroaki Kitano", "Itsuki Noda", "Manuela Veloso"], "venue": "Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Wrighteagle and ut austin villa: Robocup 2011 simulation league champions", "author": ["Aijun Bai", "Xiaoping Chen", "Patrick MacAlpine", "Daniel Urieli", "Samuel Barrett", "Peter Stone"], "venue": "In RoboCup 2011: Robot Soccer World Cup XV, Lecture Notes in Artificial Intelligence. Springer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Flexible synchronisation within RoboCup environment: A comparative analysis", "author": ["Marc Butler", "Mikhail Prokopenko", "Thomas Howard"], "venue": "In RoboCup 2000: Robot Soccer World Cup IV,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Users Manual: RoboCup Soccer Server \u2014 for Soccer Server Version 7.07 and Later", "author": ["Mao Chen", "Klaus Dorer", "Ehsan Foroughi", "Fredrick Heintz", "ZhanXiang Huang", "Spiros Kapetanakis", "Kostas Kostiadis", "Johan Kummeneje", "Jan Murray", "Itsuki Noda", "Oliver Obst", "Pat Riley", "Timo Steffens", "Yi Wang", "Xiang Yin"], "venue": "The RoboCup Federation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Computers in Sport, chapter Approaching a Formal Soccer Theory from the Behavior Specification in Robotic Soccer, pages 161\u2013186", "author": ["Frank Dylla", "Alexander Ferrein", "Gerhard Lakemeyer", "Jan Murray", "Oliver Obst", "Thomas R\u00f6fer", "Stefan Schiffer", "Frieder Stolzenburg", "Ubbo Visser", "Thomas Wagner"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "robocup2flash version 0.3", "author": ["Thilo Girmann", "Oliver Obst"], "venue": "http://robolog.cvs. sourceforge.net/viewvc/robolog/robocup2flash/,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "The RoboCup Synthetic Agent Challenge 97", "author": ["Hiroaki Kitano", "Milind Tambe", "Peter Stone", "Manuela M. Veloso", "Silvia Coradeschi", "Eiichi Osawa", "Hitoshi Matsubara", "Itsuki Noda", "Minoru Asada"], "venue": "Robot Soccer World Cup I,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "Chess Programming Part VI: Evaluation Functions", "author": ["Fran\u00e7ois Dominic Laram\u00e9e"], "venue": "http://www.gamedev.net/page/resources/_/technical/artificial-intelligence/ chess-programming-part-vi-evaluation-functions-r1208,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "The RoboCup Soccer Server and CMUnited Clients: Implemented Infrastructure for MAS Research", "author": ["Itsuki Noda", "Peter Stone"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Flexible coordination of multiagent team behavior using HTN planning", "author": ["Oliver Obst", "Joschka Boedecker"], "venue": "RoboCup 2005: Robot Soccer World Cup IX, Lecture Notes in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Revision vs. update: Taking a closer look", "author": ["Pavlos Peppas", "Abhaya C. Nayak", "Maurice Pagnucco", "Norman Y. Foo", "Rex Bing Hung Kwok", "Mikhail Prokopenko"], "venue": "12th European Conference on Artificial Intelligence, Budapest,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Gliders2012: Tactics with action-dependent evaluation functions", "author": ["Mikhail Prokopenko", "Oliver Obst", "Peter Wang", "Jason Held"], "venue": "In RoboCup 2012:", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Relating the entropy of joint beliefs to multi-agent coordination", "author": ["Mikhail Prokopenko", "Peter Wang"], "venue": "RoboCup 2002: Robot Soccer World Cup VI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Evaluating team performance at the edge of chaos", "author": ["Mikhail Prokopenko", "Peter Wang"], "venue": "RoboCup 2003: Robot Soccer World Cup VII,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Situation based strategic positioning for coordinating a team of homogeneous agents", "author": ["L\u00fa\u0131s Paulo Reis", "Nuno Lau", "Eugenio Oliveira"], "venue": "Workshop and additional contributions),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Layered disclosure: Revealing agents\u2019 internals", "author": ["Patrick Riley", "Peter Stone", "Manuela Veloso"], "venue": "International Workshop,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Defining and using ideal teammate and opponent models", "author": ["Peter Stone", "Patrick Riley", "Manuela Veloso"], "venue": "In Proceedings of the Twelfth Annual Conference on Innovative Applications of Artificial Intelligence,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "Task decomposition, dynamic role assignment, and lowbandwidth communication for real-time strategic teamwork", "author": ["Peter Stone", "Manuela Veloso"], "venue": "Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Team-partitioned, opaque-transition reinforced learning", "author": ["Peter Stone", "Manuela M. Veloso"], "venue": "Robot Soccer World Cup II,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}], "referenceMentions": [{"referenceID": 8, "context": "The RoboCup Simulation League [10] incorporates several challenging features, setting a benchmark for Artificial Intelligence (AI).", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "\u2013 distributed client/server system running on a network, leading to fragmented, localized and imprecise (noisy and latent) information about the environment (field) [12]; \u2013 concurrent communication with a medium-sized number of agents [21]; \u2013 heterogeneous sensory data (visual, auditory, kinetic) and limited range of basic commands/effectors (turn, kick, dash, .", "startOffset": 165, "endOffset": 169}, {"referenceID": 19, "context": "\u2013 distributed client/server system running on a network, leading to fragmented, localized and imprecise (noisy and latent) information about the environment (field) [12]; \u2013 concurrent communication with a medium-sized number of agents [21]; \u2013 heterogeneous sensory data (visual, auditory, kinetic) and limited range of basic commands/effectors (turn, kick, dash, .", "startOffset": 235, "endOffset": 239}, {"referenceID": 17, "context": ") [19]; \u2013 asynchronous perception-action activity and limited window of opportunity to perform an action [6]; \u2013 autonomous decision-making under constraints enforced by teamwork (collaboration) and opponent (competition) [20]; \u2013 conflicts between reactivity and deliberation [18]; \u2013 no centralized controllers and centralized world model (no global vision, etc.", "startOffset": 2, "endOffset": 6}, {"referenceID": 4, "context": ") [19]; \u2013 asynchronous perception-action activity and limited window of opportunity to perform an action [6]; \u2013 autonomous decision-making under constraints enforced by teamwork (collaboration) and opponent (competition) [20]; \u2013 conflicts between reactivity and deliberation [18]; \u2013 no centralized controllers and centralized world model (no global vision, etc.", "startOffset": 105, "endOffset": 108}, {"referenceID": 18, "context": ") [19]; \u2013 asynchronous perception-action activity and limited window of opportunity to perform an action [6]; \u2013 autonomous decision-making under constraints enforced by teamwork (collaboration) and opponent (competition) [20]; \u2013 conflicts between reactivity and deliberation [18]; \u2013 no centralized controllers and centralized world model (no global vision, etc.", "startOffset": 221, "endOffset": 225}, {"referenceID": 16, "context": ") [19]; \u2013 asynchronous perception-action activity and limited window of opportunity to perform an action [6]; \u2013 autonomous decision-making under constraints enforced by teamwork (collaboration) and opponent (competition) [20]; \u2013 conflicts between reactivity and deliberation [18]; \u2013 no centralized controllers and centralized world model (no global vision, etc.", "startOffset": 275, "endOffset": 279}, {"referenceID": 14, "context": ") [16,17].", "startOffset": 2, "endOffset": 9}, {"referenceID": 15, "context": ") [16,17].", "startOffset": 2, "endOffset": 9}, {"referenceID": 2, "context": "[4], chess and RoboCup differ in a few key elements: environment (static vs dynamic), state change (turn-taking vs real-time), information accessibility (complete vs incomplete), sensor readings (symbolic vs non-symbolic), and control (central vs distributed).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "One may argue that superior performance of recent world champions in the RoboCup 2D Simulation League [3,5] may be attributed, at least partially, to sophisticated evaluation functions employed by these teams.", "startOffset": 102, "endOffset": 107}, {"referenceID": 3, "context": "One may argue that superior performance of recent world champions in the RoboCup 2D Simulation League [3,5] may be attributed, at least partially, to sophisticated evaluation functions employed by these teams.", "startOffset": 102, "endOffset": 107}, {"referenceID": 12, "context": "In this short paper we describe a novel mechanism utilizing action-dependent evaluation functions, comparing it to some well known constructive models used by belief revision and belief update [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 5, "context": "The experiments are carried out using a new simulated soccer team for the RoboCup soccer 2D simulator [7], Gliders2012.", "startOffset": 102, "endOffset": 105}, {"referenceID": 9, "context": "As argued by Laram\u00e9e, in chess \u201cthe evaluation function, is unique in a very real sense: while search techniques are pretty much universal and move generation can be deducted from a game\u2019s rules and no more, evaluation requires a deep and thorough analysis of strategy\u201d [11].", "startOffset": 270, "endOffset": 274}, {"referenceID": 13, "context": "Moreover, we suggested [15] not only that the most desirable state can change from one cycle to another, but also that a player may entertain multiple desirable states at any given time (cycle).", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": ", with actiondependent features generalizing state space proposed by Stone and Veloso [22].", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "Another interesting point is the analogy between multiple desirable states unified by the proposed evaluation function and the constructive model for belief update and belief revision [14].", "startOffset": 184, "endOffset": 188}, {"referenceID": 12, "context": "[14] have shown that the model for belief update uses multiple systems of spheres (one for each possible world), finds in parallel the spheres consistent with the new data that are nearest to their respective central possible worlds, and collects possible worlds within these spheres.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "They suggested to use Voronoi diagrams: a Voronoi diagram is the partitioning of a plane with n points into n convex polygons such that each polygon contains exactly one point and every point in the given polygon is closer to its central point than any other [8].", "startOffset": 259, "endOffset": 262}, {"referenceID": 0, "context": "who used a dual representation of Voronoi diagrams \u2014 the Delaunay triangulation [2,3].", "startOffset": 80, "endOffset": 85}, {"referenceID": 1, "context": "who used a dual representation of Voronoi diagrams \u2014 the Delaunay triangulation [2,3].", "startOffset": 80, "endOffset": 85}, {"referenceID": 7, "context": "Even though source code for this program is available [9], a required library to create SWF files is proprietary and not anymore distributed.", "startOffset": 54, "endOffset": 57}, {"referenceID": 12, "context": "The mechanism can be contrasted with some well known constructive models used by belief revision and belief update [14].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": ", [17,13]) is not used in Gliders2012.", "startOffset": 2, "endOffset": 9}, {"referenceID": 11, "context": ", [17,13]) is not used in Gliders2012.", "startOffset": 2, "endOffset": 9}], "year": 2012, "abstractText": "The RoboCup 2D Simulation League incorporates several challenging features, setting a benchmark for Artificial Intelligence (AI). In this paper we describe some of the ideas and tools around the development of our team, Gliders2012. In our description, we focus on the evaluation function as one of our central mechanisms for action selection. We also point to a new framework for watching log files in a web browser that we release for use and further development by the RoboCup community. Finally, we also summarize results of the group and final matches we played during RoboCup 2012, with Gliders2012 finishing 4th out of 19 teams.", "creator": "LaTeX with hyperref package"}}}