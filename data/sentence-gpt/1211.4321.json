{"id": "1211.4321", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2012", "title": "Bayesian nonparametric models for ranked data", "abstract": "We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items. Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process. We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation. For these tasks, we introduce a linear model with an efficient posterior-mapping framework with a sparse distribution and a partial model for Bayesian inference. For all of the tasks, we develop a robust set of Bayesian stochastic models for each category. These models are based on an orthogonal approach. We implement two standard ways of calculating posterior estimates for the number of choice items. For the Bayesian Bayesian inference of Bayesian stochastic models, we integrate the Bayesian Bayesian covariance analysis with a probabilistic logarithmic estimation approach. As such, we integrate the Bayesian covariance approach with the Bayesian covariance and then use the Bayesian covariance approach to estimate the model. A Bayesian parameterized Bayesian covariance analysis with an efficient Bayesian covariance analysis is performed with a probabilistic logarithmic estimation. This method is also used to model the distributions and the distributions. The Bayesian covariance analysis is also performed to show the distribution of the posterior estimates for Bayesian inference of nonparametric covariance. Finally, we define an inferential probabilistic logarithmic estimation (with the posterior-mapping framework), using the Bayesian covariance approach. The Bayesian covariance analysis has been described in a previous post. The posterior-mapping framework, which has been developed in the past, has been applied in other areas, such as the probability distribution and the nonparametric covariance analysis.", "histories": [["v1", "Mon, 19 Nov 2012 07:40:51 GMT  (556kb,D)", "http://arxiv.org/abs/1211.4321v1", "NIPS - Neural Information Processing Systems (2012)"]], "COMMENTS": "NIPS - Neural Information Processing Systems (2012)", "reviews": [], "SUBJECTS": "stat.ML cs.LG stat.ME", "authors": ["francois caron", "yee whye teh"], "accepted": true, "id": "1211.4321"}, "pdf": {"name": "1211.4321.pdf", "metadata": {"source": "CRF", "title": "Bayesian nonparametric models for ranked data", "authors": ["Fran\u00e7ois Caron", "Yee Whye Teh"], "emails": [], "sections": [{"heading": null, "text": "IS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- 81\n40 --\nFR +E\nN G\nRESEARCH REPORT N\u00b0 8140 November 19, 2012\nProject-Team ALEA\nBayesian nonparametric models for ranked data Fran\u00e7ois Caron, Yee Whye Teh\nar X\niv :1\n21 1.\n43 21\nv1 [\nst at\n.M L\n] 1\n9 N\nov 2\n01 2\nRESEARCH CENTRE BORDEAUX \u2013 SUD-OUEST\n351, Cours de la Lib\u00e9ration B\u00e2timent A 29 33405 Talence Cedex\nBayesian nonparametric models for ranked data\nFranc\u0327ois Caron\u2217, Yee Whye Teh\u2020\nProject-Team ALEA\nResearch Report n\u00b0 8140 \u2014 November 19, 2012 \u2014 18 pages\nAbstract: We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that can handle an infinite number of choice items. Our framework is based on the theory of random atomic measures, with the prior specified by a gamma process. We derive a posterior characterization and a simple and effective Gibbs sampler for posterior simulation. We develop a time-varying extension of our model, and apply it to the New York Times lists of weekly bestselling books.\nKey-words: choice models, generalized Bradley-Terry model, Plackett-Luce model, gamma process, Markov Chain Monte Carlo\n\u2217 INRIA Bordeaux Sud-Ouest, Institut de Mathe\u0301matiques de Bordeaux, University of Bordeaux, France \u2020 Department of Statistics, Oxford University, United Kingdom\nMode\u0300les baye\u0301siens non parame\u0301triques pour les donne\u0301es de rang Re\u0301sume\u0301 : On s\u2019inte\u0301resse dans ce rapport a\u0300 une extension baye\u0301sienne non parame\u0301trique du mode\u0300le de Plackett-Luce pour les donne\u0301es de rang, pouvant traiter un nombre potentiellement infini d\u2019e\u0301le\u0301ments. Notre cadre se base sur la the\u0301orie des mesures comple\u0300tement ale\u0301atoires, avec comme a priori un processus de gamma. Nous de\u0301rivons une caracte\u0301risation de la loi a posteriori et un e\u0301chantillonneur de Gibbs simple pour approcher la loi a posteriori. Nous de\u0301veloppons e\u0301galement une version dynamique de notre mode\u0300le, et l\u2019appliquons aux listes hebdomadaires des 20 meilleures ventes du New York Times.\nMots-cle\u0301s : Mode\u0300les de choix, mode\u0300le de Bradley-Terry ge\u0301ne\u0301ralise\u0301, mode\u0300le de Plackett-Luce, processus de gamma, me\u0301thodes de Monte Carlo par cha\u0131\u0302ne de Markov\nBayesian nonparametric models for ranked data 3"}, {"heading": "1 Introduction", "text": "Data in the form of partial rankings, i.e. in terms of an ordered list of the top-m items, arise in many contexts. For example, in this paper we consider datasets consisting of the top 20 bestselling books as published each week by the New York Times. The Plackett-Luce model [1, 2] is a popular model for modeling such partial rankings of a finite collection ofM items. It has found many applications, including choice modeling [3], sport ranking [4], and voting [5]. [6, Chap. 9] provides detailed discussions on the statistical foundations of this model.\nIn the Plackett-Luce model, each item k \u2208 [M ] = {1, . . . ,M} is assigned a positive rating parameter wk, which represents the desirability or rating of a product in the case of choice modeling, or the skill of a player in sport rankings. The Plackett-Luce model assumes the following generative story for a top-m list \u03c1 = (\u03c11, . . . , \u03c1m) of items \u03c1i \u2208 [M ]: At each stage i = 1, . . . ,m, an item is chosen to be the ith item in the list from among the items that have not yet appeared, with the probability that \u03c1i is selected being proportional to its desirability w\u03c1i . The overall probability of a given partial ranking \u03c1 is then:\nP (\u03c1) = m\u220f i=1 w\u03c1i(\u2211M k=1 wk ) \u2212 (\u2211i\u22121 j=1 w\u03c1j ) . (1)\nwith the denominator in (1) being the sum over all items not yet selected at stage i. In many situations the collection of available items can be very large and potentially unknown. In this case, a nonparametric approach can be sensible, where the pool of items is assumed to be infinite and the model allows for the possibility of items not observed in previous top-m lists to appear in new ones. In this paper we propose such a Bayesian nonparametric Plackett-Luce model. Our approach is built upon recent work on Bayesian inference for the (finite) Plackett-Luce model and its extensions [7, 8, 9]. Our model assumes the existence of an infinite pool of items {Xk}\u221ek=1, each with its own rating parameter, {wk}\u221ek=1. The probability of a top-m list of items, say (X\u03c11 , . . . , X\u03c1m), is then a direct extension of the finite case (1):\nP (X\u03c11 , . . . , X\u03c1m) = m\u220f i=1 w\u03c1i(\u2211\u221e k=1 wk ) \u2212 (\u2211i\u22121 j=1 w\u03c1j ) . (2)\nTo formalize the framework, a natural representation to encapsulate the pool of items along with their ratings is using an atomic measure:\nG = \u221e\u2211 k=1 wk\u03b4Xk (3)\nUsing this representation, note that the top item X\u03c11 in our list is simply a draw from the probability measure obtained by normalizing G, while subsequent items in the top-m list are draws from probability measures obtained by first removing from G the atoms corresponding to previously picked items and normalizing. Described this way, it is clear that the Plackett-Luce model is basically a partial size-biased permutation of the atoms in G [10], and the existing machinery of random measures and exchangeable random partitions [11] can be brought to bear on our problem.\nIn particular, in Section 2 we will use a gamma process as the prior over the atomic measureG. This is a completely random measure [12] with gamma marginals, such that the corresponding normalized probability measure is a Dirichlet process. We will show that with the introduction of a suitable set of auxiliary variables, we can characterize the posterior law ofG given observations of top-m lists distributed according to (2). A simple Gibbs sampler can then be derived to simulate from the posterior distribution. In Section 3 we develop a time-varying extension of our model and derive a simple and effective Gibbs sampler for posterior simulation. In Section 4 we apply our time-varying Bayesian nonparametric PlackettLuce model to the aforementioned New York Times bestsellers datasets, and conclude in Section 5.\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 4"}, {"heading": "2 A Bayesian nonparametric model for partial ranking", "text": "We start this section by briefly describing a Bayesian approach to inference in finite Plackett-Luce models [9], and taking the infinite limit to arrive at the nonparametric model. This will give good intuitions for how the model operates, before we rederive the same nonparametric model more formally using gamma processes. Throughout this paper we will suppose that our data consists of L partial rankings, with \u03c1` = (\u03c1`1, . . . , \u03c1`m) for ` \u2208 [L]. For notational simplicity we assume that all the partial rankings are length m."}, {"heading": "2.1 Finite Plackett-Luce model with gamma prior", "text": "Suppose we have M choice items, with item k \u2208 [M ] having a positive desirability parameter wk. A partial ranking \u03c1` = (\u03c1`1, . . . , \u03c1`m) can be constructed generatively by picking the ith item \u03c1`i at the ith stage for i = 1, . . . ,m, with probability proportional to w\u03c1`i as in (1). An alternative Thurstonian interpretation, which will be important in the following, is as follows: For each item k let z`k \u223c Exp(wk) be exponentially distributed with rate wk. Thinking of z`k as the arrival time of item k in a race, let \u03c1`i be the index of the ith item to arrive (the ith smallest value among (z`k)Mk=1). The resulting probability of \u03c1` can then be shown to still be (1). In this interpretation (z`k) can be understood as latent variables, and the EM algorithm can be applied to derive an algorithm to find a ML parameter setting for (wk)Mk=1 given multiple partial rankings. Unfortunately the posterior distribution of (z`k) given \u03c1` is difficult to compute directly, so we instead consider an alternative parameterization: Let Z`i = z\u03c1`i \u2212 z\u03c1` i\u22121 be the waiting time for the ith item to arrive after the i \u2212 1th item (with z\u03c1`0 defined to be 0). Then it can be shown that the joint probability is:\nP ((\u03c1`) L `=1, (Z`i) L,m `=1,i=1|(wk) M k=1) = L\u220f `=1 m\u220f i=1 w\u03c1`i exp ( \u2212Z`i (\u2211M k=1 wk \u2212 \u2211i\u22121 j=1 w\u03c1`j )) (4)\nNote that the posterior of (Z`i)mi=1 is simply factorized with Z`i|\u03c1, w \u223c Exp( \u2211M k=1 wk \u2212 \u2211i\u22121 j=1 w\u03c1`j ), and the ML parameter setting can be easily derived as well. Taking a further step, we note that a factorized gamma prior over (wk) is conjugate to (4), say wk \u223c Gamma( \u03b1M , \u03c4) with hyperparameters \u03b1, \u03c4 > 0. Now Bayesian inference can be carried out either with a VB EM algorithm, or a Gibbs sampler. In this paper we shall consider only Gibbs sampling algorithms. In this case the parameter updates are of the form\nwk|(\u03c1`), (Z`i), (wk\u2032)k\u2032 6=k \u223c Gamma ( \u03b1 M + nk, \u03c4 + \u2211L `=1 \u2211m i=1 \u03b4`ikZ`i ) (5)\nwhere nk is the number of occurrences of item k among the observed partial rankings, and \u03b4`ik = 0 if there is a j < i with \u03c1`j = k and 1 otherwise. These terms arise by regrouping those in the exponential in (4).\nA nonparametric Plackett-Luce model can now be easily derived by taking the limit as the number of choice items M \u2192 \u221e. For those items k that have appeared among the observed partial rankings, the limiting conditional distribution (5) is well defined since nk > 0. For items that did not appear in the observations, (5) becomes degenerate at 0. Instead we can define w\u2217 = \u2211 k:nk=0\nwk to be the total desirability among all infinitely many previously unobserved items, and show that\nw\u2217|(\u03c1`), (Z`i), (wk)k:nk>0 \u223c Gamma ( \u03b1, \u03c4 + \u2211L `=1 \u2211m i=1 Z`i ) (6)\nThe Gibbs sampler thus alternates between updating (Z`i), and updating the ratings of the observed items (wk)k:nk>0 and of the unobserved ones w\u2217. This nonparametric model allows us to estimate the probability of seeing new items appearing in future partial rankings in a consistent manner. While intuitive, this\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 5\nderivation is ad hoc in the sense that it arises as the infinite limit of the Gibbs sampler for finite models, and is unsatisfying as it did not directly capture the structure of the underlying infinite dimensional object, which we will show in the next subsection to be a gamma process."}, {"heading": "2.2 A Bayesian nonparametric Plackett-Luce model", "text": "Let X be a measurable space of choice items. A gamma process is a completely random measure over X with gamma marginals. Specifically, it is a random atomic measure of the form (3), such that for each measurable subset A, the (random) mass G(A) is gamma distributed. Assuming that G has no fixed atoms (that is, for each element x \u2208 X we have G({x}) = 0 with probability one) and that the atom locations {Xk} are independent of their masses {wk}, it can be shown that such a random measure can be constructed as follows: each Xk is iid according to a base distribution H (which we assume is non-atomic with density h(x)), while the set of masses {wk} is distributed according to a Poisson process over R+ with intensity \u03bb(w) = \u03b1w\u22121e\u2212w\u03c4 where \u03b1 > 0 is the concentration parameter and \u03c4 > 0 the inverse scale. We write this as G \u223c \u0393(\u03b1, \u03c4,H). Under this parametrization, we have that G(A) \u223c Gamma(\u03b1H(A), \u03c4).\nEach atomXk is a choice item, with its masswk > 0 corresponding to the desirability parameter. The Thurstonian view described in the finite model can be easily extended to the nonparametric one, where a partial ranking (X\u03c1`1 . . . X\u03c1`m) can be generated as the first m items to arrive in a race. In particular, for each atom Xk let z`k \u223c Exp(wk) be the time of arrival of Xk and X\u03c1`i the ith item to arrive. The first m items to arrive (X\u03c1`1 . . . X\u03c1`m) then constitutes our top-m list, with probability as given in (2). Again reparametrizing using inter-arrival durations, let Z`i = z\u03c1`i \u2212 z\u03c1`i\u22121 for i = 1, 2, . . . (with z\u03c10 = 0). Then the joint probability is:\nP ((X\u03c1`i) m i=1, (Z`i) m i=1|G) = P ((z\u03c1`1 . . . z\u03c1`m), and z`k > z\u03c1`m for all k 6\u2208 {\u03c1`1, . . . , \u03c1`m}) (7)\n= ( m\u220f i=1 w\u03c1`ie \u2212w\u03c1`iz\u03c1`i )( \u220f k 6\u2208{\u03c1`i}mi=1 e\u2212wkz\u03c1`m ) = m\u220f i=1 w\u03c1`i exp ( \u2212Z`i ( \u221e\u2211 k=1 wk \u2212 i\u22121\u2211 j=1 w\u03c1`j )) Marginalizing out (Z`i)mi=1 gives the probability of (X\u03c1`i) m i=1 in (2). Further, conditional on \u03c1` it is seen that the inter-arrival durations Z`1 . . . Z`m are mutually independent and exponentially distributed:\nZ`i|(X\u03c1`i)mi=1, G \u223c Exp ( \u221e\u2211 k=1 wk \u2212 i\u22121\u2211 j=1 w\u03c1`j ) (8)\nThe above construction is depicted on Figure 1(left). We visualize on right some top-m lists generated from the model, with \u03c4 = 1 and different values of \u03b1."}, {"heading": "2.3 Posterior characterization", "text": "Consider a number L of partial rankings, with the `th list denoted Y` = (Y`1 . . . Y`m`) , for ` \u2208 [L]. While previously our top-m list (X\u03c11 . . . X\u03c1m) consists of an ordered list of the atoms in G. Here G is unobserved and (Y`1 . . . Y`m`) is simply a list of observed choice items, which is why they were not expressed as an ordered list of atoms in G. The task here is then to characterize the posterior law of G under a gamma process prior and supposing that the observed partial rankings were drawn iid from the nonparametric Plackett-Luce model given G. Re-expressing the conditional distribution (2) of Y` given G, we have:\nP (Y`|G) = m\u220f\u0300 i=1 G({Y`i}) G(X\\{Y`1 . . . Y` i\u22121})\n(9)\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 6\nu \u03c1 1u \u03c1\n2 u \u03c1\n3\nE\nAs before, for each `, we will also introduce a set of auxiliary variables Z` = (Z`1 . . . Z`m`) (the interarrival times) that are conditionally mutually independent given G and Y`, with:\nZ`i|Y`, G \u223c Exp(G(X\\{Y`1, . . . , Y`i\u22121})) (10)\nThe joint probability of the item lists and auxiliary variables is then (c.f. (7)):\nP ((Y`, Z`) L `=1|G) = L\u220f `=1 m\u220f\u0300 i=1 G({Y`i}) exp(\u2212Z`iG(X\\{Y`1, . . . , Y` i\u22121})) (11)\nNote that under the generative process described in Section 2.2, there is positive probability that an item appearing in a list Y` appears in another list Y`\u2032 with `\u2032 6= `. Denote the unique items among all L lists by X\u22171 . . . X \u2217 K , and for each k = 1, . . . ,K let nk be the number of occurrences of X \u2217 k among the item lists. Finally define occurrence indicators\n\u03b4`ik = { 0 if \u2203j < i with Y`j = X\u2217k ; 1 otherwise.\n(12)\ni.e. \u03b4`ik is the indicator of the occurence that item X\u2217k does not appear at a rank lower than i in the `th list. Then the joint probability under the nonparametric Plackett-Luce model is:\nP ((Y`, Z`) L `=1|G) = K\u220f k=1 G({X\u2217k})nk \u00d7 L\u220f `=1 m\u220f\u0300 i=1 exp(\u2212Z`iG(X\\{Y`1, . . . , Y` i\u22121}))\n= exp ( \u2212G(X)\n\u2211 `i Z`i ) K\u220f k=1 G({X\u2217k})nk exp ( \u2212G({X\u2217k}) \u2211 `i (\u03b4`ik \u2212 1)Z`i ) (13)\nTaking expectation of (13) with respect to G using the Palm formula gives:\nTheorem 1 The marginal probability of the L partial rankings and auxiliary variables is:\nP ((Y`, Z`) L `=1) = e\n\u2212\u03c8( \u2211 `i Z`i) K\u220f k=1 h(X\u2217k)\u03ba ( nk, \u2211 `i \u03b4`ikZ`i ) (14)\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 7\nwhere \u03c8(z) is the Laplace transform of \u03bb,\n\u03c8(z) = \u2212 logE [ e\u2212zG(X) ] = \u222b R+ \u03bb(w)(1\u2212 e\u2212zw)dw = \u03b1 log ( 1 + z \u03c4 ) (15)\nand \u03ba(n, z) is the nth moment of the exponentially tilted Le\u0301vy intensity \u03bb(w)e\u2212zw:\n\u03ba(n, z) = \u222b R+ \u03bb(w)wne\u2212zwdw =\n\u03b1\n(z + \u03c4)n \u0393(n) (16)\nDetails are given in the appendix. Another application of the Palm formula now allows us to derive a posterior characterisation of G:\nTheorem 2 Given the observations and associated auxiliary variables (Y`, Z`)L`=1, the posterior law of G is also a gamma process, but with atoms with both fixed and random locations. Specifically,\nG|(Y`, Z`)L`=1 = G\u2217 + K\u2211 k=1 w\u2217k\u03b4X\u2217k (17)\nwhere G\u2217 and w\u22171 , . . . , w \u2217 K are mutually independent. The law of G \u2217 is still a gamma process,\nG\u2217|(X`, Z`)L`=1 \u223c \u0393(\u03b1, \u03c4\u2217, h) \u03c4\u2217 = \u03c4 + \u2211 `i Z`i (18)\nwhile the masses have distributions, w\u2217k|(Y`, Z`)L`=1 \u223c Gamma ( nk, \u03c4 + \u2211 `i \u03b4`ikZ`i ) (19)"}, {"heading": "2.4 Gibbs sampling", "text": "Given the results of the previous section, a simple Gibbs sampler can now be derived, where all the conditionals are of known analytic form. In particular, we will integrate out all of G\u2217 except for its total mass w\u2217\u2217 = G\n\u2217(X). This leaves the latent variables to consist of the masses w\u2217\u2217 , (w\u2217k) and the auxiliary variables (Z`i). The update for Z`i is given by (10), while those for the masses are given in Theorem 2:\nGibbs update for Z`i: Z`i|rest \u223c Exp ( w\u2217\u2217 + \u2211 k \u03b4`ikw \u2217 k ) (20) Gibbs update for w\u2217k: w \u2217 k|rest \u223c Gamma ( nk, \u03c4 + \u2211 `i \u03b4`ikZ`i ) (21) Gibbs update for w\u2217\u2217: w \u2217 \u2217|rest \u223c Gamma ( \u03b1, \u03c4 + \u2211 `i Z`i ) (22)\nNote that the auxiliary variables are conditionally independent given the masses and vice versa. Hyperparameters of the gamma process can be simply derived from the joint distribution in Theorem 1. Since the marginal probability of the partial rankings is invariant to rescaling of the masses, it is sufficient to keep \u03c4 fixed at 1. As for \u03b1, if a Gamma(a, b) prior is placed on it, its conditional distribution is still gamma:\nGibbs update for \u03b1: \u03b1|rest \u223c Gamma ( a+K, b+ log ( 1 + \u2211 `i Z`i \u03c4 )) (23)\nNote that this update was derived with w\u2217\u2217 marginalized out, so after an update to \u03b1 it is necessary to immediately update w\u2217\u2217 via (22) before proceeding to update other variables.\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 8"}, {"heading": "3 Dynamic Bayesian nonparametric ranking models", "text": "In this section we develop an extension of the Bayesian nonparametric Plackett-Luce model to model time-varying rankings, where the rating parameters of items may change smoothly over time and reflected in a changing series of rankings. Given a series of times indexed by t = 1, 2, . . ., we may model the rankings at time t using a gamma process distributed random measure Gt as in Section 2.2, with Markov dependence among the sequence of measures (Gt) enabling dependence among the rankings over time."}, {"heading": "3.1 Pitt-Walker dependence model", "text": "We will construct a dependent sequence (Gt) which marginally follow a gamma process \u0393(\u03b1, \u03c4,H) using the construction of [13]. Suppose Gt \u223c \u0393(\u03b1, \u03c4,H). Since Gt is atomic, we can write it in the form:\nGt = \u221e\u2211 k=1 wtk\u03b4Xtk (24)\nDefine a random measure Ct with conditional law:\nCt|Gt = \u221e\u2211 k=1 ctk\u03b4Xtk ctk|Gt \u223c Poisson(\u03c6twtk) (25)\nwhere \u03c6t > 0 is a dependence parameter. Using the same method as in Section 2.3, we can show:\nProposition 3 Suppose the law of Gt is \u0393(\u03b1, \u03c4,H). The conditional law of Gt given Ct is then:\nGt = G \u2217 t + \u221e\u2211 k=1 w\u2217tk\u03b4Xtk (26)\nwhere G\u2217t and (w \u2217 tk) \u221e k=1 are all mutually independent. The law of G \u2217 t is given by a gamma process, while the masses are conditionally gamma,\nG\u2217t |Ct \u223c \u0393(\u03b1, \u03c4 + \u03c6t, H) w\u2217tk|Ct \u223c Gamma(ctk, \u03c4 + \u03c6t) (27)\nThe idea of [13] is to define the conditional law of Gt+1 given Gt and Ct to coincide with the conditional law of Gt given Ct as in Proposition 3. In other words, define\nGt+1 = G \u2217 t+1 + \u221e\u2211 k=1 wt+1,k\u03b4Xtk (28)\nwhere G\u2217t+1 \u223c \u0393(\u03b1, \u03c4 + \u03c6t, H) and wt+1,k \u223c Gamma(ctk, \u03c4 + \u03c6t) are mutually independent. If the prior law of Gt is \u0393(\u03b1, \u03c4,H), the marginal law of Gt+1 will be \u0393(\u03b1, \u03c4,H) as well when both Gt and Ct are marginalized out, thus maintaining a form of stationarity. Further, although we have described the process in order of increasing t, the joint law of Gt, Ct, Gt+1 can equivalently be described in the reverse order with the same conditional laws as above. Note that if ctk = 0, the conditional distribution of wt+1,k will be degenerate at 0. Hence Gt+1 has an atom at Xtk if and only if Ct has an atom at Xtk, that is, if ctk > 0. In addition, it also has atoms (those in G\u2217t+1) where Ct does not (nor does Gt). Finally, the parameter \u03c6t can be interpreted as controlling the strength of dependence between Gt+1 and Gt. Indeed it can be shown that\nE[Gt+1|Gt] = \u03c6t\n\u03c6t + \u03c4 Gt +\n\u03c4\n\u03c6t + \u03c4 H. (29)\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 9\nAnother measure of dependence can be gleaned by examining the \u201clifetime\u201d of an atom. Suppose X is an atom in G1 with mass w > 0. The probability that X is an atom in C2 with positive mass is 1 \u2212 exp(\u2212\u03c61w), in which case it has positive mass in G2 as well. Conversely, once it is not an atom, it will never be an atom in the future since the base distribution H is non-atomic. The lifetime of the atom is then the smallest t such that it is no longer an atom. We can show by induction that: (details in appendix)\nProposition 4 The probability that an atom X in G1 with mass w > 0 is dead at time t is given by\nP (Gt({X}) = 0|w) = exp(\u2212yt|1w)\nwhere yt|1 can be obtained by the recurrence yt|t\u22121 = \u03c6t\u22121 and yt|s\u22121 = yt|s\u03c6s\u22121\n\u03c6s\u22121+\u03c4+yt|s ."}, {"heading": "3.2 Posterior characterization and Gibbs sampling", "text": "Assume for simplicity that at each time step t = 1, . . . , T we observe one top-m list Yt = (Yt1, . . . , Ytm) (it trivially extends to multiple partial rankings of differing sizes). We extend the results of the previous section in characterizing the posterior and developing a Gibbs sampler for the dynamical model.\nSince each observed item at time t has to be an atom in its corresponding random measure Gt, and atoms in Gt can propagate to neighboring random measures via the Pitt-Walker dependence model, we conclude that the set of all observed items (through all times) has to include all fixed atoms in the posterior of Gt. Thus let X\u2217 = (X\u2217k), k = 1, . . . ,K be the set of unique items observed in Y1, . . . , YT , let ntk \u2208 {0, 1} be the number of times the item X\u2217k appears at time t, and let \u03c1t be defined as Yt = (X\u2217\u03c11 , . . . , X \u2217 \u03c1m). We write the masses of the fixed atoms as wtk = Gt({X \u2217 k}), while the total mass of all other random atoms is denoted wt\u2217 = Gt(X\\X\u2217). Note that wtk has to be positive on a random contiguous interval of time that includes all observations of X\u2217k\u2014it\u2019s lifetime\u2014but is zero outside of the interval. We also write ctk = Ct({X\u2217k}) and ct\u2217 = Ct(X\\X\u2217). As before, we introduce, for t = 1, . . . , T and i = 1, . . . ,m, latent variables\nZti \u223c Exp ( wt\u2217 + K\u2211 k=1 wtk \u2212 i\u22121\u2211 j=1 wt\u03c1j ) (30)\nEach iteration of the Gibbs sampler then proceeds as follows (details in appendix). The latent variables (Zti) are updated as above. Conditioned on the latent variables (Zti), (ctk) and (ct\u2217), we update the masses (wtk), which are independent and gamma distributed since all likelihoods are of gamma form. Note that the total masses (Gt(X)) are not likelihood identifiable, so we introduce an extra step to improve mixing by sampling them from the prior (integrating out (ctk), (ct\u2217)), scaling all masses along with it. Directly after this step we update (ctk), (ct\u2217). We update \u03b1 along with the random masses (wt\u2217) and (ct\u2217) efficiently using a forward-backward recursion. Finally, the dependence parameters (\u03c6t) are updated."}, {"heading": "3.3 Continuous time formulation using superprocesses", "text": "The dynamic model described in the previous section is formulated for discrete time data. When the time interval between ranking observations is not constant, it is desirable to work with dynamic models evolving over continuous-time instead, with the underlying random measures (Gt) defined over all t \u2208 R, but with observations at a discrete set of times t1 < t2 < \u00b7 \u00b7 \u00b7 . Here we propose a continuous-time model based on the Dawson-Watanabe superprocess [14, 15] (see also [16, 17, 18, 19]). This is a diffusion on the space of measures with the gamma process \u0393(\u03b1, \u03c4,H) as its equilibrium distribution. It is defined by a generator\nL = \u03be (\u222b G(dX) \u22022\n\u2202G(X)2 + \u03b1\n\u222b H(dX) \u2202\n\u2202G(X) \u2212 \u03c4\n\u222b G(dX) \u2202\n\u2202G(X) ) RR n\u00b0 8140\nBayesian nonparametric models for ranked data 10\nwith \u03be parametrizing the rate of evolution. Figure 2 gives a sample path, where we see that it is continuous but non-differentiable. For efficient inference, it is desirable to be able to integrate out all Gt\u2019s except those Gt1 , Gt2 , . . . at observation times. An advantage to using the Dawson-Watanabe superprocess is that, the conditional distribution of Gts given Gts\u22121 is remarkably simple [20]. In particular it is simply given by the discrete-time process of the previous section with dependence parameter \u03c6ts|ts\u22121 = \u03c4\ne\u03c4\u03be(ts\u2212ts\u22121)\u22121 . Thus the inference algorithm developed previously is directly applicable to\nthe continuous-time model too."}, {"heading": "4 Experiments", "text": "We apply the discrete-time dynamic Plackett-Luce model to the New York Times bestsellers data. These consist of the weekly top-20 best-sellers list from June 2008 to April 2012 in various categories. We consider here the categories paperback nonfiction (PN) and hardcover fiction (HF), for which respectively 249 and 916 books appear at least once in the top-20 lists over the 200 weeks. We consider that the correlation parameter \u03c6t = \u03c6 is constant over time, and assign flat improper priors p(\u03b1) \u221d 1/\u03b1 and p(\u03c6) \u221d 1/\u03c6. In order to take into account the publication date of a book, we do not consider books in the likelihood before their first appearance in a list. We run the Gibbs sampler with 10000 burn-in iterations followed by 10000 samples. Mean normalized weights for the more popular books in both categories are shown in Figure 3.\nThe model is able to estimate the weights associated to each book that appeared at least once, as well as the total weight associated to all other books, i.e. the probability that a new book enters at the first rank in the list, represented by the black curve. Moreover, the Bayesian approach enables us to have a measure of the uncertainty on the weights. The hardcover fiction category is characterized by rapid changes in successive lists, compared to the paperback nonfiction. This is quantified by the estimated value of the parameter \u03c6, which are respectively 85 \u00b1 20 and 140 \u00b1 40 for PN and HF. The estimated values of the shape parameter \u03b1 are 7\u00b1 1.5 and 2\u00b1 1 respectively."}, {"heading": "5 Discussion", "text": "We have proposed a Bayesian nonparametric Plackett-Luce model for ranked data. Our approach is based on the theory of atomic random measures, where we showed that the Plackett-Luce generative model corresponds exactly to a size-biased permutation of the atoms in the random measure. We characterized the posterior distribution, and derived a simple MCMC sampling algorithm for posterior simulation. Our approach can be see as a multi-stage generalization of posterior inference in normalized random measures [21, 22, 23], and can be easily extended from gamma processes to general completely random measures.\nWe also proposed dynamical extensions of our model for both discrete and continuous time data, and applied it to modeling the bestsellers\u2019 lists on the New York Times. Our dynamic extension may be useful\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 11\nfor modeling time varying densities or clusterings as well. In our experiments we found that our model is insufficient to capture the empirical observation that bestsellers often start off high on the lists and tail off afterwards, since our model has continuous sample paths. We adjusted for this by simply not including books in the model prior to their publication date. It may be possible to model this better using models with discontinuous sample paths, for example, the Orstein-Uhlenbeck approach of [24] where the process evolves via a series of discrete jump events instead of continuously."}, {"heading": "Acknowledgements", "text": "YWT thanks the Gatsby Charitable Foundation for generous funding.\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 12"}, {"heading": "A Proof of Theorem 1", "text": "The marginal probability (14) is obtained by taking the expectation of (13) with respect to G. Note however that (13) is a density, so to be totally precise here we need to work with the probability of infinitesimal neighborhoods around the observations instead, which introduces significant notational complexity. To keep the notation simple, we will work with densities, leaving it to the careful reader to verify that the calculations indeed carry over to the case of probabilities.\nP ((Y`, Z`) L `=1) =E [ P ((Y`, Z`) L `=1|G) ] =E [ e\u2212G(X) \u2211 `i Z`i\nK\u220f k=1 G({X\u2217k})nke\u2212G({X \u2217 k}) \u2211 `i(\u03b4`ik\u22121)Z`i\n]\nThe gamma prior on G = \u2211\u221e j=1 wj\u03b4Xj is equivalent to a Poisson process prior on N = \u2211\u221e j=1 \u03b4(wj ,Xj) defined over the space R+ \u00d7 X with mean intensity \u03bb(w)h(x). Then,\n=E e\u2212 \u222b wN(dw,dx)\u2211`i Z`i K\u220f k=1 \u221e\u2211 j=1 wnkj 1(Xj = X \u2217 k)e \u2212wj \u2211 `i(\u03b4`ik\u22121)Z`i  Applying the Palm formula for Poisson processes to pull the k = 1 term out of the expectation,\n= \u222b E e\u2212 \u222b w(N+\u03b4w\u22171 ,x\u22171 )(dw,dx)\u2211`i Z`i K\u220f k=2 \u221e\u2211 j=1 wnkj 1(Xj = X \u2217 k)e \u2212wj \u2211 `i(\u03b4`ik\u22121)Z`i  \u00d7 (w\u22171)nkh(X\u22171 )e\u2212w \u2217 1 \u2211 `i(\u03b4`i1\u22121)Z`i\u03bb(w\u22171)dw \u2217 1\n=E e\u2212 \u222b wN(dw,dx)\u2211`i Z`i K\u220f k=2 \u221e\u2211 j=1 wnkj 1(Xj = X \u2217 k)e \u2212wj \u2211 `i(\u03b4`ik\u22121)Z`i  \u00d7 h(X\u22171 ) \u222b (w\u22171) nke\u2212w \u2217 1 \u2211 `i \u03b4`i1Z`i\u03bb(w\u22171)dw \u2217 1\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 14\nNow iteratively pull out terms k = 2, . . . ,K using the same idea, and we get:\n=E [ e\u2212G(X) \u2211 `i Z`i ] K\u220f k=1 h(X\u2217k) \u222b (w\u2217k) nke\u2212w \u2217 k \u2211 `i \u03b4`ikZ`i\u03bb(w\u2217k)dw \u2217 k =e\u2212\u03c8( \u2211 `i Z`i)\nK\u220f k=1 h(X\u2217k)\u03ba ( nk, \u2211 `i \u03b4`ikZ`i ) (31)\nThis completes the proof of Theorem 1."}, {"heading": "B Proof of Theorem 2", "text": "Let f : X\u2192 R be measurable with respect to H . Then the characteristic functional of the posterior G is given by:\nE[e\u2212 \u222b f(x)G(dx)|(Y`, Z`)L`=1] =\nE[e\u2212 \u222b f(x)G(dx)P ((Y`, Z`) L `=1|G)]\nE[P ((Y`, Z`)L`=1|G)] (32)\nThe proof is essentially obtained by calculating the numerator and denominator of (32). The denominator is already given in Theorem 1. The numerator is obtained using the same technique with the inclusion of the term e \u222b f(x)G(dx), which gives:\nE [ e\u2212 \u222b f(x)G(dx)P ((Y`, Z`) L `=1|G) ] =E [ e\u2212 \u222b (f(x)+ \u2211 `i Z`i)G(dx)\n] K\u220f k=1 h(X\u2217k) \u222b (w\u2217k) nke\u2212w \u2217 k(f(X \u2217 k)+ \u2211 `i \u03b4`ikZ`i)\u03bb(w\u2217k)dw \u2217 k\nBy the Le\u0301vy-Khintchine Theorem (using the fact that G has a Poisson process representation N ),\n= exp ( \u2212 \u222b (1\u2212 e\u2212w(f(x)+ \u2211 `i Z`i))\u03bb(w)h(x)dwdx ) \u00d7\nK\u220f k=1 h(X\u2217k) \u222b (w\u2217k) nke\u2212w \u2217 k(f(X \u2217 k)+ \u2211 `i \u03b4`ikZ`i)\u03bb(w\u2217k)dw \u2217 k (33)\nDividing the numerator (31) by the denominator (33), the characteristic functional of the posterior G is: E [ e\u2212 \u222b f(x)G(dx)|(Y`, Z`)L`=1 ] = exp ( \u2212 \u222b (1\u2212 e\u2212wf(x))e\u2212 \u2211 `i Z`i\u03bb(w)h(x)dwdx\n) \u00d7\nK\u220f k=1 h(X\u2217k)\n\u222b e\u2212f(X\n\u2217 k)(w\u2217k) nke\u2212w \u2217 k \u2211 `i \u03b4`ikZ`i\u03bb(w\u2217k)dw \u2217 k\u222b\n(w\u2217k) nke\u2212w \u2217 k \u2211 `i \u03b4`ikZ`i\u03bb(w\u2217k)dw \u2217 k\n(34)\nSince the characteristic functional is the product of K + 1 terms, we see that the posterior G consists of K + 1 independent components, one corresponding to the first term above (G\u2217), and the others corresponding to the K terms in the product over k. Substituting the Le\u0301vy measure \u03bb(w) for a gamma process, we note that the first term shows that G\u2217 is a gamma process with updated inverse scale \u03c4\u2217. The kth term in the product shows that the corresponding component is an atom located at X\u2217k with density (w\u2217k) nke\u2212w \u2217 k \u2211 `i \u03b4`ikZ`i\u03bb(w\u2217k); this is the density of the gamma distribution over w \u2217 k in Theorem 2. This completes the proof.\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 15"}, {"heading": "C Proof of Proposition 4", "text": "We have\nP (Gt(X1k) = 0|wt\u22121,,k) = exp(\u2212\u03c6t\u22121wt\u22121,k)\nAssume that P (Gt(X1k) = 0|wsk) = exp(\u2212yt|swsk)\nthen P (Gt(X1k) = 0|ws\u22121,k) = \u222b exp(\u2212yt|swsk)p(wsk|ws\u22121,k)dwsk\n= \u2211 cs\u22121,k \u222b exp(\u2212yt|swsk)p(wsk|cs\u22121,k)p(cs\u22121,k|ws\u22121,k)dwsk\n= \u2211 cs\u22121,k exp [ \u2212cs\u22121,k log ( 1 +\nyt|s\n\u03c6s\u22121 + \u03c4\n)] p(cs\u22121,k|ws\u22121,k)\n= exp ( \u2212yt|s\u03c6s\u22121 \u03c6s\u22121 + \u03c4 + yt|s ws\u22121,k )"}, {"heading": "D Gibbs sampler for the dynamic nonparametric Plackett-Luce model", "text": "For ease of presentation, we assume that \u03c6t takes the same value \u03c6 at each time step. The Gibbs sampler will iterate between the following steps\n1. a. For t = 1, . . . , T , update Gt(X) given (Gt\u22121(X), \u03b1, \u03c6) b. For t = 1, . . . , T , update (ct, ct\u2217) given (wt, wt\u2217, wt+1, wt+1\u2217, \u03c6, \u03b1)\n2. a. Update \u03b1 given (Z, \u03c6)\nb. For t = 1, . . . , T\nUpdate wt\u2217 given (ct\u22121\u2217, Z, \u03c6, \u03b1)\nUpdate ct\u2217 given (wt\u2217, Z, \u03c6, \u03b1)\n3. For t = 1, . . . , T , update (wt, wt\u2217) given (ct\u22121, ct\u22121\u2217, ct, ct\u2217, Zt, \u03b1, \u03c6)\n4. For t = 1, . . . , T , update Zt given (wt, wt\u2217)\n5. Update \u03c6 given w,w\u2217, \u03b1, \u03c6\nThe steps are now fully described.\n1.a) Sample (Gt(X)) given (\u03b1, \u03c6)\nWe have G1(X)|\u03b1 \u223c Gamma(\u03b1, \u03c4)\nand for t = 1, . . . , T \u2212 1 Gt+1(X) \u223c Gamma(\u03b1+Mt, \u03c4 + \u03c6)\nwhere Mt \u223cPoisson(\u03c6Gt(X)). The weights (wt, wt\u2217) are then appropriately rescaled.\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 16\n1.b) Sample (c, c\u2217) given (w,w\u2217, \u03c6, \u03b1)\nConsider first the sampling of c1:T . We have, for t = 1, . . . , T and k = 1, . . . ,K\np(ctk|wtk, wt+1,k) \u221d p(ctk|wtk)p(wt+1,k|ctk)\nwhere p(ctk|wtk) = Poisson(ctk;\u03c6wtk)\nand\np(wt+1,k|ctk) = { \u03b40(wt+1,k) if wtk = 0 Gamma(wt+1,k; ctk, \u03c4 + \u03c6) if wtk > 0\nHence we can have the following MH update. If wt+1,k > 0, then we necessarily have ctk > 0. We sample c\u2217tk \u223czPoisson(\u03c6wtk) where zPoisson(\u03c6wtk) denotes the zero-truncated Poisson distribution and accept c\u2217tk w.p.\nmin ( 1,\nGamma(wt+1,k; c\u2217tk, \u03c4 + \u03c6) Gamma(wt+1,k; ctk, \u03c4 + \u03c6) ) If wt+1,k = 0, we only have two possible moves: ctk = 0 or ctk = 1, given by the following\nprobabilities\nP (ctk = 0|wt+1,k = 0, wtk) = exp(\u2212\u03c6wtk)\nexp(\u2212\u03c6wtk) + \u03c6wtk exp(\u2212\u03c6wtk)(\u03c4 + \u03c6) =\n1\n1 + \u03c6wtk(\u03c4 + \u03c6)\nP (ctk = 1|wt+1,k = 0, wtk) = \u03c6wtk exp(\u2212\u03c6wtk)(\u03c4 + \u03c6)\nexp(\u2212\u03c6wtk) + \u03c6wtk exp(\u2212\u03c6wtk)(\u03c4 + \u03c6) =\n\u03c6wtk(\u03c4 + \u03c6)\n1 + \u03c6wtk(\u03c4 + \u03c6)\nNote that the above Markov chain is not irreducible, as the probability is zero to go from a state (ctk > 0, wt+1,k > 0) to a state (ctk = 0, wt+1,k = 0), even though the posterior probability of this event is non zero in the case item k does not appear after time t. We can resolve that by the following procedure, that uses a backward forward recursion.\nAssume that item k does not appear after time step \u03c4+k (the same procedure applies if item k does not appear before time step \u03c4\u2212k ). Then we can sample jointly the whole sequence (wk,t, ck,t)t=\u03c4k+1,...,T using the following backward forward recursion.\nLet\nxT = m\u2211 k=1 ZTk (35)\nand for t = T \u2212 1, . . . , \u03c4+k\nxt = m\u2211 k=1 Ztk + \u03c6xt+1 1 + \u03c6+ xt+1\nWe have, for k = 1, . . . ,K and t = \u03c4+k\nctk|(Z, \u03c6,wtk) \u223c Poisson ( 1 + \u03c6\n1 + \u03c6+ xt \u03c6wtk\n) (36)\nwt+1,k|ctk, Z \u223c Gamma (ck,t, \u03c4 + \u03c6+ xt+1) (37)\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 17\n2.a) Sample \u03b1 given (Z, \u03c6)\nWe can sample from the full conditional which is given by\n\u03b1|(Z, \u03b3, \u03c6) \u223c Gamma (a+K, b+ y1 + log(1 + x1)) (38)\nwhere x1 and y1 are obtained with the following recursion\nxT = m\u2211 k=1 ZTk (39)\nyT = 0 (40)\nand for t = T \u2212 1, . . . , 1\nxt = m\u2211 k=1 Ztk + \u03c6xt+1 1 + \u03c6+ xt+1\nyt = yt+1 \u2212 log (\n1 + \u03c6\n1 + \u03c6+ xt+1 ) .\n2.b) Sample (c\u2217, w\u2217) given (Z, \u03c6, \u03b1)\nWe can sample from the full conditional which is given by\nw1\u2217|(Z, \u03c6, \u03b1) \u223c Gamma (\u03b1, \u03c4 + x1) (41)\nwhere x1 is defined above. Then for t = 2, . . . , T , let ct\u22121\u2217|(Z, \u03c6, \u03b1,wt\u22121\u2217) \u223c Poisson ( 1 + \u03c6\n1 + \u03c6+ xt \u03c6wt\u22121\u2217 ) wt\u2217|ct\u22121\u2217, Z, \u03b1 \u223c Gamma (\u03b1+ ct\u22121\u2217, \u03c4 + \u03c6+ xt)\n3) Sample (w,w\u2217) given (Z,\u03b1, c, c\u2217, \u03c6)\nFor each time step t = 1, . . . , T\n\u2022 For each item k = 1, . . . ,K, sample\nwtk|ct\u22121,k, ctk, Zt \u223c Gamma ( ntk + ct\u22121,k + ctk, \u03c4 + 2\u03c6+\nm\u2211 i=1 \u03b4tikZti\n) (42)\nif ctk + ct\u22121,k + ntk > 0, otherwise, set wtk = 0. The occurence indicator \u03b4tik is defined as\n\u03b4tik = { 0 if \u2203j < i with Ytj = X\u2217k ; 1 otherwise.\n(43)\nRR n\u00b0 8140\nBayesian nonparametric models for ranked data 18\n\u2022 Sample the total mass\nwt\u2217|ct\u2217, ct\u22121\u2217, Zt, \u03b1 \u223c Gamma ( \u03b1+ ct\u2217 + ct\u22121\u2217, \u03c4 + 2\u03c6+\nm\u2211 i=1 Zti\n) (44)\n4) Sample Z given (w,w\u2217)\nFor t = 1, . . . , T and i = 1, . . .m, sample\nZti|w,w\u2217 \u223c Exp ( wt\u2217 +\nK\u2211 k=1 \u03b4tikwtk\n) (45)\n5) Sample \u03c6 given w,w\u2217, \u03b1, \u03c6\nWe sample \u03c6 using a MH step. Propose \u03c6\u0303 = \u03c6 exp(\u03c3\u03b5) where \u03c3 > 0 and \u03b5 \u223c N (0, 1). And accept it with probability\nmin ( 1, p(\u03c6\u0303)\np(\u03c6)\n\u03c6\u0303\n\u03c6 T\u22121\u220f t=1 [ p(wt+1\u2217|\u03c6\u0303, wt\u2217) p(wt+1\u2217|\u03c6,wt\u2217) K\u220f k=1 p(wt+1,k|\u03c6\u0303, wtk) p(wt+1,k|\u03c6,wtk) ]) (46)\nRR n\u00b0 8140\nRESEARCH CENTRE BORDEAUX \u2013 SUD-OUEST\n351, Cours de la Lib\u00e9ration B\u00e2timent A 29 33405 Talence Cedex\nPublisher Inria Domaine de Voluceau - Rocquencourt BP 105 - 78153 Le Chesnay Cedex inria.fr\nISSN 0249-6399"}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "We develop a Bayesian nonparametric extension of the popular Plackett-Luce choice model that<lb>can handle an infinite number of choice items. Our framework is based on the theory of random atomic<lb>measures, with the prior specified by a gamma process. We derive a posterior characterization and a simple<lb>and effective Gibbs sampler for posterior simulation. We develop a time-varying extension of our model,<lb>and apply it to the New York Times lists of weekly bestselling books.<lb>Key-words: choice models, generalized Bradley-Terry model, Plackett-Luce model, gamma process,<lb>Markov Chain Monte Carlo \u2217 INRIA Bordeaux Sud-Ouest, Institut de Math\u00e9matiques de Bordeaux, University of Bordeaux, France<lb>\u2020 Department of Statistics, Oxford University, United Kingdom Mod\u00e8les bay\u00e9siens non param\u00e9triques pour les donn\u00e9es de rang<lb>R\u00e9sum\u00e9 : On s\u2019int\u00e9resse dans ce rapport \u00e0 une extension bay\u00e9sienne non param\u00e9trique du mod\u00e8le<lb>de Plackett-Luce pour les donn\u00e9es de rang, pouvant traiter un nombre potentiellement infini d\u2019\u00e9l\u00e9ments.<lb>Notre cadre se base sur la th\u00e9orie des mesures compl\u00e8tement al\u00e9atoires, avec comme a priori un processus<lb>de gamma. Nous d\u00e9rivons une caract\u00e9risation de la loi a posteriori et un \u00e9chantillonneur de Gibbs simple<lb>pour approcher la loi a posteriori. Nous d\u00e9veloppons \u00e9galement une version dynamique de notre mod\u00e8le,<lb>et l\u2019appliquons aux listes hebdomadaires des 20 meilleures ventes du New York Times.<lb>Mots-cl\u00e9s : Mod\u00e8les de choix, mod\u00e8le de Bradley-Terry g\u00e9n\u00e9ralis\u00e9, mod\u00e8le de Plackett-Luce, processus<lb>de gamma, m\u00e9thodes de Monte Carlo par cha\u0131\u0302ne de Markov Bayesian nonparametric models for ranked data<lb>3", "creator": "LaTeX with hyperref package"}}}